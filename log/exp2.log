nohup: ignoring input
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/Upgrade_ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-28 03:39:26.803096: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-28 03:39:26.990178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:06:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-10-28 03:39:26.990260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-28 03:39:27.403288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-28 03:39:27.403372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-28 03:39:27.403388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-28 03:39:27.403545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21568 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:06:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 51 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Found 51 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=149680.72, train_loss=11.91626

Batch 10, train_perplexity=13935.573, train_loss=9.5422

Batch 20, train_perplexity=4294.1436, train_loss=8.365007

Batch 30, train_perplexity=3824.196, train_loss=8.249104

Batch 40, train_perplexity=3328.548, train_loss=8.1102915

Batch 50, train_perplexity=2847.312, train_loss=7.9541306

Batch 60, train_perplexity=2346.9768, train_loss=7.7608833

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 70, train_perplexity=2001.0247, train_loss=7.6014147

Batch 80, train_perplexity=2059.433, train_loss=7.630186

Batch 90, train_perplexity=1996.2206, train_loss=7.599011

Batch 100, train_perplexity=1744.6309, train_loss=7.4642982

Batch 110, train_perplexity=1768.976, train_loss=7.478156

Batch 120, train_perplexity=1590.9498, train_loss=7.3720865

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 130, train_perplexity=1546.3531, train_loss=7.3436546

Batch 140, train_perplexity=1574.4285, train_loss=7.3616476

Batch 150, train_perplexity=1574.1965, train_loss=7.3615003

Batch 160, train_perplexity=1397.5, train_loss=7.24244

Batch 170, train_perplexity=1480.0094, train_loss=7.2998037

Batch 180, train_perplexity=1388.5891, train_loss=7.2360435

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 190, train_perplexity=1354.6068, train_loss=7.2112665

Batch 200, train_perplexity=1289.9224, train_loss=7.1623373

Batch 210, train_perplexity=1344.319, train_loss=7.203643

Batch 220, train_perplexity=1321.4506, train_loss=7.1864853

Batch 230, train_perplexity=1221.0375, train_loss=7.107456

Batch 240, train_perplexity=1180.134, train_loss=7.0733833

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 250, train_perplexity=1109.9398, train_loss=7.012061

Batch 260, train_perplexity=1117.9957, train_loss=7.019293

Batch 270, train_perplexity=1187.4622, train_loss=7.0795736

Batch 280, train_perplexity=1124.7035, train_loss=7.0252748

Batch 290, train_perplexity=1040.1188, train_loss=6.94709

Batch 300, train_perplexity=1124.1185, train_loss=7.0247545

Batch 310, train_perplexity=1170.9714, train_loss=7.065589

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 320, train_perplexity=902.2792, train_loss=6.804924

Batch 330, train_perplexity=1152.2039, train_loss=7.049432

Batch 340, train_perplexity=1021.9172, train_loss=6.9294357

Batch 350, train_perplexity=1007.3651, train_loss=6.9150934

Batch 360, train_perplexity=920.6043, train_loss=6.8250303

Batch 370, train_perplexity=977.24744, train_loss=6.88474

Batch 380, train_perplexity=1040.5746, train_loss=6.9475284

Batch 390, train_perplexity=999.8623, train_loss=6.9076176

Batch 400, train_perplexity=1050.0465, train_loss=6.9565897

Batch 410, train_perplexity=976.74805, train_loss=6.8842287

Batch 420, train_perplexity=904.2394, train_loss=6.807094

Batch 430, train_perplexity=964.5095, train_loss=6.8716197

Batch 440, train_perplexity=856.4988, train_loss=6.752853

Batch 450, train_perplexity=880.3388, train_loss=6.780307

Batch 460, train_perplexity=877.8564, train_loss=6.777483

Batch 470, train_perplexity=948.7175, train_loss=6.855111

Batch 480, train_perplexity=961.44354, train_loss=6.868436

Batch 490, train_perplexity=865.06903, train_loss=6.7628093

Batch 500, train_perplexity=878.08997, train_loss=6.777749

Batch 510, train_perplexity=781.5538, train_loss=6.661284

Batch 520, train_perplexity=939.6654, train_loss=6.845524
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 530, train_perplexity=835.2063, train_loss=6.727679

Batch 540, train_perplexity=819.375, train_loss=6.708542

Batch 550, train_perplexity=773.621, train_loss=6.651082

Batch 560, train_perplexity=987.9021, train_loss=6.8955836

Batch 570, train_perplexity=884.70184, train_loss=6.7852507

Batch 580, train_perplexity=842.9181, train_loss=6.73687

Batch 590, train_perplexity=824.097, train_loss=6.714288

Batch 600, train_perplexity=789.7005, train_loss=6.6716537

Batch 610, train_perplexity=813.803, train_loss=6.7017183

Batch 620, train_perplexity=809.76025, train_loss=6.6967382

Batch 630, train_perplexity=838.56396, train_loss=6.731691

Batch 640, train_perplexity=736.4336, train_loss=6.601819

Batch 650, train_perplexity=800.3843, train_loss=6.685092

Batch 660, train_perplexity=716.13367, train_loss=6.573867

Batch 670, train_perplexity=789.7396, train_loss=6.6717033

Batch 680, train_perplexity=947.43274, train_loss=6.853756

Batch 690, train_perplexity=819.87213, train_loss=6.7091484

Batch 700, train_perplexity=699.4724, train_loss=6.5503263

Batch 710, train_perplexity=831.0981, train_loss=6.722748

Batch 720, train_perplexity=757.8168, train_loss=6.6304417

Batch 730, train_perplexity=767.0752, train_loss=6.642585

Batch 740, train_perplexity=816.7069, train_loss=6.7052803

Batch 750, train_perplexity=732.3483, train_loss=6.5962563

Batch 760, train_perplexity=711.28, train_loss=6.567066

Batch 770, train_perplexity=735.3654, train_loss=6.6003675

Batch 780, train_perplexity=711.4262, train_loss=6.5672717

Batch 790, train_perplexity=683.85065, train_loss=6.5277395

Batch 800, train_perplexity=749.0001, train_loss=6.618739

Batch 810, train_perplexity=716.8867, train_loss=6.574918

Batch 820, train_perplexity=696.6551, train_loss=6.5462904

Batch 830, train_perplexity=747.04974, train_loss=6.616132

Batch 840, train_perplexity=677.694, train_loss=6.518696

Batch 850, train_perplexity=750.47015, train_loss=6.6207

Batch 860, train_perplexity=675.9925, train_loss=6.516182

Batch 870, train_perplexity=720.6293, train_loss=6.580125

Batch 880, train_perplexity=849.4089, train_loss=6.7445407

Batch 890, train_perplexity=650.68054, train_loss=6.4780188

Batch 900, train_perplexity=620.4114, train_loss=6.4303827

Batch 910, train_perplexity=632.257, train_loss=6.449296

Batch 920, train_perplexity=606.8203, train_loss=6.4082327

Batch 930, train_perplexity=708.65295, train_loss=6.563366

Batch 940, train_perplexity=654.1062, train_loss=6.4832697

Batch 950, train_perplexity=683.2721, train_loss=6.526893

Batch 960, train_perplexity=631.3878, train_loss=6.4479203

Batch 970, train_perplexity=683.3441, train_loss=6.5269985

Batch 980, train_perplexity=675.86804, train_loss=6.515998

Batch 990, train_perplexity=677.59314, train_loss=6.518547

Batch 1000, train_perplexity=646.8492, train_loss=6.472113

Batch 1010, train_perplexity=744.09863, train_loss=6.6121736

Batch 1020, train_perplexity=613.02783, train_loss=6.4184103

Batch 1030, train_perplexity=563.97375, train_loss=6.3350077

Batch 1040, train_perplexity=607.953, train_loss=6.4100976

Batch 1050, train_perplexity=728.8993, train_loss=6.5915356

Batch 1060, train_perplexity=649.7262, train_loss=6.476551

Batch 1070, train_perplexity=681.1167, train_loss=6.5237336

Batch 1080, train_perplexity=734.1161, train_loss=6.598667

Batch 1090, train_perplexity=646.6638, train_loss=6.4718266

Batch 1100, train_perplexity=622.99286, train_loss=6.434535

Batch 1110, train_perplexity=636.0129, train_loss=6.455219

Batch 1120, train_perplexity=577.0981, train_loss=6.358012

Batch 1130, train_perplexity=649.7405, train_loss=6.476573

Batch 1140, train_perplexity=660.23615, train_loss=6.4925976

Batch 1150, train_perplexity=629.22986, train_loss=6.4444966

Batch 1160, train_perplexity=600.3581, train_loss=6.3975263

Batch 1170, train_perplexity=697.75116, train_loss=6.5478625

Batch 1180, train_perplexity=588.2306, train_loss=6.377119

Batch 1190, train_perplexity=618.702, train_loss=6.4276237

Batch 1200, train_perplexity=611.6948, train_loss=6.4162335

Batch 1210, train_perplexity=563.7151, train_loss=6.334549

Batch 1220, train_perplexity=588.3035, train_loss=6.377243

Batch 1230, train_perplexity=607.9846, train_loss=6.4101496

Batch 1240, train_perplexity=587.3018, train_loss=6.375539

Batch 1250, train_perplexity=665.4516, train_loss=6.500466

Batch 1260, train_perplexity=573.5006, train_loss=6.351759

Batch 1270, train_perplexity=605.5958, train_loss=6.406213

Batch 1280, train_perplexity=555.4451, train_loss=6.31977

Batch 1290, train_perplexity=597.6201, train_loss=6.3929553

Batch 1300, train_perplexity=670.3897, train_loss=6.507859

Batch 1310, train_perplexity=521.09174, train_loss=6.255926

Batch 1320, train_perplexity=563.08453, train_loss=6.33343

Batch 1330, train_perplexity=568.5624, train_loss=6.343111

Batch 1340, train_perplexity=580.61176, train_loss=6.3640823

Batch 1350, train_perplexity=588.11896, train_loss=6.3769293

Batch 1360, train_perplexity=548.3587, train_loss=6.3069296

Batch 1370, train_perplexity=511.058, train_loss=6.236483

Batch 1380, train_perplexity=520.8421, train_loss=6.255447

Batch 1390, train_perplexity=555.40173, train_loss=6.3196917

Batch 1400, train_perplexity=496.23868, train_loss=6.207057

Batch 1410, train_perplexity=571.9115, train_loss=6.3489842

Batch 1420, train_perplexity=602.94037, train_loss=6.4018183

Batch 1430, train_perplexity=551.0217, train_loss=6.3117743

Batch 1440, train_perplexity=601.9154, train_loss=6.400117

Batch 1450, train_perplexity=482.49973, train_loss=6.1789804

Batch 1460, train_perplexity=555.5845, train_loss=6.3200207

Batch 1470, train_perplexity=506.17233, train_loss=6.226877

Batch 1480, train_perplexity=586.53326, train_loss=6.3742294

Batch 1490, train_perplexity=561.3297, train_loss=6.3303084

Batch 1500, train_perplexity=565.6179, train_loss=6.3379188

Batch 1510, train_perplexity=543.6525, train_loss=6.2983103

Batch 1520, train_perplexity=587.40796, train_loss=6.3757195

Batch 1530, train_perplexity=584.2901, train_loss=6.3703976

Batch 1540, train_perplexity=541.09216, train_loss=6.2935896

Batch 1550, train_perplexity=530.7358, train_loss=6.2742643

Batch 1560, train_perplexity=535.2367, train_loss=6.282709

Batch 1570, train_perplexity=579.6361, train_loss=6.3624005

Batch 1580, train_perplexity=524.7372, train_loss=6.2628975

Batch 1590, train_perplexity=582.2173, train_loss=6.3668437

Batch 1600, train_perplexity=575.47015, train_loss=6.3551874

Batch 1610, train_perplexity=499.00253, train_loss=6.212611

Batch 1620, train_perplexity=553.9375, train_loss=6.317052

Batch 1630, train_perplexity=499.88348, train_loss=6.214375

Batch 1640, train_perplexity=520.64667, train_loss=6.2550716

Batch 1650, train_perplexity=491.79358, train_loss=6.198059

Batch 1660, train_perplexity=541.77527, train_loss=6.2948513

Batch 1670, train_perplexity=505.84515, train_loss=6.2262306

Batch 1680, train_perplexity=534.11725, train_loss=6.2806153

Batch 1690, train_perplexity=524.9539, train_loss=6.2633104

Batch 1700, train_perplexity=481.93292, train_loss=6.177805

Batch 1710, train_perplexity=564.6459, train_loss=6.336199

Batch 1720, train_perplexity=507.56015, train_loss=6.229615

Batch 1730, train_perplexity=543.4174, train_loss=6.297878

Batch 1740, train_perplexity=532.746, train_loss=6.2780447

Batch 1750, train_perplexity=500.26596, train_loss=6.21514

Batch 1760, train_perplexity=544.5954, train_loss=6.300043

Batch 1770, train_perplexity=478.1059, train_loss=6.169832

Batch 1780, train_perplexity=543.56024, train_loss=6.2981405

Batch 1790, train_perplexity=527.87445, train_loss=6.2688584

Batch 1800, train_perplexity=480.07877, train_loss=6.17395

Batch 1810, train_perplexity=524.3812, train_loss=6.262219

Batch 1820, train_perplexity=548.0419, train_loss=6.3063517

Batch 1830, train_perplexity=510.67578, train_loss=6.235735

Batch 1840, train_perplexity=508.36624, train_loss=6.231202

Batch 1850, train_perplexity=502.07715, train_loss=6.218754

Batch 1860, train_perplexity=486.8123, train_loss=6.1878786

Batch 1870, train_perplexity=495.05743, train_loss=6.204674

Batch 1880, train_perplexity=493.03757, train_loss=6.2005854
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 1890, train_perplexity=631.0083, train_loss=6.447319

Batch 1900, train_perplexity=515.1282, train_loss=6.2444158

Batch 1910, train_perplexity=490.13794, train_loss=6.194687

Batch 1920, train_perplexity=483.0087, train_loss=6.1800346

Batch 1930, train_perplexity=472.56552, train_loss=6.1581764

Batch 1940, train_perplexity=495.11053, train_loss=6.204781

Batch 1950, train_perplexity=494.009, train_loss=6.2025537

Batch 1960, train_perplexity=504.3894, train_loss=6.2233486

Batch 1970, train_perplexity=477.9853, train_loss=6.16958

Batch 1980, train_perplexity=422.744, train_loss=6.0467668

Batch 1990, train_perplexity=506.37924, train_loss=6.227286

Batch 2000, train_perplexity=532.78, train_loss=6.2781086

Batch 2010, train_perplexity=559.41626, train_loss=6.326894

Batch 2020, train_perplexity=515.8068, train_loss=6.2457323

Batch 2030, train_perplexity=459.21677, train_loss=6.1295223

Batch 2040, train_perplexity=420.1566, train_loss=6.0406275

Batch 2050, train_perplexity=449.09738, train_loss=6.1072397

Batch 2060, train_perplexity=494.24933, train_loss=6.20304

Batch 2070, train_perplexity=444.17984, train_loss=6.0962296

Batch 2080, train_perplexity=463.30884, train_loss=6.138394

Batch 2090, train_perplexity=435.29068, train_loss=6.076014

Batch 2100, train_perplexity=482.9808, train_loss=6.179977

Batch 2110, train_perplexity=426.25348, train_loss=6.055034

Batch 2120, train_perplexity=472.00027, train_loss=6.1569796

Batch 2130, train_perplexity=425.26578, train_loss=6.0527143

Batch 2140, train_perplexity=469.65686, train_loss=6.1520023

Batch 2150, train_perplexity=484.93588, train_loss=6.1840167

Batch 2160, train_perplexity=482.93155, train_loss=6.179875

Batch 2170, train_perplexity=485.24054, train_loss=6.1846447

Batch 2180, train_perplexity=443.08408, train_loss=6.0937595

Batch 2190, train_perplexity=497.62628, train_loss=6.2098494

Batch 2200, train_perplexity=478.862, train_loss=6.1714125

Batch 2210, train_perplexity=431.66748, train_loss=6.0676556

Batch 2220, train_perplexity=531.1287, train_loss=6.2750044

Batch 2230, train_perplexity=451.9882, train_loss=6.113656

Batch 2240, train_perplexity=442.62036, train_loss=6.0927124

Batch 2250, train_perplexity=477.9887, train_loss=6.169587

Batch 2260, train_perplexity=445.58972, train_loss=6.0993986

Batch 2270, train_perplexity=446.37146, train_loss=6.1011515

Batch 2280, train_perplexity=482.44705, train_loss=6.178871

Batch 2290, train_perplexity=493.23322, train_loss=6.200982

Batch 2300, train_perplexity=460.01144, train_loss=6.1312513

Batch 2310, train_perplexity=484.55682, train_loss=6.1832347

Batch 2320, train_perplexity=401.12274, train_loss=5.9942675

Batch 2330, train_perplexity=461.10092, train_loss=6.133617

Batch 2340, train_perplexity=470.56787, train_loss=6.15394

Batch 2350, train_perplexity=506.25247, train_loss=6.2270355

Batch 2360, train_perplexity=455.30777, train_loss=6.1209736

Batch 2370, train_perplexity=414.9938, train_loss=6.0282636

Batch 2380, train_perplexity=477.10294, train_loss=6.1677322

Batch 2390, train_perplexity=473.00198, train_loss=6.1590996

Batch 2400, train_perplexity=457.95938, train_loss=6.1267805

Batch 2410, train_perplexity=466.8952, train_loss=6.146105

Batch 2420, train_perplexity=424.69937, train_loss=6.0513816

Batch 2430, train_perplexity=429.49973, train_loss=6.062621

Batch 2440, train_perplexity=502.0968, train_loss=6.218793

Batch 2450, train_perplexity=483.337, train_loss=6.180714

Batch 2460, train_perplexity=423.087, train_loss=6.047578

Batch 2470, train_perplexity=432.89847, train_loss=6.070503

Batch 2480, train_perplexity=522.23303, train_loss=6.258114

Batch 2490, train_perplexity=500.9986, train_loss=6.2166033

Batch 2500, train_perplexity=456.95163, train_loss=6.1245775

Batch 2510, train_perplexity=435.87286, train_loss=6.0773506

Batch 2520, train_perplexity=454.702, train_loss=6.1196423

Batch 2530, train_perplexity=517.7836, train_loss=6.2495575

Batch 2540, train_perplexity=416.7585, train_loss=6.032507

Batch 2550, train_perplexity=397.56946, train_loss=5.9853697

Batch 2560, train_perplexity=369.63132, train_loss=5.912506

Batch 2570, train_perplexity=417.58603, train_loss=6.0344906

Batch 2580, train_perplexity=464.93747, train_loss=6.141903

Batch 2590, train_perplexity=391.92157, train_loss=5.9710617

Batch 2600, train_perplexity=497.03745, train_loss=6.2086654

Batch 2610, train_perplexity=436.21344, train_loss=6.0781317

Batch 2620, train_perplexity=388.793, train_loss=5.963047

Batch 2630, train_perplexity=453.8111, train_loss=6.117681

Batch 2640, train_perplexity=424.7721, train_loss=6.051553

Batch 2650, train_perplexity=419.14627, train_loss=6.03822

Batch 2660, train_perplexity=400.6374, train_loss=5.993057

Batch 2670, train_perplexity=501.16058, train_loss=6.2169266

Batch 2680, train_perplexity=395.91412, train_loss=5.9811974

Batch 2690, train_perplexity=430.227, train_loss=6.064313

Batch 2700, train_perplexity=443.05258, train_loss=6.0936885

Batch 2710, train_perplexity=382.20795, train_loss=5.945965

Batch 2720, train_perplexity=353.97406, train_loss=5.8692236

Batch 2730, train_perplexity=407.04764, train_loss=6.00893

Batch 2740, train_perplexity=463.80487, train_loss=6.139464

Batch 2750, train_perplexity=444.3061, train_loss=6.0965137

Batch 2760, train_perplexity=412.87552, train_loss=6.023146

Batch 2770, train_perplexity=385.39893, train_loss=5.954279

Batch 2780, train_perplexity=444.39508, train_loss=6.096714

Batch 2790, train_perplexity=396.88077, train_loss=5.983636

Batch 2800, train_perplexity=404.1732, train_loss=6.0018435

Batch 2810, train_perplexity=440.09866, train_loss=6.086999

Batch 2820, train_perplexity=433.62775, train_loss=6.0721865

Batch 2830, train_perplexity=423.97562, train_loss=6.049676

Batch 2840, train_perplexity=384.58896, train_loss=5.952175

Batch 2850, train_perplexity=442.4606, train_loss=6.0923514

Batch 2860, train_perplexity=442.90665, train_loss=6.093359

Batch 2870, train_perplexity=439.76385, train_loss=6.086238

Batch 2880, train_perplexity=447.04797, train_loss=6.102666

Batch 2890, train_perplexity=384.54385, train_loss=5.952058

Batch 2900, train_perplexity=458.80396, train_loss=6.128623

Batch 2910, train_perplexity=415.0308, train_loss=6.0283527

Batch 2920, train_perplexity=392.89755, train_loss=5.973549

Batch 2930, train_perplexity=433.12848, train_loss=6.0710344

Batch 2940, train_perplexity=361.92313, train_loss=5.891432

Batch 2950, train_perplexity=383.18057, train_loss=5.9485064

Batch 2960, train_perplexity=398.9847, train_loss=5.988923

Batch 2970, train_perplexity=396.5675, train_loss=5.9828463

Batch 2980, train_perplexity=388.1591, train_loss=5.9614153

Batch 2990, train_perplexity=404.35266, train_loss=6.0022874

Batch 3000, train_perplexity=411.96442, train_loss=6.020937

Batch 3010, train_perplexity=400.56155, train_loss=5.9928675

Batch 3020, train_perplexity=383.59335, train_loss=5.949583

Batch 3030, train_perplexity=452.49323, train_loss=6.114773

Batch 3040, train_perplexity=407.2567, train_loss=6.0094438

Batch 3050, train_perplexity=376.22153, train_loss=5.930178

Batch 3060, train_perplexity=375.2467, train_loss=5.9275837

Batch 3070, train_perplexity=444.00748, train_loss=6.0958414

Batch 3080, train_perplexity=417.45422, train_loss=6.034175

Batch 3090, train_perplexity=422.37064, train_loss=6.045883

Batch 3100, train_perplexity=371.92105, train_loss=5.9186816

Batch 3110, train_perplexity=344.85248, train_loss=5.8431168

Batch 3120, train_perplexity=432.67932, train_loss=6.069997

Batch 3130, train_perplexity=442.94528, train_loss=6.0934463

Batch 3140, train_perplexity=445.506, train_loss=6.0992107

Batch 3150, train_perplexity=416.7454, train_loss=6.0324755

Batch 3160, train_perplexity=438.3018, train_loss=6.0829077

Batch 3170, train_perplexity=413.5537, train_loss=6.0247874

Batch 3180, train_perplexity=371.96487, train_loss=5.9187994

Batch 3190, train_perplexity=367.45117, train_loss=5.9065905

Batch 3200, train_perplexity=403.63315, train_loss=6.0005064

Batch 3210, train_perplexity=438.68567, train_loss=6.083783

Batch 3220, train_perplexity=441.59073, train_loss=6.0903835

Batch 3230, train_perplexity=418.47424, train_loss=6.0366154
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3240, train_perplexity=414.0018, train_loss=6.0258703

Batch 3250, train_perplexity=376.83325, train_loss=5.9318027

Batch 3260, train_perplexity=404.9174, train_loss=6.003683

Batch 3270, train_perplexity=414.28833, train_loss=6.026562

Batch 3280, train_perplexity=384.68103, train_loss=5.9524145

Batch 3290, train_perplexity=339.2455, train_loss=5.826724

Batch 3300, train_perplexity=402.75012, train_loss=5.9983163

Batch 3310, train_perplexity=395.8171, train_loss=5.9809523

Batch 3320, train_perplexity=382.75143, train_loss=5.947386

Batch 3330, train_perplexity=363.69775, train_loss=5.896323

Batch 3340, train_perplexity=368.65686, train_loss=5.9098663

Batch 3350, train_perplexity=386.59067, train_loss=5.9573665

Batch 3360, train_perplexity=401.4737, train_loss=5.995142

Batch 3370, train_perplexity=400.90952, train_loss=5.993736

Batch 3380, train_perplexity=393.74377, train_loss=5.9757004

Batch 3390, train_perplexity=383.5491, train_loss=5.9494677

Batch 3400, train_perplexity=416.16495, train_loss=6.0310817

Batch 3410, train_perplexity=357.13123, train_loss=5.8781033

Batch 3420, train_perplexity=317.7512, train_loss=5.7612686

Batch 3430, train_perplexity=417.8968, train_loss=6.0352345

Batch 3440, train_perplexity=426.9176, train_loss=6.056591

Batch 3450, train_perplexity=326.74057, train_loss=5.7891665

Batch 3460, train_perplexity=422.10263, train_loss=6.0452485

Batch 3470, train_perplexity=396.08597, train_loss=5.9816313

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 3480, train_perplexity=380.52728, train_loss=5.941558

Batch 3490, train_perplexity=377.82697, train_loss=5.9344363

Batch 3500, train_perplexity=363.55176, train_loss=5.8959217

Batch 3510, train_perplexity=389.94968, train_loss=5.9660177

Batch 3520, train_perplexity=351.9866, train_loss=5.863593

Batch 3530, train_perplexity=405.90717, train_loss=6.0061245

Batch 3540, train_perplexity=403.96085, train_loss=6.001318

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 3550, train_perplexity=362.65836, train_loss=5.893461

Batch 3560, train_perplexity=354.09103, train_loss=5.869554

Batch 3570, train_perplexity=367.2522, train_loss=5.906049

Batch 3580, train_perplexity=361.07056, train_loss=5.8890734

Batch 3590, train_perplexity=356.80133, train_loss=5.877179

Batch 3600, train_perplexity=318.52014, train_loss=5.7636857

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 3610, train_perplexity=361.41644, train_loss=5.890031

Batch 3620, train_perplexity=367.50024, train_loss=5.906724

Batch 3630, train_perplexity=351.7757, train_loss=5.8629937

Batch 3640, train_perplexity=377.0004, train_loss=5.932246

Batch 3650, train_perplexity=363.0539, train_loss=5.8945513

Batch 3660, train_perplexity=326.70053, train_loss=5.789044

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 3670, train_perplexity=338.14825, train_loss=5.8234844

Batch 3680, train_perplexity=360.2212, train_loss=5.8867183

Batch 3690, train_perplexity=335.7341, train_loss=5.8163195

Batch 3700, train_perplexity=345.38602, train_loss=5.8446627

Batch 3710, train_perplexity=359.7411, train_loss=5.8853846

Batch 3720, train_perplexity=395.9185, train_loss=5.9812083

Batch 3730, train_perplexity=373.21597, train_loss=5.9221573

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 3740, train_perplexity=383.10312, train_loss=5.948304

Batch 3750, train_perplexity=378.87515, train_loss=5.9372067

Batch 3760, train_perplexity=443.8411, train_loss=6.0954666

Batch 3770, train_perplexity=375.3648, train_loss=5.9278984

Batch 3780, train_perplexity=351.50925, train_loss=5.862236

Batch 3790, train_perplexity=340.8797, train_loss=5.8315296

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 3800, train_perplexity=384.63428, train_loss=5.952293

Batch 3810, train_perplexity=361.1556, train_loss=5.889309

Batch 3820, train_perplexity=353.35428, train_loss=5.867471

Batch 3830, train_perplexity=385.84482, train_loss=5.9554353

Batch 3840, train_perplexity=380.0085, train_loss=5.9401937

Batch 3850, train_perplexity=326.55023, train_loss=5.7885838

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 3860, train_perplexity=378.09515, train_loss=5.935146

Batch 3870, train_perplexity=357.63632, train_loss=5.8795166

Batch 3880, train_perplexity=369.2723, train_loss=5.9115343

Batch 3890, train_perplexity=370.06445, train_loss=5.913677

Batch 3900, train_perplexity=362.76385, train_loss=5.893752

Batch 3910, train_perplexity=336.38773, train_loss=5.8182645

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 3920, train_perplexity=385.04147, train_loss=5.953351

Batch 3930, train_perplexity=436.202, train_loss=6.0781054

Batch 3940, train_perplexity=379.03253, train_loss=5.937622

Batch 3950, train_perplexity=377.87164, train_loss=5.9345546

Batch 3960, train_perplexity=373.97736, train_loss=5.9241953

Batch 3970, train_perplexity=341.75662, train_loss=5.834099

Batch 3980, train_perplexity=348.7841, train_loss=5.854453

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 3990, train_perplexity=360.67993, train_loss=5.887991

Batch 4000, train_perplexity=367.92703, train_loss=5.9078846

Batch 4010, train_perplexity=364.4281, train_loss=5.8983293

Batch 4020, train_perplexity=382.403, train_loss=5.946475

Batch 4030, train_perplexity=358.0653, train_loss=5.8807154

Batch 4040, train_perplexity=349.33536, train_loss=5.8560324

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 4050, train_perplexity=376.3482, train_loss=5.930515

Batch 4060, train_perplexity=330.03577, train_loss=5.799201

Batch 4070, train_perplexity=372.32794, train_loss=5.919775

Batch 4080, train_perplexity=346.70972, train_loss=5.848488

Batch 4090, train_perplexity=365.86856, train_loss=5.902274

Batch 4100, train_perplexity=400.5251, train_loss=5.9927764

Batch 4110, train_perplexity=382.90057, train_loss=5.9477754

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 4120, train_perplexity=339.01007, train_loss=5.82603

Batch 4130, train_perplexity=317.52792, train_loss=5.7605658

Batch 4140, train_perplexity=363.46234, train_loss=5.8956757

Batch 4150, train_perplexity=389.85748, train_loss=5.965781

Batch 4160, train_perplexity=337.31213, train_loss=5.8210087

Batch 4170, train_perplexity=319.61023, train_loss=5.7671022

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 4180, train_perplexity=364.46024, train_loss=5.8984175

Batch 4190, train_perplexity=347.88535, train_loss=5.851873

Batch 4200, train_perplexity=331.70767, train_loss=5.804254

Batch 4210, train_perplexity=354.74286, train_loss=5.871393

Batch 4220, train_perplexity=340.77277, train_loss=5.831216

Batch 4230, train_perplexity=356.9402, train_loss=5.8775682

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 4240, train_perplexity=369.86722, train_loss=5.913144

Batch 4250, train_perplexity=327.5597, train_loss=5.7916703

Batch 4260, train_perplexity=343.00278, train_loss=5.8377385

Batch 4270, train_perplexity=409.27533, train_loss=6.014388

Batch 4280, train_perplexity=342.34607, train_loss=5.835822

Batch 4290, train_perplexity=344.6271, train_loss=5.842463

Batch 4300, train_perplexity=375.09213, train_loss=5.9271717

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 4310, train_perplexity=343.5054, train_loss=5.839203

Batch 4320, train_perplexity=343.0703, train_loss=5.8379354

Batch 4330, train_perplexity=314.85837, train_loss=5.752123

Batch 4340, train_perplexity=329.30618, train_loss=5.796988

Batch 4350, train_perplexity=392.02023, train_loss=5.9713135

Batch 4360, train_perplexity=329.9522, train_loss=5.798948

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 4370, train_perplexity=340.96878, train_loss=5.831791

Batch 4380, train_perplexity=355.06476, train_loss=5.8723

Batch 4390, train_perplexity=335.8292, train_loss=5.8166027

Batch 4400, train_perplexity=322.50626, train_loss=5.7761226

Batch 4410, train_perplexity=351.31387, train_loss=5.86168

Batch 4420, train_perplexity=358.33005, train_loss=5.8814545

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 4430, train_perplexity=347.77173, train_loss=5.8515463

Batch 4440, train_perplexity=344.63583, train_loss=5.8424883

Batch 4450, train_perplexity=352.27338, train_loss=5.8644075

Batch 4460, train_perplexity=335.92563, train_loss=5.81689

Batch 4470, train_perplexity=293.113, train_loss=5.680558

Batch 4480, train_perplexity=363.75604, train_loss=5.8964834

Batch 4490, train_perplexity=339.373, train_loss=5.8271

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 4500, train_perplexity=333.5162, train_loss=5.8096914

Batch 4510, train_perplexity=358.2101, train_loss=5.8811197

Batch 4520, train_perplexity=338.68756, train_loss=5.825078

Batch 4530, train_perplexity=318.41672, train_loss=5.763361

Batch 4540, train_perplexity=345.16788, train_loss=5.844031

Batch 4550, train_perplexity=344.04276, train_loss=5.840766

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 4560, train_perplexity=350.88477, train_loss=5.860458

Batch 4570, train_perplexity=338.15085, train_loss=5.823492

Batch 4580, train_perplexity=311.07382, train_loss=5.7400303

Batch 4590, train_perplexity=326.28018, train_loss=5.7877564

Batch 4600, train_perplexity=341.82343, train_loss=5.8342943

Batch 4610, train_perplexity=315.83395, train_loss=5.7552166

Batch 4620, train_perplexity=346.79868, train_loss=5.8487444

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6255 sentences.
Finished loading
Batch 4630, train_perplexity=331.5127, train_loss=5.803666

Batch 4640, train_perplexity=325.1703, train_loss=5.784349

Batch 4650, train_perplexity=329.3307, train_loss=5.7970624

Batch 4660, train_perplexity=332.62283, train_loss=5.807009

Batch 4670, train_perplexity=354.4686, train_loss=5.87062

Batch 4680, train_perplexity=314.32642, train_loss=5.750432

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 4690, train_perplexity=358.57822, train_loss=5.882147

Batch 4700, train_perplexity=308.16214, train_loss=5.730626

Batch 4710, train_perplexity=378.87372, train_loss=5.937203

Batch 4720, train_perplexity=354.06183, train_loss=5.8694715

Batch 4730, train_perplexity=336.14258, train_loss=5.8175354

Batch 4740, train_perplexity=359.43347, train_loss=5.884529

Batch 4750, train_perplexity=315.51184, train_loss=5.754196

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 4760, train_perplexity=352.1471, train_loss=5.864049

Batch 4770, train_perplexity=334.11502, train_loss=5.8114853

Batch 4780, train_perplexity=309.76535, train_loss=5.735815

Batch 4790, train_perplexity=334.73837, train_loss=5.8133492

Batch 4800, train_perplexity=331.91098, train_loss=5.804867

Batch 4810, train_perplexity=377.19028, train_loss=5.9327497

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 4820, train_perplexity=358.12402, train_loss=5.8808794

Batch 4830, train_perplexity=345.64978, train_loss=5.845426

Batch 4840, train_perplexity=344.29254, train_loss=5.8414917

Batch 4850, train_perplexity=370.3877, train_loss=5.9145503

Batch 4860, train_perplexity=379.9152, train_loss=5.939948

Batch 4870, train_perplexity=326.29886, train_loss=5.7878137

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 4880, train_perplexity=332.8964, train_loss=5.8078313

Batch 4890, train_perplexity=338.06024, train_loss=5.823224

Batch 4900, train_perplexity=388.9569, train_loss=5.9634686

Batch 4910, train_perplexity=320.1795, train_loss=5.768882

Batch 4920, train_perplexity=319.64346, train_loss=5.767206

Batch 4930, train_perplexity=320.63327, train_loss=5.770298

Batch 4940, train_perplexity=328.82715, train_loss=5.795532

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 4950, train_perplexity=321.94943, train_loss=5.7743945

Batch 4960, train_perplexity=294.48804, train_loss=5.6852384

Batch 4970, train_perplexity=335.65054, train_loss=5.8160706

Batch 4980, train_perplexity=333.47168, train_loss=5.809558

Batch 4990, train_perplexity=328.35162, train_loss=5.794085

Batch 5000, train_perplexity=306.4927, train_loss=5.725194

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 5010, train_perplexity=307.2199, train_loss=5.727564

Batch 5020, train_perplexity=304.02377, train_loss=5.717106

Batch 5030, train_perplexity=333.01846, train_loss=5.808198

Batch 5040, train_perplexity=309.859, train_loss=5.7361174

Batch 5050, train_perplexity=356.58975, train_loss=5.876586

Batch 5060, train_perplexity=313.79913, train_loss=5.748753

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 5070, train_perplexity=320.19354, train_loss=5.7689257

Batch 5080, train_perplexity=295.7079, train_loss=5.689372

Batch 5090, train_perplexity=331.27124, train_loss=5.8029375

Batch 5100, train_perplexity=338.54062, train_loss=5.824644

Batch 5110, train_perplexity=312.31204, train_loss=5.744003

Batch 5120, train_perplexity=338.4854, train_loss=5.824481

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 5130, train_perplexity=339.48404, train_loss=5.827427

Batch 5140, train_perplexity=343.60077, train_loss=5.8394804

Batch 5150, train_perplexity=284.93817, train_loss=5.652272

Batch 5160, train_perplexity=295.3208, train_loss=5.688062

Batch 5170, train_perplexity=306.90982, train_loss=5.726554

Batch 5180, train_perplexity=329.2426, train_loss=5.796795

Batch 5190, train_perplexity=367.74512, train_loss=5.90739

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 5200, train_perplexity=328.9551, train_loss=5.7959213

Batch 5210, train_perplexity=300.55774, train_loss=5.70564

Batch 5220, train_perplexity=340.29114, train_loss=5.8298016

Batch 5230, train_perplexity=321.2926, train_loss=5.772352

Batch 5240, train_perplexity=380.7271, train_loss=5.942083

Batch 5250, train_perplexity=295.05658, train_loss=5.687167

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 5260, train_perplexity=342.98264, train_loss=5.83768

Batch 5270, train_perplexity=345.2803, train_loss=5.8443565

Batch 5280, train_perplexity=318.9048, train_loss=5.7648926

Batch 5290, train_perplexity=318.77798, train_loss=5.764495

Batch 5300, train_perplexity=347.20206, train_loss=5.849907

Batch 5310, train_perplexity=328.8052, train_loss=5.7954655

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 5320, train_perplexity=327.36905, train_loss=5.791088

Batch 5330, train_perplexity=285.2084, train_loss=5.65322

Batch 5340, train_perplexity=340.52618, train_loss=5.830492

Batch 5350, train_perplexity=311.76346, train_loss=5.7422447

Batch 5360, train_perplexity=328.3936, train_loss=5.794213

Batch 5370, train_perplexity=335.65582, train_loss=5.8160863

Batch 5380, train_perplexity=307.65973, train_loss=5.7289944

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 5390, train_perplexity=281.83148, train_loss=5.6413093

Batch 5400, train_perplexity=313.39017, train_loss=5.747449

Batch 5410, train_perplexity=339.25262, train_loss=5.826745

Batch 5420, train_perplexity=307.97647, train_loss=5.7300234

Batch 5430, train_perplexity=318.43677, train_loss=5.763424

Batch 5440, train_perplexity=296.03125, train_loss=5.690465

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 5450, train_perplexity=348.01624, train_loss=5.852249

Batch 5460, train_perplexity=307.33817, train_loss=5.7279487

Batch 5470, train_perplexity=302.2055, train_loss=5.7111073

Batch 5480, train_perplexity=310.53644, train_loss=5.7383013

Batch 5490, train_perplexity=308.21448, train_loss=5.730796

Batch 5500, train_perplexity=277.4704, train_loss=5.6257143

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 5510, train_perplexity=344.98456, train_loss=5.8434997

Batch 5520, train_perplexity=322.16458, train_loss=5.7750626

Batch 5530, train_perplexity=295.50772, train_loss=5.688695

Batch 5540, train_perplexity=325.90094, train_loss=5.7865934

Batch 5550, train_perplexity=282.3255, train_loss=5.6430607

Batch 5560, train_perplexity=310.06888, train_loss=5.7367945

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 5570, train_perplexity=320.13446, train_loss=5.768741

Batch 5580, train_perplexity=337.62784, train_loss=5.821944

Batch 5590, train_perplexity=366.0989, train_loss=5.9029036

Batch 5600, train_perplexity=321.201, train_loss=5.772067

Batch 5610, train_perplexity=304.2873, train_loss=5.7179723

Batch 5620, train_perplexity=347.14642, train_loss=5.8497467

Batch 5630, train_perplexity=359.03503, train_loss=5.88342

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 5640, train_perplexity=317.62332, train_loss=5.760866

Batch 5650, train_perplexity=288.6298, train_loss=5.665145

Batch 5660, train_perplexity=298.29813, train_loss=5.6980934

Batch 5670, train_perplexity=324.75485, train_loss=5.7830706

Batch 5680, train_perplexity=297.87796, train_loss=5.696684

Batch 5690, train_perplexity=313.29813, train_loss=5.747155

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 5700, train_perplexity=298.7209, train_loss=5.6995096

Batch 5710, train_perplexity=323.30878, train_loss=5.778608

Batch 5720, train_perplexity=310.27505, train_loss=5.737459

Batch 5730, train_perplexity=288.5734, train_loss=5.6649494

Batch 5740, train_perplexity=312.01984, train_loss=5.743067

Batch 5750, train_perplexity=353.1353, train_loss=5.8668513

Batch 5760, train_perplexity=273.50308, train_loss=5.611313

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 5770, train_perplexity=281.34824, train_loss=5.639593

Batch 5780, train_perplexity=309.49, train_loss=5.7349257

Batch 5790, train_perplexity=302.17883, train_loss=5.711019

Batch 5800, train_perplexity=295.64316, train_loss=5.689153

Batch 5810, train_perplexity=265.04114, train_loss=5.579885

Batch 5820, train_perplexity=301.85468, train_loss=5.7099457

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 5830, train_perplexity=305.65076, train_loss=5.722443

Batch 5840, train_perplexity=301.18323, train_loss=5.707719

Batch 5850, train_perplexity=315.13022, train_loss=5.752986

Batch 5860, train_perplexity=287.71957, train_loss=5.6619864

Batch 5870, train_perplexity=318.23505, train_loss=5.76279

Batch 5880, train_perplexity=342.6652, train_loss=5.836754

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 5890, train_perplexity=326.00507, train_loss=5.786913

Batch 5900, train_perplexity=306.1048, train_loss=5.7239275

Batch 5910, train_perplexity=304.23376, train_loss=5.7177963

Batch 5920, train_perplexity=320.7091, train_loss=5.7705345
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 5930, train_perplexity=291.13834, train_loss=5.6737986

Batch 5940, train_perplexity=321.7754, train_loss=5.773854

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 5950, train_perplexity=283.23676, train_loss=5.646283

Batch 5960, train_perplexity=277.39502, train_loss=5.6254425

Batch 5970, train_perplexity=348.28085, train_loss=5.853009

Batch 5980, train_perplexity=308.801, train_loss=5.732697

Batch 5990, train_perplexity=285.16122, train_loss=5.6530547

Batch 6000, train_perplexity=315.67978, train_loss=5.7547283

Batch 6010, train_perplexity=279.41257, train_loss=5.6326895

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 6020, train_perplexity=303.4298, train_loss=5.7151504

Batch 6030, train_perplexity=323.83014, train_loss=5.780219

Batch 6040, train_perplexity=279.54425, train_loss=5.6331606

Batch 6050, train_perplexity=276.11847, train_loss=5.62083

Batch 6060, train_perplexity=303.20117, train_loss=5.7143965

Batch 6070, train_perplexity=308.03552, train_loss=5.730215

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 6080, train_perplexity=300.961, train_loss=5.7069807

Batch 6090, train_perplexity=316.68494, train_loss=5.7579074

Batch 6100, train_perplexity=289.34995, train_loss=5.667637

Batch 6110, train_perplexity=279.91852, train_loss=5.6344986

Batch 6120, train_perplexity=278.29883, train_loss=5.6286955

Batch 6130, train_perplexity=310.0352, train_loss=5.7366858

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 6140, train_perplexity=303.7476, train_loss=5.716197

Batch 6150, train_perplexity=311.1394, train_loss=5.740241

Batch 6160, train_perplexity=321.0174, train_loss=5.7714953

Batch 6170, train_perplexity=298.86963, train_loss=5.7000074

Batch 6180, train_perplexity=297.8078, train_loss=5.6964483

Batch 6190, train_perplexity=292.01636, train_loss=5.67681

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 6200, train_perplexity=318.94614, train_loss=5.7650223

Batch 6210, train_perplexity=328.73825, train_loss=5.795262

Batch 6220, train_perplexity=295.4122, train_loss=5.6883717

Batch 6230, train_perplexity=288.15646, train_loss=5.6635036

Batch 6240, train_perplexity=306.26453, train_loss=5.724449

Batch 6250, train_perplexity=349.32803, train_loss=5.8560114

Batch 6260, train_perplexity=296.97525, train_loss=5.693649

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 6270, train_perplexity=299.4198, train_loss=5.7018466

Batch 6280, train_perplexity=280.11295, train_loss=5.635193

Batch 6290, train_perplexity=306.32703, train_loss=5.7246532

Batch 6300, train_perplexity=309.52158, train_loss=5.735028

Batch 6310, train_perplexity=294.58945, train_loss=5.6855826

Batch 6320, train_perplexity=309.27267, train_loss=5.7342234

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 6330, train_perplexity=318.63376, train_loss=5.7640424

Batch 6340, train_perplexity=296.31284, train_loss=5.691416

Batch 6350, train_perplexity=289.75946, train_loss=5.669051

Batch 6360, train_perplexity=277.9808, train_loss=5.627552

Batch 6370, train_perplexity=272.31833, train_loss=5.6069717

Batch 6380, train_perplexity=299.67133, train_loss=5.7026863

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 6390, train_perplexity=263.9444, train_loss=5.5757384

Batch 6400, train_perplexity=305.76663, train_loss=5.722822

Batch 6410, train_perplexity=311.22012, train_loss=5.7405005

Batch 6420, train_perplexity=295.55704, train_loss=5.688862

Batch 6430, train_perplexity=298.6206, train_loss=5.699174

Batch 6440, train_perplexity=306.7047, train_loss=5.7258854

Batch 6450, train_perplexity=257.38852, train_loss=5.5505867

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 6460, train_perplexity=294.87262, train_loss=5.6865435

Batch 6470, train_perplexity=261.4256, train_loss=5.5661497

Batch 6480, train_perplexity=291.01593, train_loss=5.673378

Batch 6490, train_perplexity=315.61716, train_loss=5.75453

Batch 6500, train_perplexity=345.81052, train_loss=5.845891

Batch 6510, train_perplexity=277.0265, train_loss=5.624113

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 6520, train_perplexity=305.31528, train_loss=5.721345

Batch 6530, train_perplexity=318.7567, train_loss=5.764428

Batch 6540, train_perplexity=349.10822, train_loss=5.855382

Batch 6550, train_perplexity=265.81458, train_loss=5.582799

Batch 6560, train_perplexity=296.48526, train_loss=5.6919975

Batch 6570, train_perplexity=349.4973, train_loss=5.856496
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 6580, train_perplexity=282.27652, train_loss=5.642887

Batch 6590, train_perplexity=248.01938, train_loss=5.513507

Batch 6600, train_perplexity=276.54236, train_loss=5.622364

Batch 6610, train_perplexity=387.38196, train_loss=5.959411

Batch 6620, train_perplexity=294.5039, train_loss=5.6852922

Batch 6630, train_perplexity=295.89435, train_loss=5.6900024

Batch 6640, train_perplexity=280.58618, train_loss=5.636881

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 6650, train_perplexity=232.14473, train_loss=5.447361

Batch 6660, train_perplexity=299.2665, train_loss=5.7013345

Batch 6670, train_perplexity=306.28467, train_loss=5.724515

Batch 6680, train_perplexity=288.10922, train_loss=5.6633396

Batch 6690, train_perplexity=293.91467, train_loss=5.6832895

Batch 6700, train_perplexity=284.3228, train_loss=5.6501102

Batch 6710, train_perplexity=302.0839, train_loss=5.710705

Batch 6720, train_perplexity=310.68738, train_loss=5.738787

Batch 6730, train_perplexity=284.84036, train_loss=5.651929

Batch 6740, train_perplexity=250.43445, train_loss=5.523197

Batch 6750, train_perplexity=317.5682, train_loss=5.7606926

Batch 6760, train_perplexity=319.42618, train_loss=5.766526

Batch 6770, train_perplexity=288.78827, train_loss=5.6656938

Batch 6780, train_perplexity=279.46774, train_loss=5.632887

Batch 6790, train_perplexity=282.2566, train_loss=5.6428165

Batch 6800, train_perplexity=282.89365, train_loss=5.645071

Batch 6810, train_perplexity=254.63489, train_loss=5.5398307

Batch 6820, train_perplexity=296.3688, train_loss=5.6916046

Batch 6830, train_perplexity=268.10895, train_loss=5.5913935

Batch 6840, train_perplexity=317.02307, train_loss=5.7589746

Batch 6850, train_perplexity=308.3309, train_loss=5.7311735

Batch 6860, train_perplexity=279.9591, train_loss=5.6346436

Batch 6870, train_perplexity=290.0275, train_loss=5.6699758

Batch 6880, train_perplexity=271.3687, train_loss=5.6034784

Batch 6890, train_perplexity=294.24673, train_loss=5.6844187

Batch 6900, train_perplexity=274.15594, train_loss=5.613697

Batch 6910, train_perplexity=286.52875, train_loss=5.657839

Batch 6920, train_perplexity=260.09714, train_loss=5.561055

Batch 6930, train_perplexity=276.60318, train_loss=5.622584

Batch 6940, train_perplexity=280.5851, train_loss=5.636877

Batch 6950, train_perplexity=289.53546, train_loss=5.6682777

Batch 6960, train_perplexity=296.56247, train_loss=5.692258

Batch 6970, train_perplexity=263.23663, train_loss=5.5730534

Batch 6980, train_perplexity=252.08374, train_loss=5.5297613

Batch 6990, train_perplexity=266.3264, train_loss=5.5847225

Batch 7000, train_perplexity=267.73694, train_loss=5.590005

Batch 7010, train_perplexity=300.53308, train_loss=5.705558

Batch 7020, train_perplexity=299.09232, train_loss=5.7007523

Batch 7030, train_perplexity=305.51654, train_loss=5.722004

Batch 7040, train_perplexity=287.77652, train_loss=5.6621842

Batch 7050, train_perplexity=252.70245, train_loss=5.5322127

Batch 7060, train_perplexity=283.5892, train_loss=5.6475267

Batch 7070, train_perplexity=273.81454, train_loss=5.612451

Batch 7080, train_perplexity=241.66354, train_loss=5.4875464

Batch 7090, train_perplexity=289.7928, train_loss=5.669166

Batch 7100, train_perplexity=277.9885, train_loss=5.6275797

Batch 7110, train_perplexity=237.23277, train_loss=5.469042

Batch 7120, train_perplexity=271.45438, train_loss=5.603794

Batch 7130, train_perplexity=254.83968, train_loss=5.5406346

Batch 7140, train_perplexity=255.90309, train_loss=5.544799

Batch 7150, train_perplexity=278.94666, train_loss=5.6310205

Batch 7160, train_perplexity=282.64554, train_loss=5.6441936

Batch 7170, train_perplexity=288.18066, train_loss=5.6635876

Batch 7180, train_perplexity=267.80487, train_loss=5.5902586

Batch 7190, train_perplexity=245.74957, train_loss=5.504313

Batch 7200, train_perplexity=312.9089, train_loss=5.745912

Batch 7210, train_perplexity=265.44675, train_loss=5.581414

Batch 7220, train_perplexity=236.29291, train_loss=5.465072

Batch 7230, train_perplexity=281.5816, train_loss=5.6404223

Batch 7240, train_perplexity=264.8808, train_loss=5.57928

Batch 7250, train_perplexity=255.15619, train_loss=5.541876

Batch 7260, train_perplexity=257.42682, train_loss=5.5507355

Batch 7270, train_perplexity=291.11115, train_loss=5.673705

Batch 7280, train_perplexity=261.24527, train_loss=5.5654597

Batch 7290, train_perplexity=260.62576, train_loss=5.5630856

Batch 7300, train_perplexity=292.6492, train_loss=5.6789746

Batch 7310, train_perplexity=294.79236, train_loss=5.686271

Batch 7320, train_perplexity=247.27507, train_loss=5.5105014

Batch 7330, train_perplexity=287.6912, train_loss=5.6618876

Batch 7340, train_perplexity=264.24347, train_loss=5.576871

Batch 7350, train_perplexity=245.62643, train_loss=5.503812

Batch 7360, train_perplexity=292.16272, train_loss=5.677311

Batch 7370, train_perplexity=258.00488, train_loss=5.5529785

Batch 7380, train_perplexity=291.628, train_loss=5.675479

Batch 7390, train_perplexity=272.7875, train_loss=5.608693

Batch 7400, train_perplexity=224.40985, train_loss=5.413474

Batch 7410, train_perplexity=296.28006, train_loss=5.691305

Batch 7420, train_perplexity=272.69037, train_loss=5.608337

Batch 7430, train_perplexity=296.24982, train_loss=5.691203

Batch 7440, train_perplexity=280.475, train_loss=5.6364846

Batch 7450, train_perplexity=262.30417, train_loss=5.5695047

Batch 7460, train_perplexity=261.32416, train_loss=5.5657616

Batch 7470, train_perplexity=276.01147, train_loss=5.6204424

Batch 7480, train_perplexity=265.0635, train_loss=5.5799694

Batch 7490, train_perplexity=282.90607, train_loss=5.645115

Batch 7500, train_perplexity=302.35828, train_loss=5.7116127

Batch 7510, train_perplexity=285.00122, train_loss=5.6524935

Batch 7520, train_perplexity=245.2566, train_loss=5.502305

Batch 7530, train_perplexity=254.19318, train_loss=5.5380945

Batch 7540, train_perplexity=267.14954, train_loss=5.5878086

Batch 7550, train_perplexity=249.68039, train_loss=5.5201817

Batch 7560, train_perplexity=257.5599, train_loss=5.5512524

Batch 7570, train_perplexity=288.19385, train_loss=5.6636333

Batch 7580, train_perplexity=270.40744, train_loss=5.59993

Batch 7590, train_perplexity=244.61598, train_loss=5.4996896

Batch 7600, train_perplexity=279.88837, train_loss=5.634391

Batch 7610, train_perplexity=286.049, train_loss=5.656163

Batch 7620, train_perplexity=272.18567, train_loss=5.6064844

Batch 7630, train_perplexity=249.6699, train_loss=5.5201397

Batch 7640, train_perplexity=256.26846, train_loss=5.5462255

Batch 7650, train_perplexity=257.52393, train_loss=5.5511127

Batch 7660, train_perplexity=270.82422, train_loss=5.60147

Batch 7670, train_perplexity=276.41397, train_loss=5.6218996

Batch 7680, train_perplexity=264.68698, train_loss=5.578548

Batch 7690, train_perplexity=261.76938, train_loss=5.567464

Batch 7700, train_perplexity=281.24628, train_loss=5.6392307

Batch 7710, train_perplexity=299.7062, train_loss=5.7028027

Batch 7720, train_perplexity=273.60587, train_loss=5.6116886

Batch 7730, train_perplexity=273.60052, train_loss=5.611669

Batch 7740, train_perplexity=282.78885, train_loss=5.6447005

Batch 7750, train_perplexity=256.7232, train_loss=5.5479984

Batch 7760, train_perplexity=275.4482, train_loss=5.6183996

Batch 7770, train_perplexity=223.6275, train_loss=5.4099817

Batch 7780, train_perplexity=303.3352, train_loss=5.7148385
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7790, train_perplexity=277.1327, train_loss=5.6244965

Batch 7800, train_perplexity=251.50406, train_loss=5.527459

Batch 7810, train_perplexity=260.4435, train_loss=5.562386

Batch 7820, train_perplexity=268.1105, train_loss=5.591399

Batch 7830, train_perplexity=271.7147, train_loss=5.6047525

Batch 7840, train_perplexity=250.02565, train_loss=5.5215635

Batch 7850, train_perplexity=285.5418, train_loss=5.6543884

Batch 7860, train_perplexity=276.21, train_loss=5.6211615

Batch 7870, train_perplexity=278.51868, train_loss=5.629485

Batch 7880, train_perplexity=256.98798, train_loss=5.5490294

Batch 7890, train_perplexity=285.8535, train_loss=5.6554794

Batch 7900, train_perplexity=291.6038, train_loss=5.675396

Batch 7910, train_perplexity=263.943, train_loss=5.575733

Batch 7920, train_perplexity=257.36472, train_loss=5.550494

Batch 7930, train_perplexity=255.28822, train_loss=5.542393

Batch 7940, train_perplexity=259.6092, train_loss=5.5591774

Batch 7950, train_perplexity=271.00095, train_loss=5.6021223

Batch 7960, train_perplexity=234.99814, train_loss=5.4595776

Batch 7970, train_perplexity=264.49863, train_loss=5.577836

Batch 7980, train_perplexity=251.55672, train_loss=5.5276685

Batch 7990, train_perplexity=265.38535, train_loss=5.581183

Batch 8000, train_perplexity=275.79175, train_loss=5.619646

Batch 8010, train_perplexity=247.65326, train_loss=5.5120296

Batch 8020, train_perplexity=271.98755, train_loss=5.6057563

Batch 8030, train_perplexity=238.44229, train_loss=5.4741273

Batch 8040, train_perplexity=276.36993, train_loss=5.6217403

Batch 8050, train_perplexity=306.48044, train_loss=5.725154

Batch 8060, train_perplexity=235.66649, train_loss=5.4624176

Batch 8070, train_perplexity=243.25542, train_loss=5.494112

Batch 8080, train_perplexity=274.2438, train_loss=5.6140175

Batch 8090, train_perplexity=241.50066, train_loss=5.486872

Batch 8100, train_perplexity=249.39886, train_loss=5.5190535

Batch 8110, train_perplexity=250.17485, train_loss=5.52216

Batch 8120, train_perplexity=269.6881, train_loss=5.597266

Batch 8130, train_perplexity=265.85895, train_loss=5.582966

Batch 8140, train_perplexity=234.97539, train_loss=5.459481

Batch 8150, train_perplexity=261.4145, train_loss=5.5661073

Batch 8160, train_perplexity=245.70445, train_loss=5.5041294

Batch 8170, train_perplexity=259.06976, train_loss=5.5570974

Batch 8180, train_perplexity=238.07452, train_loss=5.472584

Batch 8190, train_perplexity=233.49847, train_loss=5.4531755

Batch 8200, train_perplexity=246.70375, train_loss=5.5081882

Batch 8210, train_perplexity=246.54053, train_loss=5.5075264

Batch 8220, train_perplexity=267.58582, train_loss=5.5894403

Batch 8230, train_perplexity=250.45593, train_loss=5.523283

Batch 8240, train_perplexity=258.97195, train_loss=5.55672

Batch 8250, train_perplexity=277.06927, train_loss=5.6242676

Batch 8260, train_perplexity=228.03358, train_loss=5.429493

Batch 8270, train_perplexity=235.65637, train_loss=5.4623747

Batch 8280, train_perplexity=255.00827, train_loss=5.541296

Batch 8290, train_perplexity=269.68915, train_loss=5.59727

Batch 8300, train_perplexity=269.88443, train_loss=5.597994

Batch 8310, train_perplexity=279.36276, train_loss=5.632511

Batch 8320, train_perplexity=241.81697, train_loss=5.488181

Batch 8330, train_perplexity=252.35385, train_loss=5.5308323

Batch 8340, train_perplexity=271.73996, train_loss=5.6048455

Batch 8350, train_perplexity=234.69174, train_loss=5.458273

Batch 8360, train_perplexity=280.77274, train_loss=5.6375456

Batch 8370, train_perplexity=300.0754, train_loss=5.704034

Batch 8380, train_perplexity=243.6657, train_loss=5.495797

Batch 8390, train_perplexity=285.7322, train_loss=5.655055

Batch 8400, train_perplexity=231.64845, train_loss=5.445221

Batch 8410, train_perplexity=260.4419, train_loss=5.56238

Batch 8420, train_perplexity=276.34384, train_loss=5.621646

Batch 8430, train_perplexity=255.01096, train_loss=5.5413065

Batch 8440, train_perplexity=220.03621, train_loss=5.393792

Batch 8450, train_perplexity=257.24005, train_loss=5.5500097

Batch 8460, train_perplexity=232.35934, train_loss=5.448285

Batch 8470, train_perplexity=266.8122, train_loss=5.586545

Batch 8480, train_perplexity=263.0079, train_loss=5.572184

Batch 8490, train_perplexity=233.3362, train_loss=5.4524803

Batch 8500, train_perplexity=260.87506, train_loss=5.5640416

Batch 8510, train_perplexity=247.9165, train_loss=5.513092

Batch 8520, train_perplexity=292.12485, train_loss=5.6771812

Batch 8530, train_perplexity=281.17413, train_loss=5.638974

Batch 8540, train_perplexity=254.48473, train_loss=5.539241

Batch 8550, train_perplexity=255.21265, train_loss=5.542097

Batch 8560, train_perplexity=272.97775, train_loss=5.6093903

Batch 8570, train_perplexity=271.51495, train_loss=5.6040173

Batch 8580, train_perplexity=276.92398, train_loss=5.623743

Batch 8590, train_perplexity=288.02472, train_loss=5.6630464

Batch 8600, train_perplexity=247.6365, train_loss=5.511962

Batch 8610, train_perplexity=266.03418, train_loss=5.583625

Batch 8620, train_perplexity=267.34604, train_loss=5.588544

Batch 8630, train_perplexity=271.1422, train_loss=5.6026435

Batch 8640, train_perplexity=226.78677, train_loss=5.4240103

Batch 8650, train_perplexity=246.23999, train_loss=5.5063066

Batch 8660, train_perplexity=223.54167, train_loss=5.409598

Batch 8670, train_perplexity=225.8393, train_loss=5.4198236

Batch 8680, train_perplexity=238.19762, train_loss=5.4731007

Batch 8690, train_perplexity=275.12006, train_loss=5.6172075

Batch 8700, train_perplexity=247.0303, train_loss=5.509511

Batch 8710, train_perplexity=249.85188, train_loss=5.5208683

Batch 8720, train_perplexity=294.6337, train_loss=5.685733

Batch 8730, train_perplexity=257.67047, train_loss=5.5516815

Batch 8740, train_perplexity=264.57013, train_loss=5.5781064

Batch 8750, train_perplexity=258.7288, train_loss=5.5557804

Batch 8760, train_perplexity=217.30573, train_loss=5.381305

Batch 8770, train_perplexity=250.8097, train_loss=5.5246944

Batch 8780, train_perplexity=240.02863, train_loss=5.480758

Batch 8790, train_perplexity=241.5996, train_loss=5.487282

Batch 8800, train_perplexity=232.61943, train_loss=5.449404

Batch 8810, train_perplexity=236.3958, train_loss=5.4655075

Batch 8820, train_perplexity=229.08267, train_loss=5.434083

Batch 8830, train_perplexity=270.67575, train_loss=5.6009216

Batch 8840, train_perplexity=239.10277, train_loss=5.4768934

Batch 8850, train_perplexity=245.7136, train_loss=5.5041666

Batch 8860, train_perplexity=263.1937, train_loss=5.5728903

Batch 8870, train_perplexity=261.34683, train_loss=5.5658484

Batch 8880, train_perplexity=262.29703, train_loss=5.5694776

Batch 8890, train_perplexity=254.10799, train_loss=5.5377593

Batch 8900, train_perplexity=233.23097, train_loss=5.452029

Batch 8910, train_perplexity=264.02557, train_loss=5.576046

Batch 8920, train_perplexity=246.8293, train_loss=5.508697

Batch 8930, train_perplexity=257.23294, train_loss=5.549982

Batch 8940, train_perplexity=232.47572, train_loss=5.448786

Batch 8950, train_perplexity=262.0595, train_loss=5.5685716

Batch 8960, train_perplexity=267.00055, train_loss=5.5872507

Batch 8970, train_perplexity=229.81912, train_loss=5.4372926

Batch 8980, train_perplexity=248.0545, train_loss=5.5136485

Batch 8990, train_perplexity=297.9305, train_loss=5.6968603

Batch 9000, train_perplexity=279.42538, train_loss=5.6327353

Batch 9010, train_perplexity=241.78572, train_loss=5.488052

Batch 9020, train_perplexity=239.64745, train_loss=5.479169

Batch 9030, train_perplexity=249.33525, train_loss=5.5187984

Batch 9040, train_perplexity=242.24617, train_loss=5.4899545

Batch 9050, train_perplexity=245.46779, train_loss=5.5031657

Batch 9060, train_perplexity=262.11935, train_loss=5.5688

Batch 9070, train_perplexity=205.97012, train_loss=5.327731

Batch 9080, train_perplexity=220.49037, train_loss=5.395854

Batch 9090, train_perplexity=200.821, train_loss=5.302414

Batch 9100, train_perplexity=251.68881, train_loss=5.5281935

Batch 9110, train_perplexity=238.27531, train_loss=5.473427

Batch 9120, train_perplexity=278.32697, train_loss=5.6287966
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9130, train_perplexity=247.26459, train_loss=5.510459

Batch 9140, train_perplexity=233.4821, train_loss=5.4531054

Batch 9150, train_perplexity=278.64566, train_loss=5.629941

Batch 9160, train_perplexity=260.1371, train_loss=5.5612087

Batch 9170, train_perplexity=284.3292, train_loss=5.6501327

Batch 9180, train_perplexity=257.9279, train_loss=5.55268

Batch 9190, train_perplexity=235.20642, train_loss=5.4604635

Batch 9200, train_perplexity=242.90515, train_loss=5.492671

Batch 9210, train_perplexity=225.73851, train_loss=5.4193773

Batch 9220, train_perplexity=225.635, train_loss=5.4189186

Batch 9230, train_perplexity=232.76212, train_loss=5.450017

Batch 9240, train_perplexity=242.8667, train_loss=5.4925127

Batch 9250, train_perplexity=212.85516, train_loss=5.360612

Batch 9260, train_perplexity=262.61252, train_loss=5.5706797

Batch 9270, train_perplexity=246.65201, train_loss=5.5079784

Batch 9280, train_perplexity=226.16011, train_loss=5.421243

Batch 9290, train_perplexity=288.63367, train_loss=5.6651583

Batch 9300, train_perplexity=249.76553, train_loss=5.5205226

Batch 9310, train_perplexity=240.34026, train_loss=5.4820557

Batch 9320, train_perplexity=238.14038, train_loss=5.4728603

Batch 9330, train_perplexity=248.44728, train_loss=5.5152307

Batch 9340, train_perplexity=253.11792, train_loss=5.5338554

Batch 9350, train_perplexity=249.42787, train_loss=5.51917

Batch 9360, train_perplexity=216.84262, train_loss=5.379172

Batch 9370, train_perplexity=267.3718, train_loss=5.58864

Batch 9380, train_perplexity=231.76701, train_loss=5.4457326

Batch 9390, train_perplexity=262.7483, train_loss=5.5711966

Batch 9400, train_perplexity=219.65945, train_loss=5.3920784

Batch 9410, train_perplexity=228.0921, train_loss=5.4297495

Batch 9420, train_perplexity=246.13011, train_loss=5.5058603

Batch 9430, train_perplexity=241.0091, train_loss=5.4848347

Batch 9440, train_perplexity=256.75098, train_loss=5.5481067

Batch 9450, train_perplexity=261.19385, train_loss=5.565263

Batch 9460, train_perplexity=221.419, train_loss=5.400057

Batch 9470, train_perplexity=218.84607, train_loss=5.3883686

Batch 9480, train_perplexity=226.91322, train_loss=5.4245677

Batch 9490, train_perplexity=255.78036, train_loss=5.544319

Batch 9500, train_perplexity=274.90775, train_loss=5.6164355

Batch 9510, train_perplexity=253.58315, train_loss=5.5356917

Batch 9520, train_perplexity=235.17659, train_loss=5.4603367

Batch 9530, train_perplexity=264.3443, train_loss=5.5772524

Batch 9540, train_perplexity=210.14392, train_loss=5.3477926

Batch 9550, train_perplexity=268.16638, train_loss=5.5916076

Batch 9560, train_perplexity=283.98923, train_loss=5.6489363

Batch 9570, train_perplexity=239.16457, train_loss=5.477152

Batch 9580, train_perplexity=207.67873, train_loss=5.3359923

Batch 9590, train_perplexity=241.65651, train_loss=5.4875174

Batch 9600, train_perplexity=239.7606, train_loss=5.479641

Batch 9610, train_perplexity=250.96437, train_loss=5.525311

Batch 9620, train_perplexity=232.224, train_loss=5.4477024

Batch 9630, train_perplexity=242.7773, train_loss=5.4921446

Batch 9640, train_perplexity=238.35896, train_loss=5.473778

Batch 9650, train_perplexity=245.39253, train_loss=5.502859

Batch 9660, train_perplexity=237.21378, train_loss=5.4689617

Batch 9670, train_perplexity=183.1517, train_loss=5.2103148

Batch 9680, train_perplexity=228.05882, train_loss=5.4296036

Batch 9690, train_perplexity=244.83572, train_loss=5.5005875

Batch 9700, train_perplexity=225.40605, train_loss=5.4179034

Batch 9710, train_perplexity=253.8947, train_loss=5.5369196

Batch 9720, train_perplexity=226.57513, train_loss=5.4230766

Batch 9730, train_perplexity=235.82768, train_loss=5.4631014

Batch 9740, train_perplexity=248.99628, train_loss=5.517438

Batch 9750, train_perplexity=258.7879, train_loss=5.556009

Batch 9760, train_perplexity=220.89658, train_loss=5.3976946

Batch 9770, train_perplexity=240.33086, train_loss=5.4820166

Batch 9780, train_perplexity=240.42554, train_loss=5.4824104

Batch 9790, train_perplexity=261.212, train_loss=5.5653324

Batch 9800, train_perplexity=234.66646, train_loss=5.458165

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 9810, train_perplexity=232.366, train_loss=5.4483137

Batch 9820, train_perplexity=234.51723, train_loss=5.457529

Batch 9830, train_perplexity=239.7589, train_loss=5.479634

Batch 9840, train_perplexity=245.6296, train_loss=5.5038247

Batch 9850, train_perplexity=224.66757, train_loss=5.414622

Batch 9860, train_perplexity=239.1121, train_loss=5.4769325

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 9870, train_perplexity=215.3133, train_loss=5.372094

Batch 9880, train_perplexity=227.68135, train_loss=5.427947

Batch 9890, train_perplexity=237.75937, train_loss=5.471259

Batch 9900, train_perplexity=243.11952, train_loss=5.493553

Batch 9910, train_perplexity=232.45012, train_loss=5.4486756

Batch 9920, train_perplexity=242.17215, train_loss=5.489649

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 9930, train_perplexity=259.73215, train_loss=5.559651

Batch 9940, train_perplexity=218.42749, train_loss=5.386454

Batch 9950, train_perplexity=241.13785, train_loss=5.4853687

Batch 9960, train_perplexity=234.1111, train_loss=5.455796

Batch 9970, train_perplexity=218.74841, train_loss=5.3879223

Batch 9980, train_perplexity=250.61972, train_loss=5.5239367

Batch 9990, train_perplexity=223.7333, train_loss=5.4104548

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 10000, train_perplexity=238.90503, train_loss=5.476066

Batch 10010, train_perplexity=207.27579, train_loss=5.33405

Batch 10020, train_perplexity=241.17924, train_loss=5.4855404

Batch 10030, train_perplexity=252.26904, train_loss=5.530496

Batch 10040, train_perplexity=239.80406, train_loss=5.479822

Batch 10050, train_perplexity=227.44522, train_loss=5.4269094

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 10060, train_perplexity=219.00401, train_loss=5.38909

Batch 10070, train_perplexity=251.55203, train_loss=5.52765

Batch 10080, train_perplexity=237.88718, train_loss=5.4717965

Batch 10090, train_perplexity=243.46278, train_loss=5.494964

Batch 10100, train_perplexity=233.32574, train_loss=5.4524355

Batch 10110, train_perplexity=232.95398, train_loss=5.450841

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 10120, train_perplexity=228.87938, train_loss=5.433195

Batch 10130, train_perplexity=237.89375, train_loss=5.471824

Batch 10140, train_perplexity=210.34995, train_loss=5.3487725

Batch 10150, train_perplexity=194.10333, train_loss=5.2683907

Batch 10160, train_perplexity=218.61285, train_loss=5.3873024

Batch 10170, train_perplexity=229.26845, train_loss=5.4348936

Batch 10180, train_perplexity=227.70718, train_loss=5.4280605

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 10190, train_perplexity=227.78798, train_loss=5.4284153

Batch 10200, train_perplexity=249.03653, train_loss=5.5175996

Batch 10210, train_perplexity=243.36588, train_loss=5.494566

Batch 10220, train_perplexity=229.42506, train_loss=5.4355764

Batch 10230, train_perplexity=229.04074, train_loss=5.4339

Batch 10240, train_perplexity=240.255, train_loss=5.481701

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 10250, train_perplexity=236.88643, train_loss=5.467581

Batch 10260, train_perplexity=237.3621, train_loss=5.469587

Batch 10270, train_perplexity=215.32376, train_loss=5.372143

Batch 10280, train_perplexity=209.15712, train_loss=5.343086

Batch 10290, train_perplexity=235.01987, train_loss=5.45967

Batch 10300, train_perplexity=267.34555, train_loss=5.588542

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 10310, train_perplexity=211.23677, train_loss=5.3529797

Batch 10320, train_perplexity=241.9398, train_loss=5.488689

Batch 10330, train_perplexity=239.10596, train_loss=5.476907

Batch 10340, train_perplexity=225.79417, train_loss=5.419624

Batch 10350, train_perplexity=242.24283, train_loss=5.4899406

Batch 10360, train_perplexity=220.32599, train_loss=5.395108

Batch 10370, train_perplexity=230.63678, train_loss=5.440844

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 10380, train_perplexity=234.82115, train_loss=5.458824

Batch 10390, train_perplexity=225.152, train_loss=5.4167757

Batch 10400, train_perplexity=236.8452, train_loss=5.4674067

Batch 10410, train_perplexity=208.51184, train_loss=5.339996

Batch 10420, train_perplexity=234.21739, train_loss=5.4562497

Batch 10430, train_perplexity=233.84299, train_loss=5.45465

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 10440, train_perplexity=204.84795, train_loss=5.322268

Batch 10450, train_perplexity=227.3433, train_loss=5.426461

Batch 10460, train_perplexity=247.97444, train_loss=5.5133257

Batch 10470, train_perplexity=252.19049, train_loss=5.5301847

Batch 10480, train_perplexity=208.24565, train_loss=5.3387184

Batch 10490, train_perplexity=203.4459, train_loss=5.3154

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 10500, train_perplexity=216.96777, train_loss=5.379749

Batch 10510, train_perplexity=225.3045, train_loss=5.417453

Batch 10520, train_perplexity=219.78539, train_loss=5.3926516

Batch 10530, train_perplexity=230.51462, train_loss=5.4403143

Batch 10540, train_perplexity=212.2681, train_loss=5.35785

Batch 10550, train_perplexity=222.86913, train_loss=5.4065847

Batch 10560, train_perplexity=230.5543, train_loss=5.4404864

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 10570, train_perplexity=224.138, train_loss=5.412262

Batch 10580, train_perplexity=223.01648, train_loss=5.4072456

Batch 10590, train_perplexity=221.77847, train_loss=5.401679

Batch 10600, train_perplexity=192.64093, train_loss=5.260828

Batch 10610, train_perplexity=233.44882, train_loss=5.452963

Batch 10620, train_perplexity=223.78665, train_loss=5.410693

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 10630, train_perplexity=238.80412, train_loss=5.4756436

Batch 10640, train_perplexity=244.22229, train_loss=5.498079

Batch 10650, train_perplexity=239.52795, train_loss=5.47867

Batch 10660, train_perplexity=217.40097, train_loss=5.3817434

Batch 10670, train_perplexity=227.68156, train_loss=5.427948

Batch 10680, train_perplexity=236.69437, train_loss=5.4667697

Batch 10690, train_perplexity=225.84532, train_loss=5.4198503

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 10700, train_perplexity=227.38657, train_loss=5.4266515

Batch 10710, train_perplexity=254.71321, train_loss=5.5401382

Batch 10720, train_perplexity=223.60362, train_loss=5.409875

Batch 10730, train_perplexity=223.72083, train_loss=5.410399

Batch 10740, train_perplexity=229.44585, train_loss=5.435667

Batch 10750, train_perplexity=241.35986, train_loss=5.486289
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 10760, train_perplexity=258.88776, train_loss=5.5563946

Batch 10770, train_perplexity=220.99844, train_loss=5.3981557

Batch 10780, train_perplexity=235.92667, train_loss=5.463521

Batch 10790, train_perplexity=215.55779, train_loss=5.373229

Batch 10800, train_perplexity=242.59424, train_loss=5.49139

Batch 10810, train_perplexity=216.7991, train_loss=5.378971

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 10820, train_perplexity=211.85098, train_loss=5.355883

Batch 10830, train_perplexity=218.13814, train_loss=5.3851285

Batch 10840, train_perplexity=199.61139, train_loss=5.2963724

Batch 10850, train_perplexity=204.89954, train_loss=5.32252

Batch 10860, train_perplexity=202.9622, train_loss=5.3130198

Batch 10870, train_perplexity=225.64854, train_loss=5.4189787

Batch 10880, train_perplexity=253.21086, train_loss=5.5342226

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 10890, train_perplexity=220.21477, train_loss=5.3946033

Batch 10900, train_perplexity=241.26286, train_loss=5.485887

Batch 10910, train_perplexity=216.86093, train_loss=5.3792562

Batch 10920, train_perplexity=233.00154, train_loss=5.451045

Batch 10930, train_perplexity=210.71054, train_loss=5.3504853

Batch 10940, train_perplexity=242.39096, train_loss=5.490552

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 10950, train_perplexity=217.41527, train_loss=5.381809

Batch 10960, train_perplexity=230.96638, train_loss=5.442272

Batch 10970, train_perplexity=231.4197, train_loss=5.444233

Batch 10980, train_perplexity=225.55173, train_loss=5.4185495

Batch 10990, train_perplexity=247.3666, train_loss=5.5108714

Batch 11000, train_perplexity=215.72725, train_loss=5.374015

Batch 11010, train_perplexity=200.96248, train_loss=5.303118

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 11020, train_perplexity=197.97539, train_loss=5.2881427

Batch 11030, train_perplexity=218.41968, train_loss=5.3864183

Batch 11040, train_perplexity=206.41945, train_loss=5.3299103

Batch 11050, train_perplexity=215.24503, train_loss=5.371777

Batch 11060, train_perplexity=252.98228, train_loss=5.5333195

Batch 11070, train_perplexity=202.2472, train_loss=5.3094907

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 11080, train_perplexity=232.17882, train_loss=5.447508

Batch 11090, train_perplexity=206.68646, train_loss=5.331203

Batch 11100, train_perplexity=222.16142, train_loss=5.403404

Batch 11110, train_perplexity=199.9162, train_loss=5.2978983

Batch 11120, train_perplexity=236.32277, train_loss=5.4651985

Batch 11130, train_perplexity=221.67508, train_loss=5.4012127

Batch 11140, train_perplexity=201.5399, train_loss=5.3059874

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 11150, train_perplexity=220.44673, train_loss=5.395656

Batch 11160, train_perplexity=213.97456, train_loss=5.365857

Batch 11170, train_perplexity=224.70218, train_loss=5.414776

Batch 11180, train_perplexity=233.11333, train_loss=5.4515247

Batch 11190, train_perplexity=212.18593, train_loss=5.357463

Batch 11200, train_perplexity=209.28392, train_loss=5.343692

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 11210, train_perplexity=245.89539, train_loss=5.504906

Batch 11220, train_perplexity=210.64253, train_loss=5.3501625

Batch 11230, train_perplexity=218.94524, train_loss=5.3888216

Batch 11240, train_perplexity=253.29202, train_loss=5.534543

Batch 11250, train_perplexity=208.75479, train_loss=5.3411603

Batch 11260, train_perplexity=226.45914, train_loss=5.4225645

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 11270, train_perplexity=208.49088, train_loss=5.3398952

Batch 11280, train_perplexity=190.583, train_loss=5.2500877

Batch 11290, train_perplexity=231.08734, train_loss=5.4427958

Batch 11300, train_perplexity=210.0817, train_loss=5.3474965

Batch 11310, train_perplexity=240.25822, train_loss=5.4817142

Batch 11320, train_perplexity=190.76765, train_loss=5.251056

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 11330, train_perplexity=239.60233, train_loss=5.4789805

Batch 11340, train_perplexity=220.90068, train_loss=5.397713

Batch 11350, train_perplexity=211.6443, train_loss=5.354907

Batch 11360, train_perplexity=217.1341, train_loss=5.380515

Batch 11370, train_perplexity=240.37694, train_loss=5.4822083

Batch 11380, train_perplexity=190.6388, train_loss=5.2503805

Batch 11390, train_perplexity=238.98285, train_loss=5.476392

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 11400, train_perplexity=219.317, train_loss=5.390518

Batch 11410, train_perplexity=208.87068, train_loss=5.3417153

Batch 11420, train_perplexity=215.74966, train_loss=5.374119

Batch 11430, train_perplexity=188.14925, train_loss=5.2372355

Batch 11440, train_perplexity=234.5839, train_loss=5.4578133

Batch 11450, train_perplexity=202.30353, train_loss=5.309769

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 11460, train_perplexity=217.06204, train_loss=5.380183

Batch 11470, train_perplexity=214.55591, train_loss=5.3685703

Batch 11480, train_perplexity=211.58054, train_loss=5.3546057

Batch 11490, train_perplexity=214.3422, train_loss=5.3675737

Batch 11500, train_perplexity=196.1759, train_loss=5.2790117

Batch 11510, train_perplexity=243.42436, train_loss=5.4948063

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 11520, train_perplexity=252.25027, train_loss=5.5304217

Batch 11530, train_perplexity=223.38408, train_loss=5.4088926

Batch 11540, train_perplexity=253.3152, train_loss=5.5346346

Batch 11550, train_perplexity=206.87213, train_loss=5.332101

Batch 11560, train_perplexity=234.63983, train_loss=5.4580517

Batch 11570, train_perplexity=203.8525, train_loss=5.3173966

Batch 11580, train_perplexity=196.85696, train_loss=5.2824774

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 11590, train_perplexity=200.65521, train_loss=5.301588

Batch 11600, train_perplexity=218.37312, train_loss=5.386205

Batch 11610, train_perplexity=239.98273, train_loss=5.480567

Batch 11620, train_perplexity=228.59666, train_loss=5.431959

Batch 11630, train_perplexity=212.84906, train_loss=5.3605833

Batch 11640, train_perplexity=185.71436, train_loss=5.22421

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 11650, train_perplexity=211.28493, train_loss=5.3532076

Batch 11660, train_perplexity=226.6135, train_loss=5.423246

Batch 11670, train_perplexity=214.04129, train_loss=5.366169

Batch 11680, train_perplexity=207.63011, train_loss=5.335758

Batch 11690, train_perplexity=211.7197, train_loss=5.355263

Batch 11700, train_perplexity=220.97221, train_loss=5.398037

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 11710, train_perplexity=229.98279, train_loss=5.4380045

Batch 11720, train_perplexity=215.65463, train_loss=5.373678

Batch 11730, train_perplexity=209.82071, train_loss=5.3462534

Batch 11740, train_perplexity=243.09587, train_loss=5.493456

Batch 11750, train_perplexity=203.28735, train_loss=5.3146205

Batch 11760, train_perplexity=231.26923, train_loss=5.4435825

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 11770, train_perplexity=221.86458, train_loss=5.402067

Batch 11780, train_perplexity=214.63551, train_loss=5.3689413

Batch 11790, train_perplexity=185.29472, train_loss=5.2219477

Batch 11800, train_perplexity=196.81726, train_loss=5.2822757

Batch 11810, train_perplexity=207.92467, train_loss=5.337176

Batch 11820, train_perplexity=222.58557, train_loss=5.4053116

Batch 11830, train_perplexity=195.07816, train_loss=5.2734003

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 11840, train_perplexity=199.84662, train_loss=5.29755

Batch 11850, train_perplexity=219.63515, train_loss=5.391968

Batch 11860, train_perplexity=220.64835, train_loss=5.39657

Batch 11870, train_perplexity=226.47145, train_loss=5.422619

Batch 11880, train_perplexity=215.45113, train_loss=5.372734

Batch 11890, train_perplexity=189.2597, train_loss=5.24312

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 11900, train_perplexity=226.6697, train_loss=5.423494

Batch 11910, train_perplexity=212.87718, train_loss=5.3607154

Batch 11920, train_perplexity=245.70598, train_loss=5.5041356

Batch 11930, train_perplexity=218.41135, train_loss=5.38638

Batch 11940, train_perplexity=209.40442, train_loss=5.3442674

Batch 11950, train_perplexity=214.03058, train_loss=5.366119

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 11960, train_perplexity=205.39647, train_loss=5.324942

Batch 11970, train_perplexity=206.79254, train_loss=5.331716

Batch 11980, train_perplexity=205.8794, train_loss=5.3272905

Batch 11990, train_perplexity=204.6684, train_loss=5.321391

Batch 12000, train_perplexity=204.59903, train_loss=5.321052

Batch 12010, train_perplexity=245.56834, train_loss=5.5035753

Batch 12020, train_perplexity=244.16162, train_loss=5.4978304

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 12030, train_perplexity=220.04881, train_loss=5.3938494

Batch 12040, train_perplexity=228.58412, train_loss=5.4319043

Batch 12050, train_perplexity=200.11897, train_loss=5.298912

Batch 12060, train_perplexity=195.94638, train_loss=5.277841

Batch 12070, train_perplexity=233.8062, train_loss=5.4544926

Batch 12080, train_perplexity=211.76201, train_loss=5.355463

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 12090, train_perplexity=194.15489, train_loss=5.2686563

Batch 12100, train_perplexity=196.94794, train_loss=5.2829394

Batch 12110, train_perplexity=229.70616, train_loss=5.436801

Batch 12120, train_perplexity=211.07608, train_loss=5.3522186

Batch 12130, train_perplexity=200.33035, train_loss=5.299968

Batch 12140, train_perplexity=245.78706, train_loss=5.5044656

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 12150, train_perplexity=207.7545, train_loss=5.336357

Batch 12160, train_perplexity=224.94681, train_loss=5.415864

Batch 12170, train_perplexity=202.24092, train_loss=5.3094597

Batch 12180, train_perplexity=208.67627, train_loss=5.340784

Batch 12190, train_perplexity=223.46143, train_loss=5.409239

Batch 12200, train_perplexity=208.66661, train_loss=5.340738

Batch 12210, train_perplexity=202.00037, train_loss=5.3082695

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 12220, train_perplexity=226.34016, train_loss=5.422039

Batch 12230, train_perplexity=214.95784, train_loss=5.370442

Batch 12240, train_perplexity=232.00087, train_loss=5.446741

Batch 12250, train_perplexity=221.20773, train_loss=5.399102

Batch 12260, train_perplexity=208.93124, train_loss=5.3420053

Batch 12270, train_perplexity=248.1114, train_loss=5.513878

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 12280, train_perplexity=244.36102, train_loss=5.4986467

Batch 12290, train_perplexity=216.28777, train_loss=5.37661

Batch 12300, train_perplexity=185.35765, train_loss=5.222287

Batch 12310, train_perplexity=209.66669, train_loss=5.345519

Batch 12320, train_perplexity=215.44783, train_loss=5.372719

Batch 12330, train_perplexity=204.74796, train_loss=5.3217797

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 12340, train_perplexity=220.45967, train_loss=5.3957148

Batch 12350, train_perplexity=220.70253, train_loss=5.396816

Batch 12360, train_perplexity=206.0609, train_loss=5.3281717

Batch 12370, train_perplexity=198.37605, train_loss=5.2901645

Batch 12380, train_perplexity=211.5313, train_loss=5.354373

Batch 12390, train_perplexity=212.71758, train_loss=5.3599653

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 12400, train_perplexity=223.47528, train_loss=5.409301

Batch 12410, train_perplexity=198.39166, train_loss=5.290243

Batch 12420, train_perplexity=222.88582, train_loss=5.4066596

Batch 12430, train_perplexity=232.64471, train_loss=5.4495125

Batch 12440, train_perplexity=209.2535, train_loss=5.3435464

Batch 12450, train_perplexity=240.02016, train_loss=5.480723

Batch 12460, train_perplexity=223.58868, train_loss=5.409808

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 12470, train_perplexity=225.78136, train_loss=5.419567

Batch 12480, train_perplexity=221.65266, train_loss=5.4011116

Batch 12490, train_perplexity=226.04614, train_loss=5.420739

Batch 12500, train_perplexity=204.3081, train_loss=5.319629

Batch 12510, train_perplexity=194.66847, train_loss=5.271298

Batch 12520, train_perplexity=220.04199, train_loss=5.3938184

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 12530, train_perplexity=220.33923, train_loss=5.3951683

Batch 12540, train_perplexity=219.89, train_loss=5.3931274

Batch 12550, train_perplexity=216.20352, train_loss=5.37622

Batch 12560, train_perplexity=204.2147, train_loss=5.319172

Batch 12570, train_perplexity=201.32167, train_loss=5.304904

Batch 12580, train_perplexity=202.4445, train_loss=5.310466

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 12590, train_perplexity=182.95634, train_loss=5.2092476

Batch 12600, train_perplexity=228.42647, train_loss=5.4312143

Batch 12610, train_perplexity=196.24083, train_loss=5.2793427

Batch 12620, train_perplexity=218.58388, train_loss=5.38717

Batch 12630, train_perplexity=211.61322, train_loss=5.35476

Batch 12640, train_perplexity=216.23456, train_loss=5.3763638

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 12650, train_perplexity=229.0663, train_loss=5.4340115

Batch 12660, train_perplexity=243.075, train_loss=5.49337

Batch 12670, train_perplexity=213.40761, train_loss=5.363204

Batch 12680, train_perplexity=209.89616, train_loss=5.346613

Batch 12690, train_perplexity=206.24797, train_loss=5.329079

Batch 12700, train_perplexity=213.06902, train_loss=5.361616

Batch 12710, train_perplexity=224.80399, train_loss=5.415229

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 12720, train_perplexity=206.23213, train_loss=5.3290024

Batch 12730, train_perplexity=219.6475, train_loss=5.392024

Batch 12740, train_perplexity=207.22934, train_loss=5.333826

Batch 12750, train_perplexity=194.27315, train_loss=5.269265

Batch 12760, train_perplexity=203.98833, train_loss=5.318063

Batch 12770, train_perplexity=228.43713, train_loss=5.431261

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 12780, train_perplexity=206.95027, train_loss=5.3324785

Batch 12790, train_perplexity=205.9712, train_loss=5.3277364

Batch 12800, train_perplexity=243.99286, train_loss=5.497139

Batch 12810, train_perplexity=184.59381, train_loss=5.218158

Batch 12820, train_perplexity=211.23668, train_loss=5.352979

Batch 12830, train_perplexity=220.91869, train_loss=5.3977947

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 12840, train_perplexity=205.5549, train_loss=5.325713

Batch 12850, train_perplexity=213.95547, train_loss=5.365768

Batch 12860, train_perplexity=205.3809, train_loss=5.3248663

Batch 12870, train_perplexity=199.01796, train_loss=5.293395

Batch 12880, train_perplexity=221.9395, train_loss=5.402405

Batch 12890, train_perplexity=185.33467, train_loss=5.222163

Batch 12900, train_perplexity=212.69241, train_loss=5.359847

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 12910, train_perplexity=240.83379, train_loss=5.484107

Batch 12920, train_perplexity=196.5945, train_loss=5.281143

Batch 12930, train_perplexity=194.80487, train_loss=5.2719984

Batch 12940, train_perplexity=203.09492, train_loss=5.3136735

Batch 12950, train_perplexity=228.45663, train_loss=5.4313464

Batch 12960, train_perplexity=237.9919, train_loss=5.4722366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 12970, train_perplexity=199.99248, train_loss=5.29828

Batch 12980, train_perplexity=202.65718, train_loss=5.311516

Batch 12990, train_perplexity=208.6303, train_loss=5.340564

Batch 13000, train_perplexity=212.45726, train_loss=5.358741

Batch 13010, train_perplexity=187.448, train_loss=5.2335014

Batch 13020, train_perplexity=180.21652, train_loss=5.194159

Batch 13030, train_perplexity=199.09103, train_loss=5.293762

Batch 13040, train_perplexity=186.31555, train_loss=5.227442

Batch 13050, train_perplexity=207.78879, train_loss=5.336522

Batch 13060, train_perplexity=217.05904, train_loss=5.3801694

Batch 13070, train_perplexity=205.31715, train_loss=5.324556

Batch 13080, train_perplexity=205.75082, train_loss=5.326666

Batch 13090, train_perplexity=185.04794, train_loss=5.220615

Batch 13100, train_perplexity=215.08026, train_loss=5.3710113

Batch 13110, train_perplexity=186.34363, train_loss=5.2275925

Batch 13120, train_perplexity=199.28119, train_loss=5.294717

Batch 13130, train_perplexity=179.25166, train_loss=5.188791

Batch 13140, train_perplexity=192.85858, train_loss=5.261957

Batch 13150, train_perplexity=235.61884, train_loss=5.4622154

Batch 13160, train_perplexity=200.15619, train_loss=5.299098

Batch 13170, train_perplexity=186.51868, train_loss=5.2285314

Batch 13180, train_perplexity=200.6311, train_loss=5.301468

Batch 13190, train_perplexity=197.38776, train_loss=5.28517

Batch 13200, train_perplexity=202.81282, train_loss=5.3122835

Batch 13210, train_perplexity=204.90549, train_loss=5.322549

Batch 13220, train_perplexity=219.94443, train_loss=5.393375

Batch 13230, train_perplexity=202.51054, train_loss=5.310792

Batch 13240, train_perplexity=201.59987, train_loss=5.306285

Batch 13250, train_perplexity=195.48396, train_loss=5.2754784

Batch 13260, train_perplexity=211.6003, train_loss=5.354699

Batch 13270, train_perplexity=222.16545, train_loss=5.4034224

Batch 13280, train_perplexity=217.7153, train_loss=5.3831882

Batch 13290, train_perplexity=193.47829, train_loss=5.2651653

Batch 13300, train_perplexity=180.94197, train_loss=5.1981764

Batch 13310, train_perplexity=192.07481, train_loss=5.257885

Batch 13320, train_perplexity=215.34615, train_loss=5.3722467

Batch 13330, train_perplexity=214.14767, train_loss=5.366666

Batch 13340, train_perplexity=203.31401, train_loss=5.3147516

Batch 13350, train_perplexity=194.83377, train_loss=5.2721467

Batch 13360, train_perplexity=180.78406, train_loss=5.1973033

Batch 13370, train_perplexity=221.83908, train_loss=5.4019523

Batch 13380, train_perplexity=204.7828, train_loss=5.32195

Batch 13390, train_perplexity=191.01259, train_loss=5.2523394

Batch 13400, train_perplexity=200.33856, train_loss=5.300009

Batch 13410, train_perplexity=193.70206, train_loss=5.266321

Batch 13420, train_perplexity=208.38274, train_loss=5.3393764

Batch 13430, train_perplexity=174.77608, train_loss=5.1635056

Batch 13440, train_perplexity=202.27362, train_loss=5.3096213

Batch 13450, train_perplexity=199.24413, train_loss=5.294531

Batch 13460, train_perplexity=211.54643, train_loss=5.3544445

Batch 13470, train_perplexity=203.94417, train_loss=5.3178463

Batch 13480, train_perplexity=203.04951, train_loss=5.31345

Batch 13490, train_perplexity=200.62077, train_loss=5.3014164

Batch 13500, train_perplexity=212.68137, train_loss=5.359795

Batch 13510, train_perplexity=198.7055, train_loss=5.291824

Batch 13520, train_perplexity=218.05026, train_loss=5.3847256

Batch 13530, train_perplexity=199.46448, train_loss=5.295636

Batch 13540, train_perplexity=184.10172, train_loss=5.2154884

Batch 13550, train_perplexity=205.25832, train_loss=5.3242693

Batch 13560, train_perplexity=247.96912, train_loss=5.513304

Batch 13570, train_perplexity=177.61002, train_loss=5.17959

Batch 13580, train_perplexity=196.50359, train_loss=5.2806807

Batch 13590, train_perplexity=204.25317, train_loss=5.3193603

Batch 13600, train_perplexity=197.79675, train_loss=5.28724

Batch 13610, train_perplexity=211.71152, train_loss=5.3552246

Batch 13620, train_perplexity=216.78297, train_loss=5.3788967
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13630, train_perplexity=207.28922, train_loss=5.334115

Batch 13640, train_perplexity=217.78197, train_loss=5.3834944

Batch 13650, train_perplexity=186.73189, train_loss=5.229674

Batch 13660, train_perplexity=202.24034, train_loss=5.309457

Batch 13670, train_perplexity=194.99614, train_loss=5.2729797

Batch 13680, train_perplexity=212.2508, train_loss=5.3577685

Batch 13690, train_perplexity=202.19502, train_loss=5.3092327

Batch 13700, train_perplexity=219.41878, train_loss=5.390982

Batch 13710, train_perplexity=194.13332, train_loss=5.268545

Batch 13720, train_perplexity=206.58813, train_loss=5.330727

Batch 13730, train_perplexity=206.40903, train_loss=5.3298597

Batch 13740, train_perplexity=220.74231, train_loss=5.396996

Batch 13750, train_perplexity=216.07336, train_loss=5.375618

Batch 13760, train_perplexity=209.52437, train_loss=5.34484

Batch 13770, train_perplexity=178.0873, train_loss=5.182274

Batch 13780, train_perplexity=204.78574, train_loss=5.3219643

Batch 13790, train_perplexity=179.34904, train_loss=5.189334

Batch 13800, train_perplexity=198.69395, train_loss=5.2917657

Batch 13810, train_perplexity=191.90875, train_loss=5.25702

Batch 13820, train_perplexity=209.54295, train_loss=5.3449287

Batch 13830, train_perplexity=185.2062, train_loss=5.22147

Batch 13840, train_perplexity=187.54813, train_loss=5.2340355

Batch 13850, train_perplexity=220.56502, train_loss=5.3961926

Batch 13860, train_perplexity=223.46568, train_loss=5.409258

Batch 13870, train_perplexity=197.38013, train_loss=5.2851315

Batch 13880, train_perplexity=196.98035, train_loss=5.283104

Batch 13890, train_perplexity=219.88025, train_loss=5.393083

Batch 13900, train_perplexity=217.63745, train_loss=5.3828306

Batch 13910, train_perplexity=198.83678, train_loss=5.2924843

Batch 13920, train_perplexity=197.26129, train_loss=5.284529

Batch 13930, train_perplexity=181.18008, train_loss=5.1994915

Batch 13940, train_perplexity=204.91321, train_loss=5.3225865

Batch 13950, train_perplexity=217.11049, train_loss=5.3804064

Batch 13960, train_perplexity=182.64917, train_loss=5.207567

Batch 13970, train_perplexity=217.32603, train_loss=5.3813987

Batch 13980, train_perplexity=231.01001, train_loss=5.442461

Batch 13990, train_perplexity=196.98128, train_loss=5.2831087

Batch 14000, train_perplexity=191.11955, train_loss=5.252899

Batch 14010, train_perplexity=207.54776, train_loss=5.3353615

Batch 14020, train_perplexity=184.81091, train_loss=5.219333

Batch 14030, train_perplexity=193.86043, train_loss=5.2671385

Batch 14040, train_perplexity=198.3909, train_loss=5.2902393

Batch 14050, train_perplexity=184.1321, train_loss=5.2156534

Batch 14060, train_perplexity=207.61249, train_loss=5.3356733

Batch 14070, train_perplexity=211.71082, train_loss=5.3552213

Batch 14080, train_perplexity=197.1696, train_loss=5.2840643

Batch 14090, train_perplexity=208.36952, train_loss=5.339313

Batch 14100, train_perplexity=180.48053, train_loss=5.195623

Batch 14110, train_perplexity=198.5709, train_loss=5.2911463

Batch 14120, train_perplexity=179.38446, train_loss=5.1895313

Batch 14130, train_perplexity=213.85869, train_loss=5.3653154

Batch 14140, train_perplexity=198.40225, train_loss=5.2902966

Batch 14150, train_perplexity=203.94087, train_loss=5.31783

Batch 14160, train_perplexity=179.5623, train_loss=5.190522

Batch 14170, train_perplexity=216.11809, train_loss=5.375825

Batch 14180, train_perplexity=208.574, train_loss=5.340294

Batch 14190, train_perplexity=211.62624, train_loss=5.3548217

Batch 14200, train_perplexity=193.30954, train_loss=5.2642927

Batch 14210, train_perplexity=182.92145, train_loss=5.209057

Batch 14220, train_perplexity=195.48024, train_loss=5.2754593

Batch 14230, train_perplexity=202.38322, train_loss=5.310163

Batch 14240, train_perplexity=188.76581, train_loss=5.240507

Batch 14250, train_perplexity=177.26059, train_loss=5.177621

Batch 14260, train_perplexity=209.37157, train_loss=5.3441105

Batch 14270, train_perplexity=205.91592, train_loss=5.327468

Batch 14280, train_perplexity=188.90501, train_loss=5.2412443

Batch 14290, train_perplexity=183.38992, train_loss=5.2116146

Batch 14300, train_perplexity=178.31656, train_loss=5.1835604

Batch 14310, train_perplexity=186.38522, train_loss=5.2278156

Batch 14320, train_perplexity=173.39107, train_loss=5.1555495

Batch 14330, train_perplexity=196.10558, train_loss=5.278653

Batch 14340, train_perplexity=177.7188, train_loss=5.1802025

Batch 14350, train_perplexity=172.68542, train_loss=5.1514716

Batch 14360, train_perplexity=182.95757, train_loss=5.2092543

Batch 14370, train_perplexity=193.28271, train_loss=5.264154

Batch 14380, train_perplexity=191.18764, train_loss=5.2532554

Batch 14390, train_perplexity=211.45566, train_loss=5.3540154

Batch 14400, train_perplexity=190.07712, train_loss=5.24743

Batch 14410, train_perplexity=174.7919, train_loss=5.163596

Batch 14420, train_perplexity=212.12827, train_loss=5.357191

Batch 14430, train_perplexity=195.48965, train_loss=5.2755075

Batch 14440, train_perplexity=188.05444, train_loss=5.2367315

Batch 14450, train_perplexity=206.59709, train_loss=5.3307705

Batch 14460, train_perplexity=198.56334, train_loss=5.291108

Batch 14470, train_perplexity=202.1605, train_loss=5.309062

Batch 14480, train_perplexity=182.4977, train_loss=5.2067375

Batch 14490, train_perplexity=191.16531, train_loss=5.2531385

Batch 14500, train_perplexity=186.9073, train_loss=5.2306128

Batch 14510, train_perplexity=187.37936, train_loss=5.233135

Batch 14520, train_perplexity=184.62796, train_loss=5.218343

Batch 14530, train_perplexity=169.75116, train_loss=5.1343336

Batch 14540, train_perplexity=207.82446, train_loss=5.336694

Batch 14550, train_perplexity=187.9069, train_loss=5.2359467

Batch 14560, train_perplexity=198.18384, train_loss=5.289195

Batch 14570, train_perplexity=183.95746, train_loss=5.2147045

Batch 14580, train_perplexity=193.18025, train_loss=5.2636237

Batch 14590, train_perplexity=181.71384, train_loss=5.202433

Batch 14600, train_perplexity=173.16748, train_loss=5.154259

Batch 14610, train_perplexity=212.91931, train_loss=5.3609133

Batch 14620, train_perplexity=185.23448, train_loss=5.2216225

Batch 14630, train_perplexity=195.31914, train_loss=5.274635

Batch 14640, train_perplexity=227.97607, train_loss=5.4292407

Batch 14650, train_perplexity=197.51457, train_loss=5.2858124

Batch 14660, train_perplexity=219.01477, train_loss=5.389139

Batch 14670, train_perplexity=213.09604, train_loss=5.361743

Batch 14680, train_perplexity=200.23409, train_loss=5.299487

Batch 14690, train_perplexity=193.03247, train_loss=5.2628584

Batch 14700, train_perplexity=186.75443, train_loss=5.2297945

Batch 14710, train_perplexity=186.71774, train_loss=5.229598

Batch 14720, train_perplexity=179.15417, train_loss=5.1882467

Batch 14730, train_perplexity=213.9162, train_loss=5.3655844

Batch 14740, train_perplexity=186.41508, train_loss=5.227976

Batch 14750, train_perplexity=199.49721, train_loss=5.2958

Batch 14760, train_perplexity=151.46799, train_loss=5.0203743

Batch 14770, train_perplexity=185.02438, train_loss=5.2204876

Batch 14780, train_perplexity=186.28856, train_loss=5.227297

Batch 14790, train_perplexity=191.48882, train_loss=5.2548294

Batch 14800, train_perplexity=173.73576, train_loss=5.1575356

Batch 14810, train_perplexity=180.39595, train_loss=5.195154

Batch 14820, train_perplexity=189.53986, train_loss=5.2445993

Batch 14830, train_perplexity=178.61014, train_loss=5.1852055

Batch 14840, train_perplexity=182.79938, train_loss=5.2083893

Batch 14850, train_perplexity=177.93365, train_loss=5.181411

Batch 14860, train_perplexity=184.22238, train_loss=5.2161436

Batch 14870, train_perplexity=193.77393, train_loss=5.266692

Batch 14880, train_perplexity=200.75092, train_loss=5.302065

Batch 14890, train_perplexity=166.79611, train_loss=5.116772

Batch 14900, train_perplexity=199.09474, train_loss=5.293781

Batch 14910, train_perplexity=177.63983, train_loss=5.179758

Batch 14920, train_perplexity=175.51569, train_loss=5.1677284

Batch 14930, train_perplexity=193.9701, train_loss=5.267704

Batch 14940, train_perplexity=182.96158, train_loss=5.209276
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 14950, train_perplexity=194.83441, train_loss=5.27215

Batch 14960, train_perplexity=197.05344, train_loss=5.283475

Batch 14970, train_perplexity=177.6278, train_loss=5.1796904

Batch 14980, train_perplexity=201.76933, train_loss=5.307125

Batch 14990, train_perplexity=179.18117, train_loss=5.1883974

Batch 15000, train_perplexity=188.81586, train_loss=5.2407722

Batch 15010, train_perplexity=202.50978, train_loss=5.310788

Batch 15020, train_perplexity=197.07626, train_loss=5.283591

Batch 15030, train_perplexity=211.99559, train_loss=5.3565655

Batch 15040, train_perplexity=205.6455, train_loss=5.3261538

Batch 15050, train_perplexity=200.69406, train_loss=5.3017817

Batch 15060, train_perplexity=197.2279, train_loss=5.28436

Batch 15070, train_perplexity=180.17992, train_loss=5.193956

Batch 15080, train_perplexity=160.27971, train_loss=5.0769205

Batch 15090, train_perplexity=192.64607, train_loss=5.2608547

Batch 15100, train_perplexity=203.44609, train_loss=5.315401

Batch 15110, train_perplexity=186.34755, train_loss=5.2276134

Batch 15120, train_perplexity=181.60374, train_loss=5.201827

Batch 15130, train_perplexity=192.856, train_loss=5.261944

Batch 15140, train_perplexity=214.44432, train_loss=5.36805

Batch 15150, train_perplexity=233.27623, train_loss=5.4522233

Batch 15160, train_perplexity=188.61214, train_loss=5.2396927

Batch 15170, train_perplexity=189.53905, train_loss=5.244595

Batch 15180, train_perplexity=201.06706, train_loss=5.3036385

Batch 15190, train_perplexity=200.3723, train_loss=5.300177

Batch 15200, train_perplexity=171.21198, train_loss=5.1429024

Batch 15210, train_perplexity=193.46446, train_loss=5.265094

Batch 15220, train_perplexity=200.20908, train_loss=5.299362

Batch 15230, train_perplexity=187.33862, train_loss=5.232918

Batch 15240, train_perplexity=204.20886, train_loss=5.3191433

Batch 15250, train_perplexity=216.79144, train_loss=5.378936

Batch 15260, train_perplexity=209.51778, train_loss=5.3448086

Batch 15270, train_perplexity=189.07173, train_loss=5.2421265

Batch 15280, train_perplexity=203.15933, train_loss=5.3139906

Batch 15290, train_perplexity=206.12497, train_loss=5.3284826

Batch 15300, train_perplexity=195.82823, train_loss=5.277238

Batch 15310, train_perplexity=165.65387, train_loss=5.1099005

Batch 15320, train_perplexity=192.2854, train_loss=5.2589808

Batch 15330, train_perplexity=178.90166, train_loss=5.1868362

Batch 15340, train_perplexity=192.49034, train_loss=5.260046

Batch 15350, train_perplexity=193.97786, train_loss=5.267744

Batch 15360, train_perplexity=195.26782, train_loss=5.274372

Batch 15370, train_perplexity=177.5554, train_loss=5.1792827

Batch 15380, train_perplexity=166.3694, train_loss=5.1142106

Batch 15390, train_perplexity=210.49011, train_loss=5.3494387

Batch 15400, train_perplexity=196.68564, train_loss=5.2816067

Batch 15410, train_perplexity=208.66025, train_loss=5.3407073

Batch 15420, train_perplexity=169.89336, train_loss=5.135171

Batch 15430, train_perplexity=192.14243, train_loss=5.258237

Batch 15440, train_perplexity=184.98001, train_loss=5.2202477

Batch 15450, train_perplexity=182.57986, train_loss=5.2071877

Batch 15460, train_perplexity=190.47452, train_loss=5.2495184

Batch 15470, train_perplexity=206.55139, train_loss=5.3305492

Batch 15480, train_perplexity=185.7566, train_loss=5.224437

Batch 15490, train_perplexity=195.2926, train_loss=5.274499

Batch 15500, train_perplexity=168.56079, train_loss=5.1272964

Batch 15510, train_perplexity=178.73488, train_loss=5.1859035

Batch 15520, train_perplexity=204.14996, train_loss=5.318855

Batch 15530, train_perplexity=183.48692, train_loss=5.2121434

Batch 15540, train_perplexity=183.29358, train_loss=5.211089

Batch 15550, train_perplexity=214.36417, train_loss=5.3676763

Batch 15560, train_perplexity=197.51617, train_loss=5.2858205

Batch 15570, train_perplexity=171.17165, train_loss=5.142667

Batch 15580, train_perplexity=198.30559, train_loss=5.289809

Batch 15590, train_perplexity=166.38899, train_loss=5.1143284

Batch 15600, train_perplexity=202.77618, train_loss=5.312103

Batch 15610, train_perplexity=192.06694, train_loss=5.257844

Batch 15620, train_perplexity=211.57004, train_loss=5.354556

Batch 15630, train_perplexity=190.89586, train_loss=5.251728

Batch 15640, train_perplexity=200.78442, train_loss=5.302232

Batch 15650, train_perplexity=190.40042, train_loss=5.2491293

Batch 15660, train_perplexity=208.35571, train_loss=5.3392467

Batch 15670, train_perplexity=186.94508, train_loss=5.230815

Batch 15680, train_perplexity=194.02135, train_loss=5.267968

Batch 15690, train_perplexity=197.95433, train_loss=5.2880363

Batch 15700, train_perplexity=193.47386, train_loss=5.2651424

Batch 15710, train_perplexity=206.02101, train_loss=5.327978

Batch 15720, train_perplexity=201.56604, train_loss=5.306117

Batch 15730, train_perplexity=181.977, train_loss=5.2038803

Batch 15740, train_perplexity=204.06519, train_loss=5.3184395

Batch 15750, train_perplexity=189.63885, train_loss=5.2451215

Batch 15760, train_perplexity=185.42104, train_loss=5.222629

Batch 15770, train_perplexity=173.52266, train_loss=5.156308

Batch 15780, train_perplexity=193.84029, train_loss=5.2670345

Batch 15790, train_perplexity=199.66298, train_loss=5.296631

Batch 15800, train_perplexity=185.76263, train_loss=5.2244697

Batch 15810, train_perplexity=183.67802, train_loss=5.2131844

Batch 15820, train_perplexity=168.46533, train_loss=5.12673

Batch 15830, train_perplexity=200.44215, train_loss=5.3005257

Batch 15840, train_perplexity=173.60492, train_loss=5.156782

Batch 15850, train_perplexity=183.4921, train_loss=5.2121716

Batch 15860, train_perplexity=176.36638, train_loss=5.1725636

Batch 15870, train_perplexity=201.83649, train_loss=5.307458

Batch 15880, train_perplexity=212.64546, train_loss=5.3596263

Batch 15890, train_perplexity=203.54138, train_loss=5.3158693

Batch 15900, train_perplexity=195.76016, train_loss=5.2768903

Batch 15910, train_perplexity=186.54811, train_loss=5.228689

Batch 15920, train_perplexity=205.10011, train_loss=5.3234982

Batch 15930, train_perplexity=203.49849, train_loss=5.3156586

Batch 15940, train_perplexity=166.35884, train_loss=5.114147

Batch 15950, train_perplexity=184.33943, train_loss=5.2167788

Batch 15960, train_perplexity=207.48334, train_loss=5.335051

Batch 15970, train_perplexity=192.15378, train_loss=5.258296

Batch 15980, train_perplexity=211.51677, train_loss=5.3543043

Batch 15990, train_perplexity=184.29002, train_loss=5.216511

Batch 16000, train_perplexity=185.34793, train_loss=5.2222347

Batch 16010, train_perplexity=183.97623, train_loss=5.2148066

Batch 16020, train_perplexity=176.58105, train_loss=5.17378

Batch 16030, train_perplexity=207.82188, train_loss=5.3366814

Batch 16040, train_perplexity=198.37141, train_loss=5.290141

Batch 16050, train_perplexity=188.32895, train_loss=5.23819

Batch 16060, train_perplexity=203.62206, train_loss=5.3162656

Batch 16070, train_perplexity=189.73816, train_loss=5.245645

Batch 16080, train_perplexity=204.17235, train_loss=5.3189645

Batch 16090, train_perplexity=194.45667, train_loss=5.2702093

Batch 16100, train_perplexity=181.29727, train_loss=5.200138

Batch 16110, train_perplexity=193.3194, train_loss=5.2643437

Batch 16120, train_perplexity=194.90819, train_loss=5.2725286

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 16130, train_perplexity=182.81464, train_loss=5.2084727

Batch 16140, train_perplexity=191.53703, train_loss=5.255081

Batch 16150, train_perplexity=184.60313, train_loss=5.2182083

Batch 16160, train_perplexity=161.49083, train_loss=5.0844483

Batch 16170, train_perplexity=188.92915, train_loss=5.241372

Batch 16180, train_perplexity=166.55443, train_loss=5.115322

Batch 16190, train_perplexity=173.42819, train_loss=5.1557636
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 16200, train_perplexity=185.20357, train_loss=5.2214556

Batch 16210, train_perplexity=175.3626, train_loss=5.166856

Batch 16220, train_perplexity=183.4641, train_loss=5.212019

Batch 16230, train_perplexity=172.62755, train_loss=5.1511364

Batch 16240, train_perplexity=167.09496, train_loss=5.118562

Batch 16250, train_perplexity=172.89001, train_loss=5.1526556

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 16260, train_perplexity=196.25627, train_loss=5.2794213

Batch 16270, train_perplexity=185.66779, train_loss=5.223959

Batch 16280, train_perplexity=203.90587, train_loss=5.3176584

Batch 16290, train_perplexity=194.2789, train_loss=5.2692947

Batch 16300, train_perplexity=181.46031, train_loss=5.201037

Batch 16310, train_perplexity=175.86218, train_loss=5.1697006

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 16320, train_perplexity=182.34381, train_loss=5.205894

Batch 16330, train_perplexity=202.69951, train_loss=5.3117247

Batch 16340, train_perplexity=166.60478, train_loss=5.1156244

Batch 16350, train_perplexity=181.94083, train_loss=5.2036815

Batch 16360, train_perplexity=182.4782, train_loss=5.2066307

Batch 16370, train_perplexity=185.97818, train_loss=5.2256293

Batch 16380, train_perplexity=159.13695, train_loss=5.069765

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 16390, train_perplexity=172.27495, train_loss=5.1490917

Batch 16400, train_perplexity=184.21843, train_loss=5.216122

Batch 16410, train_perplexity=213.2896, train_loss=5.362651

Batch 16420, train_perplexity=174.33966, train_loss=5.1610055

Batch 16430, train_perplexity=180.02414, train_loss=5.193091

Batch 16440, train_perplexity=198.52348, train_loss=5.2909074

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 16450, train_perplexity=185.54582, train_loss=5.223302

Batch 16460, train_perplexity=169.62656, train_loss=5.1335993

Batch 16470, train_perplexity=168.4785, train_loss=5.126808

Batch 16480, train_perplexity=188.07292, train_loss=5.2368298

Batch 16490, train_perplexity=191.5426, train_loss=5.2551103

Batch 16500, train_perplexity=157.75667, train_loss=5.0610538

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 16510, train_perplexity=216.4605, train_loss=5.377408

Batch 16520, train_perplexity=202.86032, train_loss=5.3125176

Batch 16530, train_perplexity=193.02197, train_loss=5.262804

Batch 16540, train_perplexity=194.72212, train_loss=5.2715735

Batch 16550, train_perplexity=193.7173, train_loss=5.2664

Batch 16560, train_perplexity=185.37029, train_loss=5.2223554

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 16570, train_perplexity=182.95496, train_loss=5.20924

Batch 16580, train_perplexity=183.11676, train_loss=5.210124

Batch 16590, train_perplexity=213.71188, train_loss=5.364629

Batch 16600, train_perplexity=195.46458, train_loss=5.275379

Batch 16610, train_perplexity=195.31104, train_loss=5.2745934

Batch 16620, train_perplexity=177.23709, train_loss=5.1774883

Batch 16630, train_perplexity=180.13997, train_loss=5.193734

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 16640, train_perplexity=180.42271, train_loss=5.1953025

Batch 16650, train_perplexity=190.764, train_loss=5.251037

Batch 16660, train_perplexity=186.87549, train_loss=5.2304425

Batch 16670, train_perplexity=176.43484, train_loss=5.1729517

Batch 16680, train_perplexity=184.2345, train_loss=5.2162094

Batch 16690, train_perplexity=197.48633, train_loss=5.2856693

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 16700, train_perplexity=196.3157, train_loss=5.279724

Batch 16710, train_perplexity=171.023, train_loss=5.141798

Batch 16720, train_perplexity=180.60175, train_loss=5.1962943

Batch 16730, train_perplexity=205.11, train_loss=5.3235464

Batch 16740, train_perplexity=193.60677, train_loss=5.265829

Batch 16750, train_perplexity=198.73422, train_loss=5.2919683

Batch 16760, train_perplexity=182.09158, train_loss=5.2045097

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 16770, train_perplexity=176.78232, train_loss=5.174919

Batch 16780, train_perplexity=203.372, train_loss=5.315037

Batch 16790, train_perplexity=179.4144, train_loss=5.189698

Batch 16800, train_perplexity=192.22911, train_loss=5.258688

Batch 16810, train_perplexity=190.88548, train_loss=5.2516737

Batch 16820, train_perplexity=177.2136, train_loss=5.177356

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 16830, train_perplexity=196.13858, train_loss=5.2788215

Batch 16840, train_perplexity=156.86687, train_loss=5.0553975

Batch 16850, train_perplexity=185.01855, train_loss=5.220456

Batch 16860, train_perplexity=196.59018, train_loss=5.2811213

Batch 16870, train_perplexity=193.11679, train_loss=5.263295

Batch 16880, train_perplexity=161.02615, train_loss=5.081567

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 16890, train_perplexity=220.67854, train_loss=5.396707

Batch 16900, train_perplexity=182.7369, train_loss=5.2080474

Batch 16910, train_perplexity=169.33804, train_loss=5.131897

Batch 16920, train_perplexity=185.92986, train_loss=5.2253695

Batch 16930, train_perplexity=179.174, train_loss=5.1883574

Batch 16940, train_perplexity=161.01633, train_loss=5.081506

Batch 16950, train_perplexity=190.50803, train_loss=5.2496943

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 16960, train_perplexity=186.26901, train_loss=5.227192

Batch 16970, train_perplexity=186.3518, train_loss=5.2276363

Batch 16980, train_perplexity=197.24408, train_loss=5.284442

Batch 16990, train_perplexity=177.83408, train_loss=5.180851

Batch 17000, train_perplexity=174.80875, train_loss=5.1636925

Batch 17010, train_perplexity=193.31654, train_loss=5.264329

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 17020, train_perplexity=198.15321, train_loss=5.2890406

Batch 17030, train_perplexity=180.15569, train_loss=5.1938214

Batch 17040, train_perplexity=201.02008, train_loss=5.303405

Batch 17050, train_perplexity=183.23993, train_loss=5.2107964

Batch 17060, train_perplexity=180.42572, train_loss=5.195319

Batch 17070, train_perplexity=162.47005, train_loss=5.0904937

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 17080, train_perplexity=168.26414, train_loss=5.125535

Batch 17090, train_perplexity=202.40204, train_loss=5.310256

Batch 17100, train_perplexity=192.01071, train_loss=5.257551

Batch 17110, train_perplexity=177.94962, train_loss=5.1815004

Batch 17120, train_perplexity=173.26271, train_loss=5.154809

Batch 17130, train_perplexity=193.7778, train_loss=5.266712

Batch 17140, train_perplexity=191.99698, train_loss=5.2574797

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 17150, train_perplexity=189.65848, train_loss=5.245225

Batch 17160, train_perplexity=173.24635, train_loss=5.1547146

Batch 17170, train_perplexity=166.01082, train_loss=5.112053

Batch 17180, train_perplexity=180.3429, train_loss=5.19486

Batch 17190, train_perplexity=179.80348, train_loss=5.1918645

Batch 17200, train_perplexity=181.73541, train_loss=5.202552

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 17210, train_perplexity=199.44165, train_loss=5.2955217

Batch 17220, train_perplexity=196.65881, train_loss=5.2814703

Batch 17230, train_perplexity=202.1074, train_loss=5.3087993

Batch 17240, train_perplexity=187.98389, train_loss=5.2363563

Batch 17250, train_perplexity=182.19945, train_loss=5.205102

Batch 17260, train_perplexity=192.53955, train_loss=5.2603016

Batch 17270, train_perplexity=183.30441, train_loss=5.2111483

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 17280, train_perplexity=158.05641, train_loss=5.062952

Batch 17290, train_perplexity=166.41344, train_loss=5.1144753

Batch 17300, train_perplexity=197.082, train_loss=5.28362

Batch 17310, train_perplexity=175.63339, train_loss=5.168399

Batch 17320, train_perplexity=205.7424, train_loss=5.326625

Batch 17330, train_perplexity=172.78139, train_loss=5.152027

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 17340, train_perplexity=174.6626, train_loss=5.162856

Batch 17350, train_perplexity=162.58095, train_loss=5.091176

Batch 17360, train_perplexity=185.66194, train_loss=5.2239275

Batch 17370, train_perplexity=180.63559, train_loss=5.1964817

Batch 17380, train_perplexity=195.22389, train_loss=5.274147

Batch 17390, train_perplexity=209.06908, train_loss=5.3426647

Batch 17400, train_perplexity=187.55725, train_loss=5.234084

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 17410, train_perplexity=195.27156, train_loss=5.274391

Batch 17420, train_perplexity=167.38707, train_loss=5.120309

Batch 17430, train_perplexity=179.94714, train_loss=5.192663

Batch 17440, train_perplexity=181.83449, train_loss=5.203097

Batch 17450, train_perplexity=183.83075, train_loss=5.2140155

Batch 17460, train_perplexity=193.36652, train_loss=5.2645874

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 17470, train_perplexity=166.68289, train_loss=5.116093

Batch 17480, train_perplexity=186.6362, train_loss=5.2291613

Batch 17490, train_perplexity=170.8867, train_loss=5.1410007

Batch 17500, train_perplexity=192.32594, train_loss=5.2591915

Batch 17510, train_perplexity=166.88704, train_loss=5.117317

Batch 17520, train_perplexity=183.16008, train_loss=5.2103605

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 17530, train_perplexity=171.6141, train_loss=5.1452484

Batch 17540, train_perplexity=206.91888, train_loss=5.332327

Batch 17550, train_perplexity=190.22473, train_loss=5.248206

Batch 17560, train_perplexity=198.83943, train_loss=5.2924976

Batch 17570, train_perplexity=192.13115, train_loss=5.258178

Batch 17580, train_perplexity=157.32727, train_loss=5.058328

Batch 17590, train_perplexity=187.14185, train_loss=5.231867

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 17600, train_perplexity=191.43796, train_loss=5.254564

Batch 17610, train_perplexity=184.39366, train_loss=5.217073

Batch 17620, train_perplexity=177.34868, train_loss=5.1781178

Batch 17630, train_perplexity=186.33617, train_loss=5.2275524

Batch 17640, train_perplexity=179.33331, train_loss=5.189246

Batch 17650, train_perplexity=176.84875, train_loss=5.175295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 17660, train_perplexity=186.08134, train_loss=5.226184

Batch 17670, train_perplexity=177.75363, train_loss=5.1803985

Batch 17680, train_perplexity=196.76443, train_loss=5.282007

Batch 17690, train_perplexity=171.84586, train_loss=5.146598

Batch 17700, train_perplexity=164.35815, train_loss=5.102048

Batch 17710, train_perplexity=174.10281, train_loss=5.159646

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 17720, train_perplexity=170.60109, train_loss=5.139328

Batch 17730, train_perplexity=172.75041, train_loss=5.151848

Batch 17740, train_perplexity=160.54973, train_loss=5.0786037

Batch 17750, train_perplexity=183.83574, train_loss=5.2140427

Batch 17760, train_perplexity=173.89838, train_loss=5.158471

Batch 17770, train_perplexity=212.61475, train_loss=5.359482

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 17780, train_perplexity=178.84212, train_loss=5.1865034

Batch 17790, train_perplexity=170.39719, train_loss=5.138132

Batch 17800, train_perplexity=175.5136, train_loss=5.1677165

Batch 17810, train_perplexity=188.36963, train_loss=5.238406

Batch 17820, train_perplexity=180.56738, train_loss=5.196104

Batch 17830, train_perplexity=203.31343, train_loss=5.314749

Batch 17840, train_perplexity=176.40726, train_loss=5.1727953

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 17850, train_perplexity=193.70732, train_loss=5.2663484

Batch 17860, train_perplexity=165.44815, train_loss=5.108658

Batch 17870, train_perplexity=188.38707, train_loss=5.2384987

Batch 17880, train_perplexity=169.94109, train_loss=5.135452

Batch 17890, train_perplexity=218.31119, train_loss=5.3859215

Batch 17900, train_perplexity=184.41776, train_loss=5.2172036

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 17910, train_perplexity=183.29184, train_loss=5.2110796

Batch 17920, train_perplexity=169.15065, train_loss=5.1307898

Batch 17930, train_perplexity=173.46938, train_loss=5.156001

Batch 17940, train_perplexity=165.87503, train_loss=5.1112347

Batch 17950, train_perplexity=172.39986, train_loss=5.1498165

Batch 17960, train_perplexity=166.79636, train_loss=5.1167736

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 17970, train_perplexity=191.32526, train_loss=5.253975

Batch 17980, train_perplexity=186.00789, train_loss=5.225789

Batch 17990, train_perplexity=156.00456, train_loss=5.0498853

Batch 18000, train_perplexity=185.29393, train_loss=5.2219434

Batch 18010, train_perplexity=192.2502, train_loss=5.2587976

Batch 18020, train_perplexity=177.38014, train_loss=5.178295

Batch 18030, train_perplexity=197.56073, train_loss=5.286046

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 18040, train_perplexity=167.9315, train_loss=5.123556

Batch 18050, train_perplexity=171.95839, train_loss=5.1472526

Batch 18060, train_perplexity=172.99028, train_loss=5.1532354

Batch 18070, train_perplexity=169.8263, train_loss=5.134776

Batch 18080, train_perplexity=178.34988, train_loss=5.1837473

Batch 18090, train_perplexity=163.40651, train_loss=5.096241

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 18100, train_perplexity=181.37527, train_loss=5.200568

Batch 18110, train_perplexity=187.29753, train_loss=5.2326984

Batch 18120, train_perplexity=191.5522, train_loss=5.2551603

Batch 18130, train_perplexity=186.69495, train_loss=5.229476

Batch 18140, train_perplexity=162.65663, train_loss=5.0916414

Batch 18150, train_perplexity=168.6764, train_loss=5.127982

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 18160, train_perplexity=174.20114, train_loss=5.1602106

Batch 18170, train_perplexity=199.42073, train_loss=5.295417

Batch 18180, train_perplexity=177.9094, train_loss=5.1812744

Batch 18190, train_perplexity=190.49994, train_loss=5.249652

Batch 18200, train_perplexity=180.341, train_loss=5.1948495

Batch 18210, train_perplexity=207.15079, train_loss=5.333447

Batch 18220, train_perplexity=181.88904, train_loss=5.203397

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 18230, train_perplexity=154.25949, train_loss=5.038636

Batch 18240, train_perplexity=189.33336, train_loss=5.2435093

Batch 18250, train_perplexity=154.58812, train_loss=5.0407643

Batch 18260, train_perplexity=169.04906, train_loss=5.130189

Batch 18270, train_perplexity=166.2621, train_loss=5.1135654

Batch 18280, train_perplexity=196.4099, train_loss=5.280204

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 18290, train_perplexity=180.09926, train_loss=5.193508

Batch 18300, train_perplexity=174.96017, train_loss=5.1645584

Batch 18310, train_perplexity=193.02142, train_loss=5.262801

Batch 18320, train_perplexity=159.20805, train_loss=5.070212

Batch 18330, train_perplexity=190.83034, train_loss=5.2513847

Batch 18340, train_perplexity=188.89925, train_loss=5.241214

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 18350, train_perplexity=166.49081, train_loss=5.11494

Batch 18360, train_perplexity=185.47258, train_loss=5.222907

Batch 18370, train_perplexity=159.26675, train_loss=5.0705805

Batch 18380, train_perplexity=162.32129, train_loss=5.0895777

Batch 18390, train_perplexity=185.96878, train_loss=5.225579

Batch 18400, train_perplexity=193.53163, train_loss=5.265441

Batch 18410, train_perplexity=165.9907, train_loss=5.111932

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 18420, train_perplexity=186.53122, train_loss=5.2285986

Batch 18430, train_perplexity=166.29587, train_loss=5.1137686

Batch 18440, train_perplexity=173.40488, train_loss=5.155629

Batch 18450, train_perplexity=191.17642, train_loss=5.2531967

Batch 18460, train_perplexity=180.45627, train_loss=5.1954885

Batch 18470, train_perplexity=163.81944, train_loss=5.098765

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 18480, train_perplexity=178.77997, train_loss=5.186156

Batch 18490, train_perplexity=171.44873, train_loss=5.1442842

Batch 18500, train_perplexity=186.55159, train_loss=5.228708

Batch 18510, train_perplexity=186.59224, train_loss=5.2289257

Batch 18520, train_perplexity=185.72003, train_loss=5.2242403

Batch 18530, train_perplexity=191.96843, train_loss=5.257331

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 18540, train_perplexity=180.4099, train_loss=5.1952314

Batch 18550, train_perplexity=178.10326, train_loss=5.1823635

Batch 18560, train_perplexity=165.71004, train_loss=5.1102395

Batch 18570, train_perplexity=186.19637, train_loss=5.226802

Batch 18580, train_perplexity=152.17078, train_loss=5.0250034

Batch 18590, train_perplexity=152.84819, train_loss=5.029445

Batch 18600, train_perplexity=155.29007, train_loss=5.045295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 18610, train_perplexity=161.71783, train_loss=5.085853

Batch 18620, train_perplexity=181.75777, train_loss=5.202675

Batch 18630, train_perplexity=182.57994, train_loss=5.207188

Batch 18640, train_perplexity=164.052, train_loss=5.1001835

Batch 18650, train_perplexity=181.46385, train_loss=5.2010565

Batch 18660, train_perplexity=179.01242, train_loss=5.187455

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 18670, train_perplexity=170.37363, train_loss=5.137994

Batch 18680, train_perplexity=167.59615, train_loss=5.121557

Batch 18690, train_perplexity=177.64584, train_loss=5.179792

Batch 18700, train_perplexity=182.131, train_loss=5.204726

Batch 18710, train_perplexity=184.69101, train_loss=5.218684

Batch 18720, train_perplexity=174.37018, train_loss=5.1611805

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6057 sentences.
Finished loading
Batch 18730, train_perplexity=182.2342, train_loss=5.2052927

Batch 18740, train_perplexity=163.99115, train_loss=5.0998125

Batch 18750, train_perplexity=172.2154, train_loss=5.148746

Batch 18760, train_perplexity=178.11838, train_loss=5.1824484

Batch 18770, train_perplexity=173.56519, train_loss=5.1565533

Batch 18780, train_perplexity=163.26654, train_loss=5.095384

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 18790, train_perplexity=187.55153, train_loss=5.2340536

Batch 18800, train_perplexity=178.52074, train_loss=5.184705

Batch 18810, train_perplexity=167.88162, train_loss=5.123259

Batch 18820, train_perplexity=177.99612, train_loss=5.1817617

Batch 18830, train_perplexity=170.72658, train_loss=5.1400633

Batch 18840, train_perplexity=168.67126, train_loss=5.1279516

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 18850, train_perplexity=201.96338, train_loss=5.3080864

Batch 18860, train_perplexity=169.80217, train_loss=5.134634

Batch 18870, train_perplexity=181.20152, train_loss=5.1996098

Batch 18880, train_perplexity=194.11314, train_loss=5.268441

Batch 18890, train_perplexity=185.16551, train_loss=5.22125

Batch 18900, train_perplexity=193.99988, train_loss=5.2678576

Batch 18910, train_perplexity=157.03009, train_loss=5.0564375

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 18920, train_perplexity=174.14069, train_loss=5.1598635

Batch 18930, train_perplexity=194.78119, train_loss=5.271877

Batch 18940, train_perplexity=176.66132, train_loss=5.1742344

Batch 18950, train_perplexity=166.53926, train_loss=5.115231

Batch 18960, train_perplexity=173.71994, train_loss=5.1574445

Batch 18970, train_perplexity=182.93489, train_loss=5.2091303

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 18980, train_perplexity=189.07623, train_loss=5.2421503

Batch 18990, train_perplexity=170.15636, train_loss=5.136718

Batch 19000, train_perplexity=157.74892, train_loss=5.0610046

Batch 19010, train_perplexity=181.38426, train_loss=5.200618

Batch 19020, train_perplexity=167.25023, train_loss=5.119491

Batch 19030, train_perplexity=165.24315, train_loss=5.107418

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 19040, train_perplexity=181.65051, train_loss=5.2020845

Batch 19050, train_perplexity=169.72867, train_loss=5.134201

Batch 19060, train_perplexity=177.17287, train_loss=5.177126

Batch 19070, train_perplexity=163.06013, train_loss=5.094119

Batch 19080, train_perplexity=159.31483, train_loss=5.0708823

Batch 19090, train_perplexity=178.12483, train_loss=5.1824846

Batch 19100, train_perplexity=172.34758, train_loss=5.1495132

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 19110, train_perplexity=183.27864, train_loss=5.2110076

Batch 19120, train_perplexity=178.8735, train_loss=5.186679

Batch 19130, train_perplexity=181.17395, train_loss=5.1994576

Batch 19140, train_perplexity=172.86594, train_loss=5.1525164

Batch 19150, train_perplexity=189.6895, train_loss=5.2453885

Batch 19160, train_perplexity=174.48396, train_loss=5.161833

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 19170, train_perplexity=181.3405, train_loss=5.2003765

Batch 19180, train_perplexity=193.4604, train_loss=5.265073

Batch 19190, train_perplexity=162.03107, train_loss=5.087788

Batch 19200, train_perplexity=171.19777, train_loss=5.1428194

Batch 19210, train_perplexity=180.14108, train_loss=5.1937404

Batch 19220, train_perplexity=195.75728, train_loss=5.2768755

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 19230, train_perplexity=168.99731, train_loss=5.129883

Batch 19240, train_perplexity=176.2149, train_loss=5.1717043

Batch 19250, train_perplexity=187.32076, train_loss=5.2328224

Batch 19260, train_perplexity=184.1523, train_loss=5.215763

Batch 19270, train_perplexity=178.50337, train_loss=5.1846075

Batch 19280, train_perplexity=164.3171, train_loss=5.101798

Batch 19290, train_perplexity=164.21088, train_loss=5.1011515

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 19300, train_perplexity=153.79825, train_loss=5.0356417

Batch 19310, train_perplexity=160.61389, train_loss=5.0790033

Batch 19320, train_perplexity=182.217, train_loss=5.2051983

Batch 19330, train_perplexity=166.82777, train_loss=5.116962

Batch 19340, train_perplexity=169.96603, train_loss=5.1355987

Batch 19350, train_perplexity=158.65974, train_loss=5.066762

Batch 19360, train_perplexity=163.2773, train_loss=5.09545

Batch 19370, train_perplexity=171.0734, train_loss=5.1420927

Batch 19380, train_perplexity=161.2193, train_loss=5.0827656

Batch 19390, train_perplexity=163.05656, train_loss=5.094097

Batch 19400, train_perplexity=178.6581, train_loss=5.185474

Batch 19410, train_perplexity=176.9353, train_loss=5.175784

Batch 19420, train_perplexity=154.63701, train_loss=5.0410805
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19430, train_perplexity=178.07083, train_loss=5.1821814

Batch 19440, train_perplexity=173.4923, train_loss=5.156133

Batch 19450, train_perplexity=170.36371, train_loss=5.1379356

Batch 19460, train_perplexity=146.3976, train_loss=4.986326

Batch 19470, train_perplexity=157.83545, train_loss=5.061553

Batch 19480, train_perplexity=149.37404, train_loss=5.0064535

Batch 19490, train_perplexity=172.82498, train_loss=5.1522794

Batch 19500, train_perplexity=174.26146, train_loss=5.160557

Batch 19510, train_perplexity=181.753, train_loss=5.2026486

Batch 19520, train_perplexity=170.51544, train_loss=5.138826

Batch 19530, train_perplexity=176.78796, train_loss=5.174951

Batch 19540, train_perplexity=181.58565, train_loss=5.2017274

Batch 19550, train_perplexity=168.15137, train_loss=5.1248646

Batch 19560, train_perplexity=154.69823, train_loss=5.0414762

Batch 19570, train_perplexity=173.82243, train_loss=5.1580343

Batch 19580, train_perplexity=177.60782, train_loss=5.179578

Batch 19590, train_perplexity=210.0792, train_loss=5.3474846

Batch 19600, train_perplexity=156.04199, train_loss=5.050125

Batch 19610, train_perplexity=151.7825, train_loss=5.0224485

Batch 19620, train_perplexity=143.92264, train_loss=4.969276

Batch 19630, train_perplexity=158.10104, train_loss=5.0632343

Batch 19640, train_perplexity=165.0278, train_loss=5.106114

Batch 19650, train_perplexity=176.50822, train_loss=5.1733675

Batch 19660, train_perplexity=161.71083, train_loss=5.0858097

Batch 19670, train_perplexity=172.09283, train_loss=5.148034

Batch 19680, train_perplexity=154.55113, train_loss=5.040525

Batch 19690, train_perplexity=197.74509, train_loss=5.2869787

Batch 19700, train_perplexity=154.60353, train_loss=5.040864

Batch 19710, train_perplexity=161.8534, train_loss=5.086691

Batch 19720, train_perplexity=166.5808, train_loss=5.1154804

Batch 19730, train_perplexity=166.2456, train_loss=5.1134663

Batch 19740, train_perplexity=182.88326, train_loss=5.208848

Batch 19750, train_perplexity=183.56787, train_loss=5.2125845

Batch 19760, train_perplexity=153.36179, train_loss=5.0327997

Batch 19770, train_perplexity=160.48383, train_loss=5.078193

Batch 19780, train_perplexity=165.9198, train_loss=5.1115046

Batch 19790, train_perplexity=171.25949, train_loss=5.14318

Batch 19800, train_perplexity=161.8645, train_loss=5.0867596

Batch 19810, train_perplexity=182.16609, train_loss=5.204919

Batch 19820, train_perplexity=177.65474, train_loss=5.179842

Batch 19830, train_perplexity=174.95918, train_loss=5.1645527

Batch 19840, train_perplexity=173.12083, train_loss=5.15399

Batch 19850, train_perplexity=176.9796, train_loss=5.1760345

Batch 19860, train_perplexity=157.33784, train_loss=5.0583954

Batch 19870, train_perplexity=158.81659, train_loss=5.06775

Batch 19880, train_perplexity=179.5409, train_loss=5.190403

Batch 19890, train_perplexity=170.06795, train_loss=5.136198

Batch 19900, train_perplexity=179.15845, train_loss=5.1882706

Batch 19910, train_perplexity=172.79391, train_loss=5.1520996

Batch 19920, train_perplexity=182.13864, train_loss=5.204768

Batch 19930, train_perplexity=162.17879, train_loss=5.0886993

Batch 19940, train_perplexity=168.37225, train_loss=5.1261773

Batch 19950, train_perplexity=169.62575, train_loss=5.1335945

Batch 19960, train_perplexity=182.3459, train_loss=5.2059054

Batch 19970, train_perplexity=152.71611, train_loss=5.0285807

Batch 19980, train_perplexity=163.39497, train_loss=5.0961704

Batch 19990, train_perplexity=177.81407, train_loss=5.1807384

Batch 20000, train_perplexity=170.58856, train_loss=5.1392546

Batch 20010, train_perplexity=163.30742, train_loss=5.0956345

Batch 20020, train_perplexity=176.38388, train_loss=5.1726627

Batch 20030, train_perplexity=163.5504, train_loss=5.0971212

Batch 20040, train_perplexity=187.56316, train_loss=5.2341156

Batch 20050, train_perplexity=174.84259, train_loss=5.163886

Batch 20060, train_perplexity=158.1398, train_loss=5.0634794

Batch 20070, train_perplexity=163.1986, train_loss=5.094968

Batch 20080, train_perplexity=196.51407, train_loss=5.280734

Batch 20090, train_perplexity=164.39178, train_loss=5.1022525

Batch 20100, train_perplexity=167.75719, train_loss=5.1225176

Batch 20110, train_perplexity=188.11275, train_loss=5.2370415

Batch 20120, train_perplexity=177.77702, train_loss=5.18053

Batch 20130, train_perplexity=174.58, train_loss=5.162383

Batch 20140, train_perplexity=173.80885, train_loss=5.157956

Batch 20150, train_perplexity=164.94307, train_loss=5.1056004

Batch 20160, train_perplexity=172.27051, train_loss=5.149066

Batch 20170, train_perplexity=176.21205, train_loss=5.171688

Batch 20180, train_perplexity=152.79826, train_loss=5.0291185

Batch 20190, train_perplexity=165.5659, train_loss=5.1093693

Batch 20200, train_perplexity=164.23563, train_loss=5.101302

Batch 20210, train_perplexity=163.66212, train_loss=5.097804

Batch 20220, train_perplexity=195.544, train_loss=5.2757854

Batch 20230, train_perplexity=178.65955, train_loss=5.185482

Batch 20240, train_perplexity=151.248, train_loss=5.018921

Batch 20250, train_perplexity=160.74431, train_loss=5.079815

Batch 20260, train_perplexity=181.95566, train_loss=5.203763

Batch 20270, train_perplexity=176.12787, train_loss=5.1712103

Batch 20280, train_perplexity=170.8476, train_loss=5.140772

Batch 20290, train_perplexity=184.92586, train_loss=5.219955

Batch 20300, train_perplexity=173.89456, train_loss=5.158449

Batch 20310, train_perplexity=182.9136, train_loss=5.209014

Batch 20320, train_perplexity=172.12846, train_loss=5.148241

Batch 20330, train_perplexity=163.01653, train_loss=5.0938516

Batch 20340, train_perplexity=179.73053, train_loss=5.1914587

Batch 20350, train_perplexity=179.2345, train_loss=5.188695

Batch 20360, train_perplexity=166.74768, train_loss=5.116482

Batch 20370, train_perplexity=158.73663, train_loss=5.0672464

Batch 20380, train_perplexity=156.99431, train_loss=5.0562096

Batch 20390, train_perplexity=180.36499, train_loss=5.1949825

Batch 20400, train_perplexity=164.69566, train_loss=5.1040993

Batch 20410, train_perplexity=193.70834, train_loss=5.2663536

Batch 20420, train_perplexity=167.93478, train_loss=5.1235757

Batch 20430, train_perplexity=170.88051, train_loss=5.1409645

Batch 20440, train_perplexity=154.51582, train_loss=5.0402966

Batch 20450, train_perplexity=187.85252, train_loss=5.235657

Batch 20460, train_perplexity=170.54602, train_loss=5.139005

Batch 20470, train_perplexity=140.12187, train_loss=4.9425125

Batch 20480, train_perplexity=191.40337, train_loss=5.254383

Batch 20490, train_perplexity=158.94644, train_loss=5.0685673

Batch 20500, train_perplexity=178.92009, train_loss=5.1869392

Batch 20510, train_perplexity=171.74632, train_loss=5.1460185

Batch 20520, train_perplexity=177.33752, train_loss=5.178055

Batch 20530, train_perplexity=176.41702, train_loss=5.1728506

Batch 20540, train_perplexity=163.56476, train_loss=5.097209

Batch 20550, train_perplexity=176.9245, train_loss=5.175723

Batch 20560, train_perplexity=158.3467, train_loss=5.064787

Batch 20570, train_perplexity=162.54468, train_loss=5.090953

Batch 20580, train_perplexity=181.84308, train_loss=5.203144

Batch 20590, train_perplexity=157.57751, train_loss=5.0599174

Batch 20600, train_perplexity=185.13028, train_loss=5.22106

Batch 20610, train_perplexity=148.11198, train_loss=4.9979687

Batch 20620, train_perplexity=177.95064, train_loss=5.181506

Batch 20630, train_perplexity=171.5777, train_loss=5.145036

Batch 20640, train_perplexity=162.62065, train_loss=5.09142

Batch 20650, train_perplexity=172.32375, train_loss=5.149375

Batch 20660, train_perplexity=169.23836, train_loss=5.131308

Batch 20670, train_perplexity=168.0555, train_loss=5.1242943

Batch 20680, train_perplexity=175.08153, train_loss=5.1652517

Batch 20690, train_perplexity=180.42882, train_loss=5.1953363

Batch 20700, train_perplexity=179.44768, train_loss=5.1898837

Batch 20710, train_perplexity=146.75552, train_loss=4.988768

Batch 20720, train_perplexity=181.74036, train_loss=5.202579

Batch 20730, train_perplexity=158.9459, train_loss=5.068564

Batch 20740, train_perplexity=178.97043, train_loss=5.1872206
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 20750, train_perplexity=166.69124, train_loss=5.116143

Batch 20760, train_perplexity=185.13753, train_loss=5.221099

Batch 20770, train_perplexity=160.9322, train_loss=5.080983

Batch 20780, train_perplexity=164.57068, train_loss=5.10334

Batch 20790, train_perplexity=160.6856, train_loss=5.0794497

Batch 20800, train_perplexity=175.23553, train_loss=5.166131

Batch 20810, train_perplexity=162.2874, train_loss=5.089369

Batch 20820, train_perplexity=144.38353, train_loss=4.972473

Batch 20830, train_perplexity=139.18611, train_loss=4.935812

Batch 20840, train_perplexity=179.12607, train_loss=5.18809

Batch 20850, train_perplexity=188.30183, train_loss=5.238046

Batch 20860, train_perplexity=167.82687, train_loss=5.122933

Batch 20870, train_perplexity=168.53186, train_loss=5.127125

Batch 20880, train_perplexity=161.90579, train_loss=5.0870147

Batch 20890, train_perplexity=176.98517, train_loss=5.176066

Batch 20900, train_perplexity=152.31038, train_loss=5.0259204

Batch 20910, train_perplexity=138.33746, train_loss=4.929696

Batch 20920, train_perplexity=167.31277, train_loss=5.119865

Batch 20930, train_perplexity=160.42247, train_loss=5.077811

Batch 20940, train_perplexity=162.54514, train_loss=5.0909557

Batch 20950, train_perplexity=173.74272, train_loss=5.1575756

Batch 20960, train_perplexity=164.45442, train_loss=5.1026335

Batch 20970, train_perplexity=165.66232, train_loss=5.1099515

Batch 20980, train_perplexity=172.45232, train_loss=5.1501207

Batch 20990, train_perplexity=163.30345, train_loss=5.09561

Batch 21000, train_perplexity=150.6174, train_loss=5.014743

Batch 21010, train_perplexity=204.79102, train_loss=5.32199

Batch 21020, train_perplexity=170.44359, train_loss=5.1384044

Batch 21030, train_perplexity=168.13582, train_loss=5.124772

Batch 21040, train_perplexity=177.73447, train_loss=5.1802907

Batch 21050, train_perplexity=160.1061, train_loss=5.0758367

Batch 21060, train_perplexity=202.6026, train_loss=5.3112464

Batch 21070, train_perplexity=165.60933, train_loss=5.1096315

Batch 21080, train_perplexity=166.71039, train_loss=5.116258

Batch 21090, train_perplexity=148.3976, train_loss=4.999895

Batch 21100, train_perplexity=165.64487, train_loss=5.109846

Batch 21110, train_perplexity=169.95186, train_loss=5.135515

Batch 21120, train_perplexity=165.48586, train_loss=5.108886

Batch 21130, train_perplexity=169.35403, train_loss=5.1319914

Batch 21140, train_perplexity=192.40463, train_loss=5.2596006

Batch 21150, train_perplexity=176.35864, train_loss=5.1725197

Batch 21160, train_perplexity=178.20215, train_loss=5.1829185

Batch 21170, train_perplexity=177.48607, train_loss=5.178892

Batch 21180, train_perplexity=195.00023, train_loss=5.2730007

Batch 21190, train_perplexity=149.21558, train_loss=5.005392

Batch 21200, train_perplexity=164.16281, train_loss=5.1008587

Batch 21210, train_perplexity=184.5946, train_loss=5.218162

Batch 21220, train_perplexity=168.63387, train_loss=5.12773

Batch 21230, train_perplexity=168.88388, train_loss=5.1292114

Batch 21240, train_perplexity=176.43661, train_loss=5.1729617

Batch 21250, train_perplexity=156.22127, train_loss=5.0512733

Batch 21260, train_perplexity=163.9344, train_loss=5.0994663

Batch 21270, train_perplexity=168.9252, train_loss=5.129456

Batch 21280, train_perplexity=179.06305, train_loss=5.187738

Batch 21290, train_perplexity=149.84624, train_loss=5.0096097

Batch 21300, train_perplexity=157.52507, train_loss=5.0595846

Batch 21310, train_perplexity=153.22342, train_loss=5.031897

Batch 21320, train_perplexity=165.73415, train_loss=5.110385

Batch 21330, train_perplexity=170.78667, train_loss=5.140415

Batch 21340, train_perplexity=175.67746, train_loss=5.1686497

Batch 21350, train_perplexity=155.15817, train_loss=5.044445

Batch 21360, train_perplexity=161.58226, train_loss=5.0850143

Batch 21370, train_perplexity=178.88843, train_loss=5.1867623

Batch 21380, train_perplexity=175.52272, train_loss=5.1677685

Batch 21390, train_perplexity=172.82613, train_loss=5.152286

Batch 21400, train_perplexity=161.804, train_loss=5.0863857

Batch 21410, train_perplexity=148.84421, train_loss=5.0029

Batch 21420, train_perplexity=154.06895, train_loss=5.0374002

Batch 21430, train_perplexity=181.50713, train_loss=5.201295

Batch 21440, train_perplexity=157.28444, train_loss=5.058056

Batch 21450, train_perplexity=172.29697, train_loss=5.1492195

Batch 21460, train_perplexity=169.56284, train_loss=5.1332235

Batch 21470, train_perplexity=147.04187, train_loss=4.9907174

Batch 21480, train_perplexity=161.45502, train_loss=5.0842266

Batch 21490, train_perplexity=154.24442, train_loss=5.0385385

Batch 21500, train_perplexity=174.71442, train_loss=5.1631527

Batch 21510, train_perplexity=151.976, train_loss=5.0237226

Batch 21520, train_perplexity=179.81471, train_loss=5.191927

Batch 21530, train_perplexity=170.83081, train_loss=5.1406736

Batch 21540, train_perplexity=172.36615, train_loss=5.149621

Batch 21550, train_perplexity=175.16187, train_loss=5.1657104

Batch 21560, train_perplexity=166.27605, train_loss=5.1136494

Batch 21570, train_perplexity=157.3981, train_loss=5.0587783

Batch 21580, train_perplexity=169.19373, train_loss=5.1310444

Batch 21590, train_perplexity=180.18292, train_loss=5.1939726

Batch 21600, train_perplexity=161.15244, train_loss=5.0823507

Batch 21610, train_perplexity=156.47966, train_loss=5.052926

Batch 21620, train_perplexity=167.25262, train_loss=5.1195054

Batch 21630, train_perplexity=155.16719, train_loss=5.044503

Batch 21640, train_perplexity=137.11732, train_loss=4.920837

Batch 21650, train_perplexity=169.14404, train_loss=5.1307507

Batch 21660, train_perplexity=156.27744, train_loss=5.051633

Batch 21670, train_perplexity=164.6257, train_loss=5.1036744

Batch 21680, train_perplexity=162.32617, train_loss=5.0896077

Batch 21690, train_perplexity=181.19063, train_loss=5.1995497

Batch 21700, train_perplexity=171.63293, train_loss=5.145358

Batch 21710, train_perplexity=180.53261, train_loss=5.1959114

Batch 21720, train_perplexity=143.54877, train_loss=4.966675

Batch 21730, train_perplexity=159.91794, train_loss=5.074661

Batch 21740, train_perplexity=151.00903, train_loss=5.0173397

Batch 21750, train_perplexity=142.29549, train_loss=4.957906

Batch 21760, train_perplexity=155.0558, train_loss=5.043785

Batch 21770, train_perplexity=174.28888, train_loss=5.160714

Batch 21780, train_perplexity=172.87154, train_loss=5.152549

Batch 21790, train_perplexity=172.58162, train_loss=5.1508703

Batch 21800, train_perplexity=168.23125, train_loss=5.1253395

Batch 21810, train_perplexity=162.31998, train_loss=5.0895696

Batch 21820, train_perplexity=184.40097, train_loss=5.2171125

Batch 21830, train_perplexity=149.47408, train_loss=5.007123

Batch 21840, train_perplexity=167.74086, train_loss=5.1224203

Batch 21850, train_perplexity=148.79324, train_loss=5.0025578

Batch 21860, train_perplexity=159.11456, train_loss=5.0696244

Batch 21870, train_perplexity=149.7386, train_loss=5.008891

Batch 21880, train_perplexity=162.67741, train_loss=5.091769

Batch 21890, train_perplexity=181.8429, train_loss=5.203143

Batch 21900, train_perplexity=166.06686, train_loss=5.1123905

Batch 21910, train_perplexity=178.14845, train_loss=5.182617

Batch 21920, train_perplexity=166.34361, train_loss=5.1140556

Batch 21930, train_perplexity=172.23576, train_loss=5.1488643

Batch 21940, train_perplexity=170.68864, train_loss=5.139841

Batch 21950, train_perplexity=156.23735, train_loss=5.0513763

Batch 21960, train_perplexity=172.00818, train_loss=5.147542

Batch 21970, train_perplexity=171.95651, train_loss=5.1472416

Batch 21980, train_perplexity=168.70955, train_loss=5.1281786

Batch 21990, train_perplexity=193.63779, train_loss=5.2659893

Batch 22000, train_perplexity=156.53676, train_loss=5.053291

Batch 22010, train_perplexity=165.19383, train_loss=5.1071196

Batch 22020, train_perplexity=167.50874, train_loss=5.1210356

Batch 22030, train_perplexity=148.82574, train_loss=5.002776

Batch 22040, train_perplexity=172.60294, train_loss=5.150994

Batch 22050, train_perplexity=155.50377, train_loss=5.04667

Batch 22060, train_perplexity=169.3244, train_loss=5.1318164
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22070, train_perplexity=157.38586, train_loss=5.0587006

Batch 22080, train_perplexity=161.5838, train_loss=5.085024

Batch 22090, train_perplexity=154.75267, train_loss=5.041828

Batch 22100, train_perplexity=166.6949, train_loss=5.116165

Batch 22110, train_perplexity=156.93608, train_loss=5.0558386

Batch 22120, train_perplexity=149.34106, train_loss=5.0062327

Batch 22130, train_perplexity=160.97119, train_loss=5.0812254

Batch 22140, train_perplexity=163.875, train_loss=5.099104

Batch 22150, train_perplexity=153.18813, train_loss=5.0316668

Batch 22160, train_perplexity=160.864, train_loss=5.0805593

Batch 22170, train_perplexity=151.03654, train_loss=5.017522

Batch 22180, train_perplexity=171.4408, train_loss=5.144238

Batch 22190, train_perplexity=163.33951, train_loss=5.095831

Batch 22200, train_perplexity=157.3218, train_loss=5.0582933

Batch 22210, train_perplexity=160.82312, train_loss=5.080305

Batch 22220, train_perplexity=154.80301, train_loss=5.0421534

Batch 22230, train_perplexity=155.6707, train_loss=5.047743

Batch 22240, train_perplexity=164.24768, train_loss=5.1013756

Batch 22250, train_perplexity=163.6011, train_loss=5.097431

Batch 22260, train_perplexity=187.30646, train_loss=5.232746

Batch 22270, train_perplexity=174.58008, train_loss=5.1623836

Batch 22280, train_perplexity=163.4793, train_loss=5.0966864

Batch 22290, train_perplexity=167.52296, train_loss=5.1211205

Batch 22300, train_perplexity=162.24081, train_loss=5.089082

Batch 22310, train_perplexity=159.48851, train_loss=5.071972

Batch 22320, train_perplexity=137.66127, train_loss=4.924796

Batch 22330, train_perplexity=159.2426, train_loss=5.070429

Batch 22340, train_perplexity=151.25911, train_loss=5.0189943

Batch 22350, train_perplexity=153.77112, train_loss=5.0354652

Batch 22360, train_perplexity=149.73218, train_loss=5.008848

Batch 22370, train_perplexity=171.25818, train_loss=5.1431723

Batch 22380, train_perplexity=153.48439, train_loss=5.033599

Batch 22390, train_perplexity=166.49185, train_loss=5.1149464

Batch 22400, train_perplexity=168.06367, train_loss=5.124343

Batch 22410, train_perplexity=160.17314, train_loss=5.0762553

Batch 22420, train_perplexity=155.44038, train_loss=5.0462623

Batch 22430, train_perplexity=156.3162, train_loss=5.051881

Batch 22440, train_perplexity=153.43259, train_loss=5.0332613

Batch 22450, train_perplexity=167.44598, train_loss=5.120661

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 22460, train_perplexity=164.97249, train_loss=5.1057787

Batch 22470, train_perplexity=165.18147, train_loss=5.1070447

Batch 22480, train_perplexity=175.18425, train_loss=5.1658382

Batch 22490, train_perplexity=162.94379, train_loss=5.0934052

Batch 22500, train_perplexity=155.2104, train_loss=5.0447817

Batch 22510, train_perplexity=170.42497, train_loss=5.138295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 22520, train_perplexity=159.10204, train_loss=5.0695457

Batch 22530, train_perplexity=150.15988, train_loss=5.0117006

Batch 22540, train_perplexity=165.1332, train_loss=5.1067524

Batch 22550, train_perplexity=144.98187, train_loss=4.9766088

Batch 22560, train_perplexity=159.63109, train_loss=5.0728655

Batch 22570, train_perplexity=142.24535, train_loss=4.9575534

Batch 22580, train_perplexity=163.91689, train_loss=5.0993595

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 22590, train_perplexity=143.5272, train_loss=4.9665246

Batch 22600, train_perplexity=175.72186, train_loss=5.1689024

Batch 22610, train_perplexity=157.09818, train_loss=5.056871

Batch 22620, train_perplexity=143.39635, train_loss=4.9656124

Batch 22630, train_perplexity=156.03558, train_loss=5.050084

Batch 22640, train_perplexity=139.50598, train_loss=4.9381075

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 22650, train_perplexity=180.92006, train_loss=5.1980553

Batch 22660, train_perplexity=151.64157, train_loss=5.0215197

Batch 22670, train_perplexity=171.17491, train_loss=5.142686

Batch 22680, train_perplexity=156.86844, train_loss=5.0554075

Batch 22690, train_perplexity=161.31143, train_loss=5.083337

Batch 22700, train_perplexity=158.70076, train_loss=5.0670204

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 22710, train_perplexity=144.61697, train_loss=4.9740887

Batch 22720, train_perplexity=172.96991, train_loss=5.1531177

Batch 22730, train_perplexity=170.7997, train_loss=5.1404915

Batch 22740, train_perplexity=151.41571, train_loss=5.020029

Batch 22750, train_perplexity=156.61253, train_loss=5.053775

Batch 22760, train_perplexity=163.59058, train_loss=5.097367

Batch 22770, train_perplexity=155.22336, train_loss=5.044865

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 22780, train_perplexity=158.92241, train_loss=5.068416

Batch 22790, train_perplexity=143.74384, train_loss=4.968033

Batch 22800, train_perplexity=144.57277, train_loss=4.973783

Batch 22810, train_perplexity=150.14099, train_loss=5.0115747

Batch 22820, train_perplexity=169.20454, train_loss=5.1311083

Batch 22830, train_perplexity=161.45357, train_loss=5.0842175

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 22840, train_perplexity=141.01952, train_loss=4.9488983

Batch 22850, train_perplexity=162.12126, train_loss=5.0883446

Batch 22860, train_perplexity=154.36662, train_loss=5.0393305

Batch 22870, train_perplexity=141.125, train_loss=4.949646

Batch 22880, train_perplexity=158.24977, train_loss=5.0641747

Batch 22890, train_perplexity=164.01837, train_loss=5.0999784

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 22900, train_perplexity=149.15852, train_loss=5.0050097

Batch 22910, train_perplexity=157.14905, train_loss=5.0571947

Batch 22920, train_perplexity=159.79651, train_loss=5.073901

Batch 22930, train_perplexity=163.96144, train_loss=5.0996313

Batch 22940, train_perplexity=153.93692, train_loss=5.036543

Batch 22950, train_perplexity=181.64687, train_loss=5.2020645

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 22960, train_perplexity=160.17474, train_loss=5.0762653

Batch 22970, train_perplexity=144.39551, train_loss=4.972556

Batch 22980, train_perplexity=166.35933, train_loss=5.11415

Batch 22990, train_perplexity=172.43997, train_loss=5.150049

Batch 23000, train_perplexity=155.91495, train_loss=5.0493107

Batch 23010, train_perplexity=154.08401, train_loss=5.037498

Batch 23020, train_perplexity=159.4479, train_loss=5.0717173

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 23030, train_perplexity=174.29785, train_loss=5.1607656

Batch 23040, train_perplexity=164.96571, train_loss=5.1057377

Batch 23050, train_perplexity=156.27751, train_loss=5.0516334

Batch 23060, train_perplexity=158.27982, train_loss=5.0643644

Batch 23070, train_perplexity=168.88422, train_loss=5.1292133

Batch 23080, train_perplexity=167.12978, train_loss=5.1187706

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 23090, train_perplexity=154.92506, train_loss=5.0429416

Batch 23100, train_perplexity=177.45561, train_loss=5.1787205

Batch 23110, train_perplexity=158.60498, train_loss=5.0664167

Batch 23120, train_perplexity=162.00998, train_loss=5.087658

Batch 23130, train_perplexity=143.7891, train_loss=4.9683475

Batch 23140, train_perplexity=152.41681, train_loss=5.026619

Batch 23150, train_perplexity=163.9294, train_loss=5.099436

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 23160, train_perplexity=166.15407, train_loss=5.1129155

Batch 23170, train_perplexity=149.5314, train_loss=5.0075064

Batch 23180, train_perplexity=170.0381, train_loss=5.1360226

Batch 23190, train_perplexity=165.8918, train_loss=5.1113358

Batch 23200, train_perplexity=139.02878, train_loss=4.934681

Batch 23210, train_perplexity=163.16895, train_loss=5.094786

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 23220, train_perplexity=151.48236, train_loss=5.020469

Batch 23230, train_perplexity=160.77727, train_loss=5.08002

Batch 23240, train_perplexity=153.48792, train_loss=5.033622

Batch 23250, train_perplexity=148.94487, train_loss=5.0035763

Batch 23260, train_perplexity=173.8153, train_loss=5.1579933

Batch 23270, train_perplexity=156.65631, train_loss=5.0540543

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 23280, train_perplexity=161.60507, train_loss=5.0851555

Batch 23290, train_perplexity=151.09029, train_loss=5.0178776

Batch 23300, train_perplexity=154.54236, train_loss=5.040468

Batch 23310, train_perplexity=154.8956, train_loss=5.0427513

Batch 23320, train_perplexity=161.61076, train_loss=5.085191

Batch 23330, train_perplexity=143.65785, train_loss=4.9674344

Batch 23340, train_perplexity=151.78308, train_loss=5.0224524

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 23350, train_perplexity=153.23174, train_loss=5.0319514

Batch 23360, train_perplexity=163.66118, train_loss=5.0977983

Batch 23370, train_perplexity=144.15431, train_loss=4.9708843

Batch 23380, train_perplexity=162.95256, train_loss=5.093459

Batch 23390, train_perplexity=150.7637, train_loss=5.0157137

Batch 23400, train_perplexity=155.12495, train_loss=5.044231

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 23410, train_perplexity=175.64839, train_loss=5.168484

Batch 23420, train_perplexity=160.11952, train_loss=5.0759206

Batch 23430, train_perplexity=149.20804, train_loss=5.0053415

Batch 23440, train_perplexity=168.18329, train_loss=5.1250544

Batch 23450, train_perplexity=169.34256, train_loss=5.1319237

Batch 23460, train_perplexity=171.35686, train_loss=5.1437483

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 23470, train_perplexity=156.91678, train_loss=5.0557156

Batch 23480, train_perplexity=161.28004, train_loss=5.0831423

Batch 23490, train_perplexity=144.8222, train_loss=4.975507

Batch 23500, train_perplexity=139.02043, train_loss=4.934621

Batch 23510, train_perplexity=161.6009, train_loss=5.0851297

Batch 23520, train_perplexity=150.21574, train_loss=5.0120726

Batch 23530, train_perplexity=157.96059, train_loss=5.0623455

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 23540, train_perplexity=145.6118, train_loss=4.980944

Batch 23550, train_perplexity=164.60074, train_loss=5.103523

Batch 23560, train_perplexity=155.0615, train_loss=5.043822

Batch 23570, train_perplexity=173.91861, train_loss=5.1585875

Batch 23580, train_perplexity=134.65991, train_loss=4.9027524

Batch 23590, train_perplexity=154.37627, train_loss=5.039393

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 23600, train_perplexity=166.82657, train_loss=5.116955

Batch 23610, train_perplexity=147.95769, train_loss=4.9969263

Batch 23620, train_perplexity=151.78757, train_loss=5.022482

Batch 23630, train_perplexity=146.0033, train_loss=4.983629

Batch 23640, train_perplexity=174.42082, train_loss=5.161471

Batch 23650, train_perplexity=161.48782, train_loss=5.0844297

Batch 23660, train_perplexity=146.59654, train_loss=4.9876842

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 23670, train_perplexity=159.41263, train_loss=5.071496

Batch 23680, train_perplexity=140.65158, train_loss=4.9462857

Batch 23690, train_perplexity=159.54305, train_loss=5.072314

Batch 23700, train_perplexity=139.01784, train_loss=4.9346023

Batch 23710, train_perplexity=166.12753, train_loss=5.112756

Batch 23720, train_perplexity=147.87276, train_loss=4.996352

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 23730, train_perplexity=171.0915, train_loss=5.1421986

Batch 23740, train_perplexity=139.52461, train_loss=4.938241

Batch 23750, train_perplexity=161.46927, train_loss=5.084315

Batch 23760, train_perplexity=163.92822, train_loss=5.0994287

Batch 23770, train_perplexity=153.93912, train_loss=5.036557

Batch 23780, train_perplexity=164.07672, train_loss=5.100334

Batch 23790, train_perplexity=144.85121, train_loss=4.975707

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 23800, train_perplexity=153.19785, train_loss=5.03173

Batch 23810, train_perplexity=151.56119, train_loss=5.0209894

Batch 23820, train_perplexity=170.64485, train_loss=5.1395845

Batch 23830, train_perplexity=158.41798, train_loss=5.065237

Batch 23840, train_perplexity=145.17911, train_loss=4.977968

Batch 23850, train_perplexity=151.83722, train_loss=5.022809

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 23860, train_perplexity=167.27232, train_loss=5.119623

Batch 23870, train_perplexity=150.89618, train_loss=5.016592

Batch 23880, train_perplexity=148.81638, train_loss=5.002713

Batch 23890, train_perplexity=180.0237, train_loss=5.1930885

Batch 23900, train_perplexity=144.30493, train_loss=4.9719286

Batch 23910, train_perplexity=160.11472, train_loss=5.0758905

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 23920, train_perplexity=149.7708, train_loss=5.009106

Batch 23930, train_perplexity=138.5908, train_loss=4.9315257

Batch 23940, train_perplexity=150.81654, train_loss=5.016064

Batch 23950, train_perplexity=161.55545, train_loss=5.0848484

Batch 23960, train_perplexity=159.9081, train_loss=5.0745993

Batch 23970, train_perplexity=153.66396, train_loss=5.034768

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 23980, train_perplexity=165.35445, train_loss=5.1080914

Batch 23990, train_perplexity=160.07089, train_loss=5.075617

Batch 24000, train_perplexity=151.79198, train_loss=5.022511

Batch 24010, train_perplexity=150.69966, train_loss=5.015289

Batch 24020, train_perplexity=144.6311, train_loss=4.9741864

Batch 24030, train_perplexity=148.91803, train_loss=5.003396

Batch 24040, train_perplexity=176.51581, train_loss=5.1734104

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 24050, train_perplexity=154.13965, train_loss=5.037859

Batch 24060, train_perplexity=148.35663, train_loss=4.999619

Batch 24070, train_perplexity=159.1966, train_loss=5.07014

Batch 24080, train_perplexity=137.73625, train_loss=4.9253407

Batch 24090, train_perplexity=149.27977, train_loss=5.005822

Batch 24100, train_perplexity=155.61592, train_loss=5.047391

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 24110, train_perplexity=160.86299, train_loss=5.080553

Batch 24120, train_perplexity=149.18968, train_loss=5.0052185

Batch 24130, train_perplexity=152.41391, train_loss=5.0266

Batch 24140, train_perplexity=155.82278, train_loss=5.0487194

Batch 24150, train_perplexity=149.47173, train_loss=5.0071073

Batch 24160, train_perplexity=155.14085, train_loss=5.0443335

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 24170, train_perplexity=159.2136, train_loss=5.0702467

Batch 24180, train_perplexity=149.99823, train_loss=5.0106235

Batch 24190, train_perplexity=158.30441, train_loss=5.06452

Batch 24200, train_perplexity=144.11334, train_loss=4.9706

Batch 24210, train_perplexity=154.81068, train_loss=5.042203

Batch 24220, train_perplexity=164.3703, train_loss=5.102122

Batch 24230, train_perplexity=143.18452, train_loss=4.964134

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 24240, train_perplexity=148.60039, train_loss=5.0012608

Batch 24250, train_perplexity=158.24864, train_loss=5.0641675

Batch 24260, train_perplexity=161.15782, train_loss=5.082384

Batch 24270, train_perplexity=157.63072, train_loss=5.060255

Batch 24280, train_perplexity=157.14642, train_loss=5.057178

Batch 24290, train_perplexity=164.38072, train_loss=5.1021852

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 24300, train_perplexity=177.99323, train_loss=5.1817455

Batch 24310, train_perplexity=150.15709, train_loss=5.011682

Batch 24320, train_perplexity=146.83092, train_loss=4.9892817

Batch 24330, train_perplexity=166.39867, train_loss=5.1143866

Batch 24340, train_perplexity=146.26447, train_loss=4.9854164

Batch 24350, train_perplexity=155.77272, train_loss=5.048398

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 24360, train_perplexity=156.90436, train_loss=5.0556364

Batch 24370, train_perplexity=163.48662, train_loss=5.096731

Batch 24380, train_perplexity=158.7701, train_loss=5.067457

Batch 24390, train_perplexity=165.23323, train_loss=5.107358

Batch 24400, train_perplexity=143.64175, train_loss=4.9673223

Batch 24410, train_perplexity=165.54852, train_loss=5.1092644

Batch 24420, train_perplexity=160.53886, train_loss=5.078536

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 24430, train_perplexity=144.94241, train_loss=4.9763365

Batch 24440, train_perplexity=146.1068, train_loss=4.984338

Batch 24450, train_perplexity=165.61777, train_loss=5.1096826

Batch 24460, train_perplexity=149.97527, train_loss=5.0104704

Batch 24470, train_perplexity=167.47824, train_loss=5.1208534

Batch 24480, train_perplexity=136.63988, train_loss=4.917349

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 24490, train_perplexity=153.25008, train_loss=5.032071

Batch 24500, train_perplexity=146.81845, train_loss=4.989197

Batch 24510, train_perplexity=144.49379, train_loss=4.9732366

Batch 24520, train_perplexity=167.50874, train_loss=5.1210356

Batch 24530, train_perplexity=146.90543, train_loss=4.989789

Batch 24540, train_perplexity=158.10164, train_loss=5.063238

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 24550, train_perplexity=170.49277, train_loss=5.138693

Batch 24560, train_perplexity=147.5171, train_loss=4.993944

Batch 24570, train_perplexity=149.69505, train_loss=5.0086

Batch 24580, train_perplexity=151.05023, train_loss=5.0176125

Batch 24590, train_perplexity=165.82283, train_loss=5.11092

Batch 24600, train_perplexity=161.1131, train_loss=5.0821066

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 24610, train_perplexity=152.54245, train_loss=5.027443

Batch 24620, train_perplexity=142.90201, train_loss=4.962159

Batch 24630, train_perplexity=143.43259, train_loss=4.965865

Batch 24640, train_perplexity=145.83894, train_loss=4.982503

Batch 24650, train_perplexity=134.406, train_loss=4.900865

Batch 24660, train_perplexity=191.10425, train_loss=5.252819

Batch 24670, train_perplexity=173.13074, train_loss=5.154047

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 24680, train_perplexity=152.38368, train_loss=5.0264015

Batch 24690, train_perplexity=165.95999, train_loss=5.111747

Batch 24700, train_perplexity=150.51143, train_loss=5.014039

Batch 24710, train_perplexity=134.12756, train_loss=4.8987913

Batch 24720, train_perplexity=166.47343, train_loss=5.1148357

Batch 24730, train_perplexity=170.68327, train_loss=5.1398096

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 24740, train_perplexity=145.79486, train_loss=4.9822006

Batch 24750, train_perplexity=161.68037, train_loss=5.0856214

Batch 24760, train_perplexity=161.15797, train_loss=5.082385

Batch 24770, train_perplexity=157.53957, train_loss=5.0596766

Batch 24780, train_perplexity=152.29477, train_loss=5.025818

Batch 24790, train_perplexity=172.5534, train_loss=5.150707

Batch 24800, train_perplexity=148.12485, train_loss=4.9980555

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 24810, train_perplexity=151.72208, train_loss=5.0220504

Batch 24820, train_perplexity=147.80734, train_loss=4.9959097

Batch 24830, train_perplexity=149.3175, train_loss=5.006075

Batch 24840, train_perplexity=167.1685, train_loss=5.1190023

Batch 24850, train_perplexity=156.66295, train_loss=5.0540967

Batch 24860, train_perplexity=157.6417, train_loss=5.0603247

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 24870, train_perplexity=155.47812, train_loss=5.046505

Batch 24880, train_perplexity=153.46054, train_loss=5.0334435

Batch 24890, train_perplexity=163.79828, train_loss=5.0986357

Batch 24900, train_perplexity=169.70763, train_loss=5.134077

Batch 24910, train_perplexity=153.57649, train_loss=5.0341988

Batch 24920, train_perplexity=179.05612, train_loss=5.1876993

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 24930, train_perplexity=164.64572, train_loss=5.103796

Batch 24940, train_perplexity=149.67079, train_loss=5.008438

Batch 24950, train_perplexity=149.1357, train_loss=5.0048566

Batch 24960, train_perplexity=161.27498, train_loss=5.083111

Batch 24970, train_perplexity=153.2628, train_loss=5.032154

Batch 24980, train_perplexity=140.86871, train_loss=4.9478283

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 24990, train_perplexity=156.58386, train_loss=5.0535917

Batch 25000, train_perplexity=134.77817, train_loss=4.9036303

Batch 25010, train_perplexity=162.61305, train_loss=5.0913734

Batch 25020, train_perplexity=139.64308, train_loss=4.93909

Batch 25030, train_perplexity=149.52248, train_loss=5.007447

Batch 25040, train_perplexity=172.90073, train_loss=5.1527176

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 25050, train_perplexity=163.48257, train_loss=5.0967064

Batch 25060, train_perplexity=150.09038, train_loss=5.0112376

Batch 25070, train_perplexity=163.53816, train_loss=5.0970464

Batch 25080, train_perplexity=155.88611, train_loss=5.0491257

Batch 25090, train_perplexity=150.54381, train_loss=5.014254

Batch 25100, train_perplexity=164.12602, train_loss=5.1006346

Batch 25110, train_perplexity=162.99693, train_loss=5.0937314

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 25120, train_perplexity=152.08098, train_loss=5.024413

Batch 25130, train_perplexity=152.89812, train_loss=5.029772

Batch 25140, train_perplexity=140.22995, train_loss=4.9432836

Batch 25150, train_perplexity=137.60811, train_loss=4.92441

Batch 25160, train_perplexity=132.09099, train_loss=4.883491

Batch 25170, train_perplexity=156.0481, train_loss=5.050164

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 25180, train_perplexity=162.09698, train_loss=5.088195

Batch 25190, train_perplexity=142.86182, train_loss=4.961878

Batch 25200, train_perplexity=148.88693, train_loss=5.003187

Batch 25210, train_perplexity=144.59781, train_loss=4.973956

Batch 25220, train_perplexity=148.08035, train_loss=4.997755

Batch 25230, train_perplexity=143.45392, train_loss=4.966014

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 25240, train_perplexity=172.4621, train_loss=5.1501775

Batch 25250, train_perplexity=153.07407, train_loss=5.030922

Batch 25260, train_perplexity=139.70763, train_loss=4.939552

Batch 25270, train_perplexity=145.86037, train_loss=4.98265

Batch 25280, train_perplexity=163.93518, train_loss=5.099471

Batch 25290, train_perplexity=146.42609, train_loss=4.986521

Batch 25300, train_perplexity=152.04515, train_loss=5.0241776

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 25310, train_perplexity=194.49728, train_loss=5.270418

Batch 25320, train_perplexity=157.67447, train_loss=5.0605326

Batch 25330, train_perplexity=166.94228, train_loss=5.117648

Batch 25340, train_perplexity=156.45265, train_loss=5.0527534

Batch 25350, train_perplexity=152.67212, train_loss=5.0282927

Batch 25360, train_perplexity=163.1236, train_loss=5.094508

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 25370, train_perplexity=155.83089, train_loss=5.0487714

Batch 25380, train_perplexity=149.90977, train_loss=5.0100336

Batch 25390, train_perplexity=153.84827, train_loss=5.035967

Batch 25400, train_perplexity=149.59145, train_loss=5.007908

Batch 25410, train_perplexity=160.66882, train_loss=5.079345

Batch 25420, train_perplexity=151.20618, train_loss=5.0186443

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6071 sentences.
Finished loading
Batch 25430, train_perplexity=129.75008, train_loss=4.86561

Batch 25440, train_perplexity=160.15732, train_loss=5.0761566

Batch 25450, train_perplexity=148.77481, train_loss=5.002434

Batch 25460, train_perplexity=153.05649, train_loss=5.030807

Batch 25470, train_perplexity=150.58745, train_loss=5.014544

Batch 25480, train_perplexity=153.02226, train_loss=5.0305834

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 25490, train_perplexity=172.70164, train_loss=5.1515656

Batch 25500, train_perplexity=130.34103, train_loss=4.8701544

Batch 25510, train_perplexity=152.19661, train_loss=5.025173

Batch 25520, train_perplexity=151.09216, train_loss=5.01789

Batch 25530, train_perplexity=151.13086, train_loss=5.018146

Batch 25540, train_perplexity=142.00009, train_loss=4.9558277

Batch 25550, train_perplexity=161.03983, train_loss=5.0816517

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 25560, train_perplexity=150.29987, train_loss=5.0126324

Batch 25570, train_perplexity=134.4923, train_loss=4.901507

Batch 25580, train_perplexity=128.6012, train_loss=4.856716

Batch 25590, train_perplexity=180.03821, train_loss=5.193169

Batch 25600, train_perplexity=146.00546, train_loss=4.983644

Batch 25610, train_perplexity=152.61739, train_loss=5.027934

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 25620, train_perplexity=166.21945, train_loss=5.113309

Batch 25630, train_perplexity=147.72476, train_loss=4.995351

Batch 25640, train_perplexity=145.95387, train_loss=4.9832907

Batch 25650, train_perplexity=147.18266, train_loss=4.9916744

Batch 25660, train_perplexity=160.1952, train_loss=5.076393

Batch 25670, train_perplexity=138.04924, train_loss=4.9276104

Batch 25680, train_perplexity=131.67148, train_loss=4.88031

Batch 25690, train_perplexity=138.18504, train_loss=4.9285936

Batch 25700, train_perplexity=155.33087, train_loss=5.0455575

Batch 25710, train_perplexity=154.48018, train_loss=5.040066

Batch 25720, train_perplexity=147.342, train_loss=4.9927564

Batch 25730, train_perplexity=163.1253, train_loss=5.0945187

Batch 25740, train_perplexity=148.59189, train_loss=5.0012035

Batch 25750, train_perplexity=135.66553, train_loss=4.9101925

Batch 25760, train_perplexity=160.47916, train_loss=5.078164

Batch 25770, train_perplexity=144.3215, train_loss=4.9720435

Batch 25780, train_perplexity=145.12013, train_loss=4.977562

Batch 25790, train_perplexity=138.94283, train_loss=4.9340625

Batch 25800, train_perplexity=159.0861, train_loss=5.0694456

Batch 25810, train_perplexity=144.3039, train_loss=4.9719214

Batch 25820, train_perplexity=150.3651, train_loss=5.0130663

Batch 25830, train_perplexity=131.2851, train_loss=4.8773713

Batch 25840, train_perplexity=137.48232, train_loss=4.9234953

Batch 25850, train_perplexity=155.90901, train_loss=5.0492725

Batch 25860, train_perplexity=139.81738, train_loss=4.940337

Batch 25870, train_perplexity=143.97646, train_loss=4.96965

Batch 25880, train_perplexity=134.46426, train_loss=4.9012985

Batch 25890, train_perplexity=139.92169, train_loss=4.941083

Batch 25900, train_perplexity=138.41914, train_loss=4.9302864

Batch 25910, train_perplexity=146.69948, train_loss=4.988386

Batch 25920, train_perplexity=153.2989, train_loss=5.0323896

Batch 25930, train_perplexity=159.32677, train_loss=5.070957

Batch 25940, train_perplexity=149.64781, train_loss=5.0082846

Batch 25950, train_perplexity=129.99779, train_loss=4.8675175

Batch 25960, train_perplexity=144.91815, train_loss=4.976169

Batch 25970, train_perplexity=143.64996, train_loss=4.9673796

Batch 25980, train_perplexity=143.96362, train_loss=4.9695606

Batch 25990, train_perplexity=139.56148, train_loss=4.938505

Batch 26000, train_perplexity=139.666, train_loss=4.939254

Batch 26010, train_perplexity=146.98474, train_loss=4.990329

Batch 26020, train_perplexity=144.02473, train_loss=4.969985

Batch 26030, train_perplexity=160.77711, train_loss=5.080019

Batch 26040, train_perplexity=144.89003, train_loss=4.975975

Batch 26050, train_perplexity=149.35382, train_loss=5.006318

Batch 26060, train_perplexity=151.1266, train_loss=5.018118

Batch 26070, train_perplexity=138.14616, train_loss=4.9283123

Batch 26080, train_perplexity=131.18817, train_loss=4.8766327

Batch 26090, train_perplexity=143.44598, train_loss=4.9659586

Batch 26100, train_perplexity=151.94557, train_loss=5.0235224

Batch 26110, train_perplexity=153.50328, train_loss=5.033722

Batch 26120, train_perplexity=156.09528, train_loss=5.0504665

Batch 26130, train_perplexity=152.62933, train_loss=5.0280123

Batch 26140, train_perplexity=146.03345, train_loss=4.9838357

Batch 26150, train_perplexity=142.59515, train_loss=4.9600096

Batch 26160, train_perplexity=161.92148, train_loss=5.0871115

Batch 26170, train_perplexity=151.94977, train_loss=5.02355

Batch 26180, train_perplexity=145.81218, train_loss=4.9823194

Batch 26190, train_perplexity=130.14653, train_loss=4.868661

Batch 26200, train_perplexity=140.75604, train_loss=4.947028

Batch 26210, train_perplexity=168.2964, train_loss=5.1257267

Batch 26220, train_perplexity=130.02011, train_loss=4.867689

Batch 26230, train_perplexity=134.91243, train_loss=4.904626

Batch 26240, train_perplexity=152.58421, train_loss=5.0277166

Batch 26250, train_perplexity=154.39299, train_loss=5.039501

Batch 26260, train_perplexity=131.66382, train_loss=4.880252

Batch 26270, train_perplexity=155.01817, train_loss=5.0435424

Batch 26280, train_perplexity=143.39757, train_loss=4.965621

Batch 26290, train_perplexity=163.31996, train_loss=5.095711

Batch 26300, train_perplexity=143.34158, train_loss=4.9652305

Batch 26310, train_perplexity=146.065, train_loss=4.9840517

Batch 26320, train_perplexity=124.522064, train_loss=4.824483

Batch 26330, train_perplexity=159.66725, train_loss=5.073092

Batch 26340, train_perplexity=147.48953, train_loss=4.9937572

Batch 26350, train_perplexity=143.24585, train_loss=4.9645624

Batch 26360, train_perplexity=159.80504, train_loss=5.0739546

Batch 26370, train_perplexity=147.13074, train_loss=4.9913216

Batch 26380, train_perplexity=141.32648, train_loss=4.9510727

Batch 26390, train_perplexity=159.7262, train_loss=5.073461

Batch 26400, train_perplexity=143.79704, train_loss=4.968403

Batch 26410, train_perplexity=155.02654, train_loss=5.0435963

Batch 26420, train_perplexity=146.55573, train_loss=4.987406

Batch 26430, train_perplexity=143.51038, train_loss=4.9664073

Batch 26440, train_perplexity=136.06046, train_loss=4.9130993

Batch 26450, train_perplexity=153.07954, train_loss=5.0309577

Batch 26460, train_perplexity=139.70343, train_loss=4.939522

Batch 26470, train_perplexity=144.02899, train_loss=4.9700146

Batch 26480, train_perplexity=134.95927, train_loss=4.904973

Batch 26490, train_perplexity=135.82709, train_loss=4.9113827

Batch 26500, train_perplexity=168.08827, train_loss=5.1244893

Batch 26510, train_perplexity=153.60257, train_loss=5.0343685

Batch 26520, train_perplexity=144.09775, train_loss=4.970492

Batch 26530, train_perplexity=143.86754, train_loss=4.968893
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 26540, train_perplexity=149.81017, train_loss=5.009369

Batch 26550, train_perplexity=146.69347, train_loss=4.988345

Batch 26560, train_perplexity=151.0053, train_loss=5.017315

Batch 26570, train_perplexity=139.78445, train_loss=4.9401016

Batch 26580, train_perplexity=135.76027, train_loss=4.9108906

Batch 26590, train_perplexity=135.8574, train_loss=4.911606

Batch 26600, train_perplexity=151.00386, train_loss=5.0173054

Batch 26610, train_perplexity=144.95692, train_loss=4.9764366

Batch 26620, train_perplexity=155.09958, train_loss=5.0440674

Batch 26630, train_perplexity=165.20471, train_loss=5.1071854

Batch 26640, train_perplexity=140.42487, train_loss=4.9446726

Batch 26650, train_perplexity=163.91345, train_loss=5.0993385

Batch 26660, train_perplexity=157.00778, train_loss=5.0562954

Batch 26670, train_perplexity=149.62782, train_loss=5.008151

Batch 26680, train_perplexity=139.48131, train_loss=4.9379306

Batch 26690, train_perplexity=136.77716, train_loss=4.918353

Batch 26700, train_perplexity=155.81358, train_loss=5.0486603

Batch 26710, train_perplexity=148.86969, train_loss=5.0030713

Batch 26720, train_perplexity=162.83194, train_loss=5.0927186

Batch 26730, train_perplexity=149.16705, train_loss=5.005067

Batch 26740, train_perplexity=143.337, train_loss=4.9651985

Batch 26750, train_perplexity=145.2978, train_loss=4.9787855

Batch 26760, train_perplexity=150.11592, train_loss=5.011408

Batch 26770, train_perplexity=144.93564, train_loss=4.9762897

Batch 26780, train_perplexity=153.7385, train_loss=5.035253

Batch 26790, train_perplexity=149.92836, train_loss=5.0101576

Batch 26800, train_perplexity=163.48102, train_loss=5.096697

Batch 26810, train_perplexity=160.62769, train_loss=5.079089

Batch 26820, train_perplexity=166.323, train_loss=5.1139317

Batch 26830, train_perplexity=166.2939, train_loss=5.1137567

Batch 26840, train_perplexity=140.22641, train_loss=4.9432583

Batch 26850, train_perplexity=159.72946, train_loss=5.0734816

Batch 26860, train_perplexity=136.59323, train_loss=4.9170074

Batch 26870, train_perplexity=148.8107, train_loss=5.002675

Batch 26880, train_perplexity=166.76804, train_loss=5.116604

Batch 26890, train_perplexity=150.11708, train_loss=5.0114155

Batch 26900, train_perplexity=126.098076, train_loss=4.83706

Batch 26910, train_perplexity=151.73393, train_loss=5.0221286

Batch 26920, train_perplexity=146.51898, train_loss=4.987155

Batch 26930, train_perplexity=147.13214, train_loss=4.991331

Batch 26940, train_perplexity=134.72144, train_loss=4.903209

Batch 26950, train_perplexity=138.30086, train_loss=4.9294314

Batch 26960, train_perplexity=157.17723, train_loss=5.057374

Batch 26970, train_perplexity=136.95403, train_loss=4.9196453

Batch 26980, train_perplexity=159.72093, train_loss=5.073428

Batch 26990, train_perplexity=134.37633, train_loss=4.9006443

Batch 27000, train_perplexity=137.05542, train_loss=4.9203854

Batch 27010, train_perplexity=146.59334, train_loss=4.9876623

Batch 27020, train_perplexity=146.02057, train_loss=4.9837475

Batch 27030, train_perplexity=143.33932, train_loss=4.9652147

Batch 27040, train_perplexity=154.0647, train_loss=5.0373726

Batch 27050, train_perplexity=143.56642, train_loss=4.966798

Batch 27060, train_perplexity=142.36885, train_loss=4.958421

Batch 27070, train_perplexity=140.30025, train_loss=4.9437847

Batch 27080, train_perplexity=146.5925, train_loss=4.9876566

Batch 27090, train_perplexity=149.96811, train_loss=5.0104227

Batch 27100, train_perplexity=143.6599, train_loss=4.9674487

Batch 27110, train_perplexity=137.0813, train_loss=4.920574

Batch 27120, train_perplexity=144.32709, train_loss=4.972082

Batch 27130, train_perplexity=146.5802, train_loss=4.9875727

Batch 27140, train_perplexity=152.73547, train_loss=5.0287075

Batch 27150, train_perplexity=144.85797, train_loss=4.975754

Batch 27160, train_perplexity=133.87607, train_loss=4.8969145

Batch 27170, train_perplexity=123.73096, train_loss=4.8181095

Batch 27180, train_perplexity=135.12527, train_loss=4.9062023

Batch 27190, train_perplexity=147.77126, train_loss=4.9956656

Batch 27200, train_perplexity=149.29756, train_loss=5.0059414

Batch 27210, train_perplexity=137.75832, train_loss=4.925501

Batch 27220, train_perplexity=146.76315, train_loss=4.98882

Batch 27230, train_perplexity=133.16516, train_loss=4.89159

Batch 27240, train_perplexity=146.86565, train_loss=4.989518

Batch 27250, train_perplexity=144.33548, train_loss=4.9721403

Batch 27260, train_perplexity=139.8602, train_loss=4.9406433

Batch 27270, train_perplexity=141.64668, train_loss=4.953336

Batch 27280, train_perplexity=151.0619, train_loss=5.0176897

Batch 27290, train_perplexity=142.77852, train_loss=4.9612947

Batch 27300, train_perplexity=145.48375, train_loss=4.9800644

Batch 27310, train_perplexity=152.31422, train_loss=5.0259457

Batch 27320, train_perplexity=134.95451, train_loss=4.9049377

Batch 27330, train_perplexity=147.59395, train_loss=4.994465

Batch 27340, train_perplexity=139.10371, train_loss=4.93522

Batch 27350, train_perplexity=143.5656, train_loss=4.966792

Batch 27360, train_perplexity=119.69564, train_loss=4.784952

Batch 27370, train_perplexity=157.7144, train_loss=5.060786

Batch 27380, train_perplexity=137.06927, train_loss=4.9204865

Batch 27390, train_perplexity=155.78929, train_loss=5.0485044

Batch 27400, train_perplexity=145.46571, train_loss=4.9799404

Batch 27410, train_perplexity=129.1065, train_loss=4.8606377

Batch 27420, train_perplexity=150.86696, train_loss=5.0163984

Batch 27430, train_perplexity=148.44983, train_loss=5.000247

Batch 27440, train_perplexity=141.221, train_loss=4.950326

Batch 27450, train_perplexity=138.11298, train_loss=4.928072

Batch 27460, train_perplexity=133.02591, train_loss=4.890544

Batch 27470, train_perplexity=159.73267, train_loss=5.0735016

Batch 27480, train_perplexity=146.39914, train_loss=4.9863367

Batch 27490, train_perplexity=147.23615, train_loss=4.992038

Batch 27500, train_perplexity=184.52357, train_loss=5.2177773

Batch 27510, train_perplexity=150.11879, train_loss=5.011427

Batch 27520, train_perplexity=158.64992, train_loss=5.0667

Batch 27530, train_perplexity=153.35718, train_loss=5.0327697

Batch 27540, train_perplexity=147.86635, train_loss=4.996309

Batch 27550, train_perplexity=140.95538, train_loss=4.9484434

Batch 27560, train_perplexity=148.69623, train_loss=5.0019054

Batch 27570, train_perplexity=143.89375, train_loss=4.969075

Batch 27580, train_perplexity=138.49191, train_loss=4.930812

Batch 27590, train_perplexity=142.37964, train_loss=4.958497

Batch 27600, train_perplexity=161.23283, train_loss=5.0828495

Batch 27610, train_perplexity=145.84576, train_loss=4.9825497

Batch 27620, train_perplexity=123.36405, train_loss=4.81514

Batch 27630, train_perplexity=148.10817, train_loss=4.997943

Batch 27640, train_perplexity=136.68445, train_loss=4.917675

Batch 27650, train_perplexity=141.22166, train_loss=4.9503307

Batch 27660, train_perplexity=145.97726, train_loss=4.983451

Batch 27670, train_perplexity=147.28825, train_loss=4.9923916

Batch 27680, train_perplexity=137.36542, train_loss=4.9226446

Batch 27690, train_perplexity=140.69792, train_loss=4.946615

Batch 27700, train_perplexity=142.2143, train_loss=4.957335

Batch 27710, train_perplexity=151.02084, train_loss=5.017418

Batch 27720, train_perplexity=126.72599, train_loss=4.842027

Batch 27730, train_perplexity=125.16439, train_loss=4.829628

Batch 27740, train_perplexity=135.63164, train_loss=4.9099426

Batch 27750, train_perplexity=147.97652, train_loss=4.9970536

Batch 27760, train_perplexity=139.76926, train_loss=4.939993

Batch 27770, train_perplexity=148.24426, train_loss=4.9988613

Batch 27780, train_perplexity=152.55365, train_loss=5.0275164

Batch 27790, train_perplexity=157.31834, train_loss=5.0582714

Batch 27800, train_perplexity=164.58151, train_loss=5.103406

Batch 27810, train_perplexity=126.315926, train_loss=4.838786

Batch 27820, train_perplexity=134.62016, train_loss=4.902457

Batch 27830, train_perplexity=134.00252, train_loss=4.8978586

Batch 27840, train_perplexity=150.08865, train_loss=5.011226

Batch 27850, train_perplexity=138.61255, train_loss=4.9316826
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 27860, train_perplexity=140.97037, train_loss=4.9485497

Batch 27870, train_perplexity=148.3171, train_loss=4.9993525

Batch 27880, train_perplexity=150.40547, train_loss=5.0133348

Batch 27890, train_perplexity=153.58397, train_loss=5.0342474

Batch 27900, train_perplexity=148.11157, train_loss=4.997966

Batch 27910, train_perplexity=134.67114, train_loss=4.902836

Batch 27920, train_perplexity=128.74294, train_loss=4.8578176

Batch 27930, train_perplexity=151.26935, train_loss=5.019062

Batch 27940, train_perplexity=154.4513, train_loss=5.039879

Batch 27950, train_perplexity=158.15578, train_loss=5.0635805

Batch 27960, train_perplexity=137.72798, train_loss=4.9252806

Batch 27970, train_perplexity=143.31473, train_loss=4.965043

Batch 27980, train_perplexity=153.10823, train_loss=5.031145

Batch 27990, train_perplexity=157.21411, train_loss=5.0576086

Batch 28000, train_perplexity=158.33145, train_loss=5.0646906

Batch 28010, train_perplexity=161.18533, train_loss=5.082555

Batch 28020, train_perplexity=152.73111, train_loss=5.028679

Batch 28030, train_perplexity=154.68723, train_loss=5.041405

Batch 28040, train_perplexity=141.07588, train_loss=4.949298

Batch 28050, train_perplexity=175.95244, train_loss=5.1702137

Batch 28060, train_perplexity=152.54071, train_loss=5.0274315

Batch 28070, train_perplexity=147.3512, train_loss=4.992819

Batch 28080, train_perplexity=154.34735, train_loss=5.0392056

Batch 28090, train_perplexity=144.75606, train_loss=4.97505

Batch 28100, train_perplexity=146.64738, train_loss=4.988031

Batch 28110, train_perplexity=153.62308, train_loss=5.034502

Batch 28120, train_perplexity=151.335, train_loss=5.019496

Batch 28130, train_perplexity=138.23888, train_loss=4.928983

Batch 28140, train_perplexity=146.20018, train_loss=4.984977

Batch 28150, train_perplexity=128.37463, train_loss=4.854953

Batch 28160, train_perplexity=141.96489, train_loss=4.9555798

Batch 28170, train_perplexity=144.80783, train_loss=4.9754076

Batch 28180, train_perplexity=132.18784, train_loss=4.884224

Batch 28190, train_perplexity=154.74971, train_loss=5.041809

Batch 28200, train_perplexity=148.7216, train_loss=5.002076

Batch 28210, train_perplexity=142.42398, train_loss=4.9588084

Batch 28220, train_perplexity=147.0503, train_loss=4.9907746

Batch 28230, train_perplexity=149.66536, train_loss=5.008402

Batch 28240, train_perplexity=149.1655, train_loss=5.0050564

Batch 28250, train_perplexity=147.71758, train_loss=4.995302

Batch 28260, train_perplexity=152.84373, train_loss=5.029416

Batch 28270, train_perplexity=152.03653, train_loss=5.024121

Batch 28280, train_perplexity=140.22119, train_loss=4.943221

Batch 28290, train_perplexity=148.0113, train_loss=4.9972887

Batch 28300, train_perplexity=141.33498, train_loss=4.951133

Batch 28310, train_perplexity=145.3197, train_loss=4.978936

Batch 28320, train_perplexity=130.9701, train_loss=4.874969

Batch 28330, train_perplexity=140.91278, train_loss=4.948141

Batch 28340, train_perplexity=146.02063, train_loss=4.983748

Batch 28350, train_perplexity=135.0691, train_loss=4.9057865

Batch 28360, train_perplexity=159.35852, train_loss=5.0711565

Batch 28370, train_perplexity=155.91911, train_loss=5.0493374

Batch 28380, train_perplexity=146.54259, train_loss=4.987316

Batch 28390, train_perplexity=152.78304, train_loss=5.029019

Batch 28400, train_perplexity=152.20517, train_loss=5.0252295

Batch 28410, train_perplexity=152.78304, train_loss=5.029019

Batch 28420, train_perplexity=153.46895, train_loss=5.0334983

Batch 28430, train_perplexity=164.57979, train_loss=5.1033955

Batch 28440, train_perplexity=144.38435, train_loss=4.972479

Batch 28450, train_perplexity=136.01796, train_loss=4.912787

Batch 28460, train_perplexity=149.97112, train_loss=5.0104427

Batch 28470, train_perplexity=137.48566, train_loss=4.9235196

Batch 28480, train_perplexity=146.15578, train_loss=4.984673

Batch 28490, train_perplexity=148.0915, train_loss=4.9978304

Batch 28500, train_perplexity=155.52408, train_loss=5.0468006

Batch 28510, train_perplexity=136.97754, train_loss=4.919817

Batch 28520, train_perplexity=137.362, train_loss=4.92262

Batch 28530, train_perplexity=153.91945, train_loss=5.0364294

Batch 28540, train_perplexity=148.7802, train_loss=5.00247

Batch 28550, train_perplexity=134.05742, train_loss=4.898268

Batch 28560, train_perplexity=143.84196, train_loss=4.968715

Batch 28570, train_perplexity=138.61612, train_loss=4.9317083

Batch 28580, train_perplexity=146.42915, train_loss=4.9865417

Batch 28590, train_perplexity=139.85526, train_loss=4.940608

Batch 28600, train_perplexity=144.14098, train_loss=4.970792

Batch 28610, train_perplexity=164.6228, train_loss=5.103657

Batch 28620, train_perplexity=145.71487, train_loss=4.981652

Batch 28630, train_perplexity=159.83568, train_loss=5.0741463

Batch 28640, train_perplexity=140.82652, train_loss=4.947529

Batch 28650, train_perplexity=143.28186, train_loss=4.9648137

Batch 28660, train_perplexity=148.35966, train_loss=4.9996395

Batch 28670, train_perplexity=138.66187, train_loss=4.9320383

Batch 28680, train_perplexity=141.43347, train_loss=4.9518294

Batch 28690, train_perplexity=145.14879, train_loss=4.9777594

Batch 28700, train_perplexity=142.88913, train_loss=4.962069

Batch 28710, train_perplexity=145.80717, train_loss=4.982285

Batch 28720, train_perplexity=150.32925, train_loss=5.012828

Batch 28730, train_perplexity=159.26796, train_loss=5.070588

Batch 28740, train_perplexity=155.10136, train_loss=5.044079

Batch 28750, train_perplexity=139.80125, train_loss=4.940222

Batch 28760, train_perplexity=142.77301, train_loss=4.961256

Batch 28770, train_perplexity=146.09146, train_loss=4.984233

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 28780, train_perplexity=145.59215, train_loss=4.980809

Batch 28790, train_perplexity=152.64578, train_loss=5.02812

Batch 28800, train_perplexity=136.19293, train_loss=4.9140725

Batch 28810, train_perplexity=122.49172, train_loss=4.8080435

Batch 28820, train_perplexity=140.46545, train_loss=4.9449615

Batch 28830, train_perplexity=131.89305, train_loss=4.8819914

Batch 28840, train_perplexity=126.69994, train_loss=4.8418217

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 28850, train_perplexity=136.87672, train_loss=4.9190807

Batch 28860, train_perplexity=136.68146, train_loss=4.917653

Batch 28870, train_perplexity=152.5701, train_loss=5.027624

Batch 28880, train_perplexity=150.96124, train_loss=5.017023

Batch 28890, train_perplexity=151.20633, train_loss=5.0186453

Batch 28900, train_perplexity=143.5743, train_loss=4.9668527

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 28910, train_perplexity=148.19478, train_loss=4.9985275

Batch 28920, train_perplexity=128.09372, train_loss=4.852762

Batch 28930, train_perplexity=152.8675, train_loss=5.0295715

Batch 28940, train_perplexity=149.43973, train_loss=5.006893

Batch 28950, train_perplexity=144.2587, train_loss=4.971608

Batch 28960, train_perplexity=143.21156, train_loss=4.964323

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 28970, train_perplexity=150.73409, train_loss=5.015517

Batch 28980, train_perplexity=133.30682, train_loss=4.8926535

Batch 28990, train_perplexity=132.1702, train_loss=4.8840904

Batch 29000, train_perplexity=141.63762, train_loss=4.953272

Batch 29010, train_perplexity=140.54611, train_loss=4.9455357

Batch 29020, train_perplexity=139.95706, train_loss=4.9413357

Batch 29030, train_perplexity=132.38585, train_loss=4.8857207

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 29040, train_perplexity=123.31048, train_loss=4.8147054

Batch 29050, train_perplexity=137.53712, train_loss=4.923894

Batch 29060, train_perplexity=133.30447, train_loss=4.892636

Batch 29070, train_perplexity=126.5175, train_loss=4.8403807

Batch 29080, train_perplexity=141.90533, train_loss=4.95516

Batch 29090, train_perplexity=142.96976, train_loss=4.962633

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 29100, train_perplexity=142.76648, train_loss=4.9612103

Batch 29110, train_perplexity=148.99545, train_loss=5.003916

Batch 29120, train_perplexity=137.89397, train_loss=4.926485

Batch 29130, train_perplexity=145.07281, train_loss=4.977236

Batch 29140, train_perplexity=140.0329, train_loss=4.9418774

Batch 29150, train_perplexity=123.66295, train_loss=4.8175597

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 29160, train_perplexity=162.22612, train_loss=5.088991

Batch 29170, train_perplexity=144.10985, train_loss=4.970576

Batch 29180, train_perplexity=130.31767, train_loss=4.869975

Batch 29190, train_perplexity=137.0375, train_loss=4.9202547

Batch 29200, train_perplexity=133.32819, train_loss=4.8928137

Batch 29210, train_perplexity=135.05133, train_loss=4.905655

Batch 29220, train_perplexity=139.37021, train_loss=4.937134

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 29230, train_perplexity=141.50842, train_loss=4.952359

Batch 29240, train_perplexity=166.72113, train_loss=5.1163225

Batch 29250, train_perplexity=148.65652, train_loss=5.0016384

Batch 29260, train_perplexity=142.99594, train_loss=4.962816

Batch 29270, train_perplexity=134.14183, train_loss=4.8988976

Batch 29280, train_perplexity=130.77982, train_loss=4.873515

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 29290, train_perplexity=126.25282, train_loss=4.8382864

Batch 29300, train_perplexity=144.05399, train_loss=4.970188

Batch 29310, train_perplexity=134.07858, train_loss=4.898426

Batch 29320, train_perplexity=146.29886, train_loss=4.9856515

Batch 29330, train_perplexity=135.42729, train_loss=4.908435

Batch 29340, train_perplexity=140.2446, train_loss=4.943388

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 29350, train_perplexity=137.48048, train_loss=4.923482

Batch 29360, train_perplexity=132.6582, train_loss=4.887776

Batch 29370, train_perplexity=134.59006, train_loss=4.9022336

Batch 29380, train_perplexity=137.63606, train_loss=4.924613

Batch 29390, train_perplexity=143.1444, train_loss=4.963854

Batch 29400, train_perplexity=150.46393, train_loss=5.0137234

Batch 29410, train_perplexity=143.94893, train_loss=4.9694586

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 29420, train_perplexity=133.00676, train_loss=4.8904

Batch 29430, train_perplexity=142.57619, train_loss=4.9598765

Batch 29440, train_perplexity=135.81413, train_loss=4.9112873

Batch 29450, train_perplexity=130.92482, train_loss=4.8746233

Batch 29460, train_perplexity=146.48824, train_loss=4.986945

Batch 29470, train_perplexity=142.04764, train_loss=4.9561625

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 29480, train_perplexity=151.47348, train_loss=5.0204105

Batch 29490, train_perplexity=133.34344, train_loss=4.892928

Batch 29500, train_perplexity=132.02954, train_loss=4.8830256

Batch 29510, train_perplexity=151.30853, train_loss=5.019321

Batch 29520, train_perplexity=157.3542, train_loss=5.0584993

Batch 29530, train_perplexity=145.8391, train_loss=4.982504

Batch 29540, train_perplexity=140.38536, train_loss=4.9443913

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 29550, train_perplexity=135.8381, train_loss=4.9114637

Batch 29560, train_perplexity=133.73633, train_loss=4.89587

Batch 29570, train_perplexity=131.37779, train_loss=4.878077

Batch 29580, train_perplexity=154.57051, train_loss=5.0406504

Batch 29590, train_perplexity=143.43915, train_loss=4.965911

Batch 29600, train_perplexity=143.65558, train_loss=4.9674187

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 29610, train_perplexity=136.96245, train_loss=4.919707

Batch 29620, train_perplexity=135.98865, train_loss=4.9125714

Batch 29630, train_perplexity=130.57132, train_loss=4.8719196

Batch 29640, train_perplexity=144.20697, train_loss=4.9712496

Batch 29650, train_perplexity=143.84052, train_loss=4.968705

Batch 29660, train_perplexity=136.95259, train_loss=4.919635

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 29670, train_perplexity=149.59721, train_loss=5.0079465

Batch 29680, train_perplexity=134.91101, train_loss=4.9046154

Batch 29690, train_perplexity=149.16194, train_loss=5.0050325

Batch 29700, train_perplexity=139.47285, train_loss=4.93787

Batch 29710, train_perplexity=135.15736, train_loss=4.90644

Batch 29720, train_perplexity=141.00943, train_loss=4.948827

Batch 29730, train_perplexity=142.79439, train_loss=4.9614058

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 29740, train_perplexity=152.45374, train_loss=5.026861

Batch 29750, train_perplexity=161.9241, train_loss=5.0871277

Batch 29760, train_perplexity=131.16803, train_loss=4.876479

Batch 29770, train_perplexity=149.00923, train_loss=5.0040083

Batch 29780, train_perplexity=155.26134, train_loss=5.0451097

Batch 29790, train_perplexity=150.11278, train_loss=5.011387

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 29800, train_perplexity=128.6039, train_loss=4.856737

Batch 29810, train_perplexity=135.21048, train_loss=4.9068327

Batch 29820, train_perplexity=124.07434, train_loss=4.820881

Batch 29830, train_perplexity=142.93336, train_loss=4.9623785

Batch 29840, train_perplexity=132.48746, train_loss=4.886488

Batch 29850, train_perplexity=144.50034, train_loss=4.973282

Batch 29860, train_perplexity=137.3053, train_loss=4.922207

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 29870, train_perplexity=150.3592, train_loss=5.013027

Batch 29880, train_perplexity=151.61353, train_loss=5.0213346

Batch 29890, train_perplexity=139.31148, train_loss=4.9367123

Batch 29900, train_perplexity=140.37921, train_loss=4.9443474

Batch 29910, train_perplexity=152.27733, train_loss=5.0257034

Batch 29920, train_perplexity=140.13469, train_loss=4.942604

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 29930, train_perplexity=137.88963, train_loss=4.9264536

Batch 29940, train_perplexity=139.70349, train_loss=4.9395223

Batch 29950, train_perplexity=138.7149, train_loss=4.9324207

Batch 29960, train_perplexity=149.62526, train_loss=5.008134

Batch 29970, train_perplexity=141.91473, train_loss=4.9552264

Batch 29980, train_perplexity=141.67355, train_loss=4.9535255

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 29990, train_perplexity=119.04016, train_loss=4.779461

Batch 30000, train_perplexity=132.2847, train_loss=4.8849564

Batch 30010, train_perplexity=139.59009, train_loss=4.93871

Batch 30020, train_perplexity=142.28964, train_loss=4.9578648

Batch 30030, train_perplexity=149.47743, train_loss=5.0071454

Batch 30040, train_perplexity=156.92485, train_loss=5.055767

Batch 30050, train_perplexity=147.87784, train_loss=4.9963865

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 30060, train_perplexity=156.43729, train_loss=5.052655

Batch 30070, train_perplexity=143.66148, train_loss=4.9674597

Batch 30080, train_perplexity=134.84644, train_loss=4.9041367

Batch 30090, train_perplexity=143.52809, train_loss=4.966531

Batch 30100, train_perplexity=140.03563, train_loss=4.941897

Batch 30110, train_perplexity=146.76721, train_loss=4.9888477

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 30120, train_perplexity=166.27304, train_loss=5.1136312

Batch 30130, train_perplexity=126.070175, train_loss=4.8368387

Batch 30140, train_perplexity=130.38977, train_loss=4.870528

Batch 30150, train_perplexity=131.71217, train_loss=4.880619

Batch 30160, train_perplexity=130.05298, train_loss=4.867942

Batch 30170, train_perplexity=157.0414, train_loss=5.0565095

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 30180, train_perplexity=136.35701, train_loss=4.9152765

Batch 30190, train_perplexity=132.93967, train_loss=4.8898954

Batch 30200, train_perplexity=145.06161, train_loss=4.9771585

Batch 30210, train_perplexity=154.42398, train_loss=5.039702

Batch 30220, train_perplexity=144.54678, train_loss=4.9736032

Batch 30230, train_perplexity=137.90128, train_loss=4.926538

Batch 30240, train_perplexity=140.55891, train_loss=4.9456267

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 30250, train_perplexity=150.15215, train_loss=5.011649

Batch 30260, train_perplexity=145.55682, train_loss=4.9805665

Batch 30270, train_perplexity=150.60074, train_loss=5.014632

Batch 30280, train_perplexity=144.60849, train_loss=4.97403

Batch 30290, train_perplexity=133.89943, train_loss=4.897089

Batch 30300, train_perplexity=127.768585, train_loss=4.8502207

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 30310, train_perplexity=122.45458, train_loss=4.80774

Batch 30320, train_perplexity=137.78407, train_loss=4.925688

Batch 30330, train_perplexity=133.50238, train_loss=4.8941193

Batch 30340, train_perplexity=133.22232, train_loss=4.8920193

Batch 30350, train_perplexity=141.87251, train_loss=4.954929

Batch 30360, train_perplexity=136.75734, train_loss=4.918208

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 30370, train_perplexity=129.07228, train_loss=4.8603725

Batch 30380, train_perplexity=139.70975, train_loss=4.939567

Batch 30390, train_perplexity=150.89891, train_loss=5.01661

Batch 30400, train_perplexity=143.27783, train_loss=4.9647856

Batch 30410, train_perplexity=132.29094, train_loss=4.8850036

Batch 30420, train_perplexity=157.64365, train_loss=5.060337

Batch 30430, train_perplexity=145.46655, train_loss=4.979946

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 30440, train_perplexity=132.21854, train_loss=4.884456

Batch 30450, train_perplexity=140.01273, train_loss=4.9417334

Batch 30460, train_perplexity=146.29181, train_loss=4.9856033

Batch 30470, train_perplexity=151.64215, train_loss=5.0215235

Batch 30480, train_perplexity=132.55083, train_loss=4.886966

Batch 30490, train_perplexity=135.59225, train_loss=4.909652

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 30500, train_perplexity=157.36246, train_loss=5.058552

Batch 30510, train_perplexity=154.64777, train_loss=5.04115

Batch 30520, train_perplexity=144.5818, train_loss=4.9738455

Batch 30530, train_perplexity=143.4107, train_loss=4.9657125

Batch 30540, train_perplexity=148.09448, train_loss=4.9978504

Batch 30550, train_perplexity=142.24216, train_loss=4.957531

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 30560, train_perplexity=138.89227, train_loss=4.9336987

Batch 30570, train_perplexity=123.46869, train_loss=4.8159876

Batch 30580, train_perplexity=136.78212, train_loss=4.9183893

Batch 30590, train_perplexity=148.8178, train_loss=5.0027227

Batch 30600, train_perplexity=124.18941, train_loss=4.821808

Batch 30610, train_perplexity=135.93102, train_loss=4.9121475

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 30620, train_perplexity=131.96225, train_loss=4.882516

Batch 30630, train_perplexity=131.50508, train_loss=4.8790455

Batch 30640, train_perplexity=131.13295, train_loss=4.8762116

Batch 30650, train_perplexity=146.31456, train_loss=4.985759

Batch 30660, train_perplexity=148.95056, train_loss=5.0036144

Batch 30670, train_perplexity=150.23308, train_loss=5.012188

Batch 30680, train_perplexity=135.24956, train_loss=4.9071217

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 30690, train_perplexity=127.7, train_loss=4.849684

Batch 30700, train_perplexity=137.48914, train_loss=4.923545

Batch 30710, train_perplexity=159.94516, train_loss=5.074831

Batch 30720, train_perplexity=140.5102, train_loss=4.94528

Batch 30730, train_perplexity=147.2655, train_loss=4.992237

Batch 30740, train_perplexity=152.42233, train_loss=5.026655

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 30750, train_perplexity=128.14725, train_loss=4.85318

Batch 30760, train_perplexity=148.45987, train_loss=5.0003147

Batch 30770, train_perplexity=145.14685, train_loss=4.977746

Batch 30780, train_perplexity=140.54678, train_loss=4.9455404

Batch 30790, train_perplexity=125.99037, train_loss=4.8362055

Batch 30800, train_perplexity=126.427284, train_loss=4.8396673

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 30810, train_perplexity=143.7474, train_loss=4.9680576

Batch 30820, train_perplexity=142.97235, train_loss=4.9626513

Batch 30830, train_perplexity=141.18597, train_loss=4.950078

Batch 30840, train_perplexity=144.73798, train_loss=4.974925

Batch 30850, train_perplexity=138.18747, train_loss=4.9286113

Batch 30860, train_perplexity=136.96696, train_loss=4.9197397

Batch 30870, train_perplexity=146.2213, train_loss=4.9851213

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loadingWARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 30880, train_perplexity=143.90076, train_loss=4.969124

Batch 30890, train_perplexity=137.37216, train_loss=4.9226937

Batch 30900, train_perplexity=118.84727, train_loss=4.777839

Batch 30910, train_perplexity=156.89552, train_loss=5.05558

Batch 30920, train_perplexity=133.40106, train_loss=4.89336

Batch 30930, train_perplexity=154.94693, train_loss=5.0430827

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 30940, train_perplexity=146.14386, train_loss=4.9845915

Batch 30950, train_perplexity=148.49838, train_loss=5.000574

Batch 30960, train_perplexity=144.41507, train_loss=4.9726915

Batch 30970, train_perplexity=133.46501, train_loss=4.8938394

Batch 30980, train_perplexity=157.85126, train_loss=5.061653

Batch 30990, train_perplexity=154.63878, train_loss=5.041092

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 31000, train_perplexity=128.0483, train_loss=4.8524075

Batch 31010, train_perplexity=150.63234, train_loss=5.014842

Batch 31020, train_perplexity=127.127686, train_loss=4.845192

Batch 31030, train_perplexity=130.8518, train_loss=4.8740654

Batch 31040, train_perplexity=162.7633, train_loss=5.092297

Batch 31050, train_perplexity=146.93976, train_loss=4.9900227

Batch 31060, train_perplexity=124.99214, train_loss=4.828251

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 31070, train_perplexity=156.07086, train_loss=5.05031

Batch 31080, train_perplexity=125.5412, train_loss=4.832634

Batch 31090, train_perplexity=141.57631, train_loss=4.952839

Batch 31100, train_perplexity=140.21103, train_loss=4.9431486

Batch 31110, train_perplexity=152.18602, train_loss=5.0251036

Batch 31120, train_perplexity=136.07486, train_loss=4.913205

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 31130, train_perplexity=149.22453, train_loss=5.005452

Batch 31140, train_perplexity=129.46005, train_loss=4.8633723

Batch 31150, train_perplexity=147.55426, train_loss=4.994196

Batch 31160, train_perplexity=139.82678, train_loss=4.9404044

Batch 31170, train_perplexity=132.8602, train_loss=4.8892975

Batch 31180, train_perplexity=145.3423, train_loss=4.9790916

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 31190, train_perplexity=146.11745, train_loss=4.984411

Batch 31200, train_perplexity=125.39235, train_loss=4.8314476

Batch 31210, train_perplexity=127.92812, train_loss=4.8514686

Batch 31220, train_perplexity=149.89633, train_loss=5.009944

Batch 31230, train_perplexity=134.21725, train_loss=4.89946

Batch 31240, train_perplexity=134.446, train_loss=4.9011626

Batch 31250, train_perplexity=138.206, train_loss=4.9287453

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 31260, train_perplexity=152.12782, train_loss=5.024721

Batch 31270, train_perplexity=146.90935, train_loss=4.9898157

Batch 31280, train_perplexity=149.28987, train_loss=5.00589

Batch 31290, train_perplexity=124.804184, train_loss=4.826746

Batch 31300, train_perplexity=142.44456, train_loss=4.958953

Batch 31310, train_perplexity=133.12871, train_loss=4.8913164

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 31320, train_perplexity=150.38733, train_loss=5.013214

Batch 31330, train_perplexity=121.72263, train_loss=4.801745

Batch 31340, train_perplexity=149.50009, train_loss=5.007297

Batch 31350, train_perplexity=157.4767, train_loss=5.0592775

Batch 31360, train_perplexity=129.37747, train_loss=4.8627343

Batch 31370, train_perplexity=134.5744, train_loss=4.9021173

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 31380, train_perplexity=145.5784, train_loss=4.980715

Batch 31390, train_perplexity=143.43108, train_loss=4.9658546

Batch 31400, train_perplexity=119.28494, train_loss=4.781515

Batch 31410, train_perplexity=144.60414, train_loss=4.974

Batch 31420, train_perplexity=139.10623, train_loss=4.935238

Batch 31430, train_perplexity=141.67923, train_loss=4.9535656

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 31440, train_perplexity=136.23846, train_loss=4.914407

Batch 31450, train_perplexity=148.0344, train_loss=4.9974446

Batch 31460, train_perplexity=128.88335, train_loss=4.8589077

Batch 31470, train_perplexity=143.4921, train_loss=4.96628

Batch 31480, train_perplexity=139.61772, train_loss=4.938908

Batch 31490, train_perplexity=149.7568, train_loss=5.0090127

Batch 31500, train_perplexity=161.09642, train_loss=5.082003

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 31510, train_perplexity=142.05008, train_loss=4.9561796

Batch 31520, train_perplexity=119.84881, train_loss=4.786231

Batch 31530, train_perplexity=149.15866, train_loss=5.0050106

Batch 31540, train_perplexity=134.92915, train_loss=4.90475

Batch 31550, train_perplexity=144.18016, train_loss=4.9710636

Batch 31560, train_perplexity=137.86281, train_loss=4.926259

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 31570, train_perplexity=138.82117, train_loss=4.9331865

Batch 31580, train_perplexity=155.25201, train_loss=5.0450497

Batch 31590, train_perplexity=134.55034, train_loss=4.9019384

Batch 31600, train_perplexity=137.74768, train_loss=4.9254236

Batch 31610, train_perplexity=131.67017, train_loss=4.8803

Batch 31620, train_perplexity=140.68303, train_loss=4.9465094

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 31630, train_perplexity=148.9946, train_loss=5.00391

Batch 31640, train_perplexity=144.74446, train_loss=4.97497

Batch 31650, train_perplexity=133.27162, train_loss=4.8923893

Batch 31660, train_perplexity=133.19551, train_loss=4.891818

Batch 31670, train_perplexity=140.29837, train_loss=4.9437714

Batch 31680, train_perplexity=120.15506, train_loss=4.788783

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 31690, train_perplexity=143.41371, train_loss=4.9657335

Batch 31700, train_perplexity=143.70232, train_loss=4.967744

Batch 31710, train_perplexity=130.08275, train_loss=4.8681707

Batch 31720, train_perplexity=130.68027, train_loss=4.8727536

Batch 31730, train_perplexity=134.96616, train_loss=4.905024

Batch 31740, train_perplexity=149.84253, train_loss=5.009585

Batch 31750, train_perplexity=143.1364, train_loss=4.963798

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 31760, train_perplexity=142.00165, train_loss=4.9558387

Batch 31770, train_perplexity=129.78064, train_loss=4.8658457

Batch 31780, train_perplexity=150.78304, train_loss=5.015842

Batch 31790, train_perplexity=137.7176, train_loss=4.925205

Batch 31800, train_perplexity=141.06606, train_loss=4.9492283

Batch 31810, train_perplexity=143.45816, train_loss=4.9660435

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 31820, train_perplexity=146.40068, train_loss=4.986347

Batch 31830, train_perplexity=162.62546, train_loss=5.0914497

Batch 31840, train_perplexity=138.77246, train_loss=4.9328356

Batch 31850, train_perplexity=134.77946, train_loss=4.90364

Batch 31860, train_perplexity=137.84546, train_loss=4.926133

Batch 31870, train_perplexity=142.05603, train_loss=4.9562216

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 31880, train_perplexity=134.96358, train_loss=4.905005

Batch 31890, train_perplexity=127.50347, train_loss=4.8481436

Batch 31900, train_perplexity=133.53395, train_loss=4.894356

Batch 31910, train_perplexity=129.80911, train_loss=4.866065

Batch 31920, train_perplexity=130.878, train_loss=4.8742657

Batch 31930, train_perplexity=131.76375, train_loss=4.8810105

Batch 31940, train_perplexity=131.58638, train_loss=4.8796635

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 31950, train_perplexity=136.55208, train_loss=4.916706

Batch 31960, train_perplexity=121.56666, train_loss=4.8004627

Batch 31970, train_perplexity=153.41284, train_loss=5.0331326

Batch 31980, train_perplexity=130.3669, train_loss=4.8703527

Batch 31990, train_perplexity=120.69749, train_loss=4.7932873

Batch 32000, train_perplexity=144.77097, train_loss=4.975153

Batch 32010, train_perplexity=131.78976, train_loss=4.881208

Batch 32020, train_perplexity=126.51696, train_loss=4.8403764

Batch 32030, train_perplexity=125.25592, train_loss=4.830359

Batch 32040, train_perplexity=143.44121, train_loss=4.965925

Batch 32050, train_perplexity=136.15845, train_loss=4.9138193

Batch 32060, train_perplexity=134.27301, train_loss=4.899875

Batch 32070, train_perplexity=132.51866, train_loss=4.8867235

Batch 32080, train_perplexity=125.0303, train_loss=4.828556

Batch 32090, train_perplexity=133.94414, train_loss=4.897423

Batch 32100, train_perplexity=125.89435, train_loss=4.835443

Batch 32110, train_perplexity=123.32447, train_loss=4.814819

Batch 32120, train_perplexity=129.65787, train_loss=4.864899

Batch 32130, train_perplexity=128.68236, train_loss=4.857347

Batch 32140, train_perplexity=143.43793, train_loss=4.9659023

Batch 32150, train_perplexity=139.34868, train_loss=4.9369793

Batch 32160, train_perplexity=143.01436, train_loss=4.962945

Batch 32170, train_perplexity=127.26063, train_loss=4.846237

Batch 32180, train_perplexity=124.47006, train_loss=4.824065

Batch 32190, train_perplexity=123.55138, train_loss=4.816657

Batch 32200, train_perplexity=135.16838, train_loss=4.9065213

Batch 32210, train_perplexity=140.24017, train_loss=4.9433565

Batch 32220, train_perplexity=143.10863, train_loss=4.963604

Batch 32230, train_perplexity=142.39696, train_loss=4.9586186

Batch 32240, train_perplexity=155.70662, train_loss=5.0479736

Batch 32250, train_perplexity=121.61286, train_loss=4.800843

Batch 32260, train_perplexity=138.6657, train_loss=4.932066

Batch 32270, train_perplexity=131.68367, train_loss=4.8804026

Batch 32280, train_perplexity=131.45749, train_loss=4.8786836

Batch 32290, train_perplexity=137.12099, train_loss=4.9208636

Batch 32300, train_perplexity=120.07293, train_loss=4.7880993

Batch 32310, train_perplexity=129.52483, train_loss=4.8638725

Batch 32320, train_perplexity=135.16756, train_loss=4.906515

Batch 32330, train_perplexity=127.89659, train_loss=4.851222

Batch 32340, train_perplexity=154.87935, train_loss=5.0426464
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 32350, train_perplexity=131.64587, train_loss=4.8801155

Batch 32360, train_perplexity=140.30547, train_loss=4.943822

Batch 32370, train_perplexity=127.62963, train_loss=4.8491325

Batch 32380, train_perplexity=131.38298, train_loss=4.8781166

Batch 32390, train_perplexity=142.4049, train_loss=4.9586744

Batch 32400, train_perplexity=143.5771, train_loss=4.966872

Batch 32410, train_perplexity=148.75537, train_loss=5.002303

Batch 32420, train_perplexity=120.89125, train_loss=4.7948914

Batch 32430, train_perplexity=135.62115, train_loss=4.9098654

Batch 32440, train_perplexity=127.6832, train_loss=4.849552

Batch 32450, train_perplexity=143.11798, train_loss=4.9636693

Batch 32460, train_perplexity=144.23936, train_loss=4.971474

Batch 32470, train_perplexity=144.30809, train_loss=4.9719505

Batch 32480, train_perplexity=127.14629, train_loss=4.8453383

Batch 32490, train_perplexity=131.03343, train_loss=4.8754525

Batch 32500, train_perplexity=141.21782, train_loss=4.9503036

Batch 32510, train_perplexity=121.72466, train_loss=4.8017616

Batch 32520, train_perplexity=126.66998, train_loss=4.841585

Batch 32530, train_perplexity=128.63861, train_loss=4.857007

Batch 32540, train_perplexity=141.39227, train_loss=4.951538

Batch 32550, train_perplexity=143.35846, train_loss=4.9653482

Batch 32560, train_perplexity=133.5423, train_loss=4.8944182

Batch 32570, train_perplexity=151.1488, train_loss=5.018265

Batch 32580, train_perplexity=148.35033, train_loss=4.9995766

Batch 32590, train_perplexity=128.60979, train_loss=4.856783

Batch 32600, train_perplexity=136.78487, train_loss=4.9184093

Batch 32610, train_perplexity=149.00334, train_loss=5.0039687

Batch 32620, train_perplexity=138.87082, train_loss=4.933544

Batch 32630, train_perplexity=127.096405, train_loss=4.844946

Batch 32640, train_perplexity=134.12328, train_loss=4.8987594

Batch 32650, train_perplexity=144.97165, train_loss=4.976538

Batch 32660, train_perplexity=134.95502, train_loss=4.9049416

Batch 32670, train_perplexity=137.00981, train_loss=4.9200525

Batch 32680, train_perplexity=136.77977, train_loss=4.918372

Batch 32690, train_perplexity=146.88834, train_loss=4.9896727

Batch 32700, train_perplexity=130.0784, train_loss=4.8681374

Batch 32710, train_perplexity=132.00165, train_loss=4.8828144

Batch 32720, train_perplexity=137.70946, train_loss=4.925146

Batch 32730, train_perplexity=138.06201, train_loss=4.927703

Batch 32740, train_perplexity=137.04025, train_loss=4.9202747

Batch 32750, train_perplexity=133.39897, train_loss=4.8933444

Batch 32760, train_perplexity=147.86888, train_loss=4.996326

Batch 32770, train_perplexity=136.15274, train_loss=4.9137774

Batch 32780, train_perplexity=118.19998, train_loss=4.772378

Batch 32790, train_perplexity=129.21748, train_loss=4.861497

Batch 32800, train_perplexity=140.06361, train_loss=4.9420967

Batch 32810, train_perplexity=153.08101, train_loss=5.030967

Batch 32820, train_perplexity=139.52261, train_loss=4.9382267

Batch 32830, train_perplexity=137.61139, train_loss=4.9244337

Batch 32840, train_perplexity=142.20833, train_loss=4.957293

Batch 32850, train_perplexity=144.49207, train_loss=4.9732246

Batch 32860, train_perplexity=145.63263, train_loss=4.981087

Batch 32870, train_perplexity=127.31222, train_loss=4.8466425

Batch 32880, train_perplexity=127.2846, train_loss=4.8464255

Batch 32890, train_perplexity=129.05357, train_loss=4.8602276

Batch 32900, train_perplexity=126.87394, train_loss=4.843194

Batch 32910, train_perplexity=153.50797, train_loss=5.0337524

Batch 32920, train_perplexity=135.20996, train_loss=4.906829

Batch 32930, train_perplexity=123.62734, train_loss=4.8172717

Batch 32940, train_perplexity=139.05377, train_loss=4.9348607

Batch 32950, train_perplexity=124.072685, train_loss=4.8208675

Batch 32960, train_perplexity=139.04243, train_loss=4.934779

Batch 32970, train_perplexity=132.45511, train_loss=4.886244

Batch 32980, train_perplexity=132.70457, train_loss=4.8881254

Batch 32990, train_perplexity=147.45213, train_loss=4.9935036

Batch 33000, train_perplexity=138.38371, train_loss=4.9300303

Batch 33010, train_perplexity=138.68408, train_loss=4.9321985

Batch 33020, train_perplexity=120.83748, train_loss=4.7944465

Batch 33030, train_perplexity=133.47513, train_loss=4.893915

Batch 33040, train_perplexity=143.44612, train_loss=4.9659595

Batch 33050, train_perplexity=139.72101, train_loss=4.9396477

Batch 33060, train_perplexity=137.24913, train_loss=4.9217978

Batch 33070, train_perplexity=141.74492, train_loss=4.954029

Batch 33080, train_perplexity=147.44383, train_loss=4.9934473

Batch 33090, train_perplexity=121.744514, train_loss=4.8019247

Batch 33100, train_perplexity=144.8193, train_loss=4.9754868

Batch 33110, train_perplexity=137.90948, train_loss=4.9265976

Batch 33120, train_perplexity=139.20457, train_loss=4.9359446

Batch 33130, train_perplexity=135.3142, train_loss=4.9075994

Batch 33140, train_perplexity=120.57059, train_loss=4.7922354

Batch 33150, train_perplexity=146.52876, train_loss=4.9872217

Batch 33160, train_perplexity=150.16847, train_loss=5.011758

Batch 33170, train_perplexity=135.77898, train_loss=4.9110284

Batch 33180, train_perplexity=141.83112, train_loss=4.954637

Batch 33190, train_perplexity=138.89227, train_loss=4.9336987

Batch 33200, train_perplexity=144.49965, train_loss=4.973277

Batch 33210, train_perplexity=125.62431, train_loss=4.833296

Batch 33220, train_perplexity=139.65866, train_loss=4.9392014

Batch 33230, train_perplexity=139.65102, train_loss=4.9391465

Batch 33240, train_perplexity=136.25432, train_loss=4.914523

Batch 33250, train_perplexity=119.46152, train_loss=4.7829943

Batch 33260, train_perplexity=133.179, train_loss=4.891694

Batch 33270, train_perplexity=142.54077, train_loss=4.959628

Batch 33280, train_perplexity=140.91432, train_loss=4.948152

Batch 33290, train_perplexity=160.78294, train_loss=5.080055

Batch 33300, train_perplexity=153.48863, train_loss=5.0336266

Batch 33310, train_perplexity=127.08259, train_loss=4.844837

Batch 33320, train_perplexity=153.38548, train_loss=5.032954

Batch 33330, train_perplexity=127.103676, train_loss=4.845003

Batch 33340, train_perplexity=149.0995, train_loss=5.004614

Batch 33350, train_perplexity=129.87994, train_loss=4.8666105

Batch 33360, train_perplexity=127.83446, train_loss=4.850736

Batch 33370, train_perplexity=135.73697, train_loss=4.910719

Batch 33380, train_perplexity=139.05078, train_loss=4.9348392

Batch 33390, train_perplexity=136.96225, train_loss=4.9197054

Batch 33400, train_perplexity=155.14218, train_loss=5.044342

Batch 33410, train_perplexity=137.31845, train_loss=4.9223027

Batch 33420, train_perplexity=131.58122, train_loss=4.8796244

Batch 33430, train_perplexity=135.2446, train_loss=4.907085

Batch 33440, train_perplexity=141.19164, train_loss=4.950118

Batch 33450, train_perplexity=137.62563, train_loss=4.924537

Batch 33460, train_perplexity=150.19312, train_loss=5.011922

Batch 33470, train_perplexity=142.33919, train_loss=4.958213

Batch 33480, train_perplexity=126.18469, train_loss=4.8377466

Batch 33490, train_perplexity=132.69691, train_loss=4.8880677

Batch 33500, train_perplexity=128.63469, train_loss=4.8569765

Batch 33510, train_perplexity=141.05125, train_loss=4.9491234

Batch 33520, train_perplexity=145.36357, train_loss=4.979238

Batch 33530, train_perplexity=148.9615, train_loss=5.003688

Batch 33540, train_perplexity=140.0162, train_loss=4.941758

Batch 33550, train_perplexity=128.78825, train_loss=4.8581696

Batch 33560, train_perplexity=117.68921, train_loss=4.7680473

Batch 33570, train_perplexity=147.87552, train_loss=4.996371

Batch 33580, train_perplexity=133.96687, train_loss=4.8975925

Batch 33590, train_perplexity=139.75153, train_loss=4.939866

Batch 33600, train_perplexity=130.65279, train_loss=4.8725433

Batch 33610, train_perplexity=132.49844, train_loss=4.886571

Batch 33620, train_perplexity=122.90854, train_loss=4.8114405

Batch 33630, train_perplexity=134.38095, train_loss=4.9006786

Batch 33640, train_perplexity=127.95003, train_loss=4.8516397

Batch 33650, train_perplexity=128.63187, train_loss=4.8569546

Batch 33660, train_perplexity=142.51407, train_loss=4.9594407
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33670, train_perplexity=134.32188, train_loss=4.900239

Batch 33680, train_perplexity=140.74731, train_loss=4.946966

Batch 33690, train_perplexity=145.89807, train_loss=4.9829082

Batch 33700, train_perplexity=128.87007, train_loss=4.8588047

Batch 33710, train_perplexity=149.71333, train_loss=5.0087223

Batch 33720, train_perplexity=122.1073, train_loss=4.8049

Batch 33730, train_perplexity=141.70937, train_loss=4.9537783

Batch 33740, train_perplexity=134.97028, train_loss=4.9050546

Batch 33750, train_perplexity=141.27124, train_loss=4.9506817

Batch 33760, train_perplexity=137.12047, train_loss=4.92086

Batch 33770, train_perplexity=137.10497, train_loss=4.920747

Batch 33780, train_perplexity=138.87804, train_loss=4.933596

Batch 33790, train_perplexity=136.55469, train_loss=4.916725

Batch 33800, train_perplexity=138.0089, train_loss=4.927318

Batch 33810, train_perplexity=143.31274, train_loss=4.9650292

Batch 33820, train_perplexity=119.6808, train_loss=4.784828

Batch 33830, train_perplexity=122.38734, train_loss=4.807191

Batch 33840, train_perplexity=139.58809, train_loss=4.938696

Batch 33850, train_perplexity=145.79459, train_loss=4.9821987

Batch 33860, train_perplexity=159.39804, train_loss=5.0714045

Batch 33870, train_perplexity=133.59337, train_loss=4.8948007

Batch 33880, train_perplexity=120.13329, train_loss=4.788602

Batch 33890, train_perplexity=145.83353, train_loss=4.9824657

Batch 33900, train_perplexity=134.24388, train_loss=4.899658

Batch 33910, train_perplexity=138.14604, train_loss=4.9283113

Batch 33920, train_perplexity=121.506615, train_loss=4.7999687

Batch 33930, train_perplexity=149.87811, train_loss=5.0098224

Batch 33940, train_perplexity=133.19405, train_loss=4.891807

Batch 33950, train_perplexity=124.45059, train_loss=4.823909

Batch 33960, train_perplexity=129.24855, train_loss=4.8617373

Batch 33970, train_perplexity=140.00032, train_loss=4.9416447

Batch 33980, train_perplexity=143.0102, train_loss=4.962916

Batch 33990, train_perplexity=139.82059, train_loss=4.94036

Batch 34000, train_perplexity=113.358955, train_loss=4.7305593

Batch 34010, train_perplexity=137.72102, train_loss=4.92523

Batch 34020, train_perplexity=129.9694, train_loss=4.867299

Batch 34030, train_perplexity=146.17494, train_loss=4.984804

Batch 34040, train_perplexity=146.00218, train_loss=4.9836216

Batch 34050, train_perplexity=120.656975, train_loss=4.7929516

Batch 34060, train_perplexity=121.878105, train_loss=4.8030214

Batch 34070, train_perplexity=134.37921, train_loss=4.9006658

Batch 34080, train_perplexity=138.72151, train_loss=4.9324684

Batch 34090, train_perplexity=135.55586, train_loss=4.909384

Batch 34100, train_perplexity=137.94856, train_loss=4.926881

Batch 34110, train_perplexity=131.83496, train_loss=4.881551

Batch 34120, train_perplexity=148.78148, train_loss=5.0024786

Batch 34130, train_perplexity=136.90291, train_loss=4.919272

Batch 34140, train_perplexity=142.93268, train_loss=4.9623737

Batch 34150, train_perplexity=126.40619, train_loss=4.8395004

Batch 34160, train_perplexity=130.0717, train_loss=4.868086

Batch 34170, train_perplexity=142.43573, train_loss=4.958891

Batch 34180, train_perplexity=127.53922, train_loss=4.848424

Batch 34190, train_perplexity=132.20297, train_loss=4.8843384

Batch 34200, train_perplexity=147.21768, train_loss=4.9919124

Batch 34210, train_perplexity=130.9076, train_loss=4.8744917

Batch 34220, train_perplexity=140.1665, train_loss=4.942831

Batch 34230, train_perplexity=139.39148, train_loss=4.9372864

Batch 34240, train_perplexity=125.69262, train_loss=4.8338394

Batch 34250, train_perplexity=134.50807, train_loss=4.901624

Batch 34260, train_perplexity=145.59048, train_loss=4.980798

Batch 34270, train_perplexity=132.7466, train_loss=4.888442

Batch 34280, train_perplexity=123.77492, train_loss=4.8184648

Batch 34290, train_perplexity=111.90288, train_loss=4.7176313

Batch 34300, train_perplexity=149.84096, train_loss=5.0095744

Batch 34310, train_perplexity=124.18052, train_loss=4.8217363

Batch 34320, train_perplexity=139.71442, train_loss=4.9396005

Batch 34330, train_perplexity=138.20428, train_loss=4.928733

Batch 34340, train_perplexity=118.23538, train_loss=4.7726774

Batch 34350, train_perplexity=120.35392, train_loss=4.7904367

Batch 34360, train_perplexity=121.97258, train_loss=4.8037963

Batch 34370, train_perplexity=138.76868, train_loss=4.9328084

Batch 34380, train_perplexity=137.14688, train_loss=4.9210525

Batch 34390, train_perplexity=131.11006, train_loss=4.876037

Batch 34400, train_perplexity=127.480736, train_loss=4.8479652

Batch 34410, train_perplexity=128.48094, train_loss=4.8557806

Batch 34420, train_perplexity=140.38805, train_loss=4.9444103

Batch 34430, train_perplexity=138.81879, train_loss=4.9331694

Batch 34440, train_perplexity=145.06645, train_loss=4.977192

Batch 34450, train_perplexity=136.5629, train_loss=4.9167852

Batch 34460, train_perplexity=135.268, train_loss=4.907258

Batch 34470, train_perplexity=120.367004, train_loss=4.7905455

Batch 34480, train_perplexity=125.7347, train_loss=4.834174

Batch 34490, train_perplexity=136.73712, train_loss=4.9180603

Batch 34500, train_perplexity=140.15448, train_loss=4.942745

Batch 34510, train_perplexity=144.08243, train_loss=4.9703856

Batch 34520, train_perplexity=118.180595, train_loss=4.772214

Batch 34530, train_perplexity=135.23383, train_loss=4.9070053

Batch 34540, train_perplexity=133.24734, train_loss=4.892207

Batch 34550, train_perplexity=130.24425, train_loss=4.8694115

Batch 34560, train_perplexity=119.51268, train_loss=4.7834225

Batch 34570, train_perplexity=146.32544, train_loss=4.985833

Batch 34580, train_perplexity=139.81178, train_loss=4.940297

Batch 34590, train_perplexity=131.00644, train_loss=4.8752465

Batch 34600, train_perplexity=132.67023, train_loss=4.8878665

Batch 34610, train_perplexity=129.53001, train_loss=4.8639126

Batch 34620, train_perplexity=151.18715, train_loss=5.0185184

Batch 34630, train_perplexity=148.87344, train_loss=5.0030966

Batch 34640, train_perplexity=117.30889, train_loss=4.7648106

Batch 34650, train_perplexity=138.8891, train_loss=4.933676

Batch 34660, train_perplexity=126.38666, train_loss=4.839346

Batch 34670, train_perplexity=133.58478, train_loss=4.8947363

Batch 34680, train_perplexity=135.17296, train_loss=4.906555

Batch 34690, train_perplexity=122.672455, train_loss=4.809518

Batch 34700, train_perplexity=136.20145, train_loss=4.914135

Batch 34710, train_perplexity=141.81131, train_loss=4.9544973

Batch 34720, train_perplexity=132.33296, train_loss=4.885321

Batch 34730, train_perplexity=132.09975, train_loss=4.8835573

Batch 34740, train_perplexity=122.922134, train_loss=4.811551

Batch 34750, train_perplexity=142.51141, train_loss=4.959422

Batch 34760, train_perplexity=133.71759, train_loss=4.89573

Batch 34770, train_perplexity=139.42897, train_loss=4.9375553

Batch 34780, train_perplexity=137.26523, train_loss=4.921915

Batch 34790, train_perplexity=127.12695, train_loss=4.845186

Batch 34800, train_perplexity=143.54219, train_loss=4.966629

Batch 34810, train_perplexity=147.00157, train_loss=4.990443

Batch 34820, train_perplexity=129.66707, train_loss=4.86497

Batch 34830, train_perplexity=135.05821, train_loss=4.905706

Batch 34840, train_perplexity=144.39964, train_loss=4.9725847

Batch 34850, train_perplexity=133.59433, train_loss=4.894808

Batch 34860, train_perplexity=128.55154, train_loss=4.85633

Batch 34870, train_perplexity=128.28383, train_loss=4.854245

Batch 34880, train_perplexity=124.26078, train_loss=4.8223825

Batch 34890, train_perplexity=143.14064, train_loss=4.9638276

Batch 34900, train_perplexity=137.14433, train_loss=4.921034

Batch 34910, train_perplexity=134.24023, train_loss=4.899631

Batch 34920, train_perplexity=142.5622, train_loss=4.9597783

Batch 34930, train_perplexity=139.70955, train_loss=4.9395657

Batch 34940, train_perplexity=145.92104, train_loss=4.9830656

Batch 34950, train_perplexity=142.72754, train_loss=4.9609375

Batch 34960, train_perplexity=133.861, train_loss=4.896802

Batch 34970, train_perplexity=128.88408, train_loss=4.8589134

Batch 34980, train_perplexity=120.69518, train_loss=4.793268
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34990, train_perplexity=141.4512, train_loss=4.951955

Batch 35000, train_perplexity=123.924034, train_loss=4.819669

Batch 35010, train_perplexity=125.04031, train_loss=4.828636

Batch 35020, train_perplexity=129.10269, train_loss=4.860608

Batch 35030, train_perplexity=131.00345, train_loss=4.8752236

Batch 35040, train_perplexity=138.44266, train_loss=4.930456

Batch 35050, train_perplexity=125.959076, train_loss=4.835957

Batch 35060, train_perplexity=130.80739, train_loss=4.873726

Batch 35070, train_perplexity=130.2866, train_loss=4.8697367

Batch 35080, train_perplexity=131.59409, train_loss=4.879722

Batch 35090, train_perplexity=141.85255, train_loss=4.954788

Batch 35100, train_perplexity=121.54863, train_loss=4.8003144

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 35110, train_perplexity=128.31967, train_loss=4.8545246

Batch 35120, train_perplexity=128.3149, train_loss=4.8544874

Batch 35130, train_perplexity=121.74678, train_loss=4.8019433

Batch 35140, train_perplexity=120.22303, train_loss=4.7893486

Batch 35150, train_perplexity=126.597824, train_loss=4.8410153

Batch 35160, train_perplexity=130.26393, train_loss=4.8695626

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 35170, train_perplexity=111.82799, train_loss=4.716962

Batch 35180, train_perplexity=119.09523, train_loss=4.7799234

Batch 35190, train_perplexity=123.97156, train_loss=4.820052

Batch 35200, train_perplexity=127.62622, train_loss=4.849106

Batch 35210, train_perplexity=112.5592, train_loss=4.7234793

Batch 35220, train_perplexity=126.809586, train_loss=4.8426867

Batch 35230, train_perplexity=117.57433, train_loss=4.767071

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 35240, train_perplexity=126.836136, train_loss=4.842896

Batch 35250, train_perplexity=139.73207, train_loss=4.939727

Batch 35260, train_perplexity=132.32664, train_loss=4.8852735

Batch 35270, train_perplexity=142.45964, train_loss=4.959059

Batch 35280, train_perplexity=113.392685, train_loss=4.730857

Batch 35290, train_perplexity=119.76706, train_loss=4.7855487

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 35300, train_perplexity=151.82607, train_loss=5.0227356

Batch 35310, train_perplexity=138.06042, train_loss=4.9276915

Batch 35320, train_perplexity=119.95058, train_loss=4.78708

Batch 35330, train_perplexity=127.335655, train_loss=4.8468266

Batch 35340, train_perplexity=127.80136, train_loss=4.850477

Batch 35350, train_perplexity=128.82675, train_loss=4.8584685

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 35360, train_perplexity=140.47751, train_loss=4.9450474

Batch 35370, train_perplexity=132.6112, train_loss=4.8874216

Batch 35380, train_perplexity=127.24552, train_loss=4.8461185

Batch 35390, train_perplexity=127.89305, train_loss=4.8511944

Batch 35400, train_perplexity=123.55739, train_loss=4.8167057

Batch 35410, train_perplexity=129.17744, train_loss=4.861187

Batch 35420, train_perplexity=117.7505, train_loss=4.768568

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 35430, train_perplexity=143.73143, train_loss=4.9679465

Batch 35440, train_perplexity=129.61514, train_loss=4.8645697

Batch 35450, train_perplexity=117.835205, train_loss=4.769287

Batch 35460, train_perplexity=122.977715, train_loss=4.812003

Batch 35470, train_perplexity=128.07565, train_loss=4.852621

Batch 35480, train_perplexity=117.51492, train_loss=4.7665653

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 35490, train_perplexity=125.62156, train_loss=4.833274

Batch 35500, train_perplexity=122.18197, train_loss=4.8055115

Batch 35510, train_perplexity=123.92043, train_loss=4.8196397

Batch 35520, train_perplexity=121.46387, train_loss=4.799617

Batch 35530, train_perplexity=133.91194, train_loss=4.8971825

Batch 35540, train_perplexity=128.2511, train_loss=4.85399

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 35550, train_perplexity=150.60103, train_loss=5.014634

Batch 35560, train_perplexity=122.942764, train_loss=4.811719

Batch 35570, train_perplexity=134.97278, train_loss=4.905073

Batch 35580, train_perplexity=131.44904, train_loss=4.878619

Batch 35590, train_perplexity=133.70381, train_loss=4.895627

Batch 35600, train_perplexity=129.00417, train_loss=4.8598447

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 35610, train_perplexity=117.44076, train_loss=4.765934

Batch 35620, train_perplexity=123.95973, train_loss=4.819957

Batch 35630, train_perplexity=128.28052, train_loss=4.8542194

Batch 35640, train_perplexity=122.09199, train_loss=4.8047748

Batch 35650, train_perplexity=122.81293, train_loss=4.8106623

Batch 35660, train_perplexity=121.37887, train_loss=4.798917

Batch 35670, train_perplexity=132.7371, train_loss=4.8883705

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 35680, train_perplexity=134.11574, train_loss=4.898703

Batch 35690, train_perplexity=129.58351, train_loss=4.8643255

Batch 35700, train_perplexity=114.58247, train_loss=4.741295

Batch 35710, train_perplexity=137.27812, train_loss=4.922009

Batch 35720, train_perplexity=134.14905, train_loss=4.8989515

Batch 35730, train_perplexity=128.91505, train_loss=4.8591537

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 35740, train_perplexity=129.89642, train_loss=4.8667374

Batch 35750, train_perplexity=150.48222, train_loss=5.013845

Batch 35760, train_perplexity=142.79915, train_loss=4.961439

Batch 35770, train_perplexity=127.657684, train_loss=4.8493524

Batch 35780, train_perplexity=123.764534, train_loss=4.818381

Batch 35790, train_perplexity=125.410164, train_loss=4.8315897

Batch 35800, train_perplexity=133.24265, train_loss=4.892172

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 35810, train_perplexity=132.84183, train_loss=4.889159

Batch 35820, train_perplexity=128.51906, train_loss=4.856077

Batch 35830, train_perplexity=146.44096, train_loss=4.9866223

Batch 35840, train_perplexity=131.22484, train_loss=4.876912

Batch 35850, train_perplexity=143.80074, train_loss=4.9684286

Batch 35860, train_perplexity=130.05583, train_loss=4.867964

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 35870, train_perplexity=140.29944, train_loss=4.943779

Batch 35880, train_perplexity=126.58123, train_loss=4.840884

Batch 35890, train_perplexity=127.06053, train_loss=4.8446636

Batch 35900, train_perplexity=147.23573, train_loss=4.992035

Batch 35910, train_perplexity=128.29416, train_loss=4.854326

Batch 35920, train_perplexity=125.480156, train_loss=4.8321476

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 35930, train_perplexity=136.62411, train_loss=4.9172335

Batch 35940, train_perplexity=136.45575, train_loss=4.9160004

Batch 35950, train_perplexity=114.9795, train_loss=4.744754

Batch 35960, train_perplexity=133.59714, train_loss=4.894829

Batch 35970, train_perplexity=127.855736, train_loss=4.8509026

Batch 35980, train_perplexity=131.33902, train_loss=4.877782

Batch 35990, train_perplexity=136.05527, train_loss=4.913061

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 36000, train_perplexity=115.78198, train_loss=4.751709

Batch 36010, train_perplexity=131.46245, train_loss=4.878721

Batch 36020, train_perplexity=127.35053, train_loss=4.8469434

Batch 36030, train_perplexity=133.75853, train_loss=4.896036

Batch 36040, train_perplexity=136.08069, train_loss=4.913248

Batch 36050, train_perplexity=114.00652, train_loss=4.7362556

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 36060, train_perplexity=143.31752, train_loss=4.9650626

Batch 36070, train_perplexity=131.75778, train_loss=4.880965

Batch 36080, train_perplexity=135.3666, train_loss=4.9079866

Batch 36090, train_perplexity=147.81819, train_loss=4.995983

Batch 36100, train_perplexity=129.28084, train_loss=4.861987

Batch 36110, train_perplexity=127.02843, train_loss=4.844411

Batch 36120, train_perplexity=134.65869, train_loss=4.9027433

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 36130, train_perplexity=130.46646, train_loss=4.871116

Batch 36140, train_perplexity=124.39832, train_loss=4.8234887

Batch 36150, train_perplexity=144.18008, train_loss=4.971063

Batch 36160, train_perplexity=135.54047, train_loss=4.9092703

Batch 36170, train_perplexity=124.36161, train_loss=4.8231936

Batch 36180, train_perplexity=132.57277, train_loss=4.8871317

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 36190, train_perplexity=125.13861, train_loss=4.829422

Batch 36200, train_perplexity=139.20079, train_loss=4.9359174

Batch 36210, train_perplexity=130.69498, train_loss=4.872866

Batch 36220, train_perplexity=144.5472, train_loss=4.973606

Batch 36230, train_perplexity=121.269646, train_loss=4.7980165

Batch 36240, train_perplexity=136.5276, train_loss=4.916527

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 36250, train_perplexity=125.87784, train_loss=4.835312

Batch 36260, train_perplexity=123.737686, train_loss=4.818164

Batch 36270, train_perplexity=143.61916, train_loss=4.967165

Batch 36280, train_perplexity=124.35562, train_loss=4.8231454

Batch 36290, train_perplexity=125.60131, train_loss=4.8331127

Batch 36300, train_perplexity=116.24944, train_loss=4.7557383

Batch 36310, train_perplexity=120.09182, train_loss=4.7882566

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 36320, train_perplexity=116.107346, train_loss=4.754515

Batch 36330, train_perplexity=121.3901, train_loss=4.7990093

Batch 36340, train_perplexity=133.61102, train_loss=4.8949327

Batch 36350, train_perplexity=131.72894, train_loss=4.8807464

Batch 36360, train_perplexity=127.21215, train_loss=4.845856

Batch 36370, train_perplexity=132.12155, train_loss=4.8837223

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 36380, train_perplexity=130.87177, train_loss=4.874218

Batch 36390, train_perplexity=130.78519, train_loss=4.873556

Batch 36400, train_perplexity=157.56924, train_loss=5.059865

Batch 36410, train_perplexity=129.48653, train_loss=4.863577

Batch 36420, train_perplexity=126.53174, train_loss=4.840493

Batch 36430, train_perplexity=127.63261, train_loss=4.849156

Batch 36440, train_perplexity=133.54427, train_loss=4.894433

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 36450, train_perplexity=137.68202, train_loss=4.924947

Batch 36460, train_perplexity=112.79334, train_loss=4.7255573

Batch 36470, train_perplexity=133.7005, train_loss=4.895602

Batch 36480, train_perplexity=131.05962, train_loss=4.8756523

Batch 36490, train_perplexity=127.17995, train_loss=4.845603

Batch 36500, train_perplexity=126.95661, train_loss=4.8438454

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 36510, train_perplexity=128.24236, train_loss=4.853922

Batch 36520, train_perplexity=140.6945, train_loss=4.946591

Batch 36530, train_perplexity=133.05902, train_loss=4.890793

Batch 36540, train_perplexity=133.65709, train_loss=4.8952775

Batch 36550, train_perplexity=119.1184, train_loss=4.780118

Batch 36560, train_perplexity=118.88076, train_loss=4.778121

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 36570, train_perplexity=131.48514, train_loss=4.878894

Batch 36580, train_perplexity=119.15254, train_loss=4.7804046

Batch 36590, train_perplexity=123.26468, train_loss=4.814334

Batch 36600, train_perplexity=131.02925, train_loss=4.8754206

Batch 36610, train_perplexity=125.16033, train_loss=4.8295956

Batch 36620, train_perplexity=121.91955, train_loss=4.8033614

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 36630, train_perplexity=152.18202, train_loss=5.0250773

Batch 36640, train_perplexity=127.46286, train_loss=4.847825

Batch 36650, train_perplexity=129.728, train_loss=4.86544

Batch 36660, train_perplexity=139.35214, train_loss=4.937004

Batch 36670, train_perplexity=126.036095, train_loss=4.8365684

Batch 36680, train_perplexity=122.33453, train_loss=4.8067594

Batch 36690, train_perplexity=132.75534, train_loss=4.888508

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 36700, train_perplexity=134.88213, train_loss=4.9044013

Batch 36710, train_perplexity=120.24166, train_loss=4.7895036

Batch 36720, train_perplexity=140.60571, train_loss=4.9459596

Batch 36730, train_perplexity=131.39357, train_loss=4.878197

Batch 36740, train_perplexity=123.82964, train_loss=4.818907

Batch 36750, train_perplexity=124.82204, train_loss=4.826889

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 36760, train_perplexity=137.70316, train_loss=4.9251003

Batch 36770, train_perplexity=129.4385, train_loss=4.863206

Batch 36780, train_perplexity=125.07931, train_loss=4.828948

Batch 36790, train_perplexity=135.86609, train_loss=4.9116697

Batch 36800, train_perplexity=111.69604, train_loss=4.715781

Batch 36810, train_perplexity=128.78284, train_loss=4.8581276

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 36820, train_perplexity=139.03593, train_loss=4.9347324

Batch 36830, train_perplexity=113.82433, train_loss=4.7346563

Batch 36840, train_perplexity=124.21025, train_loss=4.8219757

Batch 36850, train_perplexity=132.9005, train_loss=4.8896008

Batch 36860, train_perplexity=133.62836, train_loss=4.8950624

Batch 36870, train_perplexity=137.43277, train_loss=4.923135

Batch 36880, train_perplexity=131.51575, train_loss=4.8791265

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 36890, train_perplexity=131.1694, train_loss=4.8764896

Batch 36900, train_perplexity=122.59217, train_loss=4.808863

Batch 36910, train_perplexity=139.22562, train_loss=4.9360957

Batch 36920, train_perplexity=140.77295, train_loss=4.9471483

Batch 36930, train_perplexity=140.71054, train_loss=4.946705

Batch 36940, train_perplexity=137.77869, train_loss=4.9256487

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 36950, train_perplexity=127.60371, train_loss=4.8489294

Batch 36960, train_perplexity=122.91639, train_loss=4.8115044

Batch 36970, train_perplexity=130.78555, train_loss=4.873559

Batch 36980, train_perplexity=140.50873, train_loss=4.9452696

Batch 36990, train_perplexity=132.81282, train_loss=4.888941

Batch 37000, train_perplexity=125.64372, train_loss=4.8334503

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 37010, train_perplexity=129.35156, train_loss=4.862534

Batch 37020, train_perplexity=138.28484, train_loss=4.9293156

Batch 37030, train_perplexity=120.45411, train_loss=4.791269

Batch 37040, train_perplexity=122.18506, train_loss=4.8055367

Batch 37050, train_perplexity=117.963165, train_loss=4.7703724

Batch 37060, train_perplexity=142.81311, train_loss=4.961537

Batch 37070, train_perplexity=125.51013, train_loss=4.8323865

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 37080, train_perplexity=122.86494, train_loss=4.8110857

Batch 37090, train_perplexity=134.4003, train_loss=4.9008226

Batch 37100, train_perplexity=135.18063, train_loss=4.906612

Batch 37110, train_perplexity=134.71797, train_loss=4.9031835

Batch 37120, train_perplexity=133.51657, train_loss=4.8942256

Batch 37130, train_perplexity=118.97297, train_loss=4.7788963

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 37140, train_perplexity=142.1966, train_loss=4.9572105

Batch 37150, train_perplexity=123.82492, train_loss=4.8188686

Batch 37160, train_perplexity=121.11881, train_loss=4.796772

Batch 37170, train_perplexity=121.88996, train_loss=4.8031187

Batch 37180, train_perplexity=108.1982, train_loss=4.6839647

Batch 37190, train_perplexity=132.1838, train_loss=4.8841934

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 37200, train_perplexity=127.86537, train_loss=4.850978

Batch 37210, train_perplexity=119.58314, train_loss=4.784012

Batch 37220, train_perplexity=118.67087, train_loss=4.776354

Batch 37230, train_perplexity=127.8697, train_loss=4.8510118

Batch 37240, train_perplexity=145.64673, train_loss=4.981184

Batch 37250, train_perplexity=139.86032, train_loss=4.9406443

Batch 37260, train_perplexity=127.4687, train_loss=4.847871

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 37270, train_perplexity=130.67017, train_loss=4.8726764

Batch 37280, train_perplexity=126.607185, train_loss=4.8410892

Batch 37290, train_perplexity=119.5869, train_loss=4.7840433

Batch 37300, train_perplexity=141.43759, train_loss=4.9518585

Batch 37310, train_perplexity=147.6373, train_loss=4.9947586

Batch 37320, train_perplexity=126.51202, train_loss=4.8403373

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 37330, train_perplexity=126.510025, train_loss=4.8403215

Batch 37340, train_perplexity=141.83606, train_loss=4.954672

Batch 37350, train_perplexity=135.37396, train_loss=4.908041

Batch 37360, train_perplexity=128.81932, train_loss=4.858411

Batch 37370, train_perplexity=127.17952, train_loss=4.8455997

Batch 37380, train_perplexity=147.82411, train_loss=4.996023

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 37390, train_perplexity=128.7136, train_loss=4.8575897

Batch 37400, train_perplexity=139.7576, train_loss=4.9399095

Batch 37410, train_perplexity=122.220894, train_loss=4.80583

Batch 37420, train_perplexity=120.49403, train_loss=4.7916

Batch 37430, train_perplexity=130.17761, train_loss=4.8689

Batch 37440, train_perplexity=123.12681, train_loss=4.813215

Batch 37450, train_perplexity=122.77094, train_loss=4.8103204

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 37460, train_perplexity=138.70908, train_loss=4.932379

Batch 37470, train_perplexity=125.4561, train_loss=4.831956

Batch 37480, train_perplexity=134.26219, train_loss=4.8997946

Batch 37490, train_perplexity=131.40002, train_loss=4.8782463

Batch 37500, train_perplexity=118.28224, train_loss=4.7730737

Batch 37510, train_perplexity=147.1327, train_loss=4.991335

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 37520, train_perplexity=118.25173, train_loss=4.7728157

Batch 37530, train_perplexity=136.24405, train_loss=4.914448

Batch 37540, train_perplexity=132.17952, train_loss=4.884161

Batch 37550, train_perplexity=114.1529, train_loss=4.737539

Batch 37560, train_perplexity=133.45235, train_loss=4.8937445

Batch 37570, train_perplexity=126.67663, train_loss=4.8416376

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 37580, train_perplexity=129.66231, train_loss=4.8649335

Batch 37590, train_perplexity=121.52209, train_loss=4.800096

Batch 37600, train_perplexity=121.55762, train_loss=4.8003883

Batch 37610, train_perplexity=138.99908, train_loss=4.9344673

Batch 37620, train_perplexity=119.6768, train_loss=4.784795

Batch 37630, train_perplexity=135.32852, train_loss=4.9077053

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 37640, train_perplexity=127.93385, train_loss=4.8515134

Batch 37650, train_perplexity=122.78646, train_loss=4.8104467

Batch 37660, train_perplexity=115.01059, train_loss=4.745024

Batch 37670, train_perplexity=131.28873, train_loss=4.877399

Batch 37680, train_perplexity=126.239456, train_loss=4.8381805

Batch 37690, train_perplexity=130.17023, train_loss=4.868843

Batch 37700, train_perplexity=129.23912, train_loss=4.8616643

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 37710, train_perplexity=122.12133, train_loss=4.805015

Batch 37720, train_perplexity=128.9362, train_loss=4.859318

Batch 37730, train_perplexity=141.10716, train_loss=4.9495196

Batch 37740, train_perplexity=111.9492, train_loss=4.718045

Batch 37750, train_perplexity=135.74933, train_loss=4.91081

Batch 37760, train_perplexity=138.82712, train_loss=4.9332294

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 37770, train_perplexity=130.90479, train_loss=4.87447

Batch 37780, train_perplexity=137.05711, train_loss=4.9203978

Batch 37790, train_perplexity=128.82098, train_loss=4.8584237

Batch 37800, train_perplexity=114.572205, train_loss=4.741205

Batch 37810, train_perplexity=120.486565, train_loss=4.7915382

Batch 37820, train_perplexity=117.08033, train_loss=4.7628603

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 37830, train_perplexity=119.28574, train_loss=4.781522

Batch 37840, train_perplexity=116.67995, train_loss=4.7594347

Batch 37850, train_perplexity=139.23232, train_loss=4.936144

Batch 37860, train_perplexity=122.12582, train_loss=4.805052

Batch 37870, train_perplexity=123.4615, train_loss=4.8159294

Batch 37880, train_perplexity=119.21296, train_loss=4.7809114

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 37890, train_perplexity=122.96645, train_loss=4.8119116

Batch 37900, train_perplexity=124.52296, train_loss=4.82449

Batch 37910, train_perplexity=132.96313, train_loss=4.890072

Batch 37920, train_perplexity=130.87477, train_loss=4.874241

Batch 37930, train_perplexity=132.34444, train_loss=4.885408

Batch 37940, train_perplexity=119.59089, train_loss=4.7840767

Batch 37950, train_perplexity=112.44965, train_loss=4.7225056

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 37960, train_perplexity=148.69423, train_loss=5.001892

Batch 37970, train_perplexity=135.44066, train_loss=4.9085336

Batch 37980, train_perplexity=136.77704, train_loss=4.918352

Batch 37990, train_perplexity=132.69623, train_loss=4.8880625

Batch 38000, train_perplexity=134.51237, train_loss=4.901656

Batch 38010, train_perplexity=125.63803, train_loss=4.833405

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 38020, train_perplexity=117.24961, train_loss=4.764305

Batch 38030, train_perplexity=115.2893, train_loss=4.7474446

Batch 38040, train_perplexity=127.54829, train_loss=4.848495

Batch 38050, train_perplexity=132.85425, train_loss=4.8892527

Batch 38060, train_perplexity=126.986824, train_loss=4.8440833

Batch 38070, train_perplexity=126.359665, train_loss=4.8391323

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 38080, train_perplexity=130.84032, train_loss=4.8739777

Batch 38090, train_perplexity=130.54985, train_loss=4.871755

Batch 38100, train_perplexity=120.36976, train_loss=4.7905684

Batch 38110, train_perplexity=128.04823, train_loss=4.852407

Batch 38120, train_perplexity=111.52456, train_loss=4.714245

Batch 38130, train_perplexity=137.50264, train_loss=4.923643

Batch 38140, train_perplexity=125.586945, train_loss=4.8329983

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 38150, train_perplexity=111.52424, train_loss=4.714242

Batch 38160, train_perplexity=123.08824, train_loss=4.8129015

Batch 38170, train_perplexity=146.6029, train_loss=4.9877276

Batch 38180, train_perplexity=125.485535, train_loss=4.8321905

Batch 38190, train_perplexity=122.057236, train_loss=4.80449

Batch 38200, train_perplexity=139.57571, train_loss=4.938607

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Batch 38210, train_perplexity=137.28415, train_loss=4.922053

Batch 38220, train_perplexity=118.37038, train_loss=4.7738185

Batch 38230, train_perplexity=122.94359, train_loss=4.8117256

Batch 38240, train_perplexity=143.0962, train_loss=4.963517

Batch 38250, train_perplexity=127.322296, train_loss=4.8467216

Batch 38260, train_perplexity=130.24008, train_loss=4.8693795

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 38270, train_perplexity=118.44677, train_loss=4.7744637

Batch 38280, train_perplexity=119.2734, train_loss=4.7814183

Batch 38290, train_perplexity=115.98677, train_loss=4.753476

Batch 38300, train_perplexity=135.57991, train_loss=4.909561

Batch 38310, train_perplexity=120.15323, train_loss=4.788768

Batch 38320, train_perplexity=127.19741, train_loss=4.8457403

Batch 38330, train_perplexity=114.06655, train_loss=4.736782

Batch 38340, train_perplexity=121.51936, train_loss=4.8000736

Batch 38350, train_perplexity=111.56068, train_loss=4.7145686

Batch 38360, train_perplexity=118.81298, train_loss=4.7775507

Batch 38370, train_perplexity=132.50174, train_loss=4.8865957

Batch 38380, train_perplexity=139.2418, train_loss=4.936212

Batch 38390, train_perplexity=107.291916, train_loss=4.6755533

Batch 38400, train_perplexity=122.195366, train_loss=4.805621

Batch 38410, train_perplexity=129.80565, train_loss=4.8660383

Batch 38420, train_perplexity=124.67333, train_loss=4.825697

Batch 38430, train_perplexity=123.21908, train_loss=4.813964

Batch 38440, train_perplexity=109.14481, train_loss=4.6926756

Batch 38450, train_perplexity=118.38901, train_loss=4.773976

Batch 38460, train_perplexity=110.902405, train_loss=4.7086506

Batch 38470, train_perplexity=125.24511, train_loss=4.8302727

Batch 38480, train_perplexity=109.529785, train_loss=4.6961966

Batch 38490, train_perplexity=117.12846, train_loss=4.7632713

Batch 38500, train_perplexity=123.709015, train_loss=4.817932

Batch 38510, train_perplexity=129.14615, train_loss=4.8609447

Batch 38520, train_perplexity=111.63768, train_loss=4.7152586

Batch 38530, train_perplexity=121.009766, train_loss=4.7958713

Batch 38540, train_perplexity=111.14477, train_loss=4.7108335

Batch 38550, train_perplexity=131.21832, train_loss=4.8768625

Batch 38560, train_perplexity=130.19258, train_loss=4.8690147

Batch 38570, train_perplexity=115.30552, train_loss=4.7475853

Batch 38580, train_perplexity=127.54044, train_loss=4.8484335

Batch 38590, train_perplexity=109.17334, train_loss=4.692937

Batch 38600, train_perplexity=128.94168, train_loss=4.85936

Batch 38610, train_perplexity=129.79303, train_loss=4.865941

Batch 38620, train_perplexity=126.28847, train_loss=4.8385687

Batch 38630, train_perplexity=113.61199, train_loss=4.732789

Batch 38640, train_perplexity=122.78587, train_loss=4.810442

Batch 38650, train_perplexity=123.955185, train_loss=4.81992

Batch 38660, train_perplexity=136.13528, train_loss=4.913649

Batch 38670, train_perplexity=118.62646, train_loss=4.7759795

Batch 38680, train_perplexity=135.3353, train_loss=4.9077554

Batch 38690, train_perplexity=120.87269, train_loss=4.794738

Batch 38700, train_perplexity=115.616806, train_loss=4.7502813

Batch 38710, train_perplexity=116.574394, train_loss=4.7585297

Batch 38720, train_perplexity=131.38612, train_loss=4.8781404

Batch 38730, train_perplexity=131.6352, train_loss=4.8800344

Batch 38740, train_perplexity=128.27747, train_loss=4.8541956

Batch 38750, train_perplexity=128.89613, train_loss=4.859007

Batch 38760, train_perplexity=123.5006, train_loss=4.816246

Batch 38770, train_perplexity=119.57972, train_loss=4.783983

Batch 38780, train_perplexity=126.00623, train_loss=4.8363314

Batch 38790, train_perplexity=132.10643, train_loss=4.883608

Batch 38800, train_perplexity=114.64396, train_loss=4.7418313

Batch 38810, train_perplexity=150.07593, train_loss=5.0111413

Batch 38820, train_perplexity=132.85722, train_loss=4.889275

Batch 38830, train_perplexity=123.64656, train_loss=4.817427

Batch 38840, train_perplexity=116.96332, train_loss=4.7618604

Batch 38850, train_perplexity=118.78681, train_loss=4.7773304

Batch 38860, train_perplexity=127.00638, train_loss=4.8442373

Batch 38870, train_perplexity=116.71333, train_loss=4.759721

Batch 38880, train_perplexity=120.64294, train_loss=4.792835

Batch 38890, train_perplexity=129.18687, train_loss=4.86126

Batch 38900, train_perplexity=126.858215, train_loss=4.84307

Batch 38910, train_perplexity=123.06629, train_loss=4.812723

Batch 38920, train_perplexity=128.47812, train_loss=4.8557587

Batch 38930, train_perplexity=113.70564, train_loss=4.733613

Batch 38940, train_perplexity=132.08464, train_loss=4.883443

Batch 38950, train_perplexity=124.42704, train_loss=4.8237195

Batch 38960, train_perplexity=121.62829, train_loss=4.8009696

Batch 38970, train_perplexity=127.20803, train_loss=4.845824

Batch 38980, train_perplexity=125.16081, train_loss=4.8295994

Batch 38990, train_perplexity=127.433266, train_loss=4.847593

Batch 39000, train_perplexity=130.49863, train_loss=4.8713627

Batch 39010, train_perplexity=125.91361, train_loss=4.835596

Batch 39020, train_perplexity=123.214554, train_loss=4.813927

Batch 39030, train_perplexity=126.991844, train_loss=4.844123

Batch 39040, train_perplexity=122.15121, train_loss=4.8052597

Batch 39050, train_perplexity=123.79115, train_loss=4.818596

Batch 39060, train_perplexity=122.03489, train_loss=4.804307

Batch 39070, train_perplexity=131.04375, train_loss=4.875531

Batch 39080, train_perplexity=117.485115, train_loss=4.7663116

Batch 39090, train_perplexity=108.257034, train_loss=4.6845083

Batch 39100, train_perplexity=129.9823, train_loss=4.8673983

Batch 39110, train_perplexity=113.425026, train_loss=4.731142

Batch 39120, train_perplexity=130.88231, train_loss=4.8742986

Batch 39130, train_perplexity=114.563354, train_loss=4.741128

Batch 39140, train_perplexity=119.51496, train_loss=4.7834415

Batch 39150, train_perplexity=123.442665, train_loss=4.815777

Batch 39160, train_perplexity=126.3769, train_loss=4.8392687

Batch 39170, train_perplexity=125.54635, train_loss=4.832675

Batch 39180, train_perplexity=121.4729, train_loss=4.799691

Batch 39190, train_perplexity=126.93881, train_loss=4.843705

Batch 39200, train_perplexity=120.43832, train_loss=4.7911377

Batch 39210, train_perplexity=127.956856, train_loss=4.851693

Batch 39220, train_perplexity=115.19165, train_loss=4.7465973

Batch 39230, train_perplexity=130.1752, train_loss=4.868881

Batch 39240, train_perplexity=122.62462, train_loss=4.809128

Batch 39250, train_perplexity=115.99424, train_loss=4.7535405

Batch 39260, train_perplexity=130.6721, train_loss=4.872691

Batch 39270, train_perplexity=119.03874, train_loss=4.779449

Batch 39280, train_perplexity=114.88962, train_loss=4.743972

Batch 39290, train_perplexity=115.62403, train_loss=4.750344

Batch 39300, train_perplexity=124.247925, train_loss=4.822279

Batch 39310, train_perplexity=128.74226, train_loss=4.8578124

Batch 39320, train_perplexity=126.307434, train_loss=4.838719

Batch 39330, train_perplexity=116.804695, train_loss=4.7605033

Batch 39340, train_perplexity=130.60046, train_loss=4.872143

Batch 39350, train_perplexity=122.81462, train_loss=4.810676

Batch 39360, train_perplexity=137.91646, train_loss=4.926648

Batch 39370, train_perplexity=128.0472, train_loss=4.852399

Batch 39380, train_perplexity=133.4164, train_loss=4.893475

Batch 39390, train_perplexity=117.61437, train_loss=4.767411

Batch 39400, train_perplexity=133.89215, train_loss=4.8970346

Batch 39410, train_perplexity=139.8878, train_loss=4.9408407

Batch 39420, train_perplexity=114.25904, train_loss=4.738468

Batch 39430, train_perplexity=133.95703, train_loss=4.897519

Batch 39440, train_perplexity=123.29854, train_loss=4.8146086

Batch 39450, train_perplexity=137.92119, train_loss=4.9266825
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 39460, train_perplexity=133.76982, train_loss=4.8961205

Batch 39470, train_perplexity=125.149826, train_loss=4.8295116

Batch 39480, train_perplexity=136.14534, train_loss=4.913723

Batch 39490, train_perplexity=121.81234, train_loss=4.8024817

Batch 39500, train_perplexity=127.419716, train_loss=4.8474865

Batch 39510, train_perplexity=126.38871, train_loss=4.839362

Batch 39520, train_perplexity=126.390274, train_loss=4.8393745

Batch 39530, train_perplexity=125.894226, train_loss=4.835442

Batch 39540, train_perplexity=123.11319, train_loss=4.813104

Batch 39550, train_perplexity=110.821846, train_loss=4.707924

Batch 39560, train_perplexity=123.528404, train_loss=4.816471

Batch 39570, train_perplexity=126.632904, train_loss=4.8412924

Batch 39580, train_perplexity=124.660965, train_loss=4.825598

Batch 39590, train_perplexity=133.4022, train_loss=4.8933687

Batch 39600, train_perplexity=141.87582, train_loss=4.9549522

Batch 39610, train_perplexity=136.52266, train_loss=4.9164906

Batch 39620, train_perplexity=126.647514, train_loss=4.841408

Batch 39630, train_perplexity=123.87813, train_loss=4.8192983

Batch 39640, train_perplexity=111.650986, train_loss=4.715378

Batch 39650, train_perplexity=129.07579, train_loss=4.8603997

Batch 39660, train_perplexity=130.8422, train_loss=4.873992

Batch 39670, train_perplexity=136.2651, train_loss=4.9146023

Batch 39680, train_perplexity=128.37537, train_loss=4.8549585

Batch 39690, train_perplexity=119.10352, train_loss=4.779993

Batch 39700, train_perplexity=128.5437, train_loss=4.856269

Batch 39710, train_perplexity=110.48247, train_loss=4.704857

Batch 39720, train_perplexity=106.30124, train_loss=4.666277

Batch 39730, train_perplexity=130.7214, train_loss=4.8730683

Batch 39740, train_perplexity=124.54029, train_loss=4.8246293

Batch 39750, train_perplexity=117.6822, train_loss=4.7679877

Batch 39760, train_perplexity=139.00359, train_loss=4.9344997

Batch 39770, train_perplexity=120.24539, train_loss=4.7895346

Batch 39780, train_perplexity=124.81353, train_loss=4.826821

Batch 39790, train_perplexity=129.05357, train_loss=4.8602276

Batch 39800, train_perplexity=121.68253, train_loss=4.8014154

Batch 39810, train_perplexity=122.31359, train_loss=4.806588

Batch 39820, train_perplexity=125.20223, train_loss=4.8299303

Batch 39830, train_perplexity=124.00975, train_loss=4.82036

Batch 39840, train_perplexity=118.264366, train_loss=4.7729225

Batch 39850, train_perplexity=122.06748, train_loss=4.804574

Batch 39860, train_perplexity=122.784706, train_loss=4.8104324

Batch 39870, train_perplexity=132.23991, train_loss=4.884618

Batch 39880, train_perplexity=103.70072, train_loss=4.641509

Batch 39890, train_perplexity=133.14243, train_loss=4.8914194

Batch 39900, train_perplexity=127.889275, train_loss=4.851165

Batch 39910, train_perplexity=120.874535, train_loss=4.794753

Batch 39920, train_perplexity=123.737625, train_loss=4.8181634

Batch 39930, train_perplexity=128.55424, train_loss=4.856351

Batch 39940, train_perplexity=124.35889, train_loss=4.8231716

Batch 39950, train_perplexity=121.85504, train_loss=4.802832

Batch 39960, train_perplexity=138.15816, train_loss=4.928399

Batch 39970, train_perplexity=124.8399, train_loss=4.827032

Batch 39980, train_perplexity=124.58092, train_loss=4.8249555

Batch 39990, train_perplexity=130.67036, train_loss=4.872678

Batch 40000, train_perplexity=147.11531, train_loss=4.9912167

Batch 40010, train_perplexity=117.0604, train_loss=4.76269

Batch 40020, train_perplexity=122.37846, train_loss=4.8071184

Batch 40030, train_perplexity=113.67604, train_loss=4.7333527

Batch 40040, train_perplexity=122.732895, train_loss=4.8100104

Batch 40050, train_perplexity=129.3145, train_loss=4.8622475

Batch 40060, train_perplexity=125.84867, train_loss=4.83508

Batch 40070, train_perplexity=127.322235, train_loss=4.846721

Batch 40080, train_perplexity=117.2182, train_loss=4.764037

Batch 40090, train_perplexity=120.72097, train_loss=4.793482

Batch 40100, train_perplexity=137.1964, train_loss=4.9214134

Batch 40110, train_perplexity=116.570724, train_loss=4.758498

Batch 40120, train_perplexity=134.02962, train_loss=4.898061

Batch 40130, train_perplexity=137.05437, train_loss=4.9203777

Batch 40140, train_perplexity=116.98736, train_loss=4.762066

Batch 40150, train_perplexity=145.79001, train_loss=4.9821672

Batch 40160, train_perplexity=125.371605, train_loss=4.831282

Batch 40170, train_perplexity=128.52727, train_loss=4.856141

Batch 40180, train_perplexity=123.75993, train_loss=4.8183436

Batch 40190, train_perplexity=133.44554, train_loss=4.8936934

Batch 40200, train_perplexity=127.33675, train_loss=4.846835

Batch 40210, train_perplexity=127.5419, train_loss=4.848445

Batch 40220, train_perplexity=119.41219, train_loss=4.7825813

Batch 40230, train_perplexity=119.99314, train_loss=4.7874346

Batch 40240, train_perplexity=127.73703, train_loss=4.8499737

Batch 40250, train_perplexity=144.76862, train_loss=4.9751368

Batch 40260, train_perplexity=129.89519, train_loss=4.866728

Batch 40270, train_perplexity=136.7117, train_loss=4.9178743

Batch 40280, train_perplexity=124.456764, train_loss=4.8239584

Batch 40290, train_perplexity=130.9879, train_loss=4.875105

Batch 40300, train_perplexity=117.87803, train_loss=4.7696505

Batch 40310, train_perplexity=133.97523, train_loss=4.897655

Batch 40320, train_perplexity=122.06131, train_loss=4.8045235

Batch 40330, train_perplexity=122.79442, train_loss=4.8105116

Batch 40340, train_perplexity=118.1647, train_loss=4.7720795

Batch 40350, train_perplexity=121.46045, train_loss=4.7995887

Batch 40360, train_perplexity=127.72369, train_loss=4.8498693

Batch 40370, train_perplexity=117.53196, train_loss=4.7667103

Batch 40380, train_perplexity=118.21712, train_loss=4.772523

Batch 40390, train_perplexity=114.867874, train_loss=4.7437825

Batch 40400, train_perplexity=117.81723, train_loss=4.7691345

Batch 40410, train_perplexity=136.56575, train_loss=4.916806

Batch 40420, train_perplexity=123.78318, train_loss=4.8185315

Batch 40430, train_perplexity=133.52217, train_loss=4.8942676

Batch 40440, train_perplexity=129.05905, train_loss=4.86027

Batch 40450, train_perplexity=126.23548, train_loss=4.838149

Batch 40460, train_perplexity=112.207016, train_loss=4.7203455

Batch 40470, train_perplexity=120.0711, train_loss=4.788084

Batch 40480, train_perplexity=114.06916, train_loss=4.736805

Batch 40490, train_perplexity=124.945724, train_loss=4.8278794

Batch 40500, train_perplexity=120.90278, train_loss=4.7949867

Batch 40510, train_perplexity=127.16176, train_loss=4.84546

Batch 40520, train_perplexity=138.57916, train_loss=4.931442

Batch 40530, train_perplexity=139.57166, train_loss=4.938578

Batch 40540, train_perplexity=126.89185, train_loss=4.843335

Batch 40550, train_perplexity=121.78684, train_loss=4.8022723

Batch 40560, train_perplexity=121.79305, train_loss=4.8023233

Batch 40570, train_perplexity=120.03801, train_loss=4.7878084

Batch 40580, train_perplexity=115.5909, train_loss=4.750057

Batch 40590, train_perplexity=130.34949, train_loss=4.870219

Batch 40600, train_perplexity=124.98714, train_loss=4.828211

Batch 40610, train_perplexity=122.28035, train_loss=4.8063164

Batch 40620, train_perplexity=111.980385, train_loss=4.7183237

Batch 40630, train_perplexity=125.54581, train_loss=4.8326707

Batch 40640, train_perplexity=127.70426, train_loss=4.849717

Batch 40650, train_perplexity=145.54842, train_loss=4.980509

Batch 40660, train_perplexity=138.66821, train_loss=4.932084

Batch 40670, train_perplexity=146.49138, train_loss=4.9869666

Batch 40680, train_perplexity=123.153114, train_loss=4.8134284

Batch 40690, train_perplexity=118.74411, train_loss=4.776971

Batch 40700, train_perplexity=132.78198, train_loss=4.8887086

Batch 40710, train_perplexity=129.90665, train_loss=4.866816

Batch 40720, train_perplexity=117.72704, train_loss=4.7683687

Batch 40730, train_perplexity=127.24655, train_loss=4.8461266

Batch 40740, train_perplexity=130.16776, train_loss=4.868824

Batch 40750, train_perplexity=132.13081, train_loss=4.8837924

Batch 40760, train_perplexity=123.07562, train_loss=4.812799
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40770, train_perplexity=115.9803, train_loss=4.7534204

Batch 40780, train_perplexity=119.21125, train_loss=4.780897

Batch 40790, train_perplexity=143.63559, train_loss=4.9672794

Batch 40800, train_perplexity=129.08754, train_loss=4.860491

Batch 40810, train_perplexity=120.45359, train_loss=4.7912645

Batch 40820, train_perplexity=149.07661, train_loss=5.0044603

Batch 40830, train_perplexity=123.905304, train_loss=4.8195176

Batch 40840, train_perplexity=126.63218, train_loss=4.8412867

Batch 40850, train_perplexity=124.666435, train_loss=4.8256416

Batch 40860, train_perplexity=122.48413, train_loss=4.8079815

Batch 40870, train_perplexity=125.41848, train_loss=4.831656

Batch 40880, train_perplexity=113.65214, train_loss=4.7331424

Batch 40890, train_perplexity=116.52788, train_loss=4.7581306

Batch 40900, train_perplexity=120.390884, train_loss=4.790744

Batch 40910, train_perplexity=112.655205, train_loss=4.724332

Batch 40920, train_perplexity=127.88244, train_loss=4.8511114

Batch 40930, train_perplexity=129.847, train_loss=4.866357

Batch 40940, train_perplexity=128.89447, train_loss=4.858994

Batch 40950, train_perplexity=125.0978, train_loss=4.829096

Batch 40960, train_perplexity=118.42745, train_loss=4.7743006

Batch 40970, train_perplexity=124.9515, train_loss=4.8279257

Batch 40980, train_perplexity=128.88138, train_loss=4.8588924

Batch 40990, train_perplexity=116.82826, train_loss=4.760705

Batch 41000, train_perplexity=120.90514, train_loss=4.7950063

Batch 41010, train_perplexity=134.67038, train_loss=4.90283

Batch 41020, train_perplexity=138.87042, train_loss=4.9335413

Batch 41030, train_perplexity=115.897484, train_loss=4.752706

Batch 41040, train_perplexity=126.983246, train_loss=4.844055

Batch 41050, train_perplexity=138.15347, train_loss=4.928365

Batch 41060, train_perplexity=120.27062, train_loss=4.7897444

Batch 41070, train_perplexity=120.23507, train_loss=4.7894487

Batch 41080, train_perplexity=125.926704, train_loss=4.8357

Batch 41090, train_perplexity=139.6331, train_loss=4.9390182

Batch 41100, train_perplexity=124.0754, train_loss=4.8208895

Batch 41110, train_perplexity=123.84287, train_loss=4.8190136

Batch 41120, train_perplexity=132.69508, train_loss=4.888054

Batch 41130, train_perplexity=132.69685, train_loss=4.8880672

Batch 41140, train_perplexity=124.22311, train_loss=4.822079

Batch 41150, train_perplexity=117.69471, train_loss=4.768094

Batch 41160, train_perplexity=133.3694, train_loss=4.8931227

Batch 41170, train_perplexity=127.33675, train_loss=4.846835

Batch 41180, train_perplexity=147.35036, train_loss=4.992813

Batch 41190, train_perplexity=122.601234, train_loss=4.808937

Batch 41200, train_perplexity=126.20846, train_loss=4.837935

Batch 41210, train_perplexity=114.99748, train_loss=4.7449102

Batch 41220, train_perplexity=134.3151, train_loss=4.9001884

Batch 41230, train_perplexity=128.69984, train_loss=4.857483

Batch 41240, train_perplexity=135.7211, train_loss=4.910602

Batch 41250, train_perplexity=139.83418, train_loss=4.9404573

Batch 41260, train_perplexity=136.04242, train_loss=4.9129667

Batch 41270, train_perplexity=120.46111, train_loss=4.791327

Batch 41280, train_perplexity=118.13124, train_loss=4.771796

Batch 41290, train_perplexity=118.83667, train_loss=4.77775

Batch 41300, train_perplexity=130.62694, train_loss=4.8723454

Batch 41310, train_perplexity=131.33174, train_loss=4.8777266

Batch 41320, train_perplexity=133.56477, train_loss=4.8945866

Batch 41330, train_perplexity=120.50437, train_loss=4.791686

Batch 41340, train_perplexity=124.98082, train_loss=4.8281603

Batch 41350, train_perplexity=130.35527, train_loss=4.8702636

Batch 41360, train_perplexity=119.83761, train_loss=4.7861376

Batch 41370, train_perplexity=119.48607, train_loss=4.7832

Batch 41380, train_perplexity=131.32924, train_loss=4.8777075

Batch 41390, train_perplexity=137.98198, train_loss=4.927123

Batch 41400, train_perplexity=130.83957, train_loss=4.873972

Batch 41410, train_perplexity=128.29979, train_loss=4.8543696

Batch 41420, train_perplexity=129.97765, train_loss=4.8673625

Batch 41430, train_perplexity=117.804306, train_loss=4.769025

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 41440, train_perplexity=120.34692, train_loss=4.7903786

Batch 41450, train_perplexity=110.757706, train_loss=4.707345

Batch 41460, train_perplexity=104.680786, train_loss=4.6509156

Batch 41470, train_perplexity=121.055244, train_loss=4.796247

Batch 41480, train_perplexity=112.07825, train_loss=4.7191973

Batch 41490, train_perplexity=123.56693, train_loss=4.816783

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 41500, train_perplexity=122.22282, train_loss=4.8058457

Batch 41510, train_perplexity=128.91727, train_loss=4.859171

Batch 41520, train_perplexity=131.31885, train_loss=4.8776283

Batch 41530, train_perplexity=122.675964, train_loss=4.8095465

Batch 41540, train_perplexity=113.91887, train_loss=4.7354865

Batch 41550, train_perplexity=129.8366, train_loss=4.8662767

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 41560, train_perplexity=116.785484, train_loss=4.760339

Batch 41570, train_perplexity=116.68251, train_loss=4.7594566

Batch 41580, train_perplexity=122.61433, train_loss=4.809044

Batch 41590, train_perplexity=133.6599, train_loss=4.8952985

Batch 41600, train_perplexity=122.83881, train_loss=4.810873

Batch 41610, train_perplexity=132.0852, train_loss=4.883447

Batch 41620, train_perplexity=112.4969, train_loss=4.7229257

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 41630, train_perplexity=123.9251, train_loss=4.8196774

Batch 41640, train_perplexity=125.18684, train_loss=4.8298073

Batch 41650, train_perplexity=120.84819, train_loss=4.794535

Batch 41660, train_perplexity=134.52725, train_loss=4.901767

Batch 41670, train_perplexity=129.2144, train_loss=4.861473

Batch 41680, train_perplexity=124.91957, train_loss=4.82767

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 41690, train_perplexity=141.76344, train_loss=4.9541597

Batch 41700, train_perplexity=118.844376, train_loss=4.777815

Batch 41710, train_perplexity=117.80678, train_loss=4.769046

Batch 41720, train_perplexity=116.5943, train_loss=4.7587004

Batch 41730, train_perplexity=127.704445, train_loss=4.8497186

Batch 41740, train_perplexity=122.18476, train_loss=4.8055344

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 41750, train_perplexity=128.67763, train_loss=4.8573103

Batch 41760, train_perplexity=125.49313, train_loss=4.832251

Batch 41770, train_perplexity=123.9163, train_loss=4.8196063

Batch 41780, train_perplexity=128.96486, train_loss=4.85954

Batch 41790, train_perplexity=110.13258, train_loss=4.701685

Batch 41800, train_perplexity=119.52659, train_loss=4.783539

Batch 41810, train_perplexity=134.71257, train_loss=4.9031434

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 41820, train_perplexity=114.06193, train_loss=4.7367415

Batch 41830, train_perplexity=121.56921, train_loss=4.8004837

Batch 41840, train_perplexity=117.72895, train_loss=4.768385

Batch 41850, train_perplexity=106.55579, train_loss=4.6686687

Batch 41860, train_perplexity=124.554016, train_loss=4.8247395

Batch 41870, train_perplexity=120.34187, train_loss=4.7903366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 41880, train_perplexity=116.17403, train_loss=4.7550893

Batch 41890, train_perplexity=124.032875, train_loss=4.8205466

Batch 41900, train_perplexity=138.25015, train_loss=4.9290648

Batch 41910, train_perplexity=120.72362, train_loss=4.7935038

Batch 41920, train_perplexity=117.41432, train_loss=4.765709

Batch 41930, train_perplexity=115.242805, train_loss=4.747041

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 41940, train_perplexity=129.81902, train_loss=4.8661413

Batch 41950, train_perplexity=133.65141, train_loss=4.895235

Batch 41960, train_perplexity=125.57892, train_loss=4.8329344

Batch 41970, train_perplexity=111.025055, train_loss=4.709756

Batch 41980, train_perplexity=108.179115, train_loss=4.6837883

Batch 41990, train_perplexity=125.181816, train_loss=4.829767

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 42000, train_perplexity=114.33338, train_loss=4.7391186

Batch 42010, train_perplexity=111.59952, train_loss=4.7149167

Batch 42020, train_perplexity=116.56084, train_loss=4.7584133

Batch 42030, train_perplexity=128.05855, train_loss=4.8524876

Batch 42040, train_perplexity=123.34994, train_loss=4.8150253

Batch 42050, train_perplexity=124.68564, train_loss=4.8257957

Batch 42060, train_perplexity=120.66365, train_loss=4.793007

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 42070, train_perplexity=133.32184, train_loss=4.892766

Batch 42080, train_perplexity=120.482544, train_loss=4.791505

Batch 42090, train_perplexity=122.5948, train_loss=4.8088846

Batch 42100, train_perplexity=116.08083, train_loss=4.754287

Batch 42110, train_perplexity=118.812416, train_loss=4.777546

Batch 42120, train_perplexity=129.41457, train_loss=4.863021

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 42130, train_perplexity=110.6229, train_loss=4.706127

Batch 42140, train_perplexity=110.571594, train_loss=4.705663

Batch 42150, train_perplexity=124.47303, train_loss=4.824089

Batch 42160, train_perplexity=116.648575, train_loss=4.759166

Batch 42170, train_perplexity=121.296364, train_loss=4.798237

Batch 42180, train_perplexity=131.49625, train_loss=4.8789783

Batch 42190, train_perplexity=111.55626, train_loss=4.714529

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 42200, train_perplexity=124.750694, train_loss=4.8263173

Batch 42210, train_perplexity=106.47748, train_loss=4.6679335

Batch 42220, train_perplexity=118.277336, train_loss=4.773032

Batch 42230, train_perplexity=126.431145, train_loss=4.839698

Batch 42240, train_perplexity=109.74937, train_loss=4.6981993

Batch 42250, train_perplexity=116.156685, train_loss=4.75494

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 42260, train_perplexity=121.38703, train_loss=4.798984

Batch 42270, train_perplexity=109.96802, train_loss=4.7001896

Batch 42280, train_perplexity=103.7575, train_loss=4.6420565

Batch 42290, train_perplexity=115.34951, train_loss=4.747967

Batch 42300, train_perplexity=128.30241, train_loss=4.85439

Batch 42310, train_perplexity=110.56131, train_loss=4.70557

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 42320, train_perplexity=132.67952, train_loss=4.8879366

Batch 42330, train_perplexity=105.92281, train_loss=4.6627107

Batch 42340, train_perplexity=124.9151, train_loss=4.8276343

Batch 42350, train_perplexity=126.46889, train_loss=4.8399963

Batch 42360, train_perplexity=133.56331, train_loss=4.8945756

Batch 42370, train_perplexity=120.25996, train_loss=4.7896557

Batch 42380, train_perplexity=107.62313, train_loss=4.6786356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 42390, train_perplexity=102.675026, train_loss=4.631569

Batch 42400, train_perplexity=131.62679, train_loss=4.8799706

Batch 42410, train_perplexity=143.6473, train_loss=4.967361

Batch 42420, train_perplexity=123.65711, train_loss=4.8175125

Batch 42430, train_perplexity=118.60117, train_loss=4.7757664

Batch 42440, train_perplexity=130.80862, train_loss=4.8737354

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 42450, train_perplexity=114.28258, train_loss=4.738674

Batch 42460, train_perplexity=128.25362, train_loss=4.8540096

Batch 42470, train_perplexity=127.210815, train_loss=4.8458457

Batch 42480, train_perplexity=128.11247, train_loss=4.8529086

Batch 42490, train_perplexity=127.30482, train_loss=4.8465843

Batch 42500, train_perplexity=124.51969, train_loss=4.824464

Batch 42510, train_perplexity=117.235306, train_loss=4.764183

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 42520, train_perplexity=136.9876, train_loss=4.9198904

Batch 42530, train_perplexity=119.02409, train_loss=4.779326

Batch 42540, train_perplexity=110.47746, train_loss=4.7048116

Batch 42550, train_perplexity=111.92145, train_loss=4.7177973

Batch 42560, train_perplexity=125.34608, train_loss=4.8310785

Batch 42570, train_perplexity=119.003494, train_loss=4.779153

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 42580, train_perplexity=133.2828, train_loss=4.892473

Batch 42590, train_perplexity=137.96461, train_loss=4.926997

Batch 42600, train_perplexity=128.69371, train_loss=4.857435

Batch 42610, train_perplexity=118.204605, train_loss=4.772417

Batch 42620, train_perplexity=104.10044, train_loss=4.645356

Batch 42630, train_perplexity=114.58225, train_loss=4.741293

Batch 42640, train_perplexity=112.43539, train_loss=4.7223787

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 42650, train_perplexity=114.015434, train_loss=4.736334

Batch 42660, train_perplexity=117.388016, train_loss=4.765485

Batch 42670, train_perplexity=132.10756, train_loss=4.8836164

Batch 42680, train_perplexity=117.59385, train_loss=4.7672367

Batch 42690, train_perplexity=119.10375, train_loss=4.779995

Batch 42700, train_perplexity=120.86727, train_loss=4.794693

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 42710, train_perplexity=122.1882, train_loss=4.8055625

Batch 42720, train_perplexity=113.3636, train_loss=4.7306004

Batch 42730, train_perplexity=130.50323, train_loss=4.871398

Batch 42740, train_perplexity=121.82692, train_loss=4.8026013

Batch 42750, train_perplexity=111.93127, train_loss=4.717885

Batch 42760, train_perplexity=119.07774, train_loss=4.7797766

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 42770, train_perplexity=120.5588, train_loss=4.7921376

Batch 42780, train_perplexity=102.85584, train_loss=4.6333284

Batch 42790, train_perplexity=106.29982, train_loss=4.6662636

Batch 42800, train_perplexity=120.79076, train_loss=4.7940598

Batch 42810, train_perplexity=111.402954, train_loss=4.713154

Batch 42820, train_perplexity=130.21394, train_loss=4.869179

Batch 42830, train_perplexity=136.67715, train_loss=4.9176216

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 42840, train_perplexity=133.75458, train_loss=4.8960066

Batch 42850, train_perplexity=123.97865, train_loss=4.8201094

Batch 42860, train_perplexity=131.12157, train_loss=4.876125

Batch 42870, train_perplexity=119.66505, train_loss=4.7846966

Batch 42880, train_perplexity=112.963104, train_loss=4.7270613

Batch 42890, train_perplexity=126.277565, train_loss=4.8384824

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 42900, train_perplexity=127.26282, train_loss=4.8462543

Batch 42910, train_perplexity=106.343925, train_loss=4.6666784

Batch 42920, train_perplexity=117.055885, train_loss=4.7626514

Batch 42930, train_perplexity=131.75677, train_loss=4.8809576

Batch 42940, train_perplexity=114.71707, train_loss=4.742469

Batch 42950, train_perplexity=118.56566, train_loss=4.775467

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 42960, train_perplexity=124.57403, train_loss=4.8249

Batch 42970, train_perplexity=124.60819, train_loss=4.8251743

Batch 42980, train_perplexity=101.44864, train_loss=4.6195526

Batch 42990, train_perplexity=125.09935, train_loss=4.829108

Batch 43000, train_perplexity=136.08304, train_loss=4.913265

Batch 43010, train_perplexity=121.68056, train_loss=4.801399

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 43020, train_perplexity=133.72198, train_loss=4.895763

Batch 43030, train_perplexity=123.99473, train_loss=4.820239

Batch 43040, train_perplexity=119.17454, train_loss=4.780589

Batch 43050, train_perplexity=128.52824, train_loss=4.8561487

Batch 43060, train_perplexity=128.71469, train_loss=4.8575983

Batch 43070, train_perplexity=129.35823, train_loss=4.8625855

Batch 43080, train_perplexity=120.46238, train_loss=4.7913375

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 43090, train_perplexity=104.86524, train_loss=4.652676

Batch 43100, train_perplexity=112.62244, train_loss=4.724041

Batch 43110, train_perplexity=120.760864, train_loss=4.7938123

Batch 43120, train_perplexity=130.5097, train_loss=4.8714476

Batch 43130, train_perplexity=120.82209, train_loss=4.794319

Batch 43140, train_perplexity=124.8761, train_loss=4.827322

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 43150, train_perplexity=123.94809, train_loss=4.819863

Batch 43160, train_perplexity=126.59137, train_loss=4.8409643

Batch 43170, train_perplexity=134.39369, train_loss=4.9007735

Batch 43180, train_perplexity=124.803055, train_loss=4.826737

Batch 43190, train_perplexity=118.50818, train_loss=4.774982

Batch 43200, train_perplexity=119.645134, train_loss=4.78453

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 43210, train_perplexity=139.80992, train_loss=4.940284

Batch 43220, train_perplexity=120.26093, train_loss=4.789664

Batch 43230, train_perplexity=127.04285, train_loss=4.8445244

Batch 43240, train_perplexity=106.73815, train_loss=4.6703787

Batch 43250, train_perplexity=110.02089, train_loss=4.7006702

Batch 43260, train_perplexity=119.925186, train_loss=4.786868

Batch 43270, train_perplexity=127.53588, train_loss=4.8483977

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 43280, train_perplexity=102.252594, train_loss=4.627446

Batch 43290, train_perplexity=112.76678, train_loss=4.725322

Batch 43300, train_perplexity=116.939285, train_loss=4.761655

Batch 43310, train_perplexity=130.54045, train_loss=4.871683

Batch 43320, train_perplexity=112.48038, train_loss=4.722779

Batch 43330, train_perplexity=117.49733, train_loss=4.7664156

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 43340, train_perplexity=118.63624, train_loss=4.776062

Batch 43350, train_perplexity=131.14476, train_loss=4.876302

Batch 43360, train_perplexity=114.4548, train_loss=4.74018

Batch 43370, train_perplexity=121.920364, train_loss=4.803368

Batch 43380, train_perplexity=126.71058, train_loss=4.8419056

Batch 43390, train_perplexity=125.42021, train_loss=4.83167

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 43400, train_perplexity=119.7115, train_loss=4.7850847

Batch 43410, train_perplexity=132.34085, train_loss=4.8853807

Batch 43420, train_perplexity=126.277504, train_loss=4.838482

Batch 43430, train_perplexity=114.99523, train_loss=4.7448907

Batch 43440, train_perplexity=112.56059, train_loss=4.7234917

Batch 43450, train_perplexity=115.34478, train_loss=4.7479258

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 43460, train_perplexity=117.54536, train_loss=4.7668242

Batch 43470, train_perplexity=110.23609, train_loss=4.7026243

Batch 43480, train_perplexity=124.55829, train_loss=4.824774

Batch 43490, train_perplexity=136.63689, train_loss=4.917327

Batch 43500, train_perplexity=127.37889, train_loss=4.847166

Batch 43510, train_perplexity=116.78481, train_loss=4.760333

Batch 43520, train_perplexity=121.29, train_loss=4.7981844

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 43530, train_perplexity=126.72436, train_loss=4.8420143

Batch 43540, train_perplexity=118.04374, train_loss=4.771055

Batch 43550, train_perplexity=125.20749, train_loss=4.8299723

Batch 43560, train_perplexity=123.72046, train_loss=4.8180246

Batch 43570, train_perplexity=103.15595, train_loss=4.636242

Batch 43580, train_perplexity=117.99433, train_loss=4.7706366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 43590, train_perplexity=112.9908, train_loss=4.7273064

Batch 43600, train_perplexity=122.40338, train_loss=4.807322

Batch 43610, train_perplexity=130.81786, train_loss=4.873806

Batch 43620, train_perplexity=130.3306, train_loss=4.8700743

Batch 43630, train_perplexity=134.51686, train_loss=4.9016895

Batch 43640, train_perplexity=135.31316, train_loss=4.907592

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 43650, train_perplexity=114.33905, train_loss=4.739168

Batch 43660, train_perplexity=131.9605, train_loss=4.8825026

Batch 43670, train_perplexity=112.73936, train_loss=4.7250786

Batch 43680, train_perplexity=109.46734, train_loss=4.6956263

Batch 43690, train_perplexity=123.28584, train_loss=4.8145056

Batch 43700, train_perplexity=123.98829, train_loss=4.820187

Batch 43710, train_perplexity=117.030655, train_loss=4.762436

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 43720, train_perplexity=117.57142, train_loss=4.767046

Batch 43730, train_perplexity=130.99065, train_loss=4.875126

Batch 43740, train_perplexity=124.59061, train_loss=4.825033

Batch 43750, train_perplexity=113.543915, train_loss=4.7321897

Batch 43760, train_perplexity=110.12912, train_loss=4.7016535

Batch 43770, train_perplexity=119.87927, train_loss=4.786485

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 43780, train_perplexity=111.86414, train_loss=4.717285

Batch 43790, train_perplexity=122.95162, train_loss=4.811791

Batch 43800, train_perplexity=116.38311, train_loss=4.7568874

Batch 43810, train_perplexity=112.76909, train_loss=4.7253423

Batch 43820, train_perplexity=109.21718, train_loss=4.6933384

Batch 43830, train_perplexity=118.87135, train_loss=4.778042

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 43840, train_perplexity=119.70991, train_loss=4.7850714

Batch 43850, train_perplexity=114.50989, train_loss=4.740661

Batch 43860, train_perplexity=114.176476, train_loss=4.7377453

Batch 43870, train_perplexity=114.65183, train_loss=4.7419

Batch 43880, train_perplexity=126.66008, train_loss=4.841507

Batch 43890, train_perplexity=114.4851, train_loss=4.7404447

Batch 43900, train_perplexity=129.0852, train_loss=4.8604727

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 43910, train_perplexity=123.04663, train_loss=4.8125634

Batch 43920, train_perplexity=128.14174, train_loss=4.853137

Batch 43930, train_perplexity=117.44596, train_loss=4.7659783

Batch 43940, train_perplexity=105.83325, train_loss=4.6618648

Batch 43950, train_perplexity=126.789696, train_loss=4.84253

Batch 43960, train_perplexity=128.04707, train_loss=4.852398

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 43970, train_perplexity=118.4008, train_loss=4.7740755

Batch 43980, train_perplexity=125.79863, train_loss=4.8346825

Batch 43990, train_perplexity=129.54076, train_loss=4.8639956

Batch 44000, train_perplexity=124.52183, train_loss=4.824481

Batch 44010, train_perplexity=110.996155, train_loss=4.7094955

Batch 44020, train_perplexity=123.95069, train_loss=4.819884

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 44030, train_perplexity=130.03363, train_loss=4.867793

Batch 44040, train_perplexity=114.338066, train_loss=4.7391596

Batch 44050, train_perplexity=117.6643, train_loss=4.7678356

Batch 44060, train_perplexity=115.50935, train_loss=4.7493515

Batch 44070, train_perplexity=120.19999, train_loss=4.789157

Batch 44080, train_perplexity=130.74028, train_loss=4.873213

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 44090, train_perplexity=124.68712, train_loss=4.8258076

Batch 44100, train_perplexity=125.80031, train_loss=4.834696

Batch 44110, train_perplexity=110.591995, train_loss=4.7058477

Batch 44120, train_perplexity=130.42982, train_loss=4.8708353

Batch 44130, train_perplexity=115.78133, train_loss=4.7517033

Batch 44140, train_perplexity=123.11178, train_loss=4.8130927

Batch 44150, train_perplexity=121.32199, train_loss=4.798448

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 44160, train_perplexity=119.96259, train_loss=4.78718

Batch 44170, train_perplexity=126.8111, train_loss=4.8426986

Batch 44180, train_perplexity=141.96495, train_loss=4.95558

Batch 44190, train_perplexity=120.27154, train_loss=4.789752

Batch 44200, train_perplexity=135.8644, train_loss=4.9116573

Batch 44210, train_perplexity=120.37717, train_loss=4.79063

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 44220, train_perplexity=115.2572, train_loss=4.747166

Batch 44230, train_perplexity=136.2108, train_loss=4.9142036

Batch 44240, train_perplexity=124.855736, train_loss=4.827159

Batch 44250, train_perplexity=112.3002, train_loss=4.7211757

Batch 44260, train_perplexity=118.75612, train_loss=4.777072

Batch 44270, train_perplexity=122.58732, train_loss=4.8088236

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6111 sentences.
Finished loading
Batch 44280, train_perplexity=117.134834, train_loss=4.7633257

Batch 44290, train_perplexity=123.24699, train_loss=4.8141904

Batch 44300, train_perplexity=104.68284, train_loss=4.650935

Batch 44310, train_perplexity=124.53922, train_loss=4.8246207

Batch 44320, train_perplexity=118.659096, train_loss=4.7762547

Batch 44330, train_perplexity=122.87654, train_loss=4.81118

Batch 44340, train_perplexity=117.9243, train_loss=4.770043

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 44350, train_perplexity=122.903786, train_loss=4.811402

Batch 44360, train_perplexity=123.477165, train_loss=4.8160563

Batch 44370, train_perplexity=115.14794, train_loss=4.7462177

Batch 44380, train_perplexity=114.469215, train_loss=4.740306

Batch 44390, train_perplexity=118.86115, train_loss=4.777956

Batch 44400, train_perplexity=112.83439, train_loss=4.725921

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 44410, train_perplexity=131.75143, train_loss=4.880917

Batch 44420, train_perplexity=117.528595, train_loss=4.7666817

Batch 44430, train_perplexity=135.57318, train_loss=4.9095116

Batch 44440, train_perplexity=125.89308, train_loss=4.835433

Batch 44450, train_perplexity=134.8986, train_loss=4.9045234

Batch 44460, train_perplexity=121.8795, train_loss=4.803033

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 44470, train_perplexity=117.144775, train_loss=4.7634106

Batch 44480, train_perplexity=125.516655, train_loss=4.8324385

Batch 44490, train_perplexity=126.41716, train_loss=4.839587

Batch 44500, train_perplexity=119.011215, train_loss=4.7792177

Batch 44510, train_perplexity=130.16849, train_loss=4.8688297

Batch 44520, train_perplexity=112.16325, train_loss=4.7199554

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 44530, train_perplexity=115.5607, train_loss=4.749796

Batch 44540, train_perplexity=125.88816, train_loss=4.835394

Batch 44550, train_perplexity=115.417076, train_loss=4.7485523

Batch 44560, train_perplexity=104.48356, train_loss=4.6490297

Batch 44570, train_perplexity=127.43685, train_loss=4.847621

Batch 44580, train_perplexity=114.04687, train_loss=4.7366095

Batch 44590, train_perplexity=124.16726, train_loss=4.8216295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 44600, train_perplexity=116.48038, train_loss=4.757723

Batch 44610, train_perplexity=116.65135, train_loss=4.7591896

Batch 44620, train_perplexity=115.2589, train_loss=4.747181

Batch 44630, train_perplexity=119.33848, train_loss=4.781964

Batch 44640, train_perplexity=121.53837, train_loss=4.80023

Batch 44650, train_perplexity=111.06048, train_loss=4.710075

Batch 44660, train_perplexity=120.77037, train_loss=4.793891

Batch 44670, train_perplexity=106.93648, train_loss=4.672235

Batch 44680, train_perplexity=118.53864, train_loss=4.775239

Batch 44690, train_perplexity=115.25143, train_loss=4.747116

Batch 44700, train_perplexity=128.11241, train_loss=4.852908

Batch 44710, train_perplexity=118.310394, train_loss=4.7733116

Batch 44720, train_perplexity=112.26781, train_loss=4.720887

Batch 44730, train_perplexity=123.92362, train_loss=4.8196654

Batch 44740, train_perplexity=123.98492, train_loss=4.82016

Batch 44750, train_perplexity=118.74088, train_loss=4.7769437

Batch 44760, train_perplexity=110.689285, train_loss=4.706727

Batch 44770, train_perplexity=129.51932, train_loss=4.86383

Batch 44780, train_perplexity=119.86984, train_loss=4.7864065

Batch 44790, train_perplexity=112.25646, train_loss=4.720786

Batch 44800, train_perplexity=123.6419, train_loss=4.8173895

Batch 44810, train_perplexity=124.62477, train_loss=4.8253074

Batch 44820, train_perplexity=109.156784, train_loss=4.6927853

Batch 44830, train_perplexity=111.66025, train_loss=4.715461

Batch 44840, train_perplexity=123.85161, train_loss=4.819084

Batch 44850, train_perplexity=121.26039, train_loss=4.7979403

Batch 44860, train_perplexity=116.52065, train_loss=4.7580686

Batch 44870, train_perplexity=125.35809, train_loss=4.8311744

Batch 44880, train_perplexity=118.84715, train_loss=4.777838

Batch 44890, train_perplexity=134.60925, train_loss=4.902376

Batch 44900, train_perplexity=114.554504, train_loss=4.7410507

Batch 44910, train_perplexity=118.1086, train_loss=4.7716045

Batch 44920, train_perplexity=105.708626, train_loss=4.6606865

Batch 44930, train_perplexity=118.50654, train_loss=4.774968

Batch 44940, train_perplexity=124.39868, train_loss=4.8234916

Batch 44950, train_perplexity=109.90453, train_loss=4.699612

Batch 44960, train_perplexity=112.715385, train_loss=4.724866

Batch 44970, train_perplexity=113.53314, train_loss=4.732095

Batch 44980, train_perplexity=106.92068, train_loss=4.672087

Batch 44990, train_perplexity=131.8406, train_loss=4.8815937

Batch 45000, train_perplexity=115.52588, train_loss=4.7494946

Batch 45010, train_perplexity=131.46521, train_loss=4.878742

Batch 45020, train_perplexity=110.86312, train_loss=4.7082963

Batch 45030, train_perplexity=124.99059, train_loss=4.8282385

Batch 45040, train_perplexity=117.023735, train_loss=4.762377

Batch 45050, train_perplexity=118.82624, train_loss=4.7776623

Batch 45060, train_perplexity=117.90597, train_loss=4.7698874

Batch 45070, train_perplexity=108.48264, train_loss=4.68659

Batch 45080, train_perplexity=114.209694, train_loss=4.738036

Batch 45090, train_perplexity=107.65947, train_loss=4.678973

Batch 45100, train_perplexity=123.28678, train_loss=4.814513

Batch 45110, train_perplexity=129.49011, train_loss=4.8636045

Batch 45120, train_perplexity=105.783554, train_loss=4.661395

Batch 45130, train_perplexity=120.44957, train_loss=4.791231

Batch 45140, train_perplexity=115.03149, train_loss=4.745206

Batch 45150, train_perplexity=114.15285, train_loss=4.7375383

Batch 45160, train_perplexity=125.6061, train_loss=4.833151

Batch 45170, train_perplexity=115.11577, train_loss=4.7459383

Batch 45180, train_perplexity=103.40721, train_loss=4.6386747

Batch 45190, train_perplexity=111.10534, train_loss=4.710479

Batch 45200, train_perplexity=133.22073, train_loss=4.8920074

Batch 45210, train_perplexity=117.31119, train_loss=4.76483

Batch 45220, train_perplexity=115.26396, train_loss=4.747225

Batch 45230, train_perplexity=113.80643, train_loss=4.734499

Batch 45240, train_perplexity=114.30416, train_loss=4.738863
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 45250, train_perplexity=118.129326, train_loss=4.77178

Batch 45260, train_perplexity=109.89479, train_loss=4.6995234

Batch 45270, train_perplexity=100.56152, train_loss=4.6107697

Batch 45280, train_perplexity=107.58352, train_loss=4.6782675

Batch 45290, train_perplexity=118.69582, train_loss=4.776564

Batch 45300, train_perplexity=113.9287, train_loss=4.735573

Batch 45310, train_perplexity=97.937325, train_loss=4.5843277

Batch 45320, train_perplexity=128.58269, train_loss=4.856572

Batch 45330, train_perplexity=113.370735, train_loss=4.7306633

Batch 45340, train_perplexity=117.52456, train_loss=4.7666473

Batch 45350, train_perplexity=118.851456, train_loss=4.7778745

Batch 45360, train_perplexity=116.83511, train_loss=4.7607636

Batch 45370, train_perplexity=120.01534, train_loss=4.7876196

Batch 45380, train_perplexity=105.2956, train_loss=4.6567717

Batch 45390, train_perplexity=131.29494, train_loss=4.877446

Batch 45400, train_perplexity=121.40399, train_loss=4.799124

Batch 45410, train_perplexity=120.51018, train_loss=4.791734

Batch 45420, train_perplexity=124.107475, train_loss=4.821148

Batch 45430, train_perplexity=124.813644, train_loss=4.826822

Batch 45440, train_perplexity=110.789246, train_loss=4.7076297

Batch 45450, train_perplexity=119.119316, train_loss=4.7801256

Batch 45460, train_perplexity=113.08598, train_loss=4.7281485

Batch 45470, train_perplexity=108.00984, train_loss=4.6822224

Batch 45480, train_perplexity=130.38605, train_loss=4.8704996

Batch 45490, train_perplexity=100.936485, train_loss=4.6144915

Batch 45500, train_perplexity=110.56089, train_loss=4.7055664

Batch 45510, train_perplexity=121.05005, train_loss=4.796204

Batch 45520, train_perplexity=113.47479, train_loss=4.7315807

Batch 45530, train_perplexity=114.96799, train_loss=4.7446537

Batch 45540, train_perplexity=115.39551, train_loss=4.7483654

Batch 45550, train_perplexity=110.30797, train_loss=4.703276

Batch 45560, train_perplexity=121.90903, train_loss=4.803275

Batch 45570, train_perplexity=119.9563, train_loss=4.7871275

Batch 45580, train_perplexity=116.19325, train_loss=4.7552547

Batch 45590, train_perplexity=107.08584, train_loss=4.6736307

Batch 45600, train_perplexity=116.060356, train_loss=4.7541103

Batch 45610, train_perplexity=116.62488, train_loss=4.7589626

Batch 45620, train_perplexity=104.95814, train_loss=4.6535616

Batch 45630, train_perplexity=125.80019, train_loss=4.834695

Batch 45640, train_perplexity=118.02269, train_loss=4.770877

Batch 45650, train_perplexity=122.46211, train_loss=4.8078017

Batch 45660, train_perplexity=117.329475, train_loss=4.764986

Batch 45670, train_perplexity=117.59923, train_loss=4.7672825

Batch 45680, train_perplexity=109.78962, train_loss=4.698566

Batch 45690, train_perplexity=117.45173, train_loss=4.7660275

Batch 45700, train_perplexity=121.16052, train_loss=4.7971163

Batch 45710, train_perplexity=118.04233, train_loss=4.7710433

Batch 45720, train_perplexity=120.91886, train_loss=4.79512

Batch 45730, train_perplexity=123.86047, train_loss=4.8191557

Batch 45740, train_perplexity=116.47588, train_loss=4.757684

Batch 45750, train_perplexity=107.55767, train_loss=4.678027

Batch 45760, train_perplexity=118.972916, train_loss=4.778896

Batch 45770, train_perplexity=118.91886, train_loss=4.7784414

Batch 45780, train_perplexity=123.332054, train_loss=4.8148804

Batch 45790, train_perplexity=111.68975, train_loss=4.715725

Batch 45800, train_perplexity=121.92112, train_loss=4.8033743

Batch 45810, train_perplexity=125.2813, train_loss=4.8305616

Batch 45820, train_perplexity=127.2332, train_loss=4.8460217

Batch 45830, train_perplexity=109.64841, train_loss=4.697279

Batch 45840, train_perplexity=119.7203, train_loss=4.785158

Batch 45850, train_perplexity=125.02731, train_loss=4.828532

Batch 45860, train_perplexity=117.914795, train_loss=4.7699623

Batch 45870, train_perplexity=117.557236, train_loss=4.7669253

Batch 45880, train_perplexity=120.71567, train_loss=4.793438

Batch 45890, train_perplexity=123.401474, train_loss=4.815443

Batch 45900, train_perplexity=125.01658, train_loss=4.8284464

Batch 45910, train_perplexity=102.30741, train_loss=4.627982

Batch 45920, train_perplexity=110.78829, train_loss=4.707621

Batch 45930, train_perplexity=112.73764, train_loss=4.7250633

Batch 45940, train_perplexity=134.32393, train_loss=4.9002542

Batch 45950, train_perplexity=126.92907, train_loss=4.8436284

Batch 45960, train_perplexity=111.039986, train_loss=4.7098904

Batch 45970, train_perplexity=115.3768, train_loss=4.7482033

Batch 45980, train_perplexity=110.66733, train_loss=4.7065287

Batch 45990, train_perplexity=122.60304, train_loss=4.808952

Batch 46000, train_perplexity=118.28016, train_loss=4.773056

Batch 46010, train_perplexity=125.12029, train_loss=4.8292756

Batch 46020, train_perplexity=113.086365, train_loss=4.728152

Batch 46030, train_perplexity=112.598274, train_loss=4.7238264

Batch 46040, train_perplexity=113.4353, train_loss=4.7312326

Batch 46050, train_perplexity=124.69628, train_loss=4.825881

Batch 46060, train_perplexity=111.87012, train_loss=4.7173386

Batch 46070, train_perplexity=111.93447, train_loss=4.7179136

Batch 46080, train_perplexity=122.8534, train_loss=4.810992

Batch 46090, train_perplexity=128.09409, train_loss=4.852765

Batch 46100, train_perplexity=129.69861, train_loss=4.8652134

Batch 46110, train_perplexity=136.37535, train_loss=4.915411

Batch 46120, train_perplexity=129.4211, train_loss=4.8630714

Batch 46130, train_perplexity=124.17383, train_loss=4.8216825

Batch 46140, train_perplexity=118.94075, train_loss=4.7786255

Batch 46150, train_perplexity=104.622154, train_loss=4.6503553

Batch 46160, train_perplexity=125.44138, train_loss=4.8318386

Batch 46170, train_perplexity=114.26689, train_loss=4.738537

Batch 46180, train_perplexity=114.17397, train_loss=4.7377234

Batch 46190, train_perplexity=116.32397, train_loss=4.756379

Batch 46200, train_perplexity=115.517395, train_loss=4.749421

Batch 46210, train_perplexity=127.7698, train_loss=4.85023

Batch 46220, train_perplexity=123.9835, train_loss=4.8201485

Batch 46230, train_perplexity=110.882416, train_loss=4.7084703

Batch 46240, train_perplexity=109.79166, train_loss=4.6985846

Batch 46250, train_perplexity=120.36115, train_loss=4.790497

Batch 46260, train_perplexity=120.503914, train_loss=4.7916822

Batch 46270, train_perplexity=117.19936, train_loss=4.7638764

Batch 46280, train_perplexity=115.137505, train_loss=4.746127

Batch 46290, train_perplexity=130.92583, train_loss=4.874631

Batch 46300, train_perplexity=121.704, train_loss=4.801592

Batch 46310, train_perplexity=115.110825, train_loss=4.7458954

Batch 46320, train_perplexity=106.552185, train_loss=4.668635

Batch 46330, train_perplexity=121.44811, train_loss=4.799487

Batch 46340, train_perplexity=135.67322, train_loss=4.910249

Batch 46350, train_perplexity=112.14523, train_loss=4.7197948

Batch 46360, train_perplexity=117.04165, train_loss=4.76253

Batch 46370, train_perplexity=132.14473, train_loss=4.883898

Batch 46380, train_perplexity=124.54617, train_loss=4.8246765

Batch 46390, train_perplexity=114.971054, train_loss=4.7446804

Batch 46400, train_perplexity=123.44591, train_loss=4.815803

Batch 46410, train_perplexity=113.99108, train_loss=4.73612

Batch 46420, train_perplexity=113.41724, train_loss=4.7310734

Batch 46430, train_perplexity=130.04776, train_loss=4.867902

Batch 46440, train_perplexity=128.63481, train_loss=4.8569775

Batch 46450, train_perplexity=116.63478, train_loss=4.7590475

Batch 46460, train_perplexity=120.68885, train_loss=4.7932158

Batch 46470, train_perplexity=113.97065, train_loss=4.735941

Batch 46480, train_perplexity=114.15366, train_loss=4.7375455

Batch 46490, train_perplexity=135.02982, train_loss=4.9054956

Batch 46500, train_perplexity=120.521385, train_loss=4.791827

Batch 46510, train_perplexity=135.88539, train_loss=4.911812

Batch 46520, train_perplexity=115.87599, train_loss=4.7525206

Batch 46530, train_perplexity=115.23813, train_loss=4.7470007

Batch 46540, train_perplexity=121.30122, train_loss=4.798277

Batch 46550, train_perplexity=108.756165, train_loss=4.6891084
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46560, train_perplexity=117.66564, train_loss=4.767847

Batch 46570, train_perplexity=111.44078, train_loss=4.7134933

Batch 46580, train_perplexity=114.58526, train_loss=4.741319

Batch 46590, train_perplexity=131.2647, train_loss=4.877216

Batch 46600, train_perplexity=119.25139, train_loss=4.781234

Batch 46610, train_perplexity=112.84359, train_loss=4.7260027

Batch 46620, train_perplexity=111.77542, train_loss=4.7164917

Batch 46630, train_perplexity=119.19397, train_loss=4.780752

Batch 46640, train_perplexity=123.31418, train_loss=4.8147354

Batch 46650, train_perplexity=110.21159, train_loss=4.702402

Batch 46660, train_perplexity=130.04355, train_loss=4.8678694

Batch 46670, train_perplexity=104.73816, train_loss=4.6514635

Batch 46680, train_perplexity=122.371635, train_loss=4.8070626

Batch 46690, train_perplexity=128.29532, train_loss=4.854335

Batch 46700, train_perplexity=110.15968, train_loss=4.701931

Batch 46710, train_perplexity=127.21949, train_loss=4.845914

Batch 46720, train_perplexity=125.610115, train_loss=4.833183

Batch 46730, train_perplexity=130.52675, train_loss=4.871578

Batch 46740, train_perplexity=113.7417, train_loss=4.73393

Batch 46750, train_perplexity=110.45508, train_loss=4.704609

Batch 46760, train_perplexity=112.592316, train_loss=4.7237735

Batch 46770, train_perplexity=124.86942, train_loss=4.8272686

Batch 46780, train_perplexity=117.568726, train_loss=4.767023

Batch 46790, train_perplexity=107.308495, train_loss=4.675708

Batch 46800, train_perplexity=113.1806, train_loss=4.728985

Batch 46810, train_perplexity=107.84351, train_loss=4.680681

Batch 46820, train_perplexity=117.601585, train_loss=4.7673025

Batch 46830, train_perplexity=114.21013, train_loss=4.73804

Batch 46840, train_perplexity=115.372505, train_loss=4.748166

Batch 46850, train_perplexity=129.19124, train_loss=4.861294

Batch 46860, train_perplexity=119.13726, train_loss=4.7802763

Batch 46870, train_perplexity=131.43349, train_loss=4.878501

Batch 46880, train_perplexity=132.0985, train_loss=4.883548

Batch 46890, train_perplexity=120.4328, train_loss=4.791092

Batch 46900, train_perplexity=109.90469, train_loss=4.6996136

Batch 46910, train_perplexity=118.280495, train_loss=4.773059

Batch 46920, train_perplexity=120.554955, train_loss=4.7921057

Batch 46930, train_perplexity=111.511375, train_loss=4.7141266

Batch 46940, train_perplexity=114.71882, train_loss=4.742484

Batch 46950, train_perplexity=125.58712, train_loss=4.8329997

Batch 46960, train_perplexity=123.488, train_loss=4.816144

Batch 46970, train_perplexity=116.09921, train_loss=4.754445

Batch 46980, train_perplexity=119.30759, train_loss=4.781705

Batch 46990, train_perplexity=111.91761, train_loss=4.717763

Batch 47000, train_perplexity=108.84441, train_loss=4.6899195

Batch 47010, train_perplexity=104.53888, train_loss=4.649559

Batch 47020, train_perplexity=117.015144, train_loss=4.7623034

Batch 47030, train_perplexity=117.10892, train_loss=4.7631044

Batch 47040, train_perplexity=111.1339, train_loss=4.710736

Batch 47050, train_perplexity=127.245705, train_loss=4.84612

Batch 47060, train_perplexity=106.60214, train_loss=4.6691036

Batch 47070, train_perplexity=118.99283, train_loss=4.779063

Batch 47080, train_perplexity=123.17537, train_loss=4.813609

Batch 47090, train_perplexity=128.90596, train_loss=4.859083

Batch 47100, train_perplexity=110.27052, train_loss=4.7029366

Batch 47110, train_perplexity=125.97211, train_loss=4.8360605

Batch 47120, train_perplexity=111.08543, train_loss=4.7102995

Batch 47130, train_perplexity=122.453766, train_loss=4.8077335

Batch 47140, train_perplexity=110.84256, train_loss=4.708111

Batch 47150, train_perplexity=129.51976, train_loss=4.8638334

Batch 47160, train_perplexity=106.177124, train_loss=4.6651087

Batch 47170, train_perplexity=105.72123, train_loss=4.6608057

Batch 47180, train_perplexity=111.785866, train_loss=4.716585

Batch 47190, train_perplexity=103.52024, train_loss=4.639767

Batch 47200, train_perplexity=133.807, train_loss=4.8963985

Batch 47210, train_perplexity=109.66995, train_loss=4.6974754

Batch 47220, train_perplexity=113.19415, train_loss=4.7291045

Batch 47230, train_perplexity=120.04808, train_loss=4.7878923

Batch 47240, train_perplexity=113.86624, train_loss=4.7350245

Batch 47250, train_perplexity=118.3166, train_loss=4.773364

Batch 47260, train_perplexity=112.16887, train_loss=4.7200055

Batch 47270, train_perplexity=119.56022, train_loss=4.78382

Batch 47280, train_perplexity=128.04182, train_loss=4.852357

Batch 47290, train_perplexity=119.1655, train_loss=4.7805133

Batch 47300, train_perplexity=125.3606, train_loss=4.8311944

Batch 47310, train_perplexity=122.538345, train_loss=4.808424

Batch 47320, train_perplexity=116.54116, train_loss=4.7582445

Batch 47330, train_perplexity=119.21728, train_loss=4.7809477

Batch 47340, train_perplexity=107.97606, train_loss=4.6819096

Batch 47350, train_perplexity=129.23788, train_loss=4.8616548

Batch 47360, train_perplexity=142.83171, train_loss=4.961667

Batch 47370, train_perplexity=125.3254, train_loss=4.8309135

Batch 47380, train_perplexity=113.01914, train_loss=4.727557

Batch 47390, train_perplexity=113.357925, train_loss=4.7305503

Batch 47400, train_perplexity=139.10835, train_loss=4.935253

Batch 47410, train_perplexity=122.156456, train_loss=4.8053026

Batch 47420, train_perplexity=124.22139, train_loss=4.8220654

Batch 47430, train_perplexity=111.13274, train_loss=4.7107253

Batch 47440, train_perplexity=118.12764, train_loss=4.7717657

Batch 47450, train_perplexity=133.24138, train_loss=4.8921623

Batch 47460, train_perplexity=110.12828, train_loss=4.701646

Batch 47470, train_perplexity=123.47935, train_loss=4.816074

Batch 47480, train_perplexity=113.04404, train_loss=4.7277775

Batch 47490, train_perplexity=116.29813, train_loss=4.756157

Batch 47500, train_perplexity=145.4895, train_loss=4.980104

Batch 47510, train_perplexity=109.14274, train_loss=4.6926565

Batch 47520, train_perplexity=124.1868, train_loss=4.821787

Batch 47530, train_perplexity=113.76557, train_loss=4.73414

Batch 47540, train_perplexity=131.1542, train_loss=4.876374

Batch 47550, train_perplexity=116.968506, train_loss=4.7619047

Batch 47560, train_perplexity=114.21895, train_loss=4.738117

Batch 47570, train_perplexity=118.01177, train_loss=4.7707844

Batch 47580, train_perplexity=121.63067, train_loss=4.800989

Batch 47590, train_perplexity=130.48058, train_loss=4.8712244

Batch 47600, train_perplexity=117.20121, train_loss=4.763892

Batch 47610, train_perplexity=120.91252, train_loss=4.7950673

Batch 47620, train_perplexity=113.58404, train_loss=4.732543

Batch 47630, train_perplexity=113.704124, train_loss=4.7335997

Batch 47640, train_perplexity=137.7312, train_loss=4.925304

Batch 47650, train_perplexity=110.47578, train_loss=4.7047963

Batch 47660, train_perplexity=126.49591, train_loss=4.84021

Batch 47670, train_perplexity=108.05456, train_loss=4.6826363

Batch 47680, train_perplexity=118.18736, train_loss=4.772271

Batch 47690, train_perplexity=115.3598, train_loss=4.748056

Batch 47700, train_perplexity=113.47864, train_loss=4.7316146

Batch 47710, train_perplexity=101.48724, train_loss=4.619933

Batch 47720, train_perplexity=116.63161, train_loss=4.7590203

Batch 47730, train_perplexity=108.95159, train_loss=4.6909037

Batch 47740, train_perplexity=122.75695, train_loss=4.8102064

Batch 47750, train_perplexity=118.268425, train_loss=4.772957

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 47760, train_perplexity=115.762276, train_loss=4.7515388

Batch 47770, train_perplexity=107.68853, train_loss=4.679243

Batch 47780, train_perplexity=99.483734, train_loss=4.599994

Batch 47790, train_perplexity=111.566154, train_loss=4.7146177

Batch 47800, train_perplexity=101.964355, train_loss=4.6246233

Batch 47810, train_perplexity=106.85396, train_loss=4.671463

Batch 47820, train_perplexity=112.18786, train_loss=4.720175

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 47830, train_perplexity=122.312546, train_loss=4.8065796

Batch 47840, train_perplexity=119.652725, train_loss=4.7845936

Batch 47850, train_perplexity=117.50484, train_loss=4.7664795

Batch 47860, train_perplexity=102.437996, train_loss=4.6292577

Batch 47870, train_perplexity=107.31156, train_loss=4.6757364

Batch 47880, train_perplexity=107.03723, train_loss=4.673177

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 47890, train_perplexity=103.07718, train_loss=4.635478

Batch 47900, train_perplexity=116.35581, train_loss=4.756653

Batch 47910, train_perplexity=104.36176, train_loss=4.6478634

Batch 47920, train_perplexity=124.258766, train_loss=4.822366

Batch 47930, train_perplexity=106.79151, train_loss=4.6708784

Batch 47940, train_perplexity=102.054344, train_loss=4.6255054

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 47950, train_perplexity=106.8155, train_loss=4.671103

Batch 47960, train_perplexity=120.49208, train_loss=4.791584

Batch 47970, train_perplexity=101.74706, train_loss=4.62249

Batch 47980, train_perplexity=112.420166, train_loss=4.7222433

Batch 47990, train_perplexity=119.66465, train_loss=4.7846932

Batch 48000, train_perplexity=122.89312, train_loss=4.811315

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 48010, train_perplexity=119.57339, train_loss=4.7839303

Batch 48020, train_perplexity=100.38508, train_loss=4.6090136

Batch 48030, train_perplexity=111.854546, train_loss=4.7171993

Batch 48040, train_perplexity=104.96174, train_loss=4.653596

Batch 48050, train_perplexity=118.94926, train_loss=4.778697

Batch 48060, train_perplexity=105.200905, train_loss=4.655872

Batch 48070, train_perplexity=117.449936, train_loss=4.766012

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 48080, train_perplexity=120.408226, train_loss=4.790888

Batch 48090, train_perplexity=122.10235, train_loss=4.8048596

Batch 48100, train_perplexity=101.82321, train_loss=4.623238

Batch 48110, train_perplexity=115.2954, train_loss=4.7474976

Batch 48120, train_perplexity=110.8912, train_loss=4.7085495

Batch 48130, train_perplexity=104.5834, train_loss=4.649985

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 48140, train_perplexity=116.99495, train_loss=4.7621307

Batch 48150, train_perplexity=101.282845, train_loss=4.617917

Batch 48160, train_perplexity=122.91, train_loss=4.8114524

Batch 48170, train_perplexity=103.97968, train_loss=4.6441956

Batch 48180, train_perplexity=125.837746, train_loss=4.8349934

Batch 48190, train_perplexity=110.794685, train_loss=4.707679

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 48200, train_perplexity=119.53833, train_loss=4.783637

Batch 48210, train_perplexity=124.73172, train_loss=4.826165

Batch 48220, train_perplexity=120.383995, train_loss=4.7906866

Batch 48230, train_perplexity=122.25505, train_loss=4.8061094

Batch 48240, train_perplexity=113.27099, train_loss=4.729783

Batch 48250, train_perplexity=114.548935, train_loss=4.741002

Batch 48260, train_perplexity=116.88978, train_loss=4.7612314

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 48270, train_perplexity=106.47809, train_loss=4.667939

Batch 48280, train_perplexity=100.68488, train_loss=4.6119957

Batch 48290, train_perplexity=108.548874, train_loss=4.6872005

Batch 48300, train_perplexity=95.34626, train_loss=4.557515

Batch 48310, train_perplexity=128.59888, train_loss=4.856698

Batch 48320, train_perplexity=116.93605, train_loss=4.761627

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 48330, train_perplexity=111.249435, train_loss=4.711775

Batch 48340, train_perplexity=106.64312, train_loss=4.669488

Batch 48350, train_perplexity=102.70137, train_loss=4.6318254

Batch 48360, train_perplexity=118.8874, train_loss=4.778177

Batch 48370, train_perplexity=103.86105, train_loss=4.643054

Batch 48380, train_perplexity=123.48624, train_loss=4.8161297

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 48390, train_perplexity=104.61916, train_loss=4.6503267

Batch 48400, train_perplexity=111.67404, train_loss=4.7155843

Batch 48410, train_perplexity=115.08602, train_loss=4.74568

Batch 48420, train_perplexity=126.016624, train_loss=4.836414

Batch 48430, train_perplexity=106.14159, train_loss=4.664774

Batch 48440, train_perplexity=116.011, train_loss=4.753685
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48450, train_perplexity=109.387924, train_loss=4.6949005

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 48460, train_perplexity=122.58884, train_loss=4.808836

Batch 48470, train_perplexity=111.21316, train_loss=4.7114487

Batch 48480, train_perplexity=119.96259, train_loss=4.78718

Batch 48490, train_perplexity=124.40918, train_loss=4.823576

Batch 48500, train_perplexity=110.13605, train_loss=4.7017164

Batch 48510, train_perplexity=119.51667, train_loss=4.783456

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 48520, train_perplexity=109.20114, train_loss=4.6931915

Batch 48530, train_perplexity=120.30423, train_loss=4.790024

Batch 48540, train_perplexity=111.65998, train_loss=4.7154584

Batch 48550, train_perplexity=121.80363, train_loss=4.80241

Batch 48560, train_perplexity=113.839424, train_loss=4.734789

Batch 48570, train_perplexity=107.5828, train_loss=4.678261

Batch 48580, train_perplexity=116.074524, train_loss=4.7542324

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 48590, train_perplexity=109.83758, train_loss=4.6990027

Batch 48600, train_perplexity=107.55351, train_loss=4.6779885

Batch 48610, train_perplexity=123.861176, train_loss=4.8191614

Batch 48620, train_perplexity=113.54992, train_loss=4.7322426

Batch 48630, train_perplexity=117.41242, train_loss=4.7656927

Batch 48640, train_perplexity=126.38027, train_loss=4.8392954

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 48650, train_perplexity=108.69546, train_loss=4.68855

Batch 48660, train_perplexity=119.836464, train_loss=4.786128

Batch 48670, train_perplexity=114.55122, train_loss=4.741022

Batch 48680, train_perplexity=117.71317, train_loss=4.768251

Batch 48690, train_perplexity=110.17996, train_loss=4.702115

Batch 48700, train_perplexity=111.17991, train_loss=4.7111497

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 48710, train_perplexity=114.270485, train_loss=4.7385683

Batch 48720, train_perplexity=118.87787, train_loss=4.7780967

Batch 48730, train_perplexity=104.7442, train_loss=4.651521

Batch 48740, train_perplexity=119.77717, train_loss=4.785633

Batch 48750, train_perplexity=114.738625, train_loss=4.7426567

Batch 48760, train_perplexity=122.467896, train_loss=4.807849

Batch 48770, train_perplexity=115.78944, train_loss=4.7517734

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 48780, train_perplexity=117.34123, train_loss=4.765086

Batch 48790, train_perplexity=113.520035, train_loss=4.7319794

Batch 48800, train_perplexity=112.000946, train_loss=4.7185073

Batch 48810, train_perplexity=111.91878, train_loss=4.7177734

Batch 48820, train_perplexity=109.410614, train_loss=4.695108

Batch 48830, train_perplexity=123.877716, train_loss=4.819295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 48840, train_perplexity=121.230446, train_loss=4.7976933

Batch 48850, train_perplexity=112.11898, train_loss=4.7195606

Batch 48860, train_perplexity=108.63328, train_loss=4.687978

Batch 48870, train_perplexity=113.884056, train_loss=4.735181

Batch 48880, train_perplexity=122.14346, train_loss=4.8051963

Batch 48890, train_perplexity=117.49996, train_loss=4.766438

Batch 48900, train_perplexity=112.652084, train_loss=4.724304

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 48910, train_perplexity=114.73709, train_loss=4.7426434

Batch 48920, train_perplexity=121.52881, train_loss=4.8001513

Batch 48930, train_perplexity=111.27002, train_loss=4.71196

Batch 48940, train_perplexity=111.56062, train_loss=4.714568

Batch 48950, train_perplexity=106.287506, train_loss=4.6661477

Batch 48960, train_perplexity=107.24241, train_loss=4.6750917

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 48970, train_perplexity=116.45905, train_loss=4.7575397

Batch 48980, train_perplexity=105.43125, train_loss=4.658059

Batch 48990, train_perplexity=115.315414, train_loss=4.747671

Batch 49000, train_perplexity=117.509094, train_loss=4.7665157

Batch 49010, train_perplexity=126.453575, train_loss=4.839875

Batch 49020, train_perplexity=122.127335, train_loss=4.805064

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 49030, train_perplexity=108.1161, train_loss=4.6832056

Batch 49040, train_perplexity=108.18035, train_loss=4.6837997

Batch 49050, train_perplexity=122.225204, train_loss=4.8058653

Batch 49060, train_perplexity=103.50884, train_loss=4.639657

Batch 49070, train_perplexity=105.03329, train_loss=4.6542773

Batch 49080, train_perplexity=123.06089, train_loss=4.8126793

Batch 49090, train_perplexity=132.00606, train_loss=4.882848

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 49100, train_perplexity=128.86086, train_loss=4.858733

Batch 49110, train_perplexity=115.17331, train_loss=4.746438

Batch 49120, train_perplexity=122.90256, train_loss=4.811392

Batch 49130, train_perplexity=129.46109, train_loss=4.8633804

Batch 49140, train_perplexity=117.673386, train_loss=4.767913

Batch 49150, train_perplexity=115.37405, train_loss=4.7481794

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 49160, train_perplexity=104.43106, train_loss=4.648527

Batch 49170, train_perplexity=102.16473, train_loss=4.6265864

Batch 49180, train_perplexity=121.26548, train_loss=4.797982

Batch 49190, train_perplexity=99.94938, train_loss=4.604664

Batch 49200, train_perplexity=109.16355, train_loss=4.6928473

Batch 49210, train_perplexity=102.56072, train_loss=4.630455

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 49220, train_perplexity=129.17535, train_loss=4.861171

Batch 49230, train_perplexity=113.28109, train_loss=4.729872

Batch 49240, train_perplexity=115.334114, train_loss=4.7478333

Batch 49250, train_perplexity=118.98109, train_loss=4.7789645

Batch 49260, train_perplexity=110.85926, train_loss=4.7082615

Batch 49270, train_perplexity=114.779335, train_loss=4.7430115

Batch 49280, train_perplexity=116.84932, train_loss=4.7608852

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 49290, train_perplexity=101.53071, train_loss=4.6203613

Batch 49300, train_perplexity=108.96474, train_loss=4.6910243

Batch 49310, train_perplexity=118.65842, train_loss=4.776249

Batch 49320, train_perplexity=108.69405, train_loss=4.688537

Batch 49330, train_perplexity=110.18469, train_loss=4.702158

Batch 49340, train_perplexity=115.66467, train_loss=4.750695

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 49350, train_perplexity=108.19582, train_loss=4.683943

Batch 49360, train_perplexity=109.79579, train_loss=4.698622

Batch 49370, train_perplexity=122.355064, train_loss=4.806927

Batch 49380, train_perplexity=119.55383, train_loss=4.7837667

Batch 49390, train_perplexity=111.69263, train_loss=4.7157507

Batch 49400, train_perplexity=114.31561, train_loss=4.738963

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 49410, train_perplexity=130.10886, train_loss=4.8683715

Batch 49420, train_perplexity=102.778725, train_loss=4.6325784

Batch 49430, train_perplexity=100.3023, train_loss=4.6081886

Batch 49440, train_perplexity=115.827705, train_loss=4.752104

Batch 49450, train_perplexity=113.75033, train_loss=4.734006

Batch 49460, train_perplexity=122.76269, train_loss=4.810253

Batch 49470, train_perplexity=124.93077, train_loss=4.8277597

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 49480, train_perplexity=112.61879, train_loss=4.7240086

Batch 49490, train_perplexity=104.111855, train_loss=4.645466

Batch 49500, train_perplexity=125.57497, train_loss=4.832903

Batch 49510, train_perplexity=116.761765, train_loss=4.7601357

Batch 49520, train_perplexity=115.0531, train_loss=4.7453938

Batch 49530, train_perplexity=112.122505, train_loss=4.719592

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 49540, train_perplexity=111.163, train_loss=4.7109976

Batch 49550, train_perplexity=130.28226, train_loss=4.8697033

Batch 49560, train_perplexity=104.61787, train_loss=4.6503143

Batch 49570, train_perplexity=128.20189, train_loss=4.853606

Batch 49580, train_perplexity=112.15646, train_loss=4.719895

Batch 49590, train_perplexity=122.521286, train_loss=4.8082848

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 49600, train_perplexity=115.03083, train_loss=4.7452

Batch 49610, train_perplexity=111.77105, train_loss=4.7164526

Batch 49620, train_perplexity=105.908676, train_loss=4.662577

Batch 49630, train_perplexity=111.125, train_loss=4.7106557

Batch 49640, train_perplexity=110.72032, train_loss=4.7070074

Batch 49650, train_perplexity=122.01435, train_loss=4.8041387

Batch 49660, train_perplexity=110.51793, train_loss=4.705178

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 49670, train_perplexity=115.868034, train_loss=4.752452

Batch 49680, train_perplexity=104.63622, train_loss=4.65049

Batch 49690, train_perplexity=113.46051, train_loss=4.731455

Batch 49700, train_perplexity=119.26356, train_loss=4.781336

Batch 49710, train_perplexity=126.18842, train_loss=4.837776
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 49720, train_perplexity=97.9831, train_loss=4.584795

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 49730, train_perplexity=124.351, train_loss=4.823108

Batch 49740, train_perplexity=107.20212, train_loss=4.674716

Batch 49750, train_perplexity=127.13969, train_loss=4.8452864

Batch 49760, train_perplexity=124.5729, train_loss=4.824891

Batch 49770, train_perplexity=113.625046, train_loss=4.732904

Batch 49780, train_perplexity=120.13787, train_loss=4.78864

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 49790, train_perplexity=112.942474, train_loss=4.7268786

Batch 49800, train_perplexity=116.759705, train_loss=4.760118

Batch 49810, train_perplexity=103.95896, train_loss=4.6439962

Batch 49820, train_perplexity=102.33405, train_loss=4.6282425

Batch 49830, train_perplexity=105.9936, train_loss=4.6633787

Batch 49840, train_perplexity=112.98735, train_loss=4.727276

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 49850, train_perplexity=125.09136, train_loss=4.8290443

Batch 49860, train_perplexity=124.82645, train_loss=4.8269243

Batch 49870, train_perplexity=122.841156, train_loss=4.810892

Batch 49880, train_perplexity=112.809425, train_loss=4.7257

Batch 49890, train_perplexity=116.01211, train_loss=4.7536945

Batch 49900, train_perplexity=112.02845, train_loss=4.718753

Batch 49910, train_perplexity=113.75358, train_loss=4.7340345

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 49920, train_perplexity=114.4619, train_loss=4.740242

Batch 49930, train_perplexity=105.599304, train_loss=4.6596518

Batch 49940, train_perplexity=102.869385, train_loss=4.63346

Batch 49950, train_perplexity=125.49673, train_loss=4.8322797

Batch 49960, train_perplexity=121.34293, train_loss=4.7986207

Batch 49970, train_perplexity=105.64785, train_loss=4.6601114

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 49980, train_perplexity=105.70515, train_loss=4.6606536

Batch 49990, train_perplexity=114.704216, train_loss=4.742357

Batch 50000, train_perplexity=109.07125, train_loss=4.6920013

Batch 50010, train_perplexity=105.19699, train_loss=4.6558347

Batch 50020, train_perplexity=119.016495, train_loss=4.779262

Batch 50030, train_perplexity=121.114655, train_loss=4.7967377

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 50040, train_perplexity=113.88916, train_loss=4.7352257

Batch 50050, train_perplexity=105.04877, train_loss=4.6544247

Batch 50060, train_perplexity=110.42948, train_loss=4.704377

Batch 50070, train_perplexity=113.582146, train_loss=4.7325263

Batch 50080, train_perplexity=112.19663, train_loss=4.720253

Batch 50090, train_perplexity=102.590996, train_loss=4.63075

Batch 50100, train_perplexity=110.152756, train_loss=4.701868

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 50110, train_perplexity=118.63217, train_loss=4.7760277

Batch 50120, train_perplexity=117.799255, train_loss=4.768982

Batch 50130, train_perplexity=123.74477, train_loss=4.818221

Batch 50140, train_perplexity=102.56693, train_loss=4.6305156

Batch 50150, train_perplexity=116.48994, train_loss=4.757805

Batch 50160, train_perplexity=132.08904, train_loss=4.8834763

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 50170, train_perplexity=106.15242, train_loss=4.664876

Batch 50180, train_perplexity=115.31113, train_loss=4.747634

Batch 50190, train_perplexity=128.72096, train_loss=4.857647

Batch 50200, train_perplexity=105.15732, train_loss=4.6554575

Batch 50210, train_perplexity=114.833755, train_loss=4.7434855

Batch 50220, train_perplexity=112.72495, train_loss=4.724951

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 50230, train_perplexity=98.259056, train_loss=4.5876074

Batch 50240, train_perplexity=116.22101, train_loss=4.7554936

Batch 50250, train_perplexity=117.37402, train_loss=4.7653656

Batch 50260, train_perplexity=132.0847, train_loss=4.8834434

Batch 50270, train_perplexity=112.03561, train_loss=4.7188168

Batch 50280, train_perplexity=126.47968, train_loss=4.8400817

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 50290, train_perplexity=103.41954, train_loss=4.638794

Batch 50300, train_perplexity=127.13084, train_loss=4.8452168

Batch 50310, train_perplexity=99.3682, train_loss=4.598832

Batch 50320, train_perplexity=110.68221, train_loss=4.706663

Batch 50330, train_perplexity=106.73398, train_loss=4.6703396

Batch 50340, train_perplexity=101.15562, train_loss=4.61666

Batch 50350, train_perplexity=112.65236, train_loss=4.7243066

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 50360, train_perplexity=102.50616, train_loss=4.629923

Batch 50370, train_perplexity=109.722885, train_loss=4.697958

Batch 50380, train_perplexity=113.660164, train_loss=4.733213

Batch 50390, train_perplexity=106.17763, train_loss=4.6651134

Batch 50400, train_perplexity=111.37188, train_loss=4.712875

Batch 50410, train_perplexity=127.13326, train_loss=4.845236

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 50420, train_perplexity=119.96328, train_loss=4.7871857

Batch 50430, train_perplexity=126.22248, train_loss=4.838046

Batch 50440, train_perplexity=128.16551, train_loss=4.8533225

Batch 50450, train_perplexity=99.09593, train_loss=4.5960884

Batch 50460, train_perplexity=119.568886, train_loss=4.7838926

Batch 50470, train_perplexity=106.37086, train_loss=4.6669316

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 50480, train_perplexity=105.71841, train_loss=4.660779

Batch 50490, train_perplexity=111.77367, train_loss=4.716476

Batch 50500, train_perplexity=122.890724, train_loss=4.8112955

Batch 50510, train_perplexity=110.68158, train_loss=4.7066574

Batch 50520, train_perplexity=103.12526, train_loss=4.6359444

Batch 50530, train_perplexity=115.99518, train_loss=4.7535486

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 50540, train_perplexity=121.2678, train_loss=4.7980013

Batch 50550, train_perplexity=119.4499, train_loss=4.782897

Batch 50560, train_perplexity=107.22389, train_loss=4.674919

Batch 50570, train_perplexity=125.06595, train_loss=4.828841

Batch 50580, train_perplexity=109.718285, train_loss=4.697916

Batch 50590, train_perplexity=116.65069, train_loss=4.759184

Batch 50600, train_perplexity=109.17854, train_loss=4.6929846

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 50610, train_perplexity=113.60543, train_loss=4.7327313

Batch 50620, train_perplexity=116.75074, train_loss=4.760041

Batch 50630, train_perplexity=116.34405, train_loss=4.7565517

Batch 50640, train_perplexity=117.47884, train_loss=4.7662582

Batch 50650, train_perplexity=123.345406, train_loss=4.8149886

Batch 50660, train_perplexity=122.05805, train_loss=4.804497

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 50670, train_perplexity=111.79637, train_loss=4.716679

Batch 50680, train_perplexity=113.06517, train_loss=4.7279644

Batch 50690, train_perplexity=127.02261, train_loss=4.844365

Batch 50700, train_perplexity=114.112526, train_loss=4.737185

Batch 50710, train_perplexity=99.45703, train_loss=4.5997257

Batch 50720, train_perplexity=124.69854, train_loss=4.825899

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 50730, train_perplexity=105.695114, train_loss=4.6605587

Batch 50740, train_perplexity=114.130646, train_loss=4.737344

Batch 50750, train_perplexity=106.49215, train_loss=4.6680713

Batch 50760, train_perplexity=116.60497, train_loss=4.758792

Batch 50770, train_perplexity=115.45076, train_loss=4.748844

Batch 50780, train_perplexity=118.00817, train_loss=4.770754

Batch 50790, train_perplexity=113.81837, train_loss=4.734604

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 50800, train_perplexity=108.25187, train_loss=4.6844606

Batch 50810, train_perplexity=123.17496, train_loss=4.813606

Batch 50820, train_perplexity=143.74234, train_loss=4.9680223

Batch 50830, train_perplexity=111.67186, train_loss=4.7155647

Batch 50840, train_perplexity=105.44684, train_loss=4.658207

Batch 50850, train_perplexity=116.08393, train_loss=4.7543135

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 50860, train_perplexity=132.58794, train_loss=4.887246

Batch 50870, train_perplexity=112.3129, train_loss=4.7212887

Batch 50880, train_perplexity=105.57322, train_loss=4.6594048

Batch 50890, train_perplexity=120.1538, train_loss=4.7887726

Batch 50900, train_perplexity=114.45399, train_loss=4.740173

Batch 50910, train_perplexity=109.56729, train_loss=4.696539

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 50920, train_perplexity=113.7602, train_loss=4.7340927

Batch 50930, train_perplexity=105.60746, train_loss=4.659729

Batch 50940, train_perplexity=106.27813, train_loss=4.6660595

Batch 50950, train_perplexity=123.49248, train_loss=4.81618

Batch 50960, train_perplexity=125.541794, train_loss=4.8326387

Batch 50970, train_perplexity=116.19181, train_loss=4.7552423

Batch 50980, train_perplexity=116.54961, train_loss=4.758317

Batch 50990, train_perplexity=107.7736, train_loss=4.6800327
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 51000, train_perplexity=115.94872, train_loss=4.753148

Batch 51010, train_perplexity=109.56855, train_loss=4.6965504

Batch 51020, train_perplexity=93.51597, train_loss=4.538132

Batch 51030, train_perplexity=112.41496, train_loss=4.722197

Batch 51040, train_perplexity=117.184944, train_loss=4.7637534

Batch 51050, train_perplexity=120.15643, train_loss=4.7887945

Batch 51060, train_perplexity=124.39981, train_loss=4.8235006

Batch 51070, train_perplexity=124.63332, train_loss=4.825376

Batch 51080, train_perplexity=102.19347, train_loss=4.626868

Batch 51090, train_perplexity=95.845406, train_loss=4.5627365

Batch 51100, train_perplexity=101.33096, train_loss=4.618392

Batch 51110, train_perplexity=96.98204, train_loss=4.574526

Batch 51120, train_perplexity=115.08355, train_loss=4.7456584

Batch 51130, train_perplexity=118.93996, train_loss=4.778619

Batch 51140, train_perplexity=99.52312, train_loss=4.60039

Batch 51150, train_perplexity=112.25314, train_loss=4.7207565

Batch 51160, train_perplexity=103.88142, train_loss=4.64325

Batch 51170, train_perplexity=117.21216, train_loss=4.7639856

Batch 51180, train_perplexity=111.89172, train_loss=4.7175317

Batch 51190, train_perplexity=106.62451, train_loss=4.6693134

Batch 51200, train_perplexity=112.19054, train_loss=4.7201986

Batch 51210, train_perplexity=92.48365, train_loss=4.527032

Batch 51220, train_perplexity=98.563324, train_loss=4.590699

Batch 51230, train_perplexity=118.823296, train_loss=4.7776375

Batch 51240, train_perplexity=119.57276, train_loss=4.783925

Batch 51250, train_perplexity=106.48103, train_loss=4.667967

Batch 51260, train_perplexity=96.46787, train_loss=4.56921

Batch 51270, train_perplexity=103.095566, train_loss=4.6356564

Batch 51280, train_perplexity=105.989204, train_loss=4.663337

Batch 51290, train_perplexity=104.69422, train_loss=4.651044

Batch 51300, train_perplexity=114.51442, train_loss=4.7407007

Batch 51310, train_perplexity=104.54974, train_loss=4.649663

Batch 51320, train_perplexity=102.71714, train_loss=4.631979

Batch 51330, train_perplexity=106.09028, train_loss=4.6642904

Batch 51340, train_perplexity=107.60948, train_loss=4.6785088

Batch 51350, train_perplexity=103.629684, train_loss=4.640824

Batch 51360, train_perplexity=108.22823, train_loss=4.6842422

Batch 51370, train_perplexity=119.47302, train_loss=4.7830906

Batch 51380, train_perplexity=121.60237, train_loss=4.8007565

Batch 51390, train_perplexity=108.07476, train_loss=4.682823

Batch 51400, train_perplexity=127.128105, train_loss=4.8451953

Batch 51410, train_perplexity=117.227646, train_loss=4.7641177

Batch 51420, train_perplexity=101.47486, train_loss=4.619811

Batch 51430, train_perplexity=111.865746, train_loss=4.7172995

Batch 51440, train_perplexity=109.18328, train_loss=4.693028

Batch 51450, train_perplexity=106.904816, train_loss=4.671939

Batch 51460, train_perplexity=118.6152, train_loss=4.7758846

Batch 51470, train_perplexity=106.50657, train_loss=4.6682067

Batch 51480, train_perplexity=111.87492, train_loss=4.7173815

Batch 51490, train_perplexity=91.58663, train_loss=4.5172853

Batch 51500, train_perplexity=101.56848, train_loss=4.6207333

Batch 51510, train_perplexity=113.42232, train_loss=4.731118

Batch 51520, train_perplexity=108.167046, train_loss=4.6836767

Batch 51530, train_perplexity=108.06981, train_loss=4.6827774

Batch 51540, train_perplexity=111.707436, train_loss=4.7158833

Batch 51550, train_perplexity=117.78066, train_loss=4.768824

Batch 51560, train_perplexity=118.36106, train_loss=4.77374

Batch 51570, train_perplexity=112.889015, train_loss=4.726405

Batch 51580, train_perplexity=112.29736, train_loss=4.7211504

Batch 51590, train_perplexity=111.27925, train_loss=4.712043

Batch 51600, train_perplexity=112.103584, train_loss=4.7194233

Batch 51610, train_perplexity=107.18489, train_loss=4.6745553

Batch 51620, train_perplexity=109.06948, train_loss=4.691985

Batch 51630, train_perplexity=110.17124, train_loss=4.702036

Batch 51640, train_perplexity=107.54592, train_loss=4.677918

Batch 51650, train_perplexity=107.69639, train_loss=4.679316

Batch 51660, train_perplexity=119.44796, train_loss=4.782881

Batch 51670, train_perplexity=102.67728, train_loss=4.631591

Batch 51680, train_perplexity=101.27604, train_loss=4.61785

Batch 51690, train_perplexity=109.364456, train_loss=4.694686

Batch 51700, train_perplexity=110.20382, train_loss=4.7023315

Batch 51710, train_perplexity=107.80331, train_loss=4.6803083

Batch 51720, train_perplexity=107.7699, train_loss=4.6799984

Batch 51730, train_perplexity=98.546776, train_loss=4.5905313

Batch 51740, train_perplexity=104.84759, train_loss=4.652508

Batch 51750, train_perplexity=105.98253, train_loss=4.6632743

Batch 51760, train_perplexity=97.10346, train_loss=4.575777

Batch 51770, train_perplexity=117.02005, train_loss=4.7623453

Batch 51780, train_perplexity=101.932274, train_loss=4.6243086

Batch 51790, train_perplexity=119.71327, train_loss=4.7850995

Batch 51800, train_perplexity=107.26731, train_loss=4.675324

Batch 51810, train_perplexity=113.2266, train_loss=4.729391

Batch 51820, train_perplexity=103.65701, train_loss=4.6410875

Batch 51830, train_perplexity=113.97885, train_loss=4.736013

Batch 51840, train_perplexity=107.069595, train_loss=4.673479

Batch 51850, train_perplexity=109.07365, train_loss=4.6920233

Batch 51860, train_perplexity=104.27541, train_loss=4.6470356

Batch 51870, train_perplexity=100.969124, train_loss=4.6148148

Batch 51880, train_perplexity=116.57539, train_loss=4.7585382

Batch 51890, train_perplexity=114.3082, train_loss=4.7388983

Batch 51900, train_perplexity=109.38683, train_loss=4.6948905

Batch 51910, train_perplexity=121.46392, train_loss=4.7996173

Batch 51920, train_perplexity=111.52158, train_loss=4.714218

Batch 51930, train_perplexity=114.46823, train_loss=4.7402973

Batch 51940, train_perplexity=109.07271, train_loss=4.6920147

Batch 51950, train_perplexity=114.21596, train_loss=4.738091

Batch 51960, train_perplexity=115.34616, train_loss=4.7479377

Batch 51970, train_perplexity=110.9571, train_loss=4.7091436

Batch 51980, train_perplexity=108.41749, train_loss=4.6859894

Batch 51990, train_perplexity=131.35042, train_loss=4.8778687

Batch 52000, train_perplexity=112.04416, train_loss=4.718893

Batch 52010, train_perplexity=115.523346, train_loss=4.7494726

Batch 52020, train_perplexity=108.327774, train_loss=4.6851616

Batch 52030, train_perplexity=117.835655, train_loss=4.769291

Batch 52040, train_perplexity=107.34115, train_loss=4.676012

Batch 52050, train_perplexity=110.579346, train_loss=4.7057333

Batch 52060, train_perplexity=119.505844, train_loss=4.7833652

Batch 52070, train_perplexity=111.14191, train_loss=4.710808

Batch 52080, train_perplexity=123.99615, train_loss=4.8202505

Batch 52090, train_perplexity=103.22464, train_loss=4.6369076

Batch 52100, train_perplexity=118.23223, train_loss=4.7726507

Batch 52110, train_perplexity=111.117584, train_loss=4.710589

Batch 52120, train_perplexity=109.68664, train_loss=4.6976275

Batch 52130, train_perplexity=119.47849, train_loss=4.7831364

Batch 52140, train_perplexity=111.66605, train_loss=4.7155128

Batch 52150, train_perplexity=115.85643, train_loss=4.7523518

Batch 52160, train_perplexity=116.11931, train_loss=4.754618

Batch 52170, train_perplexity=117.858925, train_loss=4.7694883

Batch 52180, train_perplexity=116.314316, train_loss=4.756296

Batch 52190, train_perplexity=107.026215, train_loss=4.673074

Batch 52200, train_perplexity=110.165886, train_loss=4.7019873

Batch 52210, train_perplexity=115.75427, train_loss=4.7514696

Batch 52220, train_perplexity=110.35058, train_loss=4.7036624

Batch 52230, train_perplexity=128.52101, train_loss=4.8560925

Batch 52240, train_perplexity=111.576584, train_loss=4.714711

Batch 52250, train_perplexity=114.44689, train_loss=4.740111

Batch 52260, train_perplexity=111.663605, train_loss=4.715491

Batch 52270, train_perplexity=110.92562, train_loss=4.70886

Batch 52280, train_perplexity=110.662895, train_loss=4.7064886

Batch 52290, train_perplexity=93.9483, train_loss=4.5427446

Batch 52300, train_perplexity=111.58233, train_loss=4.7147627
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52310, train_perplexity=106.026764, train_loss=4.6636915

Batch 52320, train_perplexity=109.48243, train_loss=4.695764

Batch 52330, train_perplexity=126.666, train_loss=4.8415537

Batch 52340, train_perplexity=94.18917, train_loss=4.5453053

Batch 52350, train_perplexity=110.531, train_loss=4.705296

Batch 52360, train_perplexity=112.37536, train_loss=4.7218447

Batch 52370, train_perplexity=99.57519, train_loss=4.600913

Batch 52380, train_perplexity=111.92241, train_loss=4.717806

Batch 52390, train_perplexity=115.365685, train_loss=4.748107

Batch 52400, train_perplexity=123.775566, train_loss=4.81847

Batch 52410, train_perplexity=126.324, train_loss=4.83885

Batch 52420, train_perplexity=107.48133, train_loss=4.677317

Batch 52430, train_perplexity=110.25338, train_loss=4.702781

Batch 52440, train_perplexity=118.270454, train_loss=4.772974

Batch 52450, train_perplexity=109.04566, train_loss=4.6917667

Batch 52460, train_perplexity=105.25544, train_loss=4.65639

Batch 52470, train_perplexity=111.42909, train_loss=4.7133884

Batch 52480, train_perplexity=106.16427, train_loss=4.6649876

Batch 52490, train_perplexity=97.35531, train_loss=4.578367

Batch 52500, train_perplexity=109.5721, train_loss=4.696583

Batch 52510, train_perplexity=108.230194, train_loss=4.6842604

Batch 52520, train_perplexity=110.37684, train_loss=4.7039003

Batch 52530, train_perplexity=97.781, train_loss=4.5827303

Batch 52540, train_perplexity=101.650024, train_loss=4.621536

Batch 52550, train_perplexity=105.73615, train_loss=4.660947

Batch 52560, train_perplexity=111.26774, train_loss=4.7119393

Batch 52570, train_perplexity=115.39947, train_loss=4.7483997

Batch 52580, train_perplexity=119.71225, train_loss=4.785091

Batch 52590, train_perplexity=111.89728, train_loss=4.7175813

Batch 52600, train_perplexity=113.51625, train_loss=4.731946

Batch 52610, train_perplexity=114.55041, train_loss=4.741015

Batch 52620, train_perplexity=118.44672, train_loss=4.774463

Batch 52630, train_perplexity=96.71106, train_loss=4.5717278

Batch 52640, train_perplexity=107.12067, train_loss=4.673956

Batch 52650, train_perplexity=118.138115, train_loss=4.7718544

Batch 52660, train_perplexity=102.84839, train_loss=4.633256

Batch 52670, train_perplexity=111.15081, train_loss=4.710888

Batch 52680, train_perplexity=105.85662, train_loss=4.6620855

Batch 52690, train_perplexity=98.541374, train_loss=4.5904765

Batch 52700, train_perplexity=113.839966, train_loss=4.7347937

Batch 52710, train_perplexity=109.33901, train_loss=4.6944532

Batch 52720, train_perplexity=110.59965, train_loss=4.705917

Batch 52730, train_perplexity=117.92734, train_loss=4.7700686

Batch 52740, train_perplexity=116.5436, train_loss=4.7582655

Batch 52750, train_perplexity=111.04136, train_loss=4.709903

Batch 52760, train_perplexity=111.28514, train_loss=4.7120957

Batch 52770, train_perplexity=107.73177, train_loss=4.6796446

Batch 52780, train_perplexity=124.21096, train_loss=4.8219814

Batch 52790, train_perplexity=103.7767, train_loss=4.6422415

Batch 52800, train_perplexity=112.26803, train_loss=4.720889

Batch 52810, train_perplexity=111.16692, train_loss=4.711033

Batch 52820, train_perplexity=115.72961, train_loss=4.7512565

Batch 52830, train_perplexity=104.24623, train_loss=4.6467557

Batch 52840, train_perplexity=109.033554, train_loss=4.6916556

Batch 52850, train_perplexity=109.61913, train_loss=4.697012

Batch 52860, train_perplexity=111.82756, train_loss=4.716958

Batch 52870, train_perplexity=125.02511, train_loss=4.8285146

Batch 52880, train_perplexity=117.15092, train_loss=4.763463

Batch 52890, train_perplexity=110.95562, train_loss=4.7091303

Batch 52900, train_perplexity=113.14003, train_loss=4.7286263

Batch 52910, train_perplexity=103.138535, train_loss=4.636073

Batch 52920, train_perplexity=113.52816, train_loss=4.732051

Batch 52930, train_perplexity=119.7497, train_loss=4.7854037

Batch 52940, train_perplexity=114.60684, train_loss=4.7415075

Batch 52950, train_perplexity=95.362816, train_loss=4.5576887

Batch 52960, train_perplexity=102.11451, train_loss=4.626095

Batch 52970, train_perplexity=108.23824, train_loss=4.6843348

Batch 52980, train_perplexity=108.76949, train_loss=4.689231

Batch 52990, train_perplexity=112.55501, train_loss=4.723442

Batch 53000, train_perplexity=111.69582, train_loss=4.7157793

Batch 53010, train_perplexity=116.19536, train_loss=4.755273

Batch 53020, train_perplexity=110.61204, train_loss=4.706029

Batch 53030, train_perplexity=112.53022, train_loss=4.723222

Batch 53040, train_perplexity=107.37933, train_loss=4.6763678

Batch 53050, train_perplexity=108.20981, train_loss=4.684072

Batch 53060, train_perplexity=122.44507, train_loss=4.8076625

Batch 53070, train_perplexity=101.1562, train_loss=4.616666

Batch 53080, train_perplexity=104.9029, train_loss=4.653035

Batch 53090, train_perplexity=106.95306, train_loss=4.67239

Batch 53100, train_perplexity=110.02319, train_loss=4.700691

Batch 53110, train_perplexity=108.985466, train_loss=4.6912146

Batch 53120, train_perplexity=98.52479, train_loss=4.590308

Batch 53130, train_perplexity=103.38666, train_loss=4.638476

Batch 53140, train_perplexity=110.34811, train_loss=4.70364

Batch 53150, train_perplexity=124.30754, train_loss=4.8227587

Batch 53160, train_perplexity=111.363335, train_loss=4.712798

Batch 53170, train_perplexity=113.116295, train_loss=4.7284164

Batch 53180, train_perplexity=102.594864, train_loss=4.630788

Batch 53190, train_perplexity=111.75837, train_loss=4.716339

Batch 53200, train_perplexity=115.21835, train_loss=4.746829

Batch 53210, train_perplexity=103.45742, train_loss=4.63916

Batch 53220, train_perplexity=104.39282, train_loss=4.648161

Batch 53230, train_perplexity=107.99512, train_loss=4.682086

Batch 53240, train_perplexity=117.54653, train_loss=4.7668343

Batch 53250, train_perplexity=109.485664, train_loss=4.6957936

Batch 53260, train_perplexity=117.73467, train_loss=4.7684336

Batch 53270, train_perplexity=110.91923, train_loss=4.708802

Batch 53280, train_perplexity=115.65325, train_loss=4.7505965

Batch 53290, train_perplexity=101.13377, train_loss=4.616444

Batch 53300, train_perplexity=103.83214, train_loss=4.6427755

Batch 53310, train_perplexity=128.668, train_loss=4.8572354

Batch 53320, train_perplexity=119.661285, train_loss=4.784665

Batch 53330, train_perplexity=116.20987, train_loss=4.755398

Batch 53340, train_perplexity=106.89024, train_loss=4.6718025

Batch 53350, train_perplexity=112.41952, train_loss=4.7222376

Batch 53360, train_perplexity=108.81359, train_loss=4.689636

Batch 53370, train_perplexity=90.02134, train_loss=4.5000467

Batch 53380, train_perplexity=124.65228, train_loss=4.825528

Batch 53390, train_perplexity=123.14319, train_loss=4.813348

Batch 53400, train_perplexity=111.84697, train_loss=4.7171316

Batch 53410, train_perplexity=109.22541, train_loss=4.6934137

Batch 53420, train_perplexity=117.066154, train_loss=4.762739

Batch 53430, train_perplexity=118.5582, train_loss=4.775404

Batch 53440, train_perplexity=118.67947, train_loss=4.7764263

Batch 53450, train_perplexity=112.7378, train_loss=4.7250648

Batch 53460, train_perplexity=112.31268, train_loss=4.721287

Batch 53470, train_perplexity=108.89155, train_loss=4.6903524

Batch 53480, train_perplexity=119.68474, train_loss=4.784861

Batch 53490, train_perplexity=104.55503, train_loss=4.6497135

Batch 53500, train_perplexity=110.56711, train_loss=4.7056227

Batch 53510, train_perplexity=114.783005, train_loss=4.7430434

Batch 53520, train_perplexity=128.82657, train_loss=4.858467

Batch 53530, train_perplexity=105.01856, train_loss=4.654137

Batch 53540, train_perplexity=108.92749, train_loss=4.6906824

Batch 53550, train_perplexity=107.37765, train_loss=4.676352

Batch 53560, train_perplexity=103.72139, train_loss=4.6417084

Batch 53570, train_perplexity=120.52586, train_loss=4.7918644

Batch 53580, train_perplexity=111.97152, train_loss=4.7182446

Batch 53590, train_perplexity=101.260925, train_loss=4.6177006

Batch 53600, train_perplexity=107.5187, train_loss=4.6776648

Batch 53610, train_perplexity=123.90767, train_loss=4.8195367

Batch 53620, train_perplexity=112.68873, train_loss=4.7246294
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53630, train_perplexity=107.71955, train_loss=4.679531

Batch 53640, train_perplexity=106.85763, train_loss=4.6714973

Batch 53650, train_perplexity=114.206314, train_loss=4.7380066

Batch 53660, train_perplexity=126.34954, train_loss=4.839052

Batch 53670, train_perplexity=113.619576, train_loss=4.732856

Batch 53680, train_perplexity=106.47595, train_loss=4.667919

Batch 53690, train_perplexity=107.78943, train_loss=4.6801796

Batch 53700, train_perplexity=108.013756, train_loss=4.6822586

Batch 53710, train_perplexity=103.12123, train_loss=4.6359053

Batch 53720, train_perplexity=108.86891, train_loss=4.6901445

Batch 53730, train_perplexity=115.93358, train_loss=4.7530174

Batch 53740, train_perplexity=109.49861, train_loss=4.695912

Batch 53750, train_perplexity=120.09938, train_loss=4.7883196

Batch 53760, train_perplexity=108.378876, train_loss=4.685633

Batch 53770, train_perplexity=117.90378, train_loss=4.769869

Batch 53780, train_perplexity=114.65861, train_loss=4.741959

Batch 53790, train_perplexity=120.91189, train_loss=4.795062

Batch 53800, train_perplexity=104.40248, train_loss=4.6482534

Batch 53810, train_perplexity=123.806145, train_loss=4.818717

Batch 53820, train_perplexity=115.53452, train_loss=4.7495694

Batch 53830, train_perplexity=127.562096, train_loss=4.8486032

Batch 53840, train_perplexity=120.40248, train_loss=4.79084

Batch 53850, train_perplexity=107.523415, train_loss=4.6777086

Batch 53860, train_perplexity=111.3043, train_loss=4.712268

Batch 53870, train_perplexity=118.54073, train_loss=4.7752566

Batch 53880, train_perplexity=119.009514, train_loss=4.7792034

Batch 53890, train_perplexity=118.33476, train_loss=4.7735176

Batch 53900, train_perplexity=111.287315, train_loss=4.7121153

Batch 53910, train_perplexity=115.404526, train_loss=4.7484436

Batch 53920, train_perplexity=100.51814, train_loss=4.610338

Batch 53930, train_perplexity=96.19241, train_loss=4.5663505

Batch 53940, train_perplexity=103.39139, train_loss=4.6385217

Batch 53950, train_perplexity=114.92962, train_loss=4.74432

Batch 53960, train_perplexity=112.9295, train_loss=4.7267637

Batch 53970, train_perplexity=122.292244, train_loss=4.8064137

Batch 53980, train_perplexity=109.90029, train_loss=4.6995735

Batch 53990, train_perplexity=116.86637, train_loss=4.761031

Batch 54000, train_perplexity=107.78049, train_loss=4.6800966

Batch 54010, train_perplexity=116.76867, train_loss=4.760195

Batch 54020, train_perplexity=121.41233, train_loss=4.7991924

Batch 54030, train_perplexity=122.43368, train_loss=4.8075695

Batch 54040, train_perplexity=113.644936, train_loss=4.733079

Batch 54050, train_perplexity=111.47283, train_loss=4.713781

Batch 54060, train_perplexity=122.775276, train_loss=4.8103557

Batch 54070, train_perplexity=125.07597, train_loss=4.8289213

Batch 54080, train_perplexity=106.11669, train_loss=4.6645393

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 54090, train_perplexity=114.56576, train_loss=4.741149

Batch 54100, train_perplexity=118.21317, train_loss=4.7724895

Batch 54110, train_perplexity=102.49775, train_loss=4.629841

Batch 54120, train_perplexity=117.91761, train_loss=4.769986

Batch 54130, train_perplexity=104.48984, train_loss=4.64909

Batch 54140, train_perplexity=114.3469, train_loss=4.739237

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 54150, train_perplexity=119.53001, train_loss=4.7835674

Batch 54160, train_perplexity=105.36964, train_loss=4.6574745

Batch 54170, train_perplexity=112.164536, train_loss=4.719967

Batch 54180, train_perplexity=103.8359, train_loss=4.642812

Batch 54190, train_perplexity=101.46736, train_loss=4.619737

Batch 54200, train_perplexity=123.93066, train_loss=4.819722

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 54210, train_perplexity=117.64085, train_loss=4.7676363

Batch 54220, train_perplexity=98.0547, train_loss=4.5855255

Batch 54230, train_perplexity=107.25314, train_loss=4.675192

Batch 54240, train_perplexity=105.202156, train_loss=4.655884

Batch 54250, train_perplexity=94.87524, train_loss=4.5525627

Batch 54260, train_perplexity=103.2589, train_loss=4.6372395

Batch 54270, train_perplexity=89.24632, train_loss=4.4914002

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 54280, train_perplexity=109.35794, train_loss=4.6946263

Batch 54290, train_perplexity=113.48854, train_loss=4.731702

Batch 54300, train_perplexity=110.71425, train_loss=4.7069526

Batch 54310, train_perplexity=114.34903, train_loss=4.7392554

Batch 54320, train_perplexity=100.31235, train_loss=4.608289

Batch 54330, train_perplexity=112.84165, train_loss=4.7259855

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 54340, train_perplexity=117.79555, train_loss=4.7689505

Batch 54350, train_perplexity=102.015076, train_loss=4.6251206

Batch 54360, train_perplexity=115.53332, train_loss=4.749559

Batch 54370, train_perplexity=103.47572, train_loss=4.639337

Batch 54380, train_perplexity=112.89149, train_loss=4.726427

Batch 54390, train_perplexity=106.39353, train_loss=4.667145

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 54400, train_perplexity=101.01901, train_loss=4.615309

Batch 54410, train_perplexity=102.96007, train_loss=4.6343412

Batch 54420, train_perplexity=104.65663, train_loss=4.650685

Batch 54430, train_perplexity=103.60316, train_loss=4.640568

Batch 54440, train_perplexity=106.175705, train_loss=4.6650953

Batch 54450, train_perplexity=116.49155, train_loss=4.7578187

Batch 54460, train_perplexity=110.91859, train_loss=4.7087965

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Batch 54470, train_perplexity=101.13527, train_loss=4.616459

Batch 54480, train_perplexity=110.25123, train_loss=4.7027617

Batch 54490, train_perplexity=109.45554, train_loss=4.6955185

Batch 54500, train_perplexity=107.22584, train_loss=4.6749372

Batch 54510, train_perplexity=114.69251, train_loss=4.7422547

Batch 54520, train_perplexity=107.4367, train_loss=4.676902

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 54530, train_perplexity=109.2051, train_loss=4.693228

Batch 54540, train_perplexity=107.63134, train_loss=4.678712

Batch 54550, train_perplexity=112.66176, train_loss=4.72439

Batch 54560, train_perplexity=98.78004, train_loss=4.5928955

Batch 54570, train_perplexity=108.896065, train_loss=4.690394

Batch 54580, train_perplexity=108.33573, train_loss=4.685235

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 54590, train_perplexity=109.24656, train_loss=4.6936073

Batch 54600, train_perplexity=116.43129, train_loss=4.7573013

Batch 54610, train_perplexity=105.67445, train_loss=4.660363

Batch 54620, train_perplexity=99.96768, train_loss=4.604847

Batch 54630, train_perplexity=112.27038, train_loss=4.72091

Batch 54640, train_perplexity=110.56352, train_loss=4.7055902

Batch 54650, train_perplexity=110.34064, train_loss=4.7035723

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 54660, train_perplexity=105.40421, train_loss=4.6578026

Batch 54670, train_perplexity=102.91585, train_loss=4.6339116

Batch 54680, train_perplexity=98.99704, train_loss=4.59509

Batch 54690, train_perplexity=101.611595, train_loss=4.6211576

Batch 54700, train_perplexity=110.88369, train_loss=4.708482

Batch 54710, train_perplexity=101.32473, train_loss=4.6183305

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 54720, train_perplexity=102.19371, train_loss=4.62687

Batch 54730, train_perplexity=115.472786, train_loss=4.749035

Batch 54740, train_perplexity=110.90886, train_loss=4.708709

Batch 54750, train_perplexity=111.07674, train_loss=4.7102213

Batch 54760, train_perplexity=105.48586, train_loss=4.658577

Batch 54770, train_perplexity=108.28042, train_loss=4.6847243

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 54780, train_perplexity=105.45287, train_loss=4.658264

Batch 54790, train_perplexity=104.20881, train_loss=4.6463966

Batch 54800, train_perplexity=107.442024, train_loss=4.6769514

Batch 54810, train_perplexity=113.21753, train_loss=4.729311

Batch 54820, train_perplexity=109.31482, train_loss=4.694232

Batch 54830, train_perplexity=110.15464, train_loss=4.701885

Batch 54840, train_perplexity=113.8315, train_loss=4.7347193

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 54850, train_perplexity=104.95854, train_loss=4.6535654

Batch 54860, train_perplexity=96.24917, train_loss=4.5669403

Batch 54870, train_perplexity=108.584236, train_loss=4.687526

Batch 54880, train_perplexity=110.4909, train_loss=4.704933

Batch 54890, train_perplexity=108.9198, train_loss=4.690612

Batch 54900, train_perplexity=103.762054, train_loss=4.6421003

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 54910, train_perplexity=109.696785, train_loss=4.69772

Batch 54920, train_perplexity=107.445404, train_loss=4.676983

Batch 54930, train_perplexity=101.654724, train_loss=4.621582

Batch 54940, train_perplexity=110.91637, train_loss=4.7087765

Batch 54950, train_perplexity=117.95518, train_loss=4.7703047

Batch 54960, train_perplexity=108.01922, train_loss=4.682309

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 54970, train_perplexity=98.032265, train_loss=4.5852966

Batch 54980, train_perplexity=104.50927, train_loss=4.649276

Batch 54990, train_perplexity=109.70578, train_loss=4.697802

Batch 55000, train_perplexity=102.45231, train_loss=4.6293974

Batch 55010, train_perplexity=112.58544, train_loss=4.7237124

Batch 55020, train_perplexity=112.663795, train_loss=4.724408

Batch 55030, train_perplexity=92.03467, train_loss=4.5221653

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 55040, train_perplexity=86.10992, train_loss=4.4556246

Batch 55050, train_perplexity=113.55371, train_loss=4.732276

Batch 55060, train_perplexity=118.14093, train_loss=4.7718782

Batch 55070, train_perplexity=113.7269, train_loss=4.7338

Batch 55080, train_perplexity=126.12718, train_loss=4.837291

Batch 55090, train_perplexity=119.96831, train_loss=4.7872276

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 55100, train_perplexity=93.3363, train_loss=4.536209

Batch 55110, train_perplexity=105.94191, train_loss=4.662891

Batch 55120, train_perplexity=109.88567, train_loss=4.6994405

Batch 55130, train_perplexity=108.71318, train_loss=4.688713

Batch 55140, train_perplexity=119.07695, train_loss=4.77977

Batch 55150, train_perplexity=107.04413, train_loss=4.673241

Batch 55160, train_perplexity=108.944214, train_loss=4.690836

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 55170, train_perplexity=110.306175, train_loss=4.70326

Batch 55180, train_perplexity=108.11687, train_loss=4.6832128

Batch 55190, train_perplexity=97.130974, train_loss=4.5760603

Batch 55200, train_perplexity=101.665, train_loss=4.621683

Batch 55210, train_perplexity=115.73374, train_loss=4.751292

Batch 55220, train_perplexity=102.22022, train_loss=4.6271296

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 55230, train_perplexity=116.340385, train_loss=4.7565203

Batch 55240, train_perplexity=114.00771, train_loss=4.736266

Batch 55250, train_perplexity=108.7737, train_loss=4.6892695

Batch 55260, train_perplexity=105.8325, train_loss=4.6618576

Batch 55270, train_perplexity=103.72278, train_loss=4.6417217

Batch 55280, train_perplexity=102.77201, train_loss=4.632513

Batch 55290, train_perplexity=104.376, train_loss=4.648

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 55300, train_perplexity=98.567696, train_loss=4.5907435

Batch 55310, train_perplexity=109.610565, train_loss=4.6969337

Batch 55320, train_perplexity=119.952126, train_loss=4.7870927

Batch 55330, train_perplexity=99.09938, train_loss=4.596123

Batch 55340, train_perplexity=108.78241, train_loss=4.6893497

Batch 55350, train_perplexity=105.646545, train_loss=4.660099

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 55360, train_perplexity=108.50127, train_loss=4.686762

Batch 55370, train_perplexity=104.91556, train_loss=4.653156

Batch 55380, train_perplexity=103.08042, train_loss=4.6355095

Batch 55390, train_perplexity=110.92864, train_loss=4.708887

Batch 55400, train_perplexity=124.93101, train_loss=4.8277617

Batch 55410, train_perplexity=101.86444, train_loss=4.623643

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 55420, train_perplexity=105.14368, train_loss=4.655328

Batch 55430, train_perplexity=93.88117, train_loss=4.54203

Batch 55440, train_perplexity=108.45326, train_loss=4.6863194

Batch 55450, train_perplexity=101.38992, train_loss=4.6189737

Batch 55460, train_perplexity=99.03183, train_loss=4.5954413

Batch 55470, train_perplexity=95.79852, train_loss=4.5622473

Batch 55480, train_perplexity=108.974556, train_loss=4.6911144

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 55490, train_perplexity=117.74236, train_loss=4.768499

Batch 55500, train_perplexity=120.00939, train_loss=4.78757

Batch 55510, train_perplexity=121.04145, train_loss=4.796133

Batch 55520, train_perplexity=104.14373, train_loss=4.645772

Batch 55530, train_perplexity=97.53392, train_loss=4.5802

Batch 55540, train_perplexity=96.666794, train_loss=4.57127

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 55550, train_perplexity=106.24631, train_loss=4.66576

Batch 55560, train_perplexity=103.66443, train_loss=4.641159

Batch 55570, train_perplexity=116.709274, train_loss=4.759686

Batch 55580, train_perplexity=108.78625, train_loss=4.689385

Batch 55590, train_perplexity=123.16644, train_loss=4.8135366

Batch 55600, train_perplexity=99.12599, train_loss=4.5963917

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 55610, train_perplexity=117.59205, train_loss=4.7672215

Batch 55620, train_perplexity=105.73544, train_loss=4.66094

Batch 55630, train_perplexity=106.451126, train_loss=4.667686

Batch 55640, train_perplexity=107.95953, train_loss=4.6817565

Batch 55650, train_perplexity=119.43366, train_loss=4.782761

Batch 55660, train_perplexity=108.57341, train_loss=4.6874266

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 55670, train_perplexity=100.88452, train_loss=4.6139765

Batch 55680, train_perplexity=115.18852, train_loss=4.74657

Batch 55690, train_perplexity=111.96703, train_loss=4.7182045

Batch 55700, train_perplexity=109.38016, train_loss=4.6948295

Batch 55710, train_perplexity=110.06234, train_loss=4.701047

Batch 55720, train_perplexity=114.75931, train_loss=4.742837

Batch 55730, train_perplexity=108.21027, train_loss=4.6840763

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 55740, train_perplexity=102.35289, train_loss=4.6284266
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55750, train_perplexity=105.768875, train_loss=4.6612563

Batch 55760, train_perplexity=108.62945, train_loss=4.6879425

Batch 55770, train_perplexity=104.01733, train_loss=4.6445575

Batch 55780, train_perplexity=103.37685, train_loss=4.638381

Batch 55790, train_perplexity=115.08525, train_loss=4.745673

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 55800, train_perplexity=113.1437, train_loss=4.7286587

Batch 55810, train_perplexity=98.61005, train_loss=4.591173

Batch 55820, train_perplexity=116.04049, train_loss=4.753939

Batch 55830, train_perplexity=122.27172, train_loss=4.806246

Batch 55840, train_perplexity=112.92185, train_loss=4.726696

Batch 55850, train_perplexity=113.82607, train_loss=4.7346716

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 55860, train_perplexity=110.75179, train_loss=4.7072916

Batch 55870, train_perplexity=113.35614, train_loss=4.7305346

Batch 55880, train_perplexity=104.61557, train_loss=4.6502924

Batch 55890, train_perplexity=103.559, train_loss=4.6401415

Batch 55900, train_perplexity=105.05708, train_loss=4.654504

Batch 55910, train_perplexity=105.12498, train_loss=4.65515

Batch 55920, train_perplexity=101.45299, train_loss=4.6195955

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 55930, train_perplexity=118.29426, train_loss=4.7731752

Batch 55940, train_perplexity=112.254105, train_loss=4.720765

Batch 55950, train_perplexity=98.234924, train_loss=4.587362

Batch 55960, train_perplexity=114.98564, train_loss=4.7448072

Batch 55970, train_perplexity=118.96877, train_loss=4.778861

Batch 55980, train_perplexity=106.22706, train_loss=4.665579

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 55990, train_perplexity=116.49532, train_loss=4.757851

Batch 56000, train_perplexity=112.17315, train_loss=4.7200437

Batch 56010, train_perplexity=106.37385, train_loss=4.66696

Batch 56020, train_perplexity=107.58013, train_loss=4.678236

Batch 56030, train_perplexity=108.83217, train_loss=4.689807

Batch 56040, train_perplexity=112.39122, train_loss=4.721986

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 56050, train_perplexity=103.1291, train_loss=4.6359816

Batch 56060, train_perplexity=109.7299, train_loss=4.698022

Batch 56070, train_perplexity=100.522934, train_loss=4.610386

Batch 56080, train_perplexity=100.06798, train_loss=4.6058497

Batch 56090, train_perplexity=107.03846, train_loss=4.673188

Batch 56100, train_perplexity=110.33385, train_loss=4.7035108

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 56110, train_perplexity=93.32259, train_loss=4.5360622

Batch 56120, train_perplexity=103.75656, train_loss=4.6420474

Batch 56130, train_perplexity=121.15613, train_loss=4.79708

Batch 56140, train_perplexity=110.901985, train_loss=4.708647

Batch 56150, train_perplexity=109.93782, train_loss=4.699915

Batch 56160, train_perplexity=105.242, train_loss=4.6562624

Batch 56170, train_perplexity=103.47222, train_loss=4.639303

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 56180, train_perplexity=115.24242, train_loss=4.747038

Batch 56190, train_perplexity=96.37573, train_loss=4.5682545

Batch 56200, train_perplexity=104.39919, train_loss=4.648222

Batch 56210, train_perplexity=106.37608, train_loss=4.6669807

Batch 56220, train_perplexity=95.016945, train_loss=4.554055

Batch 56230, train_perplexity=105.34005, train_loss=4.6571937

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 56240, train_perplexity=101.62996, train_loss=4.6213384

Batch 56250, train_perplexity=101.887856, train_loss=4.6238728

Batch 56260, train_perplexity=110.05311, train_loss=4.700963

Batch 56270, train_perplexity=113.8214, train_loss=4.7346306

Batch 56280, train_perplexity=121.83767, train_loss=4.8026896

Batch 56290, train_perplexity=111.144554, train_loss=4.7108316

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 56300, train_perplexity=121.08221, train_loss=4.7964697

Batch 56310, train_perplexity=100.11709, train_loss=4.6063404

Batch 56320, train_perplexity=117.283165, train_loss=4.764591

Batch 56330, train_perplexity=91.26046, train_loss=4.5137177

Batch 56340, train_perplexity=105.018715, train_loss=4.6541386

Batch 56350, train_perplexity=113.5025, train_loss=4.731825

Batch 56360, train_perplexity=107.30891, train_loss=4.6757116

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 56370, train_perplexity=105.14438, train_loss=4.6553345

Batch 56380, train_perplexity=104.64905, train_loss=4.6506124

Batch 56390, train_perplexity=116.038994, train_loss=4.7539263

Batch 56400, train_perplexity=106.905785, train_loss=4.671948

Batch 56410, train_perplexity=102.82534, train_loss=4.633032

Batch 56420, train_perplexity=121.844986, train_loss=4.8027496

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 56430, train_perplexity=108.61981, train_loss=4.687854

Batch 56440, train_perplexity=104.71099, train_loss=4.651204

Batch 56450, train_perplexity=112.083916, train_loss=4.719248

Batch 56460, train_perplexity=111.83364, train_loss=4.7170124

Batch 56470, train_perplexity=102.1694, train_loss=4.626632

Batch 56480, train_perplexity=105.12273, train_loss=4.6551285

Batch 56490, train_perplexity=107.64972, train_loss=4.6788826

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 56500, train_perplexity=103.89207, train_loss=4.6433525

Batch 56510, train_perplexity=107.87072, train_loss=4.6809335

Batch 56520, train_perplexity=117.61134, train_loss=4.7673855

Batch 56530, train_perplexity=108.59645, train_loss=4.6876388

Batch 56540, train_perplexity=108.95502, train_loss=4.690935

Batch 56550, train_perplexity=122.67351, train_loss=4.8095264

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 56560, train_perplexity=112.57761, train_loss=4.723643

Batch 56570, train_perplexity=119.73183, train_loss=4.7852545

Batch 56580, train_perplexity=109.92409, train_loss=4.69979

Batch 56590, train_perplexity=99.898254, train_loss=4.604152

Batch 56600, train_perplexity=115.36073, train_loss=4.748064

Batch 56610, train_perplexity=105.10799, train_loss=4.6549883

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 56620, train_perplexity=103.54438, train_loss=4.6400003

Batch 56630, train_perplexity=110.23661, train_loss=4.702629

Batch 56640, train_perplexity=107.324974, train_loss=4.6758614

Batch 56650, train_perplexity=113.05687, train_loss=4.727891

Batch 56660, train_perplexity=100.70755, train_loss=4.612221

Batch 56670, train_perplexity=106.64048, train_loss=4.669463

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 56680, train_perplexity=112.256996, train_loss=4.720791

Batch 56690, train_perplexity=103.36876, train_loss=4.638303

Batch 56700, train_perplexity=120.37171, train_loss=4.7905846

Batch 56710, train_perplexity=109.24776, train_loss=4.6936183

Batch 56720, train_perplexity=119.06219, train_loss=4.779646

Batch 56730, train_perplexity=112.01147, train_loss=4.718601

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 56740, train_perplexity=101.010635, train_loss=4.615226

Batch 56750, train_perplexity=107.86661, train_loss=4.6808953

Batch 56760, train_perplexity=106.64012, train_loss=4.66946

Batch 56770, train_perplexity=105.962875, train_loss=4.663089

Batch 56780, train_perplexity=107.1601, train_loss=4.674324

Batch 56790, train_perplexity=98.338455, train_loss=4.588415

Batch 56800, train_perplexity=127.80648, train_loss=4.8505173

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 56810, train_perplexity=115.39022, train_loss=4.7483196

Batch 56820, train_perplexity=111.27225, train_loss=4.71198

Batch 56830, train_perplexity=131.84488, train_loss=4.881626

Batch 56840, train_perplexity=104.7062, train_loss=4.6511583

Batch 56850, train_perplexity=104.93597, train_loss=4.6533504

Batch 56860, train_perplexity=107.50193, train_loss=4.677509

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 56870, train_perplexity=107.13901, train_loss=4.674127

Batch 56880, train_perplexity=110.979004, train_loss=4.709341

Batch 56890, train_perplexity=107.62714, train_loss=4.678673

Batch 56900, train_perplexity=108.80518, train_loss=4.689559

Batch 56910, train_perplexity=115.38929, train_loss=4.7483115

Batch 56920, train_perplexity=113.04652, train_loss=4.7277994

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 56930, train_perplexity=112.56703, train_loss=4.723549

Batch 56940, train_perplexity=114.69431, train_loss=4.7422705

Batch 56950, train_perplexity=106.62502, train_loss=4.669318

Batch 56960, train_perplexity=106.73022, train_loss=4.6703043

Batch 56970, train_perplexity=112.55936, train_loss=4.7234807

Batch 56980, train_perplexity=112.65026, train_loss=4.724288

Batch 56990, train_perplexity=99.1396, train_loss=4.596529

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 57000, train_perplexity=103.46892, train_loss=4.6392713

Batch 57010, train_perplexity=110.16967, train_loss=4.7020216

Batch 57020, train_perplexity=105.87192, train_loss=4.66223
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 57030, train_perplexity=103.78348, train_loss=4.642307

Batch 57040, train_perplexity=101.67538, train_loss=4.621785

Batch 57050, train_perplexity=99.34778, train_loss=4.5986266

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 57060, train_perplexity=107.82336, train_loss=4.6804943

Batch 57070, train_perplexity=108.807724, train_loss=4.6895823

Batch 57080, train_perplexity=114.682396, train_loss=4.7421665

Batch 57090, train_perplexity=97.536804, train_loss=4.5802298

Batch 57100, train_perplexity=114.72375, train_loss=4.742527

Batch 57110, train_perplexity=114.696014, train_loss=4.7422853

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 57120, train_perplexity=118.24564, train_loss=4.772764

Batch 57130, train_perplexity=107.11117, train_loss=4.673867

Batch 57140, train_perplexity=108.24774, train_loss=4.6844225

Batch 57150, train_perplexity=117.80341, train_loss=4.769017

Batch 57160, train_perplexity=104.926414, train_loss=4.6532593

Batch 57170, train_perplexity=106.254814, train_loss=4.66584

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 57180, train_perplexity=101.18283, train_loss=4.616929

Batch 57190, train_perplexity=105.31559, train_loss=4.6569614

Batch 57200, train_perplexity=114.21187, train_loss=4.738055

Batch 57210, train_perplexity=104.98011, train_loss=4.653771

Batch 57220, train_perplexity=107.63134, train_loss=4.678712

Batch 57230, train_perplexity=108.15487, train_loss=4.683564

Batch 57240, train_perplexity=120.79173, train_loss=4.794068

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 57250, train_perplexity=91.52512, train_loss=4.5166135

Batch 57260, train_perplexity=117.845604, train_loss=4.7693753

Batch 57270, train_perplexity=110.54344, train_loss=4.7054086

Batch 57280, train_perplexity=102.27805, train_loss=4.627695

Batch 57290, train_perplexity=106.9707, train_loss=4.672555

Batch 57300, train_perplexity=96.2499, train_loss=4.566948

Batch 57310, train_perplexity=105.660904, train_loss=4.660235

Batch 57320, train_perplexity=99.486824, train_loss=4.600025

Batch 57330, train_perplexity=104.58285, train_loss=4.6499796

Batch 57340, train_perplexity=110.55477, train_loss=4.705511

Batch 57350, train_perplexity=101.287, train_loss=4.617958

Batch 57360, train_perplexity=105.67718, train_loss=4.660389

Batch 57370, train_perplexity=103.97116, train_loss=4.6441135

Batch 57380, train_perplexity=95.212234, train_loss=4.5561085

Batch 57390, train_perplexity=99.31174, train_loss=4.5982637

Batch 57400, train_perplexity=110.786446, train_loss=4.7076044

Batch 57410, train_perplexity=93.38109, train_loss=4.536689

Batch 57420, train_perplexity=98.21474, train_loss=4.5871563

Batch 57430, train_perplexity=111.7718, train_loss=4.7164593

Batch 57440, train_perplexity=92.26808, train_loss=4.5246983

Batch 57450, train_perplexity=104.83379, train_loss=4.652376

Batch 57460, train_perplexity=103.04121, train_loss=4.635129

Batch 57470, train_perplexity=99.5337, train_loss=4.6004963

Batch 57480, train_perplexity=100.062965, train_loss=4.6057997

Batch 57490, train_perplexity=125.13927, train_loss=4.8294272

Batch 57500, train_perplexity=104.39272, train_loss=4.64816

Batch 57510, train_perplexity=98.28192, train_loss=4.58784

Batch 57520, train_perplexity=98.695854, train_loss=4.592043

Batch 57530, train_perplexity=106.94709, train_loss=4.672334

Batch 57540, train_perplexity=111.82223, train_loss=4.7169104

Batch 57550, train_perplexity=100.78422, train_loss=4.612982

Batch 57560, train_perplexity=108.658356, train_loss=4.6882086

Batch 57570, train_perplexity=99.5012, train_loss=4.6001697

Batch 57580, train_perplexity=111.60249, train_loss=4.7149434

Batch 57590, train_perplexity=102.48534, train_loss=4.6297197

Batch 57600, train_perplexity=101.431076, train_loss=4.6193795

Batch 57610, train_perplexity=108.580765, train_loss=4.6874943

Batch 57620, train_perplexity=106.25228, train_loss=4.6658163

Batch 57630, train_perplexity=92.54338, train_loss=4.5276775

Batch 57640, train_perplexity=103.7619, train_loss=4.642099

Batch 57650, train_perplexity=105.65914, train_loss=4.6602182

Batch 57660, train_perplexity=112.62448, train_loss=4.724059

Batch 57670, train_perplexity=109.72833, train_loss=4.6980076

Batch 57680, train_perplexity=115.120926, train_loss=4.745983

Batch 57690, train_perplexity=108.2465, train_loss=4.684411

Batch 57700, train_perplexity=113.514084, train_loss=4.731927

Batch 57710, train_perplexity=100.738625, train_loss=4.6125293

Batch 57720, train_perplexity=94.98899, train_loss=4.553761

Batch 57730, train_perplexity=105.02432, train_loss=4.654192

Batch 57740, train_perplexity=110.91299, train_loss=4.708746

Batch 57750, train_perplexity=101.176315, train_loss=4.6168647

Batch 57760, train_perplexity=116.98457, train_loss=4.762042

Batch 57770, train_perplexity=94.520226, train_loss=4.548814

Batch 57780, train_perplexity=112.54664, train_loss=4.7233677

Batch 57790, train_perplexity=91.55908, train_loss=4.5169845

Batch 57800, train_perplexity=104.643906, train_loss=4.6505632

Batch 57810, train_perplexity=105.60076, train_loss=4.6596656

Batch 57820, train_perplexity=98.503746, train_loss=4.5900946

Batch 57830, train_perplexity=95.9197, train_loss=4.5635114

Batch 57840, train_perplexity=109.01, train_loss=4.6914396

Batch 57850, train_perplexity=100.276474, train_loss=4.607931

Batch 57860, train_perplexity=106.02833, train_loss=4.6637063

Batch 57870, train_perplexity=92.94507, train_loss=4.5320086

Batch 57880, train_perplexity=116.35459, train_loss=4.7566423

Batch 57890, train_perplexity=110.46435, train_loss=4.704693

Batch 57900, train_perplexity=105.078926, train_loss=4.6547117

Batch 57910, train_perplexity=106.134094, train_loss=4.6647034

Batch 57920, train_perplexity=110.38642, train_loss=4.703987

Batch 57930, train_perplexity=105.438896, train_loss=4.6581316

Batch 57940, train_perplexity=118.265945, train_loss=4.772936

Batch 57950, train_perplexity=110.685165, train_loss=4.70669

Batch 57960, train_perplexity=99.50874, train_loss=4.6002455

Batch 57970, train_perplexity=116.73693, train_loss=4.759923

Batch 57980, train_perplexity=102.663666, train_loss=4.6314583

Batch 57990, train_perplexity=95.63932, train_loss=4.560584

Batch 58000, train_perplexity=102.33312, train_loss=4.6282334

Batch 58010, train_perplexity=107.57018, train_loss=4.6781435

Batch 58020, train_perplexity=104.74455, train_loss=4.6515245

Batch 58030, train_perplexity=112.70098, train_loss=4.724738

Batch 58040, train_perplexity=102.79725, train_loss=4.6327586

Batch 58050, train_perplexity=106.389824, train_loss=4.66711

Batch 58060, train_perplexity=104.51241, train_loss=4.649306

Batch 58070, train_perplexity=104.88149, train_loss=4.652831
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 58080, train_perplexity=107.086655, train_loss=4.6736383

Batch 58090, train_perplexity=96.54095, train_loss=4.5699673

Batch 58100, train_perplexity=103.16579, train_loss=4.6363373

Batch 58110, train_perplexity=104.238525, train_loss=4.646682

Batch 58120, train_perplexity=111.65077, train_loss=4.715376

Batch 58130, train_perplexity=116.55478, train_loss=4.7583613

Batch 58140, train_perplexity=96.81275, train_loss=4.5727787

Batch 58150, train_perplexity=100.1828, train_loss=4.6069965

Batch 58160, train_perplexity=102.74849, train_loss=4.632284

Batch 58170, train_perplexity=105.07251, train_loss=4.6546507

Batch 58180, train_perplexity=111.70818, train_loss=4.71589

Batch 58190, train_perplexity=106.85528, train_loss=4.6714754

Batch 58200, train_perplexity=117.41018, train_loss=4.7656736

Batch 58210, train_perplexity=102.97038, train_loss=4.6344414

Batch 58220, train_perplexity=107.725, train_loss=4.6795816

Batch 58230, train_perplexity=120.61924, train_loss=4.792639

Batch 58240, train_perplexity=87.420296, train_loss=4.4707274

Batch 58250, train_perplexity=105.244804, train_loss=4.656289

Batch 58260, train_perplexity=107.84557, train_loss=4.6807003

Batch 58270, train_perplexity=109.32827, train_loss=4.694355

Batch 58280, train_perplexity=102.20258, train_loss=4.626957

Batch 58290, train_perplexity=109.925766, train_loss=4.6998053

Batch 58300, train_perplexity=100.95583, train_loss=4.614683

Batch 58310, train_perplexity=99.41294, train_loss=4.5992823

Batch 58320, train_perplexity=109.607216, train_loss=4.696903

Batch 58330, train_perplexity=105.82376, train_loss=4.661775

Batch 58340, train_perplexity=105.770645, train_loss=4.661273

Batch 58350, train_perplexity=97.17647, train_loss=4.5765285

Batch 58360, train_perplexity=99.3356, train_loss=4.598504

Batch 58370, train_perplexity=100.23522, train_loss=4.6075196

Batch 58380, train_perplexity=98.03366, train_loss=4.585311

Batch 58390, train_perplexity=114.66686, train_loss=4.742031

Batch 58400, train_perplexity=104.15376, train_loss=4.6458683

Batch 58410, train_perplexity=107.764656, train_loss=4.6799498

Batch 58420, train_perplexity=118.80392, train_loss=4.7774744

Batch 58430, train_perplexity=118.336685, train_loss=4.773534

Batch 58440, train_perplexity=106.15232, train_loss=4.664875

Batch 58450, train_perplexity=118.23888, train_loss=4.772707

Batch 58460, train_perplexity=113.03995, train_loss=4.7277412

Batch 58470, train_perplexity=97.900764, train_loss=4.5839543

Batch 58480, train_perplexity=105.142426, train_loss=4.655316

Batch 58490, train_perplexity=112.01852, train_loss=4.718664

Batch 58500, train_perplexity=117.422554, train_loss=4.765779

Batch 58510, train_perplexity=88.60549, train_loss=4.484194

Batch 58520, train_perplexity=106.1216, train_loss=4.6645856

Batch 58530, train_perplexity=109.671, train_loss=4.697485

Batch 58540, train_perplexity=104.461044, train_loss=4.648814

Batch 58550, train_perplexity=99.02026, train_loss=4.5953245

Batch 58560, train_perplexity=90.92332, train_loss=4.5100164

Batch 58570, train_perplexity=98.71388, train_loss=4.5922256

Batch 58580, train_perplexity=103.63616, train_loss=4.6408863

Batch 58590, train_perplexity=112.65971, train_loss=4.724372

Batch 58600, train_perplexity=99.37891, train_loss=4.59894

Batch 58610, train_perplexity=104.05706, train_loss=4.6449394

Batch 58620, train_perplexity=100.78413, train_loss=4.612981

Batch 58630, train_perplexity=98.613716, train_loss=4.5912104

Batch 58640, train_perplexity=100.83277, train_loss=4.6134634

Batch 58650, train_perplexity=98.65868, train_loss=4.591666

Batch 58660, train_perplexity=122.6452, train_loss=4.8092957

Batch 58670, train_perplexity=106.631836, train_loss=4.669382

Batch 58680, train_perplexity=103.0474, train_loss=4.635189

Batch 58690, train_perplexity=106.68971, train_loss=4.6699247

Batch 58700, train_perplexity=106.63127, train_loss=4.669377

Batch 58710, train_perplexity=106.16285, train_loss=4.664974

Batch 58720, train_perplexity=101.36832, train_loss=4.6187606

Batch 58730, train_perplexity=106.340324, train_loss=4.6666446

Batch 58740, train_perplexity=112.81771, train_loss=4.7257733

Batch 58750, train_perplexity=115.82506, train_loss=4.752081

Batch 58760, train_perplexity=104.779816, train_loss=4.651861

Batch 58770, train_perplexity=95.99313, train_loss=4.5642767

Batch 58780, train_perplexity=97.86716, train_loss=4.583611

Batch 58790, train_perplexity=104.951035, train_loss=4.653494

Batch 58800, train_perplexity=91.90065, train_loss=4.520708

Batch 58810, train_perplexity=110.77725, train_loss=4.7075214

Batch 58820, train_perplexity=103.646935, train_loss=4.6409903

Batch 58830, train_perplexity=111.001976, train_loss=4.709548

Batch 58840, train_perplexity=100.43056, train_loss=4.6094666

Batch 58850, train_perplexity=106.79808, train_loss=4.67094

Batch 58860, train_perplexity=112.78151, train_loss=4.7254524

Batch 58870, train_perplexity=112.31086, train_loss=4.7212706

Batch 58880, train_perplexity=97.236725, train_loss=4.5771484

Batch 58890, train_perplexity=107.44889, train_loss=4.6770153

Batch 58900, train_perplexity=103.9664, train_loss=4.644068

Batch 58910, train_perplexity=103.498474, train_loss=4.639557

Batch 58920, train_perplexity=112.08723, train_loss=4.7192774

Batch 58930, train_perplexity=119.21751, train_loss=4.7809496

Batch 58940, train_perplexity=111.85278, train_loss=4.7171836

Batch 58950, train_perplexity=123.37629, train_loss=4.815239

Batch 58960, train_perplexity=101.505394, train_loss=4.620112

Batch 58970, train_perplexity=90.54954, train_loss=4.505897

Batch 58980, train_perplexity=99.72306, train_loss=4.602397

Batch 58990, train_perplexity=115.07395, train_loss=4.745575

Batch 59000, train_perplexity=95.33299, train_loss=4.557376

Batch 59010, train_perplexity=94.302155, train_loss=4.546504

Batch 59020, train_perplexity=105.87206, train_loss=4.6622314

Batch 59030, train_perplexity=105.12533, train_loss=4.6551533

Batch 59040, train_perplexity=88.94626, train_loss=4.4880323

Batch 59050, train_perplexity=104.203835, train_loss=4.646349

Batch 59060, train_perplexity=99.38943, train_loss=4.5990458

Batch 59070, train_perplexity=104.807, train_loss=4.6521206

Batch 59080, train_perplexity=126.37919, train_loss=4.839287

Batch 59090, train_perplexity=106.129845, train_loss=4.6646633

Batch 59100, train_perplexity=111.61101, train_loss=4.7150197

Batch 59110, train_perplexity=101.77414, train_loss=4.622756

Batch 59120, train_perplexity=109.11167, train_loss=4.692372

Batch 59130, train_perplexity=104.58874, train_loss=4.650036

Batch 59140, train_perplexity=112.0728, train_loss=4.7191486

Batch 59150, train_perplexity=105.733475, train_loss=4.6609216

Batch 59160, train_perplexity=119.12238, train_loss=4.7801514

Batch 59170, train_perplexity=99.73381, train_loss=4.6025047

Batch 59180, train_perplexity=104.03573, train_loss=4.6447344

Batch 59190, train_perplexity=98.35895, train_loss=4.5886235

Batch 59200, train_perplexity=108.009224, train_loss=4.6822166

Batch 59210, train_perplexity=108.91108, train_loss=4.6905317

Batch 59220, train_perplexity=109.76972, train_loss=4.698385

Batch 59230, train_perplexity=113.808105, train_loss=4.7345138

Batch 59240, train_perplexity=111.00097, train_loss=4.709539

Batch 59250, train_perplexity=96.312065, train_loss=4.5675936

Batch 59260, train_perplexity=111.1814, train_loss=4.711163

Batch 59270, train_perplexity=110.64453, train_loss=4.7063227

Batch 59280, train_perplexity=110.29687, train_loss=4.7031755

Batch 59290, train_perplexity=110.1391, train_loss=4.701744

Batch 59300, train_perplexity=102.28195, train_loss=4.627733

Batch 59310, train_perplexity=120.73582, train_loss=4.793605

Batch 59320, train_perplexity=94.44917, train_loss=4.548062

Batch 59330, train_perplexity=109.09341, train_loss=4.6922045

Batch 59340, train_perplexity=114.84317, train_loss=4.7435675

Batch 59350, train_perplexity=97.69953, train_loss=4.581897

Batch 59360, train_perplexity=106.75363, train_loss=4.6705236

Batch 59370, train_perplexity=108.826355, train_loss=4.6897535

Batch 59380, train_perplexity=113.40972, train_loss=4.731007

Batch 59390, train_perplexity=109.2762, train_loss=4.6938787
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 59400, train_perplexity=102.937294, train_loss=4.63412

Batch 59410, train_perplexity=112.7371, train_loss=4.7250586

Batch 59420, train_perplexity=115.94945, train_loss=4.7531543

Batch 59430, train_perplexity=115.2987, train_loss=4.747526

Batch 59440, train_perplexity=102.62735, train_loss=4.6311045

Batch 59450, train_perplexity=95.83745, train_loss=4.5626535

Batch 59460, train_perplexity=112.920456, train_loss=4.7266836

Batch 59470, train_perplexity=100.43372, train_loss=4.609498

Batch 59480, train_perplexity=105.529335, train_loss=4.658989

Batch 59490, train_perplexity=98.60347, train_loss=4.5911064

Batch 59500, train_perplexity=105.74074, train_loss=4.66099

Batch 59510, train_perplexity=101.81049, train_loss=4.623113

Batch 59520, train_perplexity=109.762344, train_loss=4.6983175

Batch 59530, train_perplexity=114.38977, train_loss=4.7396116

Batch 59540, train_perplexity=104.79751, train_loss=4.65203

Batch 59550, train_perplexity=101.41904, train_loss=4.619261

Batch 59560, train_perplexity=104.93617, train_loss=4.6533523

Batch 59570, train_perplexity=116.73838, train_loss=4.7599354

Batch 59580, train_perplexity=106.30843, train_loss=4.6663446

Batch 59590, train_perplexity=92.25792, train_loss=4.524588

Batch 59600, train_perplexity=118.52248, train_loss=4.7751026

Batch 59610, train_perplexity=108.14863, train_loss=4.6835065

Batch 59620, train_perplexity=105.56552, train_loss=4.659332

Batch 59630, train_perplexity=112.04277, train_loss=4.7188807

Batch 59640, train_perplexity=98.44505, train_loss=4.5894985

Batch 59650, train_perplexity=93.97272, train_loss=4.5430045

Batch 59660, train_perplexity=105.79556, train_loss=4.6615086

Batch 59670, train_perplexity=99.637695, train_loss=4.6015406

Batch 59680, train_perplexity=107.00034, train_loss=4.672832

Batch 59690, train_perplexity=96.712814, train_loss=4.571746

Batch 59700, train_perplexity=100.62643, train_loss=4.611415

Batch 59710, train_perplexity=94.80062, train_loss=4.551776

Batch 59720, train_perplexity=99.0469, train_loss=4.5955935

Batch 59730, train_perplexity=101.82073, train_loss=4.623214

Batch 59740, train_perplexity=111.59558, train_loss=4.7148814

Batch 59750, train_perplexity=101.24094, train_loss=4.617503

Batch 59760, train_perplexity=106.23167, train_loss=4.665622

Batch 59770, train_perplexity=99.743324, train_loss=4.6026

Batch 59780, train_perplexity=116.05543, train_loss=4.754068

Batch 59790, train_perplexity=112.48177, train_loss=4.722791

Batch 59800, train_perplexity=110.83431, train_loss=4.7080364

Batch 59810, train_perplexity=113.230865, train_loss=4.729429

Batch 59820, train_perplexity=91.47098, train_loss=4.5160217

Batch 59830, train_perplexity=113.16086, train_loss=4.7288103

Batch 59840, train_perplexity=98.08108, train_loss=4.5857944

Batch 59850, train_perplexity=99.15766, train_loss=4.596711

Batch 59860, train_perplexity=100.6593, train_loss=4.6117415

Batch 59870, train_perplexity=91.75125, train_loss=4.519081

Batch 59880, train_perplexity=98.009735, train_loss=4.585067

Batch 59890, train_perplexity=100.57404, train_loss=4.610894

Batch 59900, train_perplexity=102.427246, train_loss=4.629153

Batch 59910, train_perplexity=105.502266, train_loss=4.6587324

Batch 59920, train_perplexity=107.541824, train_loss=4.67788

Batch 59930, train_perplexity=106.86241, train_loss=4.671542

Batch 59940, train_perplexity=111.87044, train_loss=4.7173414

Batch 59950, train_perplexity=103.17189, train_loss=4.6363964

Batch 59960, train_perplexity=107.43034, train_loss=4.6768427

Batch 59970, train_perplexity=102.659706, train_loss=4.6314197

Batch 59980, train_perplexity=104.213974, train_loss=4.646446

Batch 59990, train_perplexity=110.736694, train_loss=4.707155

Batch 60000, train_perplexity=109.12884, train_loss=4.692529

Batch 60010, train_perplexity=115.605675, train_loss=4.750185

Batch 60020, train_perplexity=108.071045, train_loss=4.682789

Batch 60030, train_perplexity=102.28693, train_loss=4.627782

Batch 60040, train_perplexity=89.30094, train_loss=4.492012

Batch 60050, train_perplexity=118.040474, train_loss=4.7710276

Batch 60060, train_perplexity=126.83487, train_loss=4.842886

Batch 60070, train_perplexity=111.58063, train_loss=4.7147474

Batch 60080, train_perplexity=111.97312, train_loss=4.718259

Batch 60090, train_perplexity=121.775925, train_loss=4.8021827

Batch 60100, train_perplexity=110.22431, train_loss=4.7025175

Batch 60110, train_perplexity=102.95811, train_loss=4.634322

Batch 60120, train_perplexity=104.39387, train_loss=4.648171

Batch 60130, train_perplexity=109.84801, train_loss=4.6990976

Batch 60140, train_perplexity=110.54418, train_loss=4.7054152

Batch 60150, train_perplexity=122.69586, train_loss=4.8097086

Batch 60160, train_perplexity=107.39808, train_loss=4.6765423

Batch 60170, train_perplexity=113.244095, train_loss=4.7295456

Batch 60180, train_perplexity=106.50886, train_loss=4.668228

Batch 60190, train_perplexity=109.29293, train_loss=4.6940317

Batch 60200, train_perplexity=109.48875, train_loss=4.695822

Batch 60210, train_perplexity=102.066605, train_loss=4.6256256

Batch 60220, train_perplexity=98.37363, train_loss=4.588773

Batch 60230, train_perplexity=111.53222, train_loss=4.7143135

Batch 60240, train_perplexity=105.13761, train_loss=4.65527

Batch 60250, train_perplexity=106.92083, train_loss=4.6720886

Batch 60260, train_perplexity=116.48188, train_loss=4.7577357

Batch 60270, train_perplexity=112.36164, train_loss=4.7217226

Batch 60280, train_perplexity=91.280136, train_loss=4.513933

Batch 60290, train_perplexity=111.11223, train_loss=4.710541

Batch 60300, train_perplexity=105.799095, train_loss=4.661542

Batch 60310, train_perplexity=120.29723, train_loss=4.7899656

Batch 60320, train_perplexity=112.05596, train_loss=4.7189984

Batch 60330, train_perplexity=111.59712, train_loss=4.7148952

Batch 60340, train_perplexity=107.374725, train_loss=4.676325

Batch 60350, train_perplexity=98.24022, train_loss=4.5874157

Batch 60360, train_perplexity=106.34241, train_loss=4.666664

Batch 60370, train_perplexity=110.391106, train_loss=4.7040296

Batch 60380, train_perplexity=98.111015, train_loss=4.5860996

Batch 60390, train_perplexity=123.199104, train_loss=4.813802

Batch 60400, train_perplexity=103.8719, train_loss=4.6431584

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 60410, train_perplexity=100.70188, train_loss=4.6121645

Batch 60420, train_perplexity=106.45392, train_loss=4.667712

Batch 60430, train_perplexity=98.38751, train_loss=4.588914

Batch 60440, train_perplexity=99.085495, train_loss=4.595983

Batch 60450, train_perplexity=95.277725, train_loss=4.556796

Batch 60460, train_perplexity=105.536224, train_loss=4.6590543

Batch 60470, train_perplexity=110.122925, train_loss=4.701597

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 60480, train_perplexity=106.52983, train_loss=4.668425

Batch 60490, train_perplexity=100.04488, train_loss=4.605619

Batch 60500, train_perplexity=103.14611, train_loss=4.6361465

Batch 60510, train_perplexity=97.78524, train_loss=4.5827737

Batch 60520, train_perplexity=100.83637, train_loss=4.613499

Batch 60530, train_perplexity=84.59319, train_loss=4.437854

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 60540, train_perplexity=102.45953, train_loss=4.629468

Batch 60550, train_perplexity=98.44721, train_loss=4.5895205

Batch 60560, train_perplexity=96.956795, train_loss=4.5742655

Batch 60570, train_perplexity=102.66097, train_loss=4.631432

Batch 60580, train_perplexity=98.919464, train_loss=4.594306

Batch 60590, train_perplexity=97.624466, train_loss=4.581128

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 60600, train_perplexity=103.30865, train_loss=4.637721

Batch 60610, train_perplexity=113.43238, train_loss=4.731207

Batch 60620, train_perplexity=87.41383, train_loss=4.4706535

Batch 60630, train_perplexity=95.76304, train_loss=4.561877

Batch 60640, train_perplexity=89.5095, train_loss=4.4943447

Batch 60650, train_perplexity=104.72967, train_loss=4.6513824

Batch 60660, train_perplexity=102.88655, train_loss=4.633627

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 60670, train_perplexity=96.80878, train_loss=4.5727377

Batch 60680, train_perplexity=94.24749, train_loss=4.545924

Batch 60690, train_perplexity=120.04076, train_loss=4.7878313

Batch 60700, train_perplexity=110.49211, train_loss=4.704944

Batch 60710, train_perplexity=106.086235, train_loss=4.6642523

Batch 60720, train_perplexity=99.37464, train_loss=4.598897

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 60730, train_perplexity=101.06985, train_loss=4.615812

Batch 60740, train_perplexity=101.00225, train_loss=4.615143

Batch 60750, train_perplexity=93.91919, train_loss=4.5424347

Batch 60760, train_perplexity=100.52883, train_loss=4.6104445

Batch 60770, train_perplexity=104.33336, train_loss=4.647591

Batch 60780, train_perplexity=94.91071, train_loss=4.5529366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 60790, train_perplexity=106.440926, train_loss=4.66759

Batch 60800, train_perplexity=104.49352, train_loss=4.649125

Batch 60810, train_perplexity=101.016075, train_loss=4.6152797

Batch 60820, train_perplexity=104.12943, train_loss=4.6456347

Batch 60830, train_perplexity=108.56632, train_loss=4.6873612

Batch 60840, train_perplexity=102.362656, train_loss=4.628522

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 60850, train_perplexity=97.967964, train_loss=4.5846405

Batch 60860, train_perplexity=108.225235, train_loss=4.6842146

Batch 60870, train_perplexity=107.6568, train_loss=4.6789484

Batch 60880, train_perplexity=110.7369, train_loss=4.707157

Batch 60890, train_perplexity=102.7334, train_loss=4.6321373

Batch 60900, train_perplexity=90.446014, train_loss=4.504753

Batch 60910, train_perplexity=97.477844, train_loss=4.579625

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 60920, train_perplexity=98.8001, train_loss=4.5930986

Batch 60930, train_perplexity=111.41857, train_loss=4.713294

Batch 60940, train_perplexity=99.168495, train_loss=4.5968204

Batch 60950, train_perplexity=100.20765, train_loss=4.6072445

Batch 60960, train_perplexity=97.20084, train_loss=4.5767794

Batch 60970, train_perplexity=99.527534, train_loss=4.6004343

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 60980, train_perplexity=100.21739, train_loss=4.607342

Batch 60990, train_perplexity=94.55372, train_loss=4.549168

Batch 61000, train_perplexity=96.990555, train_loss=4.5746136

Batch 61010, train_perplexity=103.85655, train_loss=4.6430106

Batch 61020, train_perplexity=112.05543, train_loss=4.7189937

Batch 61030, train_perplexity=113.58035, train_loss=4.7325106

Batch 61040, train_perplexity=107.74359, train_loss=4.6797543

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 61050, train_perplexity=90.87624, train_loss=4.5094986

Batch 61060, train_perplexity=106.49758, train_loss=4.6681223

Batch 61070, train_perplexity=98.54462, train_loss=4.5905094

Batch 61080, train_perplexity=106.80077, train_loss=4.670965

Batch 61090, train_perplexity=102.29912, train_loss=4.627901

Batch 61100, train_perplexity=100.66088, train_loss=4.6117573

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 61110, train_perplexity=96.1809, train_loss=4.566231

Batch 61120, train_perplexity=91.349976, train_loss=4.514698

Batch 61130, train_perplexity=97.858246, train_loss=4.58352

Batch 61140, train_perplexity=111.619156, train_loss=4.7150927

Batch 61150, train_perplexity=91.53887, train_loss=4.5167637

Batch 61160, train_perplexity=110.68126, train_loss=4.7066545

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loadingWARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61170, train_perplexity=112.101875, train_loss=4.719408

Batch 61180, train_perplexity=104.840096, train_loss=4.6524363

Batch 61190, train_perplexity=98.104, train_loss=4.586028

Batch 61200, train_perplexity=108.81525, train_loss=4.6896515

Batch 61210, train_perplexity=93.22208, train_loss=4.5349846

Batch 61220, train_perplexity=102.01513, train_loss=4.625121

Batch 61230, train_perplexity=109.80815, train_loss=4.6987348

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 61240, train_perplexity=103.07521, train_loss=4.635459

Batch 61250, train_perplexity=95.74409, train_loss=4.561679

Batch 61260, train_perplexity=113.601265, train_loss=4.7326946

Batch 61270, train_perplexity=92.243706, train_loss=4.524434

Batch 61280, train_perplexity=101.40641, train_loss=4.6191363

Batch 61290, train_perplexity=101.76637, train_loss=4.6226797

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 61300, train_perplexity=104.934166, train_loss=4.653333

Batch 61310, train_perplexity=98.09464, train_loss=4.5859327

Batch 61320, train_perplexity=95.73094, train_loss=4.5615416

Batch 61330, train_perplexity=90.9084, train_loss=4.5098524

Batch 61340, train_perplexity=102.66778, train_loss=4.6314983

Batch 61350, train_perplexity=100.83527, train_loss=4.613488

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 61360, train_perplexity=99.03471, train_loss=4.5954704

Batch 61370, train_perplexity=113.57916, train_loss=4.7325

Batch 61380, train_perplexity=112.391754, train_loss=4.7219906

Batch 61390, train_perplexity=97.28329, train_loss=4.577627

Batch 61400, train_perplexity=94.029686, train_loss=4.5436106

Batch 61410, train_perplexity=113.431404, train_loss=4.7311983

Batch 61420, train_perplexity=102.92875, train_loss=4.634037

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 61430, train_perplexity=101.415405, train_loss=4.619225

Batch 61440, train_perplexity=110.320854, train_loss=4.703393

Batch 61450, train_perplexity=97.519035, train_loss=4.5800476

Batch 61460, train_perplexity=100.254196, train_loss=4.607709

Batch 61470, train_perplexity=101.92508, train_loss=4.624238

Batch 61480, train_perplexity=109.762764, train_loss=4.6983213

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 61490, train_perplexity=105.87429, train_loss=4.6622524

Batch 61500, train_perplexity=108.48942, train_loss=4.6866527

Batch 61510, train_perplexity=93.66515, train_loss=4.5397263

Batch 61520, train_perplexity=94.95643, train_loss=4.553418

Batch 61530, train_perplexity=104.574425, train_loss=4.649899

Batch 61540, train_perplexity=102.92306, train_loss=4.6339817

Batch 61550, train_perplexity=101.48057, train_loss=4.6198673

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 61560, train_perplexity=112.0712, train_loss=4.7191343

Batch 61570, train_perplexity=108.97476, train_loss=4.6911163

Batch 61580, train_perplexity=100.71845, train_loss=4.612329

Batch 61590, train_perplexity=92.260956, train_loss=4.524621

Batch 61600, train_perplexity=97.64732, train_loss=4.5813622

Batch 61610, train_perplexity=95.74135, train_loss=4.5616503

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 61620, train_perplexity=110.04691, train_loss=4.7009068

Batch 61630, train_perplexity=97.974174, train_loss=4.584704

Batch 61640, train_perplexity=112.23163, train_loss=4.720565

Batch 61650, train_perplexity=107.85652, train_loss=4.680802

Batch 61660, train_perplexity=106.850136, train_loss=4.6714272

Batch 61670, train_perplexity=102.42544, train_loss=4.629135

Batch 61680, train_perplexity=113.471115, train_loss=4.7315483

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 61690, train_perplexity=101.441666, train_loss=4.619484

Batch 61700, train_perplexity=100.95603, train_loss=4.614685

Batch 61710, train_perplexity=99.621635, train_loss=4.6013794

Batch 61720, train_perplexity=112.36523, train_loss=4.7217546

Batch 61730, train_perplexity=104.37351, train_loss=4.647976

Batch 61740, train_perplexity=121.2249, train_loss=4.7976475

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 61750, train_perplexity=108.11651, train_loss=4.6832094

Batch 61760, train_perplexity=119.97128, train_loss=4.7872524

Batch 61770, train_perplexity=109.61119, train_loss=4.6969395

Batch 61780, train_perplexity=111.6215, train_loss=4.7151136

Batch 61790, train_perplexity=112.34943, train_loss=4.721614

Batch 61800, train_perplexity=97.09564, train_loss=4.5756965

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 61810, train_perplexity=104.85539, train_loss=4.652582

Batch 61820, train_perplexity=108.79647, train_loss=4.689479

Batch 61830, train_perplexity=120.846756, train_loss=4.7945232

Batch 61840, train_perplexity=103.873436, train_loss=4.643173

Batch 61850, train_perplexity=101.157455, train_loss=4.616678

Batch 61860, train_perplexity=94.38893, train_loss=4.547424

Batch 61870, train_perplexity=115.33873, train_loss=4.7478733

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 61880, train_perplexity=114.90693, train_loss=4.7441225

Batch 61890, train_perplexity=96.769455, train_loss=4.5723314

Batch 61900, train_perplexity=104.4874, train_loss=4.6490664

Batch 61910, train_perplexity=104.33629, train_loss=4.6476192

Batch 61920, train_perplexity=105.53859, train_loss=4.6590767

Batch 61930, train_perplexity=104.42364, train_loss=4.648456

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 61940, train_perplexity=99.54329, train_loss=4.6005926

Batch 61950, train_perplexity=96.924904, train_loss=4.5739365

Batch 61960, train_perplexity=96.08468, train_loss=4.56523

Batch 61970, train_perplexity=94.68325, train_loss=4.550537

Batch 61980, train_perplexity=115.01054, train_loss=4.7450237

Batch 61990, train_perplexity=91.07258, train_loss=4.5116568

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 62000, train_perplexity=94.630486, train_loss=4.5499797

Batch 62010, train_perplexity=106.630714, train_loss=4.6693716

Batch 62020, train_perplexity=107.54439, train_loss=4.6779037

Batch 62030, train_perplexity=113.530594, train_loss=4.7320724

Batch 62040, train_perplexity=101.968925, train_loss=4.624668

Batch 62050, train_perplexity=105.41336, train_loss=4.6578894

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 62060, train_perplexity=113.41069, train_loss=4.7310157

Batch 62070, train_perplexity=103.02608, train_loss=4.634982

Batch 62080, train_perplexity=88.76872, train_loss=4.4860344

Batch 62090, train_perplexity=91.36984, train_loss=4.5149155

Batch 62100, train_perplexity=101.3877, train_loss=4.618952

Batch 62110, train_perplexity=107.95949, train_loss=4.681756

Batch 62120, train_perplexity=111.68235, train_loss=4.7156587

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 62130, train_perplexity=102.756035, train_loss=4.6323576

Batch 62140, train_perplexity=97.51211, train_loss=4.5799766

Batch 62150, train_perplexity=110.89172, train_loss=4.7085543

Batch 62160, train_perplexity=107.967415, train_loss=4.6818295

Batch 62170, train_perplexity=124.90801, train_loss=4.8275776

Batch 62180, train_perplexity=100.65604, train_loss=4.611709

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 62190, train_perplexity=95.0648, train_loss=4.5545588

Batch 62200, train_perplexity=108.725784, train_loss=4.688829

Batch 62210, train_perplexity=89.49221, train_loss=4.4941516

Batch 62220, train_perplexity=105.14859, train_loss=4.6553745

Batch 62230, train_perplexity=108.322044, train_loss=4.6851087

Batch 62240, train_perplexity=100.434296, train_loss=4.6095037

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 62250, train_perplexity=100.89366, train_loss=4.614067

Batch 62260, train_perplexity=99.89087, train_loss=4.6040783

Batch 62270, train_perplexity=105.90696, train_loss=4.662561

Batch 62280, train_perplexity=110.69741, train_loss=4.7068005

Batch 62290, train_perplexity=99.21153, train_loss=4.5972543

Batch 62300, train_perplexity=110.376, train_loss=4.7038927

Batch 62310, train_perplexity=99.4338, train_loss=4.599492

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 62320, train_perplexity=98.54057, train_loss=4.5904684

Batch 62330, train_perplexity=100.462746, train_loss=4.609787

Batch 62340, train_perplexity=109.36112, train_loss=4.6946554

Batch 62350, train_perplexity=108.62137, train_loss=4.687868

Batch 62360, train_perplexity=116.08039, train_loss=4.754283

Batch 62370, train_perplexity=101.52393, train_loss=4.6202946

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 62380, train_perplexity=121.46653, train_loss=4.7996387

Batch 62390, train_perplexity=106.53207, train_loss=4.668446

Batch 62400, train_perplexity=113.332146, train_loss=4.730323

Batch 62410, train_perplexity=106.3134, train_loss=4.6663914

Batch 62420, train_perplexity=100.914734, train_loss=4.614276

Batch 62430, train_perplexity=104.24752, train_loss=4.646768

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Batch 62440, train_perplexity=104.48824, train_loss=4.6490746

Batch 62450, train_perplexity=105.68393, train_loss=4.660453

Batch 62460, train_perplexity=101.554535, train_loss=4.620596

Batch 62470, train_perplexity=100.63833, train_loss=4.611533

Batch 62480, train_perplexity=106.78127, train_loss=4.6707826

Batch 62490, train_perplexity=107.12639, train_loss=4.6740093

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 62500, train_perplexity=101.1451, train_loss=4.616556

Batch 62510, train_perplexity=104.164246, train_loss=4.645969

Batch 62520, train_perplexity=117.380455, train_loss=4.7654204

Batch 62530, train_perplexity=97.73793, train_loss=4.5822897

Batch 62540, train_perplexity=102.23144, train_loss=4.627239

Batch 62550, train_perplexity=98.31623, train_loss=4.588189

Batch 62560, train_perplexity=107.94039, train_loss=4.681579

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 62570, train_perplexity=114.77901, train_loss=4.7430086

Batch 62580, train_perplexity=100.07481, train_loss=4.605918

Batch 62590, train_perplexity=93.3558, train_loss=4.536418

Batch 62600, train_perplexity=107.60261, train_loss=4.678445

Batch 62610, train_perplexity=99.26789, train_loss=4.597822

Batch 62620, train_perplexity=109.28991, train_loss=4.694004

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 62630, train_perplexity=97.500854, train_loss=4.579861

Batch 62640, train_perplexity=109.311905, train_loss=4.6942053

Batch 62650, train_perplexity=97.32346, train_loss=4.57804

Batch 62660, train_perplexity=94.765816, train_loss=4.551409

Batch 62670, train_perplexity=103.84501, train_loss=4.6428995

Batch 62680, train_perplexity=115.77558, train_loss=4.7516537

Batch 62690, train_perplexity=95.27028, train_loss=4.556718

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 62700, train_perplexity=90.586975, train_loss=4.5063105

Batch 62710, train_perplexity=107.953514, train_loss=4.6817007

Batch 62720, train_perplexity=102.13793, train_loss=4.626324

Batch 62730, train_perplexity=106.28684, train_loss=4.6661415

Batch 62740, train_perplexity=98.52606, train_loss=4.590321

Batch 62750, train_perplexity=101.277245, train_loss=4.6178617

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 62760, train_perplexity=112.085144, train_loss=4.719259

Batch 62770, train_perplexity=110.14172, train_loss=4.701768

Batch 62780, train_perplexity=108.961464, train_loss=4.6909943

Batch 62790, train_perplexity=100.37115, train_loss=4.608875

Batch 62800, train_perplexity=107.646225, train_loss=4.67885

Batch 62810, train_perplexity=119.83595, train_loss=4.7861238

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 62820, train_perplexity=104.73926, train_loss=4.651474

Batch 62830, train_perplexity=113.75651, train_loss=4.7340603

Batch 62840, train_perplexity=99.78847, train_loss=4.6030526

Batch 62850, train_perplexity=95.99217, train_loss=4.5642667

Batch 62860, train_perplexity=100.48124, train_loss=4.609971

Batch 62870, train_perplexity=108.23711, train_loss=4.6843243

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 62880, train_perplexity=98.17157, train_loss=4.5867167

Batch 62890, train_perplexity=98.47468, train_loss=4.5897994

Batch 62900, train_perplexity=112.02172, train_loss=4.718693

Batch 62910, train_perplexity=111.00134, train_loss=4.7095423

Batch 62920, train_perplexity=109.45706, train_loss=4.6955323

Batch 62930, train_perplexity=105.3299, train_loss=4.6570973

Batch 62940, train_perplexity=91.611534, train_loss=4.517557

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 62950, train_perplexity=103.0023, train_loss=4.6347513

Batch 62960, train_perplexity=114.03294, train_loss=4.7364874

Batch 62970, train_perplexity=104.26522, train_loss=4.646938

Batch 62980, train_perplexity=104.21745, train_loss=4.6464796

Batch 62990, train_perplexity=105.785576, train_loss=4.661414

Batch 63000, train_perplexity=106.814224, train_loss=4.671091

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 63010, train_perplexity=106.56697, train_loss=4.6687737

Batch 63020, train_perplexity=98.22991, train_loss=4.587311

Batch 63030, train_perplexity=95.71647, train_loss=4.5613904

Batch 63040, train_perplexity=108.900375, train_loss=4.6904335

Batch 63050, train_perplexity=101.92867, train_loss=4.6242733

Batch 63060, train_perplexity=111.46172, train_loss=4.713681

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 63070, train_perplexity=109.363, train_loss=4.6946726
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 63080, train_perplexity=104.394615, train_loss=4.648178

Batch 63090, train_perplexity=116.60775, train_loss=4.758816

Batch 63100, train_perplexity=93.99105, train_loss=4.5431995

Batch 63110, train_perplexity=103.4812, train_loss=4.63939

Batch 63120, train_perplexity=104.162704, train_loss=4.645954

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 63130, train_perplexity=97.75447, train_loss=4.582459

Batch 63140, train_perplexity=109.03204, train_loss=4.691642

Batch 63150, train_perplexity=109.36571, train_loss=4.6946974

Batch 63160, train_perplexity=103.74127, train_loss=4.6419

Batch 63170, train_perplexity=100.63871, train_loss=4.611537

Batch 63180, train_perplexity=114.10191, train_loss=4.737092

Batch 63190, train_perplexity=106.45707, train_loss=4.667742

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 63200, train_perplexity=104.72857, train_loss=4.651372

Batch 63210, train_perplexity=99.85639, train_loss=4.603733

Batch 63220, train_perplexity=112.579056, train_loss=4.7236557

Batch 63230, train_perplexity=94.435844, train_loss=4.5479207

Batch 63240, train_perplexity=105.66675, train_loss=4.6602902

Batch 63250, train_perplexity=101.20073, train_loss=4.617106

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 63260, train_perplexity=106.54609, train_loss=4.6685777

Batch 63270, train_perplexity=106.71739, train_loss=4.670184

Batch 63280, train_perplexity=103.779175, train_loss=4.6422653

Batch 63290, train_perplexity=91.88075, train_loss=4.5204916

Batch 63300, train_perplexity=105.72102, train_loss=4.660804

Batch 63310, train_perplexity=115.62111, train_loss=4.7503185

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 63320, train_perplexity=98.36838, train_loss=4.5887194

Batch 63330, train_perplexity=116.41131, train_loss=4.7571297

Batch 63340, train_perplexity=112.564026, train_loss=4.723522

Batch 63350, train_perplexity=108.13352, train_loss=4.683367

Batch 63360, train_perplexity=103.44677, train_loss=4.639057

Batch 63370, train_perplexity=98.23085, train_loss=4.5873203

Batch 63380, train_perplexity=93.82841, train_loss=4.5414677

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 63390, train_perplexity=116.50516, train_loss=4.7579355

Batch 63400, train_perplexity=96.75903, train_loss=4.5722237

Batch 63410, train_perplexity=102.31303, train_loss=4.628037

Batch 63420, train_perplexity=108.44013, train_loss=4.686198

Batch 63430, train_perplexity=111.66936, train_loss=4.7155423

Batch 63440, train_perplexity=108.46899, train_loss=4.6864643

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 63450, train_perplexity=100.947655, train_loss=4.614602

Batch 63460, train_perplexity=108.057236, train_loss=4.682661

Batch 63470, train_perplexity=122.57849, train_loss=4.8087516

Batch 63480, train_perplexity=105.01981, train_loss=4.654149

Batch 63490, train_perplexity=114.88469, train_loss=4.743929

Batch 63500, train_perplexity=114.63013, train_loss=4.7417107

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 63510, train_perplexity=98.11218, train_loss=4.5861115

Batch 63520, train_perplexity=102.53696, train_loss=4.6302233

Batch 63530, train_perplexity=99.465004, train_loss=4.599806

Batch 63540, train_perplexity=126.893425, train_loss=4.8433475

Batch 63550, train_perplexity=104.59532, train_loss=4.650099

Batch 63560, train_perplexity=97.13143, train_loss=4.576065

Batch 63570, train_perplexity=103.60548, train_loss=4.64059

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 63580, train_perplexity=91.99917, train_loss=4.5217795

Batch 63590, train_perplexity=98.3152, train_loss=4.5881786

Batch 63600, train_perplexity=101.05457, train_loss=4.6156607

Batch 63610, train_perplexity=106.883, train_loss=4.671735

Batch 63620, train_perplexity=104.29416, train_loss=4.6472154

Batch 63630, train_perplexity=105.48456, train_loss=4.6585646

Batch 63640, train_perplexity=96.93322, train_loss=4.5740223

Batch 63650, train_perplexity=100.627, train_loss=4.6114206

Batch 63660, train_perplexity=97.59607, train_loss=4.5808372

Batch 63670, train_perplexity=94.43571, train_loss=4.5479193

Batch 63680, train_perplexity=100.34229, train_loss=4.6085873

Batch 63690, train_perplexity=102.40058, train_loss=4.6288924

Batch 63700, train_perplexity=98.86193, train_loss=4.5937243

Batch 63710, train_perplexity=101.009766, train_loss=4.615217

Batch 63720, train_perplexity=94.91189, train_loss=4.552949

Batch 63730, train_perplexity=100.08998, train_loss=4.6060696

Batch 63740, train_perplexity=90.003395, train_loss=4.4998474

Batch 63750, train_perplexity=103.95549, train_loss=4.643963

Batch 63760, train_perplexity=94.60973, train_loss=4.5497603

Batch 63770, train_perplexity=107.76692, train_loss=4.6799707

Batch 63780, train_perplexity=98.22345, train_loss=4.587245

Batch 63790, train_perplexity=104.560814, train_loss=4.649769

Batch 63800, train_perplexity=103.970566, train_loss=4.644108

Batch 63810, train_perplexity=97.977585, train_loss=4.5847387

Batch 63820, train_perplexity=106.29946, train_loss=4.6662602

Batch 63830, train_perplexity=92.94059, train_loss=4.5319605

Batch 63840, train_perplexity=106.433716, train_loss=4.6675224

Batch 63850, train_perplexity=103.84962, train_loss=4.642944
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 63860, train_perplexity=99.715645, train_loss=4.6023226

Batch 63870, train_perplexity=105.761765, train_loss=4.661189

Batch 63880, train_perplexity=111.51818, train_loss=4.7141876

Batch 63890, train_perplexity=104.160515, train_loss=4.645933

Batch 63900, train_perplexity=103.70112, train_loss=4.641513

Batch 63910, train_perplexity=85.91911, train_loss=4.4534063

Batch 63920, train_perplexity=99.9225, train_loss=4.604395

Batch 63930, train_perplexity=95.94632, train_loss=4.563789

Batch 63940, train_perplexity=106.84188, train_loss=4.67135

Batch 63950, train_perplexity=100.55611, train_loss=4.610716

Batch 63960, train_perplexity=105.96439, train_loss=4.663103

Batch 63970, train_perplexity=109.84801, train_loss=4.6990976

Batch 63980, train_perplexity=91.9746, train_loss=4.5215125

Batch 63990, train_perplexity=102.012405, train_loss=4.6250944

Batch 64000, train_perplexity=104.86379, train_loss=4.6526623

Batch 64010, train_perplexity=94.45926, train_loss=4.5481687

Batch 64020, train_perplexity=110.4515, train_loss=4.7045765

Batch 64030, train_perplexity=99.0315, train_loss=4.595438

Batch 64040, train_perplexity=100.90314, train_loss=4.614161

Batch 64050, train_perplexity=103.76671, train_loss=4.642145

Batch 64060, train_perplexity=100.599174, train_loss=4.611144

Batch 64070, train_perplexity=87.45761, train_loss=4.471154

Batch 64080, train_perplexity=99.60169, train_loss=4.601179

Batch 64090, train_perplexity=100.35196, train_loss=4.6086836

Batch 64100, train_perplexity=108.793304, train_loss=4.68945

Batch 64110, train_perplexity=100.717926, train_loss=4.6123238

Batch 64120, train_perplexity=93.280685, train_loss=4.535613

Batch 64130, train_perplexity=110.580925, train_loss=4.7057476

Batch 64140, train_perplexity=96.34321, train_loss=4.567917

Batch 64150, train_perplexity=94.34974, train_loss=4.5470085

Batch 64160, train_perplexity=112.35639, train_loss=4.721676

Batch 64170, train_perplexity=96.68985, train_loss=4.5715084

Batch 64180, train_perplexity=95.93045, train_loss=4.5636234

Batch 64190, train_perplexity=103.28619, train_loss=4.6375036

Batch 64200, train_perplexity=108.99347, train_loss=4.691288

Batch 64210, train_perplexity=102.15552, train_loss=4.6264963

Batch 64220, train_perplexity=96.0993, train_loss=4.565382

Batch 64230, train_perplexity=113.331985, train_loss=4.7303214

Batch 64240, train_perplexity=108.613075, train_loss=4.687792

Batch 64250, train_perplexity=99.85582, train_loss=4.6037273

Batch 64260, train_perplexity=98.31909, train_loss=4.588218

Batch 64270, train_perplexity=106.53588, train_loss=4.668482

Batch 64280, train_perplexity=106.83276, train_loss=4.6712646

Batch 64290, train_perplexity=109.83366, train_loss=4.698967

Batch 64300, train_perplexity=90.74097, train_loss=4.508009

Batch 64310, train_perplexity=103.34402, train_loss=4.6380634

Batch 64320, train_perplexity=98.39859, train_loss=4.5890265

Batch 64330, train_perplexity=90.840935, train_loss=4.50911

Batch 64340, train_perplexity=104.89765, train_loss=4.652985

Batch 64350, train_perplexity=109.026535, train_loss=4.6915913

Batch 64360, train_perplexity=101.86405, train_loss=4.623639

Batch 64370, train_perplexity=98.78182, train_loss=4.5929136

Batch 64380, train_perplexity=95.61306, train_loss=4.5603094

Batch 64390, train_perplexity=101.98468, train_loss=4.6248226

Batch 64400, train_perplexity=100.79715, train_loss=4.61311

Batch 64410, train_perplexity=101.353386, train_loss=4.6186132

Batch 64420, train_perplexity=111.57626, train_loss=4.7147083

Batch 64430, train_perplexity=107.35455, train_loss=4.676137

Batch 64440, train_perplexity=105.42673, train_loss=4.658016

Batch 64450, train_perplexity=88.59873, train_loss=4.4841175

Batch 64460, train_perplexity=98.656, train_loss=4.591639

Batch 64470, train_perplexity=97.49951, train_loss=4.5798473

Batch 64480, train_perplexity=108.46785, train_loss=4.686454

Batch 64490, train_perplexity=97.04764, train_loss=4.575202

Batch 64500, train_perplexity=109.68972, train_loss=4.6976557

Batch 64510, train_perplexity=111.534454, train_loss=4.7143335

Batch 64520, train_perplexity=87.199974, train_loss=4.468204

Batch 64530, train_perplexity=109.601364, train_loss=4.69685

Batch 64540, train_perplexity=108.47685, train_loss=4.686537

Batch 64550, train_perplexity=105.16243, train_loss=4.655506

Batch 64560, train_perplexity=95.879456, train_loss=4.5630918

Batch 64570, train_perplexity=90.27504, train_loss=4.502861

Batch 64580, train_perplexity=108.18303, train_loss=4.6838245

Batch 64590, train_perplexity=112.14122, train_loss=4.719759

Batch 64600, train_perplexity=105.69093, train_loss=4.660519

Batch 64610, train_perplexity=117.12266, train_loss=4.7632217

Batch 64620, train_perplexity=102.18075, train_loss=4.6267433

Batch 64630, train_perplexity=106.72177, train_loss=4.670225

Batch 64640, train_perplexity=112.23318, train_loss=4.7205787

Batch 64650, train_perplexity=104.57293, train_loss=4.6498847

Batch 64660, train_perplexity=112.327354, train_loss=4.7214174

Batch 64670, train_perplexity=99.20368, train_loss=4.597175

Batch 64680, train_perplexity=100.620575, train_loss=4.6113567

Batch 64690, train_perplexity=107.202835, train_loss=4.6747227

Batch 64700, train_perplexity=91.382126, train_loss=4.51505

Batch 64710, train_perplexity=101.29231, train_loss=4.6180105

Batch 64720, train_perplexity=110.95742, train_loss=4.7091465

Batch 64730, train_perplexity=99.47112, train_loss=4.5998673

Batch 64740, train_perplexity=99.359764, train_loss=4.5987473

Batch 64750, train_perplexity=95.64762, train_loss=4.560671

Batch 64760, train_perplexity=105.57095, train_loss=4.6593833

Batch 64770, train_perplexity=97.33498, train_loss=4.5781584

Batch 64780, train_perplexity=104.58076, train_loss=4.6499596

Batch 64790, train_perplexity=102.384476, train_loss=4.628735

Batch 64800, train_perplexity=105.12563, train_loss=4.655156

Batch 64810, train_perplexity=97.4606, train_loss=4.579448

Batch 64820, train_perplexity=109.509735, train_loss=4.6960135

Batch 64830, train_perplexity=111.31746, train_loss=4.712386

Batch 64840, train_perplexity=105.96894, train_loss=4.663146

Batch 64850, train_perplexity=98.722496, train_loss=4.592313

Batch 64860, train_perplexity=109.78627, train_loss=4.6985354

Batch 64870, train_perplexity=101.00047, train_loss=4.615125

Batch 64880, train_perplexity=104.65209, train_loss=4.6506414

Batch 64890, train_perplexity=104.11742, train_loss=4.6455193

Batch 64900, train_perplexity=88.09309, train_loss=4.478394

Batch 64910, train_perplexity=99.7751, train_loss=4.6029186

Batch 64920, train_perplexity=99.42355, train_loss=4.599389

Batch 64930, train_perplexity=99.01101, train_loss=4.595231

Batch 64940, train_perplexity=93.5931, train_loss=4.5389566

Batch 64950, train_perplexity=99.39294, train_loss=4.599081

Batch 64960, train_perplexity=102.78137, train_loss=4.632604

Batch 64970, train_perplexity=104.91751, train_loss=4.6531744

Batch 64980, train_perplexity=102.83191, train_loss=4.6330957

Batch 64990, train_perplexity=94.74404, train_loss=4.551179

Batch 65000, train_perplexity=105.21961, train_loss=4.6560497

Batch 65010, train_perplexity=107.62775, train_loss=4.6786785

Batch 65020, train_perplexity=95.0949, train_loss=4.5548754

Batch 65030, train_perplexity=96.72913, train_loss=4.5719147

Batch 65040, train_perplexity=108.29472, train_loss=4.6848564

Batch 65050, train_perplexity=101.984535, train_loss=4.624821

Batch 65060, train_perplexity=102.21789, train_loss=4.6271067

Batch 65070, train_perplexity=97.79144, train_loss=4.582837

Batch 65080, train_perplexity=96.95324, train_loss=4.574229

Batch 65090, train_perplexity=97.40406, train_loss=4.578868

Batch 65100, train_perplexity=107.54341, train_loss=4.6778946

Batch 65110, train_perplexity=90.73612, train_loss=4.5079556

Batch 65120, train_perplexity=91.675156, train_loss=4.5182514

Batch 65130, train_perplexity=91.327896, train_loss=4.5144563

Batch 65140, train_perplexity=90.06509, train_loss=4.5005326

Batch 65150, train_perplexity=103.25713, train_loss=4.6372223

Batch 65160, train_perplexity=101.386635, train_loss=4.6189413

Batch 65170, train_perplexity=92.6289, train_loss=4.528601
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65180, train_perplexity=94.03615, train_loss=4.543679

Batch 65190, train_perplexity=96.143616, train_loss=4.565843

Batch 65200, train_perplexity=106.2837, train_loss=4.666112

Batch 65210, train_perplexity=105.33749, train_loss=4.6571693

Batch 65220, train_perplexity=106.55234, train_loss=4.6686363

Batch 65230, train_perplexity=116.37812, train_loss=4.7568445

Batch 65240, train_perplexity=112.226494, train_loss=4.720519

Batch 65250, train_perplexity=103.72812, train_loss=4.641773

Batch 65260, train_perplexity=100.35224, train_loss=4.6086864

Batch 65270, train_perplexity=107.65875, train_loss=4.6789665

Batch 65280, train_perplexity=90.21531, train_loss=4.502199

Batch 65290, train_perplexity=106.31589, train_loss=4.6664147

Batch 65300, train_perplexity=105.00209, train_loss=4.6539803

Batch 65310, train_perplexity=115.174515, train_loss=4.7464485

Batch 65320, train_perplexity=107.539055, train_loss=4.677854

Batch 65330, train_perplexity=96.083305, train_loss=4.5652156

Batch 65340, train_perplexity=95.05401, train_loss=4.5544453

Batch 65350, train_perplexity=107.58973, train_loss=4.678325

Batch 65360, train_perplexity=103.197174, train_loss=4.6366415

Batch 65370, train_perplexity=90.820404, train_loss=4.508884

Batch 65380, train_perplexity=106.250565, train_loss=4.6658

Batch 65390, train_perplexity=104.791214, train_loss=4.65197

Batch 65400, train_perplexity=105.96136, train_loss=4.6630745

Batch 65410, train_perplexity=98.29692, train_loss=4.5879927

Batch 65420, train_perplexity=95.93507, train_loss=4.5636716

Batch 65430, train_perplexity=104.19032, train_loss=4.6462193

Batch 65440, train_perplexity=97.55983, train_loss=4.580466

Batch 65450, train_perplexity=105.33628, train_loss=4.657158

Batch 65460, train_perplexity=105.63546, train_loss=4.659994

Batch 65470, train_perplexity=99.238266, train_loss=4.5975237

Batch 65480, train_perplexity=107.204056, train_loss=4.674734

Batch 65490, train_perplexity=104.19748, train_loss=4.646288

Batch 65500, train_perplexity=104.0921, train_loss=4.645276

Batch 65510, train_perplexity=112.52131, train_loss=4.7231426

Batch 65520, train_perplexity=111.843346, train_loss=4.717099

Batch 65530, train_perplexity=97.692314, train_loss=4.581823

Batch 65540, train_perplexity=96.21328, train_loss=4.5665674

Batch 65550, train_perplexity=100.89809, train_loss=4.614111

Batch 65560, train_perplexity=97.27698, train_loss=4.5775623

Batch 65570, train_perplexity=97.11485, train_loss=4.5758944

Batch 65580, train_perplexity=93.864876, train_loss=4.5418563

Batch 65590, train_perplexity=90.77131, train_loss=4.508343

Batch 65600, train_perplexity=96.84008, train_loss=4.573061

Batch 65610, train_perplexity=98.51192, train_loss=4.5901775

Batch 65620, train_perplexity=100.61731, train_loss=4.6113243

Batch 65630, train_perplexity=106.037735, train_loss=4.663795

Batch 65640, train_perplexity=98.605865, train_loss=4.5911307

Batch 65650, train_perplexity=111.62692, train_loss=4.7151623

Batch 65660, train_perplexity=104.23902, train_loss=4.6466866

Batch 65670, train_perplexity=104.83219, train_loss=4.652361

Batch 65680, train_perplexity=104.835045, train_loss=4.652388

Batch 65690, train_perplexity=95.31262, train_loss=4.5571623

Batch 65700, train_perplexity=102.02082, train_loss=4.625177

Batch 65710, train_perplexity=101.64998, train_loss=4.6215353

Batch 65720, train_perplexity=93.95919, train_loss=4.5428605

Batch 65730, train_perplexity=98.70753, train_loss=4.592161

Batch 65740, train_perplexity=95.18019, train_loss=4.555772

Batch 65750, train_perplexity=95.59938, train_loss=4.5601664

Batch 65760, train_perplexity=105.86303, train_loss=4.662146

Batch 65770, train_perplexity=91.83809, train_loss=4.520027

Batch 65780, train_perplexity=98.090996, train_loss=4.5858955

Batch 65790, train_perplexity=104.91511, train_loss=4.6531515

Batch 65800, train_perplexity=113.57689, train_loss=4.73248

Batch 65810, train_perplexity=108.97331, train_loss=4.691103

Batch 65820, train_perplexity=105.301025, train_loss=4.656823

Batch 65830, train_perplexity=95.84842, train_loss=4.562768

Batch 65840, train_perplexity=109.51809, train_loss=4.6960897

Batch 65850, train_perplexity=99.186844, train_loss=4.5970054

Batch 65860, train_perplexity=113.18708, train_loss=4.729042

Batch 65870, train_perplexity=95.4798, train_loss=4.5589147

Batch 65880, train_perplexity=101.93043, train_loss=4.6242905

Batch 65890, train_perplexity=103.84333, train_loss=4.6428833

Batch 65900, train_perplexity=93.23604, train_loss=4.5351343

Batch 65910, train_perplexity=94.82445, train_loss=4.552027

Batch 65920, train_perplexity=112.101555, train_loss=4.719405

Batch 65930, train_perplexity=110.314384, train_loss=4.7033343

Batch 65940, train_perplexity=94.6133, train_loss=4.549798

Batch 65950, train_perplexity=96.028114, train_loss=4.564641

Batch 65960, train_perplexity=110.70712, train_loss=4.706888

Batch 65970, train_perplexity=102.90706, train_loss=4.6338263

Batch 65980, train_perplexity=101.0445, train_loss=4.615561

Batch 65990, train_perplexity=112.86662, train_loss=4.726207

Batch 66000, train_perplexity=89.20608, train_loss=4.490949

Batch 66010, train_perplexity=96.45748, train_loss=4.5691023

Batch 66020, train_perplexity=85.95952, train_loss=4.4538765

Batch 66030, train_perplexity=108.85443, train_loss=4.6900115

Batch 66040, train_perplexity=100.46045, train_loss=4.609764

Batch 66050, train_perplexity=108.8983, train_loss=4.6904144

Batch 66060, train_perplexity=95.67171, train_loss=4.5609226

Batch 66070, train_perplexity=101.67882, train_loss=4.621819

Batch 66080, train_perplexity=107.36418, train_loss=4.6762266

Batch 66090, train_perplexity=107.6024, train_loss=4.678443

Batch 66100, train_perplexity=107.205284, train_loss=4.6747456

Batch 66110, train_perplexity=106.0837, train_loss=4.6642284

Batch 66120, train_perplexity=102.514854, train_loss=4.6300077

Batch 66130, train_perplexity=100.64571, train_loss=4.6116066

Batch 66140, train_perplexity=95.65569, train_loss=4.5607553

Batch 66150, train_perplexity=99.08029, train_loss=4.5959306

Batch 66160, train_perplexity=101.571045, train_loss=4.6207585

Batch 66170, train_perplexity=105.319954, train_loss=4.657003

Batch 66180, train_perplexity=99.332664, train_loss=4.5984745

Batch 66190, train_perplexity=105.26217, train_loss=4.656454

Batch 66200, train_perplexity=102.16706, train_loss=4.6266093

Batch 66210, train_perplexity=95.606766, train_loss=4.5602436

Batch 66220, train_perplexity=113.47295, train_loss=4.7315645

Batch 66230, train_perplexity=105.48064, train_loss=4.6585274

Batch 66240, train_perplexity=102.105644, train_loss=4.626008

Batch 66250, train_perplexity=97.3049, train_loss=4.5778494

Batch 66260, train_perplexity=102.63969, train_loss=4.6312246

Batch 66270, train_perplexity=103.53165, train_loss=4.6398773

Batch 66280, train_perplexity=101.88878, train_loss=4.623882

Batch 66290, train_perplexity=95.58133, train_loss=4.5599775

Batch 66300, train_perplexity=102.03814, train_loss=4.6253467

Batch 66310, train_perplexity=99.07543, train_loss=4.5958815

Batch 66320, train_perplexity=115.61118, train_loss=4.7502327

Batch 66330, train_perplexity=99.72772, train_loss=4.6024437

Batch 66340, train_perplexity=92.745445, train_loss=4.5298586

Batch 66350, train_perplexity=104.46552, train_loss=4.648857

Batch 66360, train_perplexity=104.590034, train_loss=4.6500483

Batch 66370, train_perplexity=99.11833, train_loss=4.5963144

Batch 66380, train_perplexity=112.779305, train_loss=4.725433

Batch 66390, train_perplexity=97.87234, train_loss=4.583664

Batch 66400, train_perplexity=97.28996, train_loss=4.577696

Batch 66410, train_perplexity=98.24593, train_loss=4.587474

Batch 66420, train_perplexity=111.850174, train_loss=4.71716

Batch 66430, train_perplexity=104.788216, train_loss=4.6519413

Batch 66440, train_perplexity=111.16682, train_loss=4.711032

Batch 66450, train_perplexity=95.29136, train_loss=4.556939

Batch 66460, train_perplexity=125.34859, train_loss=4.8310986

Batch 66470, train_perplexity=90.941696, train_loss=4.5102186

Batch 66480, train_perplexity=111.08235, train_loss=4.710272

Batch 66490, train_perplexity=110.18427, train_loss=4.702154

Batch 66500, train_perplexity=101.17733, train_loss=4.6168747

Batch 66510, train_perplexity=88.10296, train_loss=4.478506

Batch 66520, train_perplexity=110.61162, train_loss=4.706025

Batch 66530, train_perplexity=107.74041, train_loss=4.6797247

Batch 66540, train_perplexity=95.13236, train_loss=4.5552692

Batch 66550, train_perplexity=111.247734, train_loss=4.7117596

Batch 66560, train_perplexity=102.738106, train_loss=4.632183

Batch 66570, train_perplexity=106.17282, train_loss=4.665068

Batch 66580, train_perplexity=101.200874, train_loss=4.6171074

Batch 66590, train_perplexity=111.536476, train_loss=4.7143517

Batch 66600, train_perplexity=80.63291, train_loss=4.389907

Batch 66610, train_perplexity=101.450424, train_loss=4.6195703

Batch 66620, train_perplexity=100.70875, train_loss=4.6122327

Batch 66630, train_perplexity=93.015785, train_loss=4.532769

Batch 66640, train_perplexity=101.16257, train_loss=4.616729

Batch 66650, train_perplexity=98.35975, train_loss=4.5886316

Batch 66660, train_perplexity=114.43417, train_loss=4.74

Batch 66670, train_perplexity=111.309235, train_loss=4.712312

Batch 66680, train_perplexity=115.68486, train_loss=4.7508698

Batch 66690, train_perplexity=96.05202, train_loss=4.56489

Batch 66700, train_perplexity=105.47329, train_loss=4.6584578

Batch 66710, train_perplexity=99.42432, train_loss=4.5993967

Batch 66720, train_perplexity=112.23901, train_loss=4.7206306

Batch 66730, train_perplexity=94.3789, train_loss=4.5473175

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 66740, train_perplexity=106.68249, train_loss=4.669857

Batch 66750, train_perplexity=98.19002, train_loss=4.5869045

Batch 66760, train_perplexity=99.96658, train_loss=4.604836

Batch 66770, train_perplexity=97.993004, train_loss=4.584896

Batch 66780, train_perplexity=100.63295, train_loss=4.6114798

Batch 66790, train_perplexity=100.58378, train_loss=4.610991

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 66800, train_perplexity=93.82, train_loss=4.541378

Batch 66810, train_perplexity=93.597786, train_loss=4.5390067

Batch 66820, train_perplexity=96.98991, train_loss=4.574607

Batch 66830, train_perplexity=92.103676, train_loss=4.522915

Batch 66840, train_perplexity=90.36204, train_loss=4.503824

Batch 66850, train_perplexity=104.77822, train_loss=4.651846

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 66860, train_perplexity=108.14672, train_loss=4.683489

Batch 66870, train_perplexity=89.974434, train_loss=4.4995255

Batch 66880, train_perplexity=93.4471, train_loss=4.5373955

Batch 66890, train_perplexity=99.26903, train_loss=4.5978336

Batch 66900, train_perplexity=93.75126, train_loss=4.540645

Batch 66910, train_perplexity=101.00697, train_loss=4.6151896

Batch 66920, train_perplexity=98.772026, train_loss=4.5928144

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 66930, train_perplexity=96.24205, train_loss=4.5668664

Batch 66940, train_perplexity=90.18383, train_loss=4.50185

Batch 66950, train_perplexity=110.01082, train_loss=4.7005787

Batch 66960, train_perplexity=108.17529, train_loss=4.683753

Batch 66970, train_perplexity=92.74978, train_loss=4.5299053

Batch 66980, train_perplexity=97.81766, train_loss=4.583105

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 66990, train_perplexity=94.693, train_loss=4.55064

Batch 67000, train_perplexity=98.43191, train_loss=4.589365

Batch 67010, train_perplexity=103.58552, train_loss=4.6403975

Batch 67020, train_perplexity=108.533035, train_loss=4.6870546

Batch 67030, train_perplexity=104.646904, train_loss=4.650592

Batch 67040, train_perplexity=96.78173, train_loss=4.5724583

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 67050, train_perplexity=96.502655, train_loss=4.5695705

Batch 67060, train_perplexity=100.90324, train_loss=4.614162

Batch 67070, train_perplexity=96.41311, train_loss=4.568642

Batch 67080, train_perplexity=87.3201, train_loss=4.4695807

Batch 67090, train_perplexity=91.23044, train_loss=4.5133886

Batch 67100, train_perplexity=103.8412, train_loss=4.642863

Batch 67110, train_perplexity=102.840836, train_loss=4.6331825

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 67120, train_perplexity=92.697075, train_loss=4.529337

Batch 67130, train_perplexity=106.53415, train_loss=4.6684656

Batch 67140, train_perplexity=101.18273, train_loss=4.616928

Batch 67150, train_perplexity=93.49122, train_loss=4.5378675

Batch 67160, train_perplexity=98.47895, train_loss=4.589843

Batch 67170, train_perplexity=101.55928, train_loss=4.6206427

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 67180, train_perplexity=96.94524, train_loss=4.5741463

Batch 67190, train_perplexity=98.600266, train_loss=4.591074

Batch 67200, train_perplexity=105.34804, train_loss=4.6572695

Batch 67210, train_perplexity=87.58669, train_loss=4.472629

Batch 67220, train_perplexity=102.89043, train_loss=4.6336646

Batch 67230, train_perplexity=112.81357, train_loss=4.7257366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loadingWARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 67240, train_perplexity=95.85381, train_loss=4.5628242

Batch 67250, train_perplexity=108.31848, train_loss=4.6850758

Batch 67260, train_perplexity=98.46777, train_loss=4.5897293

Batch 67270, train_perplexity=100.04846, train_loss=4.6056547

Batch 67280, train_perplexity=96.62449, train_loss=4.5708323

Batch 67290, train_perplexity=98.266174, train_loss=4.58768

Batch 67300, train_perplexity=100.12444, train_loss=4.606414

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 67310, train_perplexity=104.40398, train_loss=4.6482677

Batch 67320, train_perplexity=100.85056, train_loss=4.61364

Batch 67330, train_perplexity=99.20676, train_loss=4.597206

Batch 67340, train_perplexity=93.30186, train_loss=4.53584

Batch 67350, train_perplexity=95.71593, train_loss=4.5613847

Batch 67360, train_perplexity=94.33436, train_loss=4.5468454

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 67370, train_perplexity=89.06692, train_loss=4.489388

Batch 67380, train_perplexity=96.63113, train_loss=4.570901

Batch 67390, train_perplexity=104.689026, train_loss=4.6509943

Batch 67400, train_perplexity=112.614815, train_loss=4.7239733

Batch 67410, train_perplexity=92.87131, train_loss=4.5312147

Batch 67420, train_perplexity=87.599556, train_loss=4.472776

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 67430, train_perplexity=109.30424, train_loss=4.694135

Batch 67440, train_perplexity=102.529915, train_loss=4.6301546

Batch 67450, train_perplexity=98.73059, train_loss=4.592395

Batch 67460, train_perplexity=100.03234, train_loss=4.6054935

Batch 67470, train_perplexity=89.45159, train_loss=4.4936976

Batch 67480, train_perplexity=99.65523, train_loss=4.6017165

Batch 67490, train_perplexity=105.5841, train_loss=4.6595078

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 67500, train_perplexity=99.99004, train_loss=4.6050706

Batch 67510, train_perplexity=95.763855, train_loss=4.5618854

Batch 67520, train_perplexity=103.12192, train_loss=4.635912

Batch 67530, train_perplexity=89.1903, train_loss=4.4907722

Batch 67540, train_perplexity=100.80835, train_loss=4.613221

Batch 67550, train_perplexity=92.36589, train_loss=4.525758

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 67560, train_perplexity=95.99945, train_loss=4.5643425

Batch 67570, train_perplexity=109.96288, train_loss=4.700143

Batch 67580, train_perplexity=94.56246, train_loss=4.5492606

Batch 67590, train_perplexity=90.88777, train_loss=4.5096254

Batch 67600, train_perplexity=106.52409, train_loss=4.668371

Batch 67610, train_perplexity=101.677315, train_loss=4.621804

Batch 67620, train_perplexity=95.39137, train_loss=4.557988

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 67630, train_perplexity=105.89817, train_loss=4.662478

Batch 67640, train_perplexity=112.7884, train_loss=4.7255135

Batch 67650, train_perplexity=96.16283, train_loss=4.566043

Batch 67660, train_perplexity=107.16466, train_loss=4.6743665

Batch 67670, train_perplexity=101.08951, train_loss=4.6160064

Batch 67680, train_perplexity=91.27874, train_loss=4.513918

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 67690, train_perplexity=101.995575, train_loss=4.6249294

Batch 67700, train_perplexity=89.74857, train_loss=4.497012

Batch 67710, train_perplexity=101.962944, train_loss=4.6246095

Batch 67720, train_perplexity=96.12134, train_loss=4.5656114

Batch 67730, train_perplexity=113.740944, train_loss=4.7339234

Batch 67740, train_perplexity=101.402596, train_loss=4.6190987

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 67750, train_perplexity=97.56564, train_loss=4.5805254

Batch 67760, train_perplexity=94.52261, train_loss=4.548839

Batch 67770, train_perplexity=87.84694, train_loss=4.475596

Batch 67780, train_perplexity=98.19105, train_loss=4.586915

Batch 67790, train_perplexity=95.703285, train_loss=4.5612526

Batch 67800, train_perplexity=98.4419, train_loss=4.5894666

Batch 67810, train_perplexity=100.69799, train_loss=4.612126

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 67820, train_perplexity=100.168236, train_loss=4.606851

Batch 67830, train_perplexity=104.87109, train_loss=4.652732

Batch 67840, train_perplexity=98.875465, train_loss=4.593861

Batch 67850, train_perplexity=102.55549, train_loss=4.630404

Batch 67860, train_perplexity=97.887596, train_loss=4.58382

Batch 67870, train_perplexity=105.75521, train_loss=4.661127

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 67880, train_perplexity=92.70415, train_loss=4.529413

Batch 67890, train_perplexity=97.93662, train_loss=4.5843205

Batch 67900, train_perplexity=100.081245, train_loss=4.6059823

Batch 67910, train_perplexity=103.88949, train_loss=4.6433277

Batch 67920, train_perplexity=103.28185, train_loss=4.6374617

Batch 67930, train_perplexity=89.38875, train_loss=4.492995

Batch 67940, train_perplexity=103.03197, train_loss=4.6350393

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 67950, train_perplexity=87.52186, train_loss=4.4718885

Batch 67960, train_perplexity=98.82187, train_loss=4.593319

Batch 67970, train_perplexity=97.15738, train_loss=4.576332

Batch 67980, train_perplexity=91.40688, train_loss=4.515321

Batch 67990, train_perplexity=108.86258, train_loss=4.6900864

Batch 68000, train_perplexity=87.884224, train_loss=4.4760203

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 68010, train_perplexity=96.61205, train_loss=4.5707035

Batch 68020, train_perplexity=84.68267, train_loss=4.438911

Batch 68030, train_perplexity=106.67221, train_loss=4.6697607

Batch 68040, train_perplexity=87.53504, train_loss=4.472039

Batch 68050, train_perplexity=110.955086, train_loss=4.7091255

Batch 68060, train_perplexity=99.82026, train_loss=4.603371

Batch 68070, train_perplexity=86.338104, train_loss=4.458271

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 68080, train_perplexity=92.35387, train_loss=4.5256276

Batch 68090, train_perplexity=97.40848, train_loss=4.578913

Batch 68100, train_perplexity=101.56277, train_loss=4.620677

Batch 68110, train_perplexity=105.93756, train_loss=4.66285

Batch 68120, train_perplexity=86.513, train_loss=4.4602947

Batch 68130, train_perplexity=92.580635, train_loss=4.52808

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 68140, train_perplexity=102.391945, train_loss=4.628808

Batch 68150, train_perplexity=100.08793, train_loss=4.606049

Batch 68160, train_perplexity=99.14395, train_loss=4.596573

Batch 68170, train_perplexity=98.5809, train_loss=4.5908775

Batch 68180, train_perplexity=93.230255, train_loss=4.5350723

Batch 68190, train_perplexity=100.36038, train_loss=4.6087675

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 68200, train_perplexity=97.18675, train_loss=4.5766344

Batch 68210, train_perplexity=92.5813, train_loss=4.528087

Batch 68220, train_perplexity=99.98904, train_loss=4.6050606

Batch 68230, train_perplexity=93.150185, train_loss=4.534213

Batch 68240, train_perplexity=109.32838, train_loss=4.694356

Batch 68250, train_perplexity=91.079094, train_loss=4.5117283

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 68260, train_perplexity=101.9452, train_loss=4.6244354

Batch 68270, train_perplexity=93.82608, train_loss=4.541443

Batch 68280, train_perplexity=94.97119, train_loss=4.5535736

Batch 68290, train_perplexity=99.26851, train_loss=4.5978284

Batch 68300, train_perplexity=101.344154, train_loss=4.618522

Batch 68310, train_perplexity=91.999916, train_loss=4.5217876

Batch 68320, train_perplexity=104.12069, train_loss=4.6455507

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 68330, train_perplexity=96.288185, train_loss=4.5673456

Batch 68340, train_perplexity=89.49989, train_loss=4.4942374

Batch 68350, train_perplexity=98.32589, train_loss=4.5882874

Batch 68360, train_perplexity=90.00812, train_loss=4.4999

Batch 68370, train_perplexity=103.70972, train_loss=4.641596

Batch 68380, train_perplexity=95.97817, train_loss=4.564121

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 68390, train_perplexity=94.284164, train_loss=4.5463133

Batch 68400, train_perplexity=103.76373, train_loss=4.6421165

Batch 68410, train_perplexity=91.16934, train_loss=4.5127187

Batch 68420, train_perplexity=99.89182, train_loss=4.604088

Batch 68430, train_perplexity=94.089165, train_loss=4.544243

Batch 68440, train_perplexity=102.555534, train_loss=4.6304045

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 68450, train_perplexity=100.97567, train_loss=4.6148796

Batch 68460, train_perplexity=101.97952, train_loss=4.624772

Batch 68470, train_perplexity=113.10971, train_loss=4.7283583

Batch 68480, train_perplexity=97.79275, train_loss=4.5828505

Batch 68490, train_perplexity=108.05811, train_loss=4.682669

Batch 68500, train_perplexity=92.44318, train_loss=4.526594

Batch 68510, train_perplexity=99.86196, train_loss=4.603789

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 68520, train_perplexity=98.62373, train_loss=4.591312

Batch 68530, train_perplexity=94.15065, train_loss=4.544896

Batch 68540, train_perplexity=99.236755, train_loss=4.5975084

Batch 68550, train_perplexity=99.90335, train_loss=4.604203

Batch 68560, train_perplexity=93.86036, train_loss=4.541808

Batch 68570, train_perplexity=96.5485, train_loss=4.5700455

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 68580, train_perplexity=104.329124, train_loss=4.6475506

Batch 68590, train_perplexity=103.11444, train_loss=4.6358395

Batch 68600, train_perplexity=96.293785, train_loss=4.567404

Batch 68610, train_perplexity=93.98419, train_loss=4.5431266

Batch 68620, train_perplexity=92.50672, train_loss=4.5272813

Batch 68630, train_perplexity=92.59666, train_loss=4.528253

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 68640, train_perplexity=108.704216, train_loss=4.6886306

Batch 68650, train_perplexity=93.6035, train_loss=4.5390677

Batch 68660, train_perplexity=95.016174, train_loss=4.554047

Batch 68670, train_perplexity=98.98986, train_loss=4.5950174

Batch 68680, train_perplexity=85.396225, train_loss=4.447302

Batch 68690, train_perplexity=103.69845, train_loss=4.641487

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 68700, train_perplexity=106.35417, train_loss=4.6667747

Batch 68710, train_perplexity=105.926605, train_loss=4.6627464

Batch 68720, train_perplexity=84.492004, train_loss=4.436657

Batch 68730, train_perplexity=93.05997, train_loss=4.533244

Batch 68740, train_perplexity=101.439926, train_loss=4.619467

Batch 68750, train_perplexity=103.51486, train_loss=4.639715

Batch 68760, train_perplexity=91.94264, train_loss=4.521165

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 68770, train_perplexity=103.12005, train_loss=4.635894

Batch 68780, train_perplexity=99.6895, train_loss=4.6020603

Batch 68790, train_perplexity=97.74604, train_loss=4.5823727

Batch 68800, train_perplexity=109.70761, train_loss=4.6978188

Batch 68810, train_perplexity=96.1765, train_loss=4.566185

Batch 68820, train_perplexity=89.57158, train_loss=4.495038

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 68830, train_perplexity=102.159805, train_loss=4.6265383

Batch 68840, train_perplexity=94.298195, train_loss=4.546462

Batch 68850, train_perplexity=105.942566, train_loss=4.662897

Batch 68860, train_perplexity=96.11992, train_loss=4.5655966

Batch 68870, train_perplexity=100.98386, train_loss=4.6149607

Batch 68880, train_perplexity=93.04648, train_loss=4.533099

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 68890, train_perplexity=98.89937, train_loss=4.594103

Batch 68900, train_perplexity=98.49656, train_loss=4.5900216

Batch 68910, train_perplexity=100.97264, train_loss=4.6148496

Batch 68920, train_perplexity=96.30866, train_loss=4.5675583

Batch 68930, train_perplexity=98.808014, train_loss=4.5931787

Batch 68940, train_perplexity=116.22739, train_loss=4.7555485

Batch 68950, train_perplexity=109.5066, train_loss=4.695985

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 68960, train_perplexity=96.10397, train_loss=4.5654306

Batch 68970, train_perplexity=102.75447, train_loss=4.6323423

Batch 68980, train_perplexity=90.91811, train_loss=4.509959

Batch 68990, train_perplexity=90.7988, train_loss=4.508646

Batch 69000, train_perplexity=104.08654, train_loss=4.6452227

Batch 69010, train_perplexity=97.301056, train_loss=4.57781

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 69020, train_perplexity=96.10837, train_loss=4.5654764

Batch 69030, train_perplexity=87.20937, train_loss=4.468312

Batch 69040, train_perplexity=105.89615, train_loss=4.662459

Batch 69050, train_perplexity=92.76296, train_loss=4.5300474

Batch 69060, train_perplexity=89.174904, train_loss=4.4905996

Batch 69070, train_perplexity=112.70808, train_loss=4.724801

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 69080, train_perplexity=100.85378, train_loss=4.613672

Batch 69090, train_perplexity=96.03663, train_loss=4.5647297

Batch 69100, train_perplexity=101.38466, train_loss=4.6189218

Batch 69110, train_perplexity=96.109055, train_loss=4.5654836

Batch 69120, train_perplexity=104.94333, train_loss=4.6534204

Batch 69130, train_perplexity=95.17425, train_loss=4.5557094

Batch 69140, train_perplexity=105.0393, train_loss=4.6543345

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 69150, train_perplexity=104.64076, train_loss=4.650533

Batch 69160, train_perplexity=97.708755, train_loss=4.581991

Batch 69170, train_perplexity=102.61443, train_loss=4.6309786

Batch 69180, train_perplexity=98.74538, train_loss=4.5925446

Batch 69190, train_perplexity=100.844406, train_loss=4.613579

Batch 69200, train_perplexity=122.28467, train_loss=4.8063517

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 69210, train_perplexity=102.35641, train_loss=4.628461

Batch 69220, train_perplexity=107.50701, train_loss=4.677556

Batch 69230, train_perplexity=92.43727, train_loss=4.5265303

Batch 69240, train_perplexity=91.17287, train_loss=4.5127573

Batch 69250, train_perplexity=102.39146, train_loss=4.6288033

Batch 69260, train_perplexity=91.89101, train_loss=4.520603

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 69270, train_perplexity=91.083694, train_loss=4.511779

Batch 69280, train_perplexity=100.063065, train_loss=4.6058006

Batch 69290, train_perplexity=88.33356, train_loss=4.48112

Batch 69300, train_perplexity=98.18224, train_loss=4.5868254

Batch 69310, train_perplexity=98.03703, train_loss=4.5853453

Batch 69320, train_perplexity=97.68183, train_loss=4.5817156

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 69330, train_perplexity=103.435814, train_loss=4.6389513

Batch 69340, train_perplexity=96.98967, train_loss=4.5746045

Batch 69350, train_perplexity=98.22485, train_loss=4.5872593

Batch 69360, train_perplexity=96.88793, train_loss=4.573555

Batch 69370, train_perplexity=99.22866, train_loss=4.597427

Batch 69380, train_perplexity=103.46887, train_loss=4.639271

Batch 69390, train_perplexity=98.54387, train_loss=4.590502

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 69400, train_perplexity=109.02331, train_loss=4.6915617

Batch 69410, train_perplexity=109.40681, train_loss=4.695073

Batch 69420, train_perplexity=95.40812, train_loss=4.5581636

Batch 69430, train_perplexity=101.01362, train_loss=4.6152554

Batch 69440, train_perplexity=91.533195, train_loss=4.5167017

Batch 69450, train_perplexity=115.73628, train_loss=4.751314

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 69460, train_perplexity=104.99548, train_loss=4.6539173

Batch 69470, train_perplexity=94.84773, train_loss=4.552273

Batch 69480, train_perplexity=103.41279, train_loss=4.6387286

Batch 69490, train_perplexity=95.282, train_loss=4.556841

Batch 69500, train_perplexity=98.84421, train_loss=4.593545

Batch 69510, train_perplexity=103.16834, train_loss=4.636362

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 69520, train_perplexity=97.39951, train_loss=4.578821

Batch 69530, train_perplexity=105.08644, train_loss=4.6547832

Batch 69540, train_perplexity=95.159454, train_loss=4.555554

Batch 69550, train_perplexity=105.46806, train_loss=4.658408

Batch 69560, train_perplexity=104.76863, train_loss=4.6517544

Batch 69570, train_perplexity=103.96779, train_loss=4.644081

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 69580, train_perplexity=102.10633, train_loss=4.6260147

Batch 69590, train_perplexity=115.66842, train_loss=4.7507277

Batch 69600, train_perplexity=96.68187, train_loss=4.571426

Batch 69610, train_perplexity=105.95055, train_loss=4.6629725

Batch 69620, train_perplexity=99.44414, train_loss=4.599596

Batch 69630, train_perplexity=99.499344, train_loss=4.600151

Batch 69640, train_perplexity=106.61465, train_loss=4.669221

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 69650, train_perplexity=93.35589, train_loss=4.536419

Batch 69660, train_perplexity=96.5654, train_loss=4.5702205

Batch 69670, train_perplexity=96.06411, train_loss=4.565016

Batch 69680, train_perplexity=97.9292, train_loss=4.5842447

Batch 69690, train_perplexity=100.509895, train_loss=4.610256

Batch 69700, train_perplexity=108.22008, train_loss=4.684167

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 69710, train_perplexity=101.80976, train_loss=4.623106

Batch 69720, train_perplexity=94.80478, train_loss=4.55182

Batch 69730, train_perplexity=114.113556, train_loss=4.737194

Batch 69740, train_perplexity=92.98253, train_loss=4.5324116

Batch 69750, train_perplexity=97.83193, train_loss=4.583251

Batch 69760, train_perplexity=97.111244, train_loss=4.575857

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 69770, train_perplexity=102.89352, train_loss=4.6336946

Batch 69780, train_perplexity=100.20067, train_loss=4.607175
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69790, train_perplexity=102.51603, train_loss=4.630019

Batch 69800, train_perplexity=95.343445, train_loss=4.5574856

Batch 69810, train_perplexity=104.841095, train_loss=4.652446

Batch 69820, train_perplexity=95.30181, train_loss=4.557049

Batch 69830, train_perplexity=99.7058, train_loss=4.602224

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 69840, train_perplexity=116.34749, train_loss=4.7565813

Batch 69850, train_perplexity=87.99766, train_loss=4.47731

Batch 69860, train_perplexity=86.74598, train_loss=4.462984

Batch 69870, train_perplexity=95.64612, train_loss=4.560655

Batch 69880, train_perplexity=101.964066, train_loss=4.6246204

Batch 69890, train_perplexity=108.91606, train_loss=4.6905775

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 69900, train_perplexity=96.87435, train_loss=4.573415

Batch 69910, train_perplexity=100.03177, train_loss=4.605488

Batch 69920, train_perplexity=90.96191, train_loss=4.510441

Batch 69930, train_perplexity=98.84553, train_loss=4.5935583

Batch 69940, train_perplexity=96.99148, train_loss=4.574623

Batch 69950, train_perplexity=89.97293, train_loss=4.499509

Batch 69960, train_perplexity=91.00373, train_loss=4.5109005

Batch 69970, train_perplexity=89.84029, train_loss=4.4980335

Batch 69980, train_perplexity=93.988266, train_loss=4.54317

Batch 69990, train_perplexity=97.800026, train_loss=4.582925

Batch 70000, train_perplexity=102.01372, train_loss=4.6251073

Batch 70010, train_perplexity=98.55058, train_loss=4.59057

Batch 70020, train_perplexity=85.13618, train_loss=4.444252

Batch 70030, train_perplexity=94.38115, train_loss=4.5473413

Batch 70040, train_perplexity=94.82765, train_loss=4.552061

Batch 70050, train_perplexity=95.78048, train_loss=4.562059

Batch 70060, train_perplexity=83.40537, train_loss=4.4237127

Batch 70070, train_perplexity=95.72656, train_loss=4.561496

Batch 70080, train_perplexity=108.31951, train_loss=4.6850853

Batch 70090, train_perplexity=87.311104, train_loss=4.4694777

Batch 70100, train_perplexity=97.012474, train_loss=4.5748396

Batch 70110, train_perplexity=83.47277, train_loss=4.4245205

Batch 70120, train_perplexity=96.471275, train_loss=4.5692453

Batch 70130, train_perplexity=87.580635, train_loss=4.47256

Batch 70140, train_perplexity=97.67442, train_loss=4.58164

Batch 70150, train_perplexity=103.91148, train_loss=4.6435394

Batch 70160, train_perplexity=87.488434, train_loss=4.4715066

Batch 70170, train_perplexity=91.189735, train_loss=4.5129423

Batch 70180, train_perplexity=95.57285, train_loss=4.559889

Batch 70190, train_perplexity=102.53676, train_loss=4.6302214

Batch 70200, train_perplexity=95.15473, train_loss=4.5555043

Batch 70210, train_perplexity=93.91515, train_loss=4.542392

Batch 70220, train_perplexity=92.65072, train_loss=4.5288367

Batch 70230, train_perplexity=85.2248, train_loss=4.4452925

Batch 70240, train_perplexity=93.74264, train_loss=4.540553

Batch 70250, train_perplexity=101.87717, train_loss=4.623768

Batch 70260, train_perplexity=93.315384, train_loss=4.535985

Batch 70270, train_perplexity=99.56589, train_loss=4.6008196

Batch 70280, train_perplexity=88.161285, train_loss=4.479168

Batch 70290, train_perplexity=96.67127, train_loss=4.5713162

Batch 70300, train_perplexity=107.4367, train_loss=4.676902

Batch 70310, train_perplexity=102.70813, train_loss=4.6318913

Batch 70320, train_perplexity=98.93423, train_loss=4.5944552

Batch 70330, train_perplexity=100.257355, train_loss=4.6077404

Batch 70340, train_perplexity=94.630486, train_loss=4.5499797

Batch 70350, train_perplexity=101.45251, train_loss=4.6195908

Batch 70360, train_perplexity=82.74218, train_loss=4.4157295

Batch 70370, train_perplexity=94.766266, train_loss=4.5514135

Batch 70380, train_perplexity=93.25365, train_loss=4.535323

Batch 70390, train_perplexity=92.89903, train_loss=4.531513

Batch 70400, train_perplexity=92.59277, train_loss=4.528211

Batch 70410, train_perplexity=103.03678, train_loss=4.635086

Batch 70420, train_perplexity=94.13197, train_loss=4.5446978

Batch 70430, train_perplexity=103.14198, train_loss=4.6361065

Batch 70440, train_perplexity=88.11871, train_loss=4.478685

Batch 70450, train_perplexity=99.70238, train_loss=4.6021895

Batch 70460, train_perplexity=103.383354, train_loss=4.638444

Batch 70470, train_perplexity=90.34106, train_loss=4.503592

Batch 70480, train_perplexity=94.45282, train_loss=4.5481005

Batch 70490, train_perplexity=108.06105, train_loss=4.6826963

Batch 70500, train_perplexity=86.9329, train_loss=4.4651365

Batch 70510, train_perplexity=105.01451, train_loss=4.6540985

Batch 70520, train_perplexity=99.44333, train_loss=4.599588

Batch 70530, train_perplexity=98.41454, train_loss=4.5891886

Batch 70540, train_perplexity=105.53432, train_loss=4.659036

Batch 70550, train_perplexity=92.58086, train_loss=4.5280824

Batch 70560, train_perplexity=95.69461, train_loss=4.561162

Batch 70570, train_perplexity=104.839195, train_loss=4.6524277

Batch 70580, train_perplexity=91.42632, train_loss=4.5155334

Batch 70590, train_perplexity=99.951096, train_loss=4.604681

Batch 70600, train_perplexity=93.78502, train_loss=4.541005

Batch 70610, train_perplexity=89.18179, train_loss=4.490677

Batch 70620, train_perplexity=99.08374, train_loss=4.5959654

Batch 70630, train_perplexity=98.65962, train_loss=4.5916758

Batch 70640, train_perplexity=95.335396, train_loss=4.557401

Batch 70650, train_perplexity=96.73799, train_loss=4.572006

Batch 70660, train_perplexity=99.56969, train_loss=4.6008577

Batch 70670, train_perplexity=103.72139, train_loss=4.6417084

Batch 70680, train_perplexity=105.5082, train_loss=4.6587887

Batch 70690, train_perplexity=91.52944, train_loss=4.5166607

Batch 70700, train_perplexity=90.75642, train_loss=4.508179

Batch 70710, train_perplexity=93.66426, train_loss=4.5397167

Batch 70720, train_perplexity=86.418915, train_loss=4.4592066

Batch 70730, train_perplexity=104.94638, train_loss=4.6534495

Batch 70740, train_perplexity=93.2291, train_loss=4.53506

Batch 70750, train_perplexity=105.26518, train_loss=4.6564827

Batch 70760, train_perplexity=98.73511, train_loss=4.5924406

Batch 70770, train_perplexity=91.61463, train_loss=4.517591

Batch 70780, train_perplexity=106.36036, train_loss=4.666833

Batch 70790, train_perplexity=101.23302, train_loss=4.617425

Batch 70800, train_perplexity=98.48364, train_loss=4.5898905

Batch 70810, train_perplexity=91.33952, train_loss=4.5145836

Batch 70820, train_perplexity=101.91944, train_loss=4.6241827

Batch 70830, train_perplexity=95.237526, train_loss=4.556374

Batch 70840, train_perplexity=94.79302, train_loss=4.551696

Batch 70850, train_perplexity=96.74621, train_loss=4.572091

Batch 70860, train_perplexity=88.928024, train_loss=4.4878273

Batch 70870, train_perplexity=103.91595, train_loss=4.6435823

Batch 70880, train_perplexity=105.73353, train_loss=4.660922

Batch 70890, train_perplexity=92.24828, train_loss=4.5244837

Batch 70900, train_perplexity=109.534904, train_loss=4.6962433

Batch 70910, train_perplexity=110.89526, train_loss=4.708586

Batch 70920, train_perplexity=100.418396, train_loss=4.6093454

Batch 70930, train_perplexity=95.05473, train_loss=4.554453

Batch 70940, train_perplexity=99.838585, train_loss=4.6035547

Batch 70950, train_perplexity=91.64906, train_loss=4.5179667

Batch 70960, train_perplexity=93.87182, train_loss=4.54193

Batch 70970, train_perplexity=100.3487, train_loss=4.608651
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70980, train_perplexity=90.24698, train_loss=4.50255

Batch 70990, train_perplexity=93.85069, train_loss=4.541705

Batch 71000, train_perplexity=99.26737, train_loss=4.597817

Batch 71010, train_perplexity=91.593445, train_loss=4.5173597

Batch 71020, train_perplexity=92.25158, train_loss=4.5245194

Batch 71030, train_perplexity=90.90255, train_loss=4.509788

Batch 71040, train_perplexity=89.058975, train_loss=4.489299

Batch 71050, train_perplexity=91.3498, train_loss=4.514696

Batch 71060, train_perplexity=94.39285, train_loss=4.5474653

Batch 71070, train_perplexity=99.802505, train_loss=4.6031933

Batch 71080, train_perplexity=105.75249, train_loss=4.6611013

Batch 71090, train_perplexity=91.52525, train_loss=4.516615

Batch 71100, train_perplexity=106.93347, train_loss=4.672207

Batch 71110, train_perplexity=103.86542, train_loss=4.643096

Batch 71120, train_perplexity=101.630394, train_loss=4.6213427

Batch 71130, train_perplexity=100.46466, train_loss=4.609806

Batch 71140, train_perplexity=92.20017, train_loss=4.523962

Batch 71150, train_perplexity=103.50247, train_loss=4.6395955

Batch 71160, train_perplexity=100.02857, train_loss=4.605456

Batch 71170, train_perplexity=95.720535, train_loss=4.561433

Batch 71180, train_perplexity=96.60532, train_loss=4.570634

Batch 71190, train_perplexity=98.37485, train_loss=4.588785

Batch 71200, train_perplexity=95.5641, train_loss=4.5597973

Batch 71210, train_perplexity=94.860535, train_loss=4.5524077

Batch 71220, train_perplexity=89.39953, train_loss=4.4931154

Batch 71230, train_perplexity=92.28462, train_loss=4.5248775

Batch 71240, train_perplexity=90.54461, train_loss=4.5058427

Batch 71250, train_perplexity=93.51061, train_loss=4.538075

Batch 71260, train_perplexity=94.50553, train_loss=4.5486584

Batch 71270, train_perplexity=84.59134, train_loss=4.437832

Batch 71280, train_perplexity=85.14251, train_loss=4.4443264

Batch 71290, train_perplexity=85.96178, train_loss=4.4539027

Batch 71300, train_perplexity=96.3608, train_loss=4.5680995

Batch 71310, train_perplexity=99.49199, train_loss=4.600077

Batch 71320, train_perplexity=102.221886, train_loss=4.627146

Batch 71330, train_perplexity=98.16909, train_loss=4.5866914

Batch 71340, train_perplexity=96.12432, train_loss=4.5656424

Batch 71350, train_perplexity=93.74804, train_loss=4.540611

Batch 71360, train_perplexity=99.37734, train_loss=4.598924

Batch 71370, train_perplexity=91.7083, train_loss=4.518613

Batch 71380, train_perplexity=97.91482, train_loss=4.584098

Batch 71390, train_perplexity=97.124115, train_loss=4.5759897

Batch 71400, train_perplexity=107.70209, train_loss=4.679369

Batch 71410, train_perplexity=96.48158, train_loss=4.569352

Batch 71420, train_perplexity=99.11271, train_loss=4.5962577

Batch 71430, train_perplexity=93.470535, train_loss=4.5376463

Batch 71440, train_perplexity=98.42947, train_loss=4.58934

Batch 71450, train_perplexity=93.8064, train_loss=4.541233

Batch 71460, train_perplexity=88.79226, train_loss=4.4862995

Batch 71470, train_perplexity=103.68169, train_loss=4.6413255

Batch 71480, train_perplexity=96.74971, train_loss=4.5721273

Batch 71490, train_perplexity=96.66841, train_loss=4.5712867

Batch 71500, train_perplexity=98.82333, train_loss=4.5933337

Batch 71510, train_perplexity=93.01716, train_loss=4.532784

Batch 71520, train_perplexity=89.487816, train_loss=4.4941025

Batch 71530, train_perplexity=79.51776, train_loss=4.3759804

Batch 71540, train_perplexity=105.33467, train_loss=4.6571426

Batch 71550, train_perplexity=96.27965, train_loss=4.567257

Batch 71560, train_perplexity=96.57907, train_loss=4.570362

Batch 71570, train_perplexity=101.52868, train_loss=4.6203413

Batch 71580, train_perplexity=100.07041, train_loss=4.605874

Batch 71590, train_perplexity=106.57592, train_loss=4.6688576

Batch 71600, train_perplexity=113.56313, train_loss=4.732359

Batch 71610, train_perplexity=96.9672, train_loss=4.574373

Batch 71620, train_perplexity=92.093796, train_loss=4.5228076

Batch 71630, train_perplexity=91.28527, train_loss=4.5139894

Batch 71640, train_perplexity=89.07282, train_loss=4.4894543

Batch 71650, train_perplexity=91.05786, train_loss=4.511495

Batch 71660, train_perplexity=104.39153, train_loss=4.6481485

Batch 71670, train_perplexity=96.39237, train_loss=4.568427

Batch 71680, train_perplexity=92.726036, train_loss=4.5296493

Batch 71690, train_perplexity=85.077736, train_loss=4.4435654

Batch 71700, train_perplexity=101.134834, train_loss=4.6164546

Batch 71710, train_perplexity=89.180046, train_loss=4.4906573

Batch 71720, train_perplexity=93.7395, train_loss=4.5405197

Batch 71730, train_perplexity=97.35772, train_loss=4.578392

Batch 71740, train_perplexity=93.77044, train_loss=4.5408497

Batch 71750, train_perplexity=94.86438, train_loss=4.5524483

Batch 71760, train_perplexity=93.5496, train_loss=4.5384917

Batch 71770, train_perplexity=97.06925, train_loss=4.5754247

Batch 71780, train_perplexity=90.03121, train_loss=4.5001564

Batch 71790, train_perplexity=93.4959, train_loss=4.5379176

Batch 71800, train_perplexity=87.836845, train_loss=4.475481

Batch 71810, train_perplexity=112.45812, train_loss=4.722581

Batch 71820, train_perplexity=84.578835, train_loss=4.437684

Batch 71830, train_perplexity=96.12217, train_loss=4.56562

Batch 71840, train_perplexity=83.05327, train_loss=4.419482

Batch 71850, train_perplexity=93.720375, train_loss=4.5403156

Batch 71860, train_perplexity=103.0789, train_loss=4.6354947

Batch 71870, train_perplexity=87.74366, train_loss=4.4744196

Batch 71880, train_perplexity=98.93465, train_loss=4.5944595

Batch 71890, train_perplexity=106.92572, train_loss=4.6721344

Batch 71900, train_perplexity=89.58789, train_loss=4.49522

Batch 71910, train_perplexity=95.99011, train_loss=4.564245

Batch 71920, train_perplexity=90.302765, train_loss=4.503168

Batch 71930, train_perplexity=95.140076, train_loss=4.5553503

Batch 71940, train_perplexity=101.65075, train_loss=4.621543

Batch 71950, train_perplexity=98.06078, train_loss=4.5855875

Batch 71960, train_perplexity=107.09686, train_loss=4.6737337

Batch 71970, train_perplexity=98.83473, train_loss=4.593449

Batch 71980, train_perplexity=100.90309, train_loss=4.6141605

Batch 71990, train_perplexity=101.98138, train_loss=4.62479

Batch 72000, train_perplexity=94.87442, train_loss=4.552554

Batch 72010, train_perplexity=83.558235, train_loss=4.425544

Batch 72020, train_perplexity=101.18012, train_loss=4.6169024

Batch 72030, train_perplexity=102.79779, train_loss=4.632764

Batch 72040, train_perplexity=98.249214, train_loss=4.5875072

Batch 72050, train_perplexity=101.65618, train_loss=4.6215963

Batch 72060, train_perplexity=102.1354, train_loss=4.6262994

Batch 72070, train_perplexity=98.70065, train_loss=4.5920916

Batch 72080, train_perplexity=118.356094, train_loss=4.773698

Batch 72090, train_perplexity=92.912056, train_loss=4.5316534

Batch 72100, train_perplexity=90.96477, train_loss=4.5104723

Batch 72110, train_perplexity=100.001625, train_loss=4.6051865

Batch 72120, train_perplexity=104.69432, train_loss=4.651045

Batch 72130, train_perplexity=98.367065, train_loss=4.588706

Batch 72140, train_perplexity=89.69766, train_loss=4.4964447

Batch 72150, train_perplexity=106.28796, train_loss=4.666152

Batch 72160, train_perplexity=87.53104, train_loss=4.4719934

Batch 72170, train_perplexity=105.82553, train_loss=4.661792

Batch 72180, train_perplexity=106.381355, train_loss=4.6670303

Batch 72190, train_perplexity=105.16865, train_loss=4.6555653

Batch 72200, train_perplexity=82.438545, train_loss=4.412053

Batch 72210, train_perplexity=97.21382, train_loss=4.576913

Batch 72220, train_perplexity=104.207016, train_loss=4.6463795

Batch 72230, train_perplexity=100.47894, train_loss=4.609948

Batch 72240, train_perplexity=91.48, train_loss=4.5161204

Batch 72250, train_perplexity=104.271034, train_loss=4.6469936

Batch 72260, train_perplexity=93.079765, train_loss=4.533457

Batch 72270, train_perplexity=91.42759, train_loss=4.5155473

Batch 72280, train_perplexity=100.95694, train_loss=4.614694

Batch 72290, train_perplexity=96.46217, train_loss=4.569151

Batch 72300, train_perplexity=88.39752, train_loss=4.481844
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72310, train_perplexity=85.28184, train_loss=4.4459615

Batch 72320, train_perplexity=105.72496, train_loss=4.660841

Batch 72330, train_perplexity=99.72644, train_loss=4.602431

Batch 72340, train_perplexity=101.3761, train_loss=4.6188374

Batch 72350, train_perplexity=92.003426, train_loss=4.521826

Batch 72360, train_perplexity=99.668205, train_loss=4.6018467

Batch 72370, train_perplexity=89.94676, train_loss=4.499218

Batch 72380, train_perplexity=98.85359, train_loss=4.59364

Batch 72390, train_perplexity=106.08062, train_loss=4.6641994

Batch 72400, train_perplexity=103.90401, train_loss=4.6434674

Batch 72410, train_perplexity=96.52116, train_loss=4.569762

Batch 72420, train_perplexity=97.994736, train_loss=4.5849137

Batch 72430, train_perplexity=95.53422, train_loss=4.5594845

Batch 72440, train_perplexity=100.23068, train_loss=4.6074743

Batch 72450, train_perplexity=104.67909, train_loss=4.6508994

Batch 72460, train_perplexity=97.99936, train_loss=4.584961

Batch 72470, train_perplexity=93.07595, train_loss=4.533416

Batch 72480, train_perplexity=105.55334, train_loss=4.6592164

Batch 72490, train_perplexity=90.22839, train_loss=4.502344

Batch 72500, train_perplexity=91.053215, train_loss=4.511444

Batch 72510, train_perplexity=108.61774, train_loss=4.6878347

Batch 72520, train_perplexity=89.69039, train_loss=4.4963636

Batch 72530, train_perplexity=90.02018, train_loss=4.500034

Batch 72540, train_perplexity=99.621544, train_loss=4.6013784

Batch 72550, train_perplexity=100.620476, train_loss=4.611356

Batch 72560, train_perplexity=98.761475, train_loss=4.5927076

Batch 72570, train_perplexity=100.88976, train_loss=4.6140285

Batch 72580, train_perplexity=101.38408, train_loss=4.618916

Batch 72590, train_perplexity=108.81032, train_loss=4.689606

Batch 72600, train_perplexity=99.831535, train_loss=4.603484

Batch 72610, train_perplexity=101.40134, train_loss=4.6190863

Batch 72620, train_perplexity=96.339806, train_loss=4.5678816

Batch 72630, train_perplexity=85.66949, train_loss=4.4504967

Batch 72640, train_perplexity=105.85753, train_loss=4.662094

Batch 72650, train_perplexity=98.57855, train_loss=4.5908537

Batch 72660, train_perplexity=99.702095, train_loss=4.6021867

Batch 72670, train_perplexity=96.83722, train_loss=4.5730314

Batch 72680, train_perplexity=95.08202, train_loss=4.55474

Batch 72690, train_perplexity=93.65265, train_loss=4.5395927

Batch 72700, train_perplexity=97.75433, train_loss=4.5824575

Batch 72710, train_perplexity=103.175575, train_loss=4.636432

Batch 72720, train_perplexity=110.196465, train_loss=4.702265

Batch 72730, train_perplexity=93.58507, train_loss=4.538871

Batch 72740, train_perplexity=94.21617, train_loss=4.545592

Batch 72750, train_perplexity=95.854774, train_loss=4.5628343

Batch 72760, train_perplexity=100.418976, train_loss=4.609351

Batch 72770, train_perplexity=96.224205, train_loss=4.566681

Batch 72780, train_perplexity=101.57642, train_loss=4.6208115

Batch 72790, train_perplexity=98.773674, train_loss=4.592831

Batch 72800, train_perplexity=109.832184, train_loss=4.6989536

Batch 72810, train_perplexity=112.29511, train_loss=4.7211304

Batch 72820, train_perplexity=100.818344, train_loss=4.6133204

Batch 72830, train_perplexity=91.6453, train_loss=4.5179257

Batch 72840, train_perplexity=92.881935, train_loss=4.531329

Batch 72850, train_perplexity=98.49285, train_loss=4.589984

Batch 72860, train_perplexity=94.353516, train_loss=4.5470486

Batch 72870, train_perplexity=92.31719, train_loss=4.5252304

Batch 72880, train_perplexity=100.87413, train_loss=4.6138735

Batch 72890, train_perplexity=108.296684, train_loss=4.6848745

Batch 72900, train_perplexity=102.514175, train_loss=4.630001

Batch 72910, train_perplexity=101.72771, train_loss=4.6222997

Batch 72920, train_perplexity=96.084816, train_loss=4.5652313

Batch 72930, train_perplexity=100.29637, train_loss=4.6081295

Batch 72940, train_perplexity=85.14762, train_loss=4.4443865

Batch 72950, train_perplexity=87.50182, train_loss=4.4716597

Batch 72960, train_perplexity=99.645584, train_loss=4.6016197

Batch 72970, train_perplexity=100.37837, train_loss=4.608947

Batch 72980, train_perplexity=91.41495, train_loss=4.515409

Batch 72990, train_perplexity=104.22093, train_loss=4.646513

Batch 73000, train_perplexity=96.12377, train_loss=4.5656366

Batch 73010, train_perplexity=95.09472, train_loss=4.5548735

Batch 73020, train_perplexity=99.47202, train_loss=4.5998764

Batch 73030, train_perplexity=98.87876, train_loss=4.5938945

Batch 73040, train_perplexity=101.30748, train_loss=4.6181602

Batch 73050, train_perplexity=100.994545, train_loss=4.6150665

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 73060, train_perplexity=95.812004, train_loss=4.562388

Batch 73070, train_perplexity=94.159584, train_loss=4.544991

Batch 73080, train_perplexity=98.94168, train_loss=4.5945306

Batch 73090, train_perplexity=87.805435, train_loss=4.4751234

Batch 73100, train_perplexity=91.056335, train_loss=4.5114784

Batch 73110, train_perplexity=91.8734, train_loss=4.5204115

Batch 73120, train_perplexity=101.4886, train_loss=4.6199465

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 73130, train_perplexity=91.37995, train_loss=4.515026

Batch 73140, train_perplexity=92.18989, train_loss=4.5238504

Batch 73150, train_perplexity=98.97509, train_loss=4.594868

Batch 73160, train_perplexity=89.83716, train_loss=4.4979987

Batch 73170, train_perplexity=91.27343, train_loss=4.5138597

Batch 73180, train_perplexity=87.89441, train_loss=4.476136

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 73190, train_perplexity=97.62111, train_loss=4.581094

Batch 73200, train_perplexity=95.28672, train_loss=4.5568905

Batch 73210, train_perplexity=97.43495, train_loss=4.579185

Batch 73220, train_perplexity=99.03599, train_loss=4.5954833

Batch 73230, train_perplexity=96.74971, train_loss=4.5721273

Batch 73240, train_perplexity=95.079895, train_loss=4.5547175

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 73250, train_perplexity=89.23351, train_loss=4.4912567

Batch 73260, train_perplexity=102.85815, train_loss=4.633351

Batch 73270, train_perplexity=85.98674, train_loss=4.454193

Batch 73280, train_perplexity=96.82946, train_loss=4.5729513

Batch 73290, train_perplexity=91.80775, train_loss=4.5196967

Batch 73300, train_perplexity=93.77831, train_loss=4.5409336

Batch 73310, train_perplexity=89.83506, train_loss=4.4979753

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6159 sentences.
Finished loading
Batch 73320, train_perplexity=88.597885, train_loss=4.484108

Batch 73330, train_perplexity=95.38446, train_loss=4.5579157

Batch 73340, train_perplexity=99.09003, train_loss=4.596029

Batch 73350, train_perplexity=90.08347, train_loss=4.5007367

Batch 73360, train_perplexity=99.22729, train_loss=4.597413

Batch 73370, train_perplexity=98.113075, train_loss=4.5861206

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 73380, train_perplexity=95.39747, train_loss=4.558052

Batch 73390, train_perplexity=91.48241, train_loss=4.5161467

Batch 73400, train_perplexity=88.98329, train_loss=4.4884486

Batch 73410, train_perplexity=93.56516, train_loss=4.538658

Batch 73420, train_perplexity=101.44757, train_loss=4.619542

Batch 73430, train_perplexity=80.804924, train_loss=4.392038

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 73440, train_perplexity=107.913315, train_loss=4.6813283

Batch 73450, train_perplexity=101.084496, train_loss=4.615957

Batch 73460, train_perplexity=90.484924, train_loss=4.505183

Batch 73470, train_perplexity=97.72027, train_loss=4.582109

Batch 73480, train_perplexity=99.8343, train_loss=4.603512

Batch 73490, train_perplexity=94.18643, train_loss=4.545276

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 73500, train_perplexity=91.29637, train_loss=4.514111

Batch 73510, train_perplexity=94.16852, train_loss=4.545086

Batch 73520, train_perplexity=115.22302, train_loss=4.7468696

Batch 73530, train_perplexity=100.31579, train_loss=4.608323

Batch 73540, train_perplexity=102.98236, train_loss=4.6345577

Batch 73550, train_perplexity=89.89505, train_loss=4.498643

Batch 73560, train_perplexity=90.04916, train_loss=4.5003557

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 73570, train_perplexity=89.7674, train_loss=4.497222

Batch 73580, train_perplexity=107.02999, train_loss=4.673109

Batch 73590, train_perplexity=98.05068, train_loss=4.5854845

Batch 73600, train_perplexity=91.228355, train_loss=4.5133657

Batch 73610, train_perplexity=96.075836, train_loss=4.565138

Batch 73620, train_perplexity=109.50169, train_loss=4.69594

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 73630, train_perplexity=99.836395, train_loss=4.603533

Batch 73640, train_perplexity=97.75364, train_loss=4.5824504

Batch 73650, train_perplexity=96.02601, train_loss=4.564619

Batch 73660, train_perplexity=104.97495, train_loss=4.653722

Batch 73670, train_perplexity=104.69272, train_loss=4.6510296

Batch 73680, train_perplexity=99.66934, train_loss=4.601858

Batch 73690, train_perplexity=95.40812, train_loss=4.5581636

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 73700, train_perplexity=91.62979, train_loss=4.5177565

Batch 73710, train_perplexity=99.6816, train_loss=4.601981

Batch 73720, train_perplexity=93.456726, train_loss=4.5374985

Batch 73730, train_perplexity=99.18992, train_loss=4.5970364

Batch 73740, train_perplexity=97.50862, train_loss=4.579941

Batch 73750, train_perplexity=91.58419, train_loss=4.5172586

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 73760, train_perplexity=98.87947, train_loss=4.5939016

Batch 73770, train_perplexity=89.27795, train_loss=4.4917545

Batch 73780, train_perplexity=92.640205, train_loss=4.5287232

Batch 73790, train_perplexity=105.825836, train_loss=4.6617947

Batch 73800, train_perplexity=105.977425, train_loss=4.663226

Batch 73810, train_perplexity=89.44882, train_loss=4.4936666

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 73820, train_perplexity=114.39402, train_loss=4.739649

Batch 73830, train_perplexity=95.56406, train_loss=4.559797

Batch 73840, train_perplexity=95.92981, train_loss=4.5636168

Batch 73850, train_perplexity=95.00616, train_loss=4.5539417

Batch 73860, train_perplexity=90.19716, train_loss=4.501998

Batch 73870, train_perplexity=83.13414, train_loss=4.4204555

Batch 73880, train_perplexity=103.1288, train_loss=4.6359787

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 73890, train_perplexity=94.15415, train_loss=4.5449333

Batch 73900, train_perplexity=100.44292, train_loss=4.6095896

Batch 73910, train_perplexity=104.91806, train_loss=4.6531796

Batch 73920, train_perplexity=90.9829, train_loss=4.5106716

Batch 73930, train_perplexity=92.719315, train_loss=4.529577

Batch 73940, train_perplexity=102.962326, train_loss=4.634363

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 73950, train_perplexity=87.438515, train_loss=4.470936

Batch 73960, train_perplexity=94.46408, train_loss=4.5482197

Batch 73970, train_perplexity=103.40514, train_loss=4.6386547

Batch 73980, train_perplexity=91.93992, train_loss=4.5211353

Batch 73990, train_perplexity=99.625626, train_loss=4.6014194

Batch 74000, train_perplexity=85.7101, train_loss=4.4509706

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 74010, train_perplexity=100.91127, train_loss=4.6142416

Batch 74020, train_perplexity=109.2276, train_loss=4.693434

Batch 74030, train_perplexity=93.72444, train_loss=4.540359

Batch 74040, train_perplexity=90.02554, train_loss=4.5000935

Batch 74050, train_perplexity=89.66106, train_loss=4.4960365

Batch 74060, train_perplexity=101.768845, train_loss=4.622704

Batch 74070, train_perplexity=98.770706, train_loss=4.592801

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 74080, train_perplexity=100.84051, train_loss=4.61354

Batch 74090, train_perplexity=97.13713, train_loss=4.5761237

Batch 74100, train_perplexity=87.69618, train_loss=4.4738784

Batch 74110, train_perplexity=94.44183, train_loss=4.547984

Batch 74120, train_perplexity=99.82278, train_loss=4.6033964

Batch 74130, train_perplexity=100.61731, train_loss=4.6113243

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 74140, train_perplexity=97.45098, train_loss=4.5793495

Batch 74150, train_perplexity=100.3399, train_loss=4.6085634

Batch 74160, train_perplexity=105.69602, train_loss=4.6605673

Batch 74170, train_perplexity=90.672844, train_loss=4.507258

Batch 74180, train_perplexity=94.12524, train_loss=4.544626

Batch 74190, train_perplexity=93.19675, train_loss=4.534713

Batch 74200, train_perplexity=91.91239, train_loss=4.520836

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 74210, train_perplexity=91.54192, train_loss=4.516797

Batch 74220, train_perplexity=95.91471, train_loss=4.5634594

Batch 74230, train_perplexity=92.29791, train_loss=4.5250216

Batch 74240, train_perplexity=86.381836, train_loss=4.4587774

Batch 74250, train_perplexity=103.53821, train_loss=4.6399407

Batch 74260, train_perplexity=94.31101, train_loss=4.546598

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 74270, train_perplexity=93.886055, train_loss=4.542082

Batch 74280, train_perplexity=88.02825, train_loss=4.477658

Batch 74290, train_perplexity=99.91164, train_loss=4.604286

Batch 74300, train_perplexity=87.219765, train_loss=4.468431

Batch 74310, train_perplexity=100.496, train_loss=4.610118

Batch 74320, train_perplexity=105.28978, train_loss=4.6567163

Batch 74330, train_perplexity=99.155014, train_loss=4.5966845

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 74340, train_perplexity=105.07341, train_loss=4.6546593

Batch 74350, train_perplexity=91.0151, train_loss=4.5110254

Batch 74360, train_perplexity=94.874695, train_loss=4.552557

Batch 74370, train_perplexity=101.84492, train_loss=4.623451

Batch 74380, train_perplexity=90.69741, train_loss=4.507529

Batch 74390, train_perplexity=103.69538, train_loss=4.6414576

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 74400, train_perplexity=90.74391, train_loss=4.5080414

Batch 74410, train_perplexity=95.100525, train_loss=4.5549345

Batch 74420, train_perplexity=89.69433, train_loss=4.4964075

Batch 74430, train_perplexity=95.30117, train_loss=4.557042

Batch 74440, train_perplexity=89.35103, train_loss=4.492573

Batch 74450, train_perplexity=89.03104, train_loss=4.488985

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 74460, train_perplexity=93.21954, train_loss=4.5349574

Batch 74470, train_perplexity=109.62645, train_loss=4.6970787

Batch 74480, train_perplexity=111.681335, train_loss=4.7156496

Batch 74490, train_perplexity=101.86648, train_loss=4.623663

Batch 74500, train_perplexity=101.47631, train_loss=4.6198254

Batch 74510, train_perplexity=89.68774, train_loss=4.496334

Batch 74520, train_perplexity=100.67365, train_loss=4.611884

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 74530, train_perplexity=105.81493, train_loss=4.6616917

Batch 74540, train_perplexity=92.04353, train_loss=4.5222616

Batch 74550, train_perplexity=98.12945, train_loss=4.5862875

Batch 74560, train_perplexity=93.69585, train_loss=4.540054

Batch 74570, train_perplexity=96.78801, train_loss=4.572523

Batch 74580, train_perplexity=96.25733, train_loss=4.567025

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 74590, train_perplexity=106.32978, train_loss=4.6665454
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74600, train_perplexity=93.46537, train_loss=4.537591

Batch 74610, train_perplexity=100.70255, train_loss=4.612171

Batch 74620, train_perplexity=89.99936, train_loss=4.4998026

Batch 74630, train_perplexity=93.6043, train_loss=4.5390763

Batch 74640, train_perplexity=91.204346, train_loss=4.5131025

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 74650, train_perplexity=95.176926, train_loss=4.5557375

Batch 74660, train_perplexity=91.35594, train_loss=4.5147634

Batch 74670, train_perplexity=89.60772, train_loss=4.4954414

Batch 74680, train_perplexity=98.18533, train_loss=4.586857

Batch 74690, train_perplexity=97.25351, train_loss=4.577321

Batch 74700, train_perplexity=109.270836, train_loss=4.6938295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 74710, train_perplexity=99.76368, train_loss=4.602804

Batch 74720, train_perplexity=86.71856, train_loss=4.462668

Batch 74730, train_perplexity=98.61626, train_loss=4.591236

Batch 74740, train_perplexity=92.94897, train_loss=4.5320506

Batch 74750, train_perplexity=92.83013, train_loss=4.5307713

Batch 74760, train_perplexity=104.34813, train_loss=4.6477327

Batch 74770, train_perplexity=100.96238, train_loss=4.614748

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 74780, train_perplexity=103.25142, train_loss=4.637167

Batch 74790, train_perplexity=89.57995, train_loss=4.4951315

Batch 74800, train_perplexity=90.7318, train_loss=4.507908

Batch 74810, train_perplexity=91.890656, train_loss=4.5205994

Batch 74820, train_perplexity=116.20876, train_loss=4.7553883

Batch 74830, train_perplexity=99.33674, train_loss=4.5985155

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 74840, train_perplexity=98.89503, train_loss=4.594059

Batch 74850, train_perplexity=92.25884, train_loss=4.524598

Batch 74860, train_perplexity=92.427574, train_loss=4.5264254

Batch 74870, train_perplexity=89.95028, train_loss=4.499257

Batch 74880, train_perplexity=95.565384, train_loss=4.5598106

Batch 74890, train_perplexity=86.08914, train_loss=4.4553833

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 74900, train_perplexity=95.007065, train_loss=4.5539513

Batch 74910, train_perplexity=93.22559, train_loss=4.5350223

Batch 74920, train_perplexity=86.89328, train_loss=4.4646807

Batch 74930, train_perplexity=110.17461, train_loss=4.7020664

Batch 74940, train_perplexity=99.580505, train_loss=4.6009665

Batch 74950, train_perplexity=105.71377, train_loss=4.660735

Batch 74960, train_perplexity=101.084404, train_loss=4.615956

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 74970, train_perplexity=83.26065, train_loss=4.421976

Batch 74980, train_perplexity=94.65084, train_loss=4.5501947

Batch 74990, train_perplexity=103.17513, train_loss=4.636428

Batch 75000, train_perplexity=86.563095, train_loss=4.4608736

Batch 75010, train_perplexity=107.60548, train_loss=4.6784716

Batch 75020, train_perplexity=88.68859, train_loss=4.4851313

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 75030, train_perplexity=95.848785, train_loss=4.562772

Batch 75040, train_perplexity=103.34323, train_loss=4.638056

Batch 75050, train_perplexity=109.73293, train_loss=4.6980495

Batch 75060, train_perplexity=92.97277, train_loss=4.5323067

Batch 75070, train_perplexity=91.48485, train_loss=4.5161734

Batch 75080, train_perplexity=92.80278, train_loss=4.5304766

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 75090, train_perplexity=96.71945, train_loss=4.5718145

Batch 75100, train_perplexity=98.68635, train_loss=4.5919466

Batch 75110, train_perplexity=99.58174, train_loss=4.600979

Batch 75120, train_perplexity=99.8602, train_loss=4.603771

Batch 75130, train_perplexity=94.188095, train_loss=4.545294

Batch 75140, train_perplexity=105.31413, train_loss=4.6569476

Batch 75150, train_perplexity=98.19414, train_loss=4.5869465

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 75160, train_perplexity=87.019165, train_loss=4.4661283

Batch 75170, train_perplexity=92.410385, train_loss=4.5262394

Batch 75180, train_perplexity=91.2441, train_loss=4.5135384

Batch 75190, train_perplexity=105.41859, train_loss=4.657939

Batch 75200, train_perplexity=98.20177, train_loss=4.587024

Batch 75210, train_perplexity=101.83656, train_loss=4.623369

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 75220, train_perplexity=99.959724, train_loss=4.6047673

Batch 75230, train_perplexity=104.87959, train_loss=4.652813

Batch 75240, train_perplexity=95.97602, train_loss=4.5640984

Batch 75250, train_perplexity=96.94561, train_loss=4.57415

Batch 75260, train_perplexity=95.5277, train_loss=4.5594163

Batch 75270, train_perplexity=97.86963, train_loss=4.5836363

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 75280, train_perplexity=90.895874, train_loss=4.5097146

Batch 75290, train_perplexity=95.71506, train_loss=4.5613756

Batch 75300, train_perplexity=91.062416, train_loss=4.511545

Batch 75310, train_perplexity=82.493286, train_loss=4.412717

Batch 75320, train_perplexity=99.758545, train_loss=4.6027527

Batch 75330, train_perplexity=100.004776, train_loss=4.605218

Batch 75340, train_perplexity=89.53502, train_loss=4.49463

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 75350, train_perplexity=102.02023, train_loss=4.625171

Batch 75360, train_perplexity=91.557335, train_loss=4.5169654

Batch 75370, train_perplexity=88.38505, train_loss=4.481703

Batch 75380, train_perplexity=100.27466, train_loss=4.607913

Batch 75390, train_perplexity=99.95262, train_loss=4.6046963

Batch 75400, train_perplexity=91.20726, train_loss=4.5131345

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 75410, train_perplexity=98.06878, train_loss=4.585669

Batch 75420, train_perplexity=96.489685, train_loss=4.569436

Batch 75430, train_perplexity=90.91109, train_loss=4.509882

Batch 75440, train_perplexity=98.03007, train_loss=4.585274

Batch 75450, train_perplexity=97.28969, train_loss=4.577693

Batch 75460, train_perplexity=101.90914, train_loss=4.6240816

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 75470, train_perplexity=103.69459, train_loss=4.64145

Batch 75480, train_perplexity=91.45301, train_loss=4.5158253

Batch 75490, train_perplexity=92.65227, train_loss=4.5288534

Batch 75500, train_perplexity=102.85241, train_loss=4.633295

Batch 75510, train_perplexity=83.97331, train_loss=4.430499

Batch 75520, train_perplexity=86.48562, train_loss=4.459978

Batch 75530, train_perplexity=89.245094, train_loss=4.4913864

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 75540, train_perplexity=86.019714, train_loss=4.4545765

Batch 75550, train_perplexity=95.90721, train_loss=4.563381

Batch 75560, train_perplexity=115.30761, train_loss=4.7476034

Batch 75570, train_perplexity=87.63118, train_loss=4.473137

Batch 75580, train_perplexity=107.25948, train_loss=4.675251

Batch 75590, train_perplexity=90.34877, train_loss=4.5036774

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 75600, train_perplexity=91.59187, train_loss=4.5173426

Batch 75610, train_perplexity=91.83805, train_loss=4.5200267

Batch 75620, train_perplexity=101.59701, train_loss=4.621014

Batch 75630, train_perplexity=98.34408, train_loss=4.5884724

Batch 75640, train_perplexity=93.972404, train_loss=4.543001

Batch 75650, train_perplexity=100.21773, train_loss=4.607345

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 75660, train_perplexity=98.583435, train_loss=4.5909033

Batch 75670, train_perplexity=90.80867, train_loss=4.5087547

Batch 75680, train_perplexity=95.7756, train_loss=4.562008

Batch 75690, train_perplexity=89.11284, train_loss=4.4899035

Batch 75700, train_perplexity=97.71561, train_loss=4.5820613

Batch 75710, train_perplexity=99.20998, train_loss=4.5972385

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 75720, train_perplexity=107.01912, train_loss=4.6730075

Batch 75730, train_perplexity=97.54313, train_loss=4.5802946

Batch 75740, train_perplexity=96.64205, train_loss=4.571014

Batch 75750, train_perplexity=88.59552, train_loss=4.4840813

Batch 75760, train_perplexity=99.54196, train_loss=4.6005793

Batch 75770, train_perplexity=96.49143, train_loss=4.569454

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 75780, train_perplexity=104.614075, train_loss=4.650278

Batch 75790, train_perplexity=93.98101, train_loss=4.5430927

Batch 75800, train_perplexity=101.77467, train_loss=4.6227612

Batch 75810, train_perplexity=105.123825, train_loss=4.655139

Batch 75820, train_perplexity=93.58404, train_loss=4.53886

Batch 75830, train_perplexity=104.622856, train_loss=4.650362

Batch 75840, train_perplexity=94.85366, train_loss=4.5523353

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 75850, train_perplexity=87.33226, train_loss=4.46972

Batch 75860, train_perplexity=98.845436, train_loss=4.5935574

Batch 75870, train_perplexity=89.524605, train_loss=4.4945135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 75880, train_perplexity=90.70307, train_loss=4.5075912

Batch 75890, train_perplexity=103.24103, train_loss=4.6370664

Batch 75900, train_perplexity=99.51026, train_loss=4.6002607

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 75910, train_perplexity=105.73504, train_loss=4.6609364

Batch 75920, train_perplexity=96.33824, train_loss=4.5678654

Batch 75930, train_perplexity=89.62284, train_loss=4.49561

Batch 75940, train_perplexity=100.12692, train_loss=4.6064386

Batch 75950, train_perplexity=92.4639, train_loss=4.5268183

Batch 75960, train_perplexity=94.52302, train_loss=4.5488434

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 75970, train_perplexity=91.92484, train_loss=4.5209713

Batch 75980, train_perplexity=94.86465, train_loss=4.552451

Batch 75990, train_perplexity=93.60274, train_loss=4.5390596

Batch 76000, train_perplexity=92.99893, train_loss=4.532588

Batch 76010, train_perplexity=84.738045, train_loss=4.4395647

Batch 76020, train_perplexity=94.821236, train_loss=4.5519934

Batch 76030, train_perplexity=98.674774, train_loss=4.5918293

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 76040, train_perplexity=105.72446, train_loss=4.660836

Batch 76050, train_perplexity=95.70004, train_loss=4.5612187

Batch 76060, train_perplexity=102.80171, train_loss=4.632802

Batch 76070, train_perplexity=97.974174, train_loss=4.584704

Batch 76080, train_perplexity=98.58306, train_loss=4.5908995

Batch 76090, train_perplexity=92.54894, train_loss=4.5277376

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 76100, train_perplexity=93.0518, train_loss=4.5331564

Batch 76110, train_perplexity=109.03506, train_loss=4.6916695

Batch 76120, train_perplexity=95.509346, train_loss=4.559224

Batch 76130, train_perplexity=93.4684, train_loss=4.5376234

Batch 76140, train_perplexity=103.70913, train_loss=4.64159

Batch 76150, train_perplexity=94.80731, train_loss=4.5518465

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 76160, train_perplexity=89.52077, train_loss=4.4944706

Batch 76170, train_perplexity=101.44525, train_loss=4.619519

Batch 76180, train_perplexity=103.14513, train_loss=4.636137

Batch 76190, train_perplexity=102.24884, train_loss=4.6274095

Batch 76200, train_perplexity=96.87666, train_loss=4.5734386

Batch 76210, train_perplexity=88.11858, train_loss=4.4786835

Batch 76220, train_perplexity=90.94508, train_loss=4.510256

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 76230, train_perplexity=89.379326, train_loss=4.4928894

Batch 76240, train_perplexity=86.46566, train_loss=4.4597473

Batch 76250, train_perplexity=96.15985, train_loss=4.566012

Batch 76260, train_perplexity=83.699, train_loss=4.427227

Batch 76270, train_perplexity=92.2591, train_loss=4.524601

Batch 76280, train_perplexity=90.9293, train_loss=4.5100822

Batch 76290, train_perplexity=89.30635, train_loss=4.4920726

Batch 76300, train_perplexity=85.638115, train_loss=4.4501305

Batch 76310, train_perplexity=83.1217, train_loss=4.4203057

Batch 76320, train_perplexity=87.1176, train_loss=4.467259

Batch 76330, train_perplexity=93.31779, train_loss=4.5360107

Batch 76340, train_perplexity=91.965004, train_loss=4.521408

Batch 76350, train_perplexity=88.64386, train_loss=4.484627

Batch 76360, train_perplexity=90.920715, train_loss=4.509988

Batch 76370, train_perplexity=90.17961, train_loss=4.5018034

Batch 76380, train_perplexity=92.53623, train_loss=4.5276003

Batch 76390, train_perplexity=78.06569, train_loss=4.3575506

Batch 76400, train_perplexity=90.902725, train_loss=4.50979

Batch 76410, train_perplexity=86.7375, train_loss=4.4628863

Batch 76420, train_perplexity=93.33848, train_loss=4.5362325

Batch 76430, train_perplexity=82.39603, train_loss=4.411537

Batch 76440, train_perplexity=91.95342, train_loss=4.521282

Batch 76450, train_perplexity=91.87212, train_loss=4.5203977

Batch 76460, train_perplexity=96.73458, train_loss=4.571971

Batch 76470, train_perplexity=92.14673, train_loss=4.523382

Batch 76480, train_perplexity=93.02146, train_loss=4.53283

Batch 76490, train_perplexity=84.29342, train_loss=4.4343038

Batch 76500, train_perplexity=88.982994, train_loss=4.4884453

Batch 76510, train_perplexity=96.28984, train_loss=4.567363

Batch 76520, train_perplexity=112.87405, train_loss=4.7262726

Batch 76530, train_perplexity=85.23098, train_loss=4.445365

Batch 76540, train_perplexity=92.98589, train_loss=4.532448

Batch 76550, train_perplexity=79.048294, train_loss=4.370059

Batch 76560, train_perplexity=81.35776, train_loss=4.398856

Batch 76570, train_perplexity=92.90085, train_loss=4.531533

Batch 76580, train_perplexity=92.27767, train_loss=4.524802

Batch 76590, train_perplexity=93.08758, train_loss=4.5335407

Batch 76600, train_perplexity=90.37203, train_loss=4.503935

Batch 76610, train_perplexity=87.696976, train_loss=4.4738874

Batch 76620, train_perplexity=106.1086, train_loss=4.664463

Batch 76630, train_perplexity=82.06364, train_loss=4.407495

Batch 76640, train_perplexity=90.42777, train_loss=4.5045514

Batch 76650, train_perplexity=95.33163, train_loss=4.5573616

Batch 76660, train_perplexity=87.86391, train_loss=4.475789

Batch 76670, train_perplexity=102.29785, train_loss=4.6278887

Batch 76680, train_perplexity=100.041405, train_loss=4.605584

Batch 76690, train_perplexity=87.56577, train_loss=4.47239

Batch 76700, train_perplexity=88.87995, train_loss=4.4872866

Batch 76710, train_perplexity=91.47652, train_loss=4.5160823

Batch 76720, train_perplexity=93.069466, train_loss=4.533346

Batch 76730, train_perplexity=90.07252, train_loss=4.500615

Batch 76740, train_perplexity=105.89231, train_loss=4.6624227

Batch 76750, train_perplexity=94.747925, train_loss=4.55122

Batch 76760, train_perplexity=92.27376, train_loss=4.52476

Batch 76770, train_perplexity=87.01576, train_loss=4.4660892

Batch 76780, train_perplexity=98.14868, train_loss=4.5864835

Batch 76790, train_perplexity=94.35163, train_loss=4.5470285
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76800, train_perplexity=84.75979, train_loss=4.4398212

Batch 76810, train_perplexity=95.47465, train_loss=4.558861

Batch 76820, train_perplexity=93.91211, train_loss=4.5423594

Batch 76830, train_perplexity=100.234886, train_loss=4.6075163

Batch 76840, train_perplexity=101.34517, train_loss=4.618532

Batch 76850, train_perplexity=100.80123, train_loss=4.6131506

Batch 76860, train_perplexity=91.92791, train_loss=4.5210047

Batch 76870, train_perplexity=89.07928, train_loss=4.4895267

Batch 76880, train_perplexity=98.77109, train_loss=4.592805

Batch 76890, train_perplexity=101.94656, train_loss=4.624449

Batch 76900, train_perplexity=89.70844, train_loss=4.496565

Batch 76910, train_perplexity=90.11694, train_loss=4.501108

Batch 76920, train_perplexity=98.013565, train_loss=4.585106

Batch 76930, train_perplexity=96.961975, train_loss=4.574319

Batch 76940, train_perplexity=95.308716, train_loss=4.5571213

Batch 76950, train_perplexity=101.31647, train_loss=4.618249

Batch 76960, train_perplexity=90.50287, train_loss=4.5053816

Batch 76970, train_perplexity=94.22035, train_loss=4.545636

Batch 76980, train_perplexity=93.62979, train_loss=4.5393486

Batch 76990, train_perplexity=92.650764, train_loss=4.528837

Batch 77000, train_perplexity=94.03059, train_loss=4.54362

Batch 77010, train_perplexity=103.595695, train_loss=4.640496

Batch 77020, train_perplexity=86.72001, train_loss=4.4626846

Batch 77030, train_perplexity=85.05218, train_loss=4.443265

Batch 77040, train_perplexity=100.312775, train_loss=4.608293

Batch 77050, train_perplexity=98.3408, train_loss=4.588439

Batch 77060, train_perplexity=90.31831, train_loss=4.5033402

Batch 77070, train_perplexity=97.386696, train_loss=4.5786896

Batch 77080, train_perplexity=92.73855, train_loss=4.529784

Batch 77090, train_perplexity=102.439316, train_loss=4.6292706

Batch 77100, train_perplexity=97.87523, train_loss=4.5836935

Batch 77110, train_perplexity=83.972435, train_loss=4.4304886

Batch 77120, train_perplexity=97.999916, train_loss=4.5849667

Batch 77130, train_perplexity=91.66694, train_loss=4.518162

Batch 77140, train_perplexity=101.481926, train_loss=4.6198807

Batch 77150, train_perplexity=103.24167, train_loss=4.6370726

Batch 77160, train_perplexity=95.37591, train_loss=4.557826

Batch 77170, train_perplexity=86.40136, train_loss=4.4590034

Batch 77180, train_perplexity=96.978065, train_loss=4.574485

Batch 77190, train_perplexity=98.10802, train_loss=4.586069

Batch 77200, train_perplexity=92.869576, train_loss=4.531196

Batch 77210, train_perplexity=89.77079, train_loss=4.4972596

Batch 77220, train_perplexity=89.86094, train_loss=4.4982634

Batch 77230, train_perplexity=96.33181, train_loss=4.5677986

Batch 77240, train_perplexity=100.46773, train_loss=4.6098366

Batch 77250, train_perplexity=100.43478, train_loss=4.6095085

Batch 77260, train_perplexity=95.91197, train_loss=4.563431

Batch 77270, train_perplexity=95.06416, train_loss=4.554552

Batch 77280, train_perplexity=92.1339, train_loss=4.523243

Batch 77290, train_perplexity=96.27579, train_loss=4.567217

Batch 77300, train_perplexity=91.94509, train_loss=4.5211916

Batch 77310, train_perplexity=95.361176, train_loss=4.5576715

Batch 77320, train_perplexity=93.47985, train_loss=4.537746

Batch 77330, train_perplexity=90.32004, train_loss=4.5033593

Batch 77340, train_perplexity=113.37587, train_loss=4.7307086

Batch 77350, train_perplexity=90.19458, train_loss=4.5019693

Batch 77360, train_perplexity=99.36673, train_loss=4.5988173

Batch 77370, train_perplexity=85.88496, train_loss=4.4530087

Batch 77380, train_perplexity=102.07985, train_loss=4.6257553

Batch 77390, train_perplexity=97.55997, train_loss=4.580467

Batch 77400, train_perplexity=78.13719, train_loss=4.358466

Batch 77410, train_perplexity=101.039536, train_loss=4.615512

Batch 77420, train_perplexity=85.9753, train_loss=4.45406

Batch 77430, train_perplexity=97.51327, train_loss=4.5799885

Batch 77440, train_perplexity=97.41363, train_loss=4.578966

Batch 77450, train_perplexity=97.471756, train_loss=4.5795627

Batch 77460, train_perplexity=104.1196, train_loss=4.64554

Batch 77470, train_perplexity=93.30462, train_loss=4.5358696

Batch 77480, train_perplexity=103.49581, train_loss=4.639531

Batch 77490, train_perplexity=93.427986, train_loss=4.537191

Batch 77500, train_perplexity=87.29029, train_loss=4.469239

Batch 77510, train_perplexity=101.74706, train_loss=4.62249

Batch 77520, train_perplexity=85.543594, train_loss=4.449026

Batch 77530, train_perplexity=94.224754, train_loss=4.545683

Batch 77540, train_perplexity=82.93613, train_loss=4.418071

Batch 77550, train_perplexity=96.575386, train_loss=4.570324

Batch 77560, train_perplexity=92.87737, train_loss=4.53128

Batch 77570, train_perplexity=97.66218, train_loss=4.5815144

Batch 77580, train_perplexity=88.91357, train_loss=4.4876647

Batch 77590, train_perplexity=92.406425, train_loss=4.5261965

Batch 77600, train_perplexity=97.44894, train_loss=4.5793285

Batch 77610, train_perplexity=97.93116, train_loss=4.5842648

Batch 77620, train_perplexity=103.780266, train_loss=4.642276

Batch 77630, train_perplexity=93.99548, train_loss=4.5432467

Batch 77640, train_perplexity=79.7376, train_loss=4.3787413

Batch 77650, train_perplexity=101.44191, train_loss=4.6194863

Batch 77660, train_perplexity=89.84449, train_loss=4.4980803

Batch 77670, train_perplexity=97.92985, train_loss=4.5842514

Batch 77680, train_perplexity=98.79826, train_loss=4.59308

Batch 77690, train_perplexity=93.99607, train_loss=4.543253

Batch 77700, train_perplexity=92.91573, train_loss=4.531693

Batch 77710, train_perplexity=92.28225, train_loss=4.524852

Batch 77720, train_perplexity=94.429405, train_loss=4.5478525

Batch 77730, train_perplexity=102.53119, train_loss=4.630167

Batch 77740, train_perplexity=89.14123, train_loss=4.490222

Batch 77750, train_perplexity=93.159065, train_loss=4.5343084

Batch 77760, train_perplexity=77.94584, train_loss=4.3560143

Batch 77770, train_perplexity=95.03053, train_loss=4.5541983

Batch 77780, train_perplexity=99.73871, train_loss=4.602554

Batch 77790, train_perplexity=94.68948, train_loss=4.550603

Batch 77800, train_perplexity=99.96144, train_loss=4.6047845

Batch 77810, train_perplexity=85.23268, train_loss=4.445385

Batch 77820, train_perplexity=97.43969, train_loss=4.5792336

Batch 77830, train_perplexity=92.143875, train_loss=4.523351

Batch 77840, train_perplexity=80.957954, train_loss=4.39393

Batch 77850, train_perplexity=95.679375, train_loss=4.5610027

Batch 77860, train_perplexity=99.332054, train_loss=4.5984683

Batch 77870, train_perplexity=94.5874, train_loss=4.5495243

Batch 77880, train_perplexity=94.02431, train_loss=4.5435534

Batch 77890, train_perplexity=91.32315, train_loss=4.5144043

Batch 77900, train_perplexity=92.55653, train_loss=4.5278196

Batch 77910, train_perplexity=93.48369, train_loss=4.537787

Batch 77920, train_perplexity=95.06684, train_loss=4.55458

Batch 77930, train_perplexity=91.86639, train_loss=4.520335

Batch 77940, train_perplexity=101.7231, train_loss=4.6222544

Batch 77950, train_perplexity=96.098656, train_loss=4.5653753

Batch 77960, train_perplexity=91.47098, train_loss=4.5160217

Batch 77970, train_perplexity=104.79941, train_loss=4.652048

Batch 77980, train_perplexity=89.82444, train_loss=4.497857

Batch 77990, train_perplexity=108.05121, train_loss=4.6826053

Batch 78000, train_perplexity=96.694824, train_loss=4.57156

Batch 78010, train_perplexity=100.39403, train_loss=4.6091027

Batch 78020, train_perplexity=81.64542, train_loss=4.4023857

Batch 78030, train_perplexity=92.6505, train_loss=4.5288343

Batch 78040, train_perplexity=98.44482, train_loss=4.589496

Batch 78050, train_perplexity=101.16652, train_loss=4.616768

Batch 78060, train_perplexity=92.23104, train_loss=4.5242968

Batch 78070, train_perplexity=108.62882, train_loss=4.687937

Batch 78080, train_perplexity=95.99991, train_loss=4.5643473

Batch 78090, train_perplexity=104.82435, train_loss=4.652286

Batch 78100, train_perplexity=96.764656, train_loss=4.572282

Batch 78110, train_perplexity=114.52354, train_loss=4.7407804

Batch 78120, train_perplexity=90.664505, train_loss=4.507166
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78130, train_perplexity=90.36238, train_loss=4.503828

Batch 78140, train_perplexity=102.00598, train_loss=4.6250315

Batch 78150, train_perplexity=94.13242, train_loss=4.5447025

Batch 78160, train_perplexity=96.474686, train_loss=4.5692806

Batch 78170, train_perplexity=104.97225, train_loss=4.653696

Batch 78180, train_perplexity=92.20017, train_loss=4.523962

Batch 78190, train_perplexity=93.94185, train_loss=4.542676

Batch 78200, train_perplexity=96.58755, train_loss=4.57045

Batch 78210, train_perplexity=97.95703, train_loss=4.584529

Batch 78220, train_perplexity=93.2291, train_loss=4.53506

Batch 78230, train_perplexity=92.36338, train_loss=4.5257306

Batch 78240, train_perplexity=92.123665, train_loss=4.523132

Batch 78250, train_perplexity=98.78013, train_loss=4.5928965

Batch 78260, train_perplexity=101.77375, train_loss=4.622752

Batch 78270, train_perplexity=110.46909, train_loss=4.7047358

Batch 78280, train_perplexity=83.92968, train_loss=4.4299793

Batch 78290, train_perplexity=98.8452, train_loss=4.593555

Batch 78300, train_perplexity=95.71414, train_loss=4.561366

Batch 78310, train_perplexity=100.59122, train_loss=4.611065

Batch 78320, train_perplexity=101.426094, train_loss=4.6193304

Batch 78330, train_perplexity=89.45241, train_loss=4.4937067

Batch 78340, train_perplexity=93.647026, train_loss=4.5395327

Batch 78350, train_perplexity=86.73564, train_loss=4.462865

Batch 78360, train_perplexity=106.34088, train_loss=4.66665

Batch 78370, train_perplexity=97.61027, train_loss=4.5809827

Batch 78380, train_perplexity=93.524086, train_loss=4.538219

Batch 78390, train_perplexity=89.90886, train_loss=4.4987965

Batch 78400, train_perplexity=90.67678, train_loss=4.5073013

Batch 78410, train_perplexity=94.9682, train_loss=4.553542

Batch 78420, train_perplexity=91.89434, train_loss=4.5206394

Batch 78430, train_perplexity=98.06536, train_loss=4.585634

Batch 78440, train_perplexity=91.12575, train_loss=4.5122404

Batch 78450, train_perplexity=101.01641, train_loss=4.615283

Batch 78460, train_perplexity=96.199524, train_loss=4.5664244

Batch 78470, train_perplexity=98.2346, train_loss=4.5873585

Batch 78480, train_perplexity=108.23701, train_loss=4.6843233

Batch 78490, train_perplexity=97.3063, train_loss=4.5778637

Batch 78500, train_perplexity=96.16806, train_loss=4.5660973

Batch 78510, train_perplexity=96.701004, train_loss=4.571624

Batch 78520, train_perplexity=95.15278, train_loss=4.555484

Batch 78530, train_perplexity=92.49997, train_loss=4.5272083

Batch 78540, train_perplexity=91.80504, train_loss=4.519667

Batch 78550, train_perplexity=94.28498, train_loss=4.546322

Batch 78560, train_perplexity=80.29749, train_loss=4.3857384

Batch 78570, train_perplexity=85.75269, train_loss=4.4514675

Batch 78580, train_perplexity=96.48246, train_loss=4.569361

Batch 78590, train_perplexity=92.746506, train_loss=4.52987

Batch 78600, train_perplexity=93.45191, train_loss=4.537447

Batch 78610, train_perplexity=100.776726, train_loss=4.6129074

Batch 78620, train_perplexity=108.291214, train_loss=4.684824

Batch 78630, train_perplexity=91.19104, train_loss=4.5129566

Batch 78640, train_perplexity=97.17424, train_loss=4.5765057

Batch 78650, train_perplexity=86.47069, train_loss=4.4598055

Batch 78660, train_perplexity=87.20284, train_loss=4.468237

Batch 78670, train_perplexity=87.531746, train_loss=4.4720016

Batch 78680, train_perplexity=83.99854, train_loss=4.4307995

Batch 78690, train_perplexity=87.98482, train_loss=4.4771643

Batch 78700, train_perplexity=99.03717, train_loss=4.595495

Batch 78710, train_perplexity=91.73786, train_loss=4.518935

Batch 78720, train_perplexity=96.84498, train_loss=4.5731115

Batch 78730, train_perplexity=94.75511, train_loss=4.5512958

Batch 78740, train_perplexity=93.48534, train_loss=4.5378046

Batch 78750, train_perplexity=100.50582, train_loss=4.6102157

Batch 78760, train_perplexity=88.55894, train_loss=4.4836683

Batch 78770, train_perplexity=97.751816, train_loss=4.582432

Batch 78780, train_perplexity=86.99842, train_loss=4.46589

Batch 78790, train_perplexity=89.410995, train_loss=4.4932437

Batch 78800, train_perplexity=90.12369, train_loss=4.501183

Batch 78810, train_perplexity=96.03214, train_loss=4.564683

Batch 78820, train_perplexity=91.90223, train_loss=4.5207253

Batch 78830, train_perplexity=91.84195, train_loss=4.520069

Batch 78840, train_perplexity=100.36794, train_loss=4.608843

Batch 78850, train_perplexity=87.276054, train_loss=4.469076

Batch 78860, train_perplexity=100.44277, train_loss=4.609588

Batch 78870, train_perplexity=96.35359, train_loss=4.5680246

Batch 78880, train_perplexity=87.0909, train_loss=4.4669523

Batch 78890, train_perplexity=101.255516, train_loss=4.617647

Batch 78900, train_perplexity=109.66394, train_loss=4.6974206

Batch 78910, train_perplexity=92.43533, train_loss=4.5265093

Batch 78920, train_perplexity=102.42193, train_loss=4.629101

Batch 78930, train_perplexity=88.904366, train_loss=4.487561

Batch 78940, train_perplexity=89.86004, train_loss=4.4982533

Batch 78950, train_perplexity=92.34511, train_loss=4.5255327

Batch 78960, train_perplexity=87.727425, train_loss=4.4742346

Batch 78970, train_perplexity=97.068, train_loss=4.575412

Batch 78980, train_perplexity=99.37815, train_loss=4.5989323

Batch 78990, train_perplexity=91.68976, train_loss=4.5184107

Batch 79000, train_perplexity=88.3205, train_loss=4.4809723

Batch 79010, train_perplexity=99.394455, train_loss=4.5990963

Batch 79020, train_perplexity=82.610825, train_loss=4.4141407

Batch 79030, train_perplexity=93.8272, train_loss=4.541455

Batch 79040, train_perplexity=83.08623, train_loss=4.419879

Batch 79050, train_perplexity=88.53699, train_loss=4.4834204

Batch 79060, train_perplexity=91.80048, train_loss=4.5196176

Batch 79070, train_perplexity=86.37574, train_loss=4.458707

Batch 79080, train_perplexity=94.95258, train_loss=4.5533776

Batch 79090, train_perplexity=94.08023, train_loss=4.544148

Batch 79100, train_perplexity=92.698494, train_loss=4.529352

Batch 79110, train_perplexity=90.26454, train_loss=4.5027447

Batch 79120, train_perplexity=93.356865, train_loss=4.5364294

Batch 79130, train_perplexity=93.06574, train_loss=4.533306

Batch 79140, train_perplexity=89.16028, train_loss=4.4904356

Batch 79150, train_perplexity=93.0133, train_loss=4.5327425

Batch 79160, train_perplexity=86.31044, train_loss=4.4579506

Batch 79170, train_perplexity=92.605, train_loss=4.528343

Batch 79180, train_perplexity=96.22384, train_loss=4.566677

Batch 79190, train_perplexity=104.34226, train_loss=4.6476765

Batch 79200, train_perplexity=96.668, train_loss=4.5712824

Batch 79210, train_perplexity=95.78199, train_loss=4.5620747

Batch 79220, train_perplexity=100.783455, train_loss=4.612974

Batch 79230, train_perplexity=92.351974, train_loss=4.525607

Batch 79240, train_perplexity=88.51672, train_loss=4.4831915

Batch 79250, train_perplexity=81.53567, train_loss=4.4010406

Batch 79260, train_perplexity=88.33659, train_loss=4.4811544

Batch 79270, train_perplexity=87.856865, train_loss=4.475709

Batch 79280, train_perplexity=92.287796, train_loss=4.524912

Batch 79290, train_perplexity=82.139, train_loss=4.408413

Batch 79300, train_perplexity=94.00082, train_loss=4.5433035

Batch 79310, train_perplexity=94.310425, train_loss=4.5465918

Batch 79320, train_perplexity=101.272896, train_loss=4.617819

Batch 79330, train_perplexity=103.293526, train_loss=4.6375747

Batch 79340, train_perplexity=95.1575, train_loss=4.5555334

Batch 79350, train_perplexity=90.49044, train_loss=4.5052443

Batch 79360, train_perplexity=94.829506, train_loss=4.5520806

Batch 79370, train_perplexity=92.96466, train_loss=4.5322194

Batch 79380, train_perplexity=101.11116, train_loss=4.6162205

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Finished loading
Batch 79390, train_perplexity=90.18047, train_loss=4.501813

Batch 79400, train_perplexity=98.846565, train_loss=4.593569

Batch 79410, train_perplexity=89.968895, train_loss=4.499464

Batch 79420, train_perplexity=87.97588, train_loss=4.4770627

Batch 79430, train_perplexity=86.97473, train_loss=4.4656177

Batch 79440, train_perplexity=95.095535, train_loss=4.554882

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 79450, train_perplexity=88.79963, train_loss=4.4863825

Batch 79460, train_perplexity=84.73785, train_loss=4.4395623

Batch 79470, train_perplexity=90.1828, train_loss=4.5018387

Batch 79480, train_perplexity=75.74645, train_loss=4.3273916

Batch 79490, train_perplexity=87.88691, train_loss=4.476051

Batch 79500, train_perplexity=82.97422, train_loss=4.41853

Batch 79510, train_perplexity=91.950615, train_loss=4.5212517

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 79520, train_perplexity=84.66147, train_loss=4.4386606

Batch 79530, train_perplexity=96.595795, train_loss=4.570535

Batch 79540, train_perplexity=95.29313, train_loss=4.5569577

Batch 79550, train_perplexity=86.67751, train_loss=4.4621944

Batch 79560, train_perplexity=92.36017, train_loss=4.525696

Batch 79570, train_perplexity=81.94457, train_loss=4.406043

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 79580, train_perplexity=105.37486, train_loss=4.657524

Batch 79590, train_perplexity=84.04951, train_loss=4.431406

Batch 79600, train_perplexity=95.88796, train_loss=4.5631804

Batch 79610, train_perplexity=94.93262, train_loss=4.5531673

Batch 79620, train_perplexity=97.60548, train_loss=4.5809336

Batch 79630, train_perplexity=89.04565, train_loss=4.489149

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 79640, train_perplexity=85.22838, train_loss=4.4453344

Batch 79650, train_perplexity=96.86623, train_loss=4.573331

Batch 79660, train_perplexity=96.54841, train_loss=4.5700445

Batch 79670, train_perplexity=91.00833, train_loss=4.510951

Batch 79680, train_perplexity=96.80721, train_loss=4.5727215

Batch 79690, train_perplexity=97.75895, train_loss=4.5825047

Batch 79700, train_perplexity=84.54993, train_loss=4.437342

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 79710, train_perplexity=85.78353, train_loss=4.451827

Batch 79720, train_perplexity=88.01629, train_loss=4.477522

Batch 79730, train_perplexity=84.91359, train_loss=4.441634

Batch 79740, train_perplexity=80.37809, train_loss=4.3867416

Batch 79750, train_perplexity=94.74531, train_loss=4.5511923

Batch 79760, train_perplexity=94.51013, train_loss=4.548707

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 79770, train_perplexity=87.179474, train_loss=4.467969

Batch 79780, train_perplexity=90.12304, train_loss=4.501176

Batch 79790, train_perplexity=93.39747, train_loss=4.5368643

Batch 79800, train_perplexity=83.54293, train_loss=4.4253607

Batch 79810, train_perplexity=88.936, train_loss=4.487917

Batch 79820, train_perplexity=100.61165, train_loss=4.611268

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 79830, train_perplexity=94.17476, train_loss=4.545152

Batch 79840, train_perplexity=88.57284, train_loss=4.483825

Batch 79850, train_perplexity=94.8547, train_loss=4.552346

Batch 79860, train_perplexity=94.1115, train_loss=4.5444803

Batch 79870, train_perplexity=87.77291, train_loss=4.474753

Batch 79880, train_perplexity=102.12902, train_loss=4.626237

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 79890, train_perplexity=88.081154, train_loss=4.4782586

Batch 79900, train_perplexity=90.46507, train_loss=4.504964

Batch 79910, train_perplexity=96.84914, train_loss=4.5731544

Batch 79920, train_perplexity=88.93133, train_loss=4.4878645

Batch 79930, train_perplexity=86.54106, train_loss=4.460619

Batch 79940, train_perplexity=91.42179, train_loss=4.515484

Batch 79950, train_perplexity=89.95946, train_loss=4.499359

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 79960, train_perplexity=102.82515, train_loss=4.63303

Batch 79970, train_perplexity=84.564316, train_loss=4.4375124

Batch 79980, train_perplexity=94.610405, train_loss=4.5497675

Batch 79990, train_perplexity=93.835075, train_loss=4.5415387

Batch 80000, train_perplexity=92.71122, train_loss=4.5294895

Batch 80010, train_perplexity=92.96187, train_loss=4.5321894

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 80020, train_perplexity=101.0619, train_loss=4.615733

Batch 80030, train_perplexity=101.86036, train_loss=4.623603

Batch 80040, train_perplexity=93.62997, train_loss=4.5393505

Batch 80050, train_perplexity=91.77926, train_loss=4.5193863

Batch 80060, train_perplexity=88.49697, train_loss=4.4829683

Batch 80070, train_perplexity=80.52318, train_loss=4.388545

Batch 80080, train_perplexity=86.50145, train_loss=4.460161

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 80090, train_perplexity=101.0247, train_loss=4.615365

Batch 80100, train_perplexity=82.691536, train_loss=4.4151173

Batch 80110, train_perplexity=94.9817, train_loss=4.553684

Batch 80120, train_perplexity=93.28985, train_loss=4.5357113

Batch 80130, train_perplexity=95.439514, train_loss=4.5584927

Batch 80140, train_perplexity=94.31011, train_loss=4.5465884

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 80150, train_perplexity=93.76517, train_loss=4.5407934

Batch 80160, train_perplexity=89.80666, train_loss=4.497659

Batch 80170, train_perplexity=88.49824, train_loss=4.4829826

Batch 80180, train_perplexity=93.870834, train_loss=4.5419197

Batch 80190, train_perplexity=97.19037, train_loss=4.5766716

Batch 80200, train_perplexity=95.77971, train_loss=4.562051

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 80210, train_perplexity=96.831726, train_loss=4.5729747

Batch 80220, train_perplexity=89.68611, train_loss=4.496316

Batch 80230, train_perplexity=89.7713, train_loss=4.4972653

Batch 80240, train_perplexity=94.45237, train_loss=4.5480957

Batch 80250, train_perplexity=91.99816, train_loss=4.5217686

Batch 80260, train_perplexity=87.66737, train_loss=4.47355

Batch 80270, train_perplexity=93.1104, train_loss=4.533786

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 80280, train_perplexity=90.862335, train_loss=4.5093455

Batch 80290, train_perplexity=95.75824, train_loss=4.5618267

Batch 80300, train_perplexity=90.51508, train_loss=4.5055165

Batch 80310, train_perplexity=96.452515, train_loss=4.569051

Batch 80320, train_perplexity=91.92405, train_loss=4.5209627

Batch 80330, train_perplexity=95.440926, train_loss=4.5585074

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 80340, train_perplexity=102.78799, train_loss=4.6326685

Batch 80350, train_perplexity=89.72098, train_loss=4.4967046

Batch 80360, train_perplexity=85.96846, train_loss=4.4539804

Batch 80370, train_perplexity=94.646866, train_loss=4.550153

Batch 80380, train_perplexity=93.87795, train_loss=4.5419955

Batch 80390, train_perplexity=101.698265, train_loss=4.62201

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 80400, train_perplexity=89.76261, train_loss=4.4971685

Batch 80410, train_perplexity=93.83064, train_loss=4.5414915

Batch 80420, train_perplexity=82.53743, train_loss=4.413252

Batch 80430, train_perplexity=90.70035, train_loss=4.507561

Batch 80440, train_perplexity=99.40189, train_loss=4.599171

Batch 80450, train_perplexity=85.25618, train_loss=4.4456606

Batch 80460, train_perplexity=96.66993, train_loss=4.5713024

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 80470, train_perplexity=85.05421, train_loss=4.443289

Batch 80480, train_perplexity=99.78538, train_loss=4.6030216

Batch 80490, train_perplexity=84.93178, train_loss=4.4418483

Batch 80500, train_perplexity=104.70001, train_loss=4.651099

Batch 80510, train_perplexity=79.19234, train_loss=4.3718796

Batch 80520, train_perplexity=93.930565, train_loss=4.542556

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 80530, train_perplexity=89.696724, train_loss=4.496434

Batch 80540, train_perplexity=89.347534, train_loss=4.4925337

Batch 80550, train_perplexity=91.819176, train_loss=4.519821

Batch 80560, train_perplexity=83.797714, train_loss=4.428406

Batch 80570, train_perplexity=101.65967, train_loss=4.6216307

Batch 80580, train_perplexity=88.05323, train_loss=4.4779415

Batch 80590, train_perplexity=92.12327, train_loss=4.5231276

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 80600, train_perplexity=88.510895, train_loss=4.4831257

Batch 80610, train_perplexity=87.094505, train_loss=4.466994

Batch 80620, train_perplexity=86.75144, train_loss=4.463047

Batch 80630, train_perplexity=85.41316, train_loss=4.4475

Batch 80640, train_perplexity=95.7084, train_loss=4.561306

Batch 80650, train_perplexity=81.43802, train_loss=4.3998423

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 80660, train_perplexity=97.393616, train_loss=4.5787606
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 80670, train_perplexity=84.75696, train_loss=4.439788

Batch 80680, train_perplexity=93.50674, train_loss=4.5380335

Batch 80690, train_perplexity=93.57279, train_loss=4.5387397

Batch 80700, train_perplexity=89.90191, train_loss=4.498719

Batch 80710, train_perplexity=96.18127, train_loss=4.5662346

Batch 80720, train_perplexity=95.13232, train_loss=4.555269

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 80730, train_perplexity=89.87868, train_loss=4.498461

Batch 80740, train_perplexity=85.12724, train_loss=4.444147

Batch 80750, train_perplexity=97.554756, train_loss=4.580414

Batch 80760, train_perplexity=94.78299, train_loss=4.55159

Batch 80770, train_perplexity=81.926796, train_loss=4.405826

Batch 80780, train_perplexity=84.895615, train_loss=4.4414225

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 80790, train_perplexity=95.789665, train_loss=4.562155

Batch 80800, train_perplexity=87.063736, train_loss=4.4666405

Batch 80810, train_perplexity=92.05898, train_loss=4.5224295

Batch 80820, train_perplexity=100.21524, train_loss=4.6073203

Batch 80830, train_perplexity=89.39284, train_loss=4.4930406

Batch 80840, train_perplexity=99.006004, train_loss=4.5951805

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 80850, train_perplexity=87.55341, train_loss=4.472249

Batch 80860, train_perplexity=80.7483, train_loss=4.391337

Batch 80870, train_perplexity=90.56711, train_loss=4.506091

Batch 80880, train_perplexity=100.31813, train_loss=4.6083465

Batch 80890, train_perplexity=94.8353, train_loss=4.5521417

Batch 80900, train_perplexity=86.997215, train_loss=4.465876

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 80910, train_perplexity=91.30982, train_loss=4.5142584

Batch 80920, train_perplexity=89.31308, train_loss=4.492148

Batch 80930, train_perplexity=89.49635, train_loss=4.494198

Batch 80940, train_perplexity=90.77157, train_loss=4.508346

Batch 80950, train_perplexity=87.036674, train_loss=4.4663296

Batch 80960, train_perplexity=83.65383, train_loss=4.4266872

Batch 80970, train_perplexity=103.27239, train_loss=4.63737

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 80980, train_perplexity=87.29749, train_loss=4.4693217

Batch 80990, train_perplexity=95.45826, train_loss=4.558689

Batch 81000, train_perplexity=88.519554, train_loss=4.4832234

Batch 81010, train_perplexity=83.86551, train_loss=4.4292145

Batch 81020, train_perplexity=92.9929, train_loss=4.532523

Batch 81030, train_perplexity=91.71984, train_loss=4.5187387

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 81040, train_perplexity=96.40828, train_loss=4.568592

Batch 81050, train_perplexity=85.88684, train_loss=4.4530306

Batch 81060, train_perplexity=89.267136, train_loss=4.4916334

Batch 81070, train_perplexity=92.323006, train_loss=4.5252934

Batch 81080, train_perplexity=80.96579, train_loss=4.3940268

Batch 81090, train_perplexity=95.89775, train_loss=4.5632825

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 81100, train_perplexity=87.45477, train_loss=4.471122

Batch 81110, train_perplexity=92.40995, train_loss=4.5262346

Batch 81120, train_perplexity=93.85096, train_loss=4.541708

Batch 81130, train_perplexity=86.06681, train_loss=4.455124

Batch 81140, train_perplexity=91.35756, train_loss=4.514781

Batch 81150, train_perplexity=89.13825, train_loss=4.4901886

Batch 81160, train_perplexity=89.02085, train_loss=4.4888706

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 81170, train_perplexity=83.15044, train_loss=4.4206514

Batch 81180, train_perplexity=88.37518, train_loss=4.481591

Batch 81190, train_perplexity=100.14163, train_loss=4.6065855

Batch 81200, train_perplexity=88.768684, train_loss=4.486034

Batch 81210, train_perplexity=94.3402, train_loss=4.5469074

Batch 81220, train_perplexity=98.651245, train_loss=4.591591

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 81230, train_perplexity=105.85076, train_loss=4.66203

Batch 81240, train_perplexity=95.33108, train_loss=4.557356

Batch 81250, train_perplexity=85.84074, train_loss=4.4524937

Batch 81260, train_perplexity=95.9957, train_loss=4.5643034

Batch 81270, train_perplexity=85.695305, train_loss=4.450798

Batch 81280, train_perplexity=88.78181, train_loss=4.4861817

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 81290, train_perplexity=87.71814, train_loss=4.4741287

Batch 81300, train_perplexity=99.983986, train_loss=4.60501

Batch 81310, train_perplexity=94.46769, train_loss=4.548258

Batch 81320, train_perplexity=106.10956, train_loss=4.664472

Batch 81330, train_perplexity=86.8314, train_loss=4.4639683

Batch 81340, train_perplexity=98.043625, train_loss=4.5854125

Batch 81350, train_perplexity=93.581856, train_loss=4.5388365

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 81360, train_perplexity=90.84579, train_loss=4.5091634

Batch 81370, train_perplexity=87.76106, train_loss=4.474618

Batch 81380, train_perplexity=97.0892, train_loss=4.57563

Batch 81390, train_perplexity=98.06213, train_loss=4.5856013

Batch 81400, train_perplexity=97.81453, train_loss=4.583073

Batch 81410, train_perplexity=84.723946, train_loss=4.4393983

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 81420, train_perplexity=90.48298, train_loss=4.505162

Batch 81430, train_perplexity=88.06608, train_loss=4.4780874

Batch 81440, train_perplexity=92.75557, train_loss=4.529968

Batch 81450, train_perplexity=100.11117, train_loss=4.6062813

Batch 81460, train_perplexity=82.61658, train_loss=4.4142103

Batch 81470, train_perplexity=92.767075, train_loss=4.530092

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 81480, train_perplexity=94.58623, train_loss=4.549512

Batch 81490, train_perplexity=90.60279, train_loss=4.506485

Batch 81500, train_perplexity=89.8753, train_loss=4.498423

Batch 81510, train_perplexity=92.31834, train_loss=4.525243

Batch 81520, train_perplexity=95.02854, train_loss=4.5541773

Batch 81530, train_perplexity=107.34642, train_loss=4.676061

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 81540, train_perplexity=98.15865, train_loss=4.586585

Batch 81550, train_perplexity=86.35671, train_loss=4.4584866

Batch 81560, train_perplexity=79.47928, train_loss=4.3754964

Batch 81570, train_perplexity=91.50815, train_loss=4.516428

Batch 81580, train_perplexity=83.29528, train_loss=4.422392

Batch 81590, train_perplexity=108.16163, train_loss=4.6836267

Batch 81600, train_perplexity=99.232635, train_loss=4.597467

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 81610, train_perplexity=98.0468, train_loss=4.585445

Batch 81620, train_perplexity=102.2563, train_loss=4.6274824

Batch 81630, train_perplexity=90.52424, train_loss=4.5056176

Batch 81640, train_perplexity=90.66234, train_loss=4.507142

Batch 81650, train_perplexity=93.00847, train_loss=4.5326905

Batch 81660, train_perplexity=98.36866, train_loss=4.588722

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 81670, train_perplexity=86.41751, train_loss=4.4591904

Batch 81680, train_perplexity=95.165306, train_loss=4.5556154

Batch 81690, train_perplexity=97.204735, train_loss=4.5768194

Batch 81700, train_perplexity=91.329765, train_loss=4.514477

Batch 81710, train_perplexity=89.19982, train_loss=4.490879

Batch 81720, train_perplexity=91.633896, train_loss=4.5178013

Batch 81730, train_perplexity=90.133354, train_loss=4.5012903

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 81740, train_perplexity=95.284454, train_loss=4.5568666

Batch 81750, train_perplexity=85.75208, train_loss=4.4514604

Batch 81760, train_perplexity=93.775986, train_loss=4.540909

Batch 81770, train_perplexity=98.93616, train_loss=4.594475

Batch 81780, train_perplexity=87.63381, train_loss=4.473167

Batch 81790, train_perplexity=95.3339, train_loss=4.5573854

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 81800, train_perplexity=89.73317, train_loss=4.4968405

Batch 81810, train_perplexity=95.72409, train_loss=4.56147

Batch 81820, train_perplexity=103.85888, train_loss=4.643033

Batch 81830, train_perplexity=98.9816, train_loss=4.594934

Batch 81840, train_perplexity=91.94325, train_loss=4.5211716

Batch 81850, train_perplexity=99.45153, train_loss=4.5996704

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 81860, train_perplexity=96.641174, train_loss=4.571005

Batch 81870, train_perplexity=85.765945, train_loss=4.451622

Batch 81880, train_perplexity=90.60547, train_loss=4.5065145

Batch 81890, train_perplexity=91.39694, train_loss=4.515212

Batch 81900, train_perplexity=93.8625, train_loss=4.541831

Batch 81910, train_perplexity=83.54616, train_loss=4.4253993

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 81920, train_perplexity=84.94494, train_loss=4.4420033

Batch 81930, train_perplexity=76.95084, train_loss=4.343167

Batch 81940, train_perplexity=89.37362, train_loss=4.4928255

Batch 81950, train_perplexity=87.16655, train_loss=4.4678206

Batch 81960, train_perplexity=95.2482, train_loss=4.556486
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 81970, train_perplexity=97.85596, train_loss=4.5834966

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 81980, train_perplexity=100.83989, train_loss=4.613534

Batch 81990, train_perplexity=97.020805, train_loss=4.5749254

Batch 82000, train_perplexity=98.307274, train_loss=4.588098

Batch 82010, train_perplexity=94.120575, train_loss=4.5445766

Batch 82020, train_perplexity=88.215195, train_loss=4.4797792

Batch 82030, train_perplexity=95.73541, train_loss=4.5615883

Batch 82040, train_perplexity=95.18473, train_loss=4.5558195

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 82050, train_perplexity=102.41255, train_loss=4.6290092

Batch 82060, train_perplexity=94.33624, train_loss=4.5468655

Batch 82070, train_perplexity=83.94353, train_loss=4.4301443

Batch 82080, train_perplexity=83.74946, train_loss=4.4278297

Batch 82090, train_perplexity=81.37564, train_loss=4.399076

Batch 82100, train_perplexity=95.87594, train_loss=4.563055

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 82110, train_perplexity=102.78642, train_loss=4.632653

Batch 82120, train_perplexity=84.08575, train_loss=4.431837

Batch 82130, train_perplexity=89.886055, train_loss=4.498543

Batch 82140, train_perplexity=87.7804, train_loss=4.4748383

Batch 82150, train_perplexity=87.028625, train_loss=4.466237

Batch 82160, train_perplexity=87.66646, train_loss=4.4735394

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 82170, train_perplexity=109.96309, train_loss=4.700145

Batch 82180, train_perplexity=97.86156, train_loss=4.583554

Batch 82190, train_perplexity=82.804146, train_loss=4.416478

Batch 82200, train_perplexity=86.68247, train_loss=4.4622517

Batch 82210, train_perplexity=98.9339, train_loss=4.594452

Batch 82220, train_perplexity=83.53369, train_loss=4.42525

Batch 82230, train_perplexity=92.41175, train_loss=4.526254

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 82240, train_perplexity=109.42638, train_loss=4.695252

Batch 82250, train_perplexity=90.27685, train_loss=4.502881

Batch 82260, train_perplexity=96.515175, train_loss=4.5697002

Batch 82270, train_perplexity=90.47763, train_loss=4.5051026

Batch 82280, train_perplexity=101.46557, train_loss=4.6197195

Batch 82290, train_perplexity=99.058136, train_loss=4.595707

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 82300, train_perplexity=93.291985, train_loss=4.535734

Batch 82310, train_perplexity=94.84149, train_loss=4.552207

Batch 82320, train_perplexity=89.902855, train_loss=4.4987297

Batch 82330, train_perplexity=94.26587, train_loss=4.546119

Batch 82340, train_perplexity=94.786514, train_loss=4.551627

Batch 82350, train_perplexity=91.92668, train_loss=4.5209913

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 82360, train_perplexity=84.59642, train_loss=4.437892

Batch 82370, train_perplexity=88.465324, train_loss=4.4826107

Batch 82380, train_perplexity=96.35506, train_loss=4.56804

Batch 82390, train_perplexity=96.11547, train_loss=4.5655503

Batch 82400, train_perplexity=86.519684, train_loss=4.460372

Batch 82410, train_perplexity=90.87819, train_loss=4.50952

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 82420, train_perplexity=97.76226, train_loss=4.5825386

Batch 82430, train_perplexity=93.92751, train_loss=4.5425234

Batch 82440, train_perplexity=95.1811, train_loss=4.5557814

Batch 82450, train_perplexity=98.74198, train_loss=4.59251

Batch 82460, train_perplexity=90.13628, train_loss=4.5013227

Batch 82470, train_perplexity=87.4697, train_loss=4.4712925

Batch 82480, train_perplexity=95.74528, train_loss=4.5616913

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 82490, train_perplexity=95.388916, train_loss=4.5579624

Batch 82500, train_perplexity=80.44991, train_loss=4.3876348

Batch 82510, train_perplexity=87.227715, train_loss=4.468522

Batch 82520, train_perplexity=110.374, train_loss=4.7038746

Batch 82530, train_perplexity=94.39321, train_loss=4.547469

Batch 82540, train_perplexity=99.83192, train_loss=4.603488

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 82550, train_perplexity=101.955315, train_loss=4.6245346

Batch 82560, train_perplexity=84.748474, train_loss=4.4396877

Batch 82570, train_perplexity=89.374725, train_loss=4.492838

Batch 82580, train_perplexity=102.78093, train_loss=4.6326

Batch 82590, train_perplexity=96.444695, train_loss=4.5689697

Batch 82600, train_perplexity=91.950966, train_loss=4.5212555

Batch 82610, train_perplexity=75.01077, train_loss=4.3176317
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82620, train_perplexity=77.50118, train_loss=4.350293

Batch 82630, train_perplexity=88.7413, train_loss=4.4857254

Batch 82640, train_perplexity=88.24305, train_loss=4.480095

Batch 82650, train_perplexity=88.637436, train_loss=4.4845543

Batch 82660, train_perplexity=94.05314, train_loss=4.54386

Batch 82670, train_perplexity=81.48673, train_loss=4.40044

Batch 82680, train_perplexity=77.95803, train_loss=4.3561707

Batch 82690, train_perplexity=88.1718, train_loss=4.479287

Batch 82700, train_perplexity=83.84384, train_loss=4.428956

Batch 82710, train_perplexity=92.206985, train_loss=4.524036

Batch 82720, train_perplexity=84.02538, train_loss=4.431119

Batch 82730, train_perplexity=92.49261, train_loss=4.5271287

Batch 82740, train_perplexity=86.25696, train_loss=4.4573307

Batch 82750, train_perplexity=85.05117, train_loss=4.443253

Batch 82760, train_perplexity=86.80176, train_loss=4.463627

Batch 82770, train_perplexity=97.68239, train_loss=4.5817213

Batch 82780, train_perplexity=97.426125, train_loss=4.5790944

Batch 82790, train_perplexity=89.10553, train_loss=4.4898214

Batch 82800, train_perplexity=85.15339, train_loss=4.444454

Batch 82810, train_perplexity=85.601616, train_loss=4.449704

Batch 82820, train_perplexity=90.03293, train_loss=4.5001755

Batch 82830, train_perplexity=82.24988, train_loss=4.409762

Batch 82840, train_perplexity=85.59117, train_loss=4.449582

Batch 82850, train_perplexity=91.272385, train_loss=4.5138483

Batch 82860, train_perplexity=99.39909, train_loss=4.599143

Batch 82870, train_perplexity=89.1091, train_loss=4.4898615

Batch 82880, train_perplexity=77.25313, train_loss=4.3470874

Batch 82890, train_perplexity=86.98593, train_loss=4.4657464

Batch 82900, train_perplexity=80.25936, train_loss=4.3852634

Batch 82910, train_perplexity=93.955605, train_loss=4.5428224

Batch 82920, train_perplexity=88.77914, train_loss=4.4861517

Batch 82930, train_perplexity=82.82014, train_loss=4.4166713

Batch 82940, train_perplexity=83.19311, train_loss=4.4211645

Batch 82950, train_perplexity=91.12154, train_loss=4.512194

Batch 82960, train_perplexity=92.87068, train_loss=4.531208

Batch 82970, train_perplexity=83.439186, train_loss=4.424118

Batch 82980, train_perplexity=89.735825, train_loss=4.49687

Batch 82990, train_perplexity=91.42711, train_loss=4.515542

Batch 83000, train_perplexity=92.865456, train_loss=4.531152

Batch 83010, train_perplexity=79.01973, train_loss=4.3696976

Batch 83020, train_perplexity=88.423996, train_loss=4.4821434

Batch 83030, train_perplexity=88.74282, train_loss=4.4857426

Batch 83040, train_perplexity=86.91615, train_loss=4.464944

Batch 83050, train_perplexity=88.60321, train_loss=4.484168

Batch 83060, train_perplexity=89.44085, train_loss=4.4935775

Batch 83070, train_perplexity=94.0162, train_loss=4.543467

Batch 83080, train_perplexity=82.74494, train_loss=4.415763

Batch 83090, train_perplexity=92.96963, train_loss=4.532273

Batch 83100, train_perplexity=88.53884, train_loss=4.4834414

Batch 83110, train_perplexity=95.43232, train_loss=4.5584173

Batch 83120, train_perplexity=74.759926, train_loss=4.314282

Batch 83130, train_perplexity=84.89861, train_loss=4.4414577

Batch 83140, train_perplexity=99.7966, train_loss=4.603134

Batch 83150, train_perplexity=77.72708, train_loss=4.353204

Batch 83160, train_perplexity=82.95045, train_loss=4.4182434

Batch 83170, train_perplexity=88.98596, train_loss=4.4884787

Batch 83180, train_perplexity=92.19164, train_loss=4.5238695

Batch 83190, train_perplexity=79.59591, train_loss=4.3769627

Batch 83200, train_perplexity=91.94448, train_loss=4.521185

Batch 83210, train_perplexity=88.16003, train_loss=4.4791536

Batch 83220, train_perplexity=93.304794, train_loss=4.5358715

Batch 83230, train_perplexity=82.05722, train_loss=4.407417

Batch 83240, train_perplexity=85.30815, train_loss=4.44627

Batch 83250, train_perplexity=86.857155, train_loss=4.464265

Batch 83260, train_perplexity=85.63885, train_loss=4.450139

Batch 83270, train_perplexity=98.64729, train_loss=4.591551

Batch 83280, train_perplexity=88.17331, train_loss=4.4793043

Batch 83290, train_perplexity=96.15632, train_loss=4.565975

Batch 83300, train_perplexity=94.54871, train_loss=4.549115

Batch 83310, train_perplexity=84.8422, train_loss=4.440793

Batch 83320, train_perplexity=94.363014, train_loss=4.547149

Batch 83330, train_perplexity=91.88838, train_loss=4.5205746

Batch 83340, train_perplexity=90.625, train_loss=4.50673

Batch 83350, train_perplexity=88.91979, train_loss=4.487735

Batch 83360, train_perplexity=90.769966, train_loss=4.5083284

Batch 83370, train_perplexity=79.23657, train_loss=4.372438

Batch 83380, train_perplexity=91.24045, train_loss=4.5134983

Batch 83390, train_perplexity=84.98545, train_loss=4.44248

Batch 83400, train_perplexity=87.88963, train_loss=4.476082

Batch 83410, train_perplexity=87.28134, train_loss=4.4691367

Batch 83420, train_perplexity=80.78023, train_loss=4.391732

Batch 83430, train_perplexity=94.74228, train_loss=4.5511603

Batch 83440, train_perplexity=94.03651, train_loss=4.543683

Batch 83450, train_perplexity=97.576805, train_loss=4.58064

Batch 83460, train_perplexity=79.076645, train_loss=4.3704176

Batch 83470, train_perplexity=88.587746, train_loss=4.4839935

Batch 83480, train_perplexity=91.772865, train_loss=4.5193167

Batch 83490, train_perplexity=94.89397, train_loss=4.55276

Batch 83500, train_perplexity=84.432, train_loss=4.4359465

Batch 83510, train_perplexity=87.08961, train_loss=4.4669375

Batch 83520, train_perplexity=88.09649, train_loss=4.4784327

Batch 83530, train_perplexity=87.86218, train_loss=4.4757695

Batch 83540, train_perplexity=93.8081, train_loss=4.541251

Batch 83550, train_perplexity=96.11199, train_loss=4.565514

Batch 83560, train_perplexity=96.79558, train_loss=4.5726013

Batch 83570, train_perplexity=88.291664, train_loss=4.4806457

Batch 83580, train_perplexity=98.91739, train_loss=4.594285

Batch 83590, train_perplexity=100.062965, train_loss=4.6057997

Batch 83600, train_perplexity=90.32529, train_loss=4.5034175

Batch 83610, train_perplexity=90.03675, train_loss=4.500218

Batch 83620, train_perplexity=87.59576, train_loss=4.4727325

Batch 83630, train_perplexity=87.34663, train_loss=4.4698844

Batch 83640, train_perplexity=89.47254, train_loss=4.493932

Batch 83650, train_perplexity=100.28992, train_loss=4.608065

Batch 83660, train_perplexity=94.50851, train_loss=4.54869

Batch 83670, train_perplexity=78.685844, train_loss=4.3654633

Batch 83680, train_perplexity=86.58523, train_loss=4.461129

Batch 83690, train_perplexity=89.46925, train_loss=4.493895

Batch 83700, train_perplexity=87.177315, train_loss=4.467944

Batch 83710, train_perplexity=96.02088, train_loss=4.5645657

Batch 83720, train_perplexity=91.106544, train_loss=4.5120296

Batch 83730, train_perplexity=90.43782, train_loss=4.5046625

Batch 83740, train_perplexity=97.04255, train_loss=4.5751495

Batch 83750, train_perplexity=98.48101, train_loss=4.589864

Batch 83760, train_perplexity=96.14742, train_loss=4.5658827

Batch 83770, train_perplexity=82.335304, train_loss=4.4108

Batch 83780, train_perplexity=89.79686, train_loss=4.49755

Batch 83790, train_perplexity=83.231, train_loss=4.42162

Batch 83800, train_perplexity=93.57213, train_loss=4.5387325

Batch 83810, train_perplexity=96.778824, train_loss=4.572428

Batch 83820, train_perplexity=90.851974, train_loss=4.5092316

Batch 83830, train_perplexity=81.944534, train_loss=4.4060426

Batch 83840, train_perplexity=88.686516, train_loss=4.485108

Batch 83850, train_perplexity=88.09758, train_loss=4.478445

Batch 83860, train_perplexity=86.19668, train_loss=4.4566317

Batch 83870, train_perplexity=87.17299, train_loss=4.4678946

Batch 83880, train_perplexity=86.6081, train_loss=4.4613934

Batch 83890, train_perplexity=100.360855, train_loss=4.6087723

Batch 83900, train_perplexity=88.61567, train_loss=4.4843087

Batch 83910, train_perplexity=92.08919, train_loss=4.5227575

Batch 83920, train_perplexity=83.761086, train_loss=4.4279685

Batch 83930, train_perplexity=86.96043, train_loss=4.465453

Batch 83940, train_perplexity=87.81063, train_loss=4.4751825

Batch 83950, train_perplexity=86.81509, train_loss=4.4637804
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 83960, train_perplexity=87.72579, train_loss=4.474216

Batch 83970, train_perplexity=95.27782, train_loss=4.556797

Batch 83980, train_perplexity=84.89619, train_loss=4.441429

Batch 83990, train_perplexity=92.30241, train_loss=4.52507

Batch 84000, train_perplexity=89.46316, train_loss=4.493827

Batch 84010, train_perplexity=97.50593, train_loss=4.579913

Batch 84020, train_perplexity=99.32329, train_loss=4.59838

Batch 84030, train_perplexity=87.12944, train_loss=4.467395

Batch 84040, train_perplexity=81.934494, train_loss=4.40592

Batch 84050, train_perplexity=90.37971, train_loss=4.5040197

Batch 84060, train_perplexity=94.29586, train_loss=4.5464373

Batch 84070, train_perplexity=86.110535, train_loss=4.4556317

Batch 84080, train_perplexity=87.45811, train_loss=4.47116

Batch 84090, train_perplexity=89.22969, train_loss=4.491214

Batch 84100, train_perplexity=81.92621, train_loss=4.405819

Batch 84110, train_perplexity=80.10272, train_loss=4.38331

Batch 84120, train_perplexity=86.22677, train_loss=4.4569807

Batch 84130, train_perplexity=87.55625, train_loss=4.4722815

Batch 84140, train_perplexity=84.29402, train_loss=4.434311

Batch 84150, train_perplexity=94.8206, train_loss=4.5519867

Batch 84160, train_perplexity=83.183426, train_loss=4.421048

Batch 84170, train_perplexity=91.953865, train_loss=4.521287

Batch 84180, train_perplexity=84.38354, train_loss=4.4353724

Batch 84190, train_perplexity=89.8468, train_loss=4.498106

Batch 84200, train_perplexity=86.742836, train_loss=4.462948

Batch 84210, train_perplexity=91.47865, train_loss=4.5161057

Batch 84220, train_perplexity=89.86154, train_loss=4.49827

Batch 84230, train_perplexity=97.239784, train_loss=4.57718

Batch 84240, train_perplexity=92.04998, train_loss=4.5223317

Batch 84250, train_perplexity=90.059296, train_loss=4.5004683

Batch 84260, train_perplexity=102.336006, train_loss=4.6282616

Batch 84270, train_perplexity=88.12363, train_loss=4.4787407

Batch 84280, train_perplexity=85.60198, train_loss=4.4497085

Batch 84290, train_perplexity=79.253006, train_loss=4.3726454

Batch 84300, train_perplexity=91.521065, train_loss=4.516569

Batch 84310, train_perplexity=84.55577, train_loss=4.4374113

Batch 84320, train_perplexity=95.23762, train_loss=4.556375

Batch 84330, train_perplexity=89.242455, train_loss=4.491357

Batch 84340, train_perplexity=74.58214, train_loss=4.311901

Batch 84350, train_perplexity=89.96263, train_loss=4.4993944

Batch 84360, train_perplexity=94.79348, train_loss=4.5517006

Batch 84370, train_perplexity=82.64242, train_loss=4.414523

Batch 84380, train_perplexity=92.91551, train_loss=4.5316906

Batch 84390, train_perplexity=80.14849, train_loss=4.383881

Batch 84400, train_perplexity=94.342995, train_loss=4.546937

Batch 84410, train_perplexity=94.991936, train_loss=4.553792

Batch 84420, train_perplexity=91.53533, train_loss=4.516725

Batch 84430, train_perplexity=103.850655, train_loss=4.642954

Batch 84440, train_perplexity=94.88895, train_loss=4.552707

Batch 84450, train_perplexity=90.94586, train_loss=4.5102644

Batch 84460, train_perplexity=97.25541, train_loss=4.5773406

Batch 84470, train_perplexity=90.97436, train_loss=4.5105777

Batch 84480, train_perplexity=87.45386, train_loss=4.4711113

Batch 84490, train_perplexity=87.72676, train_loss=4.474227

Batch 84500, train_perplexity=87.05311, train_loss=4.4665184

Batch 84510, train_perplexity=86.744576, train_loss=4.462968

Batch 84520, train_perplexity=85.66278, train_loss=4.4504185

Batch 84530, train_perplexity=94.3205, train_loss=4.5466986

Batch 84540, train_perplexity=86.05803, train_loss=4.455022

Batch 84550, train_perplexity=81.73295, train_loss=4.403457

Batch 84560, train_perplexity=93.991585, train_loss=4.5432053

Batch 84570, train_perplexity=89.7695, train_loss=4.4972453

Batch 84580, train_perplexity=91.70252, train_loss=4.51855

Batch 84590, train_perplexity=91.813484, train_loss=4.519759

Batch 84600, train_perplexity=93.01197, train_loss=4.532728

Batch 84610, train_perplexity=88.42451, train_loss=4.482149

Batch 84620, train_perplexity=86.50916, train_loss=4.4602504

Batch 84630, train_perplexity=81.9266, train_loss=4.4058237

Batch 84640, train_perplexity=96.40883, train_loss=4.568598

Batch 84650, train_perplexity=85.08877, train_loss=4.443695

Batch 84660, train_perplexity=81.05943, train_loss=4.3951826

Batch 84670, train_perplexity=88.59497, train_loss=4.484075

Batch 84680, train_perplexity=95.283, train_loss=4.5568514

Batch 84690, train_perplexity=94.431564, train_loss=4.5478754

Batch 84700, train_perplexity=94.324905, train_loss=4.5467453

Batch 84710, train_perplexity=95.630295, train_loss=4.5604897

Batch 84720, train_perplexity=87.74985, train_loss=4.47449

Batch 84730, train_perplexity=91.45693, train_loss=4.515868

Batch 84740, train_perplexity=88.46727, train_loss=4.4826326

Batch 84750, train_perplexity=87.051575, train_loss=4.4665008

Batch 84760, train_perplexity=78.83735, train_loss=4.367387

Batch 84770, train_perplexity=103.16579, train_loss=4.6363373

Batch 84780, train_perplexity=93.31574, train_loss=4.535989

Batch 84790, train_perplexity=88.84453, train_loss=4.486888

Batch 84800, train_perplexity=96.101585, train_loss=4.565406

Batch 84810, train_perplexity=100.78427, train_loss=4.6129823

Batch 84820, train_perplexity=100.11738, train_loss=4.6063433

Batch 84830, train_perplexity=90.517715, train_loss=4.5055456

Batch 84840, train_perplexity=90.114105, train_loss=4.5010767

Batch 84850, train_perplexity=81.136696, train_loss=4.3961353

Batch 84860, train_perplexity=87.69012, train_loss=4.4738092

Batch 84870, train_perplexity=100.44416, train_loss=4.609602

Batch 84880, train_perplexity=89.99018, train_loss=4.4997005

Batch 84890, train_perplexity=88.247086, train_loss=4.4801407

Batch 84900, train_perplexity=91.754135, train_loss=4.5191126

Batch 84910, train_perplexity=89.25675, train_loss=4.491517

Batch 84920, train_perplexity=92.93346, train_loss=4.5318837

Batch 84930, train_perplexity=95.560005, train_loss=4.5597544

Batch 84940, train_perplexity=98.48571, train_loss=4.5899115

Batch 84950, train_perplexity=92.37567, train_loss=4.5258636

Batch 84960, train_perplexity=89.90984, train_loss=4.4988074

Batch 84970, train_perplexity=86.89825, train_loss=4.464738

Batch 84980, train_perplexity=98.86985, train_loss=4.5938044

Batch 84990, train_perplexity=86.47407, train_loss=4.4598446

Batch 85000, train_perplexity=93.174614, train_loss=4.5344753

Batch 85010, train_perplexity=88.68977, train_loss=4.4851446

Batch 85020, train_perplexity=88.695656, train_loss=4.485211

Batch 85030, train_perplexity=93.677124, train_loss=4.539854

Batch 85040, train_perplexity=98.93927, train_loss=4.5945063

Batch 85050, train_perplexity=85.996376, train_loss=4.454305

Batch 85060, train_perplexity=86.8328, train_loss=4.4639845

Batch 85070, train_perplexity=95.52055, train_loss=4.5593414

Batch 85080, train_perplexity=84.403496, train_loss=4.435609

Batch 85090, train_perplexity=94.064896, train_loss=4.543985

Batch 85100, train_perplexity=89.92318, train_loss=4.4989557

Batch 85110, train_perplexity=84.985695, train_loss=4.442483

Batch 85120, train_perplexity=90.55083, train_loss=4.5059114

Batch 85130, train_perplexity=93.8064, train_loss=4.541233

Batch 85140, train_perplexity=87.0368, train_loss=4.466331

Batch 85150, train_perplexity=92.31614, train_loss=4.525219

Batch 85160, train_perplexity=92.75425, train_loss=4.5299535

Batch 85170, train_perplexity=88.793106, train_loss=4.486309

Batch 85180, train_perplexity=88.882195, train_loss=4.487312

Batch 85190, train_perplexity=88.99972, train_loss=4.488633

Batch 85200, train_perplexity=94.487144, train_loss=4.548464

Batch 85210, train_perplexity=93.24857, train_loss=4.535269

Batch 85220, train_perplexity=94.87044, train_loss=4.552512

Batch 85230, train_perplexity=96.50165, train_loss=4.56956

Batch 85240, train_perplexity=91.786606, train_loss=4.5194664

Batch 85250, train_perplexity=85.16135, train_loss=4.4445477

Batch 85260, train_perplexity=95.9946, train_loss=4.564292

Batch 85270, train_perplexity=88.119804, train_loss=4.4786973

Batch 85280, train_perplexity=84.92647, train_loss=4.441786

Batch 85290, train_perplexity=92.49486, train_loss=4.527153

Batch 85300, train_perplexity=96.76974, train_loss=4.5723343

Batch 85310, train_perplexity=101.38427, train_loss=4.618918

Batch 85320, train_perplexity=93.52274, train_loss=4.5382047

Batch 85330, train_perplexity=93.97796, train_loss=4.5430603

Batch 85340, train_perplexity=97.82633, train_loss=4.583194

Batch 85350, train_perplexity=89.6331, train_loss=4.4957247

Batch 85360, train_perplexity=97.31734, train_loss=4.577977

Batch 85370, train_perplexity=94.44908, train_loss=4.548061

Batch 85380, train_perplexity=79.575035, train_loss=4.3767004

Batch 85390, train_perplexity=95.93493, train_loss=4.56367

Batch 85400, train_perplexity=89.18375, train_loss=4.490699

Batch 85410, train_perplexity=88.43943, train_loss=4.482318

Batch 85420, train_perplexity=91.345276, train_loss=4.5146465

Batch 85430, train_perplexity=95.80903, train_loss=4.562357

Batch 85440, train_perplexity=91.906654, train_loss=4.5207734

Batch 85450, train_perplexity=96.81446, train_loss=4.5727963

Batch 85460, train_perplexity=96.55568, train_loss=4.57012

Batch 85470, train_perplexity=87.38808, train_loss=4.470359

Batch 85480, train_perplexity=96.15944, train_loss=4.5660076

Batch 85490, train_perplexity=93.43801, train_loss=4.537298

Batch 85500, train_perplexity=86.90811, train_loss=4.4648514

Batch 85510, train_perplexity=91.23148, train_loss=4.5134

Batch 85520, train_perplexity=85.410805, train_loss=4.4474726

Batch 85530, train_perplexity=90.50382, train_loss=4.505392

Batch 85540, train_perplexity=99.448494, train_loss=4.59964

Batch 85550, train_perplexity=85.09859, train_loss=4.4438105

Batch 85560, train_perplexity=95.34658, train_loss=4.5575185

Batch 85570, train_perplexity=89.448906, train_loss=4.4936676

Batch 85580, train_perplexity=91.38069, train_loss=4.515034

Batch 85590, train_perplexity=93.23755, train_loss=4.5351505

Batch 85600, train_perplexity=87.52786, train_loss=4.471957

Batch 85610, train_perplexity=87.84497, train_loss=4.4755735

Batch 85620, train_perplexity=93.39035, train_loss=4.536788

Batch 85630, train_perplexity=93.18208, train_loss=4.5345554

Batch 85640, train_perplexity=97.63196, train_loss=4.581205

Batch 85650, train_perplexity=94.287674, train_loss=4.5463505

Batch 85660, train_perplexity=94.08602, train_loss=4.5442095

Batch 85670, train_perplexity=91.92125, train_loss=4.520932

Batch 85680, train_perplexity=89.78342, train_loss=4.4974003

Batch 85690, train_perplexity=93.39062, train_loss=4.536791

Batch 85700, train_perplexity=92.93527, train_loss=4.5319033

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 85710, train_perplexity=89.109825, train_loss=4.4898696

Batch 85720, train_perplexity=88.36545, train_loss=4.481481

Batch 85730, train_perplexity=87.45611, train_loss=4.471137

Batch 85740, train_perplexity=78.51224, train_loss=4.3632545

Batch 85750, train_perplexity=89.21437, train_loss=4.491042

Batch 85760, train_perplexity=81.838745, train_loss=4.404751

Batch 85770, train_perplexity=87.73219, train_loss=4.474289

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 85780, train_perplexity=88.51271, train_loss=4.483146

Batch 85790, train_perplexity=97.03311, train_loss=4.5750523

Batch 85800, train_perplexity=82.65172, train_loss=4.4146357

Batch 85810, train_perplexity=87.539215, train_loss=4.472087

Batch 85820, train_perplexity=81.34434, train_loss=4.398691

Batch 85830, train_perplexity=89.204544, train_loss=4.490932

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 85840, train_perplexity=91.75562, train_loss=4.519129

Batch 85850, train_perplexity=79.469086, train_loss=4.375368

Batch 85860, train_perplexity=85.22277, train_loss=4.4452686

Batch 85870, train_perplexity=93.705765, train_loss=4.5401597

Batch 85880, train_perplexity=96.94061, train_loss=4.5740986

Batch 85890, train_perplexity=86.81036, train_loss=4.463726

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 85900, train_perplexity=91.10255, train_loss=4.511986

Batch 85910, train_perplexity=89.31371, train_loss=4.492155

Batch 85920, train_perplexity=82.98969, train_loss=4.4187164

Batch 85930, train_perplexity=88.256935, train_loss=4.4802523

Batch 85940, train_perplexity=86.45568, train_loss=4.459632

Batch 85950, train_perplexity=92.98838, train_loss=4.5324745

Batch 85960, train_perplexity=78.82137, train_loss=4.367184

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 85970, train_perplexity=79.18834, train_loss=4.371829

Batch 85980, train_perplexity=91.8327, train_loss=4.5199685

Batch 85990, train_perplexity=82.455414, train_loss=4.4122577

Batch 86000, train_perplexity=83.55815, train_loss=4.425543

Batch 86010, train_perplexity=91.221436, train_loss=4.51329

Batch 86020, train_perplexity=87.53943, train_loss=4.4720893

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 86030, train_perplexity=86.573456, train_loss=4.4609933

Batch 86040, train_perplexity=91.50754, train_loss=4.5164213

Batch 86050, train_perplexity=88.603584, train_loss=4.4841723

Batch 86060, train_perplexity=87.47058, train_loss=4.4713025

Batch 86070, train_perplexity=88.96747, train_loss=4.4882708

Batch 86080, train_perplexity=79.2682, train_loss=4.372837

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 86090, train_perplexity=97.472595, train_loss=4.5795712

Batch 86100, train_perplexity=92.03185, train_loss=4.522135

Batch 86110, train_perplexity=77.66392, train_loss=4.352391

Batch 86120, train_perplexity=89.03375, train_loss=4.4890156

Batch 86130, train_perplexity=89.53669, train_loss=4.4946485
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86140, train_perplexity=83.52628, train_loss=4.4251614

Batch 86150, train_perplexity=89.41616, train_loss=4.4933014

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 86160, train_perplexity=88.17381, train_loss=4.47931

Batch 86170, train_perplexity=104.6045, train_loss=4.6501865

Batch 86180, train_perplexity=102.41528, train_loss=4.629036

Batch 86190, train_perplexity=82.95528, train_loss=4.4183016

Batch 86200, train_perplexity=91.15469, train_loss=4.512558

Batch 86210, train_perplexity=80.24689, train_loss=4.385108

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 86220, train_perplexity=87.14149, train_loss=4.467533

Batch 86230, train_perplexity=89.58593, train_loss=4.4951982

Batch 86240, train_perplexity=82.05636, train_loss=4.4074063

Batch 86250, train_perplexity=87.14831, train_loss=4.4676113

Batch 86260, train_perplexity=95.21478, train_loss=4.556135

Batch 86270, train_perplexity=100.2022, train_loss=4.60719

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 86280, train_perplexity=94.4168, train_loss=4.547719

Batch 86290, train_perplexity=83.94746, train_loss=4.430191

Batch 86300, train_perplexity=88.66766, train_loss=4.484895

Batch 86310, train_perplexity=88.9645, train_loss=4.4882374

Batch 86320, train_perplexity=99.230034, train_loss=4.5974407

Batch 86330, train_perplexity=84.226036, train_loss=4.433504

Batch 86340, train_perplexity=93.45557, train_loss=4.537486

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 86350, train_perplexity=79.01514, train_loss=4.3696394

Batch 86360, train_perplexity=90.38669, train_loss=4.504097

Batch 86370, train_perplexity=84.58585, train_loss=4.437767

Batch 86380, train_perplexity=85.99748, train_loss=4.454318

Batch 86390, train_perplexity=93.00656, train_loss=4.53267

Batch 86400, train_perplexity=91.11393, train_loss=4.5121107

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 86410, train_perplexity=95.36363, train_loss=4.5576973

Batch 86420, train_perplexity=82.60877, train_loss=4.414116

Batch 86430, train_perplexity=84.952515, train_loss=4.4420924

Batch 86440, train_perplexity=89.864456, train_loss=4.4983025

Batch 86450, train_perplexity=91.94501, train_loss=4.5211906

Batch 86460, train_perplexity=89.205055, train_loss=4.4909377

Batch 86470, train_perplexity=88.26846, train_loss=4.480383

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 86480, train_perplexity=86.82601, train_loss=4.4639063

Batch 86490, train_perplexity=88.18496, train_loss=4.4794364

Batch 86500, train_perplexity=87.87057, train_loss=4.475865

Batch 86510, train_perplexity=90.41221, train_loss=4.5043793

Batch 86520, train_perplexity=85.43606, train_loss=4.447768

Batch 86530, train_perplexity=98.0829, train_loss=4.585813

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 86540, train_perplexity=98.481346, train_loss=4.589867

Batch 86550, train_perplexity=86.0836, train_loss=4.455319

Batch 86560, train_perplexity=82.50713, train_loss=4.4128847

Batch 86570, train_perplexity=87.1966, train_loss=4.4681654

Batch 86580, train_perplexity=91.122925, train_loss=4.5122094

Batch 86590, train_perplexity=90.93025, train_loss=4.5100927

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 86600, train_perplexity=88.29027, train_loss=4.48063

Batch 86610, train_perplexity=85.60276, train_loss=4.4497175

Batch 86620, train_perplexity=91.65623, train_loss=4.518045

Batch 86630, train_perplexity=81.3576, train_loss=4.3988543

Batch 86640, train_perplexity=93.1971, train_loss=4.5347166

Batch 86650, train_perplexity=92.11334, train_loss=4.52302

Batch 86660, train_perplexity=86.06058, train_loss=4.4550514

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 86670, train_perplexity=85.69751, train_loss=4.450824

Batch 86680, train_perplexity=103.44213, train_loss=4.6390123

Batch 86690, train_perplexity=82.09006, train_loss=4.407817

Batch 86700, train_perplexity=90.132065, train_loss=4.501276

Batch 86710, train_perplexity=91.53804, train_loss=4.5167546

Batch 86720, train_perplexity=100.61693, train_loss=4.6113205

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 86730, train_perplexity=84.70625, train_loss=4.4391894

Batch 86740, train_perplexity=84.58263, train_loss=4.437729

Batch 86750, train_perplexity=80.153885, train_loss=4.3839483

Batch 86760, train_perplexity=91.79676, train_loss=4.519577

Batch 86770, train_perplexity=84.737885, train_loss=4.439563

Batch 86780, train_perplexity=92.90975, train_loss=4.5316286

Batch 86790, train_perplexity=94.57315, train_loss=4.5493736

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 86800, train_perplexity=95.04767, train_loss=4.5543785

Batch 86810, train_perplexity=92.161316, train_loss=4.5235405

Batch 86820, train_perplexity=97.521545, train_loss=4.5800734

Batch 86830, train_perplexity=92.3925, train_loss=4.526046

Batch 86840, train_perplexity=90.23536, train_loss=4.5024214

Batch 86850, train_perplexity=92.76614, train_loss=4.5300817

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 86860, train_perplexity=86.73324, train_loss=4.462837

Batch 86870, train_perplexity=88.727425, train_loss=4.485569

Batch 86880, train_perplexity=82.67238, train_loss=4.4148855

Batch 86890, train_perplexity=92.87033, train_loss=4.531204

Batch 86900, train_perplexity=89.47024, train_loss=4.493906

Batch 86910, train_perplexity=93.75104, train_loss=4.5406427

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 86920, train_perplexity=73.11128, train_loss=4.2919827

Batch 86930, train_perplexity=81.61443, train_loss=4.402006

Batch 86940, train_perplexity=91.87471, train_loss=4.520426

Batch 86950, train_perplexity=90.11041, train_loss=4.5010357

Batch 86960, train_perplexity=92.609245, train_loss=4.528389

Batch 86970, train_perplexity=92.74973, train_loss=4.529905

Batch 86980, train_perplexity=95.7209, train_loss=4.5614367

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 86990, train_perplexity=91.87217, train_loss=4.520398

Batch 87000, train_perplexity=86.8049, train_loss=4.463663

Batch 87010, train_perplexity=83.172165, train_loss=4.4209127

Batch 87020, train_perplexity=96.89995, train_loss=4.573679

Batch 87030, train_perplexity=92.29061, train_loss=4.5249424

Batch 87040, train_perplexity=92.81729, train_loss=4.530633

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 87050, train_perplexity=93.528854, train_loss=4.53827

Batch 87060, train_perplexity=85.741325, train_loss=4.451335

Batch 87070, train_perplexity=88.46398, train_loss=4.4825954

Batch 87080, train_perplexity=88.86851, train_loss=4.487158

Batch 87090, train_perplexity=79.86203, train_loss=4.3803005

Batch 87100, train_perplexity=88.469376, train_loss=4.4826565

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 87110, train_perplexity=87.715546, train_loss=4.474099

Batch 87120, train_perplexity=88.21402, train_loss=4.479766

Batch 87130, train_perplexity=87.336174, train_loss=4.4697647

Batch 87140, train_perplexity=98.69378, train_loss=4.592022

Batch 87150, train_perplexity=91.418564, train_loss=4.5154486

Batch 87160, train_perplexity=89.6868, train_loss=4.4963236

Batch 87170, train_perplexity=90.71242, train_loss=4.5076942

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 87180, train_perplexity=92.26759, train_loss=4.524693

Batch 87190, train_perplexity=93.71613, train_loss=4.5402703

Batch 87200, train_perplexity=88.73487, train_loss=4.485653

Batch 87210, train_perplexity=92.71679, train_loss=4.5295496

Batch 87220, train_perplexity=86.56186, train_loss=4.4608593

Batch 87230, train_perplexity=81.42878, train_loss=4.399729

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 87240, train_perplexity=81.723785, train_loss=4.403345

Batch 87250, train_perplexity=91.99816, train_loss=4.5217686

Batch 87260, train_perplexity=95.10769, train_loss=4.55501

Batch 87270, train_perplexity=86.451065, train_loss=4.4595785

Batch 87280, train_perplexity=96.10053, train_loss=4.565395

Batch 87290, train_perplexity=82.1002, train_loss=4.4079404

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 87300, train_perplexity=90.23506, train_loss=4.502418

Batch 87310, train_perplexity=86.712814, train_loss=4.4626017

Batch 87320, train_perplexity=101.29579, train_loss=4.618045

Batch 87330, train_perplexity=93.07031, train_loss=4.533355

Batch 87340, train_perplexity=87.32842, train_loss=4.469676

Batch 87350, train_perplexity=95.67768, train_loss=4.560985

Batch 87360, train_perplexity=91.08813, train_loss=4.5118275

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 87370, train_perplexity=81.60517, train_loss=4.4018927

Batch 87380, train_perplexity=94.14535, train_loss=4.54484

Batch 87390, train_perplexity=97.44062, train_loss=4.579243

Batch 87400, train_perplexity=98.153404, train_loss=4.5865316

Batch 87410, train_perplexity=92.31508, train_loss=4.5252075

Batch 87420, train_perplexity=90.80312, train_loss=4.5086937

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6148 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 87430, train_perplexity=98.31023, train_loss=4.588128

Batch 87440, train_perplexity=101.39137, train_loss=4.618988

Batch 87450, train_perplexity=95.017395, train_loss=4.55406

Batch 87460, train_perplexity=96.692894, train_loss=4.57154

Batch 87470, train_perplexity=91.45414, train_loss=4.5158377

Batch 87480, train_perplexity=91.15091, train_loss=4.5125165

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 87490, train_perplexity=91.73515, train_loss=4.5189056

Batch 87500, train_perplexity=84.65412, train_loss=4.438574

Batch 87510, train_perplexity=94.42112, train_loss=4.547765

Batch 87520, train_perplexity=92.870995, train_loss=4.5312114

Batch 87530, train_perplexity=86.81376, train_loss=4.463765

Batch 87540, train_perplexity=87.826164, train_loss=4.4753594

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 87550, train_perplexity=84.07789, train_loss=4.4317436

Batch 87560, train_perplexity=88.82123, train_loss=4.4866257

Batch 87570, train_perplexity=82.55333, train_loss=4.4134445

Batch 87580, train_perplexity=92.24098, train_loss=4.5244045

Batch 87590, train_perplexity=92.61887, train_loss=4.528493

Batch 87600, train_perplexity=91.63958, train_loss=4.5178633

Batch 87610, train_perplexity=83.19779, train_loss=4.421221

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 87620, train_perplexity=77.57487, train_loss=4.3512435

Batch 87630, train_perplexity=86.0923, train_loss=4.45542

Batch 87640, train_perplexity=98.6584, train_loss=4.5916634

Batch 87650, train_perplexity=85.209564, train_loss=4.4451137

Batch 87660, train_perplexity=90.86289, train_loss=4.5093517

Batch 87670, train_perplexity=99.5939, train_loss=4.601101

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 87680, train_perplexity=84.97086, train_loss=4.4423084

Batch 87690, train_perplexity=104.08912, train_loss=4.6452475

Batch 87700, train_perplexity=97.487885, train_loss=4.579728

Batch 87710, train_perplexity=84.11871, train_loss=4.432229

Batch 87720, train_perplexity=76.405876, train_loss=4.3360596

Batch 87730, train_perplexity=86.8473, train_loss=4.4641514

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 87740, train_perplexity=98.797935, train_loss=4.5930767

Batch 87750, train_perplexity=91.674805, train_loss=4.5182476

Batch 87760, train_perplexity=88.21528, train_loss=4.47978

Batch 87770, train_perplexity=92.14848, train_loss=4.5234013

Batch 87780, train_perplexity=89.1877, train_loss=4.490743

Batch 87790, train_perplexity=87.67954, train_loss=4.4736886

Batch 87800, train_perplexity=89.99919, train_loss=4.4998007

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 87810, train_perplexity=87.18534, train_loss=4.468036

Batch 87820, train_perplexity=87.174446, train_loss=4.4679112

Batch 87830, train_perplexity=85.886185, train_loss=4.453023

Batch 87840, train_perplexity=103.92908, train_loss=4.6437087

Batch 87850, train_perplexity=94.51401, train_loss=4.548748

Batch 87860, train_perplexity=92.37483, train_loss=4.5258546

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 87870, train_perplexity=98.44904, train_loss=4.589539

Batch 87880, train_perplexity=91.67983, train_loss=4.5183024

Batch 87890, train_perplexity=92.54793, train_loss=4.5277267

Batch 87900, train_perplexity=90.79174, train_loss=4.5085683

Batch 87910, train_perplexity=99.53408, train_loss=4.6005

Batch 87920, train_perplexity=95.45945, train_loss=4.5587015

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 87930, train_perplexity=79.77928, train_loss=4.379264

Batch 87940, train_perplexity=102.43609, train_loss=4.629239

Batch 87950, train_perplexity=81.95719, train_loss=4.406197

Batch 87960, train_perplexity=86.90189, train_loss=4.46478

Batch 87970, train_perplexity=91.78862, train_loss=4.5194883

Batch 87980, train_perplexity=98.12065, train_loss=4.586198

Batch 87990, train_perplexity=86.35367, train_loss=4.4584513

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 88000, train_perplexity=89.789665, train_loss=4.49747

Batch 88010, train_perplexity=84.82019, train_loss=4.4405336

Batch 88020, train_perplexity=93.89317, train_loss=4.5421576

Batch 88030, train_perplexity=84.38467, train_loss=4.4353857

Batch 88040, train_perplexity=99.6129, train_loss=4.6012917

Batch 88050, train_perplexity=87.54256, train_loss=4.472125

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 88060, train_perplexity=91.20222, train_loss=4.513079

Batch 88070, train_perplexity=83.09835, train_loss=4.420025

Batch 88080, train_perplexity=89.203224, train_loss=4.490917

Batch 88090, train_perplexity=92.70521, train_loss=4.5294247

Batch 88100, train_perplexity=87.312065, train_loss=4.4694886

Batch 88110, train_perplexity=96.771675, train_loss=4.5723543

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 88120, train_perplexity=92.23228, train_loss=4.52431

Batch 88130, train_perplexity=83.14195, train_loss=4.4205494

Batch 88140, train_perplexity=79.239784, train_loss=4.3724785

Batch 88150, train_perplexity=94.30314, train_loss=4.5465145

Batch 88160, train_perplexity=88.08338, train_loss=4.478284

Batch 88170, train_perplexity=98.22345, train_loss=4.587245

Batch 88180, train_perplexity=89.45437, train_loss=4.4937286

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 88190, train_perplexity=94.42418, train_loss=4.547797

Batch 88200, train_perplexity=88.59036, train_loss=4.484023

Batch 88210, train_perplexity=97.25119, train_loss=4.577297

Batch 88220, train_perplexity=82.24333, train_loss=4.4096823

Batch 88230, train_perplexity=85.68542, train_loss=4.4506826

Batch 88240, train_perplexity=87.90627, train_loss=4.476271

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 88250, train_perplexity=86.591415, train_loss=4.4612007

Batch 88260, train_perplexity=82.35572, train_loss=4.411048

Batch 88270, train_perplexity=96.96577, train_loss=4.574358

Batch 88280, train_perplexity=91.28353, train_loss=4.5139704

Batch 88290, train_perplexity=89.62105, train_loss=4.49559

Batch 88300, train_perplexity=87.02746, train_loss=4.4662237

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 88310, train_perplexity=98.36303, train_loss=4.588665

Batch 88320, train_perplexity=88.306984, train_loss=4.480819

Batch 88330, train_perplexity=79.54681, train_loss=4.3763456

Batch 88340, train_perplexity=89.26126, train_loss=4.4915676

Batch 88350, train_perplexity=92.16773, train_loss=4.52361

Batch 88360, train_perplexity=94.283356, train_loss=4.5463047

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 88370, train_perplexity=84.63632, train_loss=4.4383636

Batch 88380, train_perplexity=89.7728, train_loss=4.497282

Batch 88390, train_perplexity=80.577484, train_loss=4.3892193

Batch 88400, train_perplexity=92.23685, train_loss=4.5243597

Batch 88410, train_perplexity=90.83196, train_loss=4.5090113

Batch 88420, train_perplexity=92.376816, train_loss=4.525876

Batch 88430, train_perplexity=102.82333, train_loss=4.6330123

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 88440, train_perplexity=92.34207, train_loss=4.5255

Batch 88450, train_perplexity=79.41461, train_loss=4.3746824

Batch 88460, train_perplexity=92.10895, train_loss=4.522972

Batch 88470, train_perplexity=89.59896, train_loss=4.4953437

Batch 88480, train_perplexity=88.00798, train_loss=4.4774275

Batch 88490, train_perplexity=88.404434, train_loss=4.481922

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 88500, train_perplexity=87.3201, train_loss=4.4695807

Batch 88510, train_perplexity=98.24978, train_loss=4.587513

Batch 88520, train_perplexity=92.032555, train_loss=4.5221424

Batch 88530, train_perplexity=91.063286, train_loss=4.5115547

Batch 88540, train_perplexity=83.3835, train_loss=4.4234505

Batch 88550, train_perplexity=97.074486, train_loss=4.5754786

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 88560, train_perplexity=99.34134, train_loss=4.598562

Batch 88570, train_perplexity=91.642204, train_loss=4.517892

Batch 88580, train_perplexity=83.55716, train_loss=4.425531

Batch 88590, train_perplexity=90.39548, train_loss=4.5041943

Batch 88600, train_perplexity=92.67378, train_loss=4.5290856

Batch 88610, train_perplexity=73.36488, train_loss=4.2954454

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 88620, train_perplexity=95.29281, train_loss=4.5569544

Batch 88630, train_perplexity=89.70489, train_loss=4.4965253

Batch 88640, train_perplexity=92.682045, train_loss=4.529175

Batch 88650, train_perplexity=81.35186, train_loss=4.3987837

Batch 88660, train_perplexity=81.497536, train_loss=4.400573

Batch 88670, train_perplexity=95.26887, train_loss=4.556703

Batch 88680, train_perplexity=90.138084, train_loss=4.501343

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 88690, train_perplexity=94.244705, train_loss=4.5458946
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88700, train_perplexity=90.18916, train_loss=4.5019093

Batch 88710, train_perplexity=96.976906, train_loss=4.574473

Batch 88720, train_perplexity=93.20572, train_loss=4.534809

Batch 88730, train_perplexity=94.81061, train_loss=4.5518813

Batch 88740, train_perplexity=97.551125, train_loss=4.5803766

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 88750, train_perplexity=93.17008, train_loss=4.5344267

Batch 88760, train_perplexity=98.036285, train_loss=4.5853376

Batch 88770, train_perplexity=97.12894, train_loss=4.5760393

Batch 88780, train_perplexity=88.521706, train_loss=4.4832478

Batch 88790, train_perplexity=93.714836, train_loss=4.5402565

Batch 88800, train_perplexity=99.82354, train_loss=4.603404

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 88810, train_perplexity=88.81081, train_loss=4.4865084

Batch 88820, train_perplexity=85.68893, train_loss=4.4507236

Batch 88830, train_perplexity=83.92728, train_loss=4.4299507

Batch 88840, train_perplexity=80.92982, train_loss=4.3935823

Batch 88850, train_perplexity=82.90347, train_loss=4.417677

Batch 88860, train_perplexity=84.37267, train_loss=4.4352436

Batch 88870, train_perplexity=87.80941, train_loss=4.4751687

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 88880, train_perplexity=88.910805, train_loss=4.4876337

Batch 88890, train_perplexity=79.972466, train_loss=4.3816824

Batch 88900, train_perplexity=99.36375, train_loss=4.5987873

Batch 88910, train_perplexity=85.977684, train_loss=4.4540877

Batch 88920, train_perplexity=79.80463, train_loss=4.3795815

Batch 88930, train_perplexity=92.753365, train_loss=4.529944

Batch 88940, train_perplexity=86.13891, train_loss=4.455961

Batch 88950, train_perplexity=82.72404, train_loss=4.41551

Batch 88960, train_perplexity=79.57162, train_loss=4.3766575

Batch 88970, train_perplexity=88.67108, train_loss=4.484934

Batch 88980, train_perplexity=93.44514, train_loss=4.5373745

Batch 88990, train_perplexity=91.995705, train_loss=4.521742

Batch 89000, train_perplexity=84.49354, train_loss=4.436675

Batch 89010, train_perplexity=83.75074, train_loss=4.427845

Batch 89020, train_perplexity=84.62301, train_loss=4.438206

Batch 89030, train_perplexity=80.196625, train_loss=4.3844814

Batch 89040, train_perplexity=80.25167, train_loss=4.3851676

Batch 89050, train_perplexity=85.846016, train_loss=4.452555

Batch 89060, train_perplexity=84.06185, train_loss=4.431553

Batch 89070, train_perplexity=89.606865, train_loss=4.495432

Batch 89080, train_perplexity=83.51035, train_loss=4.4249706

Batch 89090, train_perplexity=98.10152, train_loss=4.586003

Batch 89100, train_perplexity=84.340454, train_loss=4.4348617

Batch 89110, train_perplexity=79.21923, train_loss=4.372219

Batch 89120, train_perplexity=81.78183, train_loss=4.404055

Batch 89130, train_perplexity=90.56823, train_loss=4.5061035

Batch 89140, train_perplexity=87.86076, train_loss=4.4757533

Batch 89150, train_perplexity=90.81793, train_loss=4.508857

Batch 89160, train_perplexity=95.12638, train_loss=4.5552063

Batch 89170, train_perplexity=99.05125, train_loss=4.5956373

Batch 89180, train_perplexity=76.32685, train_loss=4.335025

Batch 89190, train_perplexity=84.51542, train_loss=4.436934

Batch 89200, train_perplexity=85.83116, train_loss=4.452382

Batch 89210, train_perplexity=88.64411, train_loss=4.4846296

Batch 89220, train_perplexity=90.008934, train_loss=4.499909

Batch 89230, train_perplexity=76.02044, train_loss=4.331002

Batch 89240, train_perplexity=82.14911, train_loss=4.408536

Batch 89250, train_perplexity=89.461494, train_loss=4.4938083

Batch 89260, train_perplexity=79.44973, train_loss=4.3751245

Batch 89270, train_perplexity=98.00712, train_loss=4.58504

Batch 89280, train_perplexity=87.26536, train_loss=4.4689536

Batch 89290, train_perplexity=90.17914, train_loss=4.501798

Batch 89300, train_perplexity=83.77734, train_loss=4.4281626

Batch 89310, train_perplexity=85.602104, train_loss=4.44971

Batch 89320, train_perplexity=95.19921, train_loss=4.5559716

Batch 89330, train_perplexity=91.89977, train_loss=4.5206985

Batch 89340, train_perplexity=91.51906, train_loss=4.516547

Batch 89350, train_perplexity=75.238495, train_loss=4.320663

Batch 89360, train_perplexity=81.740074, train_loss=4.4035444

Batch 89370, train_perplexity=83.49009, train_loss=4.424728

Batch 89380, train_perplexity=99.23983, train_loss=4.5975394

Batch 89390, train_perplexity=97.658264, train_loss=4.5814743

Batch 89400, train_perplexity=86.01766, train_loss=4.4545527

Batch 89410, train_perplexity=89.75786, train_loss=4.4971156

Batch 89420, train_perplexity=83.05771, train_loss=4.4195356

Batch 89430, train_perplexity=81.9781, train_loss=4.406452

Batch 89440, train_perplexity=82.05538, train_loss=4.4073944

Batch 89450, train_perplexity=81.95332, train_loss=4.40615

Batch 89460, train_perplexity=82.449394, train_loss=4.4121847

Batch 89470, train_perplexity=91.28701, train_loss=4.5140085

Batch 89480, train_perplexity=92.82778, train_loss=4.530746

Batch 89490, train_perplexity=82.09656, train_loss=4.407896

Batch 89500, train_perplexity=96.726555, train_loss=4.571888

Batch 89510, train_perplexity=95.07259, train_loss=4.554641

Batch 89520, train_perplexity=84.26179, train_loss=4.4339285

Batch 89530, train_perplexity=94.60256, train_loss=4.5496845

Batch 89540, train_perplexity=99.50086, train_loss=4.6001663

Batch 89550, train_perplexity=84.23174, train_loss=4.433572

Batch 89560, train_perplexity=77.33458, train_loss=4.348141

Batch 89570, train_perplexity=85.61603, train_loss=4.4498725

Batch 89580, train_perplexity=89.87499, train_loss=4.49842

Batch 89590, train_perplexity=90.48311, train_loss=4.505163

Batch 89600, train_perplexity=92.47527, train_loss=4.5269413

Batch 89610, train_perplexity=84.83904, train_loss=4.440756

Batch 89620, train_perplexity=101.07891, train_loss=4.6159015

Batch 89630, train_perplexity=82.475975, train_loss=4.412507

Batch 89640, train_perplexity=87.07856, train_loss=4.4668107

Batch 89650, train_perplexity=86.85757, train_loss=4.4642696

Batch 89660, train_perplexity=82.43509, train_loss=4.412011

Batch 89670, train_perplexity=88.92073, train_loss=4.4877453

Batch 89680, train_perplexity=86.17276, train_loss=4.456354

Batch 89690, train_perplexity=87.4406, train_loss=4.4709597

Batch 89700, train_perplexity=87.99371, train_loss=4.4772654

Batch 89710, train_perplexity=76.48118, train_loss=4.3370447

Batch 89720, train_perplexity=87.94606, train_loss=4.4767237

Batch 89730, train_perplexity=88.77609, train_loss=4.4861174

Batch 89740, train_perplexity=93.291534, train_loss=4.5357294

Batch 89750, train_perplexity=91.3333, train_loss=4.5145154

Batch 89760, train_perplexity=91.601006, train_loss=4.517442

Batch 89770, train_perplexity=85.66025, train_loss=4.450389

Batch 89780, train_perplexity=92.525734, train_loss=4.527487

Batch 89790, train_perplexity=93.640816, train_loss=4.5394664

Batch 89800, train_perplexity=86.48521, train_loss=4.4599733

Batch 89810, train_perplexity=87.45611, train_loss=4.471137

Batch 89820, train_perplexity=88.29692, train_loss=4.4807053
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 89830, train_perplexity=80.61023, train_loss=4.3896255

Batch 89840, train_perplexity=99.165794, train_loss=4.596793

Batch 89850, train_perplexity=86.48533, train_loss=4.459975

Batch 89860, train_perplexity=77.67758, train_loss=4.3525667

Batch 89870, train_perplexity=86.18533, train_loss=4.4565

Batch 89880, train_perplexity=86.64478, train_loss=4.461817

Batch 89890, train_perplexity=85.61346, train_loss=4.4498425

Batch 89900, train_perplexity=85.45998, train_loss=4.448048

Batch 89910, train_perplexity=84.83864, train_loss=4.440751

Batch 89920, train_perplexity=88.89292, train_loss=4.4874325

Batch 89930, train_perplexity=93.86931, train_loss=4.5419035

Batch 89940, train_perplexity=82.99523, train_loss=4.418783

Batch 89950, train_perplexity=87.0798, train_loss=4.466825

Batch 89960, train_perplexity=86.70967, train_loss=4.4625654

Batch 89970, train_perplexity=91.269775, train_loss=4.5138197

Batch 89980, train_perplexity=88.06448, train_loss=4.4780693

Batch 89990, train_perplexity=92.863686, train_loss=4.5311327

Batch 90000, train_perplexity=98.48557, train_loss=4.58991

Batch 90010, train_perplexity=86.415245, train_loss=4.459164

Batch 90020, train_perplexity=81.722466, train_loss=4.403329

Batch 90030, train_perplexity=97.57197, train_loss=4.5805902

Batch 90040, train_perplexity=84.09485, train_loss=4.4319453

Batch 90050, train_perplexity=87.16343, train_loss=4.467785

Batch 90060, train_perplexity=95.38055, train_loss=4.5578747

Batch 90070, train_perplexity=80.2203, train_loss=4.3847766

Batch 90080, train_perplexity=94.305664, train_loss=4.546541

Batch 90090, train_perplexity=96.60828, train_loss=4.5706644

Batch 90100, train_perplexity=90.49856, train_loss=4.505334

Batch 90110, train_perplexity=85.317505, train_loss=4.4463797

Batch 90120, train_perplexity=87.47838, train_loss=4.4713917

Batch 90130, train_perplexity=101.39133, train_loss=4.6189876

Batch 90140, train_perplexity=81.229095, train_loss=4.3972735

Batch 90150, train_perplexity=89.26969, train_loss=4.491662

Batch 90160, train_perplexity=90.62742, train_loss=4.506757

Batch 90170, train_perplexity=90.50179, train_loss=4.5053697

Batch 90180, train_perplexity=81.260826, train_loss=4.397664

Batch 90190, train_perplexity=88.45808, train_loss=4.4825287

Batch 90200, train_perplexity=94.28606, train_loss=4.5463333

Batch 90210, train_perplexity=84.36065, train_loss=4.435101

Batch 90220, train_perplexity=93.158264, train_loss=4.5343

Batch 90230, train_perplexity=98.70028, train_loss=4.5920877

Batch 90240, train_perplexity=81.11039, train_loss=4.395811

Batch 90250, train_perplexity=97.53922, train_loss=4.5802546

Batch 90260, train_perplexity=86.41756, train_loss=4.459191

Batch 90270, train_perplexity=92.997154, train_loss=4.532569

Batch 90280, train_perplexity=83.68611, train_loss=4.427073

Batch 90290, train_perplexity=85.75057, train_loss=4.4514427

Batch 90300, train_perplexity=88.763306, train_loss=4.4859734

Batch 90310, train_perplexity=82.46178, train_loss=4.412335

Batch 90320, train_perplexity=93.31725, train_loss=4.536005

Batch 90330, train_perplexity=101.17482, train_loss=4.61685

Batch 90340, train_perplexity=81.14873, train_loss=4.3962836

Batch 90350, train_perplexity=84.104675, train_loss=4.432062

Batch 90360, train_perplexity=85.990105, train_loss=4.454232

Batch 90370, train_perplexity=89.7502, train_loss=4.4970303

Batch 90380, train_perplexity=87.603775, train_loss=4.472824

Batch 90390, train_perplexity=93.23052, train_loss=4.535075

Batch 90400, train_perplexity=97.17062, train_loss=4.5764685

Batch 90410, train_perplexity=82.56073, train_loss=4.413534

Batch 90420, train_perplexity=89.62823, train_loss=4.4956703

Batch 90430, train_perplexity=86.01159, train_loss=4.454482

Batch 90440, train_perplexity=95.05106, train_loss=4.5544143

Batch 90450, train_perplexity=100.97885, train_loss=4.614911

Batch 90460, train_perplexity=86.60422, train_loss=4.4613485

Batch 90470, train_perplexity=97.505, train_loss=4.5799036

Batch 90480, train_perplexity=90.147545, train_loss=4.5014477

Batch 90490, train_perplexity=78.52108, train_loss=4.363367

Batch 90500, train_perplexity=95.29254, train_loss=4.5569515

Batch 90510, train_perplexity=88.97948, train_loss=4.4884057

Batch 90520, train_perplexity=90.901726, train_loss=4.509779

Batch 90530, train_perplexity=92.87219, train_loss=4.5312243

Batch 90540, train_perplexity=82.70226, train_loss=4.415247

Batch 90550, train_perplexity=79.627945, train_loss=4.377365

Batch 90560, train_perplexity=83.14409, train_loss=4.420575

Batch 90570, train_perplexity=86.79232, train_loss=4.463518

Batch 90580, train_perplexity=94.80577, train_loss=4.5518303

Batch 90590, train_perplexity=89.03893, train_loss=4.4890738

Batch 90600, train_perplexity=87.739555, train_loss=4.474373

Batch 90610, train_perplexity=93.906425, train_loss=4.542299

Batch 90620, train_perplexity=95.79341, train_loss=4.562194

Batch 90630, train_perplexity=86.658165, train_loss=4.4619713

Batch 90640, train_perplexity=92.6778, train_loss=4.529129

Batch 90650, train_perplexity=80.73906, train_loss=4.3912225

Batch 90660, train_perplexity=96.65933, train_loss=4.5711927

Batch 90670, train_perplexity=80.30791, train_loss=4.385868

Batch 90680, train_perplexity=82.193306, train_loss=4.409074

Batch 90690, train_perplexity=87.20484, train_loss=4.46826

Batch 90700, train_perplexity=88.68597, train_loss=4.4851017

Batch 90710, train_perplexity=91.923836, train_loss=4.5209603

Batch 90720, train_perplexity=97.55708, train_loss=4.5804377

Batch 90730, train_perplexity=83.93116, train_loss=4.429997

Batch 90740, train_perplexity=95.90072, train_loss=4.5633135

Batch 90750, train_perplexity=84.043335, train_loss=4.4313326

Batch 90760, train_perplexity=81.87747, train_loss=4.405224

Batch 90770, train_perplexity=87.27139, train_loss=4.4690228

Batch 90780, train_perplexity=95.38856, train_loss=4.5579586

Batch 90790, train_perplexity=95.93082, train_loss=4.5636272

Batch 90800, train_perplexity=86.27292, train_loss=4.4575157

Batch 90810, train_perplexity=79.88412, train_loss=4.380577

Batch 90820, train_perplexity=89.30741, train_loss=4.4920845

Batch 90830, train_perplexity=91.263855, train_loss=4.513755

Batch 90840, train_perplexity=89.71751, train_loss=4.496666

Batch 90850, train_perplexity=81.952034, train_loss=4.406134

Batch 90860, train_perplexity=93.93397, train_loss=4.542592

Batch 90870, train_perplexity=90.37867, train_loss=4.5040083

Batch 90880, train_perplexity=84.5311, train_loss=4.4371195

Batch 90890, train_perplexity=81.69464, train_loss=4.4029884

Batch 90900, train_perplexity=92.94086, train_loss=4.5319633

Batch 90910, train_perplexity=99.51557, train_loss=4.600314

Batch 90920, train_perplexity=86.53619, train_loss=4.4605627

Batch 90930, train_perplexity=67.4311, train_loss=4.2111063

Batch 90940, train_perplexity=93.452805, train_loss=4.5374565

Batch 90950, train_perplexity=79.47234, train_loss=4.375409

Batch 90960, train_perplexity=89.38529, train_loss=4.492956

Batch 90970, train_perplexity=93.902664, train_loss=4.5422587

Batch 90980, train_perplexity=77.990906, train_loss=4.356592

Batch 90990, train_perplexity=80.347015, train_loss=4.386355

Batch 91000, train_perplexity=84.94826, train_loss=4.4420424

Batch 91010, train_perplexity=89.9595, train_loss=4.4993596

Batch 91020, train_perplexity=95.62884, train_loss=4.5604744

Batch 91030, train_perplexity=92.8748, train_loss=4.5312524

Batch 91040, train_perplexity=77.75856, train_loss=4.3536086

Batch 91050, train_perplexity=92.617195, train_loss=4.528475

Batch 91060, train_perplexity=92.04888, train_loss=4.52232

Batch 91070, train_perplexity=90.649414, train_loss=4.5069995

Batch 91080, train_perplexity=84.96495, train_loss=4.442239

Batch 91090, train_perplexity=83.93853, train_loss=4.4300847

Batch 91100, train_perplexity=95.41212, train_loss=4.5582056

Batch 91110, train_perplexity=81.32517, train_loss=4.3984556

Batch 91120, train_perplexity=82.54782, train_loss=4.413378

Batch 91130, train_perplexity=102.69236, train_loss=4.6317377

Batch 91140, train_perplexity=85.63591, train_loss=4.4501047

Batch 91150, train_perplexity=89.30878, train_loss=4.4921
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 91160, train_perplexity=93.08238, train_loss=4.533485

Batch 91170, train_perplexity=95.022835, train_loss=4.554117

Batch 91180, train_perplexity=94.53275, train_loss=4.5489464

Batch 91190, train_perplexity=99.76906, train_loss=4.602858

Batch 91200, train_perplexity=90.02683, train_loss=4.500108

Batch 91210, train_perplexity=85.34977, train_loss=4.446758

Batch 91220, train_perplexity=76.50197, train_loss=4.3373165

Batch 91230, train_perplexity=94.98378, train_loss=4.553706

Batch 91240, train_perplexity=82.03034, train_loss=4.407089

Batch 91250, train_perplexity=86.35293, train_loss=4.4584427

Batch 91260, train_perplexity=95.899254, train_loss=4.563298

Batch 91270, train_perplexity=79.72004, train_loss=4.378521

Batch 91280, train_perplexity=84.49213, train_loss=4.4366584

Batch 91290, train_perplexity=88.413246, train_loss=4.482022

Batch 91300, train_perplexity=88.78803, train_loss=4.486252

Batch 91310, train_perplexity=86.63284, train_loss=4.461679

Batch 91320, train_perplexity=93.27357, train_loss=4.535537

Batch 91330, train_perplexity=80.21525, train_loss=4.3847136

Batch 91340, train_perplexity=85.253654, train_loss=4.445631

Batch 91350, train_perplexity=82.96124, train_loss=4.4183736

Batch 91360, train_perplexity=86.51329, train_loss=4.460298

Batch 91370, train_perplexity=90.46154, train_loss=4.504925

Batch 91380, train_perplexity=94.792755, train_loss=4.551693

Batch 91390, train_perplexity=95.55595, train_loss=4.559712

Batch 91400, train_perplexity=93.09135, train_loss=4.5335813

Batch 91410, train_perplexity=84.31066, train_loss=4.4345083

Batch 91420, train_perplexity=86.422295, train_loss=4.4592457

Batch 91430, train_perplexity=88.10699, train_loss=4.478552

Batch 91440, train_perplexity=89.07656, train_loss=4.489496

Batch 91450, train_perplexity=81.411545, train_loss=4.399517

Batch 91460, train_perplexity=86.87074, train_loss=4.4644213

Batch 91470, train_perplexity=87.7889, train_loss=4.474935

Batch 91480, train_perplexity=92.27362, train_loss=4.5247583

Batch 91490, train_perplexity=77.66314, train_loss=4.3523808

Batch 91500, train_perplexity=102.64174, train_loss=4.6312447

Batch 91510, train_perplexity=88.99144, train_loss=4.48854

Batch 91520, train_perplexity=81.519806, train_loss=4.400846

Batch 91530, train_perplexity=89.822815, train_loss=4.497839

Batch 91540, train_perplexity=77.93261, train_loss=4.3558445

Batch 91550, train_perplexity=98.93984, train_loss=4.594512

Batch 91560, train_perplexity=102.614136, train_loss=4.6309757

Batch 91570, train_perplexity=88.71955, train_loss=4.4854803

Batch 91580, train_perplexity=90.67657, train_loss=4.507299

Batch 91590, train_perplexity=85.37969, train_loss=4.4471083

Batch 91600, train_perplexity=82.77904, train_loss=4.416175

Batch 91610, train_perplexity=86.61525, train_loss=4.461476

Batch 91620, train_perplexity=84.2156, train_loss=4.43338

Batch 91630, train_perplexity=90.33119, train_loss=4.503483

Batch 91640, train_perplexity=89.9294, train_loss=4.499025

Batch 91650, train_perplexity=89.533745, train_loss=4.4946156

Batch 91660, train_perplexity=81.08997, train_loss=4.3955593

Batch 91670, train_perplexity=83.182556, train_loss=4.4210377

Batch 91680, train_perplexity=83.70123, train_loss=4.4272537

Batch 91690, train_perplexity=88.97625, train_loss=4.4883695

Batch 91700, train_perplexity=90.12132, train_loss=4.501157

Batch 91710, train_perplexity=85.526955, train_loss=4.4488316

Batch 91720, train_perplexity=85.9653, train_loss=4.4539437

Batch 91730, train_perplexity=97.53736, train_loss=4.5802355

Batch 91740, train_perplexity=97.46335, train_loss=4.5794764

Batch 91750, train_perplexity=97.26788, train_loss=4.577469

Batch 91760, train_perplexity=93.72627, train_loss=4.5403786

Batch 91770, train_perplexity=97.50927, train_loss=4.5799475

Batch 91780, train_perplexity=91.883385, train_loss=4.52052

Batch 91790, train_perplexity=83.38851, train_loss=4.4235106

Batch 91800, train_perplexity=88.70999, train_loss=4.4853725

Batch 91810, train_perplexity=85.82232, train_loss=4.452279

Batch 91820, train_perplexity=97.68831, train_loss=4.581782

Batch 91830, train_perplexity=95.49605, train_loss=4.559085

Batch 91840, train_perplexity=89.53605, train_loss=4.4946413

Batch 91850, train_perplexity=91.30177, train_loss=4.51417

Batch 91860, train_perplexity=94.96983, train_loss=4.5535593

Batch 91870, train_perplexity=94.07727, train_loss=4.5441165

Batch 91880, train_perplexity=91.70252, train_loss=4.51855

Batch 91890, train_perplexity=79.9643, train_loss=4.3815804

Batch 91900, train_perplexity=83.53823, train_loss=4.4253044

Batch 91910, train_perplexity=85.27667, train_loss=4.445901

Batch 91920, train_perplexity=90.15236, train_loss=4.501501

Batch 91930, train_perplexity=87.59734, train_loss=4.4727507

Batch 91940, train_perplexity=84.27509, train_loss=4.4340863

Batch 91950, train_perplexity=85.85056, train_loss=4.452608

Batch 91960, train_perplexity=88.794044, train_loss=4.4863195

Batch 91970, train_perplexity=96.859985, train_loss=4.5732665

Batch 91980, train_perplexity=84.90274, train_loss=4.4415064

Batch 91990, train_perplexity=85.68517, train_loss=4.45068

Batch 92000, train_perplexity=83.24978, train_loss=4.4218454

Batch 92010, train_perplexity=87.942116, train_loss=4.476679

Batch 92020, train_perplexity=95.30958, train_loss=4.5571303

Batch 92030, train_perplexity=83.57306, train_loss=4.425721

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 92040, train_perplexity=81.889885, train_loss=4.4053755

Batch 92050, train_perplexity=80.81374, train_loss=4.392147

Batch 92060, train_perplexity=82.23058, train_loss=4.4095273

Batch 92070, train_perplexity=82.930435, train_loss=4.418002

Batch 92080, train_perplexity=80.1561, train_loss=4.383976

Batch 92090, train_perplexity=85.266914, train_loss=4.4457865

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 92100, train_perplexity=71.252205, train_loss=4.266226

Batch 92110, train_perplexity=82.07135, train_loss=4.407589

Batch 92120, train_perplexity=86.60273, train_loss=4.4613314

Batch 92130, train_perplexity=78.6935, train_loss=4.3655605

Batch 92140, train_perplexity=75.56535, train_loss=4.324998

Batch 92150, train_perplexity=85.443306, train_loss=4.447853

Batch 92160, train_perplexity=75.80625, train_loss=4.328181

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 92170, train_perplexity=83.15654, train_loss=4.420725

Batch 92180, train_perplexity=84.94915, train_loss=4.442053

Batch 92190, train_perplexity=83.91312, train_loss=4.429782

Batch 92200, train_perplexity=99.75103, train_loss=4.6026773

Batch 92210, train_perplexity=73.51904, train_loss=4.2975445

Batch 92220, train_perplexity=76.22567, train_loss=4.3336983

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6192 sentences.
Finished loading
Batch 92230, train_perplexity=87.634315, train_loss=4.4731727

Batch 92240, train_perplexity=88.25184, train_loss=4.4801946

Batch 92250, train_perplexity=80.177734, train_loss=4.384246

Batch 92260, train_perplexity=86.543205, train_loss=4.460644

Batch 92270, train_perplexity=86.39514, train_loss=4.4589314

Batch 92280, train_perplexity=90.114876, train_loss=4.5010853

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 92290, train_perplexity=90.96464, train_loss=4.510471

Batch 92300, train_perplexity=81.13793, train_loss=4.3961506

Batch 92310, train_perplexity=89.42486, train_loss=4.4933987

Batch 92320, train_perplexity=83.31447, train_loss=4.422622

Batch 92330, train_perplexity=83.302826, train_loss=4.4224825

Batch 92340, train_perplexity=78.43044, train_loss=4.362212

Batch 92350, train_perplexity=80.913574, train_loss=4.3933816

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 92360, train_perplexity=95.11885, train_loss=4.555127

Batch 92370, train_perplexity=83.66225, train_loss=4.426788

Batch 92380, train_perplexity=77.60258, train_loss=4.3516006

Batch 92390, train_perplexity=83.36334, train_loss=4.4232087

Batch 92400, train_perplexity=89.202545, train_loss=4.4909096

Batch 92410, train_perplexity=79.012535, train_loss=4.3696065

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 92420, train_perplexity=83.67354, train_loss=4.426923

Batch 92430, train_perplexity=83.784935, train_loss=4.428253

Batch 92440, train_perplexity=79.90389, train_loss=4.3808246

Batch 92450, train_perplexity=79.924164, train_loss=4.3810782

Batch 92460, train_perplexity=96.39486, train_loss=4.568453

Batch 92470, train_perplexity=87.36458, train_loss=4.47009

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 92480, train_perplexity=107.442024, train_loss=4.6769514

Batch 92490, train_perplexity=79.04106, train_loss=4.3699675

Batch 92500, train_perplexity=92.0036, train_loss=4.5218277

Batch 92510, train_perplexity=88.079605, train_loss=4.478241

Batch 92520, train_perplexity=88.92209, train_loss=4.4877605

Batch 92530, train_perplexity=80.822685, train_loss=4.3922577

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 92540, train_perplexity=80.49661, train_loss=4.388215

Batch 92550, train_perplexity=81.00039, train_loss=4.394454

Batch 92560, train_perplexity=88.19341, train_loss=4.4795322

Batch 92570, train_perplexity=85.09648, train_loss=4.4437857

Batch 92580, train_perplexity=81.18577, train_loss=4.39674

Batch 92590, train_perplexity=80.92588, train_loss=4.3935337

Batch 92600, train_perplexity=86.348564, train_loss=4.458392

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 92610, train_perplexity=93.810875, train_loss=4.5412807

Batch 92620, train_perplexity=87.74098, train_loss=4.474389

Batch 92630, train_perplexity=81.675476, train_loss=4.402754

Batch 92640, train_perplexity=92.06074, train_loss=4.5224485

Batch 92650, train_perplexity=88.24187, train_loss=4.4800816

Batch 92660, train_perplexity=83.07767, train_loss=4.419776

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 92670, train_perplexity=88.24869, train_loss=4.480159

Batch 92680, train_perplexity=93.7025, train_loss=4.540125

Batch 92690, train_perplexity=90.982994, train_loss=4.5106726

Batch 92700, train_perplexity=82.39885, train_loss=4.4115715

Batch 92710, train_perplexity=83.54544, train_loss=4.4253907

Batch 92720, train_perplexity=83.167885, train_loss=4.4208612

Batch 92730, train_perplexity=94.28174, train_loss=4.5462875

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 92740, train_perplexity=100.03253, train_loss=4.6054955

Batch 92750, train_perplexity=88.726746, train_loss=4.4855614

Batch 92760, train_perplexity=93.7151, train_loss=4.5402594

Batch 92770, train_perplexity=88.22428, train_loss=4.4798822

Batch 92780, train_perplexity=99.96234, train_loss=4.6047935

Batch 92790, train_perplexity=86.91864, train_loss=4.4649725

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 92800, train_perplexity=90.13194, train_loss=4.5012746

Batch 92810, train_perplexity=96.79202, train_loss=4.5725646

Batch 92820, train_perplexity=90.63446, train_loss=4.5068345

Batch 92830, train_perplexity=99.41228, train_loss=4.5992756

Batch 92840, train_perplexity=84.09906, train_loss=4.4319954

Batch 92850, train_perplexity=85.23191, train_loss=4.445376

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 92860, train_perplexity=87.13306, train_loss=4.4674363

nohup: ignoring input
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-30 12:20:56.271582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-30 12:20:56.451991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:07:00.0
totalMemory: 22.38GiB freeMemory: 21.32GiB
2020-10-30 12:20:56.452047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-30 12:20:57.045368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-30 12:20:57.045424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-30 12:20:57.045436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-30 12:20:57.046813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20683 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:07:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=150743.66, train_loss=11.923336

Batch 10, train_perplexity=149998.81, train_loss=11.918383

Batch 20, train_perplexity=148767.66, train_loss=11.910141

Batch 30, train_perplexity=147056.61, train_loss=11.898573

Batch 40, train_perplexity=145474.53, train_loss=11.887756

Batch 50, train_perplexity=143851.98, train_loss=11.87654

Batch 60, train_perplexity=141951.56, train_loss=11.863241

Batch 70, train_perplexity=140948.34, train_loss=11.856149

Batch 80, train_perplexity=139035.1, train_loss=11.842482

Batch 90, train_perplexity=137539.05, train_loss=11.831663

Batch 100, train_perplexity=135443.38, train_loss=11.816309

Batch 110, train_perplexity=132576.12, train_loss=11.794912

Batch 120, train_perplexity=130287.92, train_loss=11.777502

Batch 130, train_perplexity=127989.39, train_loss=11.759703

Batch 140, train_perplexity=125574.914, train_loss=11.740658

Batch 150, train_perplexity=122882.94, train_loss=11.718987

Batch 160, train_perplexity=119900.9, train_loss=11.694421

Batch 170, train_perplexity=117249.68, train_loss=11.672061

Batch 180, train_perplexity=113236.055, train_loss=11.63723

Batch 190, train_perplexity=109206.305, train_loss=11.600994

Batch 200, train_perplexity=104406.625, train_loss=11.556048

Batch 210, train_perplexity=103067.9, train_loss=11.543143

Batch 220, train_perplexity=96092.76, train_loss=11.473069

Batch 230, train_perplexity=94481.52, train_loss=11.45616

Batch 240, train_perplexity=86978.89, train_loss=11.373421

Batch 250, train_perplexity=85306.98, train_loss=11.354012

Batch 260, train_perplexity=79501.95, train_loss=11.283537

Batch 270, train_perplexity=75587.305, train_loss=11.233044

Batch 280, train_perplexity=72022.4, train_loss=11.184732

Batch 290, train_perplexity=68323.01, train_loss=11.132002

Batch 300, train_perplexity=60737.152, train_loss=11.014311

Batch 310, train_perplexity=56554.863, train_loss=10.942966

Batch 320, train_perplexity=55497.605, train_loss=10.924095

Batch 330, train_perplexity=49490.027, train_loss=10.809526

Batch 340, train_perplexity=48041.176, train_loss=10.779814

Batch 350, train_perplexity=44658.695, train_loss=10.706804

Batch 360, train_perplexity=39830.574, train_loss=10.59239

Batch 370, train_perplexity=34884.2, train_loss=10.459789

Batch 380, train_perplexity=30828.24, train_loss=10.336186

Batch 390, train_perplexity=31532.264, train_loss=10.358767

Batch 400, train_perplexity=28773.168, train_loss=10.267199

Batch 410, train_perplexity=27175.54, train_loss=10.2100725

Batch 420, train_perplexity=24780.709, train_loss=10.117821

Batch 430, train_perplexity=24543.14, train_loss=10.108188

Batch 440, train_perplexity=21776.826, train_loss=9.988602

Batch 450, train_perplexity=21113.588, train_loss=9.957672

Batch 460, train_perplexity=23057.312, train_loss=10.045738

Batch 470, train_perplexity=18992.963, train_loss=9.851824

Batch 480, train_perplexity=17668.744, train_loss=9.779552

Batch 490, train_perplexity=19019.807, train_loss=9.853236

Batch 500, train_perplexity=19229.238, train_loss=9.864187

Batch 510, train_perplexity=17438.0, train_loss=9.766407

Batch 520, train_perplexity=17759.75, train_loss=9.78469

Batch 530, train_perplexity=16594.465, train_loss=9.716825

Batch 540, train_perplexity=15605.435, train_loss=9.655375

Batch 550, train_perplexity=15321.093, train_loss=9.636986

Batch 560, train_perplexity=14476.587, train_loss=9.580288

Batch 570, train_perplexity=13919.635, train_loss=9.541056

Batch 580, train_perplexity=13343.836, train_loss=9.49881

Batch 590, train_perplexity=14535.617, train_loss=9.584357

Batch 600, train_perplexity=14162.494, train_loss=9.558352
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 610, train_perplexity=13203.111, train_loss=9.488208

Batch 620, train_perplexity=12486.193, train_loss=9.432379

Batch 630, train_perplexity=13998.443, train_loss=9.546701

Batch 640, train_perplexity=12682.097, train_loss=9.447947

Batch 650, train_perplexity=12498.202, train_loss=9.43334

Batch 660, train_perplexity=12451.602, train_loss=9.429605

Batch 670, train_perplexity=12331.36, train_loss=9.419901

Batch 680, train_perplexity=12433.257, train_loss=9.42813

Batch 690, train_perplexity=11716.031, train_loss=9.368713

Batch 700, train_perplexity=11954.381, train_loss=9.388853

Batch 710, train_perplexity=11671.468, train_loss=9.3649025

Batch 720, train_perplexity=10762.807, train_loss=9.283852

Batch 730, train_perplexity=10544.111, train_loss=9.263323

Batch 740, train_perplexity=11812.494, train_loss=9.376913

Batch 750, train_perplexity=12175.683, train_loss=9.407196

Batch 760, train_perplexity=11248.098, train_loss=9.327954

Batch 770, train_perplexity=10238.17, train_loss=9.233878

Batch 780, train_perplexity=11400.468, train_loss=9.34141

Batch 790, train_perplexity=10217.053, train_loss=9.231813

Batch 800, train_perplexity=10627.043, train_loss=9.271157

Batch 810, train_perplexity=10617.662, train_loss=9.270274

Batch 820, train_perplexity=10220.308, train_loss=9.232132

Batch 830, train_perplexity=10262.324, train_loss=9.236235

Batch 840, train_perplexity=10567.094, train_loss=9.2655

Batch 850, train_perplexity=10104.764, train_loss=9.220762

Batch 860, train_perplexity=9702.766, train_loss=9.180166

Batch 870, train_perplexity=9887.372, train_loss=9.199014

Batch 880, train_perplexity=9253.818, train_loss=9.1327915

Batch 890, train_perplexity=8828.152, train_loss=9.085701

Batch 900, train_perplexity=9559.976, train_loss=9.16534

Batch 910, train_perplexity=8219.111, train_loss=9.014217

Batch 920, train_perplexity=9459.814, train_loss=9.154808

Batch 930, train_perplexity=8495.134, train_loss=9.047249

Batch 940, train_perplexity=8937.771, train_loss=9.098042

Batch 950, train_perplexity=8518.158, train_loss=9.049955

Batch 960, train_perplexity=8199.062, train_loss=9.011775

Batch 970, train_perplexity=8091.794, train_loss=8.998606

Batch 980, train_perplexity=8818.988, train_loss=9.084662

Batch 990, train_perplexity=8212.091, train_loss=9.013363

Batch 1000, train_perplexity=8091.0225, train_loss=8.99851

Batch 1010, train_perplexity=9092.176, train_loss=9.11517

Batch 1020, train_perplexity=8375.37, train_loss=9.033051

Batch 1030, train_perplexity=8482.312, train_loss=9.045738

Batch 1040, train_perplexity=8252.225, train_loss=9.018238

Batch 1050, train_perplexity=9206.636, train_loss=9.12768

Batch 1060, train_perplexity=8047.0366, train_loss=8.993059

Batch 1070, train_perplexity=8219.974, train_loss=9.014322

Batch 1080, train_perplexity=7524.9224, train_loss=8.925976

Batch 1090, train_perplexity=7712.383, train_loss=8.9505825

Batch 1100, train_perplexity=7644.844, train_loss=8.941787

Batch 1110, train_perplexity=7906.3833, train_loss=8.975426

Batch 1120, train_perplexity=8029.88, train_loss=8.990925

Batch 1130, train_perplexity=7312.948, train_loss=8.897402

Batch 1140, train_perplexity=7305.044, train_loss=8.89632

Batch 1150, train_perplexity=6957.1064, train_loss=8.847519

Batch 1160, train_perplexity=7337.5806, train_loss=8.900764

Batch 1170, train_perplexity=6762.8555, train_loss=8.8192005

Batch 1180, train_perplexity=6920.1826, train_loss=8.842197

Batch 1190, train_perplexity=6886.0283, train_loss=8.83725

Batch 1200, train_perplexity=7346.361, train_loss=8.90196

Batch 1210, train_perplexity=7095.6084, train_loss=8.867231

Batch 1220, train_perplexity=6795.7637, train_loss=8.824055

Batch 1230, train_perplexity=7533.8984, train_loss=8.927168

Batch 1240, train_perplexity=7561.6978, train_loss=8.930851

Batch 1250, train_perplexity=6844.297, train_loss=8.831171

Batch 1260, train_perplexity=6938.1304, train_loss=8.844788

Batch 1270, train_perplexity=7152.147, train_loss=8.875168

Batch 1280, train_perplexity=6969.285, train_loss=8.849268

Batch 1290, train_perplexity=6371.937, train_loss=8.759659

Batch 1300, train_perplexity=6591.707, train_loss=8.793568

Batch 1310, train_perplexity=7220.998, train_loss=8.884748

Batch 1320, train_perplexity=6893.44, train_loss=8.8383255

Batch 1330, train_perplexity=6248.359, train_loss=8.740074

Batch 1340, train_perplexity=6404.9585, train_loss=8.764828

Batch 1350, train_perplexity=5983.821, train_loss=8.696815

Batch 1360, train_perplexity=6475.7383, train_loss=8.775818

Batch 1370, train_perplexity=6010.1343, train_loss=8.701202

Batch 1380, train_perplexity=6960.1655, train_loss=8.847959

Batch 1390, train_perplexity=5691.8657, train_loss=8.646793

Batch 1400, train_perplexity=6399.1216, train_loss=8.763916

Batch 1410, train_perplexity=6211.926, train_loss=8.734226

Batch 1420, train_perplexity=6439.7354, train_loss=8.770243

Batch 1430, train_perplexity=6523.006, train_loss=8.783091

Batch 1440, train_perplexity=6471.3115, train_loss=8.775134

Batch 1450, train_perplexity=6117.8296, train_loss=8.718963

Batch 1460, train_perplexity=5930.4746, train_loss=8.68786

Batch 1470, train_perplexity=5995.0615, train_loss=8.698691

Batch 1480, train_perplexity=6040.5317, train_loss=8.706247

Batch 1490, train_perplexity=6353.8057, train_loss=8.756809

Batch 1500, train_perplexity=6452.411, train_loss=8.772209

Batch 1510, train_perplexity=5434.9263, train_loss=8.600601

Batch 1520, train_perplexity=5711.1904, train_loss=8.650183

Batch 1530, train_perplexity=5765.268, train_loss=8.659607

Batch 1540, train_perplexity=5907.8555, train_loss=8.684038

Batch 1550, train_perplexity=5902.0776, train_loss=8.68306

Batch 1560, train_perplexity=6159.4297, train_loss=8.7257395

Batch 1570, train_perplexity=5730.646, train_loss=8.653584

Batch 1580, train_perplexity=5649.2627, train_loss=8.63928

Batch 1590, train_perplexity=5755.819, train_loss=8.657967

Batch 1600, train_perplexity=6049.906, train_loss=8.707798

Batch 1610, train_perplexity=5523.0254, train_loss=8.616681

Batch 1620, train_perplexity=5776.881, train_loss=8.661619

Batch 1630, train_perplexity=5869.1187, train_loss=8.67746

Batch 1640, train_perplexity=6149.751, train_loss=8.724167

Batch 1650, train_perplexity=5915.929, train_loss=8.685404

Batch 1660, train_perplexity=6026.9976, train_loss=8.704004

Batch 1670, train_perplexity=6023.102, train_loss=8.703358

Batch 1680, train_perplexity=5539.193, train_loss=8.619604

Batch 1690, train_perplexity=5698.481, train_loss=8.647955

Batch 1700, train_perplexity=4811.3457, train_loss=8.478732

Batch 1710, train_perplexity=5367.2446, train_loss=8.58807

Batch 1720, train_perplexity=5580.9873, train_loss=8.627121

Batch 1730, train_perplexity=5563.8115, train_loss=8.624039

Batch 1740, train_perplexity=5811.1284, train_loss=8.66753

Batch 1750, train_perplexity=5226.453, train_loss=8.561488

Batch 1760, train_perplexity=5392.806, train_loss=8.592821

Batch 1770, train_perplexity=5444.2324, train_loss=8.602312

Batch 1780, train_perplexity=5213.3657, train_loss=8.558981

Batch 1790, train_perplexity=5264.888, train_loss=8.568815

Batch 1800, train_perplexity=5531.5176, train_loss=8.618217

Batch 1810, train_perplexity=5378.1377, train_loss=8.590097

Batch 1820, train_perplexity=5594.128, train_loss=8.629473

Batch 1830, train_perplexity=5330.5576, train_loss=8.581211

Batch 1840, train_perplexity=5250.688, train_loss=8.566114

Batch 1850, train_perplexity=4923.983, train_loss=8.501873

Batch 1860, train_perplexity=5411.373, train_loss=8.596258

Batch 1870, train_perplexity=5301.6494, train_loss=8.575773

Batch 1880, train_perplexity=5651.111, train_loss=8.639607

Batch 1890, train_perplexity=4504.6763, train_loss=8.412871

Batch 1900, train_perplexity=5281.796, train_loss=8.5720215

Batch 1910, train_perplexity=5357.947, train_loss=8.586336

Batch 1920, train_perplexity=5194.73, train_loss=8.5554

Batch 1930, train_perplexity=5109.1475, train_loss=8.538788

Batch 1940, train_perplexity=5173.5103, train_loss=8.551307

Batch 1950, train_perplexity=4635.7314, train_loss=8.441549

Batch 1960, train_perplexity=5445.936, train_loss=8.602625

Batch 1970, train_perplexity=5114.052, train_loss=8.539747
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 1980, train_perplexity=5174.5317, train_loss=8.551504

Batch 1990, train_perplexity=5255.1665, train_loss=8.566967

Batch 2000, train_perplexity=5585.769, train_loss=8.627977

Batch 2010, train_perplexity=5082.167, train_loss=8.533493

Batch 2020, train_perplexity=5229.8286, train_loss=8.562134

Batch 2030, train_perplexity=5023.164, train_loss=8.521815

Batch 2040, train_perplexity=5160.7227, train_loss=8.548832

Batch 2050, train_perplexity=5021.0137, train_loss=8.521387

Batch 2060, train_perplexity=5003.1465, train_loss=8.517822

Batch 2070, train_perplexity=5061.987, train_loss=8.529514

Batch 2080, train_perplexity=4685.9766, train_loss=8.45233

Batch 2090, train_perplexity=5028.9688, train_loss=8.52297

Batch 2100, train_perplexity=5216.0312, train_loss=8.559492

Batch 2110, train_perplexity=5430.75, train_loss=8.599833

Batch 2120, train_perplexity=5283.66, train_loss=8.572374

Batch 2130, train_perplexity=5152.933, train_loss=8.547321

Batch 2140, train_perplexity=5290.8506, train_loss=8.573734

Batch 2150, train_perplexity=5117.3447, train_loss=8.540391

Batch 2160, train_perplexity=4469.22, train_loss=8.404969

Batch 2170, train_perplexity=5029.995, train_loss=8.523174

Batch 2180, train_perplexity=5025.555, train_loss=8.522291

Batch 2190, train_perplexity=4876.6426, train_loss=8.492212

Batch 2200, train_perplexity=4727.6772, train_loss=8.461189

Batch 2210, train_perplexity=5007.1084, train_loss=8.518614

Batch 2220, train_perplexity=4588.6064, train_loss=8.431332

Batch 2230, train_perplexity=4951.8047, train_loss=8.507507

Batch 2240, train_perplexity=4722.0537, train_loss=8.459999

Batch 2250, train_perplexity=4664.2725, train_loss=8.447687

Batch 2260, train_perplexity=5209.012, train_loss=8.5581455

Batch 2270, train_perplexity=4519.845, train_loss=8.416233

Batch 2280, train_perplexity=4934.5127, train_loss=8.504009

Batch 2290, train_perplexity=4930.6274, train_loss=8.5032215

Batch 2300, train_perplexity=4801.2344, train_loss=8.476628

Batch 2310, train_perplexity=4625.7817, train_loss=8.439401

Batch 2320, train_perplexity=4714.1343, train_loss=8.458321

Batch 2330, train_perplexity=4805.288, train_loss=8.477472

Batch 2340, train_perplexity=4729.5894, train_loss=8.461594

Batch 2350, train_perplexity=4649.4126, train_loss=8.444496

Batch 2360, train_perplexity=4657.2495, train_loss=8.44618

Batch 2370, train_perplexity=4485.065, train_loss=8.408508

Batch 2380, train_perplexity=4644.272, train_loss=8.44339

Batch 2390, train_perplexity=4576.972, train_loss=8.428793

Batch 2400, train_perplexity=4572.98, train_loss=8.42792

Batch 2410, train_perplexity=4729.544, train_loss=8.461584

Batch 2420, train_perplexity=4641.429, train_loss=8.442778

Batch 2430, train_perplexity=4548.5664, train_loss=8.422567

Batch 2440, train_perplexity=4490.012, train_loss=8.409611

Batch 2450, train_perplexity=4172.582, train_loss=8.33629

Batch 2460, train_perplexity=4836.0073, train_loss=8.483845

Batch 2470, train_perplexity=4610.029, train_loss=8.435989

Batch 2480, train_perplexity=4302.7603, train_loss=8.367012

Batch 2490, train_perplexity=4834.306, train_loss=8.483493

Batch 2500, train_perplexity=4779.8306, train_loss=8.47216

Batch 2510, train_perplexity=4448.9907, train_loss=8.400433

Batch 2520, train_perplexity=4837.3403, train_loss=8.48412

Batch 2530, train_perplexity=4419.0186, train_loss=8.393673

Batch 2540, train_perplexity=4301.324, train_loss=8.366678

Batch 2550, train_perplexity=4321.8096, train_loss=8.371429

Batch 2560, train_perplexity=4029.1838, train_loss=8.301319

Batch 2570, train_perplexity=4326.585, train_loss=8.372534

Batch 2580, train_perplexity=4246.253, train_loss=8.353792

Batch 2590, train_perplexity=4248.436, train_loss=8.354306

Batch 2600, train_perplexity=4275.926, train_loss=8.360756

Batch 2610, train_perplexity=4528.177, train_loss=8.418075

Batch 2620, train_perplexity=4165.1597, train_loss=8.33451

Batch 2630, train_perplexity=4257.883, train_loss=8.356527

Batch 2640, train_perplexity=4019.2249, train_loss=8.298844

Batch 2650, train_perplexity=4309.762, train_loss=8.368638

Batch 2660, train_perplexity=4013.4373, train_loss=8.297403

Batch 2670, train_perplexity=4592.4326, train_loss=8.432165

Batch 2680, train_perplexity=4548.063, train_loss=8.422457

Batch 2690, train_perplexity=4280.8545, train_loss=8.361908

Batch 2700, train_perplexity=4480.79, train_loss=8.407555

Batch 2710, train_perplexity=3929.3635, train_loss=8.276233

Batch 2720, train_perplexity=4408.491, train_loss=8.391288

Batch 2730, train_perplexity=4331.886, train_loss=8.373758

Batch 2740, train_perplexity=4620.469, train_loss=8.4382515

Batch 2750, train_perplexity=4244.9165, train_loss=8.3534775

Batch 2760, train_perplexity=4306.689, train_loss=8.367925

Batch 2770, train_perplexity=4312.661, train_loss=8.36931

Batch 2780, train_perplexity=4325.141, train_loss=8.3722

Batch 2790, train_perplexity=4204.759, train_loss=8.343972

Batch 2800, train_perplexity=4120.141, train_loss=8.323643

Batch 2810, train_perplexity=4139.4673, train_loss=8.328322

Batch 2820, train_perplexity=4349.4463, train_loss=8.377804

Batch 2830, train_perplexity=4297.6587, train_loss=8.365826

Batch 2840, train_perplexity=4562.8906, train_loss=8.425712

Batch 2850, train_perplexity=4252.3965, train_loss=8.355238

Batch 2860, train_perplexity=4585.7803, train_loss=8.430716

Batch 2870, train_perplexity=4369.9688, train_loss=8.382511

Batch 2880, train_perplexity=4301.275, train_loss=8.366667

Batch 2890, train_perplexity=4175.6313, train_loss=8.337021

Batch 2900, train_perplexity=4470.7847, train_loss=8.405319

Batch 2910, train_perplexity=4143.3657, train_loss=8.329264

Batch 2920, train_perplexity=4347.6216, train_loss=8.377384

Batch 2930, train_perplexity=4215.1694, train_loss=8.346445

Batch 2940, train_perplexity=4328.7886, train_loss=8.373043

Batch 2950, train_perplexity=4371.1606, train_loss=8.382784

Batch 2960, train_perplexity=4058.7332, train_loss=8.308626

Batch 2970, train_perplexity=3801.3103, train_loss=8.243101

Batch 2980, train_perplexity=3818.2852, train_loss=8.247557

Batch 2990, train_perplexity=4151.83, train_loss=8.331305

Batch 3000, train_perplexity=4240.4297, train_loss=8.35242

Batch 3010, train_perplexity=3630.4473, train_loss=8.197111

Batch 3020, train_perplexity=3791.5493, train_loss=8.24053

Batch 3030, train_perplexity=4002.1506, train_loss=8.294587

Batch 3040, train_perplexity=4520.82, train_loss=8.416449

Batch 3050, train_perplexity=4630.5, train_loss=8.44042

Batch 3060, train_perplexity=3816.567, train_loss=8.247107

Batch 3070, train_perplexity=3869.2375, train_loss=8.260813

Batch 3080, train_perplexity=4121.0806, train_loss=8.323871

Batch 3090, train_perplexity=3810.4568, train_loss=8.245504

Batch 3100, train_perplexity=3912.5105, train_loss=8.2719345

Batch 3110, train_perplexity=3757.1814, train_loss=8.231424

Batch 3120, train_perplexity=3773.9375, train_loss=8.235874

Batch 3130, train_perplexity=4114.055, train_loss=8.322165

Batch 3140, train_perplexity=4172.606, train_loss=8.336296

Batch 3150, train_perplexity=3757.3677, train_loss=8.231474

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Batch 3160, train_perplexity=4034.8672, train_loss=8.302729

Batch 3170, train_perplexity=3715.1445, train_loss=8.220173

Batch 3180, train_perplexity=4087.4622, train_loss=8.31568

Batch 3190, train_perplexity=4608.455, train_loss=8.435648

Batch 3200, train_perplexity=4097.3833, train_loss=8.318104

Batch 3210, train_perplexity=4008.5374, train_loss=8.296182

Batch 3220, train_perplexity=3781.9216, train_loss=8.2379875

Batch 3230, train_perplexity=3822.4314, train_loss=8.248642

Batch 3240, train_perplexity=3813.0596, train_loss=8.246187

Batch 3250, train_perplexity=4159.884, train_loss=8.333242

Batch 3260, train_perplexity=4049.2146, train_loss=8.306278
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3270, train_perplexity=4297.179, train_loss=8.365714

Batch 3280, train_perplexity=3860.6458, train_loss=8.25859

Batch 3290, train_perplexity=3713.9473, train_loss=8.219851

Batch 3300, train_perplexity=4037.269, train_loss=8.303324

Batch 3310, train_perplexity=3681.3218, train_loss=8.211027

Batch 3320, train_perplexity=3530.1985, train_loss=8.169109

Batch 3330, train_perplexity=3869.5254, train_loss=8.260887

Batch 3340, train_perplexity=4044.3828, train_loss=8.305084

Batch 3350, train_perplexity=4002.1392, train_loss=8.294584

Batch 3360, train_perplexity=3980.2227, train_loss=8.289093

Batch 3370, train_perplexity=4145.5435, train_loss=8.329789

Batch 3380, train_perplexity=3728.2166, train_loss=8.223685

Batch 3390, train_perplexity=4430.513, train_loss=8.396271

Batch 3400, train_perplexity=3922.3022, train_loss=8.274434

Batch 3410, train_perplexity=4025.324, train_loss=8.300361

Batch 3420, train_perplexity=4008.7896, train_loss=8.296245

Batch 3430, train_perplexity=3674.7449, train_loss=8.209239

Batch 3440, train_perplexity=3801.767, train_loss=8.243221

Batch 3450, train_perplexity=3841.8298, train_loss=8.253704

Batch 3460, train_perplexity=4523.7866, train_loss=8.417105

Batch 3470, train_perplexity=3386.1926, train_loss=8.127461

Batch 3480, train_perplexity=3746.669, train_loss=8.228622

Batch 3490, train_perplexity=3869.3188, train_loss=8.260834

Batch 3500, train_perplexity=3621.9746, train_loss=8.194775

Batch 3510, train_perplexity=4073.2432, train_loss=8.312195

Batch 3520, train_perplexity=3890.851, train_loss=8.266383

Batch 3530, train_perplexity=4083.9436, train_loss=8.314818

Batch 3540, train_perplexity=3902.1365, train_loss=8.2692795

Batch 3550, train_perplexity=3504.9817, train_loss=8.161941

Batch 3560, train_perplexity=3695.8853, train_loss=8.214975

Batch 3570, train_perplexity=3819.7273, train_loss=8.247934

Batch 3580, train_perplexity=3607.1726, train_loss=8.19068

Batch 3590, train_perplexity=3644.6912, train_loss=8.201027

Batch 3600, train_perplexity=3654.6387, train_loss=8.2037525

Batch 3610, train_perplexity=3464.4546, train_loss=8.1503105

Batch 3620, train_perplexity=3778.612, train_loss=8.237112

Batch 3630, train_perplexity=3748.4917, train_loss=8.229109

Batch 3640, train_perplexity=3652.994, train_loss=8.203302

Batch 3650, train_perplexity=3454.6294, train_loss=8.14747

Batch 3660, train_perplexity=3682.5894, train_loss=8.211371

Batch 3670, train_perplexity=3602.6277, train_loss=8.189419

Batch 3680, train_perplexity=4270.9214, train_loss=8.359585

Batch 3690, train_perplexity=3843.1345, train_loss=8.254044

Batch 3700, train_perplexity=3837.0845, train_loss=8.252468

Batch 3710, train_perplexity=3695.427, train_loss=8.214851

Batch 3720, train_perplexity=3308.7864, train_loss=8.104337

Batch 3730, train_perplexity=3442.9731, train_loss=8.144091

Batch 3740, train_perplexity=3616.2935, train_loss=8.193205

Batch 3750, train_perplexity=3539.8772, train_loss=8.171847

Batch 3760, train_perplexity=4007.4631, train_loss=8.295914

Batch 3770, train_perplexity=3793.2563, train_loss=8.24098

Batch 3780, train_perplexity=3823.5105, train_loss=8.248924

Batch 3790, train_perplexity=3627.2288, train_loss=8.196224

Batch 3800, train_perplexity=3553.6948, train_loss=8.175743

Batch 3810, train_perplexity=3525.2932, train_loss=8.167719

Batch 3820, train_perplexity=3887.6353, train_loss=8.265556

Batch 3830, train_perplexity=3718.2212, train_loss=8.221001

Batch 3840, train_perplexity=3720.8604, train_loss=8.22171

Batch 3850, train_perplexity=3702.5457, train_loss=8.216776

Batch 3860, train_perplexity=3808.7566, train_loss=8.245058

Batch 3870, train_perplexity=3760.6443, train_loss=8.232346

Batch 3880, train_perplexity=3544.5898, train_loss=8.173178

Batch 3890, train_perplexity=3423.2795, train_loss=8.138354

Batch 3900, train_perplexity=3552.3936, train_loss=8.175377

Batch 3910, train_perplexity=3388.26, train_loss=8.128072

Batch 3920, train_perplexity=3542.954, train_loss=8.172716

Batch 3930, train_perplexity=3576.4946, train_loss=8.182138

Batch 3940, train_perplexity=3367.7166, train_loss=8.12199

Batch 3950, train_perplexity=3727.5376, train_loss=8.223503

Batch 3960, train_perplexity=3481.6846, train_loss=8.155272

Batch 3970, train_perplexity=3718.3384, train_loss=8.221032

Batch 3980, train_perplexity=3310.3928, train_loss=8.104822

Batch 3990, train_perplexity=3495.5916, train_loss=8.159258

Batch 4000, train_perplexity=3457.863, train_loss=8.148406

Batch 4010, train_perplexity=3624.4106, train_loss=8.195447

Batch 4020, train_perplexity=3576.1536, train_loss=8.182043

Batch 4030, train_perplexity=3571.2017, train_loss=8.180657

Batch 4040, train_perplexity=3541.8528, train_loss=8.172405

Batch 4050, train_perplexity=3608.4492, train_loss=8.191033

Batch 4060, train_perplexity=3501.6206, train_loss=8.160981

Batch 4070, train_perplexity=3188.4626, train_loss=8.067294

Batch 4080, train_perplexity=3508.9182, train_loss=8.163063

Batch 4090, train_perplexity=3750.7732, train_loss=8.229717

Batch 4100, train_perplexity=3067.9263, train_loss=8.028757

Batch 4110, train_perplexity=3504.8413, train_loss=8.1619005

Batch 4120, train_perplexity=3498.5964, train_loss=8.160117

Batch 4130, train_perplexity=3469.272, train_loss=8.1517

Batch 4140, train_perplexity=3345.6062, train_loss=8.115403

Batch 4150, train_perplexity=3522.6584, train_loss=8.166971

Batch 4160, train_perplexity=3431.1565, train_loss=8.140653

Batch 4170, train_perplexity=3458.031, train_loss=8.148455

Batch 4180, train_perplexity=3609.327, train_loss=8.191277

Batch 4190, train_perplexity=3412.3245, train_loss=8.135149

Batch 4200, train_perplexity=3527.062, train_loss=8.1682205

Batch 4210, train_perplexity=3359.3123, train_loss=8.119492

Batch 4220, train_perplexity=3564.1687, train_loss=8.178686

Batch 4230, train_perplexity=3358.0889, train_loss=8.119127

Batch 4240, train_perplexity=3083.331, train_loss=8.033766

Batch 4250, train_perplexity=3521.7246, train_loss=8.166706

Batch 4260, train_perplexity=3484.3684, train_loss=8.156042

Batch 4270, train_perplexity=3334.8838, train_loss=8.112193

Batch 4280, train_perplexity=3638.263, train_loss=8.199262

Batch 4290, train_perplexity=3149.1318, train_loss=8.054882

Batch 4300, train_perplexity=3699.676, train_loss=8.216001

Batch 4310, train_perplexity=3641.7065, train_loss=8.200208

Batch 4320, train_perplexity=3197.7656, train_loss=8.070208

Batch 4330, train_perplexity=3185.8213, train_loss=8.066465

Batch 4340, train_perplexity=3522.5476, train_loss=8.16694

Batch 4350, train_perplexity=3284.4434, train_loss=8.096952

Batch 4360, train_perplexity=3168.7346, train_loss=8.061088

Batch 4370, train_perplexity=3240.6912, train_loss=8.083542

Batch 4380, train_perplexity=3317.8108, train_loss=8.10706

Batch 4390, train_perplexity=3698.7905, train_loss=8.215761

Batch 4400, train_perplexity=3273.074, train_loss=8.093485

Batch 4410, train_perplexity=3179.5989, train_loss=8.06451

Batch 4420, train_perplexity=3677.4548, train_loss=8.209976

Batch 4430, train_perplexity=3652.5098, train_loss=8.20317

Batch 4440, train_perplexity=3256.274, train_loss=8.088339

Batch 4450, train_perplexity=3301.8105, train_loss=8.102226

Batch 4460, train_perplexity=3344.9458, train_loss=8.115206

Batch 4470, train_perplexity=3212.7927, train_loss=8.074896

Batch 4480, train_perplexity=3458.5522, train_loss=8.148605

Batch 4490, train_perplexity=3235.324, train_loss=8.081884

Batch 4500, train_perplexity=3543.2954, train_loss=8.172812

Batch 4510, train_perplexity=3162.1626, train_loss=8.059011

Batch 4520, train_perplexity=3600.979, train_loss=8.188961

Batch 4530, train_perplexity=3193.1274, train_loss=8.068756

Batch 4540, train_perplexity=3270.846, train_loss=8.092804

Batch 4550, train_perplexity=3452.6401, train_loss=8.146894

Batch 4560, train_perplexity=3298.8018, train_loss=8.101315

Batch 4570, train_perplexity=3207.8054, train_loss=8.073342

Batch 4580, train_perplexity=3347.9458, train_loss=8.116102

Batch 4590, train_perplexity=3048.1648, train_loss=8.022295

Batch 4600, train_perplexity=3140.3894, train_loss=8.052102

Batch 4610, train_perplexity=3514.1155, train_loss=8.164543
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4620, train_perplexity=3228.4016, train_loss=8.079742

Batch 4630, train_perplexity=3149.369, train_loss=8.054957

Batch 4640, train_perplexity=3335.625, train_loss=8.112415

Batch 4650, train_perplexity=3177.5103, train_loss=8.063853

Batch 4660, train_perplexity=3191.0908, train_loss=8.068118

Batch 4670, train_perplexity=3333.9712, train_loss=8.111919

Batch 4680, train_perplexity=3345.3445, train_loss=8.115325

Batch 4690, train_perplexity=3283.726, train_loss=8.096734

Batch 4700, train_perplexity=3348.2969, train_loss=8.116207

Batch 4710, train_perplexity=3446.8235, train_loss=8.145208

Batch 4720, train_perplexity=3304.8538, train_loss=8.1031475

Batch 4730, train_perplexity=3187.6628, train_loss=8.067043

Batch 4740, train_perplexity=3206.261, train_loss=8.072861

Batch 4750, train_perplexity=3104.3416, train_loss=8.040557

Batch 4760, train_perplexity=3421.328, train_loss=8.137784

Batch 4770, train_perplexity=3660.2788, train_loss=8.205295

Batch 4780, train_perplexity=3410.9026, train_loss=8.134732

Batch 4790, train_perplexity=3296.2764, train_loss=8.100549

Batch 4800, train_perplexity=3351.594, train_loss=8.117191

Batch 4810, train_perplexity=3217.6252, train_loss=8.076399

Batch 4820, train_perplexity=3360.8057, train_loss=8.119936

Batch 4830, train_perplexity=3099.2092, train_loss=8.038902

Batch 4840, train_perplexity=3114.4385, train_loss=8.043804

Batch 4850, train_perplexity=3374.8767, train_loss=8.124114

Batch 4860, train_perplexity=3139.4702, train_loss=8.051809

Batch 4870, train_perplexity=3363.9192, train_loss=8.120862

Batch 4880, train_perplexity=3092.1328, train_loss=8.036616

Batch 4890, train_perplexity=3521.6238, train_loss=8.166677

Batch 4900, train_perplexity=3166.8162, train_loss=8.060482

Batch 4910, train_perplexity=3456.455, train_loss=8.147999

Batch 4920, train_perplexity=3317.0925, train_loss=8.106844

Batch 4930, train_perplexity=3508.3594, train_loss=8.162904

Batch 4940, train_perplexity=3105.2446, train_loss=8.040848

Batch 4950, train_perplexity=3196.8752, train_loss=8.069929

Batch 4960, train_perplexity=3136.3699, train_loss=8.050821

Batch 4970, train_perplexity=3153.0144, train_loss=8.056114

Batch 4980, train_perplexity=3344.7544, train_loss=8.115149

Batch 4990, train_perplexity=3207.8757, train_loss=8.073364

Batch 5000, train_perplexity=3319.4724, train_loss=8.107561

Batch 5010, train_perplexity=3456.877, train_loss=8.148121

Batch 5020, train_perplexity=3186.1462, train_loss=8.066567

Batch 5030, train_perplexity=3333.5896, train_loss=8.111805

Batch 5040, train_perplexity=3324.072, train_loss=8.108946

Batch 5050, train_perplexity=3318.3455, train_loss=8.107222

Batch 5060, train_perplexity=3241.4946, train_loss=8.08379

Batch 5070, train_perplexity=3171.2197, train_loss=8.061872

Batch 5080, train_perplexity=3124.0886, train_loss=8.046898

Batch 5090, train_perplexity=3286.223, train_loss=8.097494

Batch 5100, train_perplexity=3287.5771, train_loss=8.097906

Batch 5110, train_perplexity=3218.8835, train_loss=8.07679

Batch 5120, train_perplexity=3119.59, train_loss=8.045457

Batch 5130, train_perplexity=3153.7422, train_loss=8.056345

Batch 5140, train_perplexity=3174.5603, train_loss=8.062924

Batch 5150, train_perplexity=3336.923, train_loss=8.112804

Batch 5160, train_perplexity=3268.9065, train_loss=8.092211

Batch 5170, train_perplexity=3121.6555, train_loss=8.046119

Batch 5180, train_perplexity=3100.244, train_loss=8.039236

Batch 5190, train_perplexity=3338.3108, train_loss=8.11322

Batch 5200, train_perplexity=3211.8982, train_loss=8.074617

Batch 5210, train_perplexity=3076.8862, train_loss=8.031673

Batch 5220, train_perplexity=3332.7153, train_loss=8.111543

Batch 5230, train_perplexity=3135.4607, train_loss=8.050531

Batch 5240, train_perplexity=3077.3381, train_loss=8.03182

Batch 5250, train_perplexity=3065.5308, train_loss=8.027976

Batch 5260, train_perplexity=2903.2527, train_loss=7.973587

Batch 5270, train_perplexity=3211.7695, train_loss=8.074577

Batch 5280, train_perplexity=3075.214, train_loss=8.03113

Batch 5290, train_perplexity=3013.4429, train_loss=8.0108385

Batch 5300, train_perplexity=2937.8438, train_loss=7.985431

Batch 5310, train_perplexity=3127.1829, train_loss=8.047888

Batch 5320, train_perplexity=3100.0547, train_loss=8.039175

Batch 5330, train_perplexity=3314.291, train_loss=8.105999

Batch 5340, train_perplexity=2906.411, train_loss=7.974674

Batch 5350, train_perplexity=3206.9336, train_loss=8.073071

Batch 5360, train_perplexity=2816.367, train_loss=7.943203

Batch 5370, train_perplexity=2970.4565, train_loss=7.996471

Batch 5380, train_perplexity=3284.4558, train_loss=8.096956

Batch 5390, train_perplexity=3140.7458, train_loss=8.052216

Batch 5400, train_perplexity=2996.5833, train_loss=8.005228

Batch 5410, train_perplexity=2961.5662, train_loss=7.9934735

Batch 5420, train_perplexity=3168.946, train_loss=8.061154

Batch 5430, train_perplexity=3214.6807, train_loss=8.075483

Batch 5440, train_perplexity=3098.565, train_loss=8.038694

Batch 5450, train_perplexity=3129.6829, train_loss=8.048687

Batch 5460, train_perplexity=3087.212, train_loss=8.035024

Batch 5470, train_perplexity=3094.419, train_loss=8.037355

Batch 5480, train_perplexity=3004.6301, train_loss=8.00791

Batch 5490, train_perplexity=3004.57, train_loss=8.00789

Batch 5500, train_perplexity=3257.0627, train_loss=8.088581

Batch 5510, train_perplexity=3136.1814, train_loss=8.050761

Batch 5520, train_perplexity=3221.414, train_loss=8.077576

Batch 5530, train_perplexity=3358.5403, train_loss=8.119262

Batch 5540, train_perplexity=2829.9727, train_loss=7.9480224

Batch 5550, train_perplexity=3245.2559, train_loss=8.0849495

Batch 5560, train_perplexity=3212.0942, train_loss=8.074678

Batch 5570, train_perplexity=3039.161, train_loss=8.019337

Batch 5580, train_perplexity=3014.9463, train_loss=8.011337

Batch 5590, train_perplexity=2928.5386, train_loss=7.982259

Batch 5600, train_perplexity=2859.3489, train_loss=7.958349

Batch 5610, train_perplexity=2910.6743, train_loss=7.97614

Batch 5620, train_perplexity=3017.5063, train_loss=8.012186

Batch 5630, train_perplexity=2845.2666, train_loss=7.953412

Batch 5640, train_perplexity=2945.1697, train_loss=7.9879217

Batch 5650, train_perplexity=3011.3657, train_loss=8.010149

Batch 5660, train_perplexity=2936.7246, train_loss=7.98505

Batch 5670, train_perplexity=2843.658, train_loss=7.9528465

Batch 5680, train_perplexity=3011.8137, train_loss=8.010298

Batch 5690, train_perplexity=3052.6885, train_loss=8.023778

Batch 5700, train_perplexity=2999.4424, train_loss=8.006182

Batch 5710, train_perplexity=2825.5056, train_loss=7.9464426

Batch 5720, train_perplexity=2704.8777, train_loss=7.902812

Batch 5730, train_perplexity=3054.8203, train_loss=8.024476

Batch 5740, train_perplexity=3133.0784, train_loss=8.049771

Batch 5750, train_perplexity=3247.0515, train_loss=8.085503

Batch 5760, train_perplexity=3111.4224, train_loss=8.042835

Batch 5770, train_perplexity=2891.8816, train_loss=7.9696627

Batch 5780, train_perplexity=2821.1248, train_loss=7.944891

Batch 5790, train_perplexity=2854.3276, train_loss=7.9565916

Batch 5800, train_perplexity=3208.041, train_loss=8.073416

Batch 5810, train_perplexity=2982.226, train_loss=8.000425

Batch 5820, train_perplexity=2839.2126, train_loss=7.951282

Batch 5830, train_perplexity=2812.3193, train_loss=7.941765

Batch 5840, train_perplexity=2833.3362, train_loss=7.94921

Batch 5850, train_perplexity=3231.8025, train_loss=8.080795

Batch 5860, train_perplexity=2852.2827, train_loss=7.955875

Batch 5870, train_perplexity=3123.5107, train_loss=8.046713

Batch 5880, train_perplexity=2635.4812, train_loss=7.876821

Batch 5890, train_perplexity=2993.3616, train_loss=8.004152

Batch 5900, train_perplexity=2843.3882, train_loss=7.9527516

Batch 5910, train_perplexity=2734.474, train_loss=7.9136944

Batch 5920, train_perplexity=2875.9739, train_loss=7.9641466

Batch 5930, train_perplexity=3100.374, train_loss=8.039278

Batch 5940, train_perplexity=3097.8203, train_loss=8.038454

Batch 5950, train_perplexity=2768.0796, train_loss=7.925909

Batch 5960, train_perplexity=2826.9219, train_loss=7.9469438
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 5970, train_perplexity=2709.4888, train_loss=7.9045153

Batch 5980, train_perplexity=3208.2092, train_loss=8.073468

Batch 5990, train_perplexity=2807.828, train_loss=7.9401665

Batch 6000, train_perplexity=3215.0425, train_loss=8.075596

Batch 6010, train_perplexity=2968.6384, train_loss=7.9958587

Batch 6020, train_perplexity=2704.113, train_loss=7.9025292

Batch 6030, train_perplexity=3088.5342, train_loss=8.035452

Batch 6040, train_perplexity=2783.048, train_loss=7.931302

Batch 6050, train_perplexity=2834.0767, train_loss=7.9494715

Batch 6060, train_perplexity=2795.4985, train_loss=7.9357657

Batch 6070, train_perplexity=3123.4392, train_loss=8.04669

Batch 6080, train_perplexity=2906.2183, train_loss=7.974608

Batch 6090, train_perplexity=2905.1042, train_loss=7.9742246

Batch 6100, train_perplexity=2914.4866, train_loss=7.977449

Batch 6110, train_perplexity=2829.313, train_loss=7.947789

Batch 6120, train_perplexity=2877.679, train_loss=7.9647393

Batch 6130, train_perplexity=2997.2463, train_loss=8.005449

Batch 6140, train_perplexity=2831.212, train_loss=7.94846

Batch 6150, train_perplexity=2690.676, train_loss=7.8975477

Batch 6160, train_perplexity=2742.6304, train_loss=7.9166727

Batch 6170, train_perplexity=2743.8389, train_loss=7.9171133

Batch 6180, train_perplexity=2726.4253, train_loss=7.9107466

Batch 6190, train_perplexity=2948.7615, train_loss=7.9891405

Batch 6200, train_perplexity=2874.7124, train_loss=7.963708

Batch 6210, train_perplexity=3051.7134, train_loss=8.0234585

Batch 6220, train_perplexity=2699.018, train_loss=7.9006433

Batch 6230, train_perplexity=2941.6133, train_loss=7.9867134

Batch 6240, train_perplexity=2711.0862, train_loss=7.9051046

Batch 6250, train_perplexity=2500.3174, train_loss=7.824173

Batch 6260, train_perplexity=2979.6477, train_loss=7.9995604

Batch 6270, train_perplexity=2760.0476, train_loss=7.923003

Batch 6280, train_perplexity=2715.0747, train_loss=7.9065747

Batch 6290, train_perplexity=2937.7737, train_loss=7.9854074

Batch 6300, train_perplexity=3030.978, train_loss=8.016641

Batch 6310, train_perplexity=2693.7134, train_loss=7.898676

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Batch 6320, train_perplexity=2774.5256, train_loss=7.928235

Batch 6330, train_perplexity=2794.9429, train_loss=7.935567

Batch 6340, train_perplexity=2980.7932, train_loss=7.9999447

Batch 6350, train_perplexity=2748.89, train_loss=7.9189525

Batch 6360, train_perplexity=2558.681, train_loss=7.847247

Batch 6370, train_perplexity=2775.7405, train_loss=7.928673

Batch 6380, train_perplexity=2669.6377, train_loss=7.889698

Batch 6390, train_perplexity=2649.7617, train_loss=7.882225

Batch 6400, train_perplexity=2892.323, train_loss=7.9698153

Batch 6410, train_perplexity=2652.4773, train_loss=7.8832493

Batch 6420, train_perplexity=2730.0886, train_loss=7.9120893

Batch 6430, train_perplexity=2910.9226, train_loss=7.9762254

Batch 6440, train_perplexity=2924.2295, train_loss=7.9807863

Batch 6450, train_perplexity=2843.848, train_loss=7.9529133

Batch 6460, train_perplexity=2654.0334, train_loss=7.883836

Batch 6470, train_perplexity=2817.1833, train_loss=7.943493

Batch 6480, train_perplexity=2928.382, train_loss=7.9822054

Batch 6490, train_perplexity=2871.482, train_loss=7.9625835

Batch 6500, train_perplexity=2731.0286, train_loss=7.9124336

Batch 6510, train_perplexity=2797.46, train_loss=7.936467

Batch 6520, train_perplexity=2589.4, train_loss=7.8591814

Batch 6530, train_perplexity=2699.5176, train_loss=7.9008284

Batch 6540, train_perplexity=2826.6106, train_loss=7.9468336

Batch 6550, train_perplexity=2816.571, train_loss=7.9432755

Batch 6560, train_perplexity=2710.4478, train_loss=7.904869

Batch 6570, train_perplexity=2490.7146, train_loss=7.820325

Batch 6580, train_perplexity=2937.6716, train_loss=7.9853725

Batch 6590, train_perplexity=2957.5908, train_loss=7.9921303

Batch 6600, train_perplexity=2798.1191, train_loss=7.9367027

Batch 6610, train_perplexity=2639.809, train_loss=7.878462

Batch 6620, train_perplexity=2606.249, train_loss=7.8656673

Batch 6630, train_perplexity=2925.1765, train_loss=7.98111

Batch 6640, train_perplexity=2563.5022, train_loss=7.8491297

Batch 6650, train_perplexity=2569.896, train_loss=7.8516207

Batch 6660, train_perplexity=2708.3752, train_loss=7.904104

Batch 6670, train_perplexity=2857.2937, train_loss=7.95763

Batch 6680, train_perplexity=2863.5896, train_loss=7.959831

Batch 6690, train_perplexity=2567.112, train_loss=7.850537

Batch 6700, train_perplexity=2879.2258, train_loss=7.9652767

Batch 6710, train_perplexity=2700.6416, train_loss=7.9012446

Batch 6720, train_perplexity=2748.0762, train_loss=7.9186563

Batch 6730, train_perplexity=2892.8333, train_loss=7.9699917

Batch 6740, train_perplexity=2527.0464, train_loss=7.8348064

Batch 6750, train_perplexity=2694.4739, train_loss=7.898958

Batch 6760, train_perplexity=2710.9517, train_loss=7.905055

Batch 6770, train_perplexity=2983.125, train_loss=8.000727

Batch 6780, train_perplexity=2588.1753, train_loss=7.8587084

Batch 6790, train_perplexity=2654.0308, train_loss=7.883835

Batch 6800, train_perplexity=2737.6157, train_loss=7.9148426

Batch 6810, train_perplexity=2754.163, train_loss=7.920869

Batch 6820, train_perplexity=2862.6584, train_loss=7.959506

Batch 6830, train_perplexity=2658.3765, train_loss=7.885471

Batch 6840, train_perplexity=2646.6252, train_loss=7.8810406

Batch 6850, train_perplexity=2999.9517, train_loss=8.006351

Batch 6860, train_perplexity=2916.3271, train_loss=7.9780803

Batch 6870, train_perplexity=2724.7695, train_loss=7.910139

Batch 6880, train_perplexity=2676.3535, train_loss=7.8922105

Batch 6890, train_perplexity=2738.9827, train_loss=7.915342

Batch 6900, train_perplexity=2787.3247, train_loss=7.9328375

Batch 6910, train_perplexity=2829.0405, train_loss=7.947693

Batch 6920, train_perplexity=2605.7583, train_loss=7.865479

Batch 6930, train_perplexity=2732.5032, train_loss=7.9129734

Batch 6940, train_perplexity=2717.9775, train_loss=7.9076433

Batch 6950, train_perplexity=2574.1565, train_loss=7.853277

Batch 6960, train_perplexity=2809.6467, train_loss=7.940814

Batch 6970, train_perplexity=2617.3132, train_loss=7.8699036

Batch 6980, train_perplexity=2792.6675, train_loss=7.9347525

Batch 6990, train_perplexity=2400.7615, train_loss=7.783541

Batch 7000, train_perplexity=2760.1753, train_loss=7.9230494

Batch 7010, train_perplexity=2571.872, train_loss=7.8523893

Batch 7020, train_perplexity=2697.0317, train_loss=7.899907

Batch 7030, train_perplexity=2778.8672, train_loss=7.9297986

Batch 7040, train_perplexity=2983.4607, train_loss=8.000839

Batch 7050, train_perplexity=2676.7502, train_loss=7.892359

Batch 7060, train_perplexity=2537.1946, train_loss=7.8388143

Batch 7070, train_perplexity=2964.5261, train_loss=7.9944725

Batch 7080, train_perplexity=2887.3953, train_loss=7.96811

Batch 7090, train_perplexity=2644.6824, train_loss=7.8803062

Batch 7100, train_perplexity=2854.7563, train_loss=7.956742

Batch 7110, train_perplexity=2965.1907, train_loss=7.9946966

Batch 7120, train_perplexity=2677.1155, train_loss=7.892495

Batch 7130, train_perplexity=2664.7412, train_loss=7.887862

Batch 7140, train_perplexity=2691.8618, train_loss=7.8979883

Batch 7150, train_perplexity=2720.065, train_loss=7.908411

Batch 7160, train_perplexity=2751.3762, train_loss=7.9198565

Batch 7170, train_perplexity=2428.1018, train_loss=7.794865

Batch 7180, train_perplexity=2806.5403, train_loss=7.9397078

Batch 7190, train_perplexity=2482.6787, train_loss=7.8170934

Batch 7200, train_perplexity=2714.927, train_loss=7.9065204

Batch 7210, train_perplexity=2586.6455, train_loss=7.858117

Batch 7220, train_perplexity=2701.346, train_loss=7.9015055

Batch 7230, train_perplexity=2670.827, train_loss=7.8901434

Batch 7240, train_perplexity=2818.9358, train_loss=7.9441147
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7250, train_perplexity=2653.3147, train_loss=7.883565

Batch 7260, train_perplexity=2552.8872, train_loss=7.8449802

Batch 7270, train_perplexity=2876.2646, train_loss=7.9642477

Batch 7280, train_perplexity=2676.9238, train_loss=7.8924236

Batch 7290, train_perplexity=2845.4158, train_loss=7.9534645

Batch 7300, train_perplexity=2624.4592, train_loss=7.87263

Batch 7310, train_perplexity=2539.7183, train_loss=7.8398085

Batch 7320, train_perplexity=2630.789, train_loss=7.875039

Batch 7330, train_perplexity=2649.2654, train_loss=7.8820376

Batch 7340, train_perplexity=2589.6492, train_loss=7.8592777

Batch 7350, train_perplexity=2796.589, train_loss=7.936156

Batch 7360, train_perplexity=2677.58, train_loss=7.8926687

Batch 7370, train_perplexity=2320.2244, train_loss=7.749419

Batch 7380, train_perplexity=2617.308, train_loss=7.8699017

Batch 7390, train_perplexity=2836.7563, train_loss=7.9504166

Batch 7400, train_perplexity=2443.1187, train_loss=7.8010306

Batch 7410, train_perplexity=2614.964, train_loss=7.8690057

Batch 7420, train_perplexity=2513.1992, train_loss=7.829312

Batch 7430, train_perplexity=2475.0522, train_loss=7.814017

Batch 7440, train_perplexity=2643.075, train_loss=7.8796983

Batch 7450, train_perplexity=2606.8506, train_loss=7.865898

Batch 7460, train_perplexity=2623.9949, train_loss=7.872453

Batch 7470, train_perplexity=2816.7483, train_loss=7.9433384

Batch 7480, train_perplexity=2582.332, train_loss=7.856448

Batch 7490, train_perplexity=2652.2065, train_loss=7.8831472

Batch 7500, train_perplexity=2663.0784, train_loss=7.887238

Batch 7510, train_perplexity=2555.752, train_loss=7.8461018

Batch 7520, train_perplexity=2604.3782, train_loss=7.864949

Batch 7530, train_perplexity=2549.4604, train_loss=7.843637

Batch 7540, train_perplexity=2592.8643, train_loss=7.8605185

Batch 7550, train_perplexity=2436.3828, train_loss=7.7982697

Batch 7560, train_perplexity=2796.6118, train_loss=7.936164

Batch 7570, train_perplexity=2453.096, train_loss=7.805106

Batch 7580, train_perplexity=2689.233, train_loss=7.8970113

Batch 7590, train_perplexity=2680.1042, train_loss=7.893611

Batch 7600, train_perplexity=2689.4856, train_loss=7.897105

Batch 7610, train_perplexity=2622.9116, train_loss=7.8720403

Batch 7620, train_perplexity=2916.7415, train_loss=7.9782224

Batch 7630, train_perplexity=2731.6135, train_loss=7.9126477

Batch 7640, train_perplexity=2756.039, train_loss=7.92155

Batch 7650, train_perplexity=2711.2725, train_loss=7.9051733

Batch 7660, train_perplexity=2644.9497, train_loss=7.8804073

Batch 7670, train_perplexity=2560.3188, train_loss=7.847887

Batch 7680, train_perplexity=2743.0422, train_loss=7.916823

Batch 7690, train_perplexity=2553.7712, train_loss=7.8453264

Batch 7700, train_perplexity=2465.028, train_loss=7.8099585

Batch 7710, train_perplexity=2680.6846, train_loss=7.8938274

Batch 7720, train_perplexity=2508.7166, train_loss=7.8275266

Batch 7730, train_perplexity=2619.9604, train_loss=7.8709145

Batch 7740, train_perplexity=2575.2087, train_loss=7.853686

Batch 7750, train_perplexity=2492.801, train_loss=7.821162

Batch 7760, train_perplexity=2537.6772, train_loss=7.8390045

Batch 7770, train_perplexity=2353.9177, train_loss=7.7638364

Batch 7780, train_perplexity=2451.716, train_loss=7.8045435

Batch 7790, train_perplexity=2550.781, train_loss=7.844155

Batch 7800, train_perplexity=2508.9067, train_loss=7.8276024

Batch 7810, train_perplexity=2641.131, train_loss=7.8789625

Batch 7820, train_perplexity=2550.1328, train_loss=7.8439007

Batch 7830, train_perplexity=2441.713, train_loss=7.800455

Batch 7840, train_perplexity=2530.35, train_loss=7.836113

Batch 7850, train_perplexity=2397.0974, train_loss=7.782014

Batch 7860, train_perplexity=2537.5999, train_loss=7.838974

Batch 7870, train_perplexity=2596.2964, train_loss=7.861841

Batch 7880, train_perplexity=2671.8076, train_loss=7.8905106

Batch 7890, train_perplexity=2459.3035, train_loss=7.8076334

Batch 7900, train_perplexity=2800.873, train_loss=7.9376864

Batch 7910, train_perplexity=2696.8467, train_loss=7.8998384

Batch 7920, train_perplexity=2780.7056, train_loss=7.93046

Batch 7930, train_perplexity=2569.57, train_loss=7.851494

Batch 7940, train_perplexity=2747.822, train_loss=7.918564

Batch 7950, train_perplexity=2458.016, train_loss=7.80711

Batch 7960, train_perplexity=2584.5027, train_loss=7.8572884

Batch 7970, train_perplexity=2805.9688, train_loss=7.939504

Batch 7980, train_perplexity=2357.4214, train_loss=7.7653236

Batch 7990, train_perplexity=2552.914, train_loss=7.8449907

Batch 8000, train_perplexity=2428.8186, train_loss=7.7951603

Batch 8010, train_perplexity=2584.8945, train_loss=7.85744

Batch 8020, train_perplexity=2529.402, train_loss=7.835738

Batch 8030, train_perplexity=2690.1833, train_loss=7.8973646

Batch 8040, train_perplexity=2718.9912, train_loss=7.908016

Batch 8050, train_perplexity=2642.5054, train_loss=7.8794827

Batch 8060, train_perplexity=2382.1614, train_loss=7.7757635

Batch 8070, train_perplexity=2515.0586, train_loss=7.8300514

Batch 8080, train_perplexity=2598.895, train_loss=7.8628416

Batch 8090, train_perplexity=2548.1199, train_loss=7.843111

Batch 8100, train_perplexity=2362.6519, train_loss=7.76754

Batch 8110, train_perplexity=2535.713, train_loss=7.83823

Batch 8120, train_perplexity=2613.284, train_loss=7.868363

Batch 8130, train_perplexity=2382.7407, train_loss=7.7760067

Batch 8140, train_perplexity=2661.2188, train_loss=7.8865395

Batch 8150, train_perplexity=2910.1218, train_loss=7.9759502

Batch 8160, train_perplexity=2677.7742, train_loss=7.892741

Batch 8170, train_perplexity=2402.744, train_loss=7.7843666

Batch 8180, train_perplexity=2465.1572, train_loss=7.810011

Batch 8190, train_perplexity=2731.8428, train_loss=7.9127316

Batch 8200, train_perplexity=2312.3716, train_loss=7.746029

Batch 8210, train_perplexity=2650.448, train_loss=7.882484

Batch 8220, train_perplexity=2501.1401, train_loss=7.824502

Batch 8230, train_perplexity=2409.3176, train_loss=7.787099

Batch 8240, train_perplexity=2605.9211, train_loss=7.8655415

Batch 8250, train_perplexity=2575.1633, train_loss=7.853668

Batch 8260, train_perplexity=2325.428, train_loss=7.7516594

Batch 8270, train_perplexity=2242.0547, train_loss=7.715148

Batch 8280, train_perplexity=2431.7622, train_loss=7.7963715

Batch 8290, train_perplexity=2521.3699, train_loss=7.8325577

Batch 8300, train_perplexity=2616.114, train_loss=7.8694453

Batch 8310, train_perplexity=2594.925, train_loss=7.861313

Batch 8320, train_perplexity=2599.5159, train_loss=7.8630805

Batch 8330, train_perplexity=2725.4634, train_loss=7.9103937

Batch 8340, train_perplexity=2369.6404, train_loss=7.7704935

Batch 8350, train_perplexity=2546.8916, train_loss=7.842629

Batch 8360, train_perplexity=2299.6711, train_loss=7.7405214

Batch 8370, train_perplexity=2522.664, train_loss=7.8330708

Batch 8380, train_perplexity=2358.8853, train_loss=7.7659445

Batch 8390, train_perplexity=2885.9333, train_loss=7.9676037

Batch 8400, train_perplexity=2487.4517, train_loss=7.819014

Batch 8410, train_perplexity=2426.7834, train_loss=7.794322

Batch 8420, train_perplexity=2290.0974, train_loss=7.7363496

Batch 8430, train_perplexity=2469.4988, train_loss=7.8117704

Batch 8440, train_perplexity=2692.8823, train_loss=7.8983674

Batch 8450, train_perplexity=2287.924, train_loss=7.7354

Batch 8460, train_perplexity=2544.7612, train_loss=7.841792

Batch 8470, train_perplexity=2463.1504, train_loss=7.8091965

Batch 8480, train_perplexity=2520.663, train_loss=7.8322773

Batch 8490, train_perplexity=2429.5183, train_loss=7.7954483

Batch 8500, train_perplexity=2716.9512, train_loss=7.9072657

Batch 8510, train_perplexity=2498.0532, train_loss=7.823267

Batch 8520, train_perplexity=2572.2559, train_loss=7.8525386

Batch 8530, train_perplexity=2298.4521, train_loss=7.739991

Batch 8540, train_perplexity=2374.5042, train_loss=7.772544

Batch 8550, train_perplexity=2441.7246, train_loss=7.80046

Batch 8560, train_perplexity=2278.7458, train_loss=7.7313805

Batch 8570, train_perplexity=2487.6155, train_loss=7.81908

Batch 8580, train_perplexity=2623.118, train_loss=7.872119
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 8590, train_perplexity=2457.7185, train_loss=7.8069887

Batch 8600, train_perplexity=2580.84, train_loss=7.8558702

Batch 8610, train_perplexity=2343.224, train_loss=7.759283

Batch 8620, train_perplexity=2477.5781, train_loss=7.815037

Batch 8630, train_perplexity=2515.7593, train_loss=7.83033

Batch 8640, train_perplexity=2470.4055, train_loss=7.8121376

Batch 8650, train_perplexity=2406.8662, train_loss=7.786081

Batch 8660, train_perplexity=2275.0955, train_loss=7.7297773

Batch 8670, train_perplexity=2544.213, train_loss=7.8415766

Batch 8680, train_perplexity=2441.1099, train_loss=7.800208

Batch 8690, train_perplexity=2363.3257, train_loss=7.767825

Batch 8700, train_perplexity=2503.3093, train_loss=7.825369

Batch 8710, train_perplexity=2467.26, train_loss=7.8108635

Batch 8720, train_perplexity=2424.482, train_loss=7.793373

Batch 8730, train_perplexity=2540.0066, train_loss=7.839922

Batch 8740, train_perplexity=2420.4436, train_loss=7.791706

Batch 8750, train_perplexity=2428.7864, train_loss=7.795147

Batch 8760, train_perplexity=2538.1323, train_loss=7.839184

Batch 8770, train_perplexity=2393.098, train_loss=7.780344

Batch 8780, train_perplexity=2468.1506, train_loss=7.8112245

Batch 8790, train_perplexity=2493.3074, train_loss=7.8213654

Batch 8800, train_perplexity=2269.8022, train_loss=7.727448

Batch 8810, train_perplexity=2603.3687, train_loss=7.8645616

Batch 8820, train_perplexity=2615.9956, train_loss=7.8694

Batch 8830, train_perplexity=2551.0461, train_loss=7.844259

Batch 8840, train_perplexity=2384.8164, train_loss=7.7768774

Batch 8850, train_perplexity=2440.0112, train_loss=7.799758

Batch 8860, train_perplexity=2462.6384, train_loss=7.8089886

Batch 8870, train_perplexity=2346.8806, train_loss=7.7608423

Batch 8880, train_perplexity=2281.9512, train_loss=7.732786

Batch 8890, train_perplexity=2708.1353, train_loss=7.9040155

Batch 8900, train_perplexity=2421.2493, train_loss=7.792039

Batch 8910, train_perplexity=2446.9858, train_loss=7.8026123

Batch 8920, train_perplexity=2333.7837, train_loss=7.755246

Batch 8930, train_perplexity=2337.8113, train_loss=7.7569704

Batch 8940, train_perplexity=2649.9514, train_loss=7.8822966

Batch 8950, train_perplexity=2258.6504, train_loss=7.7225227

Batch 8960, train_perplexity=2406.7102, train_loss=7.786016

Batch 8970, train_perplexity=2554.788, train_loss=7.8457246

Batch 8980, train_perplexity=2356.747, train_loss=7.7650375

Batch 8990, train_perplexity=2449.5378, train_loss=7.8036547

Batch 9000, train_perplexity=2284.8955, train_loss=7.7340755

Batch 9010, train_perplexity=2326.119, train_loss=7.7519565

Batch 9020, train_perplexity=2398.3481, train_loss=7.7825356

Batch 9030, train_perplexity=2563.286, train_loss=7.8490453

Batch 9040, train_perplexity=2351.0068, train_loss=7.762599

Batch 9050, train_perplexity=2387.4958, train_loss=7.7780004

Batch 9060, train_perplexity=2389.4946, train_loss=7.778837

Batch 9070, train_perplexity=2762.5718, train_loss=7.9239173

Batch 9080, train_perplexity=2318.265, train_loss=7.7485743

Batch 9090, train_perplexity=2341.0095, train_loss=7.7583375

Batch 9100, train_perplexity=2614.908, train_loss=7.868984

Batch 9110, train_perplexity=2436.6094, train_loss=7.7983627

Batch 9120, train_perplexity=2365.283, train_loss=7.768653

Batch 9130, train_perplexity=2482.8054, train_loss=7.8171444

Batch 9140, train_perplexity=2401.6099, train_loss=7.7838945

Batch 9150, train_perplexity=2544.174, train_loss=7.8415613

Batch 9160, train_perplexity=2202.803, train_loss=7.697486

Batch 9170, train_perplexity=2210.7644, train_loss=7.7010937

Batch 9180, train_perplexity=2544.0066, train_loss=7.8414955

Batch 9190, train_perplexity=2471.664, train_loss=7.812647

Batch 9200, train_perplexity=2416.746, train_loss=7.7901773

Batch 9210, train_perplexity=2389.2544, train_loss=7.7787366

Batch 9220, train_perplexity=2517.0286, train_loss=7.8308344

Batch 9230, train_perplexity=2285.4175, train_loss=7.734304

Batch 9240, train_perplexity=2341.1533, train_loss=7.758399

Batch 9250, train_perplexity=2205.258, train_loss=7.6986

Batch 9260, train_perplexity=2354.3882, train_loss=7.764036

Batch 9270, train_perplexity=2487.2773, train_loss=7.818944

Batch 9280, train_perplexity=2385.773, train_loss=7.7772784

Batch 9290, train_perplexity=2558.144, train_loss=7.8470373

Batch 9300, train_perplexity=2394.7383, train_loss=7.781029

Batch 9310, train_perplexity=2251.7454, train_loss=7.719461

Batch 9320, train_perplexity=2416.73, train_loss=7.7901707

Batch 9330, train_perplexity=2454.0928, train_loss=7.8055124

Batch 9340, train_perplexity=2380.9248, train_loss=7.775244

Batch 9350, train_perplexity=2356.2886, train_loss=7.764843

Batch 9360, train_perplexity=2558.3918, train_loss=7.847134

Batch 9370, train_perplexity=2292.3042, train_loss=7.737313

Batch 9380, train_perplexity=2307.5833, train_loss=7.743956

Batch 9390, train_perplexity=2437.0742, train_loss=7.7985535

Batch 9400, train_perplexity=2513.5815, train_loss=7.829464

Batch 9410, train_perplexity=2150.7832, train_loss=7.6735873

Batch 9420, train_perplexity=2420.4111, train_loss=7.7916927

Batch 9430, train_perplexity=2296.6401, train_loss=7.7392025

Batch 9440, train_perplexity=2318.1145, train_loss=7.7485094

Batch 9450, train_perplexity=2339.8364, train_loss=7.7578363

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Batch 9460, train_perplexity=2375.5754, train_loss=7.772995

Batch 9470, train_perplexity=2609.0107, train_loss=7.8667264

Batch 9480, train_perplexity=2327.3015, train_loss=7.752465

Batch 9490, train_perplexity=2376.721, train_loss=7.773477

Batch 9500, train_perplexity=2512.8662, train_loss=7.8291793

Batch 9510, train_perplexity=2260.2417, train_loss=7.723227

Batch 9520, train_perplexity=2378.4067, train_loss=7.774186

Batch 9530, train_perplexity=2431.3772, train_loss=7.796213

Batch 9540, train_perplexity=2347.2612, train_loss=7.7610044

Batch 9550, train_perplexity=2330.1565, train_loss=7.7536907

Batch 9560, train_perplexity=2346.1086, train_loss=7.7605133

Batch 9570, train_perplexity=2192.1968, train_loss=7.6926594

Batch 9580, train_perplexity=2336.466, train_loss=7.756395

Batch 9590, train_perplexity=2466.1309, train_loss=7.8104057

Batch 9600, train_perplexity=2446.4736, train_loss=7.802403

Batch 9610, train_perplexity=2487.8516, train_loss=7.819175

Batch 9620, train_perplexity=2281.2441, train_loss=7.732476

Batch 9630, train_perplexity=2283.5906, train_loss=7.7335043

Batch 9640, train_perplexity=2563.753, train_loss=7.8492274

Batch 9650, train_perplexity=2378.0032, train_loss=7.7740164

Batch 9660, train_perplexity=2177.2227, train_loss=7.6858053

Batch 9670, train_perplexity=2268.7256, train_loss=7.7269735

Batch 9680, train_perplexity=2247.8374, train_loss=7.717724

Batch 9690, train_perplexity=2383.4043, train_loss=7.776285

Batch 9700, train_perplexity=2364.7202, train_loss=7.768415

Batch 9710, train_perplexity=2375.1904, train_loss=7.772833

Batch 9720, train_perplexity=2401.0784, train_loss=7.7836733

Batch 9730, train_perplexity=2081.5552, train_loss=7.6408706

Batch 9740, train_perplexity=2383.7898, train_loss=7.776447

Batch 9750, train_perplexity=2298.0696, train_loss=7.739825

Batch 9760, train_perplexity=2448.683, train_loss=7.8033056

Batch 9770, train_perplexity=2175.2551, train_loss=7.684901

Batch 9780, train_perplexity=2427.5347, train_loss=7.7946315

Batch 9790, train_perplexity=2294.7812, train_loss=7.738393

Batch 9800, train_perplexity=2311.117, train_loss=7.7454863

Batch 9810, train_perplexity=2310.2996, train_loss=7.7451324

Batch 9820, train_perplexity=2247.82, train_loss=7.717716

Batch 9830, train_perplexity=2470.8755, train_loss=7.812328

Batch 9840, train_perplexity=2439.8647, train_loss=7.799698

Batch 9850, train_perplexity=2138.432, train_loss=7.667828

Batch 9860, train_perplexity=2395.7285, train_loss=7.7814426
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9870, train_perplexity=2345.028, train_loss=7.7600527

Batch 9880, train_perplexity=2492.9055, train_loss=7.821204

Batch 9890, train_perplexity=2281.96, train_loss=7.73279

Batch 9900, train_perplexity=2434.5073, train_loss=7.7974997

Batch 9910, train_perplexity=2272.8933, train_loss=7.728809

Batch 9920, train_perplexity=2310.3855, train_loss=7.7451696

Batch 9930, train_perplexity=2278.5654, train_loss=7.7313013

Batch 9940, train_perplexity=2412.5068, train_loss=7.7884216

Batch 9950, train_perplexity=2356.3447, train_loss=7.764867

Batch 9960, train_perplexity=2453.2678, train_loss=7.8051763

Batch 9970, train_perplexity=2363.9343, train_loss=7.7680826

Batch 9980, train_perplexity=2256.0046, train_loss=7.7213507

Batch 9990, train_perplexity=2419.689, train_loss=7.791394

Batch 10000, train_perplexity=2596.357, train_loss=7.8618646

Batch 10010, train_perplexity=2099.0828, train_loss=7.6492558

Batch 10020, train_perplexity=2267.3867, train_loss=7.726383

Batch 10030, train_perplexity=2101.6326, train_loss=7.65047

Batch 10040, train_perplexity=2261.037, train_loss=7.723579

Batch 10050, train_perplexity=2279.3, train_loss=7.7316236

Batch 10060, train_perplexity=2548.6423, train_loss=7.843316

Batch 10070, train_perplexity=2239.8618, train_loss=7.7141695

Batch 10080, train_perplexity=2228.0889, train_loss=7.7088995

Batch 10090, train_perplexity=2256.1252, train_loss=7.721404

Batch 10100, train_perplexity=2081.2288, train_loss=7.6407137

Batch 10110, train_perplexity=2336.9988, train_loss=7.756623

Batch 10120, train_perplexity=2251.4824, train_loss=7.719344

Batch 10130, train_perplexity=2498.6465, train_loss=7.8235044

Batch 10140, train_perplexity=2314.2422, train_loss=7.7468376

Batch 10150, train_perplexity=2335.8367, train_loss=7.7561255

Batch 10160, train_perplexity=2390.8943, train_loss=7.7794228

Batch 10170, train_perplexity=2554.7905, train_loss=7.8457255

Batch 10180, train_perplexity=2398.4946, train_loss=7.7825966

Batch 10190, train_perplexity=2152.5017, train_loss=7.674386

Batch 10200, train_perplexity=2114.239, train_loss=7.6564503

Batch 10210, train_perplexity=2302.1277, train_loss=7.741589

Batch 10220, train_perplexity=2171.0232, train_loss=7.682954

Batch 10230, train_perplexity=2271.5444, train_loss=7.728215

Batch 10240, train_perplexity=2207.3938, train_loss=7.699568

Batch 10250, train_perplexity=2114.8945, train_loss=7.65676

Batch 10260, train_perplexity=2349.913, train_loss=7.7621336

Batch 10270, train_perplexity=2251.0735, train_loss=7.7191625

Batch 10280, train_perplexity=2204.253, train_loss=7.698144

Batch 10290, train_perplexity=2320.3562, train_loss=7.749476

Batch 10300, train_perplexity=2150.3228, train_loss=7.673373

Batch 10310, train_perplexity=2271.7751, train_loss=7.728317

Batch 10320, train_perplexity=2344.4915, train_loss=7.759824

Batch 10330, train_perplexity=2467.8083, train_loss=7.8110857

Batch 10340, train_perplexity=2297.3125, train_loss=7.7394953

Batch 10350, train_perplexity=2293.3396, train_loss=7.7377644

Batch 10360, train_perplexity=2241.0222, train_loss=7.7146873

Batch 10370, train_perplexity=2377.5405, train_loss=7.773822

Batch 10380, train_perplexity=2313.491, train_loss=7.746513

Batch 10390, train_perplexity=2509.603, train_loss=7.82788

Batch 10400, train_perplexity=2234.0103, train_loss=7.7115536

Batch 10410, train_perplexity=2399.8445, train_loss=7.7831593

Batch 10420, train_perplexity=2471.3882, train_loss=7.8125353

Batch 10430, train_perplexity=2224.803, train_loss=7.7074237

Batch 10440, train_perplexity=2260.1177, train_loss=7.723172

Batch 10450, train_perplexity=2119.4456, train_loss=7.65891

Batch 10460, train_perplexity=2224.5771, train_loss=7.707322

Batch 10470, train_perplexity=2290.6182, train_loss=7.736577

Batch 10480, train_perplexity=2343.5784, train_loss=7.759434

Batch 10490, train_perplexity=2491.5745, train_loss=7.82067

Batch 10500, train_perplexity=2222.3538, train_loss=7.706322

Batch 10510, train_perplexity=2167.1116, train_loss=7.6811504

Batch 10520, train_perplexity=1969.379, train_loss=7.5854735

Batch 10530, train_perplexity=2339.1907, train_loss=7.7575603

Batch 10540, train_perplexity=2509.3997, train_loss=7.827799

Batch 10550, train_perplexity=2413.3674, train_loss=7.7887783

Batch 10560, train_perplexity=2279.913, train_loss=7.7318926

Batch 10570, train_perplexity=2235.6216, train_loss=7.7122746

Batch 10580, train_perplexity=2183.5688, train_loss=7.688716

Batch 10590, train_perplexity=2337.9072, train_loss=7.7570114

Batch 10600, train_perplexity=2387.1055, train_loss=7.777837

Batch 10610, train_perplexity=2285.4119, train_loss=7.7343016

Batch 10620, train_perplexity=2371.5125, train_loss=7.771283

Batch 10630, train_perplexity=2342.1116, train_loss=7.758808

Batch 10640, train_perplexity=2169.502, train_loss=7.682253

Batch 10650, train_perplexity=2225.7888, train_loss=7.7078667

Batch 10660, train_perplexity=2217.8218, train_loss=7.704281

Batch 10670, train_perplexity=2186.4822, train_loss=7.690049

Batch 10680, train_perplexity=2115.8953, train_loss=7.657233

Batch 10690, train_perplexity=2406.0366, train_loss=7.785736

Batch 10700, train_perplexity=2222.09, train_loss=7.7062035

Batch 10710, train_perplexity=2349.5186, train_loss=7.7619658

Batch 10720, train_perplexity=2305.3838, train_loss=7.7430024

Batch 10730, train_perplexity=2134.2644, train_loss=7.6658773

Batch 10740, train_perplexity=2230.1936, train_loss=7.7098436

Batch 10750, train_perplexity=2255.8928, train_loss=7.721301

Batch 10760, train_perplexity=2320.6692, train_loss=7.749611

Batch 10770, train_perplexity=2205.8154, train_loss=7.6988525

Batch 10780, train_perplexity=2321.8381, train_loss=7.7501144

Batch 10790, train_perplexity=2380.7307, train_loss=7.7751627

Batch 10800, train_perplexity=2475.3154, train_loss=7.814123

Batch 10810, train_perplexity=2279.598, train_loss=7.7317543

Batch 10820, train_perplexity=2204.0154, train_loss=7.698036

Batch 10830, train_perplexity=2214.2249, train_loss=7.7026577

Batch 10840, train_perplexity=2363.008, train_loss=7.7676907

Batch 10850, train_perplexity=2255.372, train_loss=7.7210703

Batch 10860, train_perplexity=2321.6587, train_loss=7.750037

Batch 10870, train_perplexity=2181.0247, train_loss=7.68755

Batch 10880, train_perplexity=2051.593, train_loss=7.626372

Batch 10890, train_perplexity=2243.3145, train_loss=7.7157097

Batch 10900, train_perplexity=2238.9714, train_loss=7.713772

Batch 10910, train_perplexity=2123.8584, train_loss=7.6609898

Batch 10920, train_perplexity=2221.308, train_loss=7.7058516

Batch 10930, train_perplexity=2388.325, train_loss=7.7783475

Batch 10940, train_perplexity=2422.0889, train_loss=7.7923856

Batch 10950, train_perplexity=2206.594, train_loss=7.6992054

Batch 10960, train_perplexity=2260.6567, train_loss=7.7234106

Batch 10970, train_perplexity=2128.6887, train_loss=7.6632614

Batch 10980, train_perplexity=2200.3423, train_loss=7.696368

Batch 10990, train_perplexity=2245.8853, train_loss=7.716855

Batch 11000, train_perplexity=2214.791, train_loss=7.7029133

Batch 11010, train_perplexity=2365.3518, train_loss=7.768682

Batch 11020, train_perplexity=2175.5208, train_loss=7.6850233

Batch 11030, train_perplexity=2261.7563, train_loss=7.723897

Batch 11040, train_perplexity=2108.8535, train_loss=7.6538997

Batch 11050, train_perplexity=2316.0593, train_loss=7.7476225

Batch 11060, train_perplexity=2155.4392, train_loss=7.67575

Batch 11070, train_perplexity=2131.4514, train_loss=7.6645584

Batch 11080, train_perplexity=2446.8179, train_loss=7.8025436

Batch 11090, train_perplexity=2110.937, train_loss=7.654887

Batch 11100, train_perplexity=2169.9902, train_loss=7.682478

Batch 11110, train_perplexity=2372.1128, train_loss=7.7715364

Batch 11120, train_perplexity=2098.8386, train_loss=7.6491394

Batch 11130, train_perplexity=2113.2322, train_loss=7.655974

Batch 11140, train_perplexity=2247.8535, train_loss=7.717731

Batch 11150, train_perplexity=2450.2588, train_loss=7.803949

Batch 11160, train_perplexity=2034.04, train_loss=7.6177793

Batch 11170, train_perplexity=2243.5413, train_loss=7.715811

Batch 11180, train_perplexity=2102.1018, train_loss=7.650693
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11190, train_perplexity=2215.8303, train_loss=7.7033825

Batch 11200, train_perplexity=2517.0696, train_loss=7.8308506

Batch 11210, train_perplexity=2354.505, train_loss=7.764086

Batch 11220, train_perplexity=2026.341, train_loss=7.613987

Batch 11230, train_perplexity=2361.664, train_loss=7.767122

Batch 11240, train_perplexity=2276.2783, train_loss=7.730297

Batch 11250, train_perplexity=2289.712, train_loss=7.7361813

Batch 11260, train_perplexity=2225.6987, train_loss=7.707826

Batch 11270, train_perplexity=2405.6384, train_loss=7.7855706

Batch 11280, train_perplexity=2064.802, train_loss=7.6327896

Batch 11290, train_perplexity=2005.6731, train_loss=7.603735

Batch 11300, train_perplexity=2144.2603, train_loss=7.67055

Batch 11310, train_perplexity=2356.3716, train_loss=7.7648783

Batch 11320, train_perplexity=2097.732, train_loss=7.648612

Batch 11330, train_perplexity=2192.37, train_loss=7.6927385

Batch 11340, train_perplexity=2083.139, train_loss=7.641631

Batch 11350, train_perplexity=2422.0298, train_loss=7.7923613

Batch 11360, train_perplexity=2261.9915, train_loss=7.724001

Batch 11370, train_perplexity=2164.7153, train_loss=7.680044

Batch 11380, train_perplexity=2239.6013, train_loss=7.714053

Batch 11390, train_perplexity=2145.2227, train_loss=7.6709986

Batch 11400, train_perplexity=2355.792, train_loss=7.764632

Batch 11410, train_perplexity=2187.553, train_loss=7.690539

Batch 11420, train_perplexity=2241.302, train_loss=7.7148123

Batch 11430, train_perplexity=2158.3396, train_loss=7.6770945

Batch 11440, train_perplexity=2065.6667, train_loss=7.6332083

Batch 11450, train_perplexity=2158.9849, train_loss=7.6773934

Batch 11460, train_perplexity=2227.0735, train_loss=7.7084436

Batch 11470, train_perplexity=2200.0938, train_loss=7.696255

Batch 11480, train_perplexity=2157.0452, train_loss=7.6764946

Batch 11490, train_perplexity=2437.5994, train_loss=7.798769

Batch 11500, train_perplexity=2206.7244, train_loss=7.6992645

Batch 11510, train_perplexity=2233.1604, train_loss=7.711173

Batch 11520, train_perplexity=2184.8145, train_loss=7.689286

Batch 11530, train_perplexity=2462.7957, train_loss=7.8090525

Batch 11540, train_perplexity=2157.8416, train_loss=7.6768637

Batch 11550, train_perplexity=2210.5874, train_loss=7.7010136

Batch 11560, train_perplexity=2239.4902, train_loss=7.7140036

Batch 11570, train_perplexity=2183.9397, train_loss=7.6888857

Batch 11580, train_perplexity=2384.107, train_loss=7.77658

Batch 11590, train_perplexity=2016.1459, train_loss=7.608943

Batch 11600, train_perplexity=2294.6216, train_loss=7.738323

Batch 11610, train_perplexity=2282.2058, train_loss=7.7328978

Batch 11620, train_perplexity=2181.8184, train_loss=7.687914

Batch 11630, train_perplexity=2090.2273, train_loss=7.645028

Batch 11640, train_perplexity=2142.7822, train_loss=7.6698604

Batch 11650, train_perplexity=2091.4038, train_loss=7.645591

Batch 11660, train_perplexity=2200.9802, train_loss=7.696658

Batch 11670, train_perplexity=2167.8762, train_loss=7.6815033

Batch 11680, train_perplexity=2123.1274, train_loss=7.6606455

Batch 11690, train_perplexity=2191.367, train_loss=7.692281

Batch 11700, train_perplexity=2303.3662, train_loss=7.742127

Batch 11710, train_perplexity=2040.4291, train_loss=7.6209154

Batch 11720, train_perplexity=2297.087, train_loss=7.739397

Batch 11730, train_perplexity=2251.4353, train_loss=7.719323

Batch 11740, train_perplexity=2036.8344, train_loss=7.619152

Batch 11750, train_perplexity=2191.2383, train_loss=7.692222

Batch 11760, train_perplexity=2202.0615, train_loss=7.6971493

Batch 11770, train_perplexity=2442.803, train_loss=7.8009014

Batch 11780, train_perplexity=2252.8247, train_loss=7.71994

Batch 11790, train_perplexity=2271.7004, train_loss=7.728284

Batch 11800, train_perplexity=2127.9336, train_loss=7.6629066

Batch 11810, train_perplexity=2344.9912, train_loss=7.760037

Batch 11820, train_perplexity=2210.2795, train_loss=7.7008743

Batch 11830, train_perplexity=2092.8682, train_loss=7.646291

Batch 11840, train_perplexity=2366.0918, train_loss=7.768995

Batch 11850, train_perplexity=2370.7979, train_loss=7.770982

Batch 11860, train_perplexity=2208.8762, train_loss=7.700239

Batch 11870, train_perplexity=2140.9744, train_loss=7.6690164

Batch 11880, train_perplexity=2168.5442, train_loss=7.6818113

Batch 11890, train_perplexity=1960.7612, train_loss=7.581088

Batch 11900, train_perplexity=2147.6504, train_loss=7.6721296

Batch 11910, train_perplexity=2138.7083, train_loss=7.6679573

Batch 11920, train_perplexity=2386.707, train_loss=7.77767

Batch 11930, train_perplexity=2036.912, train_loss=7.61919

Batch 11940, train_perplexity=2235.9604, train_loss=7.712426

Batch 11950, train_perplexity=2163.2356, train_loss=7.6793604

Batch 11960, train_perplexity=2257.4272, train_loss=7.721981

Batch 11970, train_perplexity=2178.689, train_loss=7.6864786

Batch 11980, train_perplexity=2258.2476, train_loss=7.7223444

Batch 11990, train_perplexity=2129.8076, train_loss=7.663787

Batch 12000, train_perplexity=2350.3813, train_loss=7.762333

Batch 12010, train_perplexity=2180.355, train_loss=7.687243

Batch 12020, train_perplexity=2108.6423, train_loss=7.6537995

Batch 12030, train_perplexity=2366.639, train_loss=7.769226

Batch 12040, train_perplexity=2314.9211, train_loss=7.747131

Batch 12050, train_perplexity=2073.8267, train_loss=7.637151

Batch 12060, train_perplexity=2396.2427, train_loss=7.781657

Batch 12070, train_perplexity=2170.162, train_loss=7.682557

Batch 12080, train_perplexity=2250.468, train_loss=7.7188935

Batch 12090, train_perplexity=2063.729, train_loss=7.63227

Batch 12100, train_perplexity=2176.4275, train_loss=7.68544

Batch 12110, train_perplexity=2272.1316, train_loss=7.7284737

Batch 12120, train_perplexity=2229.0505, train_loss=7.709331

Batch 12130, train_perplexity=2385.808, train_loss=7.777293

Batch 12140, train_perplexity=2212.127, train_loss=7.7017097

Batch 12150, train_perplexity=2213.9949, train_loss=7.7025537

Batch 12160, train_perplexity=2227.4387, train_loss=7.7086077

Batch 12170, train_perplexity=2423.8506, train_loss=7.7931128

Batch 12180, train_perplexity=2280.6155, train_loss=7.7322006

Batch 12190, train_perplexity=2172.0503, train_loss=7.683427

Batch 12200, train_perplexity=2087.9114, train_loss=7.6439195

Batch 12210, train_perplexity=2111.651, train_loss=7.6552253

Batch 12220, train_perplexity=2184.1812, train_loss=7.6889963

Batch 12230, train_perplexity=2127.0876, train_loss=7.662509

Batch 12240, train_perplexity=2315.8042, train_loss=7.7475123

Batch 12250, train_perplexity=2114.9329, train_loss=7.6567783

Batch 12260, train_perplexity=2110.3804, train_loss=7.6546235

Batch 12270, train_perplexity=2148.5015, train_loss=7.672526

Batch 12280, train_perplexity=2029.2389, train_loss=7.615416

Batch 12290, train_perplexity=2233.203, train_loss=7.711192

Batch 12300, train_perplexity=2347.5073, train_loss=7.7611094

Batch 12310, train_perplexity=2101.0254, train_loss=7.650181

Batch 12320, train_perplexity=2118.1382, train_loss=7.658293

Batch 12330, train_perplexity=2048.3271, train_loss=7.6247787

Batch 12340, train_perplexity=2197.811, train_loss=7.695217

Batch 12350, train_perplexity=2350.2717, train_loss=7.762286

Batch 12360, train_perplexity=2083.1438, train_loss=7.6416335

Batch 12370, train_perplexity=2087.5828, train_loss=7.643762

Batch 12380, train_perplexity=2302.3923, train_loss=7.741704

Batch 12390, train_perplexity=2274.603, train_loss=7.729561

Batch 12400, train_perplexity=2120.2249, train_loss=7.6592774

Batch 12410, train_perplexity=2201.4294, train_loss=7.696862

Batch 12420, train_perplexity=2203.0479, train_loss=7.697597

Batch 12430, train_perplexity=2071.9446, train_loss=7.636243

Batch 12440, train_perplexity=2339.7673, train_loss=7.757807

Batch 12450, train_perplexity=2315.5823, train_loss=7.7474165

Batch 12460, train_perplexity=2180.4216, train_loss=7.6872735

Batch 12470, train_perplexity=2123.2844, train_loss=7.6607194

Batch 12480, train_perplexity=2104.304, train_loss=7.65174

Batch 12490, train_perplexity=2364.39, train_loss=7.7682753

Batch 12500, train_perplexity=2179.7656, train_loss=7.6869726
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 12510, train_perplexity=2168.5771, train_loss=7.6818266

Batch 12520, train_perplexity=2165.5867, train_loss=7.6804466

Batch 12530, train_perplexity=2203.0298, train_loss=7.697589

Batch 12540, train_perplexity=2070.1748, train_loss=7.6353884

Batch 12550, train_perplexity=2192.207, train_loss=7.692664

Batch 12560, train_perplexity=2103.7783, train_loss=7.65149

Batch 12570, train_perplexity=2336.025, train_loss=7.756206

Batch 12580, train_perplexity=2184.6104, train_loss=7.689193

Batch 12590, train_perplexity=2105.7485, train_loss=7.6524262

Batch 12600, train_perplexity=2288.3584, train_loss=7.73559

Batch 12610, train_perplexity=2176.006, train_loss=7.6852465

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100
Loaded 305432 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100
Loaded 305432 sentences.
Finished loading
Batch 12620, train_perplexity=1992.887, train_loss=7.5973396

Batch 12630, train_perplexity=2229.6682, train_loss=7.709608

Batch 12640, train_perplexity=2064.8276, train_loss=7.632802

Batch 12650, train_perplexity=2124.7986, train_loss=7.6614323

Batch 12660, train_perplexity=2262.6594, train_loss=7.724296

Batch 12670, train_perplexity=2043.3949, train_loss=7.622368

Batch 12680, train_perplexity=2088.215, train_loss=7.644065

Batch 12690, train_perplexity=2029.6501, train_loss=7.6156187

Batch 12700, train_perplexity=2238.2583, train_loss=7.7134533

Batch 12710, train_perplexity=2382.2568, train_loss=7.7758036

Batch 12720, train_perplexity=2126.4678, train_loss=7.6622176

Batch 12730, train_perplexity=2230.5913, train_loss=7.710022

Batch 12740, train_perplexity=1970.8285, train_loss=7.5862093

Batch 12750, train_perplexity=2255.9358, train_loss=7.72132

Batch 12760, train_perplexity=2291.501, train_loss=7.7369623

Batch 12770, train_perplexity=1979.1215, train_loss=7.5904083

Batch 12780, train_perplexity=2168.3499, train_loss=7.6817217

Batch 12790, train_perplexity=2181.164, train_loss=7.687614

Batch 12800, train_perplexity=2145.822, train_loss=7.671278

Batch 12810, train_perplexity=2108.3708, train_loss=7.653671

Batch 12820, train_perplexity=2110.965, train_loss=7.6549006

Batch 12830, train_perplexity=2080.0679, train_loss=7.640156

Batch 12840, train_perplexity=2022.4681, train_loss=7.612074

Batch 12850, train_perplexity=2189.107, train_loss=7.691249

Batch 12860, train_perplexity=2050.7832, train_loss=7.625977

Batch 12870, train_perplexity=1950.1501, train_loss=7.5756617

Batch 12880, train_perplexity=2201.545, train_loss=7.6969147

Batch 12890, train_perplexity=2153.29, train_loss=7.674752

Batch 12900, train_perplexity=2124.0427, train_loss=7.6610765

Batch 12910, train_perplexity=2055.5686, train_loss=7.628308

Batch 12920, train_perplexity=2075.9775, train_loss=7.6381874

Batch 12930, train_perplexity=2036.881, train_loss=7.619175

Batch 12940, train_perplexity=2053.577, train_loss=7.6273384

Batch 12950, train_perplexity=1999.6808, train_loss=7.600743

Batch 12960, train_perplexity=2020.5432, train_loss=7.6111217

Batch 12970, train_perplexity=2200.825, train_loss=7.6965876

Batch 12980, train_perplexity=2162.2456, train_loss=7.6789026

Batch 12990, train_perplexity=2247.3635, train_loss=7.717513

Batch 13000, train_perplexity=2229.464, train_loss=7.7095165

Batch 13010, train_perplexity=1951.486, train_loss=7.5763464

Batch 13020, train_perplexity=2010.4242, train_loss=7.606101

Batch 13030, train_perplexity=2223.7139, train_loss=7.706934

Batch 13040, train_perplexity=2281.627, train_loss=7.732644

Batch 13050, train_perplexity=2193.669, train_loss=7.693331

Batch 13060, train_perplexity=2053.866, train_loss=7.627479

Batch 13070, train_perplexity=1997.6079, train_loss=7.5997057

Batch 13080, train_perplexity=2095.3945, train_loss=7.647497

Batch 13090, train_perplexity=2200.9268, train_loss=7.696634

Batch 13100, train_perplexity=2120.8123, train_loss=7.6595545

Batch 13110, train_perplexity=2325.8206, train_loss=7.751828

Batch 13120, train_perplexity=2195.6343, train_loss=7.6942263

Batch 13130, train_perplexity=2021.4365, train_loss=7.6115637

Batch 13140, train_perplexity=2410.7014, train_loss=7.787673

Batch 13150, train_perplexity=2088.891, train_loss=7.6443887

Batch 13160, train_perplexity=2169.0632, train_loss=7.6820507

Batch 13170, train_perplexity=2057.6978, train_loss=7.629343

Batch 13180, train_perplexity=2189.4607, train_loss=7.6914105

Batch 13190, train_perplexity=1962.385, train_loss=7.581916

Batch 13200, train_perplexity=2159.7231, train_loss=7.6777353

Batch 13210, train_perplexity=2013.189, train_loss=7.6074753

Batch 13220, train_perplexity=1986.9565, train_loss=7.5943594

Batch 13230, train_perplexity=2104.7146, train_loss=7.651935

Batch 13240, train_perplexity=2228.1326, train_loss=7.708919

Batch 13250, train_perplexity=2139.8926, train_loss=7.668511

Batch 13260, train_perplexity=2183.2795, train_loss=7.6885834

Batch 13270, train_perplexity=1925.1982, train_loss=7.562784

Batch 13280, train_perplexity=1965.7856, train_loss=7.5836473

Batch 13290, train_perplexity=2387.1738, train_loss=7.7778654

Batch 13300, train_perplexity=2100.5947, train_loss=7.649976

Batch 13310, train_perplexity=2080.3088, train_loss=7.6402717

Batch 13320, train_perplexity=2039.0168, train_loss=7.620223

Batch 13330, train_perplexity=2060.9048, train_loss=7.6309004

Batch 13340, train_perplexity=1985.1781, train_loss=7.593464

Batch 13350, train_perplexity=1912.3689, train_loss=7.556098

Batch 13360, train_perplexity=2007.1196, train_loss=7.604456

Batch 13370, train_perplexity=1913.4269, train_loss=7.556651

Batch 13380, train_perplexity=2088.7947, train_loss=7.6443424

Batch 13390, train_perplexity=2086.4473, train_loss=7.643218

Batch 13400, train_perplexity=2133.5176, train_loss=7.6655273

Batch 13410, train_perplexity=2141.539, train_loss=7.66928

Batch 13420, train_perplexity=2053.8757, train_loss=7.627484

Batch 13430, train_perplexity=2126.6333, train_loss=7.6622953

Batch 13440, train_perplexity=2250.2341, train_loss=7.7187896

Batch 13450, train_perplexity=2067.74, train_loss=7.6342115

Batch 13460, train_perplexity=2065.6785, train_loss=7.633214

Batch 13470, train_perplexity=2244.851, train_loss=7.7163944

Batch 13480, train_perplexity=1889.8656, train_loss=7.544261

Batch 13490, train_perplexity=2025.3267, train_loss=7.6134863

Batch 13500, train_perplexity=1901.4883, train_loss=7.550392

Batch 13510, train_perplexity=2199.8806, train_loss=7.6961584

Batch 13520, train_perplexity=2230.418, train_loss=7.7099442

Batch 13530, train_perplexity=2075.2253, train_loss=7.637825

Batch 13540, train_perplexity=2112.9783, train_loss=7.6558537

Batch 13550, train_perplexity=2043.3832, train_loss=7.622362

Batch 13560, train_perplexity=2131.8264, train_loss=7.6647344

Batch 13570, train_perplexity=2302.412, train_loss=7.7417126

Batch 13580, train_perplexity=2176.7036, train_loss=7.685567

Batch 13590, train_perplexity=2134.416, train_loss=7.6659484

Batch 13600, train_perplexity=2108.6472, train_loss=7.653802

Batch 13610, train_perplexity=2013.4136, train_loss=7.607587

Batch 13620, train_perplexity=2224.9165, train_loss=7.7074747

Batch 13630, train_perplexity=2236.7583, train_loss=7.712783

Batch 13640, train_perplexity=2094.4817, train_loss=7.6470613

Batch 13650, train_perplexity=2178.0325, train_loss=7.6861773

Batch 13660, train_perplexity=2078.545, train_loss=7.6394234

Batch 13670, train_perplexity=2122.7153, train_loss=7.6604514

Batch 13680, train_perplexity=2166.1797, train_loss=7.6807203

Batch 13690, train_perplexity=2050.831, train_loss=7.6260004

Batch 13700, train_perplexity=2407.1267, train_loss=7.786189

Batch 13710, train_perplexity=2248.427, train_loss=7.717986

Batch 13720, train_perplexity=2105.2114, train_loss=7.652171

Batch 13730, train_perplexity=1977.7083, train_loss=7.589694

Batch 13740, train_perplexity=2121.757, train_loss=7.66

Batch 13750, train_perplexity=2206.0068, train_loss=7.6989393

Batch 13760, train_perplexity=2040.7969, train_loss=7.6210957
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13770, train_perplexity=2087.8198, train_loss=7.6438756

Batch 13780, train_perplexity=2054.1086, train_loss=7.6275973

Batch 13790, train_perplexity=2145.777, train_loss=7.671257

Batch 13800, train_perplexity=1974.9188, train_loss=7.5882826

Batch 13810, train_perplexity=1946.6903, train_loss=7.573886

Batch 13820, train_perplexity=2194.3918, train_loss=7.6936603

Batch 13830, train_perplexity=1964.5178, train_loss=7.583002

Batch 13840, train_perplexity=2103.6218, train_loss=7.651416

Batch 13850, train_perplexity=2213.218, train_loss=7.702203

Batch 13860, train_perplexity=2010.1577, train_loss=7.6059685

Batch 13870, train_perplexity=2047.0051, train_loss=7.624133

Batch 13880, train_perplexity=2052.461, train_loss=7.626795

Batch 13890, train_perplexity=2059.7632, train_loss=7.6303463

Batch 13900, train_perplexity=1957.2163, train_loss=7.5792785

Batch 13910, train_perplexity=2237.0974, train_loss=7.7129345

Batch 13920, train_perplexity=2079.8704, train_loss=7.640061

Batch 13930, train_perplexity=2088.484, train_loss=7.6441936

Batch 13940, train_perplexity=2049.0286, train_loss=7.625121

Batch 13950, train_perplexity=1957.361, train_loss=7.5793524

Batch 13960, train_perplexity=2099.0789, train_loss=7.649254

Batch 13970, train_perplexity=1845.2385, train_loss=7.520364

Batch 13980, train_perplexity=2058.0803, train_loss=7.629529

Batch 13990, train_perplexity=2035.5896, train_loss=7.618541

Batch 14000, train_perplexity=2062.3735, train_loss=7.631613

Batch 14010, train_perplexity=2156.642, train_loss=7.6763077

Batch 14020, train_perplexity=1961.1035, train_loss=7.5812626

Batch 14030, train_perplexity=1974.3671, train_loss=7.588003

Batch 14040, train_perplexity=1997.3489, train_loss=7.599576

Batch 14050, train_perplexity=2027.5162, train_loss=7.614567

Batch 14060, train_perplexity=2305.2727, train_loss=7.7429543

Batch 14070, train_perplexity=1865.603, train_loss=7.5313396

Batch 14080, train_perplexity=2173.2832, train_loss=7.6839943

Batch 14090, train_perplexity=1994.954, train_loss=7.5983763

Batch 14100, train_perplexity=2195.6458, train_loss=7.6942315

Batch 14110, train_perplexity=2087.542, train_loss=7.6437426

Batch 14120, train_perplexity=1957.8724, train_loss=7.5796137

Batch 14130, train_perplexity=2023.8872, train_loss=7.6127753

Batch 14140, train_perplexity=2013.2255, train_loss=7.6074934

Batch 14150, train_perplexity=2152.7861, train_loss=7.674518

Batch 14160, train_perplexity=1970.1482, train_loss=7.585864

Batch 14170, train_perplexity=2001.8455, train_loss=7.6018248

Batch 14180, train_perplexity=2267.058, train_loss=7.7262383

Batch 14190, train_perplexity=1953.7942, train_loss=7.5775285

Batch 14200, train_perplexity=2114.9832, train_loss=7.656802

Batch 14210, train_perplexity=2266.0085, train_loss=7.7257752

Batch 14220, train_perplexity=2192.8845, train_loss=7.692973

Batch 14230, train_perplexity=2126.988, train_loss=7.662462

Batch 14240, train_perplexity=2010.0743, train_loss=7.605927

Batch 14250, train_perplexity=2072.1482, train_loss=7.636341

Batch 14260, train_perplexity=1906.3016, train_loss=7.5529203

Batch 14270, train_perplexity=2204.1963, train_loss=7.698118

Batch 14280, train_perplexity=1951.7233, train_loss=7.576468

Batch 14290, train_perplexity=2037.8164, train_loss=7.619634

Batch 14300, train_perplexity=2049.6218, train_loss=7.6254106

Batch 14310, train_perplexity=2050.616, train_loss=7.6258955

Batch 14320, train_perplexity=2208.7014, train_loss=7.70016

Batch 14330, train_perplexity=1941.2389, train_loss=7.5710816

Batch 14340, train_perplexity=1873.5408, train_loss=7.5355854

Batch 14350, train_perplexity=2111.5864, train_loss=7.6551948

Batch 14360, train_perplexity=2110.9712, train_loss=7.6549034

Batch 14370, train_perplexity=2165.8047, train_loss=7.680547

Batch 14380, train_perplexity=2310.8835, train_loss=7.745385

Batch 14390, train_perplexity=2092.4412, train_loss=7.6460867

Batch 14400, train_perplexity=2288.3235, train_loss=7.7355747

Batch 14410, train_perplexity=2095.171, train_loss=7.6473904

Batch 14420, train_perplexity=1923.5419, train_loss=7.5619235

Batch 14430, train_perplexity=2135.2407, train_loss=7.6663346

Batch 14440, train_perplexity=2083.1587, train_loss=7.6416407

Batch 14450, train_perplexity=2086.73, train_loss=7.6433535

Batch 14460, train_perplexity=2011.9692, train_loss=7.606869

Batch 14470, train_perplexity=1919.4619, train_loss=7.5598

Batch 14480, train_perplexity=2125.0935, train_loss=7.661571

Batch 14490, train_perplexity=2306.0566, train_loss=7.7432942

Batch 14500, train_perplexity=1964.4335, train_loss=7.582959

Batch 14510, train_perplexity=2090.2832, train_loss=7.645055

Batch 14520, train_perplexity=2132.1091, train_loss=7.664867

Batch 14530, train_perplexity=1995.0358, train_loss=7.5984173

Batch 14540, train_perplexity=1949.6741, train_loss=7.5754175

Batch 14550, train_perplexity=1996.3138, train_loss=7.5990577

Batch 14560, train_perplexity=2061.928, train_loss=7.631397

Batch 14570, train_perplexity=2035.8458, train_loss=7.6186666

Batch 14580, train_perplexity=1933.7878, train_loss=7.567236

Batch 14590, train_perplexity=2033.4291, train_loss=7.617479

Batch 14600, train_perplexity=1792.4039, train_loss=7.491313

Batch 14610, train_perplexity=2048.7756, train_loss=7.6249976

Batch 14620, train_perplexity=2068.241, train_loss=7.634454

Batch 14630, train_perplexity=1763.8497, train_loss=7.475254

Batch 14640, train_perplexity=2086.2732, train_loss=7.6431346

Batch 14650, train_perplexity=1986.7852, train_loss=7.594273

Batch 14660, train_perplexity=2055.947, train_loss=7.628492

Batch 14670, train_perplexity=1891.63, train_loss=7.545194

Batch 14680, train_perplexity=1948.4641, train_loss=7.5747967

Batch 14690, train_perplexity=2131.4675, train_loss=7.664566

Batch 14700, train_perplexity=1875.4357, train_loss=7.5365963

Batch 14710, train_perplexity=1959.3201, train_loss=7.580353

Batch 14720, train_perplexity=2005.2351, train_loss=7.6035166

Batch 14730, train_perplexity=2095.0908, train_loss=7.647352

Batch 14740, train_perplexity=2261.8062, train_loss=7.723919

Batch 14750, train_perplexity=2214.0898, train_loss=7.7025967

Batch 14760, train_perplexity=2170.6887, train_loss=7.6828

Batch 14770, train_perplexity=2110.1047, train_loss=7.654493

Batch 14780, train_perplexity=2042.634, train_loss=7.6219954

Batch 14790, train_perplexity=2124.678, train_loss=7.6613755

Batch 14800, train_perplexity=1854.6224, train_loss=7.5254364

Batch 14810, train_perplexity=1980.0437, train_loss=7.590874

Batch 14820, train_perplexity=2060.9558, train_loss=7.630925

Batch 14830, train_perplexity=2174.8984, train_loss=7.684737

Batch 14840, train_perplexity=2197.5103, train_loss=7.6950803

Batch 14850, train_perplexity=2004.5315, train_loss=7.6031656

Batch 14860, train_perplexity=1996.6708, train_loss=7.5992365

Batch 14870, train_perplexity=2077.0093, train_loss=7.6386843

Batch 14880, train_perplexity=1926.0944, train_loss=7.5632496

Batch 14890, train_perplexity=2037.046, train_loss=7.619256

Batch 14900, train_perplexity=1791.6554, train_loss=7.4908953

Batch 14910, train_perplexity=2285.353, train_loss=7.734276

Batch 14920, train_perplexity=2173.1382, train_loss=7.6839275

Batch 14930, train_perplexity=1974.4198, train_loss=7.58803

Batch 14940, train_perplexity=1999.9993, train_loss=7.600902

Batch 14950, train_perplexity=1932.1002, train_loss=7.566363

Batch 14960, train_perplexity=2123.824, train_loss=7.6609735

Batch 14970, train_perplexity=1993.7786, train_loss=7.597787

Batch 14980, train_perplexity=2029.4121, train_loss=7.6155014

Batch 14990, train_perplexity=1935.7823, train_loss=7.568267

Batch 15000, train_perplexity=2107.8481, train_loss=7.653423

Batch 15010, train_perplexity=2341.1892, train_loss=7.7584143

Batch 15020, train_perplexity=2002.9712, train_loss=7.602387

Batch 15030, train_perplexity=2016.2103, train_loss=7.608975

Batch 15040, train_perplexity=1850.0939, train_loss=7.5229917

Batch 15050, train_perplexity=1984.067, train_loss=7.592904

Batch 15060, train_perplexity=2020.7272, train_loss=7.6112127

Batch 15070, train_perplexity=2094.829, train_loss=7.6472273

Batch 15080, train_perplexity=2050.5496, train_loss=7.625863
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15090, train_perplexity=2101.0996, train_loss=7.650216

Batch 15100, train_perplexity=2084.0203, train_loss=7.642054

Batch 15110, train_perplexity=2301.5767, train_loss=7.7413497

Batch 15120, train_perplexity=1908.8121, train_loss=7.5542364

Batch 15130, train_perplexity=1931.6718, train_loss=7.566141

Batch 15140, train_perplexity=1974.2654, train_loss=7.5879517

Batch 15150, train_perplexity=1924.0474, train_loss=7.5621862

Batch 15160, train_perplexity=2195.7285, train_loss=7.694269

Batch 15170, train_perplexity=2098.5574, train_loss=7.6490054

Batch 15180, train_perplexity=1957.3861, train_loss=7.5793653

Batch 15190, train_perplexity=1955.4215, train_loss=7.578361

Batch 15200, train_perplexity=2037.8865, train_loss=7.6196685

Batch 15210, train_perplexity=1936.8645, train_loss=7.5688257

Batch 15220, train_perplexity=2026.4937, train_loss=7.6140623

Batch 15230, train_perplexity=1864.3812, train_loss=7.5306845

Batch 15240, train_perplexity=2114.0737, train_loss=7.656372

Batch 15250, train_perplexity=2018.5469, train_loss=7.610133

Batch 15260, train_perplexity=1894.7526, train_loss=7.5468435

Batch 15270, train_perplexity=2006.2393, train_loss=7.6040173

Batch 15280, train_perplexity=1907.6857, train_loss=7.553646

Batch 15290, train_perplexity=1908.2898, train_loss=7.5539627

Batch 15300, train_perplexity=2181.1785, train_loss=7.6876206

Batch 15310, train_perplexity=2266.1794, train_loss=7.7258506

Batch 15320, train_perplexity=2136.2703, train_loss=7.6668167

Batch 15330, train_perplexity=1887.0093, train_loss=7.5427485

Batch 15340, train_perplexity=1952.2063, train_loss=7.5767155

Batch 15350, train_perplexity=1918.2322, train_loss=7.5591593

Batch 15360, train_perplexity=1892.7605, train_loss=7.5457916

Batch 15370, train_perplexity=2038.6212, train_loss=7.620029

Batch 15380, train_perplexity=1878.176, train_loss=7.5380564

Batch 15390, train_perplexity=1911.069, train_loss=7.555418

Batch 15400, train_perplexity=1998.1871, train_loss=7.5999956

Batch 15410, train_perplexity=2297.0562, train_loss=7.7393837

Batch 15420, train_perplexity=2072.9685, train_loss=7.636737

Batch 15430, train_perplexity=1905.1858, train_loss=7.552335

Batch 15440, train_perplexity=2344.1694, train_loss=7.7596865

Batch 15450, train_perplexity=2021.4615, train_loss=7.611576

Batch 15460, train_perplexity=1971.3501, train_loss=7.586474

Batch 15470, train_perplexity=2085.729, train_loss=7.642874

Batch 15480, train_perplexity=1912.3761, train_loss=7.556102

Batch 15490, train_perplexity=2096.932, train_loss=7.6482306

Batch 15500, train_perplexity=2023.1067, train_loss=7.6123896

Batch 15510, train_perplexity=2055.9775, train_loss=7.6285067

Batch 15520, train_perplexity=1820.6215, train_loss=7.506933

Batch 15530, train_perplexity=2047.424, train_loss=7.6243377

Batch 15540, train_perplexity=2061.8503, train_loss=7.631359

Batch 15550, train_perplexity=1967.5758, train_loss=7.5845575

Batch 15560, train_perplexity=2052.7566, train_loss=7.626939

Batch 15570, train_perplexity=1983.5648, train_loss=7.592651

Batch 15580, train_perplexity=1890.3198, train_loss=7.5445013

Batch 15590, train_perplexity=2105.2534, train_loss=7.652191

Batch 15600, train_perplexity=2009.075, train_loss=7.6054296

Batch 15610, train_perplexity=2070.323, train_loss=7.63546

Batch 15620, train_perplexity=1985.3768, train_loss=7.593564

Batch 15630, train_perplexity=2008.5251, train_loss=7.605156

Batch 15640, train_perplexity=2028.7386, train_loss=7.6151695

Batch 15650, train_perplexity=1942.8983, train_loss=7.571936

Batch 15660, train_perplexity=2011.6133, train_loss=7.6066923

Batch 15670, train_perplexity=2009.3374, train_loss=7.6055603

Batch 15680, train_perplexity=1910.5077, train_loss=7.5551243

Batch 15690, train_perplexity=1860.8455, train_loss=7.528786

Batch 15700, train_perplexity=1947.1971, train_loss=7.5741463

Batch 15710, train_perplexity=1984.7787, train_loss=7.5932627

Batch 15720, train_perplexity=2133.4607, train_loss=7.6655006

Batch 15730, train_perplexity=2026.1091, train_loss=7.6138725

Batch 15740, train_perplexity=2213.9568, train_loss=7.7025366

Batch 15750, train_perplexity=1904.7697, train_loss=7.5521164

Batch 15760, train_perplexity=2041.633, train_loss=7.6215053

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Batch 15770, train_perplexity=1730.3558, train_loss=7.4560823

Batch 15780, train_perplexity=1981.4633, train_loss=7.591591

Batch 15790, train_perplexity=2040.1431, train_loss=7.620775

Batch 15800, train_perplexity=1826.5135, train_loss=7.5101643

Batch 15810, train_perplexity=2192.414, train_loss=7.6927586

Batch 15820, train_perplexity=2089.5618, train_loss=7.6447096

Batch 15830, train_perplexity=2044.0283, train_loss=7.622678

Batch 15840, train_perplexity=1926.4967, train_loss=7.5634584

Batch 15850, train_perplexity=2023.0623, train_loss=7.6123676

Batch 15860, train_perplexity=1930.6405, train_loss=7.565607

Batch 15870, train_perplexity=2092.566, train_loss=7.6461463

Batch 15880, train_perplexity=2059.3518, train_loss=7.6301465

Batch 15890, train_perplexity=1895.602, train_loss=7.5472918

Batch 15900, train_perplexity=1812.4222, train_loss=7.5024195

Batch 15910, train_perplexity=1820.9896, train_loss=7.5071354

Batch 15920, train_perplexity=1877.6423, train_loss=7.537772

Batch 15930, train_perplexity=2006.8153, train_loss=7.6043043

Batch 15940, train_perplexity=1923.5886, train_loss=7.561948

Batch 15950, train_perplexity=2028.9031, train_loss=7.6152506

Batch 15960, train_perplexity=2039.4875, train_loss=7.620454

Batch 15970, train_perplexity=1984.8221, train_loss=7.5932846

Batch 15980, train_perplexity=2023.7522, train_loss=7.6127086

Batch 15990, train_perplexity=2173.6626, train_loss=7.684169

Batch 16000, train_perplexity=1921.7844, train_loss=7.5610094

Batch 16010, train_perplexity=1954.5964, train_loss=7.577939

Batch 16020, train_perplexity=2232.511, train_loss=7.710882

Batch 16030, train_perplexity=1857.2809, train_loss=7.526869

Batch 16040, train_perplexity=1839.1334, train_loss=7.51705

Batch 16050, train_perplexity=1857.249, train_loss=7.5268517

Batch 16060, train_perplexity=2047.9004, train_loss=7.6245704

Batch 16070, train_perplexity=1834.7958, train_loss=7.5146885

Batch 16080, train_perplexity=2048.6536, train_loss=7.624938

Batch 16090, train_perplexity=2221.5305, train_loss=7.7059517

Batch 16100, train_perplexity=1777.9347, train_loss=7.4832077

Batch 16110, train_perplexity=1920.5093, train_loss=7.5603456

Batch 16120, train_perplexity=2136.747, train_loss=7.66704

Batch 16130, train_perplexity=1944.473, train_loss=7.5727463

Batch 16140, train_perplexity=2022.7478, train_loss=7.612212

Batch 16150, train_perplexity=2041.8871, train_loss=7.6216297

Batch 16160, train_perplexity=2039.6023, train_loss=7.62051

Batch 16170, train_perplexity=1882.346, train_loss=7.540274

Batch 16180, train_perplexity=1973.6903, train_loss=7.5876603

Batch 16190, train_perplexity=2005.3287, train_loss=7.6035633

Batch 16200, train_perplexity=2085.1086, train_loss=7.642576

Batch 16210, train_perplexity=2112.8877, train_loss=7.655811

Batch 16220, train_perplexity=2009.5885, train_loss=7.605685

Batch 16230, train_perplexity=1994.31, train_loss=7.5980535

Batch 16240, train_perplexity=2013.1832, train_loss=7.6074724

Batch 16250, train_perplexity=2163.8455, train_loss=7.679642

Batch 16260, train_perplexity=2033.9808, train_loss=7.61775

Batch 16270, train_perplexity=1985.1696, train_loss=7.5934596

Batch 16280, train_perplexity=1855.9185, train_loss=7.526135

Batch 16290, train_perplexity=2095.042, train_loss=7.647329

Batch 16300, train_perplexity=1887.3134, train_loss=7.5429096

Batch 16310, train_perplexity=2047.792, train_loss=7.6245174

Batch 16320, train_perplexity=1893.2064, train_loss=7.546027

Batch 16330, train_perplexity=2003.0409, train_loss=7.6024218
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 16340, train_perplexity=1863.1477, train_loss=7.5300226

Batch 16350, train_perplexity=1901.4166, train_loss=7.5503545

Batch 16360, train_perplexity=2039.3456, train_loss=7.620384

Batch 16370, train_perplexity=1918.309, train_loss=7.5591993

Batch 16380, train_perplexity=2047.7023, train_loss=7.6244736

Batch 16390, train_perplexity=2034.8607, train_loss=7.6181827

Batch 16400, train_perplexity=1935.8729, train_loss=7.5683136

Batch 16410, train_perplexity=1928.997, train_loss=7.5647554

Batch 16420, train_perplexity=1990.6001, train_loss=7.5961914

Batch 16430, train_perplexity=1957.5514, train_loss=7.5794497

Batch 16440, train_perplexity=1777.2609, train_loss=7.4828286

Batch 16450, train_perplexity=2061.111, train_loss=7.6310005

Batch 16460, train_perplexity=1854.9779, train_loss=7.525628

Batch 16470, train_perplexity=1865.7098, train_loss=7.531397

Batch 16480, train_perplexity=2197.1016, train_loss=7.6948943

Batch 16490, train_perplexity=2064.5046, train_loss=7.6326456

Batch 16500, train_perplexity=1775.4169, train_loss=7.4817905

Batch 16510, train_perplexity=1974.1307, train_loss=7.5878835

Batch 16520, train_perplexity=2189.4368, train_loss=7.6913996

Batch 16530, train_perplexity=2050.88, train_loss=7.6260242

Batch 16540, train_perplexity=1958.4028, train_loss=7.5798845

Batch 16550, train_perplexity=1819.9523, train_loss=7.5065656

Batch 16560, train_perplexity=2136.4038, train_loss=7.666879

Batch 16570, train_perplexity=1961.7319, train_loss=7.581583

Batch 16580, train_perplexity=1872.2494, train_loss=7.534896

Batch 16590, train_perplexity=1928.7633, train_loss=7.5646343

Batch 16600, train_perplexity=1934.0552, train_loss=7.567374

Batch 16610, train_perplexity=2213.9155, train_loss=7.702518

Batch 16620, train_perplexity=2023.1935, train_loss=7.6124325

Batch 16630, train_perplexity=2181.3064, train_loss=7.6876793

Batch 16640, train_perplexity=2110.8616, train_loss=7.6548514

Batch 16650, train_perplexity=1999.0087, train_loss=7.6004066

Batch 16660, train_perplexity=2215.187, train_loss=7.703092

Batch 16670, train_perplexity=2001.5573, train_loss=7.6016808

Batch 16680, train_perplexity=1933.3342, train_loss=7.5670013

Batch 16690, train_perplexity=2091.324, train_loss=7.6455526

Batch 16700, train_perplexity=1952.5173, train_loss=7.5768747

Batch 16710, train_perplexity=2034.3397, train_loss=7.6179266

Batch 16720, train_perplexity=1933.6532, train_loss=7.5671663

Batch 16730, train_perplexity=2018.6874, train_loss=7.610203

Batch 16740, train_perplexity=1989.2744, train_loss=7.5955253

Batch 16750, train_perplexity=2080.4905, train_loss=7.640359

Batch 16760, train_perplexity=1996.45, train_loss=7.599126

Batch 16770, train_perplexity=1946.6216, train_loss=7.5738506

Batch 16780, train_perplexity=2186.9075, train_loss=7.6902437

Batch 16790, train_perplexity=1933.4125, train_loss=7.567042

Batch 16800, train_perplexity=1991.7745, train_loss=7.5967813

Batch 16810, train_perplexity=1973.4917, train_loss=7.5875597

Batch 16820, train_perplexity=1897.9663, train_loss=7.548538

Batch 16830, train_perplexity=1933.6265, train_loss=7.5671525

Batch 16840, train_perplexity=2246.4636, train_loss=7.7171125

Batch 16850, train_perplexity=1881.7815, train_loss=7.539974

Batch 16860, train_perplexity=1990.247, train_loss=7.596014

Batch 16870, train_perplexity=2218.8057, train_loss=7.7047243

Batch 16880, train_perplexity=1953.4597, train_loss=7.5773573

Batch 16890, train_perplexity=1992.0063, train_loss=7.5968976

Batch 16900, train_perplexity=1975.0526, train_loss=7.5883503

Batch 16910, train_perplexity=2175.4119, train_loss=7.6849732

Batch 16920, train_perplexity=1939.6558, train_loss=7.570266

Batch 16930, train_perplexity=1931.2251, train_loss=7.56591

Batch 16940, train_perplexity=1977.2905, train_loss=7.589483

Batch 16950, train_perplexity=1899.5815, train_loss=7.549389

Batch 16960, train_perplexity=2113.1667, train_loss=7.655943

Batch 16970, train_perplexity=2025.319, train_loss=7.6134825

Batch 16980, train_perplexity=2006.5972, train_loss=7.6041956

Batch 16990, train_perplexity=2002.6322, train_loss=7.6022177

Batch 17000, train_perplexity=1997.7269, train_loss=7.5997653

Batch 17010, train_perplexity=1913.9908, train_loss=7.556946

Batch 17020, train_perplexity=1927.6858, train_loss=7.5640755

Batch 17030, train_perplexity=2008.7789, train_loss=7.6052823

Batch 17040, train_perplexity=1865.5889, train_loss=7.531332

Batch 17050, train_perplexity=1896.3922, train_loss=7.5477085

Batch 17060, train_perplexity=1874.8741, train_loss=7.536297

Batch 17070, train_perplexity=2126.4304, train_loss=7.6622

Batch 17080, train_perplexity=1995.6238, train_loss=7.598712

Batch 17090, train_perplexity=2067.3083, train_loss=7.6340027

Batch 17100, train_perplexity=2048.3057, train_loss=7.6247683

Batch 17110, train_perplexity=1856.3901, train_loss=7.526389

Batch 17120, train_perplexity=1871.7861, train_loss=7.5346484

Batch 17130, train_perplexity=1865.2152, train_loss=7.5311317

Batch 17140, train_perplexity=2069.2532, train_loss=7.634943

Batch 17150, train_perplexity=1891.2809, train_loss=7.5450096

Batch 17160, train_perplexity=1907.1781, train_loss=7.55338

Batch 17170, train_perplexity=2022.2203, train_loss=7.6119514

Batch 17180, train_perplexity=2021.2698, train_loss=7.611481

Batch 17190, train_perplexity=1962.6676, train_loss=7.58206

Batch 17200, train_perplexity=1816.5735, train_loss=7.5047073

Batch 17210, train_perplexity=1942.4982, train_loss=7.57173

Batch 17220, train_perplexity=1800.9987, train_loss=7.4960966

Batch 17230, train_perplexity=1915.5851, train_loss=7.5577784

Batch 17240, train_perplexity=1986.1221, train_loss=7.5939393

Batch 17250, train_perplexity=1796.5386, train_loss=7.493617

Batch 17260, train_perplexity=1806.8893, train_loss=7.499362

Batch 17270, train_perplexity=1987.5536, train_loss=7.59466

Batch 17280, train_perplexity=1992.4158, train_loss=7.597103

Batch 17290, train_perplexity=2147.302, train_loss=7.6719675

Batch 17300, train_perplexity=1903.0829, train_loss=7.5512304

Batch 17310, train_perplexity=1944.5695, train_loss=7.572796

Batch 17320, train_perplexity=1829.9082, train_loss=7.512021

Batch 17330, train_perplexity=1700.9431, train_loss=7.438938

Batch 17340, train_perplexity=1890.502, train_loss=7.5445976

Batch 17350, train_perplexity=1972.0111, train_loss=7.586809

Batch 17360, train_perplexity=1871.1106, train_loss=7.5342875

Batch 17370, train_perplexity=1799.0948, train_loss=7.495039

Batch 17380, train_perplexity=1951.3445, train_loss=7.576274

Batch 17390, train_perplexity=1968.2703, train_loss=7.5849104

Batch 17400, train_perplexity=2053.528, train_loss=7.6273146

Batch 17410, train_perplexity=2042.188, train_loss=7.621777

Batch 17420, train_perplexity=1852.3793, train_loss=7.524226

Batch 17430, train_perplexity=1718.5529, train_loss=7.449238

Batch 17440, train_perplexity=1828.8928, train_loss=7.511466

Batch 17450, train_perplexity=2003.2377, train_loss=7.60252

Batch 17460, train_perplexity=1902.3135, train_loss=7.550826

Batch 17470, train_perplexity=2008.8134, train_loss=7.6052995

Batch 17480, train_perplexity=1909.1671, train_loss=7.5544224

Batch 17490, train_perplexity=1974.1345, train_loss=7.5878854

Batch 17500, train_perplexity=1940.0664, train_loss=7.5704775

Batch 17510, train_perplexity=1748.7236, train_loss=7.4666414

Batch 17520, train_perplexity=1928.3854, train_loss=7.5644383

Batch 17530, train_perplexity=1796.1736, train_loss=7.493414

Batch 17540, train_perplexity=2058.8755, train_loss=7.629915

Batch 17550, train_perplexity=1884.7395, train_loss=7.541545

Batch 17560, train_perplexity=1902.4932, train_loss=7.5509205

Batch 17570, train_perplexity=1851.9509, train_loss=7.523995

Batch 17580, train_perplexity=2079.825, train_loss=7.640039

Batch 17590, train_perplexity=1975.3756, train_loss=7.588514

Batch 17600, train_perplexity=1857.6901, train_loss=7.527089

Batch 17610, train_perplexity=2059.722, train_loss=7.6303263

Batch 17620, train_perplexity=1884.957, train_loss=7.5416603

Batch 17630, train_perplexity=2023.5032, train_loss=7.6125855

Batch 17640, train_perplexity=1941.0046, train_loss=7.570961

Batch 17650, train_perplexity=1926.3138, train_loss=7.5633636
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17660, train_perplexity=1959.9247, train_loss=7.5806613

Batch 17670, train_perplexity=2067.0105, train_loss=7.6338587

Batch 17680, train_perplexity=2039.6373, train_loss=7.6205273

Batch 17690, train_perplexity=2009.071, train_loss=7.6054277

Batch 17700, train_perplexity=1861.1897, train_loss=7.528971

Batch 17710, train_perplexity=1756.1096, train_loss=7.470856

Batch 17720, train_perplexity=2018.2812, train_loss=7.6100016

Batch 17730, train_perplexity=1957.8295, train_loss=7.5795918

Batch 17740, train_perplexity=2031.1731, train_loss=7.616369

Batch 17750, train_perplexity=1822.7349, train_loss=7.5080934

Batch 17760, train_perplexity=2034.5134, train_loss=7.618012

Batch 17770, train_perplexity=1960.0938, train_loss=7.5807476

Batch 17780, train_perplexity=2047.2375, train_loss=7.6242466

Batch 17790, train_perplexity=1888.8097, train_loss=7.543702

Batch 17800, train_perplexity=2267.1824, train_loss=7.726293

Batch 17810, train_perplexity=1909.0615, train_loss=7.554367

Batch 17820, train_perplexity=1917.3479, train_loss=7.558698

Batch 17830, train_perplexity=1966.8273, train_loss=7.584177

Batch 17840, train_perplexity=2284.4946, train_loss=7.7339

Batch 17850, train_perplexity=1779.0533, train_loss=7.4838367

Batch 17860, train_perplexity=1957.0669, train_loss=7.579202

Batch 17870, train_perplexity=1805.117, train_loss=7.4983807

Batch 17880, train_perplexity=1855.3654, train_loss=7.525837

Batch 17890, train_perplexity=2005.8472, train_loss=7.6038218

Batch 17900, train_perplexity=1805.327, train_loss=7.498497

Batch 17910, train_perplexity=1974.1431, train_loss=7.5878897

Batch 17920, train_perplexity=2096.7778, train_loss=7.648157

Batch 17930, train_perplexity=1931.3826, train_loss=7.5659914

Batch 17940, train_perplexity=1929.2085, train_loss=7.564865

Batch 17950, train_perplexity=2004.4062, train_loss=7.603103

Batch 17960, train_perplexity=1974.932, train_loss=7.5882893

Batch 17970, train_perplexity=1981.4822, train_loss=7.5916004

Batch 17980, train_perplexity=1906.2689, train_loss=7.552903

Batch 17990, train_perplexity=1688.1859, train_loss=7.43141

Batch 18000, train_perplexity=1835.8723, train_loss=7.515275

Batch 18010, train_perplexity=1831.8323, train_loss=7.513072

Batch 18020, train_perplexity=1912.5513, train_loss=7.5561934

Batch 18030, train_perplexity=1953.933, train_loss=7.5775995

Batch 18040, train_perplexity=1999.9563, train_loss=7.6008806

Batch 18050, train_perplexity=1850.9136, train_loss=7.5234346

Batch 18060, train_perplexity=1978.41, train_loss=7.590049

Batch 18070, train_perplexity=1739.2019, train_loss=7.4611816

Batch 18080, train_perplexity=1883.1854, train_loss=7.54072

Batch 18090, train_perplexity=1935.1216, train_loss=7.5679255

Batch 18100, train_perplexity=1941.0288, train_loss=7.5709734

Batch 18110, train_perplexity=1983.6395, train_loss=7.5926886

Batch 18120, train_perplexity=1856.1361, train_loss=7.5262523

Batch 18130, train_perplexity=2057.588, train_loss=7.6292896

Batch 18140, train_perplexity=1998.7694, train_loss=7.600287

Batch 18150, train_perplexity=1863.9777, train_loss=7.530468

Batch 18160, train_perplexity=1760.1252, train_loss=7.4731402

Batch 18170, train_perplexity=1892.7749, train_loss=7.5457993

Batch 18180, train_perplexity=1856.7832, train_loss=7.526601

Batch 18190, train_perplexity=1947.3309, train_loss=7.574215

Batch 18200, train_perplexity=1980.6971, train_loss=7.591204

Batch 18210, train_perplexity=2078.7827, train_loss=7.639538

Batch 18220, train_perplexity=2015.796, train_loss=7.6087694

Batch 18230, train_perplexity=1870.8242, train_loss=7.5341344

Batch 18240, train_perplexity=1901.568, train_loss=7.550434

Batch 18250, train_perplexity=1903.8218, train_loss=7.5516186

Batch 18260, train_perplexity=2062.2388, train_loss=7.6315475

Batch 18270, train_perplexity=2023.535, train_loss=7.6126013

Batch 18280, train_perplexity=1801.3483, train_loss=7.4962907

Batch 18290, train_perplexity=2005.3918, train_loss=7.603595

Batch 18300, train_perplexity=1651.8069, train_loss=7.409625

Batch 18310, train_perplexity=2179.033, train_loss=7.6866364

Batch 18320, train_perplexity=2041.5814, train_loss=7.62148

Batch 18330, train_perplexity=1772.4834, train_loss=7.480137

Batch 18340, train_perplexity=2030.3586, train_loss=7.6159678

Batch 18350, train_perplexity=1798.9285, train_loss=7.4949465

Batch 18360, train_perplexity=2199.432, train_loss=7.6959543

Batch 18370, train_perplexity=1910.7883, train_loss=7.555271

Batch 18380, train_perplexity=1825.3259, train_loss=7.509514

Batch 18390, train_perplexity=2166.4668, train_loss=7.680853

Batch 18400, train_perplexity=2080.8794, train_loss=7.640546

Batch 18410, train_perplexity=1803.9691, train_loss=7.4977446

Batch 18420, train_perplexity=1884.0585, train_loss=7.5411835

Batch 18430, train_perplexity=1729.3495, train_loss=7.4555006

Batch 18440, train_perplexity=1735.8464, train_loss=7.4592505

Batch 18450, train_perplexity=1859.2622, train_loss=7.527935

Batch 18460, train_perplexity=1942.075, train_loss=7.571512

Batch 18470, train_perplexity=1771.1967, train_loss=7.4794106

Batch 18480, train_perplexity=1894.8881, train_loss=7.546915

Batch 18490, train_perplexity=2047.4806, train_loss=7.6243653

Batch 18500, train_perplexity=1947.355, train_loss=7.5742273

Batch 18510, train_perplexity=1856.5283, train_loss=7.5264635

Batch 18520, train_perplexity=1861.0095, train_loss=7.5288744

Batch 18530, train_perplexity=1830.0757, train_loss=7.5121126

Batch 18540, train_perplexity=2082.7515, train_loss=7.641445

Batch 18550, train_perplexity=2143.0488, train_loss=7.669985

Batch 18560, train_perplexity=1816.6221, train_loss=7.504734

Batch 18570, train_perplexity=1892.4185, train_loss=7.545611

Batch 18580, train_perplexity=1857.102, train_loss=7.5267725

Batch 18590, train_perplexity=1909.6141, train_loss=7.5546565

Batch 18600, train_perplexity=2008.6975, train_loss=7.605242

Batch 18610, train_perplexity=2016.9844, train_loss=7.609359

Batch 18620, train_perplexity=1883.7314, train_loss=7.54101

Batch 18630, train_perplexity=1949.1526, train_loss=7.57515

Batch 18640, train_perplexity=1957.1165, train_loss=7.5792274

Batch 18650, train_perplexity=1826.8872, train_loss=7.510369

Batch 18660, train_perplexity=1843.5024, train_loss=7.5194225

Batch 18670, train_perplexity=1854.2245, train_loss=7.525222

Batch 18680, train_perplexity=1970.338, train_loss=7.5859604

Batch 18690, train_perplexity=1916.6184, train_loss=7.5583177

Batch 18700, train_perplexity=1922.8862, train_loss=7.5615826

Batch 18710, train_perplexity=1812.7472, train_loss=7.502599

Batch 18720, train_perplexity=1912.4619, train_loss=7.5561466

Batch 18730, train_perplexity=1844.3184, train_loss=7.519865

Batch 18740, train_perplexity=2030.9688, train_loss=7.616268

Batch 18750, train_perplexity=1946.6792, train_loss=7.57388

Batch 18760, train_perplexity=1984.3661, train_loss=7.593055

Batch 18770, train_perplexity=2096.0461, train_loss=7.647808

Batch 18780, train_perplexity=2117.6594, train_loss=7.6580667

Batch 18790, train_perplexity=1836.7654, train_loss=7.5157614

Batch 18800, train_perplexity=1946.0889, train_loss=7.573577

Batch 18810, train_perplexity=1872.0513, train_loss=7.53479

Batch 18820, train_perplexity=1759.8273, train_loss=7.472971

Batch 18830, train_perplexity=1856.5885, train_loss=7.526496

Batch 18840, train_perplexity=2077.7957, train_loss=7.639063

Batch 18850, train_perplexity=1937.5758, train_loss=7.569193

Batch 18860, train_perplexity=1698.7983, train_loss=7.4376764

Batch 18870, train_perplexity=1824.963, train_loss=7.509315

Batch 18880, train_perplexity=1886.2319, train_loss=7.5423365

Batch 18890, train_perplexity=1974.5197, train_loss=7.5880804

Batch 18900, train_perplexity=1957.9136, train_loss=7.5796347

Batch 18910, train_perplexity=1858.3511, train_loss=7.527445

Batch 18920, train_perplexity=1869.9493, train_loss=7.5336666

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100
Loaded 306074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306074 sentences.
Finished loading
Batch 18930, train_perplexity=2048.9055, train_loss=7.625061

Batch 18940, train_perplexity=1911.3569, train_loss=7.5555687

Batch 18950, train_perplexity=1917.1384, train_loss=7.558589

Batch 18960, train_perplexity=1783.5576, train_loss=7.4863653

Batch 18970, train_perplexity=1933.4355, train_loss=7.567054

Batch 18980, train_perplexity=1841.9366, train_loss=7.518573

Batch 18990, train_perplexity=1893.6488, train_loss=7.546261

Batch 19000, train_perplexity=1823.3912, train_loss=7.5084534

Batch 19010, train_perplexity=1954.67, train_loss=7.5779767

Batch 19020, train_perplexity=1866.1289, train_loss=7.5316215

Batch 19030, train_perplexity=1861.1552, train_loss=7.5289526

Batch 19040, train_perplexity=1802.7385, train_loss=7.497062

Batch 19050, train_perplexity=1834.6838, train_loss=7.5146275

Batch 19060, train_perplexity=1883.6847, train_loss=7.540985

Batch 19070, train_perplexity=1955.1986, train_loss=7.578247

Batch 19080, train_perplexity=1867.8052, train_loss=7.5325193

Batch 19090, train_perplexity=2022.8558, train_loss=7.6122656

Batch 19100, train_perplexity=1940.8899, train_loss=7.570902

Batch 19110, train_perplexity=1893.7355, train_loss=7.5463066

Batch 19120, train_perplexity=1825.4608, train_loss=7.509588

Batch 19130, train_perplexity=1852.3864, train_loss=7.52423

Batch 19140, train_perplexity=2099.7253, train_loss=7.649562

Batch 19150, train_perplexity=1700.1046, train_loss=7.438445

Batch 19160, train_perplexity=1917.3158, train_loss=7.5586815

Batch 19170, train_perplexity=1958.9203, train_loss=7.5801487

Batch 19180, train_perplexity=1958.9464, train_loss=7.580162

Batch 19190, train_perplexity=1796.7253, train_loss=7.493721

Batch 19200, train_perplexity=1987.8976, train_loss=7.594833

Batch 19210, train_perplexity=1894.0065, train_loss=7.5464497

Batch 19220, train_perplexity=2031.6554, train_loss=7.616606

Batch 19230, train_perplexity=1910.0212, train_loss=7.5548697

Batch 19240, train_perplexity=1918.148, train_loss=7.5591154

Batch 19250, train_perplexity=1926.3396, train_loss=7.563377

Batch 19260, train_perplexity=1857.3774, train_loss=7.526921

Batch 19270, train_perplexity=1891.9989, train_loss=7.545389

Batch 19280, train_perplexity=1965.7716, train_loss=7.58364

Batch 19290, train_perplexity=1926.1403, train_loss=7.5632734

Batch 19300, train_perplexity=1713.2827, train_loss=7.4461665

Batch 19310, train_perplexity=1902.2528, train_loss=7.550794

Batch 19320, train_perplexity=1986.788, train_loss=7.5942745

Batch 19330, train_perplexity=1894.6089, train_loss=7.5467677

Batch 19340, train_perplexity=1785.9797, train_loss=7.4877224

Batch 19350, train_perplexity=1814.43, train_loss=7.5035267

Batch 19360, train_perplexity=2025.4078, train_loss=7.6135263

Batch 19370, train_perplexity=1840.1774, train_loss=7.517617

Batch 19380, train_perplexity=2054.1135, train_loss=7.6275997

Batch 19390, train_perplexity=1785.9763, train_loss=7.4877205

Batch 19400, train_perplexity=1873.0638, train_loss=7.535331

Batch 19410, train_perplexity=1806.4137, train_loss=7.499099

Batch 19420, train_perplexity=2009.6326, train_loss=7.605707

Batch 19430, train_perplexity=1835.916, train_loss=7.515299

Batch 19440, train_perplexity=1969.8617, train_loss=7.5857186

Batch 19450, train_perplexity=1906.1262, train_loss=7.5528283

Batch 19460, train_perplexity=1814.5217, train_loss=7.503577

Batch 19470, train_perplexity=2079.5264, train_loss=7.6398954

Batch 19480, train_perplexity=1705.7979, train_loss=7.441788

Batch 19490, train_perplexity=1853.2018, train_loss=7.52467

Batch 19500, train_perplexity=1837.5012, train_loss=7.516162

Batch 19510, train_perplexity=1708.7749, train_loss=7.443532

Batch 19520, train_perplexity=1807.502, train_loss=7.499701

Batch 19530, train_perplexity=1807.7562, train_loss=7.4998417

Batch 19540, train_perplexity=1944.5083, train_loss=7.5727644

Batch 19550, train_perplexity=1990.9332, train_loss=7.596359

Batch 19560, train_perplexity=1962.4506, train_loss=7.581949

Batch 19570, train_perplexity=1880.5598, train_loss=7.5393248

Batch 19580, train_perplexity=1782.574, train_loss=7.4858136

Batch 19590, train_perplexity=1842.6798, train_loss=7.518976

Batch 19600, train_perplexity=1864.1545, train_loss=7.530563

Batch 19610, train_perplexity=1901.0894, train_loss=7.5501823

Batch 19620, train_perplexity=2029.0085, train_loss=7.6153026

Batch 19630, train_perplexity=1779.4843, train_loss=7.484079

Batch 19640, train_perplexity=1880.5966, train_loss=7.5393443

Batch 19650, train_perplexity=2015.7191, train_loss=7.6087313

Batch 19660, train_perplexity=1744.2366, train_loss=7.464072

Batch 19670, train_perplexity=1751.1501, train_loss=7.468028

Batch 19680, train_perplexity=1970.0956, train_loss=7.5858374

Batch 19690, train_perplexity=1960.29, train_loss=7.5808477

Batch 19700, train_perplexity=1744.5277, train_loss=7.464239

Batch 19710, train_perplexity=2043.0227, train_loss=7.6221857

Batch 19720, train_perplexity=1867.1979, train_loss=7.532194

Batch 19730, train_perplexity=1900.0309, train_loss=7.5496254

Batch 19740, train_perplexity=1886.4137, train_loss=7.542433

Batch 19750, train_perplexity=1833.533, train_loss=7.514

Batch 19760, train_perplexity=1798.5459, train_loss=7.494734

Batch 19770, train_perplexity=1870.3595, train_loss=7.533886

Batch 19780, train_perplexity=1883.6667, train_loss=7.5409756

Batch 19790, train_perplexity=1888.3855, train_loss=7.5434775

Batch 19800, train_perplexity=1667.3672, train_loss=7.419001

Batch 19810, train_perplexity=1861.3025, train_loss=7.5290318

Batch 19820, train_perplexity=2109.0676, train_loss=7.654001

Batch 19830, train_perplexity=1925.2147, train_loss=7.562793

Batch 19840, train_perplexity=2056.5461, train_loss=7.628783

Batch 19850, train_perplexity=1863.2649, train_loss=7.5300856

Batch 19860, train_perplexity=1745.0153, train_loss=7.4645185

Batch 19870, train_perplexity=1966.3566, train_loss=7.5839376

Batch 19880, train_perplexity=1883.3318, train_loss=7.5407977

Batch 19890, train_perplexity=1878.7278, train_loss=7.53835

Batch 19900, train_perplexity=1734.3448, train_loss=7.458385

Batch 19910, train_perplexity=1810.3527, train_loss=7.501277

Batch 19920, train_perplexity=2048.0098, train_loss=7.624624

Batch 19930, train_perplexity=1712.9902, train_loss=7.445996

Batch 19940, train_perplexity=1837.6432, train_loss=7.516239

Batch 19950, train_perplexity=1833.2479, train_loss=7.5138445

Batch 19960, train_perplexity=1795.3524, train_loss=7.4929566

Batch 19970, train_perplexity=1832.0, train_loss=7.5131636

Batch 19980, train_perplexity=1883.1943, train_loss=7.5407248

Batch 19990, train_perplexity=1851.1166, train_loss=7.5235443

Batch 20000, train_perplexity=1954.6318, train_loss=7.577957

Batch 20010, train_perplexity=1836.7794, train_loss=7.515769

Batch 20020, train_perplexity=1862.7035, train_loss=7.529784

Batch 20030, train_perplexity=1770.4907, train_loss=7.479012

Batch 20040, train_perplexity=1873.7427, train_loss=7.535693

Batch 20050, train_perplexity=1695.7278, train_loss=7.4358673

Batch 20060, train_perplexity=1908.631, train_loss=7.5541415

Batch 20070, train_perplexity=1950.9388, train_loss=7.576066

Batch 20080, train_perplexity=1555.1423, train_loss=7.3493223

Batch 20090, train_perplexity=1766.5449, train_loss=7.476781

Batch 20100, train_perplexity=1747.59, train_loss=7.465993

Batch 20110, train_perplexity=1802.4703, train_loss=7.4969134

Batch 20120, train_perplexity=1813.0376, train_loss=7.502759

Batch 20130, train_perplexity=1939.0667, train_loss=7.569962

Batch 20140, train_perplexity=1831.4034, train_loss=7.512838

Batch 20150, train_perplexity=1940.9113, train_loss=7.570913

Batch 20160, train_perplexity=1767.4751, train_loss=7.4773073

Batch 20170, train_perplexity=1799.9976, train_loss=7.4955406

Batch 20180, train_perplexity=1925.5912, train_loss=7.5629883

Batch 20190, train_perplexity=1875.0637, train_loss=7.536398

Batch 20200, train_perplexity=1886.6125, train_loss=7.542538

Batch 20210, train_perplexity=1808.9056, train_loss=7.5004773

Batch 20220, train_perplexity=1795.087, train_loss=7.492809

Batch 20230, train_perplexity=1859.2196, train_loss=7.527912
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 20240, train_perplexity=2016.1478, train_loss=7.608944

Batch 20250, train_perplexity=1705.2594, train_loss=7.4414725

Batch 20260, train_perplexity=1847.7848, train_loss=7.521743

Batch 20270, train_perplexity=1962.3026, train_loss=7.581874

Batch 20280, train_perplexity=2051.9648, train_loss=7.626553

Batch 20290, train_perplexity=1787.1996, train_loss=7.488405

Batch 20300, train_perplexity=1913.5382, train_loss=7.5567093

Batch 20310, train_perplexity=1764.9822, train_loss=7.475896

Batch 20320, train_perplexity=1815.8704, train_loss=7.50432

Batch 20330, train_perplexity=1865.3407, train_loss=7.531199

Batch 20340, train_perplexity=1840.7241, train_loss=7.5179143

Batch 20350, train_perplexity=2003.424, train_loss=7.602613

Batch 20360, train_perplexity=1824.8787, train_loss=7.5092688

Batch 20370, train_perplexity=1763.3217, train_loss=7.4749546

Batch 20380, train_perplexity=1831.807, train_loss=7.513058

Batch 20390, train_perplexity=1998.9419, train_loss=7.6003733

Batch 20400, train_perplexity=1909.5522, train_loss=7.554624

Batch 20410, train_perplexity=1934.8817, train_loss=7.5678015

Batch 20420, train_perplexity=1910.0568, train_loss=7.5548882

Batch 20430, train_perplexity=1799.4613, train_loss=7.4952426

Batch 20440, train_perplexity=1831.1223, train_loss=7.5126843

Batch 20450, train_perplexity=2007.2479, train_loss=7.60452

Batch 20460, train_perplexity=1897.6241, train_loss=7.548358

Batch 20470, train_perplexity=1913.5839, train_loss=7.556733

Batch 20480, train_perplexity=1903.1555, train_loss=7.5512686

Batch 20490, train_perplexity=1868.999, train_loss=7.5331583

Batch 20500, train_perplexity=1829.1117, train_loss=7.5115857

Batch 20510, train_perplexity=1847.5188, train_loss=7.521599

Batch 20520, train_perplexity=1797.9783, train_loss=7.494418

Batch 20530, train_perplexity=1758.1263, train_loss=7.472004

Batch 20540, train_perplexity=1712.599, train_loss=7.4457674

Batch 20550, train_perplexity=1977.4178, train_loss=7.589547

Batch 20560, train_perplexity=1749.2198, train_loss=7.466925

Batch 20570, train_perplexity=1991.1099, train_loss=7.5964475

Batch 20580, train_perplexity=1871.8656, train_loss=7.534691

Batch 20590, train_perplexity=2015.3779, train_loss=7.608562

Batch 20600, train_perplexity=1934.8218, train_loss=7.5677705

Batch 20610, train_perplexity=1854.7993, train_loss=7.525532

Batch 20620, train_perplexity=1928.961, train_loss=7.564737

Batch 20630, train_perplexity=1771.9949, train_loss=7.4798613

Batch 20640, train_perplexity=1729.0016, train_loss=7.4552994

Batch 20650, train_perplexity=1744.3297, train_loss=7.4641256

Batch 20660, train_perplexity=1778.7183, train_loss=7.4836483

Batch 20670, train_perplexity=1824.5636, train_loss=7.509096

Batch 20680, train_perplexity=1738.5635, train_loss=7.4608145

Batch 20690, train_perplexity=1902.8868, train_loss=7.5511274

Batch 20700, train_perplexity=1713.3783, train_loss=7.4462223

Batch 20710, train_perplexity=1810.5271, train_loss=7.5013733

Batch 20720, train_perplexity=1963.291, train_loss=7.5823774

Batch 20730, train_perplexity=1880.4127, train_loss=7.5392466

Batch 20740, train_perplexity=1872.7405, train_loss=7.535158

Batch 20750, train_perplexity=1884.416, train_loss=7.5413733

Batch 20760, train_perplexity=1757.558, train_loss=7.4716806

Batch 20770, train_perplexity=1827.1451, train_loss=7.51051

Batch 20780, train_perplexity=1863.3201, train_loss=7.530115

Batch 20790, train_perplexity=1875.4026, train_loss=7.5365787

Batch 20800, train_perplexity=1737.0852, train_loss=7.459964

Batch 20810, train_perplexity=1829.5994, train_loss=7.5118523

Batch 20820, train_perplexity=1790.2156, train_loss=7.4900913

Batch 20830, train_perplexity=1659.2131, train_loss=7.4140987

Batch 20840, train_perplexity=2003.6542, train_loss=7.602728

Batch 20850, train_perplexity=2024.3612, train_loss=7.6130095

Batch 20860, train_perplexity=1913.0675, train_loss=7.5564632

Batch 20870, train_perplexity=2010.6553, train_loss=7.606216

Batch 20880, train_perplexity=1767.9877, train_loss=7.477597

Batch 20890, train_perplexity=1736.6454, train_loss=7.4597106

Batch 20900, train_perplexity=1960.9482, train_loss=7.5811834

Batch 20910, train_perplexity=1960.5902, train_loss=7.581001

Batch 20920, train_perplexity=1780.7346, train_loss=7.4847813

Batch 20930, train_perplexity=2015.5845, train_loss=7.6086645

Batch 20940, train_perplexity=1805.7196, train_loss=7.4987144

Batch 20950, train_perplexity=1759.7837, train_loss=7.472946

Batch 20960, train_perplexity=1735.2059, train_loss=7.4588814

Batch 20970, train_perplexity=1838.4337, train_loss=7.5166693

Batch 20980, train_perplexity=1811.5892, train_loss=7.50196

Batch 20990, train_perplexity=1811.5496, train_loss=7.501938

Batch 21000, train_perplexity=1738.7559, train_loss=7.460925

Batch 21010, train_perplexity=1745.9033, train_loss=7.4650273

Batch 21020, train_perplexity=1973.9934, train_loss=7.587814

Batch 21030, train_perplexity=1898.876, train_loss=7.5490174

Batch 21040, train_perplexity=1765.3147, train_loss=7.476084

Batch 21050, train_perplexity=1881.8264, train_loss=7.539998

Batch 21060, train_perplexity=1888.192, train_loss=7.543375

Batch 21070, train_perplexity=1866.3745, train_loss=7.531753

Batch 21080, train_perplexity=2031.7543, train_loss=7.616655

Batch 21090, train_perplexity=1775.4829, train_loss=7.4818277

Batch 21100, train_perplexity=2009.6661, train_loss=7.605724

Batch 21110, train_perplexity=1871.837, train_loss=7.5346756

Batch 21120, train_perplexity=1800.7909, train_loss=7.495981

Batch 21130, train_perplexity=1951.5576, train_loss=7.576383

Batch 21140, train_perplexity=1811.7811, train_loss=7.5020657

Batch 21150, train_perplexity=1907.5602, train_loss=7.5535803

Batch 21160, train_perplexity=1900.1866, train_loss=7.5497074

Batch 21170, train_perplexity=1986.3503, train_loss=7.594054

Batch 21180, train_perplexity=1659.2891, train_loss=7.4141445

Batch 21190, train_perplexity=1817.5839, train_loss=7.5052633

Batch 21200, train_perplexity=1800.0397, train_loss=7.495564

Batch 21210, train_perplexity=1863.823, train_loss=7.530385

Batch 21220, train_perplexity=1810.0558, train_loss=7.501113

Batch 21230, train_perplexity=1793.4469, train_loss=7.4918947

Batch 21240, train_perplexity=1929.8986, train_loss=7.5652227

Batch 21250, train_perplexity=1740.9526, train_loss=7.462188

Batch 21260, train_perplexity=1822.5411, train_loss=7.507987

Batch 21270, train_perplexity=1815.427, train_loss=7.504076

Batch 21280, train_perplexity=1877.3845, train_loss=7.537635

Batch 21290, train_perplexity=1863.5608, train_loss=7.5302444

Batch 21300, train_perplexity=1782.9948, train_loss=7.4860497

Batch 21310, train_perplexity=1723.1151, train_loss=7.451889

Batch 21320, train_perplexity=1807.4847, train_loss=7.4996915

Batch 21330, train_perplexity=1920.6777, train_loss=7.5604334

Batch 21340, train_perplexity=1848.6855, train_loss=7.52223

Batch 21350, train_perplexity=1863.647, train_loss=7.5302906

Batch 21360, train_perplexity=1938.0674, train_loss=7.5694466

Batch 21370, train_perplexity=1761.6407, train_loss=7.474001

Batch 21380, train_perplexity=1732.6487, train_loss=7.4574065

Batch 21390, train_perplexity=1800.6346, train_loss=7.4958944

Batch 21400, train_perplexity=1700.5271, train_loss=7.4386935

Batch 21410, train_perplexity=1730.049, train_loss=7.455905

Batch 21420, train_perplexity=1925.1615, train_loss=7.562765

Batch 21430, train_perplexity=1806.1716, train_loss=7.498965

Batch 21440, train_perplexity=1605.0741, train_loss=7.380925

Batch 21450, train_perplexity=1857.8247, train_loss=7.5271616

Batch 21460, train_perplexity=1864.0007, train_loss=7.5304804

Batch 21470, train_perplexity=1978.0347, train_loss=7.589859

Batch 21480, train_perplexity=1819.8976, train_loss=7.5065355

Batch 21490, train_perplexity=1770.2737, train_loss=7.4788895

Batch 21500, train_perplexity=1852.8334, train_loss=7.5244713

Batch 21510, train_perplexity=1891.2574, train_loss=7.544997

Batch 21520, train_perplexity=1947.356, train_loss=7.574228

Batch 21530, train_perplexity=1810.5962, train_loss=7.5014114

Batch 21540, train_perplexity=1652.1945, train_loss=7.4098597

Batch 21550, train_perplexity=1863.806, train_loss=7.530376
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21560, train_perplexity=1696.1588, train_loss=7.4361215

Batch 21570, train_perplexity=1765.1101, train_loss=7.4759684

Batch 21580, train_perplexity=1762.4584, train_loss=7.474465

Batch 21590, train_perplexity=1996.5642, train_loss=7.599183

Batch 21600, train_perplexity=1649.6116, train_loss=7.408295

Batch 21610, train_perplexity=1769.8027, train_loss=7.4786234

Batch 21620, train_perplexity=1908.6483, train_loss=7.5541506

Batch 21630, train_perplexity=2047.5011, train_loss=7.6243753

Batch 21640, train_perplexity=1809.2101, train_loss=7.5006456

Batch 21650, train_perplexity=1812.6392, train_loss=7.502539

Batch 21660, train_perplexity=1804.6824, train_loss=7.49814

Batch 21670, train_perplexity=1825.7334, train_loss=7.509737

Batch 21680, train_perplexity=1692.2778, train_loss=7.4338307

Batch 21690, train_perplexity=1787.6122, train_loss=7.488636

Batch 21700, train_perplexity=1845.0405, train_loss=7.5202565

Batch 21710, train_perplexity=1936.798, train_loss=7.5687914

Batch 21720, train_perplexity=1815.775, train_loss=7.5042677

Batch 21730, train_perplexity=1836.317, train_loss=7.515517

Batch 21740, train_perplexity=1803.2742, train_loss=7.4973593

Batch 21750, train_perplexity=1653.8497, train_loss=7.410861

Batch 21760, train_perplexity=1818.5375, train_loss=7.505788

Batch 21770, train_perplexity=1740.0331, train_loss=7.4616594

Batch 21780, train_perplexity=1738.3496, train_loss=7.4606915

Batch 21790, train_perplexity=1678.9419, train_loss=7.425919

Batch 21800, train_perplexity=1797.8557, train_loss=7.49435

Batch 21810, train_perplexity=1823.5365, train_loss=7.508533

Batch 21820, train_perplexity=1877.2502, train_loss=7.5375633

Batch 21830, train_perplexity=1920.2327, train_loss=7.5602016

Batch 21840, train_perplexity=1822.0945, train_loss=7.507742

Batch 21850, train_perplexity=1743.8324, train_loss=7.4638405

Batch 21860, train_perplexity=1750.0057, train_loss=7.4673743

Batch 21870, train_perplexity=1706.477, train_loss=7.4421864

Batch 21880, train_perplexity=1828.061, train_loss=7.511011

Batch 21890, train_perplexity=1830.3707, train_loss=7.512274

Batch 21900, train_perplexity=1711.963, train_loss=7.445396

Batch 21910, train_perplexity=1741.3213, train_loss=7.4623995

Batch 21920, train_perplexity=1924.7126, train_loss=7.562532

Batch 21930, train_perplexity=1894.1003, train_loss=7.5464993

Batch 21940, train_perplexity=1788.1407, train_loss=7.4889317

Batch 21950, train_perplexity=1600.2687, train_loss=7.377927

Batch 21960, train_perplexity=1641.2572, train_loss=7.403218

Batch 21970, train_perplexity=1852.0472, train_loss=7.524047

Batch 21980, train_perplexity=1713.8694, train_loss=7.446509

Batch 21990, train_perplexity=1816.4246, train_loss=7.5046253

Batch 22000, train_perplexity=1733.5718, train_loss=7.457939

Batch 22010, train_perplexity=1852.0358, train_loss=7.5240407

Batch 22020, train_perplexity=1661.7753, train_loss=7.415642

Batch 22030, train_perplexity=1999.3538, train_loss=7.6005793

Batch 22040, train_perplexity=1661.6477, train_loss=7.415565

Batch 22050, train_perplexity=1922.7413, train_loss=7.561507

Batch 22060, train_perplexity=1854.5004, train_loss=7.5253706

Batch 22070, train_perplexity=1677.5623, train_loss=7.425097

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Batch 22080, train_perplexity=1780.6277, train_loss=7.484721

Batch 22090, train_perplexity=1876.8412, train_loss=7.5373454

Batch 22100, train_perplexity=1649.0146, train_loss=7.407933

Batch 22110, train_perplexity=1800.8312, train_loss=7.4960036

Batch 22120, train_perplexity=1758.6361, train_loss=7.472294

Batch 22130, train_perplexity=1811.8969, train_loss=7.5021296

Batch 22140, train_perplexity=1829.7563, train_loss=7.511938

Batch 22150, train_perplexity=1778.6792, train_loss=7.4836264

Batch 22160, train_perplexity=1876.5315, train_loss=7.5371804

Batch 22170, train_perplexity=1779.3265, train_loss=7.48399

Batch 22180, train_perplexity=1733.2229, train_loss=7.457738

Batch 22190, train_perplexity=1932.0127, train_loss=7.5663176

Batch 22200, train_perplexity=1667.5747, train_loss=7.4191256

Batch 22210, train_perplexity=1815.9673, train_loss=7.5043736

Batch 22220, train_perplexity=1717.1554, train_loss=7.4484243

Batch 22230, train_perplexity=1687.0698, train_loss=7.4307485

Batch 22240, train_perplexity=1599.8506, train_loss=7.3776655

Batch 22250, train_perplexity=1775.8148, train_loss=7.4820147

Batch 22260, train_perplexity=1708.4327, train_loss=7.4433317

Batch 22270, train_perplexity=1696.3384, train_loss=7.4362273

Batch 22280, train_perplexity=1841.1393, train_loss=7.51814

Batch 22290, train_perplexity=1724.3883, train_loss=7.4526277

Batch 22300, train_perplexity=1778.2426, train_loss=7.483381

Batch 22310, train_perplexity=1784.2585, train_loss=7.486758

Batch 22320, train_perplexity=1945.8457, train_loss=7.573452

Batch 22330, train_perplexity=1855.7609, train_loss=7.52605

Batch 22340, train_perplexity=1945.2975, train_loss=7.57317

Batch 22350, train_perplexity=1919.8628, train_loss=7.560009

Batch 22360, train_perplexity=1789.7751, train_loss=7.4898453

Batch 22370, train_perplexity=1850.2976, train_loss=7.523102

Batch 22380, train_perplexity=1725.0997, train_loss=7.45304

Batch 22390, train_perplexity=1803.632, train_loss=7.4975576

Batch 22400, train_perplexity=1771.8682, train_loss=7.4797897

Batch 22410, train_perplexity=1599.7743, train_loss=7.377618

Batch 22420, train_perplexity=1826.7766, train_loss=7.5103083

Batch 22430, train_perplexity=1814.0962, train_loss=7.5033426

Batch 22440, train_perplexity=1782.0002, train_loss=7.4854918

Batch 22450, train_perplexity=1766.0665, train_loss=7.47651

Batch 22460, train_perplexity=1921.3354, train_loss=7.5607758

Batch 22470, train_perplexity=1656.2378, train_loss=7.412304

Batch 22480, train_perplexity=1740.3169, train_loss=7.4618225

Batch 22490, train_perplexity=1731.9259, train_loss=7.4569893

Batch 22500, train_perplexity=1802.583, train_loss=7.496976

Batch 22510, train_perplexity=1722.1228, train_loss=7.451313

Batch 22520, train_perplexity=1789.5455, train_loss=7.489717

Batch 22530, train_perplexity=1680.3619, train_loss=7.4267645

Batch 22540, train_perplexity=1720.0137, train_loss=7.4500875

Batch 22550, train_perplexity=1663.7257, train_loss=7.416815

Batch 22560, train_perplexity=1661.2999, train_loss=7.4153557

Batch 22570, train_perplexity=1788.3505, train_loss=7.489049

Batch 22580, train_perplexity=1844.4265, train_loss=7.5199237

Batch 22590, train_perplexity=1781.2798, train_loss=7.4850874

Batch 22600, train_perplexity=1810.5417, train_loss=7.5013814

Batch 22610, train_perplexity=1784.3113, train_loss=7.486788

Batch 22620, train_perplexity=1752.6371, train_loss=7.468877

Batch 22630, train_perplexity=1996.1587, train_loss=7.59898

Batch 22640, train_perplexity=1727.0338, train_loss=7.4541607

Batch 22650, train_perplexity=1806.8564, train_loss=7.499344

Batch 22660, train_perplexity=1790.0414, train_loss=7.489994

Batch 22670, train_perplexity=1896.1589, train_loss=7.5475855

Batch 22680, train_perplexity=1762.3868, train_loss=7.4744244

Batch 22690, train_perplexity=1823.9513, train_loss=7.5087605

Batch 22700, train_perplexity=1682.124, train_loss=7.4278126

Batch 22710, train_perplexity=1804.863, train_loss=7.49824

Batch 22720, train_perplexity=1815.8444, train_loss=7.504306

Batch 22730, train_perplexity=1658.99, train_loss=7.4139643

Batch 22740, train_perplexity=1700.9788, train_loss=7.438959

Batch 22750, train_perplexity=1682.6198, train_loss=7.4281073

Batch 22760, train_perplexity=1880.1599, train_loss=7.539112

Batch 22770, train_perplexity=1807.82, train_loss=7.499877

Batch 22780, train_perplexity=1836.9879, train_loss=7.5158825

Batch 22790, train_perplexity=1833.9247, train_loss=7.5142136

Batch 22800, train_perplexity=1781.5347, train_loss=7.4852304

Batch 22810, train_perplexity=1762.8694, train_loss=7.474698
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22820, train_perplexity=1639.7568, train_loss=7.402303

Batch 22830, train_perplexity=1811.693, train_loss=7.502017

Batch 22840, train_perplexity=1841.9963, train_loss=7.518605

Batch 22850, train_perplexity=1804.0009, train_loss=7.497762

Batch 22860, train_perplexity=1685.6987, train_loss=7.4299355

Batch 22870, train_perplexity=1940.8973, train_loss=7.5709057

Batch 22880, train_perplexity=1906.0272, train_loss=7.5527763

Batch 22890, train_perplexity=1865.0988, train_loss=7.5310693

Batch 22900, train_perplexity=1671.6414, train_loss=7.4215612

Batch 22910, train_perplexity=1716.4758, train_loss=7.4480286

Batch 22920, train_perplexity=1611.7019, train_loss=7.385046

Batch 22930, train_perplexity=1804.561, train_loss=7.4980726

Batch 22940, train_perplexity=1735.7654, train_loss=7.4592037

Batch 22950, train_perplexity=1670.0908, train_loss=7.4206333

Batch 22960, train_perplexity=1644.0621, train_loss=7.4049253

Batch 22970, train_perplexity=1734.707, train_loss=7.458594

Batch 22980, train_perplexity=1746.651, train_loss=7.4654555

Batch 22990, train_perplexity=1710.5774, train_loss=7.4445863

Batch 23000, train_perplexity=1754.6942, train_loss=7.47005

Batch 23010, train_perplexity=1822.8297, train_loss=7.5081453

Batch 23020, train_perplexity=1759.396, train_loss=7.472726

Batch 23030, train_perplexity=1810.8171, train_loss=7.5015335

Batch 23040, train_perplexity=1763.0148, train_loss=7.4747806

Batch 23050, train_perplexity=1971.024, train_loss=7.5863085

Batch 23060, train_perplexity=1811.6221, train_loss=7.501978

Batch 23070, train_perplexity=1688.7954, train_loss=7.431771

Batch 23080, train_perplexity=1723.733, train_loss=7.4522476

Batch 23090, train_perplexity=1656.0277, train_loss=7.412177

Batch 23100, train_perplexity=1624.7781, train_loss=7.3931265

Batch 23110, train_perplexity=1779.2222, train_loss=7.4839315

Batch 23120, train_perplexity=1762.5214, train_loss=7.4745007

Batch 23130, train_perplexity=1727.1903, train_loss=7.4542513

Batch 23140, train_perplexity=1737.7827, train_loss=7.4603653

Batch 23150, train_perplexity=1763.4545, train_loss=7.47503

Batch 23160, train_perplexity=1695.3947, train_loss=7.435671

Batch 23170, train_perplexity=1730.284, train_loss=7.456041

Batch 23180, train_perplexity=1861.251, train_loss=7.529004

Batch 23190, train_perplexity=1737.39, train_loss=7.4601393

Batch 23200, train_perplexity=1707.2675, train_loss=7.4426494

Batch 23210, train_perplexity=1692.34, train_loss=7.4338675

Batch 23220, train_perplexity=1719.2528, train_loss=7.449645

Batch 23230, train_perplexity=1838.0621, train_loss=7.516467

Batch 23240, train_perplexity=1730.5869, train_loss=7.456216

Batch 23250, train_perplexity=1769.4028, train_loss=7.4783974

Batch 23260, train_perplexity=1697.3102, train_loss=7.4368

Batch 23270, train_perplexity=1651.7029, train_loss=7.409562

Batch 23280, train_perplexity=1754.9318, train_loss=7.4701853

Batch 23290, train_perplexity=1810.2543, train_loss=7.5012226

Batch 23300, train_perplexity=1716.2107, train_loss=7.447874

Batch 23310, train_perplexity=1905.6946, train_loss=7.552602

Batch 23320, train_perplexity=1619.1476, train_loss=7.389655

Batch 23330, train_perplexity=1721.5219, train_loss=7.450964

Batch 23340, train_perplexity=1668.7933, train_loss=7.419856

Batch 23350, train_perplexity=1896.8643, train_loss=7.5479574

Batch 23360, train_perplexity=1638.6008, train_loss=7.401598

Batch 23370, train_perplexity=1813.9716, train_loss=7.503274

Batch 23380, train_perplexity=1659.392, train_loss=7.4142065

Batch 23390, train_perplexity=1815.0565, train_loss=7.503872

Batch 23400, train_perplexity=1714.484, train_loss=7.4468675

Batch 23410, train_perplexity=1800.8107, train_loss=7.495992

Batch 23420, train_perplexity=1672.681, train_loss=7.422183

Batch 23430, train_perplexity=1594.3522, train_loss=7.3742228

Batch 23440, train_perplexity=1655.3583, train_loss=7.4117727

Batch 23450, train_perplexity=1598.6862, train_loss=7.3769374

Batch 23460, train_perplexity=1465.9502, train_loss=7.290259

Batch 23470, train_perplexity=1751.4156, train_loss=7.4681797

Batch 23480, train_perplexity=1614.956, train_loss=7.387063

Batch 23490, train_perplexity=1832.3215, train_loss=7.513339

Batch 23500, train_perplexity=1624.0623, train_loss=7.392686

Batch 23510, train_perplexity=1671.0643, train_loss=7.421216

Batch 23520, train_perplexity=1770.586, train_loss=7.479066

Batch 23530, train_perplexity=1628.9111, train_loss=7.395667

Batch 23540, train_perplexity=1673.8795, train_loss=7.4228992

Batch 23550, train_perplexity=1788.139, train_loss=7.4889307

Batch 23560, train_perplexity=1699.2067, train_loss=7.4379168

Batch 23570, train_perplexity=1665.9462, train_loss=7.4181485

Batch 23580, train_perplexity=1861.1995, train_loss=7.5289764

Batch 23590, train_perplexity=1688.2712, train_loss=7.4314604

Batch 23600, train_perplexity=1753.3794, train_loss=7.4693003

Batch 23610, train_perplexity=1675.6652, train_loss=7.4239655

Batch 23620, train_perplexity=1539.4823, train_loss=7.3392015

Batch 23630, train_perplexity=1671.7281, train_loss=7.421613

Batch 23640, train_perplexity=1786.1296, train_loss=7.4878063

Batch 23650, train_perplexity=1838.8003, train_loss=7.5168686

Batch 23660, train_perplexity=1743.3027, train_loss=7.4635367

Batch 23670, train_perplexity=1683.9988, train_loss=7.4289265

Batch 23680, train_perplexity=1784.604, train_loss=7.486952

Batch 23690, train_perplexity=1724.9845, train_loss=7.4529734

Batch 23700, train_perplexity=1867.5541, train_loss=7.532385

Batch 23710, train_perplexity=1660.0139, train_loss=7.4145813

Batch 23720, train_perplexity=1824.8351, train_loss=7.509245

Batch 23730, train_perplexity=1829.8611, train_loss=7.5119953

Batch 23740, train_perplexity=1773.6069, train_loss=7.4807706

Batch 23750, train_perplexity=1529.0432, train_loss=7.3323975

Batch 23760, train_perplexity=1840.6951, train_loss=7.5178986

Batch 23770, train_perplexity=1828.6207, train_loss=7.5113173

Batch 23780, train_perplexity=1734.8693, train_loss=7.4586873

Batch 23790, train_perplexity=1755.1863, train_loss=7.47033

Batch 23800, train_perplexity=1732.6412, train_loss=7.457402

Batch 23810, train_perplexity=1871.4122, train_loss=7.5344486

Batch 23820, train_perplexity=1890.857, train_loss=7.5447855

Batch 23830, train_perplexity=1687.9203, train_loss=7.4312525

Batch 23840, train_perplexity=1686.2711, train_loss=7.430275

Batch 23850, train_perplexity=1810.603, train_loss=7.5014153

Batch 23860, train_perplexity=1827.5189, train_loss=7.5107145

Batch 23870, train_perplexity=1666.7305, train_loss=7.418619

Batch 23880, train_perplexity=1822.5029, train_loss=7.507966

Batch 23890, train_perplexity=1653.5477, train_loss=7.4106784

Batch 23900, train_perplexity=1702.5903, train_loss=7.439906

Batch 23910, train_perplexity=1680.5582, train_loss=7.4268813

Batch 23920, train_perplexity=1586.5635, train_loss=7.3693256

Batch 23930, train_perplexity=1675.5485, train_loss=7.423896

Batch 23940, train_perplexity=1696.3433, train_loss=7.43623

Batch 23950, train_perplexity=1832.0839, train_loss=7.5132093

Batch 23960, train_perplexity=1876.3947, train_loss=7.5371075

Batch 23970, train_perplexity=1668.4448, train_loss=7.419647

Batch 23980, train_perplexity=1694.8967, train_loss=7.435377

Batch 23990, train_perplexity=1635.377, train_loss=7.3996286

Batch 24000, train_perplexity=1602.2837, train_loss=7.379185

Batch 24010, train_perplexity=1754.7042, train_loss=7.4700556

Batch 24020, train_perplexity=1646.5035, train_loss=7.4064093

Batch 24030, train_perplexity=1831.2655, train_loss=7.5127625

Batch 24040, train_perplexity=1719.0298, train_loss=7.4495153

Batch 24050, train_perplexity=1726.917, train_loss=7.454093

Batch 24060, train_perplexity=1620.0295, train_loss=7.3901997

Batch 24070, train_perplexity=1748.4777, train_loss=7.4665008

Batch 24080, train_perplexity=1640.424, train_loss=7.40271

Batch 24090, train_perplexity=1701.36, train_loss=7.439183

Batch 24100, train_perplexity=1742.21, train_loss=7.4629097

Batch 24110, train_perplexity=1642.5874, train_loss=7.404028

Batch 24120, train_perplexity=1692.3787, train_loss=7.4338903

Batch 24130, train_perplexity=1812.8164, train_loss=7.502637
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24140, train_perplexity=1624.1599, train_loss=7.392746

Batch 24150, train_perplexity=1679.3783, train_loss=7.426179

Batch 24160, train_perplexity=1767.0066, train_loss=7.477042

Batch 24170, train_perplexity=1763.7286, train_loss=7.4751854

Batch 24180, train_perplexity=1654.2961, train_loss=7.411131

Batch 24190, train_perplexity=1680.6592, train_loss=7.4269414

Batch 24200, train_perplexity=1678.4576, train_loss=7.4256306

Batch 24210, train_perplexity=1618.1002, train_loss=7.389008

Batch 24220, train_perplexity=1775.7589, train_loss=7.481983

Batch 24230, train_perplexity=1643.7838, train_loss=7.404756

Batch 24240, train_perplexity=1672.8286, train_loss=7.4222713

Batch 24250, train_perplexity=1741.2765, train_loss=7.4623737

Batch 24260, train_perplexity=1584.9318, train_loss=7.3682966

Batch 24270, train_perplexity=1583.4089, train_loss=7.3673353

Batch 24280, train_perplexity=1842.9153, train_loss=7.519104

Batch 24290, train_perplexity=1604.2568, train_loss=7.380416

Batch 24300, train_perplexity=1723.2367, train_loss=7.4519596

Batch 24310, train_perplexity=1675.4734, train_loss=7.423851

Batch 24320, train_perplexity=1471.1089, train_loss=7.2937717

Batch 24330, train_perplexity=1768.7296, train_loss=7.478017

Batch 24340, train_perplexity=1691.9172, train_loss=7.4336176

Batch 24350, train_perplexity=1702.8226, train_loss=7.4400425

Batch 24360, train_perplexity=1713.3685, train_loss=7.4462166

Batch 24370, train_perplexity=1741.9558, train_loss=7.462764

Batch 24380, train_perplexity=1769.3235, train_loss=7.4783525

Batch 24390, train_perplexity=1635.4893, train_loss=7.3996973

Batch 24400, train_perplexity=1758.1029, train_loss=7.4719906

Batch 24410, train_perplexity=1559.9758, train_loss=7.3524256

Batch 24420, train_perplexity=1737.467, train_loss=7.4601836

Batch 24430, train_perplexity=1748.1516, train_loss=7.4663143

Batch 24440, train_perplexity=1606.9855, train_loss=7.3821154

Batch 24450, train_perplexity=1657.0056, train_loss=7.4127674

Batch 24460, train_perplexity=1713.344, train_loss=7.4462023

Batch 24470, train_perplexity=1702.0547, train_loss=7.4395914

Batch 24480, train_perplexity=1835.882, train_loss=7.5152802

Batch 24490, train_perplexity=1724.1556, train_loss=7.4524927

Batch 24500, train_perplexity=1631.6499, train_loss=7.397347

Batch 24510, train_perplexity=1801.6326, train_loss=7.4964485

Batch 24520, train_perplexity=1688.3824, train_loss=7.431526

Batch 24530, train_perplexity=1631.8826, train_loss=7.3974895

Batch 24540, train_perplexity=1704.9424, train_loss=7.4412866

Batch 24550, train_perplexity=1770.1471, train_loss=7.478818

Batch 24560, train_perplexity=1767.4254, train_loss=7.477279

Batch 24570, train_perplexity=1883.3687, train_loss=7.5408173

Batch 24580, train_perplexity=1627.0543, train_loss=7.3945265

Batch 24590, train_perplexity=1644.7795, train_loss=7.4053617

Batch 24600, train_perplexity=1584.7814, train_loss=7.3682017

Batch 24610, train_perplexity=1711.5394, train_loss=7.4451485

Batch 24620, train_perplexity=1828.3435, train_loss=7.5111656

Batch 24630, train_perplexity=1588.7627, train_loss=7.370711

Batch 24640, train_perplexity=1716.8213, train_loss=7.44823

Batch 24650, train_perplexity=1716.0618, train_loss=7.4477873

Batch 24660, train_perplexity=1939.7279, train_loss=7.570303

Batch 24670, train_perplexity=1678.3416, train_loss=7.4255614

Batch 24680, train_perplexity=1618.1473, train_loss=7.389037

Batch 24690, train_perplexity=1723.2219, train_loss=7.451951

Batch 24700, train_perplexity=1738.8934, train_loss=7.4610043

Batch 24710, train_perplexity=1667.4578, train_loss=7.4190555

Batch 24720, train_perplexity=1670.1195, train_loss=7.4206505

Batch 24730, train_perplexity=1860.8658, train_loss=7.528797

Batch 24740, train_perplexity=1646.9974, train_loss=7.406709

Batch 24750, train_perplexity=1547.3903, train_loss=7.344325

Batch 24760, train_perplexity=1603.4974, train_loss=7.3799424

Batch 24770, train_perplexity=1691.905, train_loss=7.4336104

Batch 24780, train_perplexity=1959.6901, train_loss=7.5805416

Batch 24790, train_perplexity=1908.0187, train_loss=7.5538206

Batch 24800, train_perplexity=1669.9269, train_loss=7.420535

Batch 24810, train_perplexity=1735.5568, train_loss=7.4590836

Batch 24820, train_perplexity=1607.7582, train_loss=7.382596

Batch 24830, train_perplexity=1603.5073, train_loss=7.3799486

Batch 24840, train_perplexity=1658.498, train_loss=7.4136677

Batch 24850, train_perplexity=1681.141, train_loss=7.427228

Batch 24860, train_perplexity=1608.0111, train_loss=7.3827534

Batch 24870, train_perplexity=1524.9185, train_loss=7.329696

Batch 24880, train_perplexity=1696.2065, train_loss=7.4361496

Batch 24890, train_perplexity=1685.3323, train_loss=7.429718

Batch 24900, train_perplexity=1765.9974, train_loss=7.476471

Batch 24910, train_perplexity=1731.5262, train_loss=7.4567585

Batch 24920, train_perplexity=1719.794, train_loss=7.4499598

Batch 24930, train_perplexity=1622.0323, train_loss=7.391435

Batch 24940, train_perplexity=1848.719, train_loss=7.5222483

Batch 24950, train_perplexity=1849.0303, train_loss=7.5224166

Batch 24960, train_perplexity=1683.3741, train_loss=7.4285555

Batch 24970, train_perplexity=1703.2375, train_loss=7.440286

Batch 24980, train_perplexity=1516.8975, train_loss=7.3244224

Batch 24990, train_perplexity=1608.9, train_loss=7.383306

Batch 25000, train_perplexity=1552.9948, train_loss=7.3479404

Batch 25010, train_perplexity=1692.2045, train_loss=7.4337873

Batch 25020, train_perplexity=1766.7488, train_loss=7.4768963

Batch 25030, train_perplexity=1804.339, train_loss=7.4979496

Batch 25040, train_perplexity=1669.5502, train_loss=7.4203095

Batch 25050, train_perplexity=1748.857, train_loss=7.4667177

Batch 25060, train_perplexity=1760.5055, train_loss=7.4733562

Batch 25070, train_perplexity=1651.6022, train_loss=7.409501

Batch 25080, train_perplexity=1776.7737, train_loss=7.4825544

Batch 25090, train_perplexity=1732.3488, train_loss=7.4572334

Batch 25100, train_perplexity=1694.4814, train_loss=7.435132

Batch 25110, train_perplexity=1742.5739, train_loss=7.4631186

Batch 25120, train_perplexity=1753.3292, train_loss=7.4692717

Batch 25130, train_perplexity=1717.5304, train_loss=7.4486427

Batch 25140, train_perplexity=1587.4247, train_loss=7.3698683

Batch 25150, train_perplexity=1560.3969, train_loss=7.3526955

Batch 25160, train_perplexity=1505.2615, train_loss=7.316722

Batch 25170, train_perplexity=1714.6516, train_loss=7.446965

Batch 25180, train_perplexity=1502.8668, train_loss=7.3151298

Batch 25190, train_perplexity=1705.836, train_loss=7.4418106

Batch 25200, train_perplexity=1618.7261, train_loss=7.3893948

Batch 25210, train_perplexity=1610.9866, train_loss=7.384602

Batch 25220, train_perplexity=1645.7186, train_loss=7.4059324

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Batch 25230, train_perplexity=1614.4371, train_loss=7.3867416

Batch 25240, train_perplexity=1673.5969, train_loss=7.4227304

Batch 25250, train_perplexity=1716.3826, train_loss=7.447974

Batch 25260, train_perplexity=1646.9432, train_loss=7.4066763

Batch 25270, train_perplexity=1602.5289, train_loss=7.3793383

Batch 25280, train_perplexity=1531.969, train_loss=7.334309

Batch 25290, train_perplexity=1586.5604, train_loss=7.3693237

Batch 25300, train_perplexity=1730.4366, train_loss=7.456129

Batch 25310, train_perplexity=1577.9406, train_loss=7.363876

Batch 25320, train_perplexity=1630.0533, train_loss=7.396368

Batch 25330, train_perplexity=1587.0304, train_loss=7.36962

Batch 25340, train_perplexity=1668.588, train_loss=7.419733

Batch 25350, train_perplexity=1500.5145, train_loss=7.3135633

Batch 25360, train_perplexity=1732.3801, train_loss=7.4572515

Batch 25370, train_perplexity=1771.9569, train_loss=7.47984

Batch 25380, train_perplexity=1859.2427, train_loss=7.5279245
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 25390, train_perplexity=1833.685, train_loss=7.514083

Batch 25400, train_perplexity=1634.7314, train_loss=7.399234

Batch 25410, train_perplexity=1673.8588, train_loss=7.422887

Batch 25420, train_perplexity=1632.3712, train_loss=7.397789

Batch 25430, train_perplexity=1655.0394, train_loss=7.41158

Batch 25440, train_perplexity=1654.1163, train_loss=7.411022

Batch 25450, train_perplexity=1690.5027, train_loss=7.432781

Batch 25460, train_perplexity=1637.9836, train_loss=7.4012213

Batch 25470, train_perplexity=1767.8738, train_loss=7.477533

Batch 25480, train_perplexity=1539.3949, train_loss=7.3391447

Batch 25490, train_perplexity=1634.5771, train_loss=7.3991394

Batch 25500, train_perplexity=1695.1862, train_loss=7.435548

Batch 25510, train_perplexity=1717.9792, train_loss=7.448904

Batch 25520, train_perplexity=1616.7914, train_loss=7.388199

Batch 25530, train_perplexity=1762.376, train_loss=7.474418

Batch 25540, train_perplexity=1663.702, train_loss=7.4168005

Batch 25550, train_perplexity=1528.874, train_loss=7.332287

Batch 25560, train_perplexity=1744.8014, train_loss=7.464396

Batch 25570, train_perplexity=1708.4197, train_loss=7.443324

Batch 25580, train_perplexity=1772.12, train_loss=7.479932

Batch 25590, train_perplexity=1540.4714, train_loss=7.3398438

Batch 25600, train_perplexity=1558.7682, train_loss=7.351651

Batch 25610, train_perplexity=1556.6068, train_loss=7.3502636

Batch 25620, train_perplexity=1757.5933, train_loss=7.4717007

Batch 25630, train_perplexity=1633.6451, train_loss=7.398569

Batch 25640, train_perplexity=1744.7115, train_loss=7.4643445

Batch 25650, train_perplexity=1573.6726, train_loss=7.3611674

Batch 25660, train_perplexity=1694.5525, train_loss=7.435174

Batch 25670, train_perplexity=1571.764, train_loss=7.359954

Batch 25680, train_perplexity=1709.1644, train_loss=7.44376

Batch 25690, train_perplexity=1845.1012, train_loss=7.5202894

Batch 25700, train_perplexity=1695.7432, train_loss=7.4358764

Batch 25710, train_perplexity=1641.8372, train_loss=7.403571

Batch 25720, train_perplexity=1786.2113, train_loss=7.487852

Batch 25730, train_perplexity=1643.2219, train_loss=7.404414

Batch 25740, train_perplexity=1686.4755, train_loss=7.430396

Batch 25750, train_perplexity=1545.8547, train_loss=7.3433323

Batch 25760, train_perplexity=1687.1608, train_loss=7.4308023

Batch 25770, train_perplexity=1595.5242, train_loss=7.3749576

Batch 25780, train_perplexity=1714.3582, train_loss=7.446794

Batch 25790, train_perplexity=1541.4045, train_loss=7.3404493

Batch 25800, train_perplexity=1707.3423, train_loss=7.442693

Batch 25810, train_perplexity=1626.7378, train_loss=7.394332

Batch 25820, train_perplexity=1677.619, train_loss=7.425131

Batch 25830, train_perplexity=1628.057, train_loss=7.3951426

Batch 25840, train_perplexity=1565.6282, train_loss=7.3560424

Batch 25850, train_perplexity=1578.6044, train_loss=7.3642964

Batch 25860, train_perplexity=1815.3682, train_loss=7.5040436

Batch 25870, train_perplexity=1622.6891, train_loss=7.39184

Batch 25880, train_perplexity=1564.226, train_loss=7.3551464

Batch 25890, train_perplexity=1856.948, train_loss=7.5266895

Batch 25900, train_perplexity=1692.8323, train_loss=7.4341583

Batch 25910, train_perplexity=1820.204, train_loss=7.506704

Batch 25920, train_perplexity=1716.3843, train_loss=7.447975

Batch 25930, train_perplexity=1586.0068, train_loss=7.3689747

Batch 25940, train_perplexity=1666.078, train_loss=7.4182277

Batch 25950, train_perplexity=1762.4894, train_loss=7.4744825

Batch 25960, train_perplexity=1644.8054, train_loss=7.4053774

Batch 25970, train_perplexity=1610.6263, train_loss=7.3843784

Batch 25980, train_perplexity=1527.679, train_loss=7.331505

Batch 25990, train_perplexity=1632.6492, train_loss=7.397959

Batch 26000, train_perplexity=1647.3721, train_loss=7.4069366

Batch 26010, train_perplexity=1762.3046, train_loss=7.4743776

Batch 26020, train_perplexity=1801.3774, train_loss=7.496307

Batch 26030, train_perplexity=1453.7094, train_loss=7.2818737

Batch 26040, train_perplexity=1678.769, train_loss=7.425816

Batch 26050, train_perplexity=1581.7759, train_loss=7.3663034

Batch 26060, train_perplexity=1598.4849, train_loss=7.3768115

Batch 26070, train_perplexity=1658.2632, train_loss=7.413526

Batch 26080, train_perplexity=1720.409, train_loss=7.4503174

Batch 26090, train_perplexity=1600.4801, train_loss=7.378059

Batch 26100, train_perplexity=1629.037, train_loss=7.3957443

Batch 26110, train_perplexity=1569.9431, train_loss=7.3587947

Batch 26120, train_perplexity=1550.8872, train_loss=7.3465824

Batch 26130, train_perplexity=1598.5055, train_loss=7.3768244

Batch 26140, train_perplexity=1766.2771, train_loss=7.4766293

Batch 26150, train_perplexity=1672.1802, train_loss=7.4218836

Batch 26160, train_perplexity=1583.2926, train_loss=7.367262

Batch 26170, train_perplexity=1736.0303, train_loss=7.4593563

Batch 26180, train_perplexity=1576.3553, train_loss=7.3628707

Batch 26190, train_perplexity=1651.6077, train_loss=7.4095044

Batch 26200, train_perplexity=1583.8816, train_loss=7.367634

Batch 26210, train_perplexity=1568.9994, train_loss=7.3581934

Batch 26220, train_perplexity=1740.9236, train_loss=7.462171

Batch 26230, train_perplexity=1514.0062, train_loss=7.3225145

Batch 26240, train_perplexity=1446.8022, train_loss=7.277111

Batch 26250, train_perplexity=1675.7961, train_loss=7.4240437

Batch 26260, train_perplexity=1536.0989, train_loss=7.3370013

Batch 26270, train_perplexity=1512.1671, train_loss=7.321299

Batch 26280, train_perplexity=1529.8528, train_loss=7.3329268

Batch 26290, train_perplexity=1784.3912, train_loss=7.4868326

Batch 26300, train_perplexity=1577.299, train_loss=7.363469

Batch 26310, train_perplexity=1763.9944, train_loss=7.475336

Batch 26320, train_perplexity=1723.6427, train_loss=7.452195

Batch 26330, train_perplexity=1491.7751, train_loss=7.307722

Batch 26340, train_perplexity=1583.458, train_loss=7.3673663

Batch 26350, train_perplexity=1542.7377, train_loss=7.341314

Batch 26360, train_perplexity=1566.3854, train_loss=7.356526

Batch 26370, train_perplexity=1613.9106, train_loss=7.3864155

Batch 26380, train_perplexity=1610.7017, train_loss=7.384425

Batch 26390, train_perplexity=1679.9421, train_loss=7.4265146

Batch 26400, train_perplexity=1649.9286, train_loss=7.4084873

Batch 26410, train_perplexity=1716.4431, train_loss=7.4480095

Batch 26420, train_perplexity=1689.008, train_loss=7.4318967

Batch 26430, train_perplexity=1561.8171, train_loss=7.3536053

Batch 26440, train_perplexity=1672.5758, train_loss=7.42212

Batch 26450, train_perplexity=1651.9723, train_loss=7.409725

Batch 26460, train_perplexity=1704.3124, train_loss=7.440917

Batch 26470, train_perplexity=1554.0615, train_loss=7.348627

Batch 26480, train_perplexity=1682.3711, train_loss=7.4279594

Batch 26490, train_perplexity=1701.5288, train_loss=7.4392824

Batch 26500, train_perplexity=1623.6039, train_loss=7.3924036

Batch 26510, train_perplexity=1702.2429, train_loss=7.439702

Batch 26520, train_perplexity=1631.9744, train_loss=7.397546

Batch 26530, train_perplexity=1524.5869, train_loss=7.3294787

Batch 26540, train_perplexity=1663.6567, train_loss=7.4167733

Batch 26550, train_perplexity=1496.1848, train_loss=7.3106737

Batch 26560, train_perplexity=1704.7269, train_loss=7.44116

Batch 26570, train_perplexity=1408.6616, train_loss=7.2503953

Batch 26580, train_perplexity=1559.3674, train_loss=7.3520355

Batch 26590, train_perplexity=1605.2195, train_loss=7.381016

Batch 26600, train_perplexity=1685.2888, train_loss=7.4296923

Batch 26610, train_perplexity=1584.962, train_loss=7.3683157

Batch 26620, train_perplexity=1587.2052, train_loss=7.36973

Batch 26630, train_perplexity=1523.7518, train_loss=7.328931

Batch 26640, train_perplexity=1561.3197, train_loss=7.3532867

Batch 26650, train_perplexity=1553.5192, train_loss=7.348278

Batch 26660, train_perplexity=1615.01, train_loss=7.3870964

Batch 26670, train_perplexity=1559.5875, train_loss=7.3521767

Batch 26680, train_perplexity=1681.9556, train_loss=7.4277124

Batch 26690, train_perplexity=1561.2007, train_loss=7.3532104

Batch 26700, train_perplexity=1650.6951, train_loss=7.4089518
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 26710, train_perplexity=1571.5752, train_loss=7.3598337

Batch 26720, train_perplexity=1611.1594, train_loss=7.3847094

Batch 26730, train_perplexity=1611.5544, train_loss=7.3849545

Batch 26740, train_perplexity=1695.7795, train_loss=7.435898

Batch 26750, train_perplexity=1551.6062, train_loss=7.347046

Batch 26760, train_perplexity=1492.412, train_loss=7.308149

Batch 26770, train_perplexity=1564.8042, train_loss=7.355516

Batch 26780, train_perplexity=1676.3013, train_loss=7.424345

Batch 26790, train_perplexity=1639.251, train_loss=7.4019947

Batch 26800, train_perplexity=1861.4719, train_loss=7.529123

Batch 26810, train_perplexity=1617.0211, train_loss=7.388341

Batch 26820, train_perplexity=1574.6552, train_loss=7.3617916

Batch 26830, train_perplexity=1688.0024, train_loss=7.431301

Batch 26840, train_perplexity=1727.1146, train_loss=7.4542074

Batch 26850, train_perplexity=1624.9578, train_loss=7.393237

Batch 26860, train_perplexity=1704.1255, train_loss=7.4408073

Batch 26870, train_perplexity=1525.9564, train_loss=7.3303766

Batch 26880, train_perplexity=1740.5891, train_loss=7.461979

Batch 26890, train_perplexity=1608.8955, train_loss=7.383303

Batch 26900, train_perplexity=1763.0258, train_loss=7.4747868

Batch 26910, train_perplexity=1678.3015, train_loss=7.4255376

Batch 26920, train_perplexity=1461.7258, train_loss=7.287373

Batch 26930, train_perplexity=1742.5099, train_loss=7.463082

Batch 26940, train_perplexity=1672.0343, train_loss=7.4217963

Batch 26950, train_perplexity=1681.3822, train_loss=7.4273715

Batch 26960, train_perplexity=1689.0217, train_loss=7.431905

Batch 26970, train_perplexity=1587.5867, train_loss=7.3699703

Batch 26980, train_perplexity=1752.8561, train_loss=7.469002

Batch 26990, train_perplexity=1619.1183, train_loss=7.389637

Batch 27000, train_perplexity=1550.3193, train_loss=7.346216

Batch 27010, train_perplexity=1699.6548, train_loss=7.4381804

Batch 27020, train_perplexity=1507.5508, train_loss=7.3182416

Batch 27030, train_perplexity=1703.716, train_loss=7.440567

Batch 27040, train_perplexity=1489.4197, train_loss=7.306142

Batch 27050, train_perplexity=1545.2726, train_loss=7.3429556

Batch 27060, train_perplexity=1589.8518, train_loss=7.371396

Batch 27070, train_perplexity=1527.0657, train_loss=7.3311033

Batch 27080, train_perplexity=1525.0602, train_loss=7.329789

Batch 27090, train_perplexity=1540.2246, train_loss=7.3396835

Batch 27100, train_perplexity=1627.6254, train_loss=7.3948774

Batch 27110, train_perplexity=1647.1113, train_loss=7.4067783

Batch 27120, train_perplexity=1528.9456, train_loss=7.3323336

Batch 27130, train_perplexity=1696.1257, train_loss=7.436102

Batch 27140, train_perplexity=1609.6782, train_loss=7.3837895

Batch 27150, train_perplexity=1688.4412, train_loss=7.431561

Batch 27160, train_perplexity=1609.9023, train_loss=7.383929

Batch 27170, train_perplexity=1612.9658, train_loss=7.38583

Batch 27180, train_perplexity=1473.5499, train_loss=7.2954297

Batch 27190, train_perplexity=1645.342, train_loss=7.4057035

Batch 27200, train_perplexity=1610.4067, train_loss=7.384242

Batch 27210, train_perplexity=1667.2217, train_loss=7.418914

Batch 27220, train_perplexity=1674.9781, train_loss=7.4235554

Batch 27230, train_perplexity=1437.0985, train_loss=7.2703815

Batch 27240, train_perplexity=1745.8258, train_loss=7.464983

Batch 27250, train_perplexity=1606.7817, train_loss=7.3819885

Batch 27260, train_perplexity=1687.3828, train_loss=7.430934

Batch 27270, train_perplexity=1676.7169, train_loss=7.424593

Batch 27280, train_perplexity=1545.8002, train_loss=7.343297

Batch 27290, train_perplexity=1560.6394, train_loss=7.352851

Batch 27300, train_perplexity=1408.0834, train_loss=7.2499847

Batch 27310, train_perplexity=1540.2937, train_loss=7.3397284

Batch 27320, train_perplexity=1835.3138, train_loss=7.514971

Batch 27330, train_perplexity=1600.4977, train_loss=7.37807

Batch 27340, train_perplexity=1597.0731, train_loss=7.375928

Batch 27350, train_perplexity=1671.1472, train_loss=7.4212656

Batch 27360, train_perplexity=1515.5635, train_loss=7.3235426

Batch 27370, train_perplexity=1449.7332, train_loss=7.2791348

Batch 27380, train_perplexity=1453.5347, train_loss=7.2817535

Batch 27390, train_perplexity=1572.4874, train_loss=7.360414

Batch 27400, train_perplexity=1453.0356, train_loss=7.28141

Batch 27410, train_perplexity=1645.3444, train_loss=7.405705

Batch 27420, train_perplexity=1675.4446, train_loss=7.423834

Batch 27430, train_perplexity=1555.7935, train_loss=7.349741

Batch 27440, train_perplexity=1615.4897, train_loss=7.3873935

Batch 27450, train_perplexity=1590.5964, train_loss=7.3718643

Batch 27460, train_perplexity=1577.2365, train_loss=7.3634295

Batch 27470, train_perplexity=1518.8799, train_loss=7.3257284

Batch 27480, train_perplexity=1728.619, train_loss=7.455078

Batch 27490, train_perplexity=1716.8475, train_loss=7.448245

Batch 27500, train_perplexity=1567.311, train_loss=7.3571167

Batch 27510, train_perplexity=1564.3976, train_loss=7.355256

Batch 27520, train_perplexity=1742.0056, train_loss=7.4627924

Batch 27530, train_perplexity=1467.2026, train_loss=7.291113

Batch 27540, train_perplexity=1600.5679, train_loss=7.3781137

Batch 27550, train_perplexity=1591.475, train_loss=7.3724165

Batch 27560, train_perplexity=1614.3647, train_loss=7.386697

Batch 27570, train_perplexity=1568.0996, train_loss=7.35762

Batch 27580, train_perplexity=1495.7626, train_loss=7.3103914

Batch 27590, train_perplexity=1599.1077, train_loss=7.377201

Batch 27600, train_perplexity=1525.2231, train_loss=7.329896

Batch 27610, train_perplexity=1598.228, train_loss=7.376651

Batch 27620, train_perplexity=1514.2762, train_loss=7.322693

Batch 27630, train_perplexity=1529.2488, train_loss=7.332532

Batch 27640, train_perplexity=1631.8156, train_loss=7.3974485

Batch 27650, train_perplexity=1570.6193, train_loss=7.3592253

Batch 27660, train_perplexity=1641.5475, train_loss=7.4033947

Batch 27670, train_perplexity=1545.7324, train_loss=7.343253

Batch 27680, train_perplexity=1665.086, train_loss=7.417632

Batch 27690, train_perplexity=1676.4348, train_loss=7.4244246

Batch 27700, train_perplexity=1620.2041, train_loss=7.3903074

Batch 27710, train_perplexity=1502.7915, train_loss=7.3150797

Batch 27720, train_perplexity=1624.3055, train_loss=7.3928356

Batch 27730, train_perplexity=1473.6427, train_loss=7.2954926

Batch 27740, train_perplexity=1462.6078, train_loss=7.2879763

Batch 27750, train_perplexity=1678.1392, train_loss=7.425441

Batch 27760, train_perplexity=1552.1345, train_loss=7.3473864

Batch 27770, train_perplexity=1597.828, train_loss=7.3764005

Batch 27780, train_perplexity=1683.4608, train_loss=7.428607

Batch 27790, train_perplexity=1625.8336, train_loss=7.393776

Batch 27800, train_perplexity=1725.9735, train_loss=7.4535465

Batch 27810, train_perplexity=1423.9099, train_loss=7.261162

Batch 27820, train_perplexity=1490.8564, train_loss=7.307106

Batch 27830, train_perplexity=1514.8424, train_loss=7.3230667

Batch 27840, train_perplexity=1674.59, train_loss=7.4233236

Batch 27850, train_perplexity=1647.0839, train_loss=7.4067616

Batch 27860, train_perplexity=1539.817, train_loss=7.339419

Batch 27870, train_perplexity=1490.4563, train_loss=7.3068376

Batch 27880, train_perplexity=1698.476, train_loss=7.4374866

Batch 27890, train_perplexity=1634.2684, train_loss=7.3989506

Batch 27900, train_perplexity=1563.4982, train_loss=7.354681

Batch 27910, train_perplexity=1515.2008, train_loss=7.323303

Batch 27920, train_perplexity=1505.9407, train_loss=7.317173

Batch 27930, train_perplexity=1502.7808, train_loss=7.3150725

Batch 27940, train_perplexity=1594.7612, train_loss=7.3744793

Batch 27950, train_perplexity=1689.9337, train_loss=7.4324446

Batch 27960, train_perplexity=1509.9774, train_loss=7.31985

Batch 27970, train_perplexity=1638.8986, train_loss=7.4017797

Batch 27980, train_perplexity=1732.5354, train_loss=7.457341

Batch 27990, train_perplexity=1370.7352, train_loss=7.2231026

Batch 28000, train_perplexity=1734.1554, train_loss=7.458276

Batch 28010, train_perplexity=1645.7767, train_loss=7.4059677

Batch 28020, train_perplexity=1474.2999, train_loss=7.2959385
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28030, train_perplexity=1687.4279, train_loss=7.4309607

Batch 28040, train_perplexity=1608.1254, train_loss=7.3828244

Batch 28050, train_perplexity=1594.0769, train_loss=7.37405

Batch 28060, train_perplexity=1616.0607, train_loss=7.387747

Batch 28070, train_perplexity=1595.5675, train_loss=7.3749847

Batch 28080, train_perplexity=1578.7346, train_loss=7.364379

Batch 28090, train_perplexity=1617.4153, train_loss=7.3885846

Batch 28100, train_perplexity=1677.4255, train_loss=7.4250154

Batch 28110, train_perplexity=1601.7437, train_loss=7.378848

Batch 28120, train_perplexity=1679.6826, train_loss=7.42636

Batch 28130, train_perplexity=1709.815, train_loss=7.4441404

Batch 28140, train_perplexity=1496.7515, train_loss=7.3110523

Batch 28150, train_perplexity=1513.651, train_loss=7.32228

Batch 28160, train_perplexity=1653.5619, train_loss=7.410687

Batch 28170, train_perplexity=1599.0696, train_loss=7.3771772

Batch 28180, train_perplexity=1643.9822, train_loss=7.4048767

Batch 28190, train_perplexity=1564.8124, train_loss=7.355521

Batch 28200, train_perplexity=1526.5022, train_loss=7.3307343

Batch 28210, train_perplexity=1581.26, train_loss=7.3659773

Batch 28220, train_perplexity=1623.737, train_loss=7.3924856

Batch 28230, train_perplexity=1618.3202, train_loss=7.389144

Batch 28240, train_perplexity=1753.0249, train_loss=7.469098

Batch 28250, train_perplexity=1553.4569, train_loss=7.348238

Batch 28260, train_perplexity=1560.9744, train_loss=7.3530655

Batch 28270, train_perplexity=1675.2609, train_loss=7.423724

Batch 28280, train_perplexity=1749.1006, train_loss=7.466857

Batch 28290, train_perplexity=1782.3385, train_loss=7.4856815

Batch 28300, train_perplexity=1481.1857, train_loss=7.300598

Batch 28310, train_perplexity=1756.4521, train_loss=7.471051

Batch 28320, train_perplexity=1482.2688, train_loss=7.301329

Batch 28330, train_perplexity=1598.7905, train_loss=7.3770027

Batch 28340, train_perplexity=1535.3981, train_loss=7.336545

Batch 28350, train_perplexity=1557.6775, train_loss=7.350951

Batch 28360, train_perplexity=1513.221, train_loss=7.3219957

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Batch 28370, train_perplexity=1779.5021, train_loss=7.484089

Batch 28380, train_perplexity=1569.3137, train_loss=7.3583937

Batch 28390, train_perplexity=1634.4618, train_loss=7.399069

Batch 28400, train_perplexity=1541.5986, train_loss=7.340575

Batch 28410, train_perplexity=1408.7905, train_loss=7.250487

Batch 28420, train_perplexity=1674.368, train_loss=7.423191

Batch 28430, train_perplexity=1599.6423, train_loss=7.3775353

Batch 28440, train_perplexity=1639.7091, train_loss=7.402274

Batch 28450, train_perplexity=1677.8167, train_loss=7.4252486

Batch 28460, train_perplexity=1602.0844, train_loss=7.3790607

Batch 28470, train_perplexity=1467.6897, train_loss=7.291445

Batch 28480, train_perplexity=1612.0155, train_loss=7.3852406

Batch 28490, train_perplexity=1649.7312, train_loss=7.4083676

Batch 28500, train_perplexity=1610.2117, train_loss=7.384121

Batch 28510, train_perplexity=1571.561, train_loss=7.3598247

Batch 28520, train_perplexity=1561.4746, train_loss=7.353386

Batch 28530, train_perplexity=1650.3677, train_loss=7.4087534

Batch 28540, train_perplexity=1428.612, train_loss=7.2644587

Batch 28550, train_perplexity=1540.8446, train_loss=7.340086

Batch 28560, train_perplexity=1541.8162, train_loss=7.3407164

Batch 28570, train_perplexity=1628.578, train_loss=7.3954625

Batch 28580, train_perplexity=1555.9583, train_loss=7.349847

Batch 28590, train_perplexity=1598.4764, train_loss=7.3768063

Batch 28600, train_perplexity=1646.3026, train_loss=7.406287

Batch 28610, train_perplexity=1584.792, train_loss=7.3682084

Batch 28620, train_perplexity=1453.3212, train_loss=7.2816067

Batch 28630, train_perplexity=1592.8817, train_loss=7.3733

Batch 28640, train_perplexity=1532.2765, train_loss=7.33451

Batch 28650, train_perplexity=1607.7566, train_loss=7.382595

Batch 28660, train_perplexity=1501.3312, train_loss=7.3141074

Batch 28670, train_perplexity=1575.075, train_loss=7.362058

Batch 28680, train_perplexity=1584.1006, train_loss=7.367772

Batch 28690, train_perplexity=1580.917, train_loss=7.3657603

Batch 28700, train_perplexity=1575.25, train_loss=7.3621693

Batch 28710, train_perplexity=1597.0625, train_loss=7.3759212

Batch 28720, train_perplexity=1584.965, train_loss=7.3683176

Batch 28730, train_perplexity=1449.2942, train_loss=7.278832

Batch 28740, train_perplexity=1664.8733, train_loss=7.4175043

Batch 28750, train_perplexity=1460.7288, train_loss=7.2866907

Batch 28760, train_perplexity=1512.3864, train_loss=7.321444

Batch 28770, train_perplexity=1561.6273, train_loss=7.3534837

Batch 28780, train_perplexity=1424.746, train_loss=7.261749

Batch 28790, train_perplexity=1350.9751, train_loss=7.208582

Batch 28800, train_perplexity=1662.5631, train_loss=7.4161158

Batch 28810, train_perplexity=1479.9213, train_loss=7.299744

Batch 28820, train_perplexity=1451.9795, train_loss=7.280683

Batch 28830, train_perplexity=1516.6104, train_loss=7.324233

Batch 28840, train_perplexity=1409.9485, train_loss=7.2513084

Batch 28850, train_perplexity=1364.4749, train_loss=7.218525

Batch 28860, train_perplexity=1525.3439, train_loss=7.329975

Batch 28870, train_perplexity=1675.0348, train_loss=7.423589

Batch 28880, train_perplexity=1550.1914, train_loss=7.3461337

Batch 28890, train_perplexity=1608.3217, train_loss=7.3829465

Batch 28900, train_perplexity=1457.538, train_loss=7.284504

Batch 28910, train_perplexity=1528.189, train_loss=7.3318386

Batch 28920, train_perplexity=1595.9967, train_loss=7.3752537

Batch 28930, train_perplexity=1558.9645, train_loss=7.351777

Batch 28940, train_perplexity=1656.6809, train_loss=7.4125714

Batch 28950, train_perplexity=1477.9353, train_loss=7.2984014

Batch 28960, train_perplexity=1502.1647, train_loss=7.3146625

Batch 28970, train_perplexity=1529.8352, train_loss=7.3329153

Batch 28980, train_perplexity=1400.2316, train_loss=7.244393

Batch 28990, train_perplexity=1504.1278, train_loss=7.3159685

Batch 29000, train_perplexity=1525.2231, train_loss=7.329896

Batch 29010, train_perplexity=1707.4474, train_loss=7.4427547

Batch 29020, train_perplexity=1511.501, train_loss=7.3208585

Batch 29030, train_perplexity=1699.0155, train_loss=7.437804

Batch 29040, train_perplexity=1671.4436, train_loss=7.421443

Batch 29050, train_perplexity=1503.6022, train_loss=7.315619

Batch 29060, train_perplexity=1227.6469, train_loss=7.1128545

Batch 29070, train_perplexity=1612.8651, train_loss=7.3857675

Batch 29080, train_perplexity=1637.1793, train_loss=7.40073

Batch 29090, train_perplexity=1435.7814, train_loss=7.2694645

Batch 29100, train_perplexity=1512.1592, train_loss=7.321294

Batch 29110, train_perplexity=1427.0938, train_loss=7.2633953

Batch 29120, train_perplexity=1516.7737, train_loss=7.324341

Batch 29130, train_perplexity=1584.7397, train_loss=7.3681755

Batch 29140, train_perplexity=1496.8135, train_loss=7.311094

Batch 29150, train_perplexity=1521.8225, train_loss=7.327664

Batch 29160, train_perplexity=1427.9474, train_loss=7.2639933

Batch 29170, train_perplexity=1421.6548, train_loss=7.259577

Batch 29180, train_perplexity=1472.4802, train_loss=7.2947035

Batch 29190, train_perplexity=1397.8192, train_loss=7.2426686

Batch 29200, train_perplexity=1443.1174, train_loss=7.274561

Batch 29210, train_perplexity=1460.8945, train_loss=7.286804

Batch 29220, train_perplexity=1548.6067, train_loss=7.345111

Batch 29230, train_perplexity=1361.1217, train_loss=7.2160645

Batch 29240, train_perplexity=1568.3203, train_loss=7.3577604

Batch 29250, train_perplexity=1586.0961, train_loss=7.369031

Batch 29260, train_perplexity=1559.0656, train_loss=7.351842

Batch 29270, train_perplexity=1616.1362, train_loss=7.3877935

Batch 29280, train_perplexity=1532.8904, train_loss=7.3349104
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 29290, train_perplexity=1607.194, train_loss=7.382245

Batch 29300, train_perplexity=1590.2308, train_loss=7.3716345

Batch 29310, train_perplexity=1586.5029, train_loss=7.3692875

Batch 29320, train_perplexity=1602.4832, train_loss=7.3793097

Batch 29330, train_perplexity=1559.0879, train_loss=7.351856

Batch 29340, train_perplexity=1412.2174, train_loss=7.2529163

Batch 29350, train_perplexity=1690.4302, train_loss=7.4327383

Batch 29360, train_perplexity=1874.0376, train_loss=7.5358505

Batch 29370, train_perplexity=1555.2268, train_loss=7.3493767

Batch 29380, train_perplexity=1474.103, train_loss=7.295805

Batch 29390, train_perplexity=1625.5886, train_loss=7.3936253

Batch 29400, train_perplexity=1465.1416, train_loss=7.289707

Batch 29410, train_perplexity=1562.5345, train_loss=7.3540645

Batch 29420, train_perplexity=1574.6973, train_loss=7.3618183

Batch 29430, train_perplexity=1511.2034, train_loss=7.3206615

Batch 29440, train_perplexity=1514.4055, train_loss=7.322778

Batch 29450, train_perplexity=1567.8148, train_loss=7.357438

Batch 29460, train_perplexity=1536.0894, train_loss=7.336995

Batch 29470, train_perplexity=1402.2032, train_loss=7.2458

Batch 29480, train_perplexity=1501.8359, train_loss=7.3144436

Batch 29490, train_perplexity=1484.8359, train_loss=7.3030596

Batch 29500, train_perplexity=1511.8953, train_loss=7.3211193

Batch 29510, train_perplexity=1407.2859, train_loss=7.2494183

Batch 29520, train_perplexity=1598.6991, train_loss=7.3769455

Batch 29530, train_perplexity=1715.0621, train_loss=7.4472046

Batch 29540, train_perplexity=1550.2366, train_loss=7.346163

Batch 29550, train_perplexity=1664.9114, train_loss=7.417527

Batch 29560, train_perplexity=1694.2164, train_loss=7.4349756

Batch 29570, train_perplexity=1478.6692, train_loss=7.2988977

Batch 29580, train_perplexity=1471.7993, train_loss=7.294241

Batch 29590, train_perplexity=1574.3759, train_loss=7.361614

Batch 29600, train_perplexity=1524.5338, train_loss=7.329444

Batch 29610, train_perplexity=1605.2363, train_loss=7.3810263

Batch 29620, train_perplexity=1541.1517, train_loss=7.3402853

Batch 29630, train_perplexity=1533.6654, train_loss=7.335416

Batch 29640, train_perplexity=1765.6548, train_loss=7.476277

Batch 29650, train_perplexity=1451.7836, train_loss=7.280548

Batch 29660, train_perplexity=1436.8381, train_loss=7.2702003

Batch 29670, train_perplexity=1480.2635, train_loss=7.2999754

Batch 29680, train_perplexity=1535.6559, train_loss=7.336713

Batch 29690, train_perplexity=1485.7524, train_loss=7.3036766

Batch 29700, train_perplexity=1464.6163, train_loss=7.2893486

Batch 29710, train_perplexity=1568.9127, train_loss=7.358138

Batch 29720, train_perplexity=1676.6849, train_loss=7.424574

Batch 29730, train_perplexity=1505.2837, train_loss=7.3167367

Batch 29740, train_perplexity=1497.7946, train_loss=7.311749

Batch 29750, train_perplexity=1483.5182, train_loss=7.3021717

Batch 29760, train_perplexity=1505.2737, train_loss=7.31673

Batch 29770, train_perplexity=1430.0065, train_loss=7.2654343

Batch 29780, train_perplexity=1583.1401, train_loss=7.3671656

Batch 29790, train_perplexity=1516.162, train_loss=7.3239374

Batch 29800, train_perplexity=1540.8505, train_loss=7.34009

Batch 29810, train_perplexity=1600.0016, train_loss=7.37776

Batch 29820, train_perplexity=1601.655, train_loss=7.378793

Batch 29830, train_perplexity=1524.9075, train_loss=7.329689

Batch 29840, train_perplexity=1423.2793, train_loss=7.260719

Batch 29850, train_perplexity=1507.4595, train_loss=7.318181

Batch 29860, train_perplexity=1422.345, train_loss=7.260062

Batch 29870, train_perplexity=1531.1882, train_loss=7.3337994

Batch 29880, train_perplexity=1319.6497, train_loss=7.1851215

Batch 29890, train_perplexity=1712.0529, train_loss=7.4454484

Batch 29900, train_perplexity=1532.9591, train_loss=7.334955

Batch 29910, train_perplexity=1508.6632, train_loss=7.3189793

Batch 29920, train_perplexity=1499.6161, train_loss=7.3129644

Batch 29930, train_perplexity=1701.0989, train_loss=7.4390297

Batch 29940, train_perplexity=1541.6089, train_loss=7.340582

Batch 29950, train_perplexity=1556.5519, train_loss=7.3502283

Batch 29960, train_perplexity=1452.2363, train_loss=7.28086

Batch 29970, train_perplexity=1552.5312, train_loss=7.347642

Batch 29980, train_perplexity=1560.5331, train_loss=7.3527827

Batch 29990, train_perplexity=1442.7961, train_loss=7.2743382

Batch 30000, train_perplexity=1536.2733, train_loss=7.337115

Batch 30010, train_perplexity=1537.5887, train_loss=7.3379707

Batch 30020, train_perplexity=1513.247, train_loss=7.322013

Batch 30030, train_perplexity=1556.549, train_loss=7.3502264

Batch 30040, train_perplexity=1524.9148, train_loss=7.329694

Batch 30050, train_perplexity=1393.6058, train_loss=7.23965

Batch 30060, train_perplexity=1430.3168, train_loss=7.265651

Batch 30070, train_perplexity=1556.3827, train_loss=7.3501196

Batch 30080, train_perplexity=1632.6655, train_loss=7.3979692

Batch 30090, train_perplexity=1512.8833, train_loss=7.3217726

Batch 30100, train_perplexity=1602.2738, train_loss=7.379179

Batch 30110, train_perplexity=1515.2303, train_loss=7.323323

Batch 30120, train_perplexity=1502.3789, train_loss=7.314805

Batch 30130, train_perplexity=1744.9536, train_loss=7.4644833

Batch 30140, train_perplexity=1640.3597, train_loss=7.402671

Batch 30150, train_perplexity=1478.0854, train_loss=7.298503

Batch 30160, train_perplexity=1413.2494, train_loss=7.253647

Batch 30170, train_perplexity=1543.5596, train_loss=7.3418465

Batch 30180, train_perplexity=1525.474, train_loss=7.3300605

Batch 30190, train_perplexity=1364.7742, train_loss=7.2187443

Batch 30200, train_perplexity=1539.0969, train_loss=7.338951

Batch 30210, train_perplexity=1374.12, train_loss=7.225569

Batch 30220, train_perplexity=1591.9607, train_loss=7.3727217

Batch 30230, train_perplexity=1455.3086, train_loss=7.2829733

Batch 30240, train_perplexity=1575.5985, train_loss=7.3623905

Batch 30250, train_perplexity=1467.5203, train_loss=7.2913294

Batch 30260, train_perplexity=1502.1532, train_loss=7.314655

Batch 30270, train_perplexity=1418.5764, train_loss=7.257409

Batch 30280, train_perplexity=1426.878, train_loss=7.263244

Batch 30290, train_perplexity=1464.7036, train_loss=7.289408

Batch 30300, train_perplexity=1469.5188, train_loss=7.2926903

Batch 30310, train_perplexity=1583.8816, train_loss=7.367634

Batch 30320, train_perplexity=1355.0164, train_loss=7.211569

Batch 30330, train_perplexity=1525.3577, train_loss=7.329984

Batch 30340, train_perplexity=1580.4602, train_loss=7.3654714

Batch 30350, train_perplexity=1577.7728, train_loss=7.3637695

Batch 30360, train_perplexity=1474.972, train_loss=7.2963943

Batch 30370, train_perplexity=1531.5023, train_loss=7.3340044

Batch 30380, train_perplexity=1579.8054, train_loss=7.365057

Batch 30390, train_perplexity=1465.2604, train_loss=7.2897882

Batch 30400, train_perplexity=1357.0869, train_loss=7.2130957

Batch 30410, train_perplexity=1526.3202, train_loss=7.330615

Batch 30420, train_perplexity=1271.4412, train_loss=7.1479063

Batch 30430, train_perplexity=1449.317, train_loss=7.2788477

Batch 30440, train_perplexity=1516.1323, train_loss=7.323918

Batch 30450, train_perplexity=1572.5692, train_loss=7.360466

Batch 30460, train_perplexity=1510.2122, train_loss=7.3200054

Batch 30470, train_perplexity=1457.5837, train_loss=7.2845354

Batch 30480, train_perplexity=1439.195, train_loss=7.271839

Batch 30490, train_perplexity=1571.3445, train_loss=7.359687

Batch 30500, train_perplexity=1547.1055, train_loss=7.344141

Batch 30510, train_perplexity=1378.6998, train_loss=7.228896

Batch 30520, train_perplexity=1587.1991, train_loss=7.369726

Batch 30530, train_perplexity=1561.0547, train_loss=7.353117

Batch 30540, train_perplexity=1604.5919, train_loss=7.380625

Batch 30550, train_perplexity=1462.5624, train_loss=7.2879453

Batch 30560, train_perplexity=1414.6841, train_loss=7.2546616

Batch 30570, train_perplexity=1436.669, train_loss=7.2700825

Batch 30580, train_perplexity=1450.5283, train_loss=7.279683

Batch 30590, train_perplexity=1583.883, train_loss=7.367635

Batch 30600, train_perplexity=1504.7936, train_loss=7.316411
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 30610, train_perplexity=1591.9242, train_loss=7.372699

Batch 30620, train_perplexity=1259.4298, train_loss=7.1384144

Batch 30630, train_perplexity=1551.6135, train_loss=7.3470507

Batch 30640, train_perplexity=1526.1624, train_loss=7.3305116

Batch 30650, train_perplexity=1541.6008, train_loss=7.3405766

Batch 30660, train_perplexity=1545.4221, train_loss=7.3430524

Batch 30670, train_perplexity=1434.6877, train_loss=7.2687025

Batch 30680, train_perplexity=1422.9468, train_loss=7.260485

Batch 30690, train_perplexity=1612.4076, train_loss=7.3854837

Batch 30700, train_perplexity=1526.9368, train_loss=7.331019

Batch 30710, train_perplexity=1576.6072, train_loss=7.3630304

Batch 30720, train_perplexity=1503.8912, train_loss=7.315811

Batch 30730, train_perplexity=1544.6316, train_loss=7.3425407

Batch 30740, train_perplexity=1473.9478, train_loss=7.2956996

Batch 30750, train_perplexity=1507.5284, train_loss=7.318227

Batch 30760, train_perplexity=1475.5981, train_loss=7.2968187

Batch 30770, train_perplexity=1599.8353, train_loss=7.377656

Batch 30780, train_perplexity=1615.9282, train_loss=7.387665

Batch 30790, train_perplexity=1613.9514, train_loss=7.3864408

Batch 30800, train_perplexity=1475.7854, train_loss=7.2969456

Batch 30810, train_perplexity=1479.2291, train_loss=7.2992764

Batch 30820, train_perplexity=1507.3315, train_loss=7.318096

Batch 30830, train_perplexity=1484.0446, train_loss=7.3025265

Batch 30840, train_perplexity=1437.2836, train_loss=7.27051

Batch 30850, train_perplexity=1498.2767, train_loss=7.312071

Batch 30860, train_perplexity=1480.6715, train_loss=7.300251

Batch 30870, train_perplexity=1580.4127, train_loss=7.3654413

Batch 30880, train_perplexity=1412.5131, train_loss=7.2531257

Batch 30890, train_perplexity=1509.1641, train_loss=7.319311

Batch 30900, train_perplexity=1388.1892, train_loss=7.2357554

Batch 30910, train_perplexity=1562.7424, train_loss=7.3541975

Batch 30920, train_perplexity=1487.553, train_loss=7.304888

Batch 30930, train_perplexity=1425.1013, train_loss=7.261998

Batch 30940, train_perplexity=1564.4058, train_loss=7.3552613

Batch 30950, train_perplexity=1420.8145, train_loss=7.2589855

Batch 30960, train_perplexity=1431.2454, train_loss=7.2663

Batch 30970, train_perplexity=1506.988, train_loss=7.317868

Batch 30980, train_perplexity=1539.6959, train_loss=7.33934

Batch 30990, train_perplexity=1572.1808, train_loss=7.360219

Batch 31000, train_perplexity=1480.7435, train_loss=7.3002996

Batch 31010, train_perplexity=1324.6074, train_loss=7.1888714

Batch 31020, train_perplexity=1541.6456, train_loss=7.3406057

Batch 31030, train_perplexity=1554.9733, train_loss=7.3492136

Batch 31040, train_perplexity=1468.9373, train_loss=7.2922945

Batch 31050, train_perplexity=1485.3416, train_loss=7.3034

Batch 31060, train_perplexity=1457.153, train_loss=7.28424

Batch 31070, train_perplexity=1316.8197, train_loss=7.182975

Batch 31080, train_perplexity=1545.207, train_loss=7.342913

Batch 31090, train_perplexity=1626.4586, train_loss=7.3941603

Batch 31100, train_perplexity=1468.8308, train_loss=7.292222

Batch 31110, train_perplexity=1485.1206, train_loss=7.3032513

Batch 31120, train_perplexity=1411.0327, train_loss=7.252077

Batch 31130, train_perplexity=1429.1796, train_loss=7.264856

Batch 31140, train_perplexity=1343.8435, train_loss=7.203289

Batch 31150, train_perplexity=1383.8385, train_loss=7.2326164

Batch 31160, train_perplexity=1618.0933, train_loss=7.3890038

Batch 31170, train_perplexity=1478.9978, train_loss=7.29912

Batch 31180, train_perplexity=1506.7854, train_loss=7.317734

Batch 31190, train_perplexity=1464.587, train_loss=7.2893286

Batch 31200, train_perplexity=1404.015, train_loss=7.2470913

Batch 31210, train_perplexity=1568.3756, train_loss=7.3577957

Batch 31220, train_perplexity=1316.2798, train_loss=7.1825647

Batch 31230, train_perplexity=1346.3256, train_loss=7.2051344

Batch 31240, train_perplexity=1365.3834, train_loss=7.2191906

Batch 31250, train_perplexity=1557.1458, train_loss=7.35061

Batch 31260, train_perplexity=1448.3622, train_loss=7.2781887

Batch 31270, train_perplexity=1500.9353, train_loss=7.3138437

Batch 31280, train_perplexity=1503.3513, train_loss=7.315452

Batch 31290, train_perplexity=1525.2421, train_loss=7.3299084

Batch 31300, train_perplexity=1565.0758, train_loss=7.3556895

Batch 31310, train_perplexity=1549.357, train_loss=7.3455954

Batch 31320, train_perplexity=1516.1266, train_loss=7.323914

Batch 31330, train_perplexity=1545.3978, train_loss=7.3430367

Batch 31340, train_perplexity=1600.8014, train_loss=7.3782597

Batch 31350, train_perplexity=1383.217, train_loss=7.2321672

Batch 31360, train_perplexity=1665.2115, train_loss=7.4177074

Batch 31370, train_perplexity=1425.6029, train_loss=7.26235

Batch 31380, train_perplexity=1523.6726, train_loss=7.328879

Batch 31390, train_perplexity=1475.3857, train_loss=7.2966747

Batch 31400, train_perplexity=1444.7024, train_loss=7.2756586

Batch 31410, train_perplexity=1607.1694, train_loss=7.38223

Batch 31420, train_perplexity=1516.948, train_loss=7.3244557

Batch 31430, train_perplexity=1436.3647, train_loss=7.2698708

Batch 31440, train_perplexity=1600.7678, train_loss=7.3782387

Batch 31450, train_perplexity=1408.9485, train_loss=7.250599

Batch 31460, train_perplexity=1428.6005, train_loss=7.2644506

Batch 31470, train_perplexity=1468.4121, train_loss=7.291937

Batch 31480, train_perplexity=1479.9812, train_loss=7.2997847

Batch 31490, train_perplexity=1545.9034, train_loss=7.343364

Batch 31500, train_perplexity=1523.634, train_loss=7.3288536

Batch 31510, train_perplexity=1618.9005, train_loss=7.3895025

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Batch 31520, train_perplexity=1467.3481, train_loss=7.291212

Batch 31530, train_perplexity=1587.4535, train_loss=7.3698864

Batch 31540, train_perplexity=1381.6864, train_loss=7.23106

Batch 31550, train_perplexity=1688.694, train_loss=7.4317107

Batch 31560, train_perplexity=1503.7112, train_loss=7.3156915

Batch 31570, train_perplexity=1494.8043, train_loss=7.3097506

Batch 31580, train_perplexity=1610.2969, train_loss=7.384174

Batch 31590, train_perplexity=1448.3733, train_loss=7.2781963

Batch 31600, train_perplexity=1527.5565, train_loss=7.3314247

Batch 31610, train_perplexity=1435.2556, train_loss=7.2690983

Batch 31620, train_perplexity=1441.1858, train_loss=7.2732215

Batch 31630, train_perplexity=1652.4828, train_loss=7.410034

Batch 31640, train_perplexity=1337.6108, train_loss=7.1986403

Batch 31650, train_perplexity=1456.2736, train_loss=7.283636

Batch 31660, train_perplexity=1485.0413, train_loss=7.303198

Batch 31670, train_perplexity=1452.3485, train_loss=7.280937

Batch 31680, train_perplexity=1460.4292, train_loss=7.2864857

Batch 31690, train_perplexity=1500.4, train_loss=7.313487

Batch 31700, train_perplexity=1547.2308, train_loss=7.344222

Batch 31710, train_perplexity=1518.6539, train_loss=7.3255796

Batch 31720, train_perplexity=1577.1019, train_loss=7.363344

Batch 31730, train_perplexity=1463.4365, train_loss=7.2885427

Batch 31740, train_perplexity=1462.3463, train_loss=7.2877975

Batch 31750, train_perplexity=1560.5189, train_loss=7.3527737

Batch 31760, train_perplexity=1460.0421, train_loss=7.2862206

Batch 31770, train_perplexity=1544.8231, train_loss=7.3426647

Batch 31780, train_perplexity=1528.425, train_loss=7.331993

Batch 31790, train_perplexity=1386.6305, train_loss=7.234632

Batch 31800, train_perplexity=1447.2991, train_loss=7.2774544

Batch 31810, train_perplexity=1436.9314, train_loss=7.270265

Batch 31820, train_perplexity=1548.8636, train_loss=7.345277

Batch 31830, train_perplexity=1496.6401, train_loss=7.310978

Batch 31840, train_perplexity=1403.5679, train_loss=7.246773

Batch 31850, train_perplexity=1428.1292, train_loss=7.2641206
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 31860, train_perplexity=1407.8973, train_loss=7.2498527

Batch 31870, train_perplexity=1574.7423, train_loss=7.361847

Batch 31880, train_perplexity=1465.7559, train_loss=7.2901263

Batch 31890, train_perplexity=1484.0043, train_loss=7.3024993

Batch 31900, train_perplexity=1456.4312, train_loss=7.2837443

Batch 31910, train_perplexity=1464.682, train_loss=7.2893934

Batch 31920, train_perplexity=1500.1189, train_loss=7.3132997

Batch 31930, train_perplexity=1487.6764, train_loss=7.3049707

Batch 31940, train_perplexity=1349.9049, train_loss=7.2077894

Batch 31950, train_perplexity=1390.2579, train_loss=7.2372446

Batch 31960, train_perplexity=1363.4915, train_loss=7.217804

Batch 31970, train_perplexity=1511.8333, train_loss=7.3210783

Batch 31980, train_perplexity=1472.0408, train_loss=7.294405

Batch 31990, train_perplexity=1459.25, train_loss=7.285678

Batch 32000, train_perplexity=1489.3586, train_loss=7.306101

Batch 32010, train_perplexity=1465.5692, train_loss=7.289999

Batch 32020, train_perplexity=1403.8972, train_loss=7.2470074

Batch 32030, train_perplexity=1482.6901, train_loss=7.3016133

Batch 32040, train_perplexity=1698.4824, train_loss=7.4374905

Batch 32050, train_perplexity=1351.4146, train_loss=7.208907

Batch 32060, train_perplexity=1466.1605, train_loss=7.2904024

Batch 32070, train_perplexity=1284.7157, train_loss=7.158293

Batch 32080, train_perplexity=1460.5142, train_loss=7.286544

Batch 32090, train_perplexity=1575.8375, train_loss=7.362542

Batch 32100, train_perplexity=1468.8868, train_loss=7.29226

Batch 32110, train_perplexity=1344.437, train_loss=7.2037306

Batch 32120, train_perplexity=1468.188, train_loss=7.2917843

Batch 32130, train_perplexity=1517.9227, train_loss=7.325098

Batch 32140, train_perplexity=1360.6655, train_loss=7.215729

Batch 32150, train_perplexity=1315.6486, train_loss=7.182085

Batch 32160, train_perplexity=1456.8424, train_loss=7.2840266

Batch 32170, train_perplexity=1346.3179, train_loss=7.2051287

Batch 32180, train_perplexity=1342.8557, train_loss=7.2025537

Batch 32190, train_perplexity=1531.138, train_loss=7.3337665

Batch 32200, train_perplexity=1427.4565, train_loss=7.2636495

Batch 32210, train_perplexity=1559.4388, train_loss=7.3520813

Batch 32220, train_perplexity=1461.0186, train_loss=7.286889

Batch 32230, train_perplexity=1450.9213, train_loss=7.279954

Batch 32240, train_perplexity=1606.2684, train_loss=7.381669

Batch 32250, train_perplexity=1523.4597, train_loss=7.328739

Batch 32260, train_perplexity=1590.1111, train_loss=7.371559

Batch 32270, train_perplexity=1374.0498, train_loss=7.2255177

Batch 32280, train_perplexity=1368.1364, train_loss=7.2212048

Batch 32290, train_perplexity=1525.3387, train_loss=7.329972

Batch 32300, train_perplexity=1550.7068, train_loss=7.346466

Batch 32310, train_perplexity=1458.7832, train_loss=7.285358

Batch 32320, train_perplexity=1438.3278, train_loss=7.2712364

Batch 32330, train_perplexity=1428.484, train_loss=7.264369

Batch 32340, train_perplexity=1472.7723, train_loss=7.294902

Batch 32350, train_perplexity=1497.736, train_loss=7.31171

Batch 32360, train_perplexity=1509.9601, train_loss=7.3198385

Batch 32370, train_perplexity=1421.6575, train_loss=7.2595787

Batch 32380, train_perplexity=1514.3239, train_loss=7.3227243

Batch 32390, train_perplexity=1305.3995, train_loss=7.1742644

Batch 32400, train_perplexity=1489.5688, train_loss=7.306242

Batch 32410, train_perplexity=1351.448, train_loss=7.208932

Batch 32420, train_perplexity=1449.7332, train_loss=7.2791348

Batch 32430, train_perplexity=1451.8873, train_loss=7.2806196

Batch 32440, train_perplexity=1605.0228, train_loss=7.380893

Batch 32450, train_perplexity=1423.204, train_loss=7.260666

Batch 32460, train_perplexity=1455.6918, train_loss=7.2832365

Batch 32470, train_perplexity=1453.1514, train_loss=7.28149

Batch 32480, train_perplexity=1448.7166, train_loss=7.2784333

Batch 32490, train_perplexity=1420.2921, train_loss=7.258618

Batch 32500, train_perplexity=1382.1212, train_loss=7.2313747

Batch 32510, train_perplexity=1578.277, train_loss=7.364089

Batch 32520, train_perplexity=1478.5331, train_loss=7.2988057

Batch 32530, train_perplexity=1465.8013, train_loss=7.2901573

Batch 32540, train_perplexity=1519.634, train_loss=7.326225

Batch 32550, train_perplexity=1579.09, train_loss=7.364604

Batch 32560, train_perplexity=1538.9648, train_loss=7.3388653

Batch 32570, train_perplexity=1411.3785, train_loss=7.252322

Batch 32580, train_perplexity=1492.0896, train_loss=7.307933

Batch 32590, train_perplexity=1316.7356, train_loss=7.182911

Batch 32600, train_perplexity=1368.8516, train_loss=7.2217274

Batch 32610, train_perplexity=1432.102, train_loss=7.2668986

Batch 32620, train_perplexity=1376.9731, train_loss=7.227643

Batch 32630, train_perplexity=1406.1765, train_loss=7.2486296

Batch 32640, train_perplexity=1582.9998, train_loss=7.367077

Batch 32650, train_perplexity=1559.6031, train_loss=7.3521867

Batch 32660, train_perplexity=1475.7501, train_loss=7.2969217

Batch 32670, train_perplexity=1463.6445, train_loss=7.288685

Batch 32680, train_perplexity=1466.7997, train_loss=7.2908382

Batch 32690, train_perplexity=1377.9242, train_loss=7.2283335

Batch 32700, train_perplexity=1511.1594, train_loss=7.3206325

Batch 32710, train_perplexity=1464.4236, train_loss=7.289217

Batch 32720, train_perplexity=1523.3871, train_loss=7.3286915

Batch 32730, train_perplexity=1445.1082, train_loss=7.2759395

Batch 32740, train_perplexity=1454.0358, train_loss=7.2820983

Batch 32750, train_perplexity=1417.412, train_loss=7.256588

Batch 32760, train_perplexity=1363.7125, train_loss=7.217966

Batch 32770, train_perplexity=1441.1548, train_loss=7.2732

Batch 32780, train_perplexity=1485.0385, train_loss=7.303196

Batch 32790, train_perplexity=1446.8699, train_loss=7.277158

Batch 32800, train_perplexity=1500.7206, train_loss=7.3137007

Batch 32810, train_perplexity=1541.3428, train_loss=7.3404093

Batch 32820, train_perplexity=1396.345, train_loss=7.2416134

Batch 32830, train_perplexity=1592.0973, train_loss=7.3728075

Batch 32840, train_perplexity=1305.9355, train_loss=7.174675

Batch 32850, train_perplexity=1359.7698, train_loss=7.2150707

Batch 32860, train_perplexity=1472.896, train_loss=7.294986

Batch 32870, train_perplexity=1346.707, train_loss=7.2054176

Batch 32880, train_perplexity=1467.8367, train_loss=7.291545

Batch 32890, train_perplexity=1450.9794, train_loss=7.279994

Batch 32900, train_perplexity=1393.8477, train_loss=7.2398233

Batch 32910, train_perplexity=1500.6018, train_loss=7.3136215

Batch 32920, train_perplexity=1431.0092, train_loss=7.266135

Batch 32930, train_perplexity=1394.0319, train_loss=7.2399554

Batch 32940, train_perplexity=1578.9846, train_loss=7.3645372

Batch 32950, train_perplexity=1420.9337, train_loss=7.2590694

Batch 32960, train_perplexity=1382.7133, train_loss=7.231803

Batch 32970, train_perplexity=1454.2792, train_loss=7.2822657

Batch 32980, train_perplexity=1388.0681, train_loss=7.235668

Batch 32990, train_perplexity=1352.6846, train_loss=7.2098465

Batch 33000, train_perplexity=1383.7686, train_loss=7.232566

Batch 33010, train_perplexity=1306.2134, train_loss=7.1748877

Batch 33020, train_perplexity=1432.0181, train_loss=7.26684

Batch 33030, train_perplexity=1467.7737, train_loss=7.291502

Batch 33040, train_perplexity=1313.7252, train_loss=7.180622

Batch 33050, train_perplexity=1553.9622, train_loss=7.348563

Batch 33060, train_perplexity=1464.3789, train_loss=7.2891865

Batch 33070, train_perplexity=1492.7273, train_loss=7.30836

Batch 33080, train_perplexity=1365.0834, train_loss=7.218971

Batch 33090, train_perplexity=1455.8938, train_loss=7.2833753

Batch 33100, train_perplexity=1355.0299, train_loss=7.211579

Batch 33110, train_perplexity=1382.4146, train_loss=7.231587

Batch 33120, train_perplexity=1576.2207, train_loss=7.3627853

Batch 33130, train_perplexity=1493.032, train_loss=7.308564

Batch 33140, train_perplexity=1435.7896, train_loss=7.26947

Batch 33150, train_perplexity=1507.2345, train_loss=7.318032

Batch 33160, train_perplexity=1425.6681, train_loss=7.262396

Batch 33170, train_perplexity=1560.1261, train_loss=7.352522
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33180, train_perplexity=1370.0204, train_loss=7.222581

Batch 33190, train_perplexity=1419.6375, train_loss=7.258157

Batch 33200, train_perplexity=1345.1962, train_loss=7.204295

Batch 33210, train_perplexity=1577.0297, train_loss=7.3632984

Batch 33220, train_perplexity=1436.8375, train_loss=7.2702

Batch 33230, train_perplexity=1262.854, train_loss=7.1411295

Batch 33240, train_perplexity=1420.1892, train_loss=7.2585454

Batch 33250, train_perplexity=1373.5645, train_loss=7.2251644

Batch 33260, train_perplexity=1385.5737, train_loss=7.2338696

Batch 33270, train_perplexity=1549.561, train_loss=7.345727

Batch 33280, train_perplexity=1331.705, train_loss=7.1942153

Batch 33290, train_perplexity=1427.7847, train_loss=7.2638793

Batch 33300, train_perplexity=1309.0767, train_loss=7.1770773

Batch 33310, train_perplexity=1458.5342, train_loss=7.2851872

Batch 33320, train_perplexity=1460.8096, train_loss=7.286746

Batch 33330, train_perplexity=1493.3623, train_loss=7.3087854

Batch 33340, train_perplexity=1295.7505, train_loss=7.1668453

Batch 33350, train_perplexity=1268.5138, train_loss=7.1456013

Batch 33360, train_perplexity=1463.327, train_loss=7.288468

Batch 33370, train_perplexity=1420.9506, train_loss=7.2590814

Batch 33380, train_perplexity=1443.5393, train_loss=7.274853

Batch 33390, train_perplexity=1474.9623, train_loss=7.2963877

Batch 33400, train_perplexity=1453.737, train_loss=7.281893

Batch 33410, train_perplexity=1391.0232, train_loss=7.237795

Batch 33420, train_perplexity=1460.175, train_loss=7.2863116

Batch 33430, train_perplexity=1418.7286, train_loss=7.2575164

Batch 33440, train_perplexity=1458.7172, train_loss=7.2853127

Batch 33450, train_perplexity=1478.4379, train_loss=7.2987413

Batch 33460, train_perplexity=1500.2484, train_loss=7.313386

Batch 33470, train_perplexity=1537.127, train_loss=7.3376703

Batch 33480, train_perplexity=1306.475, train_loss=7.175088

Batch 33490, train_perplexity=1358.1149, train_loss=7.213853

Batch 33500, train_perplexity=1477.9276, train_loss=7.298396

Batch 33510, train_perplexity=1426.6814, train_loss=7.2631063

Batch 33520, train_perplexity=1387.1669, train_loss=7.2350187

Batch 33530, train_perplexity=1395.3552, train_loss=7.2409043

Batch 33540, train_perplexity=1568.2948, train_loss=7.357744

Batch 33550, train_perplexity=1344.9742, train_loss=7.20413

Batch 33560, train_perplexity=1433.61, train_loss=7.267951

Batch 33570, train_perplexity=1512.5305, train_loss=7.3215394

Batch 33580, train_perplexity=1450.307, train_loss=7.2795305

Batch 33590, train_perplexity=1279.6779, train_loss=7.1543636

Batch 33600, train_perplexity=1587.3611, train_loss=7.369828

Batch 33610, train_perplexity=1410.4232, train_loss=7.251645

Batch 33620, train_perplexity=1429.4624, train_loss=7.2650537

Batch 33630, train_perplexity=1366.2992, train_loss=7.219861

Batch 33640, train_perplexity=1419.6469, train_loss=7.2581635

Batch 33650, train_perplexity=1414.6599, train_loss=7.2546444

Batch 33660, train_perplexity=1381.419, train_loss=7.2308664

Batch 33670, train_perplexity=1379.6764, train_loss=7.2296042

Batch 33680, train_perplexity=1260.8756, train_loss=7.1395617

Batch 33690, train_perplexity=1422.345, train_loss=7.260062

Batch 33700, train_perplexity=1404.2601, train_loss=7.247266

Batch 33710, train_perplexity=1340.7239, train_loss=7.200965

Batch 33720, train_perplexity=1429.1244, train_loss=7.264817

Batch 33730, train_perplexity=1342.4658, train_loss=7.2022634

Batch 33740, train_perplexity=1646.8522, train_loss=7.406621

Batch 33750, train_perplexity=1351.5215, train_loss=7.2089863

Batch 33760, train_perplexity=1438.6254, train_loss=7.2714434

Batch 33770, train_perplexity=1435.9766, train_loss=7.2696004

Batch 33780, train_perplexity=1454.5968, train_loss=7.282484

Batch 33790, train_perplexity=1402.955, train_loss=7.246336

Batch 33800, train_perplexity=1637.234, train_loss=7.4007635

Batch 33810, train_perplexity=1369.148, train_loss=7.221944

Batch 33820, train_perplexity=1478.5704, train_loss=7.298831

Batch 33830, train_perplexity=1401.1405, train_loss=7.245042

Batch 33840, train_perplexity=1585.6696, train_loss=7.368762

Batch 33850, train_perplexity=1451.025, train_loss=7.2800255

Batch 33860, train_perplexity=1296.659, train_loss=7.1675463

Batch 33870, train_perplexity=1430.7397, train_loss=7.265947

Batch 33880, train_perplexity=1484.189, train_loss=7.3026237

Batch 33890, train_perplexity=1379.7639, train_loss=7.2296677

Batch 33900, train_perplexity=1267.7507, train_loss=7.1449995

Batch 33910, train_perplexity=1538.4102, train_loss=7.338505

Batch 33920, train_perplexity=1455.7168, train_loss=7.2832537

Batch 33930, train_perplexity=1441.3837, train_loss=7.273359

Batch 33940, train_perplexity=1388.3726, train_loss=7.2358875

Batch 33950, train_perplexity=1311.0306, train_loss=7.178569

Batch 33960, train_perplexity=1327.5408, train_loss=7.1910834

Batch 33970, train_perplexity=1357.8182, train_loss=7.2136345

Batch 33980, train_perplexity=1410.555, train_loss=7.2517385

Batch 33990, train_perplexity=1400.5941, train_loss=7.244652

Batch 34000, train_perplexity=1404.7463, train_loss=7.247612

Batch 34010, train_perplexity=1383.7145, train_loss=7.232527

Batch 34020, train_perplexity=1245.9114, train_loss=7.1276226

Batch 34030, train_perplexity=1374.7281, train_loss=7.2260113

Batch 34040, train_perplexity=1554.2764, train_loss=7.3487654

Batch 34050, train_perplexity=1412.5952, train_loss=7.253184

Batch 34060, train_perplexity=1342.9159, train_loss=7.2025986

Batch 34070, train_perplexity=1400.0986, train_loss=7.244298

Batch 34080, train_perplexity=1290.1635, train_loss=7.162524

Batch 34090, train_perplexity=1380.8947, train_loss=7.230487

Batch 34100, train_perplexity=1389.2036, train_loss=7.236486

Batch 34110, train_perplexity=1365.726, train_loss=7.2194414

Batch 34120, train_perplexity=1261.7855, train_loss=7.140283

Batch 34130, train_perplexity=1184.2853, train_loss=7.0768948

Batch 34140, train_perplexity=1326.8116, train_loss=7.190534

Batch 34150, train_perplexity=1558.9607, train_loss=7.3517747

Batch 34160, train_perplexity=1493.1359, train_loss=7.308634

Batch 34170, train_perplexity=1368.379, train_loss=7.221382

Batch 34180, train_perplexity=1401.0643, train_loss=7.2449875

Batch 34190, train_perplexity=1493.4221, train_loss=7.3088255

Batch 34200, train_perplexity=1507.9138, train_loss=7.3184824

Batch 34210, train_perplexity=1446.6628, train_loss=7.2770147

Batch 34220, train_perplexity=1424.1129, train_loss=7.2613044

Batch 34230, train_perplexity=1618.3818, train_loss=7.389182

Batch 34240, train_perplexity=1396.726, train_loss=7.241886

Batch 34250, train_perplexity=1381.739, train_loss=7.231098

Batch 34260, train_perplexity=1568.4385, train_loss=7.357836

Batch 34270, train_perplexity=1485.1292, train_loss=7.303257

Batch 34280, train_perplexity=1387.3508, train_loss=7.2351513

Batch 34290, train_perplexity=1339.9237, train_loss=7.200368

Batch 34300, train_perplexity=1438.8882, train_loss=7.271626

Batch 34310, train_perplexity=1566.9867, train_loss=7.3569098

Batch 34320, train_perplexity=1547.1254, train_loss=7.344154

Batch 34330, train_perplexity=1312.5507, train_loss=7.1797276

Batch 34340, train_perplexity=1425.2155, train_loss=7.2620783

Batch 34350, train_perplexity=1281.9253, train_loss=7.1561184

Batch 34360, train_perplexity=1376.3337, train_loss=7.2271786

Batch 34370, train_perplexity=1433.2382, train_loss=7.2676916

Batch 34380, train_perplexity=1456.3097, train_loss=7.283661

Batch 34390, train_perplexity=1331.9075, train_loss=7.1943674

Batch 34400, train_perplexity=1351.9366, train_loss=7.2092934

Batch 34410, train_perplexity=1454.259, train_loss=7.282252

Batch 34420, train_perplexity=1397.5933, train_loss=7.242507

Batch 34430, train_perplexity=1311.8285, train_loss=7.1791773

Batch 34440, train_perplexity=1403.6088, train_loss=7.246802

Batch 34450, train_perplexity=1407.5476, train_loss=7.249604

Batch 34460, train_perplexity=1465.7104, train_loss=7.2900953

Batch 34470, train_perplexity=1488.3285, train_loss=7.305409

Batch 34480, train_perplexity=1353.3846, train_loss=7.210364

Batch 34490, train_perplexity=1477.5492, train_loss=7.29814
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34500, train_perplexity=1355.1805, train_loss=7.21169

Batch 34510, train_perplexity=1286.6187, train_loss=7.159773

Batch 34520, train_perplexity=1474.9658, train_loss=7.29639

Batch 34530, train_perplexity=1435.9244, train_loss=7.269564

Batch 34540, train_perplexity=1268.849, train_loss=7.1458654

Batch 34550, train_perplexity=1450.4038, train_loss=7.2795973

Batch 34560, train_perplexity=1442.2788, train_loss=7.2739797

Batch 34570, train_perplexity=1525.4297, train_loss=7.3300314

Batch 34580, train_perplexity=1436.6134, train_loss=7.270044

Batch 34590, train_perplexity=1458.6545, train_loss=7.2852697

Batch 34600, train_perplexity=1393.8318, train_loss=7.239812

Batch 34610, train_perplexity=1424.9498, train_loss=7.261892

Batch 34620, train_perplexity=1538.8181, train_loss=7.33877

Batch 34630, train_perplexity=1348.3667, train_loss=7.2066493

Batch 34640, train_perplexity=1382.0192, train_loss=7.231301

Batch 34650, train_perplexity=1449.4814, train_loss=7.278961

Batch 34660, train_perplexity=1338.0089, train_loss=7.198938

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Batch 34670, train_perplexity=1402.563, train_loss=7.2460566

Batch 34680, train_perplexity=1483.9215, train_loss=7.3024435

Batch 34690, train_perplexity=1355.6794, train_loss=7.212058

Batch 34700, train_perplexity=1340.27, train_loss=7.2006264

Batch 34710, train_perplexity=1375.4586, train_loss=7.2265425

Batch 34720, train_perplexity=1351.9224, train_loss=7.209283

Batch 34730, train_perplexity=1431.4146, train_loss=7.2664185

Batch 34740, train_perplexity=1198.9146, train_loss=7.089172

Batch 34750, train_perplexity=1458.1149, train_loss=7.2848997

Batch 34760, train_perplexity=1355.885, train_loss=7.2122097

Batch 34770, train_perplexity=1362.5504, train_loss=7.2171135

Batch 34780, train_perplexity=1508.5985, train_loss=7.3189363

Batch 34790, train_perplexity=1441.2977, train_loss=7.273299

Batch 34800, train_perplexity=1447.1293, train_loss=7.277337

Batch 34810, train_perplexity=1266.7295, train_loss=7.1441936

Batch 34820, train_perplexity=1489.0562, train_loss=7.3058977

Batch 34830, train_perplexity=1421.4446, train_loss=7.259429

Batch 34840, train_perplexity=1314.8715, train_loss=7.181494

Batch 34850, train_perplexity=1483.1929, train_loss=7.3019524

Batch 34860, train_perplexity=1285.3634, train_loss=7.158797

Batch 34870, train_perplexity=1451.0637, train_loss=7.280052

Batch 34880, train_perplexity=1427.5334, train_loss=7.2637033

Batch 34890, train_perplexity=1492.4412, train_loss=7.3081684

Batch 34900, train_perplexity=1460.2662, train_loss=7.286374

Batch 34910, train_perplexity=1331.1792, train_loss=7.1938205

Batch 34920, train_perplexity=1355.4803, train_loss=7.211911

Batch 34930, train_perplexity=1487.807, train_loss=7.3050585

Batch 34940, train_perplexity=1356.7847, train_loss=7.212873

Batch 34950, train_perplexity=1339.3027, train_loss=7.1999044

Batch 34960, train_perplexity=1401.8729, train_loss=7.2455645

Batch 34970, train_perplexity=1418.9025, train_loss=7.257639

Batch 34980, train_perplexity=1291.5004, train_loss=7.16356

Batch 34990, train_perplexity=1424.731, train_loss=7.2617383

Batch 35000, train_perplexity=1296.3091, train_loss=7.1672764

Batch 35010, train_perplexity=1465.9718, train_loss=7.2902737

Batch 35020, train_perplexity=1374.4698, train_loss=7.2258234

Batch 35030, train_perplexity=1311.1056, train_loss=7.178626

Batch 35040, train_perplexity=1433.409, train_loss=7.267811

Batch 35050, train_perplexity=1527.7161, train_loss=7.331529

Batch 35060, train_perplexity=1420.979, train_loss=7.2591014

Batch 35070, train_perplexity=1414.6356, train_loss=7.254627

Batch 35080, train_perplexity=1266.5785, train_loss=7.1440744

Batch 35090, train_perplexity=1495.5471, train_loss=7.3102474

Batch 35100, train_perplexity=1415.8192, train_loss=7.2554636

Batch 35110, train_perplexity=1465.0061, train_loss=7.2896147

Batch 35120, train_perplexity=1413.8573, train_loss=7.254077

Batch 35130, train_perplexity=1521.2921, train_loss=7.3273153

Batch 35140, train_perplexity=1416.485, train_loss=7.255934

Batch 35150, train_perplexity=1460.0463, train_loss=7.2862234

Batch 35160, train_perplexity=1373.6168, train_loss=7.2252026

Batch 35170, train_perplexity=1431.4999, train_loss=7.266478

Batch 35180, train_perplexity=1380.2258, train_loss=7.2300024

Batch 35190, train_perplexity=1398.6387, train_loss=7.2432547

Batch 35200, train_perplexity=1402.339, train_loss=7.245897

Batch 35210, train_perplexity=1261.9751, train_loss=7.1404333

Batch 35220, train_perplexity=1298.0819, train_loss=7.168643

Batch 35230, train_perplexity=1294.0167, train_loss=7.1655064

Batch 35240, train_perplexity=1238.153, train_loss=7.121376

Batch 35250, train_perplexity=1437.6173, train_loss=7.2707424

Batch 35260, train_perplexity=1478.1228, train_loss=7.298528

Batch 35270, train_perplexity=1277.3301, train_loss=7.1525273

Batch 35280, train_perplexity=1360.4988, train_loss=7.2156067

Batch 35290, train_perplexity=1437.3035, train_loss=7.270524

Batch 35300, train_perplexity=1304.1055, train_loss=7.1732726

Batch 35310, train_perplexity=1462.4983, train_loss=7.2879014

Batch 35320, train_perplexity=1367.0616, train_loss=7.220419

Batch 35330, train_perplexity=1260.4487, train_loss=7.139223

Batch 35340, train_perplexity=1416.0947, train_loss=7.255658

Batch 35350, train_perplexity=1362.1165, train_loss=7.216795

Batch 35360, train_perplexity=1307.1106, train_loss=7.1755743

Batch 35370, train_perplexity=1415.0349, train_loss=7.2549095

Batch 35380, train_perplexity=1436.2771, train_loss=7.2698097

Batch 35390, train_perplexity=1224.077, train_loss=7.1099424

Batch 35400, train_perplexity=1378.233, train_loss=7.2285576

Batch 35410, train_perplexity=1444.6156, train_loss=7.2755985

Batch 35420, train_perplexity=1549.5485, train_loss=7.345719

Batch 35430, train_perplexity=1398.236, train_loss=7.2429667

Batch 35440, train_perplexity=1247.1156, train_loss=7.1285887

Batch 35450, train_perplexity=1402.3597, train_loss=7.2459116

Batch 35460, train_perplexity=1280.6179, train_loss=7.155098

Batch 35470, train_perplexity=1455.3628, train_loss=7.2830105

Batch 35480, train_perplexity=1343.4731, train_loss=7.2030134

Batch 35490, train_perplexity=1518.0631, train_loss=7.3251905

Batch 35500, train_perplexity=1406.2032, train_loss=7.2486486

Batch 35510, train_perplexity=1492.3856, train_loss=7.308131

Batch 35520, train_perplexity=1419.5521, train_loss=7.2580967

Batch 35530, train_perplexity=1524.0396, train_loss=7.3291197

Batch 35540, train_perplexity=1439.6589, train_loss=7.2721615

Batch 35550, train_perplexity=1319.2394, train_loss=7.1848106

Batch 35560, train_perplexity=1505.4366, train_loss=7.3168383

Batch 35570, train_perplexity=1477.6936, train_loss=7.298238

Batch 35580, train_perplexity=1587.2899, train_loss=7.3697834

Batch 35590, train_perplexity=1400.0626, train_loss=7.244272

Batch 35600, train_perplexity=1325.6954, train_loss=7.1896925

Batch 35610, train_perplexity=1280.4603, train_loss=7.154975

Batch 35620, train_perplexity=1434.2842, train_loss=7.268421

Batch 35630, train_perplexity=1440.6519, train_loss=7.272851

Batch 35640, train_perplexity=1311.3295, train_loss=7.178797

Batch 35650, train_perplexity=1343.4373, train_loss=7.2029867

Batch 35660, train_perplexity=1386.8679, train_loss=7.234803

Batch 35670, train_perplexity=1475.1943, train_loss=7.296545

Batch 35680, train_perplexity=1335.237, train_loss=7.196864

Batch 35690, train_perplexity=1417.608, train_loss=7.2567263

Batch 35700, train_perplexity=1404.962, train_loss=7.2477655

Batch 35710, train_perplexity=1399.7916, train_loss=7.2440786

Batch 35720, train_perplexity=1274.9186, train_loss=7.1506376

Batch 35730, train_perplexity=1441.4077, train_loss=7.2733755

Batch 35740, train_perplexity=1400.6301, train_loss=7.2446775
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 35750, train_perplexity=1421.0746, train_loss=7.2591686

Batch 35760, train_perplexity=1406.8069, train_loss=7.249078

Batch 35770, train_perplexity=1306.7291, train_loss=7.1752825

Batch 35780, train_perplexity=1379.0101, train_loss=7.229121

Batch 35790, train_perplexity=1367.4769, train_loss=7.2207227

Batch 35800, train_perplexity=1422.0168, train_loss=7.2598314

Batch 35810, train_perplexity=1254.74, train_loss=7.1346836

Batch 35820, train_perplexity=1482.7346, train_loss=7.3016434

Batch 35830, train_perplexity=1364.622, train_loss=7.2186327

Batch 35840, train_perplexity=1418.4045, train_loss=7.257288

Batch 35850, train_perplexity=1427.5157, train_loss=7.263691

Batch 35860, train_perplexity=1406.6687, train_loss=7.2489796

Batch 35870, train_perplexity=1505.932, train_loss=7.3171673

Batch 35880, train_perplexity=1382.9453, train_loss=7.231971

Batch 35890, train_perplexity=1366.6276, train_loss=7.2201014

Batch 35900, train_perplexity=1434.6467, train_loss=7.268674

Batch 35910, train_perplexity=1488.6245, train_loss=7.305608

Batch 35920, train_perplexity=1397.1855, train_loss=7.242215

Batch 35930, train_perplexity=1366.234, train_loss=7.2198133

Batch 35940, train_perplexity=1392.721, train_loss=7.2390146

Batch 35950, train_perplexity=1456.2097, train_loss=7.283592

Batch 35960, train_perplexity=1390.6405, train_loss=7.2375197

Batch 35970, train_perplexity=1499.4546, train_loss=7.3128567

Batch 35980, train_perplexity=1353.1724, train_loss=7.210207

Batch 35990, train_perplexity=1335.0085, train_loss=7.196693

Batch 36000, train_perplexity=1262.3699, train_loss=7.140746

Batch 36010, train_perplexity=1425.2916, train_loss=7.2621317

Batch 36020, train_perplexity=1391.9296, train_loss=7.238446

Batch 36030, train_perplexity=1386.0773, train_loss=7.234233

Batch 36040, train_perplexity=1423.5596, train_loss=7.2609158

Batch 36050, train_perplexity=1645.5193, train_loss=7.4058113

Batch 36060, train_perplexity=1347.384, train_loss=7.20592

Batch 36070, train_perplexity=1236.9468, train_loss=7.1204014

Batch 36080, train_perplexity=1281.6619, train_loss=7.155913

Batch 36090, train_perplexity=1339.9672, train_loss=7.2004004

Batch 36100, train_perplexity=1217.2078, train_loss=7.104315

Batch 36110, train_perplexity=1368.6714, train_loss=7.221596

Batch 36120, train_perplexity=1364.7079, train_loss=7.2186956

Batch 36130, train_perplexity=1436.8745, train_loss=7.2702255

Batch 36140, train_perplexity=1362.3035, train_loss=7.2169323

Batch 36150, train_perplexity=1310.9475, train_loss=7.1785054

Batch 36160, train_perplexity=1400.5568, train_loss=7.244625

Batch 36170, train_perplexity=1344.1523, train_loss=7.203519

Batch 36180, train_perplexity=1205.5869, train_loss=7.094722

Batch 36190, train_perplexity=1287.8573, train_loss=7.160735

Batch 36200, train_perplexity=1574.825, train_loss=7.3618994

Batch 36210, train_perplexity=1389.5237, train_loss=7.2367163

Batch 36220, train_perplexity=1322.6931, train_loss=7.187425

Batch 36230, train_perplexity=1441.0544, train_loss=7.2731304

Batch 36240, train_perplexity=1367.7195, train_loss=7.2209

Batch 36250, train_perplexity=1284.5137, train_loss=7.1581354

Batch 36260, train_perplexity=1321.7019, train_loss=7.1866755

Batch 36270, train_perplexity=1444.5447, train_loss=7.2755494

Batch 36280, train_perplexity=1491.7524, train_loss=7.307707

Batch 36290, train_perplexity=1343.2092, train_loss=7.202817

Batch 36300, train_perplexity=1315.5332, train_loss=7.1819973

Batch 36310, train_perplexity=1314.2572, train_loss=7.181027

Batch 36320, train_perplexity=1465.396, train_loss=7.2898808

Batch 36330, train_perplexity=1351.7084, train_loss=7.2091246

Batch 36340, train_perplexity=1351.3114, train_loss=7.208831

Batch 36350, train_perplexity=1344.2921, train_loss=7.203623

Batch 36360, train_perplexity=1485.4705, train_loss=7.303487

Batch 36370, train_perplexity=1312.7308, train_loss=7.179865

Batch 36380, train_perplexity=1435.6897, train_loss=7.2694006

Batch 36390, train_perplexity=1399.0215, train_loss=7.2435284

Batch 36400, train_perplexity=1332.3452, train_loss=7.194696

Batch 36410, train_perplexity=1309.8752, train_loss=7.177687

Batch 36420, train_perplexity=1501.4192, train_loss=7.314166

Batch 36430, train_perplexity=1401.5402, train_loss=7.245327

Batch 36440, train_perplexity=1341.2673, train_loss=7.2013702

Batch 36450, train_perplexity=1328.559, train_loss=7.19185

Batch 36460, train_perplexity=1327.9719, train_loss=7.191408

Batch 36470, train_perplexity=1439.5573, train_loss=7.272091

Batch 36480, train_perplexity=1297.8041, train_loss=7.168429

Batch 36490, train_perplexity=1532.8875, train_loss=7.3349085

Batch 36500, train_perplexity=1357.9854, train_loss=7.2137575

Batch 36510, train_perplexity=1309.7235, train_loss=7.1775713

Batch 36520, train_perplexity=1359.6635, train_loss=7.2149925

Batch 36530, train_perplexity=1360.2964, train_loss=7.215458

Batch 36540, train_perplexity=1280.632, train_loss=7.155109

Batch 36550, train_perplexity=1483.8309, train_loss=7.3023825

Batch 36560, train_perplexity=1329.3582, train_loss=7.1924515

Batch 36570, train_perplexity=1534.4789, train_loss=7.335946

Batch 36580, train_perplexity=1378.9102, train_loss=7.2290487

Batch 36590, train_perplexity=1360.3678, train_loss=7.2155104

Batch 36600, train_perplexity=1335.4421, train_loss=7.1970177

Batch 36610, train_perplexity=1262.2604, train_loss=7.1406593

Batch 36620, train_perplexity=1386.8164, train_loss=7.234766

Batch 36630, train_perplexity=1286.2151, train_loss=7.159459

Batch 36640, train_perplexity=1367.2123, train_loss=7.220529

Batch 36650, train_perplexity=1297.1674, train_loss=7.167938

Batch 36660, train_perplexity=1309.0966, train_loss=7.1770926

Batch 36670, train_perplexity=1223.399, train_loss=7.1093884

Batch 36680, train_perplexity=1409.3173, train_loss=7.2508607

Batch 36690, train_perplexity=1440.0338, train_loss=7.272422

Batch 36700, train_perplexity=1254.2698, train_loss=7.134309

Batch 36710, train_perplexity=1330.8588, train_loss=7.1935797

Batch 36720, train_perplexity=1288.211, train_loss=7.16101

Batch 36730, train_perplexity=1404.7369, train_loss=7.2476053

Batch 36740, train_perplexity=1269.6484, train_loss=7.1464953

Batch 36750, train_perplexity=1411.7421, train_loss=7.2525797

Batch 36760, train_perplexity=1454.6523, train_loss=7.282522

Batch 36770, train_perplexity=1315.7928, train_loss=7.1821947

Batch 36780, train_perplexity=1295.3866, train_loss=7.1665645

Batch 36790, train_perplexity=1468.9576, train_loss=7.2923083

Batch 36800, train_perplexity=1468.1257, train_loss=7.291742

Batch 36810, train_perplexity=1417.7013, train_loss=7.256792

Batch 36820, train_perplexity=1359.4431, train_loss=7.2148304

Batch 36830, train_perplexity=1421.9707, train_loss=7.259799

Batch 36840, train_perplexity=1342.6432, train_loss=7.2023954

Batch 36850, train_perplexity=1472.5511, train_loss=7.2947516

Batch 36860, train_perplexity=1491.9573, train_loss=7.307844

Batch 36870, train_perplexity=1562.1583, train_loss=7.3538237

Batch 36880, train_perplexity=1353.3904, train_loss=7.210368

Batch 36890, train_perplexity=1402.5717, train_loss=7.2460628

Batch 36900, train_perplexity=1371.0844, train_loss=7.223357

Batch 36910, train_perplexity=1352.19, train_loss=7.209481

Batch 36920, train_perplexity=1473.6702, train_loss=7.2955112

Batch 36930, train_perplexity=1403.2909, train_loss=7.2465754

Batch 36940, train_perplexity=1377.421, train_loss=7.227968

Batch 36950, train_perplexity=1288.7948, train_loss=7.161463

Batch 36960, train_perplexity=1415.3947, train_loss=7.2551637

Batch 36970, train_perplexity=1396.2239, train_loss=7.2415266

Batch 36980, train_perplexity=1410.0654, train_loss=7.2513914

Batch 36990, train_perplexity=1278.4531, train_loss=7.153406

Batch 37000, train_perplexity=1299.4556, train_loss=7.1697006

Batch 37010, train_perplexity=1398.4733, train_loss=7.2431364

Batch 37020, train_perplexity=1370.7732, train_loss=7.22313

Batch 37030, train_perplexity=1443.4078, train_loss=7.274762

Batch 37040, train_perplexity=1375.115, train_loss=7.2262926

Batch 37050, train_perplexity=1399.7382, train_loss=7.2440405

Batch 37060, train_perplexity=1410.0735, train_loss=7.251397
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37070, train_perplexity=1448.9376, train_loss=7.278586

Batch 37080, train_perplexity=1367.5657, train_loss=7.2207875

Batch 37090, train_perplexity=1397.4268, train_loss=7.242388

Batch 37100, train_perplexity=1305.2626, train_loss=7.1741595

Batch 37110, train_perplexity=1421.1248, train_loss=7.259204

Batch 37120, train_perplexity=1346.7565, train_loss=7.2054543

Batch 37130, train_perplexity=1422.1016, train_loss=7.259891

Batch 37140, train_perplexity=1441.9103, train_loss=7.273724

Batch 37150, train_perplexity=1325.7075, train_loss=7.1897016

Batch 37160, train_perplexity=1415.8624, train_loss=7.255494

Batch 37170, train_perplexity=1257.1487, train_loss=7.1366014

Batch 37180, train_perplexity=1418.2395, train_loss=7.2571716

Batch 37190, train_perplexity=1494.7672, train_loss=7.3097258

Batch 37200, train_perplexity=1367.46, train_loss=7.2207103

Batch 37210, train_perplexity=1322.0184, train_loss=7.186915

Batch 37220, train_perplexity=1501.1235, train_loss=7.313969

Batch 37230, train_perplexity=1336.1995, train_loss=7.1975846

Batch 37240, train_perplexity=1278.5232, train_loss=7.153461

Batch 37250, train_perplexity=1386.7985, train_loss=7.234753

Batch 37260, train_perplexity=1364.5042, train_loss=7.2185464

Batch 37270, train_perplexity=1371.0027, train_loss=7.2232976

Batch 37280, train_perplexity=1439.0542, train_loss=7.2717414

Batch 37290, train_perplexity=1253.0623, train_loss=7.1333456

Batch 37300, train_perplexity=1533.5806, train_loss=7.3353605

Batch 37310, train_perplexity=1439.1235, train_loss=7.2717896

Batch 37320, train_perplexity=1406.2744, train_loss=7.248699

Batch 37330, train_perplexity=1315.9685, train_loss=7.182328

Batch 37340, train_perplexity=1259.5505, train_loss=7.13851

Batch 37350, train_perplexity=1293.1068, train_loss=7.164803

Batch 37360, train_perplexity=1373.6993, train_loss=7.2252626

Batch 37370, train_perplexity=1414.9459, train_loss=7.2548466

Batch 37380, train_perplexity=1228.1205, train_loss=7.1132402

Batch 37390, train_perplexity=1297.7477, train_loss=7.1683855

Batch 37400, train_perplexity=1276.6584, train_loss=7.1520014

Batch 37410, train_perplexity=1318.6274, train_loss=7.1843467

Batch 37420, train_perplexity=1421.7937, train_loss=7.2596745

Batch 37430, train_perplexity=1275.9805, train_loss=7.15147

Batch 37440, train_perplexity=1363.8647, train_loss=7.2180777

Batch 37450, train_perplexity=1333.8097, train_loss=7.1957946

Batch 37460, train_perplexity=1427.0869, train_loss=7.2633905

Batch 37470, train_perplexity=1372.7656, train_loss=7.2245827

Batch 37480, train_perplexity=1321.4153, train_loss=7.1864586

Batch 37490, train_perplexity=1446.8429, train_loss=7.277139

Batch 37500, train_perplexity=1389.6097, train_loss=7.2367783

Batch 37510, train_perplexity=1373.6365, train_loss=7.225217

Batch 37520, train_perplexity=1423.7184, train_loss=7.2610273

Batch 37530, train_perplexity=1374.8304, train_loss=7.2260857

Batch 37540, train_perplexity=1507.3344, train_loss=7.318098

Batch 37550, train_perplexity=1487.197, train_loss=7.3046484

Batch 37560, train_perplexity=1400.1721, train_loss=7.2443504

Batch 37570, train_perplexity=1376.0162, train_loss=7.226948

Batch 37580, train_perplexity=1445.3507, train_loss=7.2761073

Batch 37590, train_perplexity=1332.0472, train_loss=7.1944723

Batch 37600, train_perplexity=1279.9646, train_loss=7.1545877

Batch 37610, train_perplexity=1257.4395, train_loss=7.1368327

Batch 37620, train_perplexity=1315.5206, train_loss=7.181988

Batch 37630, train_perplexity=1291.5756, train_loss=7.163618

Batch 37640, train_perplexity=1368.4678, train_loss=7.221447

Batch 37650, train_perplexity=1308.1587, train_loss=7.176376

Batch 37660, train_perplexity=1351.1335, train_loss=7.208699

Batch 37670, train_perplexity=1337.1919, train_loss=7.198327

Batch 37680, train_perplexity=1259.0264, train_loss=7.138094

Batch 37690, train_perplexity=1345.5657, train_loss=7.20457

Batch 37700, train_perplexity=1429.9547, train_loss=7.265398

Batch 37710, train_perplexity=1403.4508, train_loss=7.2466893

Batch 37720, train_perplexity=1227.8746, train_loss=7.11304

Batch 37730, train_perplexity=1248.5984, train_loss=7.129777

Batch 37740, train_perplexity=1343.5359, train_loss=7.20306

Batch 37750, train_perplexity=1403.5933, train_loss=7.246791

Batch 37760, train_perplexity=1376.221, train_loss=7.2270966

Batch 37770, train_perplexity=1252.674, train_loss=7.1330357

Batch 37780, train_perplexity=1401.1084, train_loss=7.245019

Batch 37790, train_perplexity=1259.5433, train_loss=7.1385045

Batch 37800, train_perplexity=1332.4843, train_loss=7.1948004

Batch 37810, train_perplexity=1292.4491, train_loss=7.1642942

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Batch 37820, train_perplexity=1452.6643, train_loss=7.2811546

Batch 37830, train_perplexity=1462.7494, train_loss=7.288073

Batch 37840, train_perplexity=1246.2644, train_loss=7.127906

Batch 37850, train_perplexity=1237.94, train_loss=7.121204

Batch 37860, train_perplexity=1367.6738, train_loss=7.2208667

Batch 37870, train_perplexity=1346.1896, train_loss=7.2050333

Batch 37880, train_perplexity=1269.5087, train_loss=7.146385

Batch 37890, train_perplexity=1307.0757, train_loss=7.1755476

Batch 37900, train_perplexity=1221.189, train_loss=7.10758

Batch 37910, train_perplexity=1395.5289, train_loss=7.241029

Batch 37920, train_perplexity=1301.7738, train_loss=7.171483

Batch 37930, train_perplexity=1341.151, train_loss=7.2012835

Batch 37940, train_perplexity=1362.6406, train_loss=7.21718

Batch 37950, train_perplexity=1284.9572, train_loss=7.1584806

Batch 37960, train_perplexity=1439.1345, train_loss=7.271797

Batch 37970, train_perplexity=1378.8793, train_loss=7.2290263

Batch 37980, train_perplexity=1424.7208, train_loss=7.261731

Batch 37990, train_perplexity=1294.6746, train_loss=7.1660147

Batch 38000, train_perplexity=1325.9521, train_loss=7.189886

Batch 38010, train_perplexity=1298.0912, train_loss=7.16865

Batch 38020, train_perplexity=1266.6848, train_loss=7.1441584

Batch 38030, train_perplexity=1426.189, train_loss=7.262761

Batch 38040, train_perplexity=1375.0979, train_loss=7.22628

Batch 38050, train_perplexity=1348.0177, train_loss=7.2063904

Batch 38060, train_perplexity=1496.4182, train_loss=7.3108296

Batch 38070, train_perplexity=1410.9028, train_loss=7.251985

Batch 38080, train_perplexity=1394.4673, train_loss=7.2402678

Batch 38090, train_perplexity=1335.2345, train_loss=7.196862

Batch 38100, train_perplexity=1345.3918, train_loss=7.2044406

Batch 38110, train_perplexity=1322.5228, train_loss=7.1872964

Batch 38120, train_perplexity=1391.339, train_loss=7.238022

Batch 38130, train_perplexity=1326.9382, train_loss=7.1906295

Batch 38140, train_perplexity=1387.1458, train_loss=7.2350035

Batch 38150, train_perplexity=1217.6316, train_loss=7.104663

Batch 38160, train_perplexity=1396.2625, train_loss=7.2415543

Batch 38170, train_perplexity=1362.4646, train_loss=7.2170506

Batch 38180, train_perplexity=1391.4133, train_loss=7.2380753

Batch 38190, train_perplexity=1415.7031, train_loss=7.2553816

Batch 38200, train_perplexity=1365.9058, train_loss=7.219573

Batch 38210, train_perplexity=1232.1244, train_loss=7.116495

Batch 38220, train_perplexity=1312.6119, train_loss=7.1797743

Batch 38230, train_perplexity=1330.4501, train_loss=7.1932726

Batch 38240, train_perplexity=1481.0331, train_loss=7.300495

Batch 38250, train_perplexity=1459.1241, train_loss=7.2855916

Batch 38260, train_perplexity=1292.7795, train_loss=7.16455

Batch 38270, train_perplexity=1348.6888, train_loss=7.206888

Batch 38280, train_perplexity=1489.4559, train_loss=7.306166

Batch 38290, train_perplexity=1333.1688, train_loss=7.195314

Batch 38300, train_perplexity=1417.5337, train_loss=7.256674

Batch 38310, train_perplexity=1292.4725, train_loss=7.1643124
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38320, train_perplexity=1448.7186, train_loss=7.2784348

Batch 38330, train_perplexity=1335.2281, train_loss=7.1968575

Batch 38340, train_perplexity=1273.7823, train_loss=7.149746

Batch 38350, train_perplexity=1262.6758, train_loss=7.1409883

Batch 38360, train_perplexity=1379.4928, train_loss=7.229471

Batch 38370, train_perplexity=1241.9894, train_loss=7.1244698

Batch 38380, train_perplexity=1346.7366, train_loss=7.2054396

Batch 38390, train_perplexity=1341.0671, train_loss=7.201221

Batch 38400, train_perplexity=1474.2802, train_loss=7.295925

Batch 38410, train_perplexity=1271.6667, train_loss=7.1480837

Batch 38420, train_perplexity=1320.3357, train_loss=7.1856413

Batch 38430, train_perplexity=1294.8821, train_loss=7.166175

Batch 38440, train_perplexity=1474.8047, train_loss=7.296281

Batch 38450, train_perplexity=1267.3983, train_loss=7.1447215

Batch 38460, train_perplexity=1253.4763, train_loss=7.133676

Batch 38470, train_perplexity=1448.6248, train_loss=7.27837

Batch 38480, train_perplexity=1389.2063, train_loss=7.236488

Batch 38490, train_perplexity=1324.5739, train_loss=7.188846

Batch 38500, train_perplexity=1382.3829, train_loss=7.231564

Batch 38510, train_perplexity=1452.1124, train_loss=7.2807746

Batch 38520, train_perplexity=1298.6044, train_loss=7.1690454

Batch 38530, train_perplexity=1304.1819, train_loss=7.1733313

Batch 38540, train_perplexity=1397.7599, train_loss=7.242626

Batch 38550, train_perplexity=1376.3337, train_loss=7.2271786

Batch 38560, train_perplexity=1375.5269, train_loss=7.226592

Batch 38570, train_perplexity=1338.8903, train_loss=7.1995964

Batch 38580, train_perplexity=1394.7811, train_loss=7.240493

Batch 38590, train_perplexity=1281.3514, train_loss=7.1556706

Batch 38600, train_perplexity=1204.3137, train_loss=7.093665

Batch 38610, train_perplexity=1311.0131, train_loss=7.1785555

Batch 38620, train_perplexity=1235.3735, train_loss=7.1191287

Batch 38630, train_perplexity=1299.5931, train_loss=7.1698065

Batch 38640, train_perplexity=1364.095, train_loss=7.2182465

Batch 38650, train_perplexity=1424.8384, train_loss=7.2618136

Batch 38660, train_perplexity=1221.2657, train_loss=7.107643

Batch 38670, train_perplexity=1367.5897, train_loss=7.220805

Batch 38680, train_perplexity=1322.8489, train_loss=7.187543

Batch 38690, train_perplexity=1420.809, train_loss=7.2589817

Batch 38700, train_perplexity=1286.8788, train_loss=7.159975

Batch 38710, train_perplexity=1309.3713, train_loss=7.1773024

Batch 38720, train_perplexity=1347.6461, train_loss=7.206115

Batch 38730, train_perplexity=1345.1808, train_loss=7.2042837

Batch 38740, train_perplexity=1358.5876, train_loss=7.214201

Batch 38750, train_perplexity=1396.5061, train_loss=7.241729

Batch 38760, train_perplexity=1256.0719, train_loss=7.1357446

Batch 38770, train_perplexity=1315.5494, train_loss=7.1820097

Batch 38780, train_perplexity=1319.9624, train_loss=7.1853585

Batch 38790, train_perplexity=1324.9781, train_loss=7.1891513

Batch 38800, train_perplexity=1382.1266, train_loss=7.2313786

Batch 38810, train_perplexity=1414.3374, train_loss=7.2544165

Batch 38820, train_perplexity=1469.413, train_loss=7.2926183

Batch 38830, train_perplexity=1253.7854, train_loss=7.1339226

Batch 38840, train_perplexity=1353.7777, train_loss=7.2106543

Batch 38850, train_perplexity=1298.6787, train_loss=7.1691027

Batch 38860, train_perplexity=1341.8354, train_loss=7.2017937

Batch 38870, train_perplexity=1315.9133, train_loss=7.1822863

Batch 38880, train_perplexity=1344.746, train_loss=7.2039604

Batch 38890, train_perplexity=1409.4194, train_loss=7.250933

Batch 38900, train_perplexity=1373.3535, train_loss=7.225011

Batch 38910, train_perplexity=1352.5396, train_loss=7.209739

Batch 38920, train_perplexity=1366.0673, train_loss=7.2196913

Batch 38930, train_perplexity=1384.5077, train_loss=7.2331

Batch 38940, train_perplexity=1365.5104, train_loss=7.2192836

Batch 38950, train_perplexity=1287.5393, train_loss=7.160488

Batch 38960, train_perplexity=1248.1014, train_loss=7.129379

Batch 38970, train_perplexity=1322.8483, train_loss=7.1875424

Batch 38980, train_perplexity=1375.353, train_loss=7.2264657

Batch 38990, train_perplexity=1306.3441, train_loss=7.174988

Batch 39000, train_perplexity=1388.1581, train_loss=7.235733

Batch 39010, train_perplexity=1303.3917, train_loss=7.172725

Batch 39020, train_perplexity=1417.8575, train_loss=7.256902

Batch 39030, train_perplexity=1225.3175, train_loss=7.110955

Batch 39040, train_perplexity=1357.479, train_loss=7.2133846

Batch 39050, train_perplexity=1306.2744, train_loss=7.1749344

Batch 39060, train_perplexity=1242.8253, train_loss=7.1251426

Batch 39070, train_perplexity=1315.8412, train_loss=7.1822314

Batch 39080, train_perplexity=1367.8389, train_loss=7.2209873

Batch 39090, train_perplexity=1429.1176, train_loss=7.2648125

Batch 39100, train_perplexity=1305.2744, train_loss=7.1741686

Batch 39110, train_perplexity=1273.1908, train_loss=7.1492815

Batch 39120, train_perplexity=1315.6348, train_loss=7.1820745

Batch 39130, train_perplexity=1317.2675, train_loss=7.183315

Batch 39140, train_perplexity=1300.5248, train_loss=7.170523

Batch 39150, train_perplexity=1302.7338, train_loss=7.17222

Batch 39160, train_perplexity=1311.6622, train_loss=7.1790504

Batch 39170, train_perplexity=1279.2654, train_loss=7.1540413

Batch 39180, train_perplexity=1366.7631, train_loss=7.2202005

Batch 39190, train_perplexity=1432.5303, train_loss=7.2671976

Batch 39200, train_perplexity=1307.2415, train_loss=7.1756744

Batch 39210, train_perplexity=1401.031, train_loss=7.2449636

Batch 39220, train_perplexity=1327.5496, train_loss=7.19109

Batch 39230, train_perplexity=1303.1587, train_loss=7.1725464

Batch 39240, train_perplexity=1353.5712, train_loss=7.2105017

Batch 39250, train_perplexity=1362.2463, train_loss=7.2168903

Batch 39260, train_perplexity=1392.4879, train_loss=7.2388473

Batch 39270, train_perplexity=1245.2504, train_loss=7.127092

Batch 39280, train_perplexity=1395.9695, train_loss=7.2413445

Batch 39290, train_perplexity=1319.3193, train_loss=7.184871

Batch 39300, train_perplexity=1259.6544, train_loss=7.1385927

Batch 39310, train_perplexity=1374.8855, train_loss=7.2261257

Batch 39320, train_perplexity=1206.3667, train_loss=7.0953684

Batch 39330, train_perplexity=1376.7053, train_loss=7.2274485

Batch 39340, train_perplexity=1363.1859, train_loss=7.21758

Batch 39350, train_perplexity=1292.5194, train_loss=7.1643486

Batch 39360, train_perplexity=1453.4972, train_loss=7.281728

Batch 39370, train_perplexity=1427.3448, train_loss=7.2635713

Batch 39380, train_perplexity=1286.723, train_loss=7.159854

Batch 39390, train_perplexity=1193.0582, train_loss=7.0842752

Batch 39400, train_perplexity=1425.6791, train_loss=7.2624035

Batch 39410, train_perplexity=1507.609, train_loss=7.31828

Batch 39420, train_perplexity=1410.6317, train_loss=7.251793

Batch 39430, train_perplexity=1330.756, train_loss=7.1935024

Batch 39440, train_perplexity=1366.9052, train_loss=7.2203045

Batch 39450, train_perplexity=1258.6458, train_loss=7.1377916

Batch 39460, train_perplexity=1342.615, train_loss=7.2023745

Batch 39470, train_perplexity=1404.4851, train_loss=7.247426

Batch 39480, train_perplexity=1412.9663, train_loss=7.2534466

Batch 39490, train_perplexity=1328.663, train_loss=7.1919284

Batch 39500, train_perplexity=1280.779, train_loss=7.155224

Batch 39510, train_perplexity=1339.3424, train_loss=7.199934

Batch 39520, train_perplexity=1308.7184, train_loss=7.1768036

Batch 39530, train_perplexity=1378.1936, train_loss=7.228529

Batch 39540, train_perplexity=1304.0215, train_loss=7.173208

Batch 39550, train_perplexity=1548.487, train_loss=7.3450336

Batch 39560, train_perplexity=1327.787, train_loss=7.191269

Batch 39570, train_perplexity=1324.8392, train_loss=7.1890464

Batch 39580, train_perplexity=1330.0353, train_loss=7.1929607

Batch 39590, train_perplexity=1341.4478, train_loss=7.2015047

Batch 39600, train_perplexity=1228.6254, train_loss=7.1136513

Batch 39610, train_perplexity=1350.0542, train_loss=7.2079

Batch 39620, train_perplexity=1222.9219, train_loss=7.1089983

Batch 39630, train_perplexity=1304.9894, train_loss=7.17395
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 39640, train_perplexity=1345.2584, train_loss=7.2043414

Batch 39650, train_perplexity=1186.2046, train_loss=7.078514

Batch 39660, train_perplexity=1275.8552, train_loss=7.151372

Batch 39670, train_perplexity=1217.2925, train_loss=7.1043844

Batch 39680, train_perplexity=1285.0387, train_loss=7.158544

Batch 39690, train_perplexity=1445.0634, train_loss=7.2759085

Batch 39700, train_perplexity=1251.6152, train_loss=7.13219

Batch 39710, train_perplexity=1272.2417, train_loss=7.1485357

Batch 39720, train_perplexity=1367.299, train_loss=7.2205925

Batch 39730, train_perplexity=1424.737, train_loss=7.2617426

Batch 39740, train_perplexity=1281.1273, train_loss=7.1554956

Batch 39750, train_perplexity=1345.8108, train_loss=7.204752

Batch 39760, train_perplexity=1309.9027, train_loss=7.177708

Batch 39770, train_perplexity=1372.1387, train_loss=7.224126

Batch 39780, train_perplexity=1288.4974, train_loss=7.161232

Batch 39790, train_perplexity=1309.6897, train_loss=7.1775455

Batch 39800, train_perplexity=1284.2001, train_loss=7.1578913

Batch 39810, train_perplexity=1312.5957, train_loss=7.179762

Batch 39820, train_perplexity=1289.1802, train_loss=7.1617618

Batch 39830, train_perplexity=1491.229, train_loss=7.307356

Batch 39840, train_perplexity=1264.3218, train_loss=7.142291

Batch 39850, train_perplexity=1314.7712, train_loss=7.181418

Batch 39860, train_perplexity=1464.8678, train_loss=7.2895203

Batch 39870, train_perplexity=1372.0549, train_loss=7.224065

Batch 39880, train_perplexity=1261.5088, train_loss=7.140064

Batch 39890, train_perplexity=1361.567, train_loss=7.2163916

Batch 39900, train_perplexity=1272.414, train_loss=7.148671

Batch 39910, train_perplexity=1329.0032, train_loss=7.1921844

Batch 39920, train_perplexity=1322.9738, train_loss=7.1876373

Batch 39930, train_perplexity=1435.1599, train_loss=7.2690315

Batch 39940, train_perplexity=1325.0698, train_loss=7.1892204

Batch 39950, train_perplexity=1372.4528, train_loss=7.2243547

Batch 39960, train_perplexity=1485.778, train_loss=7.303694

Batch 39970, train_perplexity=1375.0599, train_loss=7.2262526

Batch 39980, train_perplexity=1291.1814, train_loss=7.163313

Batch 39990, train_perplexity=1223.0759, train_loss=7.109124

Batch 40000, train_perplexity=1403.8678, train_loss=7.2469864

Batch 40010, train_perplexity=1363.9688, train_loss=7.218154

Batch 40020, train_perplexity=1362.6504, train_loss=7.217187

Batch 40030, train_perplexity=1247.4225, train_loss=7.1288347

Batch 40040, train_perplexity=1275.2172, train_loss=7.1508718

Batch 40050, train_perplexity=1230.4886, train_loss=7.1151667

Batch 40060, train_perplexity=1278.8007, train_loss=7.153678

Batch 40070, train_perplexity=1254.4229, train_loss=7.134431

Batch 40080, train_perplexity=1414.0043, train_loss=7.254181

Batch 40090, train_perplexity=1383.2302, train_loss=7.232177

Batch 40100, train_perplexity=1277.8986, train_loss=7.152972

Batch 40110, train_perplexity=1377.1492, train_loss=7.227771

Batch 40120, train_perplexity=1242.692, train_loss=7.1250353

Batch 40130, train_perplexity=1326.5574, train_loss=7.1903424

Batch 40140, train_perplexity=1332.4926, train_loss=7.1948066

Batch 40150, train_perplexity=1404.3994, train_loss=7.247365

Batch 40160, train_perplexity=1314.6465, train_loss=7.181323

Batch 40170, train_perplexity=1234.4485, train_loss=7.1183796

Batch 40180, train_perplexity=1462.1698, train_loss=7.287677

Batch 40190, train_perplexity=1469.9294, train_loss=7.2929697

Batch 40200, train_perplexity=1219.7502, train_loss=7.1064014

Batch 40210, train_perplexity=1376.9246, train_loss=7.2276077

Batch 40220, train_perplexity=1233.3306, train_loss=7.1174736

Batch 40230, train_perplexity=1300.5316, train_loss=7.1705284

Batch 40240, train_perplexity=1194.4836, train_loss=7.0854692

Batch 40250, train_perplexity=1531.5577, train_loss=7.3340406

Batch 40260, train_perplexity=1233.6553, train_loss=7.117737

Batch 40270, train_perplexity=1396.1599, train_loss=7.241481

Batch 40280, train_perplexity=1309.1465, train_loss=7.1771307

Batch 40290, train_perplexity=1482.6921, train_loss=7.3016148

Batch 40300, train_perplexity=1114.088, train_loss=7.0157914

Batch 40310, train_perplexity=1395.9868, train_loss=7.241357

Batch 40320, train_perplexity=1358.3104, train_loss=7.213997

Batch 40330, train_perplexity=1269.3137, train_loss=7.1462317

Batch 40340, train_perplexity=1221.2157, train_loss=7.107602

Batch 40350, train_perplexity=1355.322, train_loss=7.2117944

Batch 40360, train_perplexity=1309.9658, train_loss=7.1777563

Batch 40370, train_perplexity=1293.4214, train_loss=7.165046

Batch 40380, train_perplexity=1180.402, train_loss=7.0736103

Batch 40390, train_perplexity=1315.8958, train_loss=7.182273

Batch 40400, train_perplexity=1329.5388, train_loss=7.1925874

Batch 40410, train_perplexity=1427.3456, train_loss=7.2635717

Batch 40420, train_perplexity=1175.9177, train_loss=7.069804

Batch 40430, train_perplexity=1298.2349, train_loss=7.168761

Batch 40440, train_perplexity=1295.0513, train_loss=7.1663055

Batch 40450, train_perplexity=1326.2874, train_loss=7.190139

Batch 40460, train_perplexity=1222.8257, train_loss=7.1089196

Batch 40470, train_perplexity=1285.8754, train_loss=7.159195

Batch 40480, train_perplexity=1314.7869, train_loss=7.18143

Batch 40490, train_perplexity=1300.8752, train_loss=7.1707926

Batch 40500, train_perplexity=1214.7076, train_loss=7.1022587

Batch 40510, train_perplexity=1289.2336, train_loss=7.1618032

Batch 40520, train_perplexity=1503.0366, train_loss=7.315243

Batch 40530, train_perplexity=1304.1863, train_loss=7.1733346

Batch 40540, train_perplexity=1205.2708, train_loss=7.0944595

Batch 40550, train_perplexity=1317.8179, train_loss=7.1837325

Batch 40560, train_perplexity=1208.8947, train_loss=7.0974617

Batch 40570, train_perplexity=1349.9557, train_loss=7.207827

Batch 40580, train_perplexity=1284.5809, train_loss=7.158188

Batch 40590, train_perplexity=1376.3273, train_loss=7.227174

Batch 40600, train_perplexity=1424.335, train_loss=7.2614603

Batch 40610, train_perplexity=1290.9536, train_loss=7.1631365

Batch 40620, train_perplexity=1373.2881, train_loss=7.224963

Batch 40630, train_perplexity=1383.5568, train_loss=7.232413

Batch 40640, train_perplexity=1460.7468, train_loss=7.286703

Batch 40650, train_perplexity=1103.0271, train_loss=7.0058136

Batch 40660, train_perplexity=1244.0295, train_loss=7.126111

Batch 40670, train_perplexity=1318.6243, train_loss=7.1843443

Batch 40680, train_perplexity=1331.2192, train_loss=7.1938505

Batch 40690, train_perplexity=1244.1785, train_loss=7.1262307

Batch 40700, train_perplexity=1385.4092, train_loss=7.233751

Batch 40710, train_perplexity=1242.2263, train_loss=7.1246605

Batch 40720, train_perplexity=1292.9546, train_loss=7.1646852

Batch 40730, train_perplexity=1334.5115, train_loss=7.1963205

Batch 40740, train_perplexity=1414.1244, train_loss=7.254266

Batch 40750, train_perplexity=1199.0466, train_loss=7.089282

Batch 40760, train_perplexity=1279.8433, train_loss=7.154493

Batch 40770, train_perplexity=1297.1885, train_loss=7.1679544

Batch 40780, train_perplexity=1307.1094, train_loss=7.1755733

Batch 40790, train_perplexity=1245.8318, train_loss=7.1275587

Batch 40800, train_perplexity=1440.0742, train_loss=7.27245

Batch 40810, train_perplexity=1319.9977, train_loss=7.185385

Batch 40820, train_perplexity=1255.4761, train_loss=7.13527

Batch 40830, train_perplexity=1308.0945, train_loss=7.1763268

Batch 40840, train_perplexity=1391.9747, train_loss=7.2384787

Batch 40850, train_perplexity=1393.1128, train_loss=7.239296

Batch 40860, train_perplexity=1419.72, train_loss=7.258215

Batch 40870, train_perplexity=1261.1208, train_loss=7.139756

Batch 40880, train_perplexity=1337.0554, train_loss=7.198225

Batch 40890, train_perplexity=1222.5062, train_loss=7.1086583

Batch 40900, train_perplexity=1282.7948, train_loss=7.1567965

Batch 40910, train_perplexity=1283.5071, train_loss=7.1573515

Batch 40920, train_perplexity=1398.6206, train_loss=7.243242

Batch 40930, train_perplexity=1330.1722, train_loss=7.1930637

Batch 40940, train_perplexity=1357.4337, train_loss=7.2133512

Batch 40950, train_perplexity=1318.9451, train_loss=7.1845875
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40960, train_perplexity=1325.8903, train_loss=7.1898394

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Batch 40970, train_perplexity=1340.1179, train_loss=7.200513

Batch 40980, train_perplexity=1302.2009, train_loss=7.171811

Batch 40990, train_perplexity=1449.3529, train_loss=7.2788725

Batch 41000, train_perplexity=1281.0405, train_loss=7.155428

Batch 41010, train_perplexity=1309.962, train_loss=7.1777534

Batch 41020, train_perplexity=1348.0858, train_loss=7.206441

Batch 41030, train_perplexity=1289.067, train_loss=7.161674

Batch 41040, train_perplexity=1408.6609, train_loss=7.250395

Batch 41050, train_perplexity=1374.7229, train_loss=7.2260075

Batch 41060, train_perplexity=1432.1976, train_loss=7.2669654

Batch 41070, train_perplexity=1397.8906, train_loss=7.2427197

Batch 41080, train_perplexity=1225.2018, train_loss=7.110861

Batch 41090, train_perplexity=1266.5404, train_loss=7.1440444

Batch 41100, train_perplexity=1303.1818, train_loss=7.172564

Batch 41110, train_perplexity=1362.5516, train_loss=7.2171144

Batch 41120, train_perplexity=1395.5077, train_loss=7.2410135

Batch 41130, train_perplexity=1256.408, train_loss=7.136012

Batch 41140, train_perplexity=1402.7396, train_loss=7.2461824

Batch 41150, train_perplexity=1287.1072, train_loss=7.1601524

Batch 41160, train_perplexity=1325.0597, train_loss=7.189213

Batch 41170, train_perplexity=1241.7762, train_loss=7.124298

Batch 41180, train_perplexity=1353.6357, train_loss=7.2105494

Batch 41190, train_perplexity=1330.303, train_loss=7.193162

Batch 41200, train_perplexity=1428.7354, train_loss=7.264545

Batch 41210, train_perplexity=1452.2003, train_loss=7.280835

Batch 41220, train_perplexity=1361.6372, train_loss=7.216443

Batch 41230, train_perplexity=1460.8116, train_loss=7.2867475

Batch 41240, train_perplexity=1264.5979, train_loss=7.1425095

Batch 41250, train_perplexity=1326.8743, train_loss=7.1905813

Batch 41260, train_perplexity=1286.5917, train_loss=7.159752

Batch 41270, train_perplexity=1465.4266, train_loss=7.2899017

Batch 41280, train_perplexity=1460.6702, train_loss=7.2866507

Batch 41290, train_perplexity=1279.5643, train_loss=7.154275

Batch 41300, train_perplexity=1359.5352, train_loss=7.214898

Batch 41310, train_perplexity=1369.2706, train_loss=7.2220335

Batch 41320, train_perplexity=1404.5065, train_loss=7.2474413

Batch 41330, train_perplexity=1264.4652, train_loss=7.1424046

Batch 41340, train_perplexity=1305.9729, train_loss=7.1747036

Batch 41350, train_perplexity=1335.456, train_loss=7.197028

Batch 41360, train_perplexity=1227.823, train_loss=7.112998

Batch 41370, train_perplexity=1383.5033, train_loss=7.232374

Batch 41380, train_perplexity=1309.1653, train_loss=7.177145

Batch 41390, train_perplexity=1297.0393, train_loss=7.1678395

Batch 41400, train_perplexity=1280.044, train_loss=7.1546497

Batch 41410, train_perplexity=1264.1421, train_loss=7.142149

Batch 41420, train_perplexity=1297.3907, train_loss=7.1681104

Batch 41430, train_perplexity=1377.27, train_loss=7.2278585

Batch 41440, train_perplexity=1386.2795, train_loss=7.234379

Batch 41450, train_perplexity=1254.8895, train_loss=7.134803

Batch 41460, train_perplexity=1322.9775, train_loss=7.18764

Batch 41470, train_perplexity=1306.3317, train_loss=7.1749783

Batch 41480, train_perplexity=1283.2806, train_loss=7.157175

Batch 41490, train_perplexity=1315.4021, train_loss=7.1818976

Batch 41500, train_perplexity=1226.3246, train_loss=7.111777

Batch 41510, train_perplexity=1337.0975, train_loss=7.1982565

Batch 41520, train_perplexity=1321.4543, train_loss=7.186488

Batch 41530, train_perplexity=1350.391, train_loss=7.2081494

Batch 41540, train_perplexity=1241.81, train_loss=7.1243253

Batch 41550, train_perplexity=1391.5646, train_loss=7.238184

Batch 41560, train_perplexity=1300.7617, train_loss=7.1707053

Batch 41570, train_perplexity=1250.532, train_loss=7.1313243

Batch 41580, train_perplexity=1275.5219, train_loss=7.1511106

Batch 41590, train_perplexity=1342.6592, train_loss=7.2024074

Batch 41600, train_perplexity=1311.0607, train_loss=7.1785917

Batch 41610, train_perplexity=1235.7406, train_loss=7.119426

Batch 41620, train_perplexity=1448.2655, train_loss=7.278122

Batch 41630, train_perplexity=1295.2253, train_loss=7.16644

Batch 41640, train_perplexity=1493.9249, train_loss=7.309162

Batch 41650, train_perplexity=1324.4974, train_loss=7.1887884

Batch 41660, train_perplexity=1422.029, train_loss=7.25984

Batch 41670, train_perplexity=1329.1527, train_loss=7.192297

Batch 41680, train_perplexity=1353.8345, train_loss=7.210696

Batch 41690, train_perplexity=1307.3187, train_loss=7.1757336

Batch 41700, train_perplexity=1268.6469, train_loss=7.145706

Batch 41710, train_perplexity=1600.1405, train_loss=7.3778467

Batch 41720, train_perplexity=1292.6118, train_loss=7.16442

Batch 41730, train_perplexity=1282.4426, train_loss=7.156522

Batch 41740, train_perplexity=1289.6936, train_loss=7.16216

Batch 41750, train_perplexity=1293.7921, train_loss=7.165333

Batch 41760, train_perplexity=1233.2471, train_loss=7.117406

Batch 41770, train_perplexity=1397.7067, train_loss=7.242588

Batch 41780, train_perplexity=1297.8702, train_loss=7.16848

Batch 41790, train_perplexity=1219.381, train_loss=7.1060987

Batch 41800, train_perplexity=1392.452, train_loss=7.2388215

Batch 41810, train_perplexity=1217.4865, train_loss=7.1045437

Batch 41820, train_perplexity=1364.4047, train_loss=7.2184734

Batch 41830, train_perplexity=1321.5778, train_loss=7.1865816

Batch 41840, train_perplexity=1248.836, train_loss=7.129967

Batch 41850, train_perplexity=1331.4471, train_loss=7.1940217

Batch 41860, train_perplexity=1196.973, train_loss=7.087551

Batch 41870, train_perplexity=1284.3966, train_loss=7.1580443

Batch 41880, train_perplexity=1305.3853, train_loss=7.1742535

Batch 41890, train_perplexity=1216.768, train_loss=7.1039534

Batch 41900, train_perplexity=1407.9376, train_loss=7.2498813

Batch 41910, train_perplexity=1192.4553, train_loss=7.08377

Batch 41920, train_perplexity=1356.9199, train_loss=7.2129726

Batch 41930, train_perplexity=1366.7267, train_loss=7.220174

Batch 41940, train_perplexity=1319.2557, train_loss=7.184823

Batch 41950, train_perplexity=1287.5343, train_loss=7.1604843

Batch 41960, train_perplexity=1363.4557, train_loss=7.2177777

Batch 41970, train_perplexity=1335.3224, train_loss=7.196928

Batch 41980, train_perplexity=1302.0841, train_loss=7.1717215

Batch 41990, train_perplexity=1308.5, train_loss=7.1766367

Batch 42000, train_perplexity=1215.4574, train_loss=7.1028757

Batch 42010, train_perplexity=1358.3856, train_loss=7.214052

Batch 42020, train_perplexity=1323.5884, train_loss=7.188102

Batch 42030, train_perplexity=1336.6678, train_loss=7.197935

Batch 42040, train_perplexity=1354.1185, train_loss=7.210906

Batch 42050, train_perplexity=1277.3429, train_loss=7.1525373

Batch 42060, train_perplexity=1258.1855, train_loss=7.137426

Batch 42070, train_perplexity=1331.2002, train_loss=7.193836

Batch 42080, train_perplexity=1263.231, train_loss=7.141428

Batch 42090, train_perplexity=1297.5299, train_loss=7.1682177

Batch 42100, train_perplexity=1294.2141, train_loss=7.165659

Batch 42110, train_perplexity=1233.3942, train_loss=7.117525

Batch 42120, train_perplexity=1353.7312, train_loss=7.21062

Batch 42130, train_perplexity=1399.6261, train_loss=7.2439604

Batch 42140, train_perplexity=1320.026, train_loss=7.1854067

Batch 42150, train_perplexity=1224.5325, train_loss=7.1103144

Batch 42160, train_perplexity=1328.808, train_loss=7.1920376

Batch 42170, train_perplexity=1401.3717, train_loss=7.245207

Batch 42180, train_perplexity=1258.4873, train_loss=7.1376657

Batch 42190, train_perplexity=1368.3817, train_loss=7.221384

Batch 42200, train_perplexity=1309.6622, train_loss=7.1775246

Batch 42210, train_perplexity=1266.6564, train_loss=7.144136
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42220, train_perplexity=1397.3647, train_loss=7.2423434

Batch 42230, train_perplexity=1379.0272, train_loss=7.2291336

Batch 42240, train_perplexity=1224.9833, train_loss=7.1106825

Batch 42250, train_perplexity=1388.0231, train_loss=7.2356358

Batch 42260, train_perplexity=1394.5531, train_loss=7.2403293

Batch 42270, train_perplexity=1447.6746, train_loss=7.277714

Batch 42280, train_perplexity=1192.61, train_loss=7.0838995

Batch 42290, train_perplexity=1338.3152, train_loss=7.199167

Batch 42300, train_perplexity=1222.0277, train_loss=7.108267

Batch 42310, train_perplexity=1247.3583, train_loss=7.128783

Batch 42320, train_perplexity=1382.1462, train_loss=7.231393

Batch 42330, train_perplexity=1308.9143, train_loss=7.1769533

Batch 42340, train_perplexity=1323.293, train_loss=7.1878786

Batch 42350, train_perplexity=1476.5336, train_loss=7.2974524

Batch 42360, train_perplexity=1366.7866, train_loss=7.2202177

Batch 42370, train_perplexity=1332.1082, train_loss=7.194518

Batch 42380, train_perplexity=1294.0123, train_loss=7.165503

Batch 42390, train_perplexity=1521.0643, train_loss=7.3271656

Batch 42400, train_perplexity=1242.6648, train_loss=7.1250134

Batch 42410, train_perplexity=1211.73, train_loss=7.0998044

Batch 42420, train_perplexity=1273.8206, train_loss=7.149776

Batch 42430, train_perplexity=1269.4565, train_loss=7.146344

Batch 42440, train_perplexity=1391.4305, train_loss=7.2380877

Batch 42450, train_perplexity=1445.0992, train_loss=7.2759333

Batch 42460, train_perplexity=1315.355, train_loss=7.181862

Batch 42470, train_perplexity=1370.0151, train_loss=7.222577

Batch 42480, train_perplexity=1367.2611, train_loss=7.220565

Batch 42490, train_perplexity=1512.5458, train_loss=7.3215494

Batch 42500, train_perplexity=1359.495, train_loss=7.2148685

Batch 42510, train_perplexity=1428.8369, train_loss=7.264616

Batch 42520, train_perplexity=1384.6609, train_loss=7.2332106

Batch 42530, train_perplexity=1284.4376, train_loss=7.1580763

Batch 42540, train_perplexity=1262.9937, train_loss=7.14124

Batch 42550, train_perplexity=1362.8207, train_loss=7.217312

Batch 42560, train_perplexity=1260.2084, train_loss=7.1390324

Batch 42570, train_perplexity=1235.6805, train_loss=7.119377

Batch 42580, train_perplexity=1412.2605, train_loss=7.252947

Batch 42590, train_perplexity=1278.6677, train_loss=7.153574

Batch 42600, train_perplexity=1207.3767, train_loss=7.096205

Batch 42610, train_perplexity=1250.644, train_loss=7.131414

Batch 42620, train_perplexity=1392.4885, train_loss=7.2388477

Batch 42630, train_perplexity=1169.6036, train_loss=7.06442

Batch 42640, train_perplexity=1196.0521, train_loss=7.0867815

Batch 42650, train_perplexity=1310.5493, train_loss=7.1782017

Batch 42660, train_perplexity=1202.1157, train_loss=7.0918384

Batch 42670, train_perplexity=1302.284, train_loss=7.171875

Batch 42680, train_perplexity=1470.3108, train_loss=7.293229

Batch 42690, train_perplexity=1236.6195, train_loss=7.1201367

Batch 42700, train_perplexity=1400.9014, train_loss=7.244871

Batch 42710, train_perplexity=1386.9023, train_loss=7.234828

Batch 42720, train_perplexity=1334.7769, train_loss=7.1965194

Batch 42730, train_perplexity=1261.9005, train_loss=7.140374

Batch 42740, train_perplexity=1405.9243, train_loss=7.2484503

Batch 42750, train_perplexity=1265.2571, train_loss=7.1430306

Batch 42760, train_perplexity=1203.906, train_loss=7.0933266

Batch 42770, train_perplexity=1412.7494, train_loss=7.253293

Batch 42780, train_perplexity=1362.4503, train_loss=7.21704

Batch 42790, train_perplexity=1278.046, train_loss=7.1530876

Batch 42800, train_perplexity=1352.6924, train_loss=7.209852

Batch 42810, train_perplexity=1255.4528, train_loss=7.1352515

Batch 42820, train_perplexity=1220.1028, train_loss=7.1066904

Batch 42830, train_perplexity=1317.2744, train_loss=7.18332

Batch 42840, train_perplexity=1238.0668, train_loss=7.1213064

Batch 42850, train_perplexity=1207.8881, train_loss=7.0966287

Batch 42860, train_perplexity=1248.9604, train_loss=7.130067

Batch 42870, train_perplexity=1477.1793, train_loss=7.2978897

Batch 42880, train_perplexity=1260.218, train_loss=7.13904

Batch 42890, train_perplexity=1224.7695, train_loss=7.110508

Batch 42900, train_perplexity=1349.809, train_loss=7.2077184

Batch 42910, train_perplexity=1265.8093, train_loss=7.143467

Batch 42920, train_perplexity=1355.7771, train_loss=7.21213

Batch 42930, train_perplexity=1382.6038, train_loss=7.231724

Batch 42940, train_perplexity=1204.4274, train_loss=7.0937595

Batch 42950, train_perplexity=1341.2041, train_loss=7.201323

Batch 42960, train_perplexity=1364.1646, train_loss=7.2182975

Batch 42970, train_perplexity=1326.8376, train_loss=7.1905537

Batch 42980, train_perplexity=1179.0361, train_loss=7.0724525

Batch 42990, train_perplexity=1262.4458, train_loss=7.140806

Batch 43000, train_perplexity=1204.4263, train_loss=7.0937586

Batch 43010, train_perplexity=1255.1301, train_loss=7.1349945

Batch 43020, train_perplexity=1340.4681, train_loss=7.200774

Batch 43030, train_perplexity=1310.835, train_loss=7.1784196

Batch 43040, train_perplexity=1301.0018, train_loss=7.17089

Batch 43050, train_perplexity=1331.3506, train_loss=7.193949

Batch 43060, train_perplexity=1224.6737, train_loss=7.11043

Batch 43070, train_perplexity=1326.8154, train_loss=7.190537

Batch 43080, train_perplexity=1244.8805, train_loss=7.126795

Batch 43090, train_perplexity=1362.1346, train_loss=7.2168083

Batch 43100, train_perplexity=1299.8403, train_loss=7.1699967

Batch 43110, train_perplexity=1373.3405, train_loss=7.2250013

Batch 43120, train_perplexity=1364.6948, train_loss=7.218686

Batch 43130, train_perplexity=1351.1567, train_loss=7.2087164

Batch 43140, train_perplexity=1253.1841, train_loss=7.133443

Batch 43150, train_perplexity=1291.934, train_loss=7.1638956

Batch 43160, train_perplexity=1246.9706, train_loss=7.1284723

Batch 43170, train_perplexity=1335.6764, train_loss=7.197193

Batch 43180, train_perplexity=1420.685, train_loss=7.2588944

Batch 43190, train_perplexity=1219.0089, train_loss=7.1057935

Batch 43200, train_perplexity=1442.4817, train_loss=7.2741203

Batch 43210, train_perplexity=1322.9397, train_loss=7.1876116

Batch 43220, train_perplexity=1243.7283, train_loss=7.125869

Batch 43230, train_perplexity=1325.7638, train_loss=7.189744

Batch 43240, train_perplexity=1223.5157, train_loss=7.1094837

Batch 43250, train_perplexity=1450.1687, train_loss=7.279435

Batch 43260, train_perplexity=1330.4044, train_loss=7.1932383

Batch 43270, train_perplexity=1307.8038, train_loss=7.1761045

Batch 43280, train_perplexity=1338.3962, train_loss=7.1992273

Batch 43290, train_perplexity=1184.8868, train_loss=7.0774026

Batch 43300, train_perplexity=1336.3447, train_loss=7.1976933

Batch 43310, train_perplexity=1312.5118, train_loss=7.179698

Batch 43320, train_perplexity=1277.1285, train_loss=7.1523695

Batch 43330, train_perplexity=1293.253, train_loss=7.164916

Batch 43340, train_perplexity=1184.4536, train_loss=7.077037

Batch 43350, train_perplexity=1341.0, train_loss=7.201171

Batch 43360, train_perplexity=1207.6082, train_loss=7.096397

Batch 43370, train_perplexity=1177.5045, train_loss=7.0711527

Batch 43380, train_perplexity=1298.587, train_loss=7.169032

Batch 43390, train_perplexity=1298.3828, train_loss=7.1688747

Batch 43400, train_perplexity=1267.0038, train_loss=7.14441

Batch 43410, train_perplexity=1161.9663, train_loss=7.057869

Batch 43420, train_perplexity=1423.2976, train_loss=7.2607317

Batch 43430, train_perplexity=1253.5977, train_loss=7.133773

Batch 43440, train_perplexity=1218.0775, train_loss=7.105029

Batch 43450, train_perplexity=1299.8125, train_loss=7.1699753

Batch 43460, train_perplexity=1400.6495, train_loss=7.2446914

Batch 43470, train_perplexity=1365.2598, train_loss=7.2191

Batch 43480, train_perplexity=1334.2296, train_loss=7.1961093

Batch 43490, train_perplexity=1336.2759, train_loss=7.197642

Batch 43500, train_perplexity=1268.751, train_loss=7.145788

Batch 43510, train_perplexity=1277.5939, train_loss=7.152734

Batch 43520, train_perplexity=1392.1526, train_loss=7.2386065

Batch 43530, train_perplexity=1263.4865, train_loss=7.14163
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 43540, train_perplexity=1327.361, train_loss=7.190948

Batch 43550, train_perplexity=1296.3488, train_loss=7.167307

Batch 43560, train_perplexity=1326.367, train_loss=7.190199

Batch 43570, train_perplexity=1255.7994, train_loss=7.1355276

Batch 43580, train_perplexity=1344.4875, train_loss=7.2037683

Batch 43590, train_perplexity=1237.2418, train_loss=7.12064

Batch 43600, train_perplexity=1345.0371, train_loss=7.204177

Batch 43610, train_perplexity=1370.8667, train_loss=7.2231984

Batch 43620, train_perplexity=1304.1372, train_loss=7.173297

Batch 43630, train_perplexity=1349.1661, train_loss=7.207242

Batch 43640, train_perplexity=1233.4365, train_loss=7.1175594

Batch 43650, train_perplexity=1443.9159, train_loss=7.275114

Batch 43660, train_perplexity=1214.2896, train_loss=7.1019144

Batch 43670, train_perplexity=1422.425, train_loss=7.2601185

Batch 43680, train_perplexity=1301.415, train_loss=7.1712074

Batch 43690, train_perplexity=1382.4231, train_loss=7.231593

Batch 43700, train_perplexity=1350.729, train_loss=7.2084

Batch 43710, train_perplexity=1345.5363, train_loss=7.204548

Batch 43720, train_perplexity=1307.7015, train_loss=7.1760263

Batch 43730, train_perplexity=1276.4235, train_loss=7.1518173

Batch 43740, train_perplexity=1365.9722, train_loss=7.2196217

Batch 43750, train_perplexity=1404.1597, train_loss=7.2471943

Batch 43760, train_perplexity=1271.577, train_loss=7.148013

Batch 43770, train_perplexity=1343.5692, train_loss=7.203085

Batch 43780, train_perplexity=1199.1862, train_loss=7.0893984

Batch 43790, train_perplexity=1388.505, train_loss=7.235983

Batch 43800, train_perplexity=1311.6759, train_loss=7.179061

Batch 43810, train_perplexity=1354.4731, train_loss=7.211168

Batch 43820, train_perplexity=1303.4005, train_loss=7.172732

Batch 43830, train_perplexity=1364.458, train_loss=7.2185125

Batch 43840, train_perplexity=1372.57, train_loss=7.22444

Batch 43850, train_perplexity=1275.1411, train_loss=7.150812

Batch 43860, train_perplexity=1419.9001, train_loss=7.258342

Batch 43870, train_perplexity=1377.3173, train_loss=7.227893

Batch 43880, train_perplexity=1285.7214, train_loss=7.1590753

Batch 43890, train_perplexity=1366.7078, train_loss=7.22016

Batch 43900, train_perplexity=1309.8203, train_loss=7.177645

Batch 43910, train_perplexity=1250.6256, train_loss=7.131399

Batch 43920, train_perplexity=1372.5228, train_loss=7.224406

Batch 43930, train_perplexity=1414.1204, train_loss=7.254263

Batch 43940, train_perplexity=1540.7653, train_loss=7.3400345

Batch 43950, train_perplexity=1285.3267, train_loss=7.158768

Batch 43960, train_perplexity=1177.5192, train_loss=7.071165

Batch 43970, train_perplexity=1357.2163, train_loss=7.213191

Batch 43980, train_perplexity=1293.0114, train_loss=7.164729

Batch 43990, train_perplexity=1217.3889, train_loss=7.1044636

Batch 44000, train_perplexity=1262.9528, train_loss=7.1412077

Batch 44010, train_perplexity=1265.2987, train_loss=7.1430635

Batch 44020, train_perplexity=1321.1726, train_loss=7.186275

Batch 44030, train_perplexity=1230.3373, train_loss=7.1150436

Batch 44040, train_perplexity=1272.5383, train_loss=7.148769

Batch 44050, train_perplexity=1418.8328, train_loss=7.25759

Batch 44060, train_perplexity=1358.269, train_loss=7.2139664

Batch 44070, train_perplexity=1288.9564, train_loss=7.161588

Batch 44080, train_perplexity=1314.1174, train_loss=7.1809206

Batch 44090, train_perplexity=1367.3524, train_loss=7.2206316

Batch 44100, train_perplexity=1365.4844, train_loss=7.2192645

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100
Loaded 306206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100
Loaded 306206 sentences.
Finished loading
Batch 44110, train_perplexity=1265.6476, train_loss=7.143339

Batch 44120, train_perplexity=1336.5824, train_loss=7.197871

Batch 44130, train_perplexity=1354.5171, train_loss=7.2112

Batch 44140, train_perplexity=1317.8373, train_loss=7.1837473

Batch 44150, train_perplexity=1214.7777, train_loss=7.1023164

Batch 44160, train_perplexity=1344.2703, train_loss=7.2036066

Batch 44170, train_perplexity=1268.878, train_loss=7.1458883

Batch 44180, train_perplexity=1277.511, train_loss=7.152669

Batch 44190, train_perplexity=1343.6191, train_loss=7.203122

Batch 44200, train_perplexity=1181.086, train_loss=7.0741897

Batch 44210, train_perplexity=1282.3043, train_loss=7.156414

Batch 44220, train_perplexity=1267.816, train_loss=7.145051

Batch 44230, train_perplexity=1424.066, train_loss=7.2612715

Batch 44240, train_perplexity=1256.1659, train_loss=7.1358194

Batch 44250, train_perplexity=1406.9021, train_loss=7.2491455

Batch 44260, train_perplexity=1266.1154, train_loss=7.1437087

Batch 44270, train_perplexity=1320.8168, train_loss=7.1860056

Batch 44280, train_perplexity=1259.8864, train_loss=7.138777

Batch 44290, train_perplexity=1335.6108, train_loss=7.197144

Batch 44300, train_perplexity=1138.8905, train_loss=7.03781

Batch 44310, train_perplexity=1413.1334, train_loss=7.253565

Batch 44320, train_perplexity=1317.5382, train_loss=7.1835203

Batch 44330, train_perplexity=1246.073, train_loss=7.1277523

Batch 44340, train_perplexity=1272.6494, train_loss=7.148856

Batch 44350, train_perplexity=1311.7272, train_loss=7.1791

Batch 44360, train_perplexity=1318.8834, train_loss=7.1845407

Batch 44370, train_perplexity=1227.3782, train_loss=7.1126356

Batch 44380, train_perplexity=1322.7896, train_loss=7.187498

Batch 44390, train_perplexity=1461.693, train_loss=7.2873507

Batch 44400, train_perplexity=1462.0242, train_loss=7.287577

Batch 44410, train_perplexity=1315.4755, train_loss=7.1819534

Batch 44420, train_perplexity=1263.1786, train_loss=7.1413865

Batch 44430, train_perplexity=1317.6149, train_loss=7.1835785

Batch 44440, train_perplexity=1324.7736, train_loss=7.188997

Batch 44450, train_perplexity=1411.4404, train_loss=7.252366

Batch 44460, train_perplexity=1339.0001, train_loss=7.1996784

Batch 44470, train_perplexity=1278.9922, train_loss=7.1538277

Batch 44480, train_perplexity=1398.0573, train_loss=7.242839

Batch 44490, train_perplexity=1287.3176, train_loss=7.160316

Batch 44500, train_perplexity=1309.7185, train_loss=7.1775675

Batch 44510, train_perplexity=1267.7507, train_loss=7.1449995

Batch 44520, train_perplexity=1276.1278, train_loss=7.1515856

Batch 44530, train_perplexity=1357.9141, train_loss=7.213705

Batch 44540, train_perplexity=1311.7097, train_loss=7.1790867

Batch 44550, train_perplexity=1149.3927, train_loss=7.046989

Batch 44560, train_perplexity=1224.6364, train_loss=7.1103992

Batch 44570, train_perplexity=1303.5409, train_loss=7.1728396

Batch 44580, train_perplexity=1317.2794, train_loss=7.183324

Batch 44590, train_perplexity=1191.396, train_loss=7.082881

Batch 44600, train_perplexity=1220.2017, train_loss=7.1067715

Batch 44610, train_perplexity=1388.4097, train_loss=7.235914

Batch 44620, train_perplexity=1336.9553, train_loss=7.19815

Batch 44630, train_perplexity=1322.9813, train_loss=7.187643

Batch 44640, train_perplexity=1242.3187, train_loss=7.124735

Batch 44650, train_perplexity=1199.515, train_loss=7.0896726

Batch 44660, train_perplexity=1275.2555, train_loss=7.150902

Batch 44670, train_perplexity=1326.8452, train_loss=7.1905594

Batch 44680, train_perplexity=1285.1863, train_loss=7.158659

Batch 44690, train_perplexity=1405.2716, train_loss=7.247986

Batch 44700, train_perplexity=1149.1882, train_loss=7.046811

Batch 44710, train_perplexity=1318.818, train_loss=7.184491

Batch 44720, train_perplexity=1278.8715, train_loss=7.1537333

Batch 44730, train_perplexity=1227.5245, train_loss=7.112755

Batch 44740, train_perplexity=1426.7739, train_loss=7.263171

Batch 44750, train_perplexity=1344.0549, train_loss=7.2034464

Batch 44760, train_perplexity=1303.2395, train_loss=7.1726084

Batch 44770, train_perplexity=1203.7958, train_loss=7.093235

Batch 44780, train_perplexity=1262.7023, train_loss=7.1410093

Batch 44790, train_perplexity=1287.7197, train_loss=7.1606283
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44800, train_perplexity=1314.4258, train_loss=7.181155

Batch 44810, train_perplexity=1294.6117, train_loss=7.165966

Batch 44820, train_perplexity=1253.4686, train_loss=7.13367

Batch 44830, train_perplexity=1317.3687, train_loss=7.1833916

Batch 44840, train_perplexity=1219.6025, train_loss=7.1062803

Batch 44850, train_perplexity=1286.1354, train_loss=7.159397

Batch 44860, train_perplexity=1329.4368, train_loss=7.1925106

Batch 44870, train_perplexity=1290.4773, train_loss=7.1627674

Batch 44880, train_perplexity=1328.2157, train_loss=7.1915917

Batch 44890, train_perplexity=1307.5513, train_loss=7.1759114

Batch 44900, train_perplexity=1201.2471, train_loss=7.0911155

Batch 44910, train_perplexity=1445.7671, train_loss=7.2763953

Batch 44920, train_perplexity=1237.6152, train_loss=7.1209416

Batch 44930, train_perplexity=1367.8624, train_loss=7.2210045

Batch 44940, train_perplexity=1219.1351, train_loss=7.105897

Batch 44950, train_perplexity=1161.0392, train_loss=7.0570707

Batch 44960, train_perplexity=1280.149, train_loss=7.1547318

Batch 44970, train_perplexity=1470.423, train_loss=7.2933054

Batch 44980, train_perplexity=1407.2054, train_loss=7.249361

Batch 44990, train_perplexity=1278.7257, train_loss=7.1536193

Batch 45000, train_perplexity=1223.3395, train_loss=7.1093397

Batch 45010, train_perplexity=1313.5586, train_loss=7.1804953

Batch 45020, train_perplexity=1254.594, train_loss=7.1345673

Batch 45030, train_perplexity=1262.5535, train_loss=7.1408916

Batch 45040, train_perplexity=1401.9879, train_loss=7.2456465

Batch 45050, train_perplexity=1263.6816, train_loss=7.1417847

Batch 45060, train_perplexity=1260.4115, train_loss=7.1391935

Batch 45070, train_perplexity=1434.419, train_loss=7.268515

Batch 45080, train_perplexity=1339.9626, train_loss=7.200397

Batch 45090, train_perplexity=1266.3188, train_loss=7.1438694

Batch 45100, train_perplexity=1264.4683, train_loss=7.142407

Batch 45110, train_perplexity=1274.819, train_loss=7.1505594

Batch 45120, train_perplexity=1209.751, train_loss=7.09817

Batch 45130, train_perplexity=1287.7732, train_loss=7.16067

Batch 45140, train_perplexity=1271.5564, train_loss=7.147997

Batch 45150, train_perplexity=1287.6093, train_loss=7.1605425

Batch 45160, train_perplexity=1421.1423, train_loss=7.2592163

Batch 45170, train_perplexity=1324.6403, train_loss=7.188896

Batch 45180, train_perplexity=1308.4938, train_loss=7.176632

Batch 45190, train_perplexity=1254.2314, train_loss=7.1342783

Batch 45200, train_perplexity=1366.7546, train_loss=7.2201943

Batch 45210, train_perplexity=1195.0739, train_loss=7.0859632

Batch 45220, train_perplexity=1271.5042, train_loss=7.147956

Batch 45230, train_perplexity=1308.0458, train_loss=7.1762896

Batch 45240, train_perplexity=1295.5065, train_loss=7.166657

Batch 45250, train_perplexity=1197.46, train_loss=7.087958

Batch 45260, train_perplexity=1303.38, train_loss=7.172716

Batch 45270, train_perplexity=1358.8864, train_loss=7.214421

Batch 45280, train_perplexity=1349.1006, train_loss=7.2071934

Batch 45290, train_perplexity=1264.6877, train_loss=7.1425805

Batch 45300, train_perplexity=1315.0553, train_loss=7.181634

Batch 45310, train_perplexity=1188.8116, train_loss=7.0807095

Batch 45320, train_perplexity=1445.4438, train_loss=7.2761717

Batch 45330, train_perplexity=1210.6414, train_loss=7.0989056

Batch 45340, train_perplexity=1297.9333, train_loss=7.1685286

Batch 45350, train_perplexity=1209.3213, train_loss=7.0978146

Batch 45360, train_perplexity=1315.2233, train_loss=7.1817617

Batch 45370, train_perplexity=1318.4854, train_loss=7.184239

Batch 45380, train_perplexity=1287.732, train_loss=7.160638

Batch 45390, train_perplexity=1272.0172, train_loss=7.1483593

Batch 45400, train_perplexity=1199.9618, train_loss=7.090045

Batch 45410, train_perplexity=1250.6602, train_loss=7.131427

Batch 45420, train_perplexity=1330.1786, train_loss=7.1930685

Batch 45430, train_perplexity=1201.0231, train_loss=7.090929

Batch 45440, train_perplexity=1267.55, train_loss=7.144841

Batch 45450, train_perplexity=1118.0459, train_loss=7.0193377

Batch 45460, train_perplexity=1299.5664, train_loss=7.169786

Batch 45470, train_perplexity=1309.4762, train_loss=7.1773825

Batch 45480, train_perplexity=1435.103, train_loss=7.268992

Batch 45490, train_perplexity=1488.726, train_loss=7.305676

Batch 45500, train_perplexity=1213.7407, train_loss=7.1014624

Batch 45510, train_perplexity=1371.7762, train_loss=7.2238617

Batch 45520, train_perplexity=1354.2916, train_loss=7.211034

Batch 45530, train_perplexity=1278.969, train_loss=7.1538095

Batch 45540, train_perplexity=1287.3655, train_loss=7.160353

Batch 45550, train_perplexity=1253.5469, train_loss=7.1337323

Batch 45560, train_perplexity=1187.5674, train_loss=7.0796623

Batch 45570, train_perplexity=1243.864, train_loss=7.125978

Batch 45580, train_perplexity=1345.6299, train_loss=7.2046175

Batch 45590, train_perplexity=1270.1129, train_loss=7.146861

Batch 45600, train_perplexity=1241.5939, train_loss=7.124151

Batch 45610, train_perplexity=1239.1553, train_loss=7.122185

Batch 45620, train_perplexity=1313.4209, train_loss=7.1803904

Batch 45630, train_perplexity=1347.5587, train_loss=7.20605

Batch 45640, train_perplexity=1372.1642, train_loss=7.2241445

Batch 45650, train_perplexity=1234.9325, train_loss=7.1187716

Batch 45660, train_perplexity=1397.0616, train_loss=7.2421265

Batch 45670, train_perplexity=1237.4672, train_loss=7.120822

Batch 45680, train_perplexity=1167.0632, train_loss=7.062246

Batch 45690, train_perplexity=1551.8296, train_loss=7.34719

Batch 45700, train_perplexity=1251.5526, train_loss=7.13214

Batch 45710, train_perplexity=1236.8655, train_loss=7.1203356

Batch 45720, train_perplexity=1243.9216, train_loss=7.1260242

Batch 45730, train_perplexity=1434.7917, train_loss=7.268775

Batch 45740, train_perplexity=1198.1716, train_loss=7.088552

Batch 45750, train_perplexity=1260.5125, train_loss=7.1392736

Batch 45760, train_perplexity=1370.304, train_loss=7.222788

Batch 45770, train_perplexity=1312.6896, train_loss=7.1798334

Batch 45780, train_perplexity=1392.6943, train_loss=7.2389956

Batch 45790, train_perplexity=1329.9135, train_loss=7.192869

Batch 45800, train_perplexity=1343.4943, train_loss=7.203029

Batch 45810, train_perplexity=1289.9482, train_loss=7.1623573

Batch 45820, train_perplexity=1228.3126, train_loss=7.1133966

Batch 45830, train_perplexity=1346.033, train_loss=7.204917

Batch 45840, train_perplexity=1344.3818, train_loss=7.2036896

Batch 45850, train_perplexity=1210.0919, train_loss=7.0984516

Batch 45860, train_perplexity=1265.1914, train_loss=7.1429787

Batch 45870, train_perplexity=1400.8112, train_loss=7.244807

Batch 45880, train_perplexity=1432.7496, train_loss=7.2673507

Batch 45890, train_perplexity=1278.5305, train_loss=7.1534667

Batch 45900, train_perplexity=1368.1951, train_loss=7.2212477

Batch 45910, train_perplexity=1257.5299, train_loss=7.1369047

Batch 45920, train_perplexity=1290.4071, train_loss=7.162713

Batch 45930, train_perplexity=1267.5306, train_loss=7.144826

Batch 45940, train_perplexity=1283.8663, train_loss=7.1576314

Batch 45950, train_perplexity=1163.6985, train_loss=7.0593586

Batch 45960, train_perplexity=1267.8583, train_loss=7.1450844

Batch 45970, train_perplexity=1280.1936, train_loss=7.1547666

Batch 45980, train_perplexity=1317.2059, train_loss=7.183268

Batch 45990, train_perplexity=1191.037, train_loss=7.0825796

Batch 46000, train_perplexity=1322.3601, train_loss=7.1871734

Batch 46010, train_perplexity=1345.5208, train_loss=7.2045364

Batch 46020, train_perplexity=1196.3522, train_loss=7.0870323

Batch 46030, train_perplexity=1372.9116, train_loss=7.224689

Batch 46040, train_perplexity=1283.5695, train_loss=7.1574

Batch 46050, train_perplexity=1221.4277, train_loss=7.1077757

Batch 46060, train_perplexity=1244.0438, train_loss=7.1261225

Batch 46070, train_perplexity=1200.5977, train_loss=7.0905747

Batch 46080, train_perplexity=1458.3276, train_loss=7.2850456

Batch 46090, train_perplexity=1338.192, train_loss=7.1990747

Batch 46100, train_perplexity=1245.8556, train_loss=7.127578

Batch 46110, train_perplexity=1232.497, train_loss=7.1167974
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46120, train_perplexity=1397.0183, train_loss=7.2420955

Batch 46130, train_perplexity=1258.1897, train_loss=7.137429

Batch 46140, train_perplexity=1203.0498, train_loss=7.092615

Batch 46150, train_perplexity=1238.1908, train_loss=7.1214066

Batch 46160, train_perplexity=1160.146, train_loss=7.056301

Batch 46170, train_perplexity=1202.4368, train_loss=7.0921054

Batch 46180, train_perplexity=1274.577, train_loss=7.1503696

Batch 46190, train_perplexity=1296.6078, train_loss=7.1675067

Batch 46200, train_perplexity=1290.3591, train_loss=7.162676

Batch 46210, train_perplexity=1263.939, train_loss=7.1419883

Batch 46220, train_perplexity=1236.928, train_loss=7.120386

Batch 46230, train_perplexity=1257.1948, train_loss=7.136638

Batch 46240, train_perplexity=1320.67, train_loss=7.1858945

Batch 46250, train_perplexity=1307.7521, train_loss=7.176065

Batch 46260, train_perplexity=1321.6465, train_loss=7.1866336

Batch 46270, train_perplexity=1246.0361, train_loss=7.1277227

Batch 46280, train_perplexity=1213.1511, train_loss=7.1009765

Batch 46290, train_perplexity=1210.3158, train_loss=7.0986366

Batch 46300, train_perplexity=1224.2697, train_loss=7.1101

Batch 46310, train_perplexity=1202.2097, train_loss=7.0919166

Batch 46320, train_perplexity=1211.5654, train_loss=7.0996685

Batch 46330, train_perplexity=1314.147, train_loss=7.180943

Batch 46340, train_perplexity=1299.4902, train_loss=7.1697273

Batch 46350, train_perplexity=1287.8875, train_loss=7.1607585

Batch 46360, train_perplexity=1285.2126, train_loss=7.1586795

Batch 46370, train_perplexity=1273.7252, train_loss=7.149701

Batch 46380, train_perplexity=1400.4846, train_loss=7.2445736

Batch 46390, train_perplexity=1282.538, train_loss=7.156596

Batch 46400, train_perplexity=1350.599, train_loss=7.2083035

Batch 46410, train_perplexity=1138.6782, train_loss=7.0376234

Batch 46420, train_perplexity=1300.2687, train_loss=7.170326

Batch 46430, train_perplexity=1334.11, train_loss=7.1960196

Batch 46440, train_perplexity=1293.8156, train_loss=7.165351

Batch 46450, train_perplexity=1302.6406, train_loss=7.1721487

Batch 46460, train_perplexity=1232.1467, train_loss=7.1165133

Batch 46470, train_perplexity=1104.0869, train_loss=7.006774

Batch 46480, train_perplexity=1376.964, train_loss=7.2276363

Batch 46490, train_perplexity=1361.3502, train_loss=7.2162323

Batch 46500, train_perplexity=1275.9415, train_loss=7.1514397

Batch 46510, train_perplexity=1167.1362, train_loss=7.0623083

Batch 46520, train_perplexity=1226.9615, train_loss=7.112296

Batch 46530, train_perplexity=1343.682, train_loss=7.203169

Batch 46540, train_perplexity=1211.6058, train_loss=7.099702

Batch 46550, train_perplexity=1207.6398, train_loss=7.096423

Batch 46560, train_perplexity=1244.3232, train_loss=7.126347

Batch 46570, train_perplexity=1231.5524, train_loss=7.1160307

Batch 46580, train_perplexity=1343.955, train_loss=7.203372

Batch 46590, train_perplexity=1422.2595, train_loss=7.260002

Batch 46600, train_perplexity=1246.1467, train_loss=7.1278114

Batch 46610, train_perplexity=1224.2686, train_loss=7.110099

Batch 46620, train_perplexity=1313.3751, train_loss=7.1803555

Batch 46630, train_perplexity=1246.6864, train_loss=7.1282444

Batch 46640, train_perplexity=1289.4292, train_loss=7.161955

Batch 46650, train_perplexity=1273.656, train_loss=7.1496468

Batch 46660, train_perplexity=1305.2196, train_loss=7.1741266

Batch 46670, train_perplexity=1260.3911, train_loss=7.1391773

Batch 46680, train_perplexity=1511.9674, train_loss=7.321167

Batch 46690, train_perplexity=1224.9885, train_loss=7.110687

Batch 46700, train_perplexity=1265.8624, train_loss=7.143509

Batch 46710, train_perplexity=1399.187, train_loss=7.2436466

Batch 46720, train_perplexity=1273.506, train_loss=7.149529

Batch 46730, train_perplexity=1429.551, train_loss=7.2651157

Batch 46740, train_perplexity=1281.2935, train_loss=7.1556253

Batch 46750, train_perplexity=1352.7362, train_loss=7.2098846

Batch 46760, train_perplexity=1282.4823, train_loss=7.156553

Batch 46770, train_perplexity=1160.2168, train_loss=7.056362

Batch 46780, train_perplexity=1261.7356, train_loss=7.1402435

Batch 46790, train_perplexity=1301.5255, train_loss=7.1712923

Batch 46800, train_perplexity=1283.6956, train_loss=7.1574984

Batch 46810, train_perplexity=1287.5134, train_loss=7.160468

Batch 46820, train_perplexity=1277.9528, train_loss=7.1530147

Batch 46830, train_perplexity=1155.369, train_loss=7.052175

Batch 46840, train_perplexity=1211.2303, train_loss=7.099392

Batch 46850, train_perplexity=1417.5107, train_loss=7.2566576

Batch 46860, train_perplexity=1170.92, train_loss=7.065545

Batch 46870, train_perplexity=1278.8007, train_loss=7.153678

Batch 46880, train_perplexity=1243.3666, train_loss=7.125578

Batch 46890, train_perplexity=1251.7233, train_loss=7.1322765

Batch 46900, train_perplexity=1270.1674, train_loss=7.146904

Batch 46910, train_perplexity=1215.782, train_loss=7.1031427

Batch 46920, train_perplexity=1372.1282, train_loss=7.224118

Batch 46930, train_perplexity=1271.6606, train_loss=7.148079

Batch 46940, train_perplexity=1197.3184, train_loss=7.0878396

Batch 46950, train_perplexity=1340.69, train_loss=7.2009397

Batch 46960, train_perplexity=1307.7252, train_loss=7.1760445

Batch 46970, train_perplexity=1278.2928, train_loss=7.1532807

Batch 46980, train_perplexity=1335.2269, train_loss=7.1968565

Batch 46990, train_perplexity=1269.5885, train_loss=7.146448

Batch 47000, train_perplexity=1338.5546, train_loss=7.1993456

Batch 47010, train_perplexity=1222.3413, train_loss=7.1085234

Batch 47020, train_perplexity=1310.7919, train_loss=7.1783867

Batch 47030, train_perplexity=1293.401, train_loss=7.1650305

Batch 47040, train_perplexity=1335.2816, train_loss=7.1968975

Batch 47050, train_perplexity=1387.3772, train_loss=7.2351704

Batch 47060, train_perplexity=1395.1005, train_loss=7.2407217

Batch 47070, train_perplexity=1265.2565, train_loss=7.14303

Batch 47080, train_perplexity=1318.8444, train_loss=7.184511

Batch 47090, train_perplexity=1290.6299, train_loss=7.1628857

Batch 47100, train_perplexity=1250.3167, train_loss=7.131152

Batch 47110, train_perplexity=1374.934, train_loss=7.226161

Batch 47120, train_perplexity=1337.5942, train_loss=7.198628

Batch 47130, train_perplexity=1304.8562, train_loss=7.173848

Batch 47140, train_perplexity=1241.3807, train_loss=7.1239796

Batch 47150, train_perplexity=1205.3168, train_loss=7.0944977

Batch 47160, train_perplexity=1175.2988, train_loss=7.069278

Batch 47170, train_perplexity=1302.3661, train_loss=7.171938

Batch 47180, train_perplexity=1331.7018, train_loss=7.194213

Batch 47190, train_perplexity=1298.6384, train_loss=7.1690717

Batch 47200, train_perplexity=1265.6862, train_loss=7.1433697

Batch 47210, train_perplexity=1299.9451, train_loss=7.1700773

Batch 47220, train_perplexity=1223.0001, train_loss=7.109062

Batch 47230, train_perplexity=1277.5646, train_loss=7.152711

Batch 47240, train_perplexity=1226.372, train_loss=7.1118155

Batch 47250, train_perplexity=1255.9347, train_loss=7.1356354

Batch 47260, train_perplexity=1294.411, train_loss=7.165811

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Batch 47270, train_perplexity=1392.6254, train_loss=7.238946

Batch 47280, train_perplexity=1420.217, train_loss=7.258565

Batch 47290, train_perplexity=1282.6229, train_loss=7.1566625

Batch 47300, train_perplexity=1293.6978, train_loss=7.16526

Batch 47310, train_perplexity=1256.9034, train_loss=7.1364064

Batch 47320, train_perplexity=1199.3188, train_loss=7.089509

Batch 47330, train_perplexity=1280.464, train_loss=7.154978

Batch 47340, train_perplexity=1232.3942, train_loss=7.116714

Batch 47350, train_perplexity=1314.5249, train_loss=7.1812305

Batch 47360, train_perplexity=1340.3723, train_loss=7.2007027

Batch 47370, train_perplexity=1310.9537, train_loss=7.17851
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 47380, train_perplexity=1344.0908, train_loss=7.203473

Batch 47390, train_perplexity=1456.0139, train_loss=7.2834578

Batch 47400, train_perplexity=1261.4385, train_loss=7.140008

Batch 47410, train_perplexity=1328.7675, train_loss=7.192007

Batch 47420, train_perplexity=1374.9065, train_loss=7.226141

Batch 47430, train_perplexity=1332.6458, train_loss=7.1949215

Batch 47440, train_perplexity=1380.0475, train_loss=7.229873

Batch 47450, train_perplexity=1354.4105, train_loss=7.2111216

Batch 47460, train_perplexity=1251.8182, train_loss=7.1323524

Batch 47470, train_perplexity=1302.5648, train_loss=7.1720905

Batch 47480, train_perplexity=1373.7885, train_loss=7.2253275

Batch 47490, train_perplexity=1252.9368, train_loss=7.1332455

Batch 47500, train_perplexity=1306.1473, train_loss=7.174837

Batch 47510, train_perplexity=1256.3636, train_loss=7.135977

Batch 47520, train_perplexity=1235.9198, train_loss=7.1195707

Batch 47530, train_perplexity=1213.3206, train_loss=7.101116

Batch 47540, train_perplexity=1270.7684, train_loss=7.147377

Batch 47550, train_perplexity=1282.7435, train_loss=7.1567564

Batch 47560, train_perplexity=1256.4409, train_loss=7.1360383

Batch 47570, train_perplexity=1208.8958, train_loss=7.0974627

Batch 47580, train_perplexity=1206.1171, train_loss=7.0951614

Batch 47590, train_perplexity=1257.6559, train_loss=7.137005

Batch 47600, train_perplexity=1137.4637, train_loss=7.0365562

Batch 47610, train_perplexity=1289.7981, train_loss=7.162241

Batch 47620, train_perplexity=1269.6963, train_loss=7.146533

Batch 47630, train_perplexity=1290.8527, train_loss=7.1630583

Batch 47640, train_perplexity=1294.4869, train_loss=7.1658697

Batch 47650, train_perplexity=1245.5319, train_loss=7.127318

Batch 47660, train_perplexity=1254.9266, train_loss=7.1348324

Batch 47670, train_perplexity=1224.8029, train_loss=7.110535

Batch 47680, train_perplexity=1205.0455, train_loss=7.0942726

Batch 47690, train_perplexity=1259.114, train_loss=7.1381636

Batch 47700, train_perplexity=1229.1246, train_loss=7.1140575

Batch 47710, train_perplexity=1296.502, train_loss=7.167425

Batch 47720, train_perplexity=1221.6415, train_loss=7.1079507

Batch 47730, train_perplexity=1306.5547, train_loss=7.175149

Batch 47740, train_perplexity=1238.8823, train_loss=7.121965

Batch 47750, train_perplexity=1269.686, train_loss=7.146525

Batch 47760, train_perplexity=1345.7544, train_loss=7.20471

Batch 47770, train_perplexity=1379.4448, train_loss=7.2294364

Batch 47780, train_perplexity=1339.741, train_loss=7.2002316

Batch 47790, train_perplexity=1323.2426, train_loss=7.1878405

Batch 47800, train_perplexity=1326.6788, train_loss=7.190434

Batch 47810, train_perplexity=1095.4567, train_loss=6.9989266

Batch 47820, train_perplexity=1124.6537, train_loss=7.0252304

Batch 47830, train_perplexity=1283.709, train_loss=7.157509

Batch 47840, train_perplexity=1239.5459, train_loss=7.1225004

Batch 47850, train_perplexity=1244.6449, train_loss=7.1266055

Batch 47860, train_perplexity=1411.6182, train_loss=7.252492

Batch 47870, train_perplexity=1377.0532, train_loss=7.227701

Batch 47880, train_perplexity=1177.3496, train_loss=7.071021

Batch 47890, train_perplexity=1220.3367, train_loss=7.106882

Batch 47900, train_perplexity=1332.4373, train_loss=7.194765

Batch 47910, train_perplexity=1228.0162, train_loss=7.1131554

Batch 47920, train_perplexity=1320.1752, train_loss=7.1855197

Batch 47930, train_perplexity=1284.9614, train_loss=7.158484

Batch 47940, train_perplexity=1332.8707, train_loss=7.1950903

Batch 47950, train_perplexity=1219.1595, train_loss=7.105917

Batch 47960, train_perplexity=1342.2079, train_loss=7.202071

Batch 47970, train_perplexity=1363.0125, train_loss=7.2174525

Batch 47980, train_perplexity=1201.8102, train_loss=7.091584

Batch 47990, train_perplexity=1315.754, train_loss=7.182165

Batch 48000, train_perplexity=1123.9042, train_loss=7.024564

Batch 48010, train_perplexity=1395.328, train_loss=7.240885

Batch 48020, train_perplexity=1302.4753, train_loss=7.172022

Batch 48030, train_perplexity=1138.8162, train_loss=7.0377445

Batch 48040, train_perplexity=1291.2386, train_loss=7.1633573

Batch 48050, train_perplexity=1286.456, train_loss=7.1596465

Batch 48060, train_perplexity=1290.6078, train_loss=7.1628685

Batch 48070, train_perplexity=1163.8073, train_loss=7.059452

Batch 48080, train_perplexity=1362.562, train_loss=7.217122

Batch 48090, train_perplexity=1289.8375, train_loss=7.1622715

Batch 48100, train_perplexity=1250.9094, train_loss=7.131626

Batch 48110, train_perplexity=1273.2661, train_loss=7.1493406

Batch 48120, train_perplexity=1207.5431, train_loss=7.096343

Batch 48130, train_perplexity=1341.9365, train_loss=7.201869

Batch 48140, train_perplexity=1319.3577, train_loss=7.1849003

Batch 48150, train_perplexity=1206.2079, train_loss=7.095237

Batch 48160, train_perplexity=1353.5653, train_loss=7.2104974

Batch 48170, train_perplexity=1259.64, train_loss=7.1385813

Batch 48180, train_perplexity=1226.2404, train_loss=7.111708

Batch 48190, train_perplexity=1201.6108, train_loss=7.0914183

Batch 48200, train_perplexity=1278.8958, train_loss=7.1537523

Batch 48210, train_perplexity=1288.5374, train_loss=7.161263

Batch 48220, train_perplexity=1292.6094, train_loss=7.164418

Batch 48230, train_perplexity=1284.3373, train_loss=7.157998

Batch 48240, train_perplexity=1282.7379, train_loss=7.156752

Batch 48250, train_perplexity=1299.6501, train_loss=7.1698503

Batch 48260, train_perplexity=1459.4331, train_loss=7.2858033

Batch 48270, train_perplexity=1274.0046, train_loss=7.1499205

Batch 48280, train_perplexity=1326.7705, train_loss=7.190503

Batch 48290, train_perplexity=1267.6117, train_loss=7.14489

Batch 48300, train_perplexity=1320.8433, train_loss=7.1860256

Batch 48310, train_perplexity=1340.0732, train_loss=7.2004795

Batch 48320, train_perplexity=1249.1576, train_loss=7.1302247

Batch 48330, train_perplexity=1288.5428, train_loss=7.1612673

Batch 48340, train_perplexity=1314.3632, train_loss=7.1811075

Batch 48350, train_perplexity=1138.7281, train_loss=7.0376673

Batch 48360, train_perplexity=1212.0415, train_loss=7.1000614

Batch 48370, train_perplexity=1226.2053, train_loss=7.1116796

Batch 48380, train_perplexity=1276.2909, train_loss=7.1517134

Batch 48390, train_perplexity=1188.8762, train_loss=7.080764

Batch 48400, train_perplexity=1294.9562, train_loss=7.166232

Batch 48410, train_perplexity=1168.0626, train_loss=7.063102

Batch 48420, train_perplexity=1155.622, train_loss=7.052394

Batch 48430, train_perplexity=1239.091, train_loss=7.1221333

Batch 48440, train_perplexity=1218.6527, train_loss=7.105501

Batch 48450, train_perplexity=1134.6641, train_loss=7.034092

Batch 48460, train_perplexity=1449.5333, train_loss=7.278997

Batch 48470, train_perplexity=1145.586, train_loss=7.0436716

Batch 48480, train_perplexity=1363.4733, train_loss=7.2177906

Batch 48490, train_perplexity=1235.3524, train_loss=7.1191115

Batch 48500, train_perplexity=1229.0461, train_loss=7.1139936

Batch 48510, train_perplexity=1302.4952, train_loss=7.172037

Batch 48520, train_perplexity=1416.6241, train_loss=7.256032

Batch 48530, train_perplexity=1129.2267, train_loss=7.0292883

Batch 48540, train_perplexity=1359.3225, train_loss=7.2147417

Batch 48550, train_perplexity=1301.1494, train_loss=7.1710033

Batch 48560, train_perplexity=1244.042, train_loss=7.126121

Batch 48570, train_perplexity=1325.9332, train_loss=7.189872

Batch 48580, train_perplexity=1382.203, train_loss=7.231434

Batch 48590, train_perplexity=1266.956, train_loss=7.1443725

Batch 48600, train_perplexity=1323.105, train_loss=7.1877365

Batch 48610, train_perplexity=1270.1674, train_loss=7.146904

Batch 48620, train_perplexity=1277.8132, train_loss=7.1529055

Batch 48630, train_perplexity=1222.6339, train_loss=7.1087627

Batch 48640, train_perplexity=1157.6217, train_loss=7.054123

Batch 48650, train_perplexity=1222.5511, train_loss=7.108695

Batch 48660, train_perplexity=1253.5212, train_loss=7.133712

Batch 48670, train_perplexity=1331.651, train_loss=7.194175

Batch 48680, train_perplexity=1259.0564, train_loss=7.138118

Batch 48690, train_perplexity=1213.1957, train_loss=7.101013
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48700, train_perplexity=1178.6084, train_loss=7.0720897

Batch 48710, train_perplexity=1347.3634, train_loss=7.205905

Batch 48720, train_perplexity=1230.3356, train_loss=7.115042

Batch 48730, train_perplexity=1175.4883, train_loss=7.069439

Batch 48740, train_perplexity=1276.5264, train_loss=7.151898

Batch 48750, train_perplexity=1345.422, train_loss=7.204463

Batch 48760, train_perplexity=1278.0789, train_loss=7.1531134

Batch 48770, train_perplexity=1172.7238, train_loss=7.0670843

Batch 48780, train_perplexity=1328.9696, train_loss=7.192159

Batch 48790, train_perplexity=1311.3845, train_loss=7.1788387

Batch 48800, train_perplexity=1232.7245, train_loss=7.116982

Batch 48810, train_perplexity=1227.054, train_loss=7.1123714

Batch 48820, train_perplexity=1246.6002, train_loss=7.1281753

Batch 48830, train_perplexity=1275.4519, train_loss=7.151056

Batch 48840, train_perplexity=1123.6223, train_loss=7.024313

Batch 48850, train_perplexity=1293.2086, train_loss=7.1648817

Batch 48860, train_perplexity=1373.7065, train_loss=7.225268

Batch 48870, train_perplexity=1249.3625, train_loss=7.1303887

Batch 48880, train_perplexity=1191.3641, train_loss=7.0828543

Batch 48890, train_perplexity=1246.0172, train_loss=7.1277075

Batch 48900, train_perplexity=1288.5367, train_loss=7.1612625

Batch 48910, train_perplexity=1187.3184, train_loss=7.0794525

Batch 48920, train_perplexity=1332.0288, train_loss=7.1944585

Batch 48930, train_perplexity=1309.2196, train_loss=7.1771865

Batch 48940, train_perplexity=1285.4713, train_loss=7.1588807

Batch 48950, train_perplexity=1260.188, train_loss=7.139016

Batch 48960, train_perplexity=1220.289, train_loss=7.106843

Batch 48970, train_perplexity=1397.568, train_loss=7.242489

Batch 48980, train_perplexity=1198.3185, train_loss=7.0886745

Batch 48990, train_perplexity=1174.6545, train_loss=7.0687294

Batch 49000, train_perplexity=1226.5392, train_loss=7.111952

Batch 49010, train_perplexity=1283.3491, train_loss=7.1572285

Batch 49020, train_perplexity=1324.0751, train_loss=7.1884694

Batch 49030, train_perplexity=1249.0665, train_loss=7.1301517

Batch 49040, train_perplexity=1361.1354, train_loss=7.2160745

Batch 49050, train_perplexity=1277.9034, train_loss=7.152976

Batch 49060, train_perplexity=1186.1752, train_loss=7.0784893

Batch 49070, train_perplexity=1243.2645, train_loss=7.125496

Batch 49080, train_perplexity=1299.6093, train_loss=7.169819

Batch 49090, train_perplexity=1350.876, train_loss=7.2085085

Batch 49100, train_perplexity=1295.8383, train_loss=7.166913

Batch 49110, train_perplexity=1299.3577, train_loss=7.1696253

Batch 49120, train_perplexity=1312.9249, train_loss=7.1800127

Batch 49130, train_perplexity=1242.5297, train_loss=7.1249046

Batch 49140, train_perplexity=1240.0123, train_loss=7.1228766

Batch 49150, train_perplexity=1335.0989, train_loss=7.1967607

Batch 49160, train_perplexity=1171.3076, train_loss=7.065876

Batch 49170, train_perplexity=1214.451, train_loss=7.1020474

Batch 49180, train_perplexity=1357.8209, train_loss=7.2136364

Batch 49190, train_perplexity=1402.8773, train_loss=7.2462807

Batch 49200, train_perplexity=1301.5143, train_loss=7.1712837

Batch 49210, train_perplexity=1306.4176, train_loss=7.175044

Batch 49220, train_perplexity=1293.401, train_loss=7.1650305

Batch 49230, train_perplexity=1195.0032, train_loss=7.085904

Batch 49240, train_perplexity=1328.3291, train_loss=7.191677

Batch 49250, train_perplexity=1238.4045, train_loss=7.121579

Batch 49260, train_perplexity=1190.698, train_loss=7.082295

Batch 49270, train_perplexity=1186.9928, train_loss=7.0791783

Batch 49280, train_perplexity=1162.1891, train_loss=7.0580606

Batch 49290, train_perplexity=1456.8723, train_loss=7.284047

Batch 49300, train_perplexity=1393.2822, train_loss=7.2394176

Batch 49310, train_perplexity=1251.1772, train_loss=7.13184

Batch 49320, train_perplexity=1239.2067, train_loss=7.1222267

Batch 49330, train_perplexity=1255.5839, train_loss=7.135356

Batch 49340, train_perplexity=1251.1158, train_loss=7.131791

Batch 49350, train_perplexity=1211.2373, train_loss=7.0993977

Batch 49360, train_perplexity=1134.3401, train_loss=7.0338063

Batch 49370, train_perplexity=1235.9427, train_loss=7.1195893

Batch 49380, train_perplexity=1238.6774, train_loss=7.1217995

Batch 49390, train_perplexity=1235.2375, train_loss=7.1190186

Batch 49400, train_perplexity=1164.0182, train_loss=7.0596333

Batch 49410, train_perplexity=1225.3817, train_loss=7.1110077

Batch 49420, train_perplexity=1206.6727, train_loss=7.095622

Batch 49430, train_perplexity=1242.2311, train_loss=7.1246643

Batch 49440, train_perplexity=1360.5267, train_loss=7.215627

Batch 49450, train_perplexity=1262.5493, train_loss=7.140888

Batch 49460, train_perplexity=1412.8997, train_loss=7.2533994

Batch 49470, train_perplexity=1187.8948, train_loss=7.079938

Batch 49480, train_perplexity=1338.1244, train_loss=7.199024

Batch 49490, train_perplexity=1104.2301, train_loss=7.0069036

Batch 49500, train_perplexity=1223.975, train_loss=7.109859

Batch 49510, train_perplexity=1154.1807, train_loss=7.051146

Batch 49520, train_perplexity=1244.8514, train_loss=7.1267715

Batch 49530, train_perplexity=1232.778, train_loss=7.1170254

Batch 49540, train_perplexity=1344.7491, train_loss=7.203963

Batch 49550, train_perplexity=1322.5834, train_loss=7.187342

Batch 49560, train_perplexity=1132.7666, train_loss=7.0324183

Batch 49570, train_perplexity=1259.914, train_loss=7.1387987

Batch 49580, train_perplexity=1262.332, train_loss=7.140716

Batch 49590, train_perplexity=1262.3337, train_loss=7.1407175

Batch 49600, train_perplexity=1294.2252, train_loss=7.1656675

Batch 49610, train_perplexity=1185.8868, train_loss=7.078246

Batch 49620, train_perplexity=1176.1858, train_loss=7.070032

Batch 49630, train_perplexity=1233.5417, train_loss=7.117645

Batch 49640, train_perplexity=1189.5953, train_loss=7.0813684

Batch 49650, train_perplexity=1214.9336, train_loss=7.1024446

Batch 49660, train_perplexity=1332.7219, train_loss=7.1949787

Batch 49670, train_perplexity=1230.9922, train_loss=7.115576

Batch 49680, train_perplexity=1201.3645, train_loss=7.091213

Batch 49690, train_perplexity=1227.02, train_loss=7.112344

Batch 49700, train_perplexity=1320.0184, train_loss=7.185401

Batch 49710, train_perplexity=1404.1931, train_loss=7.247218

Batch 49720, train_perplexity=1185.9603, train_loss=7.078308

Batch 49730, train_perplexity=1338.2092, train_loss=7.1990876

Batch 49740, train_perplexity=1195.8651, train_loss=7.086625

Batch 49750, train_perplexity=1269.0892, train_loss=7.1460547

Batch 49760, train_perplexity=1234.6045, train_loss=7.118506

Batch 49770, train_perplexity=1340.674, train_loss=7.2009277

Batch 49780, train_perplexity=1302.3549, train_loss=7.1719294

Batch 49790, train_perplexity=1232.3748, train_loss=7.1166983

Batch 49800, train_perplexity=1165.191, train_loss=7.0606403

Batch 49810, train_perplexity=1235.766, train_loss=7.1194463

Batch 49820, train_perplexity=1230.8226, train_loss=7.115438

Batch 49830, train_perplexity=1228.9863, train_loss=7.113945

Batch 49840, train_perplexity=1193.5544, train_loss=7.084691

Batch 49850, train_perplexity=1238.3798, train_loss=7.121559

Batch 49860, train_perplexity=1357.7445, train_loss=7.21358

Batch 49870, train_perplexity=1233.8612, train_loss=7.1179037

Batch 49880, train_perplexity=1316.87, train_loss=7.183013

Batch 49890, train_perplexity=1309.7241, train_loss=7.177572

Batch 49900, train_perplexity=1194.7502, train_loss=7.0856924

Batch 49910, train_perplexity=1195.9426, train_loss=7.08669

Batch 49920, train_perplexity=1372.3062, train_loss=7.224248

Batch 49930, train_perplexity=1234.6333, train_loss=7.1185293

Batch 49940, train_perplexity=1193.3939, train_loss=7.0845566

Batch 49950, train_perplexity=1213.2269, train_loss=7.101039

Batch 49960, train_perplexity=1349.2543, train_loss=7.2073073

Batch 49970, train_perplexity=1218.4481, train_loss=7.1053333

Batch 49980, train_perplexity=1139.3289, train_loss=7.0381947

Batch 49990, train_perplexity=1238.277, train_loss=7.121476

Batch 50000, train_perplexity=1152.7177, train_loss=7.0498776

Batch 50010, train_perplexity=1227.4473, train_loss=7.112692
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 50020, train_perplexity=1340.0464, train_loss=7.2004595

Batch 50030, train_perplexity=1257.3993, train_loss=7.136801

Batch 50040, train_perplexity=1293.4677, train_loss=7.165082

Batch 50050, train_perplexity=1206.7332, train_loss=7.095672

Batch 50060, train_perplexity=1153.6118, train_loss=7.050653

Batch 50070, train_perplexity=1270.1335, train_loss=7.1468773

Batch 50080, train_perplexity=1219.5293, train_loss=7.1062202

Batch 50090, train_perplexity=1182.8541, train_loss=7.0756855

Batch 50100, train_perplexity=1313.833, train_loss=7.180704

Batch 50110, train_perplexity=1243.0524, train_loss=7.125325

Batch 50120, train_perplexity=1235.8043, train_loss=7.1194773

Batch 50130, train_perplexity=1088.8506, train_loss=6.992878

Batch 50140, train_perplexity=1296.9584, train_loss=7.167777

Batch 50150, train_perplexity=1218.4348, train_loss=7.1053224

Batch 50160, train_perplexity=1359.495, train_loss=7.2148685

Batch 50170, train_perplexity=1342.9121, train_loss=7.2025957

Batch 50180, train_perplexity=1225.7897, train_loss=7.1113405

Batch 50190, train_perplexity=1267.095, train_loss=7.144482

Batch 50200, train_perplexity=1125.6215, train_loss=7.0260906

Batch 50210, train_perplexity=1106.1315, train_loss=7.008624

Batch 50220, train_perplexity=1265.8438, train_loss=7.143494

Batch 50230, train_perplexity=1216.0939, train_loss=7.1033993

Batch 50240, train_perplexity=1375.8975, train_loss=7.2268615

Batch 50250, train_perplexity=1123.9277, train_loss=7.024585

Batch 50260, train_perplexity=1239.3663, train_loss=7.1223555

Batch 50270, train_perplexity=1321.6913, train_loss=7.1866674

Batch 50280, train_perplexity=1354.0643, train_loss=7.210866

Batch 50290, train_perplexity=1377.5065, train_loss=7.22803

Batch 50300, train_perplexity=1288.9681, train_loss=7.1615973

Batch 50310, train_perplexity=1133.3193, train_loss=7.032906

Batch 50320, train_perplexity=1120.361, train_loss=7.021406

Batch 50330, train_perplexity=1197.5952, train_loss=7.088071

Batch 50340, train_perplexity=1208.588, train_loss=7.097208

Batch 50350, train_perplexity=1341.4241, train_loss=7.201487

Batch 50360, train_perplexity=1263.4081, train_loss=7.141568

Batch 50370, train_perplexity=1223.1832, train_loss=7.109212

Batch 50380, train_perplexity=1218.3262, train_loss=7.105233

Batch 50390, train_perplexity=1266.9282, train_loss=7.1443505

Batch 50400, train_perplexity=1242.6031, train_loss=7.1249638

Batch 50410, train_perplexity=1367.9518, train_loss=7.22107

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Batch 50420, train_perplexity=1192.5441, train_loss=7.083844

Batch 50430, train_perplexity=1302.3711, train_loss=7.1719418

Batch 50440, train_perplexity=1244.1007, train_loss=7.1261683

Batch 50450, train_perplexity=1235.2682, train_loss=7.1190434

Batch 50460, train_perplexity=1195.7191, train_loss=7.086503

Batch 50470, train_perplexity=1215.2128, train_loss=7.1026745

Batch 50480, train_perplexity=1252.8328, train_loss=7.1331625

Batch 50490, train_perplexity=1269.763, train_loss=7.1465855

Batch 50500, train_perplexity=1281.9186, train_loss=7.156113

Batch 50510, train_perplexity=1246.9741, train_loss=7.128475

Batch 50520, train_perplexity=1418.9478, train_loss=7.257671

Batch 50530, train_perplexity=1197.424, train_loss=7.087928

Batch 50540, train_perplexity=1242.775, train_loss=7.125102

Batch 50550, train_perplexity=1288.6608, train_loss=7.161359

Batch 50560, train_perplexity=1238.0834, train_loss=7.12132

Batch 50570, train_perplexity=1367.0988, train_loss=7.220446

Batch 50580, train_perplexity=1195.8536, train_loss=7.0866156

Batch 50590, train_perplexity=1268.8386, train_loss=7.1458573

Batch 50600, train_perplexity=1188.6189, train_loss=7.0805473

Batch 50610, train_perplexity=1277.3143, train_loss=7.152515

Batch 50620, train_perplexity=1260.8551, train_loss=7.1395454

Batch 50630, train_perplexity=1230.1056, train_loss=7.1148553

Batch 50640, train_perplexity=1352.6517, train_loss=7.209822

Batch 50650, train_perplexity=1234.1337, train_loss=7.1181245

Batch 50660, train_perplexity=1184.7982, train_loss=7.0773277

Batch 50670, train_perplexity=1413.9774, train_loss=7.254162

Batch 50680, train_perplexity=1292.3654, train_loss=7.1642294

Batch 50690, train_perplexity=1253.226, train_loss=7.1334763

Batch 50700, train_perplexity=1214.0834, train_loss=7.1017447

Batch 50710, train_perplexity=1316.8398, train_loss=7.18299

Batch 50720, train_perplexity=1282.6505, train_loss=7.156684

Batch 50730, train_perplexity=1135.4581, train_loss=7.0347915

Batch 50740, train_perplexity=1198.6476, train_loss=7.088949

Batch 50750, train_perplexity=1126.2711, train_loss=7.0266676

Batch 50760, train_perplexity=1322.5625, train_loss=7.1873264

Batch 50770, train_perplexity=1187.6274, train_loss=7.079713

Batch 50780, train_perplexity=1150.547, train_loss=7.0479927

Batch 50790, train_perplexity=1298.9302, train_loss=7.1692963

Batch 50800, train_perplexity=1243.7686, train_loss=7.125901

Batch 50810, train_perplexity=1265.3386, train_loss=7.143095

Batch 50820, train_perplexity=1365.9794, train_loss=7.219627

Batch 50830, train_perplexity=1275.9988, train_loss=7.1514845

Batch 50840, train_perplexity=1300.7748, train_loss=7.1707153

Batch 50850, train_perplexity=1356.3429, train_loss=7.2125473

Batch 50860, train_perplexity=1329.788, train_loss=7.192775

Batch 50870, train_perplexity=1267.8691, train_loss=7.145093

Batch 50880, train_perplexity=1331.2141, train_loss=7.1938467

Batch 50890, train_perplexity=1230.7023, train_loss=7.11534

Batch 50900, train_perplexity=1124.274, train_loss=7.024893

Batch 50910, train_perplexity=1262.7817, train_loss=7.1410723

Batch 50920, train_perplexity=1237.7682, train_loss=7.121065

Batch 50930, train_perplexity=1256.3499, train_loss=7.135966

Batch 50940, train_perplexity=1196.3213, train_loss=7.0870066

Batch 50950, train_perplexity=1149.7792, train_loss=7.047325

Batch 50960, train_perplexity=1246.778, train_loss=7.128318

Batch 50970, train_perplexity=1213.4791, train_loss=7.101247

Batch 50980, train_perplexity=1180.6232, train_loss=7.0737977

Batch 50990, train_perplexity=1305.8683, train_loss=7.1746235

Batch 51000, train_perplexity=1219.3048, train_loss=7.106036

Batch 51010, train_perplexity=1287.093, train_loss=7.1601415

Batch 51020, train_perplexity=1274.2549, train_loss=7.150117

Batch 51030, train_perplexity=1293.2753, train_loss=7.164933

Batch 51040, train_perplexity=1275.1302, train_loss=7.1508036

Batch 51050, train_perplexity=1229.708, train_loss=7.114532

Batch 51060, train_perplexity=1293.6835, train_loss=7.165249

Batch 51070, train_perplexity=1139.306, train_loss=7.0381746

Batch 51080, train_perplexity=1210.6494, train_loss=7.0989122

Batch 51090, train_perplexity=1287.8622, train_loss=7.160739

Batch 51100, train_perplexity=1123.5822, train_loss=7.024277

Batch 51110, train_perplexity=1304.3262, train_loss=7.173442

Batch 51120, train_perplexity=1309.0242, train_loss=7.1770372

Batch 51130, train_perplexity=1340.8402, train_loss=7.2010517

Batch 51140, train_perplexity=1247.4749, train_loss=7.1288767

Batch 51150, train_perplexity=1409.2091, train_loss=7.250784

Batch 51160, train_perplexity=1250.1868, train_loss=7.131048

Batch 51170, train_perplexity=1289.7009, train_loss=7.1621656

Batch 51180, train_perplexity=1276.8082, train_loss=7.1521187

Batch 51190, train_perplexity=1189.7576, train_loss=7.081505

Batch 51200, train_perplexity=1157.6797, train_loss=7.054173

Batch 51210, train_perplexity=1384.5836, train_loss=7.233155

Batch 51220, train_perplexity=1179.4208, train_loss=7.0727787

Batch 51230, train_perplexity=1357.8429, train_loss=7.2136526

Batch 51240, train_perplexity=1249.7623, train_loss=7.1307087

Batch 51250, train_perplexity=1361.2281, train_loss=7.2161427

Batch 51260, train_perplexity=1193.5647, train_loss=7.0846996

Batch 51270, train_perplexity=1250.4861, train_loss=7.1312876
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 51280, train_perplexity=1286.5156, train_loss=7.159693

Batch 51290, train_perplexity=1259.3829, train_loss=7.138377

Batch 51300, train_perplexity=1209.2555, train_loss=7.09776

Batch 51310, train_perplexity=1119.5546, train_loss=7.020686

Batch 51320, train_perplexity=1240.8812, train_loss=7.123577

Batch 51330, train_perplexity=1202.494, train_loss=7.092153

Batch 51340, train_perplexity=1077.0574, train_loss=6.981988

Batch 51350, train_perplexity=1227.7721, train_loss=7.1129565

Batch 51360, train_perplexity=1321.5375, train_loss=7.186551

Batch 51370, train_perplexity=1361.3463, train_loss=7.2162294

Batch 51380, train_perplexity=1328.6888, train_loss=7.191948

Batch 51390, train_perplexity=1245.7914, train_loss=7.1275263

Batch 51400, train_perplexity=1331.7932, train_loss=7.1942816

Batch 51410, train_perplexity=1334.3295, train_loss=7.196184

Batch 51420, train_perplexity=1266.285, train_loss=7.1438427

Batch 51430, train_perplexity=1100.9788, train_loss=7.003955

Batch 51440, train_perplexity=1291.2085, train_loss=7.163334

Batch 51450, train_perplexity=1151.9545, train_loss=7.0492153

Batch 51460, train_perplexity=1222.9756, train_loss=7.109042

Batch 51470, train_perplexity=1287.15, train_loss=7.160186

Batch 51480, train_perplexity=1496.0614, train_loss=7.310591

Batch 51490, train_perplexity=1325.2037, train_loss=7.1893215

Batch 51500, train_perplexity=1273.684, train_loss=7.1496687

Batch 51510, train_perplexity=1192.7573, train_loss=7.084023

Batch 51520, train_perplexity=1291.5608, train_loss=7.1636066

Batch 51530, train_perplexity=1338.5532, train_loss=7.1993446

Batch 51540, train_perplexity=1337.0402, train_loss=7.1982136

Batch 51550, train_perplexity=1126.5612, train_loss=7.026925

Batch 51560, train_perplexity=1184.7056, train_loss=7.0772495

Batch 51570, train_perplexity=1134.2195, train_loss=7.0337

Batch 51580, train_perplexity=1233.6636, train_loss=7.1177435

Batch 51590, train_perplexity=1228.6617, train_loss=7.113681

Batch 51600, train_perplexity=1282.231, train_loss=7.156357

Batch 51610, train_perplexity=1190.434, train_loss=7.082073

Batch 51620, train_perplexity=1291.5398, train_loss=7.1635904

Batch 51630, train_perplexity=1236.758, train_loss=7.120249

Batch 51640, train_perplexity=1375.4639, train_loss=7.2265463

Batch 51650, train_perplexity=1163.2358, train_loss=7.058961

Batch 51660, train_perplexity=1277.2162, train_loss=7.152438

Batch 51670, train_perplexity=1187.1122, train_loss=7.079279

Batch 51680, train_perplexity=1383.5205, train_loss=7.2323866

Batch 51690, train_perplexity=1364.0305, train_loss=7.2181993

Batch 51700, train_perplexity=1261.791, train_loss=7.1402874

Batch 51710, train_perplexity=1308.6959, train_loss=7.1767864

Batch 51720, train_perplexity=1324.8846, train_loss=7.1890807

Batch 51730, train_perplexity=1294.9352, train_loss=7.166216

Batch 51740, train_perplexity=1211.1108, train_loss=7.099293

Batch 51750, train_perplexity=1260.1122, train_loss=7.138956

Batch 51760, train_perplexity=1312.95, train_loss=7.180032

Batch 51770, train_perplexity=1189.213, train_loss=7.081047

Batch 51780, train_perplexity=1311.7247, train_loss=7.179098

Batch 51790, train_perplexity=1423.6105, train_loss=7.2609515

Batch 51800, train_perplexity=1245.4036, train_loss=7.127215

Batch 51810, train_perplexity=1244.8604, train_loss=7.1267786

Batch 51820, train_perplexity=1123.6138, train_loss=7.0243053

Batch 51830, train_perplexity=1181.0083, train_loss=7.074124

Batch 51840, train_perplexity=1162.709, train_loss=7.058508

Batch 51850, train_perplexity=1244.0966, train_loss=7.126165

Batch 51860, train_perplexity=1257.3602, train_loss=7.13677

Batch 51870, train_perplexity=1263.531, train_loss=7.1416655

Batch 51880, train_perplexity=1201.4458, train_loss=7.091281

Batch 51890, train_perplexity=1224.4366, train_loss=7.110236

Batch 51900, train_perplexity=1240.2175, train_loss=7.123042

Batch 51910, train_perplexity=1234.5374, train_loss=7.1184516

Batch 51920, train_perplexity=1184.273, train_loss=7.0768843

Batch 51930, train_perplexity=1384.8683, train_loss=7.2333603

Batch 51940, train_perplexity=1323.8257, train_loss=7.188281

Batch 51950, train_perplexity=1279.8347, train_loss=7.154486

Batch 51960, train_perplexity=1373.9875, train_loss=7.2254725

Batch 51970, train_perplexity=1238.5392, train_loss=7.121688

Batch 51980, train_perplexity=1295.4762, train_loss=7.1666336

Batch 51990, train_perplexity=1245.4102, train_loss=7.12722

Batch 52000, train_perplexity=1283.9644, train_loss=7.1577077

Batch 52010, train_perplexity=1193.6876, train_loss=7.0848026

Batch 52020, train_perplexity=1257.9126, train_loss=7.137209

Batch 52030, train_perplexity=1221.2046, train_loss=7.107593

Batch 52040, train_perplexity=1423.8162, train_loss=7.261096

Batch 52050, train_perplexity=1331.4427, train_loss=7.1940184

Batch 52060, train_perplexity=1309.7017, train_loss=7.1775546

Batch 52070, train_perplexity=1262.2616, train_loss=7.1406603

Batch 52080, train_perplexity=1283.9539, train_loss=7.1576996

Batch 52090, train_perplexity=1311.3088, train_loss=7.178781

Batch 52100, train_perplexity=1199.9961, train_loss=7.0900736

Batch 52110, train_perplexity=1287.4583, train_loss=7.160425

Batch 52120, train_perplexity=1232.3818, train_loss=7.116704

Batch 52130, train_perplexity=1192.7073, train_loss=7.083981

Batch 52140, train_perplexity=1202.0785, train_loss=7.0918074

Batch 52150, train_perplexity=1151.2241, train_loss=7.048581

Batch 52160, train_perplexity=1169.1849, train_loss=7.064062

Batch 52170, train_perplexity=1292.9028, train_loss=7.164645

Batch 52180, train_perplexity=1160.8881, train_loss=7.0569406

Batch 52190, train_perplexity=1178.3055, train_loss=7.0718327

Batch 52200, train_perplexity=1320.0908, train_loss=7.185456

Batch 52210, train_perplexity=1249.8411, train_loss=7.1307716

Batch 52220, train_perplexity=1291.724, train_loss=7.163733

Batch 52230, train_perplexity=1390.437, train_loss=7.2373734

Batch 52240, train_perplexity=1230.4441, train_loss=7.1151304

Batch 52250, train_perplexity=1213.8038, train_loss=7.1015143

Batch 52260, train_perplexity=1193.048, train_loss=7.0842667

Batch 52270, train_perplexity=1172.1558, train_loss=7.0666

Batch 52280, train_perplexity=1353.4524, train_loss=7.210414

Batch 52290, train_perplexity=1261.4829, train_loss=7.1400433

Batch 52300, train_perplexity=1269.2423, train_loss=7.1461754

Batch 52310, train_perplexity=1161.6328, train_loss=7.057582

Batch 52320, train_perplexity=1192.593, train_loss=7.083885

Batch 52330, train_perplexity=1179.5636, train_loss=7.0729

Batch 52340, train_perplexity=1302.8201, train_loss=7.1722865

Batch 52350, train_perplexity=1167.9066, train_loss=7.0629683

Batch 52360, train_perplexity=1260.7745, train_loss=7.1394815

Batch 52370, train_perplexity=1223.1576, train_loss=7.109191

Batch 52380, train_perplexity=1199.3989, train_loss=7.089576

Batch 52390, train_perplexity=1202.1381, train_loss=7.091857

Batch 52400, train_perplexity=1215.6342, train_loss=7.103021

Batch 52410, train_perplexity=1235.0255, train_loss=7.118847

Batch 52420, train_perplexity=1208.2682, train_loss=7.0969434

Batch 52430, train_perplexity=1270.7695, train_loss=7.147378

Batch 52440, train_perplexity=1330.6608, train_loss=7.193431

Batch 52450, train_perplexity=1296.7178, train_loss=7.1675916

Batch 52460, train_perplexity=1226.0668, train_loss=7.1115665

Batch 52470, train_perplexity=1216.4518, train_loss=7.1036935

Batch 52480, train_perplexity=1215.7571, train_loss=7.103122

Batch 52490, train_perplexity=1204.9288, train_loss=7.094176

Batch 52500, train_perplexity=1255.2091, train_loss=7.1350574

Batch 52510, train_perplexity=1304.9944, train_loss=7.173954

Batch 52520, train_perplexity=1362.0736, train_loss=7.2167635

Batch 52530, train_perplexity=1296.7147, train_loss=7.167589

Batch 52540, train_perplexity=1284.6085, train_loss=7.1582093

Batch 52550, train_perplexity=1236.6685, train_loss=7.1201763

Batch 52560, train_perplexity=1350.237, train_loss=7.2080355

Batch 52570, train_perplexity=1204.7847, train_loss=7.094056

Batch 52580, train_perplexity=1246.5603, train_loss=7.1281433

Batch 52590, train_perplexity=1156.623, train_loss=7.05326
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52600, train_perplexity=1245.6471, train_loss=7.1274104

Batch 52610, train_perplexity=1144.41, train_loss=7.0426445

Batch 52620, train_perplexity=1392.2854, train_loss=7.238702

Batch 52630, train_perplexity=1240.5973, train_loss=7.123348

Batch 52640, train_perplexity=1213.6863, train_loss=7.1014175

Batch 52650, train_perplexity=1325.5994, train_loss=7.18962

Batch 52660, train_perplexity=1176.3569, train_loss=7.0701776

Batch 52670, train_perplexity=1193.7571, train_loss=7.084861

Batch 52680, train_perplexity=1400.5474, train_loss=7.2446184

Batch 52690, train_perplexity=1238.4896, train_loss=7.121648

Batch 52700, train_perplexity=1232.078, train_loss=7.1164575

Batch 52710, train_perplexity=1298.3369, train_loss=7.1688395

Batch 52720, train_perplexity=1298.8397, train_loss=7.1692266

Batch 52730, train_perplexity=1236.1991, train_loss=7.1197968

Batch 52740, train_perplexity=1176.7367, train_loss=7.0705004

Batch 52750, train_perplexity=1255.6964, train_loss=7.1354456

Batch 52760, train_perplexity=1341.3639, train_loss=7.2014422

Batch 52770, train_perplexity=1173.0862, train_loss=7.0673933

Batch 52780, train_perplexity=1370.7908, train_loss=7.223143

Batch 52790, train_perplexity=1195.7573, train_loss=7.086535

Batch 52800, train_perplexity=1141.4996, train_loss=7.040098

Batch 52810, train_perplexity=1247.4023, train_loss=7.1288185

Batch 52820, train_perplexity=1163.3645, train_loss=7.0590715

Batch 52830, train_perplexity=1259.263, train_loss=7.138282

Batch 52840, train_perplexity=1259.263, train_loss=7.138282

Batch 52850, train_perplexity=1214.3844, train_loss=7.1019926

Batch 52860, train_perplexity=1214.5084, train_loss=7.1020947

Batch 52870, train_perplexity=1149.186, train_loss=7.046809

Batch 52880, train_perplexity=1236.3477, train_loss=7.119917

Batch 52890, train_perplexity=1410.7716, train_loss=7.251892

Batch 52900, train_perplexity=1261.8091, train_loss=7.1403017

Batch 52910, train_perplexity=1410.06, train_loss=7.2513876

Batch 52920, train_perplexity=1187.9939, train_loss=7.0800214

Batch 52930, train_perplexity=1201.918, train_loss=7.091674

Batch 52940, train_perplexity=1235.5002, train_loss=7.119231

Batch 52950, train_perplexity=1363.9115, train_loss=7.218112

Batch 52960, train_perplexity=1229.2905, train_loss=7.1141925

Batch 52970, train_perplexity=1293.4602, train_loss=7.1650763

Batch 52980, train_perplexity=1296.4575, train_loss=7.167391

Batch 52990, train_perplexity=1173.1287, train_loss=7.0674295

Batch 53000, train_perplexity=1299.0931, train_loss=7.1694217

Batch 53010, train_perplexity=1130.9208, train_loss=7.0307875

Batch 53020, train_perplexity=1275.3314, train_loss=7.1509614

Batch 53030, train_perplexity=1140.4932, train_loss=7.039216

Batch 53040, train_perplexity=1270.4818, train_loss=7.1471515

Batch 53050, train_perplexity=1206.7913, train_loss=7.0957203

Batch 53060, train_perplexity=1290.4834, train_loss=7.162772

Batch 53070, train_perplexity=1147.4071, train_loss=7.04526

Batch 53080, train_perplexity=1255.1289, train_loss=7.1349936

Batch 53090, train_perplexity=1207.8517, train_loss=7.0965986

Batch 53100, train_perplexity=1240.2075, train_loss=7.123034

Batch 53110, train_perplexity=1227.7715, train_loss=7.112956

Batch 53120, train_perplexity=1326.4922, train_loss=7.1902933

Batch 53130, train_perplexity=1256.3839, train_loss=7.135993

Batch 53140, train_perplexity=1162.5161, train_loss=7.058342

Batch 53150, train_perplexity=1266.4009, train_loss=7.1439342

Batch 53160, train_perplexity=1215.0598, train_loss=7.1025486

Batch 53170, train_perplexity=1162.1969, train_loss=7.0580673

Batch 53180, train_perplexity=1193.8219, train_loss=7.084915

Batch 53190, train_perplexity=1258.3043, train_loss=7.1375203

Batch 53200, train_perplexity=1254.2566, train_loss=7.1342983

Batch 53210, train_perplexity=1260.8882, train_loss=7.1395717

Batch 53220, train_perplexity=1501.226, train_loss=7.3140373

Batch 53230, train_perplexity=1224.7491, train_loss=7.1104913

Batch 53240, train_perplexity=1212.5837, train_loss=7.1005087

Batch 53250, train_perplexity=1170.901, train_loss=7.065529

Batch 53260, train_perplexity=1328.0087, train_loss=7.191436

Batch 53270, train_perplexity=1236.8796, train_loss=7.120347

Batch 53280, train_perplexity=1284.2068, train_loss=7.1578965

Batch 53290, train_perplexity=1185.4825, train_loss=7.077905

Batch 53300, train_perplexity=1189.3922, train_loss=7.0811977

Batch 53310, train_perplexity=1264.9554, train_loss=7.142792

Batch 53320, train_perplexity=1283.3131, train_loss=7.1572003

Batch 53330, train_perplexity=1186.4423, train_loss=7.0787144

Batch 53340, train_perplexity=1225.0323, train_loss=7.1107225

Batch 53350, train_perplexity=1302.9711, train_loss=7.1724024

Batch 53360, train_perplexity=1249.9191, train_loss=7.130834

Batch 53370, train_perplexity=1110.8527, train_loss=7.012883

Batch 53380, train_perplexity=1191.5709, train_loss=7.083028

Batch 53390, train_perplexity=1219.435, train_loss=7.106143

Batch 53400, train_perplexity=1384.1078, train_loss=7.232811

Batch 53410, train_perplexity=1390.5437, train_loss=7.23745

Batch 53420, train_perplexity=1155.0732, train_loss=7.051919

Batch 53430, train_perplexity=1188.8824, train_loss=7.080769

Batch 53440, train_perplexity=1210.7672, train_loss=7.0990095

Batch 53450, train_perplexity=1305.2806, train_loss=7.1741734

Batch 53460, train_perplexity=1107.7202, train_loss=7.0100594

Batch 53470, train_perplexity=1216.2018, train_loss=7.103488

Batch 53480, train_perplexity=1296.4408, train_loss=7.167378

Batch 53490, train_perplexity=1292.3351, train_loss=7.164206

Batch 53500, train_perplexity=1271.7401, train_loss=7.1481414

Batch 53510, train_perplexity=1144.2998, train_loss=7.042548

Batch 53520, train_perplexity=1284.5137, train_loss=7.1581354

Batch 53530, train_perplexity=1244.1814, train_loss=7.126233

Batch 53540, train_perplexity=1194.7639, train_loss=7.085704

Batch 53550, train_perplexity=1207.4866, train_loss=7.0962963

Batch 53560, train_perplexity=1279.723, train_loss=7.154399

Batch 53570, train_perplexity=1164.4124, train_loss=7.059972

Batch 53580, train_perplexity=1139.4995, train_loss=7.0383444

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Batch 53590, train_perplexity=1195.051, train_loss=7.085944

Batch 53600, train_perplexity=1240.1892, train_loss=7.123019

Batch 53610, train_perplexity=1265.0249, train_loss=7.142847

Batch 53620, train_perplexity=1243.9696, train_loss=7.126063

Batch 53630, train_perplexity=1277.6938, train_loss=7.152812

Batch 53640, train_perplexity=1250.4932, train_loss=7.1312933

Batch 53650, train_perplexity=1271.1853, train_loss=7.147705

Batch 53660, train_perplexity=1276.7705, train_loss=7.152089

Batch 53670, train_perplexity=1361.8125, train_loss=7.216572

Batch 53680, train_perplexity=1226.3334, train_loss=7.111784

Batch 53690, train_perplexity=1288.8624, train_loss=7.161515

Batch 53700, train_perplexity=1204.9691, train_loss=7.094209

Batch 53710, train_perplexity=1237.793, train_loss=7.121085

Batch 53720, train_perplexity=1217.0679, train_loss=7.1042

Batch 53730, train_perplexity=1208.4935, train_loss=7.09713

Batch 53740, train_perplexity=1311.1156, train_loss=7.1786337

Batch 53750, train_perplexity=1276.465, train_loss=7.1518497

Batch 53760, train_perplexity=1311.3707, train_loss=7.1788282

Batch 53770, train_perplexity=1162.9613, train_loss=7.058725

Batch 53780, train_perplexity=1209.477, train_loss=7.0979433

Batch 53790, train_perplexity=1250.8755, train_loss=7.131599

Batch 53800, train_perplexity=1196.767, train_loss=7.087379

Batch 53810, train_perplexity=1163.1438, train_loss=7.0588818

Batch 53820, train_perplexity=1190.5305, train_loss=7.0821543

Batch 53830, train_perplexity=1237.138, train_loss=7.120556

Batch 53840, train_perplexity=1265.6566, train_loss=7.1433463

Batch 53850, train_perplexity=1121.2257, train_loss=7.0221777
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53860, train_perplexity=1263.0063, train_loss=7.14125

Batch 53870, train_perplexity=1283.8395, train_loss=7.1576104

Batch 53880, train_perplexity=1230.4453, train_loss=7.1151314

Batch 53890, train_perplexity=1294.0618, train_loss=7.165541

Batch 53900, train_perplexity=1253.3586, train_loss=7.133582

Batch 53910, train_perplexity=1185.1469, train_loss=7.077622

Batch 53920, train_perplexity=1239.2422, train_loss=7.1222553

Batch 53930, train_perplexity=1229.0543, train_loss=7.1140003

Batch 53940, train_perplexity=1143.5896, train_loss=7.0419273

Batch 53950, train_perplexity=1274.9047, train_loss=7.1506267

Batch 53960, train_perplexity=1255.9958, train_loss=7.135684

Batch 53970, train_perplexity=1209.0122, train_loss=7.097559

Batch 53980, train_perplexity=1273.8054, train_loss=7.149764

Batch 53990, train_perplexity=1276.944, train_loss=7.152225

Batch 54000, train_perplexity=1339.1636, train_loss=7.1998005

Batch 54010, train_perplexity=1269.3972, train_loss=7.1462975

Batch 54020, train_perplexity=1200.6852, train_loss=7.0906477

Batch 54030, train_perplexity=1121.5796, train_loss=7.0224934

Batch 54040, train_perplexity=1291.3958, train_loss=7.163479

Batch 54050, train_perplexity=1112.078, train_loss=7.0139856

Batch 54060, train_perplexity=1168.9753, train_loss=7.063883

Batch 54070, train_perplexity=1198.7236, train_loss=7.0890126

Batch 54080, train_perplexity=1227.7452, train_loss=7.1129346

Batch 54090, train_perplexity=1214.6925, train_loss=7.1022463

Batch 54100, train_perplexity=1217.0557, train_loss=7.10419

Batch 54110, train_perplexity=1087.4788, train_loss=6.991617

Batch 54120, train_perplexity=1185.1722, train_loss=7.0776434

Batch 54130, train_perplexity=1155.4412, train_loss=7.0522375

Batch 54140, train_perplexity=1188.2539, train_loss=7.0802402

Batch 54150, train_perplexity=1239.3013, train_loss=7.122303

Batch 54160, train_perplexity=1299.4103, train_loss=7.169666

Batch 54170, train_perplexity=1306.8425, train_loss=7.1753693

Batch 54180, train_perplexity=1294.5629, train_loss=7.1659284

Batch 54190, train_perplexity=1260.0293, train_loss=7.1388903

Batch 54200, train_perplexity=1048.3734, train_loss=6.954995

Batch 54210, train_perplexity=1207.4072, train_loss=7.0962305

Batch 54220, train_perplexity=1182.6329, train_loss=7.0754986

Batch 54230, train_perplexity=1246.0338, train_loss=7.127721

Batch 54240, train_perplexity=1321.5538, train_loss=7.1865635

Batch 54250, train_perplexity=1143.8241, train_loss=7.0421324

Batch 54260, train_perplexity=1310.4569, train_loss=7.178131

Batch 54270, train_perplexity=1130.8206, train_loss=7.030699

Batch 54280, train_perplexity=1212.0491, train_loss=7.1000676

Batch 54290, train_perplexity=1198.6099, train_loss=7.0889177

Batch 54300, train_perplexity=1130.9375, train_loss=7.0308022

Batch 54310, train_perplexity=1241.8604, train_loss=7.124366

Batch 54320, train_perplexity=1217.3593, train_loss=7.1044393

Batch 54330, train_perplexity=1174.3141, train_loss=7.0684395

Batch 54340, train_perplexity=1147.9779, train_loss=7.0457573

Batch 54350, train_perplexity=1209.432, train_loss=7.097906

Batch 54360, train_perplexity=1218.4958, train_loss=7.1053724

Batch 54370, train_perplexity=1165.9869, train_loss=7.061323

Batch 54380, train_perplexity=1186.8009, train_loss=7.0790167

Batch 54390, train_perplexity=1291.1088, train_loss=7.1632566

Batch 54400, train_perplexity=1332.8599, train_loss=7.195082

Batch 54410, train_perplexity=1201.7466, train_loss=7.0915313

Batch 54420, train_perplexity=1382.8536, train_loss=7.2319045

Batch 54430, train_perplexity=1188.178, train_loss=7.0801764

Batch 54440, train_perplexity=1179.8448, train_loss=7.073138

Batch 54450, train_perplexity=1154.343, train_loss=7.0512867

Batch 54460, train_perplexity=1178.2352, train_loss=7.071773

Batch 54470, train_perplexity=1274.2477, train_loss=7.150111

Batch 54480, train_perplexity=1282.5447, train_loss=7.1566014

Batch 54490, train_perplexity=1307.6024, train_loss=7.1759505

Batch 54500, train_perplexity=1320.9113, train_loss=7.186077

Batch 54510, train_perplexity=1196.278, train_loss=7.0869703

Batch 54520, train_perplexity=1294.2623, train_loss=7.165696

Batch 54530, train_perplexity=1410.7756, train_loss=7.251895

Batch 54540, train_perplexity=1155.0952, train_loss=7.051938

Batch 54550, train_perplexity=1250.6876, train_loss=7.1314487

Batch 54560, train_perplexity=1226.8334, train_loss=7.1121917

Batch 54570, train_perplexity=1211.7953, train_loss=7.0998583

Batch 54580, train_perplexity=1234.6663, train_loss=7.118556

Batch 54590, train_perplexity=1184.657, train_loss=7.0772085

Batch 54600, train_perplexity=1200.0374, train_loss=7.090108

Batch 54610, train_perplexity=1124.1765, train_loss=7.024806

Batch 54620, train_perplexity=1167.6794, train_loss=7.0627737

Batch 54630, train_perplexity=1242.9391, train_loss=7.125234

Batch 54640, train_perplexity=1289.7489, train_loss=7.162203

Batch 54650, train_perplexity=1237.8927, train_loss=7.1211658

Batch 54660, train_perplexity=1258.5725, train_loss=7.1377335

Batch 54670, train_perplexity=1403.0326, train_loss=7.2463913

Batch 54680, train_perplexity=1189.6509, train_loss=7.081415

Batch 54690, train_perplexity=1303.1935, train_loss=7.172573

Batch 54700, train_perplexity=1200.6349, train_loss=7.0906057

Batch 54710, train_perplexity=1412.9583, train_loss=7.253441

Batch 54720, train_perplexity=1176.8141, train_loss=7.070566

Batch 54730, train_perplexity=1322.3083, train_loss=7.1871343

Batch 54740, train_perplexity=1284.8419, train_loss=7.158391

Batch 54750, train_perplexity=1232.9032, train_loss=7.117127

Batch 54760, train_perplexity=1147.1702, train_loss=7.0450535

Batch 54770, train_perplexity=1221.5093, train_loss=7.1078424

Batch 54780, train_perplexity=1276.9409, train_loss=7.1522226

Batch 54790, train_perplexity=1157.3877, train_loss=7.0539207

Batch 54800, train_perplexity=1314.2592, train_loss=7.1810284

Batch 54810, train_perplexity=1401.6865, train_loss=7.2454314

Batch 54820, train_perplexity=1274.2191, train_loss=7.150089

Batch 54830, train_perplexity=1244.5933, train_loss=7.126564

Batch 54840, train_perplexity=1377.0204, train_loss=7.2276773

Batch 54850, train_perplexity=1349.7684, train_loss=7.2076883

Batch 54860, train_perplexity=1270.9792, train_loss=7.147543

Batch 54870, train_perplexity=1203.043, train_loss=7.0926094

Batch 54880, train_perplexity=1165.4867, train_loss=7.060894

Batch 54890, train_perplexity=1212.5155, train_loss=7.1004524

Batch 54900, train_perplexity=1140.9337, train_loss=7.0396023

Batch 54910, train_perplexity=1149.1477, train_loss=7.046776

Batch 54920, train_perplexity=1307.6991, train_loss=7.1760244

Batch 54930, train_perplexity=1181.5017, train_loss=7.0745416

Batch 54940, train_perplexity=1188.2704, train_loss=7.080254

Batch 54950, train_perplexity=1280.2125, train_loss=7.1547813

Batch 54960, train_perplexity=1268.278, train_loss=7.1454153

Batch 54970, train_perplexity=1190.9462, train_loss=7.0825033

Batch 54980, train_perplexity=1286.5623, train_loss=7.159729

Batch 54990, train_perplexity=1141.9843, train_loss=7.0405226

Batch 55000, train_perplexity=1111.3915, train_loss=7.013368

Batch 55010, train_perplexity=1290.284, train_loss=7.1626177

Batch 55020, train_perplexity=1279.5497, train_loss=7.1542635

Batch 55030, train_perplexity=1172.5627, train_loss=7.066947

Batch 55040, train_perplexity=1345.3796, train_loss=7.2044315

Batch 55050, train_perplexity=1178.8214, train_loss=7.0722704

Batch 55060, train_perplexity=1305.4948, train_loss=7.1743374

Batch 55070, train_perplexity=1136.0143, train_loss=7.035281

Batch 55080, train_perplexity=1294.6857, train_loss=7.1660233

Batch 55090, train_perplexity=1297.8269, train_loss=7.1684465

Batch 55100, train_perplexity=1198.531, train_loss=7.088852

Batch 55110, train_perplexity=1200.7311, train_loss=7.090686

Batch 55120, train_perplexity=1325.6677, train_loss=7.1896715

Batch 55130, train_perplexity=1209.041, train_loss=7.097583

Batch 55140, train_perplexity=1172.4341, train_loss=7.0668373

Batch 55150, train_perplexity=1247.2893, train_loss=7.128728

Batch 55160, train_perplexity=1189.7837, train_loss=7.0815268

Batch 55170, train_perplexity=1201.4183, train_loss=7.091258
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55180, train_perplexity=1132.0743, train_loss=7.031807

Batch 55190, train_perplexity=1246.8975, train_loss=7.1284137

Batch 55200, train_perplexity=1303.3551, train_loss=7.172697

Batch 55210, train_perplexity=1205.902, train_loss=7.094983

Batch 55220, train_perplexity=1207.3634, train_loss=7.0961943

Batch 55230, train_perplexity=1285.6608, train_loss=7.159028

Batch 55240, train_perplexity=1239.6003, train_loss=7.1225443

Batch 55250, train_perplexity=1246.9979, train_loss=7.1284943

Batch 55260, train_perplexity=1311.5934, train_loss=7.178998

Batch 55270, train_perplexity=1278.1344, train_loss=7.1531568

Batch 55280, train_perplexity=1259.7284, train_loss=7.1386514

Batch 55290, train_perplexity=1096.9668, train_loss=7.000304

Batch 55300, train_perplexity=1182.6307, train_loss=7.0754967

Batch 55310, train_perplexity=1270.0657, train_loss=7.146824

Batch 55320, train_perplexity=1272.4213, train_loss=7.148677

Batch 55330, train_perplexity=1176.767, train_loss=7.070526

Batch 55340, train_perplexity=1294.1167, train_loss=7.1655836

Batch 55350, train_perplexity=1160.7026, train_loss=7.056781

Batch 55360, train_perplexity=1273.2139, train_loss=7.1492996

Batch 55370, train_perplexity=1293.6206, train_loss=7.1652

Batch 55380, train_perplexity=1115.7239, train_loss=7.0172586

Batch 55390, train_perplexity=1285.0061, train_loss=7.158519

Batch 55400, train_perplexity=1208.6744, train_loss=7.0972795

Batch 55410, train_perplexity=1136.9442, train_loss=7.0360994

Batch 55420, train_perplexity=1203.5576, train_loss=7.093037

Batch 55430, train_perplexity=1163.4833, train_loss=7.0591736

Batch 55440, train_perplexity=1200.9944, train_loss=7.090905

Batch 55450, train_perplexity=1396.5181, train_loss=7.2417374

Batch 55460, train_perplexity=1199.5281, train_loss=7.0896835

Batch 55470, train_perplexity=1202.2361, train_loss=7.0919385

Batch 55480, train_perplexity=1220.8884, train_loss=7.107334

Batch 55490, train_perplexity=1210.002, train_loss=7.098377

Batch 55500, train_perplexity=1267.5718, train_loss=7.1448584

Batch 55510, train_perplexity=1216.7656, train_loss=7.1039515

Batch 55520, train_perplexity=1254.1531, train_loss=7.134216

Batch 55530, train_perplexity=1240.8837, train_loss=7.123579

Batch 55540, train_perplexity=1222.8962, train_loss=7.1089773

Batch 55550, train_perplexity=1257.7267, train_loss=7.137061

Batch 55560, train_perplexity=1225.5956, train_loss=7.111182

Batch 55570, train_perplexity=1144.1765, train_loss=7.0424404

Batch 55580, train_perplexity=1275.4026, train_loss=7.151017

Batch 55590, train_perplexity=1319.7717, train_loss=7.185214

Batch 55600, train_perplexity=1172.2485, train_loss=7.066679

Batch 55610, train_perplexity=1141.9619, train_loss=7.040503

Batch 55620, train_perplexity=1260.164, train_loss=7.138997

Batch 55630, train_perplexity=1156.5326, train_loss=7.0531816

Batch 55640, train_perplexity=1242.4337, train_loss=7.1248274

Batch 55650, train_perplexity=1204.9133, train_loss=7.094163

Batch 55660, train_perplexity=1189.6271, train_loss=7.081395

Batch 55670, train_perplexity=1121.9429, train_loss=7.022817

Batch 55680, train_perplexity=1253.3186, train_loss=7.13355

Batch 55690, train_perplexity=1271.6255, train_loss=7.1480513

Batch 55700, train_perplexity=1189.3151, train_loss=7.081133

Batch 55710, train_perplexity=1202.0028, train_loss=7.0917444

Batch 55720, train_perplexity=1282.7031, train_loss=7.156725

Batch 55730, train_perplexity=1234.7747, train_loss=7.1186438

Batch 55740, train_perplexity=1103.9148, train_loss=7.006618

Batch 55750, train_perplexity=1190.87, train_loss=7.0824394

Batch 55760, train_perplexity=1172.6813, train_loss=7.067048

Batch 55770, train_perplexity=1314.1488, train_loss=7.1809444

Batch 55780, train_perplexity=1147.0465, train_loss=7.0449457

Batch 55790, train_perplexity=1194.336, train_loss=7.0853457

Batch 55800, train_perplexity=1219.2019, train_loss=7.105952

Batch 55810, train_perplexity=1180.043, train_loss=7.073306

Batch 55820, train_perplexity=1154.0767, train_loss=7.051056

Batch 55830, train_perplexity=1248.5841, train_loss=7.1297655

Batch 55840, train_perplexity=1333.4937, train_loss=7.1955576

Batch 55850, train_perplexity=1209.5289, train_loss=7.097986

Batch 55860, train_perplexity=1165.549, train_loss=7.0609474

Batch 55870, train_perplexity=1313.2893, train_loss=7.18029

Batch 55880, train_perplexity=1249.4149, train_loss=7.1304307

Batch 55890, train_perplexity=1383.7969, train_loss=7.2325864

Batch 55900, train_perplexity=1326.6769, train_loss=7.1904325

Batch 55910, train_perplexity=1285.3364, train_loss=7.158776

Batch 55920, train_perplexity=1316.4983, train_loss=7.1827307

Batch 55930, train_perplexity=1288.3002, train_loss=7.161079

Batch 55940, train_perplexity=1108.7142, train_loss=7.0109563

Batch 55950, train_perplexity=1149.2157, train_loss=7.046835

Batch 55960, train_perplexity=1294.0507, train_loss=7.1655326

Batch 55970, train_perplexity=1242.1873, train_loss=7.124629

Batch 55980, train_perplexity=1258.3757, train_loss=7.137577

Batch 55990, train_perplexity=1296.8013, train_loss=7.167656

Batch 56000, train_perplexity=1286.4684, train_loss=7.159656

Batch 56010, train_perplexity=1214.1205, train_loss=7.101775

Batch 56020, train_perplexity=1187.8212, train_loss=7.079876

Batch 56030, train_perplexity=1301.8507, train_loss=7.171542

Batch 56040, train_perplexity=1286.5027, train_loss=7.1596828

Batch 56050, train_perplexity=1235.3058, train_loss=7.119074

Batch 56060, train_perplexity=1168.515, train_loss=7.063489

Batch 56070, train_perplexity=1199.2993, train_loss=7.089493

Batch 56080, train_perplexity=1282.3569, train_loss=7.156455

Batch 56090, train_perplexity=1273.526, train_loss=7.1495447

Batch 56100, train_perplexity=1303.7019, train_loss=7.172963

Batch 56110, train_perplexity=1204.0197, train_loss=7.093421

Batch 56120, train_perplexity=1224.7549, train_loss=7.110496

Batch 56130, train_perplexity=1346.0842, train_loss=7.204955

Batch 56140, train_perplexity=1301.564, train_loss=7.171322

Batch 56150, train_perplexity=1300.5757, train_loss=7.1705623

Batch 56160, train_perplexity=1377.3619, train_loss=7.2279253

Batch 56170, train_perplexity=1251.4553, train_loss=7.1320624

Batch 56180, train_perplexity=1143.6893, train_loss=7.0420146

Batch 56190, train_perplexity=1239.3757, train_loss=7.122363

Batch 56200, train_perplexity=1260.4933, train_loss=7.1392584

Batch 56210, train_perplexity=1118.8314, train_loss=7.02004

Batch 56220, train_perplexity=1223.2059, train_loss=7.1092305

Batch 56230, train_perplexity=1245.1785, train_loss=7.127034

Batch 56240, train_perplexity=1200.589, train_loss=7.0905676

Batch 56250, train_perplexity=1367.7711, train_loss=7.2209377

Batch 56260, train_perplexity=1172.8977, train_loss=7.0672326

Batch 56270, train_perplexity=1303.9767, train_loss=7.173174

Batch 56280, train_perplexity=1206.3253, train_loss=7.095334

Batch 56290, train_perplexity=1228.6488, train_loss=7.1136703

Batch 56300, train_perplexity=1281.3778, train_loss=7.155691

Batch 56310, train_perplexity=1189.955, train_loss=7.0816708

Batch 56320, train_perplexity=1178.6887, train_loss=7.072158

Batch 56330, train_perplexity=1334.5438, train_loss=7.196345

Batch 56340, train_perplexity=1122.7975, train_loss=7.0235786

Batch 56350, train_perplexity=1304.544, train_loss=7.173609

Batch 56360, train_perplexity=1084.3625, train_loss=6.9887476

Batch 56370, train_perplexity=1308.3989, train_loss=7.1765594

Batch 56380, train_perplexity=1264.1312, train_loss=7.1421404

Batch 56390, train_perplexity=1182.448, train_loss=7.075342

Batch 56400, train_perplexity=1355.5527, train_loss=7.2119646

Batch 56410, train_perplexity=1187.9718, train_loss=7.080003

Batch 56420, train_perplexity=1193.1805, train_loss=7.084378

Batch 56430, train_perplexity=1232.3466, train_loss=7.1166754

Batch 56440, train_perplexity=1222.0214, train_loss=7.1082616

Batch 56450, train_perplexity=1340.045, train_loss=7.2004585

Batch 56460, train_perplexity=1259.8358, train_loss=7.1387367

Batch 56470, train_perplexity=1214.1528, train_loss=7.101802

Batch 56480, train_perplexity=1247.3726, train_loss=7.1287947

Batch 56490, train_perplexity=1130.8054, train_loss=7.0306854
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 56500, train_perplexity=1213.4618, train_loss=7.1012325

Batch 56510, train_perplexity=1176.9696, train_loss=7.0706983

Batch 56520, train_perplexity=1073.1193, train_loss=6.978325

Batch 56530, train_perplexity=1220.8733, train_loss=7.1073217

Batch 56540, train_perplexity=1130.5391, train_loss=7.03045

Batch 56550, train_perplexity=1254.6371, train_loss=7.1346016

Batch 56560, train_perplexity=1272.044, train_loss=7.1483803

Batch 56570, train_perplexity=1412.548, train_loss=7.2531505

Batch 56580, train_perplexity=1212.4288, train_loss=7.100381

Batch 56590, train_perplexity=1295.2847, train_loss=7.166486

Batch 56600, train_perplexity=1238.3113, train_loss=7.121504

Batch 56610, train_perplexity=1196.1593, train_loss=7.086871

Batch 56620, train_perplexity=1330.3588, train_loss=7.193204

Batch 56630, train_perplexity=1309.1796, train_loss=7.177156

Batch 56640, train_perplexity=1171.9915, train_loss=7.0664597

Batch 56650, train_perplexity=1138.1978, train_loss=7.0372014

Batch 56660, train_perplexity=1196.7595, train_loss=7.087373

Batch 56670, train_perplexity=1128.9811, train_loss=7.029071

Batch 56680, train_perplexity=1295.1833, train_loss=7.1664076

Batch 56690, train_perplexity=1219.5945, train_loss=7.1062737

Batch 56700, train_perplexity=1304.1173, train_loss=7.1732817

Batch 56710, train_perplexity=1284.9296, train_loss=7.158459

Batch 56720, train_perplexity=1188.7947, train_loss=7.080695

Batch 56730, train_perplexity=1279.2458, train_loss=7.154026

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Batch 56740, train_perplexity=1134.5494, train_loss=7.033991

Batch 56750, train_perplexity=1184.422, train_loss=7.07701

Batch 56760, train_perplexity=1170.5159, train_loss=7.0652

Batch 56770, train_perplexity=1237.767, train_loss=7.121064

Batch 56780, train_perplexity=1282.3173, train_loss=7.156424

Batch 56790, train_perplexity=1135.5691, train_loss=7.034889

Batch 56800, train_perplexity=1190.803, train_loss=7.082383

Batch 56810, train_perplexity=1309.5361, train_loss=7.1774282

Batch 56820, train_perplexity=1286.2291, train_loss=7.15947

Batch 56830, train_perplexity=1135.3363, train_loss=7.034684

Batch 56840, train_perplexity=1241.6921, train_loss=7.1242304

Batch 56850, train_perplexity=1324.2789, train_loss=7.1886234

Batch 56860, train_perplexity=1149.8745, train_loss=7.047408

Batch 56870, train_perplexity=1331.9596, train_loss=7.1944065

Batch 56880, train_perplexity=1107.171, train_loss=7.0095634

Batch 56890, train_perplexity=1119.6672, train_loss=7.020787

Batch 56900, train_perplexity=1167.7998, train_loss=7.0628767

Batch 56910, train_perplexity=1127.9556, train_loss=7.028162

Batch 56920, train_perplexity=1159.5044, train_loss=7.055748

Batch 56930, train_perplexity=1330.926, train_loss=7.19363

Batch 56940, train_perplexity=1278.1057, train_loss=7.1531343

Batch 56950, train_perplexity=1382.2089, train_loss=7.231438

Batch 56960, train_perplexity=1084.2064, train_loss=6.9886036

Batch 56970, train_perplexity=1229.6183, train_loss=7.114459

Batch 56980, train_perplexity=1129.8633, train_loss=7.029852

Batch 56990, train_perplexity=1250.39, train_loss=7.131211

Batch 57000, train_perplexity=1267.2551, train_loss=7.1446085

Batch 57010, train_perplexity=1168.1635, train_loss=7.063188

Batch 57020, train_perplexity=1284.1315, train_loss=7.157838

Batch 57030, train_perplexity=1181.4493, train_loss=7.074497

Batch 57040, train_perplexity=1251.3993, train_loss=7.1320176

Batch 57050, train_perplexity=1197.0323, train_loss=7.0876007

Batch 57060, train_perplexity=1206.362, train_loss=7.0953646

Batch 57070, train_perplexity=1122.1113, train_loss=7.0229673

Batch 57080, train_perplexity=1216.9663, train_loss=7.1041164

Batch 57090, train_perplexity=1171.921, train_loss=7.0663996

Batch 57100, train_perplexity=1236.2887, train_loss=7.119869

Batch 57110, train_perplexity=1136.0516, train_loss=7.035314

Batch 57120, train_perplexity=1213.5312, train_loss=7.1012897

Batch 57130, train_perplexity=1174.9117, train_loss=7.0689483

Batch 57140, train_perplexity=1254.2064, train_loss=7.1342583

Batch 57150, train_perplexity=1273.3997, train_loss=7.1494455

Batch 57160, train_perplexity=1229.1815, train_loss=7.114104

Batch 57170, train_perplexity=1215.848, train_loss=7.103197

Batch 57180, train_perplexity=1095.337, train_loss=6.9988174

Batch 57190, train_perplexity=1181.3418, train_loss=7.074406

Batch 57200, train_perplexity=1214.4181, train_loss=7.1020203

Batch 57210, train_perplexity=1248.1555, train_loss=7.129422

Batch 57220, train_perplexity=1252.7886, train_loss=7.133127

Batch 57230, train_perplexity=1198.1956, train_loss=7.088572

Batch 57240, train_perplexity=1217.0093, train_loss=7.1041517

Batch 57250, train_perplexity=1245.485, train_loss=7.12728

Batch 57260, train_perplexity=1283.4752, train_loss=7.1573267

Batch 57270, train_perplexity=1220.4462, train_loss=7.1069717

Batch 57280, train_perplexity=1268.2562, train_loss=7.145398

Batch 57290, train_perplexity=1338.4115, train_loss=7.199239

Batch 57300, train_perplexity=1171.407, train_loss=7.065961

Batch 57310, train_perplexity=1197.2104, train_loss=7.0877495

Batch 57320, train_perplexity=1270.4266, train_loss=7.147108

Batch 57330, train_perplexity=1282.722, train_loss=7.1567397

Batch 57340, train_perplexity=1233.2136, train_loss=7.1173787

Batch 57350, train_perplexity=1130.5547, train_loss=7.0304637

Batch 57360, train_perplexity=1162.831, train_loss=7.058613

Batch 57370, train_perplexity=1301.407, train_loss=7.171201

Batch 57380, train_perplexity=1233.8777, train_loss=7.117917

Batch 57390, train_perplexity=1281.9277, train_loss=7.1561203

Batch 57400, train_perplexity=1206.865, train_loss=7.0957813

Batch 57410, train_perplexity=1258.5354, train_loss=7.137704

Batch 57420, train_perplexity=1165.9752, train_loss=7.061313

Batch 57430, train_perplexity=1231.3127, train_loss=7.115836

Batch 57440, train_perplexity=1103.6721, train_loss=7.006398

Batch 57450, train_perplexity=1214.5281, train_loss=7.102111

Batch 57460, train_perplexity=1239.8651, train_loss=7.122758

Batch 57470, train_perplexity=1160.0243, train_loss=7.056196

Batch 57480, train_perplexity=1164.3368, train_loss=7.059907

Batch 57490, train_perplexity=1217.9364, train_loss=7.104913

Batch 57500, train_perplexity=1239.1211, train_loss=7.1221576

Batch 57510, train_perplexity=1247.5992, train_loss=7.1289763

Batch 57520, train_perplexity=1239.1394, train_loss=7.1221724

Batch 57530, train_perplexity=1239.046, train_loss=7.122097

Batch 57540, train_perplexity=1174.5515, train_loss=7.0686417

Batch 57550, train_perplexity=1286.4984, train_loss=7.1596794

Batch 57560, train_perplexity=1269.3452, train_loss=7.1462564

Batch 57570, train_perplexity=1248.7771, train_loss=7.12992

Batch 57580, train_perplexity=1182.8406, train_loss=7.075674

Batch 57590, train_perplexity=1307.0619, train_loss=7.175537

Batch 57600, train_perplexity=1202.8651, train_loss=7.0924616

Batch 57610, train_perplexity=1091.9474, train_loss=6.995718

Batch 57620, train_perplexity=1220.0941, train_loss=7.1066833

Batch 57630, train_perplexity=1188.3673, train_loss=7.0803356

Batch 57640, train_perplexity=1217.8842, train_loss=7.1048703

Batch 57650, train_perplexity=1236.0872, train_loss=7.119706

Batch 57660, train_perplexity=1256.3499, train_loss=7.135966

Batch 57670, train_perplexity=1264.6896, train_loss=7.142582

Batch 57680, train_perplexity=1312.7196, train_loss=7.1798563

Batch 57690, train_perplexity=1289.9175, train_loss=7.1623335

Batch 57700, train_perplexity=1189.4558, train_loss=7.081251

Batch 57710, train_perplexity=1241.1736, train_loss=7.1238127

Batch 57720, train_perplexity=1193.7975, train_loss=7.0848947

Batch 57730, train_perplexity=1230.8208, train_loss=7.1154366

Batch 57740, train_perplexity=1237.1934, train_loss=7.1206007

Batch 57750, train_perplexity=1092.2239, train_loss=6.995971
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 57760, train_perplexity=1285.1165, train_loss=7.1586046

Batch 57770, train_perplexity=1181.197, train_loss=7.0742836

Batch 57780, train_perplexity=1325.7859, train_loss=7.1897607

Batch 57790, train_perplexity=1224.3829, train_loss=7.1101923

Batch 57800, train_perplexity=1222.2201, train_loss=7.108424

Batch 57810, train_perplexity=1288.4543, train_loss=7.1611986

Batch 57820, train_perplexity=1266.503, train_loss=7.144015

Batch 57830, train_perplexity=1132.3687, train_loss=7.032067

Batch 57840, train_perplexity=1199.0677, train_loss=7.0892997

Batch 57850, train_perplexity=1292.0529, train_loss=7.1639876

Batch 57860, train_perplexity=1123.3812, train_loss=7.0240984

Batch 57870, train_perplexity=1193.4019, train_loss=7.0845633

Batch 57880, train_perplexity=1115.5988, train_loss=7.0171466

Batch 57890, train_perplexity=1192.7466, train_loss=7.084014

Batch 57900, train_perplexity=1184.19, train_loss=7.076814

Batch 57910, train_perplexity=1162.2113, train_loss=7.0580797

Batch 57920, train_perplexity=1212.0751, train_loss=7.100089

Batch 57930, train_perplexity=1247.2655, train_loss=7.128709

Batch 57940, train_perplexity=1302.7288, train_loss=7.1722164

Batch 57950, train_perplexity=1192.9479, train_loss=7.0841827

Batch 57960, train_perplexity=1225.203, train_loss=7.110862

Batch 57970, train_perplexity=1190.862, train_loss=7.0824327

Batch 57980, train_perplexity=1318.6658, train_loss=7.184376

Batch 57990, train_perplexity=1198.7322, train_loss=7.08902

Batch 58000, train_perplexity=1275.3765, train_loss=7.1509967

Batch 58010, train_perplexity=1202.3524, train_loss=7.0920353

Batch 58020, train_perplexity=1247.9222, train_loss=7.1292353

Batch 58030, train_perplexity=1273.2218, train_loss=7.149306

Batch 58040, train_perplexity=1199.1793, train_loss=7.0893927

Batch 58050, train_perplexity=1226.1006, train_loss=7.111594

Batch 58060, train_perplexity=1372.2499, train_loss=7.224207

Batch 58070, train_perplexity=1255.4402, train_loss=7.1352415

Batch 58080, train_perplexity=1160.3662, train_loss=7.056491

Batch 58090, train_perplexity=1152.288, train_loss=7.0495048

Batch 58100, train_perplexity=1122.0808, train_loss=7.02294

Batch 58110, train_perplexity=1286.0115, train_loss=7.159301

Batch 58120, train_perplexity=1182.2891, train_loss=7.0752077

Batch 58130, train_perplexity=1310.1075, train_loss=7.1778646

Batch 58140, train_perplexity=1253.7974, train_loss=7.133932

Batch 58150, train_perplexity=1320.4918, train_loss=7.1857595

Batch 58160, train_perplexity=1314.862, train_loss=7.181487

Batch 58170, train_perplexity=1234.455, train_loss=7.118385

Batch 58180, train_perplexity=1167.91, train_loss=7.062971

Batch 58190, train_perplexity=1162.2811, train_loss=7.05814

Batch 58200, train_perplexity=1214.2571, train_loss=7.1018877

Batch 58210, train_perplexity=1149.1554, train_loss=7.0467825

Batch 58220, train_perplexity=1283.4073, train_loss=7.157274

Batch 58230, train_perplexity=1047.072, train_loss=6.953753

Batch 58240, train_perplexity=1249.2428, train_loss=7.130293

Batch 58250, train_perplexity=1248.7664, train_loss=7.1299114

Batch 58260, train_perplexity=1115.9026, train_loss=7.017419

Batch 58270, train_perplexity=1221.1493, train_loss=7.1075478

Batch 58280, train_perplexity=1118.2756, train_loss=7.019543

Batch 58290, train_perplexity=1233.43, train_loss=7.117554

Batch 58300, train_perplexity=1141.6107, train_loss=7.0401955

Batch 58310, train_perplexity=1278.2228, train_loss=7.153226

Batch 58320, train_perplexity=1129.5697, train_loss=7.029592

Batch 58330, train_perplexity=1277.0968, train_loss=7.1523447

Batch 58340, train_perplexity=1161.9791, train_loss=7.05788

Batch 58350, train_perplexity=1182.977, train_loss=7.0757895

Batch 58360, train_perplexity=1223.4293, train_loss=7.109413

Batch 58370, train_perplexity=1186.1141, train_loss=7.078438

Batch 58380, train_perplexity=1203.5536, train_loss=7.093034

Batch 58390, train_perplexity=1195.0168, train_loss=7.0859156

Batch 58400, train_perplexity=1186.4717, train_loss=7.078739

Batch 58410, train_perplexity=1256.0826, train_loss=7.135753

Batch 58420, train_perplexity=1155.0919, train_loss=7.051935

Batch 58430, train_perplexity=1117.5671, train_loss=7.0189095

Batch 58440, train_perplexity=1154.784, train_loss=7.0516686

Batch 58450, train_perplexity=1308.193, train_loss=7.176402

Batch 58460, train_perplexity=1298.1611, train_loss=7.168704

Batch 58470, train_perplexity=1235.6711, train_loss=7.1193695

Batch 58480, train_perplexity=1259.4022, train_loss=7.1383924

Batch 58490, train_perplexity=1146.5304, train_loss=7.0444956

Batch 58500, train_perplexity=1215.8527, train_loss=7.103201

Batch 58510, train_perplexity=1083.9092, train_loss=6.9883294

Batch 58520, train_perplexity=1135.4852, train_loss=7.0348153

Batch 58530, train_perplexity=1237.6158, train_loss=7.120942

Batch 58540, train_perplexity=1166.4696, train_loss=7.061737

Batch 58550, train_perplexity=1232.3441, train_loss=7.1166735

Batch 58560, train_perplexity=1260.6074, train_loss=7.139349

Batch 58570, train_perplexity=1195.873, train_loss=7.086632

Batch 58580, train_perplexity=1188.6733, train_loss=7.080593

Batch 58590, train_perplexity=1153.3961, train_loss=7.050466

Batch 58600, train_perplexity=1248.5758, train_loss=7.129759

Batch 58610, train_perplexity=1185.5204, train_loss=7.077937

Batch 58620, train_perplexity=1174.6086, train_loss=7.0686903

Batch 58630, train_perplexity=1212.6439, train_loss=7.1005583

Batch 58640, train_perplexity=1340.9425, train_loss=7.201128

Batch 58650, train_perplexity=1163.4711, train_loss=7.059163

Batch 58660, train_perplexity=1230.4359, train_loss=7.1151237

Batch 58670, train_perplexity=1116.6467, train_loss=7.0180855

Batch 58680, train_perplexity=1184.9462, train_loss=7.0774527

Batch 58690, train_perplexity=1210.28, train_loss=7.098607

Batch 58700, train_perplexity=1165.6123, train_loss=7.061002

Batch 58710, train_perplexity=1233.7694, train_loss=7.1178293

Batch 58720, train_perplexity=1170.7626, train_loss=7.0654106

Batch 58730, train_perplexity=1213.3906, train_loss=7.101174

Batch 58740, train_perplexity=1192.6613, train_loss=7.0839424

Batch 58750, train_perplexity=1225.675, train_loss=7.111247

Batch 58760, train_perplexity=1176.8074, train_loss=7.0705605

Batch 58770, train_perplexity=1211.4331, train_loss=7.0995593

Batch 58780, train_perplexity=1177.0992, train_loss=7.0708084

Batch 58790, train_perplexity=1204.6313, train_loss=7.093929

Batch 58800, train_perplexity=1127.6415, train_loss=7.0278835

Batch 58810, train_perplexity=1181.5857, train_loss=7.0746126

Batch 58820, train_perplexity=1189.9243, train_loss=7.081645

Batch 58830, train_perplexity=1156.1554, train_loss=7.0528555

Batch 58840, train_perplexity=1058.5557, train_loss=6.9646606

Batch 58850, train_perplexity=1177.3047, train_loss=7.070983

Batch 58860, train_perplexity=1141.4312, train_loss=7.040038

Batch 58870, train_perplexity=1291.7024, train_loss=7.1637163

Batch 58880, train_perplexity=1298.1525, train_loss=7.1686974

Batch 58890, train_perplexity=1276.849, train_loss=7.1521506

Batch 58900, train_perplexity=1204.2356, train_loss=7.0936003

Batch 58910, train_perplexity=1303.3718, train_loss=7.17271

Batch 58920, train_perplexity=1344.2075, train_loss=7.20356

Batch 58930, train_perplexity=1212.0964, train_loss=7.1001067

Batch 58940, train_perplexity=1160.2821, train_loss=7.0564184

Batch 58950, train_perplexity=1232.732, train_loss=7.116988

Batch 58960, train_perplexity=1290.0256, train_loss=7.1624174

Batch 58970, train_perplexity=1197.9762, train_loss=7.088389

Batch 58980, train_perplexity=1248.6318, train_loss=7.1298037

Batch 58990, train_perplexity=1080.2782, train_loss=6.984974

Batch 59000, train_perplexity=1206.7539, train_loss=7.0956893

Batch 59010, train_perplexity=1196.2688, train_loss=7.0869627

Batch 59020, train_perplexity=1223.1843, train_loss=7.109213

Batch 59030, train_perplexity=1375.5714, train_loss=7.2266245

Batch 59040, train_perplexity=1212.7411, train_loss=7.1006384

Batch 59050, train_perplexity=1185.9513, train_loss=7.0783005

Batch 59060, train_perplexity=1222.181, train_loss=7.1083922

Batch 59070, train_perplexity=1279.8005, train_loss=7.1544595
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 59080, train_perplexity=1141.8181, train_loss=7.040377

Batch 59090, train_perplexity=1186.6957, train_loss=7.078928

Batch 59100, train_perplexity=1325.7593, train_loss=7.1897407

Batch 59110, train_perplexity=1297.645, train_loss=7.1683064

Batch 59120, train_perplexity=1365.2767, train_loss=7.2191124

Batch 59130, train_perplexity=1166.5386, train_loss=7.061796

Batch 59140, train_perplexity=1182.9196, train_loss=7.075741

Batch 59150, train_perplexity=1171.1573, train_loss=7.0657477

Batch 59160, train_perplexity=1199.6952, train_loss=7.089823

Batch 59170, train_perplexity=1154.5204, train_loss=7.0514402

Batch 59180, train_perplexity=1194.7742, train_loss=7.0857124

Batch 59190, train_perplexity=1138.1593, train_loss=7.0371675

Batch 59200, train_perplexity=1138.8151, train_loss=7.0377436

Batch 59210, train_perplexity=1294.8005, train_loss=7.166112

Batch 59220, train_perplexity=1161.4733, train_loss=7.0574446

Batch 59230, train_perplexity=1219.3025, train_loss=7.1060343

Batch 59240, train_perplexity=1135.8984, train_loss=7.035179

Batch 59250, train_perplexity=1126.8202, train_loss=7.027155

Batch 59260, train_perplexity=1319.157, train_loss=7.184748

Batch 59270, train_perplexity=1251.3683, train_loss=7.131993

Batch 59280, train_perplexity=1234.7711, train_loss=7.118641

Batch 59290, train_perplexity=1213.9531, train_loss=7.1016374

Batch 59300, train_perplexity=1200.4912, train_loss=7.090486

Batch 59310, train_perplexity=1148.7434, train_loss=7.046424

Batch 59320, train_perplexity=1058.0662, train_loss=6.964198

Batch 59330, train_perplexity=1327.7617, train_loss=7.19125

Batch 59340, train_perplexity=1201.1926, train_loss=7.09107

Batch 59350, train_perplexity=1176.6783, train_loss=7.070451

Batch 59360, train_perplexity=1218.4841, train_loss=7.105363

Batch 59370, train_perplexity=1185.2356, train_loss=7.077697

Batch 59380, train_perplexity=1356.1166, train_loss=7.2123804

Batch 59390, train_perplexity=1119.5684, train_loss=7.0206985

Batch 59400, train_perplexity=1193.5305, train_loss=7.084671

Batch 59410, train_perplexity=1126.6439, train_loss=7.0269985

Batch 59420, train_perplexity=1290.3148, train_loss=7.1626415

Batch 59430, train_perplexity=1177.0409, train_loss=7.070759

Batch 59440, train_perplexity=1253.3096, train_loss=7.133543

Batch 59450, train_perplexity=1242.2103, train_loss=7.1246476

Batch 59460, train_perplexity=1223.9114, train_loss=7.109807

Batch 59470, train_perplexity=1134.8319, train_loss=7.03424

Batch 59480, train_perplexity=1250.7317, train_loss=7.131484

Batch 59490, train_perplexity=1192.8182, train_loss=7.084074

Batch 59500, train_perplexity=1290.196, train_loss=7.1625495

Batch 59510, train_perplexity=1102.6842, train_loss=7.0055027

Batch 59520, train_perplexity=1144.8264, train_loss=7.0430083

Batch 59530, train_perplexity=1224.2394, train_loss=7.110075

Batch 59540, train_perplexity=1306.9043, train_loss=7.1754165

Batch 59550, train_perplexity=1239.7499, train_loss=7.122665

Batch 59560, train_perplexity=1191.9022, train_loss=7.083306

Batch 59570, train_perplexity=1186.849, train_loss=7.079057

Batch 59580, train_perplexity=1221.1528, train_loss=7.1075506

Batch 59590, train_perplexity=1132.2374, train_loss=7.031951

Batch 59600, train_perplexity=1165.8424, train_loss=7.061199

Batch 59610, train_perplexity=1068.6593, train_loss=6.97416

Batch 59620, train_perplexity=1150.3933, train_loss=7.047859

Batch 59630, train_perplexity=1284.3862, train_loss=7.158036

Batch 59640, train_perplexity=1155.9448, train_loss=7.0526733

Batch 59650, train_perplexity=1269.8671, train_loss=7.1466675

Batch 59660, train_perplexity=1108.4785, train_loss=7.0107436

Batch 59670, train_perplexity=1095.6945, train_loss=6.9991436

Batch 59680, train_perplexity=1198.3573, train_loss=7.088707

Batch 59690, train_perplexity=1204.911, train_loss=7.094161

Batch 59700, train_perplexity=1248.6794, train_loss=7.129842

Batch 59710, train_perplexity=1133.9956, train_loss=7.0335026

Batch 59720, train_perplexity=1181.1981, train_loss=7.0742846

Batch 59730, train_perplexity=1246.8588, train_loss=7.1283827

Batch 59740, train_perplexity=1217.9451, train_loss=7.1049204

Batch 59750, train_perplexity=1160.9805, train_loss=7.05702

Batch 59760, train_perplexity=1127.1587, train_loss=7.0274553

Batch 59770, train_perplexity=1202.3777, train_loss=7.0920563

Batch 59780, train_perplexity=1199.6626, train_loss=7.0897956

Batch 59790, train_perplexity=1191.5198, train_loss=7.082985

Batch 59800, train_perplexity=1202.2573, train_loss=7.091956

Batch 59810, train_perplexity=1199.9401, train_loss=7.090027

Batch 59820, train_perplexity=1147.3037, train_loss=7.04517

Batch 59830, train_perplexity=1212.5225, train_loss=7.100458

Batch 59840, train_perplexity=1192.9171, train_loss=7.084157

Batch 59850, train_perplexity=1246.1658, train_loss=7.1278267

Batch 59860, train_perplexity=1190.7325, train_loss=7.082324

Batch 59870, train_perplexity=1173.6519, train_loss=7.0678754

Batch 59880, train_perplexity=1129.5282, train_loss=7.0295553

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100
Loaded 306116 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100
Loaded 306116 sentences.
Finished loading
Batch 59890, train_perplexity=1252.8197, train_loss=7.133152

Batch 59900, train_perplexity=1246.8231, train_loss=7.128354

Batch 59910, train_perplexity=1282.1687, train_loss=7.156308

Batch 59920, train_perplexity=1164.2491, train_loss=7.0598316

Batch 59930, train_perplexity=1255.2617, train_loss=7.1350994

Batch 59940, train_perplexity=1188.0647, train_loss=7.080081

Batch 59950, train_perplexity=1219.3374, train_loss=7.106063

Batch 59960, train_perplexity=1232.8744, train_loss=7.1171036

Batch 59970, train_perplexity=1267.8523, train_loss=7.1450796

Batch 59980, train_perplexity=1249.6056, train_loss=7.1305833

Batch 59990, train_perplexity=1197.8762, train_loss=7.0883055

Batch 60000, train_perplexity=1309.1852, train_loss=7.1771603

Batch 60010, train_perplexity=1182.3296, train_loss=7.075242

Batch 60020, train_perplexity=1251.623, train_loss=7.1321964

Batch 60030, train_perplexity=1200.3899, train_loss=7.0904016

Batch 60040, train_perplexity=1312.9287, train_loss=7.1800156

Batch 60050, train_perplexity=1233.5524, train_loss=7.1176534

Batch 60060, train_perplexity=1291.0743, train_loss=7.16323

Batch 60070, train_perplexity=1208.8947, train_loss=7.0974617

Batch 60080, train_perplexity=1107.5956, train_loss=7.009947

Batch 60090, train_perplexity=1169.409, train_loss=7.064254

Batch 60100, train_perplexity=1225.7476, train_loss=7.111306

Batch 60110, train_perplexity=1139.4136, train_loss=7.038269

Batch 60120, train_perplexity=1322.502, train_loss=7.1872807

Batch 60130, train_perplexity=1127.5673, train_loss=7.0278177

Batch 60140, train_perplexity=1124.437, train_loss=7.025038

Batch 60150, train_perplexity=1128.6312, train_loss=7.028761

Batch 60160, train_perplexity=1193.5112, train_loss=7.084655

Batch 60170, train_perplexity=1175.6061, train_loss=7.069539

Batch 60180, train_perplexity=1171.3925, train_loss=7.0659485

Batch 60190, train_perplexity=1119.4446, train_loss=7.020588

Batch 60200, train_perplexity=1181.1244, train_loss=7.074222

Batch 60210, train_perplexity=1232.4006, train_loss=7.1167192

Batch 60220, train_perplexity=1267.9961, train_loss=7.145193

Batch 60230, train_perplexity=1267.8323, train_loss=7.145064

Batch 60240, train_perplexity=1250.9047, train_loss=7.1316223

Batch 60250, train_perplexity=1136.114, train_loss=7.035369

Batch 60260, train_perplexity=1188.4885, train_loss=7.0804377

Batch 60270, train_perplexity=1248.689, train_loss=7.1298494

Batch 60280, train_perplexity=1177.6011, train_loss=7.0712347

Batch 60290, train_perplexity=1164.208, train_loss=7.0597963

Batch 60300, train_perplexity=1184.1295, train_loss=7.076763

Batch 60310, train_perplexity=1276.1996, train_loss=7.151642

Batch 60320, train_perplexity=1197.271, train_loss=7.0878

Batch 60330, train_perplexity=1282.0867, train_loss=7.1562443
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 60340, train_perplexity=1227.6217, train_loss=7.112834

Batch 60350, train_perplexity=1225.5289, train_loss=7.111128

Batch 60360, train_perplexity=1211.9462, train_loss=7.0999827

Batch 60370, train_perplexity=1104.2544, train_loss=7.0069256

Batch 60380, train_perplexity=1347.0044, train_loss=7.2056384

Batch 60390, train_perplexity=1270.878, train_loss=7.1474633

Batch 60400, train_perplexity=1258.1843, train_loss=7.137425

Batch 60410, train_perplexity=1266.4565, train_loss=7.143978

Batch 60420, train_perplexity=1201.1313, train_loss=7.091019

Batch 60430, train_perplexity=1124.8103, train_loss=7.0253696

Batch 60440, train_perplexity=1204.1954, train_loss=7.093567

Batch 60450, train_perplexity=1248.8706, train_loss=7.129995

Batch 60460, train_perplexity=1159.5708, train_loss=7.055805

Batch 60470, train_perplexity=1269.2677, train_loss=7.1461954

Batch 60480, train_perplexity=1201.0333, train_loss=7.0909376

Batch 60490, train_perplexity=1115.1526, train_loss=7.0167465

Batch 60500, train_perplexity=1244.3262, train_loss=7.1263494

Batch 60510, train_perplexity=1289.5276, train_loss=7.162031

Batch 60520, train_perplexity=1193.0651, train_loss=7.084281

Batch 60530, train_perplexity=1204.9961, train_loss=7.0942316

Batch 60540, train_perplexity=1109.5715, train_loss=7.0117292

Batch 60550, train_perplexity=1296.9614, train_loss=7.1677794

Batch 60560, train_perplexity=1142.5387, train_loss=7.041008

Batch 60570, train_perplexity=1086.2075, train_loss=6.9904475

Batch 60580, train_perplexity=1119.9443, train_loss=7.0210342

Batch 60590, train_perplexity=1153.8098, train_loss=7.0508246

Batch 60600, train_perplexity=1283.4305, train_loss=7.157292

Batch 60610, train_perplexity=1156.269, train_loss=7.0529537

Batch 60620, train_perplexity=1219.1566, train_loss=7.1059146

Batch 60630, train_perplexity=1325.7372, train_loss=7.189724

Batch 60640, train_perplexity=1321.6345, train_loss=7.1866245

Batch 60650, train_perplexity=1173.5046, train_loss=7.06775

Batch 60660, train_perplexity=1197.7346, train_loss=7.088187

Batch 60670, train_perplexity=1232.4393, train_loss=7.1167507

Batch 60680, train_perplexity=1125.6205, train_loss=7.0260897

Batch 60690, train_perplexity=1156.8955, train_loss=7.0534954

Batch 60700, train_perplexity=1220.8285, train_loss=7.107285

Batch 60710, train_perplexity=1169.4023, train_loss=7.064248

Batch 60720, train_perplexity=1312.6332, train_loss=7.1797905

Batch 60730, train_perplexity=1267.9465, train_loss=7.145154

Batch 60740, train_perplexity=1110.5688, train_loss=7.0126276

Batch 60750, train_perplexity=1132.327, train_loss=7.03203

Batch 60760, train_perplexity=1109.1632, train_loss=7.011361

Batch 60770, train_perplexity=1263.343, train_loss=7.1415167

Batch 60780, train_perplexity=1179.8944, train_loss=7.07318

Batch 60790, train_perplexity=1239.2776, train_loss=7.122284

Batch 60800, train_perplexity=1102.4929, train_loss=7.005329

Batch 60810, train_perplexity=1269.1672, train_loss=7.1461163

Batch 60820, train_perplexity=1223.0642, train_loss=7.1091146

Batch 60830, train_perplexity=1208.9218, train_loss=7.097484

Batch 60840, train_perplexity=1146.8246, train_loss=7.044752

Batch 60850, train_perplexity=1266.3804, train_loss=7.143918

Batch 60860, train_perplexity=1210.5444, train_loss=7.0988255

Batch 60870, train_perplexity=1036.9489, train_loss=6.944038

Batch 60880, train_perplexity=1182.9257, train_loss=7.075746

Batch 60890, train_perplexity=1165.6835, train_loss=7.061063

Batch 60900, train_perplexity=1169.8541, train_loss=7.0646343

Batch 60910, train_perplexity=1252.0159, train_loss=7.13251

Batch 60920, train_perplexity=1080.5492, train_loss=6.9852247

Batch 60930, train_perplexity=1222.6829, train_loss=7.108803

Batch 60940, train_perplexity=1180.501, train_loss=7.073694

Batch 60950, train_perplexity=1193.3075, train_loss=7.084484

Batch 60960, train_perplexity=1109.2816, train_loss=7.011468

Batch 60970, train_perplexity=1290.0521, train_loss=7.162438

Batch 60980, train_perplexity=1136.1909, train_loss=7.0354366

Batch 60990, train_perplexity=1236.1603, train_loss=7.1197653

Batch 61000, train_perplexity=1027.1818, train_loss=6.934574

Batch 61010, train_perplexity=1143.2832, train_loss=7.0416594

Batch 61020, train_perplexity=1331.8484, train_loss=7.194323

Batch 61030, train_perplexity=1179.1497, train_loss=7.072549

Batch 61040, train_perplexity=1144.565, train_loss=7.04278

Batch 61050, train_perplexity=1152.8496, train_loss=7.049992

Batch 61060, train_perplexity=1148.5944, train_loss=7.046294

Batch 61070, train_perplexity=1131.4441, train_loss=7.03125

Batch 61080, train_perplexity=1165.699, train_loss=7.061076

Batch 61090, train_perplexity=1210.9508, train_loss=7.099161

Batch 61100, train_perplexity=1164.0781, train_loss=7.0596848

Batch 61110, train_perplexity=1251.7478, train_loss=7.132296

Batch 61120, train_perplexity=1120.2787, train_loss=7.0213327

Batch 61130, train_perplexity=1256.0227, train_loss=7.1357055

Batch 61140, train_perplexity=1192.999, train_loss=7.0842257

Batch 61150, train_perplexity=1191.4619, train_loss=7.0829363

Batch 61160, train_perplexity=1113.4735, train_loss=7.0152397

Batch 61170, train_perplexity=1174.705, train_loss=7.0687723

Batch 61180, train_perplexity=1180.5641, train_loss=7.0737476

Batch 61190, train_perplexity=1232.6416, train_loss=7.1169147

Batch 61200, train_perplexity=1279.7987, train_loss=7.154458

Batch 61210, train_perplexity=1215.6232, train_loss=7.103012

Batch 61220, train_perplexity=1289.9531, train_loss=7.162361

Batch 61230, train_perplexity=1290.4281, train_loss=7.1627293

Batch 61240, train_perplexity=1124.7335, train_loss=7.0253015

Batch 61250, train_perplexity=1249.0123, train_loss=7.1301084

Batch 61260, train_perplexity=1236.8011, train_loss=7.1202836

Batch 61270, train_perplexity=1262.4409, train_loss=7.1408024

Batch 61280, train_perplexity=1266.9337, train_loss=7.144355

Batch 61290, train_perplexity=1122.4624, train_loss=7.02328

Batch 61300, train_perplexity=1261.4932, train_loss=7.1400514

Batch 61310, train_perplexity=1164.2125, train_loss=7.0598

Batch 61320, train_perplexity=1160.2046, train_loss=7.0563517

Batch 61330, train_perplexity=1243.7834, train_loss=7.125913

Batch 61340, train_perplexity=1147.5477, train_loss=7.0453825

Batch 61350, train_perplexity=1131.2584, train_loss=7.031086

Batch 61360, train_perplexity=1184.9287, train_loss=7.077438

Batch 61370, train_perplexity=1178.8726, train_loss=7.072314

Batch 61380, train_perplexity=1187.4191, train_loss=7.0795374

Batch 61390, train_perplexity=1203.2506, train_loss=7.092782

Batch 61400, train_perplexity=1210.4364, train_loss=7.0987363

Batch 61410, train_perplexity=1269.0402, train_loss=7.146016

Batch 61420, train_perplexity=1245.1975, train_loss=7.1270494

Batch 61430, train_perplexity=1321.9346, train_loss=7.1868515

Batch 61440, train_perplexity=1172.2642, train_loss=7.0666924

Batch 61450, train_perplexity=1197.8419, train_loss=7.088277

Batch 61460, train_perplexity=1257.151, train_loss=7.1366034

Batch 61470, train_perplexity=1138.2776, train_loss=7.0372715

Batch 61480, train_perplexity=1146.4429, train_loss=7.0444193

Batch 61490, train_perplexity=1230.5767, train_loss=7.115238

Batch 61500, train_perplexity=1350.7806, train_loss=7.208438

Batch 61510, train_perplexity=1140.0038, train_loss=7.038787

Batch 61520, train_perplexity=1065.7993, train_loss=6.9714804

Batch 61530, train_perplexity=1119.3912, train_loss=7.02054

Batch 61540, train_perplexity=1180.8344, train_loss=7.0739765

Batch 61550, train_perplexity=1173.2708, train_loss=7.0675507

Batch 61560, train_perplexity=1183.2416, train_loss=7.076013

Batch 61570, train_perplexity=1270.016, train_loss=7.146785

Batch 61580, train_perplexity=1194.1202, train_loss=7.085165

Batch 61590, train_perplexity=1179.4309, train_loss=7.0727873

Batch 61600, train_perplexity=1275.8223, train_loss=7.151346

Batch 61610, train_perplexity=1275.8577, train_loss=7.151374

Batch 61620, train_perplexity=1105.6912, train_loss=7.008226

Batch 61630, train_perplexity=1133.8171, train_loss=7.033345

Batch 61640, train_perplexity=1094.9622, train_loss=6.998475

Batch 61650, train_perplexity=1295.7789, train_loss=7.1668673
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61660, train_perplexity=1230.1196, train_loss=7.1148667

Batch 61670, train_perplexity=1137.0863, train_loss=7.0362244

Batch 61680, train_perplexity=1114.5168, train_loss=7.016176

Batch 61690, train_perplexity=1224.0543, train_loss=7.109924

Batch 61700, train_perplexity=1151.0645, train_loss=7.0484424

Batch 61710, train_perplexity=1248.4615, train_loss=7.1296673

Batch 61720, train_perplexity=1206.3287, train_loss=7.095337

Batch 61730, train_perplexity=1232.0404, train_loss=7.116427

Batch 61740, train_perplexity=1215.677, train_loss=7.1030564

Batch 61750, train_perplexity=1119.3154, train_loss=7.0204725

Batch 61760, train_perplexity=1273.016, train_loss=7.149144

Batch 61770, train_perplexity=1201.4126, train_loss=7.0912533

Batch 61780, train_perplexity=1258.9603, train_loss=7.1380415

Batch 61790, train_perplexity=1294.2129, train_loss=7.165658

Batch 61800, train_perplexity=1156.9231, train_loss=7.0535192

Batch 61810, train_perplexity=1242.7732, train_loss=7.1251006

Batch 61820, train_perplexity=1149.277, train_loss=7.0468884

Batch 61830, train_perplexity=1194.7325, train_loss=7.0856776

Batch 61840, train_perplexity=1139.0757, train_loss=7.0379725

Batch 61850, train_perplexity=1211.6763, train_loss=7.09976

Batch 61860, train_perplexity=1200.186, train_loss=7.090232

Batch 61870, train_perplexity=1136.6732, train_loss=7.035861

Batch 61880, train_perplexity=1180.4222, train_loss=7.0736275

Batch 61890, train_perplexity=1132.9136, train_loss=7.032548

Batch 61900, train_perplexity=1170.3295, train_loss=7.0650406

Batch 61910, train_perplexity=1200.5266, train_loss=7.0905156

Batch 61920, train_perplexity=1267.6914, train_loss=7.144953

Batch 61930, train_perplexity=1151.8771, train_loss=7.049148

Batch 61940, train_perplexity=1133.7798, train_loss=7.0333123

Batch 61950, train_perplexity=1141.4332, train_loss=7.04004

Batch 61960, train_perplexity=1130.4022, train_loss=7.0303288

Batch 61970, train_perplexity=1067.4468, train_loss=6.973025

Batch 61980, train_perplexity=1138.0225, train_loss=7.0370474

Batch 61990, train_perplexity=1307.3325, train_loss=7.175744

Batch 62000, train_perplexity=1380.1257, train_loss=7.22993

Batch 62010, train_perplexity=1250.7532, train_loss=7.131501

Batch 62020, train_perplexity=1238.0195, train_loss=7.1212683

Batch 62030, train_perplexity=1186.8779, train_loss=7.0790815

Batch 62040, train_perplexity=1183.8179, train_loss=7.0765

Batch 62050, train_perplexity=1232.7086, train_loss=7.116969

Batch 62060, train_perplexity=1163.8706, train_loss=7.0595064

Batch 62070, train_perplexity=1217.2031, train_loss=7.104311

Batch 62080, train_perplexity=1215.8376, train_loss=7.1031885

Batch 62090, train_perplexity=1127.1394, train_loss=7.027438

Batch 62100, train_perplexity=1153.3912, train_loss=7.050462

Batch 62110, train_perplexity=1223.857, train_loss=7.1097627

Batch 62120, train_perplexity=1179.405, train_loss=7.0727654

Batch 62130, train_perplexity=1215.7194, train_loss=7.1030912

Batch 62140, train_perplexity=1165.9813, train_loss=7.0613184

Batch 62150, train_perplexity=1103.9711, train_loss=7.006669

Batch 62160, train_perplexity=1242.2275, train_loss=7.1246614

Batch 62170, train_perplexity=1145.1049, train_loss=7.0432515

Batch 62180, train_perplexity=1135.2416, train_loss=7.0346007

Batch 62190, train_perplexity=1119.5887, train_loss=7.0207167

Batch 62200, train_perplexity=1063.5027, train_loss=6.969323

Batch 62210, train_perplexity=1143.1108, train_loss=7.0415087

Batch 62220, train_perplexity=1084.5942, train_loss=6.988961

Batch 62230, train_perplexity=1214.9381, train_loss=7.1024485

Batch 62240, train_perplexity=1182.5106, train_loss=7.075395

Batch 62250, train_perplexity=1115.2015, train_loss=7.0167904

Batch 62260, train_perplexity=1162.8992, train_loss=7.0586715

Batch 62270, train_perplexity=1141.4409, train_loss=7.0400467

Batch 62280, train_perplexity=1262.9883, train_loss=7.141236

Batch 62290, train_perplexity=1186.5417, train_loss=7.0787983

Batch 62300, train_perplexity=1193.5989, train_loss=7.0847282

Batch 62310, train_perplexity=1152.3126, train_loss=7.049526

Batch 62320, train_perplexity=1152.4424, train_loss=7.0496387

Batch 62330, train_perplexity=1213.2616, train_loss=7.1010675

Batch 62340, train_perplexity=1310.042, train_loss=7.1778145

Batch 62350, train_perplexity=1111.8267, train_loss=7.0137596

Batch 62360, train_perplexity=1280.9562, train_loss=7.155362

Batch 62370, train_perplexity=1177.81, train_loss=7.071412

Batch 62380, train_perplexity=1113.6652, train_loss=7.015412

Batch 62390, train_perplexity=1286.3561, train_loss=7.159569

Batch 62400, train_perplexity=1211.5908, train_loss=7.0996895

Batch 62410, train_perplexity=1045.3031, train_loss=6.952062

Batch 62420, train_perplexity=1193.8766, train_loss=7.084961

Batch 62430, train_perplexity=1231.4114, train_loss=7.1159163

Batch 62440, train_perplexity=1169.1074, train_loss=7.063996

Batch 62450, train_perplexity=1292.8984, train_loss=7.164642

Batch 62460, train_perplexity=1146.5331, train_loss=7.044498

Batch 62470, train_perplexity=1176.2571, train_loss=7.0700927

Batch 62480, train_perplexity=1183.5604, train_loss=7.0762825

Batch 62490, train_perplexity=1151.3241, train_loss=7.048668

Batch 62500, train_perplexity=1158.7914, train_loss=7.055133

Batch 62510, train_perplexity=1017.8312, train_loss=6.9254293

Batch 62520, train_perplexity=1292.5502, train_loss=7.1643724

Batch 62530, train_perplexity=1225.3689, train_loss=7.110997

Batch 62540, train_perplexity=1157.4153, train_loss=7.0539446

Batch 62550, train_perplexity=1134.2255, train_loss=7.033705

Batch 62560, train_perplexity=1203.8956, train_loss=7.093318

Batch 62570, train_perplexity=1231.547, train_loss=7.1160264

Batch 62580, train_perplexity=1186.5497, train_loss=7.078805

Batch 62590, train_perplexity=1187.1609, train_loss=7.07932

Batch 62600, train_perplexity=1220.2239, train_loss=7.1067896

Batch 62610, train_perplexity=1187.0771, train_loss=7.0792494

Batch 62620, train_perplexity=1138.6412, train_loss=7.037591

Batch 62630, train_perplexity=1189.6033, train_loss=7.081375

Batch 62640, train_perplexity=1226.9293, train_loss=7.11227

Batch 62650, train_perplexity=1178.5094, train_loss=7.0720057

Batch 62660, train_perplexity=1192.6168, train_loss=7.083905

Batch 62670, train_perplexity=1154.9222, train_loss=7.0517883

Batch 62680, train_perplexity=1297.6158, train_loss=7.168284

Batch 62690, train_perplexity=1154.5417, train_loss=7.051459

Batch 62700, train_perplexity=1110.1124, train_loss=7.0122166

Batch 62710, train_perplexity=1141.7283, train_loss=7.0402985

Batch 62720, train_perplexity=1232.537, train_loss=7.11683

Batch 62730, train_perplexity=1094.5498, train_loss=6.9980984

Batch 62740, train_perplexity=1174.6216, train_loss=7.0687013

Batch 62750, train_perplexity=1049.7351, train_loss=6.956293

Batch 62760, train_perplexity=1233.9142, train_loss=7.1179466

Batch 62770, train_perplexity=1152.7529, train_loss=7.049908

Batch 62780, train_perplexity=1093.9362, train_loss=6.9975376

Batch 62790, train_perplexity=1185.66, train_loss=7.078055

Batch 62800, train_perplexity=1218.884, train_loss=7.105691

Batch 62810, train_perplexity=1106.4628, train_loss=7.0089235

Batch 62820, train_perplexity=1326.9945, train_loss=7.190672

Batch 62830, train_perplexity=1088.3285, train_loss=6.9923983

Batch 62840, train_perplexity=1194.2542, train_loss=7.085277

Batch 62850, train_perplexity=1149.1224, train_loss=7.046754

Batch 62860, train_perplexity=1184.8434, train_loss=7.077366

Batch 62870, train_perplexity=1154.9487, train_loss=7.051811

Batch 62880, train_perplexity=1232.5627, train_loss=7.116851

Batch 62890, train_perplexity=1155.461, train_loss=7.0522547

Batch 62900, train_perplexity=1150.3955, train_loss=7.047861

Batch 62910, train_perplexity=1194.3185, train_loss=7.085331

Batch 62920, train_perplexity=1047.3601, train_loss=6.954028

Batch 62930, train_perplexity=1140.7074, train_loss=7.039404

Batch 62940, train_perplexity=1025.286, train_loss=6.932727

Batch 62950, train_perplexity=1292.6624, train_loss=7.164459

Batch 62960, train_perplexity=1196.3932, train_loss=7.0870667

Batch 62970, train_perplexity=1187.1179, train_loss=7.0792837
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 62980, train_perplexity=1216.1165, train_loss=7.103418

Batch 62990, train_perplexity=1133.7987, train_loss=7.033329

Batch 63000, train_perplexity=1169.2764, train_loss=7.0641403

Batch 63010, train_perplexity=1296.9886, train_loss=7.1678004

Batch 63020, train_perplexity=1203.8796, train_loss=7.0933046

Batch 63030, train_perplexity=1219.8958, train_loss=7.1065207

Batch 63040, train_perplexity=1315.1367, train_loss=7.181696

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Batch 63050, train_perplexity=1335.2976, train_loss=7.1969094

Batch 63060, train_perplexity=1155.5867, train_loss=7.0523634

Batch 63070, train_perplexity=1274.3163, train_loss=7.150165

Batch 63080, train_perplexity=1284.7114, train_loss=7.1582894

Batch 63090, train_perplexity=1249.1421, train_loss=7.1302123

Batch 63100, train_perplexity=1223.1692, train_loss=7.1092005

Batch 63110, train_perplexity=1199.7924, train_loss=7.089904

Batch 63120, train_perplexity=1171.9043, train_loss=7.0663853

Batch 63130, train_perplexity=1221.3217, train_loss=7.107689

Batch 63140, train_perplexity=1232.5122, train_loss=7.11681

Batch 63150, train_perplexity=1181.7992, train_loss=7.0747933

Batch 63160, train_perplexity=1344.9865, train_loss=7.204139

Batch 63170, train_perplexity=1260.5546, train_loss=7.139307

Batch 63180, train_perplexity=1222.8193, train_loss=7.1089144

Batch 63190, train_perplexity=1170.6599, train_loss=7.065323

Batch 63200, train_perplexity=1258.0038, train_loss=7.1372814

Batch 63210, train_perplexity=1075.3418, train_loss=6.980394

Batch 63220, train_perplexity=1187.1643, train_loss=7.079323

Batch 63230, train_perplexity=1142.4548, train_loss=7.0409346

Batch 63240, train_perplexity=1238.0337, train_loss=7.1212797

Batch 63250, train_perplexity=1109.7059, train_loss=7.0118504

Batch 63260, train_perplexity=1236.4974, train_loss=7.120038

Batch 63270, train_perplexity=1093.8939, train_loss=6.997499

Batch 63280, train_perplexity=1210.7828, train_loss=7.0990224

Batch 63290, train_perplexity=1192.6333, train_loss=7.083919

Batch 63300, train_perplexity=1229.7777, train_loss=7.1145887

Batch 63310, train_perplexity=1220.6859, train_loss=7.107168

Batch 63320, train_perplexity=1158.482, train_loss=7.054866

Batch 63330, train_perplexity=1065.5199, train_loss=6.971218

Batch 63340, train_perplexity=1205.4651, train_loss=7.0946207

Batch 63350, train_perplexity=1151.6595, train_loss=7.0489593

Batch 63360, train_perplexity=1193.8265, train_loss=7.084919

Batch 63370, train_perplexity=1071.6394, train_loss=6.976945

Batch 63380, train_perplexity=1095.615, train_loss=6.999071

Batch 63390, train_perplexity=1218.4377, train_loss=7.1053247

Batch 63400, train_perplexity=1176.7008, train_loss=7.07047

Batch 63410, train_perplexity=1159.5968, train_loss=7.0558276

Batch 63420, train_perplexity=1154.3177, train_loss=7.051265

Batch 63430, train_perplexity=1171.883, train_loss=7.066367

Batch 63440, train_perplexity=1213.3588, train_loss=7.1011477

Batch 63450, train_perplexity=1167.6879, train_loss=7.062781

Batch 63460, train_perplexity=1090.9149, train_loss=6.994772

Batch 63470, train_perplexity=1095.1057, train_loss=6.998606

Batch 63480, train_perplexity=1276.0006, train_loss=7.151486

Batch 63490, train_perplexity=1191.8534, train_loss=7.083265

Batch 63500, train_perplexity=1241.0127, train_loss=7.123683

Batch 63510, train_perplexity=1132.8309, train_loss=7.032475

Batch 63520, train_perplexity=1102.2847, train_loss=7.0051403

Batch 63530, train_perplexity=1177.2878, train_loss=7.0709686

Batch 63540, train_perplexity=1146.1292, train_loss=7.0441456

Batch 63550, train_perplexity=1150.3007, train_loss=7.0477786

Batch 63560, train_perplexity=1157.012, train_loss=7.053596

Batch 63570, train_perplexity=1209.7838, train_loss=7.098197

Batch 63580, train_perplexity=1115.3419, train_loss=7.0169163

Batch 63590, train_perplexity=1151.7606, train_loss=7.049047

Batch 63600, train_perplexity=1172.2731, train_loss=7.0667

Batch 63610, train_perplexity=1257.3981, train_loss=7.1368

Batch 63620, train_perplexity=1207.4325, train_loss=7.0962515

Batch 63630, train_perplexity=1156.9899, train_loss=7.053577

Batch 63640, train_perplexity=1178.4207, train_loss=7.0719304

Batch 63650, train_perplexity=1192.162, train_loss=7.0835238

Batch 63660, train_perplexity=1148.0874, train_loss=7.0458527

Batch 63670, train_perplexity=1129.2019, train_loss=7.0292664

Batch 63680, train_perplexity=1217.5253, train_loss=7.1045756

Batch 63690, train_perplexity=1185.1722, train_loss=7.0776434

Batch 63700, train_perplexity=1268.9518, train_loss=7.1459465

Batch 63710, train_perplexity=1138.6609, train_loss=7.037608

Batch 63720, train_perplexity=1101.401, train_loss=7.0043383

Batch 63730, train_perplexity=1229.1832, train_loss=7.114105

Batch 63740, train_perplexity=1198.7465, train_loss=7.0890317

Batch 63750, train_perplexity=1152.4143, train_loss=7.0496144

Batch 63760, train_perplexity=1193.5994, train_loss=7.0847287

Batch 63770, train_perplexity=1061.713, train_loss=6.967639

Batch 63780, train_perplexity=1141.1312, train_loss=7.0397754

Batch 63790, train_perplexity=1184.3367, train_loss=7.076938

Batch 63800, train_perplexity=1250.4586, train_loss=7.1312656

Batch 63810, train_perplexity=1188.8071, train_loss=7.0807056

Batch 63820, train_perplexity=1106.5118, train_loss=7.008968

Batch 63830, train_perplexity=1222.6963, train_loss=7.108814

Batch 63840, train_perplexity=1165.4989, train_loss=7.0609045

Batch 63850, train_perplexity=1130.2815, train_loss=7.030222

Batch 63860, train_perplexity=1188.0353, train_loss=7.080056

Batch 63870, train_perplexity=1199.3519, train_loss=7.0895367

Batch 63880, train_perplexity=1207.1637, train_loss=7.096029

Batch 63890, train_perplexity=1286.1083, train_loss=7.159376

Batch 63900, train_perplexity=1241.4814, train_loss=7.1240606

Batch 63910, train_perplexity=1219.9335, train_loss=7.1065516

Batch 63920, train_perplexity=1200.9469, train_loss=7.0908656

Batch 63930, train_perplexity=1113.4279, train_loss=7.0151987

Batch 63940, train_perplexity=1197.248, train_loss=7.087781

Batch 63950, train_perplexity=1198.1265, train_loss=7.0885143

Batch 63960, train_perplexity=1233.5741, train_loss=7.117671

Batch 63970, train_perplexity=1238.6195, train_loss=7.1217527

Batch 63980, train_perplexity=1184.6005, train_loss=7.077161

Batch 63990, train_perplexity=1197.5826, train_loss=7.0880604

Batch 64000, train_perplexity=1210.7921, train_loss=7.09903

Batch 64010, train_perplexity=1143.419, train_loss=7.041778

Batch 64020, train_perplexity=1228.2036, train_loss=7.113308

Batch 64030, train_perplexity=1199.2136, train_loss=7.0894213

Batch 64040, train_perplexity=1373.7714, train_loss=7.225315

Batch 64050, train_perplexity=1227.5222, train_loss=7.112753

Batch 64060, train_perplexity=1191.6448, train_loss=7.08309

Batch 64070, train_perplexity=1171.6361, train_loss=7.0661564

Batch 64080, train_perplexity=1275.6532, train_loss=7.1512136

Batch 64090, train_perplexity=1133.3414, train_loss=7.0329256

Batch 64100, train_perplexity=1219.3153, train_loss=7.106045

Batch 64110, train_perplexity=1195.9968, train_loss=7.0867352

Batch 64120, train_perplexity=1197.9156, train_loss=7.0883384

Batch 64130, train_perplexity=1232.1292, train_loss=7.116499

Batch 64140, train_perplexity=1129.7738, train_loss=7.0297728

Batch 64150, train_perplexity=1167.9457, train_loss=7.0630016

Batch 64160, train_perplexity=1299.66, train_loss=7.169858

Batch 64170, train_perplexity=1244.1909, train_loss=7.1262407

Batch 64180, train_perplexity=1213.375, train_loss=7.101161

Batch 64190, train_perplexity=1149.3932, train_loss=7.0469894

Batch 64200, train_perplexity=1191.7584, train_loss=7.083185

Batch 64210, train_perplexity=1204.995, train_loss=7.0942307

Batch 64220, train_perplexity=1138.8807, train_loss=7.0378013

Batch 64230, train_perplexity=1291.6864, train_loss=7.163704
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 64240, train_perplexity=1251.9991, train_loss=7.132497

Batch 64250, train_perplexity=1244.6852, train_loss=7.126638

Batch 64260, train_perplexity=1093.5336, train_loss=6.9971695

Batch 64270, train_perplexity=1290.6582, train_loss=7.1629076

Batch 64280, train_perplexity=1224.4016, train_loss=7.1102076

Batch 64290, train_perplexity=1160.4801, train_loss=7.056589

Batch 64300, train_perplexity=1136.0273, train_loss=7.0352926

Batch 64310, train_perplexity=1152.6985, train_loss=7.049861

Batch 64320, train_perplexity=1125.3177, train_loss=7.0258207

Batch 64330, train_perplexity=1039.7761, train_loss=6.9467607

Batch 64340, train_perplexity=1130.2906, train_loss=7.03023

Batch 64350, train_perplexity=1232.5099, train_loss=7.116808

Batch 64360, train_perplexity=1176.4034, train_loss=7.070217

Batch 64370, train_perplexity=1233.9265, train_loss=7.1179566

Batch 64380, train_perplexity=1078.6903, train_loss=6.983503

Batch 64390, train_perplexity=1210.7811, train_loss=7.099021

Batch 64400, train_perplexity=1259.0204, train_loss=7.138089

Batch 64410, train_perplexity=1274.6123, train_loss=7.1503973

Batch 64420, train_perplexity=1145.3784, train_loss=7.0434904

Batch 64430, train_perplexity=1188.9261, train_loss=7.080806

Batch 64440, train_perplexity=1233.3917, train_loss=7.117523

Batch 64450, train_perplexity=1190.8121, train_loss=7.082391

Batch 64460, train_perplexity=1191.0359, train_loss=7.0825787

Batch 64470, train_perplexity=1233.4047, train_loss=7.1175337

Batch 64480, train_perplexity=1145.2747, train_loss=7.0434

Batch 64490, train_perplexity=1189.4303, train_loss=7.0812297

Batch 64500, train_perplexity=1111.144, train_loss=7.0131454

Batch 64510, train_perplexity=1111.3931, train_loss=7.0133696

Batch 64520, train_perplexity=1095.146, train_loss=6.998643

Batch 64530, train_perplexity=1122.6626, train_loss=7.0234585

Batch 64540, train_perplexity=1152.8513, train_loss=7.0499935

Batch 64550, train_perplexity=1282.6432, train_loss=7.156678

Batch 64560, train_perplexity=1171.331, train_loss=7.065896

Batch 64570, train_perplexity=1162.116, train_loss=7.0579977

Batch 64580, train_perplexity=1218.4406, train_loss=7.105327

Batch 64590, train_perplexity=1184.0651, train_loss=7.076709

Batch 64600, train_perplexity=1244.2372, train_loss=7.126278

Batch 64610, train_perplexity=1135.7705, train_loss=7.0350666

Batch 64620, train_perplexity=1177.6893, train_loss=7.0713096

Batch 64630, train_perplexity=1116.6819, train_loss=7.018117

Batch 64640, train_perplexity=1255.1636, train_loss=7.135021

Batch 64650, train_perplexity=1256.4774, train_loss=7.1360674

Batch 64660, train_perplexity=1205.0065, train_loss=7.09424

Batch 64670, train_perplexity=1248.4818, train_loss=7.1296835

Batch 64680, train_perplexity=1297.1538, train_loss=7.1679277

Batch 64690, train_perplexity=1323.8219, train_loss=7.188278

Batch 64700, train_perplexity=1214.4377, train_loss=7.1020365

Batch 64710, train_perplexity=1191.3022, train_loss=7.0828023

Batch 64720, train_perplexity=1194.7246, train_loss=7.085671

Batch 64730, train_perplexity=1124.1652, train_loss=7.024796

Batch 64740, train_perplexity=1211.4984, train_loss=7.099613

Batch 64750, train_perplexity=1183.0013, train_loss=7.07581

Batch 64760, train_perplexity=1162.8998, train_loss=7.058672

Batch 64770, train_perplexity=1240.9215, train_loss=7.1236095

Batch 64780, train_perplexity=1238.3667, train_loss=7.1215487

Batch 64790, train_perplexity=1152.5583, train_loss=7.0497394

Batch 64800, train_perplexity=1218.2645, train_loss=7.1051826

Batch 64810, train_perplexity=1045.8002, train_loss=6.9525375

Batch 64820, train_perplexity=1258.2576, train_loss=7.137483

Batch 64830, train_perplexity=1119.641, train_loss=7.0207634

Batch 64840, train_perplexity=1205.1385, train_loss=7.09435

Batch 64850, train_perplexity=1307.8774, train_loss=7.176161

Batch 64860, train_perplexity=1143.0629, train_loss=7.0414667

Batch 64870, train_perplexity=1196.1571, train_loss=7.0868692

Batch 64880, train_perplexity=1192.241, train_loss=7.08359

Batch 64890, train_perplexity=1148.7478, train_loss=7.0464277

Batch 64900, train_perplexity=1136.5311, train_loss=7.035736

Batch 64910, train_perplexity=1191.8147, train_loss=7.0832324

Batch 64920, train_perplexity=1304.1608, train_loss=7.173315

Batch 64930, train_perplexity=1212.7468, train_loss=7.100643

Batch 64940, train_perplexity=1091.7131, train_loss=6.9955034

Batch 64950, train_perplexity=1171.5121, train_loss=7.0660505

Batch 64960, train_perplexity=1259.3097, train_loss=7.138319

Batch 64970, train_perplexity=1258.4778, train_loss=7.137658

Batch 64980, train_perplexity=1180.4155, train_loss=7.0736217

Batch 64990, train_perplexity=1160.5056, train_loss=7.056611

Batch 65000, train_perplexity=1402.638, train_loss=7.24611

Batch 65010, train_perplexity=1280.6716, train_loss=7.15514

Batch 65020, train_perplexity=1138.916, train_loss=7.0378323

Batch 65030, train_perplexity=1098.8315, train_loss=7.0020027

Batch 65040, train_perplexity=1215.6208, train_loss=7.10301

Batch 65050, train_perplexity=1132.421, train_loss=7.032113

Batch 65060, train_perplexity=1184.1385, train_loss=7.076771

Batch 65070, train_perplexity=1100.6891, train_loss=7.0036917

Batch 65080, train_perplexity=1251.7579, train_loss=7.132304

Batch 65090, train_perplexity=1173.16, train_loss=7.0674562

Batch 65100, train_perplexity=1108.0785, train_loss=7.0103827

Batch 65110, train_perplexity=1207.5713, train_loss=7.0963664

Batch 65120, train_perplexity=1203.4457, train_loss=7.092944

Batch 65130, train_perplexity=1284.1548, train_loss=7.157856

Batch 65140, train_perplexity=1270.5054, train_loss=7.14717

Batch 65150, train_perplexity=1240.2098, train_loss=7.123036

Batch 65160, train_perplexity=1199.9205, train_loss=7.0900106

Batch 65170, train_perplexity=1195.9694, train_loss=7.0867124

Batch 65180, train_perplexity=1187.106, train_loss=7.0792737

Batch 65190, train_perplexity=1268.4086, train_loss=7.1455183

Batch 65200, train_perplexity=1207.7872, train_loss=7.096545

Batch 65210, train_perplexity=1207.926, train_loss=7.09666

Batch 65220, train_perplexity=1137.7523, train_loss=7.03681

Batch 65230, train_perplexity=1168.7847, train_loss=7.0637197

Batch 65240, train_perplexity=1080.1566, train_loss=6.9848614

Batch 65250, train_perplexity=1260.8142, train_loss=7.139513

Batch 65260, train_perplexity=1066.7949, train_loss=6.972414

Batch 65270, train_perplexity=1276.583, train_loss=7.1519423

Batch 65280, train_perplexity=1126.0638, train_loss=7.0264835

Batch 65290, train_perplexity=1304.0701, train_loss=7.1732454

Batch 65300, train_perplexity=1175.2249, train_loss=7.069215

Batch 65310, train_perplexity=1151.4108, train_loss=7.0487432

Batch 65320, train_perplexity=1185.7913, train_loss=7.0781655

Batch 65330, train_perplexity=1297.197, train_loss=7.167961

Batch 65340, train_perplexity=1277.2455, train_loss=7.152461

Batch 65350, train_perplexity=1131.404, train_loss=7.0312147

Batch 65360, train_perplexity=1198.2053, train_loss=7.08858

Batch 65370, train_perplexity=1224.1144, train_loss=7.109973

Batch 65380, train_perplexity=1229.5309, train_loss=7.114388

Batch 65390, train_perplexity=1247.8295, train_loss=7.129161

Batch 65400, train_perplexity=1174.5807, train_loss=7.0686665

Batch 65410, train_perplexity=1166.1393, train_loss=7.061454

Batch 65420, train_perplexity=1239.9834, train_loss=7.1228533

Batch 65430, train_perplexity=1136.5464, train_loss=7.0357494

Batch 65440, train_perplexity=1123.6326, train_loss=7.024322

Batch 65450, train_perplexity=1191.488, train_loss=7.082958

Batch 65460, train_perplexity=1119.4178, train_loss=7.020564

Batch 65470, train_perplexity=1162.8154, train_loss=7.0585995

Batch 65480, train_perplexity=1102.0067, train_loss=7.004888

Batch 65490, train_perplexity=1148.5933, train_loss=7.0462933

Batch 65500, train_perplexity=1208.1478, train_loss=7.0968437

Batch 65510, train_perplexity=1283.0176, train_loss=7.15697

Batch 65520, train_perplexity=1222.7563, train_loss=7.108863

Batch 65530, train_perplexity=1240.4021, train_loss=7.123191

Batch 65540, train_perplexity=1230.9276, train_loss=7.1155233

Batch 65550, train_perplexity=1107.2429, train_loss=7.0096283
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65560, train_perplexity=1150.4679, train_loss=7.047924

Batch 65570, train_perplexity=1160.5515, train_loss=7.0566506

Batch 65580, train_perplexity=1212.2767, train_loss=7.1002555

Batch 65590, train_perplexity=1222.3237, train_loss=7.108509

Batch 65600, train_perplexity=1208.4001, train_loss=7.0970526

Batch 65610, train_perplexity=1270.4473, train_loss=7.1471243

Batch 65620, train_perplexity=1101.6741, train_loss=7.004586

Batch 65630, train_perplexity=1196.6979, train_loss=7.0873213

Batch 65640, train_perplexity=1084.8414, train_loss=6.989189

Batch 65650, train_perplexity=1209.4896, train_loss=7.097954

Batch 65660, train_perplexity=1138.3992, train_loss=7.0373783

Batch 65670, train_perplexity=1212.2577, train_loss=7.1002398

Batch 65680, train_perplexity=1039.5693, train_loss=6.946562

Batch 65690, train_perplexity=1267.2236, train_loss=7.1445837

Batch 65700, train_perplexity=1194.6204, train_loss=7.0855837

Batch 65710, train_perplexity=1049.9874, train_loss=6.9565334

Batch 65720, train_perplexity=1157.7211, train_loss=7.0542088

Batch 65730, train_perplexity=1062.5452, train_loss=6.9684224

Batch 65740, train_perplexity=1213.1436, train_loss=7.1009703

Batch 65750, train_perplexity=1202.0417, train_loss=7.091777

Batch 65760, train_perplexity=1153.6101, train_loss=7.0506516

Batch 65770, train_perplexity=1099.2451, train_loss=7.002379

Batch 65780, train_perplexity=1100.1554, train_loss=7.0032067

Batch 65790, train_perplexity=1152.1116, train_loss=7.0493517

Batch 65800, train_perplexity=1188.5872, train_loss=7.0805206

Batch 65810, train_perplexity=1135.148, train_loss=7.0345182

Batch 65820, train_perplexity=1171.74, train_loss=7.066245

Batch 65830, train_perplexity=1192.4838, train_loss=7.0837936

Batch 65840, train_perplexity=1124.0602, train_loss=7.0247025

Batch 65850, train_perplexity=1130.8324, train_loss=7.0307093

Batch 65860, train_perplexity=1201.4556, train_loss=7.091289

Batch 65870, train_perplexity=1204.3252, train_loss=7.0936747

Batch 65880, train_perplexity=1193.7047, train_loss=7.084817

Batch 65890, train_perplexity=1167.822, train_loss=7.062896

Batch 65900, train_perplexity=1125.7262, train_loss=7.0261836

Batch 65910, train_perplexity=1346.1979, train_loss=7.2050395

Batch 65920, train_perplexity=1171.6277, train_loss=7.066149

Batch 65930, train_perplexity=1187.2306, train_loss=7.0793786

Batch 65940, train_perplexity=1190.5781, train_loss=7.0821943

Batch 65950, train_perplexity=1161.5708, train_loss=7.0575285

Batch 65960, train_perplexity=1075.529, train_loss=6.980568

Batch 65970, train_perplexity=1141.8796, train_loss=7.040431

Batch 65980, train_perplexity=1176.7153, train_loss=7.0704823

Batch 65990, train_perplexity=1242.8461, train_loss=7.1251593

Batch 66000, train_perplexity=1080.3895, train_loss=6.985077

Batch 66010, train_perplexity=1185.4209, train_loss=7.077853

Batch 66020, train_perplexity=1253.1572, train_loss=7.1334214

Batch 66030, train_perplexity=1238.6041, train_loss=7.1217403

Batch 66040, train_perplexity=1191.8438, train_loss=7.0832567

Batch 66050, train_perplexity=1239.0383, train_loss=7.122091

Batch 66060, train_perplexity=1219.0131, train_loss=7.105797

Batch 66070, train_perplexity=1215.2163, train_loss=7.1026773

Batch 66080, train_perplexity=1196.8942, train_loss=7.0874853

Batch 66090, train_perplexity=1238.9148, train_loss=7.121991

Batch 66100, train_perplexity=1059.8673, train_loss=6.965899

Batch 66110, train_perplexity=1251.2686, train_loss=7.131913

Batch 66120, train_perplexity=1089.5481, train_loss=6.9935184

Batch 66130, train_perplexity=1169.5378, train_loss=7.064364

Batch 66140, train_perplexity=1178.537, train_loss=7.072029

Batch 66150, train_perplexity=1124.1539, train_loss=7.024786

Batch 66160, train_perplexity=1256.3445, train_loss=7.1359615

Batch 66170, train_perplexity=1217.7395, train_loss=7.1047516

Batch 66180, train_perplexity=1174.1141, train_loss=7.0682693

Batch 66190, train_perplexity=1155.0033, train_loss=7.0518584

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Batch 66200, train_perplexity=1251.4524, train_loss=7.13206

Batch 66210, train_perplexity=1133.3939, train_loss=7.032972

Batch 66220, train_perplexity=1223.2211, train_loss=7.109243

Batch 66230, train_perplexity=1183.4419, train_loss=7.0761824

Batch 66240, train_perplexity=1184.6587, train_loss=7.07721

Batch 66250, train_perplexity=1101.2108, train_loss=7.0041656

Batch 66260, train_perplexity=1084.6211, train_loss=6.988986

Batch 66270, train_perplexity=1210.414, train_loss=7.0987177

Batch 66280, train_perplexity=1265.1129, train_loss=7.1429167

Batch 66290, train_perplexity=1271.4242, train_loss=7.147893

Batch 66300, train_perplexity=1079.2695, train_loss=6.98404

Batch 66310, train_perplexity=1212.8538, train_loss=7.1007314

Batch 66320, train_perplexity=1113.464, train_loss=7.015231

Batch 66330, train_perplexity=1127.7452, train_loss=7.0279756

Batch 66340, train_perplexity=1129.72, train_loss=7.029725

Batch 66350, train_perplexity=1301.7614, train_loss=7.1714735

Batch 66360, train_perplexity=1176.8787, train_loss=7.070621

Batch 66370, train_perplexity=1161.3282, train_loss=7.0573196

Batch 66380, train_perplexity=1165.719, train_loss=7.0610933

Batch 66390, train_perplexity=1151.7837, train_loss=7.049067

Batch 66400, train_perplexity=1213.1841, train_loss=7.1010036

Batch 66410, train_perplexity=1172.6952, train_loss=7.06706

Batch 66420, train_perplexity=1164.7256, train_loss=7.0602407

Batch 66430, train_perplexity=1113.1709, train_loss=7.014968

Batch 66440, train_perplexity=1133.8842, train_loss=7.0334044

Batch 66450, train_perplexity=1160.6971, train_loss=7.056776

Batch 66460, train_perplexity=1268.4388, train_loss=7.145542

Batch 66470, train_perplexity=1178.1785, train_loss=7.071725

Batch 66480, train_perplexity=1172.4738, train_loss=7.066871

Batch 66490, train_perplexity=1179.643, train_loss=7.072967

Batch 66500, train_perplexity=1176.7188, train_loss=7.070485

Batch 66510, train_perplexity=1235.6276, train_loss=7.119334

Batch 66520, train_perplexity=1080.2113, train_loss=6.984912

Batch 66530, train_perplexity=1289.1433, train_loss=7.161733

Batch 66540, train_perplexity=1092.2854, train_loss=6.9960275

Batch 66550, train_perplexity=1190.5361, train_loss=7.082159

Batch 66560, train_perplexity=1204.5089, train_loss=7.0938272

Batch 66570, train_perplexity=1286.3064, train_loss=7.15953

Batch 66580, train_perplexity=1150.7094, train_loss=7.048134

Batch 66590, train_perplexity=1193.411, train_loss=7.084571

Batch 66600, train_perplexity=1082.1445, train_loss=6.9867

Batch 66610, train_perplexity=1228.424, train_loss=7.1134872

Batch 66620, train_perplexity=1214.6764, train_loss=7.102233

Batch 66630, train_perplexity=1099.8224, train_loss=7.002904

Batch 66640, train_perplexity=1202.8755, train_loss=7.09247

Batch 66650, train_perplexity=1205.5973, train_loss=7.0947304

Batch 66660, train_perplexity=1081.8458, train_loss=6.986424

Batch 66670, train_perplexity=1219.5316, train_loss=7.106222

Batch 66680, train_perplexity=1282.9857, train_loss=7.156945

Batch 66690, train_perplexity=1169.8262, train_loss=7.0646105

Batch 66700, train_perplexity=1175.6969, train_loss=7.0696163

Batch 66710, train_perplexity=1097.7941, train_loss=7.001058

Batch 66720, train_perplexity=1115.3818, train_loss=7.016952

Batch 66730, train_perplexity=1174.9672, train_loss=7.0689955

Batch 66740, train_perplexity=1068.1774, train_loss=6.973709

Batch 66750, train_perplexity=1114.6316, train_loss=7.016279

Batch 66760, train_perplexity=1154.1615, train_loss=7.0511293

Batch 66770, train_perplexity=1108.013, train_loss=7.0103235

Batch 66780, train_perplexity=1089.5393, train_loss=6.9935102

Batch 66790, train_perplexity=1216.1658, train_loss=7.1034584

Batch 66800, train_perplexity=1151.0474, train_loss=7.0484276

Batch 66810, train_perplexity=1181.5823, train_loss=7.0746098
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66820, train_perplexity=1213.5989, train_loss=7.1013455

Batch 66830, train_perplexity=1278.7397, train_loss=7.1536303

Batch 66840, train_perplexity=1009.47705, train_loss=6.9171877

Batch 66850, train_perplexity=1165.9502, train_loss=7.0612917

Batch 66860, train_perplexity=1276.7911, train_loss=7.1521053

Batch 66870, train_perplexity=1116.5216, train_loss=7.0179734

Batch 66880, train_perplexity=1196.3853, train_loss=7.08706

Batch 66890, train_perplexity=1221.3776, train_loss=7.1077347

Batch 66900, train_perplexity=1242.0984, train_loss=7.1245575

Batch 66910, train_perplexity=1198.4196, train_loss=7.088759

Batch 66920, train_perplexity=1244.4259, train_loss=7.1264296

Batch 66930, train_perplexity=1226.2988, train_loss=7.111756

Batch 66940, train_perplexity=1113.5553, train_loss=7.015313

Batch 66950, train_perplexity=1260.3466, train_loss=7.139142

Batch 66960, train_perplexity=1178.8326, train_loss=7.07228

Batch 66970, train_perplexity=1231.1025, train_loss=7.1156654

Batch 66980, train_perplexity=1237.3905, train_loss=7.12076

Batch 66990, train_perplexity=1278.2837, train_loss=7.1532736

Batch 67000, train_perplexity=1094.3682, train_loss=6.9979324

Batch 67010, train_perplexity=1238.7246, train_loss=7.1218376

Batch 67020, train_perplexity=1198.1058, train_loss=7.088497

Batch 67030, train_perplexity=1347.3134, train_loss=7.205868

Batch 67040, train_perplexity=1174.9313, train_loss=7.068965

Batch 67050, train_perplexity=1099.472, train_loss=7.0025854

Batch 67060, train_perplexity=1109.0596, train_loss=7.0112677

Batch 67070, train_perplexity=1188.6512, train_loss=7.0805745

Batch 67080, train_perplexity=1167.9167, train_loss=7.062977

Batch 67090, train_perplexity=1148.0709, train_loss=7.0458384

Batch 67100, train_perplexity=1020.6058, train_loss=6.9281516

Batch 67110, train_perplexity=1101.4031, train_loss=7.00434

Batch 67120, train_perplexity=1147.5685, train_loss=7.0454006

Batch 67130, train_perplexity=1233.6488, train_loss=7.1177316

Batch 67140, train_perplexity=1200.4849, train_loss=7.090481

Batch 67150, train_perplexity=1326.2588, train_loss=7.1901174

Batch 67160, train_perplexity=1170.9116, train_loss=7.065538

Batch 67170, train_perplexity=1096.6055, train_loss=6.9999747

Batch 67180, train_perplexity=1091.8474, train_loss=6.9956264

Batch 67190, train_perplexity=1123.5505, train_loss=7.024249

Batch 67200, train_perplexity=1128.7206, train_loss=7.02884

Batch 67210, train_perplexity=1221.6193, train_loss=7.1079326

Batch 67220, train_perplexity=1157.1184, train_loss=7.053688

Batch 67230, train_perplexity=1120.868, train_loss=7.0218587

Batch 67240, train_perplexity=1167.0605, train_loss=7.0622435

Batch 67250, train_perplexity=1179.4331, train_loss=7.072789

Batch 67260, train_perplexity=1149.4874, train_loss=7.0470715

Batch 67270, train_perplexity=1097.3922, train_loss=7.000692

Batch 67280, train_perplexity=1115.4324, train_loss=7.0169973

Batch 67290, train_perplexity=1172.3413, train_loss=7.066758

Batch 67300, train_perplexity=1338.432, train_loss=7.199254

Batch 67310, train_perplexity=1127.5887, train_loss=7.027837

Batch 67320, train_perplexity=1296.2523, train_loss=7.1672325

Batch 67330, train_perplexity=1182.9883, train_loss=7.075799

Batch 67340, train_perplexity=1249.1154, train_loss=7.130191

Batch 67350, train_perplexity=1196.3778, train_loss=7.087054

Batch 67360, train_perplexity=1210.2292, train_loss=7.098565

Batch 67370, train_perplexity=1188.9307, train_loss=7.0808096

Batch 67380, train_perplexity=1245.1434, train_loss=7.127006

Batch 67390, train_perplexity=1185.1033, train_loss=7.077585

Batch 67400, train_perplexity=1273.4811, train_loss=7.1495094

Batch 67410, train_perplexity=1064.2352, train_loss=6.9700117

Batch 67420, train_perplexity=1080.8749, train_loss=6.985526

Batch 67430, train_perplexity=1132.0841, train_loss=7.0318155

Batch 67440, train_perplexity=1103.1428, train_loss=7.0059185

Batch 67450, train_perplexity=1260.5822, train_loss=7.139329

Batch 67460, train_perplexity=1171.8081, train_loss=7.0663033

Batch 67470, train_perplexity=984.68854, train_loss=6.8923254

Batch 67480, train_perplexity=1263.5165, train_loss=7.141654

Batch 67490, train_perplexity=1227.1616, train_loss=7.112459

Batch 67500, train_perplexity=1194.9781, train_loss=7.085883

Batch 67510, train_perplexity=1148.0359, train_loss=7.045808

Batch 67520, train_perplexity=1259.9392, train_loss=7.1388187

Batch 67530, train_perplexity=1186.4009, train_loss=7.0786796

Batch 67540, train_perplexity=1101.527, train_loss=7.0044527

Batch 67550, train_perplexity=1141.8623, train_loss=7.040416

Batch 67560, train_perplexity=1124.0275, train_loss=7.0246735

Batch 67570, train_perplexity=1313.4835, train_loss=7.180438

Batch 67580, train_perplexity=1092.4901, train_loss=6.996215

Batch 67590, train_perplexity=1109.8235, train_loss=7.011956

Batch 67600, train_perplexity=1177.0807, train_loss=7.0707927

Batch 67610, train_perplexity=1289.056, train_loss=7.1616654

Batch 67620, train_perplexity=1080.2009, train_loss=6.9849024

Batch 67630, train_perplexity=1192.1547, train_loss=7.0835176

Batch 67640, train_perplexity=1189.5073, train_loss=7.0812945

Batch 67650, train_perplexity=1095.2524, train_loss=6.99874

Batch 67660, train_perplexity=1195.5344, train_loss=7.0863485

Batch 67670, train_perplexity=1132.8833, train_loss=7.0325212

Batch 67680, train_perplexity=1197.8419, train_loss=7.088277

Batch 67690, train_perplexity=1274.8724, train_loss=7.1506014

Batch 67700, train_perplexity=1156.6914, train_loss=7.053319

Batch 67710, train_perplexity=1017.4124, train_loss=6.925018

Batch 67720, train_perplexity=1149.0469, train_loss=7.046688

Batch 67730, train_perplexity=1229.4623, train_loss=7.114332

Batch 67740, train_perplexity=1198.0985, train_loss=7.088491

Batch 67750, train_perplexity=1106.7383, train_loss=7.0091724

Batch 67760, train_perplexity=1204.7341, train_loss=7.094014

Batch 67770, train_perplexity=1308.2648, train_loss=7.176457

Batch 67780, train_perplexity=1214.2814, train_loss=7.1019077

Batch 67790, train_perplexity=1190.9552, train_loss=7.082511

Batch 67800, train_perplexity=1180.5123, train_loss=7.073704

Batch 67810, train_perplexity=1171.7355, train_loss=7.0662413

Batch 67820, train_perplexity=1133.7036, train_loss=7.033245

Batch 67830, train_perplexity=1155.6996, train_loss=7.052461

Batch 67840, train_perplexity=1152.5253, train_loss=7.0497108

Batch 67850, train_perplexity=1150.9152, train_loss=7.0483127

Batch 67860, train_perplexity=1280.9911, train_loss=7.1553893

Batch 67870, train_perplexity=1227.1523, train_loss=7.1124516

Batch 67880, train_perplexity=1200.3212, train_loss=7.0903444

Batch 67890, train_perplexity=1151.2373, train_loss=7.0485926

Batch 67900, train_perplexity=1269.4469, train_loss=7.1463366

Batch 67910, train_perplexity=1191.8232, train_loss=7.0832396

Batch 67920, train_perplexity=1125.1589, train_loss=7.0256796

Batch 67930, train_perplexity=1141.1307, train_loss=7.039775

Batch 67940, train_perplexity=1209.9979, train_loss=7.098374

Batch 67950, train_perplexity=1197.5176, train_loss=7.088006

Batch 67960, train_perplexity=1165.4645, train_loss=7.060875

Batch 67970, train_perplexity=1220.4548, train_loss=7.106979

Batch 67980, train_perplexity=1161.0237, train_loss=7.0570574

Batch 67990, train_perplexity=1194.2968, train_loss=7.085313

Batch 68000, train_perplexity=1086.751, train_loss=6.9909477

Batch 68010, train_perplexity=1183.056, train_loss=7.075856

Batch 68020, train_perplexity=1215.0448, train_loss=7.102536

Batch 68030, train_perplexity=973.2236, train_loss=6.880614

Batch 68040, train_perplexity=1234.4591, train_loss=7.118388

Batch 68050, train_perplexity=1290.6348, train_loss=7.1628895

Batch 68060, train_perplexity=1296.2504, train_loss=7.167231

Batch 68070, train_perplexity=1217.1863, train_loss=7.104297

Batch 68080, train_perplexity=1145.2474, train_loss=7.043376

Batch 68090, train_perplexity=1119.009, train_loss=7.020199

Batch 68100, train_perplexity=1080.9296, train_loss=6.9855766

Batch 68110, train_perplexity=1244.2028, train_loss=7.1262503

Batch 68120, train_perplexity=1076.3165, train_loss=6.9813

Batch 68130, train_perplexity=1172.0026, train_loss=7.066469
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 68140, train_perplexity=1231.2869, train_loss=7.115815

Batch 68150, train_perplexity=1124.0248, train_loss=7.024671

Batch 68160, train_perplexity=1130.0374, train_loss=7.030006

Batch 68170, train_perplexity=1129.5007, train_loss=7.029531

Batch 68180, train_perplexity=1112.6997, train_loss=7.0145445

Batch 68190, train_perplexity=1184.5648, train_loss=7.077131

Batch 68200, train_perplexity=1155.5988, train_loss=7.052374

Batch 68210, train_perplexity=1232.7109, train_loss=7.116971

Batch 68220, train_perplexity=1153.7916, train_loss=7.050809

Batch 68230, train_perplexity=1327.8256, train_loss=7.191298

Batch 68240, train_perplexity=1173.1818, train_loss=7.067475

Batch 68250, train_perplexity=1129.6467, train_loss=7.02966

Batch 68260, train_perplexity=1183.2874, train_loss=7.0760517

Batch 68270, train_perplexity=1259.377, train_loss=7.1383724

Batch 68280, train_perplexity=1037.5201, train_loss=6.9445887

Batch 68290, train_perplexity=1162.6053, train_loss=7.0584188

Batch 68300, train_perplexity=1258.5474, train_loss=7.1377134

Batch 68310, train_perplexity=1145.5883, train_loss=7.0436735

Batch 68320, train_perplexity=1113.4093, train_loss=7.015182

Batch 68330, train_perplexity=1202.3433, train_loss=7.0920277

Batch 68340, train_perplexity=1132.5225, train_loss=7.0322027

Batch 68350, train_perplexity=1293.6416, train_loss=7.1652164

Batch 68360, train_perplexity=1139.067, train_loss=7.037965

Batch 68370, train_perplexity=1191.238, train_loss=7.0827484

Batch 68380, train_perplexity=1146.0352, train_loss=7.0440636

Batch 68390, train_perplexity=1115.2133, train_loss=7.016801

Batch 68400, train_perplexity=1244.4639, train_loss=7.12646

Batch 68410, train_perplexity=1178.6393, train_loss=7.072116

Batch 68420, train_perplexity=1105.7598, train_loss=7.008288

Batch 68430, train_perplexity=1118.5769, train_loss=7.0198126

Batch 68440, train_perplexity=1095.1605, train_loss=6.9986563

Batch 68450, train_perplexity=1275.197, train_loss=7.150856

Batch 68460, train_perplexity=1123.8416, train_loss=7.024508

Batch 68470, train_perplexity=1201.6108, train_loss=7.0914183

Batch 68480, train_perplexity=1290.7653, train_loss=7.1629906

Batch 68490, train_perplexity=1152.6963, train_loss=7.049859

Batch 68500, train_perplexity=1258.4729, train_loss=7.1376543

Batch 68510, train_perplexity=1073.7222, train_loss=6.9788866

Batch 68520, train_perplexity=1234.7899, train_loss=7.118656

Batch 68530, train_perplexity=1061.7981, train_loss=6.967719

Batch 68540, train_perplexity=1198.1608, train_loss=7.088543

Batch 68550, train_perplexity=1221.419, train_loss=7.1077685

Batch 68560, train_perplexity=1263.9829, train_loss=7.142023

Batch 68570, train_perplexity=1114.1831, train_loss=7.015877

Batch 68580, train_perplexity=1145.0388, train_loss=7.043194

Batch 68590, train_perplexity=1076.8519, train_loss=6.981797

Batch 68600, train_perplexity=1148.2543, train_loss=7.045998

Batch 68610, train_perplexity=1208.441, train_loss=7.0970864

Batch 68620, train_perplexity=1172.091, train_loss=7.0665445

Batch 68630, train_perplexity=1068.4933, train_loss=6.9740047

Batch 68640, train_perplexity=1079.5182, train_loss=6.98427

Batch 68650, train_perplexity=1257.1804, train_loss=7.1366267

Batch 68660, train_perplexity=1129.5013, train_loss=7.0295315

Batch 68670, train_perplexity=1195.3383, train_loss=7.0861845

Batch 68680, train_perplexity=1164.7411, train_loss=7.060254

Batch 68690, train_perplexity=1078.8965, train_loss=6.983694

Batch 68700, train_perplexity=1148.7412, train_loss=7.046422

Batch 68710, train_perplexity=1105.3469, train_loss=7.0079145

Batch 68720, train_perplexity=1077.8403, train_loss=6.9827147

Batch 68730, train_perplexity=1231.7919, train_loss=7.1162252

Batch 68740, train_perplexity=1073.6962, train_loss=6.9788623

Batch 68750, train_perplexity=1245.7599, train_loss=7.127501

Batch 68760, train_perplexity=1268.5756, train_loss=7.14565

Batch 68770, train_perplexity=1143.1458, train_loss=7.041539

Batch 68780, train_perplexity=1084.0958, train_loss=6.9885015

Batch 68790, train_perplexity=1221.2657, train_loss=7.107643

Batch 68800, train_perplexity=1116.2448, train_loss=7.0177255

Batch 68810, train_perplexity=1143.9397, train_loss=7.0422335

Batch 68820, train_perplexity=1133.7437, train_loss=7.0332804

Batch 68830, train_perplexity=1155.1294, train_loss=7.0519676

Batch 68840, train_perplexity=1242.3004, train_loss=7.12472

Batch 68850, train_perplexity=1110.3666, train_loss=7.0124454

Batch 68860, train_perplexity=1147.8936, train_loss=7.045684

Batch 68870, train_perplexity=1150.4877, train_loss=7.047941

Batch 68880, train_perplexity=1240.8115, train_loss=7.123521

Batch 68890, train_perplexity=1150.6381, train_loss=7.048072

Batch 68900, train_perplexity=1259.7385, train_loss=7.1386595

Batch 68910, train_perplexity=1150.8503, train_loss=7.0482564

Batch 68920, train_perplexity=1100.1433, train_loss=7.003196

Batch 68930, train_perplexity=1251.8666, train_loss=7.132391

Batch 68940, train_perplexity=1057.9971, train_loss=6.964133

Batch 68950, train_perplexity=1164.0659, train_loss=7.0596743

Batch 68960, train_perplexity=1098.7844, train_loss=7.00196

Batch 68970, train_perplexity=1060.214, train_loss=6.966226

Batch 68980, train_perplexity=1053.8667, train_loss=6.9602213

Batch 68990, train_perplexity=1319.3483, train_loss=7.184893

Batch 69000, train_perplexity=1103.8317, train_loss=7.0065427

Batch 69010, train_perplexity=1118.4148, train_loss=7.0196676

Batch 69020, train_perplexity=1180.0969, train_loss=7.073352

Batch 69030, train_perplexity=1174.3392, train_loss=7.068461

Batch 69040, train_perplexity=1252.1198, train_loss=7.132593

Batch 69050, train_perplexity=1099.2299, train_loss=7.002365

Batch 69060, train_perplexity=1226.2942, train_loss=7.111752

Batch 69070, train_perplexity=1086.9645, train_loss=6.991144

Batch 69080, train_perplexity=1253.1196, train_loss=7.1333914

Batch 69090, train_perplexity=1190.7554, train_loss=7.082343

Batch 69100, train_perplexity=1115.503, train_loss=7.0170608

Batch 69110, train_perplexity=1181.802, train_loss=7.0747957

Batch 69120, train_perplexity=1106.4005, train_loss=7.0088673

Batch 69130, train_perplexity=1122.8693, train_loss=7.0236425

Batch 69140, train_perplexity=1077.233, train_loss=6.982151

Batch 69150, train_perplexity=1238.9716, train_loss=7.122037

Batch 69160, train_perplexity=1160.8881, train_loss=7.0569406

Batch 69170, train_perplexity=1109.2404, train_loss=7.0114307

Batch 69180, train_perplexity=1241.5999, train_loss=7.124156

Batch 69190, train_perplexity=1063.165, train_loss=6.9690056

Batch 69200, train_perplexity=1201.6194, train_loss=7.0914254

Batch 69210, train_perplexity=1175.4839, train_loss=7.069435

Batch 69220, train_perplexity=1169.0494, train_loss=7.0639462

Batch 69230, train_perplexity=1267.4327, train_loss=7.1447487

Batch 69240, train_perplexity=1208.0216, train_loss=7.0967393

Batch 69250, train_perplexity=1118.5471, train_loss=7.019786

Batch 69260, train_perplexity=1058.4673, train_loss=6.964577

Batch 69270, train_perplexity=1137.9763, train_loss=7.037007

Batch 69280, train_perplexity=1143.5492, train_loss=7.041892

Batch 69290, train_perplexity=1078.1138, train_loss=6.9829683

Batch 69300, train_perplexity=1128.0814, train_loss=7.0282736

Batch 69310, train_perplexity=1269.0383, train_loss=7.1460147

Batch 69320, train_perplexity=1171.549, train_loss=7.066082

Batch 69330, train_perplexity=1160.0851, train_loss=7.0562487

Batch 69340, train_perplexity=1210.967, train_loss=7.0991745

Batch 69350, train_perplexity=1062.1677, train_loss=6.968067

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Batch 69360, train_perplexity=1141.8845, train_loss=7.0404353

Batch 69370, train_perplexity=1130.8917, train_loss=7.0307617

Batch 69380, train_perplexity=1248.0073, train_loss=7.1293035

Batch 69390, train_perplexity=1197.0403, train_loss=7.0876074
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69400, train_perplexity=1141.294, train_loss=7.039918

Batch 69410, train_perplexity=1198.5555, train_loss=7.0888724

Batch 69420, train_perplexity=1143.9156, train_loss=7.0422125

Batch 69430, train_perplexity=1191.9711, train_loss=7.0833635

Batch 69440, train_perplexity=1183.0046, train_loss=7.075813

Batch 69450, train_perplexity=1127.0281, train_loss=7.0273395

Batch 69460, train_perplexity=1192.1637, train_loss=7.083525

Batch 69470, train_perplexity=1177.6606, train_loss=7.0712852

Batch 69480, train_perplexity=1101.8002, train_loss=7.0047007

Batch 69490, train_perplexity=1246.363, train_loss=7.127985

Batch 69500, train_perplexity=1127.077, train_loss=7.027383

Batch 69510, train_perplexity=1120.5372, train_loss=7.0215635

Batch 69520, train_perplexity=1084.8342, train_loss=6.9891825

Batch 69530, train_perplexity=1197.3235, train_loss=7.087844

Batch 69540, train_perplexity=1196.1223, train_loss=7.08684

Batch 69550, train_perplexity=1140.1163, train_loss=7.0388856

Batch 69560, train_perplexity=1227.5251, train_loss=7.1127553

Batch 69570, train_perplexity=1117.1931, train_loss=7.0185747

Batch 69580, train_perplexity=1136.4504, train_loss=7.035665

Batch 69590, train_perplexity=1141.7849, train_loss=7.040348

Batch 69600, train_perplexity=1197.0209, train_loss=7.087591

Batch 69610, train_perplexity=1200.9756, train_loss=7.0908895

Batch 69620, train_perplexity=1033.5457, train_loss=6.9407506

Batch 69630, train_perplexity=1188.8779, train_loss=7.0807652

Batch 69640, train_perplexity=1236.1497, train_loss=7.1197567

Batch 69650, train_perplexity=976.5524, train_loss=6.8840284

Batch 69660, train_perplexity=1225.6985, train_loss=7.111266

Batch 69670, train_perplexity=1091.2119, train_loss=6.995044

Batch 69680, train_perplexity=1216.631, train_loss=7.103841

Batch 69690, train_perplexity=1073.4847, train_loss=6.9786654

Batch 69700, train_perplexity=1066.265, train_loss=6.971917

Batch 69710, train_perplexity=1235.961, train_loss=7.119604

Batch 69720, train_perplexity=1111.0306, train_loss=7.0130434

Batch 69730, train_perplexity=1079.0612, train_loss=6.9838467

Batch 69740, train_perplexity=1149.0579, train_loss=7.0466976

Batch 69750, train_perplexity=1106.2244, train_loss=7.008708

Batch 69760, train_perplexity=1108.9601, train_loss=7.011178

Batch 69770, train_perplexity=1226.2573, train_loss=7.111722

Batch 69780, train_perplexity=1169.1459, train_loss=7.0640287

Batch 69790, train_perplexity=1203.1542, train_loss=7.092702

Batch 69800, train_perplexity=1256.3857, train_loss=7.1359944

Batch 69810, train_perplexity=1262.252, train_loss=7.1406527

Batch 69820, train_perplexity=1265.8008, train_loss=7.1434603

Batch 69830, train_perplexity=1130.695, train_loss=7.0305877

Batch 69840, train_perplexity=1119.8931, train_loss=7.0209885

Batch 69850, train_perplexity=1198.8888, train_loss=7.0891504

Batch 69860, train_perplexity=1232.531, train_loss=7.116825

Batch 69870, train_perplexity=1166.3695, train_loss=7.061651

Batch 69880, train_perplexity=1070.9927, train_loss=6.9763412

Batch 69890, train_perplexity=1078.9325, train_loss=6.9837275

Batch 69900, train_perplexity=1247.5438, train_loss=7.128932

Batch 69910, train_perplexity=1042.1005, train_loss=6.9489937

Batch 69920, train_perplexity=1098.8745, train_loss=7.002042

Batch 69930, train_perplexity=1102.3036, train_loss=7.0051575

Batch 69940, train_perplexity=1310.2762, train_loss=7.1779933

Batch 69950, train_perplexity=1209.8865, train_loss=7.098282

Batch 69960, train_perplexity=1140.2843, train_loss=7.039033

Batch 69970, train_perplexity=1130.4335, train_loss=7.0303564

Batch 69980, train_perplexity=1241.5122, train_loss=7.1240854

Batch 69990, train_perplexity=1301.1631, train_loss=7.171014

Batch 70000, train_perplexity=1190.1501, train_loss=7.081835

Batch 70010, train_perplexity=1031.7942, train_loss=6.9390545

Batch 70020, train_perplexity=1106.5873, train_loss=7.009036

Batch 70030, train_perplexity=1084.6791, train_loss=6.9890394

Batch 70040, train_perplexity=1078.0635, train_loss=6.9829216

Batch 70050, train_perplexity=1210.6714, train_loss=7.0989304

Batch 70060, train_perplexity=1173.9803, train_loss=7.0681553

Batch 70070, train_perplexity=1113.5192, train_loss=7.0152807

Batch 70080, train_perplexity=1277.9874, train_loss=7.153042

Batch 70090, train_perplexity=1094.1782, train_loss=6.997759

Batch 70100, train_perplexity=1253.1818, train_loss=7.133441

Batch 70110, train_perplexity=1145.2949, train_loss=7.0434175

Batch 70120, train_perplexity=1109.3129, train_loss=7.011496

Batch 70130, train_perplexity=1092.2744, train_loss=6.9960175

Batch 70140, train_perplexity=1104.127, train_loss=7.00681

Batch 70150, train_perplexity=1248.3604, train_loss=7.129586

Batch 70160, train_perplexity=1178.755, train_loss=7.072214

Batch 70170, train_perplexity=1109.4589, train_loss=7.0116277

Batch 70180, train_perplexity=1118.9365, train_loss=7.020134

Batch 70190, train_perplexity=1137.7474, train_loss=7.0368056

Batch 70200, train_perplexity=1042.0042, train_loss=6.948901

Batch 70210, train_perplexity=1142.5011, train_loss=7.040975

Batch 70220, train_perplexity=1112.9023, train_loss=7.0147266

Batch 70230, train_perplexity=1087.4528, train_loss=6.9915934

Batch 70240, train_perplexity=1152.0017, train_loss=7.0492563

Batch 70250, train_perplexity=1189.5272, train_loss=7.081311

Batch 70260, train_perplexity=1128.5823, train_loss=7.0287175

Batch 70270, train_perplexity=1276.4656, train_loss=7.15185

Batch 70280, train_perplexity=1101.5286, train_loss=7.004454

Batch 70290, train_perplexity=1212.0392, train_loss=7.1000595

Batch 70300, train_perplexity=1154.1003, train_loss=7.0510764

Batch 70310, train_perplexity=1162.0461, train_loss=7.0579376

Batch 70320, train_perplexity=990.47986, train_loss=6.8981895

Batch 70330, train_perplexity=1102.557, train_loss=7.0053873

Batch 70340, train_perplexity=1204.5193, train_loss=7.093836

Batch 70350, train_perplexity=1210.4827, train_loss=7.0987744

Batch 70360, train_perplexity=1098.9521, train_loss=7.0021124

Batch 70370, train_perplexity=1287.6577, train_loss=7.16058

Batch 70380, train_perplexity=1152.9733, train_loss=7.0500994

Batch 70390, train_perplexity=1189.6299, train_loss=7.0813975

Batch 70400, train_perplexity=1029.3872, train_loss=6.936719

Batch 70410, train_perplexity=1248.2966, train_loss=7.129535

Batch 70420, train_perplexity=1200.4563, train_loss=7.090457

Batch 70430, train_perplexity=1199.8187, train_loss=7.089926

Batch 70440, train_perplexity=1250.1641, train_loss=7.13103

Batch 70450, train_perplexity=1100.7836, train_loss=7.0037775

Batch 70460, train_perplexity=1146.9371, train_loss=7.0448503

Batch 70470, train_perplexity=1127.8501, train_loss=7.0280685

Batch 70480, train_perplexity=1182.1685, train_loss=7.0751057

Batch 70490, train_perplexity=1186.0654, train_loss=7.078397

Batch 70500, train_perplexity=1230.3889, train_loss=7.1150856

Batch 70510, train_perplexity=1060.2742, train_loss=6.966283

Batch 70520, train_perplexity=1216.4152, train_loss=7.1036634

Batch 70530, train_perplexity=1117.5049, train_loss=7.0188537

Batch 70540, train_perplexity=1166.1215, train_loss=7.0614386

Batch 70550, train_perplexity=1231.099, train_loss=7.1156626

Batch 70560, train_perplexity=1158.3478, train_loss=7.05475

Batch 70570, train_perplexity=1164.1903, train_loss=7.059781

Batch 70580, train_perplexity=1156.825, train_loss=7.0534344

Batch 70590, train_perplexity=1146.8273, train_loss=7.0447545

Batch 70600, train_perplexity=1294.0426, train_loss=7.1655264

Batch 70610, train_perplexity=1232.0199, train_loss=7.1164103

Batch 70620, train_perplexity=1167.1841, train_loss=7.0623493

Batch 70630, train_perplexity=1204.2178, train_loss=7.0935855

Batch 70640, train_perplexity=1215.0737, train_loss=7.10256

Batch 70650, train_perplexity=1160.2168, train_loss=7.056362

Batch 70660, train_perplexity=1282.1992, train_loss=7.156332

Batch 70670, train_perplexity=1195.6472, train_loss=7.086443

Batch 70680, train_perplexity=1132.5052, train_loss=7.0321875

Batch 70690, train_perplexity=1080.4833, train_loss=6.9851637

Batch 70700, train_perplexity=1123.6202, train_loss=7.024311

Batch 70710, train_perplexity=1159.322, train_loss=7.0555906
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70720, train_perplexity=1055.138, train_loss=6.9614267

Batch 70730, train_perplexity=1119.104, train_loss=7.0202837

Batch 70740, train_perplexity=1073.106, train_loss=6.9783125

Batch 70750, train_perplexity=1170.5773, train_loss=7.0652523

Batch 70760, train_perplexity=1133.4669, train_loss=7.033036

Batch 70770, train_perplexity=1173.7554, train_loss=7.0679636

Batch 70780, train_perplexity=1077.1519, train_loss=6.9820757

Batch 70790, train_perplexity=1075.0204, train_loss=6.980095

Batch 70800, train_perplexity=1116.9269, train_loss=7.0183363

Batch 70810, train_perplexity=1152.0369, train_loss=7.049287

Batch 70820, train_perplexity=1205.9261, train_loss=7.095003

Batch 70830, train_perplexity=1223.6266, train_loss=7.1095743

Batch 70840, train_perplexity=1142.1334, train_loss=7.040653

Batch 70850, train_perplexity=1060.0316, train_loss=6.966054

Batch 70860, train_perplexity=1234.7052, train_loss=7.1185875

Batch 70870, train_perplexity=1104.6283, train_loss=7.007264

Batch 70880, train_perplexity=1161.7559, train_loss=7.0576878

Batch 70890, train_perplexity=1154.0139, train_loss=7.0510015

Batch 70900, train_perplexity=1135.5533, train_loss=7.0348754

Batch 70910, train_perplexity=1269.7241, train_loss=7.146555

Batch 70920, train_perplexity=1150.9985, train_loss=7.048385

Batch 70930, train_perplexity=1220.4962, train_loss=7.1070127

Batch 70940, train_perplexity=1048.5044, train_loss=6.95512

Batch 70950, train_perplexity=1178.7528, train_loss=7.072212

Batch 70960, train_perplexity=1058.0338, train_loss=6.9641676

Batch 70970, train_perplexity=1093.7535, train_loss=6.9973707

Batch 70980, train_perplexity=999.37897, train_loss=6.907134

Batch 70990, train_perplexity=1218.2588, train_loss=7.105178

Batch 71000, train_perplexity=1225.6237, train_loss=7.111205

Batch 71010, train_perplexity=1213.2992, train_loss=7.1010985

Batch 71020, train_perplexity=1175.2002, train_loss=7.069194

Batch 71030, train_perplexity=1110.3596, train_loss=7.0124393

Batch 71040, train_perplexity=1069.0579, train_loss=6.974533

Batch 71050, train_perplexity=1127.6156, train_loss=7.0278606

Batch 71060, train_perplexity=1116.6765, train_loss=7.018112

Batch 71070, train_perplexity=1214.2896, train_loss=7.1019144

Batch 71080, train_perplexity=1139.5842, train_loss=7.038419

Batch 71090, train_perplexity=1094.7668, train_loss=6.9982967

Batch 71100, train_perplexity=1155.3221, train_loss=7.0521345

Batch 71110, train_perplexity=1137.927, train_loss=7.0369635

Batch 71120, train_perplexity=1180.1183, train_loss=7.07337

Batch 71130, train_perplexity=1170.651, train_loss=7.0653152

Batch 71140, train_perplexity=1129.7264, train_loss=7.029731

Batch 71150, train_perplexity=1133.0356, train_loss=7.0326557

Batch 71160, train_perplexity=1137.4572, train_loss=7.0365505

Batch 71170, train_perplexity=1176.5184, train_loss=7.070315

Batch 71180, train_perplexity=1094.1271, train_loss=6.997712

Batch 71190, train_perplexity=1088.9222, train_loss=6.992944

Batch 71200, train_perplexity=1120.7222, train_loss=7.0217285

Batch 71210, train_perplexity=1088.3907, train_loss=6.9924555

Batch 71220, train_perplexity=1117.3391, train_loss=7.0187054

Batch 71230, train_perplexity=1218.1868, train_loss=7.1051188

Batch 71240, train_perplexity=1163.5731, train_loss=7.059251

Batch 71250, train_perplexity=1201.1737, train_loss=7.0910544

Batch 71260, train_perplexity=1127.7855, train_loss=7.0280113

Batch 71270, train_perplexity=1178.8961, train_loss=7.072334

Batch 71280, train_perplexity=1321.1997, train_loss=7.1862955

Batch 71290, train_perplexity=1086.9883, train_loss=6.991166

Batch 71300, train_perplexity=1285.3469, train_loss=7.158784

Batch 71310, train_perplexity=1209.8022, train_loss=7.0982122

Batch 71320, train_perplexity=1168.6743, train_loss=7.0636253

Batch 71330, train_perplexity=1133.2706, train_loss=7.032863

Batch 71340, train_perplexity=1206.3759, train_loss=7.095376

Batch 71350, train_perplexity=1274.9781, train_loss=7.1506844

Batch 71360, train_perplexity=1023.5392, train_loss=6.9310217

Batch 71370, train_perplexity=1110.5127, train_loss=7.012577

Batch 71380, train_perplexity=1062.8431, train_loss=6.968703

Batch 71390, train_perplexity=1165.451, train_loss=7.0608635

Batch 71400, train_perplexity=1290.0023, train_loss=7.1623993

Batch 71410, train_perplexity=1151.021, train_loss=7.0484047

Batch 71420, train_perplexity=1197.1151, train_loss=7.08767

Batch 71430, train_perplexity=1090.6746, train_loss=6.9945517

Batch 71440, train_perplexity=1055.1188, train_loss=6.9614086

Batch 71450, train_perplexity=1077.5012, train_loss=6.9824

Batch 71460, train_perplexity=1130.1208, train_loss=7.03008

Batch 71470, train_perplexity=1192.4803, train_loss=7.083791

Batch 71480, train_perplexity=1155.6682, train_loss=7.052434

Batch 71490, train_perplexity=1273.5466, train_loss=7.149561

Batch 71500, train_perplexity=1082.8414, train_loss=6.987344

Batch 71510, train_perplexity=1161.0735, train_loss=7.0571003

Batch 71520, train_perplexity=1124.4509, train_loss=7.02505

Batch 71530, train_perplexity=1106.5552, train_loss=7.009007

Batch 71540, train_perplexity=1187.674, train_loss=7.079752

Batch 71550, train_perplexity=1151.3164, train_loss=7.048661

Batch 71560, train_perplexity=1200.2937, train_loss=7.0903215

Batch 71570, train_perplexity=1164.5923, train_loss=7.0601263

Batch 71580, train_perplexity=1124.6521, train_loss=7.025229

Batch 71590, train_perplexity=1085.0303, train_loss=6.989363

Batch 71600, train_perplexity=1215.5629, train_loss=7.1029625

Batch 71610, train_perplexity=1135.5875, train_loss=7.0349054

Batch 71620, train_perplexity=1202.8124, train_loss=7.0924177

Batch 71630, train_perplexity=1070.1514, train_loss=6.9755554

Batch 71640, train_perplexity=1324.7034, train_loss=7.188944

Batch 71650, train_perplexity=1252.6034, train_loss=7.1329794

Batch 71660, train_perplexity=1260.97, train_loss=7.1396365

Batch 71670, train_perplexity=1178.8613, train_loss=7.0723042

Batch 71680, train_perplexity=1074.8953, train_loss=6.9799786

Batch 71690, train_perplexity=1145.5718, train_loss=7.043659

Batch 71700, train_perplexity=1193.2415, train_loss=7.084429

Batch 71710, train_perplexity=1097.105, train_loss=7.00043

Batch 71720, train_perplexity=1132.0652, train_loss=7.031799

Batch 71730, train_perplexity=1225.8761, train_loss=7.111411

Batch 71740, train_perplexity=1209.6316, train_loss=7.098071

Batch 71750, train_perplexity=1107.3638, train_loss=7.0097375

Batch 71760, train_perplexity=1081.0708, train_loss=6.9857073

Batch 71770, train_perplexity=1058.383, train_loss=6.9644976

Batch 71780, train_perplexity=1092.2004, train_loss=6.9959497

Batch 71790, train_perplexity=1184.0063, train_loss=7.076659

Batch 71800, train_perplexity=1099.6598, train_loss=7.002756

Batch 71810, train_perplexity=1065.6855, train_loss=6.9713736

Batch 71820, train_perplexity=1218.1832, train_loss=7.105116

Batch 71830, train_perplexity=1179.7008, train_loss=7.073016

Batch 71840, train_perplexity=1224.9413, train_loss=7.110648

Batch 71850, train_perplexity=1089.4573, train_loss=6.993435

Batch 71860, train_perplexity=1217.3344, train_loss=7.1044188

Batch 71870, train_perplexity=1061.542, train_loss=6.967478

Batch 71880, train_perplexity=1073.1438, train_loss=6.978348

Batch 71890, train_perplexity=1287.452, train_loss=7.1604204

Batch 71900, train_perplexity=1103.6237, train_loss=7.0063543

Batch 71910, train_perplexity=1104.8438, train_loss=7.007459

Batch 71920, train_perplexity=1163.79, train_loss=7.0594373

Batch 71930, train_perplexity=1124.2065, train_loss=7.0248327

Batch 71940, train_perplexity=1117.3093, train_loss=7.0186787

Batch 71950, train_perplexity=1173.4174, train_loss=7.0676756

Batch 71960, train_perplexity=1133.6344, train_loss=7.033184

Batch 71970, train_perplexity=1097.2258, train_loss=7.0005403

Batch 71980, train_perplexity=1156.6594, train_loss=7.0532913

Batch 71990, train_perplexity=1132.933, train_loss=7.032565

Batch 72000, train_perplexity=1073.0165, train_loss=6.978229

Batch 72010, train_perplexity=1210.959, train_loss=7.099168

Batch 72020, train_perplexity=1243.0974, train_loss=7.1253614

Batch 72030, train_perplexity=1091.379, train_loss=6.9951973
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72040, train_perplexity=1078.192, train_loss=6.983041

Batch 72050, train_perplexity=1171.0988, train_loss=7.0656977

Batch 72060, train_perplexity=1104.5123, train_loss=7.007159

Batch 72070, train_perplexity=1126.8508, train_loss=7.027182

Batch 72080, train_perplexity=1196.5131, train_loss=7.087167

Batch 72090, train_perplexity=1089.3248, train_loss=6.9933133

Batch 72100, train_perplexity=1042.7974, train_loss=6.949662

Batch 72110, train_perplexity=1084.4567, train_loss=6.9888344

Batch 72120, train_perplexity=1115.5631, train_loss=7.0171146

Batch 72130, train_perplexity=1124.095, train_loss=7.0247335

Batch 72140, train_perplexity=1339.3079, train_loss=7.1999083

Batch 72150, train_perplexity=1212.0132, train_loss=7.100038

Batch 72160, train_perplexity=1094.8395, train_loss=6.998363

Batch 72170, train_perplexity=1178.5236, train_loss=7.0720177

Batch 72180, train_perplexity=1196.0944, train_loss=7.086817

Batch 72190, train_perplexity=1083.6854, train_loss=6.988123

Batch 72200, train_perplexity=1092.7089, train_loss=6.996415

Batch 72210, train_perplexity=1128.712, train_loss=7.0288324

Batch 72220, train_perplexity=1101.5669, train_loss=7.004489

Batch 72230, train_perplexity=1220.6964, train_loss=7.107177

Batch 72240, train_perplexity=1145.4719, train_loss=7.043572

Batch 72250, train_perplexity=1178.0886, train_loss=7.0716486

Batch 72260, train_perplexity=1168.169, train_loss=7.063193

Batch 72270, train_perplexity=1208.8013, train_loss=7.0973845

Batch 72280, train_perplexity=1172.5314, train_loss=7.0669203

Batch 72290, train_perplexity=1107.3031, train_loss=7.0096827

Batch 72300, train_perplexity=1081.1058, train_loss=6.9857397

Batch 72310, train_perplexity=1191.8846, train_loss=7.083291

Batch 72320, train_perplexity=1234.4274, train_loss=7.1183624

Batch 72330, train_perplexity=1146.0625, train_loss=7.0440874

Batch 72340, train_perplexity=1151.6996, train_loss=7.048994

Batch 72350, train_perplexity=1151.7886, train_loss=7.0490713

Batch 72360, train_perplexity=1098.6619, train_loss=7.001848

Batch 72370, train_perplexity=1225.6663, train_loss=7.11124

Batch 72380, train_perplexity=1123.6052, train_loss=7.0242977

Batch 72390, train_perplexity=1071.2307, train_loss=6.9765635

Batch 72400, train_perplexity=1077.9247, train_loss=6.982793

Batch 72410, train_perplexity=1205.1248, train_loss=7.0943384

Batch 72420, train_perplexity=1153.7389, train_loss=7.050763

Batch 72430, train_perplexity=1213.7036, train_loss=7.101432

Batch 72440, train_perplexity=1150.7456, train_loss=7.0481653

Batch 72450, train_perplexity=1258.9459, train_loss=7.13803

Batch 72460, train_perplexity=1135.8583, train_loss=7.035144

Batch 72470, train_perplexity=1084.902, train_loss=6.989245

Batch 72480, train_perplexity=1122.7557, train_loss=7.0235415

Batch 72490, train_perplexity=1054.7395, train_loss=6.961049

Batch 72500, train_perplexity=1154.1196, train_loss=7.051093

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Batch 72510, train_perplexity=1083.8259, train_loss=6.9882526

Batch 72520, train_perplexity=1162.1415, train_loss=7.0580196

Batch 72530, train_perplexity=1202.0996, train_loss=7.091825

Batch 72540, train_perplexity=1109.4355, train_loss=7.0116067

Batch 72550, train_perplexity=1158.2975, train_loss=7.0547066

Batch 72560, train_perplexity=1097.0385, train_loss=7.0003695

Batch 72570, train_perplexity=1170.2, train_loss=7.06493

Batch 72580, train_perplexity=1068.0277, train_loss=6.973569

Batch 72590, train_perplexity=1168.3456, train_loss=7.063344

Batch 72600, train_perplexity=1217.7511, train_loss=7.104761

Batch 72610, train_perplexity=1029.1409, train_loss=6.9364796

Batch 72620, train_perplexity=1124.8693, train_loss=7.025422

Batch 72630, train_perplexity=1118.0277, train_loss=7.0193214

Batch 72640, train_perplexity=1108.6049, train_loss=7.0108576

Batch 72650, train_perplexity=1194.6283, train_loss=7.0855904

Batch 72660, train_perplexity=1098.8893, train_loss=7.002055

Batch 72670, train_perplexity=1147.0914, train_loss=7.044985

Batch 72680, train_perplexity=1103.4143, train_loss=7.0061646

Batch 72690, train_perplexity=1111.0682, train_loss=7.0130773

Batch 72700, train_perplexity=1157.5869, train_loss=7.054093

Batch 72710, train_perplexity=1169.6644, train_loss=7.064472

Batch 72720, train_perplexity=1089.1726, train_loss=6.9931736

Batch 72730, train_perplexity=1153.817, train_loss=7.050831

Batch 72740, train_perplexity=1154.8683, train_loss=7.0517416

Batch 72750, train_perplexity=1084.8162, train_loss=6.989166

Batch 72760, train_perplexity=1074.5653, train_loss=6.9796715

Batch 72770, train_perplexity=1128.3278, train_loss=7.028492

Batch 72780, train_perplexity=1077.3209, train_loss=6.9822326

Batch 72790, train_perplexity=1163.461, train_loss=7.0591545

Batch 72800, train_perplexity=1148.395, train_loss=7.0461206

Batch 72810, train_perplexity=1153.085, train_loss=7.050196

Batch 72820, train_perplexity=1155.715, train_loss=7.0524745

Batch 72830, train_perplexity=1220.9613, train_loss=7.1073937

Batch 72840, train_perplexity=1152.505, train_loss=7.049693

Batch 72850, train_perplexity=1201.7872, train_loss=7.091565

Batch 72860, train_perplexity=1130.9435, train_loss=7.0308075

Batch 72870, train_perplexity=1146.2319, train_loss=7.044235

Batch 72880, train_perplexity=1156.7466, train_loss=7.0533667

Batch 72890, train_perplexity=1110.4565, train_loss=7.0125265

Batch 72900, train_perplexity=1081.739, train_loss=6.9863253

Batch 72910, train_perplexity=1149.2968, train_loss=7.0469055

Batch 72920, train_perplexity=1137.4941, train_loss=7.036583

Batch 72930, train_perplexity=1216.4204, train_loss=7.1036677

Batch 72940, train_perplexity=1027.7089, train_loss=6.935087

Batch 72950, train_perplexity=1160.913, train_loss=7.056962

Batch 72960, train_perplexity=1191.7562, train_loss=7.0831833

Batch 72970, train_perplexity=1093.5251, train_loss=6.997162

Batch 72980, train_perplexity=1156.8944, train_loss=7.0534945

Batch 72990, train_perplexity=1178.8674, train_loss=7.0723095

Batch 73000, train_perplexity=1198.3162, train_loss=7.0886726

Batch 73010, train_perplexity=1161.6594, train_loss=7.057605

Batch 73020, train_perplexity=1018.67896, train_loss=6.926262

Batch 73030, train_perplexity=1156.5955, train_loss=7.053236

Batch 73040, train_perplexity=985.0887, train_loss=6.8927317

Batch 73050, train_perplexity=1177.5786, train_loss=7.0712156

Batch 73060, train_perplexity=1147.5383, train_loss=7.0453744

Batch 73070, train_perplexity=1092.2484, train_loss=6.9959936

Batch 73080, train_perplexity=1163.6586, train_loss=7.0593243

Batch 73090, train_perplexity=1118.0383, train_loss=7.019331

Batch 73100, train_perplexity=1171.0608, train_loss=7.0656652

Batch 73110, train_perplexity=1157.8513, train_loss=7.0543213

Batch 73120, train_perplexity=1162.5992, train_loss=7.0584135

Batch 73130, train_perplexity=1130.0615, train_loss=7.0300274

Batch 73140, train_perplexity=1027.432, train_loss=6.934818

Batch 73150, train_perplexity=1118.6079, train_loss=7.0198402

Batch 73160, train_perplexity=1130.5348, train_loss=7.030446

Batch 73170, train_perplexity=1108.7423, train_loss=7.0109816

Batch 73180, train_perplexity=1106.6027, train_loss=7.00905

Batch 73190, train_perplexity=1142.4836, train_loss=7.04096

Batch 73200, train_perplexity=1048.4655, train_loss=6.955083

Batch 73210, train_perplexity=1138.5886, train_loss=7.0375447

Batch 73220, train_perplexity=1137.69, train_loss=7.036755

Batch 73230, train_perplexity=1198.739, train_loss=7.0890255

Batch 73240, train_perplexity=1196.598, train_loss=7.087238

Batch 73250, train_perplexity=1204.2517, train_loss=7.0936136

Batch 73260, train_perplexity=1133.815, train_loss=7.0333433

Batch 73270, train_perplexity=1085.757, train_loss=6.9900327

Batch 73280, train_perplexity=1174.6982, train_loss=7.0687666

Batch 73290, train_perplexity=1139.0774, train_loss=7.037974
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 73300, train_perplexity=1208.8866, train_loss=7.097455

Batch 73310, train_perplexity=1132.0619, train_loss=7.031796

Batch 73320, train_perplexity=1218.798, train_loss=7.1056204

Batch 73330, train_perplexity=1235.941, train_loss=7.119588

Batch 73340, train_perplexity=1141.639, train_loss=7.0402203

Batch 73350, train_perplexity=1168.0225, train_loss=7.0630674

Batch 73360, train_perplexity=1263.2786, train_loss=7.1414657

Batch 73370, train_perplexity=1225.2205, train_loss=7.110876

Batch 73380, train_perplexity=1125.5303, train_loss=7.0260096

Batch 73390, train_perplexity=1152.6869, train_loss=7.049851

Batch 73400, train_perplexity=1068.5126, train_loss=6.974023

Batch 73410, train_perplexity=1203.991, train_loss=7.093397

Batch 73420, train_perplexity=1128.8379, train_loss=7.028944

Batch 73430, train_perplexity=1054.6007, train_loss=6.9609175

Batch 73440, train_perplexity=1056.599, train_loss=6.9628105

Batch 73450, train_perplexity=1111.8187, train_loss=7.0137525

Batch 73460, train_perplexity=1041.8287, train_loss=6.948733

Batch 73470, train_perplexity=1143.6016, train_loss=7.041938

Batch 73480, train_perplexity=1142.409, train_loss=7.0408945

Batch 73490, train_perplexity=1122.8392, train_loss=7.023616

Batch 73500, train_perplexity=1191.6948, train_loss=7.083132

Batch 73510, train_perplexity=1174.286, train_loss=7.0684156

Batch 73520, train_perplexity=1094.2502, train_loss=6.9978247

Batch 73530, train_perplexity=1135.7316, train_loss=7.0350323

Batch 73540, train_perplexity=1264.6793, train_loss=7.142574

Batch 73550, train_perplexity=1230.3514, train_loss=7.115055

Batch 73560, train_perplexity=1082.5673, train_loss=6.9870906

Batch 73570, train_perplexity=1331.2383, train_loss=7.193865

Batch 73580, train_perplexity=1172.6488, train_loss=7.0670204

Batch 73590, train_perplexity=1124.3866, train_loss=7.024993

Batch 73600, train_perplexity=1145.2944, train_loss=7.043417

Batch 73610, train_perplexity=1022.7084, train_loss=6.9302096

Batch 73620, train_perplexity=1115.1388, train_loss=7.016734

Batch 73630, train_perplexity=1023.2015, train_loss=6.9306917

Batch 73640, train_perplexity=1087.2402, train_loss=6.991398

Batch 73650, train_perplexity=1106.8617, train_loss=7.009284

Batch 73660, train_perplexity=1076.0881, train_loss=6.9810877

Batch 73670, train_perplexity=1114.9526, train_loss=7.016567

Batch 73680, train_perplexity=1033.5018, train_loss=6.940708

Batch 73690, train_perplexity=1065.9742, train_loss=6.9716444

Batch 73700, train_perplexity=936.937, train_loss=6.842616

Batch 73710, train_perplexity=1191.1937, train_loss=7.082711

Batch 73720, train_perplexity=1199.3691, train_loss=7.089551

Batch 73730, train_perplexity=1352.7936, train_loss=7.209927

Batch 73740, train_perplexity=1138.2466, train_loss=7.0372443

Batch 73750, train_perplexity=1208.7148, train_loss=7.097313

Batch 73760, train_perplexity=1160.6882, train_loss=7.0567684

Batch 73770, train_perplexity=1118.7072, train_loss=7.019929

Batch 73780, train_perplexity=1165.664, train_loss=7.061046

Batch 73790, train_perplexity=1087.276, train_loss=6.9914308

Batch 73800, train_perplexity=1129.4728, train_loss=7.029506

Batch 73810, train_perplexity=1083.9987, train_loss=6.988412

Batch 73820, train_perplexity=1077.681, train_loss=6.982567

Batch 73830, train_perplexity=1149.7759, train_loss=7.0473223

Batch 73840, train_perplexity=1127.8539, train_loss=7.028072

Batch 73850, train_perplexity=1081.8845, train_loss=6.9864597

Batch 73860, train_perplexity=1093.3844, train_loss=6.997033

Batch 73870, train_perplexity=1159.3579, train_loss=7.0556216

Batch 73880, train_perplexity=1176.7805, train_loss=7.0705376

Batch 73890, train_perplexity=1090.7417, train_loss=6.994613

Batch 73900, train_perplexity=1142.7725, train_loss=7.0412126

Batch 73910, train_perplexity=1020.97424, train_loss=6.9285126

Batch 73920, train_perplexity=1114.9761, train_loss=7.016588

Batch 73930, train_perplexity=1089.6224, train_loss=6.9935865

Batch 73940, train_perplexity=1133.8864, train_loss=7.0334063

Batch 73950, train_perplexity=1146.9514, train_loss=7.0448627

Batch 73960, train_perplexity=1190.0413, train_loss=7.0817432

Batch 73970, train_perplexity=1106.2043, train_loss=7.00869

Batch 73980, train_perplexity=1310.2257, train_loss=7.1779547

Batch 73990, train_perplexity=1160.7524, train_loss=7.0568237

Batch 74000, train_perplexity=1039.0728, train_loss=6.946084

Batch 74010, train_perplexity=1034.3223, train_loss=6.9415016

Batch 74020, train_perplexity=1144.0172, train_loss=7.042301

Batch 74030, train_perplexity=1145.7532, train_loss=7.0438175

Batch 74040, train_perplexity=1117.4723, train_loss=7.0188246

Batch 74050, train_perplexity=1106.7604, train_loss=7.0091925

Batch 74060, train_perplexity=1173.5013, train_loss=7.067747

Batch 74070, train_perplexity=1140.7738, train_loss=7.039462

Batch 74080, train_perplexity=1176.5381, train_loss=7.0703316

Batch 74090, train_perplexity=1053.4758, train_loss=6.9598503

Batch 74100, train_perplexity=1090.3969, train_loss=6.994297

Batch 74110, train_perplexity=1209.0261, train_loss=7.0975704

Batch 74120, train_perplexity=1040.7478, train_loss=6.947695

Batch 74130, train_perplexity=1141.4398, train_loss=7.0400457

Batch 74140, train_perplexity=1101.7014, train_loss=7.004611

Batch 74150, train_perplexity=1169.1409, train_loss=7.0640244

Batch 74160, train_perplexity=1137.2262, train_loss=7.0363474

Batch 74170, train_perplexity=1089.1809, train_loss=6.993181

Batch 74180, train_perplexity=1039.212, train_loss=6.946218

Batch 74190, train_perplexity=1243.8943, train_loss=7.1260023

Batch 74200, train_perplexity=1121.0963, train_loss=7.0220623

Batch 74210, train_perplexity=1193.8094, train_loss=7.0849047

Batch 74220, train_perplexity=1016.9371, train_loss=6.9245505

Batch 74230, train_perplexity=1033.7167, train_loss=6.940916

Batch 74240, train_perplexity=1200.581, train_loss=7.090561

Batch 74250, train_perplexity=1065.7379, train_loss=6.9714227

Batch 74260, train_perplexity=1312.3035, train_loss=7.179539

Batch 74270, train_perplexity=1225.0365, train_loss=7.110726

Batch 74280, train_perplexity=1021.90643, train_loss=6.9294252

Batch 74290, train_perplexity=1094.7429, train_loss=6.998275

Batch 74300, train_perplexity=1077.194, train_loss=6.982115

Batch 74310, train_perplexity=1130.3784, train_loss=7.030308

Batch 74320, train_perplexity=1203.6443, train_loss=7.093109

Batch 74330, train_perplexity=1062.3541, train_loss=6.9682426

Batch 74340, train_perplexity=1113.9717, train_loss=7.015687

Batch 74350, train_perplexity=1102.1786, train_loss=7.005044

Batch 74360, train_perplexity=1097.622, train_loss=7.000901

Batch 74370, train_perplexity=1021.6828, train_loss=6.9292064

Batch 74380, train_perplexity=1225.5494, train_loss=7.1111445

Batch 74390, train_perplexity=1093.5492, train_loss=6.997184

Batch 74400, train_perplexity=1183.2433, train_loss=7.0760145

Batch 74410, train_perplexity=1139.5946, train_loss=7.038428

Batch 74420, train_perplexity=1093.8058, train_loss=6.9974184

Batch 74430, train_perplexity=1182.1819, train_loss=7.075117

Batch 74440, train_perplexity=1124.7303, train_loss=7.0252986

Batch 74450, train_perplexity=1097.174, train_loss=7.000493

Batch 74460, train_perplexity=1237.489, train_loss=7.1208396

Batch 74470, train_perplexity=1256.0431, train_loss=7.1357217

Batch 74480, train_perplexity=1179.1041, train_loss=7.0725102

Batch 74490, train_perplexity=1004.52545, train_loss=6.9122705

Batch 74500, train_perplexity=1082.3876, train_loss=6.9869246

Batch 74510, train_perplexity=1129.8967, train_loss=7.0298815

Batch 74520, train_perplexity=1168.4425, train_loss=7.063427

Batch 74530, train_perplexity=1086.1686, train_loss=6.9904118

Batch 74540, train_perplexity=1013.7523, train_loss=6.921414

Batch 74550, train_perplexity=1101.2802, train_loss=7.0042286

Batch 74560, train_perplexity=1108.0118, train_loss=7.0103226

Batch 74570, train_perplexity=1256.015, train_loss=7.1356993

Batch 74580, train_perplexity=1237.0872, train_loss=7.120515

Batch 74590, train_perplexity=1061.3364, train_loss=6.967284

Batch 74600, train_perplexity=1083.438, train_loss=6.9878945

Batch 74610, train_perplexity=1033.5078, train_loss=6.940714
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74620, train_perplexity=1032.5383, train_loss=6.9397755

Batch 74630, train_perplexity=1182.4288, train_loss=7.075326

Batch 74640, train_perplexity=1113.0117, train_loss=7.014825

Batch 74650, train_perplexity=1145.3452, train_loss=7.0434613

Batch 74660, train_perplexity=1090.3209, train_loss=6.9942274

Batch 74670, train_perplexity=1113.6127, train_loss=7.0153646

Batch 74680, train_perplexity=1109.7875, train_loss=7.011924

Batch 74690, train_perplexity=1081.0177, train_loss=6.985658

Batch 74700, train_perplexity=1200.2777, train_loss=7.090308

Batch 74710, train_perplexity=1229.1129, train_loss=7.114048

Batch 74720, train_perplexity=1056.3103, train_loss=6.9625373

Batch 74730, train_perplexity=1185.8998, train_loss=7.078257

Batch 74740, train_perplexity=1096.1715, train_loss=6.999579

Batch 74750, train_perplexity=1031.024, train_loss=6.938308

Batch 74760, train_perplexity=1169.1593, train_loss=7.06404

Batch 74770, train_perplexity=1093.0164, train_loss=6.9966965

Batch 74780, train_perplexity=1134.5668, train_loss=7.034006

Batch 74790, train_perplexity=1064.387, train_loss=6.9701543

Batch 74800, train_perplexity=1193.1754, train_loss=7.0843735

Batch 74810, train_perplexity=1186.5417, train_loss=7.0787983

Batch 74820, train_perplexity=1191.1755, train_loss=7.082696

Batch 74830, train_perplexity=1042.9705, train_loss=6.949828

Batch 74840, train_perplexity=1136.2993, train_loss=7.035532

Batch 74850, train_perplexity=982.01117, train_loss=6.8896027

Batch 74860, train_perplexity=1094.2601, train_loss=6.9978337

Batch 74870, train_perplexity=1142.4875, train_loss=7.040963

Batch 74880, train_perplexity=1032.5836, train_loss=6.9398193

Batch 74890, train_perplexity=1078.0521, train_loss=6.982911

Batch 74900, train_perplexity=1251.7728, train_loss=7.132316

Batch 74910, train_perplexity=1132.1122, train_loss=7.0318403

Batch 74920, train_perplexity=1085.1199, train_loss=6.9894457

Batch 74930, train_perplexity=1106.5936, train_loss=7.009042

Batch 74940, train_perplexity=1105.5251, train_loss=7.0080757

Batch 74950, train_perplexity=1101.5128, train_loss=7.00444

Batch 74960, train_perplexity=1104.8569, train_loss=7.007471

Batch 74970, train_perplexity=1039.1055, train_loss=6.9461155

Batch 74980, train_perplexity=1168.7803, train_loss=7.063716

Batch 74990, train_perplexity=1189.6571, train_loss=7.0814204

Batch 75000, train_perplexity=1184.5417, train_loss=7.0771112

Batch 75010, train_perplexity=1053.1464, train_loss=6.9595375

Batch 75020, train_perplexity=1183.5751, train_loss=7.076295

Batch 75030, train_perplexity=1235.8385, train_loss=7.119505

Batch 75040, train_perplexity=1144.791, train_loss=7.0429773

Batch 75050, train_perplexity=1244.528, train_loss=7.1265116

Batch 75060, train_perplexity=1107.5956, train_loss=7.009947

Batch 75070, train_perplexity=1138.3563, train_loss=7.0373406

Batch 75080, train_perplexity=1140.9806, train_loss=7.0396433

Batch 75090, train_perplexity=1193.461, train_loss=7.084613

Batch 75100, train_perplexity=1219.9858, train_loss=7.1065946

Batch 75110, train_perplexity=1120.8595, train_loss=7.021851

Batch 75120, train_perplexity=1142.8455, train_loss=7.0412765

Batch 75130, train_perplexity=1188.4692, train_loss=7.0804214

Batch 75140, train_perplexity=1071.1138, train_loss=6.9764543

Batch 75150, train_perplexity=1116.6436, train_loss=7.0180826

Batch 75160, train_perplexity=1095.102, train_loss=6.998603

Batch 75170, train_perplexity=1040.8004, train_loss=6.9477453

Batch 75180, train_perplexity=1216.6519, train_loss=7.103858

Batch 75190, train_perplexity=1172.4224, train_loss=7.0668273

Batch 75200, train_perplexity=1079.4039, train_loss=6.984164

Batch 75210, train_perplexity=1142.4276, train_loss=7.0409107

Batch 75220, train_perplexity=1212.5294, train_loss=7.100464

Batch 75230, train_perplexity=1132.5095, train_loss=7.0321913

Batch 75240, train_perplexity=1134.9996, train_loss=7.0343876

Batch 75250, train_perplexity=1033.7207, train_loss=6.94092

Batch 75260, train_perplexity=1082.9457, train_loss=6.98744

Batch 75270, train_perplexity=1039.963, train_loss=6.9469404

Batch 75280, train_perplexity=1155.3822, train_loss=7.0521865

Batch 75290, train_perplexity=1171.554, train_loss=7.0660863

Batch 75300, train_perplexity=1091.7157, train_loss=6.995506

Batch 75310, train_perplexity=1053.4583, train_loss=6.9598336

Batch 75320, train_perplexity=1185.3237, train_loss=7.077771

Batch 75330, train_perplexity=1138.3167, train_loss=7.037306

Batch 75340, train_perplexity=977.3849, train_loss=6.8848805

Batch 75350, train_perplexity=1225.8773, train_loss=7.111412

Batch 75360, train_perplexity=1108.6323, train_loss=7.0108824

Batch 75370, train_perplexity=1102.7389, train_loss=7.0055523

Batch 75380, train_perplexity=1088.6284, train_loss=6.992674

Batch 75390, train_perplexity=1079.2228, train_loss=6.9839964

Batch 75400, train_perplexity=1071.5249, train_loss=6.976838

Batch 75410, train_perplexity=1065.1034, train_loss=6.970827

Batch 75420, train_perplexity=1089.4594, train_loss=6.993437

Batch 75430, train_perplexity=1125.1053, train_loss=7.025632

Batch 75440, train_perplexity=1180.4268, train_loss=7.0736313

Batch 75450, train_perplexity=1134.3049, train_loss=7.0337753

Batch 75460, train_perplexity=1040.8569, train_loss=6.9477997

Batch 75470, train_perplexity=1252.6005, train_loss=7.132977

Batch 75480, train_perplexity=1098.0768, train_loss=7.0013156

Batch 75490, train_perplexity=1185.1394, train_loss=7.0776157

Batch 75500, train_perplexity=1097.49, train_loss=7.000781

Batch 75510, train_perplexity=1093.7213, train_loss=6.997341

Batch 75520, train_perplexity=1164.2025, train_loss=7.0597916

Batch 75530, train_perplexity=1110.2046, train_loss=7.0122995

Batch 75540, train_perplexity=1094.4886, train_loss=6.9980426

Batch 75550, train_perplexity=1046.3339, train_loss=6.9530478

Batch 75560, train_perplexity=1297.4618, train_loss=7.168165

Batch 75570, train_perplexity=1095.452, train_loss=6.9989223

Batch 75580, train_perplexity=1146.6709, train_loss=7.044618

Batch 75590, train_perplexity=1152.8612, train_loss=7.050002

Batch 75600, train_perplexity=1066.5264, train_loss=6.9721622

Batch 75610, train_perplexity=1078.6929, train_loss=6.9835052

Batch 75620, train_perplexity=1154.6866, train_loss=7.0515842

Batch 75630, train_perplexity=1099.1848, train_loss=7.002324

Batch 75640, train_perplexity=1144.1132, train_loss=7.042385

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Batch 75650, train_perplexity=1167.5859, train_loss=7.0626936

Batch 75660, train_perplexity=1109.0288, train_loss=7.01124

Batch 75670, train_perplexity=1147.0848, train_loss=7.044979

Batch 75680, train_perplexity=1120.7708, train_loss=7.021772

Batch 75690, train_perplexity=1073.6547, train_loss=6.9788237

Batch 75700, train_perplexity=1216.0087, train_loss=7.103329

Batch 75710, train_perplexity=1055.5505, train_loss=6.9618177

Batch 75720, train_perplexity=1264.3048, train_loss=7.1422777

Batch 75730, train_perplexity=1077.5336, train_loss=6.98243

Batch 75740, train_perplexity=1159.3972, train_loss=7.0556555

Batch 75750, train_perplexity=1097.1343, train_loss=7.000457

Batch 75760, train_perplexity=1177.0874, train_loss=7.0707984

Batch 75770, train_perplexity=1159.165, train_loss=7.055455

Batch 75780, train_perplexity=1081.362, train_loss=6.9859767

Batch 75790, train_perplexity=1054.1783, train_loss=6.960517

Batch 75800, train_perplexity=1101.3442, train_loss=7.004287

Batch 75810, train_perplexity=1164.3513, train_loss=7.0599194

Batch 75820, train_perplexity=1162.7805, train_loss=7.0585694

Batch 75830, train_perplexity=1163.3928, train_loss=7.059096

Batch 75840, train_perplexity=1128.019, train_loss=7.0282183

Batch 75850, train_perplexity=1178.1448, train_loss=7.0716963

Batch 75860, train_perplexity=1055.9684, train_loss=6.9622135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 75870, train_perplexity=1207.9116, train_loss=7.096648

Batch 75880, train_perplexity=1144.6731, train_loss=7.0428743

Batch 75890, train_perplexity=1127.2786, train_loss=7.0275617

Batch 75900, train_perplexity=1109.8954, train_loss=7.012021

Batch 75910, train_perplexity=1168.1852, train_loss=7.0632067

Batch 75920, train_perplexity=1152.6215, train_loss=7.049794

Batch 75930, train_perplexity=1052.576, train_loss=6.958996

Batch 75940, train_perplexity=1185.6504, train_loss=7.078047

Batch 75950, train_perplexity=1112.7771, train_loss=7.014614

Batch 75960, train_perplexity=1049.2036, train_loss=6.9557867

Batch 75970, train_perplexity=1113.8739, train_loss=7.0155993

Batch 75980, train_perplexity=1096.8036, train_loss=7.0001554

Batch 75990, train_perplexity=1090.1588, train_loss=6.9940786

Batch 76000, train_perplexity=1005.3799, train_loss=6.9131207

Batch 76010, train_perplexity=1092.0297, train_loss=6.9957933

Batch 76020, train_perplexity=1165.9647, train_loss=7.061304

Batch 76030, train_perplexity=1154.3717, train_loss=7.0513115

Batch 76040, train_perplexity=1050.7317, train_loss=6.957242

Batch 76050, train_perplexity=1121.3594, train_loss=7.022297

Batch 76060, train_perplexity=1164.6339, train_loss=7.060162

Batch 76070, train_perplexity=1170.7057, train_loss=7.065362

Batch 76080, train_perplexity=1158.7981, train_loss=7.0551386

Batch 76090, train_perplexity=1079.8652, train_loss=6.9845915

Batch 76100, train_perplexity=1165.1477, train_loss=7.060603

Batch 76110, train_perplexity=1044.1648, train_loss=6.9509726

Batch 76120, train_perplexity=1043.7118, train_loss=6.9505386

Batch 76130, train_perplexity=995.17236, train_loss=6.902916

Batch 76140, train_perplexity=1080.861, train_loss=6.985513

Batch 76150, train_perplexity=1077.212, train_loss=6.9821315

Batch 76160, train_perplexity=1151.8556, train_loss=7.0491295

Batch 76170, train_perplexity=1112.81, train_loss=7.0146437

Batch 76180, train_perplexity=1060.7152, train_loss=6.9666986

Batch 76190, train_perplexity=1149.0721, train_loss=7.04671

Batch 76200, train_perplexity=1138.8802, train_loss=7.037801

Batch 76210, train_perplexity=1161.6118, train_loss=7.057564

Batch 76220, train_perplexity=1029.613, train_loss=6.9369383

Batch 76230, train_perplexity=1150.8163, train_loss=7.048227

Batch 76240, train_perplexity=1144.0919, train_loss=7.0423665

Batch 76250, train_perplexity=1132.9595, train_loss=7.0325885

Batch 76260, train_perplexity=968.7884, train_loss=6.876046

Batch 76270, train_perplexity=1222.2334, train_loss=7.108435

Batch 76280, train_perplexity=1154.3662, train_loss=7.0513067

Batch 76290, train_perplexity=1075.4911, train_loss=6.9805326

Batch 76300, train_perplexity=1116.1719, train_loss=7.01766

Batch 76310, train_perplexity=1056.5002, train_loss=6.962717

Batch 76320, train_perplexity=1121.2743, train_loss=7.022221

Batch 76330, train_perplexity=1167.8583, train_loss=7.062927

Batch 76340, train_perplexity=1022.13354, train_loss=6.9296474

Batch 76350, train_perplexity=1121.6337, train_loss=7.0225415

Batch 76360, train_perplexity=1165.9841, train_loss=7.061321

Batch 76370, train_perplexity=1192.4138, train_loss=7.083735

Batch 76380, train_perplexity=1186.8842, train_loss=7.079087

Batch 76390, train_perplexity=1216.9443, train_loss=7.1040983

Batch 76400, train_perplexity=1218.0695, train_loss=7.1050224

Batch 76410, train_perplexity=1202.9661, train_loss=7.0925455

Batch 76420, train_perplexity=1136.4142, train_loss=7.035633

Batch 76430, train_perplexity=1175.0176, train_loss=7.0690384

Batch 76440, train_perplexity=1157.1874, train_loss=7.0537477

Batch 76450, train_perplexity=1171.4534, train_loss=7.0660005

Batch 76460, train_perplexity=1081.1388, train_loss=6.98577

Batch 76470, train_perplexity=1078.3297, train_loss=6.9831686

Batch 76480, train_perplexity=1176.2117, train_loss=7.070054

Batch 76490, train_perplexity=1040.1802, train_loss=6.9471493

Batch 76500, train_perplexity=1145.5468, train_loss=7.0436373

Batch 76510, train_perplexity=1130.359, train_loss=7.0302906

Batch 76520, train_perplexity=1069.3914, train_loss=6.974845

Batch 76530, train_perplexity=1142.4418, train_loss=7.040923

Batch 76540, train_perplexity=1262.1953, train_loss=7.140608

Batch 76550, train_perplexity=1125.6473, train_loss=7.0261135

Batch 76560, train_perplexity=1124.6848, train_loss=7.025258

Batch 76570, train_perplexity=1030.4027, train_loss=6.937705

Batch 76580, train_perplexity=1101.0239, train_loss=7.003996

Batch 76590, train_perplexity=1168.8716, train_loss=7.063794

Batch 76600, train_perplexity=1152.4391, train_loss=7.049636

Batch 76610, train_perplexity=1101.6184, train_loss=7.0045357

Batch 76620, train_perplexity=1033.8513, train_loss=6.941046

Batch 76630, train_perplexity=1210.6887, train_loss=7.0989447

Batch 76640, train_perplexity=1059.3878, train_loss=6.9654465

Batch 76650, train_perplexity=1103.8037, train_loss=7.0065174

Batch 76660, train_perplexity=1087.123, train_loss=6.99129

Batch 76670, train_perplexity=1137.2023, train_loss=7.0363264

Batch 76680, train_perplexity=1170.4891, train_loss=7.065177

Batch 76690, train_perplexity=1174.0884, train_loss=7.0682473

Batch 76700, train_perplexity=1244.3998, train_loss=7.1264086

Batch 76710, train_perplexity=1086.8551, train_loss=6.9910436

Batch 76720, train_perplexity=1113.3132, train_loss=7.0150957

Batch 76730, train_perplexity=1146.4768, train_loss=7.044449

Batch 76740, train_perplexity=1178.1672, train_loss=7.0717154

Batch 76750, train_perplexity=1058.5042, train_loss=6.964612

Batch 76760, train_perplexity=1010.8469, train_loss=6.918544

Batch 76770, train_perplexity=1149.0195, train_loss=7.046664

Batch 76780, train_perplexity=1179.1891, train_loss=7.0725822

Batch 76790, train_perplexity=1079.9105, train_loss=6.9846334

Batch 76800, train_perplexity=1152.1951, train_loss=7.049424

Batch 76810, train_perplexity=1071.9757, train_loss=6.9772587

Batch 76820, train_perplexity=1177.3109, train_loss=7.070988

Batch 76830, train_perplexity=1086.8168, train_loss=6.9910083

Batch 76840, train_perplexity=1049.1526, train_loss=6.955738

Batch 76850, train_perplexity=1216.233, train_loss=7.1035137

Batch 76860, train_perplexity=1079.3607, train_loss=6.984124

Batch 76870, train_perplexity=1111.1414, train_loss=7.013143

Batch 76880, train_perplexity=1084.9366, train_loss=6.989277

Batch 76890, train_perplexity=1299.6897, train_loss=7.169881

Batch 76900, train_perplexity=1162.1536, train_loss=7.05803

Batch 76910, train_perplexity=1131.511, train_loss=7.031309

Batch 76920, train_perplexity=1201.4951, train_loss=7.091322

Batch 76930, train_perplexity=1163.7584, train_loss=7.05941

Batch 76940, train_perplexity=1044.0487, train_loss=6.9508615

Batch 76950, train_perplexity=1056.188, train_loss=6.9624214

Batch 76960, train_perplexity=1096.7053, train_loss=7.000066

Batch 76970, train_perplexity=1124.5491, train_loss=7.0251374

Batch 76980, train_perplexity=1152.032, train_loss=7.0492826

Batch 76990, train_perplexity=1115.6536, train_loss=7.0171957

Batch 77000, train_perplexity=1197.6335, train_loss=7.088103

Batch 77010, train_perplexity=1129.5137, train_loss=7.0295424

Batch 77020, train_perplexity=1063.9642, train_loss=6.969757

Batch 77030, train_perplexity=1122.7772, train_loss=7.0235605

Batch 77040, train_perplexity=1097.2911, train_loss=7.0006

Batch 77050, train_perplexity=1099.5906, train_loss=7.002693

Batch 77060, train_perplexity=1040.3608, train_loss=6.947323

Batch 77070, train_perplexity=1190.6815, train_loss=7.082281

Batch 77080, train_perplexity=1013.6909, train_loss=6.9213533

Batch 77090, train_perplexity=1196.9364, train_loss=7.0875206

Batch 77100, train_perplexity=1149.9414, train_loss=7.0474663

Batch 77110, train_perplexity=1001.8362, train_loss=6.90959

Batch 77120, train_perplexity=1026.5829, train_loss=6.933991

Batch 77130, train_perplexity=1108.6403, train_loss=7.0108895

Batch 77140, train_perplexity=1087.8298, train_loss=6.99194

Batch 77150, train_perplexity=1104.0043, train_loss=7.006699

Batch 77160, train_perplexity=1119.9977, train_loss=7.021082

Batch 77170, train_perplexity=1082.6442, train_loss=6.9871616

Batch 77180, train_perplexity=1006.9963, train_loss=6.914727
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 77190, train_perplexity=1044.9039, train_loss=6.95168

Batch 77200, train_perplexity=1109.3124, train_loss=7.0114956

Batch 77210, train_perplexity=1082.1353, train_loss=6.9866915

Batch 77220, train_perplexity=964.665, train_loss=6.871781

Batch 77230, train_perplexity=1168.193, train_loss=7.0632133

Batch 77240, train_perplexity=1075.7998, train_loss=6.9808197

Batch 77250, train_perplexity=1124.6976, train_loss=7.0252695

Batch 77260, train_perplexity=1186.3392, train_loss=7.0786276

Batch 77270, train_perplexity=1188.6104, train_loss=7.08054

Batch 77280, train_perplexity=1063.1519, train_loss=6.968993

Batch 77290, train_perplexity=1171.5729, train_loss=7.0661025

Batch 77300, train_perplexity=1107.181, train_loss=7.0095725

Batch 77310, train_perplexity=1009.1772, train_loss=6.9168906

Batch 77320, train_perplexity=1179.468, train_loss=7.0728188

Batch 77330, train_perplexity=1091.1417, train_loss=6.99498

Batch 77340, train_perplexity=1079.7112, train_loss=6.984449

Batch 77350, train_perplexity=1119.4814, train_loss=7.020621

Batch 77360, train_perplexity=1168.3567, train_loss=7.0633535

Batch 77370, train_perplexity=1143.7297, train_loss=7.04205

Batch 77380, train_perplexity=1121.4374, train_loss=7.0223665

Batch 77390, train_perplexity=1224.9027, train_loss=7.1106167

Batch 77400, train_perplexity=1017.28094, train_loss=6.9248886

Batch 77410, train_perplexity=1144.327, train_loss=7.042572

Batch 77420, train_perplexity=1121.0107, train_loss=7.021986

Batch 77430, train_perplexity=1183.9031, train_loss=7.076572

Batch 77440, train_perplexity=1062.6217, train_loss=6.9684944

Batch 77450, train_perplexity=1189.551, train_loss=7.0813313

Batch 77460, train_perplexity=1027.212, train_loss=6.9346037

Batch 77470, train_perplexity=1064.919, train_loss=6.970654

Batch 77480, train_perplexity=1174.547, train_loss=7.068638

Batch 77490, train_perplexity=1111.497, train_loss=7.013463

Batch 77500, train_perplexity=1132.3658, train_loss=7.0320644

Batch 77510, train_perplexity=1053.2117, train_loss=6.9595995

Batch 77520, train_perplexity=1062.2761, train_loss=6.968169

Batch 77530, train_perplexity=1057.9441, train_loss=6.9640827

Batch 77540, train_perplexity=1160.4165, train_loss=7.0565343

Batch 77550, train_perplexity=1086.7307, train_loss=6.990929

Batch 77560, train_perplexity=1129.4679, train_loss=7.029502

Batch 77570, train_perplexity=1081.4451, train_loss=6.9860535

Batch 77580, train_perplexity=1103.5132, train_loss=7.006254

Batch 77590, train_perplexity=1105.1436, train_loss=7.0077305

Batch 77600, train_perplexity=1061.4133, train_loss=6.9673567

Batch 77610, train_perplexity=1156.2866, train_loss=7.052969

Batch 77620, train_perplexity=1157.9751, train_loss=7.054428

Batch 77630, train_perplexity=1060.0295, train_loss=6.966052

Batch 77640, train_perplexity=1094.1245, train_loss=6.9977098

Batch 77650, train_perplexity=1077.571, train_loss=6.982465

Batch 77660, train_perplexity=1061.9197, train_loss=6.9678335

Batch 77670, train_perplexity=1037.4736, train_loss=6.944544

Batch 77680, train_perplexity=1146.3215, train_loss=7.0443134

Batch 77690, train_perplexity=1017.5924, train_loss=6.9251947

Batch 77700, train_perplexity=1088.0363, train_loss=6.99213

Batch 77710, train_perplexity=1110.6561, train_loss=7.0127063

Batch 77720, train_perplexity=1092.6464, train_loss=6.996358

Batch 77730, train_perplexity=1208.8912, train_loss=7.097459

Batch 77740, train_perplexity=1061.7009, train_loss=6.9676275

Batch 77750, train_perplexity=1087.9186, train_loss=6.9920216

Batch 77760, train_perplexity=1132.7963, train_loss=7.0324445

Batch 77770, train_perplexity=1084.1201, train_loss=6.988524

Batch 77780, train_perplexity=1101.8706, train_loss=7.0047646

Batch 77790, train_perplexity=1075.2234, train_loss=6.9802837

Batch 77800, train_perplexity=1053.4236, train_loss=6.9598007

Batch 77810, train_perplexity=1106.1694, train_loss=7.0086584

Batch 77820, train_perplexity=1360.7947, train_loss=7.215824

Batch 77830, train_perplexity=1091.6163, train_loss=6.9954147

Batch 77840, train_perplexity=962.159, train_loss=6.8691797

Batch 77850, train_perplexity=1125.3907, train_loss=7.0258856

Batch 77860, train_perplexity=1121.7733, train_loss=7.022666

Batch 77870, train_perplexity=1079.0767, train_loss=6.983861

Batch 77880, train_perplexity=1127.6086, train_loss=7.0278544

Batch 77890, train_perplexity=1167.1406, train_loss=7.062312

Batch 77900, train_perplexity=1033.015, train_loss=6.940237

Batch 77910, train_perplexity=1090.2762, train_loss=6.9941864

Batch 77920, train_perplexity=1169.3761, train_loss=7.0642257

Batch 77930, train_perplexity=1161.0746, train_loss=7.0571012

Batch 77940, train_perplexity=1100.9751, train_loss=7.0039515

Batch 77950, train_perplexity=1118.6565, train_loss=7.0198836

Batch 77960, train_perplexity=1071.075, train_loss=6.976418

Batch 77970, train_perplexity=997.4756, train_loss=6.9052277

Batch 77980, train_perplexity=1205.6283, train_loss=7.094756

Batch 77990, train_perplexity=1132.2957, train_loss=7.0320024

Batch 78000, train_perplexity=1053.5984, train_loss=6.9599667

Batch 78010, train_perplexity=987.6666, train_loss=6.895345

Batch 78020, train_perplexity=1108.2122, train_loss=7.0105033

Batch 78030, train_perplexity=1109.0151, train_loss=7.0112276

Batch 78040, train_perplexity=1100.795, train_loss=7.003788

Batch 78050, train_perplexity=1187.4429, train_loss=7.0795574

Batch 78060, train_perplexity=1095.6934, train_loss=6.9991426

Batch 78070, train_perplexity=1127.8425, train_loss=7.028062

Batch 78080, train_perplexity=1168.2035, train_loss=7.0632224

Batch 78090, train_perplexity=1043.3087, train_loss=6.9501524

Batch 78100, train_perplexity=1124.9423, train_loss=7.025487

Batch 78110, train_perplexity=1176.4404, train_loss=7.0702486

Batch 78120, train_perplexity=1162.1315, train_loss=7.058011

Batch 78130, train_perplexity=1066.267, train_loss=6.971919

Batch 78140, train_perplexity=1085.0619, train_loss=6.9893923

Batch 78150, train_perplexity=1048.324, train_loss=6.954948

Batch 78160, train_perplexity=1130.4744, train_loss=7.0303926

Batch 78170, train_perplexity=1208.4855, train_loss=7.097123

Batch 78180, train_perplexity=1138.8672, train_loss=7.0377893

Batch 78190, train_perplexity=1171.292, train_loss=7.0658627

Batch 78200, train_perplexity=1069.8372, train_loss=6.9752617

Batch 78210, train_perplexity=1192.5634, train_loss=7.0838604

Batch 78220, train_perplexity=1039.2075, train_loss=6.9462137

Batch 78230, train_perplexity=1089.0148, train_loss=6.9930286

Batch 78240, train_perplexity=1125.7524, train_loss=7.026207

Batch 78250, train_perplexity=1141.6074, train_loss=7.0401926

Batch 78260, train_perplexity=1213.0094, train_loss=7.1008596

Batch 78270, train_perplexity=1073.8062, train_loss=6.978965

Batch 78280, train_perplexity=1194.2559, train_loss=7.0852785

Batch 78290, train_perplexity=1078.2207, train_loss=6.9830675

Batch 78300, train_perplexity=1033.0436, train_loss=6.9402647

Batch 78310, train_perplexity=973.0922, train_loss=6.880479

Batch 78320, train_perplexity=1138.2559, train_loss=7.0372524

Batch 78330, train_perplexity=1270.0391, train_loss=7.146803

Batch 78340, train_perplexity=1186.4253, train_loss=7.0787

Batch 78350, train_perplexity=1088.7706, train_loss=6.9928045

Batch 78360, train_perplexity=1053.8401, train_loss=6.960196

Batch 78370, train_perplexity=1054.4257, train_loss=6.9607515

Batch 78380, train_perplexity=1043.3983, train_loss=6.950238

Batch 78390, train_perplexity=1138.0626, train_loss=7.0370827

Batch 78400, train_perplexity=1121.4203, train_loss=7.0223513

Batch 78410, train_perplexity=1048.6974, train_loss=6.955304

Batch 78420, train_perplexity=1011.2128, train_loss=6.9189057

Batch 78430, train_perplexity=1153.4584, train_loss=7.05052

Batch 78440, train_perplexity=1112.1724, train_loss=7.0140705

Batch 78450, train_perplexity=1115.6935, train_loss=7.0172315

Batch 78460, train_perplexity=1275.5304, train_loss=7.1511173

Batch 78470, train_perplexity=1083.9578, train_loss=6.988374

Batch 78480, train_perplexity=1193.3552, train_loss=7.084524

Batch 78490, train_perplexity=1081.997, train_loss=6.9865637

Batch 78500, train_perplexity=1118.5348, train_loss=7.019775
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78510, train_perplexity=1094.1417, train_loss=6.9977255

Batch 78520, train_perplexity=1102.5292, train_loss=7.005362

Batch 78530, train_perplexity=1085.2264, train_loss=6.989544

Batch 78540, train_perplexity=1127.4017, train_loss=7.027671

Batch 78550, train_perplexity=1107.2218, train_loss=7.009609

Batch 78560, train_perplexity=1104.8674, train_loss=7.0074806

Batch 78570, train_perplexity=1035.0588, train_loss=6.9422135

Batch 78580, train_perplexity=1219.5956, train_loss=7.1062746

Batch 78590, train_perplexity=1015.51776, train_loss=6.923154

Batch 78600, train_perplexity=1104.723, train_loss=7.00735

Batch 78610, train_perplexity=1043.1486, train_loss=6.949999

Batch 78620, train_perplexity=1036.3523, train_loss=6.9434624

Batch 78630, train_perplexity=1129.2336, train_loss=7.0292945

Batch 78640, train_perplexity=1043.7675, train_loss=6.950592

Batch 78650, train_perplexity=1067.6315, train_loss=6.973198

Batch 78660, train_perplexity=1163.3745, train_loss=7.05908

Batch 78670, train_perplexity=1196.1588, train_loss=7.0868707

Batch 78680, train_perplexity=1176.0961, train_loss=7.069956

Batch 78690, train_perplexity=1142.5714, train_loss=7.0410366

Batch 78700, train_perplexity=1124.6992, train_loss=7.025271

Batch 78710, train_perplexity=1101.318, train_loss=7.004263

Batch 78720, train_perplexity=1214.4481, train_loss=7.102045

Batch 78730, train_perplexity=1117.7612, train_loss=7.019083

Batch 78740, train_perplexity=1104.5482, train_loss=7.0071917

Batch 78750, train_perplexity=1064.945, train_loss=6.9706783

Batch 78760, train_perplexity=1122.957, train_loss=7.0237207

Batch 78770, train_perplexity=1105.9084, train_loss=7.0084224

Batch 78780, train_perplexity=1098.9857, train_loss=7.002143

Batch 78790, train_perplexity=992.5336, train_loss=6.900261

Batch 78800, train_perplexity=1198.7733, train_loss=7.089054

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Batch 78810, train_perplexity=1174.2659, train_loss=7.0683985

Batch 78820, train_perplexity=1153.893, train_loss=7.0508966

Batch 78830, train_perplexity=1081.8391, train_loss=6.986418

Batch 78840, train_perplexity=1090.8009, train_loss=6.9946675

Batch 78850, train_perplexity=1027.0524, train_loss=6.9344482

Batch 78860, train_perplexity=1196.6061, train_loss=7.0872445

Batch 78870, train_perplexity=1121.0107, train_loss=7.021986

Batch 78880, train_perplexity=1048.214, train_loss=6.954843

Batch 78890, train_perplexity=1110.2808, train_loss=7.012368

Batch 78900, train_perplexity=1103.9316, train_loss=7.0066333

Batch 78910, train_perplexity=1131.3124, train_loss=7.0311337

Batch 78920, train_perplexity=1049.737, train_loss=6.956295

Batch 78930, train_perplexity=1112.2711, train_loss=7.014159

Batch 78940, train_perplexity=1065.1958, train_loss=6.970914

Batch 78950, train_perplexity=1126.1927, train_loss=7.026598

Batch 78960, train_perplexity=1094.6604, train_loss=6.9981995

Batch 78970, train_perplexity=1119.0048, train_loss=7.020195

Batch 78980, train_perplexity=1076.5706, train_loss=6.981536

Batch 78990, train_perplexity=1034.4451, train_loss=6.9416203

Batch 79000, train_perplexity=1168.5885, train_loss=7.063552

Batch 79010, train_perplexity=1082.9617, train_loss=6.987455

Batch 79020, train_perplexity=1151.9468, train_loss=7.0492086

Batch 79030, train_perplexity=1028.2358, train_loss=6.9356

Batch 79040, train_perplexity=1152.9458, train_loss=7.0500755

Batch 79050, train_perplexity=1109.8562, train_loss=7.011986

Batch 79060, train_perplexity=1107.2434, train_loss=7.009629

Batch 79070, train_perplexity=1082.1704, train_loss=6.986724

Batch 79080, train_perplexity=1017.4944, train_loss=6.9250984

Batch 79090, train_perplexity=1110.015, train_loss=7.012129

Batch 79100, train_perplexity=1059.2595, train_loss=6.9653254

Batch 79110, train_perplexity=1027.3943, train_loss=6.934781

Batch 79120, train_perplexity=1154.9895, train_loss=7.0518465

Batch 79130, train_perplexity=1121.1674, train_loss=7.0221257

Batch 79140, train_perplexity=1132.015, train_loss=7.0317545

Batch 79150, train_perplexity=1074.9178, train_loss=6.9799995

Batch 79160, train_perplexity=1067.9976, train_loss=6.973541

Batch 79170, train_perplexity=1134.1285, train_loss=7.03362

Batch 79180, train_perplexity=1094.1652, train_loss=6.997747

Batch 79190, train_perplexity=988.8188, train_loss=6.896511

Batch 79200, train_perplexity=1015.206, train_loss=6.922847

Batch 79210, train_perplexity=1140.3507, train_loss=7.039091

Batch 79220, train_perplexity=1101.3721, train_loss=7.004312

Batch 79230, train_perplexity=1101.1553, train_loss=7.004115

Batch 79240, train_perplexity=1108.1884, train_loss=7.010482

Batch 79250, train_perplexity=1028.1471, train_loss=6.9355135

Batch 79260, train_perplexity=1005.30414, train_loss=6.9130454

Batch 79270, train_perplexity=1105.6516, train_loss=7.00819

Batch 79280, train_perplexity=1128.5284, train_loss=7.02867

Batch 79290, train_perplexity=1033.2032, train_loss=6.940419

Batch 79300, train_perplexity=1184.5654, train_loss=7.0771313

Batch 79310, train_perplexity=1130.4733, train_loss=7.0303917

Batch 79320, train_perplexity=1185.04, train_loss=7.077532

Batch 79330, train_perplexity=1046.042, train_loss=6.952769

Batch 79340, train_perplexity=1099.0978, train_loss=7.002245

Batch 79350, train_perplexity=1014.7694, train_loss=6.9224167

Batch 79360, train_perplexity=1018.1302, train_loss=6.925723

Batch 79370, train_perplexity=1134.1719, train_loss=7.033658

Batch 79380, train_perplexity=1149.3461, train_loss=7.0469484

Batch 79390, train_perplexity=1096.9569, train_loss=7.000295

Batch 79400, train_perplexity=1096.4177, train_loss=6.9998035

Batch 79410, train_perplexity=1145.604, train_loss=7.0436873

Batch 79420, train_perplexity=1197.8826, train_loss=7.0883107

Batch 79430, train_perplexity=1039.8812, train_loss=6.9468617

Batch 79440, train_perplexity=1062.5598, train_loss=6.9684362

Batch 79450, train_perplexity=1074.697, train_loss=6.979794

Batch 79460, train_perplexity=1170.7491, train_loss=7.065399

Batch 79470, train_perplexity=1123.0267, train_loss=7.0237827

Batch 79480, train_perplexity=1059.3474, train_loss=6.9654083

Batch 79490, train_perplexity=1160.1913, train_loss=7.05634

Batch 79500, train_perplexity=1154.1416, train_loss=7.051112

Batch 79510, train_perplexity=1123.5634, train_loss=7.0242605

Batch 79520, train_perplexity=1169.7805, train_loss=7.0645714

Batch 79530, train_perplexity=1096.5955, train_loss=6.9999657

Batch 79540, train_perplexity=1127.4984, train_loss=7.0277567

Batch 79550, train_perplexity=1007.90375, train_loss=6.915628

Batch 79560, train_perplexity=1079.7272, train_loss=6.9844637

Batch 79570, train_perplexity=1087.049, train_loss=6.991222

Batch 79580, train_perplexity=1096.6364, train_loss=7.000003

Batch 79590, train_perplexity=1063.2471, train_loss=6.969083

Batch 79600, train_perplexity=994.1991, train_loss=6.9019375

Batch 79610, train_perplexity=1134.9866, train_loss=7.034376

Batch 79620, train_perplexity=1183.3341, train_loss=7.0760913

Batch 79630, train_perplexity=1142.8253, train_loss=7.041259

Batch 79640, train_perplexity=1087.569, train_loss=6.9917

Batch 79650, train_perplexity=1145.8483, train_loss=7.0439005

Batch 79660, train_perplexity=1020.48456, train_loss=6.928033

Batch 79670, train_perplexity=1207.1896, train_loss=7.0960503

Batch 79680, train_perplexity=1089.0593, train_loss=6.9930696

Batch 79690, train_perplexity=1139.9745, train_loss=7.038761

Batch 79700, train_perplexity=1125.9033, train_loss=7.026341

Batch 79710, train_perplexity=1097.0332, train_loss=7.000365

Batch 79720, train_perplexity=1114.6501, train_loss=7.016296

Batch 79730, train_perplexity=1082.51, train_loss=6.9870377

Batch 79740, train_perplexity=1092.7267, train_loss=6.9964314

Batch 79750, train_perplexity=1110.8008, train_loss=7.0128365

Batch 79760, train_perplexity=1194.6511, train_loss=7.0856094
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 79770, train_perplexity=1198.2407, train_loss=7.0886097

Batch 79780, train_perplexity=1069.32, train_loss=6.974778

Batch 79790, train_perplexity=1234.3649, train_loss=7.118312

Batch 79800, train_perplexity=1124.6414, train_loss=7.0252194

Batch 79810, train_perplexity=1111.1218, train_loss=7.0131254

Batch 79820, train_perplexity=1071.0944, train_loss=6.976436

Batch 79830, train_perplexity=1155.1239, train_loss=7.051963

Batch 79840, train_perplexity=1099.8853, train_loss=7.002961

Batch 79850, train_perplexity=1146.9076, train_loss=7.0448246

Batch 79860, train_perplexity=988.0256, train_loss=6.8957086

Batch 79870, train_perplexity=1131.865, train_loss=7.031622

Batch 79880, train_perplexity=1049.5159, train_loss=6.9560843

Batch 79890, train_perplexity=1082.4531, train_loss=6.986985

Batch 79900, train_perplexity=1106.4739, train_loss=7.0089335

Batch 79910, train_perplexity=1173.0162, train_loss=7.0673337

Batch 79920, train_perplexity=1163.9705, train_loss=7.0595922

Batch 79930, train_perplexity=1077.3743, train_loss=6.982282

Batch 79940, train_perplexity=1095.7184, train_loss=6.9991655

Batch 79950, train_perplexity=1167.5046, train_loss=7.062624

Batch 79960, train_perplexity=1131.6923, train_loss=7.0314693

Batch 79970, train_perplexity=1131.866, train_loss=7.031623

Batch 79980, train_perplexity=1117.727, train_loss=7.0190525

Batch 79990, train_perplexity=1207.5459, train_loss=7.0963454

Batch 80000, train_perplexity=1044.4371, train_loss=6.9512334

Batch 80010, train_perplexity=1066.5752, train_loss=6.972208

Batch 80020, train_perplexity=1041.1334, train_loss=6.9480653

Batch 80030, train_perplexity=1098.6545, train_loss=7.0018415

Batch 80040, train_perplexity=1103.1244, train_loss=7.005902

Batch 80050, train_perplexity=1161.9813, train_loss=7.057882

Batch 80060, train_perplexity=1128.273, train_loss=7.0284433

Batch 80070, train_perplexity=1158.4484, train_loss=7.0548368

Batch 80080, train_perplexity=985.71643, train_loss=6.8933687

Batch 80090, train_perplexity=1107.8307, train_loss=7.010159

Batch 80100, train_perplexity=1087.1936, train_loss=6.991355

Batch 80110, train_perplexity=1119.8728, train_loss=7.0209703

Batch 80120, train_perplexity=1055.625, train_loss=6.9618883

Batch 80130, train_perplexity=1187.4219, train_loss=7.07954

Batch 80140, train_perplexity=1040.9215, train_loss=6.9478617

Batch 80150, train_perplexity=1129.8077, train_loss=7.029803

Batch 80160, train_perplexity=982.08325, train_loss=6.889676

Batch 80170, train_perplexity=996.7139, train_loss=6.904464

Batch 80180, train_perplexity=1065.4604, train_loss=6.9711623

Batch 80190, train_perplexity=995.67596, train_loss=6.903422

Batch 80200, train_perplexity=1037.2787, train_loss=6.944356

Batch 80210, train_perplexity=1108.3331, train_loss=7.0106125

Batch 80220, train_perplexity=1210.7101, train_loss=7.0989623

Batch 80230, train_perplexity=1083.3568, train_loss=6.9878197

Batch 80240, train_perplexity=1184.6056, train_loss=7.077165

Batch 80250, train_perplexity=1099.5098, train_loss=7.0026197

Batch 80260, train_perplexity=1119.1179, train_loss=7.020296

Batch 80270, train_perplexity=1151.4635, train_loss=7.048789

Batch 80280, train_perplexity=954.1171, train_loss=6.8607864

Batch 80290, train_perplexity=1189.4558, train_loss=7.081251

Batch 80300, train_perplexity=1003.1852, train_loss=6.9109354

Batch 80310, train_perplexity=1032.0983, train_loss=6.939349

Batch 80320, train_perplexity=1086.2122, train_loss=6.990452

Batch 80330, train_perplexity=1102.3904, train_loss=7.005236

Batch 80340, train_perplexity=1168.2097, train_loss=7.0632277

Batch 80350, train_perplexity=1124.1705, train_loss=7.024801

Batch 80360, train_perplexity=1114.6316, train_loss=7.016279

Batch 80370, train_perplexity=1095.4635, train_loss=6.998933

Batch 80380, train_perplexity=1138.568, train_loss=7.0375266

Batch 80390, train_perplexity=1060.158, train_loss=6.966173

Batch 80400, train_perplexity=1108.173, train_loss=7.010468

Batch 80410, train_perplexity=1093.9742, train_loss=6.9975724

Batch 80420, train_perplexity=1157.5361, train_loss=7.054049

Batch 80430, train_perplexity=1102.28, train_loss=7.005136

Batch 80440, train_perplexity=1099.6057, train_loss=7.002707

Batch 80450, train_perplexity=1297.1921, train_loss=7.1679573

Batch 80460, train_perplexity=1062.4382, train_loss=6.968322

Batch 80470, train_perplexity=982.82904, train_loss=6.890435

Batch 80480, train_perplexity=1116.5232, train_loss=7.017975

Batch 80490, train_perplexity=1106.5593, train_loss=7.009011

Batch 80500, train_perplexity=1010.6585, train_loss=6.9183574

Batch 80510, train_perplexity=1122.3683, train_loss=7.023196

Batch 80520, train_perplexity=1035.5668, train_loss=6.942704

Batch 80530, train_perplexity=1036.986, train_loss=6.9440737

Batch 80540, train_perplexity=1190.3568, train_loss=7.0820084

Batch 80550, train_perplexity=1107.9442, train_loss=7.0102615

Batch 80560, train_perplexity=1029.2811, train_loss=6.936616

Batch 80570, train_perplexity=1052.3322, train_loss=6.958764

Batch 80580, train_perplexity=1113.3833, train_loss=7.0151587

Batch 80590, train_perplexity=1042.7158, train_loss=6.949584

Batch 80600, train_perplexity=1167.68, train_loss=7.062774

Batch 80610, train_perplexity=1237.6866, train_loss=7.1209993

Batch 80620, train_perplexity=1232.4023, train_loss=7.1167207

Batch 80630, train_perplexity=1083.3904, train_loss=6.9878507

Batch 80640, train_perplexity=1122.3308, train_loss=7.023163

Batch 80650, train_perplexity=1019.123, train_loss=6.9266977

Batch 80660, train_perplexity=1122.1943, train_loss=7.0230412

Batch 80670, train_perplexity=1023.6954, train_loss=6.9311743

Batch 80680, train_perplexity=1183.117, train_loss=7.0759077

Batch 80690, train_perplexity=1079.9147, train_loss=6.9846373

Batch 80700, train_perplexity=1130.4496, train_loss=7.0303707

Batch 80710, train_perplexity=1063.0605, train_loss=6.9689074

Batch 80720, train_perplexity=1065.498, train_loss=6.9711976

Batch 80730, train_perplexity=1114.791, train_loss=7.0164223

Batch 80740, train_perplexity=1133.6658, train_loss=7.0332117

Batch 80750, train_perplexity=1168.706, train_loss=7.0636525

Batch 80760, train_perplexity=1103.7773, train_loss=7.0064936

Batch 80770, train_perplexity=1079.6016, train_loss=6.9843473

Batch 80780, train_perplexity=1010.7515, train_loss=6.9184494

Batch 80790, train_perplexity=1079.9166, train_loss=6.984639

Batch 80800, train_perplexity=1084.0787, train_loss=6.988486

Batch 80810, train_perplexity=1090.1941, train_loss=6.994111

Batch 80820, train_perplexity=1099.5303, train_loss=7.0026383

Batch 80830, train_perplexity=1060.6737, train_loss=6.9666595

Batch 80840, train_perplexity=986.3855, train_loss=6.8940473

Batch 80850, train_perplexity=1123.7981, train_loss=7.0244694

Batch 80860, train_perplexity=1015.8519, train_loss=6.923483

Batch 80870, train_perplexity=995.6973, train_loss=6.9034433

Batch 80880, train_perplexity=1148.0621, train_loss=7.0458307

Batch 80890, train_perplexity=1053.0354, train_loss=6.959432

Batch 80900, train_perplexity=1066.9302, train_loss=6.972541

Batch 80910, train_perplexity=1152.8931, train_loss=7.0500298

Batch 80920, train_perplexity=1045.7043, train_loss=6.952446

Batch 80930, train_perplexity=1061.1583, train_loss=6.9671164

Batch 80940, train_perplexity=1143.8557, train_loss=7.04216

Batch 80950, train_perplexity=1131.115, train_loss=7.030959

Batch 80960, train_perplexity=1008.74805, train_loss=6.9164653

Batch 80970, train_perplexity=1046.4446, train_loss=6.9531536

Batch 80980, train_perplexity=1031.6539, train_loss=6.9389186

Batch 80990, train_perplexity=1111.2341, train_loss=7.0132265

Batch 81000, train_perplexity=1029.9818, train_loss=6.9372964

Batch 81010, train_perplexity=999.38184, train_loss=6.907137

Batch 81020, train_perplexity=1199.3875, train_loss=7.089566

Batch 81030, train_perplexity=1035.5066, train_loss=6.942646

Batch 81040, train_perplexity=1073.0139, train_loss=6.9782267

Batch 81050, train_perplexity=1086.6748, train_loss=6.9908776

Batch 81060, train_perplexity=939.0007, train_loss=6.844816

Batch 81070, train_perplexity=1025.4557, train_loss=6.9328923

Batch 81080, train_perplexity=1157.3744, train_loss=7.0539093
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 81090, train_perplexity=986.6635, train_loss=6.894329

Batch 81100, train_perplexity=1102.894, train_loss=7.005693

Batch 81110, train_perplexity=1215.1195, train_loss=7.1025977

Batch 81120, train_perplexity=1009.48956, train_loss=6.9172

Batch 81130, train_perplexity=1068.8331, train_loss=6.974323

Batch 81140, train_perplexity=1054.2251, train_loss=6.9605613

Batch 81150, train_perplexity=1068.2563, train_loss=6.973783

Batch 81160, train_perplexity=1166.4496, train_loss=7.06172

Batch 81170, train_perplexity=1173.1365, train_loss=7.067436

Batch 81180, train_perplexity=1112.6514, train_loss=7.014501

Batch 81190, train_perplexity=1142.3197, train_loss=7.0408163

Batch 81200, train_perplexity=1052.2614, train_loss=6.958697

Batch 81210, train_perplexity=1064.4149, train_loss=6.9701805

Batch 81220, train_perplexity=1129.2095, train_loss=7.029273

Batch 81230, train_perplexity=1079.355, train_loss=6.984119

Batch 81240, train_perplexity=1052.5651, train_loss=6.9589853

Batch 81250, train_perplexity=1124.4628, train_loss=7.0250607

Batch 81260, train_perplexity=1174.1561, train_loss=7.068305

Batch 81270, train_perplexity=1085.9552, train_loss=6.9902153

Batch 81280, train_perplexity=1136.7479, train_loss=7.035927

Batch 81290, train_perplexity=1126.9609, train_loss=7.02728

Batch 81300, train_perplexity=1049.7542, train_loss=6.956311

Batch 81310, train_perplexity=1032.4625, train_loss=6.939702

Batch 81320, train_perplexity=1103.6979, train_loss=7.0064216

Batch 81330, train_perplexity=1112.3798, train_loss=7.014257

Batch 81340, train_perplexity=1212.9978, train_loss=7.10085

Batch 81350, train_perplexity=1039.1847, train_loss=6.946192

Batch 81360, train_perplexity=1058.3179, train_loss=6.964436

Batch 81370, train_perplexity=1160.0342, train_loss=7.056205

Batch 81380, train_perplexity=1118.8789, train_loss=7.0200825

Batch 81390, train_perplexity=1022.92395, train_loss=6.9304204

Batch 81400, train_perplexity=1048.559, train_loss=6.955172

Batch 81410, train_perplexity=987.26166, train_loss=6.894935

Batch 81420, train_perplexity=982.08044, train_loss=6.889673

Batch 81430, train_perplexity=1119.4841, train_loss=7.020623

Batch 81440, train_perplexity=1037.7146, train_loss=6.944776

Batch 81450, train_perplexity=1054.1724, train_loss=6.960511

Batch 81460, train_perplexity=1173.9104, train_loss=7.0680957

Batch 81470, train_perplexity=966.9377, train_loss=6.874134

Batch 81480, train_perplexity=1284.7819, train_loss=7.1583443

Batch 81490, train_perplexity=1085.1581, train_loss=6.989481

Batch 81500, train_perplexity=1096.9679, train_loss=7.000305

Batch 81510, train_perplexity=1154.5406, train_loss=7.051458

Batch 81520, train_perplexity=1015.32263, train_loss=6.9229617

Batch 81530, train_perplexity=1123.9717, train_loss=7.024624

Batch 81540, train_perplexity=1103.4548, train_loss=7.0062013

Batch 81550, train_perplexity=1072.1086, train_loss=6.9773827

Batch 81560, train_perplexity=1068.8296, train_loss=6.9743195

Batch 81570, train_perplexity=1095.5815, train_loss=6.9990406

Batch 81580, train_perplexity=1024.5167, train_loss=6.9319763

Batch 81590, train_perplexity=1140.4828, train_loss=7.039207

Batch 81600, train_perplexity=1228.1088, train_loss=7.1132307

Batch 81610, train_perplexity=1068.1459, train_loss=6.9736795

Batch 81620, train_perplexity=1193.3097, train_loss=7.084486

Batch 81630, train_perplexity=1140.2593, train_loss=7.039011

Batch 81640, train_perplexity=1115.6147, train_loss=7.017161

Batch 81650, train_perplexity=994.7596, train_loss=6.902501

Batch 81660, train_perplexity=1159.8855, train_loss=7.0560765

Batch 81670, train_perplexity=1047.515, train_loss=6.954176

Batch 81680, train_perplexity=1158.7434, train_loss=7.0550914

Batch 81690, train_perplexity=1051.0784, train_loss=6.957572

Batch 81700, train_perplexity=1051.4113, train_loss=6.9578886

Batch 81710, train_perplexity=1012.9111, train_loss=6.9205837

Batch 81720, train_perplexity=1011.88763, train_loss=6.919573

Batch 81730, train_perplexity=1111.6846, train_loss=7.013632

Batch 81740, train_perplexity=1085.7988, train_loss=6.9900713

Batch 81750, train_perplexity=1077.9298, train_loss=6.9827976

Batch 81760, train_perplexity=1018.9597, train_loss=6.9265375

Batch 81770, train_perplexity=1081.1821, train_loss=6.9858103

Batch 81780, train_perplexity=990.41187, train_loss=6.898121

Batch 81790, train_perplexity=1072.0718, train_loss=6.9773483

Batch 81800, train_perplexity=1119.8279, train_loss=7.0209303

Batch 81810, train_perplexity=1125.8438, train_loss=7.026288

Batch 81820, train_perplexity=1068.9998, train_loss=6.9744787

Batch 81830, train_perplexity=1048.4275, train_loss=6.9550467

Batch 81840, train_perplexity=1043.3933, train_loss=6.9502335

Batch 81850, train_perplexity=1128.1718, train_loss=7.0283537

Batch 81860, train_perplexity=1108.2782, train_loss=7.010563

Batch 81870, train_perplexity=1247.2131, train_loss=7.128667

Batch 81880, train_perplexity=1147.1254, train_loss=7.0450144

Batch 81890, train_perplexity=1042.2336, train_loss=6.9491215

Batch 81900, train_perplexity=1048.4569, train_loss=6.955075

Batch 81910, train_perplexity=1005.72943, train_loss=6.9134684

Batch 81920, train_perplexity=1082.4475, train_loss=6.98698

Batch 81930, train_perplexity=1073.3112, train_loss=6.9785037

Batch 81940, train_perplexity=1008.8813, train_loss=6.9165974

Batch 81950, train_perplexity=1096.4198, train_loss=6.9998055

Batch 81960, train_perplexity=1096.6316, train_loss=6.9999986

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Batch 81970, train_perplexity=1008.7581, train_loss=6.9164753

Batch 81980, train_perplexity=1137.1215, train_loss=7.0362554

Batch 81990, train_perplexity=1187.5804, train_loss=7.0796733

Batch 82000, train_perplexity=1063.9912, train_loss=6.9697824

Batch 82010, train_perplexity=1064.7788, train_loss=6.9705224

Batch 82020, train_perplexity=1107.5776, train_loss=7.0099306

Batch 82030, train_perplexity=1140.2104, train_loss=7.038968

Batch 82040, train_perplexity=1107.4203, train_loss=7.0097885

Batch 82050, train_perplexity=1096.5181, train_loss=6.999895

Batch 82060, train_perplexity=1069.9585, train_loss=6.975375

Batch 82070, train_perplexity=1036.8524, train_loss=6.943945

Batch 82080, train_perplexity=1016.33936, train_loss=6.9239626

Batch 82090, train_perplexity=1163.6019, train_loss=7.0592756

Batch 82100, train_perplexity=1038.8805, train_loss=6.945899

Batch 82110, train_perplexity=1062.2742, train_loss=6.9681673

Batch 82120, train_perplexity=1127.8351, train_loss=7.028055

Batch 82130, train_perplexity=1102.1438, train_loss=7.0050125

Batch 82140, train_perplexity=1138.8324, train_loss=7.037759

Batch 82150, train_perplexity=1134.9856, train_loss=7.034375

Batch 82160, train_perplexity=1017.6138, train_loss=6.9252157

Batch 82170, train_perplexity=1055.6622, train_loss=6.9619236

Batch 82180, train_perplexity=1147.3923, train_loss=7.045247

Batch 82190, train_perplexity=1053.7874, train_loss=6.960146

Batch 82200, train_perplexity=1034.5382, train_loss=6.9417105

Batch 82210, train_perplexity=1005.7846, train_loss=6.913523

Batch 82220, train_perplexity=1095.1376, train_loss=6.9986353

Batch 82230, train_perplexity=1020.8311, train_loss=6.9283724

Batch 82240, train_perplexity=1083.2742, train_loss=6.9877434

Batch 82250, train_perplexity=1034.8126, train_loss=6.9419756

Batch 82260, train_perplexity=1102.9125, train_loss=7.0057096

Batch 82270, train_perplexity=1025.0278, train_loss=6.932475

Batch 82280, train_perplexity=1178.028, train_loss=7.071597

Batch 82290, train_perplexity=1162.3187, train_loss=7.058172

Batch 82300, train_perplexity=1189.4818, train_loss=7.081273

Batch 82310, train_perplexity=1097.7863, train_loss=7.001051

Batch 82320, train_perplexity=1015.6011, train_loss=6.923236

Batch 82330, train_perplexity=1016.1819, train_loss=6.9238076
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82340, train_perplexity=1104.713, train_loss=7.007341

Batch 82350, train_perplexity=1111.0657, train_loss=7.013075

Batch 82360, train_perplexity=1108.1276, train_loss=7.010427

Batch 82370, train_perplexity=1089.8895, train_loss=6.9938316

Batch 82380, train_perplexity=1138.777, train_loss=7.03771

Batch 82390, train_perplexity=1052.5409, train_loss=6.9589624

Batch 82400, train_perplexity=1057.8542, train_loss=6.963998

Batch 82410, train_perplexity=1119.8909, train_loss=7.0209866

Batch 82420, train_perplexity=988.16174, train_loss=6.8958464

Batch 82430, train_perplexity=1012.3196, train_loss=6.9199996

Batch 82440, train_perplexity=986.45654, train_loss=6.8941193

Batch 82450, train_perplexity=1211.1229, train_loss=7.0993032

Batch 82460, train_perplexity=1083.0304, train_loss=6.9875183

Batch 82470, train_perplexity=1069.4097, train_loss=6.974862

Batch 82480, train_perplexity=1038.6101, train_loss=6.9456387

Batch 82490, train_perplexity=1161.7708, train_loss=7.0577006

Batch 82500, train_perplexity=1066.144, train_loss=6.9718037

Batch 82510, train_perplexity=1104.5914, train_loss=7.0072308

Batch 82520, train_perplexity=1107.753, train_loss=7.010089

Batch 82530, train_perplexity=1106.5546, train_loss=7.0090065

Batch 82540, train_perplexity=1058.1227, train_loss=6.9642515

Batch 82550, train_perplexity=1139.8896, train_loss=7.0386868

Batch 82560, train_perplexity=1105.2131, train_loss=7.0077934

Batch 82570, train_perplexity=1096.4408, train_loss=6.9998245

Batch 82580, train_perplexity=1046.1024, train_loss=6.9528265

Batch 82590, train_perplexity=1038.0659, train_loss=6.9451146

Batch 82600, train_perplexity=1040.8163, train_loss=6.9477606

Batch 82610, train_perplexity=1186.6776, train_loss=7.0789127

Batch 82620, train_perplexity=1078.6836, train_loss=6.9834967

Batch 82630, train_perplexity=1071.0294, train_loss=6.9763756

Batch 82640, train_perplexity=1080.4395, train_loss=6.985123

Batch 82650, train_perplexity=1013.4536, train_loss=6.921119

Batch 82660, train_perplexity=1165.4656, train_loss=7.060876

Batch 82670, train_perplexity=1172.1854, train_loss=7.066625

Batch 82680, train_perplexity=1145.5106, train_loss=7.043606

Batch 82690, train_perplexity=1050.3204, train_loss=6.9568505

Batch 82700, train_perplexity=1135.8865, train_loss=7.0351686

Batch 82710, train_perplexity=1132.0355, train_loss=7.0317726

Batch 82720, train_perplexity=1202.81, train_loss=7.092416

Batch 82730, train_perplexity=1020.3255, train_loss=6.927877

Batch 82740, train_perplexity=1070.3943, train_loss=6.9757824

Batch 82750, train_perplexity=1127.1146, train_loss=7.027416

Batch 82760, train_perplexity=1258.7262, train_loss=7.1378555

Batch 82770, train_perplexity=1003.30145, train_loss=6.9110513

Batch 82780, train_perplexity=1047.0037, train_loss=6.9536877

Batch 82790, train_perplexity=1124.0532, train_loss=7.0246964

Batch 82800, train_perplexity=1137.397, train_loss=7.0364976

Batch 82810, train_perplexity=1103.809, train_loss=7.006522

Batch 82820, train_perplexity=1077.6913, train_loss=6.9825764

Batch 82830, train_perplexity=999.2632, train_loss=6.907018

Batch 82840, train_perplexity=1079.6566, train_loss=6.9843984

Batch 82850, train_perplexity=1148.7522, train_loss=7.0464315

Batch 82860, train_perplexity=1074.1647, train_loss=6.9792986

Batch 82870, train_perplexity=1080.5018, train_loss=6.985181

Batch 82880, train_perplexity=1029.9906, train_loss=6.937305

Batch 82890, train_perplexity=1172.2463, train_loss=7.066677

Batch 82900, train_perplexity=1056.4105, train_loss=6.962632

Batch 82910, train_perplexity=1099.7347, train_loss=7.0028243

Batch 82920, train_perplexity=1068.6309, train_loss=6.9741335

Batch 82930, train_perplexity=1024.5275, train_loss=6.931987

Batch 82940, train_perplexity=1073.7919, train_loss=6.9789515

Batch 82950, train_perplexity=1190.0355, train_loss=7.0817385

Batch 82960, train_perplexity=1124.9851, train_loss=7.025525

Batch 82970, train_perplexity=1108.977, train_loss=7.0111933

Batch 82980, train_perplexity=1080.7012, train_loss=6.9853654

Batch 82990, train_perplexity=1115.726, train_loss=7.0172606

Batch 83000, train_perplexity=1170.1548, train_loss=7.0648913

Batch 83010, train_perplexity=1078.7587, train_loss=6.9835663

Batch 83020, train_perplexity=1105.1224, train_loss=7.0077114

Batch 83030, train_perplexity=1025.6478, train_loss=6.9330797

Batch 83040, train_perplexity=1087.7432, train_loss=6.9918604

Batch 83050, train_perplexity=1079.7766, train_loss=6.9845095

Batch 83060, train_perplexity=1148.3983, train_loss=7.0461235

Batch 83070, train_perplexity=1082.2101, train_loss=6.9867606

Batch 83080, train_perplexity=1051.0122, train_loss=6.957509

Batch 83090, train_perplexity=1052.3291, train_loss=6.958761

Batch 83100, train_perplexity=1151.4542, train_loss=7.048781

Batch 83110, train_perplexity=1164.4917, train_loss=7.06004

Batch 83120, train_perplexity=1147.1172, train_loss=7.045007

Batch 83130, train_perplexity=1016.62244, train_loss=6.924241

Batch 83140, train_perplexity=1045.5548, train_loss=6.952303

Batch 83150, train_perplexity=1066.4867, train_loss=6.972125

Batch 83160, train_perplexity=1158.4952, train_loss=7.0548773

Batch 83170, train_perplexity=1007.7649, train_loss=6.91549

Batch 83180, train_perplexity=1018.5089, train_loss=6.926095

Batch 83190, train_perplexity=1068.2946, train_loss=6.973819

Batch 83200, train_perplexity=1179.9794, train_loss=7.073252

Batch 83210, train_perplexity=1065.6459, train_loss=6.9713364

Batch 83220, train_perplexity=1108.023, train_loss=7.0103326

Batch 83230, train_perplexity=1272.1974, train_loss=7.148501

Batch 83240, train_perplexity=999.9567, train_loss=6.907712

Batch 83250, train_perplexity=1156.2172, train_loss=7.052909

Batch 83260, train_perplexity=1149.4376, train_loss=7.047028

Batch 83270, train_perplexity=962.91266, train_loss=6.8699627

Batch 83280, train_perplexity=1043.6919, train_loss=6.9505196

Batch 83290, train_perplexity=1068.1774, train_loss=6.973709

Batch 83300, train_perplexity=1247.0323, train_loss=7.128522

Batch 83310, train_perplexity=991.842, train_loss=6.899564

Batch 83320, train_perplexity=1173.7498, train_loss=7.067959

Batch 83330, train_perplexity=1104.5303, train_loss=7.0071754

Batch 83340, train_perplexity=1181.3552, train_loss=7.0744176

Batch 83350, train_perplexity=1183.3618, train_loss=7.0761147

Batch 83360, train_perplexity=1038.8547, train_loss=6.945874

Batch 83370, train_perplexity=1162.5128, train_loss=7.058339

Batch 83380, train_perplexity=1066.9526, train_loss=6.972562

Batch 83390, train_perplexity=1132.7855, train_loss=7.032435

Batch 83400, train_perplexity=1041.1161, train_loss=6.9480486

Batch 83410, train_perplexity=1121.5984, train_loss=7.02251

Batch 83420, train_perplexity=1120.4891, train_loss=7.0215206

Batch 83430, train_perplexity=1050.2598, train_loss=6.956793

Batch 83440, train_perplexity=1042.7805, train_loss=6.949646

Batch 83450, train_perplexity=1048.9375, train_loss=6.955533

Batch 83460, train_perplexity=1068.6644, train_loss=6.974165

Batch 83470, train_perplexity=1150.7192, train_loss=7.0481424

Batch 83480, train_perplexity=990.47797, train_loss=6.8981876

Batch 83490, train_perplexity=1013.6813, train_loss=6.921344

Batch 83500, train_perplexity=1106.5989, train_loss=7.0090466

Batch 83510, train_perplexity=1098.7991, train_loss=7.001973

Batch 83520, train_perplexity=1045.0234, train_loss=6.9517946

Batch 83530, train_perplexity=1133.5582, train_loss=7.033117

Batch 83540, train_perplexity=1082.9839, train_loss=6.9874754

Batch 83550, train_perplexity=1112.7018, train_loss=7.0145464

Batch 83560, train_perplexity=1131.3453, train_loss=7.0311627

Batch 83570, train_perplexity=1146.1444, train_loss=7.044159

Batch 83580, train_perplexity=1059.1125, train_loss=6.9651866

Batch 83590, train_perplexity=1116.8651, train_loss=7.018281

Batch 83600, train_perplexity=1087.9476, train_loss=6.9920483

Batch 83610, train_perplexity=1070.789, train_loss=6.976151

Batch 83620, train_perplexity=1140.5986, train_loss=7.0393085

Batch 83630, train_perplexity=1105.387, train_loss=7.007951

Batch 83640, train_perplexity=1063.7081, train_loss=6.9695163

Batch 83650, train_perplexity=1166.7378, train_loss=7.061967
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 83660, train_perplexity=1018.831, train_loss=6.926411

Batch 83670, train_perplexity=1176.325, train_loss=7.0701504

Batch 83680, train_perplexity=1130.4862, train_loss=7.030403

Batch 83690, train_perplexity=1096.9752, train_loss=7.000312

Batch 83700, train_perplexity=1073.5128, train_loss=6.9786916

Batch 83710, train_perplexity=1042.188, train_loss=6.9490776

Batch 83720, train_perplexity=1144.9001, train_loss=7.0430727

Batch 83730, train_perplexity=1043.9586, train_loss=6.950775

Batch 83740, train_perplexity=1136.4911, train_loss=7.035701

Batch 83750, train_perplexity=1091.2802, train_loss=6.9951067

Batch 83760, train_perplexity=1037.2521, train_loss=6.94433

Batch 83770, train_perplexity=1021.7574, train_loss=6.9292793

Batch 83780, train_perplexity=937.055, train_loss=6.842742

Batch 83790, train_perplexity=1093.8579, train_loss=6.997466

Batch 83800, train_perplexity=1032.4832, train_loss=6.939722

Batch 83810, train_perplexity=1044.4336, train_loss=6.95123

Batch 83820, train_perplexity=1041.2422, train_loss=6.9481697

Batch 83830, train_perplexity=1124.718, train_loss=7.0252876

Batch 83840, train_perplexity=1039.4375, train_loss=6.946435

Batch 83850, train_perplexity=1150.4624, train_loss=7.0479193

Batch 83860, train_perplexity=1069.1436, train_loss=6.974613

Batch 83870, train_perplexity=1130.91, train_loss=7.030778

Batch 83880, train_perplexity=1044.2534, train_loss=6.9510574

Batch 83890, train_perplexity=1115.1898, train_loss=7.01678

Batch 83900, train_perplexity=1109.3292, train_loss=7.011511

Batch 83910, train_perplexity=1105.863, train_loss=7.0083814

Batch 83920, train_perplexity=1051.653, train_loss=6.9581184

Batch 83930, train_perplexity=1068.7021, train_loss=6.9742002

Batch 83940, train_perplexity=992.0113, train_loss=6.8997345

Batch 83950, train_perplexity=1142.8286, train_loss=7.0412617

Batch 83960, train_perplexity=1110.7563, train_loss=7.0127964

Batch 83970, train_perplexity=1005.1551, train_loss=6.912897

Batch 83980, train_perplexity=1111.859, train_loss=7.0137887

Batch 83990, train_perplexity=1118.6943, train_loss=7.0199175

Batch 84000, train_perplexity=1052.6423, train_loss=6.959059

Batch 84010, train_perplexity=1039.206, train_loss=6.9462123

Batch 84020, train_perplexity=1001.73395, train_loss=6.9094877

Batch 84030, train_perplexity=1218.3058, train_loss=7.1052165

Batch 84040, train_perplexity=1125.9662, train_loss=7.0263968

Batch 84050, train_perplexity=1068.5402, train_loss=6.9740486

Batch 84060, train_perplexity=1119.3602, train_loss=7.0205126

Batch 84070, train_perplexity=1022.36365, train_loss=6.9298725

Batch 84080, train_perplexity=1122.3093, train_loss=7.023144

Batch 84090, train_perplexity=1034.5521, train_loss=6.941724

Batch 84100, train_perplexity=1018.0117, train_loss=6.9256067

Batch 84110, train_perplexity=1098.9154, train_loss=7.002079

Batch 84120, train_perplexity=1015.6335, train_loss=6.923268

Batch 84130, train_perplexity=1098.5912, train_loss=7.001784

Batch 84140, train_perplexity=1126.0461, train_loss=7.026468

Batch 84150, train_perplexity=1114.181, train_loss=7.015875

Batch 84160, train_perplexity=1010.18823, train_loss=6.917892

Batch 84170, train_perplexity=1121.6107, train_loss=7.022521

Batch 84180, train_perplexity=1050.6534, train_loss=6.9571676

Batch 84190, train_perplexity=1084.7577, train_loss=6.989112

Batch 84200, train_perplexity=1046.789, train_loss=6.9534826

Batch 84210, train_perplexity=1083.6285, train_loss=6.9880705

Batch 84220, train_perplexity=1196.4137, train_loss=7.087084

Batch 84230, train_perplexity=1152.8436, train_loss=7.049987

Batch 84240, train_perplexity=1042.3987, train_loss=6.94928

Batch 84250, train_perplexity=1017.2014, train_loss=6.9248104

Batch 84260, train_perplexity=1057.8805, train_loss=6.9640226

Batch 84270, train_perplexity=1099.0029, train_loss=7.0021586

Batch 84280, train_perplexity=1059.2706, train_loss=6.965336

Batch 84290, train_perplexity=1086.694, train_loss=6.9908953

Batch 84300, train_perplexity=1136.0803, train_loss=7.0353394

Batch 84310, train_perplexity=1089.583, train_loss=6.9935503

Batch 84320, train_perplexity=1133.3685, train_loss=7.0329494

Batch 84330, train_perplexity=1099.1188, train_loss=7.002264

Batch 84340, train_perplexity=1107.3168, train_loss=7.009695

Batch 84350, train_perplexity=1006.57477, train_loss=6.9143085

Batch 84360, train_perplexity=1087.4066, train_loss=6.991551

Batch 84370, train_perplexity=1071.8633, train_loss=6.977154

Batch 84380, train_perplexity=1147.412, train_loss=7.0452642

Batch 84390, train_perplexity=1122.3639, train_loss=7.0231924

Batch 84400, train_perplexity=1062.0675, train_loss=6.9679728

Batch 84410, train_perplexity=1124.4446, train_loss=7.0250444

Batch 84420, train_perplexity=1109.5239, train_loss=7.0116863

Batch 84430, train_perplexity=1085.4407, train_loss=6.9897413

Batch 84440, train_perplexity=1045.0304, train_loss=6.9518013

Batch 84450, train_perplexity=1155.9757, train_loss=7.0527

Batch 84460, train_perplexity=1184.9462, train_loss=7.0774527

Batch 84470, train_perplexity=1148.1443, train_loss=7.0459023

Batch 84480, train_perplexity=1117.9264, train_loss=7.019231

Batch 84490, train_perplexity=1091.2885, train_loss=6.9951143

Batch 84500, train_perplexity=1132.34, train_loss=7.0320415

Batch 84510, train_perplexity=1151.9896, train_loss=7.049246

Batch 84520, train_perplexity=1112.861, train_loss=7.0146894

Batch 84530, train_perplexity=1087.8428, train_loss=6.991952

Batch 84540, train_perplexity=1108.6312, train_loss=7.0108814

Batch 84550, train_perplexity=1020.3727, train_loss=6.927923

Batch 84560, train_perplexity=1121.9851, train_loss=7.022855

Batch 84570, train_perplexity=991.0586, train_loss=6.8987737

Batch 84580, train_perplexity=1078.7761, train_loss=6.9835825

Batch 84590, train_perplexity=1029.1987, train_loss=6.936536

Batch 84600, train_perplexity=1185.5521, train_loss=7.077964

Batch 84610, train_perplexity=1139.5332, train_loss=7.038374

Batch 84620, train_perplexity=1083.1068, train_loss=6.987589

Batch 84630, train_perplexity=1124.2906, train_loss=7.0249076

Batch 84640, train_perplexity=1094.5758, train_loss=6.998122

Batch 84650, train_perplexity=1063.8091, train_loss=6.969611

Batch 84660, train_perplexity=1130.607, train_loss=7.03051

Batch 84670, train_perplexity=921.0662, train_loss=6.825532

Batch 84680, train_perplexity=1104.109, train_loss=7.006794

Batch 84690, train_perplexity=1086.4204, train_loss=6.9906435

Batch 84700, train_perplexity=1096.7346, train_loss=7.0000925

Batch 84710, train_perplexity=1050.4226, train_loss=6.956948

Batch 84720, train_perplexity=989.43945, train_loss=6.8971386

Batch 84730, train_perplexity=1080.0269, train_loss=6.984741

Batch 84740, train_perplexity=1114.3945, train_loss=7.0160666

Batch 84750, train_perplexity=1106.9287, train_loss=7.0093446

Batch 84760, train_perplexity=1063.1154, train_loss=6.968959

Batch 84770, train_perplexity=1107.124, train_loss=7.009521

Batch 84780, train_perplexity=1195.8639, train_loss=7.086624

Batch 84790, train_perplexity=1135.5042, train_loss=7.034832

Batch 84800, train_perplexity=994.11426, train_loss=6.901852

Batch 84810, train_perplexity=1080.7728, train_loss=6.9854317

Batch 84820, train_perplexity=958.43164, train_loss=6.8652983

Batch 84830, train_perplexity=1130.4474, train_loss=7.030369

Batch 84840, train_perplexity=1057.7821, train_loss=6.9639297

Batch 84850, train_perplexity=1134.406, train_loss=7.0338645

Batch 84860, train_perplexity=1033.7887, train_loss=6.9409857

Batch 84870, train_perplexity=1113.6456, train_loss=7.015394

Batch 84880, train_perplexity=1009.9628, train_loss=6.917669

Batch 84890, train_perplexity=970.93604, train_loss=6.8782606

Batch 84900, train_perplexity=1068.4087, train_loss=6.9739256

Batch 84910, train_perplexity=1105.3016, train_loss=7.0078735

Batch 84920, train_perplexity=1223.6954, train_loss=7.1096306

Batch 84930, train_perplexity=1009.5526, train_loss=6.9172626

Batch 84940, train_perplexity=1135.9482, train_loss=7.035223

Batch 84950, train_perplexity=1157.0135, train_loss=7.0535975

Batch 84960, train_perplexity=957.74866, train_loss=6.8645854

Batch 84970, train_perplexity=1101.2162, train_loss=7.0041704
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 84980, train_perplexity=1143.7019, train_loss=7.0420256

Batch 84990, train_perplexity=1088.5505, train_loss=6.9926023

Batch 85000, train_perplexity=988.50714, train_loss=6.896196

Batch 85010, train_perplexity=1115.0781, train_loss=7.01668

Batch 85020, train_perplexity=1022.5913, train_loss=6.930095

Batch 85030, train_perplexity=1146.4594, train_loss=7.0444336

Batch 85040, train_perplexity=971.2847, train_loss=6.8786197

Batch 85050, train_perplexity=983.3231, train_loss=6.890938

Batch 85060, train_perplexity=1065.835, train_loss=6.9715137

Batch 85070, train_perplexity=1005.52515, train_loss=6.913265

Batch 85080, train_perplexity=970.11694, train_loss=6.8774166

Batch 85090, train_perplexity=1103.7896, train_loss=7.0065045

Batch 85100, train_perplexity=977.84406, train_loss=6.88535

Batch 85110, train_perplexity=1061.8842, train_loss=6.9678

Batch 85120, train_perplexity=1084.0824, train_loss=6.988489

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Batch 85130, train_perplexity=988.58966, train_loss=6.8962793

Batch 85140, train_perplexity=1246.8243, train_loss=7.128355

Batch 85150, train_perplexity=1061.0232, train_loss=6.966989

Batch 85160, train_perplexity=986.385, train_loss=6.894047

Batch 85170, train_perplexity=1107.2709, train_loss=7.0096536

Batch 85180, train_perplexity=1132.0463, train_loss=7.031782

Batch 85190, train_perplexity=1194.3344, train_loss=7.0853443

Batch 85200, train_perplexity=1039.0024, train_loss=6.9460163

Batch 85210, train_perplexity=1048.3015, train_loss=6.9549265

Batch 85220, train_perplexity=1071.6364, train_loss=6.976942

Batch 85230, train_perplexity=1110.9597, train_loss=7.0129795

Batch 85240, train_perplexity=1087.6364, train_loss=6.991762

Batch 85250, train_perplexity=1135.503, train_loss=7.034831

Batch 85260, train_perplexity=979.6507, train_loss=6.887196

Batch 85270, train_perplexity=1109.6874, train_loss=7.0118337

Batch 85280, train_perplexity=1069.5612, train_loss=6.9750037

Batch 85290, train_perplexity=1016.58655, train_loss=6.924206

Batch 85300, train_perplexity=1096.3142, train_loss=6.999709

Batch 85310, train_perplexity=1122.7119, train_loss=7.0235023

Batch 85320, train_perplexity=1139.3734, train_loss=7.0382338

Batch 85330, train_perplexity=1189.0503, train_loss=7.08091

Batch 85340, train_perplexity=1066.145, train_loss=6.9718046

Batch 85350, train_perplexity=1101.3116, train_loss=7.004257

Batch 85360, train_perplexity=1122.0878, train_loss=7.0229464

Batch 85370, train_perplexity=1049.3662, train_loss=6.9559417

Batch 85380, train_perplexity=1108.3099, train_loss=7.0105915

Batch 85390, train_perplexity=1163.0734, train_loss=7.058821

Batch 85400, train_perplexity=1131.6609, train_loss=7.0314417

Batch 85410, train_perplexity=1035.8572, train_loss=6.9429846

Batch 85420, train_perplexity=1124.526, train_loss=7.025117

Batch 85430, train_perplexity=1019.50793, train_loss=6.9270754

Batch 85440, train_perplexity=1013.1459, train_loss=6.9208155

Batch 85450, train_perplexity=1164.0199, train_loss=7.0596347

Batch 85460, train_perplexity=1102.495, train_loss=7.005331

Batch 85470, train_perplexity=1136.6819, train_loss=7.0358686

Batch 85480, train_perplexity=1172.13, train_loss=7.066578

Batch 85490, train_perplexity=1021.545, train_loss=6.9290714

Batch 85500, train_perplexity=1003.81775, train_loss=6.911566

Batch 85510, train_perplexity=1060.9069, train_loss=6.9668794

Batch 85520, train_perplexity=1060.395, train_loss=6.966397

Batch 85530, train_perplexity=1116.4934, train_loss=7.017948

Batch 85540, train_perplexity=1059.3025, train_loss=6.965366

Batch 85550, train_perplexity=1017.8399, train_loss=6.925438

Batch 85560, train_perplexity=1126.7708, train_loss=7.027111

Batch 85570, train_perplexity=1032.0854, train_loss=6.939337

Batch 85580, train_perplexity=1078.8235, train_loss=6.9836264

Batch 85590, train_perplexity=1018.2506, train_loss=6.9258413

Batch 85600, train_perplexity=1126.2035, train_loss=7.0266075

Batch 85610, train_perplexity=1053.3041, train_loss=6.959687

Batch 85620, train_perplexity=1142.81, train_loss=7.0412455

Batch 85630, train_perplexity=1115.7079, train_loss=7.0172443

Batch 85640, train_perplexity=1151.6809, train_loss=7.048978

Batch 85650, train_perplexity=1021.74664, train_loss=6.929269

Batch 85660, train_perplexity=1081.6029, train_loss=6.9861994

Batch 85670, train_perplexity=1174.6411, train_loss=7.068718

Batch 85680, train_perplexity=978.92267, train_loss=6.8864527

Batch 85690, train_perplexity=1140.0093, train_loss=7.0387917

Batch 85700, train_perplexity=1072.976, train_loss=6.9781914

Batch 85710, train_perplexity=1168.7914, train_loss=7.0637255

Batch 85720, train_perplexity=1204.995, train_loss=7.0942307

Batch 85730, train_perplexity=996.00696, train_loss=6.903754

Batch 85740, train_perplexity=1074.116, train_loss=6.9792533

Batch 85750, train_perplexity=919.23926, train_loss=6.8235464

Batch 85760, train_perplexity=1035.0287, train_loss=6.9421844

Batch 85770, train_perplexity=1063.9927, train_loss=6.969784

Batch 85780, train_perplexity=1105.4556, train_loss=7.008013

Batch 85790, train_perplexity=966.54584, train_loss=6.8737288

Batch 85800, train_perplexity=1173.7369, train_loss=7.067948

Batch 85810, train_perplexity=1010.7925, train_loss=6.91849

Batch 85820, train_perplexity=1050.6434, train_loss=6.957158

Batch 85830, train_perplexity=1081.6437, train_loss=6.986237

Batch 85840, train_perplexity=1054.5725, train_loss=6.960891

Batch 85850, train_perplexity=943.84033, train_loss=6.849957

Batch 85860, train_perplexity=1026.8408, train_loss=6.9342422

Batch 85870, train_perplexity=1141.0219, train_loss=7.0396795

Batch 85880, train_perplexity=1016.66364, train_loss=6.9242816

Batch 85890, train_perplexity=1072.9003, train_loss=6.978121

Batch 85900, train_perplexity=1126.6417, train_loss=7.0269966

Batch 85910, train_perplexity=942.4053, train_loss=6.8484354

Batch 85920, train_perplexity=1084.7866, train_loss=6.9891386

Batch 85930, train_perplexity=1109.8976, train_loss=7.012023

Batch 85940, train_perplexity=1058.9327, train_loss=6.965017

Batch 85950, train_perplexity=1056.5657, train_loss=6.962779

Batch 85960, train_perplexity=1044.5621, train_loss=6.951353

Batch 85970, train_perplexity=1182.6195, train_loss=7.075487

Batch 85980, train_perplexity=970.8823, train_loss=6.8782053

Batch 85990, train_perplexity=1120.4486, train_loss=7.0214844

Batch 86000, train_perplexity=1143.4424, train_loss=7.0417986

Batch 86010, train_perplexity=1044.6687, train_loss=6.951455

Batch 86020, train_perplexity=970.58887, train_loss=6.877903

Batch 86030, train_perplexity=993.24854, train_loss=6.900981

Batch 86040, train_perplexity=1066.3494, train_loss=6.9719963

Batch 86050, train_perplexity=1092.5432, train_loss=6.9962635

Batch 86060, train_perplexity=1143.3082, train_loss=7.0416813

Batch 86070, train_perplexity=926.10266, train_loss=6.830985

Batch 86080, train_perplexity=1060.0184, train_loss=6.9660416

Batch 86090, train_perplexity=1053.3352, train_loss=6.959717

Batch 86100, train_perplexity=957.9976, train_loss=6.8648453

Batch 86110, train_perplexity=952.8232, train_loss=6.8594294

Batch 86120, train_perplexity=1054.3518, train_loss=6.9606814

Batch 86130, train_perplexity=1046.2056, train_loss=6.952925

Batch 86140, train_perplexity=1068.6593, train_loss=6.97416

Batch 86150, train_perplexity=1067.4248, train_loss=6.9730043

Batch 86160, train_perplexity=1055.6155, train_loss=6.9618793

Batch 86170, train_perplexity=1107.9125, train_loss=7.010233

Batch 86180, train_perplexity=1048.983, train_loss=6.9555764

Batch 86190, train_perplexity=993.30634, train_loss=6.901039

Batch 86200, train_perplexity=1115.9761, train_loss=7.0174847

Batch 86210, train_perplexity=1207.6139, train_loss=7.0964017

Batch 86220, train_perplexity=1045.492, train_loss=6.952243

Batch 86230, train_perplexity=1082.2534, train_loss=6.9868007
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86240, train_perplexity=1071.5342, train_loss=6.9768467

Batch 86250, train_perplexity=1116.4785, train_loss=7.017935

Batch 86260, train_perplexity=1097.8878, train_loss=7.0011435

Batch 86270, train_perplexity=1109.8467, train_loss=7.011977

Batch 86280, train_perplexity=1081.4626, train_loss=6.9860697

Batch 86290, train_perplexity=1044.3978, train_loss=6.9511957

Batch 86300, train_perplexity=1149.2589, train_loss=7.0468726

Batch 86310, train_perplexity=1097.825, train_loss=7.001086

Batch 86320, train_perplexity=994.99585, train_loss=6.9027386

Batch 86330, train_perplexity=1082.4325, train_loss=6.986966

Batch 86340, train_perplexity=1156.5596, train_loss=7.053205

Batch 86350, train_perplexity=1005.26385, train_loss=6.9130054

Batch 86360, train_perplexity=1071.2424, train_loss=6.9765744

Batch 86370, train_perplexity=1082.1663, train_loss=6.98672

Batch 86380, train_perplexity=1156.1902, train_loss=7.0528855

Batch 86390, train_perplexity=1066.8163, train_loss=6.972434

Batch 86400, train_perplexity=937.4572, train_loss=6.843171

Batch 86410, train_perplexity=1038.2482, train_loss=6.94529

Batch 86420, train_perplexity=1111.3242, train_loss=7.0133076

Batch 86430, train_perplexity=958.3316, train_loss=6.865194

Batch 86440, train_perplexity=1114.1066, train_loss=7.015808

Batch 86450, train_perplexity=1123.0353, train_loss=7.0237904

Batch 86460, train_perplexity=1103.6532, train_loss=7.006381

Batch 86470, train_perplexity=1076.1519, train_loss=6.981147

Batch 86480, train_perplexity=1140.8032, train_loss=7.039488

Batch 86490, train_perplexity=1087.0007, train_loss=6.9911776

Batch 86500, train_perplexity=1065.5667, train_loss=6.971262

Batch 86510, train_perplexity=1021.8202, train_loss=6.929341

Batch 86520, train_perplexity=1162.0184, train_loss=7.057914

Batch 86530, train_perplexity=1130.2329, train_loss=7.030179

Batch 86540, train_perplexity=1081.7576, train_loss=6.9863424

Batch 86550, train_perplexity=941.55725, train_loss=6.847535

Batch 86560, train_perplexity=1077.6122, train_loss=6.982503

Batch 86570, train_perplexity=1050.9255, train_loss=6.9574265

Batch 86580, train_perplexity=1174.3756, train_loss=7.068492

Batch 86590, train_perplexity=1140.467, train_loss=7.039193

Batch 86600, train_perplexity=1165.2344, train_loss=7.0606775

Batch 86610, train_perplexity=987.3864, train_loss=6.8950615

Batch 86620, train_perplexity=1047.2722, train_loss=6.953944

Batch 86630, train_perplexity=1112.5903, train_loss=7.0144463

Batch 86640, train_perplexity=1078.6074, train_loss=6.983426

Batch 86650, train_perplexity=1145.7871, train_loss=7.043847

Batch 86660, train_perplexity=1005.4235, train_loss=6.913164

Batch 86670, train_perplexity=1130.4583, train_loss=7.0303783

Batch 86680, train_perplexity=1084.7607, train_loss=6.9891148

Batch 86690, train_perplexity=1066.9576, train_loss=6.9725666

Batch 86700, train_perplexity=1073.0338, train_loss=6.9782453

Batch 86710, train_perplexity=1221.9601, train_loss=7.1082115

Batch 86720, train_perplexity=1287.6583, train_loss=7.1605806

Batch 86730, train_perplexity=1102.2552, train_loss=7.0051136

Batch 86740, train_perplexity=1051.297, train_loss=6.95778

Batch 86750, train_perplexity=1094.769, train_loss=6.9982986

Batch 86760, train_perplexity=1090.9003, train_loss=6.9947586

Batch 86770, train_perplexity=1212.6173, train_loss=7.1005363

Batch 86780, train_perplexity=1118.657, train_loss=7.019884

Batch 86790, train_perplexity=1185.4017, train_loss=7.077837

Batch 86800, train_perplexity=1057.0934, train_loss=6.9632783

Batch 86810, train_perplexity=1095.0917, train_loss=6.9985933

Batch 86820, train_perplexity=1144.8597, train_loss=7.0430374

Batch 86830, train_perplexity=1014.0066, train_loss=6.9216647

Batch 86840, train_perplexity=1036.5262, train_loss=6.94363

Batch 86850, train_perplexity=1047.0515, train_loss=6.9537334

Batch 86860, train_perplexity=1007.1951, train_loss=6.9149246

Batch 86870, train_perplexity=1097.8354, train_loss=7.001096

Batch 86880, train_perplexity=1148.6776, train_loss=7.0463667

Batch 86890, train_perplexity=1014.27985, train_loss=6.921934

Batch 86900, train_perplexity=1015.3875, train_loss=6.9230256

Batch 86910, train_perplexity=1041.7795, train_loss=6.9486856

Batch 86920, train_perplexity=1118.8143, train_loss=7.020025

Batch 86930, train_perplexity=1024.1084, train_loss=6.9315777

Batch 86940, train_perplexity=1098.1805, train_loss=7.00141

Batch 86950, train_perplexity=1126.6283, train_loss=7.0269847

Batch 86960, train_perplexity=1038.3337, train_loss=6.9453726

Batch 86970, train_perplexity=1154.5577, train_loss=7.0514727

Batch 86980, train_perplexity=1045.0304, train_loss=6.9518013

Batch 86990, train_perplexity=977.0955, train_loss=6.8845844

Batch 87000, train_perplexity=1093.1482, train_loss=6.996817

Batch 87010, train_perplexity=1100.1182, train_loss=7.003173

Batch 87020, train_perplexity=1070.2698, train_loss=6.975666

Batch 87030, train_perplexity=1042.2849, train_loss=6.9491706

Batch 87040, train_perplexity=1125.6794, train_loss=7.026142

Batch 87050, train_perplexity=869.16693, train_loss=6.767535

Batch 87060, train_perplexity=1056.4387, train_loss=6.962659

Batch 87070, train_perplexity=1076.7709, train_loss=6.981722

Batch 87080, train_perplexity=1043.5992, train_loss=6.950431

Batch 87090, train_perplexity=1132.3508, train_loss=7.032051

Batch 87100, train_perplexity=1138.7987, train_loss=7.0377293

Batch 87110, train_perplexity=1136.8965, train_loss=7.0360575

Batch 87120, train_perplexity=983.9019, train_loss=6.891526

Batch 87130, train_perplexity=1088.9119, train_loss=6.992934

Batch 87140, train_perplexity=1051.2408, train_loss=6.9577265

Batch 87150, train_perplexity=1041.8203, train_loss=6.9487247

Batch 87160, train_perplexity=965.9469, train_loss=6.873109

Batch 87170, train_perplexity=1033.7522, train_loss=6.9409504

Batch 87180, train_perplexity=1039.724, train_loss=6.9467106

Batch 87190, train_perplexity=1144.1339, train_loss=7.042403

Batch 87200, train_perplexity=1220.3036, train_loss=7.106855

Batch 87210, train_perplexity=1021.4836, train_loss=6.9290113

Batch 87220, train_perplexity=1089.741, train_loss=6.9936953

Batch 87230, train_perplexity=1150.3495, train_loss=7.047821

Batch 87240, train_perplexity=1057.9344, train_loss=6.9640737

Batch 87250, train_perplexity=1006.10834, train_loss=6.913845

Batch 87260, train_perplexity=1007.2705, train_loss=6.9149995

Batch 87270, train_perplexity=1054.1774, train_loss=6.960516

Batch 87280, train_perplexity=1084.901, train_loss=6.989244

Batch 87290, train_perplexity=1024.94, train_loss=6.9323893

Batch 87300, train_perplexity=1104.8474, train_loss=7.0074625

Batch 87310, train_perplexity=1123.721, train_loss=7.0244007

Batch 87320, train_perplexity=1031.7411, train_loss=6.939003

Batch 87330, train_perplexity=1012.07007, train_loss=6.919753

Batch 87340, train_perplexity=1049.7681, train_loss=6.9563246

Batch 87350, train_perplexity=1165.8041, train_loss=7.0611663

Batch 87360, train_perplexity=1100.6366, train_loss=7.003644

Batch 87370, train_perplexity=1030.3507, train_loss=6.9376545

Batch 87380, train_perplexity=1017.1335, train_loss=6.9247437

Batch 87390, train_perplexity=1153.7988, train_loss=7.050815

Batch 87400, train_perplexity=1029.5599, train_loss=6.936887

Batch 87410, train_perplexity=1034.5614, train_loss=6.941733

Batch 87420, train_perplexity=932.4502, train_loss=6.8378158

Batch 87430, train_perplexity=1039.0619, train_loss=6.9460735

Batch 87440, train_perplexity=981.61975, train_loss=6.889204

Batch 87450, train_perplexity=1010.392, train_loss=6.9180937

Batch 87460, train_perplexity=1064.9946, train_loss=6.970725

Batch 87470, train_perplexity=1084.8373, train_loss=6.9891853

Batch 87480, train_perplexity=1189.3774, train_loss=7.0811853

Batch 87490, train_perplexity=1066.1958, train_loss=6.9718523

Batch 87500, train_perplexity=1023.45184, train_loss=6.9309363

Batch 87510, train_perplexity=1089.6277, train_loss=6.9935913

Batch 87520, train_perplexity=1056.3536, train_loss=6.9625783

Batch 87530, train_perplexity=1040.462, train_loss=6.94742

Batch 87540, train_perplexity=1048.8094, train_loss=6.955411

Batch 87550, train_perplexity=1025.5657, train_loss=6.9329996
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 87560, train_perplexity=1131.716, train_loss=7.0314903

Batch 87570, train_perplexity=988.9376, train_loss=6.8966312

Batch 87580, train_perplexity=1114.4615, train_loss=7.0161266

Batch 87590, train_perplexity=1093.8297, train_loss=6.9974403

Batch 87600, train_perplexity=993.0042, train_loss=6.900735

Batch 87610, train_perplexity=1169.9907, train_loss=7.064751

Batch 87620, train_perplexity=1038.7933, train_loss=6.945815

Batch 87630, train_perplexity=1064.4265, train_loss=6.9701915

Batch 87640, train_perplexity=1065.904, train_loss=6.9715786

Batch 87650, train_perplexity=1028.7189, train_loss=6.9360695

Batch 87660, train_perplexity=1114.343, train_loss=7.0160203

Batch 87670, train_perplexity=1043.4899, train_loss=6.950326

Batch 87680, train_perplexity=1088.4489, train_loss=6.992509

Batch 87690, train_perplexity=1114.7145, train_loss=7.0163536

Batch 87700, train_perplexity=1023.8882, train_loss=6.9313626

Batch 87710, train_perplexity=1152.3588, train_loss=7.0495663

Batch 87720, train_perplexity=1029.2184, train_loss=6.936555

Batch 87730, train_perplexity=1023.7911, train_loss=6.9312677

Batch 87740, train_perplexity=1131.3955, train_loss=7.031207

Batch 87750, train_perplexity=1111.2585, train_loss=7.0132484

Batch 87760, train_perplexity=1050.1812, train_loss=6.956718

Batch 87770, train_perplexity=1142.3323, train_loss=7.0408273

Batch 87780, train_perplexity=1120.6451, train_loss=7.02166

Batch 87790, train_perplexity=1110.0298, train_loss=7.012142

Batch 87800, train_perplexity=1211.2333, train_loss=7.0993943

Batch 87810, train_perplexity=1078.266, train_loss=6.9831095

Batch 87820, train_perplexity=1069.3867, train_loss=6.9748406

Batch 87830, train_perplexity=1102.0409, train_loss=7.004919

Batch 87840, train_perplexity=1054.8923, train_loss=6.961194

Batch 87850, train_perplexity=990.2763, train_loss=6.897984

Batch 87860, train_perplexity=1081.1785, train_loss=6.985807

Batch 87870, train_perplexity=969.4344, train_loss=6.876713

Batch 87880, train_perplexity=989.56213, train_loss=6.8972626

Batch 87890, train_perplexity=1028.7546, train_loss=6.9361043

Batch 87900, train_perplexity=1180.6699, train_loss=7.0738373

Batch 87910, train_perplexity=1078.9752, train_loss=6.983767

Batch 87920, train_perplexity=1089.7596, train_loss=6.9937124

Batch 87930, train_perplexity=1082.0435, train_loss=6.9866066

Batch 87940, train_perplexity=1033.149, train_loss=6.9403667

Batch 87950, train_perplexity=1152.294, train_loss=7.04951

Batch 87960, train_perplexity=1003.70624, train_loss=6.9114547

Batch 87970, train_perplexity=1055.0312, train_loss=6.9613256

Batch 87980, train_perplexity=1091.1917, train_loss=6.9950256

Batch 87990, train_perplexity=1220.3099, train_loss=7.10686

Batch 88000, train_perplexity=1156.7069, train_loss=7.0533323

Batch 88010, train_perplexity=1054.1764, train_loss=6.960515

Batch 88020, train_perplexity=1025.9854, train_loss=6.9334087

Batch 88030, train_perplexity=1170.6521, train_loss=7.065316

Batch 88040, train_perplexity=1147.6992, train_loss=7.0455146

Batch 88050, train_perplexity=1005.0161, train_loss=6.912759

Batch 88060, train_perplexity=1095.604, train_loss=6.999061

Batch 88070, train_perplexity=959.1897, train_loss=6.866089

Batch 88080, train_perplexity=1065.0546, train_loss=6.9707813

Batch 88090, train_perplexity=1118.1898, train_loss=7.0194664

Batch 88100, train_perplexity=969.3433, train_loss=6.876619

Batch 88110, train_perplexity=1009.8285, train_loss=6.917536

Batch 88120, train_perplexity=1065.4031, train_loss=6.9711084

Batch 88130, train_perplexity=1071.9967, train_loss=6.977278

Batch 88140, train_perplexity=1077.5248, train_loss=6.982422

Batch 88150, train_perplexity=992.93036, train_loss=6.9006605

Batch 88160, train_perplexity=1011.3657, train_loss=6.919057

Batch 88170, train_perplexity=1105.8373, train_loss=7.008358

Batch 88180, train_perplexity=1098.3528, train_loss=7.001567

Batch 88190, train_perplexity=1120.338, train_loss=7.0213857

Batch 88200, train_perplexity=1115.9894, train_loss=7.0174966

Batch 88210, train_perplexity=1188.0709, train_loss=7.080086

Batch 88220, train_perplexity=1164.489, train_loss=7.0600376

Batch 88230, train_perplexity=1040.2318, train_loss=6.947199

Batch 88240, train_perplexity=1068.9885, train_loss=6.974468

Batch 88250, train_perplexity=1069.0059, train_loss=6.9744844

Batch 88260, train_perplexity=1103.5553, train_loss=7.0062923

Batch 88270, train_perplexity=1080.7357, train_loss=6.9853973

Batch 88280, train_perplexity=1043.7087, train_loss=6.950536

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Batch 88290, train_perplexity=1096.7932, train_loss=7.000146

Batch 88300, train_perplexity=1034.9127, train_loss=6.9420724

Batch 88310, train_perplexity=1062.0098, train_loss=6.9679184

Batch 88320, train_perplexity=1101.7498, train_loss=7.004655

Batch 88330, train_perplexity=1097.4565, train_loss=7.0007505

Batch 88340, train_perplexity=995.9576, train_loss=6.9037046

Batch 88350, train_perplexity=1014.2789, train_loss=6.921933

Batch 88360, train_perplexity=1134.9282, train_loss=7.0343246

Batch 88370, train_perplexity=1077.5269, train_loss=6.982424

Batch 88380, train_perplexity=982.91064, train_loss=6.890518

Batch 88390, train_perplexity=964.4451, train_loss=6.871553

Batch 88400, train_perplexity=1045.831, train_loss=6.952567

Batch 88410, train_perplexity=998.7949, train_loss=6.9065495

Batch 88420, train_perplexity=1012.20667, train_loss=6.919888

Batch 88430, train_perplexity=1001.32465, train_loss=6.909079

Batch 88440, train_perplexity=1064.5271, train_loss=6.970286

Batch 88450, train_perplexity=1186.19, train_loss=7.0785017

Batch 88460, train_perplexity=1080.3555, train_loss=6.9850454

Batch 88470, train_perplexity=1017.0297, train_loss=6.9246416

Batch 88480, train_perplexity=953.4231, train_loss=6.860059

Batch 88490, train_perplexity=1015.6902, train_loss=6.9233236

Batch 88500, train_perplexity=1112.8843, train_loss=7.0147104

Batch 88510, train_perplexity=1114.3446, train_loss=7.0160217

Batch 88520, train_perplexity=1012.07587, train_loss=6.919759

Batch 88530, train_perplexity=1076.0225, train_loss=6.9810266

Batch 88540, train_perplexity=1081.3322, train_loss=6.985949

Batch 88550, train_perplexity=1033.4476, train_loss=6.9406557

Batch 88560, train_perplexity=1081.9207, train_loss=6.986493

Batch 88570, train_perplexity=1133.8934, train_loss=7.0334125

Batch 88580, train_perplexity=1052.6172, train_loss=6.959035

Batch 88590, train_perplexity=1040.7845, train_loss=6.94773

Batch 88600, train_perplexity=1007.26184, train_loss=6.914991

Batch 88610, train_perplexity=1092.1213, train_loss=6.9958773

Batch 88620, train_perplexity=988.8674, train_loss=6.89656

Batch 88630, train_perplexity=1145.302, train_loss=7.0434237

Batch 88640, train_perplexity=983.5928, train_loss=6.891212

Batch 88650, train_perplexity=1180.9019, train_loss=7.0740337

Batch 88660, train_perplexity=1119.0747, train_loss=7.0202575

Batch 88670, train_perplexity=1033.98, train_loss=6.9411707

Batch 88680, train_perplexity=1165.2544, train_loss=7.0606947

Batch 88690, train_perplexity=1138.0919, train_loss=7.0371084

Batch 88700, train_perplexity=934.36273, train_loss=6.8398647

Batch 88710, train_perplexity=1024.1592, train_loss=6.9316273

Batch 88720, train_perplexity=1021.7267, train_loss=6.9292493

Batch 88730, train_perplexity=1017.7176, train_loss=6.925318

Batch 88740, train_perplexity=1082.7283, train_loss=6.9872394

Batch 88750, train_perplexity=1138.3102, train_loss=7.0373

Batch 88760, train_perplexity=989.0588, train_loss=6.896754

Batch 88770, train_perplexity=1179.0232, train_loss=7.0724416

Batch 88780, train_perplexity=968.5634, train_loss=6.875814

Batch 88790, train_perplexity=1075.307, train_loss=6.9803615

Batch 88800, train_perplexity=1049.0181, train_loss=6.95561

Batch 88810, train_perplexity=1077.2197, train_loss=6.9821386
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88820, train_perplexity=1124.8253, train_loss=7.025383

Batch 88830, train_perplexity=1024.9429, train_loss=6.932392

Batch 88840, train_perplexity=1006.9113, train_loss=6.914643

Batch 88850, train_perplexity=1112.9597, train_loss=7.014778

Batch 88860, train_perplexity=1014.5672, train_loss=6.9222174

Batch 88870, train_perplexity=1052.9425, train_loss=6.959344

Batch 88880, train_perplexity=929.98846, train_loss=6.835172

Batch 88890, train_perplexity=1020.20825, train_loss=6.927762

Batch 88900, train_perplexity=1067.7354, train_loss=6.973295

Batch 88910, train_perplexity=1092.1807, train_loss=6.9959316

Batch 88920, train_perplexity=1057.602, train_loss=6.9637594

Batch 88930, train_perplexity=1032.0009, train_loss=6.9392548

Batch 88940, train_perplexity=1064.2343, train_loss=6.9700108

Batch 88950, train_perplexity=1127.4188, train_loss=7.027686

Batch 88960, train_perplexity=1173.8002, train_loss=7.0680017

Batch 88970, train_perplexity=1021.5771, train_loss=6.929103

Batch 88980, train_perplexity=1205.6536, train_loss=7.094777

Batch 88990, train_perplexity=1134.3179, train_loss=7.033787

Batch 89000, train_perplexity=1013.2985, train_loss=6.920966

Batch 89010, train_perplexity=1199.448, train_loss=7.089617

Batch 89020, train_perplexity=1014.7636, train_loss=6.922411

Batch 89030, train_perplexity=1064.0607, train_loss=6.9698477

Batch 89040, train_perplexity=1036.8198, train_loss=6.9439135

Batch 89050, train_perplexity=1095.2588, train_loss=6.998746

Batch 89060, train_perplexity=1027.8098, train_loss=6.9351854

Batch 89070, train_perplexity=1126.0842, train_loss=7.0265017

Batch 89080, train_perplexity=1017.213, train_loss=6.924822

Batch 89090, train_perplexity=994.2844, train_loss=6.9020233

Batch 89100, train_perplexity=955.50714, train_loss=6.862242

Batch 89110, train_perplexity=1051.9729, train_loss=6.9584227

Batch 89120, train_perplexity=1012.6783, train_loss=6.920354

Batch 89130, train_perplexity=1010.75684, train_loss=6.9184546

Batch 89140, train_perplexity=975.50665, train_loss=6.882957

Batch 89150, train_perplexity=1008.56573, train_loss=6.9162846

Batch 89160, train_perplexity=1125.4991, train_loss=7.025982

Batch 89170, train_perplexity=1108.8628, train_loss=7.0110903

Batch 89180, train_perplexity=1030.3438, train_loss=6.937648

Batch 89190, train_perplexity=973.77545, train_loss=6.881181

Batch 89200, train_perplexity=1021.3209, train_loss=6.928852

Batch 89210, train_perplexity=998.2845, train_loss=6.9060383

Batch 89220, train_perplexity=975.6527, train_loss=6.8831067

Batch 89230, train_perplexity=988.5086, train_loss=6.8961973

Batch 89240, train_perplexity=1108.6228, train_loss=7.010874

Batch 89250, train_perplexity=1070.2208, train_loss=6.9756203

Batch 89260, train_perplexity=1120.7959, train_loss=7.0217943

Batch 89270, train_perplexity=1074.4597, train_loss=6.9795732

Batch 89280, train_perplexity=1047.1998, train_loss=6.953875

Batch 89290, train_perplexity=1084.6946, train_loss=6.9890537

Batch 89300, train_perplexity=1015.71, train_loss=6.923343

Batch 89310, train_perplexity=1146.6731, train_loss=7.04462

Batch 89320, train_perplexity=1062.2904, train_loss=6.9681826

Batch 89330, train_perplexity=931.5134, train_loss=6.8368106

Batch 89340, train_perplexity=1107.0845, train_loss=7.0094852

Batch 89350, train_perplexity=1036.9602, train_loss=6.944049

Batch 89360, train_perplexity=1134.8562, train_loss=7.034261

Batch 89370, train_perplexity=1022.6727, train_loss=6.930175

Batch 89380, train_perplexity=970.54443, train_loss=6.877857

Batch 89390, train_perplexity=998.9273, train_loss=6.906682

Batch 89400, train_perplexity=1100.7216, train_loss=7.003721

Batch 89410, train_perplexity=1045.6301, train_loss=6.952375

Batch 89420, train_perplexity=973.44073, train_loss=6.880837

Batch 89430, train_perplexity=966.84595, train_loss=6.874039

Batch 89440, train_perplexity=1087.4631, train_loss=6.991603

Batch 89450, train_perplexity=1103.789, train_loss=7.006504

Batch 89460, train_perplexity=981.1846, train_loss=6.8887606

Batch 89470, train_perplexity=1157.9579, train_loss=7.0544133

Batch 89480, train_perplexity=1045.3041, train_loss=6.952063

Batch 89490, train_perplexity=978.11084, train_loss=6.885623

Batch 89500, train_perplexity=1153.4457, train_loss=7.050509

Batch 89510, train_perplexity=1065.1394, train_loss=6.970861

Batch 89520, train_perplexity=1080.339, train_loss=6.98503

Batch 89530, train_perplexity=1021.82263, train_loss=6.929343

Batch 89540, train_perplexity=972.9298, train_loss=6.880312

Batch 89550, train_perplexity=1055.1681, train_loss=6.9614553

Batch 89560, train_perplexity=1054.0637, train_loss=6.960408

Batch 89570, train_perplexity=1093.116, train_loss=6.9967875

Batch 89580, train_perplexity=975.82025, train_loss=6.8832784

Batch 89590, train_perplexity=1078.1179, train_loss=6.982972

Batch 89600, train_perplexity=973.10986, train_loss=6.880497

Batch 89610, train_perplexity=1032.6201, train_loss=6.9398546

Batch 89620, train_perplexity=980.0025, train_loss=6.887555

Batch 89630, train_perplexity=1063.3506, train_loss=6.96918

Batch 89640, train_perplexity=989.213, train_loss=6.8969097

Batch 89650, train_perplexity=1055.7821, train_loss=6.962037

Batch 89660, train_perplexity=1057.8431, train_loss=6.9639874

Batch 89670, train_perplexity=1084.9589, train_loss=6.9892974

Batch 89680, train_perplexity=1057.8538, train_loss=6.9639974

Batch 89690, train_perplexity=1130.6566, train_loss=7.030554

Batch 89700, train_perplexity=1088.2474, train_loss=6.992324

Batch 89710, train_perplexity=1108.7999, train_loss=7.0110335

Batch 89720, train_perplexity=1088.3025, train_loss=6.9923744

Batch 89730, train_perplexity=1104.3564, train_loss=7.007018

Batch 89740, train_perplexity=1079.126, train_loss=6.9839067

Batch 89750, train_perplexity=1126.7524, train_loss=7.027095

Batch 89760, train_perplexity=998.01746, train_loss=6.905771

Batch 89770, train_perplexity=1059.9386, train_loss=6.965966

Batch 89780, train_perplexity=1036.3304, train_loss=6.9434414

Batch 89790, train_perplexity=1082.4294, train_loss=6.9869633

Batch 89800, train_perplexity=973.82007, train_loss=6.8812265

Batch 89810, train_perplexity=1037.1229, train_loss=6.9442058

Batch 89820, train_perplexity=1028.6968, train_loss=6.936048

Batch 89830, train_perplexity=999.34705, train_loss=6.907102

Batch 89840, train_perplexity=1096.389, train_loss=6.9997773

Batch 89850, train_perplexity=937.801, train_loss=6.843538

Batch 89860, train_perplexity=1074.3403, train_loss=6.979462

Batch 89870, train_perplexity=1033.3865, train_loss=6.9405966

Batch 89880, train_perplexity=1015.82385, train_loss=6.923455

Batch 89890, train_perplexity=1011.1082, train_loss=6.9188023

Batch 89900, train_perplexity=1100.1077, train_loss=7.0031633

Batch 89910, train_perplexity=1060.0184, train_loss=6.9660416

Batch 89920, train_perplexity=1094.4673, train_loss=6.998023

Batch 89930, train_perplexity=1026.9192, train_loss=6.9343185

Batch 89940, train_perplexity=1061.551, train_loss=6.9674864

Batch 89950, train_perplexity=1079.9681, train_loss=6.984687

Batch 89960, train_perplexity=975.1383, train_loss=6.8825793

Batch 89970, train_perplexity=1071.0443, train_loss=6.9763894

Batch 89980, train_perplexity=1096.7869, train_loss=7.00014

Batch 89990, train_perplexity=1039.8718, train_loss=6.9468527

Batch 90000, train_perplexity=1002.01965, train_loss=6.909773

Batch 90010, train_perplexity=937.64856, train_loss=6.843375

Batch 90020, train_perplexity=1047.9886, train_loss=6.954628

Batch 90030, train_perplexity=1101.9452, train_loss=7.0048323

Batch 90040, train_perplexity=1062.1318, train_loss=6.9680333

Batch 90050, train_perplexity=1027.6697, train_loss=6.935049

Batch 90060, train_perplexity=1202.771, train_loss=7.0923834

Batch 90070, train_perplexity=1103.8495, train_loss=7.006559

Batch 90080, train_perplexity=945.3023, train_loss=6.851505

Batch 90090, train_perplexity=1014.68475, train_loss=6.9223332

Batch 90100, train_perplexity=1067.15, train_loss=6.972747

Batch 90110, train_perplexity=1075.0491, train_loss=6.9801216

Batch 90120, train_perplexity=1091.5591, train_loss=6.9953623

Batch 90130, train_perplexity=1038.4724, train_loss=6.945506
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 90140, train_perplexity=1085.1451, train_loss=6.989469

Batch 90150, train_perplexity=1065.9269, train_loss=6.9716

Batch 90160, train_perplexity=1071.3156, train_loss=6.9766426

Batch 90170, train_perplexity=1034.2266, train_loss=6.941409

Batch 90180, train_perplexity=1075.6039, train_loss=6.9806376

Batch 90190, train_perplexity=1030.1243, train_loss=6.9374347

Batch 90200, train_perplexity=992.289, train_loss=6.9000144

Batch 90210, train_perplexity=955.41376, train_loss=6.8621445

Batch 90220, train_perplexity=969.494, train_loss=6.8767743

Batch 90230, train_perplexity=1059.6526, train_loss=6.9656963

Batch 90240, train_perplexity=931.36066, train_loss=6.8366466

Batch 90250, train_perplexity=947.75714, train_loss=6.8540983

Batch 90260, train_perplexity=1027.8407, train_loss=6.9352155

Batch 90270, train_perplexity=1246.7065, train_loss=7.1282606

Batch 90280, train_perplexity=995.3271, train_loss=6.9030714

Batch 90290, train_perplexity=996.14087, train_loss=6.9038887

Batch 90300, train_perplexity=1072.7156, train_loss=6.9779487

Batch 90310, train_perplexity=1148.4323, train_loss=7.046153

Batch 90320, train_perplexity=1098.708, train_loss=7.00189

Batch 90330, train_perplexity=1027.5138, train_loss=6.9348974

Batch 90340, train_perplexity=1025.1951, train_loss=6.932638

Batch 90350, train_perplexity=953.79913, train_loss=6.860453

Batch 90360, train_perplexity=1151.5123, train_loss=7.0488315

Batch 90370, train_perplexity=1101.877, train_loss=7.0047703

Batch 90380, train_perplexity=994.9214, train_loss=6.9026637

Batch 90390, train_perplexity=1035.1343, train_loss=6.9422865

Batch 90400, train_perplexity=1163.7462, train_loss=7.0593996

Batch 90410, train_perplexity=1119.6848, train_loss=7.0208025

Batch 90420, train_perplexity=1086.1722, train_loss=6.990415

Batch 90430, train_perplexity=1072.6906, train_loss=6.9779253

Batch 90440, train_perplexity=1050.0054, train_loss=6.9565506

Batch 90450, train_perplexity=1046.5894, train_loss=6.953292

Batch 90460, train_perplexity=1011.877, train_loss=6.9195623

Batch 90470, train_perplexity=1008.70667, train_loss=6.9164243

Batch 90480, train_perplexity=970.4496, train_loss=6.8777595

Batch 90490, train_perplexity=974.11774, train_loss=6.881532

Batch 90500, train_perplexity=1011.2707, train_loss=6.918963

Batch 90510, train_perplexity=1018.77997, train_loss=6.926361

Batch 90520, train_perplexity=1053.7432, train_loss=6.960104

Batch 90530, train_perplexity=1006.46826, train_loss=6.9142027

Batch 90540, train_perplexity=1008.25323, train_loss=6.9159746

Batch 90550, train_perplexity=1021.076, train_loss=6.928612

Batch 90560, train_perplexity=1122.9806, train_loss=7.0237417

Batch 90570, train_perplexity=969.562, train_loss=6.8768444

Batch 90580, train_perplexity=942.72345, train_loss=6.848773

Batch 90590, train_perplexity=1170.2441, train_loss=7.0649676

Batch 90600, train_perplexity=1123.4081, train_loss=7.024122

Batch 90610, train_perplexity=1261.8944, train_loss=7.1403694

Batch 90620, train_perplexity=1209.8577, train_loss=7.098258

Batch 90630, train_perplexity=1086.4758, train_loss=6.9906945

Batch 90640, train_perplexity=992.82526, train_loss=6.9005547

Batch 90650, train_perplexity=1028.5109, train_loss=6.9358673

Batch 90660, train_perplexity=991.4571, train_loss=6.8991756

Batch 90670, train_perplexity=1091.8182, train_loss=6.9955997

Batch 90680, train_perplexity=1042.9198, train_loss=6.9497795

Batch 90690, train_perplexity=1010.26483, train_loss=6.917968

Batch 90700, train_perplexity=1051.7898, train_loss=6.9582486

Batch 90710, train_perplexity=1028.1447, train_loss=6.935511

Batch 90720, train_perplexity=1068.0531, train_loss=6.9735928

Batch 90730, train_perplexity=1077.4714, train_loss=6.9823723

Batch 90740, train_perplexity=1015.73474, train_loss=6.9233675

Batch 90750, train_perplexity=985.71124, train_loss=6.8933635

Batch 90760, train_perplexity=1097.0391, train_loss=7.00037

Batch 90770, train_perplexity=1052.3743, train_loss=6.958804

Batch 90780, train_perplexity=1097.622, train_loss=7.000901

Batch 90790, train_perplexity=1072.6649, train_loss=6.9779015

Batch 90800, train_perplexity=1068.5951, train_loss=6.9741

Batch 90810, train_perplexity=1063.5625, train_loss=6.9693794

Batch 90820, train_perplexity=1014.9001, train_loss=6.9225454

Batch 90830, train_perplexity=1154.1967, train_loss=7.05116

Batch 90840, train_perplexity=1197.5656, train_loss=7.088046

Batch 90850, train_perplexity=1175.8976, train_loss=7.069787

Batch 90860, train_perplexity=1123.1863, train_loss=7.023925

Batch 90870, train_perplexity=1028.5374, train_loss=6.935893

Batch 90880, train_perplexity=1170.311, train_loss=7.065025

Batch 90890, train_perplexity=999.1226, train_loss=6.9068775

Batch 90900, train_perplexity=955.71356, train_loss=6.862458

Batch 90910, train_perplexity=1041.3693, train_loss=6.948292

Batch 90920, train_perplexity=1031.5414, train_loss=6.9388094

Batch 90930, train_perplexity=958.9382, train_loss=6.8658266

Batch 90940, train_perplexity=1055.9115, train_loss=6.9621596

Batch 90950, train_perplexity=1089.9987, train_loss=6.993932

Batch 90960, train_perplexity=1155.871, train_loss=7.0526094

Batch 90970, train_perplexity=1056.3551, train_loss=6.9625797

Batch 90980, train_perplexity=1111.8378, train_loss=7.0137696

Batch 90990, train_perplexity=1045.1531, train_loss=6.9519186

Batch 91000, train_perplexity=1109.9441, train_loss=7.012065

Batch 91010, train_perplexity=1088.916, train_loss=6.992938

Batch 91020, train_perplexity=1105.387, train_loss=7.007951

Batch 91030, train_perplexity=997.5512, train_loss=6.9053035

Batch 91040, train_perplexity=1123.3801, train_loss=7.0240974

Batch 91050, train_perplexity=1015.77783, train_loss=6.92341

Batch 91060, train_perplexity=1026.2319, train_loss=6.933649

Batch 91070, train_perplexity=1073.4556, train_loss=6.978638

Batch 91080, train_perplexity=995.6024, train_loss=6.903348

Batch 91090, train_perplexity=985.9825, train_loss=6.8936386

Batch 91100, train_perplexity=1081.3512, train_loss=6.9859667

Batch 91110, train_perplexity=1069.3694, train_loss=6.9748244

Batch 91120, train_perplexity=1061.6766, train_loss=6.9676046

Batch 91130, train_perplexity=1022.5528, train_loss=6.9300575

Batch 91140, train_perplexity=1003.1661, train_loss=6.9109163

Batch 91150, train_perplexity=1030.9326, train_loss=6.938219

Batch 91160, train_perplexity=1225.8926, train_loss=7.1114244

Batch 91170, train_perplexity=1158.9716, train_loss=7.0552883

Batch 91180, train_perplexity=1043.459, train_loss=6.9502964

Batch 91190, train_perplexity=1107.2144, train_loss=7.0096025

Batch 91200, train_perplexity=1024.5989, train_loss=6.9320564

Batch 91210, train_perplexity=1033.8883, train_loss=6.941082

Batch 91220, train_perplexity=995.30334, train_loss=6.9030476

Batch 91230, train_perplexity=1008.5321, train_loss=6.916251

Batch 91240, train_perplexity=1090.2336, train_loss=6.9941473

Batch 91250, train_perplexity=1019.7763, train_loss=6.9273386

Batch 91260, train_perplexity=1066.629, train_loss=6.9722586

Batch 91270, train_perplexity=1120.066, train_loss=7.021143

Batch 91280, train_perplexity=1032.4645, train_loss=6.939704

Batch 91290, train_perplexity=1029.343, train_loss=6.936676

Batch 91300, train_perplexity=1022.14624, train_loss=6.92966

Batch 91310, train_perplexity=1044.1967, train_loss=6.951003

Batch 91320, train_perplexity=1106.9087, train_loss=7.0093265

Batch 91330, train_perplexity=1188.0149, train_loss=7.080039

Batch 91340, train_perplexity=939.1892, train_loss=6.845017

Batch 91350, train_perplexity=1040.586, train_loss=6.9475393

Batch 91360, train_perplexity=1060.4391, train_loss=6.9664383

Batch 91370, train_perplexity=1059.6455, train_loss=6.9656897

Batch 91380, train_perplexity=1081.5204, train_loss=6.986123

Batch 91390, train_perplexity=1046.6393, train_loss=6.9533396

Batch 91400, train_perplexity=1022.4567, train_loss=6.9299636

Batch 91410, train_perplexity=1005.459, train_loss=6.9131994

Batch 91420, train_perplexity=1022.2442, train_loss=6.9297557

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306536 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100
Loaded 306536 sentences.
Finished loading
Batch 91430, train_perplexity=1088.5413, train_loss=6.992594

Batch 91440, train_perplexity=1131.8164, train_loss=7.031579

Batch 91450, train_perplexity=962.2682, train_loss=6.869293

Batch 91460, train_perplexity=1145.0421, train_loss=7.0431967

Batch 91470, train_perplexity=1049.2767, train_loss=6.9558563

Batch 91480, train_perplexity=972.3529, train_loss=6.879719

Batch 91490, train_perplexity=1068.7551, train_loss=6.97425

Batch 91500, train_perplexity=1053.9281, train_loss=6.9602795

Batch 91510, train_perplexity=1000.76666, train_loss=6.9085217

Batch 91520, train_perplexity=974.9761, train_loss=6.882413

Batch 91530, train_perplexity=1152.5704, train_loss=7.04975

Batch 91540, train_perplexity=1124.1824, train_loss=7.0248113

Batch 91550, train_perplexity=1019.9709, train_loss=6.9275293

Batch 91560, train_perplexity=1152.2126, train_loss=7.0494394

Batch 91570, train_perplexity=1007.31805, train_loss=6.9150467

Batch 91580, train_perplexity=1098.3109, train_loss=7.0015287

Batch 91590, train_perplexity=1047.3646, train_loss=6.9540324

Batch 91600, train_perplexity=1001.4001, train_loss=6.9091544

Batch 91610, train_perplexity=1047.4191, train_loss=6.9540844

Batch 91620, train_perplexity=924.3005, train_loss=6.829037

Batch 91630, train_perplexity=1048.7715, train_loss=6.9553747

Batch 91640, train_perplexity=1036.6003, train_loss=6.9437017

Batch 91650, train_perplexity=1127.6146, train_loss=7.0278597

Batch 91660, train_perplexity=1089.9701, train_loss=6.9939055

Batch 91670, train_perplexity=991.7899, train_loss=6.8995113

Batch 91680, train_perplexity=1113.9089, train_loss=7.0156307

Batch 91690, train_perplexity=1100.222, train_loss=7.0032673

Batch 91700, train_perplexity=991.2982, train_loss=6.8990154

Batch 91710, train_perplexity=1132.3994, train_loss=7.032094

Batch 91720, train_perplexity=1121.0802, train_loss=7.022048

Batch 91730, train_perplexity=1078.1858, train_loss=6.983035

Batch 91740, train_perplexity=1055.5953, train_loss=6.96186

Batch 91750, train_perplexity=937.6043, train_loss=6.843328

Batch 91760, train_perplexity=1038.2812, train_loss=6.945322

Batch 91770, train_perplexity=898.4796, train_loss=6.800704

Batch 91780, train_perplexity=1118.7369, train_loss=7.0199556

Batch 91790, train_perplexity=965.82855, train_loss=6.8729863

Batch 91800, train_perplexity=1015.33136, train_loss=6.9229703

Batch 91810, train_perplexity=1115.6105, train_loss=7.017157

Batch 91820, train_perplexity=1010.9482, train_loss=6.918644

Batch 91830, train_perplexity=1030.4244, train_loss=6.937726

Batch 91840, train_perplexity=972.7053, train_loss=6.880081

Batch 91850, train_perplexity=978.15186, train_loss=6.885665

Batch 91860, train_perplexity=1113.4014, train_loss=7.015175

Batch 91870, train_perplexity=1096.287, train_loss=6.9996843

Batch 91880, train_perplexity=1043.3893, train_loss=6.9502296

Batch 91890, train_perplexity=1053.5808, train_loss=6.95995

Batch 91900, train_perplexity=970.4501, train_loss=6.87776

Batch 91910, train_perplexity=968.16815, train_loss=6.875406

Batch 91920, train_perplexity=1057.0404, train_loss=6.963228

Batch 91930, train_perplexity=1119.0101, train_loss=7.0202

Batch 91940, train_perplexity=1029.1713, train_loss=6.936509

Batch 91950, train_perplexity=1172.2379, train_loss=7.06667

Batch 91960, train_perplexity=1126.1771, train_loss=7.026584

Batch 91970, train_perplexity=1049.8662, train_loss=6.956418

Batch 91980, train_perplexity=1071.1122, train_loss=6.976453

Batch 91990, train_perplexity=920.2629, train_loss=6.8246593

Batch 92000, train_perplexity=1247.2899, train_loss=7.1287284

Batch 92010, train_perplexity=974.4676, train_loss=6.8818913

Batch 92020, train_perplexity=1102.6096, train_loss=7.005435

Batch 92030, train_perplexity=1194.8806, train_loss=7.0858016

Batch 92040, train_perplexity=970.9666, train_loss=6.878292

Batch 92050, train_perplexity=1033.768, train_loss=6.9409657

Batch 92060, train_perplexity=1109.0977, train_loss=7.011302

Batch 92070, train_perplexity=974.32587, train_loss=6.881746

Batch 92080, train_perplexity=1095.1084, train_loss=6.9986086

Batch 92090, train_perplexity=1023.34937, train_loss=6.930836

Batch 92100, train_perplexity=1011.88574, train_loss=6.919571

Batch 92110, train_perplexity=1118.1696, train_loss=7.0194483

Batch 92120, train_perplexity=993.78674, train_loss=6.9015226

Batch 92130, train_perplexity=1059.9365, train_loss=6.9659643

Batch 92140, train_perplexity=962.96497, train_loss=6.870017

Batch 92150, train_perplexity=972.5008, train_loss=6.879871

Batch 92160, train_perplexity=1128.1288, train_loss=7.0283155

Batch 92170, train_perplexity=1083.0934, train_loss=6.9875765

Batch 92180, train_perplexity=1001.05206, train_loss=6.908807

Batch 92190, train_perplexity=1005.0679, train_loss=6.9128103

Batch 92200, train_perplexity=1044.7853, train_loss=6.9515667

Batch 92210, train_perplexity=1081.64, train_loss=6.9862337

Batch 92220, train_perplexity=1007.93884, train_loss=6.915663

Batch 92230, train_perplexity=1083.6917, train_loss=6.9881287

Batch 92240, train_perplexity=992.48065, train_loss=6.9002075

Batch 92250, train_perplexity=1028.6335, train_loss=6.9359865

Batch 92260, train_perplexity=993.90234, train_loss=6.901639

Batch 92270, train_perplexity=1115.7286, train_loss=7.017263

Batch 92280, train_perplexity=977.2274, train_loss=6.8847194

Batch 92290, train_perplexity=1035.4404, train_loss=6.942582

Batch 92300, train_perplexity=954.6992, train_loss=6.8613963

Batch 92310, train_perplexity=1038.7527, train_loss=6.945776

Batch 92320, train_perplexity=1104.137, train_loss=7.0068192

Batch 92330, train_perplexity=1058.3306, train_loss=6.964448

Batch 92340, train_perplexity=1005.8786, train_loss=6.9136167

Batch 92350, train_perplexity=1131.2789, train_loss=7.031104

Batch 92360, train_perplexity=980.5073, train_loss=6.88807

Batch 92370, train_perplexity=951.70074, train_loss=6.8582506

Batch 92380, train_perplexity=1119.9614, train_loss=7.0210495

Batch 92390, train_perplexity=1037.1937, train_loss=6.944274

Batch 92400, train_perplexity=1088.5568, train_loss=6.992608

Batch 92410, train_perplexity=1003.71344, train_loss=6.911462

Batch 92420, train_perplexity=1034.12, train_loss=6.941306

Batch 92430, train_perplexity=1018.62256, train_loss=6.9262066

Batch 92440, train_perplexity=1017.2742, train_loss=6.924882

Batch 92450, train_perplexity=1078.2897, train_loss=6.9831314

Batch 92460, train_perplexity=1078.3627, train_loss=6.983199

Batch 92470, train_perplexity=1027.5457, train_loss=6.9349284

Batch 92480, train_perplexity=1009.13824, train_loss=6.916852

Batch 92490, train_perplexity=1027.2278, train_loss=6.934619

Batch 92500, train_perplexity=999.1207, train_loss=6.9068756

Batch 92510, train_perplexity=1021.0628, train_loss=6.9285994

Batch 92520, train_perplexity=1229.2102, train_loss=7.114127

Batch 92530, train_perplexity=971.39404, train_loss=6.878732

Batch 92540, train_perplexity=1079.1898, train_loss=6.983966

Batch 92550, train_perplexity=1045.6237, train_loss=6.9523687

Batch 92560, train_perplexity=1101.9557, train_loss=7.004842

Batch 92570, train_perplexity=1024.8519, train_loss=6.9323034

Batch 92580, train_perplexity=1046.804, train_loss=6.953497

Batch 92590, train_perplexity=987.19293, train_loss=6.8948655

Batch 92600, train_perplexity=994.7577, train_loss=6.902499

Batch 92610, train_perplexity=974.6618, train_loss=6.8820906

Batch 92620, train_perplexity=1037.8397, train_loss=6.9448967

Batch 92630, train_perplexity=1123.258, train_loss=7.0239887

Batch 92640, train_perplexity=1086.0402, train_loss=6.9902935

Batch 92650, train_perplexity=1034.7139, train_loss=6.94188

Batch 92660, train_perplexity=1112.7852, train_loss=7.0146213

Batch 92670, train_perplexity=981.17706, train_loss=6.888753

Batch 92680, train_perplexity=1085.1715, train_loss=6.9894934

Batch 92690, train_perplexity=1127.6753, train_loss=7.0279136

Batch 92700, train_perplexity=1116.0782, train_loss=7.017576
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 92710, train_perplexity=1088.6544, train_loss=6.9926977

Batch 92720, train_perplexity=1129.0404, train_loss=7.0291233

Batch 92730, train_perplexity=1073.8523, train_loss=6.9790077

Batch 92740, train_perplexity=1081.7504, train_loss=6.9863358

Batch 92750, train_perplexity=1118.5577, train_loss=7.0197954

Batch 92760, train_perplexity=1092.3682, train_loss=6.9961033

Batch 92770, train_perplexity=951.4693, train_loss=6.8580074

Batch 92780, train_perplexity=969.05817, train_loss=6.8763247

Batch 92790, train_perplexity=993.61615, train_loss=6.901351

Batch 92800, train_perplexity=1036.3092, train_loss=6.943421

Batch 92810, train_perplexity=977.65247, train_loss=6.8851542

Batch 92820, train_perplexity=1142.6935, train_loss=7.0411434

Batch 92830, train_perplexity=1087.4963, train_loss=6.9916334

Batch 92840, train_perplexity=1028.9789, train_loss=6.936322

Batch 92850, train_perplexity=991.62915, train_loss=6.899349

Batch 92860, train_perplexity=1026.7894, train_loss=6.934192

Batch 92870, train_perplexity=991.3616, train_loss=6.8990793

Batch 92880, train_perplexity=1113.978, train_loss=7.0156927

Batch 92890, train_perplexity=1084.7526, train_loss=6.989107

Batch 92900, train_perplexity=1051.1115, train_loss=6.9576035

Batch 92910, train_perplexity=1081.5153, train_loss=6.9861183

Batch 92920, train_perplexity=949.209, train_loss=6.855629

Batch 92930, train_perplexity=1118.9946, train_loss=7.020186

Batch 92940, train_perplexity=1119.2705, train_loss=7.0204325

Batch 92950, train_perplexity=1009.15265, train_loss=6.9168663

Batch 92960, train_perplexity=1075.5521, train_loss=6.9805894

Batch 92970, train_perplexity=1027.101, train_loss=6.9344954

Batch 92980, train_perplexity=980.38434, train_loss=6.8879447

Batch 92990, train_perplexity=1024.1045, train_loss=6.931574

Batch 93000, train_perplexity=1051.0243, train_loss=6.9575205

Batch 93010, train_perplexity=966.6943, train_loss=6.8738823

Batch 93020, train_perplexity=1197.846, train_loss=7.08828

Batch 93030, train_perplexity=1180.7589, train_loss=7.0739126

Batch 93040, train_perplexity=1074.2804, train_loss=6.9794064

Batch 93050, train_perplexity=1006.7966, train_loss=6.914529

Batch 93060, train_perplexity=1086.6965, train_loss=6.9908977

Batch 93070, train_perplexity=1091.6049, train_loss=6.9954042

Batch 93080, train_perplexity=1059.5414, train_loss=6.9655914

Batch 93090, train_perplexity=1002.3752, train_loss=6.9101276

Batch 93100, train_perplexity=1034.2734, train_loss=6.9414544

Batch 93110, train_perplexity=1066.1617, train_loss=6.9718204

Batch 93120, train_perplexity=990.2778, train_loss=6.8979855

Batch 93130, train_perplexity=1130.6038, train_loss=7.030507

Batch 93140, train_perplexity=1060.3379, train_loss=6.966343

Batch 93150, train_perplexity=1039.9993, train_loss=6.946975

Batch 93160, train_perplexity=1063.5498, train_loss=6.9693675

Batch 93170, train_perplexity=1120.5287, train_loss=7.021556

Batch 93180, train_perplexity=976.3797, train_loss=6.8838515

Batch 93190, train_perplexity=1031.3761, train_loss=6.938649

Batch 93200, train_perplexity=1088.3119, train_loss=6.992383

Batch 93210, train_perplexity=1016.1586, train_loss=6.9237847

Batch 93220, train_perplexity=1054.7134, train_loss=6.9610243

Batch 93230, train_perplexity=1011.5519, train_loss=6.919241

Batch 93240, train_perplexity=1074.6985, train_loss=6.9797955

Batch 93250, train_perplexity=1001.7884, train_loss=6.909542

Batch 93260, train_perplexity=1094.5654, train_loss=6.9981127

Batch 93270, train_perplexity=1081.4296, train_loss=6.986039

Batch 93280, train_perplexity=1064.4199, train_loss=6.9701853

Batch 93290, train_perplexity=1069.8035, train_loss=6.97523

Batch 93300, train_perplexity=990.3099, train_loss=6.898018

Batch 93310, train_perplexity=994.08203, train_loss=6.9018197

Batch 93320, train_perplexity=1078.9789, train_loss=6.9837704

Batch 93330, train_perplexity=1061.6857, train_loss=6.967613

Batch 93340, train_perplexity=1063.5549, train_loss=6.9693723

Batch 93350, train_perplexity=1028.7361, train_loss=6.936086

Batch 93360, train_perplexity=1104.8875, train_loss=7.0074987

Batch 93370, train_perplexity=1014.90393, train_loss=6.9225492

Batch 93380, train_perplexity=1083.222, train_loss=6.987695

Batch 93390, train_perplexity=912.75305, train_loss=6.8164654

Batch 93400, train_perplexity=1058.5162, train_loss=6.9646235

Batch 93410, train_perplexity=986.5549, train_loss=6.894219

Batch 93420, train_perplexity=1100.2808, train_loss=7.0033207

Batch 93430, train_perplexity=1083.2499, train_loss=6.987721

Batch 93440, train_perplexity=1035.9836, train_loss=6.9431067

Batch 93450, train_perplexity=971.931, train_loss=6.879285

Batch 93460, train_perplexity=985.67035, train_loss=6.893322

Batch 93470, train_perplexity=954.19946, train_loss=6.8608727

Batch 93480, train_perplexity=1058.7576, train_loss=6.9648514

Batch 93490, train_perplexity=1114.8251, train_loss=7.016453

Batch 93500, train_perplexity=1027.3928, train_loss=6.9347796

Batch 93510, train_perplexity=1063.8192, train_loss=6.9696207

Batch 93520, train_perplexity=940.07275, train_loss=6.8459573

Batch 93530, train_perplexity=1063.5828, train_loss=6.9693985

Batch 93540, train_perplexity=987.8414, train_loss=6.895522

Batch 93550, train_perplexity=1099.1041, train_loss=7.0022507

Batch 93560, train_perplexity=1165.1677, train_loss=7.0606203

Batch 93570, train_perplexity=1019.8036, train_loss=6.9273653

Batch 93580, train_perplexity=1131.2314, train_loss=7.031062

Batch 93590, train_perplexity=1157.3408, train_loss=7.05388

Batch 93600, train_perplexity=1061.9131, train_loss=6.9678273

Batch 93610, train_perplexity=1017.49054, train_loss=6.9250946

Batch 93620, train_perplexity=1024.0791, train_loss=6.931549

Batch 93630, train_perplexity=1049.3452, train_loss=6.9559216

Batch 93640, train_perplexity=1070.7936, train_loss=6.9761553

Batch 93650, train_perplexity=1058.3588, train_loss=6.9644747

Batch 93660, train_perplexity=993.49677, train_loss=6.901231

Batch 93670, train_perplexity=1058.4865, train_loss=6.9645953

Batch 93680, train_perplexity=1092.1287, train_loss=6.995884

Batch 93690, train_perplexity=1044.6418, train_loss=6.9514294

Batch 93700, train_perplexity=995.8265, train_loss=6.903573

Batch 93710, train_perplexity=955.12494, train_loss=6.861842

Batch 93720, train_perplexity=1108.7893, train_loss=7.011024

Batch 93730, train_perplexity=1035.3357, train_loss=6.942481

Batch 93740, train_perplexity=1048.841, train_loss=6.955441

Batch 93750, train_perplexity=1057.3167, train_loss=6.9634895

Batch 93760, train_perplexity=1053.1855, train_loss=6.9595747

Batch 93770, train_perplexity=1079.9753, train_loss=6.9846935

Batch 93780, train_perplexity=1106.3087, train_loss=7.0087843

Batch 93790, train_perplexity=907.9642, train_loss=6.811205

Batch 93800, train_perplexity=942.7603, train_loss=6.848812

Batch 93810, train_perplexity=989.35645, train_loss=6.8970547

Batch 93820, train_perplexity=1051.312, train_loss=6.957794

Batch 93830, train_perplexity=1002.4048, train_loss=6.910157

Batch 93840, train_perplexity=1063.8253, train_loss=6.9696264

Batch 93850, train_perplexity=1165.9625, train_loss=7.061302

Batch 93860, train_perplexity=1038.9727, train_loss=6.9459877

Batch 93870, train_perplexity=1020.9226, train_loss=6.928462

Batch 93880, train_perplexity=1079.546, train_loss=6.984296

Batch 93890, train_perplexity=985.7028, train_loss=6.893355

Batch 93900, train_perplexity=1090.1915, train_loss=6.9941087

Batch 93910, train_perplexity=1041.0973, train_loss=6.9480305

Batch 93920, train_perplexity=1024.2554, train_loss=6.931721

Batch 93930, train_perplexity=1020.0399, train_loss=6.927597

Batch 93940, train_perplexity=947.43634, train_loss=6.85376

Batch 93950, train_perplexity=954.605, train_loss=6.8612976

Batch 93960, train_perplexity=1016.8483, train_loss=6.9244633

Batch 93970, train_perplexity=1060.3698, train_loss=6.966373

Batch 93980, train_perplexity=945.1081, train_loss=6.8512993

Batch 93990, train_perplexity=1250.6912, train_loss=7.1314516

Batch 94000, train_perplexity=1029.775, train_loss=6.9370956

Batch 94010, train_perplexity=1034.0312, train_loss=6.9412203

Batch 94020, train_perplexity=1085.3713, train_loss=6.9896774
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 94030, train_perplexity=1059.9006, train_loss=6.9659305

Batch 94040, train_perplexity=1050.906, train_loss=6.957408

Batch 94050, train_perplexity=950.3747, train_loss=6.8568563

Batch 94060, train_perplexity=1101.9994, train_loss=7.0048814

Batch 94070, train_perplexity=1001.03204, train_loss=6.908787

Batch 94080, train_perplexity=1053.1031, train_loss=6.9594965

Batch 94090, train_perplexity=1104.5703, train_loss=7.0072117

Batch 94100, train_perplexity=1000.5276, train_loss=6.9082828

Batch 94110, train_perplexity=995.5891, train_loss=6.9033346

Batch 94120, train_perplexity=1035.1294, train_loss=6.9422817

Batch 94130, train_perplexity=1037.2283, train_loss=6.9443073

Batch 94140, train_perplexity=1052.1204, train_loss=6.958563

Batch 94150, train_perplexity=997.5812, train_loss=6.9053335

Batch 94160, train_perplexity=1005.20874, train_loss=6.9129505

Batch 94170, train_perplexity=1046.3563, train_loss=6.953069

Batch 94180, train_perplexity=1050.2217, train_loss=6.9567566

Batch 94190, train_perplexity=1153.3208, train_loss=7.0504007

Batch 94200, train_perplexity=1153.8346, train_loss=7.050846

Batch 94210, train_perplexity=1077.3615, train_loss=6.9822702

Batch 94220, train_perplexity=1005.8541, train_loss=6.9135923

Batch 94230, train_perplexity=1005.3142, train_loss=6.9130554

Batch 94240, train_perplexity=1004.51544, train_loss=6.9122605

Batch 94250, train_perplexity=970.33203, train_loss=6.8776383

Batch 94260, train_perplexity=976.76105, train_loss=6.884242

Batch 94270, train_perplexity=1123.6492, train_loss=7.024337

Batch 94280, train_perplexity=1069.8279, train_loss=6.975253

Batch 94290, train_perplexity=1144.9231, train_loss=7.0430927

Batch 94300, train_perplexity=1055.7357, train_loss=6.961993

Batch 94310, train_perplexity=1075.1752, train_loss=6.980239

Batch 94320, train_perplexity=1050.0806, train_loss=6.956622

Batch 94330, train_perplexity=1066.8977, train_loss=6.9725103

Batch 94340, train_perplexity=1055.2245, train_loss=6.9615088

Batch 94350, train_perplexity=1077.4436, train_loss=6.9823465

Batch 94360, train_perplexity=1104.4218, train_loss=7.007077

Batch 94370, train_perplexity=1068.454, train_loss=6.973968

Batch 94380, train_perplexity=1095.5973, train_loss=6.999055

Batch 94390, train_perplexity=1139.3126, train_loss=7.0381804

Batch 94400, train_perplexity=992.0293, train_loss=6.8997526

Batch 94410, train_perplexity=957.6322, train_loss=6.864464

Batch 94420, train_perplexity=1094.0082, train_loss=6.9976034

Batch 94430, train_perplexity=971.4343, train_loss=6.8787737

Batch 94440, train_perplexity=1134.9541, train_loss=7.0343475

Batch 94450, train_perplexity=1214.0145, train_loss=7.101688

Batch 94460, train_perplexity=993.90045, train_loss=6.901637

Batch 94470, train_perplexity=1051.3691, train_loss=6.9578485

Batch 94480, train_perplexity=1118.4996, train_loss=7.0197434

Batch 94490, train_perplexity=1100.3143, train_loss=7.003351

Batch 94500, train_perplexity=894.97375, train_loss=6.7967944

Batch 94510, train_perplexity=1015.9357, train_loss=6.9235654

Batch 94520, train_perplexity=1091.1302, train_loss=6.9949694

Batch 94530, train_perplexity=1002.09705, train_loss=6.90985

Batch 94540, train_perplexity=1004.75446, train_loss=6.9124985

Batch 94550, train_perplexity=952.6047, train_loss=6.8592

Batch 94560, train_perplexity=1125.3961, train_loss=7.0258904

Batch 94570, train_perplexity=1118.0804, train_loss=7.0193686

Batch 94580, train_perplexity=1022.0926, train_loss=6.9296074

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Batch 94590, train_perplexity=1084.2379, train_loss=6.9886327

Batch 94600, train_perplexity=1121.5652, train_loss=7.0224805

Batch 94610, train_perplexity=943.64276, train_loss=6.8497477

Batch 94620, train_perplexity=967.9609, train_loss=6.8751917

Batch 94630, train_perplexity=912.8506, train_loss=6.816572

Batch 94640, train_perplexity=942.337, train_loss=6.848363

Batch 94650, train_perplexity=1017.4983, train_loss=6.925102

Batch 94660, train_perplexity=1031.1892, train_loss=6.938468

Batch 94670, train_perplexity=958.5729, train_loss=6.8654456

Batch 94680, train_perplexity=1002.9771, train_loss=6.910728

Batch 94690, train_perplexity=993.5162, train_loss=6.9012504

Batch 94700, train_perplexity=1061.1036, train_loss=6.967065

Batch 94710, train_perplexity=1015.8684, train_loss=6.923499

Batch 94720, train_perplexity=1006.2777, train_loss=6.9140134

Batch 94730, train_perplexity=1285.936, train_loss=7.159242

Batch 94740, train_perplexity=1069.9697, train_loss=6.9753857

Batch 94750, train_perplexity=979.5671, train_loss=6.8871107

Batch 94760, train_perplexity=1078.2084, train_loss=6.983056

Batch 94770, train_perplexity=1029.995, train_loss=6.9373093

Batch 94780, train_perplexity=1050.5929, train_loss=6.95711

Batch 94790, train_perplexity=1082.4542, train_loss=6.986986

Batch 94800, train_perplexity=1040.0681, train_loss=6.9470415

Batch 94810, train_perplexity=1049.7876, train_loss=6.956343

Batch 94820, train_perplexity=1025.3711, train_loss=6.93281

Batch 94830, train_perplexity=1017.7312, train_loss=6.925331

Batch 94840, train_perplexity=1039.1401, train_loss=6.946149

Batch 94850, train_perplexity=1046.6971, train_loss=6.953395

Batch 94860, train_perplexity=1054.1482, train_loss=6.9604883

Batch 94870, train_perplexity=1066.9515, train_loss=6.972561

Batch 94880, train_perplexity=1015.10724, train_loss=6.9227495

Batch 94890, train_perplexity=1043.8362, train_loss=6.950658

Batch 94900, train_perplexity=927.2577, train_loss=6.8322315

Batch 94910, train_perplexity=1039.5664, train_loss=6.946559

Batch 94920, train_perplexity=961.7645, train_loss=6.8687696

Batch 94930, train_perplexity=955.81335, train_loss=6.8625627

Batch 94940, train_perplexity=1028.9946, train_loss=6.9363375

Batch 94950, train_perplexity=1011.9108, train_loss=6.9195957

Batch 94960, train_perplexity=1056.5789, train_loss=6.9627914

Batch 94970, train_perplexity=965.9644, train_loss=6.873127

Batch 94980, train_perplexity=930.74884, train_loss=6.8359895

Batch 94990, train_perplexity=1080.4988, train_loss=6.985178

Batch 95000, train_perplexity=1126.2223, train_loss=7.026624

Batch 95010, train_perplexity=1021.28925, train_loss=6.928821

Batch 95020, train_perplexity=905.78125, train_loss=6.808798

Batch 95030, train_perplexity=1105.8536, train_loss=7.008373

Batch 95040, train_perplexity=1000.55054, train_loss=6.9083056

Batch 95050, train_perplexity=1055.1539, train_loss=6.961442

Batch 95060, train_perplexity=1070.0922, train_loss=6.9755

Batch 95070, train_perplexity=1041.9902, train_loss=6.948888

Batch 95080, train_perplexity=961.732, train_loss=6.868736

Batch 95090, train_perplexity=941.4845, train_loss=6.847458

Batch 95100, train_perplexity=1108.1693, train_loss=7.0104647

Batch 95110, train_perplexity=1057.1503, train_loss=6.963332

Batch 95120, train_perplexity=1050.6951, train_loss=6.957207

Batch 95130, train_perplexity=1031.9526, train_loss=6.939208

Batch 95140, train_perplexity=909.0463, train_loss=6.812396

Batch 95150, train_perplexity=922.237, train_loss=6.8268023

Batch 95160, train_perplexity=966.6505, train_loss=6.873837

Batch 95170, train_perplexity=953.2413, train_loss=6.859868

Batch 95180, train_perplexity=1091.9672, train_loss=6.995736

Batch 95190, train_perplexity=1098.1497, train_loss=7.001382

Batch 95200, train_perplexity=1121.4406, train_loss=7.0223694

Batch 95210, train_perplexity=995.0841, train_loss=6.9028273

Batch 95220, train_perplexity=973.3498, train_loss=6.8807435

Batch 95230, train_perplexity=1028.2015, train_loss=6.9355664

Batch 95240, train_perplexity=971.44824, train_loss=6.878788

Batch 95250, train_perplexity=1042.8253, train_loss=6.949689

Batch 95260, train_perplexity=989.10785, train_loss=6.8968034

Batch 95270, train_perplexity=1027.6599, train_loss=6.9350395

Batch 95280, train_perplexity=1129.1345, train_loss=7.0292068
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 95290, train_perplexity=1105.3069, train_loss=7.0078783

Batch 95300, train_perplexity=1005.38275, train_loss=6.9131236

Batch 95310, train_perplexity=983.4188, train_loss=6.891035

Batch 95320, train_perplexity=1068.1255, train_loss=6.9736605

Batch 95330, train_perplexity=1014.7147, train_loss=6.922363

Batch 95340, train_perplexity=1168.6754, train_loss=7.0636263

Batch 95350, train_perplexity=1023.2347, train_loss=6.930724

Batch 95360, train_perplexity=1067.5221, train_loss=6.9730954

Batch 95370, train_perplexity=911.61304, train_loss=6.8152156

Batch 95380, train_perplexity=995.2151, train_loss=6.902959

Batch 95390, train_perplexity=1037.1957, train_loss=6.944276

Batch 95400, train_perplexity=1048.6084, train_loss=6.9552193

Batch 95410, train_perplexity=1035.6646, train_loss=6.9427986

Batch 95420, train_perplexity=1120.977, train_loss=7.021956

Batch 95430, train_perplexity=1022.4007, train_loss=6.9299088

Batch 95440, train_perplexity=956.656, train_loss=6.863444

Batch 95450, train_perplexity=1046.3717, train_loss=6.953084

Batch 95460, train_perplexity=1058.0197, train_loss=6.9641542

Batch 95470, train_perplexity=1067.825, train_loss=6.973379

Batch 95480, train_perplexity=982.7447, train_loss=6.8903494

Batch 95490, train_perplexity=996.09247, train_loss=6.90384

Batch 95500, train_perplexity=1013.4261, train_loss=6.921092

Batch 95510, train_perplexity=1030.9802, train_loss=6.9382653

Batch 95520, train_perplexity=1009.684, train_loss=6.9173927

Batch 95530, train_perplexity=994.4077, train_loss=6.9021473

Batch 95540, train_perplexity=1025.0797, train_loss=6.9325256

Batch 95550, train_perplexity=1086.2826, train_loss=6.9905167

Batch 95560, train_perplexity=983.84375, train_loss=6.891467

Batch 95570, train_perplexity=1049.702, train_loss=6.9562616

Batch 95580, train_perplexity=1008.42633, train_loss=6.9161463

Batch 95590, train_perplexity=994.3423, train_loss=6.9020815

Batch 95600, train_perplexity=1086.8302, train_loss=6.9910207

Batch 95610, train_perplexity=993.6237, train_loss=6.9013586

Batch 95620, train_perplexity=972.8658, train_loss=6.880246

Batch 95630, train_perplexity=1069.22, train_loss=6.9746847

Batch 95640, train_perplexity=1069.2552, train_loss=6.9747176

Batch 95650, train_perplexity=983.66406, train_loss=6.8912845

Batch 95660, train_perplexity=1046.0386, train_loss=6.9527655

Batch 95670, train_perplexity=1117.0675, train_loss=7.018462

Batch 95680, train_perplexity=958.81836, train_loss=6.8657017

Batch 95690, train_perplexity=971.2732, train_loss=6.8786077

Batch 95700, train_perplexity=1069.8147, train_loss=6.9752407

Batch 95710, train_perplexity=1051.304, train_loss=6.9577866

Batch 95720, train_perplexity=1005.05444, train_loss=6.912797

Batch 95730, train_perplexity=1049.1166, train_loss=6.9557037

Batch 95740, train_perplexity=1073.3726, train_loss=6.978561

Batch 95750, train_perplexity=1070.4504, train_loss=6.975835

Batch 95760, train_perplexity=999.1326, train_loss=6.9068875

Batch 95770, train_perplexity=1031.8385, train_loss=6.9390974

Batch 95780, train_perplexity=970.76294, train_loss=6.8780823

Batch 95790, train_perplexity=1179.85, train_loss=7.0731425

Batch 95800, train_perplexity=1047.8217, train_loss=6.9544687

Batch 95810, train_perplexity=1007.8427, train_loss=6.9155674

Batch 95820, train_perplexity=1024.4298, train_loss=6.9318914

Batch 95830, train_perplexity=1010.6166, train_loss=6.918316

Batch 95840, train_perplexity=1011.51086, train_loss=6.9192004

Batch 95850, train_perplexity=1058.8661, train_loss=6.964954

Batch 95860, train_perplexity=1007.94073, train_loss=6.9156647

Batch 95870, train_perplexity=1114.14, train_loss=7.015838

Batch 95880, train_perplexity=962.20764, train_loss=6.8692303

Batch 95890, train_perplexity=1062.976, train_loss=6.9688277

Batch 95900, train_perplexity=1039.5015, train_loss=6.9464965

Batch 95910, train_perplexity=986.7981, train_loss=6.8944654

Batch 95920, train_perplexity=983.4868, train_loss=6.891104

Batch 95930, train_perplexity=1128.6377, train_loss=7.0287666

Batch 95940, train_perplexity=1029.32, train_loss=6.9366536

Batch 95950, train_perplexity=1011.61456, train_loss=6.919303

Batch 95960, train_perplexity=1061.6998, train_loss=6.9676266

Batch 95970, train_perplexity=1008.3686, train_loss=6.916089

Batch 95980, train_perplexity=1054.1914, train_loss=6.9605293

Batch 95990, train_perplexity=928.1362, train_loss=6.8331785

Batch 96000, train_perplexity=1191.43, train_loss=7.0829096

Batch 96010, train_perplexity=1133.2615, train_loss=7.032855

Batch 96020, train_perplexity=967.43024, train_loss=6.8746433

Batch 96030, train_perplexity=1119.9272, train_loss=7.021019

Batch 96040, train_perplexity=1127.0018, train_loss=7.027316

Batch 96050, train_perplexity=922.5194, train_loss=6.8271084

Batch 96060, train_perplexity=1030.3276, train_loss=6.937632

Batch 96070, train_perplexity=1232.3518, train_loss=7.1166797

Batch 96080, train_perplexity=979.4961, train_loss=6.887038

Batch 96090, train_perplexity=988.7726, train_loss=6.8964643

Batch 96100, train_perplexity=1111.4122, train_loss=7.0133867

Batch 96110, train_perplexity=962.9553, train_loss=6.870007

Batch 96120, train_perplexity=1066.3778, train_loss=6.972023

Batch 96130, train_perplexity=1005.3228, train_loss=6.913064

Batch 96140, train_perplexity=1138.1571, train_loss=7.0371656

Batch 96150, train_perplexity=1074.7917, train_loss=6.9798822

Batch 96160, train_perplexity=981.4812, train_loss=6.889063

Batch 96170, train_perplexity=1083.7831, train_loss=6.988213

Batch 96180, train_perplexity=989.1376, train_loss=6.8968334

Batch 96190, train_perplexity=1004.94995, train_loss=6.912693

Batch 96200, train_perplexity=1036.9039, train_loss=6.9439945

Batch 96210, train_perplexity=1061.338, train_loss=6.9672856

Batch 96220, train_perplexity=1017.4682, train_loss=6.9250727

Batch 96230, train_perplexity=1021.9113, train_loss=6.92943

Batch 96240, train_perplexity=1140.4518, train_loss=7.03918

Batch 96250, train_perplexity=1010.7679, train_loss=6.9184656

Batch 96260, train_perplexity=1033.1244, train_loss=6.940343

Batch 96270, train_perplexity=1104.9232, train_loss=7.007531

Batch 96280, train_perplexity=937.958, train_loss=6.843705

Batch 96290, train_perplexity=1068.7612, train_loss=6.9742556

Batch 96300, train_perplexity=1121.261, train_loss=7.022209

Batch 96310, train_perplexity=1005.9942, train_loss=6.9137316

Batch 96320, train_perplexity=1023.0083, train_loss=6.930503

Batch 96330, train_perplexity=992.91614, train_loss=6.900646

Batch 96340, train_perplexity=1057.6056, train_loss=6.9637628

Batch 96350, train_perplexity=1118.936, train_loss=7.0201335

Batch 96360, train_perplexity=962.4568, train_loss=6.869489

Batch 96370, train_perplexity=1011.74677, train_loss=6.9194336

Batch 96380, train_perplexity=1009.62244, train_loss=6.9173317

Batch 96390, train_perplexity=1010.8113, train_loss=6.9185085

Batch 96400, train_perplexity=1009.10455, train_loss=6.9168186

Batch 96410, train_perplexity=1076.0604, train_loss=6.981062

Batch 96420, train_perplexity=1122.812, train_loss=7.0235915

Batch 96430, train_perplexity=1066.9583, train_loss=6.972567

Batch 96440, train_perplexity=1009.1512, train_loss=6.916865

Batch 96450, train_perplexity=1025.5774, train_loss=6.933011

Batch 96460, train_perplexity=984.1825, train_loss=6.8918114

Batch 96470, train_perplexity=1142.2565, train_loss=7.040761

Batch 96480, train_perplexity=1039.3973, train_loss=6.9463964

Batch 96490, train_perplexity=1062.191, train_loss=6.968089

Batch 96500, train_perplexity=928.2265, train_loss=6.833276

Batch 96510, train_perplexity=1086.5354, train_loss=6.9907494

Batch 96520, train_perplexity=1067.0219, train_loss=6.9726267

Batch 96530, train_perplexity=1060.0406, train_loss=6.9660625

Batch 96540, train_perplexity=1026.0265, train_loss=6.933449

Batch 96550, train_perplexity=1017.8059, train_loss=6.9254045

Batch 96560, train_perplexity=1065.7628, train_loss=6.971446

Batch 96570, train_perplexity=1004.02075, train_loss=6.911768

Batch 96580, train_perplexity=1005.2519, train_loss=6.9129934

Batch 96590, train_perplexity=1049.9508, train_loss=6.9564986

Batch 96600, train_perplexity=1015.46014, train_loss=6.923097
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 96610, train_perplexity=1041.0099, train_loss=6.9479465

Batch 96620, train_perplexity=981.3783, train_loss=6.888958

Batch 96630, train_perplexity=976.9679, train_loss=6.884454

Batch 96640, train_perplexity=1027.1093, train_loss=6.9345036

Batch 96650, train_perplexity=1041.5973, train_loss=6.9485106

Batch 96660, train_perplexity=1028.3118, train_loss=6.9356737

Batch 96670, train_perplexity=1053.4768, train_loss=6.9598513

Batch 96680, train_perplexity=995.10596, train_loss=6.902849

Batch 96690, train_perplexity=967.3869, train_loss=6.8745985

Batch 96700, train_perplexity=983.9779, train_loss=6.8916035

Batch 96710, train_perplexity=1074.7385, train_loss=6.9798326

Batch 96720, train_perplexity=1129.8417, train_loss=7.029833

Batch 96730, train_perplexity=982.6702, train_loss=6.8902736

Batch 96740, train_perplexity=1091.583, train_loss=6.995384

Batch 96750, train_perplexity=1004.4862, train_loss=6.9122314

Batch 96760, train_perplexity=1086.7437, train_loss=6.990941

Batch 96770, train_perplexity=923.87213, train_loss=6.8285737

Batch 96780, train_perplexity=1040.2388, train_loss=6.9472055

Batch 96790, train_perplexity=945.1766, train_loss=6.851372

Batch 96800, train_perplexity=1057.7059, train_loss=6.9638577

Batch 96810, train_perplexity=1039.0588, train_loss=6.9460707

Batch 96820, train_perplexity=965.35846, train_loss=6.8724995

Batch 96830, train_perplexity=987.79376, train_loss=6.895474

Batch 96840, train_perplexity=1061.1371, train_loss=6.9670963

Batch 96850, train_perplexity=934.75, train_loss=6.840279

Batch 96860, train_perplexity=1057.7443, train_loss=6.963894

Batch 96870, train_perplexity=1085.0018, train_loss=6.989337

Batch 96880, train_perplexity=1023.74567, train_loss=6.9312234

Batch 96890, train_perplexity=1017.4983, train_loss=6.925102

Batch 96900, train_perplexity=1005.80615, train_loss=6.9135447

Batch 96910, train_perplexity=977.9406, train_loss=6.885449

Batch 96920, train_perplexity=926.48956, train_loss=6.831403

Batch 96930, train_perplexity=928.3407, train_loss=6.833399

Batch 96940, train_perplexity=1114.2814, train_loss=7.015965

Batch 96950, train_perplexity=1005.4168, train_loss=6.9131575

Batch 96960, train_perplexity=967.29785, train_loss=6.8745065

Batch 96970, train_perplexity=995.20087, train_loss=6.9029446

Batch 96980, train_perplexity=1078.2557, train_loss=6.9831

Batch 96990, train_perplexity=997.1132, train_loss=6.9048643

Batch 97000, train_perplexity=1051.9218, train_loss=6.958374

Batch 97010, train_perplexity=1053.7401, train_loss=6.960101

Batch 97020, train_perplexity=1016.43243, train_loss=6.924054

Batch 97030, train_perplexity=1046.0265, train_loss=6.952754

Batch 97040, train_perplexity=1158.6743, train_loss=7.055032

Batch 97050, train_perplexity=1103.6395, train_loss=7.0063686

Batch 97060, train_perplexity=1118.9696, train_loss=7.0201635

Batch 97070, train_perplexity=957.7076, train_loss=6.8645425

Batch 97080, train_perplexity=1036.4862, train_loss=6.9435916

Batch 97090, train_perplexity=1078.701, train_loss=6.983513

Batch 97100, train_perplexity=988.60284, train_loss=6.8962927

Batch 97110, train_perplexity=988.3229, train_loss=6.8960094

Batch 97120, train_perplexity=953.8069, train_loss=6.860461

Batch 97130, train_perplexity=1104.1154, train_loss=7.0067997

Batch 97140, train_perplexity=940.5566, train_loss=6.846472

Batch 97150, train_perplexity=996.5376, train_loss=6.904287

Batch 97160, train_perplexity=987.7514, train_loss=6.895431

Batch 97170, train_perplexity=1045.2946, train_loss=6.952054

Batch 97180, train_perplexity=938.5101, train_loss=6.8442936

Batch 97190, train_perplexity=1109.2404, train_loss=7.0114307

Batch 97200, train_perplexity=1175.5399, train_loss=7.069483

Batch 97210, train_perplexity=968.77545, train_loss=6.876033

Batch 97220, train_perplexity=999.8871, train_loss=6.9076424

Batch 97230, train_perplexity=1087.2651, train_loss=6.9914207

Batch 97240, train_perplexity=1029.0858, train_loss=6.936426

Batch 97250, train_perplexity=1001.20624, train_loss=6.908961

Batch 97260, train_perplexity=895.612, train_loss=6.7975073

Batch 97270, train_perplexity=1132.9844, train_loss=7.0326104

Batch 97280, train_perplexity=1028.9131, train_loss=6.9362583

Batch 97290, train_perplexity=1089.0323, train_loss=6.993045

Batch 97300, train_perplexity=1013.5382, train_loss=6.9212027

Batch 97310, train_perplexity=1214.4348, train_loss=7.102034

Batch 97320, train_perplexity=1070.8993, train_loss=6.976254

Batch 97330, train_perplexity=1062.3055, train_loss=6.968197

Batch 97340, train_perplexity=1104.3702, train_loss=7.0070305

Batch 97350, train_perplexity=1050.053, train_loss=6.956596

Batch 97360, train_perplexity=1046.8673, train_loss=6.9535575

Batch 97370, train_perplexity=1012.7073, train_loss=6.9203825

Batch 97380, train_perplexity=953.9161, train_loss=6.8605757

Batch 97390, train_perplexity=980.54614, train_loss=6.8881097

Batch 97400, train_perplexity=908.0707, train_loss=6.811322

Batch 97410, train_perplexity=970.57684, train_loss=6.8778906

Batch 97420, train_perplexity=954.9455, train_loss=6.8616543

Batch 97430, train_perplexity=1063.5432, train_loss=6.9693613

Batch 97440, train_perplexity=1026.457, train_loss=6.9338684

Batch 97450, train_perplexity=1011.9017, train_loss=6.9195867

Batch 97460, train_perplexity=1114.42, train_loss=7.0160894

Batch 97470, train_perplexity=1021.14075, train_loss=6.9286757

Batch 97480, train_perplexity=981.17426, train_loss=6.88875

Batch 97490, train_perplexity=1019.2479, train_loss=6.9268203

Batch 97500, train_perplexity=1018.8154, train_loss=6.926396

Batch 97510, train_perplexity=1079.6422, train_loss=6.984385

Batch 97520, train_perplexity=983.59985, train_loss=6.891219

Batch 97530, train_perplexity=1018.2914, train_loss=6.9258814

Batch 97540, train_perplexity=1070.623, train_loss=6.975996

Batch 97550, train_perplexity=1066.2955, train_loss=6.971946

Batch 97560, train_perplexity=1000.4723, train_loss=6.9082274

Batch 97570, train_perplexity=1128.5387, train_loss=7.028679

Batch 97580, train_perplexity=1081.0966, train_loss=6.985731

Batch 97590, train_perplexity=990.41656, train_loss=6.8981256

Batch 97600, train_perplexity=942.6426, train_loss=6.848687

Batch 97610, train_perplexity=1060.8406, train_loss=6.966817

Batch 97620, train_perplexity=1025.4557, train_loss=6.9328923

Batch 97630, train_perplexity=1014.6054, train_loss=6.922255

Batch 97640, train_perplexity=1046.9567, train_loss=6.953643

Batch 97650, train_perplexity=993.93365, train_loss=6.9016705

Batch 97660, train_perplexity=982.61914, train_loss=6.8902216

Batch 97670, train_perplexity=1039.6208, train_loss=6.9466114

Batch 97680, train_perplexity=967.85474, train_loss=6.875082

Batch 97690, train_perplexity=1034.3834, train_loss=6.9415607

Batch 97700, train_perplexity=964.2511, train_loss=6.8713517

Batch 97710, train_perplexity=1089.8667, train_loss=6.9938107

Batch 97720, train_perplexity=1082.703, train_loss=6.987216

Batch 97730, train_perplexity=986.94867, train_loss=6.894618

Batch 97740, train_perplexity=996.34424, train_loss=6.904093

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Batch 97750, train_perplexity=1011.8597, train_loss=6.919545

Batch 97760, train_perplexity=1046.0535, train_loss=6.95278

Batch 97770, train_perplexity=1034.3448, train_loss=6.9415236

Batch 97780, train_perplexity=935.1552, train_loss=6.8407125

Batch 97790, train_perplexity=939.15875, train_loss=6.8449845

Batch 97800, train_perplexity=1105.9164, train_loss=7.0084295

Batch 97810, train_perplexity=1128.6915, train_loss=7.0288143

Batch 97820, train_perplexity=973.30334, train_loss=6.880696

Batch 97830, train_perplexity=994.18866, train_loss=6.901927

Batch 97840, train_perplexity=884.4061, train_loss=6.7849164

Batch 97850, train_perplexity=1072.7775, train_loss=6.9780064

Batch 97860, train_perplexity=1136.7219, train_loss=7.035904
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 97870, train_perplexity=1001.9403, train_loss=6.9096937

Batch 97880, train_perplexity=1145.6057, train_loss=7.043689

Batch 97890, train_perplexity=1007.3719, train_loss=6.9151

Batch 97900, train_perplexity=1078.8965, train_loss=6.983694

Batch 97910, train_perplexity=1000.5567, train_loss=6.908312

Batch 97920, train_perplexity=949.7092, train_loss=6.856156

Batch 97930, train_perplexity=915.86505, train_loss=6.819869

Batch 97940, train_perplexity=1124.3942, train_loss=7.0249996

Batch 97950, train_perplexity=1055.769, train_loss=6.9620247

Batch 97960, train_perplexity=1065.6113, train_loss=6.971304

Batch 97970, train_perplexity=1025.7017, train_loss=6.933132

Batch 97980, train_perplexity=1028.74, train_loss=6.93609

Batch 97990, train_perplexity=1036.3359, train_loss=6.9434466

Batch 98000, train_perplexity=1046.75, train_loss=6.9534454

Batch 98010, train_perplexity=894.6081, train_loss=6.796386

Batch 98020, train_perplexity=1017.7613, train_loss=6.9253607

Batch 98030, train_perplexity=1047.459, train_loss=6.9541225

Batch 98040, train_perplexity=954.41656, train_loss=6.8611

Batch 98050, train_perplexity=1138.2787, train_loss=7.0372725

Batch 98060, train_perplexity=1119.4435, train_loss=7.020587

Batch 98070, train_perplexity=1038.6458, train_loss=6.945673

Batch 98080, train_perplexity=943.26666, train_loss=6.849349

Batch 98090, train_perplexity=1039.1937, train_loss=6.9462004

Batch 98100, train_perplexity=1051.7281, train_loss=6.95819

Batch 98110, train_perplexity=1004.8752, train_loss=6.9126186

Batch 98120, train_perplexity=1040.077, train_loss=6.94705

Batch 98130, train_perplexity=1113.7528, train_loss=7.0154905

Batch 98140, train_perplexity=1011.80273, train_loss=6.919489

Batch 98150, train_perplexity=1042.522, train_loss=6.949398

Batch 98160, train_perplexity=1014.35724, train_loss=6.9220104

Batch 98170, train_perplexity=1055.1721, train_loss=6.961459

Batch 98180, train_perplexity=1065.7048, train_loss=6.9713917

Batch 98190, train_perplexity=1092.6996, train_loss=6.9964066

Batch 98200, train_perplexity=1007.3147, train_loss=6.9150434

Batch 98210, train_perplexity=968.5546, train_loss=6.875805

Batch 98220, train_perplexity=1068.43, train_loss=6.9739456

Batch 98230, train_perplexity=1035.6952, train_loss=6.942828

Batch 98240, train_perplexity=994.97974, train_loss=6.9027224

Batch 98250, train_perplexity=1008.7495, train_loss=6.9164667

Batch 98260, train_perplexity=1115.276, train_loss=7.016857

Batch 98270, train_perplexity=962.774, train_loss=6.8698187

Batch 98280, train_perplexity=974.51404, train_loss=6.881939

Batch 98290, train_perplexity=1019.99225, train_loss=6.9275503

Batch 98300, train_perplexity=1077.21, train_loss=6.9821296

Batch 98310, train_perplexity=1011.4897, train_loss=6.9191794

Batch 98320, train_perplexity=1026.1384, train_loss=6.933558

Batch 98330, train_perplexity=1088.1759, train_loss=6.992258

Batch 98340, train_perplexity=1071.9757, train_loss=6.9772587

Batch 98350, train_perplexity=943.48346, train_loss=6.849579

Batch 98360, train_perplexity=1074.3542, train_loss=6.979475

Batch 98370, train_perplexity=1047.9357, train_loss=6.9545774

Batch 98380, train_perplexity=1043.5486, train_loss=6.950382

Batch 98390, train_perplexity=1042.8496, train_loss=6.9497123

Batch 98400, train_perplexity=1008.80145, train_loss=6.916518

Batch 98410, train_perplexity=1003.50574, train_loss=6.911255

Batch 98420, train_perplexity=1069.6403, train_loss=6.9750776

Batch 98430, train_perplexity=1070.3444, train_loss=6.9757357

Batch 98440, train_perplexity=1042.9541, train_loss=6.9498124

Batch 98450, train_perplexity=972.2778, train_loss=6.8796415

Batch 98460, train_perplexity=968.872, train_loss=6.8761325

Batch 98470, train_perplexity=913.72675, train_loss=6.8175316

Batch 98480, train_perplexity=1108.1112, train_loss=7.010412

Batch 98490, train_perplexity=1059.797, train_loss=6.9658327

Batch 98500, train_perplexity=1208.111, train_loss=7.096813

Batch 98510, train_perplexity=1032.4034, train_loss=6.939645

Batch 98520, train_perplexity=1033.2875, train_loss=6.9405007

Batch 98530, train_perplexity=1058.963, train_loss=6.9650455

Batch 98540, train_perplexity=935.76544, train_loss=6.841365

Batch 98550, train_perplexity=1096.9146, train_loss=7.0002565

Batch 98560, train_perplexity=1050.5939, train_loss=6.957111

Batch 98570, train_perplexity=974.1535, train_loss=6.881569

Batch 98580, train_perplexity=999.85754, train_loss=6.907613

Batch 98590, train_perplexity=1079.6881, train_loss=6.9844275

Batch 98600, train_perplexity=1126.1842, train_loss=7.0265903

Batch 98610, train_perplexity=1103.4879, train_loss=7.0062313

Batch 98620, train_perplexity=1001.2101, train_loss=6.9089646

Batch 98630, train_perplexity=1009.41736, train_loss=6.9171286

Batch 98640, train_perplexity=978.0604, train_loss=6.8855715

Batch 98650, train_perplexity=993.15765, train_loss=6.9008894

Batch 98660, train_perplexity=1081.8154, train_loss=6.986396

Batch 98670, train_perplexity=1024.9233, train_loss=6.932373

Batch 98680, train_perplexity=1089.6365, train_loss=6.9935994

Batch 98690, train_perplexity=1035.0529, train_loss=6.942208

Batch 98700, train_perplexity=1103.5537, train_loss=7.006291

Batch 98710, train_perplexity=1053.5352, train_loss=6.9599066

Batch 98720, train_perplexity=1030.8313, train_loss=6.938121

Batch 98730, train_perplexity=1017.00305, train_loss=6.9246154

Batch 98740, train_perplexity=969.86395, train_loss=6.877156

Batch 98750, train_perplexity=983.6514, train_loss=6.8912716

Batch 98760, train_perplexity=1023.35767, train_loss=6.9308443

Batch 98770, train_perplexity=1076.7667, train_loss=6.981718

Batch 98780, train_perplexity=1031.4636, train_loss=6.938734

Batch 98790, train_perplexity=1079.1868, train_loss=6.983963

Batch 98800, train_perplexity=1062.3475, train_loss=6.9682364

Batch 98810, train_perplexity=1085.4272, train_loss=6.989729

Batch 98820, train_perplexity=964.1003, train_loss=6.8711953

Batch 98830, train_perplexity=1089.954, train_loss=6.993891

Batch 98840, train_perplexity=986.425, train_loss=6.8940873

Batch 98850, train_perplexity=1039.7771, train_loss=6.9467616

Batch 98860, train_perplexity=1014.75055, train_loss=6.922398

Batch 98870, train_perplexity=1025.9932, train_loss=6.9334164

Batch 98880, train_perplexity=937.01746, train_loss=6.842702

Batch 98890, train_perplexity=984.14355, train_loss=6.891772

Batch 98900, train_perplexity=961.48114, train_loss=6.868475

Batch 98910, train_perplexity=1057.6525, train_loss=6.963807

Batch 98920, train_perplexity=1023.61725, train_loss=6.931098

Batch 98930, train_perplexity=1015.7526, train_loss=6.923385

Batch 98940, train_perplexity=1041.4617, train_loss=6.9483805

Batch 98950, train_perplexity=975.2253, train_loss=6.8826685

Batch 98960, train_perplexity=1100.4645, train_loss=7.0034876

Batch 98970, train_perplexity=1108.4558, train_loss=7.010723

Batch 98980, train_perplexity=1099.9419, train_loss=7.0030127

Batch 98990, train_perplexity=983.74384, train_loss=6.8913655

Batch 99000, train_perplexity=918.2675, train_loss=6.822489

Batch 99010, train_perplexity=1033.3954, train_loss=6.940605

Batch 99020, train_perplexity=1106.8971, train_loss=7.009316

Batch 99030, train_perplexity=1012.7826, train_loss=6.920457

Batch 99040, train_perplexity=1122.2269, train_loss=7.0230703

Batch 99050, train_perplexity=1079.8785, train_loss=6.984604

Batch 99060, train_perplexity=989.10126, train_loss=6.8967967

Batch 99070, train_perplexity=1058.4723, train_loss=6.964582

Batch 99080, train_perplexity=1031.173, train_loss=6.9384522

Batch 99090, train_perplexity=998.5825, train_loss=6.906337

Batch 99100, train_perplexity=1030.169, train_loss=6.937478

Batch 99110, train_perplexity=1060.5149, train_loss=6.96651

Batch 99120, train_perplexity=1060.3369, train_loss=6.966342

Batch 99130, train_perplexity=959.51086, train_loss=6.8664236

Batch 99140, train_perplexity=1047.8536, train_loss=6.9544992

Batch 99150, train_perplexity=1010.6281, train_loss=6.9183273

Batch 99160, train_perplexity=1045.2926, train_loss=6.952052

Batch 99170, train_perplexity=977.2535, train_loss=6.884746

Batch 99180, train_perplexity=1029.182, train_loss=6.9365196
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 99190, train_perplexity=1030.4175, train_loss=6.9377193

Batch 99200, train_perplexity=1069.8167, train_loss=6.9752426

Batch 99210, train_perplexity=998.6311, train_loss=6.9063854

Batch 99220, train_perplexity=956.16345, train_loss=6.862929

Batch 99230, train_perplexity=878.2876, train_loss=6.777974

Batch 99240, train_perplexity=1043.9358, train_loss=6.950753

Batch 99250, train_perplexity=1040.7538, train_loss=6.9477005

Batch 99260, train_perplexity=1015.50275, train_loss=6.923139

Batch 99270, train_perplexity=946.8465, train_loss=6.853137

Batch 99280, train_perplexity=1029.8065, train_loss=6.937126

Batch 99290, train_perplexity=956.04626, train_loss=6.8628063

Batch 99300, train_perplexity=1002.8174, train_loss=6.9105687

Batch 99310, train_perplexity=1010.5231, train_loss=6.9182234

Batch 99320, train_perplexity=974.5112, train_loss=6.881936

Batch 99330, train_perplexity=989.91754, train_loss=6.8976216

Batch 99340, train_perplexity=958.71277, train_loss=6.8655915

Batch 99350, train_perplexity=1117.3188, train_loss=7.0186872

Batch 99360, train_perplexity=1078.0779, train_loss=6.982935

Batch 99370, train_perplexity=1119.2578, train_loss=7.020421

Batch 99380, train_perplexity=1039.2789, train_loss=6.9462824

Batch 99390, train_perplexity=1104.8406, train_loss=7.0074563

Batch 99400, train_perplexity=964.9442, train_loss=6.8720703

Batch 99410, train_perplexity=965.3018, train_loss=6.872441

Batch 99420, train_perplexity=967.25494, train_loss=6.874462

Batch 99430, train_perplexity=994.183, train_loss=6.9019213

Batch 99440, train_perplexity=904.8096, train_loss=6.8077245

Batch 99450, train_perplexity=1032.4817, train_loss=6.9397206

Batch 99460, train_perplexity=1056.5728, train_loss=6.9627857

Batch 99470, train_perplexity=1042.189, train_loss=6.9490786

Batch 99480, train_perplexity=1032.6772, train_loss=6.93991

Batch 99490, train_perplexity=923.58453, train_loss=6.8282623

Batch 99500, train_perplexity=1060.4552, train_loss=6.9664536

Batch 99510, train_perplexity=1020.0779, train_loss=6.9276342

Batch 99520, train_perplexity=1094.5519, train_loss=6.9981003

Batch 99530, train_perplexity=1017.4537, train_loss=6.9250584

Batch 99540, train_perplexity=1039.1779, train_loss=6.946185

Batch 99550, train_perplexity=1023.8116, train_loss=6.931288

Batch 99560, train_perplexity=991.06, train_loss=6.898775

Batch 99570, train_perplexity=969.8986, train_loss=6.8771915

Batch 99580, train_perplexity=986.04736, train_loss=6.8937044

Batch 99590, train_perplexity=1012.9362, train_loss=6.9206085

Batch 99600, train_perplexity=975.7202, train_loss=6.883176

Batch 99610, train_perplexity=1001.4359, train_loss=6.90919

Batch 99620, train_perplexity=956.0727, train_loss=6.862834

Batch 99630, train_perplexity=931.9715, train_loss=6.837302

Batch 99640, train_perplexity=1078.8595, train_loss=6.9836597

Batch 99650, train_perplexity=897.1794, train_loss=6.799256

Batch 99660, train_perplexity=1031.382, train_loss=6.938655

Batch 99670, train_perplexity=1048.163, train_loss=6.9547944

Batch 99680, train_perplexity=1035.6251, train_loss=6.9427605

Batch 99690, train_perplexity=1068.0756, train_loss=6.9736137

Batch 99700, train_perplexity=941.568, train_loss=6.8475466

Batch 99710, train_perplexity=965.6264, train_loss=6.872777

Batch 99720, train_perplexity=1055.304, train_loss=6.961584

Batch 99730, train_perplexity=1017.0258, train_loss=6.924638

Batch 99740, train_perplexity=1085.3154, train_loss=6.989626

Batch 99750, train_perplexity=966.9741, train_loss=6.8741717

Batch 99760, train_perplexity=964.75836, train_loss=6.8718777

Batch 99770, train_perplexity=1021.29755, train_loss=6.928829

Batch 99780, train_perplexity=932.4542, train_loss=6.83782

Batch 99790, train_perplexity=952.12646, train_loss=6.858698

Batch 99800, train_perplexity=993.5446, train_loss=6.901279

Batch 99810, train_perplexity=1139.457, train_loss=7.038307

Batch 99820, train_perplexity=928.069, train_loss=6.833106

Batch 99830, train_perplexity=1093.6045, train_loss=6.9972343

Batch 99840, train_perplexity=938.3906, train_loss=6.8441663

Batch 99850, train_perplexity=1033.6704, train_loss=6.9408712

Batch 99860, train_perplexity=857.1635, train_loss=6.7536287

Batch 99870, train_perplexity=1033.8755, train_loss=6.9410696

Batch 99880, train_perplexity=1049.5078, train_loss=6.9560766

Batch 99890, train_perplexity=907.83514, train_loss=6.811063

Batch 99900, train_perplexity=1037.0562, train_loss=6.9441414

Batch 99910, train_perplexity=967.4967, train_loss=6.874712

Batch 99920, train_perplexity=936.3631, train_loss=6.8420033

Batch 99930, train_perplexity=970.8407, train_loss=6.8781624

Batch 99940, train_perplexity=1022.4962, train_loss=6.930002

Batch 99950, train_perplexity=955.267, train_loss=6.861991

Batch 99960, train_perplexity=1097.4252, train_loss=7.000722

Batch 99970, train_perplexity=1038.2952, train_loss=6.9453354

Batch 99980, train_perplexity=1032.051, train_loss=6.9393034

Batch 99990, train_perplexity=1095.7692, train_loss=6.999212

Batch 100000, train_perplexity=935.612, train_loss=6.841201

Batch 100010, train_perplexity=981.8384, train_loss=6.8894267

Batch 100020, train_perplexity=1114.436, train_loss=7.0161037

Batch 100030, train_perplexity=950.04846, train_loss=6.856513

Batch 100040, train_perplexity=1009.427, train_loss=6.917138

Batch 100050, train_perplexity=1082.6318, train_loss=6.98715

Batch 100060, train_perplexity=1044.589, train_loss=6.951379

Batch 100070, train_perplexity=1022.89026, train_loss=6.9303875

Batch 100080, train_perplexity=962.1916, train_loss=6.8692136

Batch 100090, train_perplexity=1113.8336, train_loss=7.015563

Batch 100100, train_perplexity=802.91254, train_loss=6.688246

Batch 100110, train_perplexity=978.9133, train_loss=6.886443

Batch 100120, train_perplexity=998.2359, train_loss=6.9059896

Batch 100130, train_perplexity=1000.9996, train_loss=6.9087543

Batch 100140, train_perplexity=979.6175, train_loss=6.887162

Batch 100150, train_perplexity=1055.7781, train_loss=6.9620333

Batch 100160, train_perplexity=1042.1403, train_loss=6.949032

Batch 100170, train_perplexity=1016.9458, train_loss=6.924559

Batch 100180, train_perplexity=1075.8357, train_loss=6.980853

Batch 100190, train_perplexity=1028.7532, train_loss=6.936103

Batch 100200, train_perplexity=1036.126, train_loss=6.943244

Batch 100210, train_perplexity=975.4113, train_loss=6.882859

Batch 100220, train_perplexity=1026.0626, train_loss=6.933484

Batch 100230, train_perplexity=1081.4858, train_loss=6.986091

Batch 100240, train_perplexity=1053.5195, train_loss=6.959892

Batch 100250, train_perplexity=1023.3513, train_loss=6.930838

Batch 100260, train_perplexity=1069.5413, train_loss=6.974985

Batch 100270, train_perplexity=973.96124, train_loss=6.8813715

Batch 100280, train_perplexity=1091.0449, train_loss=6.994891

Batch 100290, train_perplexity=939.6618, train_loss=6.84552

Batch 100300, train_perplexity=1000.239, train_loss=6.9079943

Batch 100310, train_perplexity=1065.4945, train_loss=6.9711943

Batch 100320, train_perplexity=1071.4473, train_loss=6.9767656

Batch 100330, train_perplexity=935.9243, train_loss=6.8415346

Batch 100340, train_perplexity=987.56445, train_loss=6.8952417

Batch 100350, train_perplexity=1174.7672, train_loss=7.0688252

Batch 100360, train_perplexity=1132.6235, train_loss=7.032292

Batch 100370, train_perplexity=913.66315, train_loss=6.817462

Batch 100380, train_perplexity=1018.54584, train_loss=6.9261312

Batch 100390, train_perplexity=988.3615, train_loss=6.8960485

Batch 100400, train_perplexity=1207.0883, train_loss=7.0959663

Batch 100410, train_perplexity=995.2293, train_loss=6.902973

Batch 100420, train_perplexity=1077.5957, train_loss=6.9824877

Batch 100430, train_perplexity=1071.6185, train_loss=6.9769254

Batch 100440, train_perplexity=927.84283, train_loss=6.8328624

Batch 100450, train_perplexity=1007.5823, train_loss=6.915309

Batch 100460, train_perplexity=999.6535, train_loss=6.9074087

Batch 100470, train_perplexity=962.6744, train_loss=6.869715

Batch 100480, train_perplexity=993.8635, train_loss=6.9016

Batch 100490, train_perplexity=1084.2333, train_loss=6.9886284

Batch 100500, train_perplexity=1015.4219, train_loss=6.9230595
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 100510, train_perplexity=1163.4811, train_loss=7.0591717

Batch 100520, train_perplexity=974.7901, train_loss=6.882222

Batch 100530, train_perplexity=959.9904, train_loss=6.8669233

Batch 100540, train_perplexity=1057.4407, train_loss=6.963607

Batch 100550, train_perplexity=975.6783, train_loss=6.883133

Batch 100560, train_perplexity=1000.1656, train_loss=6.907921

Batch 100570, train_perplexity=975.64014, train_loss=6.883094

Batch 100580, train_perplexity=997.6254, train_loss=6.905378

Batch 100590, train_perplexity=1063.1483, train_loss=6.96899

Batch 100600, train_perplexity=1013.6523, train_loss=6.921315

Batch 100610, train_perplexity=1099.177, train_loss=7.002317

Batch 100620, train_perplexity=1041.412, train_loss=6.948333

Batch 100630, train_perplexity=1069.8871, train_loss=6.9753084

Batch 100640, train_perplexity=1024.0723, train_loss=6.9315424

Batch 100650, train_perplexity=1017.02, train_loss=6.924632

Batch 100660, train_perplexity=1039.1451, train_loss=6.9461536

Batch 100670, train_perplexity=1002.0827, train_loss=6.909836

Batch 100680, train_perplexity=1035.5668, train_loss=6.942704

Batch 100690, train_perplexity=1039.9279, train_loss=6.9469066

Batch 100700, train_perplexity=1112.0461, train_loss=7.013957

Batch 100710, train_perplexity=1047.086, train_loss=6.9537663

Batch 100720, train_perplexity=1006.19086, train_loss=6.913927

Batch 100730, train_perplexity=1078.8976, train_loss=6.983695

Batch 100740, train_perplexity=1030.6858, train_loss=6.9379797

Batch 100750, train_perplexity=982.8094, train_loss=6.890415

Batch 100760, train_perplexity=1047.7273, train_loss=6.9543786

Batch 100770, train_perplexity=1062.2174, train_loss=6.968114

Batch 100780, train_perplexity=1049.1521, train_loss=6.9557376

Batch 100790, train_perplexity=1074.9015, train_loss=6.9799843

Batch 100800, train_perplexity=945.1784, train_loss=6.8513737

Batch 100810, train_perplexity=1134.6095, train_loss=7.034044

Batch 100820, train_perplexity=1099.6136, train_loss=7.002714

Batch 100830, train_perplexity=962.83966, train_loss=6.869887

Batch 100840, train_perplexity=965.5794, train_loss=6.8727283

Batch 100850, train_perplexity=1000.28577, train_loss=6.908041

Batch 100860, train_perplexity=1070.424, train_loss=6.97581

Batch 100870, train_perplexity=968.72095, train_loss=6.8759766

Batch 100880, train_perplexity=994.2911, train_loss=6.90203

Batch 100890, train_perplexity=902.65875, train_loss=6.8053446

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Batch 100900, train_perplexity=941.19635, train_loss=6.8471518

Batch 100910, train_perplexity=974.6353, train_loss=6.8820634

Batch 100920, train_perplexity=961.20795, train_loss=6.868191

Batch 100930, train_perplexity=969.0887, train_loss=6.876356

Batch 100940, train_perplexity=983.69366, train_loss=6.8913145

Batch 100950, train_perplexity=1056.5386, train_loss=6.9627533

Batch 100960, train_perplexity=912.0352, train_loss=6.8156786

Batch 100970, train_perplexity=1040.34, train_loss=6.947303

Batch 100980, train_perplexity=985.6638, train_loss=6.8933153

Batch 100990, train_perplexity=1073.927, train_loss=6.9790773

Batch 101000, train_perplexity=991.1716, train_loss=6.8988876

Batch 101010, train_perplexity=1002.684, train_loss=6.9104357

Batch 101020, train_perplexity=1009.45105, train_loss=6.917162

Batch 101030, train_perplexity=1016.88617, train_loss=6.9245005

Batch 101040, train_perplexity=1103.4574, train_loss=7.0062037

Batch 101050, train_perplexity=971.06525, train_loss=6.8783937

Batch 101060, train_perplexity=959.80005, train_loss=6.866725

Batch 101070, train_perplexity=1058.8141, train_loss=6.964905

Batch 101080, train_perplexity=945.7176, train_loss=6.851944

Batch 101090, train_perplexity=1103.0255, train_loss=7.005812

Batch 101100, train_perplexity=947.3035, train_loss=6.8536196

Batch 101110, train_perplexity=1153.8522, train_loss=7.0508614

Batch 101120, train_perplexity=996.6502, train_loss=6.9044

Batch 101130, train_perplexity=1047.9617, train_loss=6.9546022

Batch 101140, train_perplexity=1060.7688, train_loss=6.966749

Batch 101150, train_perplexity=1035.8088, train_loss=6.942938

Batch 101160, train_perplexity=992.7078, train_loss=6.9004364

Batch 101170, train_perplexity=1064.4667, train_loss=6.970229

Batch 101180, train_perplexity=972.53094, train_loss=6.879902

Batch 101190, train_perplexity=1033.0481, train_loss=6.940269

Batch 101200, train_perplexity=1117.9675, train_loss=7.0192676

Batch 101210, train_perplexity=925.2216, train_loss=6.8300333

Batch 101220, train_perplexity=928.1911, train_loss=6.8332376

Batch 101230, train_perplexity=979.9735, train_loss=6.8875256

Batch 101240, train_perplexity=948.2626, train_loss=6.8546314

Batch 101250, train_perplexity=1040.0186, train_loss=6.946994

Batch 101260, train_perplexity=1018.29236, train_loss=6.9258823

Batch 101270, train_perplexity=1065.8522, train_loss=6.97153

Batch 101280, train_perplexity=1071.5153, train_loss=6.976829

Batch 101290, train_perplexity=1041.2904, train_loss=6.948216

Batch 101300, train_perplexity=978.317, train_loss=6.8858337

Batch 101310, train_perplexity=957.8386, train_loss=6.8646793

Batch 101320, train_perplexity=1011.196, train_loss=6.918889

Batch 101330, train_perplexity=1001.6585, train_loss=6.9094124

Batch 101340, train_perplexity=1098.5157, train_loss=7.001715

Batch 101350, train_perplexity=951.4956, train_loss=6.858035

Batch 101360, train_perplexity=1037.8715, train_loss=6.944927

Batch 101370, train_perplexity=989.1668, train_loss=6.896863

Batch 101380, train_perplexity=1025.5496, train_loss=6.932984

Batch 101390, train_perplexity=1070.2545, train_loss=6.9756517

Batch 101400, train_perplexity=1066.7888, train_loss=6.9724083

Batch 101410, train_perplexity=943.4808, train_loss=6.849576

Batch 101420, train_perplexity=1024.9917, train_loss=6.93244

Batch 101430, train_perplexity=978.99133, train_loss=6.886523

Batch 101440, train_perplexity=1014.7065, train_loss=6.9223547

Batch 101450, train_perplexity=989.8906, train_loss=6.8975945

Batch 101460, train_perplexity=1215.7031, train_loss=7.103078

Batch 101470, train_perplexity=956.7399, train_loss=6.8635316

Batch 101480, train_perplexity=896.9976, train_loss=6.799053

Batch 101490, train_perplexity=1076.1621, train_loss=6.9811563

Batch 101500, train_perplexity=942.7545, train_loss=6.848806

Batch 101510, train_perplexity=1002.925, train_loss=6.910676

Batch 101520, train_perplexity=942.2449, train_loss=6.848265

Batch 101530, train_perplexity=1103.3827, train_loss=7.006136

Batch 101540, train_perplexity=1032.1017, train_loss=6.9393525

Batch 101550, train_perplexity=1033.0486, train_loss=6.9402695

Batch 101560, train_perplexity=951.5596, train_loss=6.8581023

Batch 101570, train_perplexity=987.475, train_loss=6.895151

Batch 101580, train_perplexity=993.59296, train_loss=6.9013276

Batch 101590, train_perplexity=1067.7853, train_loss=6.973342

Batch 101600, train_perplexity=966.43896, train_loss=6.873618

Batch 101610, train_perplexity=1040.6396, train_loss=6.947591

Batch 101620, train_perplexity=969.8367, train_loss=6.8771276

Batch 101630, train_perplexity=1091.112, train_loss=6.9949527

Batch 101640, train_perplexity=1073.815, train_loss=6.978973

Batch 101650, train_perplexity=1002.07025, train_loss=6.9098234

Batch 101660, train_perplexity=957.4122, train_loss=6.864234

Batch 101670, train_perplexity=998.7739, train_loss=6.9065285

Batch 101680, train_perplexity=1023.7579, train_loss=6.9312353

Batch 101690, train_perplexity=1018.29236, train_loss=6.9258823

Batch 101700, train_perplexity=991.9139, train_loss=6.8996363

Batch 101710, train_perplexity=1043.5237, train_loss=6.9503584

Batch 101720, train_perplexity=1076.2421, train_loss=6.9812307

Batch 101730, train_perplexity=1045.7816, train_loss=6.95252

Batch 101740, train_perplexity=1088.3124, train_loss=6.9923835
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 101750, train_perplexity=1026.1586, train_loss=6.9335775

Batch 101760, train_perplexity=999.3971, train_loss=6.907152

Batch 101770, train_perplexity=1018.94324, train_loss=6.9265213

Batch 101780, train_perplexity=1149.7073, train_loss=7.0472627

Batch 101790, train_perplexity=1018.8256, train_loss=6.926406

Batch 101800, train_perplexity=926.6893, train_loss=6.8316183

Batch 101810, train_perplexity=934.6194, train_loss=6.8401394

Batch 101820, train_perplexity=1081.011, train_loss=6.985652

Batch 101830, train_perplexity=1033.6506, train_loss=6.940852

Batch 101840, train_perplexity=1050.9476, train_loss=6.9574475

Batch 101850, train_perplexity=1101.2429, train_loss=7.0041947

Batch 101860, train_perplexity=1126.8943, train_loss=7.0272207

Batch 101870, train_perplexity=1033.5472, train_loss=6.940752

Batch 101880, train_perplexity=1083.9924, train_loss=6.988406

Batch 101890, train_perplexity=1026.9241, train_loss=6.9343233

Batch 101900, train_perplexity=963.95734, train_loss=6.871047

Batch 101910, train_perplexity=977.4301, train_loss=6.884927

Batch 101920, train_perplexity=1057.8381, train_loss=6.9639826

Batch 101930, train_perplexity=1042.7244, train_loss=6.949592

Batch 101940, train_perplexity=1071.5106, train_loss=6.9768248

Batch 101950, train_perplexity=964.4437, train_loss=6.8715515

Batch 101960, train_perplexity=994.7871, train_loss=6.902529

Batch 101970, train_perplexity=1106.8085, train_loss=7.009236

Batch 101980, train_perplexity=1005.5031, train_loss=6.9132433

Batch 101990, train_perplexity=1038.7443, train_loss=6.945768

Batch 102000, train_perplexity=913.27026, train_loss=6.817032

Batch 102010, train_perplexity=1058.5879, train_loss=6.964691

Batch 102020, train_perplexity=1088.4265, train_loss=6.9924884

Batch 102030, train_perplexity=999.32367, train_loss=6.9070787

Batch 102040, train_perplexity=998.66205, train_loss=6.9064164

Batch 102050, train_perplexity=1022.8756, train_loss=6.930373

Batch 102060, train_perplexity=987.17694, train_loss=6.8948493

Batch 102070, train_perplexity=943.9632, train_loss=6.850087

Batch 102080, train_perplexity=1045.9303, train_loss=6.952662

Batch 102090, train_perplexity=1083.547, train_loss=6.987995

Batch 102100, train_perplexity=1030.4323, train_loss=6.9377337

Batch 102110, train_perplexity=970.1512, train_loss=6.877452

Batch 102120, train_perplexity=989.1715, train_loss=6.8968678

Batch 102130, train_perplexity=1022.39044, train_loss=6.9298987

Batch 102140, train_perplexity=981.8051, train_loss=6.889393

Batch 102150, train_perplexity=1000.9227, train_loss=6.9086776

Batch 102160, train_perplexity=945.28925, train_loss=6.851491

Batch 102170, train_perplexity=982.9969, train_loss=6.890606

Batch 102180, train_perplexity=992.10876, train_loss=6.8998327

Batch 102190, train_perplexity=1050.5372, train_loss=6.957057

Batch 102200, train_perplexity=1061.1776, train_loss=6.9671345

Batch 102210, train_perplexity=945.5674, train_loss=6.851785

Batch 102220, train_perplexity=992.0794, train_loss=6.899803

Batch 102230, train_perplexity=957.49023, train_loss=6.8643155

Batch 102240, train_perplexity=1023.1361, train_loss=6.930628

Batch 102250, train_perplexity=1026.1752, train_loss=6.9335938

Batch 102260, train_perplexity=1020.9981, train_loss=6.928536

Batch 102270, train_perplexity=1050.7457, train_loss=6.9572554

Batch 102280, train_perplexity=1011.878, train_loss=6.9195633

Batch 102290, train_perplexity=1076.4413, train_loss=6.9814157

Batch 102300, train_perplexity=1004.2036, train_loss=6.91195

Batch 102310, train_perplexity=1002.68207, train_loss=6.910434

Batch 102320, train_perplexity=942.63, train_loss=6.848674

Batch 102330, train_perplexity=1052.0071, train_loss=6.958455

Batch 102340, train_perplexity=1033.0707, train_loss=6.940291

Batch 102350, train_perplexity=905.1069, train_loss=6.808053

Batch 102360, train_perplexity=1015.1421, train_loss=6.922784

Batch 102370, train_perplexity=1092.7501, train_loss=6.996453

Batch 102380, train_perplexity=1054.4005, train_loss=6.9607277

Batch 102390, train_perplexity=1109.8451, train_loss=7.011976

Batch 102400, train_perplexity=928.30975, train_loss=6.8333654

Batch 102410, train_perplexity=965.7111, train_loss=6.8728647

Batch 102420, train_perplexity=986.05585, train_loss=6.893713

Batch 102430, train_perplexity=1141.2194, train_loss=7.0398526

Batch 102440, train_perplexity=1003.04456, train_loss=6.910795

Batch 102450, train_perplexity=968.09155, train_loss=6.8753266

Batch 102460, train_perplexity=1073.1223, train_loss=6.9783278

Batch 102470, train_perplexity=984.56415, train_loss=6.892199

Batch 102480, train_perplexity=1010.99445, train_loss=6.9186897

Batch 102490, train_perplexity=939.11444, train_loss=6.8449373

Batch 102500, train_perplexity=1051.4022, train_loss=6.95788

Batch 102510, train_perplexity=1056.4226, train_loss=6.9626436

Batch 102520, train_perplexity=952.53516, train_loss=6.859127

Batch 102530, train_perplexity=999.3771, train_loss=6.907132

Batch 102540, train_perplexity=921.9688, train_loss=6.8265114

Batch 102550, train_perplexity=998.384, train_loss=6.906138

Batch 102560, train_perplexity=1179.089, train_loss=7.0724974

Batch 102570, train_perplexity=1054.9044, train_loss=6.9612055

Batch 102580, train_perplexity=1169.6399, train_loss=7.064451

Batch 102590, train_perplexity=1093.1993, train_loss=6.996864

Batch 102600, train_perplexity=1020.3931, train_loss=6.927943

Batch 102610, train_perplexity=951.0611, train_loss=6.8575783

Batch 102620, train_perplexity=919.0832, train_loss=6.8233767

Batch 102630, train_perplexity=992.50476, train_loss=6.900232

Batch 102640, train_perplexity=1056.8394, train_loss=6.963038

Batch 102650, train_perplexity=996.4839, train_loss=6.904233

Batch 102660, train_perplexity=969.7155, train_loss=6.8770027

Batch 102670, train_perplexity=1037.067, train_loss=6.944152

Batch 102680, train_perplexity=1015.527, train_loss=6.923163

Batch 102690, train_perplexity=1009.1969, train_loss=6.91691

Batch 102700, train_perplexity=942.42596, train_loss=6.8484573

Batch 102710, train_perplexity=1053.7009, train_loss=6.960064

Batch 102720, train_perplexity=1089.3046, train_loss=6.9932947

Batch 102730, train_perplexity=1096.5197, train_loss=6.9998965

Batch 102740, train_perplexity=1050.2648, train_loss=6.9567976

Batch 102750, train_perplexity=986.164, train_loss=6.8938227

Batch 102760, train_perplexity=1007.0165, train_loss=6.914747

Batch 102770, train_perplexity=1100.3364, train_loss=7.0033712

Batch 102780, train_perplexity=1039.2031, train_loss=6.9462094

Batch 102790, train_perplexity=979.7521, train_loss=6.8872995

Batch 102800, train_perplexity=1014.2557, train_loss=6.9219103

Batch 102810, train_perplexity=1059.307, train_loss=6.96537

Batch 102820, train_perplexity=1000.1227, train_loss=6.907878

Batch 102830, train_perplexity=1042.4692, train_loss=6.9493475

Batch 102840, train_perplexity=968.8632, train_loss=6.8761234

Batch 102850, train_perplexity=1075.5065, train_loss=6.980547

Batch 102860, train_perplexity=947.81683, train_loss=6.8541613

Batch 102870, train_perplexity=989.8944, train_loss=6.8975983

Batch 102880, train_perplexity=962.8176, train_loss=6.869864

Batch 102890, train_perplexity=1149.3762, train_loss=7.0469747

Batch 102900, train_perplexity=1048.9725, train_loss=6.9555664

Batch 102910, train_perplexity=1107.4235, train_loss=7.0097914

Batch 102920, train_perplexity=1044.5671, train_loss=6.951358

Batch 102930, train_perplexity=1047.7803, train_loss=6.954429

Batch 102940, train_perplexity=1092.5474, train_loss=6.9962673

Batch 102950, train_perplexity=1046.3997, train_loss=6.9531107

Batch 102960, train_perplexity=1137.2072, train_loss=7.0363307

Batch 102970, train_perplexity=1044.7235, train_loss=6.9515076

Batch 102980, train_perplexity=923.27716, train_loss=6.8279295

Batch 102990, train_perplexity=911.14496, train_loss=6.814702

Batch 103000, train_perplexity=950.2107, train_loss=6.8566837

Batch 103010, train_perplexity=993.6664, train_loss=6.9014015

Batch 103020, train_perplexity=1068.0781, train_loss=6.973616

Batch 103030, train_perplexity=1003.17035, train_loss=6.9109206

Batch 103040, train_perplexity=970.75275, train_loss=6.878072
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 103050, train_perplexity=996.2549, train_loss=6.904003

Batch 103060, train_perplexity=1138.7977, train_loss=7.0377283

Batch 103070, train_perplexity=1028.5874, train_loss=6.9359417

Batch 103080, train_perplexity=991.0203, train_loss=6.898735

Batch 103090, train_perplexity=1058.2013, train_loss=6.964326

Batch 103100, train_perplexity=1024.1094, train_loss=6.9315786

Batch 103110, train_perplexity=1026.623, train_loss=6.93403

Batch 103120, train_perplexity=938.9183, train_loss=6.8447285

Batch 103130, train_perplexity=1021.9347, train_loss=6.929453

Batch 103140, train_perplexity=1023.36646, train_loss=6.930853

Batch 103150, train_perplexity=1026.0646, train_loss=6.933486

Batch 103160, train_perplexity=1070.6741, train_loss=6.9760437

Batch 103170, train_perplexity=988.26447, train_loss=6.8959503

Batch 103180, train_perplexity=1063.2562, train_loss=6.9690914

Batch 103190, train_perplexity=1036.1442, train_loss=6.9432616

Batch 103200, train_perplexity=987.1077, train_loss=6.894779

Batch 103210, train_perplexity=986.27594, train_loss=6.893936

Batch 103220, train_perplexity=1018.0613, train_loss=6.9256554

Batch 103230, train_perplexity=960.5944, train_loss=6.8675523

Batch 103240, train_perplexity=1050.7196, train_loss=6.9572306

Batch 103250, train_perplexity=1131.0756, train_loss=7.0309243

Batch 103260, train_perplexity=1047.482, train_loss=6.9541445

Batch 103270, train_perplexity=952.51245, train_loss=6.859103

Batch 103280, train_perplexity=975.0383, train_loss=6.882477

Batch 103290, train_perplexity=997.8833, train_loss=6.9056363

Batch 103300, train_perplexity=1146.4232, train_loss=7.044402

Batch 103310, train_perplexity=1030.3379, train_loss=6.937642

Batch 103320, train_perplexity=992.54974, train_loss=6.900277

Batch 103330, train_perplexity=1128.9558, train_loss=7.0290484

Batch 103340, train_perplexity=1028.3511, train_loss=6.935712

Batch 103350, train_perplexity=1033.9597, train_loss=6.941151

Batch 103360, train_perplexity=947.11566, train_loss=6.853421

Batch 103370, train_perplexity=1080.4276, train_loss=6.985112

Batch 103380, train_perplexity=1041.13, train_loss=6.948062

Batch 103390, train_perplexity=1096.3027, train_loss=6.9996986

Batch 103400, train_perplexity=961.93604, train_loss=6.868948

Batch 103410, train_perplexity=942.6183, train_loss=6.8486614

Batch 103420, train_perplexity=994.568, train_loss=6.9023085

Batch 103430, train_perplexity=1048.4674, train_loss=6.955085

Batch 103440, train_perplexity=969.92084, train_loss=6.8772144

Batch 103450, train_perplexity=1061.0753, train_loss=6.967038

Batch 103460, train_perplexity=1039.8698, train_loss=6.946851

Batch 103470, train_perplexity=980.4087, train_loss=6.8879695

Batch 103480, train_perplexity=990.5932, train_loss=6.898304

Batch 103490, train_perplexity=993.9763, train_loss=6.9017134

Batch 103500, train_perplexity=950.333, train_loss=6.8568125

Batch 103510, train_perplexity=1028.3324, train_loss=6.9356937

Batch 103520, train_perplexity=962.0113, train_loss=6.869026

Batch 103530, train_perplexity=916.97327, train_loss=6.8210783

Batch 103540, train_perplexity=984.14215, train_loss=6.8917704

Batch 103550, train_perplexity=1084.5591, train_loss=6.988929

Batch 103560, train_perplexity=1000.198, train_loss=6.9079533

Batch 103570, train_perplexity=983.2425, train_loss=6.890856

Batch 103580, train_perplexity=982.0252, train_loss=6.889617

Batch 103590, train_perplexity=1098.2124, train_loss=7.001439

Batch 103600, train_perplexity=1100.5841, train_loss=7.0035963

Batch 103610, train_perplexity=925.98914, train_loss=6.8308625

Batch 103620, train_perplexity=1036.2811, train_loss=6.9433937

Batch 103630, train_perplexity=959.43396, train_loss=6.8663435

Batch 103640, train_perplexity=976.74475, train_loss=6.8842254

Batch 103650, train_perplexity=1011.57794, train_loss=6.9192667

Batch 103660, train_perplexity=1018.4424, train_loss=6.9260297

Batch 103670, train_perplexity=989.2583, train_loss=6.8969555

Batch 103680, train_perplexity=1072.0892, train_loss=6.9773645

Batch 103690, train_perplexity=962.1535, train_loss=6.869174

Batch 103700, train_perplexity=1011.01855, train_loss=6.9187136

Batch 103710, train_perplexity=1098.4916, train_loss=7.0016932

Batch 103720, train_perplexity=1022.1014, train_loss=6.929616

Batch 103730, train_perplexity=1099.5287, train_loss=7.002637

Batch 103740, train_perplexity=1027.2229, train_loss=6.934614

Batch 103750, train_perplexity=1008.1311, train_loss=6.9158535

Batch 103760, train_perplexity=1007.38245, train_loss=6.9151106

Batch 103770, train_perplexity=952.05023, train_loss=6.858618

Batch 103780, train_perplexity=1012.85986, train_loss=6.920533

Batch 103790, train_perplexity=917.1849, train_loss=6.821309

Batch 103800, train_perplexity=984.0638, train_loss=6.8916907

Batch 103810, train_perplexity=1063.0149, train_loss=6.9688644

Batch 103820, train_perplexity=979.4905, train_loss=6.8870325

Batch 103830, train_perplexity=924.1114, train_loss=6.8288326

Batch 103840, train_perplexity=964.4957, train_loss=6.8716054

Batch 103850, train_perplexity=1105.1208, train_loss=7.00771

Batch 103860, train_perplexity=942.34235, train_loss=6.8483686

Batch 103870, train_perplexity=956.54694, train_loss=6.86333

Batch 103880, train_perplexity=1093.4126, train_loss=6.997059

Batch 103890, train_perplexity=981.7499, train_loss=6.8893366

Batch 103900, train_perplexity=1044.6289, train_loss=6.951417

Batch 103910, train_perplexity=1045.5359, train_loss=6.952285

Batch 103920, train_perplexity=954.078, train_loss=6.8607454

Batch 103930, train_perplexity=1027.3875, train_loss=6.9347744

Batch 103940, train_perplexity=1058.6597, train_loss=6.964759

Batch 103950, train_perplexity=994.62964, train_loss=6.9023705

Batch 103960, train_perplexity=1096.4376, train_loss=6.9998217

Batch 103970, train_perplexity=939.5336, train_loss=6.8453836

Batch 103980, train_perplexity=1040.5632, train_loss=6.9475174

Batch 103990, train_perplexity=1018.31274, train_loss=6.9259024

Batch 104000, train_perplexity=991.5942, train_loss=6.899314

Batch 104010, train_perplexity=979.2448, train_loss=6.8867817

Batch 104020, train_perplexity=923.6982, train_loss=6.8283854

Batch 104030, train_perplexity=984.191, train_loss=6.89182

Batch 104040, train_perplexity=1133.5734, train_loss=7.03313

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Batch 104050, train_perplexity=1063.243, train_loss=6.969079

Batch 104060, train_perplexity=1036.3627, train_loss=6.9434724

Batch 104070, train_perplexity=1023.92676, train_loss=6.9314003

Batch 104080, train_perplexity=1035.3397, train_loss=6.942485

Batch 104090, train_perplexity=974.37695, train_loss=6.8817983

Batch 104100, train_perplexity=964.579, train_loss=6.8716917

Batch 104110, train_perplexity=1125.3699, train_loss=7.025867

Batch 104120, train_perplexity=949.37103, train_loss=6.8557997

Batch 104130, train_perplexity=989.9992, train_loss=6.897704

Batch 104140, train_perplexity=1003.48804, train_loss=6.9112372

Batch 104150, train_perplexity=1029.1643, train_loss=6.9365025

Batch 104160, train_perplexity=1000.67505, train_loss=6.90843

Batch 104170, train_perplexity=1040.3846, train_loss=6.9473457

Batch 104180, train_perplexity=982.46826, train_loss=6.890068

Batch 104190, train_perplexity=976.05475, train_loss=6.8835187

Batch 104200, train_perplexity=1093.6096, train_loss=6.997239

Batch 104210, train_perplexity=1039.9363, train_loss=6.9469147

Batch 104220, train_perplexity=1043.3754, train_loss=6.9502163

Batch 104230, train_perplexity=953.95605, train_loss=6.8606176

Batch 104240, train_perplexity=980.90857, train_loss=6.888479

Batch 104250, train_perplexity=1009.5988, train_loss=6.9173083

Batch 104260, train_perplexity=1034.6547, train_loss=6.941823

Batch 104270, train_perplexity=977.1855, train_loss=6.8846765

Batch 104280, train_perplexity=970.073, train_loss=6.8773713
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 104290, train_perplexity=1105.2837, train_loss=7.0078573

Batch 104300, train_perplexity=966.5643, train_loss=6.873748

Batch 104310, train_perplexity=1111.9264, train_loss=7.0138493

Batch 104320, train_perplexity=993.9076, train_loss=6.901644

Batch 104330, train_perplexity=1057.6782, train_loss=6.9638314

Batch 104340, train_perplexity=918.617, train_loss=6.8228693

Batch 104350, train_perplexity=908.94965, train_loss=6.8122897

Batch 104360, train_perplexity=1053.9823, train_loss=6.960331

Batch 104370, train_perplexity=1142.006, train_loss=7.0405416

Batch 104380, train_perplexity=994.15405, train_loss=6.901892

Batch 104390, train_perplexity=990.273, train_loss=6.8979807

Batch 104400, train_perplexity=1005.7179, train_loss=6.913457

Batch 104410, train_perplexity=1043.1884, train_loss=6.950037

Batch 104420, train_perplexity=1029.8507, train_loss=6.937169

Batch 104430, train_perplexity=1048.4075, train_loss=6.9550276

Batch 104440, train_perplexity=973.58514, train_loss=6.8809853

Batch 104450, train_perplexity=941.2206, train_loss=6.8471775

Batch 104460, train_perplexity=978.69495, train_loss=6.88622

Batch 104470, train_perplexity=1027.406, train_loss=6.9347925

Batch 104480, train_perplexity=945.57416, train_loss=6.8517923

Batch 104490, train_perplexity=1019.0176, train_loss=6.9265943

Batch 104500, train_perplexity=1083.7495, train_loss=6.988182

Batch 104510, train_perplexity=988.49066, train_loss=6.896179

Batch 104520, train_perplexity=1031.9471, train_loss=6.939203

Batch 104530, train_perplexity=991.458, train_loss=6.8991766

Batch 104540, train_perplexity=1110.9883, train_loss=7.0130053

Batch 104550, train_perplexity=1006.8398, train_loss=6.914572

Batch 104560, train_perplexity=895.82214, train_loss=6.797742

Batch 104570, train_perplexity=973.07135, train_loss=6.8804574

Batch 104580, train_perplexity=956.24506, train_loss=6.863014

Batch 104590, train_perplexity=1048.2545, train_loss=6.9548817

Batch 104600, train_perplexity=932.11633, train_loss=6.8374577

Batch 104610, train_perplexity=1055.1752, train_loss=6.961462

Batch 104620, train_perplexity=1031.5555, train_loss=6.938823

Batch 104630, train_perplexity=858.6739, train_loss=6.755389

Batch 104640, train_perplexity=925.31915, train_loss=6.8301387

Batch 104650, train_perplexity=1052.8923, train_loss=6.959296

Batch 104660, train_perplexity=947.6279, train_loss=6.853962

Batch 104670, train_perplexity=1087.0349, train_loss=6.991209

Batch 104680, train_perplexity=962.6941, train_loss=6.8697357

Batch 104690, train_perplexity=1081.6989, train_loss=6.986288

Batch 104700, train_perplexity=1064.622, train_loss=6.970375

Batch 104710, train_perplexity=925.4047, train_loss=6.830231

Batch 104720, train_perplexity=974.57446, train_loss=6.882001

Batch 104730, train_perplexity=984.2989, train_loss=6.8919296

Batch 104740, train_perplexity=930.0133, train_loss=6.835199

Batch 104750, train_perplexity=1103.7552, train_loss=7.0064735

Batch 104760, train_perplexity=1028.5692, train_loss=6.935924

Batch 104770, train_perplexity=901.01434, train_loss=6.803521

Batch 104780, train_perplexity=982.72974, train_loss=6.890334

Batch 104790, train_perplexity=1004.0839, train_loss=6.911831

Batch 104800, train_perplexity=961.3537, train_loss=6.8683424

Batch 104810, train_perplexity=1024.8295, train_loss=6.9322815

Batch 104820, train_perplexity=978.4374, train_loss=6.885957

Batch 104830, train_perplexity=1089.6328, train_loss=6.993596

Batch 104840, train_perplexity=1016.31226, train_loss=6.923936

Batch 104850, train_perplexity=979.1197, train_loss=6.886654

Batch 104860, train_perplexity=1048.4445, train_loss=6.955063

Batch 104870, train_perplexity=1085.7616, train_loss=6.990037

Batch 104880, train_perplexity=908.5523, train_loss=6.8118525

Batch 104890, train_perplexity=1027.4335, train_loss=6.934819

Batch 104900, train_perplexity=938.20044, train_loss=6.8439636

Batch 104910, train_perplexity=993.20123, train_loss=6.9009333

Batch 104920, train_perplexity=1117.757, train_loss=7.019079

Batch 104930, train_perplexity=867.9592, train_loss=6.7661448

Batch 104940, train_perplexity=1123.4283, train_loss=7.0241404

Batch 104950, train_perplexity=1042.4723, train_loss=6.9493504

Batch 104960, train_perplexity=949.78033, train_loss=6.8562307

Batch 104970, train_perplexity=1011.4993, train_loss=6.919189

Batch 104980, train_perplexity=1010.6233, train_loss=6.9183226

Batch 104990, train_perplexity=1036.8193, train_loss=6.943913

Batch 105000, train_perplexity=1043.0729, train_loss=6.9499264

Batch 105010, train_perplexity=1028.0764, train_loss=6.935445

Batch 105020, train_perplexity=1033.5432, train_loss=6.940748

Batch 105030, train_perplexity=1037.5992, train_loss=6.944665

Batch 105040, train_perplexity=1077.3677, train_loss=6.982276

Batch 105050, train_perplexity=1078.4912, train_loss=6.9833183

Batch 105060, train_perplexity=989.96234, train_loss=6.897667

Batch 105070, train_perplexity=1026.624, train_loss=6.934031

Batch 105080, train_perplexity=969.5615, train_loss=6.876844

Batch 105090, train_perplexity=955.5887, train_loss=6.8623276

Batch 105100, train_perplexity=1025.0992, train_loss=6.9325447

Batch 105110, train_perplexity=1022.99365, train_loss=6.9304886

Batch 105120, train_perplexity=981.34454, train_loss=6.8889236

Batch 105130, train_perplexity=1023.9087, train_loss=6.9313827

Batch 105140, train_perplexity=1112.8207, train_loss=7.014653

Batch 105150, train_perplexity=891.8367, train_loss=6.793283

Batch 105160, train_perplexity=1051.2769, train_loss=6.957761

Batch 105170, train_perplexity=998.7387, train_loss=6.906493

Batch 105180, train_perplexity=955.36816, train_loss=6.862097

Batch 105190, train_perplexity=970.2687, train_loss=6.877573

Batch 105200, train_perplexity=928.90485, train_loss=6.8340063

Batch 105210, train_perplexity=990.248, train_loss=6.8979554

Batch 105220, train_perplexity=1098.9227, train_loss=7.0020857

Batch 105230, train_perplexity=1054.5896, train_loss=6.960907

Batch 105240, train_perplexity=912.9746, train_loss=6.816708

Batch 105250, train_perplexity=1084.4081, train_loss=6.9887896

Batch 105260, train_perplexity=904.3342, train_loss=6.807199

Batch 105270, train_perplexity=1124.6815, train_loss=7.025255

Batch 105280, train_perplexity=992.2947, train_loss=6.90002

Batch 105290, train_perplexity=972.09607, train_loss=6.8794546

Batch 105300, train_perplexity=962.06635, train_loss=6.8690834

Batch 105310, train_perplexity=1016.49835, train_loss=6.924119

Batch 105320, train_perplexity=973.0792, train_loss=6.8804655

Batch 105330, train_perplexity=1276.387, train_loss=7.1517887

Batch 105340, train_perplexity=1002.4134, train_loss=6.910166

Batch 105350, train_perplexity=1021.8977, train_loss=6.9294167

Batch 105360, train_perplexity=1011.97644, train_loss=6.9196606

Batch 105370, train_perplexity=982.67914, train_loss=6.8902826

Batch 105380, train_perplexity=1022.2915, train_loss=6.929802

Batch 105390, train_perplexity=990.2778, train_loss=6.8979855

Batch 105400, train_perplexity=1044.859, train_loss=6.9516373

Batch 105410, train_perplexity=1005.63446, train_loss=6.913374

Batch 105420, train_perplexity=987.4589, train_loss=6.895135

Batch 105430, train_perplexity=980.2189, train_loss=6.887776

Batch 105440, train_perplexity=960.7332, train_loss=6.867697

Batch 105450, train_perplexity=986.0737, train_loss=6.893731

Batch 105460, train_perplexity=1041.7319, train_loss=6.94864

Batch 105470, train_perplexity=1000.1141, train_loss=6.9078693

Batch 105480, train_perplexity=983.3841, train_loss=6.891

Batch 105490, train_perplexity=1087.1288, train_loss=6.9912953

Batch 105500, train_perplexity=954.33734, train_loss=6.861017

Batch 105510, train_perplexity=986.9317, train_loss=6.894601

Batch 105520, train_perplexity=996.5941, train_loss=6.9043436

Batch 105530, train_perplexity=1129.8881, train_loss=7.029874

Batch 105540, train_perplexity=1030.4578, train_loss=6.9377584

Batch 105550, train_perplexity=1055.8174, train_loss=6.9620705

Batch 105560, train_perplexity=1033.8173, train_loss=6.9410133

Batch 105570, train_perplexity=1102.5969, train_loss=7.0054235

Batch 105580, train_perplexity=1048.1326, train_loss=6.9547653
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 105590, train_perplexity=1047.8517, train_loss=6.9544973

Batch 105600, train_perplexity=1054.647, train_loss=6.9609613

Batch 105610, train_perplexity=1011.63727, train_loss=6.9193254

Batch 105620, train_perplexity=1003.9575, train_loss=6.911705

Batch 105630, train_perplexity=986.72565, train_loss=6.894392

Batch 105640, train_perplexity=969.4538, train_loss=6.876733

Batch 105650, train_perplexity=995.15247, train_loss=6.902896

Batch 105660, train_perplexity=1011.65173, train_loss=6.9193397

Batch 105670, train_perplexity=952.2632, train_loss=6.8588414

Batch 105680, train_perplexity=1078.2239, train_loss=6.9830704

Batch 105690, train_perplexity=1009.88916, train_loss=6.917596

Batch 105700, train_perplexity=967.7006, train_loss=6.8749228

Batch 105710, train_perplexity=1014.602, train_loss=6.9222517

Batch 105720, train_perplexity=1060.5538, train_loss=6.9665465

Batch 105730, train_perplexity=976.7355, train_loss=6.884216

Batch 105740, train_perplexity=1013.6402, train_loss=6.9213033

Batch 105750, train_perplexity=1065.5352, train_loss=6.9712324

Batch 105760, train_perplexity=992.29706, train_loss=6.9000225

Batch 105770, train_perplexity=970.531, train_loss=6.8778434

Batch 105780, train_perplexity=1006.72266, train_loss=6.9144554

Batch 105790, train_perplexity=1000.0459, train_loss=6.907801

Batch 105800, train_perplexity=927.3727, train_loss=6.8323555

Batch 105810, train_perplexity=908.40283, train_loss=6.811688

Batch 105820, train_perplexity=975.8737, train_loss=6.883333

Batch 105830, train_perplexity=1068.6328, train_loss=6.9741354

Batch 105840, train_perplexity=1056.7935, train_loss=6.9629946

Batch 105850, train_perplexity=850.99, train_loss=6.7464004

Batch 105860, train_perplexity=1155.5315, train_loss=7.0523157

Batch 105870, train_perplexity=971.4566, train_loss=6.8787966

Batch 105880, train_perplexity=1031.6731, train_loss=6.938937

Batch 105890, train_perplexity=1029.13, train_loss=6.936469

Batch 105900, train_perplexity=901.4191, train_loss=6.8039703

Batch 105910, train_perplexity=979.04126, train_loss=6.886574

Batch 105920, train_perplexity=1011.9166, train_loss=6.9196014

Batch 105930, train_perplexity=1076.0841, train_loss=6.981084

Batch 105940, train_perplexity=949.132, train_loss=6.855548

Batch 105950, train_perplexity=917.39526, train_loss=6.8215384

Batch 105960, train_perplexity=1063.1265, train_loss=6.9689693

Batch 105970, train_perplexity=957.8697, train_loss=6.8647118

Batch 105980, train_perplexity=946.1605, train_loss=6.852412

Batch 105990, train_perplexity=988.6745, train_loss=6.896365

Batch 106000, train_perplexity=990.9636, train_loss=6.898678

Batch 106010, train_perplexity=1005.50696, train_loss=6.913247

Batch 106020, train_perplexity=1020.09296, train_loss=6.927649

Batch 106030, train_perplexity=1084.3771, train_loss=6.988761

Batch 106040, train_perplexity=970.0295, train_loss=6.8773265

Batch 106050, train_perplexity=941.39923, train_loss=6.8473673

Batch 106060, train_perplexity=933.9485, train_loss=6.8394213

Batch 106070, train_perplexity=1094.5267, train_loss=6.9980774

Batch 106080, train_perplexity=993.02124, train_loss=6.900752

Batch 106090, train_perplexity=943.9299, train_loss=6.850052

Batch 106100, train_perplexity=1038.2922, train_loss=6.9453325

Batch 106110, train_perplexity=1115.3834, train_loss=7.0169535

Batch 106120, train_perplexity=976.477, train_loss=6.883951

Batch 106130, train_perplexity=910.45703, train_loss=6.8139467

Batch 106140, train_perplexity=989.6528, train_loss=6.897354

Batch 106150, train_perplexity=1125.0012, train_loss=7.0255394

Batch 106160, train_perplexity=1025.326, train_loss=6.932766

Batch 106170, train_perplexity=934.46075, train_loss=6.8399696

Batch 106180, train_perplexity=1041.6823, train_loss=6.948592

Batch 106190, train_perplexity=887.47314, train_loss=6.7883782

Batch 106200, train_perplexity=914.531, train_loss=6.8184114

Batch 106210, train_perplexity=998.7468, train_loss=6.9065013

Batch 106220, train_perplexity=1052.0231, train_loss=6.9584703

Batch 106230, train_perplexity=995.1804, train_loss=6.902924

Batch 106240, train_perplexity=1011.713, train_loss=6.9194

Batch 106250, train_perplexity=1027.5237, train_loss=6.934907

Batch 106260, train_perplexity=939.84955, train_loss=6.84572

Batch 106270, train_perplexity=1021.6633, train_loss=6.9291873

Batch 106280, train_perplexity=1070.4918, train_loss=6.9758735

Batch 106290, train_perplexity=1001.968, train_loss=6.9097214

Batch 106300, train_perplexity=1064.4474, train_loss=6.970211

Batch 106310, train_perplexity=1074.4495, train_loss=6.9795637

Batch 106320, train_perplexity=1061.4569, train_loss=6.9673977

Batch 106330, train_perplexity=1086.3209, train_loss=6.990552

Batch 106340, train_perplexity=967.3472, train_loss=6.8745575

Batch 106350, train_perplexity=1021.22107, train_loss=6.9287543

Batch 106360, train_perplexity=1085.1643, train_loss=6.9894867

Batch 106370, train_perplexity=1001.09125, train_loss=6.908846

Batch 106380, train_perplexity=846.4977, train_loss=6.7411075

Batch 106390, train_perplexity=955.0889, train_loss=6.8618045

Batch 106400, train_perplexity=937.96246, train_loss=6.84371

Batch 106410, train_perplexity=1001.5849, train_loss=6.909339

Batch 106420, train_perplexity=1011.9827, train_loss=6.919667

Batch 106430, train_perplexity=983.7936, train_loss=6.891416

Batch 106440, train_perplexity=946.415, train_loss=6.852681

Batch 106450, train_perplexity=1059.45, train_loss=6.965505

Batch 106460, train_perplexity=1105.036, train_loss=7.007633

Batch 106470, train_perplexity=1043.7954, train_loss=6.9506187

Batch 106480, train_perplexity=874.529, train_loss=6.7736855

Batch 106490, train_perplexity=1023.324, train_loss=6.9308114

Batch 106500, train_perplexity=1082.8079, train_loss=6.987313

Batch 106510, train_perplexity=1032.9048, train_loss=6.94013

Batch 106520, train_perplexity=1038.1189, train_loss=6.9451656

Batch 106530, train_perplexity=988.8122, train_loss=6.8965044

Batch 106540, train_perplexity=920.08295, train_loss=6.824464

Batch 106550, train_perplexity=988.64246, train_loss=6.8963327

Batch 106560, train_perplexity=1028.1843, train_loss=6.9355497

Batch 106570, train_perplexity=1027.5325, train_loss=6.9349155

Batch 106580, train_perplexity=957.0931, train_loss=6.8639007

Batch 106590, train_perplexity=922.5273, train_loss=6.827117

Batch 106600, train_perplexity=967.61664, train_loss=6.874836

Batch 106610, train_perplexity=936.064, train_loss=6.841684

Batch 106620, train_perplexity=951.39856, train_loss=6.857933

Batch 106630, train_perplexity=1037.3193, train_loss=6.944395

Batch 106640, train_perplexity=1001.2087, train_loss=6.908963

Batch 106650, train_perplexity=955.5094, train_loss=6.8622446

Batch 106660, train_perplexity=989.4522, train_loss=6.8971515

Batch 106670, train_perplexity=1010.3641, train_loss=6.918066

Batch 106680, train_perplexity=1025.2136, train_loss=6.9326563

Batch 106690, train_perplexity=971.0925, train_loss=6.878422

Batch 106700, train_perplexity=965.7166, train_loss=6.8728704

Batch 106710, train_perplexity=987.0969, train_loss=6.894768

Batch 106720, train_perplexity=972.7355, train_loss=6.880112

Batch 106730, train_perplexity=946.9558, train_loss=6.8532524

Batch 106740, train_perplexity=1106.8422, train_loss=7.0092664

Batch 106750, train_perplexity=1004.8867, train_loss=6.91263

Batch 106760, train_perplexity=976.15295, train_loss=6.8836193

Batch 106770, train_perplexity=990.3486, train_loss=6.898057

Batch 106780, train_perplexity=1069.9779, train_loss=6.9753933

Batch 106790, train_perplexity=943.8597, train_loss=6.8499775

Batch 106800, train_perplexity=1025.3721, train_loss=6.932811

Batch 106810, train_perplexity=1039.3319, train_loss=6.9463334

Batch 106820, train_perplexity=993.1728, train_loss=6.9009047

Batch 106830, train_perplexity=1161.3477, train_loss=7.0573363

Batch 106840, train_perplexity=1089.3367, train_loss=6.9933243

Batch 106850, train_perplexity=948.2544, train_loss=6.854623

Batch 106860, train_perplexity=991.23584, train_loss=6.8989525

Batch 106870, train_perplexity=922.10596, train_loss=6.82666

Batch 106880, train_perplexity=1023.10394, train_loss=6.9305964
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 106890, train_perplexity=1076.4125, train_loss=6.981389

Batch 106900, train_perplexity=1040.3042, train_loss=6.9472685

Batch 106910, train_perplexity=1024.5167, train_loss=6.9319763

Batch 106920, train_perplexity=1001.21344, train_loss=6.908968

Batch 106930, train_perplexity=1062.0006, train_loss=6.96791

Batch 106940, train_perplexity=1052.703, train_loss=6.9591165

Batch 106950, train_perplexity=974.63763, train_loss=6.882066

Batch 106960, train_perplexity=861.2253, train_loss=6.758356

Batch 106970, train_perplexity=1012.8464, train_loss=6.92052

Batch 106980, train_perplexity=1071.0846, train_loss=6.976427

Batch 106990, train_perplexity=1053.9521, train_loss=6.9603024

Batch 107000, train_perplexity=1025.6791, train_loss=6.93311

Batch 107010, train_perplexity=971.3982, train_loss=6.8787365

Batch 107020, train_perplexity=1045.7528, train_loss=6.952492

Batch 107030, train_perplexity=982.1086, train_loss=6.889702

Batch 107040, train_perplexity=1091.0834, train_loss=6.9949265

Batch 107050, train_perplexity=966.9386, train_loss=6.874135

Batch 107060, train_perplexity=962.5844, train_loss=6.8696218

Batch 107070, train_perplexity=1004.40906, train_loss=6.9121547

Batch 107080, train_perplexity=1039.7136, train_loss=6.9467006

Batch 107090, train_perplexity=980.13617, train_loss=6.8876915

Batch 107100, train_perplexity=1027.9657, train_loss=6.935337

Batch 107110, train_perplexity=1061.9506, train_loss=6.9678626

Batch 107120, train_perplexity=920.7106, train_loss=6.8251457

Batch 107130, train_perplexity=942.3284, train_loss=6.848354

Batch 107140, train_perplexity=987.831, train_loss=6.8955116

Batch 107150, train_perplexity=1043.0839, train_loss=6.949937

Batch 107160, train_perplexity=965.5997, train_loss=6.8727493

Batch 107170, train_perplexity=1022.79663, train_loss=6.930296

Batch 107180, train_perplexity=1002.25476, train_loss=6.9100075

Batch 107190, train_perplexity=1029.0447, train_loss=6.936386

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Batch 107200, train_perplexity=947.6686, train_loss=6.854005

Batch 107210, train_perplexity=1060.7268, train_loss=6.9667096

Batch 107220, train_perplexity=938.8543, train_loss=6.8446603

Batch 107230, train_perplexity=1012.68506, train_loss=6.9203606

Batch 107240, train_perplexity=960.44006, train_loss=6.8673916

Batch 107250, train_perplexity=963.847, train_loss=6.8709326

Batch 107260, train_perplexity=963.25525, train_loss=6.8703184

Batch 107270, train_perplexity=1008.6619, train_loss=6.91638

Batch 107280, train_perplexity=1104.2512, train_loss=7.0069227

Batch 107290, train_perplexity=995.263, train_loss=6.903007

Batch 107300, train_perplexity=1021.3979, train_loss=6.9289274

Batch 107310, train_perplexity=1071.4994, train_loss=6.9768143

Batch 107320, train_perplexity=1045.2472, train_loss=6.9520087

Batch 107330, train_perplexity=960.5399, train_loss=6.8674955

Batch 107340, train_perplexity=989.50977, train_loss=6.8972096

Batch 107350, train_perplexity=1105.0665, train_loss=7.007661

Batch 107360, train_perplexity=1022.51184, train_loss=6.9300175

Batch 107370, train_perplexity=1021.1373, train_loss=6.9286723

Batch 107380, train_perplexity=1065.0282, train_loss=6.9707565

Batch 107390, train_perplexity=1041.2124, train_loss=6.948141

Batch 107400, train_perplexity=1023.0639, train_loss=6.9305573

Batch 107410, train_perplexity=993.2552, train_loss=6.9009876

Batch 107420, train_perplexity=915.4398, train_loss=6.8194046

Batch 107430, train_perplexity=951.9994, train_loss=6.8585644

Batch 107440, train_perplexity=1004.6342, train_loss=6.912379

Batch 107450, train_perplexity=982.36755, train_loss=6.8899655

Batch 107460, train_perplexity=1100.0321, train_loss=7.0030947

Batch 107470, train_perplexity=1076.4576, train_loss=6.981431

Batch 107480, train_perplexity=898.66644, train_loss=6.800912

Batch 107490, train_perplexity=1008.4234, train_loss=6.9161434

Batch 107500, train_perplexity=1038.9905, train_loss=6.946005

Batch 107510, train_perplexity=1038.3486, train_loss=6.945387

Batch 107520, train_perplexity=1063.3151, train_loss=6.9691467

Batch 107530, train_perplexity=1007.0001, train_loss=6.914731

Batch 107540, train_perplexity=951.94855, train_loss=6.858511

Batch 107550, train_perplexity=1051.3391, train_loss=6.95782

Batch 107560, train_perplexity=1023.0932, train_loss=6.930586

Batch 107570, train_perplexity=882.45703, train_loss=6.78271

Batch 107580, train_perplexity=893.05927, train_loss=6.794653

Batch 107590, train_perplexity=1057.1614, train_loss=6.9633427

Batch 107600, train_perplexity=924.3168, train_loss=6.829055

Batch 107610, train_perplexity=1028.1873, train_loss=6.9355526

Batch 107620, train_perplexity=938.35614, train_loss=6.8441296

Batch 107630, train_perplexity=1056.19, train_loss=6.9624233

Batch 107640, train_perplexity=953.62775, train_loss=6.8602734

Batch 107650, train_perplexity=961.80304, train_loss=6.8688097

Batch 107660, train_perplexity=865.26825, train_loss=6.7630396

Batch 107670, train_perplexity=1004.16486, train_loss=6.9119115

Batch 107680, train_perplexity=1003.6206, train_loss=6.9113693

Batch 107690, train_perplexity=1010.3766, train_loss=6.9180784

Batch 107700, train_perplexity=1004.4795, train_loss=6.912225

Batch 107710, train_perplexity=971.9088, train_loss=6.879262

Batch 107720, train_perplexity=1027.6942, train_loss=6.935073

Batch 107730, train_perplexity=961.38855, train_loss=6.8683786

Batch 107740, train_perplexity=953.07404, train_loss=6.8596926

Batch 107750, train_perplexity=944.5323, train_loss=6.85069

Batch 107760, train_perplexity=965.7765, train_loss=6.8729324

Batch 107770, train_perplexity=1014.5183, train_loss=6.922169

Batch 107780, train_perplexity=924.81934, train_loss=6.8295984

Batch 107790, train_perplexity=887.3521, train_loss=6.788242

Batch 107800, train_perplexity=977.12256, train_loss=6.884612

Batch 107810, train_perplexity=923.9497, train_loss=6.8286576

Batch 107820, train_perplexity=973.80615, train_loss=6.881212

Batch 107830, train_perplexity=983.3288, train_loss=6.8909435

Batch 107840, train_perplexity=949.44165, train_loss=6.855874

Batch 107850, train_perplexity=1060.2489, train_loss=6.966259

Batch 107860, train_perplexity=974.8013, train_loss=6.8822336

Batch 107870, train_perplexity=933.2451, train_loss=6.838668

Batch 107880, train_perplexity=1030.8333, train_loss=6.9381227

Batch 107890, train_perplexity=1014.77234, train_loss=6.9224195

Batch 107900, train_perplexity=1031.143, train_loss=6.938423

Batch 107910, train_perplexity=914.6169, train_loss=6.8185053

Batch 107920, train_perplexity=1040.9592, train_loss=6.947898

Batch 107930, train_perplexity=916.1341, train_loss=6.820163

Batch 107940, train_perplexity=995.78186, train_loss=6.903528

Batch 107950, train_perplexity=939.1422, train_loss=6.844967

Batch 107960, train_perplexity=930.4555, train_loss=6.8356743

Batch 107970, train_perplexity=984.29236, train_loss=6.891923

Batch 107980, train_perplexity=1008.72253, train_loss=6.91644

Batch 107990, train_perplexity=952.08154, train_loss=6.8586507

Batch 108000, train_perplexity=1004.84644, train_loss=6.91259

Batch 108010, train_perplexity=874.3672, train_loss=6.7735004

Batch 108020, train_perplexity=910.64374, train_loss=6.814152

Batch 108030, train_perplexity=1003.57416, train_loss=6.911323

Batch 108040, train_perplexity=851.6951, train_loss=6.7472286

Batch 108050, train_perplexity=904.18933, train_loss=6.807039

Batch 108060, train_perplexity=1041.8342, train_loss=6.948738

Batch 108070, train_perplexity=983.16846, train_loss=6.8907804

Batch 108080, train_perplexity=1057.3792, train_loss=6.9635487

Batch 108090, train_perplexity=1031.4203, train_loss=6.938692

Batch 108100, train_perplexity=989.4102, train_loss=6.897109

Batch 108110, train_perplexity=1101.2124, train_loss=7.004167

Batch 108120, train_perplexity=1013.3729, train_loss=6.9210396
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 108130, train_perplexity=1016.93176, train_loss=6.9245453

Batch 108140, train_perplexity=1057.4846, train_loss=6.9636483

Batch 108150, train_perplexity=932.10657, train_loss=6.837447

Batch 108160, train_perplexity=935.3372, train_loss=6.840907

Batch 108170, train_perplexity=1029.8673, train_loss=6.9371853

Batch 108180, train_perplexity=1087.8812, train_loss=6.991987

Batch 108190, train_perplexity=971.0148, train_loss=6.8783417

Batch 108200, train_perplexity=882.96423, train_loss=6.7832847

Batch 108210, train_perplexity=940.03107, train_loss=6.845913

Batch 108220, train_perplexity=1070.3535, train_loss=6.9757442

Batch 108230, train_perplexity=942.1285, train_loss=6.8481417

Batch 108240, train_perplexity=1065.7161, train_loss=6.971402

Batch 108250, train_perplexity=1031.0879, train_loss=6.9383698

Batch 108260, train_perplexity=911.75433, train_loss=6.8153706

Batch 108270, train_perplexity=1087.4823, train_loss=6.9916205

Batch 108280, train_perplexity=996.51, train_loss=6.904259

Batch 108290, train_perplexity=908.46173, train_loss=6.811753

Batch 108300, train_perplexity=1036.8832, train_loss=6.9439745

Batch 108310, train_perplexity=951.7688, train_loss=6.858322

Batch 108320, train_perplexity=989.6735, train_loss=6.897375

Batch 108330, train_perplexity=1026.9398, train_loss=6.9343386

Batch 108340, train_perplexity=1074.6523, train_loss=6.9797525

Batch 108350, train_perplexity=991.4864, train_loss=6.899205

Batch 108360, train_perplexity=932.1097, train_loss=6.8374505

Batch 108370, train_perplexity=846.2511, train_loss=6.740816

Batch 108380, train_perplexity=1005.3794, train_loss=6.9131203

Batch 108390, train_perplexity=935.4188, train_loss=6.8409944

Batch 108400, train_perplexity=1048.2625, train_loss=6.9548893

Batch 108410, train_perplexity=1053.729, train_loss=6.9600906

Batch 108420, train_perplexity=958.4646, train_loss=6.8653326

Batch 108430, train_perplexity=1064.0555, train_loss=6.969843

Batch 108440, train_perplexity=965.0616, train_loss=6.872192

Batch 108450, train_perplexity=996.2416, train_loss=6.90399

Batch 108460, train_perplexity=1011.21814, train_loss=6.918911

Batch 108470, train_perplexity=1012.1352, train_loss=6.9198174

Batch 108480, train_perplexity=998.0227, train_loss=6.905776

Batch 108490, train_perplexity=968.3681, train_loss=6.8756123

Batch 108500, train_perplexity=1017.0651, train_loss=6.9246764

Batch 108510, train_perplexity=932.8358, train_loss=6.838229

Batch 108520, train_perplexity=1105.523, train_loss=7.008074

Batch 108530, train_perplexity=1103.9453, train_loss=7.0066457

Batch 108540, train_perplexity=940.32605, train_loss=6.8462267

Batch 108550, train_perplexity=1049.8221, train_loss=6.956376

Batch 108560, train_perplexity=984.0821, train_loss=6.8917093

Batch 108570, train_perplexity=959.1961, train_loss=6.8660955

Batch 108580, train_perplexity=1104.8558, train_loss=7.00747

Batch 108590, train_perplexity=1046.3339, train_loss=6.9530478

Batch 108600, train_perplexity=1070.3561, train_loss=6.9757466

Batch 108610, train_perplexity=961.52014, train_loss=6.8685155

Batch 108620, train_perplexity=946.5071, train_loss=6.8527784

Batch 108630, train_perplexity=885.3787, train_loss=6.7860155

Batch 108640, train_perplexity=972.39044, train_loss=6.8797574

Batch 108650, train_perplexity=1034.4297, train_loss=6.9416056

Batch 108660, train_perplexity=974.24457, train_loss=6.8816624

Batch 108670, train_perplexity=1004.5427, train_loss=6.9122877

Batch 108680, train_perplexity=1029.7122, train_loss=6.9370346

Batch 108690, train_perplexity=959.7964, train_loss=6.866721

Batch 108700, train_perplexity=1092.6516, train_loss=6.9963627

Batch 108710, train_perplexity=1008.13544, train_loss=6.915858

Batch 108720, train_perplexity=1061.051, train_loss=6.9670153

Batch 108730, train_perplexity=1016.415, train_loss=6.924037

Batch 108740, train_perplexity=967.2762, train_loss=6.874484

Batch 108750, train_perplexity=981.79767, train_loss=6.889385

Batch 108760, train_perplexity=970.17615, train_loss=6.8774776

Batch 108770, train_perplexity=1156.975, train_loss=7.053564

Batch 108780, train_perplexity=950.47534, train_loss=6.856962

Batch 108790, train_perplexity=910.9834, train_loss=6.8145247

Batch 108800, train_perplexity=1067.8474, train_loss=6.9734

Batch 108810, train_perplexity=965.29675, train_loss=6.8724356

Batch 108820, train_perplexity=978.6744, train_loss=6.886199

Batch 108830, train_perplexity=886.2708, train_loss=6.7870226

Batch 108840, train_perplexity=989.2597, train_loss=6.896957

Batch 108850, train_perplexity=917.604, train_loss=6.821766

Batch 108860, train_perplexity=1064.2708, train_loss=6.970045

Batch 108870, train_perplexity=999.3866, train_loss=6.9071417

Batch 108880, train_perplexity=933.2024, train_loss=6.838622

Batch 108890, train_perplexity=1010.0481, train_loss=6.917753

Batch 108900, train_perplexity=1035.8286, train_loss=6.942957

Batch 108910, train_perplexity=1049.5333, train_loss=6.956101

Batch 108920, train_perplexity=969.78485, train_loss=6.8770742

Batch 108930, train_perplexity=1032.7855, train_loss=6.940015

Batch 108940, train_perplexity=1057.0127, train_loss=6.963202

Batch 108950, train_perplexity=1020.35126, train_loss=6.927902

Batch 108960, train_perplexity=1013.96454, train_loss=6.921623

Batch 108970, train_perplexity=963.55566, train_loss=6.8706303

Batch 108980, train_perplexity=1058.276, train_loss=6.9643965

Batch 108990, train_perplexity=942.6646, train_loss=6.8487105

Batch 109000, train_perplexity=951.49384, train_loss=6.858033

Batch 109010, train_perplexity=1061.0996, train_loss=6.967061

Batch 109020, train_perplexity=954.02344, train_loss=6.860688

Batch 109030, train_perplexity=927.2161, train_loss=6.8321867

Batch 109040, train_perplexity=980.815, train_loss=6.888384

Batch 109050, train_perplexity=973.0115, train_loss=6.880396

Batch 109060, train_perplexity=1013.50195, train_loss=6.921167

Batch 109070, train_perplexity=1045.1521, train_loss=6.9519176

Batch 109080, train_perplexity=957.4514, train_loss=6.864275

Batch 109090, train_perplexity=1137.8564, train_loss=7.0369015

Batch 109100, train_perplexity=967.9378, train_loss=6.875168

Batch 109110, train_perplexity=901.0702, train_loss=6.803583

Batch 109120, train_perplexity=1025.7222, train_loss=6.933152

Batch 109130, train_perplexity=948.02563, train_loss=6.8543816

Batch 109140, train_perplexity=1021.4514, train_loss=6.92898

Batch 109150, train_perplexity=1037.9052, train_loss=6.9449596

Batch 109160, train_perplexity=898.18317, train_loss=6.800374

Batch 109170, train_perplexity=928.9863, train_loss=6.834094

Batch 109180, train_perplexity=1012.36206, train_loss=6.9200416

Batch 109190, train_perplexity=908.4838, train_loss=6.811777

Batch 109200, train_perplexity=918.73883, train_loss=6.823002

Batch 109210, train_perplexity=954.27, train_loss=6.8609467

Batch 109220, train_perplexity=898.9656, train_loss=6.8012447

Batch 109230, train_perplexity=892.3407, train_loss=6.793848

Batch 109240, train_perplexity=943.821, train_loss=6.8499365

Batch 109250, train_perplexity=1033.2944, train_loss=6.9405074

Batch 109260, train_perplexity=947.4937, train_loss=6.8538203

Batch 109270, train_perplexity=986.10004, train_loss=6.893758

Batch 109280, train_perplexity=1026.6548, train_loss=6.934061

Batch 109290, train_perplexity=1000.177, train_loss=6.9079323

Batch 109300, train_perplexity=1102.0115, train_loss=7.0048923

Batch 109310, train_perplexity=1076.5763, train_loss=6.981541

Batch 109320, train_perplexity=868.1422, train_loss=6.7663555

Batch 109330, train_perplexity=976.8915, train_loss=6.8843756

Batch 109340, train_perplexity=1034.3888, train_loss=6.941566

Batch 109350, train_perplexity=929.0076, train_loss=6.834117

Batch 109360, train_perplexity=1120.5474, train_loss=7.0215726

Batch 109370, train_perplexity=1010.19165, train_loss=6.9178953

Batch 109380, train_perplexity=1005.458, train_loss=6.9131985

Batch 109390, train_perplexity=954.452, train_loss=6.8611374

Batch 109400, train_perplexity=1010.56354, train_loss=6.9182634

Batch 109410, train_perplexity=1116.685, train_loss=7.01812

Batch 109420, train_perplexity=983.4066, train_loss=6.8910227
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 109430, train_perplexity=974.1368, train_loss=6.8815517

Batch 109440, train_perplexity=950.64935, train_loss=6.8571453

Batch 109450, train_perplexity=973.7198, train_loss=6.8811235

Batch 109460, train_perplexity=1048.6904, train_loss=6.9552975

Batch 109470, train_perplexity=918.4081, train_loss=6.822642

Batch 109480, train_perplexity=913.4279, train_loss=6.8172045

Batch 109490, train_perplexity=998.1122, train_loss=6.9058657

Batch 109500, train_perplexity=936.84546, train_loss=6.8425183

Batch 109510, train_perplexity=1077.1118, train_loss=6.9820385

Batch 109520, train_perplexity=1036.6449, train_loss=6.9437447

Batch 109530, train_perplexity=958.4006, train_loss=6.865266

Batch 109540, train_perplexity=928.48413, train_loss=6.8335533

Batch 109550, train_perplexity=1017.134, train_loss=6.924744

Batch 109560, train_perplexity=927.6257, train_loss=6.8326283

Batch 109570, train_perplexity=888.0281, train_loss=6.7890034

Batch 109580, train_perplexity=1082.2292, train_loss=6.9867783

Batch 109590, train_perplexity=907.8126, train_loss=6.811038

Batch 109600, train_perplexity=982.26215, train_loss=6.8898582

Batch 109610, train_perplexity=962.3297, train_loss=6.869357

Batch 109620, train_perplexity=1062.569, train_loss=6.968445

Batch 109630, train_perplexity=1011.4897, train_loss=6.9191794

Batch 109640, train_perplexity=988.84045, train_loss=6.896533

Batch 109650, train_perplexity=1041.3579, train_loss=6.948281

Batch 109660, train_perplexity=991.35876, train_loss=6.8990765

Batch 109670, train_perplexity=921.5161, train_loss=6.8260202

Batch 109680, train_perplexity=954.13715, train_loss=6.8608074

Batch 109690, train_perplexity=1058.9545, train_loss=6.9650373

Batch 109700, train_perplexity=959.5877, train_loss=6.8665037

Batch 109710, train_perplexity=1091.3754, train_loss=6.995194

Batch 109720, train_perplexity=940.0643, train_loss=6.845948

Batch 109730, train_perplexity=1099.4396, train_loss=7.002556

Batch 109740, train_perplexity=1002.8619, train_loss=6.910613

Batch 109750, train_perplexity=1058.7817, train_loss=6.9648743

Batch 109760, train_perplexity=1000.4885, train_loss=6.9082437

Batch 109770, train_perplexity=976.0831, train_loss=6.883548

Batch 109780, train_perplexity=917.807, train_loss=6.821987

Batch 109790, train_perplexity=1026.55, train_loss=6.933959

Batch 109800, train_perplexity=1024.0049, train_loss=6.9314766

Batch 109810, train_perplexity=858.58215, train_loss=6.7552824

Batch 109820, train_perplexity=1002.6257, train_loss=6.9103775

Batch 109830, train_perplexity=1106.1974, train_loss=7.0086837

Batch 109840, train_perplexity=931.636, train_loss=6.836942

Batch 109850, train_perplexity=966.01324, train_loss=6.8731775

Batch 109860, train_perplexity=965.56006, train_loss=6.8727083

Batch 109870, train_perplexity=975.3113, train_loss=6.8827567

Batch 109880, train_perplexity=1100.6843, train_loss=7.0036874

Batch 109890, train_perplexity=989.7943, train_loss=6.897497

Batch 109900, train_perplexity=938.7661, train_loss=6.8445663

Batch 109910, train_perplexity=986.48285, train_loss=6.894146

Batch 109920, train_perplexity=967.81415, train_loss=6.87504

Batch 109930, train_perplexity=956.83344, train_loss=6.8636293

Batch 109940, train_perplexity=917.70374, train_loss=6.8218746

Batch 109950, train_perplexity=999.8823, train_loss=6.9076376

Batch 109960, train_perplexity=969.71826, train_loss=6.8770056

Batch 109970, train_perplexity=1004.77936, train_loss=6.9125233

Batch 109980, train_perplexity=981.85474, train_loss=6.8894434

Batch 109990, train_perplexity=1048.6064, train_loss=6.9552174

Batch 110000, train_perplexity=1100.7227, train_loss=7.003722

Batch 110010, train_perplexity=947.2525, train_loss=6.8535657

Batch 110020, train_perplexity=956.70294, train_loss=6.863493

Batch 110030, train_perplexity=957.66284, train_loss=6.8644958

Batch 110040, train_perplexity=1032.3906, train_loss=6.9396324

Batch 110050, train_perplexity=873.5854, train_loss=6.772606

Batch 110060, train_perplexity=1021.76807, train_loss=6.92929

Batch 110070, train_perplexity=980.1726, train_loss=6.8877287

Batch 110080, train_perplexity=972.23926, train_loss=6.879602

Batch 110090, train_perplexity=910.8153, train_loss=6.81434

Batch 110100, train_perplexity=1015.0414, train_loss=6.9226847

Batch 110110, train_perplexity=1010.8224, train_loss=6.9185195

Batch 110120, train_perplexity=1001.53143, train_loss=6.9092855

Batch 110130, train_perplexity=1082.6132, train_loss=6.987133

Batch 110140, train_perplexity=1008.2782, train_loss=6.9159994

Batch 110150, train_perplexity=907.20074, train_loss=6.810364

Batch 110160, train_perplexity=941.278, train_loss=6.8472385

Batch 110170, train_perplexity=949.1234, train_loss=6.855539

Batch 110180, train_perplexity=1022.87805, train_loss=6.9303756

Batch 110190, train_perplexity=937.8685, train_loss=6.84361

Batch 110200, train_perplexity=1047.2258, train_loss=6.9539

Batch 110210, train_perplexity=1013.9926, train_loss=6.921651

Batch 110220, train_perplexity=933.0244, train_loss=6.8384314

Batch 110230, train_perplexity=969.3757, train_loss=6.8766522

Batch 110240, train_perplexity=944.69714, train_loss=6.8508644

Batch 110250, train_perplexity=1018.5818, train_loss=6.9261665

Batch 110260, train_perplexity=968.3838, train_loss=6.8756285

Batch 110270, train_perplexity=1073.9993, train_loss=6.9791446

Batch 110280, train_perplexity=1035.4562, train_loss=6.9425974

Batch 110290, train_perplexity=1072.3796, train_loss=6.9776354

Batch 110300, train_perplexity=964.71326, train_loss=6.871831

Batch 110310, train_perplexity=966.509, train_loss=6.8736906

Batch 110320, train_perplexity=992.97577, train_loss=6.9007063

Batch 110330, train_perplexity=1094.7961, train_loss=6.9983234

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Batch 110340, train_perplexity=947.6469, train_loss=6.853982

Batch 110350, train_perplexity=986.6823, train_loss=6.894348

Batch 110360, train_perplexity=1072.6537, train_loss=6.977891

Batch 110370, train_perplexity=993.51715, train_loss=6.9012513

Batch 110380, train_perplexity=1006.7658, train_loss=6.9144983

Batch 110390, train_perplexity=987.59784, train_loss=6.8952756

Batch 110400, train_perplexity=1034.7139, train_loss=6.94188

Batch 110410, train_perplexity=913.4767, train_loss=6.817258

Batch 110420, train_perplexity=1015.4892, train_loss=6.9231257

Batch 110430, train_perplexity=1031.8892, train_loss=6.9391465

Batch 110440, train_perplexity=987.2998, train_loss=6.8949738

Batch 110450, train_perplexity=941.9156, train_loss=6.8479156

Batch 110460, train_perplexity=926.7891, train_loss=6.831726

Batch 110470, train_perplexity=957.5637, train_loss=6.8643923

Batch 110480, train_perplexity=1006.53784, train_loss=6.914272

Batch 110490, train_perplexity=1079.7097, train_loss=6.9844475

Batch 110500, train_perplexity=958.2745, train_loss=6.8651342

Batch 110510, train_perplexity=978.10803, train_loss=6.88562

Batch 110520, train_perplexity=1027.0201, train_loss=6.934417

Batch 110530, train_perplexity=970.01056, train_loss=6.877307

Batch 110540, train_perplexity=1037.6508, train_loss=6.9447145

Batch 110550, train_perplexity=1037.8932, train_loss=6.944948

Batch 110560, train_perplexity=999.4557, train_loss=6.907211

Batch 110570, train_perplexity=940.9154, train_loss=6.8468533

Batch 110580, train_perplexity=911.25793, train_loss=6.814826

Batch 110590, train_perplexity=1026.4923, train_loss=6.9339027

Batch 110600, train_perplexity=971.2065, train_loss=6.878539

Batch 110610, train_perplexity=873.19476, train_loss=6.7721586

Batch 110620, train_perplexity=922.3171, train_loss=6.826889

Batch 110630, train_perplexity=952.62964, train_loss=6.859226

Batch 110640, train_perplexity=887.3661, train_loss=6.7882576

Batch 110650, train_perplexity=1018.6391, train_loss=6.926223

Batch 110660, train_perplexity=959.4312, train_loss=6.8663406
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 110670, train_perplexity=941.5316, train_loss=6.847508

Batch 110680, train_perplexity=964.68616, train_loss=6.871803

Batch 110690, train_perplexity=972.9892, train_loss=6.880373

Batch 110700, train_perplexity=1008.61676, train_loss=6.916335

Batch 110710, train_perplexity=922.0664, train_loss=6.8266172

Batch 110720, train_perplexity=924.4358, train_loss=6.8291836

Batch 110730, train_perplexity=1043.3838, train_loss=6.9502244

Batch 110740, train_perplexity=1085.2808, train_loss=6.989594

Batch 110750, train_perplexity=1103.3827, train_loss=7.006136

Batch 110760, train_perplexity=960.354, train_loss=6.867302

Batch 110770, train_perplexity=971.1018, train_loss=6.8784313

Batch 110780, train_perplexity=945.714, train_loss=6.85194

Batch 110790, train_perplexity=929.96985, train_loss=6.835152

Batch 110800, train_perplexity=1057.0394, train_loss=6.9632273

Batch 110810, train_perplexity=972.76373, train_loss=6.8801413

Batch 110820, train_perplexity=893.5806, train_loss=6.7952366

Batch 110830, train_perplexity=1033.7074, train_loss=6.940907

Batch 110840, train_perplexity=1124.7261, train_loss=7.025295

Batch 110850, train_perplexity=1031.932, train_loss=6.939188

Batch 110860, train_perplexity=948.9112, train_loss=6.855315

Batch 110870, train_perplexity=891.09656, train_loss=6.792453

Batch 110880, train_perplexity=1001.13226, train_loss=6.908887

Batch 110890, train_perplexity=993.41766, train_loss=6.901151

Batch 110900, train_perplexity=1017.7962, train_loss=6.925395

Batch 110910, train_perplexity=973.2964, train_loss=6.8806887

Batch 110920, train_perplexity=955.7901, train_loss=6.8625383

Batch 110930, train_perplexity=994.35364, train_loss=6.902093

Batch 110940, train_perplexity=793.8717, train_loss=6.676922

Batch 110950, train_perplexity=924.92694, train_loss=6.829715

Batch 110960, train_perplexity=1106.7266, train_loss=7.009162

Batch 110970, train_perplexity=1010.13574, train_loss=6.91784

Batch 110980, train_perplexity=909.6794, train_loss=6.813092

Batch 110990, train_perplexity=1036.2119, train_loss=6.943327

Batch 111000, train_perplexity=1035.391, train_loss=6.9425344

Batch 111010, train_perplexity=903.1974, train_loss=6.805941

Batch 111020, train_perplexity=1063.7628, train_loss=6.969568

Batch 111030, train_perplexity=1054.2899, train_loss=6.960623

Batch 111040, train_perplexity=953.3249, train_loss=6.859956

Batch 111050, train_perplexity=859.2055, train_loss=6.756008

Batch 111060, train_perplexity=884.2868, train_loss=6.7847815

Batch 111070, train_perplexity=952.43756, train_loss=6.8590245

Batch 111080, train_perplexity=1006.0076, train_loss=6.913745

Batch 111090, train_perplexity=1011.5143, train_loss=6.9192038

Batch 111100, train_perplexity=980.6513, train_loss=6.888217

Batch 111110, train_perplexity=951.97534, train_loss=6.858539

Batch 111120, train_perplexity=1000.012, train_loss=6.9077673

Batch 111130, train_perplexity=1048.0446, train_loss=6.9546814

Batch 111140, train_perplexity=947.7852, train_loss=6.854128

Batch 111150, train_perplexity=911.2484, train_loss=6.8148155

Batch 111160, train_perplexity=935.4567, train_loss=6.841035

Batch 111170, train_perplexity=1094.4313, train_loss=6.99799

Batch 111180, train_perplexity=1071.5929, train_loss=6.9769015

Batch 111190, train_perplexity=1007.94366, train_loss=6.9156675

Batch 111200, train_perplexity=1090.5674, train_loss=6.9944534

Batch 111210, train_perplexity=998.6225, train_loss=6.906377

Batch 111220, train_perplexity=961.0929, train_loss=6.868071

Batch 111230, train_perplexity=1031.7125, train_loss=6.9389753

Batch 111240, train_perplexity=963.99774, train_loss=6.871089

Batch 111250, train_perplexity=1036.5747, train_loss=6.943677

Batch 111260, train_perplexity=1106.62, train_loss=7.0090656

Batch 111270, train_perplexity=962.8823, train_loss=6.869931

Batch 111280, train_perplexity=1037.2985, train_loss=6.944375

Batch 111290, train_perplexity=1015.17206, train_loss=6.9228134

Batch 111300, train_perplexity=977.1319, train_loss=6.8846216

Batch 111310, train_perplexity=993.0847, train_loss=6.900816

Batch 111320, train_perplexity=1029.241, train_loss=6.936577

Batch 111330, train_perplexity=966.42145, train_loss=6.8736

Batch 111340, train_perplexity=1007.00446, train_loss=6.9147353

Batch 111350, train_perplexity=931.1, train_loss=6.8363667

Batch 111360, train_perplexity=974.4434, train_loss=6.8818665

Batch 111370, train_perplexity=958.33844, train_loss=6.865201

Batch 111380, train_perplexity=969.35486, train_loss=6.876631

Batch 111390, train_perplexity=964.762, train_loss=6.8718815

Batch 111400, train_perplexity=981.6446, train_loss=6.8892293

Batch 111410, train_perplexity=969.3785, train_loss=6.876655

Batch 111420, train_perplexity=936.6623, train_loss=6.842323

Batch 111430, train_perplexity=954.686, train_loss=6.8613825

Batch 111440, train_perplexity=958.18677, train_loss=6.8650427

Batch 111450, train_perplexity=995.6233, train_loss=6.903369

Batch 111460, train_perplexity=924.7263, train_loss=6.829498

Batch 111470, train_perplexity=990.7883, train_loss=6.898501

Batch 111480, train_perplexity=978.54376, train_loss=6.8860655

Batch 111490, train_perplexity=964.85223, train_loss=6.871975

Batch 111500, train_perplexity=931.7964, train_loss=6.8371143

Batch 111510, train_perplexity=896.5486, train_loss=6.7985525

Batch 111520, train_perplexity=929.0249, train_loss=6.8341355

Batch 111530, train_perplexity=1044.1512, train_loss=6.9509597

Batch 111540, train_perplexity=891.9783, train_loss=6.793442

Batch 111550, train_perplexity=950.0639, train_loss=6.856529

Batch 111560, train_perplexity=1019.9752, train_loss=6.9275336

Batch 111570, train_perplexity=1026.6793, train_loss=6.934085

Batch 111580, train_perplexity=1011.454, train_loss=6.919144

Batch 111590, train_perplexity=1039.5941, train_loss=6.9465857

Batch 111600, train_perplexity=919.7176, train_loss=6.8240666

Batch 111610, train_perplexity=927.80615, train_loss=6.832823

Batch 111620, train_perplexity=1067.4926, train_loss=6.9730678

Batch 111630, train_perplexity=961.65814, train_loss=6.868659

Batch 111640, train_perplexity=983.5745, train_loss=6.8911934

Batch 111650, train_perplexity=1024.0161, train_loss=6.9314876

Batch 111660, train_perplexity=1044.3938, train_loss=6.951192

Batch 111670, train_perplexity=952.75867, train_loss=6.8593616

Batch 111680, train_perplexity=952.7396, train_loss=6.8593416

Batch 111690, train_perplexity=965.19275, train_loss=6.872328

Batch 111700, train_perplexity=979.7838, train_loss=6.887332

Batch 111710, train_perplexity=982.3362, train_loss=6.8899336

Batch 111720, train_perplexity=956.80743, train_loss=6.863602

Batch 111730, train_perplexity=1027.5629, train_loss=6.934945

Batch 111740, train_perplexity=993.43616, train_loss=6.90117

Batch 111750, train_perplexity=1043.0471, train_loss=6.9499016

Batch 111760, train_perplexity=1010.40314, train_loss=6.9181046

Batch 111770, train_perplexity=1096.3723, train_loss=6.999762

Batch 111780, train_perplexity=928.9288, train_loss=6.834032

Batch 111790, train_perplexity=1001.45215, train_loss=6.9092064

Batch 111800, train_perplexity=924.05237, train_loss=6.8287687

Batch 111810, train_perplexity=1079.7375, train_loss=6.984473

Batch 111820, train_perplexity=1108.4473, train_loss=7.0107155

Batch 111830, train_perplexity=985.03986, train_loss=6.892682

Batch 111840, train_perplexity=947.5877, train_loss=6.8539195

Batch 111850, train_perplexity=992.8011, train_loss=6.9005303

Batch 111860, train_perplexity=1052.1816, train_loss=6.958621

Batch 111870, train_perplexity=1012.3635, train_loss=6.920043

Batch 111880, train_perplexity=952.96814, train_loss=6.8595815

Batch 111890, train_perplexity=990.4865, train_loss=6.898196

Batch 111900, train_perplexity=993.33, train_loss=6.901063

Batch 111910, train_perplexity=1027.1533, train_loss=6.9345465

Batch 111920, train_perplexity=1079.9043, train_loss=6.9846277

Batch 111930, train_perplexity=913.56384, train_loss=6.8173532

Batch 111940, train_perplexity=1069.2333, train_loss=6.974697

Batch 111950, train_perplexity=965.0252, train_loss=6.872154

Batch 111960, train_perplexity=969.61096, train_loss=6.876895

Batch 111970, train_perplexity=969.4538, train_loss=6.876733
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 111980, train_perplexity=935.6619, train_loss=6.841254

Batch 111990, train_perplexity=1025.0446, train_loss=6.9324913

Batch 112000, train_perplexity=939.7859, train_loss=6.845652

Batch 112010, train_perplexity=939.7635, train_loss=6.8456283

Batch 112020, train_perplexity=1008.71344, train_loss=6.916431

Batch 112030, train_perplexity=1050.6334, train_loss=6.9571486

Batch 112040, train_perplexity=1053.4095, train_loss=6.9597874

Batch 112050, train_perplexity=1007.0021, train_loss=6.914733

Batch 112060, train_perplexity=1027.6908, train_loss=6.9350696

Batch 112070, train_perplexity=1067.8983, train_loss=6.973448

Batch 112080, train_perplexity=945.5629, train_loss=6.8517804

Batch 112090, train_perplexity=964.67694, train_loss=6.8717933

Batch 112100, train_perplexity=1033.908, train_loss=6.941101

Batch 112110, train_perplexity=1032.8284, train_loss=6.9400563

Batch 112120, train_perplexity=1003.167, train_loss=6.9109173

Batch 112130, train_perplexity=1001.3758, train_loss=6.90913

Batch 112140, train_perplexity=1015.4824, train_loss=6.923119

Batch 112150, train_perplexity=1033.8197, train_loss=6.9410157

Batch 112160, train_perplexity=969.9652, train_loss=6.87726

Batch 112170, train_perplexity=973.64453, train_loss=6.8810463

Batch 112180, train_perplexity=925.88715, train_loss=6.8307524

Batch 112190, train_perplexity=855.6632, train_loss=6.751877

Batch 112200, train_perplexity=968.81793, train_loss=6.8760767

Batch 112210, train_perplexity=1042.4584, train_loss=6.949337

Batch 112220, train_perplexity=1005.019, train_loss=6.9127617

Batch 112230, train_perplexity=978.12573, train_loss=6.885638

Batch 112240, train_perplexity=989.9043, train_loss=6.8976083

Batch 112250, train_perplexity=944.5341, train_loss=6.850692

Batch 112260, train_perplexity=1028.82, train_loss=6.9361677

Batch 112270, train_perplexity=968.369, train_loss=6.875613

Batch 112280, train_perplexity=1023.2171, train_loss=6.930707

Batch 112290, train_perplexity=1009.0598, train_loss=6.9167743

Batch 112300, train_perplexity=1015.3115, train_loss=6.9229507

Batch 112310, train_perplexity=905.49713, train_loss=6.808484

Batch 112320, train_perplexity=1049.0746, train_loss=6.9556637

Batch 112330, train_perplexity=976.4388, train_loss=6.883912

Batch 112340, train_perplexity=994.43524, train_loss=6.902175

Batch 112350, train_perplexity=963.48584, train_loss=6.870558

Batch 112360, train_perplexity=1059.1771, train_loss=6.9652476

Batch 112370, train_perplexity=880.6684, train_loss=6.780681

Batch 112380, train_perplexity=1015.25586, train_loss=6.922896

Batch 112390, train_perplexity=1076.236, train_loss=6.981225

Batch 112400, train_perplexity=970.54816, train_loss=6.877861

Batch 112410, train_perplexity=1057.4336, train_loss=6.9636

Batch 112420, train_perplexity=1060.3546, train_loss=6.9663587

Batch 112430, train_perplexity=942.7581, train_loss=6.8488097

Batch 112440, train_perplexity=931.413, train_loss=6.836703

Batch 112450, train_perplexity=1003.78046, train_loss=6.9115286

Batch 112460, train_perplexity=994.3565, train_loss=6.902096

Batch 112470, train_perplexity=1057.8214, train_loss=6.963967

Batch 112480, train_perplexity=987.65765, train_loss=6.895336

Batch 112490, train_perplexity=949.5879, train_loss=6.856028

Batch 112500, train_perplexity=1150.2189, train_loss=7.0477076

Batch 112510, train_perplexity=1145.9893, train_loss=7.0440235

Batch 112520, train_perplexity=960.8597, train_loss=6.8678284

Batch 112530, train_perplexity=990.104, train_loss=6.89781

Batch 112540, train_perplexity=931.1035, train_loss=6.8363705

Batch 112550, train_perplexity=1058.1868, train_loss=6.964312

Batch 112560, train_perplexity=910.8014, train_loss=6.814325

Batch 112570, train_perplexity=952.413, train_loss=6.858999

Batch 112580, train_perplexity=949.79663, train_loss=6.856248

Batch 112590, train_perplexity=996.5932, train_loss=6.9043427

Batch 112600, train_perplexity=1074.8871, train_loss=6.979971

Batch 112610, train_perplexity=978.2834, train_loss=6.8857994

Batch 112620, train_perplexity=1033.1033, train_loss=6.9403224

Batch 112630, train_perplexity=954.9564, train_loss=6.8616657

Batch 112640, train_perplexity=1004.60834, train_loss=6.912353

Batch 112650, train_perplexity=1020.3036, train_loss=6.9278555

Batch 112660, train_perplexity=1012.5426, train_loss=6.92022

Batch 112670, train_perplexity=1033.6409, train_loss=6.9408426

Batch 112680, train_perplexity=992.7741, train_loss=6.900503

Batch 112690, train_perplexity=1049.701, train_loss=6.9562607

Batch 112700, train_perplexity=962.4453, train_loss=6.8694773

Batch 112710, train_perplexity=999.7202, train_loss=6.9074755

Batch 112720, train_perplexity=933.47833, train_loss=6.8389177

Batch 112730, train_perplexity=1068.1274, train_loss=6.9736624

Batch 112740, train_perplexity=940.7315, train_loss=6.8466578

Batch 112750, train_perplexity=1054.5409, train_loss=6.9608607

Batch 112760, train_perplexity=1005.61914, train_loss=6.9133587

Batch 112770, train_perplexity=945.347, train_loss=6.851552

Batch 112780, train_perplexity=1139.086, train_loss=7.0379815

Batch 112790, train_perplexity=958.04785, train_loss=6.8648977

Batch 112800, train_perplexity=1012.462, train_loss=6.9201403

Batch 112810, train_perplexity=945.85693, train_loss=6.8520913

Batch 112820, train_perplexity=936.6092, train_loss=6.842266

Batch 112830, train_perplexity=1159.4198, train_loss=7.055675

Batch 112840, train_perplexity=938.32214, train_loss=6.8440933

Batch 112850, train_perplexity=947.80505, train_loss=6.854149

Batch 112860, train_perplexity=980.12775, train_loss=6.887683

Batch 112870, train_perplexity=997.9489, train_loss=6.905702

Batch 112880, train_perplexity=968.53296, train_loss=6.8757825

Batch 112890, train_perplexity=954.2236, train_loss=6.860898

Batch 112900, train_perplexity=925.7106, train_loss=6.8305616

Batch 112910, train_perplexity=966.568, train_loss=6.8737516

Batch 112920, train_perplexity=1024.4625, train_loss=6.9319234

Batch 112930, train_perplexity=984.1947, train_loss=6.891824

Batch 112940, train_perplexity=954.9209, train_loss=6.8616285

Batch 112950, train_perplexity=895.45825, train_loss=6.7973356

Batch 112960, train_perplexity=986.11884, train_loss=6.893777

Batch 112970, train_perplexity=927.12244, train_loss=6.8320856

Batch 112980, train_perplexity=902.14325, train_loss=6.8047733

Batch 112990, train_perplexity=971.17267, train_loss=6.8785043

Batch 113000, train_perplexity=979.46246, train_loss=6.887004

Batch 113010, train_perplexity=948.5601, train_loss=6.854945

Batch 113020, train_perplexity=1014.1058, train_loss=6.9217625

Batch 113030, train_perplexity=985.03516, train_loss=6.8926773

Batch 113040, train_perplexity=1051.7979, train_loss=6.9582562

Batch 113050, train_perplexity=922.63464, train_loss=6.8272333

Batch 113060, train_perplexity=975.5792, train_loss=6.8830314

Batch 113070, train_perplexity=1009.67584, train_loss=6.9173846

Batch 113080, train_perplexity=1076.1174, train_loss=6.981115

Batch 113090, train_perplexity=962.8681, train_loss=6.8699164

Batch 113100, train_perplexity=937.70667, train_loss=6.843437

Batch 113110, train_perplexity=940.7566, train_loss=6.8466845

Batch 113120, train_perplexity=953.8542, train_loss=6.860511

Batch 113130, train_perplexity=922.4939, train_loss=6.8270807

Batch 113140, train_perplexity=964.83563, train_loss=6.871958

Batch 113150, train_perplexity=968.2591, train_loss=6.8754997

Batch 113160, train_perplexity=1041.867, train_loss=6.9487696

Batch 113170, train_perplexity=1024.7719, train_loss=6.932225

Batch 113180, train_perplexity=1013.07434, train_loss=6.920745

Batch 113190, train_perplexity=947.1369, train_loss=6.8534436

Batch 113200, train_perplexity=926.56995, train_loss=6.8314896

Batch 113210, train_perplexity=956.53143, train_loss=6.8633137

Batch 113220, train_perplexity=1050.8229, train_loss=6.957329

Batch 113230, train_perplexity=977.52985, train_loss=6.885029

Batch 113240, train_perplexity=1008.48785, train_loss=6.9162073

Batch 113250, train_perplexity=881.3472, train_loss=6.7814517

Batch 113260, train_perplexity=1026.2623, train_loss=6.9336786

Batch 113270, train_perplexity=984.1999, train_loss=6.891829
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 113280, train_perplexity=868.2705, train_loss=6.7665033

Batch 113290, train_perplexity=899.4861, train_loss=6.8018236

Batch 113300, train_perplexity=1001.1571, train_loss=6.9089117

Batch 113310, train_perplexity=930.0944, train_loss=6.835286

Batch 113320, train_perplexity=951.71704, train_loss=6.858268

Batch 113330, train_perplexity=954.08844, train_loss=6.8607564

Batch 113340, train_perplexity=889.36676, train_loss=6.7905097

Batch 113350, train_perplexity=1009.684, train_loss=6.9173927

Batch 113360, train_perplexity=886.2565, train_loss=6.7870064

Batch 113370, train_perplexity=945.45337, train_loss=6.8516645

Batch 113380, train_perplexity=986.9138, train_loss=6.8945827

Batch 113390, train_perplexity=1000.3034, train_loss=6.9080586

Batch 113400, train_perplexity=909.718, train_loss=6.8131347

Batch 113410, train_perplexity=944.60706, train_loss=6.850769

Batch 113420, train_perplexity=936.636, train_loss=6.8422947

Batch 113430, train_perplexity=947.50867, train_loss=6.853836

Batch 113440, train_perplexity=919.35583, train_loss=6.8236732

Batch 113450, train_perplexity=965.28754, train_loss=6.872426

Batch 113460, train_perplexity=975.9659, train_loss=6.8834276

Batch 113470, train_perplexity=958.52356, train_loss=6.865394

Batch 113480, train_perplexity=1020.2977, train_loss=6.92785

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Batch 113490, train_perplexity=918.82556, train_loss=6.8230963

Batch 113500, train_perplexity=889.273, train_loss=6.7904043

Batch 113510, train_perplexity=1089.6754, train_loss=6.993635

Batch 113520, train_perplexity=892.4833, train_loss=6.794008

Batch 113530, train_perplexity=998.20355, train_loss=6.905957

Batch 113540, train_perplexity=1196.1593, train_loss=7.086871

Batch 113550, train_perplexity=1042.0548, train_loss=6.94895

Batch 113560, train_perplexity=999.3408, train_loss=6.907096

Batch 113570, train_perplexity=990.68866, train_loss=6.8984003

Batch 113580, train_perplexity=1035.2893, train_loss=6.942436

Batch 113590, train_perplexity=898.4436, train_loss=6.800664

Batch 113600, train_perplexity=977.6389, train_loss=6.8851404

Batch 113610, train_perplexity=861.6426, train_loss=6.7588406

Batch 113620, train_perplexity=1010.3679, train_loss=6.91807

Batch 113630, train_perplexity=995.9148, train_loss=6.9036617

Batch 113640, train_perplexity=977.61285, train_loss=6.8851137

Batch 113650, train_perplexity=1040.2784, train_loss=6.9472437

Batch 113660, train_perplexity=1025.5535, train_loss=6.9329877

Batch 113670, train_perplexity=967.2697, train_loss=6.8744774

Batch 113680, train_perplexity=951.35864, train_loss=6.857891

Batch 113690, train_perplexity=1072.083, train_loss=6.977359

Batch 113700, train_perplexity=1011.8066, train_loss=6.9194927

Batch 113710, train_perplexity=889.80963, train_loss=6.7910075

Batch 113720, train_perplexity=988.40344, train_loss=6.896091

Batch 113730, train_perplexity=972.5615, train_loss=6.8799334

Batch 113740, train_perplexity=886.45935, train_loss=6.7872353

Batch 113750, train_perplexity=899.8173, train_loss=6.8021917

Batch 113760, train_perplexity=891.66956, train_loss=6.7930956

Batch 113770, train_perplexity=963.0743, train_loss=6.8701305

Batch 113780, train_perplexity=969.13763, train_loss=6.8764067

Batch 113790, train_perplexity=1017.2237, train_loss=6.9248323

Batch 113800, train_perplexity=973.0254, train_loss=6.88041

Batch 113810, train_perplexity=995.14343, train_loss=6.902887

Batch 113820, train_perplexity=1026.1439, train_loss=6.933563

Batch 113830, train_perplexity=917.8245, train_loss=6.822006

Batch 113840, train_perplexity=990.32117, train_loss=6.8980293

Batch 113850, train_perplexity=961.7875, train_loss=6.8687935

Batch 113860, train_perplexity=927.3125, train_loss=6.8322906

Batch 113870, train_perplexity=1012.42145, train_loss=6.9201

Batch 113880, train_perplexity=974.33936, train_loss=6.8817596

Batch 113890, train_perplexity=1048.338, train_loss=6.9549613

Batch 113900, train_perplexity=894.9836, train_loss=6.7968054

Batch 113910, train_perplexity=1012.1077, train_loss=6.9197903

Batch 113920, train_perplexity=974.9937, train_loss=6.882431

Batch 113930, train_perplexity=1023.1971, train_loss=6.9306874

Batch 113940, train_perplexity=952.45886, train_loss=6.859047

Batch 113950, train_perplexity=954.402, train_loss=6.861085

Batch 113960, train_perplexity=994.7188, train_loss=6.90246

Batch 113970, train_perplexity=974.0597, train_loss=6.8814726

Batch 113980, train_perplexity=1018.20886, train_loss=6.9258003

Batch 113990, train_perplexity=917.8114, train_loss=6.821992

Batch 114000, train_perplexity=923.0795, train_loss=6.8277154

Batch 114010, train_perplexity=925.8986, train_loss=6.830765

Batch 114020, train_perplexity=863.7183, train_loss=6.7612467

Batch 114030, train_perplexity=1001.47076, train_loss=6.909225

Batch 114040, train_perplexity=913.5486, train_loss=6.8173366

Batch 114050, train_perplexity=917.7755, train_loss=6.821953

Batch 114060, train_perplexity=1043.645, train_loss=6.9504747

Batch 114070, train_perplexity=931.7831, train_loss=6.8371

Batch 114080, train_perplexity=930.063, train_loss=6.8352523

Batch 114090, train_perplexity=907.1402, train_loss=6.810297

Batch 114100, train_perplexity=910.88477, train_loss=6.8144164

Batch 114110, train_perplexity=954.6878, train_loss=6.8613844

Batch 114120, train_perplexity=998.0503, train_loss=6.9058037

Batch 114130, train_perplexity=984.3787, train_loss=6.8920107

Batch 114140, train_perplexity=1050.345, train_loss=6.956874

Batch 114150, train_perplexity=1066.6952, train_loss=6.9723206

Batch 114160, train_perplexity=956.0709, train_loss=6.862832

Batch 114170, train_perplexity=961.693, train_loss=6.8686953

Batch 114180, train_perplexity=996.9544, train_loss=6.904705

Batch 114190, train_perplexity=1050.0656, train_loss=6.956608

Batch 114200, train_perplexity=967.0806, train_loss=6.874282

Batch 114210, train_perplexity=1005.7841, train_loss=6.9135227

Batch 114220, train_perplexity=1063.5742, train_loss=6.9693904

Batch 114230, train_perplexity=917.45215, train_loss=6.8216004

Batch 114240, train_perplexity=1034.0175, train_loss=6.941207

Batch 114250, train_perplexity=981.69183, train_loss=6.8892775

Batch 114260, train_perplexity=962.5775, train_loss=6.8696146

Batch 114270, train_perplexity=929.15826, train_loss=6.834279

Batch 114280, train_perplexity=970.25385, train_loss=6.8775578

Batch 114290, train_perplexity=907.8871, train_loss=6.81112

Batch 114300, train_perplexity=1013.3343, train_loss=6.9210014

Batch 114310, train_perplexity=897.7888, train_loss=6.799935

Batch 114320, train_perplexity=861.52594, train_loss=6.758705

Batch 114330, train_perplexity=1000.3874, train_loss=6.9081426

Batch 114340, train_perplexity=966.5689, train_loss=6.8737526

Batch 114350, train_perplexity=888.6385, train_loss=6.7896905

Batch 114360, train_perplexity=1016.55457, train_loss=6.9241743

Batch 114370, train_perplexity=1035.4187, train_loss=6.942561

Batch 114380, train_perplexity=1068.1173, train_loss=6.973653

Batch 114390, train_perplexity=905.3857, train_loss=6.808361

Batch 114400, train_perplexity=1024.5026, train_loss=6.9319625

Batch 114410, train_perplexity=1006.3305, train_loss=6.914066

Batch 114420, train_perplexity=978.2867, train_loss=6.8858027

Batch 114430, train_perplexity=980.58026, train_loss=6.8881445

Batch 114440, train_perplexity=905.6206, train_loss=6.8086205

Batch 114450, train_perplexity=1011.1883, train_loss=6.9188814

Batch 114460, train_perplexity=968.6345, train_loss=6.8758874

Batch 114470, train_perplexity=952.0974, train_loss=6.8586674

Batch 114480, train_perplexity=980.1988, train_loss=6.8877554

Batch 114490, train_perplexity=991.736, train_loss=6.899457

Batch 114500, train_perplexity=1088.0155, train_loss=6.9921107

Batch 114510, train_perplexity=943.62384, train_loss=6.8497276
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 114520, train_perplexity=966.05927, train_loss=6.873225

Batch 114530, train_perplexity=1019.332, train_loss=6.926903

Batch 114540, train_perplexity=1058.5708, train_loss=6.964675

Batch 114550, train_perplexity=998.94824, train_loss=6.906703

Batch 114560, train_perplexity=991.82776, train_loss=6.8995495

Batch 114570, train_perplexity=1020.0652, train_loss=6.927622

Batch 114580, train_perplexity=1006.31085, train_loss=6.9140463

Batch 114590, train_perplexity=906.4138, train_loss=6.809496

Batch 114600, train_perplexity=1074.3757, train_loss=6.979495

Batch 114610, train_perplexity=917.73615, train_loss=6.82191

Batch 114620, train_perplexity=1035.623, train_loss=6.9427586

Batch 114630, train_perplexity=1053.5784, train_loss=6.9599476

Batch 114640, train_perplexity=985.317, train_loss=6.8929634

Batch 114650, train_perplexity=1055.779, train_loss=6.962034

Batch 114660, train_perplexity=986.8216, train_loss=6.8944893

Batch 114670, train_perplexity=1007.69666, train_loss=6.9154224

Batch 114680, train_perplexity=987.80365, train_loss=6.895484

Batch 114690, train_perplexity=938.83276, train_loss=6.8446374

Batch 114700, train_perplexity=933.4832, train_loss=6.838923

Batch 114710, train_perplexity=1010.88837, train_loss=6.918585

Batch 114720, train_perplexity=1001.1896, train_loss=6.908944

Batch 114730, train_perplexity=1084.5105, train_loss=6.988884

Batch 114740, train_perplexity=964.92535, train_loss=6.872051

Batch 114750, train_perplexity=879.78906, train_loss=6.779682

Batch 114760, train_perplexity=887.284, train_loss=6.788165

Batch 114770, train_perplexity=981.3998, train_loss=6.88898

Batch 114780, train_perplexity=975.50714, train_loss=6.8829575

Batch 114790, train_perplexity=980.6242, train_loss=6.8881893

Batch 114800, train_perplexity=1024.3185, train_loss=6.9317827

Batch 114810, train_perplexity=984.93365, train_loss=6.8925743

Batch 114820, train_perplexity=973.8721, train_loss=6.88128

Batch 114830, train_perplexity=1046.6063, train_loss=6.953308

Batch 114840, train_perplexity=901.37787, train_loss=6.8039246

Batch 114850, train_perplexity=905.1509, train_loss=6.8081017

Batch 114860, train_perplexity=993.19836, train_loss=6.9009304

Batch 114870, train_perplexity=932.9688, train_loss=6.8383718

Batch 114880, train_perplexity=1024.2856, train_loss=6.931751

Batch 114890, train_perplexity=948.5312, train_loss=6.8549147

Batch 114900, train_perplexity=989.99445, train_loss=6.8976994

Batch 114910, train_perplexity=899.25793, train_loss=6.80157

Batch 114920, train_perplexity=972.9679, train_loss=6.880351

Batch 114930, train_perplexity=915.42145, train_loss=6.8193846

Batch 114940, train_perplexity=981.5505, train_loss=6.8891335

Batch 114950, train_perplexity=1000.7333, train_loss=6.9084883

Batch 114960, train_perplexity=1017.57007, train_loss=6.925173

Batch 114970, train_perplexity=930.6632, train_loss=6.8358974

Batch 114980, train_perplexity=847.6557, train_loss=6.7424746

Batch 114990, train_perplexity=1001.9088, train_loss=6.9096622

Batch 115000, train_perplexity=1034.1516, train_loss=6.9413366

Batch 115010, train_perplexity=1110.644, train_loss=7.0126953

Batch 115020, train_perplexity=967.14984, train_loss=6.8743534

Batch 115030, train_perplexity=857.24896, train_loss=6.7537284

Batch 115040, train_perplexity=1084.8435, train_loss=6.989191

Batch 115050, train_perplexity=927.7973, train_loss=6.8328133

Batch 115060, train_perplexity=1001.18097, train_loss=6.9089355

Batch 115070, train_perplexity=1022.76733, train_loss=6.9302673

Batch 115080, train_perplexity=821.4315, train_loss=6.7110486

Batch 115090, train_perplexity=895.9794, train_loss=6.7979174

Batch 115100, train_perplexity=964.6875, train_loss=6.871804

Batch 115110, train_perplexity=934.6818, train_loss=6.840206

Batch 115120, train_perplexity=994.7136, train_loss=6.902455

Batch 115130, train_perplexity=980.0861, train_loss=6.8876405

Batch 115140, train_perplexity=970.92773, train_loss=6.878252

Batch 115150, train_perplexity=1029.1261, train_loss=6.9364653

Batch 115160, train_perplexity=1040.7975, train_loss=6.9477425

Batch 115170, train_perplexity=860.9411, train_loss=6.758026

Batch 115180, train_perplexity=885.39056, train_loss=6.786029

Batch 115190, train_perplexity=989.691, train_loss=6.8973927

Batch 115200, train_perplexity=987.91296, train_loss=6.8955946

Batch 115210, train_perplexity=961.2272, train_loss=6.868211

Batch 115220, train_perplexity=885.9159, train_loss=6.786622

Batch 115230, train_perplexity=968.7061, train_loss=6.8759613

Batch 115240, train_perplexity=945.86096, train_loss=6.8520956

Batch 115250, train_perplexity=1009.56464, train_loss=6.9172745

Batch 115260, train_perplexity=959.6801, train_loss=6.8666

Batch 115270, train_perplexity=917.99786, train_loss=6.822195

Batch 115280, train_perplexity=918.02765, train_loss=6.8222275

Batch 115290, train_perplexity=875.5529, train_loss=6.7748556

Batch 115300, train_perplexity=1044.5731, train_loss=6.9513636

Batch 115310, train_perplexity=1012.4832, train_loss=6.9201612

Batch 115320, train_perplexity=1046.6971, train_loss=6.953395

Batch 115330, train_perplexity=959.77716, train_loss=6.866701

Batch 115340, train_perplexity=1042.7054, train_loss=6.949574

Batch 115350, train_perplexity=966.1238, train_loss=6.873292

Batch 115360, train_perplexity=959.3855, train_loss=6.866293

Batch 115370, train_perplexity=878.74927, train_loss=6.7784996

Batch 115380, train_perplexity=1073.8738, train_loss=6.9790277

Batch 115390, train_perplexity=967.4708, train_loss=6.8746853

Batch 115400, train_perplexity=1009.3038, train_loss=6.917016

Batch 115410, train_perplexity=1050.4436, train_loss=6.956968

Batch 115420, train_perplexity=961.361, train_loss=6.86835

Batch 115430, train_perplexity=972.70905, train_loss=6.880085

Batch 115440, train_perplexity=996.2511, train_loss=6.9039993

Batch 115450, train_perplexity=936.09344, train_loss=6.8417153

Batch 115460, train_perplexity=982.45233, train_loss=6.890052

Batch 115470, train_perplexity=1008.1835, train_loss=6.9159055

Batch 115480, train_perplexity=859.8904, train_loss=6.756805

Batch 115490, train_perplexity=932.6854, train_loss=6.838068

Batch 115500, train_perplexity=1026.6597, train_loss=6.934066

Batch 115510, train_perplexity=1123.4177, train_loss=7.024131

Batch 115520, train_perplexity=973.7699, train_loss=6.881175

Batch 115530, train_perplexity=991.67316, train_loss=6.8993936

Batch 115540, train_perplexity=1040.7687, train_loss=6.947715

Batch 115550, train_perplexity=981.14667, train_loss=6.888722

Batch 115560, train_perplexity=944.5859, train_loss=6.8507466

Batch 115570, train_perplexity=990.4936, train_loss=6.8982034

Batch 115580, train_perplexity=964.0226, train_loss=6.8711147

Batch 115590, train_perplexity=941.1577, train_loss=6.8471107

Batch 115600, train_perplexity=1012.54504, train_loss=6.9202223

Batch 115610, train_perplexity=974.7297, train_loss=6.88216

Batch 115620, train_perplexity=964.9525, train_loss=6.872079

Batch 115630, train_perplexity=949.9407, train_loss=6.8563995

Batch 115640, train_perplexity=939.4351, train_loss=6.8452787

Batch 115650, train_perplexity=921.9301, train_loss=6.8264694

Batch 115660, train_perplexity=930.7746, train_loss=6.836017

Batch 115670, train_perplexity=986.101, train_loss=6.893759

Batch 115680, train_perplexity=919.6562, train_loss=6.824

Batch 115690, train_perplexity=981.8838, train_loss=6.889473

Batch 115700, train_perplexity=988.32666, train_loss=6.8960133

Batch 115710, train_perplexity=930.5886, train_loss=6.8358173

Batch 115720, train_perplexity=943.1758, train_loss=6.8492527

Batch 115730, train_perplexity=856.99554, train_loss=6.7534328

Batch 115740, train_perplexity=1059.6232, train_loss=6.9656687

Batch 115750, train_perplexity=958.1776, train_loss=6.865033

Batch 115760, train_perplexity=976.2247, train_loss=6.8836927

Batch 115770, train_perplexity=916.0826, train_loss=6.8201065

Batch 115780, train_perplexity=966.59564, train_loss=6.8737803

Batch 115790, train_perplexity=1002.1945, train_loss=6.9099474

Batch 115800, train_perplexity=962.6556, train_loss=6.8696957

Batch 115810, train_perplexity=934.22064, train_loss=6.8397126
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 115820, train_perplexity=1018.8888, train_loss=6.926468

Batch 115830, train_perplexity=929.57306, train_loss=6.8347254

Batch 115840, train_perplexity=995.0371, train_loss=6.90278

Batch 115850, train_perplexity=878.63947, train_loss=6.7783747

Batch 115860, train_perplexity=1054.1501, train_loss=6.96049

Batch 115870, train_perplexity=889.54486, train_loss=6.79071

Batch 115880, train_perplexity=886.89905, train_loss=6.787731

Batch 115890, train_perplexity=918.6451, train_loss=6.8229

Batch 115900, train_perplexity=1030.5148, train_loss=6.9378138

Batch 115910, train_perplexity=891.2538, train_loss=6.7926292

Batch 115920, train_perplexity=988.0788, train_loss=6.8957624

Batch 115930, train_perplexity=951.1214, train_loss=6.8576417

Batch 115940, train_perplexity=902.16907, train_loss=6.804802

Batch 115950, train_perplexity=875.87573, train_loss=6.775224

Batch 115960, train_perplexity=930.6894, train_loss=6.8359256

Batch 115970, train_perplexity=1006.9113, train_loss=6.914643

Batch 115980, train_perplexity=902.1235, train_loss=6.8047514

Batch 115990, train_perplexity=962.94617, train_loss=6.8699975

Batch 116000, train_perplexity=879.24634, train_loss=6.779065

Batch 116010, train_perplexity=880.9384, train_loss=6.7809877

Batch 116020, train_perplexity=929.6626, train_loss=6.8348217

Batch 116030, train_perplexity=977.7154, train_loss=6.8852186

Batch 116040, train_perplexity=1006.5383, train_loss=6.9142723

Batch 116050, train_perplexity=1084.0674, train_loss=6.9884753

Batch 116060, train_perplexity=1001.32227, train_loss=6.9090767

Batch 116070, train_perplexity=998.6578, train_loss=6.906412

Batch 116080, train_perplexity=901.75446, train_loss=6.8043423

Batch 116090, train_perplexity=995.39825, train_loss=6.903143

Batch 116100, train_perplexity=1055.459, train_loss=6.961731

Batch 116110, train_perplexity=950.1763, train_loss=6.8566475

Batch 116120, train_perplexity=976.93524, train_loss=6.8844204

Batch 116130, train_perplexity=975.5457, train_loss=6.882997

Batch 116140, train_perplexity=893.9668, train_loss=6.7956686

Batch 116150, train_perplexity=999.7436, train_loss=6.907499

Batch 116160, train_perplexity=1017.67975, train_loss=6.9252806

Batch 116170, train_perplexity=1000.15796, train_loss=6.907913

Batch 116180, train_perplexity=963.24695, train_loss=6.87031

Batch 116190, train_perplexity=1031.4852, train_loss=6.938755

Batch 116200, train_perplexity=882.0187, train_loss=6.782213

Batch 116210, train_perplexity=878.49036, train_loss=6.778205

Batch 116220, train_perplexity=994.7416, train_loss=6.902483

Batch 116230, train_perplexity=941.8949, train_loss=6.8478937

Batch 116240, train_perplexity=990.6452, train_loss=6.8983564

Batch 116250, train_perplexity=907.58154, train_loss=6.8107834

Batch 116260, train_perplexity=974.2348, train_loss=6.8816524

Batch 116270, train_perplexity=947.80914, train_loss=6.854153

Batch 116280, train_perplexity=939.96204, train_loss=6.8458395

Batch 116290, train_perplexity=977.0704, train_loss=6.8845587

Batch 116300, train_perplexity=941.68744, train_loss=6.8476734

Batch 116310, train_perplexity=962.6124, train_loss=6.869651

Batch 116320, train_perplexity=977.6837, train_loss=6.885186

Batch 116330, train_perplexity=905.0538, train_loss=6.8079944

Batch 116340, train_perplexity=943.1452, train_loss=6.8492203

Batch 116350, train_perplexity=974.61444, train_loss=6.882042

Batch 116360, train_perplexity=936.5645, train_loss=6.8422184

Batch 116370, train_perplexity=896.41394, train_loss=6.7984023

Batch 116380, train_perplexity=896.3853, train_loss=6.7983704

Batch 116390, train_perplexity=983.8231, train_loss=6.891446

Batch 116400, train_perplexity=1024.0518, train_loss=6.9315224

Batch 116410, train_perplexity=930.44934, train_loss=6.8356676

Batch 116420, train_perplexity=979.952, train_loss=6.8875036

Batch 116430, train_perplexity=910.6377, train_loss=6.814145

Batch 116440, train_perplexity=985.71265, train_loss=6.893365

Batch 116450, train_perplexity=977.0214, train_loss=6.8845086

Batch 116460, train_perplexity=1089.6328, train_loss=6.993596

Batch 116470, train_perplexity=957.8807, train_loss=6.864723

Batch 116480, train_perplexity=943.4412, train_loss=6.849534

Batch 116490, train_perplexity=971.5302, train_loss=6.8788724

Batch 116500, train_perplexity=1047.8223, train_loss=6.954469

Batch 116510, train_perplexity=903.3916, train_loss=6.806156

Batch 116520, train_perplexity=999.88806, train_loss=6.9076433

Batch 116530, train_perplexity=989.71643, train_loss=6.8974185

Batch 116540, train_perplexity=992.04346, train_loss=6.899767

Batch 116550, train_perplexity=1044.862, train_loss=6.95164

Batch 116560, train_perplexity=964.8582, train_loss=6.871981

Batch 116570, train_perplexity=965.16693, train_loss=6.872301

Batch 116580, train_perplexity=1013.67065, train_loss=6.9213333

Batch 116590, train_perplexity=1003.0666, train_loss=6.910817

Batch 116600, train_perplexity=1063.49, train_loss=6.969311

Batch 116610, train_perplexity=953.4045, train_loss=6.860039

Batch 116620, train_perplexity=957.3857, train_loss=6.8642063

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Batch 116630, train_perplexity=940.5696, train_loss=6.8464856

Batch 116640, train_perplexity=975.1518, train_loss=6.882593

Batch 116650, train_perplexity=909.83685, train_loss=6.8132653

Batch 116660, train_perplexity=885.2234, train_loss=6.78584

Batch 116670, train_perplexity=1020.4495, train_loss=6.9279985

Batch 116680, train_perplexity=972.45764, train_loss=6.8798265

Batch 116690, train_perplexity=941.7997, train_loss=6.8477926

Batch 116700, train_perplexity=927.77515, train_loss=6.8327894

Batch 116710, train_perplexity=941.1842, train_loss=6.847139

Batch 116720, train_perplexity=910.8613, train_loss=6.8143907

Batch 116730, train_perplexity=1044.2604, train_loss=6.951064

Batch 116740, train_perplexity=1012.1043, train_loss=6.919787

Batch 116750, train_perplexity=973.25323, train_loss=6.8806443

Batch 116760, train_perplexity=995.5302, train_loss=6.9032755

Batch 116770, train_perplexity=874.1192, train_loss=6.7732167

Batch 116780, train_perplexity=1020.6179, train_loss=6.9281635

Batch 116790, train_perplexity=1008.95636, train_loss=6.9166718

Batch 116800, train_perplexity=907.2587, train_loss=6.8104277

Batch 116810, train_perplexity=991.0288, train_loss=6.8987436

Batch 116820, train_perplexity=947.5791, train_loss=6.8539104

Batch 116830, train_perplexity=1003.9236, train_loss=6.911671

Batch 116840, train_perplexity=1010.9395, train_loss=6.9186354

Batch 116850, train_perplexity=981.8351, train_loss=6.8894234

Batch 116860, train_perplexity=1020.0166, train_loss=6.927574

Batch 116870, train_perplexity=958.26715, train_loss=6.8651266

Batch 116880, train_perplexity=1001.669, train_loss=6.909423

Batch 116890, train_perplexity=998.41205, train_loss=6.906166

Batch 116900, train_perplexity=946.24713, train_loss=6.852504

Batch 116910, train_perplexity=903.0652, train_loss=6.8057947

Batch 116920, train_perplexity=910.2604, train_loss=6.8137307

Batch 116930, train_perplexity=1020.40967, train_loss=6.9279594

Batch 116940, train_perplexity=958.68304, train_loss=6.8655605

Batch 116950, train_perplexity=954.7083, train_loss=6.861406

Batch 116960, train_perplexity=882.075, train_loss=6.782277

Batch 116970, train_perplexity=861.3234, train_loss=6.75847

Batch 116980, train_perplexity=992.5109, train_loss=6.900238

Batch 116990, train_perplexity=910.84, train_loss=6.8143673

Batch 117000, train_perplexity=982.9678, train_loss=6.8905764

Batch 117010, train_perplexity=1079.2758, train_loss=6.9840455

Batch 117020, train_perplexity=933.5504, train_loss=6.838995

Batch 117030, train_perplexity=1032.9279, train_loss=6.9401526

Batch 117040, train_perplexity=976.3937, train_loss=6.883866

Batch 117050, train_perplexity=903.5657, train_loss=6.806349
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 117060, train_perplexity=964.7846, train_loss=6.871905

Batch 117070, train_perplexity=1082.1538, train_loss=6.9867086

Batch 117080, train_perplexity=938.36597, train_loss=6.84414

Batch 117090, train_perplexity=1018.38367, train_loss=6.925972

Batch 117100, train_perplexity=960.719, train_loss=6.867682

Batch 117110, train_perplexity=1024.6799, train_loss=6.9321356

Batch 117120, train_perplexity=931.6658, train_loss=6.836974

Batch 117130, train_perplexity=955.4146, train_loss=6.8621454

Batch 117140, train_perplexity=1015.43494, train_loss=6.9230723

Batch 117150, train_perplexity=932.3004, train_loss=6.837655

Batch 117160, train_perplexity=1075.6521, train_loss=6.9806824

Batch 117170, train_perplexity=904.25446, train_loss=6.807111

Batch 117180, train_perplexity=1020.38873, train_loss=6.927939

Batch 117190, train_perplexity=927.81055, train_loss=6.8328276

Batch 117200, train_perplexity=968.58234, train_loss=6.8758335

Batch 117210, train_perplexity=981.3319, train_loss=6.888911

Batch 117220, train_perplexity=902.9265, train_loss=6.805641

Batch 117230, train_perplexity=957.4555, train_loss=6.8642793

Batch 117240, train_perplexity=1076.4269, train_loss=6.9814024

Batch 117250, train_perplexity=939.0777, train_loss=6.844898

Batch 117260, train_perplexity=936.5458, train_loss=6.8421984

Batch 117270, train_perplexity=1088.5085, train_loss=6.9925637

Batch 117280, train_perplexity=979.60864, train_loss=6.887153

Batch 117290, train_perplexity=889.87115, train_loss=6.7910767

Batch 117300, train_perplexity=1049.4108, train_loss=6.955984

Batch 117310, train_perplexity=1006.713, train_loss=6.914446

Batch 117320, train_perplexity=887.4693, train_loss=6.788374

Batch 117330, train_perplexity=952.84955, train_loss=6.859457

Batch 117340, train_perplexity=943.0616, train_loss=6.8491316

Batch 117350, train_perplexity=1001.80176, train_loss=6.9095554

Batch 117360, train_perplexity=942.08984, train_loss=6.8481007

Batch 117370, train_perplexity=982.80237, train_loss=6.890408

Batch 117380, train_perplexity=1020.31525, train_loss=6.927867

Batch 117390, train_perplexity=963.7441, train_loss=6.870826

Batch 117400, train_perplexity=1003.1039, train_loss=6.9108543

Batch 117410, train_perplexity=911.5326, train_loss=6.8151274

Batch 117420, train_perplexity=1010.552, train_loss=6.918252

Batch 117430, train_perplexity=1011.4395, train_loss=6.91913

Batch 117440, train_perplexity=1002.6008, train_loss=6.9103527

Batch 117450, train_perplexity=960.14703, train_loss=6.8670864

Batch 117460, train_perplexity=990.3099, train_loss=6.898018

Batch 117470, train_perplexity=996.8308, train_loss=6.904581

Batch 117480, train_perplexity=989.50507, train_loss=6.897205

Batch 117490, train_perplexity=969.1927, train_loss=6.8764634

Batch 117500, train_perplexity=999.0007, train_loss=6.9067554

Batch 117510, train_perplexity=935.3889, train_loss=6.8409624

Batch 117520, train_perplexity=891.6534, train_loss=6.7930775

Batch 117530, train_perplexity=1049.9172, train_loss=6.9564667

Batch 117540, train_perplexity=1036.8401, train_loss=6.943933

Batch 117550, train_perplexity=958.71, train_loss=6.8655887

Batch 117560, train_perplexity=943.9533, train_loss=6.8500767

Batch 117570, train_perplexity=1095.3987, train_loss=6.9988737

Batch 117580, train_perplexity=912.06476, train_loss=6.815711

Batch 117590, train_perplexity=965.40173, train_loss=6.8725443

Batch 117600, train_perplexity=947.5466, train_loss=6.853876

Batch 117610, train_perplexity=869.2341, train_loss=6.7676125

Batch 117620, train_perplexity=846.66846, train_loss=6.741309

Batch 117630, train_perplexity=834.8794, train_loss=6.7272873

Batch 117640, train_perplexity=1066.0616, train_loss=6.9717264

Batch 117650, train_perplexity=899.50757, train_loss=6.8018475

Batch 117660, train_perplexity=1018.1821, train_loss=6.925774

Batch 117670, train_perplexity=965.8723, train_loss=6.8730316

Batch 117680, train_perplexity=945.0964, train_loss=6.851287

Batch 117690, train_perplexity=954.29004, train_loss=6.8609676

Batch 117700, train_perplexity=976.87006, train_loss=6.8843536

Batch 117710, train_perplexity=960.76434, train_loss=6.867729

Batch 117720, train_perplexity=977.9322, train_loss=6.8854403

Batch 117730, train_perplexity=927.60175, train_loss=6.8326025

Batch 117740, train_perplexity=970.8879, train_loss=6.878211

Batch 117750, train_perplexity=957.7838, train_loss=6.864622

Batch 117760, train_perplexity=998.4968, train_loss=6.906251

Batch 117770, train_perplexity=1011.1396, train_loss=6.9188333

Batch 117780, train_perplexity=918.8711, train_loss=6.823146

Batch 117790, train_perplexity=987.9431, train_loss=6.895625

Batch 117800, train_perplexity=901.9764, train_loss=6.8045883

Batch 117810, train_perplexity=1016.4654, train_loss=6.9240866

Batch 117820, train_perplexity=1066.8672, train_loss=6.9724817

Batch 117830, train_perplexity=921.9547, train_loss=6.826496

Batch 117840, train_perplexity=1087.9564, train_loss=6.9920564

Batch 117850, train_perplexity=1027.7295, train_loss=6.935107

Batch 117860, train_perplexity=973.74854, train_loss=6.881153

Batch 117870, train_perplexity=955.77185, train_loss=6.8625193

Batch 117880, train_perplexity=1001.68524, train_loss=6.909439

Batch 117890, train_perplexity=971.10736, train_loss=6.878437

Batch 117900, train_perplexity=930.4005, train_loss=6.835615

Batch 117910, train_perplexity=927.7645, train_loss=6.832778

Batch 117920, train_perplexity=987.897, train_loss=6.8955784

Batch 117930, train_perplexity=938.64795, train_loss=6.8444405

Batch 117940, train_perplexity=940.9244, train_loss=6.846863

Batch 117950, train_perplexity=935.43665, train_loss=6.8410134

Batch 117960, train_perplexity=949.86456, train_loss=6.8563194

Batch 117970, train_perplexity=1013.4947, train_loss=6.9211597

Batch 117980, train_perplexity=912.50586, train_loss=6.8161945

Batch 117990, train_perplexity=952.111, train_loss=6.8586817

Batch 118000, train_perplexity=884.4036, train_loss=6.7849135

Batch 118010, train_perplexity=962.31683, train_loss=6.8693438

Batch 118020, train_perplexity=911.2753, train_loss=6.814845

Batch 118030, train_perplexity=1005.90497, train_loss=6.913643

Batch 118040, train_perplexity=889.89575, train_loss=6.7911043

Batch 118050, train_perplexity=938.0627, train_loss=6.8438168

Batch 118060, train_perplexity=1016.06946, train_loss=6.923697

Batch 118070, train_perplexity=953.4163, train_loss=6.8600516

Batch 118080, train_perplexity=896.3665, train_loss=6.7983494

Batch 118090, train_perplexity=960.3522, train_loss=6.8673

Batch 118100, train_perplexity=886.24927, train_loss=6.7869983

Batch 118110, train_perplexity=934.37787, train_loss=6.839881

Batch 118120, train_perplexity=936.74896, train_loss=6.8424153

Batch 118130, train_perplexity=936.1158, train_loss=6.841739

Batch 118140, train_perplexity=991.2727, train_loss=6.8989897

Batch 118150, train_perplexity=892.50964, train_loss=6.7940373

Batch 118160, train_perplexity=969.8274, train_loss=6.877118

Batch 118170, train_perplexity=1015.0027, train_loss=6.9226465

Batch 118180, train_perplexity=968.76575, train_loss=6.876023

Batch 118190, train_perplexity=900.35205, train_loss=6.802786

Batch 118200, train_perplexity=1008.7918, train_loss=6.9165087

Batch 118210, train_perplexity=1111.604, train_loss=7.0135593

Batch 118220, train_perplexity=971.8708, train_loss=6.879223

Batch 118230, train_perplexity=1013.55804, train_loss=6.921222

Batch 118240, train_perplexity=1031.4882, train_loss=6.938758

Batch 118250, train_perplexity=1028.4904, train_loss=6.9358473

Batch 118260, train_perplexity=995.9699, train_loss=6.903717

Batch 118270, train_perplexity=942.33606, train_loss=6.848362

Batch 118280, train_perplexity=935.3925, train_loss=6.840966

Batch 118290, train_perplexity=906.5521, train_loss=6.8096485

Batch 118300, train_perplexity=980.39514, train_loss=6.8879557

Batch 118310, train_perplexity=1064.088, train_loss=6.9698734

Batch 118320, train_perplexity=1065.178, train_loss=6.970897

Batch 118330, train_perplexity=1036.1437, train_loss=6.943261

Batch 118340, train_perplexity=875.00287, train_loss=6.774227

Batch 118350, train_perplexity=878.57495, train_loss=6.7783012
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 118360, train_perplexity=887.5984, train_loss=6.7885194

Batch 118370, train_perplexity=1001.2679, train_loss=6.9090223

Batch 118380, train_perplexity=946.72375, train_loss=6.8530073

Batch 118390, train_perplexity=969.86255, train_loss=6.8771544

Batch 118400, train_perplexity=917.3861, train_loss=6.8215284

Batch 118410, train_perplexity=967.0115, train_loss=6.8742104

Batch 118420, train_perplexity=971.2926, train_loss=6.878628

Batch 118430, train_perplexity=1009.0944, train_loss=6.9168086

Batch 118440, train_perplexity=929.5083, train_loss=6.834656

Batch 118450, train_perplexity=1002.61755, train_loss=6.9103694

Batch 118460, train_perplexity=879.1088, train_loss=6.7789087

Batch 118470, train_perplexity=956.08093, train_loss=6.8628426

Batch 118480, train_perplexity=1011.1844, train_loss=6.9188776

Batch 118490, train_perplexity=1056.1477, train_loss=6.9623833

Batch 118500, train_perplexity=944.3206, train_loss=6.850466

Batch 118510, train_perplexity=1010.02783, train_loss=6.917733

Batch 118520, train_perplexity=871.8002, train_loss=6.7705603

Batch 118530, train_perplexity=1042.1616, train_loss=6.9490523

Batch 118540, train_perplexity=1004.08966, train_loss=6.9118366

Batch 118550, train_perplexity=909.6538, train_loss=6.813064

Batch 118560, train_perplexity=985.6873, train_loss=6.893339

Batch 118570, train_perplexity=940.4153, train_loss=6.8463216

Batch 118580, train_perplexity=892.06757, train_loss=6.793542

Batch 118590, train_perplexity=1039.4346, train_loss=6.946432

Batch 118600, train_perplexity=980.3245, train_loss=6.8878837

Batch 118610, train_perplexity=841.52295, train_loss=6.7352133

Batch 118620, train_perplexity=870.41077, train_loss=6.7689652

Batch 118630, train_perplexity=897.167, train_loss=6.799242

Batch 118640, train_perplexity=901.0032, train_loss=6.8035088

Batch 118650, train_perplexity=980.95953, train_loss=6.888531

Batch 118660, train_perplexity=964.91754, train_loss=6.8720427

Batch 118670, train_perplexity=891.9251, train_loss=6.793382

Batch 118680, train_perplexity=920.2646, train_loss=6.8246613

Batch 118690, train_perplexity=935.3158, train_loss=6.840884

Batch 118700, train_perplexity=918.6757, train_loss=6.822933

Batch 118710, train_perplexity=946.3888, train_loss=6.8526535

Batch 118720, train_perplexity=945.136, train_loss=6.851329

Batch 118730, train_perplexity=969.23425, train_loss=6.8765063

Batch 118740, train_perplexity=979.724, train_loss=6.887271

Batch 118750, train_perplexity=887.9646, train_loss=6.788932

Batch 118760, train_perplexity=1004.22327, train_loss=6.9119697

Batch 118770, train_perplexity=943.50415, train_loss=6.849601

Batch 118780, train_perplexity=1024.1299, train_loss=6.9315987

Batch 118790, train_perplexity=915.2975, train_loss=6.819249

Batch 118800, train_perplexity=981.8126, train_loss=6.8894005

Batch 118810, train_perplexity=860.45764, train_loss=6.7574644

Batch 118820, train_perplexity=905.5407, train_loss=6.808532

Batch 118830, train_perplexity=965.085, train_loss=6.872216

Batch 118840, train_perplexity=963.86676, train_loss=6.870953

Batch 118850, train_perplexity=965.81244, train_loss=6.8729696

Batch 118860, train_perplexity=1093.2489, train_loss=6.996909

Batch 118870, train_perplexity=911.0386, train_loss=6.814585

Batch 118880, train_perplexity=974.22504, train_loss=6.8816423

Batch 118890, train_perplexity=899.56244, train_loss=6.8019085

Batch 118900, train_perplexity=840.14813, train_loss=6.733578

Batch 118910, train_perplexity=913.9542, train_loss=6.8177805

Batch 118920, train_perplexity=1128.4419, train_loss=7.028593

Batch 118930, train_perplexity=1024.0947, train_loss=6.9315643

Batch 118940, train_perplexity=859.369, train_loss=6.7561984

Batch 118950, train_perplexity=981.8875, train_loss=6.889477

Batch 118960, train_perplexity=899.80096, train_loss=6.8021736

Batch 118970, train_perplexity=933.35724, train_loss=6.838788

Batch 118980, train_perplexity=880.5, train_loss=6.78049

Batch 118990, train_perplexity=927.5053, train_loss=6.8324986

Batch 119000, train_perplexity=888.01917, train_loss=6.7889934

Batch 119010, train_perplexity=1027.752, train_loss=6.935129

Batch 119020, train_perplexity=999.83606, train_loss=6.9075913

Batch 119030, train_perplexity=922.64343, train_loss=6.827243

Batch 119040, train_perplexity=1015.4456, train_loss=6.923083

Batch 119050, train_perplexity=935.5058, train_loss=6.8410873

Batch 119060, train_perplexity=1022.68835, train_loss=6.93019

Batch 119070, train_perplexity=900.10913, train_loss=6.802516

Batch 119080, train_perplexity=974.65906, train_loss=6.8820877

Batch 119090, train_perplexity=921.9653, train_loss=6.8265076

Batch 119100, train_perplexity=983.29034, train_loss=6.8909044

Batch 119110, train_perplexity=936.67615, train_loss=6.8423376

Batch 119120, train_perplexity=968.32697, train_loss=6.87557

Batch 119130, train_perplexity=1020.1197, train_loss=6.9276752

Batch 119140, train_perplexity=975.5476, train_loss=6.882999

Batch 119150, train_perplexity=1040.6193, train_loss=6.9475713

Batch 119160, train_perplexity=897.2855, train_loss=6.799374

Batch 119170, train_perplexity=991.54974, train_loss=6.899269

Batch 119180, train_perplexity=937.0693, train_loss=6.842757

Batch 119190, train_perplexity=864.40393, train_loss=6.76204

Batch 119200, train_perplexity=922.0242, train_loss=6.8265715

Batch 119210, train_perplexity=937.4688, train_loss=6.8431835

Batch 119220, train_perplexity=991.0033, train_loss=6.898718

Batch 119230, train_perplexity=950.1237, train_loss=6.856592

Batch 119240, train_perplexity=942.11053, train_loss=6.8481226

Batch 119250, train_perplexity=972.10626, train_loss=6.879465

Batch 119260, train_perplexity=925.1493, train_loss=6.829955

Batch 119270, train_perplexity=871.95154, train_loss=6.770734

Batch 119280, train_perplexity=948.36206, train_loss=6.8547363

Batch 119290, train_perplexity=964.1302, train_loss=6.8712263

Batch 119300, train_perplexity=950.0847, train_loss=6.856551

Batch 119310, train_perplexity=965.2857, train_loss=6.872424

Batch 119320, train_perplexity=936.9254, train_loss=6.8426037

Batch 119330, train_perplexity=927.02515, train_loss=6.8319807

Batch 119340, train_perplexity=927.8973, train_loss=6.832921

Batch 119350, train_perplexity=892.39136, train_loss=6.793905

Batch 119360, train_perplexity=964.6774, train_loss=6.8717937

Batch 119370, train_perplexity=974.9082, train_loss=6.8823433

Batch 119380, train_perplexity=904.7927, train_loss=6.807706

Batch 119390, train_perplexity=905.00244, train_loss=6.8079376

Batch 119400, train_perplexity=959.7319, train_loss=6.866654

Batch 119410, train_perplexity=917.3218, train_loss=6.8214583

Batch 119420, train_perplexity=949.26056, train_loss=6.8556833

Batch 119430, train_perplexity=884.03723, train_loss=6.784499

Batch 119440, train_perplexity=857.8141, train_loss=6.7543874

Batch 119450, train_perplexity=1038.122, train_loss=6.9451685

Batch 119460, train_perplexity=977.06573, train_loss=6.884554

Batch 119470, train_perplexity=972.1758, train_loss=6.8795366

Batch 119480, train_perplexity=917.1254, train_loss=6.8212442

Batch 119490, train_perplexity=1124.0264, train_loss=7.0246725

Batch 119500, train_perplexity=978.6156, train_loss=6.886139

Batch 119510, train_perplexity=943.74536, train_loss=6.8498564

Batch 119520, train_perplexity=887.3305, train_loss=6.7882175

Batch 119530, train_perplexity=891.33325, train_loss=6.7927184

Batch 119540, train_perplexity=827.33624, train_loss=6.718211

Batch 119550, train_perplexity=1022.01465, train_loss=6.929531

Batch 119560, train_perplexity=933.31274, train_loss=6.8387403

Batch 119570, train_perplexity=947.9506, train_loss=6.8543024

Batch 119580, train_perplexity=1029.3322, train_loss=6.9366655

Batch 119590, train_perplexity=972.3306, train_loss=6.879696

Batch 119600, train_perplexity=1014.1938, train_loss=6.9218493

Batch 119610, train_perplexity=927.784, train_loss=6.832799

Batch 119620, train_perplexity=967.4584, train_loss=6.8746724

Batch 119630, train_perplexity=929.65015, train_loss=6.8348083

Batch 119640, train_perplexity=939.06604, train_loss=6.844886

Batch 119650, train_perplexity=947.53845, train_loss=6.8538675

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 119660, train_perplexity=1033.0629, train_loss=6.9402833

Batch 119670, train_perplexity=847.6808, train_loss=6.742504

Batch 119680, train_perplexity=916.27655, train_loss=6.820318

Batch 119690, train_perplexity=914.91046, train_loss=6.818826

Batch 119700, train_perplexity=915.99524, train_loss=6.820011

Batch 119710, train_perplexity=996.18933, train_loss=6.9039373

Batch 119720, train_perplexity=923.61536, train_loss=6.8282957

Batch 119730, train_perplexity=996.5181, train_loss=6.9042673

Batch 119740, train_perplexity=904.898, train_loss=6.807822

Batch 119750, train_perplexity=991.3786, train_loss=6.8990965

Batch 119760, train_perplexity=864.9638, train_loss=6.7626877

Batch 119770, train_perplexity=911.52264, train_loss=6.8151164

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100
Loaded 304503 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100
Loaded 304503 sentences.
Finished loading
Batch 119780, train_perplexity=988.09296, train_loss=6.8957767

Batch 119790, train_perplexity=887.60645, train_loss=6.7885284

Batch 119800, train_perplexity=911.444, train_loss=6.81503

Batch 119810, train_perplexity=988.04016, train_loss=6.8957233

Batch 119820, train_perplexity=890.3134, train_loss=6.7915735

Batch 119830, train_perplexity=934.71387, train_loss=6.8402405

Batch 119840, train_perplexity=1022.9176, train_loss=6.930414

Batch 119850, train_perplexity=1031.8286, train_loss=6.939088

Batch 119860, train_perplexity=910.09375, train_loss=6.8135476

Batch 119870, train_perplexity=1010.8166, train_loss=6.918514

Batch 119880, train_perplexity=956.23505, train_loss=6.8630037

Batch 119890, train_perplexity=802.7311, train_loss=6.6880198

Batch 119900, train_perplexity=836.3888, train_loss=6.7290936

Batch 119910, train_perplexity=1008.69995, train_loss=6.9164176

Batch 119920, train_perplexity=827.5359, train_loss=6.7184525

Batch 119930, train_perplexity=1086.8027, train_loss=6.9909954

Batch 119940, train_perplexity=950.707, train_loss=6.857206

Batch 119950, train_perplexity=905.6219, train_loss=6.808622

Batch 119960, train_perplexity=890.18774, train_loss=6.7914324

Batch 119970, train_perplexity=899.4913, train_loss=6.8018293

Batch 119980, train_perplexity=877.6077, train_loss=6.7771997

Batch 119990, train_perplexity=954.06256, train_loss=6.860729

Batch 120000, train_perplexity=954.62134, train_loss=6.861315

Batch 120010, train_perplexity=1009.74854, train_loss=6.9174566

Batch 120020, train_perplexity=959.49164, train_loss=6.8664036

Batch 120030, train_perplexity=992.7935, train_loss=6.9005227

Batch 120040, train_perplexity=936.36176, train_loss=6.842002

Batch 120050, train_perplexity=938.2076, train_loss=6.8439713

Batch 120060, train_perplexity=1000.812, train_loss=6.908567

Batch 120070, train_perplexity=963.2695, train_loss=6.870333

Batch 120080, train_perplexity=982.92, train_loss=6.8905277

Batch 120090, train_perplexity=911.1597, train_loss=6.8147182

Batch 120100, train_perplexity=908.0508, train_loss=6.8113003

Batch 120110, train_perplexity=964.8467, train_loss=6.871969

Batch 120120, train_perplexity=1016.6185, train_loss=6.9242373

Batch 120130, train_perplexity=922.7948, train_loss=6.827407

Batch 120140, train_perplexity=944.61475, train_loss=6.850777

Batch 120150, train_perplexity=966.22235, train_loss=6.873394

Batch 120160, train_perplexity=996.97675, train_loss=6.9047275

Batch 120170, train_perplexity=895.1296, train_loss=6.7969685

Batch 120180, train_perplexity=976.3075, train_loss=6.8837776

Batch 120190, train_perplexity=1041.9286, train_loss=6.9488287

Batch 120200, train_perplexity=968.80316, train_loss=6.8760614

Batch 120210, train_perplexity=919.255, train_loss=6.8235636

Batch 120220, train_perplexity=1007.3373, train_loss=6.915066

Batch 120230, train_perplexity=842.73645, train_loss=6.7366543

Batch 120240, train_perplexity=985.00507, train_loss=6.892647

Batch 120250, train_perplexity=974.45734, train_loss=6.8818808

Batch 120260, train_perplexity=974.4806, train_loss=6.8819046

Batch 120270, train_perplexity=866.6102, train_loss=6.7645893

Batch 120280, train_perplexity=934.24554, train_loss=6.8397393

Batch 120290, train_perplexity=956.4284, train_loss=6.863206

Batch 120300, train_perplexity=940.0158, train_loss=6.8458967

Batch 120310, train_perplexity=913.30817, train_loss=6.8170733

Batch 120320, train_perplexity=917.062, train_loss=6.821175

Batch 120330, train_perplexity=953.6677, train_loss=6.8603153

Batch 120340, train_perplexity=973.4904, train_loss=6.880888

Batch 120350, train_perplexity=987.1172, train_loss=6.8947887

Batch 120360, train_perplexity=923.96735, train_loss=6.8286767

Batch 120370, train_perplexity=923.5669, train_loss=6.8282433

Batch 120380, train_perplexity=1007.41125, train_loss=6.915139

Batch 120390, train_perplexity=983.7002, train_loss=6.891321

Batch 120400, train_perplexity=967.63324, train_loss=6.874853

Batch 120410, train_perplexity=915.6502, train_loss=6.8196344

Batch 120420, train_perplexity=1021.24445, train_loss=6.928777

Batch 120430, train_perplexity=1102.9125, train_loss=7.0057096

Batch 120440, train_perplexity=928.35486, train_loss=6.833414

Batch 120450, train_perplexity=827.25336, train_loss=6.718111

Batch 120460, train_perplexity=942.8885, train_loss=6.848948

Batch 120470, train_perplexity=860.43634, train_loss=6.7574396

Batch 120480, train_perplexity=982.6941, train_loss=6.890298

Batch 120490, train_perplexity=870.8836, train_loss=6.7695084

Batch 120500, train_perplexity=880.1226, train_loss=6.7800612

Batch 120510, train_perplexity=981.50885, train_loss=6.889091

Batch 120520, train_perplexity=931.0369, train_loss=6.836299

Batch 120530, train_perplexity=939.9961, train_loss=6.8458757

Batch 120540, train_perplexity=931.0609, train_loss=6.8363247

Batch 120550, train_perplexity=905.3633, train_loss=6.8083363

Batch 120560, train_perplexity=959.8751, train_loss=6.866803

Batch 120570, train_perplexity=967.9295, train_loss=6.8751593

Batch 120580, train_perplexity=959.9584, train_loss=6.86689

Batch 120590, train_perplexity=1018.77026, train_loss=6.9263515

Batch 120600, train_perplexity=989.54846, train_loss=6.8972487

Batch 120610, train_perplexity=1017.5133, train_loss=6.925117

Batch 120620, train_perplexity=1019.36066, train_loss=6.926931

Batch 120630, train_perplexity=1022.95123, train_loss=6.930447

Batch 120640, train_perplexity=936.168, train_loss=6.841795

Batch 120650, train_perplexity=955.9387, train_loss=6.862694

Batch 120660, train_perplexity=944.635, train_loss=6.8507986

Batch 120670, train_perplexity=927.10254, train_loss=6.832064

Batch 120680, train_perplexity=1020.54395, train_loss=6.928091

Batch 120690, train_perplexity=817.13153, train_loss=6.7058

Batch 120700, train_perplexity=987.42267, train_loss=6.895098

Batch 120710, train_perplexity=981.0264, train_loss=6.8885994

Batch 120720, train_perplexity=859.54236, train_loss=6.7564

Batch 120730, train_perplexity=941.2412, train_loss=6.8471994

Batch 120740, train_perplexity=863.81964, train_loss=6.761364

Batch 120750, train_perplexity=853.663, train_loss=6.7495365

Batch 120760, train_perplexity=894.8398, train_loss=6.7966447

Batch 120770, train_perplexity=1027.9647, train_loss=6.935336

Batch 120780, train_perplexity=942.07007, train_loss=6.8480797

Batch 120790, train_perplexity=990.6759, train_loss=6.8983874

Batch 120800, train_perplexity=997.5165, train_loss=6.9052687

Batch 120810, train_perplexity=888.6152, train_loss=6.7896643

Batch 120820, train_perplexity=833.9472, train_loss=6.72617

Batch 120830, train_perplexity=995.2136, train_loss=6.9029574

Batch 120840, train_perplexity=944.9323, train_loss=6.8511133

Batch 120850, train_perplexity=913.02905, train_loss=6.8167677

Batch 120860, train_perplexity=1000.07306, train_loss=6.9078283

Batch 120870, train_perplexity=970.178, train_loss=6.8774796

Batch 120880, train_perplexity=979.7857, train_loss=6.887334

Batch 120890, train_perplexity=1010.5308, train_loss=6.918231
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 120900, train_perplexity=877.9443, train_loss=6.777583

Batch 120910, train_perplexity=968.8041, train_loss=6.8760624

Batch 120920, train_perplexity=1023.8721, train_loss=6.931347

Batch 120930, train_perplexity=879.7907, train_loss=6.779684

Batch 120940, train_perplexity=862.90735, train_loss=6.7603073

Batch 120950, train_perplexity=1021.355, train_loss=6.9288855

Batch 120960, train_perplexity=951.01166, train_loss=6.8575263

Batch 120970, train_perplexity=939.1547, train_loss=6.8449802

Batch 120980, train_perplexity=921.5363, train_loss=6.826042

Batch 120990, train_perplexity=845.563, train_loss=6.7400026

Batch 121000, train_perplexity=951.14545, train_loss=6.857667

Batch 121010, train_perplexity=960.15894, train_loss=6.867099

Batch 121020, train_perplexity=951.8301, train_loss=6.8583865

Batch 121030, train_perplexity=919.72766, train_loss=6.8240776

Batch 121040, train_perplexity=868.0561, train_loss=6.7662563

Batch 121050, train_perplexity=968.4258, train_loss=6.875672

Batch 121060, train_perplexity=962.6436, train_loss=6.8696833

Batch 121070, train_perplexity=957.08307, train_loss=6.86389

Batch 121080, train_perplexity=996.54755, train_loss=6.904297

Batch 121090, train_perplexity=1061.7252, train_loss=6.9676504

Batch 121100, train_perplexity=967.1037, train_loss=6.8743057

Batch 121110, train_perplexity=986.30084, train_loss=6.8939614

Batch 121120, train_perplexity=955.04156, train_loss=6.861755

Batch 121130, train_perplexity=943.22705, train_loss=6.849307

Batch 121140, train_perplexity=890.50275, train_loss=6.791786

Batch 121150, train_perplexity=916.19354, train_loss=6.8202276

Batch 121160, train_perplexity=925.00726, train_loss=6.8298016

Batch 121170, train_perplexity=1010.65515, train_loss=6.918354

Batch 121180, train_perplexity=859.9982, train_loss=6.7569304

Batch 121190, train_perplexity=945.57416, train_loss=6.8517923

Batch 121200, train_perplexity=973.98724, train_loss=6.881398

Batch 121210, train_perplexity=994.302, train_loss=6.902041

Batch 121220, train_perplexity=1043.6849, train_loss=6.950513

Batch 121230, train_perplexity=906.04865, train_loss=6.809093

Batch 121240, train_perplexity=967.1862, train_loss=6.874391

Batch 121250, train_perplexity=910.98425, train_loss=6.8145256

Batch 121260, train_perplexity=889.6055, train_loss=6.790778

Batch 121270, train_perplexity=931.84485, train_loss=6.8371663

Batch 121280, train_perplexity=969.86163, train_loss=6.8771534

Batch 121290, train_perplexity=925.6391, train_loss=6.8304844

Batch 121300, train_perplexity=959.77625, train_loss=6.8667

Batch 121310, train_perplexity=1051.0583, train_loss=6.957553

Batch 121320, train_perplexity=901.8181, train_loss=6.804413

Batch 121330, train_perplexity=993.38214, train_loss=6.9011154

Batch 121340, train_perplexity=910.54736, train_loss=6.814046

Batch 121350, train_perplexity=1018.6488, train_loss=6.9262323

Batch 121360, train_perplexity=973.25745, train_loss=6.8806486

Batch 121370, train_perplexity=968.7135, train_loss=6.875969

Batch 121380, train_perplexity=917.6171, train_loss=6.82178

Batch 121390, train_perplexity=951.04926, train_loss=6.857566

Batch 121400, train_perplexity=902.5959, train_loss=6.805275

Batch 121410, train_perplexity=981.42883, train_loss=6.8890095

Batch 121420, train_perplexity=913.30945, train_loss=6.817075

Batch 121430, train_perplexity=865.3549, train_loss=6.7631397

Batch 121440, train_perplexity=944.3071, train_loss=6.8504515

Batch 121450, train_perplexity=873.3988, train_loss=6.7723923

Batch 121460, train_perplexity=1020.6685, train_loss=6.928213

Batch 121470, train_perplexity=963.4766, train_loss=6.8705482

Batch 121480, train_perplexity=866.09467, train_loss=6.763994

Batch 121490, train_perplexity=912.4876, train_loss=6.8161745

Batch 121500, train_perplexity=846.6878, train_loss=6.741332

Batch 121510, train_perplexity=991.44763, train_loss=6.899166

Batch 121520, train_perplexity=897.5615, train_loss=6.7996817

Batch 121530, train_perplexity=1070.0718, train_loss=6.975481

Batch 121540, train_perplexity=993.1501, train_loss=6.900882

Batch 121550, train_perplexity=933.9222, train_loss=6.839393

Batch 121560, train_perplexity=921.7305, train_loss=6.826253

Batch 121570, train_perplexity=966.4643, train_loss=6.8736444

Batch 121580, train_perplexity=928.9624, train_loss=6.8340683

Batch 121590, train_perplexity=931.14435, train_loss=6.8364143

Batch 121600, train_perplexity=842.8156, train_loss=6.736748

Batch 121610, train_perplexity=919.9636, train_loss=6.824334

Batch 121620, train_perplexity=1025.7604, train_loss=6.9331894

Batch 121630, train_perplexity=899.82245, train_loss=6.8021975

Batch 121640, train_perplexity=978.4639, train_loss=6.885984

Batch 121650, train_perplexity=951.47656, train_loss=6.858015

Batch 121660, train_perplexity=898.4085, train_loss=6.800625

Batch 121670, train_perplexity=980.33295, train_loss=6.8878922

Batch 121680, train_perplexity=968.5237, train_loss=6.875773

Batch 121690, train_perplexity=990.9211, train_loss=6.898635

Batch 121700, train_perplexity=910.3772, train_loss=6.813859

Batch 121710, train_perplexity=921.94244, train_loss=6.826483

Batch 121720, train_perplexity=950.7337, train_loss=6.857234

Batch 121730, train_perplexity=955.8703, train_loss=6.8626223

Batch 121740, train_perplexity=878.66125, train_loss=6.7783995

Batch 121750, train_perplexity=1119.986, train_loss=7.0210714

Batch 121760, train_perplexity=1111.8936, train_loss=7.0138197

Batch 121770, train_perplexity=898.0615, train_loss=6.8002386

Batch 121780, train_perplexity=913.2489, train_loss=6.8170085

Batch 121790, train_perplexity=939.33923, train_loss=6.8451767

Batch 121800, train_perplexity=955.6502, train_loss=6.862392

Batch 121810, train_perplexity=861.51196, train_loss=6.758689

Batch 121820, train_perplexity=944.9882, train_loss=6.8511724

Batch 121830, train_perplexity=969.9421, train_loss=6.8772364

Batch 121840, train_perplexity=923.05884, train_loss=6.827693

Batch 121850, train_perplexity=922.55896, train_loss=6.8271513

Batch 121860, train_perplexity=968.4401, train_loss=6.8756866

Batch 121870, train_perplexity=941.34625, train_loss=6.847311

Batch 121880, train_perplexity=884.9145, train_loss=6.785491

Batch 121890, train_perplexity=929.22736, train_loss=6.8343534

Batch 121900, train_perplexity=940.7252, train_loss=6.846651

Batch 121910, train_perplexity=936.5261, train_loss=6.8421774

Batch 121920, train_perplexity=968.1917, train_loss=6.87543

Batch 121930, train_perplexity=1091.6798, train_loss=6.995473

Batch 121940, train_perplexity=939.9777, train_loss=6.845856

Batch 121950, train_perplexity=954.39923, train_loss=6.861082

Batch 121960, train_perplexity=875.89703, train_loss=6.7752485

Batch 121970, train_perplexity=884.682, train_loss=6.7852283

Batch 121980, train_perplexity=935.92786, train_loss=6.8415384

Batch 121990, train_perplexity=1016.8585, train_loss=6.9244733

Batch 122000, train_perplexity=979.20654, train_loss=6.8867426

Batch 122010, train_perplexity=906.47864, train_loss=6.8095675

Batch 122020, train_perplexity=968.9057, train_loss=6.8761673

Batch 122030, train_perplexity=830.8397, train_loss=6.722437

Batch 122040, train_perplexity=985.1807, train_loss=6.892825

Batch 122050, train_perplexity=956.42017, train_loss=6.8631973

Batch 122060, train_perplexity=946.3514, train_loss=6.852614

Batch 122070, train_perplexity=1008.42633, train_loss=6.9161463

Batch 122080, train_perplexity=947.421, train_loss=6.8537436

Batch 122090, train_perplexity=949.6911, train_loss=6.856137

Batch 122100, train_perplexity=892.42413, train_loss=6.7939415

Batch 122110, train_perplexity=912.60333, train_loss=6.8163013

Batch 122120, train_perplexity=867.9137, train_loss=6.7660923

Batch 122130, train_perplexity=966.43524, train_loss=6.8736143

Batch 122140, train_perplexity=916.09045, train_loss=6.820115

Batch 122150, train_perplexity=944.019, train_loss=6.8501463

Batch 122160, train_perplexity=1055.0967, train_loss=6.9613876

Batch 122170, train_perplexity=932.47644, train_loss=6.837844

Batch 122180, train_perplexity=1057.6605, train_loss=6.9638147

Batch 122190, train_perplexity=914.2515, train_loss=6.8181057

Batch 122200, train_perplexity=841.2822, train_loss=6.734927
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 122210, train_perplexity=939.5377, train_loss=6.845388

Batch 122220, train_perplexity=951.4475, train_loss=6.8579845

Batch 122230, train_perplexity=1000.20465, train_loss=6.90796

Batch 122240, train_perplexity=1036.4086, train_loss=6.9435167

Batch 122250, train_perplexity=920.43225, train_loss=6.8248434

Batch 122260, train_perplexity=975.3918, train_loss=6.882839

Batch 122270, train_perplexity=974.64606, train_loss=6.8820744

Batch 122280, train_perplexity=923.4674, train_loss=6.8281355

Batch 122290, train_perplexity=860.5069, train_loss=6.7575216

Batch 122300, train_perplexity=939.161, train_loss=6.844987

Batch 122310, train_perplexity=989.2083, train_loss=6.896905

Batch 122320, train_perplexity=1128.1315, train_loss=7.028318

Batch 122330, train_perplexity=923.4128, train_loss=6.8280764

Batch 122340, train_perplexity=917.31085, train_loss=6.8214464

Batch 122350, train_perplexity=1000.54333, train_loss=6.9082985

Batch 122360, train_perplexity=962.63306, train_loss=6.8696723

Batch 122370, train_perplexity=965.5932, train_loss=6.8727427

Batch 122380, train_perplexity=823.3754, train_loss=6.7134123

Batch 122390, train_perplexity=1031.7991, train_loss=6.9390593

Batch 122400, train_perplexity=902.18066, train_loss=6.804815

Batch 122410, train_perplexity=925.19336, train_loss=6.830003

Batch 122420, train_perplexity=911.69604, train_loss=6.8153067

Batch 122430, train_perplexity=941.4019, train_loss=6.84737

Batch 122440, train_perplexity=887.67755, train_loss=6.7886086

Batch 122450, train_perplexity=980.57556, train_loss=6.8881397

Batch 122460, train_perplexity=894.8513, train_loss=6.7966576

Batch 122470, train_perplexity=901.3418, train_loss=6.8038845

Batch 122480, train_perplexity=909.2735, train_loss=6.812646

Batch 122490, train_perplexity=1012.3539, train_loss=6.9200335

Batch 122500, train_perplexity=926.25543, train_loss=6.83115

Batch 122510, train_perplexity=901.10455, train_loss=6.8036213

Batch 122520, train_perplexity=883.19495, train_loss=6.783546

Batch 122530, train_perplexity=876.0006, train_loss=6.775367

Batch 122540, train_perplexity=1069.3867, train_loss=6.9748406

Batch 122550, train_perplexity=910.7402, train_loss=6.8142576

Batch 122560, train_perplexity=947.39026, train_loss=6.853711

Batch 122570, train_perplexity=843.0073, train_loss=6.7369757

Batch 122580, train_perplexity=847.35504, train_loss=6.74212

Batch 122590, train_perplexity=909.2284, train_loss=6.8125963

Batch 122600, train_perplexity=890.42633, train_loss=6.7917004

Batch 122610, train_perplexity=873.0932, train_loss=6.7720423

Batch 122620, train_perplexity=1007.8725, train_loss=6.915597

Batch 122630, train_perplexity=996.7738, train_loss=6.904524

Batch 122640, train_perplexity=948.46246, train_loss=6.854842

Batch 122650, train_perplexity=946.2273, train_loss=6.852483

Batch 122660, train_perplexity=911.8074, train_loss=6.8154287

Batch 122670, train_perplexity=950.42096, train_loss=6.856905

Batch 122680, train_perplexity=876.7097, train_loss=6.776176

Batch 122690, train_perplexity=926.94385, train_loss=6.831893

Batch 122700, train_perplexity=907.5508, train_loss=6.8107495

Batch 122710, train_perplexity=961.3262, train_loss=6.868314

Batch 122720, train_perplexity=943.75165, train_loss=6.849863

Batch 122730, train_perplexity=925.4714, train_loss=6.830303

Batch 122740, train_perplexity=991.31055, train_loss=6.899028

Batch 122750, train_perplexity=900.08936, train_loss=6.802494

Batch 122760, train_perplexity=904.9222, train_loss=6.807849

Batch 122770, train_perplexity=1029.428, train_loss=6.9367585

Batch 122780, train_perplexity=911.4005, train_loss=6.8149824

Batch 122790, train_perplexity=887.33984, train_loss=6.788228

Batch 122800, train_perplexity=1029.7804, train_loss=6.937101

Batch 122810, train_perplexity=924.35376, train_loss=6.829095

Batch 122820, train_perplexity=981.5636, train_loss=6.889147

Batch 122830, train_perplexity=963.7514, train_loss=6.8708334

Batch 122840, train_perplexity=927.3404, train_loss=6.8323207

Batch 122850, train_perplexity=989.648, train_loss=6.8973494

Batch 122860, train_perplexity=955.02747, train_loss=6.86174

Batch 122870, train_perplexity=950.58453, train_loss=6.857077

Batch 122880, train_perplexity=893.84314, train_loss=6.7955303

Batch 122890, train_perplexity=900.146, train_loss=6.802557

Batch 122900, train_perplexity=948.1902, train_loss=6.854555

Batch 122910, train_perplexity=936.5288, train_loss=6.8421803

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Batch 122920, train_perplexity=942.4057, train_loss=6.848436

Batch 122930, train_perplexity=914.418, train_loss=6.818288

Batch 122940, train_perplexity=998.2978, train_loss=6.9060516

Batch 122950, train_perplexity=938.62286, train_loss=6.8444138

Batch 122960, train_perplexity=960.6068, train_loss=6.867565

Batch 122970, train_perplexity=898.21484, train_loss=6.8004093

Batch 122980, train_perplexity=903.4993, train_loss=6.8062754

Batch 122990, train_perplexity=899.472, train_loss=6.801808

Batch 123000, train_perplexity=954.6824, train_loss=6.8613787

Batch 123010, train_perplexity=841.476, train_loss=6.7351575

Batch 123020, train_perplexity=864.2036, train_loss=6.7618084

Batch 123030, train_perplexity=905.8322, train_loss=6.808854

Batch 123040, train_perplexity=991.52277, train_loss=6.899242

Batch 123050, train_perplexity=869.3738, train_loss=6.767773

Batch 123060, train_perplexity=910.5873, train_loss=6.81409

Batch 123070, train_perplexity=870.2452, train_loss=6.768775

Batch 123080, train_perplexity=1009.43176, train_loss=6.917143

Batch 123090, train_perplexity=952.46436, train_loss=6.8590527

Batch 123100, train_perplexity=985.43964, train_loss=6.893088

Batch 123110, train_perplexity=1023.875, train_loss=6.9313498

Batch 123120, train_perplexity=907.09607, train_loss=6.8102484

Batch 123130, train_perplexity=899.3459, train_loss=6.8016677

Batch 123140, train_perplexity=932.2533, train_loss=6.8376045

Batch 123150, train_perplexity=986.85876, train_loss=6.894527

Batch 123160, train_perplexity=943.1484, train_loss=6.8492236

Batch 123170, train_perplexity=902.7276, train_loss=6.805421

Batch 123180, train_perplexity=949.05145, train_loss=6.855463

Batch 123190, train_perplexity=1134.7432, train_loss=7.0341616

Batch 123200, train_perplexity=997.6188, train_loss=6.905371

Batch 123210, train_perplexity=979.3868, train_loss=6.8869267

Batch 123220, train_perplexity=925.38794, train_loss=6.830213

Batch 123230, train_perplexity=962.9677, train_loss=6.87002

Batch 123240, train_perplexity=1008.2763, train_loss=6.9159975

Batch 123250, train_perplexity=964.15405, train_loss=6.871251

Batch 123260, train_perplexity=832.7769, train_loss=6.724766

Batch 123270, train_perplexity=933.4801, train_loss=6.8389196

Batch 123280, train_perplexity=941.0689, train_loss=6.8470163

Batch 123290, train_perplexity=912.50195, train_loss=6.8161902

Batch 123300, train_perplexity=921.5991, train_loss=6.8261104

Batch 123310, train_perplexity=970.25385, train_loss=6.8775578

Batch 123320, train_perplexity=889.8469, train_loss=6.7910495

Batch 123330, train_perplexity=942.40704, train_loss=6.8484373

Batch 123340, train_perplexity=882.6296, train_loss=6.7829056

Batch 123350, train_perplexity=989.35645, train_loss=6.8970547

Batch 123360, train_perplexity=919.1292, train_loss=6.8234267

Batch 123370, train_perplexity=861.9191, train_loss=6.7591615

Batch 123380, train_perplexity=1034.8165, train_loss=6.9419794

Batch 123390, train_perplexity=990.98157, train_loss=6.898696

Batch 123400, train_perplexity=969.0073, train_loss=6.876272

Batch 123410, train_perplexity=1012.6696, train_loss=6.9203453

Batch 123420, train_perplexity=964.39594, train_loss=6.871502

Batch 123430, train_perplexity=928.5404, train_loss=6.833614

Batch 123440, train_perplexity=856.1481, train_loss=6.7524433
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 123450, train_perplexity=956.6647, train_loss=6.863453

Batch 123460, train_perplexity=901.6014, train_loss=6.8041725

Batch 123470, train_perplexity=867.2431, train_loss=6.7653193

Batch 123480, train_perplexity=939.65106, train_loss=6.8455086

Batch 123490, train_perplexity=994.7231, train_loss=6.9024644

Batch 123500, train_perplexity=833.9404, train_loss=6.726162

Batch 123510, train_perplexity=874.35974, train_loss=6.773492

Batch 123520, train_perplexity=970.7694, train_loss=6.878089

Batch 123530, train_perplexity=923.0302, train_loss=6.827662

Batch 123540, train_perplexity=892.7076, train_loss=6.794259

Batch 123550, train_perplexity=1020.9606, train_loss=6.928499

Batch 123560, train_perplexity=1058.2861, train_loss=6.964406

Batch 123570, train_perplexity=936.9379, train_loss=6.842617

Batch 123580, train_perplexity=967.00226, train_loss=6.874201

Batch 123590, train_perplexity=902.8387, train_loss=6.805544

Batch 123600, train_perplexity=908.9917, train_loss=6.812336

Batch 123610, train_perplexity=906.41425, train_loss=6.8094964

Batch 123620, train_perplexity=1149.6086, train_loss=7.047177

Batch 123630, train_perplexity=973.07275, train_loss=6.880459

Batch 123640, train_perplexity=933.4583, train_loss=6.8388963

Batch 123650, train_perplexity=947.95966, train_loss=6.854312

Batch 123660, train_perplexity=941.07605, train_loss=6.847024

Batch 123670, train_perplexity=859.6981, train_loss=6.7565813

Batch 123680, train_perplexity=906.3965, train_loss=6.809477

Batch 123690, train_perplexity=970.0827, train_loss=6.8773813

Batch 123700, train_perplexity=906.50714, train_loss=6.809599

Batch 123710, train_perplexity=916.4854, train_loss=6.820546

Batch 123720, train_perplexity=1030.8608, train_loss=6.9381495

Batch 123730, train_perplexity=922.0233, train_loss=6.8265705

Batch 123740, train_perplexity=1014.3287, train_loss=6.9219823

Batch 123750, train_perplexity=881.1048, train_loss=6.7811766

Batch 123760, train_perplexity=976.38574, train_loss=6.8838577

Batch 123770, train_perplexity=1008.2792, train_loss=6.9160004

Batch 123780, train_perplexity=959.7447, train_loss=6.8666673

Batch 123790, train_perplexity=1006.9478, train_loss=6.914679

Batch 123800, train_perplexity=960.26654, train_loss=6.867211

Batch 123810, train_perplexity=985.31793, train_loss=6.8929644

Batch 123820, train_perplexity=981.6478, train_loss=6.8892326

Batch 123830, train_perplexity=963.22766, train_loss=6.87029

Batch 123840, train_perplexity=833.27344, train_loss=6.725362

Batch 123850, train_perplexity=899.81683, train_loss=6.8021913

Batch 123860, train_perplexity=894.0861, train_loss=6.795802

Batch 123870, train_perplexity=878.65625, train_loss=6.7783937

Batch 123880, train_perplexity=881.0292, train_loss=6.7810907

Batch 123890, train_perplexity=951.8709, train_loss=6.8584294

Batch 123900, train_perplexity=937.53815, train_loss=6.8432574

Batch 123910, train_perplexity=830.0284, train_loss=6.72146

Batch 123920, train_perplexity=959.3832, train_loss=6.8662906

Batch 123930, train_perplexity=934.6925, train_loss=6.8402176

Batch 123940, train_perplexity=931.6098, train_loss=6.836914

Batch 123950, train_perplexity=943.4439, train_loss=6.849537

Batch 123960, train_perplexity=986.4001, train_loss=6.894062

Batch 123970, train_perplexity=952.31946, train_loss=6.8589005

Batch 123980, train_perplexity=892.1518, train_loss=6.7936363

Batch 123990, train_perplexity=888.47705, train_loss=6.789509

Batch 124000, train_perplexity=898.42004, train_loss=6.8006377

Batch 124010, train_perplexity=921.86945, train_loss=6.8264036

Batch 124020, train_perplexity=931.94257, train_loss=6.837271

Batch 124030, train_perplexity=876.91296, train_loss=6.7764077

Batch 124040, train_perplexity=914.3295, train_loss=6.818191

Batch 124050, train_perplexity=978.8172, train_loss=6.886345

Batch 124060, train_perplexity=825.80615, train_loss=6.71636

Batch 124070, train_perplexity=922.33026, train_loss=6.8269033

Batch 124080, train_perplexity=995.61096, train_loss=6.9033566

Batch 124090, train_perplexity=980.0651, train_loss=6.887619

Batch 124100, train_perplexity=932.1386, train_loss=6.8374815

Batch 124110, train_perplexity=936.86646, train_loss=6.8425407

Batch 124120, train_perplexity=923.6171, train_loss=6.8282976

Batch 124130, train_perplexity=894.2695, train_loss=6.796007

Batch 124140, train_perplexity=923.6312, train_loss=6.828313

Batch 124150, train_perplexity=905.8253, train_loss=6.8088465

Batch 124160, train_perplexity=913.563, train_loss=6.8173523

Batch 124170, train_perplexity=937.5726, train_loss=6.843294

Batch 124180, train_perplexity=942.8682, train_loss=6.8489265

Batch 124190, train_perplexity=915.66504, train_loss=6.8196507

Batch 124200, train_perplexity=924.3644, train_loss=6.8291063

Batch 124210, train_perplexity=965.24335, train_loss=6.8723803

Batch 124220, train_perplexity=914.49915, train_loss=6.8183765

Batch 124230, train_perplexity=959.78033, train_loss=6.8667045

Batch 124240, train_perplexity=871.23584, train_loss=6.7699127

Batch 124250, train_perplexity=1007.985, train_loss=6.9157085

Batch 124260, train_perplexity=896.0939, train_loss=6.798045

Batch 124270, train_perplexity=929.9313, train_loss=6.8351107

Batch 124280, train_perplexity=904.2864, train_loss=6.807146

Batch 124290, train_perplexity=941.75256, train_loss=6.8477426

Batch 124300, train_perplexity=904.87775, train_loss=6.8078

Batch 124310, train_perplexity=943.7206, train_loss=6.84983

Batch 124320, train_perplexity=881.70496, train_loss=6.7818575

Batch 124330, train_perplexity=946.29315, train_loss=6.8525524

Batch 124340, train_perplexity=921.982, train_loss=6.8265257

Batch 124350, train_perplexity=924.6782, train_loss=6.829446

Batch 124360, train_perplexity=977.8739, train_loss=6.8853807

Batch 124370, train_perplexity=959.92914, train_loss=6.8668594

Batch 124380, train_perplexity=903.01953, train_loss=6.805744

Batch 124390, train_perplexity=941.4652, train_loss=6.8474374

Batch 124400, train_perplexity=1009.76154, train_loss=6.9174695

Batch 124410, train_perplexity=900.17865, train_loss=6.802593

Batch 124420, train_perplexity=1079.8152, train_loss=6.984545

Batch 124430, train_perplexity=888.6474, train_loss=6.7897005

Batch 124440, train_perplexity=908.366, train_loss=6.8116474

Batch 124450, train_perplexity=931.49695, train_loss=6.836793

Batch 124460, train_perplexity=952.7923, train_loss=6.859397

Batch 124470, train_perplexity=1026.024, train_loss=6.9334464

Batch 124480, train_perplexity=933.8585, train_loss=6.839325

Batch 124490, train_perplexity=941.88416, train_loss=6.8478823

Batch 124500, train_perplexity=976.4011, train_loss=6.8838735

Batch 124510, train_perplexity=944.3711, train_loss=6.850519

Batch 124520, train_perplexity=962.93195, train_loss=6.8699827

Batch 124530, train_perplexity=890.2943, train_loss=6.791552

Batch 124540, train_perplexity=915.37256, train_loss=6.819331

Batch 124550, train_perplexity=952.28265, train_loss=6.858862

Batch 124560, train_perplexity=924.9905, train_loss=6.8297834

Batch 124570, train_perplexity=917.17395, train_loss=6.821297

Batch 124580, train_perplexity=1041.8033, train_loss=6.9487085

Batch 124590, train_perplexity=870.31616, train_loss=6.7688565

Batch 124600, train_perplexity=995.8379, train_loss=6.9035845

Batch 124610, train_perplexity=955.68713, train_loss=6.8624306

Batch 124620, train_perplexity=948.7411, train_loss=6.855136

Batch 124630, train_perplexity=905.3477, train_loss=6.808319

Batch 124640, train_perplexity=932.55695, train_loss=6.83793

Batch 124650, train_perplexity=1073.8553, train_loss=6.9790106

Batch 124660, train_perplexity=993.65405, train_loss=6.901389

Batch 124670, train_perplexity=928.2212, train_loss=6.83327

Batch 124680, train_perplexity=900.3031, train_loss=6.8027315

Batch 124690, train_perplexity=998.56726, train_loss=6.9063215

Batch 124700, train_perplexity=868.6018, train_loss=6.766885

Batch 124710, train_perplexity=978.7743, train_loss=6.886301

Batch 124720, train_perplexity=854.9446, train_loss=6.7510366

Batch 124730, train_perplexity=1022.5401, train_loss=6.930045

Batch 124740, train_perplexity=929.4755, train_loss=6.8346205

Batch 124750, train_perplexity=826.0606, train_loss=6.716668
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 124760, train_perplexity=956.31165, train_loss=6.863084

Batch 124770, train_perplexity=900.85364, train_loss=6.803343

Batch 124780, train_perplexity=1008.3931, train_loss=6.9161134

Batch 124790, train_perplexity=915.48083, train_loss=6.8194494

Batch 124800, train_perplexity=893.6113, train_loss=6.795271

Batch 124810, train_perplexity=940.7167, train_loss=6.846642

Batch 124820, train_perplexity=1009.6773, train_loss=6.917386

Batch 124830, train_perplexity=964.6144, train_loss=6.8717284

Batch 124840, train_perplexity=1009.70233, train_loss=6.917411

Batch 124850, train_perplexity=888.2974, train_loss=6.7893066

Batch 124860, train_perplexity=962.7354, train_loss=6.8697786

Batch 124870, train_perplexity=999.3046, train_loss=6.9070597

Batch 124880, train_perplexity=984.3576, train_loss=6.891989

Batch 124890, train_perplexity=979.7829, train_loss=6.887331

Batch 124900, train_perplexity=826.2694, train_loss=6.716921

Batch 124910, train_perplexity=870.6777, train_loss=6.769272

Batch 124920, train_perplexity=912.4206, train_loss=6.816101

Batch 124930, train_perplexity=951.7239, train_loss=6.858275

Batch 124940, train_perplexity=950.3077, train_loss=6.856786

Batch 124950, train_perplexity=946.1628, train_loss=6.8524146

Batch 124960, train_perplexity=991.9612, train_loss=6.899684

Batch 124970, train_perplexity=949.95605, train_loss=6.8564157

Batch 124980, train_perplexity=928.7808, train_loss=6.833873

Batch 124990, train_perplexity=936.4797, train_loss=6.842128

Batch 125000, train_perplexity=955.61694, train_loss=6.862357

Batch 125010, train_perplexity=922.0137, train_loss=6.82656

Batch 125020, train_perplexity=901.3409, train_loss=6.8038836

Batch 125030, train_perplexity=883.9782, train_loss=6.7844324

Batch 125040, train_perplexity=1001.712, train_loss=6.909466

Batch 125050, train_perplexity=947.8277, train_loss=6.8541727

Batch 125060, train_perplexity=944.9589, train_loss=6.8511415

Batch 125070, train_perplexity=878.26544, train_loss=6.777949

Batch 125080, train_perplexity=919.1858, train_loss=6.823488

Batch 125090, train_perplexity=964.2148, train_loss=6.871314

Batch 125100, train_perplexity=931.95105, train_loss=6.8372803

Batch 125110, train_perplexity=952.11786, train_loss=6.858689

Batch 125120, train_perplexity=927.85834, train_loss=6.832879

Batch 125130, train_perplexity=963.41644, train_loss=6.870486

Batch 125140, train_perplexity=948.1314, train_loss=6.854493

Batch 125150, train_perplexity=939.5498, train_loss=6.845401

Batch 125160, train_perplexity=966.61084, train_loss=6.873796

Batch 125170, train_perplexity=1088.5641, train_loss=6.9926147

Batch 125180, train_perplexity=993.6076, train_loss=6.9013424

Batch 125190, train_perplexity=938.1964, train_loss=6.8439593

Batch 125200, train_perplexity=894.4853, train_loss=6.7962484

Batch 125210, train_perplexity=976.92316, train_loss=6.884408

Batch 125220, train_perplexity=965.5021, train_loss=6.8726482

Batch 125230, train_perplexity=954.17084, train_loss=6.8608427

Batch 125240, train_perplexity=908.80884, train_loss=6.8121347

Batch 125250, train_perplexity=895.7042, train_loss=6.7976103

Batch 125260, train_perplexity=919.10864, train_loss=6.8234043

Batch 125270, train_perplexity=1059.5717, train_loss=6.96562

Batch 125280, train_perplexity=954.41925, train_loss=6.861103

Batch 125290, train_perplexity=949.18634, train_loss=6.855605

Batch 125300, train_perplexity=890.51636, train_loss=6.7918015

Batch 125310, train_perplexity=942.0234, train_loss=6.84803

Batch 125320, train_perplexity=934.8414, train_loss=6.840377

Batch 125330, train_perplexity=979.2728, train_loss=6.8868103

Batch 125340, train_perplexity=1050.926, train_loss=6.957427

Batch 125350, train_perplexity=860.00397, train_loss=6.756937

Batch 125360, train_perplexity=889.09326, train_loss=6.790202

Batch 125370, train_perplexity=1027.8059, train_loss=6.9351816

Batch 125380, train_perplexity=955.8202, train_loss=6.86257

Batch 125390, train_perplexity=921.6084, train_loss=6.8261204

Batch 125400, train_perplexity=903.00616, train_loss=6.8057294

Batch 125410, train_perplexity=923.69025, train_loss=6.828377

Batch 125420, train_perplexity=891.51013, train_loss=6.792917

Batch 125430, train_perplexity=972.48083, train_loss=6.8798504

Batch 125440, train_perplexity=895.61884, train_loss=6.797515

Batch 125450, train_perplexity=968.15845, train_loss=6.875396

Batch 125460, train_perplexity=841.85443, train_loss=6.735607

Batch 125470, train_perplexity=1055.8385, train_loss=6.9620905

Batch 125480, train_perplexity=957.5619, train_loss=6.8643904

Batch 125490, train_perplexity=993.68536, train_loss=6.9014206

Batch 125500, train_perplexity=1011.73663, train_loss=6.9194236

Batch 125510, train_perplexity=901.1054, train_loss=6.8036222

Batch 125520, train_perplexity=933.3168, train_loss=6.8387446

Batch 125530, train_perplexity=854.8932, train_loss=6.7509766

Batch 125540, train_perplexity=965.039, train_loss=6.8721685

Batch 125550, train_perplexity=864.37836, train_loss=6.7620106

Batch 125560, train_perplexity=917.2409, train_loss=6.82137

Batch 125570, train_perplexity=927.68225, train_loss=6.8326893

Batch 125580, train_perplexity=904.69867, train_loss=6.807602

Batch 125590, train_perplexity=957.7953, train_loss=6.864634

Batch 125600, train_perplexity=934.3868, train_loss=6.8398905

Batch 125610, train_perplexity=851.5164, train_loss=6.747019

Batch 125620, train_perplexity=975.8598, train_loss=6.883319

Batch 125630, train_perplexity=883.86945, train_loss=6.7843094

Batch 125640, train_perplexity=974.47687, train_loss=6.881901

Batch 125650, train_perplexity=866.1826, train_loss=6.764096

Batch 125660, train_perplexity=882.4246, train_loss=6.7826734

Batch 125670, train_perplexity=1020.30554, train_loss=6.9278574

Batch 125680, train_perplexity=937.63245, train_loss=6.843358

Batch 125690, train_perplexity=918.1498, train_loss=6.8223605

Batch 125700, train_perplexity=954.4548, train_loss=6.8611403

Batch 125710, train_perplexity=900.9615, train_loss=6.8034625

Batch 125720, train_perplexity=947.97504, train_loss=6.854328

Batch 125730, train_perplexity=856.6503, train_loss=6.75303

Batch 125740, train_perplexity=919.60223, train_loss=6.823941

Batch 125750, train_perplexity=917.8206, train_loss=6.822002

Batch 125760, train_perplexity=1015.52356, train_loss=6.9231596

Batch 125770, train_perplexity=978.6492, train_loss=6.8861732

Batch 125780, train_perplexity=977.1319, train_loss=6.8846216

Batch 125790, train_perplexity=972.6803, train_loss=6.8800554

Batch 125800, train_perplexity=910.8492, train_loss=6.8143773

Batch 125810, train_perplexity=939.1126, train_loss=6.8449354

Batch 125820, train_perplexity=835.96295, train_loss=6.7285843

Batch 125830, train_perplexity=997.1731, train_loss=6.9049244

Batch 125840, train_perplexity=965.1725, train_loss=6.872307

Batch 125850, train_perplexity=918.2404, train_loss=6.822459

Batch 125860, train_perplexity=920.53674, train_loss=6.824957

Batch 125870, train_perplexity=885.8538, train_loss=6.786552

Batch 125880, train_perplexity=1031.777, train_loss=6.939038

Batch 125890, train_perplexity=945.7338, train_loss=6.851961

Batch 125900, train_perplexity=964.8849, train_loss=6.872009

Batch 125910, train_perplexity=947.6329, train_loss=6.853967

Batch 125920, train_perplexity=1054.6067, train_loss=6.960923

Batch 125930, train_perplexity=882.8514, train_loss=6.783157

Batch 125940, train_perplexity=957.9131, train_loss=6.864757

Batch 125950, train_perplexity=1037.3273, train_loss=6.9444027

Batch 125960, train_perplexity=986.1918, train_loss=6.893851

Batch 125970, train_perplexity=941.34576, train_loss=6.8473105

Batch 125980, train_perplexity=990.8795, train_loss=6.898593

Batch 125990, train_perplexity=890.5214, train_loss=6.791807

Batch 126000, train_perplexity=904.53564, train_loss=6.8074217

Batch 126010, train_perplexity=906.47516, train_loss=6.8095636

Batch 126020, train_perplexity=959.3974, train_loss=6.8663054

Batch 126030, train_perplexity=925.4908, train_loss=6.830324

Batch 126040, train_perplexity=849.88495, train_loss=6.745101

Batch 126050, train_perplexity=1030.8647, train_loss=6.9381533

Batch 126060, train_perplexity=872.8326, train_loss=6.771744
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 126070, train_perplexity=968.29236, train_loss=6.875534

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Batch 126080, train_perplexity=831.86884, train_loss=6.723675

Batch 126090, train_perplexity=967.029, train_loss=6.8742285

Batch 126100, train_perplexity=968.71216, train_loss=6.8759675

Batch 126110, train_perplexity=933.96136, train_loss=6.839435

Batch 126120, train_perplexity=988.54816, train_loss=6.8962374

Batch 126130, train_perplexity=907.2509, train_loss=6.810419

Batch 126140, train_perplexity=917.47974, train_loss=6.8216305

Batch 126150, train_perplexity=908.0014, train_loss=6.811246

Batch 126160, train_perplexity=909.69586, train_loss=6.8131104

Batch 126170, train_perplexity=880.10376, train_loss=6.78004

Batch 126180, train_perplexity=933.1503, train_loss=6.8385663

Batch 126190, train_perplexity=972.1897, train_loss=6.879551

Batch 126200, train_perplexity=874.55237, train_loss=6.773712

Batch 126210, train_perplexity=926.236, train_loss=6.831129

Batch 126220, train_perplexity=909.0212, train_loss=6.8123684

Batch 126230, train_perplexity=914.9519, train_loss=6.8188715

Batch 126240, train_perplexity=950.2442, train_loss=6.856719

Batch 126250, train_perplexity=870.9833, train_loss=6.769623

Batch 126260, train_perplexity=859.78503, train_loss=6.7566824

Batch 126270, train_perplexity=948.699, train_loss=6.8550916

Batch 126280, train_perplexity=941.4921, train_loss=6.847466

Batch 126290, train_perplexity=959.79865, train_loss=6.8667235

Batch 126300, train_perplexity=865.2406, train_loss=6.7630076

Batch 126310, train_perplexity=916.41724, train_loss=6.820472

Batch 126320, train_perplexity=989.9435, train_loss=6.897648

Batch 126330, train_perplexity=911.60736, train_loss=6.8152094

Batch 126340, train_perplexity=910.58295, train_loss=6.814085

Batch 126350, train_perplexity=907.29333, train_loss=6.810466

Batch 126360, train_perplexity=974.4778, train_loss=6.8819017

Batch 126370, train_perplexity=989.5353, train_loss=6.8972354

Batch 126380, train_perplexity=899.5668, train_loss=6.8019133

Batch 126390, train_perplexity=972.8259, train_loss=6.880205

Batch 126400, train_perplexity=955.00244, train_loss=6.861714

Batch 126410, train_perplexity=909.84076, train_loss=6.8132696

Batch 126420, train_perplexity=832.1545, train_loss=6.724018

Batch 126430, train_perplexity=911.9204, train_loss=6.8155527

Batch 126440, train_perplexity=894.36414, train_loss=6.796113

Batch 126450, train_perplexity=845.5807, train_loss=6.7400236

Batch 126460, train_perplexity=982.2013, train_loss=6.8897963

Batch 126470, train_perplexity=913.7459, train_loss=6.8175526

Batch 126480, train_perplexity=1019.8736, train_loss=6.927434

Batch 126490, train_perplexity=887.48285, train_loss=6.788389

Batch 126500, train_perplexity=976.1004, train_loss=6.8835654

Batch 126510, train_perplexity=882.69354, train_loss=6.782978

Batch 126520, train_perplexity=907.8507, train_loss=6.81108

Batch 126530, train_perplexity=914.92706, train_loss=6.8188443

Batch 126540, train_perplexity=839.9522, train_loss=6.733345

Batch 126550, train_perplexity=936.7534, train_loss=6.84242

Batch 126560, train_perplexity=1027.4766, train_loss=6.934861

Batch 126570, train_perplexity=989.1951, train_loss=6.8968916

Batch 126580, train_perplexity=899.23566, train_loss=6.801545

Batch 126590, train_perplexity=874.2446, train_loss=6.7733603

Batch 126600, train_perplexity=884.66724, train_loss=6.7852116

Batch 126610, train_perplexity=964.6866, train_loss=6.8718033

Batch 126620, train_perplexity=831.31366, train_loss=6.723007

Batch 126630, train_perplexity=986.87103, train_loss=6.8945394

Batch 126640, train_perplexity=946.39105, train_loss=6.852656

Batch 126650, train_perplexity=1020.86957, train_loss=6.92841

Batch 126660, train_perplexity=826.32965, train_loss=6.716994

Batch 126670, train_perplexity=927.00616, train_loss=6.83196

Batch 126680, train_perplexity=881.04553, train_loss=6.7811093

Batch 126690, train_perplexity=884.3762, train_loss=6.7848825

Batch 126700, train_perplexity=922.97876, train_loss=6.827606

Batch 126710, train_perplexity=905.80414, train_loss=6.808823

Batch 126720, train_perplexity=1000.07117, train_loss=6.9078264

Batch 126730, train_perplexity=936.6985, train_loss=6.8423615

Batch 126740, train_perplexity=906.1873, train_loss=6.809246

Batch 126750, train_perplexity=912.2883, train_loss=6.815956

Batch 126760, train_perplexity=866.3499, train_loss=6.764289

Batch 126770, train_perplexity=922.41205, train_loss=6.826992

Batch 126780, train_perplexity=941.6461, train_loss=6.8476295

Batch 126790, train_perplexity=936.45154, train_loss=6.8420978

Batch 126800, train_perplexity=953.154, train_loss=6.8597765

Batch 126810, train_perplexity=874.273, train_loss=6.7733927

Batch 126820, train_perplexity=846.4327, train_loss=6.7410307

Batch 126830, train_perplexity=895.609, train_loss=6.797504

Batch 126840, train_perplexity=905.02875, train_loss=6.8079667

Batch 126850, train_perplexity=849.5705, train_loss=6.744731

Batch 126860, train_perplexity=895.41986, train_loss=6.7972927

Batch 126870, train_perplexity=870.57184, train_loss=6.7691503

Batch 126880, train_perplexity=879.48035, train_loss=6.779331

Batch 126890, train_perplexity=951.18896, train_loss=6.8577127

Batch 126900, train_perplexity=929.7042, train_loss=6.8348665

Batch 126910, train_perplexity=998.00555, train_loss=6.905759

Batch 126920, train_perplexity=988.14667, train_loss=6.895831

Batch 126930, train_perplexity=950.8706, train_loss=6.857378

Batch 126940, train_perplexity=934.4251, train_loss=6.8399315

Batch 126950, train_perplexity=909.7332, train_loss=6.8131514

Batch 126960, train_perplexity=959.39417, train_loss=6.866302

Batch 126970, train_perplexity=960.4785, train_loss=6.8674316

Batch 126980, train_perplexity=971.131, train_loss=6.8784614

Batch 126990, train_perplexity=1019.1502, train_loss=6.9267244

Batch 127000, train_perplexity=938.2067, train_loss=6.8439703

Batch 127010, train_perplexity=877.36176, train_loss=6.7769194

Batch 127020, train_perplexity=970.4103, train_loss=6.877719

Batch 127030, train_perplexity=859.47186, train_loss=6.756318

Batch 127040, train_perplexity=951.9821, train_loss=6.8585463

Batch 127050, train_perplexity=882.3745, train_loss=6.7826166

Batch 127060, train_perplexity=904.81775, train_loss=6.8077335

Batch 127070, train_perplexity=958.84033, train_loss=6.8657246

Batch 127080, train_perplexity=918.63367, train_loss=6.8228874

Batch 127090, train_perplexity=936.4828, train_loss=6.842131

Batch 127100, train_perplexity=922.1025, train_loss=6.8266563

Batch 127110, train_perplexity=880.4786, train_loss=6.7804656

Batch 127120, train_perplexity=914.11115, train_loss=6.817952

Batch 127130, train_perplexity=908.07544, train_loss=6.8113275

Batch 127140, train_perplexity=935.7146, train_loss=6.8413105

Batch 127150, train_perplexity=977.2544, train_loss=6.884747

Batch 127160, train_perplexity=902.0985, train_loss=6.8047237

Batch 127170, train_perplexity=921.9002, train_loss=6.826437

Batch 127180, train_perplexity=947.2566, train_loss=6.85357

Batch 127190, train_perplexity=888.75165, train_loss=6.789818

Batch 127200, train_perplexity=873.78455, train_loss=6.772834

Batch 127210, train_perplexity=972.74286, train_loss=6.88012

Batch 127220, train_perplexity=986.7501, train_loss=6.894417

Batch 127230, train_perplexity=929.2185, train_loss=6.834344

Batch 127240, train_perplexity=948.85596, train_loss=6.855257

Batch 127250, train_perplexity=1005.7558, train_loss=6.9134946

Batch 127260, train_perplexity=1014.9373, train_loss=6.922582

Batch 127270, train_perplexity=989.4216, train_loss=6.8971205

Batch 127280, train_perplexity=924.32294, train_loss=6.8290615

Batch 127290, train_perplexity=940.90015, train_loss=6.846837

Batch 127300, train_perplexity=943.1929, train_loss=6.849271
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 127310, train_perplexity=907.32104, train_loss=6.8104963

Batch 127320, train_perplexity=908.1958, train_loss=6.81146

Batch 127330, train_perplexity=886.0401, train_loss=6.786762

Batch 127340, train_perplexity=905.44183, train_loss=6.808423

Batch 127350, train_perplexity=937.3687, train_loss=6.8430767

Batch 127360, train_perplexity=984.6233, train_loss=6.892259

Batch 127370, train_perplexity=900.60583, train_loss=6.8030677

Batch 127380, train_perplexity=945.2618, train_loss=6.851462

Batch 127390, train_perplexity=925.52435, train_loss=6.8303604

Batch 127400, train_perplexity=948.5402, train_loss=6.854924

Batch 127410, train_perplexity=873.9854, train_loss=6.7730637

Batch 127420, train_perplexity=970.7342, train_loss=6.8780527

Batch 127430, train_perplexity=938.4555, train_loss=6.8442354

Batch 127440, train_perplexity=934.9145, train_loss=6.840455

Batch 127450, train_perplexity=1002.91736, train_loss=6.9106684

Batch 127460, train_perplexity=900.68616, train_loss=6.803157

Batch 127470, train_perplexity=865.07434, train_loss=6.7628155

Batch 127480, train_perplexity=859.37555, train_loss=6.756206

Batch 127490, train_perplexity=840.552, train_loss=6.734059

Batch 127500, train_perplexity=872.3237, train_loss=6.7711606

Batch 127510, train_perplexity=928.857, train_loss=6.833955

Batch 127520, train_perplexity=905.32526, train_loss=6.8082943

Batch 127530, train_perplexity=914.86554, train_loss=6.818777

Batch 127540, train_perplexity=937.2543, train_loss=6.8429546

Batch 127550, train_perplexity=868.76587, train_loss=6.7670736

Batch 127560, train_perplexity=942.6021, train_loss=6.8486443

Batch 127570, train_perplexity=901.61346, train_loss=6.804186

Batch 127580, train_perplexity=926.93054, train_loss=6.8318787

Batch 127590, train_perplexity=921.6901, train_loss=6.826209

Batch 127600, train_perplexity=909.7306, train_loss=6.8131485

Batch 127610, train_perplexity=806.4172, train_loss=6.692601

Batch 127620, train_perplexity=947.8042, train_loss=6.854148

Batch 127630, train_perplexity=949.7038, train_loss=6.85615

Batch 127640, train_perplexity=912.841, train_loss=6.8165617

Batch 127650, train_perplexity=900.4259, train_loss=6.802868

Batch 127660, train_perplexity=920.5797, train_loss=6.8250036

Batch 127670, train_perplexity=926.5735, train_loss=6.8314934

Batch 127680, train_perplexity=970.18726, train_loss=6.877489

Batch 127690, train_perplexity=961.38306, train_loss=6.868373

Batch 127700, train_perplexity=855.59955, train_loss=6.7518024

Batch 127710, train_perplexity=901.9798, train_loss=6.804592

Batch 127720, train_perplexity=1035.9274, train_loss=6.9430523

Batch 127730, train_perplexity=903.6993, train_loss=6.8064966

Batch 127740, train_perplexity=991.816, train_loss=6.8995376

Batch 127750, train_perplexity=946.2688, train_loss=6.8525267

Batch 127760, train_perplexity=943.22797, train_loss=6.849308

Batch 127770, train_perplexity=1049.0181, train_loss=6.95561

Batch 127780, train_perplexity=905.41205, train_loss=6.80839

Batch 127790, train_perplexity=1003.9575, train_loss=6.911705

Batch 127800, train_perplexity=904.2113, train_loss=6.807063

Batch 127810, train_perplexity=967.7048, train_loss=6.874927

Batch 127820, train_perplexity=883.3057, train_loss=6.7836714

Batch 127830, train_perplexity=906.266, train_loss=6.809333

Batch 127840, train_perplexity=934.1213, train_loss=6.8396063

Batch 127850, train_perplexity=962.9411, train_loss=6.8699923

Batch 127860, train_perplexity=984.98346, train_loss=6.892625

Batch 127870, train_perplexity=1043.5764, train_loss=6.950409

Batch 127880, train_perplexity=861.0417, train_loss=6.758143

Batch 127890, train_perplexity=900.9387, train_loss=6.803437

Batch 127900, train_perplexity=865.8362, train_loss=6.7636957

Batch 127910, train_perplexity=989.4112, train_loss=6.89711

Batch 127920, train_perplexity=953.7046, train_loss=6.860354

Batch 127930, train_perplexity=981.4775, train_loss=6.889059

Batch 127940, train_perplexity=912.2009, train_loss=6.8158603

Batch 127950, train_perplexity=906.7942, train_loss=6.8099155

Batch 127960, train_perplexity=930.4564, train_loss=6.8356752

Batch 127970, train_perplexity=985.224, train_loss=6.892869

Batch 127980, train_perplexity=1019.32904, train_loss=6.9269

Batch 127990, train_perplexity=982.6079, train_loss=6.89021

Batch 128000, train_perplexity=990.4298, train_loss=6.898139

Batch 128010, train_perplexity=966.2233, train_loss=6.873395

Batch 128020, train_perplexity=875.3359, train_loss=6.7746077

Batch 128030, train_perplexity=911.2015, train_loss=6.814764

Batch 128040, train_perplexity=988.3054, train_loss=6.895992

Batch 128050, train_perplexity=926.00415, train_loss=6.8308787

Batch 128060, train_perplexity=925.3055, train_loss=6.830124

Batch 128070, train_perplexity=963.2566, train_loss=6.87032

Batch 128080, train_perplexity=828.06793, train_loss=6.719095

Batch 128090, train_perplexity=936.1332, train_loss=6.841758

Batch 128100, train_perplexity=935.8618, train_loss=6.841468

Batch 128110, train_perplexity=910.4623, train_loss=6.8139524

Batch 128120, train_perplexity=860.63947, train_loss=6.7576756

Batch 128130, train_perplexity=894.93494, train_loss=6.796751

Batch 128140, train_perplexity=850.4241, train_loss=6.745735

Batch 128150, train_perplexity=803.42035, train_loss=6.688878

Batch 128160, train_perplexity=937.40045, train_loss=6.8431106

Batch 128170, train_perplexity=938.1566, train_loss=6.843917

Batch 128180, train_perplexity=1016.1601, train_loss=6.923786

Batch 128190, train_perplexity=1005.83014, train_loss=6.9135685

Batch 128200, train_perplexity=940.0369, train_loss=6.845919

Batch 128210, train_perplexity=921.7411, train_loss=6.8262644

Batch 128220, train_perplexity=964.4447, train_loss=6.8715525

Batch 128230, train_perplexity=926.7573, train_loss=6.8316917

Batch 128240, train_perplexity=977.5112, train_loss=6.88501

Batch 128250, train_perplexity=968.2134, train_loss=6.8754525

Batch 128260, train_perplexity=904.0949, train_loss=6.8069344

Batch 128270, train_perplexity=834.8866, train_loss=6.727296

Batch 128280, train_perplexity=919.85925, train_loss=6.8242207

Batch 128290, train_perplexity=992.0856, train_loss=6.8998094

Batch 128300, train_perplexity=877.45123, train_loss=6.7770214

Batch 128310, train_perplexity=889.9534, train_loss=6.791169

Batch 128320, train_perplexity=891.4608, train_loss=6.7928615

Batch 128330, train_perplexity=872.10205, train_loss=6.7709064

Batch 128340, train_perplexity=847.0706, train_loss=6.741784

Batch 128350, train_perplexity=874.25214, train_loss=6.773369

Batch 128360, train_perplexity=925.04694, train_loss=6.8298445

Batch 128370, train_perplexity=1115.9069, train_loss=7.0174227

Batch 128380, train_perplexity=904.27167, train_loss=6.80713

Batch 128390, train_perplexity=848.9837, train_loss=6.74404

Batch 128400, train_perplexity=927.9911, train_loss=6.833022

Batch 128410, train_perplexity=1070.9733, train_loss=6.976323

Batch 128420, train_perplexity=943.66345, train_loss=6.8497696

Batch 128430, train_perplexity=953.0999, train_loss=6.8597198

Batch 128440, train_perplexity=962.1315, train_loss=6.869151

Batch 128450, train_perplexity=952.69824, train_loss=6.859298

Batch 128460, train_perplexity=868.297, train_loss=6.766534

Batch 128470, train_perplexity=852.03143, train_loss=6.7476234

Batch 128480, train_perplexity=828.08295, train_loss=6.7191133

Batch 128490, train_perplexity=1000.56915, train_loss=6.9083242

Batch 128500, train_perplexity=913.9734, train_loss=6.8178015

Batch 128510, train_perplexity=925.61346, train_loss=6.8304567

Batch 128520, train_perplexity=911.8943, train_loss=6.815524

Batch 128530, train_perplexity=895.94135, train_loss=6.797875

Batch 128540, train_perplexity=1044.6443, train_loss=6.9514318

Batch 128550, train_perplexity=950.79266, train_loss=6.857296

Batch 128560, train_perplexity=927.5712, train_loss=6.8325696

Batch 128570, train_perplexity=916.9383, train_loss=6.82104

Batch 128580, train_perplexity=932.3818, train_loss=6.8377423

Batch 128590, train_perplexity=998.33496, train_loss=6.906089

Batch 128600, train_perplexity=861.6492, train_loss=6.758848

Batch 128610, train_perplexity=936.2506, train_loss=6.841883
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 128620, train_perplexity=874.26465, train_loss=6.773383

Batch 128630, train_perplexity=1002.465, train_loss=6.9102173

Batch 128640, train_perplexity=866.02527, train_loss=6.763914

Batch 128650, train_perplexity=888.69696, train_loss=6.7897563

Batch 128660, train_perplexity=964.04926, train_loss=6.8711424

Batch 128670, train_perplexity=994.0256, train_loss=6.901763

Batch 128680, train_perplexity=1014.50525, train_loss=6.9221563

Batch 128690, train_perplexity=978.303, train_loss=6.8858194

Batch 128700, train_perplexity=901.71405, train_loss=6.8042974

Batch 128710, train_perplexity=885.2956, train_loss=6.7859216

Batch 128720, train_perplexity=849.45953, train_loss=6.7446003

Batch 128730, train_perplexity=948.98, train_loss=6.8553877

Batch 128740, train_perplexity=899.3146, train_loss=6.801633

Batch 128750, train_perplexity=935.3425, train_loss=6.840913

Batch 128760, train_perplexity=983.92865, train_loss=6.8915534

Batch 128770, train_perplexity=873.9754, train_loss=6.773052

Batch 128780, train_perplexity=887.8016, train_loss=6.7887483

Batch 128790, train_perplexity=906.91705, train_loss=6.810051

Batch 128800, train_perplexity=888.4262, train_loss=6.7894516

Batch 128810, train_perplexity=1000.04443, train_loss=6.9077997

Batch 128820, train_perplexity=936.5167, train_loss=6.8421674

Batch 128830, train_perplexity=933.5611, train_loss=6.8390064

Batch 128840, train_perplexity=925.339, train_loss=6.83016

Batch 128850, train_perplexity=924.02765, train_loss=6.828742

Batch 128860, train_perplexity=904.66156, train_loss=6.807561

Batch 128870, train_perplexity=925.0487, train_loss=6.8298464

Batch 128880, train_perplexity=980.1927, train_loss=6.887749

Batch 128890, train_perplexity=923.72546, train_loss=6.828415

Batch 128900, train_perplexity=1079.0298, train_loss=6.9838176

Batch 128910, train_perplexity=851.58466, train_loss=6.747099

Batch 128920, train_perplexity=976.40015, train_loss=6.8838725

Batch 128930, train_perplexity=1044.2772, train_loss=6.9510803

Batch 128940, train_perplexity=829.84595, train_loss=6.72124

Batch 128950, train_perplexity=971.94867, train_loss=6.879303

Batch 128960, train_perplexity=955.1131, train_loss=6.8618298

Batch 128970, train_perplexity=1009.99945, train_loss=6.917705

Batch 128980, train_perplexity=911.178, train_loss=6.8147383

Batch 128990, train_perplexity=989.80286, train_loss=6.8975058

Batch 129000, train_perplexity=946.94855, train_loss=6.853245

Batch 129010, train_perplexity=941.899, train_loss=6.847898

Batch 129020, train_perplexity=906.13464, train_loss=6.809188

Batch 129030, train_perplexity=925.1846, train_loss=6.8299932

Batch 129040, train_perplexity=1004.11456, train_loss=6.9118614

Batch 129050, train_perplexity=963.3117, train_loss=6.870377

Batch 129060, train_perplexity=947.32074, train_loss=6.8536377

Batch 129070, train_perplexity=896.3648, train_loss=6.7983475

Batch 129080, train_perplexity=976.6926, train_loss=6.884172

Batch 129090, train_perplexity=921.11633, train_loss=6.8255863

Batch 129100, train_perplexity=904.4196, train_loss=6.8072934

Batch 129110, train_perplexity=889.08734, train_loss=6.7901955

Batch 129120, train_perplexity=875.92206, train_loss=6.775277

Batch 129130, train_perplexity=927.20197, train_loss=6.8321714

Batch 129140, train_perplexity=960.1214, train_loss=6.8670597

Batch 129150, train_perplexity=908.379, train_loss=6.8116617

Batch 129160, train_perplexity=850.2197, train_loss=6.745495

Batch 129170, train_perplexity=997.9128, train_loss=6.905666

Batch 129180, train_perplexity=925.6779, train_loss=6.8305264

Batch 129190, train_perplexity=941.9866, train_loss=6.847991

Batch 129200, train_perplexity=923.5114, train_loss=6.828183

Batch 129210, train_perplexity=915.79565, train_loss=6.819793

Batch 129220, train_perplexity=917.54974, train_loss=6.821707

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Batch 129230, train_perplexity=932.49603, train_loss=6.837865

Batch 129240, train_perplexity=854.5892, train_loss=6.750621

Batch 129250, train_perplexity=945.3533, train_loss=6.8515587

Batch 129260, train_perplexity=907.09, train_loss=6.8102417

Batch 129270, train_perplexity=956.0499, train_loss=6.86281

Batch 129280, train_perplexity=876.6086, train_loss=6.7760606

Batch 129290, train_perplexity=901.6994, train_loss=6.804281

Batch 129300, train_perplexity=793.9482, train_loss=6.677018

Batch 129310, train_perplexity=884.4281, train_loss=6.784941

Batch 129320, train_perplexity=944.2279, train_loss=6.8503675

Batch 129330, train_perplexity=968.8304, train_loss=6.8760896

Batch 129340, train_perplexity=960.00507, train_loss=6.8669386

Batch 129350, train_perplexity=998.334, train_loss=6.906088

Batch 129360, train_perplexity=835.9151, train_loss=6.728527

Batch 129370, train_perplexity=930.4924, train_loss=6.835714

Batch 129380, train_perplexity=861.2035, train_loss=6.758331

Batch 129390, train_perplexity=867.9567, train_loss=6.766142

Batch 129400, train_perplexity=944.11176, train_loss=6.8502445

Batch 129410, train_perplexity=968.2111, train_loss=6.87545

Batch 129420, train_perplexity=918.94867, train_loss=6.8232303

Batch 129430, train_perplexity=934.9091, train_loss=6.8404493

Batch 129440, train_perplexity=904.3234, train_loss=6.807187

Batch 129450, train_perplexity=986.9247, train_loss=6.8945937

Batch 129460, train_perplexity=839.4814, train_loss=6.7327843

Batch 129470, train_perplexity=946.7215, train_loss=6.853005

Batch 129480, train_perplexity=864.5828, train_loss=6.762247

Batch 129490, train_perplexity=975.48663, train_loss=6.8829365

Batch 129500, train_perplexity=898.59955, train_loss=6.8008375

Batch 129510, train_perplexity=960.21796, train_loss=6.8671603

Batch 129520, train_perplexity=916.06116, train_loss=6.820083

Batch 129530, train_perplexity=1018.3438, train_loss=6.925933

Batch 129540, train_perplexity=905.3011, train_loss=6.8082676

Batch 129550, train_perplexity=981.03534, train_loss=6.8886085

Batch 129560, train_perplexity=947.2493, train_loss=6.8535624

Batch 129570, train_perplexity=926.30756, train_loss=6.8312063

Batch 129580, train_perplexity=1009.3201, train_loss=6.9170322

Batch 129590, train_perplexity=936.4288, train_loss=6.8420734

Batch 129600, train_perplexity=828.7632, train_loss=6.7199345

Batch 129610, train_perplexity=851.91486, train_loss=6.7474866

Batch 129620, train_perplexity=871.71704, train_loss=6.770465

Batch 129630, train_perplexity=938.97876, train_loss=6.844793

Batch 129640, train_perplexity=950.98944, train_loss=6.857503

Batch 129650, train_perplexity=837.7187, train_loss=6.7306824

Batch 129660, train_perplexity=899.56244, train_loss=6.8019085

Batch 129670, train_perplexity=825.03906, train_loss=6.7154307

Batch 129680, train_perplexity=903.21246, train_loss=6.805958

Batch 129690, train_perplexity=976.04175, train_loss=6.8835053

Batch 129700, train_perplexity=928.77905, train_loss=6.833871

Batch 129710, train_perplexity=875.4778, train_loss=6.77477

Batch 129720, train_perplexity=799.3492, train_loss=6.683798

Batch 129730, train_perplexity=928.84814, train_loss=6.8339453

Batch 129740, train_perplexity=978.96375, train_loss=6.8864946

Batch 129750, train_perplexity=960.43823, train_loss=6.8673897

Batch 129760, train_perplexity=891.2487, train_loss=6.7926235

Batch 129770, train_perplexity=888.01117, train_loss=6.7889843

Batch 129780, train_perplexity=883.20337, train_loss=6.7835555

Batch 129790, train_perplexity=962.96497, train_loss=6.870017

Batch 129800, train_perplexity=850.5417, train_loss=6.7458735

Batch 129810, train_perplexity=860.3838, train_loss=6.7573786

Batch 129820, train_perplexity=1082.0377, train_loss=6.9866014

Batch 129830, train_perplexity=989.4197, train_loss=6.8971186

Batch 129840, train_perplexity=905.3425, train_loss=6.8083134

Batch 129850, train_perplexity=913.22327, train_loss=6.8169804
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 129860, train_perplexity=867.155, train_loss=6.765218

Batch 129870, train_perplexity=895.81744, train_loss=6.7977366

Batch 129880, train_perplexity=975.85095, train_loss=6.88331

Batch 129890, train_perplexity=1028.9082, train_loss=6.9362535

Batch 129900, train_perplexity=844.3518, train_loss=6.7385693

Batch 129910, train_perplexity=912.39624, train_loss=6.8160744

Batch 129920, train_perplexity=916.8447, train_loss=6.820938

Batch 129930, train_perplexity=975.86395, train_loss=6.883323

Batch 129940, train_perplexity=933.1125, train_loss=6.838526

Batch 129950, train_perplexity=932.45953, train_loss=6.837826

Batch 129960, train_perplexity=903.84576, train_loss=6.8066587

Batch 129970, train_perplexity=877.4956, train_loss=6.777072

Batch 129980, train_perplexity=866.4639, train_loss=6.7644205

Batch 129990, train_perplexity=912.56854, train_loss=6.816263

Batch 130000, train_perplexity=1014.26825, train_loss=6.9219227

Batch 130010, train_perplexity=838.4348, train_loss=6.731537

Batch 130020, train_perplexity=883.68823, train_loss=6.7841043

Batch 130030, train_perplexity=906.90924, train_loss=6.8100424

Batch 130040, train_perplexity=926.0571, train_loss=6.830936

Batch 130050, train_perplexity=858.515, train_loss=6.755204

Batch 130060, train_perplexity=925.28735, train_loss=6.8301044

Batch 130070, train_perplexity=948.12915, train_loss=6.8544908

Batch 130080, train_perplexity=897.0006, train_loss=6.7990565

Batch 130090, train_perplexity=972.53467, train_loss=6.8799057

Batch 130100, train_perplexity=902.44354, train_loss=6.805106

Batch 130110, train_perplexity=846.6184, train_loss=6.74125

Batch 130120, train_perplexity=962.9687, train_loss=6.870021

Batch 130130, train_perplexity=971.4047, train_loss=6.878743

Batch 130140, train_perplexity=824.7555, train_loss=6.715087

Batch 130150, train_perplexity=839.2332, train_loss=6.7324886

Batch 130160, train_perplexity=929.6014, train_loss=6.834756

Batch 130170, train_perplexity=995.68024, train_loss=6.903426

Batch 130180, train_perplexity=1013.75134, train_loss=6.921413

Batch 130190, train_perplexity=935.6548, train_loss=6.8412466

Batch 130200, train_perplexity=950.36926, train_loss=6.8568506

Batch 130210, train_perplexity=973.6334, train_loss=6.881035

Batch 130220, train_perplexity=893.59686, train_loss=6.7952547

Batch 130230, train_perplexity=1061.048, train_loss=6.9670124

Batch 130240, train_perplexity=989.7764, train_loss=6.897479

Batch 130250, train_perplexity=952.1419, train_loss=6.858714

Batch 130260, train_perplexity=856.26483, train_loss=6.7525797

Batch 130270, train_perplexity=847.78186, train_loss=6.7426233

Batch 130280, train_perplexity=944.363, train_loss=6.8505106

Batch 130290, train_perplexity=895.81146, train_loss=6.79773

Batch 130300, train_perplexity=1030.1248, train_loss=6.937435

Batch 130310, train_perplexity=865.02985, train_loss=6.762764

Batch 130320, train_perplexity=961.1868, train_loss=6.868169

Batch 130330, train_perplexity=921.81055, train_loss=6.8263397

Batch 130340, train_perplexity=1030.1444, train_loss=6.937454

Batch 130350, train_perplexity=914.044, train_loss=6.8178787

Batch 130360, train_perplexity=930.098, train_loss=6.83529

Batch 130370, train_perplexity=880.7486, train_loss=6.780772

Batch 130380, train_perplexity=871.4685, train_loss=6.7701797

Batch 130390, train_perplexity=966.4408, train_loss=6.87362

Batch 130400, train_perplexity=979.6997, train_loss=6.887246

Batch 130410, train_perplexity=864.563, train_loss=6.762224

Batch 130420, train_perplexity=865.14984, train_loss=6.7629027

Batch 130430, train_perplexity=889.9806, train_loss=6.7911997

Batch 130440, train_perplexity=969.2458, train_loss=6.8765182

Batch 130450, train_perplexity=893.1261, train_loss=6.794728

Batch 130460, train_perplexity=976.31494, train_loss=6.8837852

Batch 130470, train_perplexity=898.543, train_loss=6.8007746

Batch 130480, train_perplexity=916.2193, train_loss=6.8202558

Batch 130490, train_perplexity=922.78687, train_loss=6.8273983

Batch 130500, train_perplexity=927.88135, train_loss=6.832904

Batch 130510, train_perplexity=933.0373, train_loss=6.838445

Batch 130520, train_perplexity=853.2361, train_loss=6.7490363

Batch 130530, train_perplexity=984.7679, train_loss=6.892406

Batch 130540, train_perplexity=953.7414, train_loss=6.8603926

Batch 130550, train_perplexity=929.63245, train_loss=6.8347893

Batch 130560, train_perplexity=1010.0751, train_loss=6.91778

Batch 130570, train_perplexity=879.63806, train_loss=6.7795105

Batch 130580, train_perplexity=1049.4628, train_loss=6.9560337

Batch 130590, train_perplexity=872.5646, train_loss=6.7714367

Batch 130600, train_perplexity=852.794, train_loss=6.748518

Batch 130610, train_perplexity=881.80505, train_loss=6.781971

Batch 130620, train_perplexity=964.01434, train_loss=6.871106

Batch 130630, train_perplexity=948.6569, train_loss=6.855047

Batch 130640, train_perplexity=867.21954, train_loss=6.765292

Batch 130650, train_perplexity=833.7615, train_loss=6.7259474

Batch 130660, train_perplexity=979.98004, train_loss=6.887532

Batch 130670, train_perplexity=953.35266, train_loss=6.859985

Batch 130680, train_perplexity=902.68115, train_loss=6.8053694

Batch 130690, train_perplexity=892.1565, train_loss=6.7936416

Batch 130700, train_perplexity=847.3057, train_loss=6.7420616

Batch 130710, train_perplexity=989.8953, train_loss=6.897599

Batch 130720, train_perplexity=903.0324, train_loss=6.8057585

Batch 130730, train_perplexity=941.2421, train_loss=6.8472004

Batch 130740, train_perplexity=947.0136, train_loss=6.8533134

Batch 130750, train_perplexity=948.52936, train_loss=6.8549128

Batch 130760, train_perplexity=981.54626, train_loss=6.889129

Batch 130770, train_perplexity=821.6121, train_loss=6.7112684

Batch 130780, train_perplexity=898.43463, train_loss=6.800654

Batch 130790, train_perplexity=895.01855, train_loss=6.7968445

Batch 130800, train_perplexity=979.369, train_loss=6.8869085

Batch 130810, train_perplexity=889.3655, train_loss=6.7905083

Batch 130820, train_perplexity=826.28357, train_loss=6.716938

Batch 130830, train_perplexity=836.46497, train_loss=6.7291846

Batch 130840, train_perplexity=898.60645, train_loss=6.800845

Batch 130850, train_perplexity=973.9287, train_loss=6.881338

Batch 130860, train_perplexity=831.37396, train_loss=6.7230797

Batch 130870, train_perplexity=911.48047, train_loss=6.81507

Batch 130880, train_perplexity=941.49615, train_loss=6.8474703

Batch 130890, train_perplexity=974.00116, train_loss=6.8814125

Batch 130900, train_perplexity=930.0634, train_loss=6.835253

Batch 130910, train_perplexity=1006.0575, train_loss=6.9137945

Batch 130920, train_perplexity=964.12695, train_loss=6.871223

Batch 130930, train_perplexity=841.7405, train_loss=6.7354717

Batch 130940, train_perplexity=986.32764, train_loss=6.8939886

Batch 130950, train_perplexity=1017.9132, train_loss=6.92551

Batch 130960, train_perplexity=910.5695, train_loss=6.81407

Batch 130970, train_perplexity=900.2143, train_loss=6.802633

Batch 130980, train_perplexity=988.83484, train_loss=6.8965273

Batch 130990, train_perplexity=979.0487, train_loss=6.8865814

Batch 131000, train_perplexity=964.5859, train_loss=6.871699

Batch 131010, train_perplexity=945.3109, train_loss=6.851514

Batch 131020, train_perplexity=878.7899, train_loss=6.778546

Batch 131030, train_perplexity=879.6414, train_loss=6.7795143

Batch 131040, train_perplexity=961.3106, train_loss=6.8682976

Batch 131050, train_perplexity=831.6975, train_loss=6.723469

Batch 131060, train_perplexity=902.68286, train_loss=6.8053713

Batch 131070, train_perplexity=937.08716, train_loss=6.8427763

Batch 131080, train_perplexity=1009.0882, train_loss=6.9168024

Batch 131090, train_perplexity=953.5877, train_loss=6.8602314

Batch 131100, train_perplexity=985.1873, train_loss=6.892832

Batch 131110, train_perplexity=1024.1299, train_loss=6.9315987

Batch 131120, train_perplexity=917.79126, train_loss=6.82197

Batch 131130, train_perplexity=924.01447, train_loss=6.8287277

Batch 131140, train_perplexity=998.703, train_loss=6.9064574

Batch 131150, train_perplexity=874.79425, train_loss=6.7739887

Batch 131160, train_perplexity=916.2337, train_loss=6.8202715
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 131170, train_perplexity=950.2261, train_loss=6.8567

Batch 131180, train_perplexity=902.7044, train_loss=6.805395

Batch 131190, train_perplexity=836.15826, train_loss=6.728818

Batch 131200, train_perplexity=849.5422, train_loss=6.7446976

Batch 131210, train_perplexity=958.8824, train_loss=6.8657684

Batch 131220, train_perplexity=1000.2915, train_loss=6.9080467

Batch 131230, train_perplexity=867.7697, train_loss=6.7659264

Batch 131240, train_perplexity=837.74426, train_loss=6.730713

Batch 131250, train_perplexity=902.90283, train_loss=6.805615

Batch 131260, train_perplexity=915.9524, train_loss=6.8199644

Batch 131270, train_perplexity=932.70325, train_loss=6.838087

Batch 131280, train_perplexity=871.4864, train_loss=6.7702003

Batch 131290, train_perplexity=937.06305, train_loss=6.8427505

Batch 131300, train_perplexity=924.6765, train_loss=6.829444

Batch 131310, train_perplexity=946.26654, train_loss=6.8525243

Batch 131320, train_perplexity=897.47766, train_loss=6.799588

Batch 131330, train_perplexity=900.4405, train_loss=6.802884

Batch 131340, train_perplexity=936.5578, train_loss=6.8422112

Batch 131350, train_perplexity=929.8217, train_loss=6.834993

Batch 131360, train_perplexity=854.27014, train_loss=6.7502475

Batch 131370, train_perplexity=967.40027, train_loss=6.8746123

Batch 131380, train_perplexity=913.3818, train_loss=6.817154

Batch 131390, train_perplexity=911.67737, train_loss=6.815286

Batch 131400, train_perplexity=936.093, train_loss=6.841715

Batch 131410, train_perplexity=1024.8202, train_loss=6.9322724

Batch 131420, train_perplexity=919.4032, train_loss=6.8237247

Batch 131430, train_perplexity=890.8553, train_loss=6.792182

Batch 131440, train_perplexity=911.97565, train_loss=6.8156133

Batch 131450, train_perplexity=801.8986, train_loss=6.686982

Batch 131460, train_perplexity=961.3294, train_loss=6.868317

Batch 131470, train_perplexity=946.67676, train_loss=6.8529577

Batch 131480, train_perplexity=910.2747, train_loss=6.8137465

Batch 131490, train_perplexity=890.569, train_loss=6.7918606

Batch 131500, train_perplexity=875.25574, train_loss=6.774516

Batch 131510, train_perplexity=911.583, train_loss=6.8151827

Batch 131520, train_perplexity=833.0621, train_loss=6.725108

Batch 131530, train_perplexity=883.01263, train_loss=6.7833395

Batch 131540, train_perplexity=916.22845, train_loss=6.820266

Batch 131550, train_perplexity=899.64996, train_loss=6.802006

Batch 131560, train_perplexity=922.04266, train_loss=6.8265915

Batch 131570, train_perplexity=886.3824, train_loss=6.7871485

Batch 131580, train_perplexity=909.0979, train_loss=6.812453

Batch 131590, train_perplexity=888.6919, train_loss=6.7897506

Batch 131600, train_perplexity=902.97345, train_loss=6.805693

Batch 131610, train_perplexity=962.89795, train_loss=6.8699474

Batch 131620, train_perplexity=1004.18066, train_loss=6.911927

Batch 131630, train_perplexity=958.1328, train_loss=6.8649864

Batch 131640, train_perplexity=881.7386, train_loss=6.7818956

Batch 131650, train_perplexity=944.6156, train_loss=6.850778

Batch 131660, train_perplexity=974.0114, train_loss=6.881423

Batch 131670, train_perplexity=861.70667, train_loss=6.758915

Batch 131680, train_perplexity=871.39874, train_loss=6.7700996

Batch 131690, train_perplexity=1030.9911, train_loss=6.938276

Batch 131700, train_perplexity=915.8948, train_loss=6.8199015

Batch 131710, train_perplexity=964.7758, train_loss=6.871896

Batch 131720, train_perplexity=958.97565, train_loss=6.8658657

Batch 131730, train_perplexity=843.6652, train_loss=6.737756

Batch 131740, train_perplexity=831.8474, train_loss=6.723649

Batch 131750, train_perplexity=952.3785, train_loss=6.8589625

Batch 131760, train_perplexity=868.0002, train_loss=6.766192

Batch 131770, train_perplexity=877.29016, train_loss=6.776838

Batch 131780, train_perplexity=842.9768, train_loss=6.7369394

Batch 131790, train_perplexity=900.602, train_loss=6.8030634

Batch 131800, train_perplexity=907.8711, train_loss=6.8111024

Batch 131810, train_perplexity=893.22363, train_loss=6.794837

Batch 131820, train_perplexity=953.7141, train_loss=6.860364

Batch 131830, train_perplexity=925.4886, train_loss=6.830322

Batch 131840, train_perplexity=925.2022, train_loss=6.8300123

Batch 131850, train_perplexity=876.88745, train_loss=6.7763786

Batch 131860, train_perplexity=914.75214, train_loss=6.818653

Batch 131870, train_perplexity=871.8946, train_loss=6.7706685

Batch 131880, train_perplexity=884.52295, train_loss=6.7850485

Batch 131890, train_perplexity=969.4464, train_loss=6.876725

Batch 131900, train_perplexity=928.0893, train_loss=6.833128

Batch 131910, train_perplexity=894.08356, train_loss=6.7957993

Batch 131920, train_perplexity=866.0063, train_loss=6.763892

Batch 131930, train_perplexity=936.6927, train_loss=6.8423553

Batch 131940, train_perplexity=926.6106, train_loss=6.8315334

Batch 131950, train_perplexity=985.65533, train_loss=6.8933067

Batch 131960, train_perplexity=843.82056, train_loss=6.73794

Batch 131970, train_perplexity=844.66833, train_loss=6.738944

Batch 131980, train_perplexity=927.3174, train_loss=6.832296

Batch 131990, train_perplexity=933.2638, train_loss=6.838688

Batch 132000, train_perplexity=944.3026, train_loss=6.8504467

Batch 132010, train_perplexity=844.9354, train_loss=6.73926

Batch 132020, train_perplexity=933.11163, train_loss=6.838525

Batch 132030, train_perplexity=960.58295, train_loss=6.8675404

Batch 132040, train_perplexity=867.94104, train_loss=6.766124

Batch 132050, train_perplexity=888.16656, train_loss=6.7891593

Batch 132060, train_perplexity=845.5751, train_loss=6.740017

Batch 132070, train_perplexity=865.0595, train_loss=6.7627983

Batch 132080, train_perplexity=1031.0338, train_loss=6.9383173

Batch 132090, train_perplexity=978.4047, train_loss=6.8859234

Batch 132100, train_perplexity=919.0657, train_loss=6.8233576

Batch 132110, train_perplexity=936.26624, train_loss=6.8419

Batch 132120, train_perplexity=937.4022, train_loss=6.8431125

Batch 132130, train_perplexity=951.2647, train_loss=6.8577924

Batch 132140, train_perplexity=907.52094, train_loss=6.8107166

Batch 132150, train_perplexity=889.24164, train_loss=6.790369

Batch 132160, train_perplexity=888.8915, train_loss=6.789975

Batch 132170, train_perplexity=921.3298, train_loss=6.825818

Batch 132180, train_perplexity=861.2363, train_loss=6.758369

Batch 132190, train_perplexity=840.3256, train_loss=6.7337894

Batch 132200, train_perplexity=1006.3953, train_loss=6.91413

Batch 132210, train_perplexity=809.20447, train_loss=6.6960516

Batch 132220, train_perplexity=1015.4451, train_loss=6.9230824

Batch 132230, train_perplexity=856.41833, train_loss=6.752759

Batch 132240, train_perplexity=860.39325, train_loss=6.7573895

Batch 132250, train_perplexity=886.199, train_loss=6.7869415

Batch 132260, train_perplexity=873.0978, train_loss=6.7720475

Batch 132270, train_perplexity=1030.1423, train_loss=6.9374523

Batch 132280, train_perplexity=798.27765, train_loss=6.6824565

Batch 132290, train_perplexity=959.05524, train_loss=6.8659487

Batch 132300, train_perplexity=883.38995, train_loss=6.7837667

Batch 132310, train_perplexity=903.32745, train_loss=6.806085

Batch 132320, train_perplexity=835.4031, train_loss=6.7279143

Batch 132330, train_perplexity=990.6442, train_loss=6.8983555

Batch 132340, train_perplexity=837.91925, train_loss=6.7309217

Batch 132350, train_perplexity=869.1371, train_loss=6.767501

Batch 132360, train_perplexity=912.14526, train_loss=6.815799

Batch 132370, train_perplexity=997.7848, train_loss=6.9055376

Batch 132380, train_perplexity=827.1461, train_loss=6.7179813

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Batch 132390, train_perplexity=925.7331, train_loss=6.830586

Batch 132400, train_perplexity=962.7951, train_loss=6.8698406
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 132410, train_perplexity=982.12866, train_loss=6.8897223

Batch 132420, train_perplexity=922.8458, train_loss=6.827462

Batch 132430, train_perplexity=931.76355, train_loss=6.837079

Batch 132440, train_perplexity=894.6687, train_loss=6.7964535

Batch 132450, train_perplexity=931.36774, train_loss=6.836654

Batch 132460, train_perplexity=911.63696, train_loss=6.815242

Batch 132470, train_perplexity=902.8749, train_loss=6.805584

Batch 132480, train_perplexity=818.55334, train_loss=6.7075386

Batch 132490, train_perplexity=885.0605, train_loss=6.785656

Batch 132500, train_perplexity=871.6655, train_loss=6.770406

Batch 132510, train_perplexity=956.90735, train_loss=6.8637066

Batch 132520, train_perplexity=960.21295, train_loss=6.867155

Batch 132530, train_perplexity=875.8323, train_loss=6.7751746

Batch 132540, train_perplexity=850.99524, train_loss=6.7464066

Batch 132550, train_perplexity=961.17267, train_loss=6.868154

Batch 132560, train_perplexity=941.757, train_loss=6.8477473

Batch 132570, train_perplexity=998.0746, train_loss=6.905828

Batch 132580, train_perplexity=905.05164, train_loss=6.807992

Batch 132590, train_perplexity=853.49896, train_loss=6.7493443

Batch 132600, train_perplexity=982.3858, train_loss=6.889984

Batch 132610, train_perplexity=889.7612, train_loss=6.790953

Batch 132620, train_perplexity=873.7562, train_loss=6.7728014

Batch 132630, train_perplexity=929.8789, train_loss=6.8350544

Batch 132640, train_perplexity=976.0999, train_loss=6.883565

Batch 132650, train_perplexity=901.2279, train_loss=6.803758

Batch 132660, train_perplexity=919.51544, train_loss=6.823847

Batch 132670, train_perplexity=919.08844, train_loss=6.8233824

Batch 132680, train_perplexity=922.6021, train_loss=6.827198

Batch 132690, train_perplexity=853.04047, train_loss=6.748807

Batch 132700, train_perplexity=869.7814, train_loss=6.768242

Batch 132710, train_perplexity=941.13617, train_loss=6.847088

Batch 132720, train_perplexity=857.7069, train_loss=6.7542624

Batch 132730, train_perplexity=880.4605, train_loss=6.780445

Batch 132740, train_perplexity=910.6359, train_loss=6.814143

Batch 132750, train_perplexity=947.5579, train_loss=6.853888

Batch 132760, train_perplexity=858.48804, train_loss=6.7551727

Batch 132770, train_perplexity=923.36566, train_loss=6.8280253

Batch 132780, train_perplexity=924.11316, train_loss=6.8288345

Batch 132790, train_perplexity=955.99524, train_loss=6.862753

Batch 132800, train_perplexity=926.94293, train_loss=6.831892

Batch 132810, train_perplexity=907.3522, train_loss=6.8105307

Batch 132820, train_perplexity=867.2423, train_loss=6.7653184

Batch 132830, train_perplexity=990.6197, train_loss=6.8983307

Batch 132840, train_perplexity=953.83325, train_loss=6.860489

Batch 132850, train_perplexity=984.15625, train_loss=6.8917847

Batch 132860, train_perplexity=912.5015, train_loss=6.81619

Batch 132870, train_perplexity=952.29083, train_loss=6.8588705

Batch 132880, train_perplexity=987.31866, train_loss=6.894993

Batch 132890, train_perplexity=964.9562, train_loss=6.8720827

Batch 132900, train_perplexity=828.00476, train_loss=6.719019

Batch 132910, train_perplexity=867.3643, train_loss=6.765459

Batch 132920, train_perplexity=911.7282, train_loss=6.815342

Batch 132930, train_perplexity=1027.5809, train_loss=6.9349627

Batch 132940, train_perplexity=902.1544, train_loss=6.8047857

Batch 132950, train_perplexity=883.19794, train_loss=6.7835493

Batch 132960, train_perplexity=990.9627, train_loss=6.898677

Batch 132970, train_perplexity=905.1017, train_loss=6.8080473

Batch 132980, train_perplexity=850.8723, train_loss=6.746262

Batch 132990, train_perplexity=874.9457, train_loss=6.774162

Batch 133000, train_perplexity=827.77106, train_loss=6.7187366

Batch 133010, train_perplexity=986.51483, train_loss=6.8941784

Batch 133020, train_perplexity=912.6913, train_loss=6.8163977

Batch 133030, train_perplexity=986.4904, train_loss=6.8941536

Batch 133040, train_perplexity=884.09204, train_loss=6.784561

Batch 133050, train_perplexity=966.5348, train_loss=6.8737173

Batch 133060, train_perplexity=875.1226, train_loss=6.774364

Batch 133070, train_perplexity=876.6671, train_loss=6.7761273

Batch 133080, train_perplexity=900.77637, train_loss=6.803257

Batch 133090, train_perplexity=881.8134, train_loss=6.7819805

Batch 133100, train_perplexity=949.4217, train_loss=6.855853

Batch 133110, train_perplexity=997.8233, train_loss=6.905576

Batch 133120, train_perplexity=920.8405, train_loss=6.825287

Batch 133130, train_perplexity=910.1532, train_loss=6.813613

Batch 133140, train_perplexity=892.791, train_loss=6.7943525

Batch 133150, train_perplexity=804.58276, train_loss=6.690324

Batch 133160, train_perplexity=966.6791, train_loss=6.8738666

Batch 133170, train_perplexity=944.70074, train_loss=6.850868

Batch 133180, train_perplexity=928.4939, train_loss=6.833564

Batch 133190, train_perplexity=823.5961, train_loss=6.7136803

Batch 133200, train_perplexity=951.73065, train_loss=6.858282

Batch 133210, train_perplexity=878.37976, train_loss=6.778079

Batch 133220, train_perplexity=796.8899, train_loss=6.6807165

Batch 133230, train_perplexity=914.27545, train_loss=6.818132

Batch 133240, train_perplexity=950.005, train_loss=6.8564672

Batch 133250, train_perplexity=914.5698, train_loss=6.818454

Batch 133260, train_perplexity=831.39215, train_loss=6.7231016

Batch 133270, train_perplexity=901.80695, train_loss=6.8044004

Batch 133280, train_perplexity=883.78687, train_loss=6.784216

Batch 133290, train_perplexity=951.8178, train_loss=6.8583736

Batch 133300, train_perplexity=851.56195, train_loss=6.747072

Batch 133310, train_perplexity=921.9275, train_loss=6.8264666

Batch 133320, train_perplexity=952.1056, train_loss=6.858676

Batch 133330, train_perplexity=989.94446, train_loss=6.897649

Batch 133340, train_perplexity=893.26624, train_loss=6.7948847

Batch 133350, train_perplexity=981.7733, train_loss=6.8893604

Batch 133360, train_perplexity=1030.7974, train_loss=6.938088

Batch 133370, train_perplexity=872.4465, train_loss=6.7713013

Batch 133380, train_perplexity=928.93854, train_loss=6.8340425

Batch 133390, train_perplexity=1017.4774, train_loss=6.9250817

Batch 133400, train_perplexity=801.03186, train_loss=6.6859007

Batch 133410, train_perplexity=863.79987, train_loss=6.761341

Batch 133420, train_perplexity=884.2733, train_loss=6.784766

Batch 133430, train_perplexity=928.632, train_loss=6.8337126

Batch 133440, train_perplexity=835.79, train_loss=6.7283773

Batch 133450, train_perplexity=861.6344, train_loss=6.758831

Batch 133460, train_perplexity=825.529, train_loss=6.7160244

Batch 133470, train_perplexity=941.608, train_loss=6.847589

Batch 133480, train_perplexity=884.4783, train_loss=6.784998

Batch 133490, train_perplexity=922.61707, train_loss=6.8272142

Batch 133500, train_perplexity=957.1483, train_loss=6.8639584

Batch 133510, train_perplexity=932.5218, train_loss=6.8378925

Batch 133520, train_perplexity=883.37646, train_loss=6.7837515

Batch 133530, train_perplexity=936.82043, train_loss=6.8424916

Batch 133540, train_perplexity=893.80396, train_loss=6.7954865

Batch 133550, train_perplexity=922.6751, train_loss=6.827277

Batch 133560, train_perplexity=1045.0743, train_loss=6.9518433

Batch 133570, train_perplexity=824.63275, train_loss=6.714938

Batch 133580, train_perplexity=834.19336, train_loss=6.726465

Batch 133590, train_perplexity=981.21265, train_loss=6.888789

Batch 133600, train_perplexity=917.4561, train_loss=6.8216047

Batch 133610, train_perplexity=915.3839, train_loss=6.8193436

Batch 133620, train_perplexity=875.31415, train_loss=6.774583

Batch 133630, train_perplexity=878.0958, train_loss=6.7777557

Batch 133640, train_perplexity=920.53937, train_loss=6.8249598

Batch 133650, train_perplexity=861.9056, train_loss=6.7591457

Batch 133660, train_perplexity=928.7109, train_loss=6.8337975

Batch 133670, train_perplexity=1017.7972, train_loss=6.925396

Batch 133680, train_perplexity=794.97174, train_loss=6.6783066

Batch 133690, train_perplexity=958.3074, train_loss=6.8651686

Batch 133700, train_perplexity=923.6039, train_loss=6.8282833

Batch 133710, train_perplexity=911.9708, train_loss=6.815608
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 133720, train_perplexity=832.7634, train_loss=6.7247496

Batch 133730, train_perplexity=898.1823, train_loss=6.800373

Batch 133740, train_perplexity=853.3273, train_loss=6.749143

Batch 133750, train_perplexity=967.6037, train_loss=6.8748226

Batch 133760, train_perplexity=936.6547, train_loss=6.8423147

Batch 133770, train_perplexity=900.2241, train_loss=6.802644

Batch 133780, train_perplexity=970.5005, train_loss=6.877812

Batch 133790, train_perplexity=925.2397, train_loss=6.830053

Batch 133800, train_perplexity=804.49835, train_loss=6.690219

Batch 133810, train_perplexity=912.5929, train_loss=6.81629

Batch 133820, train_perplexity=794.3155, train_loss=6.6774807

Batch 133830, train_perplexity=905.35376, train_loss=6.808326

Batch 133840, train_perplexity=913.3739, train_loss=6.8171453

Batch 133850, train_perplexity=895.73114, train_loss=6.7976403

Batch 133860, train_perplexity=945.4263, train_loss=6.851636

Batch 133870, train_perplexity=933.3528, train_loss=6.8387833

Batch 133880, train_perplexity=877.1417, train_loss=6.7766685

Batch 133890, train_perplexity=905.03955, train_loss=6.8079786

Batch 133900, train_perplexity=855.0, train_loss=6.7511015

Batch 133910, train_perplexity=932.2386, train_loss=6.837589

Batch 133920, train_perplexity=990.4917, train_loss=6.8982015

Batch 133930, train_perplexity=1006.56616, train_loss=6.9143

Batch 133940, train_perplexity=880.4051, train_loss=6.780382

Batch 133950, train_perplexity=893.93695, train_loss=6.795635

Batch 133960, train_perplexity=919.5211, train_loss=6.823853

Batch 133970, train_perplexity=887.3872, train_loss=6.7882814

Batch 133980, train_perplexity=857.0516, train_loss=6.753498

Batch 133990, train_perplexity=869.5359, train_loss=6.7679596

Batch 134000, train_perplexity=943.28107, train_loss=6.8493643

Batch 134010, train_perplexity=954.3237, train_loss=6.861003

Batch 134020, train_perplexity=961.22034, train_loss=6.8682036

Batch 134030, train_perplexity=936.20996, train_loss=6.84184

Batch 134040, train_perplexity=1058.5476, train_loss=6.964653

Batch 134050, train_perplexity=871.2566, train_loss=6.7699366

Batch 134060, train_perplexity=1032.7797, train_loss=6.940009

Batch 134070, train_perplexity=949.48236, train_loss=6.855917

Batch 134080, train_perplexity=815.4714, train_loss=6.7037663

Batch 134090, train_perplexity=881.8147, train_loss=6.781982

Batch 134100, train_perplexity=941.2969, train_loss=6.8472586

Batch 134110, train_perplexity=927.4806, train_loss=6.832472

Batch 134120, train_perplexity=921.31354, train_loss=6.8258004

Batch 134130, train_perplexity=974.331, train_loss=6.881751

Batch 134140, train_perplexity=930.75995, train_loss=6.8360014

Batch 134150, train_perplexity=925.5632, train_loss=6.8304024

Batch 134160, train_perplexity=899.1276, train_loss=6.801425

Batch 134170, train_perplexity=883.09094, train_loss=6.783428

Batch 134180, train_perplexity=903.2159, train_loss=6.8059616

Batch 134190, train_perplexity=942.6902, train_loss=6.8487377

Batch 134200, train_perplexity=854.3219, train_loss=6.750308

Batch 134210, train_perplexity=828.26935, train_loss=6.7193384

Batch 134220, train_perplexity=886.27167, train_loss=6.7870235

Batch 134230, train_perplexity=888.96014, train_loss=6.7900524

Batch 134240, train_perplexity=834.078, train_loss=6.726327

Batch 134250, train_perplexity=849.1732, train_loss=6.744263

Batch 134260, train_perplexity=906.57764, train_loss=6.8096766

Batch 134270, train_perplexity=837.67316, train_loss=6.730628

Batch 134280, train_perplexity=952.3263, train_loss=6.8589077

Batch 134290, train_perplexity=918.25964, train_loss=6.82248

Batch 134300, train_perplexity=970.693, train_loss=6.8780103

Batch 134310, train_perplexity=837.1309, train_loss=6.7299805

Batch 134320, train_perplexity=923.0395, train_loss=6.827672

Batch 134330, train_perplexity=819.0086, train_loss=6.7080946

Batch 134340, train_perplexity=833.099, train_loss=6.7251525

Batch 134350, train_perplexity=847.0658, train_loss=6.7417784

Batch 134360, train_perplexity=844.97614, train_loss=6.7393084

Batch 134370, train_perplexity=827.438, train_loss=6.718334

Batch 134380, train_perplexity=893.33356, train_loss=6.79496

Batch 134390, train_perplexity=859.03674, train_loss=6.7558117

Batch 134400, train_perplexity=906.0888, train_loss=6.8091373

Batch 134410, train_perplexity=781.36334, train_loss=6.6610403

Batch 134420, train_perplexity=941.85, train_loss=6.847846

Batch 134430, train_perplexity=1013.3807, train_loss=6.921047

Batch 134440, train_perplexity=870.676, train_loss=6.76927

Batch 134450, train_perplexity=882.8611, train_loss=6.783168

Batch 134460, train_perplexity=891.6823, train_loss=6.79311

Batch 134470, train_perplexity=844.42267, train_loss=6.738653

Batch 134480, train_perplexity=861.41705, train_loss=6.758579

Batch 134490, train_perplexity=856.08923, train_loss=6.7523746

Batch 134500, train_perplexity=841.2517, train_loss=6.734891

Batch 134510, train_perplexity=814.2905, train_loss=6.702317

Batch 134520, train_perplexity=1010.2687, train_loss=6.9179716

Batch 134530, train_perplexity=925.98035, train_loss=6.830853

Batch 134540, train_perplexity=892.23096, train_loss=6.793725

Batch 134550, train_perplexity=943.88934, train_loss=6.850009

Batch 134560, train_perplexity=892.87787, train_loss=6.79445

Batch 134570, train_perplexity=983.73724, train_loss=6.891359

Batch 134580, train_perplexity=910.9356, train_loss=6.814472

Batch 134590, train_perplexity=836.50006, train_loss=6.7292266

Batch 134600, train_perplexity=959.9085, train_loss=6.866838

Batch 134610, train_perplexity=1017.4124, train_loss=6.925018

Batch 134620, train_perplexity=875.6356, train_loss=6.77495

Batch 134630, train_perplexity=910.2626, train_loss=6.813733

Batch 134640, train_perplexity=940.0983, train_loss=6.8459845

Batch 134650, train_perplexity=849.30884, train_loss=6.744423

Batch 134660, train_perplexity=966.3555, train_loss=6.873532

Batch 134670, train_perplexity=870.307, train_loss=6.768846

Batch 134680, train_perplexity=957.46875, train_loss=6.864293

Batch 134690, train_perplexity=872.0584, train_loss=6.7708564

Batch 134700, train_perplexity=950.0394, train_loss=6.8565035

Batch 134710, train_perplexity=881.6175, train_loss=6.7817583

Batch 134720, train_perplexity=972.7466, train_loss=6.8801236

Batch 134730, train_perplexity=968.03754, train_loss=6.875271

Batch 134740, train_perplexity=898.5327, train_loss=6.800763

Batch 134750, train_perplexity=876.2112, train_loss=6.775607

Batch 134760, train_perplexity=910.99036, train_loss=6.8145323

Batch 134770, train_perplexity=860.41455, train_loss=6.7574143

Batch 134780, train_perplexity=964.5868, train_loss=6.8717

Batch 134790, train_perplexity=987.2123, train_loss=6.894885

Batch 134800, train_perplexity=888.26776, train_loss=6.7892733

Batch 134810, train_perplexity=917.716, train_loss=6.821888

Batch 134820, train_perplexity=1005.9563, train_loss=6.913694

Batch 134830, train_perplexity=917.66437, train_loss=6.8218317

Batch 134840, train_perplexity=878.0598, train_loss=6.7777147

Batch 134850, train_perplexity=948.2083, train_loss=6.854574

Batch 134860, train_perplexity=906.98535, train_loss=6.8101263

Batch 134870, train_perplexity=851.0395, train_loss=6.7464585

Batch 134880, train_perplexity=978.2899, train_loss=6.885806

Batch 134890, train_perplexity=921.1598, train_loss=6.8256335

Batch 134900, train_perplexity=865.4779, train_loss=6.763282

Batch 134910, train_perplexity=855.1313, train_loss=6.751255

Batch 134920, train_perplexity=907.26135, train_loss=6.8104305

Batch 134930, train_perplexity=978.0786, train_loss=6.88559

Batch 134940, train_perplexity=883.17303, train_loss=6.783521

Batch 134950, train_perplexity=896.499, train_loss=6.798497

Batch 134960, train_perplexity=845.8529, train_loss=6.7403455

Batch 134970, train_perplexity=903.4231, train_loss=6.806191

Batch 134980, train_perplexity=866.5879, train_loss=6.7645636

Batch 134990, train_perplexity=814.65405, train_loss=6.7027636

Batch 135000, train_perplexity=935.4264, train_loss=6.8410025

Batch 135010, train_perplexity=895.65985, train_loss=6.7975607

Batch 135020, train_perplexity=1001.0693, train_loss=6.908824
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 135030, train_perplexity=906.751, train_loss=6.809868

Batch 135040, train_perplexity=935.7186, train_loss=6.841315

Batch 135050, train_perplexity=833.21185, train_loss=6.725288

Batch 135060, train_perplexity=892.0055, train_loss=6.7934723

Batch 135070, train_perplexity=910.174, train_loss=6.813636

Batch 135080, train_perplexity=845.91016, train_loss=6.740413

Batch 135090, train_perplexity=826.4333, train_loss=6.717119

Batch 135100, train_perplexity=851.402, train_loss=6.7468843

Batch 135110, train_perplexity=882.7537, train_loss=6.7830462

Batch 135120, train_perplexity=955.6096, train_loss=6.8623495

Batch 135130, train_perplexity=924.09375, train_loss=6.8288136

Batch 135140, train_perplexity=895.7008, train_loss=6.7976065

Batch 135150, train_perplexity=845.7952, train_loss=6.7402773

Batch 135160, train_perplexity=789.28033, train_loss=6.6711216

Batch 135170, train_perplexity=868.1716, train_loss=6.7663894

Batch 135180, train_perplexity=899.61694, train_loss=6.801969

Batch 135190, train_perplexity=885.02673, train_loss=6.785618

Batch 135200, train_perplexity=831.018, train_loss=6.7226515

Batch 135210, train_perplexity=910.5656, train_loss=6.814066

Batch 135220, train_perplexity=1037.2124, train_loss=6.944292

Batch 135230, train_perplexity=897.7499, train_loss=6.7998915

Batch 135240, train_perplexity=871.95026, train_loss=6.7707324

Batch 135250, train_perplexity=817.94824, train_loss=6.706799

Batch 135260, train_perplexity=861.369, train_loss=6.758523

Batch 135270, train_perplexity=897.9314, train_loss=6.8000937

Batch 135280, train_perplexity=949.1954, train_loss=6.8556147

Batch 135290, train_perplexity=909.46515, train_loss=6.8128567

Batch 135300, train_perplexity=890.83826, train_loss=6.792163

Batch 135310, train_perplexity=998.9368, train_loss=6.9066916

Batch 135320, train_perplexity=941.18286, train_loss=6.8471375

Batch 135330, train_perplexity=925.6073, train_loss=6.83045

Batch 135340, train_perplexity=955.6789, train_loss=6.862422

Batch 135350, train_perplexity=924.92346, train_loss=6.829711

Batch 135360, train_perplexity=903.1694, train_loss=6.80591

Batch 135370, train_perplexity=888.1979, train_loss=6.7891946

Batch 135380, train_perplexity=864.4134, train_loss=6.762051

Batch 135390, train_perplexity=903.38214, train_loss=6.8061457

Batch 135400, train_perplexity=956.8964, train_loss=6.863695

Batch 135410, train_perplexity=969.0517, train_loss=6.876318

Batch 135420, train_perplexity=836.8679, train_loss=6.729666

Batch 135430, train_perplexity=934.0803, train_loss=6.8395624

Batch 135440, train_perplexity=915.54803, train_loss=6.819523

Batch 135450, train_perplexity=909.133, train_loss=6.8124914

Batch 135460, train_perplexity=911.55695, train_loss=6.815154

Batch 135470, train_perplexity=895.61115, train_loss=6.7975063

Batch 135480, train_perplexity=934.6163, train_loss=6.840136

Batch 135490, train_perplexity=825.8747, train_loss=6.716443

Batch 135500, train_perplexity=1013.36615, train_loss=6.921033

Batch 135510, train_perplexity=855.57794, train_loss=6.751777

Batch 135520, train_perplexity=849.57336, train_loss=6.7447343

Batch 135530, train_perplexity=927.1768, train_loss=6.8321443

Batch 135540, train_perplexity=881.2477, train_loss=6.7813387

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Batch 135550, train_perplexity=938.09216, train_loss=6.843848

Batch 135560, train_perplexity=913.82874, train_loss=6.817643

Batch 135570, train_perplexity=903.00446, train_loss=6.8057275

Batch 135580, train_perplexity=860.91077, train_loss=6.757991

Batch 135590, train_perplexity=961.81635, train_loss=6.8688235

Batch 135600, train_perplexity=875.65814, train_loss=6.774976

Batch 135610, train_perplexity=896.20496, train_loss=6.798169

Batch 135620, train_perplexity=821.28467, train_loss=6.71087

Batch 135630, train_perplexity=941.8208, train_loss=6.847815

Batch 135640, train_perplexity=871.7337, train_loss=6.770484

Batch 135650, train_perplexity=960.91144, train_loss=6.8678823

Batch 135660, train_perplexity=822.6604, train_loss=6.7125435

Batch 135670, train_perplexity=892.1986, train_loss=6.793689

Batch 135680, train_perplexity=938.0349, train_loss=6.843787

Batch 135690, train_perplexity=937.0903, train_loss=6.8427796

Batch 135700, train_perplexity=820.0743, train_loss=6.709395

Batch 135710, train_perplexity=889.2255, train_loss=6.790351

Batch 135720, train_perplexity=874.40894, train_loss=6.773548

Batch 135730, train_perplexity=894.7391, train_loss=6.796532

Batch 135740, train_perplexity=898.5953, train_loss=6.8008327

Batch 135750, train_perplexity=1036.6522, train_loss=6.943752

Batch 135760, train_perplexity=839.96826, train_loss=6.733364

Batch 135770, train_perplexity=1002.6701, train_loss=6.910422

Batch 135780, train_perplexity=976.2433, train_loss=6.883712

Batch 135790, train_perplexity=905.068, train_loss=6.80801

Batch 135800, train_perplexity=905.0952, train_loss=6.80804

Batch 135810, train_perplexity=905.34424, train_loss=6.8083153

Batch 135820, train_perplexity=924.3476, train_loss=6.829088

Batch 135830, train_perplexity=958.3622, train_loss=6.865226

Batch 135840, train_perplexity=909.93365, train_loss=6.8133717

Batch 135850, train_perplexity=1013.8862, train_loss=6.921546

Batch 135860, train_perplexity=826.14014, train_loss=6.7167645

Batch 135870, train_perplexity=932.2106, train_loss=6.8375587

Batch 135880, train_perplexity=949.10034, train_loss=6.8555145

Batch 135890, train_perplexity=962.4155, train_loss=6.8694463

Batch 135900, train_perplexity=824.7956, train_loss=6.7151356

Batch 135910, train_perplexity=906.6455, train_loss=6.8097515

Batch 135920, train_perplexity=887.6141, train_loss=6.788537

Batch 135930, train_perplexity=952.83185, train_loss=6.8594384

Batch 135940, train_perplexity=886.4534, train_loss=6.7872286

Batch 135950, train_perplexity=935.03577, train_loss=6.8405848

Batch 135960, train_perplexity=911.6491, train_loss=6.815255

Batch 135970, train_perplexity=847.18256, train_loss=6.741916

Batch 135980, train_perplexity=881.60913, train_loss=6.781749

Batch 135990, train_perplexity=997.9837, train_loss=6.905737

Batch 136000, train_perplexity=953.1499, train_loss=6.859772

Batch 136010, train_perplexity=900.66254, train_loss=6.8031306

Batch 136020, train_perplexity=812.2035, train_loss=6.699751

Batch 136030, train_perplexity=970.3205, train_loss=6.8776264

Batch 136040, train_perplexity=922.54315, train_loss=6.827134

Batch 136050, train_perplexity=903.04663, train_loss=6.805774

Batch 136060, train_perplexity=906.55817, train_loss=6.809655

Batch 136070, train_perplexity=926.85364, train_loss=6.8317957

Batch 136080, train_perplexity=933.8096, train_loss=6.8392725

Batch 136090, train_perplexity=876.52167, train_loss=6.7759614

Batch 136100, train_perplexity=915.4922, train_loss=6.819462

Batch 136110, train_perplexity=910.48224, train_loss=6.8139744

Batch 136120, train_perplexity=950.614, train_loss=6.857108

Batch 136130, train_perplexity=911.55347, train_loss=6.8151503

Batch 136140, train_perplexity=910.79224, train_loss=6.814315

Batch 136150, train_perplexity=912.8027, train_loss=6.8165197

Batch 136160, train_perplexity=788.095, train_loss=6.6696186

Batch 136170, train_perplexity=922.6078, train_loss=6.827204

Batch 136180, train_perplexity=892.2765, train_loss=6.793776

Batch 136190, train_perplexity=879.3273, train_loss=6.779157

Batch 136200, train_perplexity=973.05, train_loss=6.8804355

Batch 136210, train_perplexity=954.4698, train_loss=6.861156

Batch 136220, train_perplexity=904.4174, train_loss=6.807291

Batch 136230, train_perplexity=948.03107, train_loss=6.8543873

Batch 136240, train_perplexity=1028.2279, train_loss=6.935592

Batch 136250, train_perplexity=880.32117, train_loss=6.780287

Batch 136260, train_perplexity=878.8582, train_loss=6.7786236

Batch 136270, train_perplexity=867.2295, train_loss=6.7653036
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 136280, train_perplexity=861.85297, train_loss=6.7590847

Batch 136290, train_perplexity=872.5712, train_loss=6.7714443

Batch 136300, train_perplexity=971.1634, train_loss=6.8784947

Batch 136310, train_perplexity=812.6366, train_loss=6.700284

Batch 136320, train_perplexity=839.05676, train_loss=6.7322783

Batch 136330, train_perplexity=953.02356, train_loss=6.8596396

Batch 136340, train_perplexity=828.2054, train_loss=6.719261

Batch 136350, train_perplexity=973.6538, train_loss=6.881056

Batch 136360, train_perplexity=905.504, train_loss=6.8084917

Batch 136370, train_perplexity=892.4658, train_loss=6.793988

Batch 136380, train_perplexity=819.2523, train_loss=6.708392

Batch 136390, train_perplexity=859.34937, train_loss=6.7561755

Batch 136400, train_perplexity=851.5416, train_loss=6.7470484

Batch 136410, train_perplexity=845.6009, train_loss=6.7400475

Batch 136420, train_perplexity=824.47864, train_loss=6.7147512

Batch 136430, train_perplexity=951.0475, train_loss=6.857564

Batch 136440, train_perplexity=887.1198, train_loss=6.78798

Batch 136450, train_perplexity=916.82635, train_loss=6.820918

Batch 136460, train_perplexity=873.23474, train_loss=6.7722044

Batch 136470, train_perplexity=834.0971, train_loss=6.72635

Batch 136480, train_perplexity=916.00134, train_loss=6.820018

Batch 136490, train_perplexity=904.0863, train_loss=6.806925

Batch 136500, train_perplexity=898.1403, train_loss=6.8003263

Batch 136510, train_perplexity=899.83234, train_loss=6.8022084

Batch 136520, train_perplexity=877.63116, train_loss=6.7772264

Batch 136530, train_perplexity=933.97565, train_loss=6.8394504

Batch 136540, train_perplexity=893.52783, train_loss=6.7951775

Batch 136550, train_perplexity=928.3744, train_loss=6.833435

Batch 136560, train_perplexity=914.4477, train_loss=6.8183203

Batch 136570, train_perplexity=901.80005, train_loss=6.804393

Batch 136580, train_perplexity=948.2074, train_loss=6.8545732

Batch 136590, train_perplexity=945.6242, train_loss=6.8518453

Batch 136600, train_perplexity=881.69824, train_loss=6.78185

Batch 136610, train_perplexity=929.97296, train_loss=6.8351555

Batch 136620, train_perplexity=892.8149, train_loss=6.794379

Batch 136630, train_perplexity=930.2524, train_loss=6.835456

Batch 136640, train_perplexity=879.4929, train_loss=6.7793455

Batch 136650, train_perplexity=886.2861, train_loss=6.7870398

Batch 136660, train_perplexity=849.76624, train_loss=6.7449613

Batch 136670, train_perplexity=887.46045, train_loss=6.788364

Batch 136680, train_perplexity=984.42285, train_loss=6.8920555

Batch 136690, train_perplexity=912.1379, train_loss=6.815791

Batch 136700, train_perplexity=960.5619, train_loss=6.8675184

Batch 136710, train_perplexity=903.551, train_loss=6.8063326

Batch 136720, train_perplexity=799.9867, train_loss=6.684595

Batch 136730, train_perplexity=963.3269, train_loss=6.870393

Batch 136740, train_perplexity=949.2044, train_loss=6.855624

Batch 136750, train_perplexity=901.79144, train_loss=6.8043833

Batch 136760, train_perplexity=892.074, train_loss=6.793549

Batch 136770, train_perplexity=841.9235, train_loss=6.735689

Batch 136780, train_perplexity=998.74634, train_loss=6.906501

Batch 136790, train_perplexity=925.14136, train_loss=6.8299465

Batch 136800, train_perplexity=885.0563, train_loss=6.785651

Batch 136810, train_perplexity=922.85815, train_loss=6.8274755

Batch 136820, train_perplexity=954.0871, train_loss=6.860755

Batch 136830, train_perplexity=916.4723, train_loss=6.820532

Batch 136840, train_perplexity=920.64734, train_loss=6.825077

Batch 136850, train_perplexity=890.33374, train_loss=6.7915964

Batch 136860, train_perplexity=917.0113, train_loss=6.82112

Batch 136870, train_perplexity=889.6221, train_loss=6.7907968

Batch 136880, train_perplexity=950.2388, train_loss=6.8567133

Batch 136890, train_perplexity=773.44244, train_loss=6.6508512

Batch 136900, train_perplexity=942.27045, train_loss=6.8482924

Batch 136910, train_perplexity=874.6491, train_loss=6.773823

Batch 136920, train_perplexity=927.9548, train_loss=6.832983

Batch 136930, train_perplexity=873.4188, train_loss=6.772415

Batch 136940, train_perplexity=841.4555, train_loss=6.735133

Batch 136950, train_perplexity=969.66925, train_loss=6.876955

Batch 136960, train_perplexity=868.4469, train_loss=6.7667065

Batch 136970, train_perplexity=888.5609, train_loss=6.789603

Batch 136980, train_perplexity=901.75745, train_loss=6.8043456

Batch 136990, train_perplexity=869.07825, train_loss=6.767433

Batch 137000, train_perplexity=892.4565, train_loss=6.7939777

Batch 137010, train_perplexity=880.91406, train_loss=6.78096

Batch 137020, train_perplexity=931.5068, train_loss=6.8368034

Batch 137030, train_perplexity=874.91693, train_loss=6.774129

Batch 137040, train_perplexity=938.4018, train_loss=6.844178

Batch 137050, train_perplexity=890.8757, train_loss=6.792205

Batch 137060, train_perplexity=922.4653, train_loss=6.8270497

Batch 137070, train_perplexity=901.843, train_loss=6.8044405

Batch 137080, train_perplexity=788.04913, train_loss=6.6695604

Batch 137090, train_perplexity=886.46313, train_loss=6.7872396

Batch 137100, train_perplexity=879.3822, train_loss=6.7792196

Batch 137110, train_perplexity=918.00836, train_loss=6.8222065

Batch 137120, train_perplexity=876.1293, train_loss=6.7755136

Batch 137130, train_perplexity=862.4935, train_loss=6.7598276

Batch 137140, train_perplexity=839.2544, train_loss=6.732514

Batch 137150, train_perplexity=884.2059, train_loss=6.78469

Batch 137160, train_perplexity=882.8223, train_loss=6.783124

Batch 137170, train_perplexity=976.2214, train_loss=6.8836894

Batch 137180, train_perplexity=880.84515, train_loss=6.780882

Batch 137190, train_perplexity=893.82697, train_loss=6.795512

Batch 137200, train_perplexity=969.61096, train_loss=6.876895

Batch 137210, train_perplexity=911.5, train_loss=6.8150916

Batch 137220, train_perplexity=802.4517, train_loss=6.6876717

Batch 137230, train_perplexity=862.4721, train_loss=6.759803

Batch 137240, train_perplexity=991.1309, train_loss=6.8988466

Batch 137250, train_perplexity=958.492, train_loss=6.865361

Batch 137260, train_perplexity=790.2271, train_loss=6.6723204

Batch 137270, train_perplexity=908.4371, train_loss=6.8117256

Batch 137280, train_perplexity=879.779, train_loss=6.7796707

Batch 137290, train_perplexity=919.106, train_loss=6.8234015

Batch 137300, train_perplexity=863.8979, train_loss=6.7614546

Batch 137310, train_perplexity=902.89124, train_loss=6.805602

Batch 137320, train_perplexity=926.47186, train_loss=6.8313837

Batch 137330, train_perplexity=888.7737, train_loss=6.7898426

Batch 137340, train_perplexity=880.3178, train_loss=6.780283

Batch 137350, train_perplexity=857.2224, train_loss=6.7536974

Batch 137360, train_perplexity=932.31464, train_loss=6.8376703

Batch 137370, train_perplexity=908.9024, train_loss=6.8122377

Batch 137380, train_perplexity=815.9253, train_loss=6.704323

Batch 137390, train_perplexity=886.90076, train_loss=6.787733

Batch 137400, train_perplexity=824.9655, train_loss=6.7153416

Batch 137410, train_perplexity=1052.3231, train_loss=6.9587555

Batch 137420, train_perplexity=844.9729, train_loss=6.7393045

Batch 137430, train_perplexity=874.5057, train_loss=6.7736588

Batch 137440, train_perplexity=825.505, train_loss=6.7159953

Batch 137450, train_perplexity=953.49133, train_loss=6.8601303

Batch 137460, train_perplexity=925.30634, train_loss=6.830125

Batch 137470, train_perplexity=823.1281, train_loss=6.713112

Batch 137480, train_perplexity=925.52344, train_loss=6.8303595

Batch 137490, train_perplexity=902.0512, train_loss=6.8046713

Batch 137500, train_perplexity=928.36285, train_loss=6.8334227

Batch 137510, train_perplexity=817.3306, train_loss=6.7060437

Batch 137520, train_perplexity=850.81915, train_loss=6.7461996

Batch 137530, train_perplexity=810.4637, train_loss=6.6976066

Batch 137540, train_perplexity=973.2639, train_loss=6.8806553

Batch 137550, train_perplexity=841.40656, train_loss=6.735075

Batch 137560, train_perplexity=910.9451, train_loss=6.8144827

Batch 137570, train_perplexity=918.9623, train_loss=6.823245

Batch 137580, train_perplexity=904.0134, train_loss=6.806844
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 137590, train_perplexity=872.3474, train_loss=6.771188

Batch 137600, train_perplexity=1000.0921, train_loss=6.9078474

Batch 137610, train_perplexity=917.05194, train_loss=6.821164

Batch 137620, train_perplexity=881.6734, train_loss=6.7818217

Batch 137630, train_perplexity=935.94037, train_loss=6.841552

Batch 137640, train_perplexity=898.1455, train_loss=6.800332

Batch 137650, train_perplexity=936.01, train_loss=6.841626

Batch 137660, train_perplexity=896.8334, train_loss=6.79887

Batch 137670, train_perplexity=898.885, train_loss=6.801155

Batch 137680, train_perplexity=809.9773, train_loss=6.697006

Batch 137690, train_perplexity=1006.2494, train_loss=6.9139853

Batch 137700, train_perplexity=807.9895, train_loss=6.694549

Batch 137710, train_perplexity=864.9952, train_loss=6.762724

Batch 137720, train_perplexity=994.08295, train_loss=6.9018207

Batch 137730, train_perplexity=903.45886, train_loss=6.8062305

Batch 137740, train_perplexity=835.2752, train_loss=6.7277613

Batch 137750, train_perplexity=895.0382, train_loss=6.7968664

Batch 137760, train_perplexity=927.2011, train_loss=6.8321705

Batch 137770, train_perplexity=871.14197, train_loss=6.769805

Batch 137780, train_perplexity=865.4358, train_loss=6.763233

Batch 137790, train_perplexity=923.91754, train_loss=6.828623

Batch 137800, train_perplexity=872.7868, train_loss=6.7716913

Batch 137810, train_perplexity=926.0638, train_loss=6.830943

Batch 137820, train_perplexity=883.99677, train_loss=6.7844534

Batch 137830, train_perplexity=957.7893, train_loss=6.864628

Batch 137840, train_perplexity=990.72174, train_loss=6.8984337

Batch 137850, train_perplexity=968.9722, train_loss=6.876236

Batch 137860, train_perplexity=877.025, train_loss=6.7765355

Batch 137870, train_perplexity=790.45245, train_loss=6.6726055

Batch 137880, train_perplexity=1044.1318, train_loss=6.950941

Batch 137890, train_perplexity=926.40564, train_loss=6.831312

Batch 137900, train_perplexity=955.8767, train_loss=6.862629

Batch 137910, train_perplexity=943.7822, train_loss=6.8498955

Batch 137920, train_perplexity=930.76526, train_loss=6.836007

Batch 137930, train_perplexity=957.4925, train_loss=6.864318

Batch 137940, train_perplexity=934.3467, train_loss=6.8398476

Batch 137950, train_perplexity=871.7229, train_loss=6.7704716

Batch 137960, train_perplexity=838.41724, train_loss=6.731516

Batch 137970, train_perplexity=850.7741, train_loss=6.7461467

Batch 137980, train_perplexity=897.77985, train_loss=6.799925

Batch 137990, train_perplexity=898.13776, train_loss=6.8003235

Batch 138000, train_perplexity=906.89197, train_loss=6.8100233

Batch 138010, train_perplexity=845.22516, train_loss=6.739603

Batch 138020, train_perplexity=863.4877, train_loss=6.7609797

Batch 138030, train_perplexity=828.6383, train_loss=6.719784

Batch 138040, train_perplexity=947.3162, train_loss=6.853633

Batch 138050, train_perplexity=878.8247, train_loss=6.7785854

Batch 138060, train_perplexity=926.29034, train_loss=6.8311877

Batch 138070, train_perplexity=995.7491, train_loss=6.9034953

Batch 138080, train_perplexity=846.06226, train_loss=6.740593

Batch 138090, train_perplexity=924.1784, train_loss=6.828905

Batch 138100, train_perplexity=870.63116, train_loss=6.7692184

Batch 138110, train_perplexity=867.8607, train_loss=6.7660313

Batch 138120, train_perplexity=929.2469, train_loss=6.8343744

Batch 138130, train_perplexity=935.4206, train_loss=6.8409963

Batch 138140, train_perplexity=855.90234, train_loss=6.7521563

Batch 138150, train_perplexity=880.9288, train_loss=6.780977

Batch 138160, train_perplexity=892.4203, train_loss=6.793937

Batch 138170, train_perplexity=852.683, train_loss=6.748388

Batch 138180, train_perplexity=869.6889, train_loss=6.7681355

Batch 138190, train_perplexity=822.2424, train_loss=6.712035

Batch 138200, train_perplexity=873.8412, train_loss=6.7728987

Batch 138210, train_perplexity=966.61176, train_loss=6.873797

Batch 138220, train_perplexity=933.1379, train_loss=6.838553

Batch 138230, train_perplexity=930.1916, train_loss=6.8353906

Batch 138240, train_perplexity=980.7551, train_loss=6.888323

Batch 138250, train_perplexity=891.8779, train_loss=6.7933292

Batch 138260, train_perplexity=925.01074, train_loss=6.8298054

Batch 138270, train_perplexity=867.22864, train_loss=6.7653027

Batch 138280, train_perplexity=960.01605, train_loss=6.86695

Batch 138290, train_perplexity=839.162, train_loss=6.7324038

Batch 138300, train_perplexity=955.31305, train_loss=6.862039

Batch 138310, train_perplexity=911.6022, train_loss=6.8152037

Batch 138320, train_perplexity=921.5873, train_loss=6.8260975

Batch 138330, train_perplexity=884.5225, train_loss=6.785048

Batch 138340, train_perplexity=947.3577, train_loss=6.853677

Batch 138350, train_perplexity=858.4381, train_loss=6.7551146

Batch 138360, train_perplexity=955.39825, train_loss=6.8621283

Batch 138370, train_perplexity=914.1687, train_loss=6.818015

Batch 138380, train_perplexity=836.3369, train_loss=6.7290316

Batch 138390, train_perplexity=977.2614, train_loss=6.884754

Batch 138400, train_perplexity=908.2292, train_loss=6.8114967

Batch 138410, train_perplexity=833.1042, train_loss=6.7251587

Batch 138420, train_perplexity=888.39825, train_loss=6.78942

Batch 138430, train_perplexity=895.55347, train_loss=6.797442

Batch 138440, train_perplexity=961.02826, train_loss=6.868004

Batch 138450, train_perplexity=904.5865, train_loss=6.807478

Batch 138460, train_perplexity=889.0331, train_loss=6.7901344

Batch 138470, train_perplexity=1030.3143, train_loss=6.937619

Batch 138480, train_perplexity=862.72217, train_loss=6.7600927

Batch 138490, train_perplexity=884.2965, train_loss=6.7847924

Batch 138500, train_perplexity=908.0486, train_loss=6.811298

Batch 138510, train_perplexity=901.27, train_loss=6.803805

Batch 138520, train_perplexity=842.68585, train_loss=6.736594

Batch 138530, train_perplexity=899.0084, train_loss=6.8012924

Batch 138540, train_perplexity=822.18195, train_loss=6.7119617

Batch 138550, train_perplexity=801.1617, train_loss=6.686063

Batch 138560, train_perplexity=922.6619, train_loss=6.827263

Batch 138570, train_perplexity=817.2344, train_loss=6.705926

Batch 138580, train_perplexity=862.45935, train_loss=6.759788

Batch 138590, train_perplexity=916.5641, train_loss=6.820632

Batch 138600, train_perplexity=955.5951, train_loss=6.8623343

Batch 138610, train_perplexity=968.0578, train_loss=6.875292

Batch 138620, train_perplexity=819.51294, train_loss=6.70871

Batch 138630, train_perplexity=877.85095, train_loss=6.777477

Batch 138640, train_perplexity=840.10046, train_loss=6.7335215

Batch 138650, train_perplexity=953.7423, train_loss=6.8603935

Batch 138660, train_perplexity=984.2022, train_loss=6.8918314

Batch 138670, train_perplexity=1035.8187, train_loss=6.9429474

Batch 138680, train_perplexity=927.77606, train_loss=6.8327904

Batch 138690, train_perplexity=946.805, train_loss=6.853093

Batch 138700, train_perplexity=881.5965, train_loss=6.7817345

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Batch 138710, train_perplexity=764.9698, train_loss=6.6398363

Batch 138720, train_perplexity=848.5219, train_loss=6.743496

Batch 138730, train_perplexity=940.88135, train_loss=6.846817

Batch 138740, train_perplexity=927.8751, train_loss=6.832897

Batch 138750, train_perplexity=969.83386, train_loss=6.877125

Batch 138760, train_perplexity=941.1981, train_loss=6.8471537

Batch 138770, train_perplexity=970.962, train_loss=6.8782873

Batch 138780, train_perplexity=901.8211, train_loss=6.804416

Batch 138790, train_perplexity=884.5567, train_loss=6.7850866

Batch 138800, train_perplexity=949.83105, train_loss=6.856284

Batch 138810, train_perplexity=1009.697, train_loss=6.9174056

Batch 138820, train_perplexity=976.2628, train_loss=6.883732

Batch 138830, train_perplexity=857.5127, train_loss=6.754036
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 138840, train_perplexity=852.0079, train_loss=6.747596

Batch 138850, train_perplexity=941.67664, train_loss=6.847662

Batch 138860, train_perplexity=843.1782, train_loss=6.7371783

Batch 138870, train_perplexity=888.905, train_loss=6.7899904

Batch 138880, train_perplexity=784.15, train_loss=6.6646004

Batch 138890, train_perplexity=882.0132, train_loss=6.782207

Batch 138900, train_perplexity=850.85, train_loss=6.746236

Batch 138910, train_perplexity=773.8678, train_loss=6.651401

Batch 138920, train_perplexity=937.03357, train_loss=6.842719

Batch 138930, train_perplexity=922.43317, train_loss=6.827015

Batch 138940, train_perplexity=924.4076, train_loss=6.829153

Batch 138950, train_perplexity=841.7296, train_loss=6.735459

Batch 138960, train_perplexity=887.6094, train_loss=6.788532

Batch 138970, train_perplexity=787.1699, train_loss=6.668444

Batch 138980, train_perplexity=879.16125, train_loss=6.7789683

Batch 138990, train_perplexity=936.506, train_loss=6.842156

Batch 139000, train_perplexity=847.70746, train_loss=6.7425356

Batch 139010, train_perplexity=906.49506, train_loss=6.8095856

Batch 139020, train_perplexity=904.2355, train_loss=6.80709

Batch 139030, train_perplexity=1028.9847, train_loss=6.936328

Batch 139040, train_perplexity=911.79346, train_loss=6.8154135

Batch 139050, train_perplexity=864.14056, train_loss=6.7617354

Batch 139060, train_perplexity=997.3961, train_loss=6.905148

Batch 139070, train_perplexity=1009.84, train_loss=6.917547

Batch 139080, train_perplexity=884.29016, train_loss=6.7847853

Batch 139090, train_perplexity=991.2789, train_loss=6.898996

Batch 139100, train_perplexity=1036.9795, train_loss=6.9440675

Batch 139110, train_perplexity=977.3719, train_loss=6.884867

Batch 139120, train_perplexity=954.78705, train_loss=6.8614883

Batch 139130, train_perplexity=950.8371, train_loss=6.8573427

Batch 139140, train_perplexity=931.3611, train_loss=6.836647

Batch 139150, train_perplexity=923.0782, train_loss=6.827714

Batch 139160, train_perplexity=880.2838, train_loss=6.7802444

Batch 139170, train_perplexity=882.9478, train_loss=6.783266

Batch 139180, train_perplexity=980.5522, train_loss=6.888116

Batch 139190, train_perplexity=896.41864, train_loss=6.7984076

Batch 139200, train_perplexity=844.47424, train_loss=6.738714

Batch 139210, train_perplexity=894.16797, train_loss=6.7958937

Batch 139220, train_perplexity=914.5955, train_loss=6.818482

Batch 139230, train_perplexity=881.1363, train_loss=6.7812123

Batch 139240, train_perplexity=976.1455, train_loss=6.8836117

Batch 139250, train_perplexity=954.10803, train_loss=6.860777

Batch 139260, train_perplexity=973.05975, train_loss=6.8804455

Batch 139270, train_perplexity=864.4257, train_loss=6.7620654

Batch 139280, train_perplexity=904.47003, train_loss=6.807349

Batch 139290, train_perplexity=883.67474, train_loss=6.784089

Batch 139300, train_perplexity=944.2459, train_loss=6.8503866

Batch 139310, train_perplexity=918.1292, train_loss=6.822338

Batch 139320, train_perplexity=895.3703, train_loss=6.7972374

Batch 139330, train_perplexity=959.25006, train_loss=6.866152

Batch 139340, train_perplexity=783.3129, train_loss=6.6635323

Batch 139350, train_perplexity=808.804, train_loss=6.6955566

Batch 139360, train_perplexity=826.14014, train_loss=6.7167645

Batch 139370, train_perplexity=838.2342, train_loss=6.7312975

Batch 139380, train_perplexity=818.8255, train_loss=6.707871

Batch 139390, train_perplexity=827.8555, train_loss=6.7188387

Batch 139400, train_perplexity=851.9713, train_loss=6.747553

Batch 139410, train_perplexity=940.55475, train_loss=6.84647

Batch 139420, train_perplexity=874.92523, train_loss=6.7741385

Batch 139430, train_perplexity=781.537, train_loss=6.6612625

Batch 139440, train_perplexity=805.054, train_loss=6.6909094

Batch 139450, train_perplexity=993.3461, train_loss=6.901079

Batch 139460, train_perplexity=852.10583, train_loss=6.7477107

Batch 139470, train_perplexity=952.4975, train_loss=6.8590875

Batch 139480, train_perplexity=834.8611, train_loss=6.7272654

Batch 139490, train_perplexity=876.5672, train_loss=6.7760134

Batch 139500, train_perplexity=897.4828, train_loss=6.799594

Batch 139510, train_perplexity=825.4621, train_loss=6.7159433

Batch 139520, train_perplexity=877.77686, train_loss=6.7773924

Batch 139530, train_perplexity=841.5278, train_loss=6.735219

Batch 139540, train_perplexity=909.67114, train_loss=6.813083

Batch 139550, train_perplexity=989.20074, train_loss=6.8968973

Batch 139560, train_perplexity=871.4768, train_loss=6.7701893

Batch 139570, train_perplexity=894.2285, train_loss=6.7959614

Batch 139580, train_perplexity=848.60046, train_loss=6.7435884

Batch 139590, train_perplexity=857.08264, train_loss=6.7535343

Batch 139600, train_perplexity=901.2309, train_loss=6.8037615

Batch 139610, train_perplexity=905.62146, train_loss=6.8086214

Batch 139620, train_perplexity=858.8852, train_loss=6.7556353

Batch 139630, train_perplexity=840.4875, train_loss=6.733982

Batch 139640, train_perplexity=968.068, train_loss=6.8753023

Batch 139650, train_perplexity=992.2677, train_loss=6.899993

Batch 139660, train_perplexity=977.2344, train_loss=6.8847265

Batch 139670, train_perplexity=990.8266, train_loss=6.8985395

Batch 139680, train_perplexity=833.51227, train_loss=6.7256484

Batch 139690, train_perplexity=904.2971, train_loss=6.807158

Batch 139700, train_perplexity=920.04083, train_loss=6.824418

Batch 139710, train_perplexity=887.0661, train_loss=6.7879195

Batch 139720, train_perplexity=871.75946, train_loss=6.7705135

Batch 139730, train_perplexity=852.20984, train_loss=6.747833

Batch 139740, train_perplexity=830.16016, train_loss=6.7216187

Batch 139750, train_perplexity=864.5981, train_loss=6.7622647

Batch 139760, train_perplexity=837.8138, train_loss=6.730796

Batch 139770, train_perplexity=915.10284, train_loss=6.8190365

Batch 139780, train_perplexity=910.1836, train_loss=6.8136463

Batch 139790, train_perplexity=851.1608, train_loss=6.746601

Batch 139800, train_perplexity=853.0201, train_loss=6.748783

Batch 139810, train_perplexity=915.21155, train_loss=6.819155

Batch 139820, train_perplexity=825.02527, train_loss=6.715414

Batch 139830, train_perplexity=910.73535, train_loss=6.8142524

Batch 139840, train_perplexity=871.7158, train_loss=6.7704635

Batch 139850, train_perplexity=892.7876, train_loss=6.7943487

Batch 139860, train_perplexity=963.92377, train_loss=6.871012

Batch 139870, train_perplexity=910.7462, train_loss=6.8142643

Batch 139880, train_perplexity=866.65405, train_loss=6.76464

Batch 139890, train_perplexity=899.3626, train_loss=6.8016863

Batch 139900, train_perplexity=875.4185, train_loss=6.774702

Batch 139910, train_perplexity=896.11096, train_loss=6.798064

Batch 139920, train_perplexity=788.36487, train_loss=6.669961

Batch 139930, train_perplexity=976.9809, train_loss=6.884467

Batch 139940, train_perplexity=892.2199, train_loss=6.7937126

Batch 139950, train_perplexity=937.9267, train_loss=6.843672

Batch 139960, train_perplexity=847.32434, train_loss=6.7420835

Batch 139970, train_perplexity=877.12494, train_loss=6.7766495

Batch 139980, train_perplexity=896.6294, train_loss=6.7986426

Batch 139990, train_perplexity=824.35284, train_loss=6.7145987

Batch 140000, train_perplexity=992.6411, train_loss=6.900369

Batch 140010, train_perplexity=1020.64905, train_loss=6.928194

Batch 140020, train_perplexity=951.23975, train_loss=6.857766

Batch 140030, train_perplexity=892.61395, train_loss=6.794154

Batch 140040, train_perplexity=796.59204, train_loss=6.6803427

Batch 140050, train_perplexity=915.4188, train_loss=6.8193817

Batch 140060, train_perplexity=862.853, train_loss=6.7602444

Batch 140070, train_perplexity=944.46747, train_loss=6.850621

Batch 140080, train_perplexity=820.10284, train_loss=6.7094297

Batch 140090, train_perplexity=917.82104, train_loss=6.8220024

Batch 140100, train_perplexity=937.20557, train_loss=6.8429027

Batch 140110, train_perplexity=833.51105, train_loss=6.725647

Batch 140120, train_perplexity=953.63226, train_loss=6.860278

Batch 140130, train_perplexity=933.1183, train_loss=6.838532

Batch 140140, train_perplexity=892.2271, train_loss=6.7937207
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 140150, train_perplexity=861.3448, train_loss=6.758495

Batch 140160, train_perplexity=805.64, train_loss=6.691637

Batch 140170, train_perplexity=974.8738, train_loss=6.882308

Batch 140180, train_perplexity=763.5503, train_loss=6.637979

Batch 140190, train_perplexity=854.56635, train_loss=6.750594

Batch 140200, train_perplexity=880.8788, train_loss=6.78092

Batch 140210, train_perplexity=917.86127, train_loss=6.8220463

Batch 140220, train_perplexity=903.4136, train_loss=6.8061805

Batch 140230, train_perplexity=885.3289, train_loss=6.7859592

Batch 140240, train_perplexity=904.9714, train_loss=6.8079033

Batch 140250, train_perplexity=833.5218, train_loss=6.72566

Batch 140260, train_perplexity=928.0256, train_loss=6.8330593

Batch 140270, train_perplexity=817.347, train_loss=6.7060637

Batch 140280, train_perplexity=902.0409, train_loss=6.80466

Batch 140290, train_perplexity=896.37506, train_loss=6.798359

Batch 140300, train_perplexity=913.5194, train_loss=6.8173046

Batch 140310, train_perplexity=805.708, train_loss=6.6917214

Batch 140320, train_perplexity=908.41974, train_loss=6.8117065

Batch 140330, train_perplexity=833.62115, train_loss=6.725779

Batch 140340, train_perplexity=935.68427, train_loss=6.841278

Batch 140350, train_perplexity=859.9876, train_loss=6.756918

Batch 140360, train_perplexity=1069.7576, train_loss=6.9751873

Batch 140370, train_perplexity=919.9768, train_loss=6.8243484

Batch 140380, train_perplexity=839.4866, train_loss=6.7327905

Batch 140390, train_perplexity=889.7498, train_loss=6.7909403

Batch 140400, train_perplexity=889.34045, train_loss=6.79048

Batch 140410, train_perplexity=885.6351, train_loss=6.786305

Batch 140420, train_perplexity=922.2872, train_loss=6.8268566

Batch 140430, train_perplexity=858.78937, train_loss=6.7555237

Batch 140440, train_perplexity=861.55426, train_loss=6.758738

Batch 140450, train_perplexity=914.1626, train_loss=6.8180084

Batch 140460, train_perplexity=837.6196, train_loss=6.730564

Batch 140470, train_perplexity=891.5981, train_loss=6.7930155

Batch 140480, train_perplexity=834.8961, train_loss=6.7273073

Batch 140490, train_perplexity=907.96765, train_loss=6.8112087

Batch 140500, train_perplexity=859.8843, train_loss=6.756798

Batch 140510, train_perplexity=860.6419, train_loss=6.7576785

Batch 140520, train_perplexity=857.71674, train_loss=6.754274

Batch 140530, train_perplexity=962.5895, train_loss=6.869627

Batch 140540, train_perplexity=876.9192, train_loss=6.776415

Batch 140550, train_perplexity=877.9434, train_loss=6.777582

Batch 140560, train_perplexity=883.71356, train_loss=6.784133

Batch 140570, train_perplexity=836.1615, train_loss=6.7288218

Batch 140580, train_perplexity=800.46826, train_loss=6.685197

Batch 140590, train_perplexity=870.73083, train_loss=6.769333

Batch 140600, train_perplexity=779.2291, train_loss=6.658305

Batch 140610, train_perplexity=1008.18494, train_loss=6.915907

Batch 140620, train_perplexity=888.38385, train_loss=6.789404

Batch 140630, train_perplexity=972.0608, train_loss=6.8794184

Batch 140640, train_perplexity=900.23444, train_loss=6.802655

Batch 140650, train_perplexity=832.86664, train_loss=6.7248735

Batch 140660, train_perplexity=961.0814, train_loss=6.868059

Batch 140670, train_perplexity=838.5304, train_loss=6.731651

Batch 140680, train_perplexity=972.23236, train_loss=6.879595

Batch 140690, train_perplexity=826.25165, train_loss=6.7168994

Batch 140700, train_perplexity=934.5998, train_loss=6.8401184

Batch 140710, train_perplexity=844.2238, train_loss=6.7384176

Batch 140720, train_perplexity=896.8291, train_loss=6.7988653

Batch 140730, train_perplexity=857.4967, train_loss=6.7540174

Batch 140740, train_perplexity=862.3545, train_loss=6.7596664

Batch 140750, train_perplexity=856.4984, train_loss=6.7528524

Batch 140760, train_perplexity=958.3549, train_loss=6.865218

Batch 140770, train_perplexity=812.20233, train_loss=6.6997495

Batch 140780, train_perplexity=980.1446, train_loss=6.8877

Batch 140790, train_perplexity=853.6626, train_loss=6.749536

Batch 140800, train_perplexity=843.61816, train_loss=6.7377

Batch 140810, train_perplexity=941.68744, train_loss=6.8476734

Batch 140820, train_perplexity=909.564, train_loss=6.8129654

Batch 140830, train_perplexity=774.5216, train_loss=6.6522455

Batch 140840, train_perplexity=911.0216, train_loss=6.8145666

Batch 140850, train_perplexity=944.2999, train_loss=6.850444

Batch 140860, train_perplexity=857.7155, train_loss=6.7542725

Batch 140870, train_perplexity=899.2297, train_loss=6.8015385

Batch 140880, train_perplexity=870.91974, train_loss=6.76955

Batch 140890, train_perplexity=872.61536, train_loss=6.771495

Batch 140900, train_perplexity=872.471, train_loss=6.7713294

Batch 140910, train_perplexity=909.9371, train_loss=6.8133755

Batch 140920, train_perplexity=843.61774, train_loss=6.7376995

Batch 140930, train_perplexity=868.27924, train_loss=6.7665133

Batch 140940, train_perplexity=880.11633, train_loss=6.780054

Batch 140950, train_perplexity=900.83215, train_loss=6.803319

Batch 140960, train_perplexity=884.65375, train_loss=6.7851963

Batch 140970, train_perplexity=922.3945, train_loss=6.826973

Batch 140980, train_perplexity=786.9331, train_loss=6.6681433

Batch 140990, train_perplexity=857.65826, train_loss=6.7542057

Batch 141000, train_perplexity=892.00464, train_loss=6.7934713

Batch 141010, train_perplexity=889.08734, train_loss=6.7901955

Batch 141020, train_perplexity=907.4802, train_loss=6.810672

Batch 141030, train_perplexity=836.3114, train_loss=6.729001

Batch 141040, train_perplexity=853.53033, train_loss=6.749381

Batch 141050, train_perplexity=920.664, train_loss=6.825095

Batch 141060, train_perplexity=909.5883, train_loss=6.812992

Batch 141070, train_perplexity=956.12286, train_loss=6.8628864

Batch 141080, train_perplexity=925.6196, train_loss=6.8304634

Batch 141090, train_perplexity=927.84064, train_loss=6.83286

Batch 141100, train_perplexity=938.2022, train_loss=6.8439655

Batch 141110, train_perplexity=871.2267, train_loss=6.769902

Batch 141120, train_perplexity=946.16815, train_loss=6.8524203

Batch 141130, train_perplexity=844.436, train_loss=6.738669

Batch 141140, train_perplexity=877.9296, train_loss=6.7775664

Batch 141150, train_perplexity=878.9785, train_loss=6.7787604

Batch 141160, train_perplexity=924.89655, train_loss=6.829682

Batch 141170, train_perplexity=836.75415, train_loss=6.7295303

Batch 141180, train_perplexity=897.3056, train_loss=6.7993965

Batch 141190, train_perplexity=943.1713, train_loss=6.849248

Batch 141200, train_perplexity=941.9084, train_loss=6.847908

Batch 141210, train_perplexity=895.73755, train_loss=6.7976475

Batch 141220, train_perplexity=872.8401, train_loss=6.7717524

Batch 141230, train_perplexity=881.85547, train_loss=6.782028

Batch 141240, train_perplexity=948.6619, train_loss=6.8550525

Batch 141250, train_perplexity=892.10077, train_loss=6.793579

Batch 141260, train_perplexity=794.68823, train_loss=6.67795

Batch 141270, train_perplexity=919.2923, train_loss=6.823604

Batch 141280, train_perplexity=793.3313, train_loss=6.676241

Batch 141290, train_perplexity=777.64935, train_loss=6.6562757

Batch 141300, train_perplexity=862.2789, train_loss=6.7595787

Batch 141310, train_perplexity=925.4405, train_loss=6.83027

Batch 141320, train_perplexity=898.84515, train_loss=6.8011107

Batch 141330, train_perplexity=972.6232, train_loss=6.879997

Batch 141340, train_perplexity=915.11597, train_loss=6.819051

Batch 141350, train_perplexity=1016.8852, train_loss=6.9244995

Batch 141360, train_perplexity=915.9921, train_loss=6.820008

Batch 141370, train_perplexity=832.31244, train_loss=6.724208

Batch 141380, train_perplexity=819.46173, train_loss=6.7086477

Batch 141390, train_perplexity=847.41284, train_loss=6.742188

Batch 141400, train_perplexity=837.8433, train_loss=6.730831

Batch 141410, train_perplexity=901.5713, train_loss=6.804139

Batch 141420, train_perplexity=839.4661, train_loss=6.732766

Batch 141430, train_perplexity=921.5693, train_loss=6.826078

Batch 141440, train_perplexity=930.0386, train_loss=6.835226

Batch 141450, train_perplexity=846.83966, train_loss=6.7415113
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 141460, train_perplexity=878.45685, train_loss=6.778167

Batch 141470, train_perplexity=846.09814, train_loss=6.7406354

Batch 141480, train_perplexity=843.7666, train_loss=6.737876

Batch 141490, train_perplexity=846.69104, train_loss=6.741336

Batch 141500, train_perplexity=915.2347, train_loss=6.8191805

Batch 141510, train_perplexity=812.4568, train_loss=6.7000628

Batch 141520, train_perplexity=829.3277, train_loss=6.7206154

Batch 141530, train_perplexity=856.99146, train_loss=6.753428

Batch 141540, train_perplexity=1029.8202, train_loss=6.9371395

Batch 141550, train_perplexity=901.0599, train_loss=6.8035717

Batch 141560, train_perplexity=958.8714, train_loss=6.865757

Batch 141570, train_perplexity=883.88885, train_loss=6.7843313

Batch 141580, train_perplexity=752.12164, train_loss=6.622898

Batch 141590, train_perplexity=847.1539, train_loss=6.7418823

Batch 141600, train_perplexity=917.50775, train_loss=6.821661

Batch 141610, train_perplexity=936.4899, train_loss=6.842139

Batch 141620, train_perplexity=890.06805, train_loss=6.791298

Batch 141630, train_perplexity=882.9512, train_loss=6.78327

Batch 141640, train_perplexity=882.6447, train_loss=6.7829227

Batch 141650, train_perplexity=852.22485, train_loss=6.7478504

Batch 141660, train_perplexity=931.7911, train_loss=6.8371086

Batch 141670, train_perplexity=837.68353, train_loss=6.7306404

Batch 141680, train_perplexity=961.35876, train_loss=6.8683476

Batch 141690, train_perplexity=855.4996, train_loss=6.7516856

Batch 141700, train_perplexity=855.3295, train_loss=6.751487

Batch 141710, train_perplexity=841.77014, train_loss=6.735507

Batch 141720, train_perplexity=900.7162, train_loss=6.80319

Batch 141730, train_perplexity=847.3809, train_loss=6.7421503

Batch 141740, train_perplexity=832.6518, train_loss=6.7246156

Batch 141750, train_perplexity=792.69604, train_loss=6.67544

Batch 141760, train_perplexity=856.7909, train_loss=6.753194

Batch 141770, train_perplexity=893.1534, train_loss=6.7947583

Batch 141780, train_perplexity=880.14276, train_loss=6.780084

Batch 141790, train_perplexity=825.9267, train_loss=6.716506

Batch 141800, train_perplexity=791.0603, train_loss=6.673374

Batch 141810, train_perplexity=913.7664, train_loss=6.817575

Batch 141820, train_perplexity=898.63043, train_loss=6.800872

Batch 141830, train_perplexity=883.8821, train_loss=6.7843237

Batch 141840, train_perplexity=957.9465, train_loss=6.864792

Batch 141850, train_perplexity=850.9011, train_loss=6.746296

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Batch 141860, train_perplexity=832.5482, train_loss=6.724491

Batch 141870, train_perplexity=915.054, train_loss=6.818983

Batch 141880, train_perplexity=877.1509, train_loss=6.776679

Batch 141890, train_perplexity=933.07513, train_loss=6.8384857

Batch 141900, train_perplexity=857.5985, train_loss=6.754136

Batch 141910, train_perplexity=799.9329, train_loss=6.684528

Batch 141920, train_perplexity=878.5926, train_loss=6.7783213

Batch 141930, train_perplexity=875.48865, train_loss=6.774782

Batch 141940, train_perplexity=826.0645, train_loss=6.716673

Batch 141950, train_perplexity=907.148, train_loss=6.8103056

Batch 141960, train_perplexity=864.60425, train_loss=6.762272

Batch 141970, train_perplexity=953.5345, train_loss=6.8601756

Batch 141980, train_perplexity=911.59564, train_loss=6.8151965

Batch 141990, train_perplexity=844.13684, train_loss=6.7383146

Batch 142000, train_perplexity=921.11584, train_loss=6.825586

Batch 142010, train_perplexity=948.00397, train_loss=6.8543587

Batch 142020, train_perplexity=908.86774, train_loss=6.8121996

Batch 142030, train_perplexity=894.1441, train_loss=6.795867

Batch 142040, train_perplexity=879.7773, train_loss=6.779669

Batch 142050, train_perplexity=824.51166, train_loss=6.7147913

Batch 142060, train_perplexity=918.1541, train_loss=6.8223653

Batch 142070, train_perplexity=845.39764, train_loss=6.739807

Batch 142080, train_perplexity=883.97736, train_loss=6.7844315

Batch 142090, train_perplexity=865.638, train_loss=6.763467

Batch 142100, train_perplexity=805.99774, train_loss=6.692081

Batch 142110, train_perplexity=874.3797, train_loss=6.7735147

Batch 142120, train_perplexity=829.5033, train_loss=6.720827

Batch 142130, train_perplexity=933.11786, train_loss=6.8385315

Batch 142140, train_perplexity=852.12366, train_loss=6.7477317

Batch 142150, train_perplexity=864.90155, train_loss=6.7626157

Batch 142160, train_perplexity=951.7161, train_loss=6.858267

Batch 142170, train_perplexity=920.4586, train_loss=6.824872

Batch 142180, train_perplexity=910.10284, train_loss=6.8135576

Batch 142190, train_perplexity=900.05676, train_loss=6.802458

Batch 142200, train_perplexity=910.553, train_loss=6.814052

Batch 142210, train_perplexity=782.37744, train_loss=6.6623373

Batch 142220, train_perplexity=809.73785, train_loss=6.6967106

Batch 142230, train_perplexity=972.2949, train_loss=6.879659

Batch 142240, train_perplexity=851.71545, train_loss=6.7472525

Batch 142250, train_perplexity=840.34485, train_loss=6.7338123

Batch 142260, train_perplexity=905.4824, train_loss=6.808468

Batch 142270, train_perplexity=821.3215, train_loss=6.7109146

Batch 142280, train_perplexity=813.8864, train_loss=6.701821

Batch 142290, train_perplexity=872.19025, train_loss=6.7710075

Batch 142300, train_perplexity=878.4719, train_loss=6.778184

Batch 142310, train_perplexity=902.1282, train_loss=6.8047566

Batch 142320, train_perplexity=954.25, train_loss=6.8609257

Batch 142330, train_perplexity=872.9067, train_loss=6.7718287

Batch 142340, train_perplexity=815.74054, train_loss=6.7040963

Batch 142350, train_perplexity=923.9845, train_loss=6.8286953

Batch 142360, train_perplexity=850.9393, train_loss=6.7463408

Batch 142370, train_perplexity=827.45776, train_loss=6.718358

Batch 142380, train_perplexity=880.0026, train_loss=6.779925

Batch 142390, train_perplexity=907.804, train_loss=6.8110285

Batch 142400, train_perplexity=819.6048, train_loss=6.7088223

Batch 142410, train_perplexity=837.90967, train_loss=6.7309103

Batch 142420, train_perplexity=859.65875, train_loss=6.7565355

Batch 142430, train_perplexity=828.8952, train_loss=6.7200937

Batch 142440, train_perplexity=891.5781, train_loss=6.792993

Batch 142450, train_perplexity=874.50146, train_loss=6.773654

Batch 142460, train_perplexity=878.1838, train_loss=6.777856

Batch 142470, train_perplexity=870.32654, train_loss=6.7688684

Batch 142480, train_perplexity=928.2823, train_loss=6.833336

Batch 142490, train_perplexity=893.22705, train_loss=6.794841

Batch 142500, train_perplexity=877.6764, train_loss=6.777278

Batch 142510, train_perplexity=817.74896, train_loss=6.7065554

Batch 142520, train_perplexity=903.81647, train_loss=6.8066263

Batch 142530, train_perplexity=907.76587, train_loss=6.8109865

Batch 142540, train_perplexity=855.9431, train_loss=6.752204

Batch 142550, train_perplexity=914.7957, train_loss=6.818701

Batch 142560, train_perplexity=924.39435, train_loss=6.8291388

Batch 142570, train_perplexity=828.4408, train_loss=6.7195454

Batch 142580, train_perplexity=934.898, train_loss=6.8404374

Batch 142590, train_perplexity=822.1526, train_loss=6.711926

Batch 142600, train_perplexity=851.2014, train_loss=6.746649

Batch 142610, train_perplexity=876.8724, train_loss=6.7763615

Batch 142620, train_perplexity=983.7185, train_loss=6.89134

Batch 142630, train_perplexity=848.6551, train_loss=6.743653

Batch 142640, train_perplexity=848.71375, train_loss=6.743722

Batch 142650, train_perplexity=873.09235, train_loss=6.7720413

Batch 142660, train_perplexity=940.38617, train_loss=6.8462906

Batch 142670, train_perplexity=844.0145, train_loss=6.7381697

Batch 142680, train_perplexity=875.7993, train_loss=6.775137

Batch 142690, train_perplexity=880.57556, train_loss=6.7805758

Batch 142700, train_perplexity=984.56555, train_loss=6.8922005
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 142710, train_perplexity=812.0664, train_loss=6.699582

Batch 142720, train_perplexity=865.4779, train_loss=6.763282

Batch 142730, train_perplexity=834.4878, train_loss=6.726818

Batch 142740, train_perplexity=854.72205, train_loss=6.7507763

Batch 142750, train_perplexity=880.2179, train_loss=6.7801695

Batch 142760, train_perplexity=961.0063, train_loss=6.867981

Batch 142770, train_perplexity=846.83154, train_loss=6.741502

Batch 142780, train_perplexity=860.27795, train_loss=6.7572556

Batch 142790, train_perplexity=848.8975, train_loss=6.7439384

Batch 142800, train_perplexity=785.08765, train_loss=6.6657953

Batch 142810, train_perplexity=893.28796, train_loss=6.794909

Batch 142820, train_perplexity=884.34247, train_loss=6.7848444

Batch 142830, train_perplexity=909.6065, train_loss=6.813012

Batch 142840, train_perplexity=886.4881, train_loss=6.7872677

Batch 142850, train_perplexity=889.2179, train_loss=6.7903423

Batch 142860, train_perplexity=894.9503, train_loss=6.796768

Batch 142870, train_perplexity=1003.01776, train_loss=6.9107685

Batch 142880, train_perplexity=906.3304, train_loss=6.809404

Batch 142890, train_perplexity=940.50183, train_loss=6.8464136

Batch 142900, train_perplexity=871.70166, train_loss=6.7704473

Batch 142910, train_perplexity=952.75415, train_loss=6.859357

Batch 142920, train_perplexity=873.18604, train_loss=6.7721486

Batch 142930, train_perplexity=775.2465, train_loss=6.653181

Batch 142940, train_perplexity=981.2903, train_loss=6.8888683

Batch 142950, train_perplexity=904.1583, train_loss=6.8070045

Batch 142960, train_perplexity=845.98236, train_loss=6.7404985

Batch 142970, train_perplexity=984.0793, train_loss=6.8917065

Batch 142980, train_perplexity=927.7672, train_loss=6.832781

Batch 142990, train_perplexity=845.7904, train_loss=6.7402716

Batch 143000, train_perplexity=871.33057, train_loss=6.7700214

Batch 143010, train_perplexity=888.3377, train_loss=6.789352

Batch 143020, train_perplexity=850.22864, train_loss=6.7455053

Batch 143030, train_perplexity=875.8561, train_loss=6.775202

Batch 143040, train_perplexity=873.52625, train_loss=6.772538

Batch 143050, train_perplexity=914.7085, train_loss=6.8186054

Batch 143060, train_perplexity=805.7468, train_loss=6.6917696

Batch 143070, train_perplexity=898.22943, train_loss=6.8004255

Batch 143080, train_perplexity=865.26373, train_loss=6.7630343

Batch 143090, train_perplexity=766.41046, train_loss=6.641718

Batch 143100, train_perplexity=892.01227, train_loss=6.79348

Batch 143110, train_perplexity=901.54596, train_loss=6.804111

Batch 143120, train_perplexity=893.21173, train_loss=6.7948236

Batch 143130, train_perplexity=897.9973, train_loss=6.800167

Batch 143140, train_perplexity=866.5573, train_loss=6.7645283

Batch 143150, train_perplexity=909.31946, train_loss=6.8126965

Batch 143160, train_perplexity=801.13495, train_loss=6.6860294

Batch 143170, train_perplexity=850.0491, train_loss=6.745294

Batch 143180, train_perplexity=896.0093, train_loss=6.7979507

Batch 143190, train_perplexity=797.7476, train_loss=6.6817923

Batch 143200, train_perplexity=853.6597, train_loss=6.7495327

Batch 143210, train_perplexity=887.3669, train_loss=6.7882586

Batch 143220, train_perplexity=864.52014, train_loss=6.7621746

Batch 143230, train_perplexity=872.4606, train_loss=6.7713175

Batch 143240, train_perplexity=956.3654, train_loss=6.86314

Batch 143250, train_perplexity=905.4194, train_loss=6.8083982

Batch 143260, train_perplexity=929.76764, train_loss=6.8349347

Batch 143270, train_perplexity=971.39496, train_loss=6.878733

Batch 143280, train_perplexity=948.7049, train_loss=6.855098

Batch 143290, train_perplexity=848.8627, train_loss=6.7438974

Batch 143300, train_perplexity=770.8835, train_loss=6.647537

Batch 143310, train_perplexity=750.2215, train_loss=6.6203685

Batch 143320, train_perplexity=897.6779, train_loss=6.7998114

Batch 143330, train_perplexity=910.7883, train_loss=6.8143106

Batch 143340, train_perplexity=903.5002, train_loss=6.8062763

Batch 143350, train_perplexity=905.26917, train_loss=6.8082323

Batch 143360, train_perplexity=872.08374, train_loss=6.7708855

Batch 143370, train_perplexity=987.3648, train_loss=6.8950396

Batch 143380, train_perplexity=936.76056, train_loss=6.8424277

Batch 143390, train_perplexity=956.09735, train_loss=6.8628597

Batch 143400, train_perplexity=878.11926, train_loss=6.7777824

Batch 143410, train_perplexity=827.84607, train_loss=6.7188272

Batch 143420, train_perplexity=760.3226, train_loss=6.633743

Batch 143430, train_perplexity=862.16785, train_loss=6.75945

Batch 143440, train_perplexity=916.8635, train_loss=6.8209586

Batch 143450, train_perplexity=879.3059, train_loss=6.779133

Batch 143460, train_perplexity=869.4173, train_loss=6.767823

Batch 143470, train_perplexity=774.5157, train_loss=6.652238

Batch 143480, train_perplexity=863.15674, train_loss=6.7605963

Batch 143490, train_perplexity=818.0165, train_loss=6.7068825

Batch 143500, train_perplexity=829.3127, train_loss=6.7205973

Batch 143510, train_perplexity=991.5365, train_loss=6.8992558

Batch 143520, train_perplexity=943.27295, train_loss=6.8493557

Batch 143530, train_perplexity=797.051, train_loss=6.6809187

Batch 143540, train_perplexity=917.47534, train_loss=6.8216257

Batch 143550, train_perplexity=885.7394, train_loss=6.7864227

Batch 143560, train_perplexity=947.5529, train_loss=6.853883

Batch 143570, train_perplexity=947.98, train_loss=6.8543334

Batch 143580, train_perplexity=870.3601, train_loss=6.768907

Batch 143590, train_perplexity=972.34174, train_loss=6.8797073

Batch 143600, train_perplexity=904.2044, train_loss=6.8070555

Batch 143610, train_perplexity=902.13293, train_loss=6.804762

Batch 143620, train_perplexity=730.03253, train_loss=6.593089

Batch 143630, train_perplexity=865.15607, train_loss=6.76291

Batch 143640, train_perplexity=885.3218, train_loss=6.785951

Batch 143650, train_perplexity=853.15027, train_loss=6.7489357

Batch 143660, train_perplexity=836.0203, train_loss=6.728653

Batch 143670, train_perplexity=872.38947, train_loss=6.771236

Batch 143680, train_perplexity=894.1117, train_loss=6.7958307

Batch 143690, train_perplexity=876.85986, train_loss=6.776347

Batch 143700, train_perplexity=834.9037, train_loss=6.7273164

Batch 143710, train_perplexity=884.598, train_loss=6.7851334

Batch 143720, train_perplexity=889.61743, train_loss=6.7907915

Batch 143730, train_perplexity=840.8876, train_loss=6.734458

Batch 143740, train_perplexity=920.34845, train_loss=6.8247523

Batch 143750, train_perplexity=905.17114, train_loss=6.808124

Batch 143760, train_perplexity=966.6016, train_loss=6.8737864

Batch 143770, train_perplexity=888.15936, train_loss=6.789151

Batch 143780, train_perplexity=909.9562, train_loss=6.8133965

Batch 143790, train_perplexity=815.71216, train_loss=6.7040615

Batch 143800, train_perplexity=883.33606, train_loss=6.7837057

Batch 143810, train_perplexity=796.6961, train_loss=6.6804733

Batch 143820, train_perplexity=861.51605, train_loss=6.7586937

Batch 143830, train_perplexity=788.1303, train_loss=6.6696634

Batch 143840, train_perplexity=798.6017, train_loss=6.6828623

Batch 143850, train_perplexity=886.11957, train_loss=6.786852

Batch 143860, train_perplexity=904.90576, train_loss=6.807831

Batch 143870, train_perplexity=846.25836, train_loss=6.7408247

Batch 143880, train_perplexity=835.14856, train_loss=6.7276096

Batch 143890, train_perplexity=836.64526, train_loss=6.7294

Batch 143900, train_perplexity=948.4846, train_loss=6.8548656

Batch 143910, train_perplexity=871.6672, train_loss=6.7704077

Batch 143920, train_perplexity=1022.4051, train_loss=6.929913

Batch 143930, train_perplexity=851.5891, train_loss=6.747104

Batch 143940, train_perplexity=815.9432, train_loss=6.7043447

Batch 143950, train_perplexity=909.3663, train_loss=6.812748

Batch 143960, train_perplexity=817.0906, train_loss=6.70575

Batch 143970, train_perplexity=872.0979, train_loss=6.7709017

Batch 143980, train_perplexity=882.0481, train_loss=6.7822466

Batch 143990, train_perplexity=825.178, train_loss=6.715599

Batch 144000, train_perplexity=936.6422, train_loss=6.8423014

Batch 144010, train_perplexity=887.9472, train_loss=6.7889123
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 144020, train_perplexity=904.3002, train_loss=6.8071613

Batch 144030, train_perplexity=820.5644, train_loss=6.7099924

Batch 144040, train_perplexity=835.99243, train_loss=6.7286196

Batch 144050, train_perplexity=820.4517, train_loss=6.709855

Batch 144060, train_perplexity=855.2047, train_loss=6.751341

Batch 144070, train_perplexity=832.114, train_loss=6.7239695

Batch 144080, train_perplexity=807.13275, train_loss=6.693488

Batch 144090, train_perplexity=859.92975, train_loss=6.7568507

Batch 144100, train_perplexity=864.08496, train_loss=6.761671

Batch 144110, train_perplexity=888.2068, train_loss=6.7892046

Batch 144120, train_perplexity=876.98737, train_loss=6.7764926

Batch 144130, train_perplexity=877.26886, train_loss=6.7768135

Batch 144140, train_perplexity=897.1644, train_loss=6.799239

Batch 144150, train_perplexity=922.77106, train_loss=6.827381

Batch 144160, train_perplexity=856.0672, train_loss=6.752349

Batch 144170, train_perplexity=848.8384, train_loss=6.743869

Batch 144180, train_perplexity=920.1444, train_loss=6.8245306

Batch 144190, train_perplexity=886.4864, train_loss=6.787266

Batch 144200, train_perplexity=829.7351, train_loss=6.7211065

Batch 144210, train_perplexity=915.3036, train_loss=6.819256

Batch 144220, train_perplexity=860.38135, train_loss=6.7573757

Batch 144230, train_perplexity=827.0459, train_loss=6.71786

Batch 144240, train_perplexity=816.7813, train_loss=6.7053714

Batch 144250, train_perplexity=914.681, train_loss=6.8185754

Batch 144260, train_perplexity=780.72205, train_loss=6.660219

Batch 144270, train_perplexity=841.1739, train_loss=6.7347984

Batch 144280, train_perplexity=902.0409, train_loss=6.80466

Batch 144290, train_perplexity=843.50995, train_loss=6.7375717

Batch 144300, train_perplexity=894.21747, train_loss=6.795949

Batch 144310, train_perplexity=961.71313, train_loss=6.8687162

Batch 144320, train_perplexity=898.181, train_loss=6.8003716

Batch 144330, train_perplexity=932.713, train_loss=6.8380976

Batch 144340, train_perplexity=855.01715, train_loss=6.7511215

Batch 144350, train_perplexity=922.6826, train_loss=6.8272853

Batch 144360, train_perplexity=951.0919, train_loss=6.8576107

Batch 144370, train_perplexity=809.92554, train_loss=6.6969423

Batch 144380, train_perplexity=896.4939, train_loss=6.7984915

Batch 144390, train_perplexity=802.60895, train_loss=6.6878676

Batch 144400, train_perplexity=836.34174, train_loss=6.7290373

Batch 144410, train_perplexity=803.07025, train_loss=6.688442

Batch 144420, train_perplexity=877.1793, train_loss=6.7767115

Batch 144430, train_perplexity=939.8419, train_loss=6.8457117

Batch 144440, train_perplexity=720.6307, train_loss=6.580127

Batch 144450, train_perplexity=893.23773, train_loss=6.7948527

Batch 144460, train_perplexity=885.6127, train_loss=6.7862797

Batch 144470, train_perplexity=814.31616, train_loss=6.7023487

Batch 144480, train_perplexity=884.32056, train_loss=6.7848196

Batch 144490, train_perplexity=956.2943, train_loss=6.8630657

Batch 144500, train_perplexity=893.68976, train_loss=6.7953587

Batch 144510, train_perplexity=857.6059, train_loss=6.7541447

Batch 144520, train_perplexity=840.957, train_loss=6.7345405

Batch 144530, train_perplexity=855.30664, train_loss=6.75146

Batch 144540, train_perplexity=920.59424, train_loss=6.8250194

Batch 144550, train_perplexity=875.1556, train_loss=6.7744017

Batch 144560, train_perplexity=836.2923, train_loss=6.728978

Batch 144570, train_perplexity=874.0575, train_loss=6.773146

Batch 144580, train_perplexity=848.2121, train_loss=6.7431307

Batch 144590, train_perplexity=863.23413, train_loss=6.760686

Batch 144600, train_perplexity=812.2321, train_loss=6.699786

Batch 144610, train_perplexity=858.9638, train_loss=6.755727

Batch 144620, train_perplexity=851.4072, train_loss=6.7468905

Batch 144630, train_perplexity=839.11035, train_loss=6.7323422

Batch 144640, train_perplexity=891.8732, train_loss=6.793324

Batch 144650, train_perplexity=822.5043, train_loss=6.7123537

Batch 144660, train_perplexity=832.08105, train_loss=6.72393

Batch 144670, train_perplexity=836.0255, train_loss=6.728659

Batch 144680, train_perplexity=860.76544, train_loss=6.757822

Batch 144690, train_perplexity=903.9277, train_loss=6.8067493

Batch 144700, train_perplexity=931.6431, train_loss=6.83695

Batch 144710, train_perplexity=786.08295, train_loss=6.6670623

Batch 144720, train_perplexity=860.9025, train_loss=6.7579813

Batch 144730, train_perplexity=781.9277, train_loss=6.661762

Batch 144740, train_perplexity=911.8117, train_loss=6.8154335

Batch 144750, train_perplexity=805.9082, train_loss=6.69197

Batch 144760, train_perplexity=859.07117, train_loss=6.7558517

Batch 144770, train_perplexity=895.09454, train_loss=6.7969294

Batch 144780, train_perplexity=821.6435, train_loss=6.7113066

Batch 144790, train_perplexity=838.9503, train_loss=6.7321515

Batch 144800, train_perplexity=845.1647, train_loss=6.7395315

Batch 144810, train_perplexity=887.6293, train_loss=6.788554

Batch 144820, train_perplexity=847.49, train_loss=6.742279

Batch 144830, train_perplexity=890.58, train_loss=6.791873

Batch 144840, train_perplexity=860.0015, train_loss=6.756934

Batch 144850, train_perplexity=793.94666, train_loss=6.6770163

Batch 144860, train_perplexity=833.1248, train_loss=6.7251835

Batch 144870, train_perplexity=952.79956, train_loss=6.8594046

Batch 144880, train_perplexity=863.45807, train_loss=6.7609453

Batch 144890, train_perplexity=952.9218, train_loss=6.859533

Batch 144900, train_perplexity=1010.83826, train_loss=6.918535

Batch 144910, train_perplexity=854.69714, train_loss=6.750747

Batch 144920, train_perplexity=893.5287, train_loss=6.7951784

Batch 144930, train_perplexity=909.8798, train_loss=6.8133125

Batch 144940, train_perplexity=844.93665, train_loss=6.7392616

Batch 144950, train_perplexity=825.6691, train_loss=6.716194

Batch 144960, train_perplexity=895.4267, train_loss=6.7973003

Batch 144970, train_perplexity=841.93317, train_loss=6.7357006

Batch 144980, train_perplexity=941.4971, train_loss=6.847471

Batch 144990, train_perplexity=924.6086, train_loss=6.8293705

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Batch 145000, train_perplexity=890.558, train_loss=6.791848

Batch 145010, train_perplexity=866.40283, train_loss=6.76435

Batch 145020, train_perplexity=884.9558, train_loss=6.7855377

Batch 145030, train_perplexity=840.33484, train_loss=6.7338004

Batch 145040, train_perplexity=945.6391, train_loss=6.851861

Batch 145050, train_perplexity=864.756, train_loss=6.7624474

Batch 145060, train_perplexity=816.52545, train_loss=6.705058

Batch 145070, train_perplexity=874.9561, train_loss=6.7741737

Batch 145080, train_perplexity=854.8207, train_loss=6.7508917

Batch 145090, train_perplexity=850.31866, train_loss=6.745611

Batch 145100, train_perplexity=894.7967, train_loss=6.7965965

Batch 145110, train_perplexity=853.9981, train_loss=6.749929

Batch 145120, train_perplexity=872.0555, train_loss=6.770853

Batch 145130, train_perplexity=885.0411, train_loss=6.785634

Batch 145140, train_perplexity=837.9456, train_loss=6.730953

Batch 145150, train_perplexity=764.4133, train_loss=6.6391087

Batch 145160, train_perplexity=894.5079, train_loss=6.7962737

Batch 145170, train_perplexity=888.2242, train_loss=6.789224

Batch 145180, train_perplexity=892.6642, train_loss=6.7942104

Batch 145190, train_perplexity=852.6419, train_loss=6.7483397

Batch 145200, train_perplexity=949.9334, train_loss=6.856392

Batch 145210, train_perplexity=800.0035, train_loss=6.684616

Batch 145220, train_perplexity=890.9479, train_loss=6.792286

Batch 145230, train_perplexity=816.7268, train_loss=6.7053046

Batch 145240, train_perplexity=782.1313, train_loss=6.6620226

Batch 145250, train_perplexity=830.9641, train_loss=6.7225866

Batch 145260, train_perplexity=862.3512, train_loss=6.7596626
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 145270, train_perplexity=946.4962, train_loss=6.852767

Batch 145280, train_perplexity=883.1078, train_loss=6.7834473

Batch 145290, train_perplexity=880.2112, train_loss=6.780162

Batch 145300, train_perplexity=880.7893, train_loss=6.7808185

Batch 145310, train_perplexity=921.24634, train_loss=6.8257275

Batch 145320, train_perplexity=891.4642, train_loss=6.7928653

Batch 145330, train_perplexity=963.21893, train_loss=6.8702807

Batch 145340, train_perplexity=801.21594, train_loss=6.6861305

Batch 145350, train_perplexity=797.0822, train_loss=6.680958

Batch 145360, train_perplexity=824.66656, train_loss=6.714979

Batch 145370, train_perplexity=923.8858, train_loss=6.8285885

Batch 145380, train_perplexity=792.938, train_loss=6.675745

Batch 145390, train_perplexity=852.49963, train_loss=6.7481728

Batch 145400, train_perplexity=919.9522, train_loss=6.8243217

Batch 145410, train_perplexity=979.341, train_loss=6.88688

Batch 145420, train_perplexity=942.63806, train_loss=6.8486824

Batch 145430, train_perplexity=906.15967, train_loss=6.8092155

Batch 145440, train_perplexity=957.8368, train_loss=6.8646774

Batch 145450, train_perplexity=836.6688, train_loss=6.7294283

Batch 145460, train_perplexity=943.4952, train_loss=6.8495913

Batch 145470, train_perplexity=853.569, train_loss=6.7494264

Batch 145480, train_perplexity=847.59106, train_loss=6.7423983

Batch 145490, train_perplexity=848.38684, train_loss=6.7433367

Batch 145500, train_perplexity=785.8401, train_loss=6.6667533

Batch 145510, train_perplexity=845.1937, train_loss=6.739566

Batch 145520, train_perplexity=791.4685, train_loss=6.67389

Batch 145530, train_perplexity=852.7407, train_loss=6.7484555

Batch 145540, train_perplexity=816.10315, train_loss=6.7045407

Batch 145550, train_perplexity=873.31134, train_loss=6.772292

Batch 145560, train_perplexity=892.2148, train_loss=6.793707

Batch 145570, train_perplexity=786.9388, train_loss=6.6681504

Batch 145580, train_perplexity=868.6279, train_loss=6.766915

Batch 145590, train_perplexity=802.8317, train_loss=6.688145

Batch 145600, train_perplexity=857.5544, train_loss=6.7540846

Batch 145610, train_perplexity=822.41254, train_loss=6.712242

Batch 145620, train_perplexity=910.54083, train_loss=6.8140388

Batch 145630, train_perplexity=958.4312, train_loss=6.865298

Batch 145640, train_perplexity=793.0469, train_loss=6.6758823

Batch 145650, train_perplexity=815.57056, train_loss=6.703888

Batch 145660, train_perplexity=878.2759, train_loss=6.777961

Batch 145670, train_perplexity=970.042, train_loss=6.8773394

Batch 145680, train_perplexity=802.741, train_loss=6.688032

Batch 145690, train_perplexity=922.38654, train_loss=6.8269644

Batch 145700, train_perplexity=831.5635, train_loss=6.7233076

Batch 145710, train_perplexity=858.5216, train_loss=6.755212

Batch 145720, train_perplexity=891.1063, train_loss=6.792464

Batch 145730, train_perplexity=927.4151, train_loss=6.8324013

Batch 145740, train_perplexity=918.99207, train_loss=6.8232775

Batch 145750, train_perplexity=867.37714, train_loss=6.765474

Batch 145760, train_perplexity=895.83154, train_loss=6.7977524

Batch 145770, train_perplexity=849.3429, train_loss=6.744463

Batch 145780, train_perplexity=839.96625, train_loss=6.7333617

Batch 145790, train_perplexity=949.50726, train_loss=6.855943

Batch 145800, train_perplexity=885.3247, train_loss=6.7859545

Batch 145810, train_perplexity=896.1178, train_loss=6.798072

Batch 145820, train_perplexity=842.5042, train_loss=6.7363787

Batch 145830, train_perplexity=901.7962, train_loss=6.8043885

Batch 145840, train_perplexity=896.4503, train_loss=6.798443

Batch 145850, train_perplexity=713.2016, train_loss=6.569764

Batch 145860, train_perplexity=807.2336, train_loss=6.693613

Batch 145870, train_perplexity=833.12085, train_loss=6.7251787

Batch 145880, train_perplexity=776.6766, train_loss=6.655024

Batch 145890, train_perplexity=835.73096, train_loss=6.728307

Batch 145900, train_perplexity=767.0686, train_loss=6.642576

Batch 145910, train_perplexity=878.2034, train_loss=6.7778783

Batch 145920, train_perplexity=842.9953, train_loss=6.7369614

Batch 145930, train_perplexity=869.5429, train_loss=6.7679677

Batch 145940, train_perplexity=898.537, train_loss=6.800768

Batch 145950, train_perplexity=869.0915, train_loss=6.7674484

Batch 145960, train_perplexity=923.74133, train_loss=6.828432

Batch 145970, train_perplexity=817.2874, train_loss=6.705991

Batch 145980, train_perplexity=921.1848, train_loss=6.8256607

Batch 145990, train_perplexity=971.93427, train_loss=6.879288

Batch 146000, train_perplexity=924.74, train_loss=6.8295126

Batch 146010, train_perplexity=798.07025, train_loss=6.6821966

Batch 146020, train_perplexity=921.4915, train_loss=6.8259935

Batch 146030, train_perplexity=867.27826, train_loss=6.76536

Batch 146040, train_perplexity=906.98663, train_loss=6.8101277

Batch 146050, train_perplexity=893.1248, train_loss=6.7947264

Batch 146060, train_perplexity=824.63354, train_loss=6.714939

Batch 146070, train_perplexity=842.1367, train_loss=6.7359424

Batch 146080, train_perplexity=804.24365, train_loss=6.6899023

Batch 146090, train_perplexity=810.7544, train_loss=6.697965

Batch 146100, train_perplexity=813.1071, train_loss=6.700863

Batch 146110, train_perplexity=844.3446, train_loss=6.7385607

Batch 146120, train_perplexity=937.8659, train_loss=6.843607

Batch 146130, train_perplexity=893.7145, train_loss=6.7953863

Batch 146140, train_perplexity=878.1829, train_loss=6.777855

Batch 146150, train_perplexity=841.0207, train_loss=6.7346163

Batch 146160, train_perplexity=860.7962, train_loss=6.757858

Batch 146170, train_perplexity=851.88763, train_loss=6.7474546

Batch 146180, train_perplexity=864.9548, train_loss=6.762677

Batch 146190, train_perplexity=931.3691, train_loss=6.8366556

Batch 146200, train_perplexity=854.49994, train_loss=6.7505164

Batch 146210, train_perplexity=864.75226, train_loss=6.762443

Batch 146220, train_perplexity=859.0318, train_loss=6.755806

Batch 146230, train_perplexity=914.15125, train_loss=6.817996

Batch 146240, train_perplexity=841.88336, train_loss=6.7356415

Batch 146250, train_perplexity=782.2387, train_loss=6.66216

Batch 146260, train_perplexity=849.6815, train_loss=6.7448616

Batch 146270, train_perplexity=828.63873, train_loss=6.7197843

Batch 146280, train_perplexity=808.5094, train_loss=6.6951923

Batch 146290, train_perplexity=983.93475, train_loss=6.8915596

Batch 146300, train_perplexity=904.7164, train_loss=6.8076215

Batch 146310, train_perplexity=789.47986, train_loss=6.6713743

Batch 146320, train_perplexity=844.8665, train_loss=6.7391787

Batch 146330, train_perplexity=812.83887, train_loss=6.700533

Batch 146340, train_perplexity=846.74756, train_loss=6.7414026

Batch 146350, train_perplexity=885.8213, train_loss=6.786515

Batch 146360, train_perplexity=792.73535, train_loss=6.6754894

Batch 146370, train_perplexity=799.51764, train_loss=6.6840086

Batch 146380, train_perplexity=963.3719, train_loss=6.8704395

Batch 146390, train_perplexity=754.0975, train_loss=6.6255217

Batch 146400, train_perplexity=905.2135, train_loss=6.808171

Batch 146410, train_perplexity=864.9135, train_loss=6.7626295

Batch 146420, train_perplexity=892.3135, train_loss=6.7938175

Batch 146430, train_perplexity=885.55945, train_loss=6.7862196

Batch 146440, train_perplexity=763.8544, train_loss=6.638377

Batch 146450, train_perplexity=824.2915, train_loss=6.7145243

Batch 146460, train_perplexity=922.4147, train_loss=6.826995

Batch 146470, train_perplexity=811.759, train_loss=6.6992035

Batch 146480, train_perplexity=905.8193, train_loss=6.80884

Batch 146490, train_perplexity=759.20966, train_loss=6.632278

Batch 146500, train_perplexity=803.9607, train_loss=6.6895504

Batch 146510, train_perplexity=872.6403, train_loss=6.7715235

Batch 146520, train_perplexity=900.0001, train_loss=6.802395

Batch 146530, train_perplexity=842.156, train_loss=6.7359653

Batch 146540, train_perplexity=910.39453, train_loss=6.813878

Batch 146550, train_perplexity=881.53766, train_loss=6.7816677

Batch 146560, train_perplexity=823.4045, train_loss=6.7134476

Batch 146570, train_perplexity=895.2055, train_loss=6.7970533
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 146580, train_perplexity=866.1566, train_loss=6.7640657

Batch 146590, train_perplexity=961.37665, train_loss=6.8683662

Batch 146600, train_perplexity=838.97156, train_loss=6.732177

Batch 146610, train_perplexity=863.19543, train_loss=6.760641

Batch 146620, train_perplexity=822.6777, train_loss=6.7125645

Batch 146630, train_perplexity=910.2131, train_loss=6.8136787

Batch 146640, train_perplexity=828.0308, train_loss=6.7190504

Batch 146650, train_perplexity=895.5488, train_loss=6.7974367

Batch 146660, train_perplexity=836.986, train_loss=6.7298074

Batch 146670, train_perplexity=800.8802, train_loss=6.6857114

Batch 146680, train_perplexity=825.9046, train_loss=6.7164793

Batch 146690, train_perplexity=864.0965, train_loss=6.7616844

Batch 146700, train_perplexity=855.9027, train_loss=6.7521567

Batch 146710, train_perplexity=869.2076, train_loss=6.767582

Batch 146720, train_perplexity=951.07104, train_loss=6.857589

Batch 146730, train_perplexity=817.0065, train_loss=6.705647

Batch 146740, train_perplexity=874.5032, train_loss=6.773656

Batch 146750, train_perplexity=938.6587, train_loss=6.844452

Batch 146760, train_perplexity=882.915, train_loss=6.783229

Batch 146770, train_perplexity=858.87616, train_loss=6.755625

Batch 146780, train_perplexity=843.3756, train_loss=6.7374125

Batch 146790, train_perplexity=832.77295, train_loss=6.724761

Batch 146800, train_perplexity=837.85535, train_loss=6.7308455

Batch 146810, train_perplexity=891.34686, train_loss=6.7927337

Batch 146820, train_perplexity=829.914, train_loss=6.721322

Batch 146830, train_perplexity=881.7453, train_loss=6.7819033

Batch 146840, train_perplexity=841.0119, train_loss=6.734606

Batch 146850, train_perplexity=849.0432, train_loss=6.74411

Batch 146860, train_perplexity=864.8121, train_loss=6.762512

Batch 146870, train_perplexity=853.3781, train_loss=6.7492027

Batch 146880, train_perplexity=851.7146, train_loss=6.7472515

Batch 146890, train_perplexity=815.2463, train_loss=6.7034903

Batch 146900, train_perplexity=881.8412, train_loss=6.782012

Batch 146910, train_perplexity=846.84, train_loss=6.741512

Batch 146920, train_perplexity=820.08795, train_loss=6.7094116

Batch 146930, train_perplexity=934.66, train_loss=6.840183

Batch 146940, train_perplexity=894.9606, train_loss=6.7967796

Batch 146950, train_perplexity=823.6307, train_loss=6.713722

Batch 146960, train_perplexity=774.963, train_loss=6.6528153

Batch 146970, train_perplexity=849.47003, train_loss=6.7446127

Batch 146980, train_perplexity=773.98364, train_loss=6.651551

Batch 146990, train_perplexity=908.0438, train_loss=6.8112926

Batch 147000, train_perplexity=877.5609, train_loss=6.7771463

Batch 147010, train_perplexity=823.8758, train_loss=6.71402

Batch 147020, train_perplexity=853.16656, train_loss=6.748955

Batch 147030, train_perplexity=844.03705, train_loss=6.7381964

Batch 147040, train_perplexity=853.49615, train_loss=6.749341

Batch 147050, train_perplexity=887.49554, train_loss=6.7884035

Batch 147060, train_perplexity=898.48004, train_loss=6.8007045

Batch 147070, train_perplexity=897.04297, train_loss=6.7991037

Batch 147080, train_perplexity=934.0901, train_loss=6.839573

Batch 147090, train_perplexity=893.38464, train_loss=6.7950172

Batch 147100, train_perplexity=854.98206, train_loss=6.7510805

Batch 147110, train_perplexity=885.3011, train_loss=6.785928

Batch 147120, train_perplexity=818.57056, train_loss=6.7075596

Batch 147130, train_perplexity=836.2261, train_loss=6.728899

Batch 147140, train_perplexity=924.8886, train_loss=6.8296733

Batch 147150, train_perplexity=800.0691, train_loss=6.684698

Batch 147160, train_perplexity=809.5549, train_loss=6.6964846

Batch 147170, train_perplexity=874.67035, train_loss=6.773847

Batch 147180, train_perplexity=798.92236, train_loss=6.683264

Batch 147190, train_perplexity=870.47925, train_loss=6.769044

Batch 147200, train_perplexity=858.4917, train_loss=6.755177

Batch 147210, train_perplexity=879.5982, train_loss=6.779465

Batch 147220, train_perplexity=869.7366, train_loss=6.7681904

Batch 147230, train_perplexity=878.3563, train_loss=6.7780523

Batch 147240, train_perplexity=901.68225, train_loss=6.804262

Batch 147250, train_perplexity=805.5909, train_loss=6.691576

Batch 147260, train_perplexity=1007.18695, train_loss=6.9149165

Batch 147270, train_perplexity=865.97327, train_loss=6.763854

Batch 147280, train_perplexity=885.0162, train_loss=6.785606

Batch 147290, train_perplexity=845.8606, train_loss=6.7403545

Batch 147300, train_perplexity=838.10504, train_loss=6.7311435

Batch 147310, train_perplexity=943.69763, train_loss=6.849806

Batch 147320, train_perplexity=765.5883, train_loss=6.6406446

Batch 147330, train_perplexity=787.0953, train_loss=6.6683493

Batch 147340, train_perplexity=872.3732, train_loss=6.7712173

Batch 147350, train_perplexity=938.8637, train_loss=6.8446703

Batch 147360, train_perplexity=855.26794, train_loss=6.751415

Batch 147370, train_perplexity=834.8615, train_loss=6.727266

Batch 147380, train_perplexity=806.9915, train_loss=6.693313

Batch 147390, train_perplexity=788.7593, train_loss=6.670461

Batch 147400, train_perplexity=850.4375, train_loss=6.745751

Batch 147410, train_perplexity=930.28564, train_loss=6.8354917

Batch 147420, train_perplexity=829.6584, train_loss=6.721014

Batch 147430, train_perplexity=858.678, train_loss=6.755394

Batch 147440, train_perplexity=918.7852, train_loss=6.8230524

Batch 147450, train_perplexity=855.52203, train_loss=6.751712

Batch 147460, train_perplexity=885.06805, train_loss=6.7856646

Batch 147470, train_perplexity=902.4711, train_loss=6.8051367

Batch 147480, train_perplexity=832.8956, train_loss=6.7249084

Batch 147490, train_perplexity=891.70184, train_loss=6.793132

Batch 147500, train_perplexity=786.5152, train_loss=6.667612

Batch 147510, train_perplexity=882.8653, train_loss=6.7831726

Batch 147520, train_perplexity=760.9588, train_loss=6.634579

Batch 147530, train_perplexity=870.1024, train_loss=6.768611

Batch 147540, train_perplexity=855.45636, train_loss=6.751635

Batch 147550, train_perplexity=923.15967, train_loss=6.827802

Batch 147560, train_perplexity=871.2587, train_loss=6.769939

Batch 147570, train_perplexity=873.17975, train_loss=6.7721415

Batch 147580, train_perplexity=849.334, train_loss=6.7444525

Batch 147590, train_perplexity=926.85144, train_loss=6.8317933

Batch 147600, train_perplexity=846.2874, train_loss=6.740859

Batch 147610, train_perplexity=952.9559, train_loss=6.8595686

Batch 147620, train_perplexity=834.60315, train_loss=6.7269564

Batch 147630, train_perplexity=853.099, train_loss=6.7488756

Batch 147640, train_perplexity=892.0204, train_loss=6.793489

Batch 147650, train_perplexity=812.3286, train_loss=6.699905

Batch 147660, train_perplexity=870.74164, train_loss=6.7693453

Batch 147670, train_perplexity=791.28516, train_loss=6.6736584

Batch 147680, train_perplexity=882.6199, train_loss=6.7828946

Batch 147690, train_perplexity=790.32886, train_loss=6.672449

Batch 147700, train_perplexity=821.6678, train_loss=6.711336

Batch 147710, train_perplexity=987.2791, train_loss=6.894953

Batch 147720, train_perplexity=907.26434, train_loss=6.810434

Batch 147730, train_perplexity=875.08295, train_loss=6.7743187

Batch 147740, train_perplexity=836.0578, train_loss=6.728698

Batch 147750, train_perplexity=922.8384, train_loss=6.827454

Batch 147760, train_perplexity=777.2634, train_loss=6.6557794

Batch 147770, train_perplexity=787.60736, train_loss=6.6689997

Batch 147780, train_perplexity=905.0067, train_loss=6.8079424

Batch 147790, train_perplexity=797.143, train_loss=6.681034

Batch 147800, train_perplexity=792.04315, train_loss=6.674616

Batch 147810, train_perplexity=808.3194, train_loss=6.6949573

Batch 147820, train_perplexity=867.32043, train_loss=6.7654085

Batch 147830, train_perplexity=863.0456, train_loss=6.7604675

Batch 147840, train_perplexity=904.1087, train_loss=6.8069496

Batch 147850, train_perplexity=854.3516, train_loss=6.750343

Batch 147860, train_perplexity=855.4959, train_loss=6.7516813

Batch 147870, train_perplexity=909.2093, train_loss=6.8125753

Batch 147880, train_perplexity=871.19183, train_loss=6.769862
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 147890, train_perplexity=849.515, train_loss=6.7446656

Batch 147900, train_perplexity=897.41943, train_loss=6.7995234

Batch 147910, train_perplexity=829.70703, train_loss=6.7210727

Batch 147920, train_perplexity=823.3715, train_loss=6.7134075

Batch 147930, train_perplexity=870.0178, train_loss=6.7685137

Batch 147940, train_perplexity=836.1017, train_loss=6.72875

Batch 147950, train_perplexity=805.99664, train_loss=6.6920795

Batch 147960, train_perplexity=745.1855, train_loss=6.613633

Batch 147970, train_perplexity=912.92633, train_loss=6.816655

Batch 147980, train_perplexity=851.58386, train_loss=6.747098

Batch 147990, train_perplexity=820.7268, train_loss=6.7101903

Batch 148000, train_perplexity=902.2388, train_loss=6.804879

Batch 148010, train_perplexity=862.8884, train_loss=6.7602854

Batch 148020, train_perplexity=856.7398, train_loss=6.7531343

Batch 148030, train_perplexity=843.43036, train_loss=6.7374773

Batch 148040, train_perplexity=889.54364, train_loss=6.7907085

Batch 148050, train_perplexity=864.47974, train_loss=6.762128

Batch 148060, train_perplexity=826.93396, train_loss=6.717725

Batch 148070, train_perplexity=862.80774, train_loss=6.760192

Batch 148080, train_perplexity=905.7363, train_loss=6.8087482

Batch 148090, train_perplexity=853.0217, train_loss=6.748785

Batch 148100, train_perplexity=857.6501, train_loss=6.754196

Batch 148110, train_perplexity=879.8167, train_loss=6.7797136

Batch 148120, train_perplexity=761.39685, train_loss=6.6351547

Batch 148130, train_perplexity=845.03815, train_loss=6.739382

Batch 148140, train_perplexity=799.1716, train_loss=6.6835756

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Batch 148150, train_perplexity=891.4408, train_loss=6.792839

Batch 148160, train_perplexity=772.27014, train_loss=6.6493344

Batch 148170, train_perplexity=853.3366, train_loss=6.749154

Batch 148180, train_perplexity=895.1987, train_loss=6.7970457

Batch 148190, train_perplexity=909.2076, train_loss=6.8125734

Batch 148200, train_perplexity=847.5005, train_loss=6.7422915

Batch 148210, train_perplexity=955.709, train_loss=6.8624535

Batch 148220, train_perplexity=848.69635, train_loss=6.7437015

Batch 148230, train_perplexity=882.7777, train_loss=6.7830734

Batch 148240, train_perplexity=964.9516, train_loss=6.872078

Batch 148250, train_perplexity=779.9764, train_loss=6.6592636

Batch 148260, train_perplexity=822.89813, train_loss=6.7128325

Batch 148270, train_perplexity=883.08167, train_loss=6.7834177

Batch 148280, train_perplexity=894.21234, train_loss=6.7959433

Batch 148290, train_perplexity=855.88763, train_loss=6.752139

Batch 148300, train_perplexity=982.2968, train_loss=6.8898935

Batch 148310, train_perplexity=941.84235, train_loss=6.847838

Batch 148320, train_perplexity=804.6449, train_loss=6.690401

Batch 148330, train_perplexity=835.88007, train_loss=6.728485

Batch 148340, train_perplexity=874.49896, train_loss=6.773651

Batch 148350, train_perplexity=845.5896, train_loss=6.740034

Batch 148360, train_perplexity=773.3141, train_loss=6.6506853

Batch 148370, train_perplexity=830.8706, train_loss=6.722474

Batch 148380, train_perplexity=857.7867, train_loss=6.7543554

Batch 148390, train_perplexity=873.5771, train_loss=6.7725964

Batch 148400, train_perplexity=807.39215, train_loss=6.6938095

Batch 148410, train_perplexity=850.04663, train_loss=6.745291

Batch 148420, train_perplexity=954.21356, train_loss=6.8608875

Batch 148430, train_perplexity=840.6759, train_loss=6.734206

Batch 148440, train_perplexity=826.282, train_loss=6.716936

Batch 148450, train_perplexity=917.4841, train_loss=6.8216352

Batch 148460, train_perplexity=854.7726, train_loss=6.7508354

Batch 148470, train_perplexity=945.9539, train_loss=6.852194

Batch 148480, train_perplexity=843.0214, train_loss=6.7369924

Batch 148490, train_perplexity=1038.1586, train_loss=6.945204

Batch 148500, train_perplexity=896.09644, train_loss=6.798048

Batch 148510, train_perplexity=898.7384, train_loss=6.800992

Batch 148520, train_perplexity=768.23846, train_loss=6.6441

Batch 148530, train_perplexity=795.6172, train_loss=6.679118

Batch 148540, train_perplexity=950.02856, train_loss=6.856492

Batch 148550, train_perplexity=807.6243, train_loss=6.694097

Batch 148560, train_perplexity=905.0335, train_loss=6.807972

Batch 148570, train_perplexity=790.38385, train_loss=6.6725187

Batch 148580, train_perplexity=889.8686, train_loss=6.791074

Batch 148590, train_perplexity=990.71936, train_loss=6.8984313

Batch 148600, train_perplexity=884.2552, train_loss=6.7847457

Batch 148610, train_perplexity=839.6959, train_loss=6.73304

Batch 148620, train_perplexity=816.66095, train_loss=6.705224

Batch 148630, train_perplexity=874.37305, train_loss=6.773507

Batch 148640, train_perplexity=890.823, train_loss=6.7921457

Batch 148650, train_perplexity=842.19977, train_loss=6.736017

Batch 148660, train_perplexity=862.9863, train_loss=6.760399

Batch 148670, train_perplexity=836.50006, train_loss=6.7292266

Batch 148680, train_perplexity=772.8036, train_loss=6.650025

Batch 148690, train_perplexity=931.4801, train_loss=6.836775

Batch 148700, train_perplexity=936.218, train_loss=6.8418484

Batch 148710, train_perplexity=857.98755, train_loss=6.7545896

Batch 148720, train_perplexity=737.2473, train_loss=6.6029234

Batch 148730, train_perplexity=816.5714, train_loss=6.7051144

Batch 148740, train_perplexity=828.4155, train_loss=6.719515

Batch 148750, train_perplexity=850.8743, train_loss=6.7462645

Batch 148760, train_perplexity=834.33496, train_loss=6.726635

Batch 148770, train_perplexity=880.45636, train_loss=6.7804403

Batch 148780, train_perplexity=905.0365, train_loss=6.8079753

Batch 148790, train_perplexity=829.5294, train_loss=6.7208586

Batch 148800, train_perplexity=855.1256, train_loss=6.7512484

Batch 148810, train_perplexity=952.8377, train_loss=6.8594446

Batch 148820, train_perplexity=855.8411, train_loss=6.7520847

Batch 148830, train_perplexity=866.89624, train_loss=6.7649193

Batch 148840, train_perplexity=854.2636, train_loss=6.75024

Batch 148850, train_perplexity=841.7296, train_loss=6.735459

Batch 148860, train_perplexity=893.032, train_loss=6.7946224

Batch 148870, train_perplexity=834.21246, train_loss=6.726488

Batch 148880, train_perplexity=889.1645, train_loss=6.7902822

Batch 148890, train_perplexity=805.6696, train_loss=6.6916738

Batch 148900, train_perplexity=895.25037, train_loss=6.7971034

Batch 148910, train_perplexity=883.91455, train_loss=6.7843604

Batch 148920, train_perplexity=786.5599, train_loss=6.667669

Batch 148930, train_perplexity=931.55383, train_loss=6.836854

Batch 148940, train_perplexity=886.62, train_loss=6.7874165

Batch 148950, train_perplexity=873.3705, train_loss=6.77236

Batch 148960, train_perplexity=791.2783, train_loss=6.67365

Batch 148970, train_perplexity=838.243, train_loss=6.731308

Batch 148980, train_perplexity=817.65967, train_loss=6.706446

Batch 148990, train_perplexity=789.3255, train_loss=6.671179

Batch 149000, train_perplexity=907.1307, train_loss=6.8102865

Batch 149010, train_perplexity=979.38586, train_loss=6.8869257

Batch 149020, train_perplexity=796.9343, train_loss=6.6807723

Batch 149030, train_perplexity=866.993, train_loss=6.765031

Batch 149040, train_perplexity=891.6593, train_loss=6.793084

Batch 149050, train_perplexity=815.95874, train_loss=6.704364

Batch 149060, train_perplexity=842.24835, train_loss=6.736075

Batch 149070, train_perplexity=832.87537, train_loss=6.724884

Batch 149080, train_perplexity=928.3159, train_loss=6.833372

Batch 149090, train_perplexity=857.352, train_loss=6.7538486

Batch 149100, train_perplexity=833.37396, train_loss=6.7254825

Batch 149110, train_perplexity=940.79877, train_loss=6.8467293

Batch 149120, train_perplexity=936.7266, train_loss=6.8423915

Batch 149130, train_perplexity=822.5408, train_loss=6.712398
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 149140, train_perplexity=789.55927, train_loss=6.671475

Batch 149150, train_perplexity=965.7535, train_loss=6.8729086

Batch 149160, train_perplexity=928.1637, train_loss=6.833208

Batch 149170, train_perplexity=811.54535, train_loss=6.6989403

Batch 149180, train_perplexity=873.2722, train_loss=6.7722473

Batch 149190, train_perplexity=777.48773, train_loss=6.656068

Batch 149200, train_perplexity=865.76434, train_loss=6.7636127

Batch 149210, train_perplexity=829.03357, train_loss=6.7202606

Batch 149220, train_perplexity=800.01074, train_loss=6.684625

Batch 149230, train_perplexity=827.91516, train_loss=6.7189107

Batch 149240, train_perplexity=882.8219, train_loss=6.7831235

Batch 149250, train_perplexity=845.2284, train_loss=6.739607

Batch 149260, train_perplexity=913.0186, train_loss=6.8167562

Batch 149270, train_perplexity=898.00287, train_loss=6.8001733

Batch 149280, train_perplexity=881.4603, train_loss=6.78158

Batch 149290, train_perplexity=809.1265, train_loss=6.6959553

Batch 149300, train_perplexity=943.9411, train_loss=6.850064

Batch 149310, train_perplexity=790.0123, train_loss=6.6720486

Batch 149320, train_perplexity=835.9159, train_loss=6.728528

Batch 149330, train_perplexity=828.6581, train_loss=6.7198076

Batch 149340, train_perplexity=827.3757, train_loss=6.718259

Batch 149350, train_perplexity=868.2055, train_loss=6.7664285

Batch 149360, train_perplexity=810.9121, train_loss=6.6981597

Batch 149370, train_perplexity=835.2187, train_loss=6.7276936

Batch 149380, train_perplexity=922.4974, train_loss=6.8270845

Batch 149390, train_perplexity=863.60297, train_loss=6.761113

Batch 149400, train_perplexity=789.72986, train_loss=6.671691

Batch 149410, train_perplexity=819.47736, train_loss=6.708667

Batch 149420, train_perplexity=956.37915, train_loss=6.8631544

Batch 149430, train_perplexity=798.8073, train_loss=6.68312

Batch 149440, train_perplexity=827.07513, train_loss=6.7178955

Batch 149450, train_perplexity=864.0998, train_loss=6.761688

Batch 149460, train_perplexity=871.99725, train_loss=6.7707863

Batch 149470, train_perplexity=857.9548, train_loss=6.7545514

Batch 149480, train_perplexity=938.6636, train_loss=6.844457

Batch 149490, train_perplexity=851.54205, train_loss=6.747049

Batch 149500, train_perplexity=897.27997, train_loss=6.799368

Batch 149510, train_perplexity=794.5617, train_loss=6.6777906

Batch 149520, train_perplexity=858.3898, train_loss=6.7550583

Batch 149530, train_perplexity=900.8348, train_loss=6.803322

Batch 149540, train_perplexity=873.75665, train_loss=6.772802

Batch 149550, train_perplexity=860.05975, train_loss=6.757002

Batch 149560, train_perplexity=955.1523, train_loss=6.861871

Batch 149570, train_perplexity=815.0399, train_loss=6.703237

Batch 149580, train_perplexity=862.40137, train_loss=6.759721

Batch 149590, train_perplexity=816.7061, train_loss=6.7052794

Batch 149600, train_perplexity=911.6161, train_loss=6.815219

Batch 149610, train_perplexity=861.8078, train_loss=6.7590322

Batch 149620, train_perplexity=809.25616, train_loss=6.6961155

Batch 149630, train_perplexity=801.65546, train_loss=6.686679

Batch 149640, train_perplexity=886.95404, train_loss=6.787793

Batch 149650, train_perplexity=872.5101, train_loss=6.771374

Batch 149660, train_perplexity=808.7701, train_loss=6.6955147

Batch 149670, train_perplexity=887.69995, train_loss=6.788634

Batch 149680, train_perplexity=906.78296, train_loss=6.809903

Batch 149690, train_perplexity=829.2803, train_loss=6.720558

Batch 149700, train_perplexity=913.8815, train_loss=6.817701

Batch 149710, train_perplexity=796.66345, train_loss=6.6804323

Batch 149720, train_perplexity=837.75867, train_loss=6.73073

Batch 149730, train_perplexity=893.46643, train_loss=6.795109

Batch 149740, train_perplexity=838.14264, train_loss=6.7311883

Batch 149750, train_perplexity=834.6175, train_loss=6.7269735

Batch 149760, train_perplexity=881.6217, train_loss=6.781763

Batch 149770, train_perplexity=797.72974, train_loss=6.68177

Batch 149780, train_perplexity=843.9521, train_loss=6.7380958

Batch 149790, train_perplexity=847.7434, train_loss=6.742578

Batch 149800, train_perplexity=809.43756, train_loss=6.6963396

Batch 149810, train_perplexity=815.86304, train_loss=6.7042465

Batch 149820, train_perplexity=756.30927, train_loss=6.6284504

Batch 149830, train_perplexity=849.76294, train_loss=6.7449574

Batch 149840, train_perplexity=880.9418, train_loss=6.7809916

Batch 149850, train_perplexity=811.6684, train_loss=6.699092

Batch 149860, train_perplexity=899.8696, train_loss=6.80225

Batch 149870, train_perplexity=811.86505, train_loss=6.699334

Batch 149880, train_perplexity=919.3155, train_loss=6.8236294

Batch 149890, train_perplexity=789.1908, train_loss=6.671008

Batch 149900, train_perplexity=830.178, train_loss=6.72164

Batch 149910, train_perplexity=950.60315, train_loss=6.8570967

Batch 149920, train_perplexity=882.5976, train_loss=6.7828693

Batch 149930, train_perplexity=873.77454, train_loss=6.7728224

Batch 149940, train_perplexity=856.1521, train_loss=6.752448

Batch 149950, train_perplexity=862.70325, train_loss=6.760071

Batch 149960, train_perplexity=1019.4263, train_loss=6.9269953

Batch 149970, train_perplexity=740.71606, train_loss=6.6076174

Batch 149980, train_perplexity=837.6524, train_loss=6.730603

Batch 149990, train_perplexity=850.9575, train_loss=6.746362

Batch 150000, train_perplexity=936.9978, train_loss=6.842681

Batch 150010, train_perplexity=838.90515, train_loss=6.7320976

Batch 150020, train_perplexity=842.354, train_loss=6.7362003

Batch 150030, train_perplexity=805.2506, train_loss=6.6911535

Batch 150040, train_perplexity=792.1274, train_loss=6.674722

Batch 150050, train_perplexity=950.52747, train_loss=6.857017

Batch 150060, train_perplexity=999.7202, train_loss=6.9074755

Batch 150070, train_perplexity=844.68085, train_loss=6.738959

Batch 150080, train_perplexity=901.79144, train_loss=6.8043833

Batch 150090, train_perplexity=833.6088, train_loss=6.7257643

Batch 150100, train_perplexity=753.5655, train_loss=6.624816

Batch 150110, train_perplexity=859.21655, train_loss=6.756021

Batch 150120, train_perplexity=850.22705, train_loss=6.7455034

Batch 150130, train_perplexity=864.5585, train_loss=6.762219

Batch 150140, train_perplexity=897.9956, train_loss=6.800165

Batch 150150, train_perplexity=849.02704, train_loss=6.744091

Batch 150160, train_perplexity=851.7548, train_loss=6.7472987

Batch 150170, train_perplexity=826.2355, train_loss=6.71688

Batch 150180, train_perplexity=831.78394, train_loss=6.7235727

Batch 150190, train_perplexity=903.2822, train_loss=6.806035

Batch 150200, train_perplexity=850.09735, train_loss=6.745351

Batch 150210, train_perplexity=901.3783, train_loss=6.803925

Batch 150220, train_perplexity=886.9904, train_loss=6.787834

Batch 150230, train_perplexity=892.27136, train_loss=6.7937703

Batch 150240, train_perplexity=804.6246, train_loss=6.690376

Batch 150250, train_perplexity=857.6607, train_loss=6.7542086

Batch 150260, train_perplexity=849.6147, train_loss=6.744783

Batch 150270, train_perplexity=805.0855, train_loss=6.6909485

Batch 150280, train_perplexity=851.6907, train_loss=6.7472234

Batch 150290, train_perplexity=855.57587, train_loss=6.751775

Batch 150300, train_perplexity=860.68744, train_loss=6.7577314

Batch 150310, train_perplexity=829.55554, train_loss=6.72089

Batch 150320, train_perplexity=845.1736, train_loss=6.739542

Batch 150330, train_perplexity=936.90845, train_loss=6.8425856

Batch 150340, train_perplexity=904.74524, train_loss=6.8076534

Batch 150350, train_perplexity=921.5354, train_loss=6.826041

Batch 150360, train_perplexity=821.80725, train_loss=6.711506

Batch 150370, train_perplexity=835.1784, train_loss=6.7276454

Batch 150380, train_perplexity=975.3197, train_loss=6.8827653

Batch 150390, train_perplexity=861.01874, train_loss=6.7581162

Batch 150400, train_perplexity=855.2459, train_loss=6.751389

Batch 150410, train_perplexity=834.4062, train_loss=6.7267203

Batch 150420, train_perplexity=908.1876, train_loss=6.811451

Batch 150430, train_perplexity=858.7779, train_loss=6.7555103

Batch 150440, train_perplexity=911.50653, train_loss=6.815099
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 150450, train_perplexity=831.6725, train_loss=6.7234387

Batch 150460, train_perplexity=813.526, train_loss=6.701378

Batch 150470, train_perplexity=870.96625, train_loss=6.7696033

Batch 150480, train_perplexity=771.8236, train_loss=6.648756

Batch 150490, train_perplexity=841.1679, train_loss=6.7347913

Batch 150500, train_perplexity=869.4973, train_loss=6.7679152

Batch 150510, train_perplexity=788.6747, train_loss=6.670354

Batch 150520, train_perplexity=855.20636, train_loss=6.751343

Batch 150530, train_perplexity=883.14105, train_loss=6.783485

Batch 150540, train_perplexity=749.6361, train_loss=6.619588

Batch 150550, train_perplexity=835.22266, train_loss=6.7276983

Batch 150560, train_perplexity=871.6306, train_loss=6.7703657

Batch 150570, train_perplexity=862.67944, train_loss=6.760043

Batch 150580, train_perplexity=880.8561, train_loss=6.7808943

Batch 150590, train_perplexity=838.0184, train_loss=6.73104

Batch 150600, train_perplexity=868.7344, train_loss=6.7670374

Batch 150610, train_perplexity=851.4068, train_loss=6.74689

Batch 150620, train_perplexity=890.6106, train_loss=6.7919073

Batch 150630, train_perplexity=919.8777, train_loss=6.8242407

Batch 150640, train_perplexity=845.3283, train_loss=6.739725

Batch 150650, train_perplexity=786.9399, train_loss=6.668152

Batch 150660, train_perplexity=897.43396, train_loss=6.7995396

Batch 150670, train_perplexity=832.1485, train_loss=6.724011

Batch 150680, train_perplexity=899.5985, train_loss=6.8019485

Batch 150690, train_perplexity=887.9307, train_loss=6.7888937

Batch 150700, train_perplexity=881.76215, train_loss=6.7819223

Batch 150710, train_perplexity=880.6856, train_loss=6.7807007

Batch 150720, train_perplexity=803.8737, train_loss=6.689442

Batch 150730, train_perplexity=803.31464, train_loss=6.6887465

Batch 150740, train_perplexity=857.43256, train_loss=6.7539425

Batch 150750, train_perplexity=801.5415, train_loss=6.686537

Batch 150760, train_perplexity=879.81635, train_loss=6.779713

Batch 150770, train_perplexity=821.6505, train_loss=6.711315

Batch 150780, train_perplexity=790.7654, train_loss=6.6730013

Batch 150790, train_perplexity=846.5275, train_loss=6.7411427

Batch 150800, train_perplexity=908.977, train_loss=6.8123198

Batch 150810, train_perplexity=824.99304, train_loss=6.715375

Batch 150820, train_perplexity=830.0192, train_loss=6.721449

Batch 150830, train_perplexity=921.7534, train_loss=6.8262777

Batch 150840, train_perplexity=796.4705, train_loss=6.68019

Batch 150850, train_perplexity=770.1039, train_loss=6.6465254

Batch 150860, train_perplexity=921.01483, train_loss=6.825476

Batch 150870, train_perplexity=818.9215, train_loss=6.7079883

Batch 150880, train_perplexity=797.56006, train_loss=6.681557

Batch 150890, train_perplexity=927.6986, train_loss=6.832707

Batch 150900, train_perplexity=863.7933, train_loss=6.7613335

Batch 150910, train_perplexity=812.2674, train_loss=6.6998296

Batch 150920, train_perplexity=762.7086, train_loss=6.636876

Batch 150930, train_perplexity=802.6997, train_loss=6.6879807

Batch 150940, train_perplexity=791.0716, train_loss=6.6733885

Batch 150950, train_perplexity=840.80255, train_loss=6.734357

Batch 150960, train_perplexity=848.51385, train_loss=6.7434864

Batch 150970, train_perplexity=903.18616, train_loss=6.8059287

Batch 150980, train_perplexity=733.58136, train_loss=6.5979385

Batch 150990, train_perplexity=860.53766, train_loss=6.7575574

Batch 151000, train_perplexity=945.2622, train_loss=6.8514624

Batch 151010, train_perplexity=857.24774, train_loss=6.753727

Batch 151020, train_perplexity=878.5599, train_loss=6.778284

Batch 151030, train_perplexity=864.3742, train_loss=6.762006

Batch 151040, train_perplexity=839.06433, train_loss=6.7322874

Batch 151050, train_perplexity=825.68646, train_loss=6.716215

Batch 151060, train_perplexity=807.1527, train_loss=6.693513

Batch 151070, train_perplexity=879.633, train_loss=6.779505

Batch 151080, train_perplexity=796.04144, train_loss=6.6796513

Batch 151090, train_perplexity=884.19745, train_loss=6.7846804

Batch 151100, train_perplexity=823.4304, train_loss=6.713479

Batch 151110, train_perplexity=856.594, train_loss=6.752964

Batch 151120, train_perplexity=889.90424, train_loss=6.791114

Batch 151130, train_perplexity=876.70013, train_loss=6.776165

Batch 151140, train_perplexity=807.45416, train_loss=6.6938863

Batch 151150, train_perplexity=791.09235, train_loss=6.6734147

Batch 151160, train_perplexity=909.6291, train_loss=6.813037

Batch 151170, train_perplexity=753.29895, train_loss=6.624462

Batch 151180, train_perplexity=998.59015, train_loss=6.9063444

Batch 151190, train_perplexity=895.1432, train_loss=6.7969837

Batch 151200, train_perplexity=752.48755, train_loss=6.6233845

Batch 151210, train_perplexity=786.9586, train_loss=6.6681757

Batch 151220, train_perplexity=842.7545, train_loss=6.7366757

Batch 151230, train_perplexity=794.0042, train_loss=6.6770887

Batch 151240, train_perplexity=864.70734, train_loss=6.762391

Batch 151250, train_perplexity=756.85767, train_loss=6.629175

Batch 151260, train_perplexity=802.69434, train_loss=6.687974

Batch 151270, train_perplexity=870.56976, train_loss=6.769148

Batch 151280, train_perplexity=831.2776, train_loss=6.722964

Batch 151290, train_perplexity=888.8169, train_loss=6.7898912

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Batch 151300, train_perplexity=817.13617, train_loss=6.705806

Batch 151310, train_perplexity=721.82263, train_loss=6.5817795

Batch 151320, train_perplexity=882.20667, train_loss=6.7824264

Batch 151330, train_perplexity=808.97375, train_loss=6.6957664

Batch 151340, train_perplexity=882.561, train_loss=6.782828

Batch 151350, train_perplexity=861.69684, train_loss=6.7589035

Batch 151360, train_perplexity=821.667, train_loss=6.711335

Batch 151370, train_perplexity=911.63257, train_loss=6.815237

Batch 151380, train_perplexity=853.9154, train_loss=6.749832

Batch 151390, train_perplexity=884.72546, train_loss=6.7852774

Batch 151400, train_perplexity=821.8006, train_loss=6.711498

Batch 151410, train_perplexity=880.18854, train_loss=6.780136

Batch 151420, train_perplexity=884.3707, train_loss=6.7848763

Batch 151430, train_perplexity=907.2129, train_loss=6.810377

Batch 151440, train_perplexity=806.8245, train_loss=6.693106

Batch 151450, train_perplexity=907.9447, train_loss=6.8111835

Batch 151460, train_perplexity=892.45905, train_loss=6.7939806

Batch 151470, train_perplexity=841.1667, train_loss=6.73479

Batch 151480, train_perplexity=792.1421, train_loss=6.674741

Batch 151490, train_perplexity=810.18823, train_loss=6.6972666

Batch 151500, train_perplexity=879.56335, train_loss=6.7794256

Batch 151510, train_perplexity=821.15936, train_loss=6.710717

Batch 151520, train_perplexity=845.334, train_loss=6.739732

Batch 151530, train_perplexity=842.78143, train_loss=6.7367077

Batch 151540, train_perplexity=841.64856, train_loss=6.7353625

Batch 151550, train_perplexity=800.03094, train_loss=6.6846504

Batch 151560, train_perplexity=856.7872, train_loss=6.7531896

Batch 151570, train_perplexity=867.51855, train_loss=6.765637

Batch 151580, train_perplexity=753.4304, train_loss=6.6246367

Batch 151590, train_perplexity=800.76984, train_loss=6.6855736

Batch 151600, train_perplexity=911.9978, train_loss=6.8156376

Batch 151610, train_perplexity=871.74365, train_loss=6.7704954

Batch 151620, train_perplexity=904.73663, train_loss=6.807644

Batch 151630, train_perplexity=802.15674, train_loss=6.687304

Batch 151640, train_perplexity=859.65137, train_loss=6.756527

Batch 151650, train_perplexity=792.68317, train_loss=6.6754236

Batch 151660, train_perplexity=794.02386, train_loss=6.6771135

Batch 151670, train_perplexity=878.9328, train_loss=6.7787085

Batch 151680, train_perplexity=843.9328, train_loss=6.738073
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 151690, train_perplexity=811.05945, train_loss=6.6983414

Batch 151700, train_perplexity=845.4545, train_loss=6.7398744

Batch 151710, train_perplexity=816.2335, train_loss=6.7047005

Batch 151720, train_perplexity=873.9362, train_loss=6.7730074

Batch 151730, train_perplexity=887.4414, train_loss=6.7883425

Batch 151740, train_perplexity=763.9367, train_loss=6.638485

Batch 151750, train_perplexity=867.4189, train_loss=6.765522

Batch 151760, train_perplexity=886.68256, train_loss=6.787487

Batch 151770, train_perplexity=836.10645, train_loss=6.728756

Batch 151780, train_perplexity=781.82477, train_loss=6.6616306

Batch 151790, train_perplexity=842.67816, train_loss=6.736585

Batch 151800, train_perplexity=842.0512, train_loss=6.735841

Batch 151810, train_perplexity=777.21936, train_loss=6.6557226

Batch 151820, train_perplexity=811.02, train_loss=6.6982927

Batch 151830, train_perplexity=721.6747, train_loss=6.5815744

Batch 151840, train_perplexity=901.1742, train_loss=6.8036985

Batch 151850, train_perplexity=886.25903, train_loss=6.7870092

Batch 151860, train_perplexity=841.958, train_loss=6.73573

Batch 151870, train_perplexity=815.8102, train_loss=6.7041817

Batch 151880, train_perplexity=864.5601, train_loss=6.762221

Batch 151890, train_perplexity=860.0622, train_loss=6.7570047

Batch 151900, train_perplexity=849.8088, train_loss=6.7450113

Batch 151910, train_perplexity=792.54407, train_loss=6.675248

Batch 151920, train_perplexity=901.4995, train_loss=6.8040595

Batch 151930, train_perplexity=908.014, train_loss=6.8112597

Batch 151940, train_perplexity=802.7422, train_loss=6.6880336

Batch 151950, train_perplexity=780.48456, train_loss=6.659915

Batch 151960, train_perplexity=816.54144, train_loss=6.7050776

Batch 151970, train_perplexity=949.8537, train_loss=6.856308

Batch 151980, train_perplexity=813.13043, train_loss=6.7008915

Batch 151990, train_perplexity=918.52985, train_loss=6.8227744

Batch 152000, train_perplexity=824.13947, train_loss=6.7143397

Batch 152010, train_perplexity=844.0998, train_loss=6.7382708

Batch 152020, train_perplexity=802.4104, train_loss=6.68762

Batch 152030, train_perplexity=764.386, train_loss=6.639073

Batch 152040, train_perplexity=821.1022, train_loss=6.7106476

Batch 152050, train_perplexity=835.6736, train_loss=6.728238

Batch 152060, train_perplexity=813.4244, train_loss=6.701253

Batch 152070, train_perplexity=968.77637, train_loss=6.876034

Batch 152080, train_perplexity=828.71497, train_loss=6.7198763

Batch 152090, train_perplexity=755.1399, train_loss=6.626903

Batch 152100, train_perplexity=857.3013, train_loss=6.7537894

Batch 152110, train_perplexity=837.7267, train_loss=6.730692

Batch 152120, train_perplexity=863.4729, train_loss=6.7609625

Batch 152130, train_perplexity=867.64185, train_loss=6.765779

Batch 152140, train_perplexity=877.637, train_loss=6.777233

Batch 152150, train_perplexity=897.19354, train_loss=6.7992716

Batch 152160, train_perplexity=784.2256, train_loss=6.6646967

Batch 152170, train_perplexity=851.44336, train_loss=6.746933

Batch 152180, train_perplexity=880.1407, train_loss=6.7800817

Batch 152190, train_perplexity=817.0345, train_loss=6.7056813

Batch 152200, train_perplexity=838.74115, train_loss=6.731902

Batch 152210, train_perplexity=868.46014, train_loss=6.7667217

Batch 152220, train_perplexity=810.121, train_loss=6.6971836

Batch 152230, train_perplexity=878.5373, train_loss=6.7782583

Batch 152240, train_perplexity=813.5058, train_loss=6.701353

Batch 152250, train_perplexity=774.7764, train_loss=6.6525745

Batch 152260, train_perplexity=878.4535, train_loss=6.778163

Batch 152270, train_perplexity=831.75854, train_loss=6.723542

Batch 152280, train_perplexity=934.7638, train_loss=6.840294

Batch 152290, train_perplexity=816.8674, train_loss=6.7054768

Batch 152300, train_perplexity=876.0215, train_loss=6.7753906

Batch 152310, train_perplexity=899.1572, train_loss=6.801458

Batch 152320, train_perplexity=742.2866, train_loss=6.6097355

Batch 152330, train_perplexity=824.0836, train_loss=6.714272

Batch 152340, train_perplexity=879.602, train_loss=6.7794695

Batch 152350, train_perplexity=902.87054, train_loss=6.805579

Batch 152360, train_perplexity=821.9856, train_loss=6.711723

Batch 152370, train_perplexity=841.42865, train_loss=6.735101

Batch 152380, train_perplexity=878.1578, train_loss=6.7778263

Batch 152390, train_perplexity=854.1671, train_loss=6.750127

Batch 152400, train_perplexity=884.8385, train_loss=6.785405

Batch 152410, train_perplexity=895.71576, train_loss=6.797623

Batch 152420, train_perplexity=792.5411, train_loss=6.6752443

Batch 152430, train_perplexity=916.09784, train_loss=6.820123

Batch 152440, train_perplexity=811.83563, train_loss=6.699298

Batch 152450, train_perplexity=811.15845, train_loss=6.6984634

Batch 152460, train_perplexity=834.1074, train_loss=6.726362

Batch 152470, train_perplexity=795.81525, train_loss=6.679367

Batch 152480, train_perplexity=897.47424, train_loss=6.7995844

Batch 152490, train_perplexity=720.0275, train_loss=6.5792894

Batch 152500, train_perplexity=810.71106, train_loss=6.6979117

Batch 152510, train_perplexity=841.7645, train_loss=6.7355003

Batch 152520, train_perplexity=826.44354, train_loss=6.7171316

Batch 152530, train_perplexity=886.99805, train_loss=6.7878428

Batch 152540, train_perplexity=860.99945, train_loss=6.758094

Batch 152550, train_perplexity=988.33417, train_loss=6.896021

Batch 152560, train_perplexity=841.3504, train_loss=6.7350082

Batch 152570, train_perplexity=895.5966, train_loss=6.79749

Batch 152580, train_perplexity=867.3717, train_loss=6.7654676

Batch 152590, train_perplexity=919.209, train_loss=6.8235135

Batch 152600, train_perplexity=863.3765, train_loss=6.760851

Batch 152610, train_perplexity=808.9595, train_loss=6.695749

Batch 152620, train_perplexity=816.418, train_loss=6.7049265

Batch 152630, train_perplexity=838.43085, train_loss=6.731532

Batch 152640, train_perplexity=952.94495, train_loss=6.859557

Batch 152650, train_perplexity=851.3183, train_loss=6.746786

Batch 152660, train_perplexity=839.2364, train_loss=6.7324924

Batch 152670, train_perplexity=814.3379, train_loss=6.7023754

Batch 152680, train_perplexity=867.40814, train_loss=6.7655096

Batch 152690, train_perplexity=872.60455, train_loss=6.7714825

Batch 152700, train_perplexity=831.7883, train_loss=6.723578

Batch 152710, train_perplexity=903.26587, train_loss=6.806017

Batch 152720, train_perplexity=805.9082, train_loss=6.69197

Batch 152730, train_perplexity=842.7228, train_loss=6.736638

Batch 152740, train_perplexity=814.18805, train_loss=6.7021914

Batch 152750, train_perplexity=916.2477, train_loss=6.8202868

Batch 152760, train_perplexity=832.1767, train_loss=6.724045

Batch 152770, train_perplexity=875.85693, train_loss=6.7752028

Batch 152780, train_perplexity=711.97766, train_loss=6.5680466

Batch 152790, train_perplexity=822.25806, train_loss=6.7120543

Batch 152800, train_perplexity=707.0652, train_loss=6.561123

Batch 152810, train_perplexity=933.12854, train_loss=6.838543

Batch 152820, train_perplexity=725.46893, train_loss=6.586818

Batch 152830, train_perplexity=828.615, train_loss=6.7197556

Batch 152840, train_perplexity=882.6338, train_loss=6.7829103

Batch 152850, train_perplexity=839.2772, train_loss=6.732541

Batch 152860, train_perplexity=840.03796, train_loss=6.733447

Batch 152870, train_perplexity=843.82697, train_loss=6.7379475

Batch 152880, train_perplexity=736.1362, train_loss=6.601415

Batch 152890, train_perplexity=902.62646, train_loss=6.805309

Batch 152900, train_perplexity=819.8909, train_loss=6.7091713

Batch 152910, train_perplexity=854.1862, train_loss=6.7501493

Batch 152920, train_perplexity=818.07965, train_loss=6.7069597

Batch 152930, train_perplexity=808.3375, train_loss=6.6949797

Batch 152940, train_perplexity=761.9591, train_loss=6.635893

Batch 152950, train_perplexity=929.347, train_loss=6.834482

Batch 152960, train_perplexity=874.9186, train_loss=6.774131

Batch 152970, train_perplexity=828.55615, train_loss=6.7196846

Batch 152980, train_perplexity=815.87476, train_loss=6.704261

Batch 152990, train_perplexity=899.41064, train_loss=6.8017397
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 153000, train_perplexity=832.44183, train_loss=6.7243633

Batch 153010, train_perplexity=778.001, train_loss=6.656728

Batch 153020, train_perplexity=920.9635, train_loss=6.8254204

Batch 153030, train_perplexity=803.5521, train_loss=6.689042

Batch 153040, train_perplexity=833.80286, train_loss=6.725997

Batch 153050, train_perplexity=850.9129, train_loss=6.7463098

Batch 153060, train_perplexity=795.2125, train_loss=6.6786094

Batch 153070, train_perplexity=853.72614, train_loss=6.7496104

Batch 153080, train_perplexity=845.0849, train_loss=6.739437

Batch 153090, train_perplexity=896.1614, train_loss=6.7981205

Batch 153100, train_perplexity=812.8141, train_loss=6.7005024

Batch 153110, train_perplexity=864.4303, train_loss=6.7620707

Batch 153120, train_perplexity=782.54834, train_loss=6.6625557

Batch 153130, train_perplexity=807.81964, train_loss=6.694339

Batch 153140, train_perplexity=851.55786, train_loss=6.7470675

Batch 153150, train_perplexity=824.21765, train_loss=6.7144346

Batch 153160, train_perplexity=931.5858, train_loss=6.8368883

Batch 153170, train_perplexity=833.61163, train_loss=6.7257676

Batch 153180, train_perplexity=879.5357, train_loss=6.779394

Batch 153190, train_perplexity=787.3422, train_loss=6.668663

Batch 153200, train_perplexity=805.8352, train_loss=6.6918793

Batch 153210, train_perplexity=782.67224, train_loss=6.662714

Batch 153220, train_perplexity=830.4832, train_loss=6.7220078

Batch 153230, train_perplexity=920.75885, train_loss=6.825198

Batch 153240, train_perplexity=870.2402, train_loss=6.7687693

Batch 153250, train_perplexity=862.1892, train_loss=6.7594748

Batch 153260, train_perplexity=726.7157, train_loss=6.5885353

Batch 153270, train_perplexity=818.65796, train_loss=6.7076664

Batch 153280, train_perplexity=749.4713, train_loss=6.619368

Batch 153290, train_perplexity=873.7879, train_loss=6.7728376

Batch 153300, train_perplexity=868.5972, train_loss=6.7668796

Batch 153310, train_perplexity=812.935, train_loss=6.700651

Batch 153320, train_perplexity=891.2419, train_loss=6.792616

Batch 153330, train_perplexity=879.5793, train_loss=6.7794437

Batch 153340, train_perplexity=872.9883, train_loss=6.771922

Batch 153350, train_perplexity=870.3589, train_loss=6.7689056

Batch 153360, train_perplexity=875.9037, train_loss=6.775256

Batch 153370, train_perplexity=823.5957, train_loss=6.71368

Batch 153380, train_perplexity=792.1436, train_loss=6.6747427

Batch 153390, train_perplexity=804.05695, train_loss=6.68967

Batch 153400, train_perplexity=965.085, train_loss=6.872216

Batch 153410, train_perplexity=766.9201, train_loss=6.6423826

Batch 153420, train_perplexity=917.7449, train_loss=6.8219194

Batch 153430, train_perplexity=795.879, train_loss=6.679447

Batch 153440, train_perplexity=818.56195, train_loss=6.707549

Batch 153450, train_perplexity=803.65405, train_loss=6.689169

Batch 153460, train_perplexity=811.02313, train_loss=6.6982965

Batch 153470, train_perplexity=896.37506, train_loss=6.798359

Batch 153480, train_perplexity=893.35657, train_loss=6.794986

Batch 153490, train_perplexity=823.0288, train_loss=6.712991

Batch 153500, train_perplexity=784.0255, train_loss=6.6644416

Batch 153510, train_perplexity=888.14496, train_loss=6.789135

Batch 153520, train_perplexity=926.4171, train_loss=6.8313246

Batch 153530, train_perplexity=871.3846, train_loss=6.7700834

Batch 153540, train_perplexity=884.2809, train_loss=6.784775

Batch 153550, train_perplexity=831.5587, train_loss=6.723302

Batch 153560, train_perplexity=833.5397, train_loss=6.7256813

Batch 153570, train_perplexity=814.9012, train_loss=6.703067

Batch 153580, train_perplexity=935.8355, train_loss=6.8414397

Batch 153590, train_perplexity=800.59344, train_loss=6.6853533

Batch 153600, train_perplexity=870.2842, train_loss=6.76882

Batch 153610, train_perplexity=860.0975, train_loss=6.7570457

Batch 153620, train_perplexity=855.4094, train_loss=6.75158

Batch 153630, train_perplexity=858.52484, train_loss=6.7552156

Batch 153640, train_perplexity=870.64325, train_loss=6.7692323

Batch 153650, train_perplexity=800.35986, train_loss=6.6850615

Batch 153660, train_perplexity=880.0366, train_loss=6.7799635

Batch 153670, train_perplexity=806.52527, train_loss=6.692735

Batch 153680, train_perplexity=803.1009, train_loss=6.6884804

Batch 153690, train_perplexity=735.4769, train_loss=6.600519

Batch 153700, train_perplexity=961.2446, train_loss=6.868229

Batch 153710, train_perplexity=800.9375, train_loss=6.685783

Batch 153720, train_perplexity=882.86694, train_loss=6.7831745

Batch 153730, train_perplexity=831.39136, train_loss=6.7231007

Batch 153740, train_perplexity=909.8217, train_loss=6.8132486

Batch 153750, train_perplexity=848.8983, train_loss=6.7439394

Batch 153760, train_perplexity=796.7489, train_loss=6.6805396

Batch 153770, train_perplexity=893.9378, train_loss=6.795636

Batch 153780, train_perplexity=849.7561, train_loss=6.7449493

Batch 153790, train_perplexity=906.3161, train_loss=6.809388

Batch 153800, train_perplexity=849.66693, train_loss=6.7448444

Batch 153810, train_perplexity=874.4823, train_loss=6.773632

Batch 153820, train_perplexity=875.62054, train_loss=6.774933

Batch 153830, train_perplexity=835.98285, train_loss=6.728608

Batch 153840, train_perplexity=810.7308, train_loss=6.697936

Batch 153850, train_perplexity=871.97565, train_loss=6.7707615

Batch 153860, train_perplexity=882.8569, train_loss=6.783163

Batch 153870, train_perplexity=900.79224, train_loss=6.8032746

Batch 153880, train_perplexity=801.52435, train_loss=6.6865153

Batch 153890, train_perplexity=874.99786, train_loss=6.7742214

Batch 153900, train_perplexity=790.61304, train_loss=6.6728086

Batch 153910, train_perplexity=840.1241, train_loss=6.7335496

Batch 153920, train_perplexity=766.0659, train_loss=6.6412683

Batch 153930, train_perplexity=768.81824, train_loss=6.6448545

Batch 153940, train_perplexity=846.8639, train_loss=6.74154

Batch 153950, train_perplexity=848.721, train_loss=6.7437305

Batch 153960, train_perplexity=780.59625, train_loss=6.660058

Batch 153970, train_perplexity=768.3854, train_loss=6.6442914

Batch 153980, train_perplexity=845.6025, train_loss=6.7400494

Batch 153990, train_perplexity=765.26526, train_loss=6.6402225

Batch 154000, train_perplexity=877.8413, train_loss=6.777466

Batch 154010, train_perplexity=795.5049, train_loss=6.678977

Batch 154020, train_perplexity=785.0554, train_loss=6.6657543

Batch 154030, train_perplexity=858.19946, train_loss=6.7548366

Batch 154040, train_perplexity=856.99023, train_loss=6.7534266

Batch 154050, train_perplexity=950.28046, train_loss=6.856757

Batch 154060, train_perplexity=866.8169, train_loss=6.7648277

Batch 154070, train_perplexity=836.69275, train_loss=6.729457

Batch 154080, train_perplexity=840.6222, train_loss=6.7341423

Batch 154090, train_perplexity=880.4437, train_loss=6.780426

Batch 154100, train_perplexity=853.66705, train_loss=6.7495413

Batch 154110, train_perplexity=880.4089, train_loss=6.7803864

Batch 154120, train_perplexity=833.4753, train_loss=6.725604

Batch 154130, train_perplexity=897.8462, train_loss=6.7999988

Batch 154140, train_perplexity=947.81775, train_loss=6.854162

Batch 154150, train_perplexity=819.08594, train_loss=6.708189

Batch 154160, train_perplexity=778.6438, train_loss=6.6575537

Batch 154170, train_perplexity=795.5148, train_loss=6.6789894

Batch 154180, train_perplexity=749.05975, train_loss=6.6188188

Batch 154190, train_perplexity=841.958, train_loss=6.73573

Batch 154200, train_perplexity=855.53955, train_loss=6.7517323

Batch 154210, train_perplexity=839.27563, train_loss=6.732539

Batch 154220, train_perplexity=897.38434, train_loss=6.7994843

Batch 154230, train_perplexity=874.0846, train_loss=6.773177

Batch 154240, train_perplexity=839.8457, train_loss=6.733218

Batch 154250, train_perplexity=795.2159, train_loss=6.6786137

Batch 154260, train_perplexity=927.18256, train_loss=6.8321505

Batch 154270, train_perplexity=803.2403, train_loss=6.688654

Batch 154280, train_perplexity=877.4956, train_loss=6.777072

Batch 154290, train_perplexity=880.86285, train_loss=6.780902

Batch 154300, train_perplexity=798.559, train_loss=6.682809
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 154310, train_perplexity=733.71643, train_loss=6.5981226

Batch 154320, train_perplexity=727.74695, train_loss=6.5899534

Batch 154330, train_perplexity=870.4963, train_loss=6.7690635

Batch 154340, train_perplexity=841.1514, train_loss=6.7347717

Batch 154350, train_perplexity=880.2112, train_loss=6.780162

Batch 154360, train_perplexity=924.17395, train_loss=6.8289003

Batch 154370, train_perplexity=930.0053, train_loss=6.8351903

Batch 154380, train_perplexity=895.93066, train_loss=6.797863

Batch 154390, train_perplexity=845.8775, train_loss=6.7403746

Batch 154400, train_perplexity=867.92737, train_loss=6.766108

Batch 154410, train_perplexity=866.4193, train_loss=6.764369

Batch 154420, train_perplexity=820.7291, train_loss=6.710193

Batch 154430, train_perplexity=808.76776, train_loss=6.695512

Batch 154440, train_perplexity=812.0323, train_loss=6.69954

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Batch 154450, train_perplexity=868.34503, train_loss=6.766589

Batch 154460, train_perplexity=808.2527, train_loss=6.694875

Batch 154470, train_perplexity=820.3031, train_loss=6.709674

Batch 154480, train_perplexity=819.1273, train_loss=6.7082396

Batch 154490, train_perplexity=876.6178, train_loss=6.776071

Batch 154500, train_perplexity=788.52686, train_loss=6.6701665

Batch 154510, train_perplexity=871.08545, train_loss=6.76974

Batch 154520, train_perplexity=886.7388, train_loss=6.7875504

Batch 154530, train_perplexity=836.9665, train_loss=6.729784

Batch 154540, train_perplexity=855.7142, train_loss=6.7519364

Batch 154550, train_perplexity=784.3954, train_loss=6.664913

Batch 154560, train_perplexity=794.7997, train_loss=6.67809

Batch 154570, train_perplexity=836.9609, train_loss=6.7297773

Batch 154580, train_perplexity=792.43225, train_loss=6.675107

Batch 154590, train_perplexity=848.2877, train_loss=6.74322

Batch 154600, train_perplexity=815.52386, train_loss=6.7038307

Batch 154610, train_perplexity=845.9235, train_loss=6.740429

Batch 154620, train_perplexity=798.50574, train_loss=6.682742

Batch 154630, train_perplexity=877.26636, train_loss=6.7768106

Batch 154640, train_perplexity=882.2517, train_loss=6.7824774

Batch 154650, train_perplexity=866.15247, train_loss=6.764061

Batch 154660, train_perplexity=816.66876, train_loss=6.7052336

Batch 154670, train_perplexity=856.77246, train_loss=6.7531724

Batch 154680, train_perplexity=866.0781, train_loss=6.763975

Batch 154690, train_perplexity=901.9011, train_loss=6.804505

Batch 154700, train_perplexity=886.0181, train_loss=6.7867374

Batch 154710, train_perplexity=782.7674, train_loss=6.6628356

Batch 154720, train_perplexity=781.37976, train_loss=6.6610613

Batch 154730, train_perplexity=843.5904, train_loss=6.737667

Batch 154740, train_perplexity=824.4071, train_loss=6.7146645

Batch 154750, train_perplexity=850.21405, train_loss=6.745488

Batch 154760, train_perplexity=902.3833, train_loss=6.8050394

Batch 154770, train_perplexity=919.27954, train_loss=6.8235903

Batch 154780, train_perplexity=817.8554, train_loss=6.7066855

Batch 154790, train_perplexity=919.85266, train_loss=6.8242135

Batch 154800, train_perplexity=858.9978, train_loss=6.7557664

Batch 154810, train_perplexity=885.47205, train_loss=6.786121

Batch 154820, train_perplexity=778.1516, train_loss=6.6569214

Batch 154830, train_perplexity=857.702, train_loss=6.7542567

Batch 154840, train_perplexity=777.4232, train_loss=6.655985

Batch 154850, train_perplexity=947.0443, train_loss=6.853346

Batch 154860, train_perplexity=832.193, train_loss=6.7240644

Batch 154870, train_perplexity=831.6372, train_loss=6.7233963

Batch 154880, train_perplexity=780.1333, train_loss=6.659465

Batch 154890, train_perplexity=826.9087, train_loss=6.7176943

Batch 154900, train_perplexity=876.609, train_loss=6.776061

Batch 154910, train_perplexity=815.9805, train_loss=6.7043905

Batch 154920, train_perplexity=818.5877, train_loss=6.7075806

Batch 154930, train_perplexity=788.19684, train_loss=6.669748

Batch 154940, train_perplexity=897.0241, train_loss=6.7990828

Batch 154950, train_perplexity=919.3063, train_loss=6.8236194

Batch 154960, train_perplexity=798.4665, train_loss=6.682693

Batch 154970, train_perplexity=854.3818, train_loss=6.750378

Batch 154980, train_perplexity=785.6231, train_loss=6.666477

Batch 154990, train_perplexity=801.829, train_loss=6.6868954

Batch 155000, train_perplexity=863.66315, train_loss=6.761183

Batch 155010, train_perplexity=878.2382, train_loss=6.777918

Batch 155020, train_perplexity=863.1427, train_loss=6.76058

Batch 155030, train_perplexity=881.03546, train_loss=6.781098

Batch 155040, train_perplexity=795.04266, train_loss=6.6783957

Batch 155050, train_perplexity=946.53326, train_loss=6.852806

Batch 155060, train_perplexity=922.30383, train_loss=6.8268747

Batch 155070, train_perplexity=848.24603, train_loss=6.7431707

Batch 155080, train_perplexity=865.9902, train_loss=6.7638736

Batch 155090, train_perplexity=863.2901, train_loss=6.760751

Batch 155100, train_perplexity=819.27423, train_loss=6.708419

Batch 155110, train_perplexity=869.11554, train_loss=6.767476

Batch 155120, train_perplexity=788.1378, train_loss=6.669673

Batch 155130, train_perplexity=840.1317, train_loss=6.7335587

Batch 155140, train_perplexity=879.732, train_loss=6.7796173

Batch 155150, train_perplexity=871.56244, train_loss=6.7702875

Batch 155160, train_perplexity=790.49243, train_loss=6.672656

Batch 155170, train_perplexity=820.476, train_loss=6.7098846

Batch 155180, train_perplexity=830.70026, train_loss=6.722269

Batch 155190, train_perplexity=800.566, train_loss=6.685319

Batch 155200, train_perplexity=844.4674, train_loss=6.738706

Batch 155210, train_perplexity=837.82495, train_loss=6.730809

Batch 155220, train_perplexity=830.6108, train_loss=6.7221613

Batch 155230, train_perplexity=928.6577, train_loss=6.83374

Batch 155240, train_perplexity=776.34, train_loss=6.6545906

Batch 155250, train_perplexity=818.2361, train_loss=6.707151

Batch 155260, train_perplexity=855.5159, train_loss=6.7517047

Batch 155270, train_perplexity=812.2372, train_loss=6.6997924

Batch 155280, train_perplexity=864.4109, train_loss=6.7620482

Batch 155290, train_perplexity=821.4147, train_loss=6.711028

Batch 155300, train_perplexity=770.07153, train_loss=6.6464834

Batch 155310, train_perplexity=914.7194, train_loss=6.8186173

Batch 155320, train_perplexity=851.71625, train_loss=6.7472534

Batch 155330, train_perplexity=941.64166, train_loss=6.847625

Batch 155340, train_perplexity=821.9178, train_loss=6.7116404

Batch 155350, train_perplexity=817.36536, train_loss=6.706086

Batch 155360, train_perplexity=813.0036, train_loss=6.7007356

Batch 155370, train_perplexity=895.3652, train_loss=6.7972317

Batch 155380, train_perplexity=839.6103, train_loss=6.732938

Batch 155390, train_perplexity=842.1291, train_loss=6.7359333

Batch 155400, train_perplexity=812.04626, train_loss=6.6995573

Batch 155410, train_perplexity=876.2751, train_loss=6.77568

Batch 155420, train_perplexity=829.2233, train_loss=6.7204895

Batch 155430, train_perplexity=844.8915, train_loss=6.739208

Batch 155440, train_perplexity=832.02356, train_loss=6.7238607

Batch 155450, train_perplexity=923.69904, train_loss=6.8283863

Batch 155460, train_perplexity=921.5697, train_loss=6.8260784

Batch 155470, train_perplexity=843.16455, train_loss=6.737162

Batch 155480, train_perplexity=780.43994, train_loss=6.6598577

Batch 155490, train_perplexity=860.0483, train_loss=6.7569885

Batch 155500, train_perplexity=815.2362, train_loss=6.703478

Batch 155510, train_perplexity=868.7762, train_loss=6.7670856

Batch 155520, train_perplexity=838.1342, train_loss=6.7311783

Batch 155530, train_perplexity=832.3069, train_loss=6.724201

Batch 155540, train_perplexity=876.6734, train_loss=6.7761345

Batch 155550, train_perplexity=788.43854, train_loss=6.6700544
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 155560, train_perplexity=881.0972, train_loss=6.781168

Batch 155570, train_perplexity=787.82367, train_loss=6.6692743

Batch 155580, train_perplexity=738.43646, train_loss=6.604535

Batch 155590, train_perplexity=852.15985, train_loss=6.747774

Batch 155600, train_perplexity=847.7099, train_loss=6.7425385

Batch 155610, train_perplexity=878.7367, train_loss=6.7784853

Batch 155620, train_perplexity=843.6427, train_loss=6.737729

Batch 155630, train_perplexity=873.4733, train_loss=6.7724776

Batch 155640, train_perplexity=832.5657, train_loss=6.724512

Batch 155650, train_perplexity=822.62195, train_loss=6.7124968

Batch 155660, train_perplexity=740.3714, train_loss=6.607152

Batch 155670, train_perplexity=897.92194, train_loss=6.800083

Batch 155680, train_perplexity=882.83496, train_loss=6.7831383

Batch 155690, train_perplexity=818.39294, train_loss=6.7073426

Batch 155700, train_perplexity=885.01953, train_loss=6.7856097

Batch 155710, train_perplexity=864.864, train_loss=6.7625723

Batch 155720, train_perplexity=865.8213, train_loss=6.7636786

Batch 155730, train_perplexity=770.77576, train_loss=6.6473975

Batch 155740, train_perplexity=979.9483, train_loss=6.8875

Batch 155750, train_perplexity=777.60156, train_loss=6.656214

Batch 155760, train_perplexity=941.7943, train_loss=6.847787

Batch 155770, train_perplexity=785.14264, train_loss=6.6658654

Batch 155780, train_perplexity=832.0291, train_loss=6.7238674

Batch 155790, train_perplexity=875.90204, train_loss=6.7752542

Batch 155800, train_perplexity=883.4911, train_loss=6.783881

Batch 155810, train_perplexity=867.77386, train_loss=6.765931

Batch 155820, train_perplexity=907.1186, train_loss=6.810273

Batch 155830, train_perplexity=828.2512, train_loss=6.7193165

Batch 155840, train_perplexity=910.1988, train_loss=6.813663

Batch 155850, train_perplexity=789.59546, train_loss=6.6715207

Batch 155860, train_perplexity=783.23895, train_loss=6.663438

Batch 155870, train_perplexity=854.71466, train_loss=6.7507677

Batch 155880, train_perplexity=926.86426, train_loss=6.831807

Batch 155890, train_perplexity=790.3914, train_loss=6.6725283

Batch 155900, train_perplexity=843.77185, train_loss=6.737882

Batch 155910, train_perplexity=892.9673, train_loss=6.79455

Batch 155920, train_perplexity=828.6865, train_loss=6.719842

Batch 155930, train_perplexity=859.56366, train_loss=6.756425

Batch 155940, train_perplexity=909.6404, train_loss=6.8130493

Batch 155950, train_perplexity=843.4778, train_loss=6.7375336

Batch 155960, train_perplexity=941.74225, train_loss=6.8477316

Batch 155970, train_perplexity=823.721, train_loss=6.713832

Batch 155980, train_perplexity=949.6617, train_loss=6.856106

Batch 155990, train_perplexity=835.0697, train_loss=6.727515

Batch 156000, train_perplexity=857.02783, train_loss=6.7534704

Batch 156010, train_perplexity=816.0989, train_loss=6.7045355

Batch 156020, train_perplexity=787.57654, train_loss=6.6689606

Batch 156030, train_perplexity=815.3606, train_loss=6.7036304

Batch 156040, train_perplexity=932.689, train_loss=6.838072

Batch 156050, train_perplexity=745.312, train_loss=6.613803

Batch 156060, train_perplexity=866.87885, train_loss=6.7648993

Batch 156070, train_perplexity=765.73615, train_loss=6.6408377

Batch 156080, train_perplexity=908.16376, train_loss=6.8114247

Batch 156090, train_perplexity=877.0112, train_loss=6.77652

Batch 156100, train_perplexity=859.3334, train_loss=6.756157

Batch 156110, train_perplexity=847.8263, train_loss=6.742676

Batch 156120, train_perplexity=855.2675, train_loss=6.7514143

Batch 156130, train_perplexity=817.00214, train_loss=6.7056417

Batch 156140, train_perplexity=827.64087, train_loss=6.7185793

Batch 156150, train_perplexity=806.9611, train_loss=6.6932755

Batch 156160, train_perplexity=849.53687, train_loss=6.7446914

Batch 156170, train_perplexity=855.36865, train_loss=6.7515326

Batch 156180, train_perplexity=848.37384, train_loss=6.7433214

Batch 156190, train_perplexity=911.1276, train_loss=6.814683

Batch 156200, train_perplexity=823.05634, train_loss=6.7130246

Batch 156210, train_perplexity=864.47974, train_loss=6.762128

Batch 156220, train_perplexity=851.38165, train_loss=6.7468605

Batch 156230, train_perplexity=935.9618, train_loss=6.8415747

Batch 156240, train_perplexity=822.8181, train_loss=6.712735

Batch 156250, train_perplexity=764.24133, train_loss=6.6388836

Batch 156260, train_perplexity=845.92267, train_loss=6.740428

Batch 156270, train_perplexity=801.46814, train_loss=6.686445

Batch 156280, train_perplexity=871.0622, train_loss=6.7697134

Batch 156290, train_perplexity=866.2599, train_loss=6.764185

Batch 156300, train_perplexity=778.5777, train_loss=6.657469

Batch 156310, train_perplexity=909.6126, train_loss=6.813019

Batch 156320, train_perplexity=814.28125, train_loss=6.702306

Batch 156330, train_perplexity=944.2765, train_loss=6.850419

Batch 156340, train_perplexity=930.5035, train_loss=6.835726

Batch 156350, train_perplexity=823.33856, train_loss=6.7133675

Batch 156360, train_perplexity=865.7458, train_loss=6.7635913

Batch 156370, train_perplexity=875.80054, train_loss=6.7751384

Batch 156380, train_perplexity=925.1034, train_loss=6.8299055

Batch 156390, train_perplexity=861.79913, train_loss=6.759022

Batch 156400, train_perplexity=883.76324, train_loss=6.784189

Batch 156410, train_perplexity=770.53656, train_loss=6.647087

Batch 156420, train_perplexity=882.2618, train_loss=6.782489

Batch 156430, train_perplexity=784.1119, train_loss=6.6645517

Batch 156440, train_perplexity=877.42114, train_loss=6.776987

Batch 156450, train_perplexity=703.43365, train_loss=6.5559735

Batch 156460, train_perplexity=833.0708, train_loss=6.7251186

Batch 156470, train_perplexity=877.78937, train_loss=6.7774067

Batch 156480, train_perplexity=754.6169, train_loss=6.62621

Batch 156490, train_perplexity=794.64087, train_loss=6.6778903

Batch 156500, train_perplexity=841.45154, train_loss=6.7351284

Batch 156510, train_perplexity=843.7819, train_loss=6.737894

Batch 156520, train_perplexity=858.59283, train_loss=6.755295

Batch 156530, train_perplexity=789.4166, train_loss=6.671294

Batch 156540, train_perplexity=857.6742, train_loss=6.7542243

Batch 156550, train_perplexity=828.09796, train_loss=6.7191315

Batch 156560, train_perplexity=850.32355, train_loss=6.745617

Batch 156570, train_perplexity=877.0196, train_loss=6.7765293

Batch 156580, train_perplexity=800.70874, train_loss=6.6854973

Batch 156590, train_perplexity=866.3743, train_loss=6.764317

Batch 156600, train_perplexity=857.26776, train_loss=6.7537503

Batch 156610, train_perplexity=721.449, train_loss=6.5812616

Batch 156620, train_perplexity=879.7857, train_loss=6.7796783

Batch 156630, train_perplexity=817.64874, train_loss=6.706433

Batch 156640, train_perplexity=799.2615, train_loss=6.683688

Batch 156650, train_perplexity=913.2568, train_loss=6.817017

Batch 156660, train_perplexity=958.3677, train_loss=6.8652315

Batch 156670, train_perplexity=749.575, train_loss=6.6195064

Batch 156680, train_perplexity=824.1469, train_loss=6.714349

Batch 156690, train_perplexity=866.5747, train_loss=6.7645483

Batch 156700, train_perplexity=814.1275, train_loss=6.702117

Batch 156710, train_perplexity=832.5482, train_loss=6.724491

Batch 156720, train_perplexity=822.2118, train_loss=6.711998

Batch 156730, train_perplexity=853.1222, train_loss=6.748903

Batch 156740, train_perplexity=853.9537, train_loss=6.749877

Batch 156750, train_perplexity=882.9528, train_loss=6.783272

Batch 156760, train_perplexity=874.30634, train_loss=6.773431

Batch 156770, train_perplexity=913.752, train_loss=6.8175592

Batch 156780, train_perplexity=851.48804, train_loss=6.7469854

Batch 156790, train_perplexity=852.4858, train_loss=6.7481565

Batch 156800, train_perplexity=897.835, train_loss=6.7999864

Batch 156810, train_perplexity=904.5417, train_loss=6.8074284

Batch 156820, train_perplexity=893.27734, train_loss=6.794897

Batch 156830, train_perplexity=807.2259, train_loss=6.6936035

Batch 156840, train_perplexity=871.9561, train_loss=6.770739

Batch 156850, train_perplexity=827.3228, train_loss=6.718195

Batch 156860, train_perplexity=873.7104, train_loss=6.772749
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 156870, train_perplexity=837.4567, train_loss=6.7303696

Batch 156880, train_perplexity=851.7495, train_loss=6.7472925

Batch 156890, train_perplexity=793.33887, train_loss=6.6762505

Batch 156900, train_perplexity=801.03644, train_loss=6.6859064

Batch 156910, train_perplexity=802.18005, train_loss=6.687333

Batch 156920, train_perplexity=855.40985, train_loss=6.7515807

Batch 156930, train_perplexity=784.63477, train_loss=6.6652184

Batch 156940, train_perplexity=799.2028, train_loss=6.6836147

Batch 156950, train_perplexity=794.89404, train_loss=6.678209

Batch 156960, train_perplexity=833.81396, train_loss=6.7260103

Batch 156970, train_perplexity=733.64996, train_loss=6.598032

Batch 156980, train_perplexity=823.4324, train_loss=6.7134814

Batch 156990, train_perplexity=833.5906, train_loss=6.7257423

Batch 157000, train_perplexity=816.6462, train_loss=6.705206

Batch 157010, train_perplexity=857.3062, train_loss=6.753795

Batch 157020, train_perplexity=821.1778, train_loss=6.7107396

Batch 157030, train_perplexity=907.1242, train_loss=6.8102794

Batch 157040, train_perplexity=818.4831, train_loss=6.707453

Batch 157050, train_perplexity=782.02014, train_loss=6.6618805

Batch 157060, train_perplexity=832.2148, train_loss=6.7240906

Batch 157070, train_perplexity=892.4871, train_loss=6.794012

Batch 157080, train_perplexity=823.8632, train_loss=6.7140045

Batch 157090, train_perplexity=819.1965, train_loss=6.708324

Batch 157100, train_perplexity=829.5219, train_loss=6.7208495

Batch 157110, train_perplexity=869.0393, train_loss=6.7673883

Batch 157120, train_perplexity=775.5556, train_loss=6.6535797

Batch 157130, train_perplexity=797.8332, train_loss=6.6818995

Batch 157140, train_perplexity=799.0165, train_loss=6.6833816

Batch 157150, train_perplexity=837.62805, train_loss=6.730574

Batch 157160, train_perplexity=906.57025, train_loss=6.8096685

Batch 157170, train_perplexity=846.2366, train_loss=6.740799

Batch 157180, train_perplexity=824.538, train_loss=6.7148232

Batch 157190, train_perplexity=796.5898, train_loss=6.68034

Batch 157200, train_perplexity=787.8526, train_loss=6.669311

Batch 157210, train_perplexity=809.2407, train_loss=6.6960964

Batch 157220, train_perplexity=778.1442, train_loss=6.656912

Batch 157230, train_perplexity=769.61194, train_loss=6.6458864

Batch 157240, train_perplexity=896.6538, train_loss=6.79867

Batch 157250, train_perplexity=884.4285, train_loss=6.7849417

Batch 157260, train_perplexity=818.1475, train_loss=6.7070427

Batch 157270, train_perplexity=875.6214, train_loss=6.774934

Batch 157280, train_perplexity=823.03705, train_loss=6.7130013

Batch 157290, train_perplexity=770.5211, train_loss=6.647067

Batch 157300, train_perplexity=841.6032, train_loss=6.7353086

Batch 157310, train_perplexity=856.64667, train_loss=6.7530255

Batch 157320, train_perplexity=826.0433, train_loss=6.716647

Batch 157330, train_perplexity=785.74976, train_loss=6.6666384

Batch 157340, train_perplexity=808.9097, train_loss=6.6956873

Batch 157350, train_perplexity=776.93256, train_loss=6.6553535

Batch 157360, train_perplexity=861.199, train_loss=6.7583256

Batch 157370, train_perplexity=823.8695, train_loss=6.714012

Batch 157380, train_perplexity=868.6996, train_loss=6.7669973

Batch 157390, train_perplexity=818.6123, train_loss=6.7076106

Batch 157400, train_perplexity=862.60535, train_loss=6.7599573

Batch 157410, train_perplexity=849.17847, train_loss=6.7442694

Batch 157420, train_perplexity=882.13434, train_loss=6.7823443

Batch 157430, train_perplexity=846.68744, train_loss=6.7413316

Batch 157440, train_perplexity=795.8183, train_loss=6.679371

Batch 157450, train_perplexity=855.4323, train_loss=6.751607

Batch 157460, train_perplexity=813.4771, train_loss=6.701318

Batch 157470, train_perplexity=917.26404, train_loss=6.8213954

Batch 157480, train_perplexity=806.1227, train_loss=6.692236

Batch 157490, train_perplexity=814.59576, train_loss=6.702692

Batch 157500, train_perplexity=760.0076, train_loss=6.6333284

Batch 157510, train_perplexity=877.1986, train_loss=6.7767334

Batch 157520, train_perplexity=793.85956, train_loss=6.6769066

Batch 157530, train_perplexity=860.1697, train_loss=6.7571297

Batch 157540, train_perplexity=836.08014, train_loss=6.7287245

Batch 157550, train_perplexity=728.37115, train_loss=6.590811

Batch 157560, train_perplexity=867.706, train_loss=6.765853

Batch 157570, train_perplexity=870.08295, train_loss=6.7685885

Batch 157580, train_perplexity=798.2674, train_loss=6.6824436

Batch 157590, train_perplexity=764.17206, train_loss=6.638793

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Batch 157600, train_perplexity=818.8329, train_loss=6.70788

Batch 157610, train_perplexity=880.6898, train_loss=6.7807055

Batch 157620, train_perplexity=815.4776, train_loss=6.703774

Batch 157630, train_perplexity=824.95056, train_loss=6.7153234

Batch 157640, train_perplexity=827.8177, train_loss=6.718793

Batch 157650, train_perplexity=819.09143, train_loss=6.7081957

Batch 157660, train_perplexity=841.98175, train_loss=6.7357583

Batch 157670, train_perplexity=864.90607, train_loss=6.762621

Batch 157680, train_perplexity=848.32697, train_loss=6.743266

Batch 157690, train_perplexity=855.6648, train_loss=6.7518787

Batch 157700, train_perplexity=898.9154, train_loss=6.801189

Batch 157710, train_perplexity=852.82166, train_loss=6.7485504

Batch 157720, train_perplexity=833.88635, train_loss=6.726097

Batch 157730, train_perplexity=846.43353, train_loss=6.7410316

Batch 157740, train_perplexity=852.99084, train_loss=6.748749

Batch 157750, train_perplexity=853.0152, train_loss=6.7487774

Batch 157760, train_perplexity=763.7386, train_loss=6.6382256

Batch 157770, train_perplexity=749.2269, train_loss=6.619042

Batch 157780, train_perplexity=854.46204, train_loss=6.750472

Batch 157790, train_perplexity=883.54834, train_loss=6.783946

Batch 157800, train_perplexity=908.7689, train_loss=6.812091

Batch 157810, train_perplexity=837.87213, train_loss=6.7308655

Batch 157820, train_perplexity=958.4102, train_loss=6.865276

Batch 157830, train_perplexity=773.2651, train_loss=6.650622

Batch 157840, train_perplexity=809.98193, train_loss=6.697012

Batch 157850, train_perplexity=862.46515, train_loss=6.7597947

Batch 157860, train_perplexity=796.5077, train_loss=6.680237

Batch 157870, train_perplexity=809.0883, train_loss=6.695908

Batch 157880, train_perplexity=886.83984, train_loss=6.7876644

Batch 157890, train_perplexity=803.57056, train_loss=6.689065

Batch 157900, train_perplexity=822.7357, train_loss=6.712635

Batch 157910, train_perplexity=761.77094, train_loss=6.635646

Batch 157920, train_perplexity=835.20276, train_loss=6.7276745

Batch 157930, train_perplexity=812.1016, train_loss=6.6996255

Batch 157940, train_perplexity=798.2781, train_loss=6.682457

Batch 157950, train_perplexity=758.94073, train_loss=6.6319237

Batch 157960, train_perplexity=872.4564, train_loss=6.7713127

Batch 157970, train_perplexity=774.08295, train_loss=6.651679

Batch 157980, train_perplexity=850.7417, train_loss=6.7461085

Batch 157990, train_perplexity=825.8156, train_loss=6.7163715

Batch 158000, train_perplexity=913.0221, train_loss=6.81676

Batch 158010, train_perplexity=995.789, train_loss=6.9035354

Batch 158020, train_perplexity=810.9783, train_loss=6.698241

Batch 158030, train_perplexity=833.5981, train_loss=6.7257514

Batch 158040, train_perplexity=815.2922, train_loss=6.7035465

Batch 158050, train_perplexity=760.8383, train_loss=6.634421

Batch 158060, train_perplexity=849.6912, train_loss=6.744873

Batch 158070, train_perplexity=829.1506, train_loss=6.720402

Batch 158080, train_perplexity=823.2702, train_loss=6.7132845

Batch 158090, train_perplexity=869.2034, train_loss=6.767577

Batch 158100, train_perplexity=786.7294, train_loss=6.6678843
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 158110, train_perplexity=865.5175, train_loss=6.7633276

Batch 158120, train_perplexity=833.6009, train_loss=6.7257547

Batch 158130, train_perplexity=860.896, train_loss=6.7579737

Batch 158140, train_perplexity=900.67456, train_loss=6.803144

Batch 158150, train_perplexity=882.67334, train_loss=6.782955

Batch 158160, train_perplexity=797.93134, train_loss=6.6820226

Batch 158170, train_perplexity=818.7446, train_loss=6.7077723

Batch 158180, train_perplexity=792.0201, train_loss=6.674587

Batch 158190, train_perplexity=892.65906, train_loss=6.7942047

Batch 158200, train_perplexity=800.187, train_loss=6.6848454

Batch 158210, train_perplexity=884.28937, train_loss=6.7847843

Batch 158220, train_perplexity=794.8076, train_loss=6.6781

Batch 158230, train_perplexity=790.6937, train_loss=6.6729107

Batch 158240, train_perplexity=794.0201, train_loss=6.677109

Batch 158250, train_perplexity=775.078, train_loss=6.6529636

Batch 158260, train_perplexity=725.53986, train_loss=6.586916

Batch 158270, train_perplexity=800.53125, train_loss=6.6852756

Batch 158280, train_perplexity=786.96124, train_loss=6.668179

Batch 158290, train_perplexity=816.1662, train_loss=6.704618

Batch 158300, train_perplexity=783.91003, train_loss=6.6642942

Batch 158310, train_perplexity=869.8249, train_loss=6.768292

Batch 158320, train_perplexity=793.9701, train_loss=6.677046

Batch 158330, train_perplexity=892.54755, train_loss=6.79408

Batch 158340, train_perplexity=793.6174, train_loss=6.6766014

Batch 158350, train_perplexity=795.6915, train_loss=6.6792116

Batch 158360, train_perplexity=782.6297, train_loss=6.6626596

Batch 158370, train_perplexity=815.209, train_loss=6.7034445

Batch 158380, train_perplexity=764.4717, train_loss=6.639185

Batch 158390, train_perplexity=886.2205, train_loss=6.786966

Batch 158400, train_perplexity=811.5786, train_loss=6.6989813

Batch 158410, train_perplexity=811.99207, train_loss=6.6994905

Batch 158420, train_perplexity=782.97235, train_loss=6.6630974

Batch 158430, train_perplexity=842.85944, train_loss=6.7368

Batch 158440, train_perplexity=819.55396, train_loss=6.7087603

Batch 158450, train_perplexity=812.3003, train_loss=6.69987

Batch 158460, train_perplexity=825.0682, train_loss=6.715466

Batch 158470, train_perplexity=870.6353, train_loss=6.769223

Batch 158480, train_perplexity=891.6636, train_loss=6.793089

Batch 158490, train_perplexity=695.84033, train_loss=6.5451202

Batch 158500, train_perplexity=836.64685, train_loss=6.729402

Batch 158510, train_perplexity=804.7113, train_loss=6.6904836

Batch 158520, train_perplexity=795.9352, train_loss=6.6795177

Batch 158530, train_perplexity=955.2821, train_loss=6.8620067

Batch 158540, train_perplexity=829.33484, train_loss=6.720624

Batch 158550, train_perplexity=767.12274, train_loss=6.642647

Batch 158560, train_perplexity=765.1011, train_loss=6.640008

Batch 158570, train_perplexity=826.5003, train_loss=6.7172003

Batch 158580, train_perplexity=778.66345, train_loss=6.657579

Batch 158590, train_perplexity=858.9204, train_loss=6.7556763

Batch 158600, train_perplexity=831.1512, train_loss=6.7228117

Batch 158610, train_perplexity=903.0187, train_loss=6.805743

Batch 158620, train_perplexity=823.04333, train_loss=6.713009

Batch 158630, train_perplexity=826.77936, train_loss=6.717538

Batch 158640, train_perplexity=778.3528, train_loss=6.65718

Batch 158650, train_perplexity=816.9297, train_loss=6.705553

Batch 158660, train_perplexity=884.3585, train_loss=6.7848625

Batch 158670, train_perplexity=776.5703, train_loss=6.654887

Batch 158680, train_perplexity=834.21564, train_loss=6.726492

Batch 158690, train_perplexity=878.40906, train_loss=6.7781124

Batch 158700, train_perplexity=749.2809, train_loss=6.619114

Batch 158710, train_perplexity=839.73236, train_loss=6.7330832

Batch 158720, train_perplexity=811.85266, train_loss=6.699319

Batch 158730, train_perplexity=830.8801, train_loss=6.7224855

Batch 158740, train_perplexity=829.40564, train_loss=6.7207093

Batch 158750, train_perplexity=809.10565, train_loss=6.6959295

Batch 158760, train_perplexity=891.5883, train_loss=6.7930045

Batch 158770, train_perplexity=933.757, train_loss=6.839216

Batch 158780, train_perplexity=827.81213, train_loss=6.7187862

Batch 158790, train_perplexity=809.3931, train_loss=6.696285

Batch 158800, train_perplexity=730.7141, train_loss=6.5940223

Batch 158810, train_perplexity=800.2286, train_loss=6.6848974

Batch 158820, train_perplexity=821.2028, train_loss=6.71077

Batch 158830, train_perplexity=854.00745, train_loss=6.74994

Batch 158840, train_perplexity=840.03876, train_loss=6.733448

Batch 158850, train_perplexity=771.856, train_loss=6.648798

Batch 158860, train_perplexity=828.3357, train_loss=6.7194185

Batch 158870, train_perplexity=866.07153, train_loss=6.7639675

Batch 158880, train_perplexity=833.3183, train_loss=6.7254157

Batch 158890, train_perplexity=729.672, train_loss=6.592595

Batch 158900, train_perplexity=786.36487, train_loss=6.667421

Batch 158910, train_perplexity=792.2052, train_loss=6.6748204

Batch 158920, train_perplexity=774.6664, train_loss=6.6524324

Batch 158930, train_perplexity=813.187, train_loss=6.700961

Batch 158940, train_perplexity=845.28644, train_loss=6.7396755

Batch 158950, train_perplexity=871.85175, train_loss=6.7706194

Batch 158960, train_perplexity=820.7065, train_loss=6.7101655

Batch 158970, train_perplexity=788.5716, train_loss=6.670223

Batch 158980, train_perplexity=803.47974, train_loss=6.688952

Batch 158990, train_perplexity=776.5925, train_loss=6.654916

Batch 159000, train_perplexity=755.13885, train_loss=6.6269016

Batch 159010, train_perplexity=859.75635, train_loss=6.756649

Batch 159020, train_perplexity=760.17975, train_loss=6.633555

Batch 159030, train_perplexity=872.9533, train_loss=6.771882

Batch 159040, train_perplexity=825.5542, train_loss=6.716055

Batch 159050, train_perplexity=840.6502, train_loss=6.7341757

Batch 159060, train_perplexity=830.08417, train_loss=6.721527

Batch 159070, train_perplexity=836.6772, train_loss=6.7294383

Batch 159080, train_perplexity=844.2049, train_loss=6.738395

Batch 159090, train_perplexity=792.724, train_loss=6.675475

Batch 159100, train_perplexity=740.8609, train_loss=6.607813

Batch 159110, train_perplexity=813.2374, train_loss=6.701023

Batch 159120, train_perplexity=792.8117, train_loss=6.6755857

Batch 159130, train_perplexity=756.8252, train_loss=6.6291323

Batch 159140, train_perplexity=695.72656, train_loss=6.5449567

Batch 159150, train_perplexity=835.0594, train_loss=6.727503

Batch 159160, train_perplexity=829.5642, train_loss=6.7209005

Batch 159170, train_perplexity=723.3573, train_loss=6.5839033

Batch 159180, train_perplexity=834.7648, train_loss=6.72715

Batch 159190, train_perplexity=874.6929, train_loss=6.773873

Batch 159200, train_perplexity=858.3816, train_loss=6.7550488

Batch 159210, train_perplexity=814.0425, train_loss=6.7020125

Batch 159220, train_perplexity=828.59485, train_loss=6.7197313

Batch 159230, train_perplexity=744.31256, train_loss=6.612461

Batch 159240, train_perplexity=923.2578, train_loss=6.8279085

Batch 159250, train_perplexity=806.46796, train_loss=6.692664

Batch 159260, train_perplexity=932.0755, train_loss=6.837414

Batch 159270, train_perplexity=730.464, train_loss=6.59368

Batch 159280, train_perplexity=909.30383, train_loss=6.8126793

Batch 159290, train_perplexity=832.9993, train_loss=6.725033

Batch 159300, train_perplexity=783.73584, train_loss=6.664072

Batch 159310, train_perplexity=787.4406, train_loss=6.668788

Batch 159320, train_perplexity=796.3186, train_loss=6.6799994

Batch 159330, train_perplexity=850.2749, train_loss=6.7455597

Batch 159340, train_perplexity=908.66754, train_loss=6.8119793

Batch 159350, train_perplexity=841.99255, train_loss=6.735771

Batch 159360, train_perplexity=810.53326, train_loss=6.6976924

Batch 159370, train_perplexity=911.02246, train_loss=6.8145676

Batch 159380, train_perplexity=737.937, train_loss=6.6038585

Batch 159390, train_perplexity=749.3116, train_loss=6.619155

Batch 159400, train_perplexity=820.22565, train_loss=6.7095795

Batch 159410, train_perplexity=827.2794, train_loss=6.7181425
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 159420, train_perplexity=852.1956, train_loss=6.747816

Batch 159430, train_perplexity=718.85974, train_loss=6.5776663

Batch 159440, train_perplexity=780.16754, train_loss=6.6595087

Batch 159450, train_perplexity=794.7239, train_loss=6.6779947

Batch 159460, train_perplexity=859.6506, train_loss=6.756526

Batch 159470, train_perplexity=801.1434, train_loss=6.68604

Batch 159480, train_perplexity=807.3175, train_loss=6.693717

Batch 159490, train_perplexity=742.78094, train_loss=6.610401

Batch 159500, train_perplexity=864.523, train_loss=6.762178

Batch 159510, train_perplexity=791.78186, train_loss=6.674286

Batch 159520, train_perplexity=903.1035, train_loss=6.805837

Batch 159530, train_perplexity=858.3521, train_loss=6.7550144

Batch 159540, train_perplexity=860.14343, train_loss=6.757099

Batch 159550, train_perplexity=747.3615, train_loss=6.616549

Batch 159560, train_perplexity=874.5932, train_loss=6.773759

Batch 159570, train_perplexity=974.964, train_loss=6.8824005

Batch 159580, train_perplexity=855.766, train_loss=6.751997

Batch 159590, train_perplexity=760.34033, train_loss=6.633766

Batch 159600, train_perplexity=846.70074, train_loss=6.7413473

Batch 159610, train_perplexity=843.47296, train_loss=6.737528

Batch 159620, train_perplexity=847.5967, train_loss=6.742405

Batch 159630, train_perplexity=845.9324, train_loss=6.7404394

Batch 159640, train_perplexity=780.935, train_loss=6.660492

Batch 159650, train_perplexity=830.0644, train_loss=6.7215033

Batch 159660, train_perplexity=890.6578, train_loss=6.7919602

Batch 159670, train_perplexity=741.292, train_loss=6.6083946

Batch 159680, train_perplexity=773.9457, train_loss=6.6515017

Batch 159690, train_perplexity=769.83655, train_loss=6.6461782

Batch 159700, train_perplexity=801.12274, train_loss=6.686014

Batch 159710, train_perplexity=864.6405, train_loss=6.762314

Batch 159720, train_perplexity=897.87317, train_loss=6.800029

Batch 159730, train_perplexity=755.37836, train_loss=6.6272187

Batch 159740, train_perplexity=813.35144, train_loss=6.7011633

Batch 159750, train_perplexity=784.7029, train_loss=6.665305

Batch 159760, train_perplexity=843.00574, train_loss=6.736974

Batch 159770, train_perplexity=768.32715, train_loss=6.6442156

Batch 159780, train_perplexity=769.27954, train_loss=6.6454544

Batch 159790, train_perplexity=782.7275, train_loss=6.6627846

Batch 159800, train_perplexity=883.85175, train_loss=6.7842894

Batch 159810, train_perplexity=872.24304, train_loss=6.771068

Batch 159820, train_perplexity=806.66486, train_loss=6.6929083

Batch 159830, train_perplexity=839.78406, train_loss=6.7331448

Batch 159840, train_perplexity=862.1374, train_loss=6.7594147

Batch 159850, train_perplexity=914.8991, train_loss=6.818814

Batch 159860, train_perplexity=870.0776, train_loss=6.7685823

Batch 159870, train_perplexity=832.99854, train_loss=6.725032

Batch 159880, train_perplexity=838.3093, train_loss=6.731387

Batch 159890, train_perplexity=850.01624, train_loss=6.7452555

Batch 159900, train_perplexity=904.31006, train_loss=6.8071723

Batch 159910, train_perplexity=845.6605, train_loss=6.740118

Batch 159920, train_perplexity=774.9708, train_loss=6.6528254

Batch 159930, train_perplexity=944.75574, train_loss=6.8509264

Batch 159940, train_perplexity=774.52747, train_loss=6.652253

Batch 159950, train_perplexity=792.9599, train_loss=6.6757727

Batch 159960, train_perplexity=815.54565, train_loss=6.7038574

Batch 159970, train_perplexity=831.6384, train_loss=6.7233977

Batch 159980, train_perplexity=861.447, train_loss=6.7586136

Batch 159990, train_perplexity=816.72174, train_loss=6.7052984

Batch 160000, train_perplexity=934.8106, train_loss=6.840344

Batch 160010, train_perplexity=750.8596, train_loss=6.6212187

Batch 160020, train_perplexity=828.65173, train_loss=6.7198

Batch 160030, train_perplexity=805.78217, train_loss=6.6918135

Batch 160040, train_perplexity=875.35175, train_loss=6.774626

Batch 160050, train_perplexity=830.4571, train_loss=6.7219763

Batch 160060, train_perplexity=768.3689, train_loss=6.64427

Batch 160070, train_perplexity=868.384, train_loss=6.766634

Batch 160080, train_perplexity=837.5877, train_loss=6.730526

Batch 160090, train_perplexity=760.3505, train_loss=6.6337795

Batch 160100, train_perplexity=889.0331, train_loss=6.7901344

Batch 160110, train_perplexity=745.9637, train_loss=6.614677

Batch 160120, train_perplexity=809.6402, train_loss=6.69659

Batch 160130, train_perplexity=832.7118, train_loss=6.7246876

Batch 160140, train_perplexity=779.24774, train_loss=6.658329

Batch 160150, train_perplexity=827.4704, train_loss=6.7183733

Batch 160160, train_perplexity=825.366, train_loss=6.715827

Batch 160170, train_perplexity=824.2538, train_loss=6.7144785

Batch 160180, train_perplexity=792.43604, train_loss=6.675112

Batch 160190, train_perplexity=797.5278, train_loss=6.6815166

Batch 160200, train_perplexity=811.1917, train_loss=6.6985044

Batch 160210, train_perplexity=848.44104, train_loss=6.7434006

Batch 160220, train_perplexity=914.27985, train_loss=6.8181367

Batch 160230, train_perplexity=805.76685, train_loss=6.6917944

Batch 160240, train_perplexity=832.3589, train_loss=6.7242637

Batch 160250, train_perplexity=745.43427, train_loss=6.613967

Batch 160260, train_perplexity=852.1119, train_loss=6.747718

Batch 160270, train_perplexity=808.71185, train_loss=6.6954427

Batch 160280, train_perplexity=842.76215, train_loss=6.736685

Batch 160290, train_perplexity=883.87366, train_loss=6.784314

Batch 160300, train_perplexity=728.613, train_loss=6.5911427

Batch 160310, train_perplexity=783.19116, train_loss=6.663377

Batch 160320, train_perplexity=800.682, train_loss=6.685464

Batch 160330, train_perplexity=797.14716, train_loss=6.6810393

Batch 160340, train_perplexity=830.9475, train_loss=6.7225666

Batch 160350, train_perplexity=824.3926, train_loss=6.714647

Batch 160360, train_perplexity=801.8168, train_loss=6.68688

Batch 160370, train_perplexity=905.7946, train_loss=6.8088126

Batch 160380, train_perplexity=845.1095, train_loss=6.739466

Batch 160390, train_perplexity=817.3969, train_loss=6.706125

Batch 160400, train_perplexity=796.4029, train_loss=6.680105

Batch 160410, train_perplexity=810.6519, train_loss=6.697839

Batch 160420, train_perplexity=839.6655, train_loss=6.7330036

Batch 160430, train_perplexity=831.8363, train_loss=6.7236357

Batch 160440, train_perplexity=827.9274, train_loss=6.7189255

Batch 160450, train_perplexity=820.22644, train_loss=6.7095804

Batch 160460, train_perplexity=768.4114, train_loss=6.6443253

Batch 160470, train_perplexity=862.60126, train_loss=6.7599525

Batch 160480, train_perplexity=830.1079, train_loss=6.7215557

Batch 160490, train_perplexity=740.3757, train_loss=6.6071577

Batch 160500, train_perplexity=782.9843, train_loss=6.6631126

Batch 160510, train_perplexity=837.3305, train_loss=6.730219

Batch 160520, train_perplexity=798.2027, train_loss=6.6823626

Batch 160530, train_perplexity=775.12604, train_loss=6.6530256

Batch 160540, train_perplexity=789.4859, train_loss=6.671382

Batch 160550, train_perplexity=773.2584, train_loss=6.6506133

Batch 160560, train_perplexity=837.3569, train_loss=6.7302504

Batch 160570, train_perplexity=818.2802, train_loss=6.707205

Batch 160580, train_perplexity=754.9977, train_loss=6.6267147

Batch 160590, train_perplexity=867.23114, train_loss=6.7653055

Batch 160600, train_perplexity=734.037, train_loss=6.5985594

Batch 160610, train_perplexity=846.8784, train_loss=6.741557

Batch 160620, train_perplexity=779.35956, train_loss=6.6584725

Batch 160630, train_perplexity=848.80884, train_loss=6.743834

Batch 160640, train_perplexity=827.0341, train_loss=6.717846

Batch 160650, train_perplexity=797.2783, train_loss=6.681204

Batch 160660, train_perplexity=784.22107, train_loss=6.664691

Batch 160670, train_perplexity=879.11554, train_loss=6.7789164

Batch 160680, train_perplexity=737.67633, train_loss=6.603505

Batch 160690, train_perplexity=712.9731, train_loss=6.5694437

Batch 160700, train_perplexity=914.1861, train_loss=6.818034

Batch 160710, train_perplexity=810.347, train_loss=6.6974626

Batch 160720, train_perplexity=817.9342, train_loss=6.706782
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 160730, train_perplexity=772.3379, train_loss=6.649422

Batch 160740, train_perplexity=851.42834, train_loss=6.7469153

Batch 160750, train_perplexity=835.52496, train_loss=6.7280602

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Batch 160760, train_perplexity=819.7642, train_loss=6.709017

Batch 160770, train_perplexity=841.6674, train_loss=6.735385

Batch 160780, train_perplexity=782.52966, train_loss=6.662532

Batch 160790, train_perplexity=783.54004, train_loss=6.663822

Batch 160800, train_perplexity=794.195, train_loss=6.677329

Batch 160810, train_perplexity=884.93896, train_loss=6.7855186

Batch 160820, train_perplexity=891.6968, train_loss=6.793126

Batch 160830, train_perplexity=793.41077, train_loss=6.676341

Batch 160840, train_perplexity=898.8181, train_loss=6.8010807

Batch 160850, train_perplexity=812.0869, train_loss=6.6996074

Batch 160860, train_perplexity=836.4933, train_loss=6.7292185

Batch 160870, train_perplexity=792.5683, train_loss=6.6752787

Batch 160880, train_perplexity=813.80536, train_loss=6.701721

Batch 160890, train_perplexity=775.88074, train_loss=6.653999

Batch 160900, train_perplexity=834.9065, train_loss=6.7273197

Batch 160910, train_perplexity=793.2341, train_loss=6.6761184

Batch 160920, train_perplexity=868.3525, train_loss=6.7665977

Batch 160930, train_perplexity=848.0669, train_loss=6.7429595

Batch 160940, train_perplexity=841.2854, train_loss=6.734931

Batch 160950, train_perplexity=755.0985, train_loss=6.626848

Batch 160960, train_perplexity=885.92773, train_loss=6.7866354

Batch 160970, train_perplexity=867.7962, train_loss=6.765957

Batch 160980, train_perplexity=768.0531, train_loss=6.643859

Batch 160990, train_perplexity=855.855, train_loss=6.752101

Batch 161000, train_perplexity=782.0253, train_loss=6.661887

Batch 161010, train_perplexity=868.0271, train_loss=6.766223

Batch 161020, train_perplexity=856.3983, train_loss=6.7527356

Batch 161030, train_perplexity=874.9261, train_loss=6.7741394

Batch 161040, train_perplexity=783.7594, train_loss=6.664102

Batch 161050, train_perplexity=777.926, train_loss=6.6566315

Batch 161060, train_perplexity=884.02203, train_loss=6.784482

Batch 161070, train_perplexity=843.9767, train_loss=6.738125

Batch 161080, train_perplexity=762.71375, train_loss=6.636883

Batch 161090, train_perplexity=816.74274, train_loss=6.705324

Batch 161100, train_perplexity=839.49615, train_loss=6.732802

Batch 161110, train_perplexity=834.97614, train_loss=6.727403

Batch 161120, train_perplexity=820.11694, train_loss=6.709447

Batch 161130, train_perplexity=904.684, train_loss=6.8075857

Batch 161140, train_perplexity=895.3225, train_loss=6.797184

Batch 161150, train_perplexity=894.63885, train_loss=6.79642

Batch 161160, train_perplexity=861.3296, train_loss=6.758477

Batch 161170, train_perplexity=887.5133, train_loss=6.7884235

Batch 161180, train_perplexity=839.9586, train_loss=6.7333527

Batch 161190, train_perplexity=839.14716, train_loss=6.732386

Batch 161200, train_perplexity=860.22955, train_loss=6.7571993

Batch 161210, train_perplexity=820.5957, train_loss=6.7100306

Batch 161220, train_perplexity=881.68225, train_loss=6.7818317

Batch 161230, train_perplexity=788.03186, train_loss=6.6695385

Batch 161240, train_perplexity=863.74384, train_loss=6.7612762

Batch 161250, train_perplexity=817.1962, train_loss=6.705879

Batch 161260, train_perplexity=803.12506, train_loss=6.6885104

Batch 161270, train_perplexity=807.4738, train_loss=6.6939106

Batch 161280, train_perplexity=794.5579, train_loss=6.677786

Batch 161290, train_perplexity=810.9891, train_loss=6.6982546

Batch 161300, train_perplexity=703.0493, train_loss=6.555427

Batch 161310, train_perplexity=824.89233, train_loss=6.715253

Batch 161320, train_perplexity=852.398, train_loss=6.7480536

Batch 161330, train_perplexity=797.8906, train_loss=6.6819715

Batch 161340, train_perplexity=806.8718, train_loss=6.693165

Batch 161350, train_perplexity=945.88446, train_loss=6.8521204

Batch 161360, train_perplexity=756.79126, train_loss=6.6290874

Batch 161370, train_perplexity=804.76886, train_loss=6.690555

Batch 161380, train_perplexity=722.9566, train_loss=6.583349

Batch 161390, train_perplexity=819.7525, train_loss=6.7090025

Batch 161400, train_perplexity=761.91766, train_loss=6.6358385

Batch 161410, train_perplexity=782.08276, train_loss=6.6619606

Batch 161420, train_perplexity=857.1852, train_loss=6.753654

Batch 161430, train_perplexity=866.86896, train_loss=6.764888

Batch 161440, train_perplexity=795.5345, train_loss=6.679014

Batch 161450, train_perplexity=766.1372, train_loss=6.641361

Batch 161460, train_perplexity=823.9021, train_loss=6.7140517

Batch 161470, train_perplexity=788.1288, train_loss=6.6696615

Batch 161480, train_perplexity=789.4354, train_loss=6.671318

Batch 161490, train_perplexity=814.92096, train_loss=6.703091

Batch 161500, train_perplexity=907.67804, train_loss=6.8108897

Batch 161510, train_perplexity=877.3048, train_loss=6.7768545

Batch 161520, train_perplexity=824.70197, train_loss=6.715022

Batch 161530, train_perplexity=797.3605, train_loss=6.681307

Batch 161540, train_perplexity=817.26984, train_loss=6.7059693

Batch 161550, train_perplexity=754.49603, train_loss=6.62605

Batch 161560, train_perplexity=793.2428, train_loss=6.6761293

Batch 161570, train_perplexity=837.7003, train_loss=6.7306604

Batch 161580, train_perplexity=833.0176, train_loss=6.7250547

Batch 161590, train_perplexity=863.9947, train_loss=6.7615666

Batch 161600, train_perplexity=813.3096, train_loss=6.701112

Batch 161610, train_perplexity=780.6126, train_loss=6.660079

Batch 161620, train_perplexity=907.16876, train_loss=6.8103285

Batch 161630, train_perplexity=897.7678, train_loss=6.7999115

Batch 161640, train_perplexity=815.77124, train_loss=6.704134

Batch 161650, train_perplexity=799.7411, train_loss=6.684288

Batch 161660, train_perplexity=834.2801, train_loss=6.726569

Batch 161670, train_perplexity=782.7633, train_loss=6.6628304

Batch 161680, train_perplexity=782.51404, train_loss=6.662512

Batch 161690, train_perplexity=777.8422, train_loss=6.6565237

Batch 161700, train_perplexity=863.48114, train_loss=6.760972

Batch 161710, train_perplexity=899.2901, train_loss=6.8016057

Batch 161720, train_perplexity=766.74603, train_loss=6.6421556

Batch 161730, train_perplexity=878.4518, train_loss=6.778161

Batch 161740, train_perplexity=710.7613, train_loss=6.5663366

Batch 161750, train_perplexity=741.4935, train_loss=6.6086664

Batch 161760, train_perplexity=783.96985, train_loss=6.6643705

Batch 161770, train_perplexity=727.6221, train_loss=6.5897818

Batch 161780, train_perplexity=830.9055, train_loss=6.722516

Batch 161790, train_perplexity=836.7626, train_loss=6.7295403

Batch 161800, train_perplexity=836.4785, train_loss=6.729201

Batch 161810, train_perplexity=867.90625, train_loss=6.7660837

Batch 161820, train_perplexity=840.90845, train_loss=6.734483

Batch 161830, train_perplexity=884.60736, train_loss=6.785144

Batch 161840, train_perplexity=769.2553, train_loss=6.645423

Batch 161850, train_perplexity=821.5287, train_loss=6.711167

Batch 161860, train_perplexity=797.65784, train_loss=6.6816797

Batch 161870, train_perplexity=834.7528, train_loss=6.7271357

Batch 161880, train_perplexity=711.449, train_loss=6.5673037

Batch 161890, train_perplexity=937.493, train_loss=6.8432093

Batch 161900, train_perplexity=742.4488, train_loss=6.609954

Batch 161910, train_perplexity=787.86426, train_loss=6.669326

Batch 161920, train_perplexity=763.33734, train_loss=6.6377

Batch 161930, train_perplexity=717.4537, train_loss=6.5757084

Batch 161940, train_perplexity=862.33105, train_loss=6.7596393

Batch 161950, train_perplexity=888.0196, train_loss=6.788994

Batch 161960, train_perplexity=797.68256, train_loss=6.6817107

Batch 161970, train_perplexity=872.7602, train_loss=6.771661
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 161980, train_perplexity=763.7976, train_loss=6.638303

Batch 161990, train_perplexity=762.3829, train_loss=6.636449

Batch 162000, train_perplexity=827.513, train_loss=6.718425

Batch 162010, train_perplexity=841.6674, train_loss=6.735385

Batch 162020, train_perplexity=791.9487, train_loss=6.6744967

Batch 162030, train_perplexity=733.2911, train_loss=6.597543

Batch 162040, train_perplexity=864.51605, train_loss=6.76217

Batch 162050, train_perplexity=798.4128, train_loss=6.682626

Batch 162060, train_perplexity=752.86224, train_loss=6.6238823

Batch 162070, train_perplexity=868.9245, train_loss=6.7672563

Batch 162080, train_perplexity=888.8601, train_loss=6.78994

Batch 162090, train_perplexity=812.9257, train_loss=6.7006397

Batch 162100, train_perplexity=864.59845, train_loss=6.762265

Batch 162110, train_perplexity=791.2444, train_loss=6.673607

Batch 162120, train_perplexity=815.0621, train_loss=6.703264

Batch 162130, train_perplexity=787.3993, train_loss=6.6687355

Batch 162140, train_perplexity=773.5627, train_loss=6.6510067

Batch 162150, train_perplexity=699.6018, train_loss=6.5505114

Batch 162160, train_perplexity=779.87744, train_loss=6.659137

Batch 162170, train_perplexity=740.7189, train_loss=6.607621

Batch 162180, train_perplexity=797.9671, train_loss=6.6820674

Batch 162190, train_perplexity=772.52246, train_loss=6.649661

Batch 162200, train_perplexity=834.6565, train_loss=6.7270203

Batch 162210, train_perplexity=844.0974, train_loss=6.738268

Batch 162220, train_perplexity=757.06555, train_loss=6.62945

Batch 162230, train_perplexity=856.5074, train_loss=6.752863

Batch 162240, train_perplexity=748.31757, train_loss=6.6178274

Batch 162250, train_perplexity=800.82446, train_loss=6.685642

Batch 162260, train_perplexity=793.64307, train_loss=6.676634

Batch 162270, train_perplexity=781.36414, train_loss=6.6610413

Batch 162280, train_perplexity=853.5975, train_loss=6.7494597

Batch 162290, train_perplexity=818.19867, train_loss=6.707105

Batch 162300, train_perplexity=822.26276, train_loss=6.71206

Batch 162310, train_perplexity=786.9617, train_loss=6.6681795

Batch 162320, train_perplexity=765.3069, train_loss=6.640277

Batch 162330, train_perplexity=856.62177, train_loss=6.7529964

Batch 162340, train_perplexity=754.6083, train_loss=6.626199

Batch 162350, train_perplexity=761.12024, train_loss=6.6347914

Batch 162360, train_perplexity=765.05255, train_loss=6.6399446

Batch 162370, train_perplexity=803.5644, train_loss=6.6890574

Batch 162380, train_perplexity=799.66327, train_loss=6.6841908

Batch 162390, train_perplexity=844.37274, train_loss=6.738594

Batch 162400, train_perplexity=842.9165, train_loss=6.736868

Batch 162410, train_perplexity=793.20197, train_loss=6.676078

Batch 162420, train_perplexity=837.54016, train_loss=6.730469

Batch 162430, train_perplexity=717.88074, train_loss=6.5763035

Batch 162440, train_perplexity=722.07465, train_loss=6.5821285

Batch 162450, train_perplexity=759.12604, train_loss=6.632168

Batch 162460, train_perplexity=915.3918, train_loss=6.819352

Batch 162470, train_perplexity=887.30853, train_loss=6.7881927

Batch 162480, train_perplexity=830.7589, train_loss=6.7223396

Batch 162490, train_perplexity=787.87555, train_loss=6.66934

Batch 162500, train_perplexity=912.84143, train_loss=6.816562

Batch 162510, train_perplexity=846.4396, train_loss=6.741039

Batch 162520, train_perplexity=814.3395, train_loss=6.7023773

Batch 162530, train_perplexity=849.624, train_loss=6.744794

Batch 162540, train_perplexity=897.1914, train_loss=6.799269

Batch 162550, train_perplexity=888.05096, train_loss=6.789029

Batch 162560, train_perplexity=910.4358, train_loss=6.8139234

Batch 162570, train_perplexity=822.7126, train_loss=6.712607

Batch 162580, train_perplexity=877.6839, train_loss=6.7772865

Batch 162590, train_perplexity=935.46655, train_loss=6.8410454

Batch 162600, train_perplexity=764.8129, train_loss=6.6396313

Batch 162610, train_perplexity=801.13574, train_loss=6.6860304

Batch 162620, train_perplexity=778.9249, train_loss=6.6579146

Batch 162630, train_perplexity=813.52057, train_loss=6.701371

Batch 162640, train_perplexity=850.6407, train_loss=6.74599

Batch 162650, train_perplexity=924.88196, train_loss=6.829666

Batch 162660, train_perplexity=792.4844, train_loss=6.675173

Batch 162670, train_perplexity=868.22705, train_loss=6.7664533

Batch 162680, train_perplexity=757.5986, train_loss=6.6301537

Batch 162690, train_perplexity=808.88965, train_loss=6.6956625

Batch 162700, train_perplexity=813.51746, train_loss=6.7013674

Batch 162710, train_perplexity=722.92285, train_loss=6.5833025

Batch 162720, train_perplexity=759.5964, train_loss=6.632787

Batch 162730, train_perplexity=865.7272, train_loss=6.76357

Batch 162740, train_perplexity=873.03033, train_loss=6.7719703

Batch 162750, train_perplexity=746.2451, train_loss=6.615054

Batch 162760, train_perplexity=787.3081, train_loss=6.6686196

Batch 162770, train_perplexity=802.0037, train_loss=6.6871133

Batch 162780, train_perplexity=800.70416, train_loss=6.6854916

Batch 162790, train_perplexity=803.5529, train_loss=6.689043

Batch 162800, train_perplexity=856.96, train_loss=6.7533913

Batch 162810, train_perplexity=825.6585, train_loss=6.7161813

Batch 162820, train_perplexity=772.3729, train_loss=6.6494675

Batch 162830, train_perplexity=707.22437, train_loss=6.561348

Batch 162840, train_perplexity=770.0539, train_loss=6.6464605

Batch 162850, train_perplexity=912.7113, train_loss=6.8164196

Batch 162860, train_perplexity=828.9521, train_loss=6.7201624

Batch 162870, train_perplexity=797.8511, train_loss=6.681922

Batch 162880, train_perplexity=790.83624, train_loss=6.673091

Batch 162890, train_perplexity=816.9414, train_loss=6.7055674

Batch 162900, train_perplexity=802.63385, train_loss=6.6878986

Batch 162910, train_perplexity=780.98157, train_loss=6.6605515

Batch 162920, train_perplexity=875.58966, train_loss=6.7748976

Batch 162930, train_perplexity=808.292, train_loss=6.6949234

Batch 162940, train_perplexity=860.214, train_loss=6.757181

Batch 162950, train_perplexity=793.6654, train_loss=6.676662

Batch 162960, train_perplexity=891.8209, train_loss=6.7932653

Batch 162970, train_perplexity=750.66626, train_loss=6.620961

Batch 162980, train_perplexity=886.05786, train_loss=6.7867823

Batch 162990, train_perplexity=865.6925, train_loss=6.76353

Batch 163000, train_perplexity=834.84595, train_loss=6.727247

Batch 163010, train_perplexity=779.2076, train_loss=6.6582775

Batch 163020, train_perplexity=800.43085, train_loss=6.68515

Batch 163030, train_perplexity=895.4348, train_loss=6.7973094

Batch 163040, train_perplexity=860.3268, train_loss=6.7573123

Batch 163050, train_perplexity=877.5006, train_loss=6.7770777

Batch 163060, train_perplexity=754.961, train_loss=6.626666

Batch 163070, train_perplexity=845.54926, train_loss=6.7399864

Batch 163080, train_perplexity=790.53235, train_loss=6.6727066

Batch 163090, train_perplexity=802.7525, train_loss=6.6880465

Batch 163100, train_perplexity=789.0926, train_loss=6.6708837

Batch 163110, train_perplexity=757.5899, train_loss=6.630142

Batch 163120, train_perplexity=817.54816, train_loss=6.70631

Batch 163130, train_perplexity=786.78265, train_loss=6.667952

Batch 163140, train_perplexity=784.12463, train_loss=6.664568

Batch 163150, train_perplexity=791.1591, train_loss=6.673499

Batch 163160, train_perplexity=857.9519, train_loss=6.754548

Batch 163170, train_perplexity=819.2422, train_loss=6.7083797

Batch 163180, train_perplexity=750.67236, train_loss=6.6209693

Batch 163190, train_perplexity=838.10547, train_loss=6.731144

Batch 163200, train_perplexity=832.3767, train_loss=6.724285

Batch 163210, train_perplexity=814.6036, train_loss=6.7027016

Batch 163220, train_perplexity=876.12256, train_loss=6.775506

Batch 163230, train_perplexity=785.5924, train_loss=6.666438

Batch 163240, train_perplexity=736.4125, train_loss=6.6017904

Batch 163250, train_perplexity=728.91077, train_loss=6.5915513

Batch 163260, train_perplexity=775.3534, train_loss=6.653319

Batch 163270, train_perplexity=762.36176, train_loss=6.636421

Batch 163280, train_perplexity=810.7424, train_loss=6.6979504
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 163290, train_perplexity=899.9091, train_loss=6.802294

Batch 163300, train_perplexity=827.0337, train_loss=6.7178454

Batch 163310, train_perplexity=709.88605, train_loss=6.5651045

Batch 163320, train_perplexity=720.9919, train_loss=6.580628

Batch 163330, train_perplexity=837.3305, train_loss=6.730219

Batch 163340, train_perplexity=904.46747, train_loss=6.8073463

Batch 163350, train_perplexity=838.9619, train_loss=6.7321653

Batch 163360, train_perplexity=943.9524, train_loss=6.8500757

Batch 163370, train_perplexity=816.4935, train_loss=6.705019

Batch 163380, train_perplexity=818.0551, train_loss=6.7069297

Batch 163390, train_perplexity=819.86005, train_loss=6.7091336

Batch 163400, train_perplexity=796.5457, train_loss=6.6802845

Batch 163410, train_perplexity=873.51874, train_loss=6.7725296

Batch 163420, train_perplexity=775.0928, train_loss=6.6529827

Batch 163430, train_perplexity=799.67395, train_loss=6.684204

Batch 163440, train_perplexity=737.33417, train_loss=6.603041

Batch 163450, train_perplexity=793.0431, train_loss=6.6758776

Batch 163460, train_perplexity=930.2559, train_loss=6.8354597

Batch 163470, train_perplexity=713.86884, train_loss=6.570699

Batch 163480, train_perplexity=832.9485, train_loss=6.724972

Batch 163490, train_perplexity=791.6176, train_loss=6.6740785

Batch 163500, train_perplexity=731.83344, train_loss=6.595553

Batch 163510, train_perplexity=828.3614, train_loss=6.7194495

Batch 163520, train_perplexity=766.2366, train_loss=6.641491

Batch 163530, train_perplexity=861.2729, train_loss=6.7584114

Batch 163540, train_perplexity=948.52576, train_loss=6.854909

Batch 163550, train_perplexity=839.0347, train_loss=6.732252

Batch 163560, train_perplexity=804.85864, train_loss=6.6906667

Batch 163570, train_perplexity=771.8751, train_loss=6.648823

Batch 163580, train_perplexity=879.69006, train_loss=6.7795696

Batch 163590, train_perplexity=848.07495, train_loss=6.742969

Batch 163600, train_perplexity=786.4462, train_loss=6.6675243

Batch 163610, train_perplexity=743.3932, train_loss=6.611225

Batch 163620, train_perplexity=843.6733, train_loss=6.7377653

Batch 163630, train_perplexity=813.33673, train_loss=6.701145

Batch 163640, train_perplexity=842.27484, train_loss=6.7361064

Batch 163650, train_perplexity=803.14307, train_loss=6.688533

Batch 163660, train_perplexity=794.8937, train_loss=6.6782084

Batch 163670, train_perplexity=828.85565, train_loss=6.720046

Batch 163680, train_perplexity=730.1032, train_loss=6.593186

Batch 163690, train_perplexity=867.8376, train_loss=6.7660046

Batch 163700, train_perplexity=817.326, train_loss=6.706038

Batch 163710, train_perplexity=838.37286, train_loss=6.731463

Batch 163720, train_perplexity=755.8716, train_loss=6.6278715

Batch 163730, train_perplexity=771.0982, train_loss=6.6478157

Batch 163740, train_perplexity=762.047, train_loss=6.6360083

Batch 163750, train_perplexity=765.28204, train_loss=6.6402445

Batch 163760, train_perplexity=833.34094, train_loss=6.725443

Batch 163770, train_perplexity=784.4309, train_loss=6.6649585

Batch 163780, train_perplexity=772.6595, train_loss=6.6498384

Batch 163790, train_perplexity=810.4579, train_loss=6.6975994

Batch 163800, train_perplexity=799.7899, train_loss=6.684349

Batch 163810, train_perplexity=819.1637, train_loss=6.708284

Batch 163820, train_perplexity=803.87946, train_loss=6.6894493

Batch 163830, train_perplexity=829.3712, train_loss=6.720668

Batch 163840, train_perplexity=819.6556, train_loss=6.7088842

Batch 163850, train_perplexity=833.6339, train_loss=6.7257943

Batch 163860, train_perplexity=789.1133, train_loss=6.67091

Batch 163870, train_perplexity=817.4448, train_loss=6.7061834

Batch 163880, train_perplexity=779.6231, train_loss=6.6588106

Batch 163890, train_perplexity=777.4729, train_loss=6.656049

Batch 163900, train_perplexity=781.5437, train_loss=6.661271

Batch 163910, train_perplexity=788.6641, train_loss=6.6703405

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Batch 163920, train_perplexity=805.0268, train_loss=6.6908755

Batch 163930, train_perplexity=823.47284, train_loss=6.7135305

Batch 163940, train_perplexity=779.5064, train_loss=6.658661

Batch 163950, train_perplexity=802.06305, train_loss=6.687187

Batch 163960, train_perplexity=796.48224, train_loss=6.680205

Batch 163970, train_perplexity=787.47253, train_loss=6.6688285

Batch 163980, train_perplexity=794.0852, train_loss=6.677191

Batch 163990, train_perplexity=839.8321, train_loss=6.733202

Batch 164000, train_perplexity=835.12146, train_loss=6.727577

Batch 164010, train_perplexity=873.8312, train_loss=6.772887

Batch 164020, train_perplexity=818.5557, train_loss=6.7075415

Batch 164030, train_perplexity=822.2165, train_loss=6.7120037

Batch 164040, train_perplexity=819.3859, train_loss=6.708555

Batch 164050, train_perplexity=729.9044, train_loss=6.5929136

Batch 164060, train_perplexity=815.3672, train_loss=6.7036386

Batch 164070, train_perplexity=821.5283, train_loss=6.7111664

Batch 164080, train_perplexity=794.08673, train_loss=6.6771927

Batch 164090, train_perplexity=781.0799, train_loss=6.6606774

Batch 164100, train_perplexity=718.8824, train_loss=6.5776978

Batch 164110, train_perplexity=698.681, train_loss=6.5491943

Batch 164120, train_perplexity=861.2811, train_loss=6.758421

Batch 164130, train_perplexity=779.5536, train_loss=6.6587214

Batch 164140, train_perplexity=726.7614, train_loss=6.5885983

Batch 164150, train_perplexity=735.82526, train_loss=6.6009927

Batch 164160, train_perplexity=788.9624, train_loss=6.6707187

Batch 164170, train_perplexity=730.6416, train_loss=6.593923

Batch 164180, train_perplexity=793.3699, train_loss=6.6762896

Batch 164190, train_perplexity=774.39233, train_loss=6.6520786

Batch 164200, train_perplexity=759.65106, train_loss=6.632859

Batch 164210, train_perplexity=814.7644, train_loss=6.702899

Batch 164220, train_perplexity=763.87115, train_loss=6.638399

Batch 164230, train_perplexity=743.91693, train_loss=6.6119294

Batch 164240, train_perplexity=815.923, train_loss=6.70432

Batch 164250, train_perplexity=768.4253, train_loss=6.6443434

Batch 164260, train_perplexity=740.55646, train_loss=6.607402

Batch 164270, train_perplexity=818.5838, train_loss=6.707576

Batch 164280, train_perplexity=811.4432, train_loss=6.6988144

Batch 164290, train_perplexity=789.24274, train_loss=6.671074

Batch 164300, train_perplexity=777.94684, train_loss=6.656658

Batch 164310, train_perplexity=737.03253, train_loss=6.602632

Batch 164320, train_perplexity=757.8591, train_loss=6.6304975

Batch 164330, train_perplexity=783.43024, train_loss=6.663682

Batch 164340, train_perplexity=849.6321, train_loss=6.7448034

Batch 164350, train_perplexity=789.9913, train_loss=6.672022

Batch 164360, train_perplexity=779.14294, train_loss=6.6581945

Batch 164370, train_perplexity=857.1096, train_loss=6.753566

Batch 164380, train_perplexity=864.2547, train_loss=6.7618675

Batch 164390, train_perplexity=789.18176, train_loss=6.6709967

Batch 164400, train_perplexity=789.3654, train_loss=6.6712294

Batch 164410, train_perplexity=854.3911, train_loss=6.750389

Batch 164420, train_perplexity=801.07, train_loss=6.6859484

Batch 164430, train_perplexity=874.6958, train_loss=6.773876

Batch 164440, train_perplexity=786.1943, train_loss=6.667204

Batch 164450, train_perplexity=781.5892, train_loss=6.6613293

Batch 164460, train_perplexity=792.48663, train_loss=6.6751757

Batch 164470, train_perplexity=812.1787, train_loss=6.6997204

Batch 164480, train_perplexity=876.6299, train_loss=6.776085

Batch 164490, train_perplexity=798.623, train_loss=6.682889

Batch 164500, train_perplexity=822.5427, train_loss=6.7124004

Batch 164510, train_perplexity=743.9145, train_loss=6.611926

Batch 164520, train_perplexity=772.0614, train_loss=6.649064

Batch 164530, train_perplexity=801.9338, train_loss=6.687026
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 164540, train_perplexity=765.7719, train_loss=6.6408844

Batch 164550, train_perplexity=780.8579, train_loss=6.660393

Batch 164560, train_perplexity=834.21484, train_loss=6.726491

Batch 164570, train_perplexity=820.2796, train_loss=6.7096453

Batch 164580, train_perplexity=894.7412, train_loss=6.7965345

Batch 164590, train_perplexity=888.7406, train_loss=6.7898054

Batch 164600, train_perplexity=775.9089, train_loss=6.654035

Batch 164610, train_perplexity=808.32056, train_loss=6.6949587

Batch 164620, train_perplexity=784.8185, train_loss=6.6654525

Batch 164630, train_perplexity=802.2252, train_loss=6.6873894

Batch 164640, train_perplexity=849.57214, train_loss=6.744733

Batch 164650, train_perplexity=849.8736, train_loss=6.7450876

Batch 164660, train_perplexity=786.9879, train_loss=6.668213

Batch 164670, train_perplexity=802.8233, train_loss=6.6881347

Batch 164680, train_perplexity=856.65155, train_loss=6.7530313

Batch 164690, train_perplexity=797.38025, train_loss=6.6813316

Batch 164700, train_perplexity=936.7985, train_loss=6.8424683

Batch 164710, train_perplexity=795.8373, train_loss=6.6793947

Batch 164720, train_perplexity=721.70496, train_loss=6.5816164

Batch 164730, train_perplexity=917.86566, train_loss=6.822051

Batch 164740, train_perplexity=860.14056, train_loss=6.757096

Batch 164750, train_perplexity=817.1525, train_loss=6.705826

Batch 164760, train_perplexity=809.20135, train_loss=6.696048

Batch 164770, train_perplexity=797.57947, train_loss=6.6815815

Batch 164780, train_perplexity=760.9464, train_loss=6.634563

Batch 164790, train_perplexity=833.3012, train_loss=6.725395

Batch 164800, train_perplexity=843.9163, train_loss=6.7380533

Batch 164810, train_perplexity=838.4956, train_loss=6.7316093

Batch 164820, train_perplexity=796.202, train_loss=6.679853

Batch 164830, train_perplexity=843.8793, train_loss=6.7380095

Batch 164840, train_perplexity=853.4172, train_loss=6.7492485

Batch 164850, train_perplexity=793.47845, train_loss=6.6764264

Batch 164860, train_perplexity=827.3962, train_loss=6.7182837

Batch 164870, train_perplexity=795.49274, train_loss=6.6789618

Batch 164880, train_perplexity=828.094, train_loss=6.7191267

Batch 164890, train_perplexity=765.4288, train_loss=6.640436

Batch 164900, train_perplexity=803.54297, train_loss=6.6890306

Batch 164910, train_perplexity=819.8581, train_loss=6.7091312

Batch 164920, train_perplexity=769.66187, train_loss=6.6459513

Batch 164930, train_perplexity=837.1165, train_loss=6.7299633

Batch 164940, train_perplexity=864.5218, train_loss=6.7621765

Batch 164950, train_perplexity=775.49164, train_loss=6.653497

Batch 164960, train_perplexity=845.91907, train_loss=6.7404237

Batch 164970, train_perplexity=765.5346, train_loss=6.6405745

Batch 164980, train_perplexity=786.10876, train_loss=6.667095

Batch 164990, train_perplexity=763.5667, train_loss=6.6380005

Batch 165000, train_perplexity=754.80695, train_loss=6.626462

Batch 165010, train_perplexity=824.864, train_loss=6.7152185

Batch 165020, train_perplexity=828.7434, train_loss=6.7199106

Batch 165030, train_perplexity=852.4338, train_loss=6.7480955

Batch 165040, train_perplexity=729.3902, train_loss=6.592209

Batch 165050, train_perplexity=806.25183, train_loss=6.692396

Batch 165060, train_perplexity=862.0001, train_loss=6.7592554

Batch 165070, train_perplexity=896.7872, train_loss=6.7988186

Batch 165080, train_perplexity=771.018, train_loss=6.6477118

Batch 165090, train_perplexity=907.978, train_loss=6.81122

Batch 165100, train_perplexity=816.7038, train_loss=6.7052765

Batch 165110, train_perplexity=773.45166, train_loss=6.650863

Batch 165120, train_perplexity=748.9915, train_loss=6.6187277

Batch 165130, train_perplexity=793.9345, train_loss=6.677001

Batch 165140, train_perplexity=800.31903, train_loss=6.6850104

Batch 165150, train_perplexity=800.2835, train_loss=6.684966

Batch 165160, train_perplexity=879.3864, train_loss=6.7792244

Batch 165170, train_perplexity=843.12555, train_loss=6.737116

Batch 165180, train_perplexity=875.33923, train_loss=6.7746115

Batch 165190, train_perplexity=845.5206, train_loss=6.7399526

Batch 165200, train_perplexity=782.9141, train_loss=6.663023

Batch 165210, train_perplexity=723.6975, train_loss=6.5843735

Batch 165220, train_perplexity=727.0155, train_loss=6.588948

Batch 165230, train_perplexity=889.2501, train_loss=6.7903786

Batch 165240, train_perplexity=793.841, train_loss=6.676883

Batch 165250, train_perplexity=830.64404, train_loss=6.7222013

Batch 165260, train_perplexity=760.88586, train_loss=6.6344833

Batch 165270, train_perplexity=872.53503, train_loss=6.771403

Batch 165280, train_perplexity=769.84247, train_loss=6.646186

Batch 165290, train_perplexity=897.9438, train_loss=6.8001075

Batch 165300, train_perplexity=810.05225, train_loss=6.6970987

Batch 165310, train_perplexity=759.9837, train_loss=6.633297

Batch 165320, train_perplexity=810.8228, train_loss=6.6980495

Batch 165330, train_perplexity=741.45, train_loss=6.608608

Batch 165340, train_perplexity=769.186, train_loss=6.645333

Batch 165350, train_perplexity=786.1103, train_loss=6.667097

Batch 165360, train_perplexity=813.97687, train_loss=6.701932

Batch 165370, train_perplexity=850.459, train_loss=6.745776

Batch 165380, train_perplexity=824.9726, train_loss=6.71535

Batch 165390, train_perplexity=836.3254, train_loss=6.7290177

Batch 165400, train_perplexity=860.1442, train_loss=6.7571

Batch 165410, train_perplexity=843.47455, train_loss=6.7375298

Batch 165420, train_perplexity=795.4146, train_loss=6.6788635

Batch 165430, train_perplexity=760.12573, train_loss=6.633484

Batch 165440, train_perplexity=877.74084, train_loss=6.7773514

Batch 165450, train_perplexity=834.2136, train_loss=6.7264895

Batch 165460, train_perplexity=776.97925, train_loss=6.6554136

Batch 165470, train_perplexity=773.1176, train_loss=6.650431

Batch 165480, train_perplexity=725.1307, train_loss=6.586352

Batch 165490, train_perplexity=831.99817, train_loss=6.72383

Batch 165500, train_perplexity=894.6337, train_loss=6.7964144

Batch 165510, train_perplexity=829.8673, train_loss=6.721266

Batch 165520, train_perplexity=729.1496, train_loss=6.591879

Batch 165530, train_perplexity=767.5289, train_loss=6.643176

Batch 165540, train_perplexity=834.15594, train_loss=6.7264204

Batch 165550, train_perplexity=775.63477, train_loss=6.6536818

Batch 165560, train_perplexity=834.61115, train_loss=6.726966

Batch 165570, train_perplexity=824.8404, train_loss=6.71519

Batch 165580, train_perplexity=874.30383, train_loss=6.773428

Batch 165590, train_perplexity=805.8948, train_loss=6.691953

Batch 165600, train_perplexity=787.3918, train_loss=6.668726

Batch 165610, train_perplexity=798.32983, train_loss=6.682522

Batch 165620, train_perplexity=715.2119, train_loss=6.572579

Batch 165630, train_perplexity=877.1484, train_loss=6.776676

Batch 165640, train_perplexity=740.2267, train_loss=6.6069565

Batch 165650, train_perplexity=798.03485, train_loss=6.6821523

Batch 165660, train_perplexity=817.52045, train_loss=6.706276

Batch 165670, train_perplexity=831.5543, train_loss=6.7232966

Batch 165680, train_perplexity=809.6892, train_loss=6.6966505

Batch 165690, train_perplexity=851.5396, train_loss=6.747046

Batch 165700, train_perplexity=779.3269, train_loss=6.6584306

Batch 165710, train_perplexity=771.99585, train_loss=6.648979

Batch 165720, train_perplexity=781.79083, train_loss=6.661587

Batch 165730, train_perplexity=797.7415, train_loss=6.6817846

Batch 165740, train_perplexity=783.96985, train_loss=6.6643705

Batch 165750, train_perplexity=847.1385, train_loss=6.741864

Batch 165760, train_perplexity=832.4918, train_loss=6.7244234

Batch 165770, train_perplexity=770.9173, train_loss=6.647581

Batch 165780, train_perplexity=776.3578, train_loss=6.6546135

Batch 165790, train_perplexity=764.16986, train_loss=6.63879

Batch 165800, train_perplexity=784.8713, train_loss=6.6655197

Batch 165810, train_perplexity=882.05023, train_loss=6.782249

Batch 165820, train_perplexity=814.63074, train_loss=6.702735

Batch 165830, train_perplexity=730.2888, train_loss=6.59344

Batch 165840, train_perplexity=812.05945, train_loss=6.6995735
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 165850, train_perplexity=729.85675, train_loss=6.5928483

Batch 165860, train_perplexity=869.12714, train_loss=6.7674894

Batch 165870, train_perplexity=829.198, train_loss=6.720459

Batch 165880, train_perplexity=748.6916, train_loss=6.618327

Batch 165890, train_perplexity=814.216, train_loss=6.7022257

Batch 165900, train_perplexity=739.9042, train_loss=6.6065207

Batch 165910, train_perplexity=895.43866, train_loss=6.7973137

Batch 165920, train_perplexity=816.5122, train_loss=6.705042

Batch 165930, train_perplexity=905.14185, train_loss=6.8080916

Batch 165940, train_perplexity=817.0431, train_loss=6.705692

Batch 165950, train_perplexity=766.1708, train_loss=6.641405

Batch 165960, train_perplexity=825.248, train_loss=6.715684

Batch 165970, train_perplexity=739.6537, train_loss=6.606182

Batch 165980, train_perplexity=753.9141, train_loss=6.6252785

Batch 165990, train_perplexity=776.04095, train_loss=6.6542053

Batch 166000, train_perplexity=774.8363, train_loss=6.652652

Batch 166010, train_perplexity=795.8794, train_loss=6.6794477

Batch 166020, train_perplexity=791.2629, train_loss=6.67363

Batch 166030, train_perplexity=807.1343, train_loss=6.69349

Batch 166040, train_perplexity=785.3482, train_loss=6.666127

Batch 166050, train_perplexity=792.38763, train_loss=6.6750507

Batch 166060, train_perplexity=816.69135, train_loss=6.705261

Batch 166070, train_perplexity=795.06195, train_loss=6.67842

Batch 166080, train_perplexity=873.02576, train_loss=6.771965

Batch 166090, train_perplexity=824.39764, train_loss=6.714653

Batch 166100, train_perplexity=784.33215, train_loss=6.6648326

Batch 166110, train_perplexity=743.0792, train_loss=6.6108027

Batch 166120, train_perplexity=798.14557, train_loss=6.682291

Batch 166130, train_perplexity=826.85034, train_loss=6.7176237

Batch 166140, train_perplexity=814.2525, train_loss=6.7022705

Batch 166150, train_perplexity=805.08856, train_loss=6.6909523

Batch 166160, train_perplexity=834.41614, train_loss=6.7267323

Batch 166170, train_perplexity=880.03784, train_loss=6.779965

Batch 166180, train_perplexity=793.0053, train_loss=6.67583

Batch 166190, train_perplexity=806.4441, train_loss=6.6926346

Batch 166200, train_perplexity=775.0658, train_loss=6.652948

Batch 166210, train_perplexity=809.1041, train_loss=6.6959276

Batch 166220, train_perplexity=767.32465, train_loss=6.64291

Batch 166230, train_perplexity=784.71185, train_loss=6.6653166

Batch 166240, train_perplexity=835.0737, train_loss=6.72752

Batch 166250, train_perplexity=743.428, train_loss=6.611272

Batch 166260, train_perplexity=946.2133, train_loss=6.852468

Batch 166270, train_perplexity=879.29, train_loss=6.7791147

Batch 166280, train_perplexity=747.95294, train_loss=6.61734

Batch 166290, train_perplexity=816.418, train_loss=6.7049265

Batch 166300, train_perplexity=785.37555, train_loss=6.666162

Batch 166310, train_perplexity=795.58606, train_loss=6.679079

Batch 166320, train_perplexity=824.1595, train_loss=6.714364

Batch 166330, train_perplexity=746.49994, train_loss=6.6153955

Batch 166340, train_perplexity=816.8775, train_loss=6.705489

Batch 166350, train_perplexity=769.06165, train_loss=6.645171

Batch 166360, train_perplexity=794.4363, train_loss=6.677633

Batch 166370, train_perplexity=822.9178, train_loss=6.7128563

Batch 166380, train_perplexity=767.19916, train_loss=6.6427464

Batch 166390, train_perplexity=758.0821, train_loss=6.6307917

Batch 166400, train_perplexity=841.03595, train_loss=6.7346344

Batch 166410, train_perplexity=798.0832, train_loss=6.682213

Batch 166420, train_perplexity=809.87225, train_loss=6.6968765

Batch 166430, train_perplexity=768.6485, train_loss=6.644634

Batch 166440, train_perplexity=742.36774, train_loss=6.6098447

Batch 166450, train_perplexity=712.91364, train_loss=6.5693603

Batch 166460, train_perplexity=774.32733, train_loss=6.6519947

Batch 166470, train_perplexity=790.34467, train_loss=6.672469

Batch 166480, train_perplexity=767.877, train_loss=6.6436296

Batch 166490, train_perplexity=802.80994, train_loss=6.688118

Batch 166500, train_perplexity=736.05023, train_loss=6.6012983

Batch 166510, train_perplexity=723.05005, train_loss=6.5834785

Batch 166520, train_perplexity=810.7084, train_loss=6.6979084

Batch 166530, train_perplexity=913.3826, train_loss=6.817155

Batch 166540, train_perplexity=798.079, train_loss=6.6822076

Batch 166550, train_perplexity=834.68, train_loss=6.7270484

Batch 166560, train_perplexity=890.26416, train_loss=6.791518

Batch 166570, train_perplexity=815.8086, train_loss=6.70418

Batch 166580, train_perplexity=761.5283, train_loss=6.6353273

Batch 166590, train_perplexity=759.88873, train_loss=6.633172

Batch 166600, train_perplexity=776.7318, train_loss=6.655095

Batch 166610, train_perplexity=753.60864, train_loss=6.624873

Batch 166620, train_perplexity=837.49506, train_loss=6.7304153

Batch 166630, train_perplexity=841.4704, train_loss=6.735151

Batch 166640, train_perplexity=683.247, train_loss=6.5268564

Batch 166650, train_perplexity=721.7655, train_loss=6.5817003

Batch 166660, train_perplexity=862.6934, train_loss=6.7600594

Batch 166670, train_perplexity=867.28076, train_loss=6.7653627

Batch 166680, train_perplexity=764.71155, train_loss=6.6394987

Batch 166690, train_perplexity=766.0747, train_loss=6.6412797

Batch 166700, train_perplexity=800.24005, train_loss=6.6849117

Batch 166710, train_perplexity=782.5506, train_loss=6.6625586

Batch 166720, train_perplexity=768.8769, train_loss=6.644931

Batch 166730, train_perplexity=815.0562, train_loss=6.703257

Batch 166740, train_perplexity=759.20276, train_loss=6.632269

Batch 166750, train_perplexity=782.76855, train_loss=6.662837

Batch 166760, train_perplexity=748.67017, train_loss=6.6182985

Batch 166770, train_perplexity=769.71985, train_loss=6.6460266

Batch 166780, train_perplexity=712.3621, train_loss=6.5685863

Batch 166790, train_perplexity=861.8497, train_loss=6.759081

Batch 166800, train_perplexity=855.79297, train_loss=6.7520285

Batch 166810, train_perplexity=714.8236, train_loss=6.572036

Batch 166820, train_perplexity=890.1224, train_loss=6.791359

Batch 166830, train_perplexity=898.7517, train_loss=6.801007

Batch 166840, train_perplexity=796.0821, train_loss=6.6797023

Batch 166850, train_perplexity=779.86554, train_loss=6.6591215

Batch 166860, train_perplexity=775.3574, train_loss=6.653324

Batch 166870, train_perplexity=821.03876, train_loss=6.7105703

Batch 166880, train_perplexity=824.30884, train_loss=6.7145452

Batch 166890, train_perplexity=782.5439, train_loss=6.66255

Batch 166900, train_perplexity=732.6812, train_loss=6.5967107

Batch 166910, train_perplexity=852.32324, train_loss=6.747966

Batch 166920, train_perplexity=838.39685, train_loss=6.7314916

Batch 166930, train_perplexity=789.67413, train_loss=6.6716204

Batch 166940, train_perplexity=766.56036, train_loss=6.6419134

Batch 166950, train_perplexity=754.90015, train_loss=6.6265855

Batch 166960, train_perplexity=860.3366, train_loss=6.7573237

Batch 166970, train_perplexity=811.46875, train_loss=6.698846

Batch 166980, train_perplexity=728.7262, train_loss=6.591298

Batch 166990, train_perplexity=832.2815, train_loss=6.7241707

Batch 167000, train_perplexity=773.3845, train_loss=6.6507764

Batch 167010, train_perplexity=814.6113, train_loss=6.702711

Batch 167020, train_perplexity=882.4991, train_loss=6.7827578

Batch 167030, train_perplexity=856.31055, train_loss=6.752633

Batch 167040, train_perplexity=758.7229, train_loss=6.6316366

Batch 167050, train_perplexity=801.13385, train_loss=6.686028

Batch 167060, train_perplexity=798.4486, train_loss=6.6826706

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Batch 167070, train_perplexity=875.3609, train_loss=6.7746363

Batch 167080, train_perplexity=764.09735, train_loss=6.6386952

Batch 167090, train_perplexity=808.2693, train_loss=6.6948953
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 167100, train_perplexity=807.61395, train_loss=6.694084

Batch 167110, train_perplexity=849.47046, train_loss=6.744613

Batch 167120, train_perplexity=794.5435, train_loss=6.6777678

Batch 167130, train_perplexity=847.151, train_loss=6.741879

Batch 167140, train_perplexity=781.7264, train_loss=6.6615047

Batch 167150, train_perplexity=841.4299, train_loss=6.7351027

Batch 167160, train_perplexity=788.6818, train_loss=6.670363

Batch 167170, train_perplexity=775.6159, train_loss=6.6536574

Batch 167180, train_perplexity=797.8199, train_loss=6.681883

Batch 167190, train_perplexity=755.5894, train_loss=6.627498

Batch 167200, train_perplexity=924.01886, train_loss=6.8287325

Batch 167210, train_perplexity=731.17523, train_loss=6.594653

Batch 167220, train_perplexity=805.57166, train_loss=6.691552

Batch 167230, train_perplexity=739.15234, train_loss=6.605504

Batch 167240, train_perplexity=804.6741, train_loss=6.6904373

Batch 167250, train_perplexity=782.7913, train_loss=6.662866

Batch 167260, train_perplexity=831.5508, train_loss=6.7232924

Batch 167270, train_perplexity=794.99225, train_loss=6.6783323

Batch 167280, train_perplexity=769.91034, train_loss=6.646274

Batch 167290, train_perplexity=779.7875, train_loss=6.6590214

Batch 167300, train_perplexity=794.4814, train_loss=6.6776896

Batch 167310, train_perplexity=837.5837, train_loss=6.730521

Batch 167320, train_perplexity=813.822, train_loss=6.7017417

Batch 167330, train_perplexity=755.3924, train_loss=6.6272373

Batch 167340, train_perplexity=784.5023, train_loss=6.6650496

Batch 167350, train_perplexity=893.7362, train_loss=6.7954106

Batch 167360, train_perplexity=854.9662, train_loss=6.751062

Batch 167370, train_perplexity=801.70514, train_loss=6.686741

Batch 167380, train_perplexity=806.33105, train_loss=6.6924944

Batch 167390, train_perplexity=760.1584, train_loss=6.633527

Batch 167400, train_perplexity=779.4261, train_loss=6.658558

Batch 167410, train_perplexity=736.42865, train_loss=6.6018124

Batch 167420, train_perplexity=745.2978, train_loss=6.613784

Batch 167430, train_perplexity=749.3116, train_loss=6.619155

Batch 167440, train_perplexity=849.0659, train_loss=6.744137

Batch 167450, train_perplexity=871.80725, train_loss=6.7705684

Batch 167460, train_perplexity=795.1461, train_loss=6.678526

Batch 167470, train_perplexity=787.9789, train_loss=6.6694713

Batch 167480, train_perplexity=747.8706, train_loss=6.61723

Batch 167490, train_perplexity=779.166, train_loss=6.658224

Batch 167500, train_perplexity=764.69476, train_loss=6.639477

Batch 167510, train_perplexity=783.4451, train_loss=6.663701

Batch 167520, train_perplexity=891.83624, train_loss=6.7932825

Batch 167530, train_perplexity=767.85175, train_loss=6.6435966

Batch 167540, train_perplexity=835.5158, train_loss=6.7280493

Batch 167550, train_perplexity=768.7629, train_loss=6.6447825

Batch 167560, train_perplexity=775.4314, train_loss=6.6534195

Batch 167570, train_perplexity=753.9558, train_loss=6.625334

Batch 167580, train_perplexity=800.9035, train_loss=6.6857405

Batch 167590, train_perplexity=745.6959, train_loss=6.614318

Batch 167600, train_perplexity=768.6796, train_loss=6.6446743

Batch 167610, train_perplexity=805.04095, train_loss=6.690893

Batch 167620, train_perplexity=711.0813, train_loss=6.566787

Batch 167630, train_perplexity=801.79767, train_loss=6.6868563

Batch 167640, train_perplexity=862.5206, train_loss=6.759859

Batch 167650, train_perplexity=786.93915, train_loss=6.668151

Batch 167660, train_perplexity=848.1627, train_loss=6.7430725

Batch 167670, train_perplexity=803.49963, train_loss=6.688977

Batch 167680, train_perplexity=839.69714, train_loss=6.7330413

Batch 167690, train_perplexity=778.71655, train_loss=6.657647

Batch 167700, train_perplexity=738.98035, train_loss=6.6052713

Batch 167710, train_perplexity=863.74835, train_loss=6.7612815

Batch 167720, train_perplexity=798.51636, train_loss=6.6827555

Batch 167730, train_perplexity=791.53796, train_loss=6.673978

Batch 167740, train_perplexity=798.5857, train_loss=6.6828423

Batch 167750, train_perplexity=811.11285, train_loss=6.698407

Batch 167760, train_perplexity=781.2956, train_loss=6.6609535

Batch 167770, train_perplexity=858.7705, train_loss=6.7555017

Batch 167780, train_perplexity=837.5781, train_loss=6.7305145

Batch 167790, train_perplexity=772.8235, train_loss=6.6500506

Batch 167800, train_perplexity=708.49786, train_loss=6.563147

Batch 167810, train_perplexity=842.6159, train_loss=6.736511

Batch 167820, train_perplexity=857.1582, train_loss=6.7536225

Batch 167830, train_perplexity=827.40015, train_loss=6.7182884

Batch 167840, train_perplexity=861.6611, train_loss=6.758862

Batch 167850, train_perplexity=771.57446, train_loss=6.648433

Batch 167860, train_perplexity=803.9308, train_loss=6.689513

Batch 167870, train_perplexity=722.8256, train_loss=6.583168

Batch 167880, train_perplexity=848.4835, train_loss=6.7434506

Batch 167890, train_perplexity=710.0801, train_loss=6.5653777

Batch 167900, train_perplexity=836.6193, train_loss=6.729369

Batch 167910, train_perplexity=817.19305, train_loss=6.7058754

Batch 167920, train_perplexity=878.2248, train_loss=6.7779026

Batch 167930, train_perplexity=846.08325, train_loss=6.7406178

Batch 167940, train_perplexity=823.57965, train_loss=6.7136602

Batch 167950, train_perplexity=786.168, train_loss=6.6671705

Batch 167960, train_perplexity=756.7386, train_loss=6.629018

Batch 167970, train_perplexity=773.41, train_loss=6.6508093

Batch 167980, train_perplexity=879.00323, train_loss=6.7787886

Batch 167990, train_perplexity=839.0123, train_loss=6.7322254

Batch 168000, train_perplexity=893.7315, train_loss=6.7954054

Batch 168010, train_perplexity=826.5042, train_loss=6.717205

Batch 168020, train_perplexity=860.69604, train_loss=6.7577415

Batch 168030, train_perplexity=818.4113, train_loss=6.707365

Batch 168040, train_perplexity=756.5856, train_loss=6.6288157

Batch 168050, train_perplexity=724.918, train_loss=6.5860586

Batch 168060, train_perplexity=746.93506, train_loss=6.6159782

Batch 168070, train_perplexity=801.644, train_loss=6.6866646

Batch 168080, train_perplexity=862.1892, train_loss=6.7594748

Batch 168090, train_perplexity=864.10803, train_loss=6.761698

Batch 168100, train_perplexity=815.8868, train_loss=6.7042756

Batch 168110, train_perplexity=758.7652, train_loss=6.6316924

Batch 168120, train_perplexity=777.9624, train_loss=6.656678

Batch 168130, train_perplexity=815.5515, train_loss=6.7038646

Batch 168140, train_perplexity=748.2915, train_loss=6.6177926

Batch 168150, train_perplexity=822.59845, train_loss=6.712468

Batch 168160, train_perplexity=770.45276, train_loss=6.6469784

Batch 168170, train_perplexity=821.7077, train_loss=6.711385

Batch 168180, train_perplexity=762.2454, train_loss=6.6362686

Batch 168190, train_perplexity=876.6174, train_loss=6.7760706

Batch 168200, train_perplexity=720.63995, train_loss=6.5801396

Batch 168210, train_perplexity=755.52856, train_loss=6.6274176

Batch 168220, train_perplexity=757.97656, train_loss=6.6306524

Batch 168230, train_perplexity=816.6508, train_loss=6.7052116

Batch 168240, train_perplexity=721.76624, train_loss=6.5817013

Batch 168250, train_perplexity=855.2928, train_loss=6.751444

Batch 168260, train_perplexity=756.3024, train_loss=6.6284413

Batch 168270, train_perplexity=809.54486, train_loss=6.696472

Batch 168280, train_perplexity=818.5003, train_loss=6.7074738

Batch 168290, train_perplexity=809.7518, train_loss=6.6967278

Batch 168300, train_perplexity=857.1533, train_loss=6.753617

Batch 168310, train_perplexity=796.1861, train_loss=6.679833

Batch 168320, train_perplexity=838.2985, train_loss=6.7313743

Batch 168330, train_perplexity=833.56433, train_loss=6.725711

Batch 168340, train_perplexity=805.2191, train_loss=6.6911144

Batch 168350, train_perplexity=766.108, train_loss=6.641323

Batch 168360, train_perplexity=800.2362, train_loss=6.684907

Batch 168370, train_perplexity=880.3073, train_loss=6.780271

Batch 168380, train_perplexity=727.8455, train_loss=6.590089

Batch 168390, train_perplexity=805.30707, train_loss=6.6912236

Batch 168400, train_perplexity=861.2228, train_loss=6.758353
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 168410, train_perplexity=765.2835, train_loss=6.6402464

Batch 168420, train_perplexity=826.8101, train_loss=6.717575

Batch 168430, train_perplexity=790.6387, train_loss=6.672841

Batch 168440, train_perplexity=825.25824, train_loss=6.7156963

Batch 168450, train_perplexity=820.27374, train_loss=6.709638

Batch 168460, train_perplexity=699.302, train_loss=6.5500827

Batch 168470, train_perplexity=793.326, train_loss=6.6762342

Batch 168480, train_perplexity=765.22736, train_loss=6.640173

Batch 168490, train_perplexity=892.77655, train_loss=6.7943363

Batch 168500, train_perplexity=822.267, train_loss=6.712065

Batch 168510, train_perplexity=841.7734, train_loss=6.735511

Batch 168520, train_perplexity=786.77405, train_loss=6.667941

Batch 168530, train_perplexity=734.08453, train_loss=6.598624

Batch 168540, train_perplexity=783.67755, train_loss=6.6639977

Batch 168550, train_perplexity=772.27423, train_loss=6.6493397

Batch 168560, train_perplexity=825.9023, train_loss=6.7164764

Batch 168570, train_perplexity=917.0804, train_loss=6.821195

Batch 168580, train_perplexity=801.17316, train_loss=6.686077

Batch 168590, train_perplexity=806.7568, train_loss=6.6930223

Batch 168600, train_perplexity=755.25696, train_loss=6.627058

Batch 168610, train_perplexity=887.2586, train_loss=6.7881365

Batch 168620, train_perplexity=812.75555, train_loss=6.7004304

Batch 168630, train_perplexity=840.3845, train_loss=6.7338595

Batch 168640, train_perplexity=821.52515, train_loss=6.7111626

Batch 168650, train_perplexity=776.06024, train_loss=6.65423

Batch 168660, train_perplexity=790.5158, train_loss=6.6726856

Batch 168670, train_perplexity=774.61615, train_loss=6.6523676

Batch 168680, train_perplexity=776.437, train_loss=6.6547155

Batch 168690, train_perplexity=742.12317, train_loss=6.609515

Batch 168700, train_perplexity=820.1873, train_loss=6.7095327

Batch 168710, train_perplexity=813.2316, train_loss=6.701016

Batch 168720, train_perplexity=929.0324, train_loss=6.8341436

Batch 168730, train_perplexity=779.71533, train_loss=6.658929

Batch 168740, train_perplexity=711.21356, train_loss=6.5669727

Batch 168750, train_perplexity=805.97473, train_loss=6.6920524

Batch 168760, train_perplexity=766.2636, train_loss=6.641526

Batch 168770, train_perplexity=815.5087, train_loss=6.703812

Batch 168780, train_perplexity=753.53314, train_loss=6.624773

Batch 168790, train_perplexity=796.23395, train_loss=6.679893

Batch 168800, train_perplexity=772.0761, train_loss=6.649083

Batch 168810, train_perplexity=869.80334, train_loss=6.768267

Batch 168820, train_perplexity=890.4081, train_loss=6.79168

Batch 168830, train_perplexity=690.5316, train_loss=6.5374618

Batch 168840, train_perplexity=805.3428, train_loss=6.691268

Batch 168850, train_perplexity=776.5422, train_loss=6.654851

Batch 168860, train_perplexity=800.3217, train_loss=6.685014

Batch 168870, train_perplexity=850.3653, train_loss=6.745666

Batch 168880, train_perplexity=736.59686, train_loss=6.602041

Batch 168890, train_perplexity=786.35474, train_loss=6.667408

Batch 168900, train_perplexity=848.3229, train_loss=6.7432613

Batch 168910, train_perplexity=731.14734, train_loss=6.594615

Batch 168920, train_perplexity=742.6988, train_loss=6.6102905

Batch 168930, train_perplexity=887.2315, train_loss=6.788106

Batch 168940, train_perplexity=696.77, train_loss=6.5464554

Batch 168950, train_perplexity=891.73755, train_loss=6.793172

Batch 168960, train_perplexity=781.32983, train_loss=6.6609974

Batch 168970, train_perplexity=746.2348, train_loss=6.6150403

Batch 168980, train_perplexity=767.7569, train_loss=6.643473

Batch 168990, train_perplexity=708.4962, train_loss=6.5631447

Batch 169000, train_perplexity=882.9078, train_loss=6.783221

Batch 169010, train_perplexity=796.75464, train_loss=6.6805468

Batch 169020, train_perplexity=889.8992, train_loss=6.791108

Batch 169030, train_perplexity=857.9847, train_loss=6.754586

Batch 169040, train_perplexity=790.98975, train_loss=6.673285

Batch 169050, train_perplexity=800.3904, train_loss=6.6850996

Batch 169060, train_perplexity=796.74097, train_loss=6.6805296

Batch 169070, train_perplexity=769.9834, train_loss=6.646369

Batch 169080, train_perplexity=727.942, train_loss=6.5902214

Batch 169090, train_perplexity=777.85187, train_loss=6.656536

Batch 169100, train_perplexity=819.2699, train_loss=6.7084136

Batch 169110, train_perplexity=824.75073, train_loss=6.715081

Batch 169120, train_perplexity=725.26794, train_loss=6.586541

Batch 169130, train_perplexity=848.7676, train_loss=6.7437854

Batch 169140, train_perplexity=910.1914, train_loss=6.813655

Batch 169150, train_perplexity=826.8093, train_loss=6.717574

Batch 169160, train_perplexity=749.7008, train_loss=6.619674

Batch 169170, train_perplexity=743.04944, train_loss=6.6107626

Batch 169180, train_perplexity=787.98865, train_loss=6.6694837

Batch 169190, train_perplexity=828.46924, train_loss=6.7195797

Batch 169200, train_perplexity=873.9154, train_loss=6.7729836

Batch 169210, train_perplexity=745.95337, train_loss=6.614663

Batch 169220, train_perplexity=741.11743, train_loss=6.608159

Batch 169230, train_perplexity=773.90393, train_loss=6.651448

Batch 169240, train_perplexity=763.6905, train_loss=6.6381626

Batch 169250, train_perplexity=794.32416, train_loss=6.6774917

Batch 169260, train_perplexity=792.63293, train_loss=6.67536

Batch 169270, train_perplexity=819.15936, train_loss=6.7082787

Batch 169280, train_perplexity=809.1296, train_loss=6.695959

Batch 169290, train_perplexity=810.9864, train_loss=6.6982512

Batch 169300, train_perplexity=804.61957, train_loss=6.6903696

Batch 169310, train_perplexity=777.9272, train_loss=6.656633

Batch 169320, train_perplexity=806.3095, train_loss=6.6924677

Batch 169330, train_perplexity=814.3884, train_loss=6.7024374

Batch 169340, train_perplexity=757.1724, train_loss=6.629591

Batch 169350, train_perplexity=825.47235, train_loss=6.7159557

Batch 169360, train_perplexity=932.8358, train_loss=6.838229

Batch 169370, train_perplexity=806.681, train_loss=6.6929283

Batch 169380, train_perplexity=813.30725, train_loss=6.701109

Batch 169390, train_perplexity=799.42346, train_loss=6.683891

Batch 169400, train_perplexity=840.11847, train_loss=6.733543

Batch 169410, train_perplexity=763.0688, train_loss=6.637348

Batch 169420, train_perplexity=785.27106, train_loss=6.666029

Batch 169430, train_perplexity=729.57526, train_loss=6.5924625

Batch 169440, train_perplexity=835.2195, train_loss=6.7276945

Batch 169450, train_perplexity=796.50085, train_loss=6.680228

Batch 169460, train_perplexity=815.3746, train_loss=6.7036476

Batch 169470, train_perplexity=788.05963, train_loss=6.669574

Batch 169480, train_perplexity=789.2254, train_loss=6.671052

Batch 169490, train_perplexity=748.73303, train_loss=6.6183825

Batch 169500, train_perplexity=870.8932, train_loss=6.7695193

Batch 169510, train_perplexity=810.63184, train_loss=6.697814

Batch 169520, train_perplexity=743.59283, train_loss=6.6114936

Batch 169530, train_perplexity=768.9993, train_loss=6.64509

Batch 169540, train_perplexity=800.65796, train_loss=6.685434

Batch 169550, train_perplexity=754.72345, train_loss=6.6263514

Batch 169560, train_perplexity=778.2733, train_loss=6.657078

Batch 169570, train_perplexity=778.19244, train_loss=6.656974

Batch 169580, train_perplexity=800.1836, train_loss=6.684841

Batch 169590, train_perplexity=742.064, train_loss=6.6094356

Batch 169600, train_perplexity=783.50195, train_loss=6.6637735

Batch 169610, train_perplexity=770.42487, train_loss=6.646942

Batch 169620, train_perplexity=771.58295, train_loss=6.648444

Batch 169630, train_perplexity=843.45605, train_loss=6.737508

Batch 169640, train_perplexity=788.84393, train_loss=6.6705685

Batch 169650, train_perplexity=803.9841, train_loss=6.6895795

Batch 169660, train_perplexity=787.1245, train_loss=6.6683865

Batch 169670, train_perplexity=913.225, train_loss=6.8169823

Batch 169680, train_perplexity=732.87164, train_loss=6.5969706

Batch 169690, train_perplexity=756.023, train_loss=6.628072

Batch 169700, train_perplexity=827.27075, train_loss=6.718132

Batch 169710, train_perplexity=850.09814, train_loss=6.745352
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 169720, train_perplexity=841.9645, train_loss=6.735738

Batch 169730, train_perplexity=835.36206, train_loss=6.727865

Batch 169740, train_perplexity=748.89655, train_loss=6.618601

Batch 169750, train_perplexity=806.803, train_loss=6.6930795

Batch 169760, train_perplexity=778.8324, train_loss=6.657796

Batch 169770, train_perplexity=838.6404, train_loss=6.731782

Batch 169780, train_perplexity=752.5629, train_loss=6.6234846

Batch 169790, train_perplexity=802.2344, train_loss=6.687401

Batch 169800, train_perplexity=858.0624, train_loss=6.754677

Batch 169810, train_perplexity=756.65845, train_loss=6.628912

Batch 169820, train_perplexity=794.41925, train_loss=6.6776114

Batch 169830, train_perplexity=773.1456, train_loss=6.6504674

Batch 169840, train_perplexity=825.163, train_loss=6.715581

Batch 169850, train_perplexity=862.788, train_loss=6.760169

Batch 169860, train_perplexity=796.5024, train_loss=6.68023

Batch 169870, train_perplexity=788.9752, train_loss=6.670735

Batch 169880, train_perplexity=772.7501, train_loss=6.6499557

Batch 169890, train_perplexity=780.94696, train_loss=6.660507

Batch 169900, train_perplexity=717.3401, train_loss=6.57555

Batch 169910, train_perplexity=770.76697, train_loss=6.647386

Batch 169920, train_perplexity=872.86755, train_loss=6.771784

Batch 169930, train_perplexity=798.946, train_loss=6.6832933

Batch 169940, train_perplexity=798.88544, train_loss=6.6832175

Batch 169950, train_perplexity=782.72375, train_loss=6.66278

Batch 169960, train_perplexity=794.905, train_loss=6.6782227

Batch 169970, train_perplexity=810.1264, train_loss=6.6971903

Batch 169980, train_perplexity=848.6077, train_loss=6.743597

Batch 169990, train_perplexity=785.7962, train_loss=6.6666975

Batch 170000, train_perplexity=795.5512, train_loss=6.679035

Batch 170010, train_perplexity=791.3319, train_loss=6.6737175

Batch 170020, train_perplexity=751.1855, train_loss=6.6216526

Batch 170030, train_perplexity=826.98206, train_loss=6.717783

Batch 170040, train_perplexity=754.9142, train_loss=6.626604

Batch 170050, train_perplexity=791.20966, train_loss=6.673563

Batch 170060, train_perplexity=782.6349, train_loss=6.6626663

Batch 170070, train_perplexity=796.1485, train_loss=6.6797857

Batch 170080, train_perplexity=760.2606, train_loss=6.6336613

Batch 170090, train_perplexity=806.59906, train_loss=6.6928267

Batch 170100, train_perplexity=744.20825, train_loss=6.612321

Batch 170110, train_perplexity=818.153, train_loss=6.7070494

Batch 170120, train_perplexity=808.7531, train_loss=6.6954937

Batch 170130, train_perplexity=765.4868, train_loss=6.640512

Batch 170140, train_perplexity=914.6169, train_loss=6.8185053

Batch 170150, train_perplexity=794.5602, train_loss=6.6777887

Batch 170160, train_perplexity=782.29614, train_loss=6.6622334

Batch 170170, train_perplexity=870.0361, train_loss=6.7685347

Batch 170180, train_perplexity=821.7093, train_loss=6.7113867

Batch 170190, train_perplexity=822.5239, train_loss=6.7123775

Batch 170200, train_perplexity=668.6995, train_loss=6.505335

Batch 170210, train_perplexity=818.0457, train_loss=6.7069182

Batch 170220, train_perplexity=772.87177, train_loss=6.650113

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Batch 170230, train_perplexity=826.7167, train_loss=6.717462

Batch 170240, train_perplexity=759.7424, train_loss=6.6329794

Batch 170250, train_perplexity=909.7896, train_loss=6.8132133

Batch 170260, train_perplexity=730.34174, train_loss=6.5935125

Batch 170270, train_perplexity=806.78796, train_loss=6.693061

Batch 170280, train_perplexity=831.36206, train_loss=6.7230654

Batch 170290, train_perplexity=837.50385, train_loss=6.730426

Batch 170300, train_perplexity=817.731, train_loss=6.7065334

Batch 170310, train_perplexity=809.7448, train_loss=6.696719

Batch 170320, train_perplexity=816.2958, train_loss=6.704777

Batch 170330, train_perplexity=856.195, train_loss=6.752498

Batch 170340, train_perplexity=893.87256, train_loss=6.795563

Batch 170350, train_perplexity=778.6089, train_loss=6.657509

Batch 170360, train_perplexity=862.8086, train_loss=6.760193

Batch 170370, train_perplexity=759.5808, train_loss=6.6327667

Batch 170380, train_perplexity=732.2652, train_loss=6.596143

Batch 170390, train_perplexity=844.96967, train_loss=6.7393007

Batch 170400, train_perplexity=837.7974, train_loss=6.7307763

Batch 170410, train_perplexity=836.7681, train_loss=6.729547

Batch 170420, train_perplexity=760.4995, train_loss=6.6339755

Batch 170430, train_perplexity=848.4548, train_loss=6.743417

Batch 170440, train_perplexity=765.4813, train_loss=6.640505

Batch 170450, train_perplexity=796.6855, train_loss=6.68046

Batch 170460, train_perplexity=815.4477, train_loss=6.7037373

Batch 170470, train_perplexity=742.2297, train_loss=6.6096587

Batch 170480, train_perplexity=831.5635, train_loss=6.7233076

Batch 170490, train_perplexity=805.1308, train_loss=6.6910048

Batch 170500, train_perplexity=829.8997, train_loss=6.721305

Batch 170510, train_perplexity=777.4032, train_loss=6.655959

Batch 170520, train_perplexity=833.74243, train_loss=6.7259245

Batch 170530, train_perplexity=836.7478, train_loss=6.7295227

Batch 170540, train_perplexity=817.5688, train_loss=6.706335

Batch 170550, train_perplexity=792.6454, train_loss=6.675376

Batch 170560, train_perplexity=824.3757, train_loss=6.7146263

Batch 170570, train_perplexity=772.24036, train_loss=6.649296

Batch 170580, train_perplexity=810.5031, train_loss=6.697655

Batch 170590, train_perplexity=804.5087, train_loss=6.690232

Batch 170600, train_perplexity=742.22473, train_loss=6.609652

Batch 170610, train_perplexity=828.9695, train_loss=6.7201834

Batch 170620, train_perplexity=752.3857, train_loss=6.623249

Batch 170630, train_perplexity=768.037, train_loss=6.643838

Batch 170640, train_perplexity=807.8385, train_loss=6.694362

Batch 170650, train_perplexity=772.2786, train_loss=6.6493454

Batch 170660, train_perplexity=821.46716, train_loss=6.711092

Batch 170670, train_perplexity=833.0553, train_loss=6.7251

Batch 170680, train_perplexity=790.70996, train_loss=6.672931

Batch 170690, train_perplexity=713.9311, train_loss=6.5707865

Batch 170700, train_perplexity=799.66174, train_loss=6.684189

Batch 170710, train_perplexity=753.6765, train_loss=6.6249633

Batch 170720, train_perplexity=916.18176, train_loss=6.8202147

Batch 170730, train_perplexity=761.4063, train_loss=6.635167

Batch 170740, train_perplexity=885.4214, train_loss=6.7860637

Batch 170750, train_perplexity=945.6418, train_loss=6.851864

Batch 170760, train_perplexity=836.9154, train_loss=6.729723

Batch 170770, train_perplexity=801.6195, train_loss=6.686634

Batch 170780, train_perplexity=760.67834, train_loss=6.6342106

Batch 170790, train_perplexity=749.2323, train_loss=6.619049

Batch 170800, train_perplexity=840.5841, train_loss=6.734097

Batch 170810, train_perplexity=767.87225, train_loss=6.6436234

Batch 170820, train_perplexity=815.96655, train_loss=6.7043734

Batch 170830, train_perplexity=787.52545, train_loss=6.6688957

Batch 170840, train_perplexity=752.4581, train_loss=6.6233454

Batch 170850, train_perplexity=743.3769, train_loss=6.611203

Batch 170860, train_perplexity=720.84515, train_loss=6.5804243

Batch 170870, train_perplexity=804.2011, train_loss=6.6898494

Batch 170880, train_perplexity=765.77264, train_loss=6.6408854

Batch 170890, train_perplexity=849.32385, train_loss=6.7444406

Batch 170900, train_perplexity=734.5236, train_loss=6.599222

Batch 170910, train_perplexity=790.6168, train_loss=6.6728134

Batch 170920, train_perplexity=754.64136, train_loss=6.6262426

Batch 170930, train_perplexity=763.4491, train_loss=6.6378465

Batch 170940, train_perplexity=769.8454, train_loss=6.6461897

Batch 170950, train_perplexity=879.9208, train_loss=6.779832

Batch 170960, train_perplexity=781.2963, train_loss=6.6609545
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 170970, train_perplexity=756.0936, train_loss=6.6281652

Batch 170980, train_perplexity=909.09705, train_loss=6.812452

Batch 170990, train_perplexity=841.9058, train_loss=6.735668

Batch 171000, train_perplexity=756.7014, train_loss=6.6289687

Batch 171010, train_perplexity=806.8295, train_loss=6.6931124

Batch 171020, train_perplexity=750.22003, train_loss=6.6203666

Batch 171030, train_perplexity=746.90015, train_loss=6.6159315

Batch 171040, train_perplexity=912.579, train_loss=6.8162746

Batch 171050, train_perplexity=766.36444, train_loss=6.641658

Batch 171060, train_perplexity=869.8179, train_loss=6.768284

Batch 171070, train_perplexity=768.728, train_loss=6.6447372

Batch 171080, train_perplexity=831.24866, train_loss=6.722929

Batch 171090, train_perplexity=754.6867, train_loss=6.6263027

Batch 171100, train_perplexity=849.02625, train_loss=6.74409

Batch 171110, train_perplexity=767.0415, train_loss=6.642541

Batch 171120, train_perplexity=729.12836, train_loss=6.59185

Batch 171130, train_perplexity=818.9266, train_loss=6.7079945

Batch 171140, train_perplexity=853.0962, train_loss=6.7488723

Batch 171150, train_perplexity=776.8966, train_loss=6.6553073

Batch 171160, train_perplexity=830.1321, train_loss=6.721585

Batch 171170, train_perplexity=837.4603, train_loss=6.730374

Batch 171180, train_perplexity=843.2747, train_loss=6.737293

Batch 171190, train_perplexity=855.181, train_loss=6.751313

Batch 171200, train_perplexity=804.41394, train_loss=6.690114

Batch 171210, train_perplexity=805.83673, train_loss=6.691881

Batch 171220, train_perplexity=773.08734, train_loss=6.650392

Batch 171230, train_perplexity=739.40155, train_loss=6.605841

Batch 171240, train_perplexity=850.75305, train_loss=6.746122

Batch 171250, train_perplexity=775.1512, train_loss=6.653058

Batch 171260, train_perplexity=715.74554, train_loss=6.5733247

Batch 171270, train_perplexity=766.54425, train_loss=6.6418924

Batch 171280, train_perplexity=796.5423, train_loss=6.68028

Batch 171290, train_perplexity=743.7577, train_loss=6.6117153

Batch 171300, train_perplexity=757.7496, train_loss=6.630353

Batch 171310, train_perplexity=754.2018, train_loss=6.62566

Batch 171320, train_perplexity=828.5447, train_loss=6.719671

Batch 171330, train_perplexity=801.2083, train_loss=6.686121

Batch 171340, train_perplexity=862.25665, train_loss=6.759553

Batch 171350, train_perplexity=796.068, train_loss=6.6796846

Batch 171360, train_perplexity=809.3765, train_loss=6.6962643

Batch 171370, train_perplexity=710.0333, train_loss=6.565312

Batch 171380, train_perplexity=801.0116, train_loss=6.6858754

Batch 171390, train_perplexity=800.0012, train_loss=6.684613

Batch 171400, train_perplexity=753.02203, train_loss=6.6240945

Batch 171410, train_perplexity=786.2036, train_loss=6.667216

Batch 171420, train_perplexity=796.12384, train_loss=6.6797547

Batch 171430, train_perplexity=827.38043, train_loss=6.7182646

Batch 171440, train_perplexity=738.7457, train_loss=6.604954

Batch 171450, train_perplexity=772.5829, train_loss=6.6497393

Batch 171460, train_perplexity=743.5549, train_loss=6.6114426

Batch 171470, train_perplexity=786.3862, train_loss=6.667448

Batch 171480, train_perplexity=767.0415, train_loss=6.642541

Batch 171490, train_perplexity=820.8689, train_loss=6.7103634

Batch 171500, train_perplexity=725.58307, train_loss=6.5869756

Batch 171510, train_perplexity=724.0019, train_loss=6.584794

Batch 171520, train_perplexity=765.7705, train_loss=6.6408825

Batch 171530, train_perplexity=825.7589, train_loss=6.716303

Batch 171540, train_perplexity=796.0726, train_loss=6.6796904

Batch 171550, train_perplexity=743.98615, train_loss=6.6120224

Batch 171560, train_perplexity=754.00977, train_loss=6.6254053

Batch 171570, train_perplexity=786.67725, train_loss=6.667818

Batch 171580, train_perplexity=885.50037, train_loss=6.786153

Batch 171590, train_perplexity=795.4874, train_loss=6.678955

Batch 171600, train_perplexity=831.54285, train_loss=6.723283

Batch 171610, train_perplexity=768.3828, train_loss=6.644288

Batch 171620, train_perplexity=826.4002, train_loss=6.717079

Batch 171630, train_perplexity=810.9056, train_loss=6.6981516

Batch 171640, train_perplexity=810.96436, train_loss=6.698224

Batch 171650, train_perplexity=776.5289, train_loss=6.654834

Batch 171660, train_perplexity=818.448, train_loss=6.70741

Batch 171670, train_perplexity=725.7153, train_loss=6.5871577

Batch 171680, train_perplexity=765.76135, train_loss=6.6408706

Batch 171690, train_perplexity=760.8605, train_loss=6.63445

Batch 171700, train_perplexity=835.63055, train_loss=6.7281866

Batch 171710, train_perplexity=774.92865, train_loss=6.652771

Batch 171720, train_perplexity=836.5942, train_loss=6.729339

Batch 171730, train_perplexity=919.9856, train_loss=6.824358

Batch 171740, train_perplexity=713.0986, train_loss=6.5696197

Batch 171750, train_perplexity=727.6911, train_loss=6.5898767

Batch 171760, train_perplexity=722.9132, train_loss=6.583289

Batch 171770, train_perplexity=769.9559, train_loss=6.646333

Batch 171780, train_perplexity=774.0076, train_loss=6.651582

Batch 171790, train_perplexity=695.94916, train_loss=6.5452766

Batch 171800, train_perplexity=824.3631, train_loss=6.714611

Batch 171810, train_perplexity=713.34686, train_loss=6.5699677

Batch 171820, train_perplexity=939.05444, train_loss=6.8448734

Batch 171830, train_perplexity=945.6616, train_loss=6.851885

Batch 171840, train_perplexity=753.2745, train_loss=6.6244297

Batch 171850, train_perplexity=766.6016, train_loss=6.6419673

Batch 171860, train_perplexity=786.3401, train_loss=6.6673894

Batch 171870, train_perplexity=757.39233, train_loss=6.6298814

Batch 171880, train_perplexity=786.00757, train_loss=6.6669664

Batch 171890, train_perplexity=727.9663, train_loss=6.590255

Batch 171900, train_perplexity=729.00146, train_loss=6.5916758

Batch 171910, train_perplexity=786.579, train_loss=6.667693

Batch 171920, train_perplexity=785.73553, train_loss=6.6666203

Batch 171930, train_perplexity=748.879, train_loss=6.6185775

Batch 171940, train_perplexity=755.09924, train_loss=6.626849

Batch 171950, train_perplexity=906.02576, train_loss=6.8090677

Batch 171960, train_perplexity=734.2365, train_loss=6.598831

Batch 171970, train_perplexity=775.98175, train_loss=6.654129

Batch 171980, train_perplexity=801.2083, train_loss=6.686121

Batch 171990, train_perplexity=758.62665, train_loss=6.63151

Batch 172000, train_perplexity=836.16785, train_loss=6.7288294

Batch 172010, train_perplexity=798.28644, train_loss=6.6824675

Batch 172020, train_perplexity=851.58386, train_loss=6.747098

Batch 172030, train_perplexity=793.4478, train_loss=6.676388

Batch 172040, train_perplexity=719.9918, train_loss=6.57924

Batch 172050, train_perplexity=837.76666, train_loss=6.7307396

Batch 172060, train_perplexity=722.0368, train_loss=6.582076

Batch 172070, train_perplexity=741.8744, train_loss=6.60918

Batch 172080, train_perplexity=778.5818, train_loss=6.657474

Batch 172090, train_perplexity=767.7763, train_loss=6.6434984

Batch 172100, train_perplexity=858.9024, train_loss=6.7556553

Batch 172110, train_perplexity=783.103, train_loss=6.6632643

Batch 172120, train_perplexity=815.5449, train_loss=6.7038565

Batch 172130, train_perplexity=781.0989, train_loss=6.6607018

Batch 172140, train_perplexity=768.5081, train_loss=6.644451

Batch 172150, train_perplexity=774.3118, train_loss=6.6519747

Batch 172160, train_perplexity=804.8989, train_loss=6.6907167

Batch 172170, train_perplexity=793.5693, train_loss=6.676541

Batch 172180, train_perplexity=819.4407, train_loss=6.708622

Batch 172190, train_perplexity=809.43524, train_loss=6.6963367

Batch 172200, train_perplexity=802.751, train_loss=6.6880445

Batch 172210, train_perplexity=778.44293, train_loss=6.6572957

Batch 172220, train_perplexity=826.7928, train_loss=6.717554

Batch 172230, train_perplexity=767.96594, train_loss=6.6437454

Batch 172240, train_perplexity=754.8321, train_loss=6.6264954

Batch 172250, train_perplexity=806.2503, train_loss=6.6923943

Batch 172260, train_perplexity=804.40704, train_loss=6.6901054

Batch 172270, train_perplexity=791.3538, train_loss=6.673745
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 172280, train_perplexity=756.33167, train_loss=6.62848

Batch 172290, train_perplexity=814.4358, train_loss=6.7024956

Batch 172300, train_perplexity=737.1018, train_loss=6.602726

Batch 172310, train_perplexity=796.57153, train_loss=6.680317

Batch 172320, train_perplexity=765.3317, train_loss=6.6403093

Batch 172330, train_perplexity=897.8252, train_loss=6.7999754

Batch 172340, train_perplexity=812.0215, train_loss=6.699527

Batch 172350, train_perplexity=833.60724, train_loss=6.7257624

Batch 172360, train_perplexity=756.0079, train_loss=6.6280518

Batch 172370, train_perplexity=783.9631, train_loss=6.664362

Batch 172380, train_perplexity=795.7933, train_loss=6.6793394

Batch 172390, train_perplexity=747.89734, train_loss=6.6172657

Batch 172400, train_perplexity=738.35126, train_loss=6.6044197

Batch 172410, train_perplexity=716.13776, train_loss=6.5738726

Batch 172420, train_perplexity=728.3559, train_loss=6.59079

Batch 172430, train_perplexity=792.8374, train_loss=6.675618

Batch 172440, train_perplexity=777.78436, train_loss=6.6564493

Batch 172450, train_perplexity=838.27295, train_loss=6.7313437

Batch 172460, train_perplexity=802.77625, train_loss=6.688076

Batch 172470, train_perplexity=745.6077, train_loss=6.6141996

Batch 172480, train_perplexity=772.4731, train_loss=6.649597

Batch 172490, train_perplexity=743.21387, train_loss=6.610984

Batch 172500, train_perplexity=794.1079, train_loss=6.6772194

Batch 172510, train_perplexity=805.8129, train_loss=6.6918516

Batch 172520, train_perplexity=808.05, train_loss=6.694624

Batch 172530, train_perplexity=813.4992, train_loss=6.701345

Batch 172540, train_perplexity=813.2359, train_loss=6.701021

Batch 172550, train_perplexity=856.2518, train_loss=6.7525644

Batch 172560, train_perplexity=848.2776, train_loss=6.743208

Batch 172570, train_perplexity=745.94666, train_loss=6.614654

Batch 172580, train_perplexity=821.8876, train_loss=6.7116036

Batch 172590, train_perplexity=794.7269, train_loss=6.6779985

Batch 172600, train_perplexity=882.14276, train_loss=6.782354

Batch 172610, train_perplexity=819.86786, train_loss=6.709143

Batch 172620, train_perplexity=728.89026, train_loss=6.591523

Batch 172630, train_perplexity=831.1603, train_loss=6.7228227

Batch 172640, train_perplexity=840.80176, train_loss=6.734356

Batch 172650, train_perplexity=712.43884, train_loss=6.568694

Batch 172660, train_perplexity=794.42303, train_loss=6.677616

Batch 172670, train_perplexity=805.9989, train_loss=6.6920824

Batch 172680, train_perplexity=754.9952, train_loss=6.6267114

Batch 172690, train_perplexity=757.7727, train_loss=6.6303835

Batch 172700, train_perplexity=697.31445, train_loss=6.5472364

Batch 172710, train_perplexity=807.4268, train_loss=6.6938524

Batch 172720, train_perplexity=768.18646, train_loss=6.6440325

Batch 172730, train_perplexity=780.83, train_loss=6.6603575

Batch 172740, train_perplexity=813.4298, train_loss=6.7012596

Batch 172750, train_perplexity=858.81885, train_loss=6.755558

Batch 172760, train_perplexity=850.70557, train_loss=6.746066

Batch 172770, train_perplexity=789.40985, train_loss=6.6712856

Batch 172780, train_perplexity=804.07263, train_loss=6.6896896

Batch 172790, train_perplexity=738.3893, train_loss=6.604471

Batch 172800, train_perplexity=769.86005, train_loss=6.646209

Batch 172810, train_perplexity=699.18097, train_loss=6.5499096

Batch 172820, train_perplexity=728.38995, train_loss=6.5908365

Batch 172830, train_perplexity=826.4034, train_loss=6.717083

Batch 172840, train_perplexity=735.0092, train_loss=6.599883

Batch 172850, train_perplexity=740.44275, train_loss=6.6072483

Batch 172860, train_perplexity=811.9673, train_loss=6.69946

Batch 172870, train_perplexity=796.8317, train_loss=6.6806436

Batch 172880, train_perplexity=761.85736, train_loss=6.6357594

Batch 172890, train_perplexity=831.2558, train_loss=6.7229376

Batch 172900, train_perplexity=775.2495, train_loss=6.653185

Batch 172910, train_perplexity=685.116, train_loss=6.529588

Batch 172920, train_perplexity=787.91614, train_loss=6.6693916

Batch 172930, train_perplexity=814.6016, train_loss=6.702699

Batch 172940, train_perplexity=754.6586, train_loss=6.6262655

Batch 172950, train_perplexity=845.74036, train_loss=6.7402124

Batch 172960, train_perplexity=781.5624, train_loss=6.661295

Batch 172970, train_perplexity=728.3976, train_loss=6.590847

Batch 172980, train_perplexity=742.89496, train_loss=6.6105547

Batch 172990, train_perplexity=880.1344, train_loss=6.7800746

Batch 173000, train_perplexity=821.94366, train_loss=6.711672

Batch 173010, train_perplexity=763.2624, train_loss=6.637602

Batch 173020, train_perplexity=713.8576, train_loss=6.5706835

Batch 173030, train_perplexity=754.1953, train_loss=6.6256514

Batch 173040, train_perplexity=811.84686, train_loss=6.6993117

Batch 173050, train_perplexity=810.16003, train_loss=6.697232

Batch 173060, train_perplexity=710.3686, train_loss=6.565784

Batch 173070, train_perplexity=875.6406, train_loss=6.7749557

Batch 173080, train_perplexity=699.77234, train_loss=6.550755

Batch 173090, train_perplexity=788.8582, train_loss=6.6705866

Batch 173100, train_perplexity=780.4794, train_loss=6.6599083

Batch 173110, train_perplexity=808.46857, train_loss=6.695142

Batch 173120, train_perplexity=765.585, train_loss=6.6406403

Batch 173130, train_perplexity=751.08234, train_loss=6.6215153

Batch 173140, train_perplexity=671.6517, train_loss=6.50974

Batch 173150, train_perplexity=809.6985, train_loss=6.696662

Batch 173160, train_perplexity=775.3094, train_loss=6.653262

Batch 173170, train_perplexity=812.4533, train_loss=6.7000585

Batch 173180, train_perplexity=897.63043, train_loss=6.7997584

Batch 173190, train_perplexity=749.06586, train_loss=6.618827

Batch 173200, train_perplexity=756.9811, train_loss=6.6293383

Batch 173210, train_perplexity=836.94495, train_loss=6.7297583

Batch 173220, train_perplexity=692.6798, train_loss=6.540568

Batch 173230, train_perplexity=826.8196, train_loss=6.7175865

Batch 173240, train_perplexity=754.301, train_loss=6.6257915

Batch 173250, train_perplexity=785.24335, train_loss=6.6659937

Batch 173260, train_perplexity=799.39453, train_loss=6.6838546

Batch 173270, train_perplexity=748.18445, train_loss=6.6176496

Batch 173280, train_perplexity=780.3115, train_loss=6.6596932

Batch 173290, train_perplexity=873.05695, train_loss=6.772001

Batch 173300, train_perplexity=883.60565, train_loss=6.784011

Batch 173310, train_perplexity=867.0397, train_loss=6.7650847

Batch 173320, train_perplexity=808.3537, train_loss=6.6949997

Batch 173330, train_perplexity=745.2633, train_loss=6.6137376

Batch 173340, train_perplexity=762.0463, train_loss=6.6360073

Batch 173350, train_perplexity=776.8103, train_loss=6.655196

Batch 173360, train_perplexity=746.1796, train_loss=6.6149664

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Batch 173370, train_perplexity=791.4787, train_loss=6.673903

Batch 173380, train_perplexity=823.145, train_loss=6.7131324

Batch 173390, train_perplexity=740.2288, train_loss=6.6069593

Batch 173400, train_perplexity=795.44574, train_loss=6.6789026

Batch 173410, train_perplexity=743.0778, train_loss=6.6108007

Batch 173420, train_perplexity=736.02704, train_loss=6.601267

Batch 173430, train_perplexity=824.3564, train_loss=6.714603

Batch 173440, train_perplexity=800.8878, train_loss=6.685721

Batch 173450, train_perplexity=819.32733, train_loss=6.7084837

Batch 173460, train_perplexity=781.7733, train_loss=6.661565

Batch 173470, train_perplexity=774.22656, train_loss=6.6518645

Batch 173480, train_perplexity=826.44, train_loss=6.7171273

Batch 173490, train_perplexity=748.2341, train_loss=6.617716

Batch 173500, train_perplexity=789.51825, train_loss=6.671423

Batch 173510, train_perplexity=795.7758, train_loss=6.6793175

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 173520, train_perplexity=830.5498, train_loss=6.722088

Batch 173530, train_perplexity=830.0612, train_loss=6.7214994

Batch 173540, train_perplexity=794.3382, train_loss=6.6775093

Batch 173550, train_perplexity=815.8759, train_loss=6.7042623

Batch 173560, train_perplexity=771.6624, train_loss=6.648547

Batch 173570, train_perplexity=907.9997, train_loss=6.811244

Batch 173580, train_perplexity=809.9116, train_loss=6.696925

Batch 173590, train_perplexity=847.58295, train_loss=6.7423887

Batch 173600, train_perplexity=809.0833, train_loss=6.695902

Batch 173610, train_perplexity=750.16534, train_loss=6.6202936

Batch 173620, train_perplexity=822.4494, train_loss=6.712287

Batch 173630, train_perplexity=830.9998, train_loss=6.7226295

Batch 173640, train_perplexity=785.50104, train_loss=6.6663218

Batch 173650, train_perplexity=825.26215, train_loss=6.715701

Batch 173660, train_perplexity=726.8453, train_loss=6.5887136

Batch 173670, train_perplexity=794.1223, train_loss=6.6772375

Batch 173680, train_perplexity=815.0449, train_loss=6.7032433

Batch 173690, train_perplexity=846.2971, train_loss=6.7408705

Batch 173700, train_perplexity=777.6509, train_loss=6.6562777

Batch 173710, train_perplexity=841.1202, train_loss=6.7347345

Batch 173720, train_perplexity=826.6004, train_loss=6.7173214

Batch 173730, train_perplexity=808.0508, train_loss=6.694625

Batch 173740, train_perplexity=733.02856, train_loss=6.5971847

Batch 173750, train_perplexity=717.92664, train_loss=6.5763674

Batch 173760, train_perplexity=810.7988, train_loss=6.69802

Batch 173770, train_perplexity=755.3668, train_loss=6.6272035

Batch 173780, train_perplexity=825.4015, train_loss=6.71587

Batch 173790, train_perplexity=814.75, train_loss=6.7028813

Batch 173800, train_perplexity=834.97894, train_loss=6.7274065

Batch 173810, train_perplexity=816.56165, train_loss=6.7051024

Batch 173820, train_perplexity=705.83026, train_loss=6.559375

Batch 173830, train_perplexity=762.0514, train_loss=6.636014

Batch 173840, train_perplexity=761.8443, train_loss=6.635742

Batch 173850, train_perplexity=839.78406, train_loss=6.7331448

Batch 173860, train_perplexity=769.81824, train_loss=6.6461544

Batch 173870, train_perplexity=787.726, train_loss=6.6691504

Batch 173880, train_perplexity=823.09595, train_loss=6.713073

Batch 173890, train_perplexity=717.26483, train_loss=6.575445

Batch 173900, train_perplexity=792.69073, train_loss=6.675433

Batch 173910, train_perplexity=842.7891, train_loss=6.7367167

Batch 173920, train_perplexity=797.0609, train_loss=6.680931

Batch 173930, train_perplexity=849.46967, train_loss=6.744612

Batch 173940, train_perplexity=800.9138, train_loss=6.6857533

Batch 173950, train_perplexity=823.36487, train_loss=6.7133994

Batch 173960, train_perplexity=788.6175, train_loss=6.6702814

Batch 173970, train_perplexity=784.4186, train_loss=6.6649427

Batch 173980, train_perplexity=774.01654, train_loss=6.651593

Batch 173990, train_perplexity=899.45264, train_loss=6.8017864

Batch 174000, train_perplexity=815.0209, train_loss=6.7032137

Batch 174010, train_perplexity=864.5424, train_loss=6.7622004

Batch 174020, train_perplexity=839.94025, train_loss=6.7333307

Batch 174030, train_perplexity=846.7096, train_loss=6.741358

Batch 174040, train_perplexity=774.7971, train_loss=6.6526012

Batch 174050, train_perplexity=843.7529, train_loss=6.7378597

Batch 174060, train_perplexity=759.003, train_loss=6.6320057

Batch 174070, train_perplexity=792.98413, train_loss=6.675803

Batch 174080, train_perplexity=766.2742, train_loss=6.64154

Batch 174090, train_perplexity=753.0971, train_loss=6.624194

Batch 174100, train_perplexity=711.4998, train_loss=6.567375

Batch 174110, train_perplexity=808.22955, train_loss=6.694846

Batch 174120, train_perplexity=813.4065, train_loss=6.701231

Batch 174130, train_perplexity=792.9418, train_loss=6.67575

Batch 174140, train_perplexity=797.3228, train_loss=6.6812596

Batch 174150, train_perplexity=788.83636, train_loss=6.670559

Batch 174160, train_perplexity=737.7298, train_loss=6.6035776

Batch 174170, train_perplexity=726.3623, train_loss=6.588049

Batch 174180, train_perplexity=719.628, train_loss=6.5787344

Batch 174190, train_perplexity=721.75586, train_loss=6.581687

Batch 174200, train_perplexity=765.45654, train_loss=6.6404724

Batch 174210, train_perplexity=793.8092, train_loss=6.676843

Batch 174220, train_perplexity=750.12384, train_loss=6.6202383

Batch 174230, train_perplexity=756.3035, train_loss=6.628443

Batch 174240, train_perplexity=848.3492, train_loss=6.7432923

Batch 174250, train_perplexity=810.2894, train_loss=6.6973915

Batch 174260, train_perplexity=841.4363, train_loss=6.7351103

Batch 174270, train_perplexity=769.5591, train_loss=6.6458178

Batch 174280, train_perplexity=753.908, train_loss=6.6252704

Batch 174290, train_perplexity=794.6913, train_loss=6.6779537

Batch 174300, train_perplexity=810.0167, train_loss=6.697055

Batch 174310, train_perplexity=810.65814, train_loss=6.6978464

Batch 174320, train_perplexity=805.0778, train_loss=6.690939

Batch 174330, train_perplexity=782.08875, train_loss=6.661968

Batch 174340, train_perplexity=757.0533, train_loss=6.6294336

Batch 174350, train_perplexity=727.05914, train_loss=6.589008

Batch 174360, train_perplexity=726.1739, train_loss=6.5877895

Batch 174370, train_perplexity=860.81757, train_loss=6.7578826

Batch 174380, train_perplexity=797.3612, train_loss=6.681308

Batch 174390, train_perplexity=760.31573, train_loss=6.6337337

Batch 174400, train_perplexity=797.7735, train_loss=6.6818247

Batch 174410, train_perplexity=818.7189, train_loss=6.707741

Batch 174420, train_perplexity=740.69556, train_loss=6.6075897

Batch 174430, train_perplexity=739.12695, train_loss=6.6054697

Batch 174440, train_perplexity=740.30927, train_loss=6.607068

Batch 174450, train_perplexity=827.18317, train_loss=6.718026

Batch 174460, train_perplexity=797.3133, train_loss=6.6812477

Batch 174470, train_perplexity=770.4682, train_loss=6.6469984

Batch 174480, train_perplexity=819.80023, train_loss=6.7090607

Batch 174490, train_perplexity=802.4926, train_loss=6.6877227

Batch 174500, train_perplexity=676.85205, train_loss=6.5174527

Batch 174510, train_perplexity=815.5122, train_loss=6.7038164

Batch 174520, train_perplexity=817.11127, train_loss=6.7057753

Batch 174530, train_perplexity=843.21277, train_loss=6.7372193

Batch 174540, train_perplexity=743.62616, train_loss=6.6115384

Batch 174550, train_perplexity=737.32043, train_loss=6.6030226

Batch 174560, train_perplexity=815.9012, train_loss=6.7042933

Batch 174570, train_perplexity=779.38446, train_loss=6.6585045

Batch 174580, train_perplexity=905.0486, train_loss=6.8079886

Batch 174590, train_perplexity=746.0754, train_loss=6.6148267

Batch 174600, train_perplexity=770.4844, train_loss=6.6470194

Batch 174610, train_perplexity=808.67755, train_loss=6.6954

Batch 174620, train_perplexity=819.10974, train_loss=6.708218

Batch 174630, train_perplexity=841.2437, train_loss=6.7348814

Batch 174640, train_perplexity=807.39026, train_loss=6.693807

Batch 174650, train_perplexity=827.5655, train_loss=6.718488

Batch 174660, train_perplexity=813.952, train_loss=6.7019014

Batch 174670, train_perplexity=777.7762, train_loss=6.656439

Batch 174680, train_perplexity=677.81323, train_loss=6.518872

Batch 174690, train_perplexity=773.07117, train_loss=6.650371

Batch 174700, train_perplexity=752.89636, train_loss=6.6239276

Batch 174710, train_perplexity=754.33344, train_loss=6.6258345

Batch 174720, train_perplexity=816.021, train_loss=6.70444

Batch 174730, train_perplexity=776.9963, train_loss=6.6554356

Batch 174740, train_perplexity=770.8184, train_loss=6.647453

Batch 174750, train_perplexity=829.4701, train_loss=6.720787

Batch 174760, train_perplexity=818.6947, train_loss=6.707711

Batch 174770, train_perplexity=733.38763, train_loss=6.5976744

Batch 174780, train_perplexity=738.0528, train_loss=6.6040154

Batch 174790, train_perplexity=721.43726, train_loss=6.5812454

Batch 174800, train_perplexity=780.09686, train_loss=6.659418

Batch 174810, train_perplexity=807.9367, train_loss=6.6944838

Batch 174820, train_perplexity=797.01794, train_loss=6.680877
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 174830, train_perplexity=790.74084, train_loss=6.6729703

Batch 174840, train_perplexity=824.1548, train_loss=6.7143583

Batch 174850, train_perplexity=719.8696, train_loss=6.57907

Batch 174860, train_perplexity=865.30457, train_loss=6.7630816

Batch 174870, train_perplexity=796.0058, train_loss=6.6796064

Batch 174880, train_perplexity=790.69824, train_loss=6.6729164

Batch 174890, train_perplexity=778.8762, train_loss=6.657852

Batch 174900, train_perplexity=718.7912, train_loss=6.577571

Batch 174910, train_perplexity=729.7736, train_loss=6.5927343

Batch 174920, train_perplexity=804.47455, train_loss=6.6901894

Batch 174930, train_perplexity=813.95435, train_loss=6.7019043

Batch 174940, train_perplexity=753.33344, train_loss=6.624508

Batch 174950, train_perplexity=789.43396, train_loss=6.671316

Batch 174960, train_perplexity=781.2583, train_loss=6.660906

Batch 174970, train_perplexity=818.0126, train_loss=6.7068777

Batch 174980, train_perplexity=815.21906, train_loss=6.703457

Batch 174990, train_perplexity=793.0703, train_loss=6.675912

Batch 175000, train_perplexity=782.6394, train_loss=6.662672

Batch 175010, train_perplexity=733.2848, train_loss=6.597534

Batch 175020, train_perplexity=759.68335, train_loss=6.6329017

Batch 175030, train_perplexity=804.5544, train_loss=6.6902885

Batch 175040, train_perplexity=787.1654, train_loss=6.6684384

Batch 175050, train_perplexity=794.26624, train_loss=6.6774187

Batch 175060, train_perplexity=797.90283, train_loss=6.681987

Batch 175070, train_perplexity=832.8055, train_loss=6.7248

Batch 175080, train_perplexity=727.4437, train_loss=6.5895367

Batch 175090, train_perplexity=788.05286, train_loss=6.669565

Batch 175100, train_perplexity=860.8828, train_loss=6.7579584

Batch 175110, train_perplexity=775.8685, train_loss=6.653983

Batch 175120, train_perplexity=727.2266, train_loss=6.589238

Batch 175130, train_perplexity=837.0846, train_loss=6.729925

Batch 175140, train_perplexity=784.2128, train_loss=6.6646805

Batch 175150, train_perplexity=750.6641, train_loss=6.6209583

Batch 175160, train_perplexity=780.50543, train_loss=6.6599417

Batch 175170, train_perplexity=836.7849, train_loss=6.729567

Batch 175180, train_perplexity=854.33655, train_loss=6.750325

Batch 175190, train_perplexity=769.71545, train_loss=6.646021

Batch 175200, train_perplexity=760.71063, train_loss=6.634253

Batch 175210, train_perplexity=773.98553, train_loss=6.651553

Batch 175220, train_perplexity=849.13477, train_loss=6.744218

Batch 175230, train_perplexity=736.4877, train_loss=6.6018925

Batch 175240, train_perplexity=779.02856, train_loss=6.6580477

Batch 175250, train_perplexity=766.59326, train_loss=6.6419563

Batch 175260, train_perplexity=856.56866, train_loss=6.7529345

Batch 175270, train_perplexity=738.8493, train_loss=6.605094

Batch 175280, train_perplexity=817.48615, train_loss=6.706234

Batch 175290, train_perplexity=801.8638, train_loss=6.686939

Batch 175300, train_perplexity=728.565, train_loss=6.591077

Batch 175310, train_perplexity=756.42035, train_loss=6.6285973

Batch 175320, train_perplexity=755.4082, train_loss=6.6272583

Batch 175330, train_perplexity=747.0875, train_loss=6.6161823

Batch 175340, train_perplexity=774.0298, train_loss=6.6516104

Batch 175350, train_perplexity=786.7403, train_loss=6.667898

Batch 175360, train_perplexity=869.34143, train_loss=6.767736

Batch 175370, train_perplexity=772.9344, train_loss=6.650194

Batch 175380, train_perplexity=820.05707, train_loss=6.709374

Batch 175390, train_perplexity=728.292, train_loss=6.590702

Batch 175400, train_perplexity=698.75635, train_loss=6.549302

Batch 175410, train_perplexity=812.9978, train_loss=6.7007284

Batch 175420, train_perplexity=795.01044, train_loss=6.678355

Batch 175430, train_perplexity=746.1747, train_loss=6.6149597

Batch 175440, train_perplexity=720.50903, train_loss=6.579958

Batch 175450, train_perplexity=767.27637, train_loss=6.642847

Batch 175460, train_perplexity=847.1773, train_loss=6.74191

Batch 175470, train_perplexity=801.07153, train_loss=6.6859503

Batch 175480, train_perplexity=703.32294, train_loss=6.555816

Batch 175490, train_perplexity=740.2373, train_loss=6.606971

Batch 175500, train_perplexity=709.3325, train_loss=6.5643244

Batch 175510, train_perplexity=791.5591, train_loss=6.6740046

Batch 175520, train_perplexity=787.7636, train_loss=6.669198

Batch 175530, train_perplexity=773.26984, train_loss=6.650628

Batch 175540, train_perplexity=726.9146, train_loss=6.588809

Batch 175550, train_perplexity=810.13416, train_loss=6.6972

Batch 175560, train_perplexity=754.87024, train_loss=6.626546

Batch 175570, train_perplexity=771.1853, train_loss=6.6479287

Batch 175580, train_perplexity=700.97394, train_loss=6.5524707

Batch 175590, train_perplexity=830.78345, train_loss=6.722369

Batch 175600, train_perplexity=766.2219, train_loss=6.641472

Batch 175610, train_perplexity=790.1065, train_loss=6.672168

Batch 175620, train_perplexity=769.82336, train_loss=6.646161

Batch 175630, train_perplexity=840.6278, train_loss=6.734149

Batch 175640, train_perplexity=776.67255, train_loss=6.655019

Batch 175650, train_perplexity=850.82404, train_loss=6.7462053

Batch 175660, train_perplexity=772.94324, train_loss=6.6502056

Batch 175670, train_perplexity=858.32184, train_loss=6.754979

Batch 175680, train_perplexity=717.3394, train_loss=6.575549

Batch 175690, train_perplexity=790.74274, train_loss=6.6729727

Batch 175700, train_perplexity=853.56165, train_loss=6.749418

Batch 175710, train_perplexity=788.77734, train_loss=6.670484

Batch 175720, train_perplexity=722.1711, train_loss=6.582262

Batch 175730, train_perplexity=736.886, train_loss=6.602433

Batch 175740, train_perplexity=758.3489, train_loss=6.6311436

Batch 175750, train_perplexity=777.21344, train_loss=6.655715

Batch 175760, train_perplexity=736.7353, train_loss=6.6022286

Batch 175770, train_perplexity=824.6257, train_loss=6.7149296

Batch 175780, train_perplexity=759.6931, train_loss=6.6329145

Batch 175790, train_perplexity=793.82324, train_loss=6.676861

Batch 175800, train_perplexity=860.8406, train_loss=6.7579093

Batch 175810, train_perplexity=805.52094, train_loss=6.691489

Batch 175820, train_perplexity=787.7929, train_loss=6.669235

Batch 175830, train_perplexity=692.377, train_loss=6.5401306

Batch 175840, train_perplexity=780.71234, train_loss=6.660207

Batch 175850, train_perplexity=764.1735, train_loss=6.638795

Batch 175860, train_perplexity=783.21094, train_loss=6.663402

Batch 175870, train_perplexity=773.3451, train_loss=6.6507254

Batch 175880, train_perplexity=824.1237, train_loss=6.7143207

Batch 175890, train_perplexity=812.43353, train_loss=6.700034

Batch 175900, train_perplexity=731.2366, train_loss=6.594737

Batch 175910, train_perplexity=759.76196, train_loss=6.633005

Batch 175920, train_perplexity=782.28345, train_loss=6.662217

Batch 175930, train_perplexity=707.84784, train_loss=6.562229

Batch 175940, train_perplexity=765.7953, train_loss=6.640915

Batch 175950, train_perplexity=778.3245, train_loss=6.6571436

Batch 175960, train_perplexity=914.0985, train_loss=6.8179383

Batch 175970, train_perplexity=797.7575, train_loss=6.6818047

Batch 175980, train_perplexity=739.91156, train_loss=6.6065307

Batch 175990, train_perplexity=821.15546, train_loss=6.7107124

Batch 176000, train_perplexity=772.2278, train_loss=6.6492796

Batch 176010, train_perplexity=746.44867, train_loss=6.615327

Batch 176020, train_perplexity=803.8005, train_loss=6.689351

Batch 176030, train_perplexity=884.79376, train_loss=6.7853546

Batch 176040, train_perplexity=816.0436, train_loss=6.704468

Batch 176050, train_perplexity=844.54834, train_loss=6.738802

Batch 176060, train_perplexity=756.78186, train_loss=6.629075

Batch 176070, train_perplexity=837.95123, train_loss=6.73096

Batch 176080, train_perplexity=723.2156, train_loss=6.5837073

Batch 176090, train_perplexity=704.65796, train_loss=6.5577126

Batch 176100, train_perplexity=756.4445, train_loss=6.628629

Batch 176110, train_perplexity=759.91046, train_loss=6.6332006

Batch 176120, train_perplexity=728.1059, train_loss=6.5904465

Batch 176130, train_perplexity=785.58716, train_loss=6.6664314
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 176140, train_perplexity=746.3213, train_loss=6.615156

Batch 176150, train_perplexity=755.4813, train_loss=6.627355

Batch 176160, train_perplexity=867.7937, train_loss=6.765954

Batch 176170, train_perplexity=799.11743, train_loss=6.683508

Batch 176180, train_perplexity=777.2757, train_loss=6.655795

Batch 176190, train_perplexity=759.6409, train_loss=6.632846

Batch 176200, train_perplexity=716.9195, train_loss=6.5749636

Batch 176210, train_perplexity=786.84155, train_loss=6.668027

Batch 176220, train_perplexity=762.22437, train_loss=6.636241

Batch 176230, train_perplexity=759.2846, train_loss=6.6323767

Batch 176240, train_perplexity=785.2606, train_loss=6.6660156

Batch 176250, train_perplexity=699.7099, train_loss=6.550666

Batch 176260, train_perplexity=802.0416, train_loss=6.6871605

Batch 176270, train_perplexity=790.5689, train_loss=6.672753

Batch 176280, train_perplexity=795.68665, train_loss=6.6792054

Batch 176290, train_perplexity=745.8542, train_loss=6.61453

Batch 176300, train_perplexity=765.7289, train_loss=6.640828

Batch 176310, train_perplexity=815.39166, train_loss=6.7036686

Batch 176320, train_perplexity=723.3621, train_loss=6.58391

Batch 176330, train_perplexity=766.7493, train_loss=6.64216

Batch 176340, train_perplexity=690.8827, train_loss=6.53797

Batch 176350, train_perplexity=719.6455, train_loss=6.5787587

Batch 176360, train_perplexity=774.5818, train_loss=6.6523232

Batch 176370, train_perplexity=838.1662, train_loss=6.7312164

Batch 176380, train_perplexity=788.89355, train_loss=6.6706314

Batch 176390, train_perplexity=685.5455, train_loss=6.530215

Batch 176400, train_perplexity=781.3194, train_loss=6.660984

Batch 176410, train_perplexity=779.1214, train_loss=6.658167

Batch 176420, train_perplexity=823.17914, train_loss=6.713174

Batch 176430, train_perplexity=825.1221, train_loss=6.7155313

Batch 176440, train_perplexity=733.2939, train_loss=6.5975466

Batch 176450, train_perplexity=831.918, train_loss=6.723734

Batch 176460, train_perplexity=738.7225, train_loss=6.6049223

Batch 176470, train_perplexity=758.6704, train_loss=6.6315675

Batch 176480, train_perplexity=704.50275, train_loss=6.5574923

Batch 176490, train_perplexity=865.18616, train_loss=6.7629447

Batch 176500, train_perplexity=742.9527, train_loss=6.6106324

Batch 176510, train_perplexity=771.79785, train_loss=6.6487226

Batch 176520, train_perplexity=816.2771, train_loss=6.704754

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Batch 176530, train_perplexity=807.9668, train_loss=6.694521

Batch 176540, train_perplexity=805.42645, train_loss=6.691372

Batch 176550, train_perplexity=751.1128, train_loss=6.621556

Batch 176560, train_perplexity=741.4472, train_loss=6.608604

Batch 176570, train_perplexity=749.9922, train_loss=6.620063

Batch 176580, train_perplexity=715.7957, train_loss=6.573395

Batch 176590, train_perplexity=756.15857, train_loss=6.628251

Batch 176600, train_perplexity=829.0628, train_loss=6.720296

Batch 176610, train_perplexity=661.8153, train_loss=6.4949865

Batch 176620, train_perplexity=770.6806, train_loss=6.647274

Batch 176630, train_perplexity=770.03186, train_loss=6.646432

Batch 176640, train_perplexity=735.6677, train_loss=6.6007786

Batch 176650, train_perplexity=815.8164, train_loss=6.7041893

Batch 176660, train_perplexity=742.8617, train_loss=6.61051

Batch 176670, train_perplexity=836.26276, train_loss=6.728943

Batch 176680, train_perplexity=708.61884, train_loss=6.563318

Batch 176690, train_perplexity=774.4846, train_loss=6.652198

Batch 176700, train_perplexity=775.0654, train_loss=6.6529474

Batch 176710, train_perplexity=793.6726, train_loss=6.676671

Batch 176720, train_perplexity=856.8865, train_loss=6.7533054

Batch 176730, train_perplexity=766.95337, train_loss=6.642426

Batch 176740, train_perplexity=756.5159, train_loss=6.6287236

Batch 176750, train_perplexity=853.71185, train_loss=6.7495937

Batch 176760, train_perplexity=735.97614, train_loss=6.6011977

Batch 176770, train_perplexity=752.9556, train_loss=6.6240063

Batch 176780, train_perplexity=747.83276, train_loss=6.6171794

Batch 176790, train_perplexity=777.7888, train_loss=6.656455

Batch 176800, train_perplexity=691.4868, train_loss=6.538844

Batch 176810, train_perplexity=771.85675, train_loss=6.648799

Batch 176820, train_perplexity=712.01843, train_loss=6.568104

Batch 176830, train_perplexity=794.3306, train_loss=6.6775

Batch 176840, train_perplexity=849.70825, train_loss=6.744893

Batch 176850, train_perplexity=752.8917, train_loss=6.6239214

Batch 176860, train_perplexity=754.86957, train_loss=6.626545

Batch 176870, train_perplexity=813.20483, train_loss=6.700983

Batch 176880, train_perplexity=799.3762, train_loss=6.6838317

Batch 176890, train_perplexity=806.0489, train_loss=6.6921444

Batch 176900, train_perplexity=806.7218, train_loss=6.692979

Batch 176910, train_perplexity=836.2955, train_loss=6.728982

Batch 176920, train_perplexity=817.87994, train_loss=6.7067156

Batch 176930, train_perplexity=743.08844, train_loss=6.610815

Batch 176940, train_perplexity=799.1731, train_loss=6.6835775

Batch 176950, train_perplexity=689.5047, train_loss=6.5359735

Batch 176960, train_perplexity=787.6742, train_loss=6.6690845

Batch 176970, train_perplexity=815.75494, train_loss=6.704114

Batch 176980, train_perplexity=828.64545, train_loss=6.7197924

Batch 176990, train_perplexity=724.1711, train_loss=6.5850277

Batch 177000, train_perplexity=751.7961, train_loss=6.622465

Batch 177010, train_perplexity=794.6223, train_loss=6.677867

Batch 177020, train_perplexity=863.66064, train_loss=6.76118

Batch 177030, train_perplexity=767.2903, train_loss=6.642865

Batch 177040, train_perplexity=761.7266, train_loss=6.6355877

Batch 177050, train_perplexity=721.7211, train_loss=6.581639

Batch 177060, train_perplexity=798.2065, train_loss=6.6823673

Batch 177070, train_perplexity=762.9233, train_loss=6.6371574

Batch 177080, train_perplexity=844.533, train_loss=6.738784

Batch 177090, train_perplexity=788.7405, train_loss=6.6704373

Batch 177100, train_perplexity=842.01666, train_loss=6.7358

Batch 177110, train_perplexity=767.5223, train_loss=6.6431675

Batch 177120, train_perplexity=762.33417, train_loss=6.636385

Batch 177130, train_perplexity=750.9627, train_loss=6.621356

Batch 177140, train_perplexity=802.7544, train_loss=6.688049

Batch 177150, train_perplexity=723.7355, train_loss=6.584426

Batch 177160, train_perplexity=776.00397, train_loss=6.6541576

Batch 177170, train_perplexity=795.1215, train_loss=6.678495

Batch 177180, train_perplexity=810.114, train_loss=6.697175

Batch 177190, train_perplexity=815.2113, train_loss=6.7034473

Batch 177200, train_perplexity=803.6924, train_loss=6.6892166

Batch 177210, train_perplexity=746.30347, train_loss=6.6151323

Batch 177220, train_perplexity=853.21497, train_loss=6.7490115

Batch 177230, train_perplexity=801.62866, train_loss=6.6866455

Batch 177240, train_perplexity=772.72876, train_loss=6.649928

Batch 177250, train_perplexity=831.2701, train_loss=6.7229548

Batch 177260, train_perplexity=722.28296, train_loss=6.582417

Batch 177270, train_perplexity=719.4736, train_loss=6.57852

Batch 177280, train_perplexity=781.8218, train_loss=6.661627

Batch 177290, train_perplexity=691.9644, train_loss=6.5395346

Batch 177300, train_perplexity=827.65625, train_loss=6.718598

Batch 177310, train_perplexity=859.7289, train_loss=6.756617

Batch 177320, train_perplexity=786.8904, train_loss=6.668089

Batch 177330, train_perplexity=821.9374, train_loss=6.711664

Batch 177340, train_perplexity=797.42206, train_loss=6.681384

Batch 177350, train_perplexity=688.2237, train_loss=6.534114

Batch 177360, train_perplexity=790.08545, train_loss=6.672141

Batch 177370, train_perplexity=719.4818, train_loss=6.5785313

Batch 177380, train_perplexity=818.2794, train_loss=6.707204
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 177390, train_perplexity=762.0579, train_loss=6.6360226

Batch 177400, train_perplexity=849.50653, train_loss=6.7446556

Batch 177410, train_perplexity=726.9014, train_loss=6.588791

Batch 177420, train_perplexity=737.60315, train_loss=6.603406

Batch 177430, train_perplexity=734.55414, train_loss=6.5992637

Batch 177440, train_perplexity=728.0743, train_loss=6.590403

Batch 177450, train_perplexity=736.42303, train_loss=6.6018047

Batch 177460, train_perplexity=826.56335, train_loss=6.7172766

Batch 177470, train_perplexity=767.22003, train_loss=6.6427736

Batch 177480, train_perplexity=781.60785, train_loss=6.661353

Batch 177490, train_perplexity=791.41345, train_loss=6.6738205

Batch 177500, train_perplexity=745.2438, train_loss=6.6137114

Batch 177510, train_perplexity=778.7322, train_loss=6.657667

Batch 177520, train_perplexity=798.0505, train_loss=6.682172

Batch 177530, train_perplexity=660.2283, train_loss=6.4925857

Batch 177540, train_perplexity=718.8131, train_loss=6.5776014

Batch 177550, train_perplexity=784.9469, train_loss=6.665616

Batch 177560, train_perplexity=800.8913, train_loss=6.685725

Batch 177570, train_perplexity=745.67456, train_loss=6.6142893

Batch 177580, train_perplexity=796.1842, train_loss=6.6798306

Batch 177590, train_perplexity=748.0728, train_loss=6.6175003

Batch 177600, train_perplexity=752.96747, train_loss=6.624022

Batch 177610, train_perplexity=745.6959, train_loss=6.614318

Batch 177620, train_perplexity=797.1282, train_loss=6.6810155

Batch 177630, train_perplexity=736.14465, train_loss=6.6014266

Batch 177640, train_perplexity=778.8209, train_loss=6.657781

Batch 177650, train_perplexity=782.36255, train_loss=6.662318

Batch 177660, train_perplexity=712.7161, train_loss=6.569083

Batch 177670, train_perplexity=786.00385, train_loss=6.6669617

Batch 177680, train_perplexity=853.19135, train_loss=6.748984

Batch 177690, train_perplexity=815.1973, train_loss=6.70343

Batch 177700, train_perplexity=801.7969, train_loss=6.6868553

Batch 177710, train_perplexity=771.3574, train_loss=6.648152

Batch 177720, train_perplexity=754.99664, train_loss=6.6267133

Batch 177730, train_perplexity=763.28564, train_loss=6.6376324

Batch 177740, train_perplexity=736.7641, train_loss=6.6022677

Batch 177750, train_perplexity=822.57764, train_loss=6.712443

Batch 177760, train_perplexity=752.94196, train_loss=6.623988

Batch 177770, train_perplexity=849.89465, train_loss=6.7451124

Batch 177780, train_perplexity=738.47876, train_loss=6.6045923

Batch 177790, train_perplexity=908.8192, train_loss=6.812146

Batch 177800, train_perplexity=685.232, train_loss=6.5297575

Batch 177810, train_perplexity=850.796, train_loss=6.7461724

Batch 177820, train_perplexity=830.6587, train_loss=6.722219

Batch 177830, train_perplexity=728.292, train_loss=6.590702

Batch 177840, train_perplexity=762.7065, train_loss=6.6368732

Batch 177850, train_perplexity=810.59973, train_loss=6.6977744

Batch 177860, train_perplexity=800.4278, train_loss=6.6851463

Batch 177870, train_perplexity=727.5652, train_loss=6.5897036

Batch 177880, train_perplexity=793.39825, train_loss=6.6763253

Batch 177890, train_perplexity=776.14905, train_loss=6.6543446

Batch 177900, train_perplexity=752.7413, train_loss=6.6237216

Batch 177910, train_perplexity=794.2814, train_loss=6.677438

Batch 177920, train_perplexity=766.05536, train_loss=6.6412544

Batch 177930, train_perplexity=772.476, train_loss=6.649601

Batch 177940, train_perplexity=782.5312, train_loss=6.6625338

Batch 177950, train_perplexity=817.85187, train_loss=6.7066813

Batch 177960, train_perplexity=840.72, train_loss=6.7342587

Batch 177970, train_perplexity=682.95386, train_loss=6.5264273

Batch 177980, train_perplexity=742.82556, train_loss=6.610461

Batch 177990, train_perplexity=807.0392, train_loss=6.6933722

Batch 178000, train_perplexity=859.62476, train_loss=6.756496

Batch 178010, train_perplexity=816.18445, train_loss=6.7046404

Batch 178020, train_perplexity=831.3985, train_loss=6.7231092

Batch 178030, train_perplexity=811.55927, train_loss=6.6989574

Batch 178040, train_perplexity=782.72595, train_loss=6.6627827

Batch 178050, train_perplexity=740.32697, train_loss=6.607092

Batch 178060, train_perplexity=774.3602, train_loss=6.652037

Batch 178070, train_perplexity=732.07666, train_loss=6.5958853

Batch 178080, train_perplexity=807.0042, train_loss=6.693329

Batch 178090, train_perplexity=744.2718, train_loss=6.6124063

Batch 178100, train_perplexity=803.2108, train_loss=6.688617

Batch 178110, train_perplexity=765.891, train_loss=6.64104

Batch 178120, train_perplexity=769.6501, train_loss=6.645936

Batch 178130, train_perplexity=756.4088, train_loss=6.628582

Batch 178140, train_perplexity=782.944, train_loss=6.663061

Batch 178150, train_perplexity=760.0772, train_loss=6.63342

Batch 178160, train_perplexity=712.3967, train_loss=6.568635

Batch 178170, train_perplexity=762.2887, train_loss=6.6363254

Batch 178180, train_perplexity=813.0909, train_loss=6.700843

Batch 178190, train_perplexity=787.26605, train_loss=6.668566

Batch 178200, train_perplexity=755.37006, train_loss=6.6272078

Batch 178210, train_perplexity=777.8144, train_loss=6.656488

Batch 178220, train_perplexity=819.3621, train_loss=6.708526

Batch 178230, train_perplexity=765.79095, train_loss=6.640909

Batch 178240, train_perplexity=819.5032, train_loss=6.7086983

Batch 178250, train_perplexity=779.26184, train_loss=6.658347

Batch 178260, train_perplexity=719.14703, train_loss=6.578066

Batch 178270, train_perplexity=803.97144, train_loss=6.6895638

Batch 178280, train_perplexity=743.1462, train_loss=6.610893

Batch 178290, train_perplexity=785.12134, train_loss=6.6658382

Batch 178300, train_perplexity=740.8362, train_loss=6.6077795

Batch 178310, train_perplexity=777.18933, train_loss=6.655684

Batch 178320, train_perplexity=794.4397, train_loss=6.677637

Batch 178330, train_perplexity=759.95905, train_loss=6.6332645

Batch 178340, train_perplexity=743.40594, train_loss=6.6112423

Batch 178350, train_perplexity=734.9374, train_loss=6.5997853

Batch 178360, train_perplexity=743.6889, train_loss=6.611623

Batch 178370, train_perplexity=745.5878, train_loss=6.614173

Batch 178380, train_perplexity=744.70844, train_loss=6.612993

Batch 178390, train_perplexity=802.6886, train_loss=6.687967

Batch 178400, train_perplexity=813.49493, train_loss=6.7013397

Batch 178410, train_perplexity=812.35297, train_loss=6.699935

Batch 178420, train_perplexity=788.6904, train_loss=6.670374

Batch 178430, train_perplexity=737.13763, train_loss=6.6027746

Batch 178440, train_perplexity=749.7855, train_loss=6.619787

Batch 178450, train_perplexity=774.5881, train_loss=6.6523314

Batch 178460, train_perplexity=810.50195, train_loss=6.697654

Batch 178470, train_perplexity=794.8603, train_loss=6.6781664

Batch 178480, train_perplexity=813.1738, train_loss=6.700945

Batch 178490, train_perplexity=754.45715, train_loss=6.6259985

Batch 178500, train_perplexity=768.4154, train_loss=6.6443305

Batch 178510, train_perplexity=826.84406, train_loss=6.717616

Batch 178520, train_perplexity=899.67145, train_loss=6.8020296

Batch 178530, train_perplexity=803.5073, train_loss=6.6889863

Batch 178540, train_perplexity=714.6191, train_loss=6.5717497

Batch 178550, train_perplexity=717.73566, train_loss=6.5761013

Batch 178560, train_perplexity=674.00977, train_loss=6.5132446

Batch 178570, train_perplexity=765.22296, train_loss=6.640167

Batch 178580, train_perplexity=812.1315, train_loss=6.699662

Batch 178590, train_perplexity=887.5819, train_loss=6.788501

Batch 178600, train_perplexity=860.0327, train_loss=6.7569704

Batch 178610, train_perplexity=843.6894, train_loss=6.7377844

Batch 178620, train_perplexity=715.1874, train_loss=6.5725446

Batch 178630, train_perplexity=818.67163, train_loss=6.707683

Batch 178640, train_perplexity=765.9998, train_loss=6.641182

Batch 178650, train_perplexity=780.25275, train_loss=6.659618

Batch 178660, train_perplexity=822.795, train_loss=6.712707

Batch 178670, train_perplexity=781.16406, train_loss=6.660785

Batch 178680, train_perplexity=733.14813, train_loss=6.5973477

Batch 178690, train_perplexity=754.4323, train_loss=6.6259656
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 178700, train_perplexity=803.41156, train_loss=6.688867

Batch 178710, train_perplexity=754.2208, train_loss=6.625685

Batch 178720, train_perplexity=747.0383, train_loss=6.6161165

Batch 178730, train_perplexity=771.57336, train_loss=6.648432

Batch 178740, train_perplexity=750.35675, train_loss=6.6205487

Batch 178750, train_perplexity=744.0936, train_loss=6.612167

Batch 178760, train_perplexity=778.3409, train_loss=6.6571646

Batch 178770, train_perplexity=726.4087, train_loss=6.588113

Batch 178780, train_perplexity=719.9132, train_loss=6.5791306

Batch 178790, train_perplexity=822.96484, train_loss=6.7129135

Batch 178800, train_perplexity=785.2703, train_loss=6.666028

Batch 178810, train_perplexity=727.75116, train_loss=6.589959

Batch 178820, train_perplexity=806.7853, train_loss=6.6930575

Batch 178830, train_perplexity=758.3214, train_loss=6.6311073

Batch 178840, train_perplexity=778.2414, train_loss=6.657037

Batch 178850, train_perplexity=757.96246, train_loss=6.630634

Batch 178860, train_perplexity=760.1305, train_loss=6.63349

Batch 178870, train_perplexity=867.7606, train_loss=6.765916

Batch 178880, train_perplexity=755.43774, train_loss=6.6272974

Batch 178890, train_perplexity=766.23254, train_loss=6.6414857

Batch 178900, train_perplexity=814.35657, train_loss=6.7023983

Batch 178910, train_perplexity=761.22626, train_loss=6.6349306

Batch 178920, train_perplexity=777.146, train_loss=6.655628

Batch 178930, train_perplexity=763.4036, train_loss=6.637787

Batch 178940, train_perplexity=785.7809, train_loss=6.666678

Batch 178950, train_perplexity=707.52893, train_loss=6.5617785

Batch 178960, train_perplexity=709.48474, train_loss=6.564539

Batch 178970, train_perplexity=738.2696, train_loss=6.604309

Batch 178980, train_perplexity=674.8723, train_loss=6.5145235

Batch 178990, train_perplexity=786.8806, train_loss=6.6680765

Batch 179000, train_perplexity=804.9696, train_loss=6.6908045

Batch 179010, train_perplexity=699.75397, train_loss=6.550729

Batch 179020, train_perplexity=773.2761, train_loss=6.650636

Batch 179030, train_perplexity=768.8758, train_loss=6.6449294

Batch 179040, train_perplexity=666.48206, train_loss=6.502013

Batch 179050, train_perplexity=788.50543, train_loss=6.6701393

Batch 179060, train_perplexity=830.2627, train_loss=6.721742

Batch 179070, train_perplexity=751.23816, train_loss=6.6217227

Batch 179080, train_perplexity=755.70435, train_loss=6.6276503

Batch 179090, train_perplexity=798.4585, train_loss=6.682683

Batch 179100, train_perplexity=721.55426, train_loss=6.5814075

Batch 179110, train_perplexity=709.7669, train_loss=6.5649366

Batch 179120, train_perplexity=719.03186, train_loss=6.5779057

Batch 179130, train_perplexity=735.92035, train_loss=6.601122

Batch 179140, train_perplexity=758.2274, train_loss=6.6309834

Batch 179150, train_perplexity=768.6811, train_loss=6.644676

Batch 179160, train_perplexity=704.13873, train_loss=6.5569754

Batch 179170, train_perplexity=765.61273, train_loss=6.6406765

Batch 179180, train_perplexity=751.92444, train_loss=6.622636

Batch 179190, train_perplexity=695.1641, train_loss=6.544148

Batch 179200, train_perplexity=790.0794, train_loss=6.6721334

Batch 179210, train_perplexity=829.8681, train_loss=6.7212667

Batch 179220, train_perplexity=745.8645, train_loss=6.614544

Batch 179230, train_perplexity=751.566, train_loss=6.622159

Batch 179240, train_perplexity=761.492, train_loss=6.6352797

Batch 179250, train_perplexity=808.8935, train_loss=6.6956673

Batch 179260, train_perplexity=805.5244, train_loss=6.6914935

Batch 179270, train_perplexity=811.71643, train_loss=6.699151

Batch 179280, train_perplexity=881.7071, train_loss=6.78186

Batch 179290, train_perplexity=875.1898, train_loss=6.774441

Batch 179300, train_perplexity=856.4163, train_loss=6.7527566

Batch 179310, train_perplexity=739.7137, train_loss=6.606263

Batch 179320, train_perplexity=727.2433, train_loss=6.589261

Batch 179330, train_perplexity=810.5742, train_loss=6.697743

Batch 179340, train_perplexity=853.74603, train_loss=6.749634

Batch 179350, train_perplexity=859.1277, train_loss=6.7559175

Batch 179360, train_perplexity=717.34766, train_loss=6.5755606

Batch 179370, train_perplexity=713.0955, train_loss=6.5696154

Batch 179380, train_perplexity=847.701, train_loss=6.742528

Batch 179390, train_perplexity=724.98096, train_loss=6.5861454

Batch 179400, train_perplexity=775.1131, train_loss=6.653009

Batch 179410, train_perplexity=748.15485, train_loss=6.61761

Batch 179420, train_perplexity=682.56055, train_loss=6.5258512

Batch 179430, train_perplexity=740.57623, train_loss=6.6074286

Batch 179440, train_perplexity=780.8818, train_loss=6.6604238

Batch 179450, train_perplexity=765.7917, train_loss=6.64091

Batch 179460, train_perplexity=758.73303, train_loss=6.63165

Batch 179470, train_perplexity=858.1021, train_loss=6.754723

Batch 179480, train_perplexity=718.3876, train_loss=6.577009

Batch 179490, train_perplexity=690.3937, train_loss=6.537262

Batch 179500, train_perplexity=738.064, train_loss=6.6040306

Batch 179510, train_perplexity=781.40173, train_loss=6.6610894

Batch 179520, train_perplexity=855.82477, train_loss=6.7520657

Batch 179530, train_perplexity=748.3111, train_loss=6.617819

Batch 179540, train_perplexity=713.86847, train_loss=6.5706987

Batch 179550, train_perplexity=677.9461, train_loss=6.519068

Batch 179560, train_perplexity=715.61993, train_loss=6.573149

Batch 179570, train_perplexity=761.1112, train_loss=6.6347795

Batch 179580, train_perplexity=843.7397, train_loss=6.737844

Batch 179590, train_perplexity=859.39685, train_loss=6.756231

Batch 179600, train_perplexity=809.206, train_loss=6.6960535

Batch 179610, train_perplexity=750.56964, train_loss=6.6208324

Batch 179620, train_perplexity=680.7053, train_loss=6.5231295

Batch 179630, train_perplexity=746.5569, train_loss=6.615472

Batch 179640, train_perplexity=714.2246, train_loss=6.5711975

Batch 179650, train_perplexity=766.1551, train_loss=6.6413846

Batch 179660, train_perplexity=819.20276, train_loss=6.7083316

Batch 179670, train_perplexity=748.15344, train_loss=6.617608

Batch 179680, train_perplexity=750.56604, train_loss=6.6208277

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Batch 179690, train_perplexity=793.09906, train_loss=6.675948

Batch 179700, train_perplexity=791.01465, train_loss=6.6733165

Batch 179710, train_perplexity=780.9991, train_loss=6.660574

Batch 179720, train_perplexity=717.0918, train_loss=6.575204

Batch 179730, train_perplexity=753.0098, train_loss=6.6240783

Batch 179740, train_perplexity=748.3818, train_loss=6.6179132

Batch 179750, train_perplexity=832.94727, train_loss=6.7249703

Batch 179760, train_perplexity=784.2005, train_loss=6.6646647

Batch 179770, train_perplexity=775.6259, train_loss=6.6536703

Batch 179780, train_perplexity=709.861, train_loss=6.565069

Batch 179790, train_perplexity=749.15656, train_loss=6.618948

Batch 179800, train_perplexity=743.0955, train_loss=6.6108246

Batch 179810, train_perplexity=723.6333, train_loss=6.584285

Batch 179820, train_perplexity=737.7671, train_loss=6.603628

Batch 179830, train_perplexity=744.4489, train_loss=6.612644

Batch 179840, train_perplexity=781.8538, train_loss=6.661668

Batch 179850, train_perplexity=680.9699, train_loss=6.523518

Batch 179860, train_perplexity=779.12885, train_loss=6.6581764

Batch 179870, train_perplexity=782.282, train_loss=6.662215

Batch 179880, train_perplexity=815.24664, train_loss=6.7034907

Batch 179890, train_perplexity=733.46454, train_loss=6.5977793

Batch 179900, train_perplexity=817.42847, train_loss=6.7061634

Batch 179910, train_perplexity=769.3294, train_loss=6.6455193

Batch 179920, train_perplexity=762.1615, train_loss=6.6361585

Batch 179930, train_perplexity=830.27576, train_loss=6.721758

Batch 179940, train_perplexity=777.65234, train_loss=6.6562796
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 179950, train_perplexity=737.0041, train_loss=6.6025934

Batch 179960, train_perplexity=710.93823, train_loss=6.5665855

Batch 179970, train_perplexity=788.8883, train_loss=6.6706247

Batch 179980, train_perplexity=760.9918, train_loss=6.6346226

Batch 179990, train_perplexity=778.80975, train_loss=6.657767

Batch 180000, train_perplexity=740.75635, train_loss=6.6076717

Batch 180010, train_perplexity=748.0575, train_loss=6.61748

Batch 180020, train_perplexity=847.23224, train_loss=6.741975

Batch 180030, train_perplexity=734.5916, train_loss=6.5993147

Batch 180040, train_perplexity=729.97125, train_loss=6.593005

Batch 180050, train_perplexity=741.8535, train_loss=6.609152

Batch 180060, train_perplexity=752.30817, train_loss=6.623146

Batch 180070, train_perplexity=738.8267, train_loss=6.6050634

Batch 180080, train_perplexity=756.98865, train_loss=6.6293483

Batch 180090, train_perplexity=746.37714, train_loss=6.615231

Batch 180100, train_perplexity=677.6355, train_loss=6.5186095

Batch 180110, train_perplexity=794.15564, train_loss=6.6772795

Batch 180120, train_perplexity=723.9374, train_loss=6.584705

Batch 180130, train_perplexity=795.70294, train_loss=6.679226

Batch 180140, train_perplexity=767.3839, train_loss=6.6429873

Batch 180150, train_perplexity=841.23126, train_loss=6.7348666

Batch 180160, train_perplexity=742.1514, train_loss=6.6095533

Batch 180170, train_perplexity=775.5719, train_loss=6.6536007

Batch 180180, train_perplexity=812.6924, train_loss=6.7003527

Batch 180190, train_perplexity=804.4938, train_loss=6.690213

Batch 180200, train_perplexity=819.7951, train_loss=6.7090545

Batch 180210, train_perplexity=806.20416, train_loss=6.692337

Batch 180220, train_perplexity=769.8733, train_loss=6.646226

Batch 180230, train_perplexity=813.38556, train_loss=6.7012053

Batch 180240, train_perplexity=797.64075, train_loss=6.6816583

Batch 180250, train_perplexity=728.2725, train_loss=6.5906754

Batch 180260, train_perplexity=744.4024, train_loss=6.6125817

Batch 180270, train_perplexity=789.40454, train_loss=6.671279

Batch 180280, train_perplexity=701.52936, train_loss=6.5532627

Batch 180290, train_perplexity=725.6758, train_loss=6.5871034

Batch 180300, train_perplexity=769.08, train_loss=6.645195

Batch 180310, train_perplexity=800.6904, train_loss=6.6854744

Batch 180320, train_perplexity=792.2603, train_loss=6.67489

Batch 180330, train_perplexity=755.93933, train_loss=6.627961

Batch 180340, train_perplexity=774.9169, train_loss=6.6527557

Batch 180350, train_perplexity=826.39233, train_loss=6.7170696

Batch 180360, train_perplexity=760.19354, train_loss=6.633573

Batch 180370, train_perplexity=695.2576, train_loss=6.5442824

Batch 180380, train_perplexity=750.546, train_loss=6.620801

Batch 180390, train_perplexity=792.97656, train_loss=6.6757936

Batch 180400, train_perplexity=783.1381, train_loss=6.663309

Batch 180410, train_perplexity=749.01404, train_loss=6.6187577

Batch 180420, train_perplexity=701.8847, train_loss=6.553769

Batch 180430, train_perplexity=698.697, train_loss=6.549217

Batch 180440, train_perplexity=793.1554, train_loss=6.676019

Batch 180450, train_perplexity=865.86426, train_loss=6.763728

Batch 180460, train_perplexity=668.4668, train_loss=6.504987

Batch 180470, train_perplexity=714.2941, train_loss=6.571295

Batch 180480, train_perplexity=764.2388, train_loss=6.6388803

Batch 180490, train_perplexity=714.6334, train_loss=6.5717697

Batch 180500, train_perplexity=731.42523, train_loss=6.594995

Batch 180510, train_perplexity=730.66705, train_loss=6.593958

Batch 180520, train_perplexity=867.3192, train_loss=6.765407

Batch 180530, train_perplexity=734.35486, train_loss=6.5989923

Batch 180540, train_perplexity=737.7843, train_loss=6.6036515

Batch 180550, train_perplexity=767.1813, train_loss=6.642723

Batch 180560, train_perplexity=738.3217, train_loss=6.6043797

Batch 180570, train_perplexity=851.50183, train_loss=6.7470016

Batch 180580, train_perplexity=800.80725, train_loss=6.6856203

Batch 180590, train_perplexity=804.69745, train_loss=6.6904664

Batch 180600, train_perplexity=693.19165, train_loss=6.5413065

Batch 180610, train_perplexity=821.25806, train_loss=6.7108374

Batch 180620, train_perplexity=715.3392, train_loss=6.572757

Batch 180630, train_perplexity=805.1289, train_loss=6.6910024

Batch 180640, train_perplexity=795.0521, train_loss=6.6784077

Batch 180650, train_perplexity=749.96216, train_loss=6.620023

Batch 180660, train_perplexity=754.1917, train_loss=6.6256466

Batch 180670, train_perplexity=818.92896, train_loss=6.7079973

Batch 180680, train_perplexity=750.1002, train_loss=6.620207

Batch 180690, train_perplexity=723.1242, train_loss=6.583581

Batch 180700, train_perplexity=803.56134, train_loss=6.6890535

Batch 180710, train_perplexity=773.82385, train_loss=6.6513443

Batch 180720, train_perplexity=729.3311, train_loss=6.592128

Batch 180730, train_perplexity=709.34467, train_loss=6.5643415

Batch 180740, train_perplexity=755.32465, train_loss=6.6271477

Batch 180750, train_perplexity=802.45245, train_loss=6.6876726

Batch 180760, train_perplexity=723.77893, train_loss=6.584486

Batch 180770, train_perplexity=709.0795, train_loss=6.5639677

Batch 180780, train_perplexity=734.3934, train_loss=6.599045

Batch 180790, train_perplexity=761.7847, train_loss=6.635664

Batch 180800, train_perplexity=754.961, train_loss=6.626666

Batch 180810, train_perplexity=792.93494, train_loss=6.675741

Batch 180820, train_perplexity=798.26776, train_loss=6.682444

Batch 180830, train_perplexity=825.52545, train_loss=6.71602

Batch 180840, train_perplexity=825.29364, train_loss=6.7157393

Batch 180850, train_perplexity=816.13696, train_loss=6.704582

Batch 180860, train_perplexity=778.26666, train_loss=6.657069

Batch 180870, train_perplexity=838.16864, train_loss=6.7312193

Batch 180880, train_perplexity=785.2707, train_loss=6.6660285

Batch 180890, train_perplexity=843.71313, train_loss=6.7378125

Batch 180900, train_perplexity=781.23114, train_loss=6.660871

Batch 180910, train_perplexity=674.1358, train_loss=6.5134315

Batch 180920, train_perplexity=714.80994, train_loss=6.5720167

Batch 180930, train_perplexity=870.4448, train_loss=6.7690043

Batch 180940, train_perplexity=792.7505, train_loss=6.6755085

Batch 180950, train_perplexity=786.6405, train_loss=6.6677713

Batch 180960, train_perplexity=794.89026, train_loss=6.678204

Batch 180970, train_perplexity=715.55066, train_loss=6.5730524

Batch 180980, train_perplexity=747.9908, train_loss=6.6173906

Batch 180990, train_perplexity=760.38605, train_loss=6.6338263

Batch 181000, train_perplexity=744.37396, train_loss=6.6125436

Batch 181010, train_perplexity=770.6784, train_loss=6.647271

Batch 181020, train_perplexity=781.8904, train_loss=6.6617146

Batch 181030, train_perplexity=787.8594, train_loss=6.6693196

Batch 181040, train_perplexity=729.19336, train_loss=6.591939

Batch 181050, train_perplexity=726.10364, train_loss=6.5876927

Batch 181060, train_perplexity=835.29395, train_loss=6.7277837

Batch 181070, train_perplexity=773.53394, train_loss=6.6509695

Batch 181080, train_perplexity=767.4641, train_loss=6.6430917

Batch 181090, train_perplexity=732.9122, train_loss=6.597026

Batch 181100, train_perplexity=851.1527, train_loss=6.7465916

Batch 181110, train_perplexity=787.40155, train_loss=6.6687384

Batch 181120, train_perplexity=739.61066, train_loss=6.606124

Batch 181130, train_perplexity=813.8255, train_loss=6.701746

Batch 181140, train_perplexity=803.3108, train_loss=6.6887417

Batch 181150, train_perplexity=837.73987, train_loss=6.7307076

Batch 181160, train_perplexity=750.29266, train_loss=6.6204634

Batch 181170, train_perplexity=810.4011, train_loss=6.6975293

Batch 181180, train_perplexity=744.2665, train_loss=6.612399

Batch 181190, train_perplexity=794.9729, train_loss=6.678308

Batch 181200, train_perplexity=732.33997, train_loss=6.596245

Batch 181210, train_perplexity=721.06616, train_loss=6.580731

Batch 181220, train_perplexity=797.9218, train_loss=6.6820107

Batch 181230, train_perplexity=727.73724, train_loss=6.58994

Batch 181240, train_perplexity=771.5278, train_loss=6.6483727

Batch 181250, train_perplexity=740.81885, train_loss=6.607756
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 181260, train_perplexity=842.5621, train_loss=6.7364473

Batch 181270, train_perplexity=680.08594, train_loss=6.522219

Batch 181280, train_perplexity=803.6966, train_loss=6.689222

Batch 181290, train_perplexity=769.5503, train_loss=6.6458063

Batch 181300, train_perplexity=763.99426, train_loss=6.6385603

Batch 181310, train_perplexity=781.0751, train_loss=6.660671

Batch 181320, train_perplexity=751.28, train_loss=6.6217785

Batch 181330, train_perplexity=836.063, train_loss=6.728704

Batch 181340, train_perplexity=789.1103, train_loss=6.670906

Batch 181350, train_perplexity=703.38837, train_loss=6.555909

Batch 181360, train_perplexity=789.6662, train_loss=6.6716104

Batch 181370, train_perplexity=713.0503, train_loss=6.569552

Batch 181380, train_perplexity=804.84064, train_loss=6.6906443

Batch 181390, train_perplexity=681.9782, train_loss=6.5249977

Batch 181400, train_perplexity=821.0975, train_loss=6.710642

Batch 181410, train_perplexity=767.9374, train_loss=6.643708

Batch 181420, train_perplexity=779.7566, train_loss=6.658982

Batch 181430, train_perplexity=716.01044, train_loss=6.5736947

Batch 181440, train_perplexity=729.33527, train_loss=6.5921335

Batch 181450, train_perplexity=751.7861, train_loss=6.622452

Batch 181460, train_perplexity=737.91345, train_loss=6.6038265

Batch 181470, train_perplexity=764.5307, train_loss=6.639262

Batch 181480, train_perplexity=803.25336, train_loss=6.68867

Batch 181490, train_perplexity=700.81116, train_loss=6.5522385

Batch 181500, train_perplexity=772.8566, train_loss=6.6500936

Batch 181510, train_perplexity=740.06396, train_loss=6.6067367

Batch 181520, train_perplexity=792.3106, train_loss=6.6749535

Batch 181530, train_perplexity=799.337, train_loss=6.6837826

Batch 181540, train_perplexity=646.6336, train_loss=6.47178

Batch 181550, train_perplexity=857.80096, train_loss=6.754372

Batch 181560, train_perplexity=706.9496, train_loss=6.5609593

Batch 181570, train_perplexity=835.4465, train_loss=6.7279663

Batch 181580, train_perplexity=754.4795, train_loss=6.626028

Batch 181590, train_perplexity=707.44696, train_loss=6.5616627

Batch 181600, train_perplexity=712.1685, train_loss=6.5683146

Batch 181610, train_perplexity=736.304, train_loss=6.601643

Batch 181620, train_perplexity=763.6679, train_loss=6.638133

Batch 181630, train_perplexity=756.52423, train_loss=6.6287346

Batch 181640, train_perplexity=665.8506, train_loss=6.5010653

Batch 181650, train_perplexity=693.29315, train_loss=6.541453

Batch 181660, train_perplexity=737.3166, train_loss=6.6030173

Batch 181670, train_perplexity=814.6498, train_loss=6.7027583

Batch 181680, train_perplexity=750.25476, train_loss=6.620413

Batch 181690, train_perplexity=833.22894, train_loss=6.7253084

Batch 181700, train_perplexity=773.30524, train_loss=6.650674

Batch 181710, train_perplexity=763.1896, train_loss=6.6375065

Batch 181720, train_perplexity=736.1411, train_loss=6.601422

Batch 181730, train_perplexity=758.6039, train_loss=6.6314797

Batch 181740, train_perplexity=726.9032, train_loss=6.5887933

Batch 181750, train_perplexity=790.5776, train_loss=6.672764

Batch 181760, train_perplexity=729.5363, train_loss=6.592409

Batch 181770, train_perplexity=713.086, train_loss=6.569602

Batch 181780, train_perplexity=754.21906, train_loss=6.625683

Batch 181790, train_perplexity=693.3186, train_loss=6.5414896

Batch 181800, train_perplexity=848.0552, train_loss=6.7429457

Batch 181810, train_perplexity=747.21936, train_loss=6.6163588

Batch 181820, train_perplexity=819.1406, train_loss=6.708256

Batch 181830, train_perplexity=723.73474, train_loss=6.584425

Batch 181840, train_perplexity=705.49646, train_loss=6.558902

Batch 181850, train_perplexity=736.4385, train_loss=6.6018257

Batch 181860, train_perplexity=706.74396, train_loss=6.5606685

Batch 181870, train_perplexity=749.1116, train_loss=6.618888

Batch 181880, train_perplexity=805.6823, train_loss=6.6916895

Batch 181890, train_perplexity=748.26154, train_loss=6.6177526

Batch 181900, train_perplexity=728.98755, train_loss=6.5916567

Batch 181910, train_perplexity=770.98566, train_loss=6.64767

Batch 181920, train_perplexity=786.693, train_loss=6.667838

Batch 181930, train_perplexity=762.9684, train_loss=6.6372166

Batch 181940, train_perplexity=733.096, train_loss=6.5972767

Batch 181950, train_perplexity=713.9366, train_loss=6.570794

Batch 181960, train_perplexity=782.6931, train_loss=6.6627407

Batch 181970, train_perplexity=796.7888, train_loss=6.6805897

Batch 181980, train_perplexity=753.8411, train_loss=6.6251817

Batch 181990, train_perplexity=753.7143, train_loss=6.6250134

Batch 182000, train_perplexity=866.3987, train_loss=6.764345

Batch 182010, train_perplexity=758.7616, train_loss=6.6316876

Batch 182020, train_perplexity=678.5308, train_loss=6.51993

Batch 182030, train_perplexity=738.36786, train_loss=6.604442

Batch 182040, train_perplexity=739.2137, train_loss=6.605587

Batch 182050, train_perplexity=773.5051, train_loss=6.6509323

Batch 182060, train_perplexity=787.83124, train_loss=6.669284

Batch 182070, train_perplexity=734.1784, train_loss=6.598752

Batch 182080, train_perplexity=798.2347, train_loss=6.6824026

Batch 182090, train_perplexity=791.9895, train_loss=6.674548

Batch 182100, train_perplexity=748.5352, train_loss=6.6181183

Batch 182110, train_perplexity=743.01544, train_loss=6.610717

Batch 182120, train_perplexity=734.0674, train_loss=6.598601

Batch 182130, train_perplexity=835.90314, train_loss=6.728513

Batch 182140, train_perplexity=721.5102, train_loss=6.5813465

Batch 182150, train_perplexity=730.8859, train_loss=6.5942574

Batch 182160, train_perplexity=694.177, train_loss=6.542727

Batch 182170, train_perplexity=690.339, train_loss=6.537183

Batch 182180, train_perplexity=697.0465, train_loss=6.546852

Batch 182190, train_perplexity=752.73303, train_loss=6.6237106

Batch 182200, train_perplexity=830.33514, train_loss=6.7218294

Batch 182210, train_perplexity=772.92334, train_loss=6.65018

Batch 182220, train_perplexity=677.91376, train_loss=6.51902

Batch 182230, train_perplexity=738.4284, train_loss=6.604524

Batch 182240, train_perplexity=741.3397, train_loss=6.608459

Batch 182250, train_perplexity=834.23114, train_loss=6.7265105

Batch 182260, train_perplexity=704.4087, train_loss=6.5573587

Batch 182270, train_perplexity=840.4587, train_loss=6.7339478

Batch 182280, train_perplexity=750.9097, train_loss=6.6212854

Batch 182290, train_perplexity=733.92426, train_loss=6.598406

Batch 182300, train_perplexity=754.89905, train_loss=6.626584

Batch 182310, train_perplexity=741.80615, train_loss=6.609088

Batch 182320, train_perplexity=713.79974, train_loss=6.5706024

Batch 182330, train_perplexity=795.0923, train_loss=6.678458

Batch 182340, train_perplexity=781.1421, train_loss=6.660757

Batch 182350, train_perplexity=774.9113, train_loss=6.6527486

Batch 182360, train_perplexity=730.03217, train_loss=6.5930886

Batch 182370, train_perplexity=796.7881, train_loss=6.6805887

Batch 182380, train_perplexity=727.52734, train_loss=6.5896516

Batch 182390, train_perplexity=810.1596, train_loss=6.6972313

Batch 182400, train_perplexity=761.2248, train_loss=6.6349287

Batch 182410, train_perplexity=770.4734, train_loss=6.647005

Batch 182420, train_perplexity=798.9917, train_loss=6.6833506

Batch 182430, train_perplexity=732.6634, train_loss=6.5966864

Batch 182440, train_perplexity=696.9053, train_loss=6.5466495

Batch 182450, train_perplexity=795.05176, train_loss=6.678407

Batch 182460, train_perplexity=778.916, train_loss=6.657903

Batch 182470, train_perplexity=823.963, train_loss=6.7141256

Batch 182480, train_perplexity=713.0588, train_loss=6.569564

Batch 182490, train_perplexity=769.1167, train_loss=6.6452427

Batch 182500, train_perplexity=781.4491, train_loss=6.66115

Batch 182510, train_perplexity=693.1093, train_loss=6.541188

Batch 182520, train_perplexity=765.5912, train_loss=6.6406484

Batch 182530, train_perplexity=746.34973, train_loss=6.6151943

Batch 182540, train_perplexity=724.6049, train_loss=6.5856266

Batch 182550, train_perplexity=796.7041, train_loss=6.6804833

Batch 182560, train_perplexity=763.54193, train_loss=6.637968
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 182570, train_perplexity=764.2756, train_loss=6.6389284

Batch 182580, train_perplexity=747.116, train_loss=6.6162205

Batch 182590, train_perplexity=804.87286, train_loss=6.6906843

Batch 182600, train_perplexity=727.9472, train_loss=6.5902286

Batch 182610, train_perplexity=811.3484, train_loss=6.6986976

Batch 182620, train_perplexity=766.13135, train_loss=6.6413536

Batch 182630, train_perplexity=682.8613, train_loss=6.526292

Batch 182640, train_perplexity=765.72595, train_loss=6.6408243

Batch 182650, train_perplexity=790.53766, train_loss=6.6727133

Batch 182660, train_perplexity=710.63794, train_loss=6.566163

Batch 182670, train_perplexity=729.5589, train_loss=6.59244

Batch 182680, train_perplexity=731.74585, train_loss=6.595433

Batch 182690, train_perplexity=769.3573, train_loss=6.6455555

Batch 182700, train_perplexity=732.8332, train_loss=6.596918

Batch 182710, train_perplexity=844.7658, train_loss=6.7390594

Batch 182720, train_perplexity=780.8807, train_loss=6.6604223

Batch 182730, train_perplexity=726.37683, train_loss=6.588069

Batch 182740, train_perplexity=839.9218, train_loss=6.733309

Batch 182750, train_perplexity=764.86835, train_loss=6.6397038

Batch 182760, train_perplexity=737.0958, train_loss=6.602718

Batch 182770, train_perplexity=843.8226, train_loss=6.737942

Batch 182780, train_perplexity=730.56885, train_loss=6.5938234

Batch 182790, train_perplexity=874.8126, train_loss=6.7740097

Batch 182800, train_perplexity=693.18567, train_loss=6.541298

Batch 182810, train_perplexity=748.48315, train_loss=6.6180487

Batch 182820, train_perplexity=832.873, train_loss=6.724881

Batch 182830, train_perplexity=814.9521, train_loss=6.7031293

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Batch 182840, train_perplexity=721.8426, train_loss=6.581807

Batch 182850, train_perplexity=791.6104, train_loss=6.6740694

Batch 182860, train_perplexity=740.904, train_loss=6.607871

Batch 182870, train_perplexity=725.82495, train_loss=6.587309

Batch 182880, train_perplexity=771.5336, train_loss=6.6483803

Batch 182890, train_perplexity=797.51636, train_loss=6.6815023

Batch 182900, train_perplexity=714.2185, train_loss=6.571189

Batch 182910, train_perplexity=697.9751, train_loss=6.5481834

Batch 182920, train_perplexity=767.8426, train_loss=6.6435847

Batch 182930, train_perplexity=731.8812, train_loss=6.5956182

Batch 182940, train_perplexity=791.3387, train_loss=6.673726

Batch 182950, train_perplexity=634.8769, train_loss=6.453431

Batch 182960, train_perplexity=804.52905, train_loss=6.690257

Batch 182970, train_perplexity=753.2357, train_loss=6.624378

Batch 182980, train_perplexity=791.9178, train_loss=6.6744576

Batch 182990, train_perplexity=756.09076, train_loss=6.6281614

Batch 183000, train_perplexity=766.0612, train_loss=6.641262

Batch 183010, train_perplexity=780.81366, train_loss=6.6603365

Batch 183020, train_perplexity=697.4847, train_loss=6.5474806

Batch 183030, train_perplexity=739.01733, train_loss=6.6053214

Batch 183040, train_perplexity=796.66876, train_loss=6.680439

Batch 183050, train_perplexity=809.3418, train_loss=6.6962214

Batch 183060, train_perplexity=703.97687, train_loss=6.5567455

Batch 183070, train_perplexity=830.806, train_loss=6.7223964

Batch 183080, train_perplexity=730.26965, train_loss=6.593414

Batch 183090, train_perplexity=755.21375, train_loss=6.627001

Batch 183100, train_perplexity=724.69965, train_loss=6.5857573

Batch 183110, train_perplexity=751.8255, train_loss=6.622504

Batch 183120, train_perplexity=730.2498, train_loss=6.5933867

Batch 183130, train_perplexity=795.0165, train_loss=6.678363

Batch 183140, train_perplexity=692.4381, train_loss=6.540219

Batch 183150, train_perplexity=703.17236, train_loss=6.555602

Batch 183160, train_perplexity=774.17377, train_loss=6.6517963

Batch 183170, train_perplexity=905.8996, train_loss=6.8089285

Batch 183180, train_perplexity=838.9403, train_loss=6.7321396

Batch 183190, train_perplexity=813.5516, train_loss=6.7014093

Batch 183200, train_perplexity=729.918, train_loss=6.592932

Batch 183210, train_perplexity=759.4729, train_loss=6.6326246

Batch 183220, train_perplexity=788.6777, train_loss=6.6703577

Batch 183230, train_perplexity=827.5982, train_loss=6.718528

Batch 183240, train_perplexity=769.562, train_loss=6.6458216

Batch 183250, train_perplexity=738.4925, train_loss=6.604611

Batch 183260, train_perplexity=745.7592, train_loss=6.614403

Batch 183270, train_perplexity=797.62665, train_loss=6.6816406

Batch 183280, train_perplexity=713.95764, train_loss=6.5708237

Batch 183290, train_perplexity=787.4451, train_loss=6.6687937

Batch 183300, train_perplexity=753.96515, train_loss=6.625346

Batch 183310, train_perplexity=786.8926, train_loss=6.668092

Batch 183320, train_perplexity=799.8658, train_loss=6.684444

Batch 183330, train_perplexity=726.7288, train_loss=6.5885534

Batch 183340, train_perplexity=714.88495, train_loss=6.5721216

Batch 183350, train_perplexity=758.12616, train_loss=6.63085

Batch 183360, train_perplexity=779.4001, train_loss=6.6585245

Batch 183370, train_perplexity=782.4454, train_loss=6.662424

Batch 183380, train_perplexity=816.15137, train_loss=6.7046

Batch 183390, train_perplexity=769.0712, train_loss=6.6451836

Batch 183400, train_perplexity=779.6491, train_loss=6.658844

Batch 183410, train_perplexity=758.2628, train_loss=6.63103

Batch 183420, train_perplexity=795.94275, train_loss=6.6795273

Batch 183430, train_perplexity=820.5417, train_loss=6.7099648

Batch 183440, train_perplexity=761.9689, train_loss=6.6359057

Batch 183450, train_perplexity=774.71106, train_loss=6.65249

Batch 183460, train_perplexity=701.1019, train_loss=6.5526533

Batch 183470, train_perplexity=763.071, train_loss=6.637351

Batch 183480, train_perplexity=706.5866, train_loss=6.560446

Batch 183490, train_perplexity=793.3907, train_loss=6.676316

Batch 183500, train_perplexity=822.78595, train_loss=6.712696

Batch 183510, train_perplexity=790.7552, train_loss=6.6729884

Batch 183520, train_perplexity=691.88293, train_loss=6.539417

Batch 183530, train_perplexity=649.18024, train_loss=6.4757104

Batch 183540, train_perplexity=736.9844, train_loss=6.6025667

Batch 183550, train_perplexity=802.95654, train_loss=6.6883006

Batch 183560, train_perplexity=656.44775, train_loss=6.486843

Batch 183570, train_perplexity=785.3452, train_loss=6.6661234

Batch 183580, train_perplexity=737.6035, train_loss=6.6034064

Batch 183590, train_perplexity=780.30524, train_loss=6.659685

Batch 183600, train_perplexity=748.28827, train_loss=6.6177883

Batch 183610, train_perplexity=794.97784, train_loss=6.678314

Batch 183620, train_perplexity=733.2572, train_loss=6.5974965

Batch 183630, train_perplexity=756.3908, train_loss=6.628558

Batch 183640, train_perplexity=797.5407, train_loss=6.681533

Batch 183650, train_perplexity=758.11536, train_loss=6.6308355

Batch 183660, train_perplexity=737.0083, train_loss=6.602599

Batch 183670, train_perplexity=823.12067, train_loss=6.713103

Batch 183680, train_perplexity=762.6905, train_loss=6.6368523

Batch 183690, train_perplexity=755.7195, train_loss=6.6276703

Batch 183700, train_perplexity=805.7069, train_loss=6.69172

Batch 183710, train_perplexity=753.67114, train_loss=6.624956

Batch 183720, train_perplexity=797.1312, train_loss=6.6810193

Batch 183730, train_perplexity=657.007, train_loss=6.4876947

Batch 183740, train_perplexity=688.6701, train_loss=6.5347624

Batch 183750, train_perplexity=765.50104, train_loss=6.6405306

Batch 183760, train_perplexity=738.302, train_loss=6.604353

Batch 183770, train_perplexity=784.2181, train_loss=6.664687

Batch 183780, train_perplexity=841.9496, train_loss=6.73572

Batch 183790, train_perplexity=699.92346, train_loss=6.550971

Batch 183800, train_perplexity=773.47675, train_loss=6.6508956

Batch 183810, train_perplexity=760.14825, train_loss=6.6335135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 183820, train_perplexity=684.20746, train_loss=6.528261

Batch 183830, train_perplexity=844.1119, train_loss=6.738285

Batch 183840, train_perplexity=765.1033, train_loss=6.640011

Batch 183850, train_perplexity=846.5885, train_loss=6.7412148

Batch 183860, train_perplexity=725.04767, train_loss=6.5862374

Batch 183870, train_perplexity=780.03326, train_loss=6.6593366

Batch 183880, train_perplexity=734.34186, train_loss=6.5989747

Batch 183890, train_perplexity=742.56244, train_loss=6.610107

Batch 183900, train_perplexity=803.5023, train_loss=6.68898

Batch 183910, train_perplexity=735.8179, train_loss=6.6009827

Batch 183920, train_perplexity=721.8282, train_loss=6.581787

Batch 183930, train_perplexity=730.34344, train_loss=6.593515

Batch 183940, train_perplexity=782.94434, train_loss=6.6630616

Batch 183950, train_perplexity=793.1354, train_loss=6.675994

Batch 183960, train_perplexity=671.2316, train_loss=6.5091143

Batch 183970, train_perplexity=744.0422, train_loss=6.6120977

Batch 183980, train_perplexity=829.1996, train_loss=6.720461

Batch 183990, train_perplexity=765.3025, train_loss=6.640271

Batch 184000, train_perplexity=709.6931, train_loss=6.5648327

Batch 184010, train_perplexity=731.4936, train_loss=6.5950885

Batch 184020, train_perplexity=763.0542, train_loss=6.637329

Batch 184030, train_perplexity=605.20264, train_loss=6.4055634

Batch 184040, train_perplexity=869.7084, train_loss=6.768158

Batch 184050, train_perplexity=784.8406, train_loss=6.6654806

Batch 184060, train_perplexity=749.15155, train_loss=6.6189413

Batch 184070, train_perplexity=814.4855, train_loss=6.7025566

Batch 184080, train_perplexity=769.4582, train_loss=6.6456866

Batch 184090, train_perplexity=766.189, train_loss=6.641429

Batch 184100, train_perplexity=718.1745, train_loss=6.5767126

Batch 184110, train_perplexity=779.6711, train_loss=6.658872

Batch 184120, train_perplexity=829.476, train_loss=6.720794

Batch 184130, train_perplexity=728.712, train_loss=6.5912786

Batch 184140, train_perplexity=739.97894, train_loss=6.6066217

Batch 184150, train_perplexity=757.46783, train_loss=6.629981

Batch 184160, train_perplexity=731.381, train_loss=6.5949345

Batch 184170, train_perplexity=787.78687, train_loss=6.6692276

Batch 184180, train_perplexity=777.28015, train_loss=6.655801

Batch 184190, train_perplexity=756.6426, train_loss=6.628891

Batch 184200, train_perplexity=748.2059, train_loss=6.617678

Batch 184210, train_perplexity=773.75635, train_loss=6.651257

Batch 184220, train_perplexity=752.72943, train_loss=6.623706

Batch 184230, train_perplexity=804.1478, train_loss=6.689783

Batch 184240, train_perplexity=796.665, train_loss=6.680434

Batch 184250, train_perplexity=723.0242, train_loss=6.5834427

Batch 184260, train_perplexity=735.3668, train_loss=6.6003695

Batch 184270, train_perplexity=781.49603, train_loss=6.66121

Batch 184280, train_perplexity=764.40314, train_loss=6.6390953

Batch 184290, train_perplexity=792.7686, train_loss=6.6755314

Batch 184300, train_perplexity=769.0998, train_loss=6.6452208

Batch 184310, train_perplexity=696.92786, train_loss=6.546682

Batch 184320, train_perplexity=751.26575, train_loss=6.6217594

Batch 184330, train_perplexity=750.61115, train_loss=6.6208878

Batch 184340, train_perplexity=793.59656, train_loss=6.676575

Batch 184350, train_perplexity=743.0906, train_loss=6.610818

Batch 184360, train_perplexity=755.5606, train_loss=6.62746

Batch 184370, train_perplexity=660.60583, train_loss=6.4931574

Batch 184380, train_perplexity=863.9226, train_loss=6.761483

Batch 184390, train_perplexity=794.7542, train_loss=6.678033

Batch 184400, train_perplexity=768.56274, train_loss=6.644522

Batch 184410, train_perplexity=797.38324, train_loss=6.6813354

Batch 184420, train_perplexity=694.1946, train_loss=6.5427523

Batch 184430, train_perplexity=716.9585, train_loss=6.575018

Batch 184440, train_perplexity=727.62134, train_loss=6.589781

Batch 184450, train_perplexity=744.96094, train_loss=6.613332

Batch 184460, train_perplexity=824.6021, train_loss=6.714901

Batch 184470, train_perplexity=811.4099, train_loss=6.6987734

Batch 184480, train_perplexity=768.643, train_loss=6.6446266

Batch 184490, train_perplexity=810.6388, train_loss=6.6978226

Batch 184500, train_perplexity=800.9921, train_loss=6.685851

Batch 184510, train_perplexity=781.7606, train_loss=6.6615486

Batch 184520, train_perplexity=770.4374, train_loss=6.6469584

Batch 184530, train_perplexity=770.753, train_loss=6.647368

Batch 184540, train_perplexity=790.13556, train_loss=6.6722045

Batch 184550, train_perplexity=797.3019, train_loss=6.6812334

Batch 184560, train_perplexity=742.4371, train_loss=6.609938

Batch 184570, train_perplexity=745.43, train_loss=6.613961

Batch 184580, train_perplexity=678.8059, train_loss=6.520335

Batch 184590, train_perplexity=791.4368, train_loss=6.67385

Batch 184600, train_perplexity=847.5345, train_loss=6.7423315

Batch 184610, train_perplexity=775.74426, train_loss=6.653823

Batch 184620, train_perplexity=774.8725, train_loss=6.6526985

Batch 184630, train_perplexity=726.38135, train_loss=6.588075

Batch 184640, train_perplexity=839.9578, train_loss=6.7333517

Batch 184650, train_perplexity=690.0218, train_loss=6.536723

Batch 184660, train_perplexity=816.4655, train_loss=6.7049847

Batch 184670, train_perplexity=748.09705, train_loss=6.6175327

Batch 184680, train_perplexity=687.7017, train_loss=6.533355

Batch 184690, train_perplexity=755.22815, train_loss=6.62702

Batch 184700, train_perplexity=678.84796, train_loss=6.520397

Batch 184710, train_perplexity=724.1587, train_loss=6.5850105

Batch 184720, train_perplexity=712.41986, train_loss=6.5686674

Batch 184730, train_perplexity=706.3892, train_loss=6.5601664

Batch 184740, train_perplexity=699.0356, train_loss=6.5497017

Batch 184750, train_perplexity=782.98615, train_loss=6.663115

Batch 184760, train_perplexity=756.94934, train_loss=6.6292963

Batch 184770, train_perplexity=720.40186, train_loss=6.579809

Batch 184780, train_perplexity=716.0381, train_loss=6.5737333

Batch 184790, train_perplexity=751.366, train_loss=6.621893

Batch 184800, train_perplexity=773.01843, train_loss=6.650303

Batch 184810, train_perplexity=789.93854, train_loss=6.671955

Batch 184820, train_perplexity=698.5458, train_loss=6.5490007

Batch 184830, train_perplexity=674.16534, train_loss=6.5134754

Batch 184840, train_perplexity=733.39044, train_loss=6.597678

Batch 184850, train_perplexity=733.79126, train_loss=6.5982246

Batch 184860, train_perplexity=789.47723, train_loss=6.671371

Batch 184870, train_perplexity=777.817, train_loss=6.6564913

Batch 184880, train_perplexity=718.74805, train_loss=6.577511

Batch 184890, train_perplexity=743.031, train_loss=6.610738

Batch 184900, train_perplexity=786.9714, train_loss=6.668192

Batch 184910, train_perplexity=897.22644, train_loss=6.7993083

Batch 184920, train_perplexity=726.523, train_loss=6.58827

Batch 184930, train_perplexity=753.8602, train_loss=6.625207

Batch 184940, train_perplexity=753.72864, train_loss=6.6250324

Batch 184950, train_perplexity=745.1059, train_loss=6.6135263

Batch 184960, train_perplexity=824.7822, train_loss=6.7151194

Batch 184970, train_perplexity=733.8011, train_loss=6.598238

Batch 184980, train_perplexity=802.50946, train_loss=6.6877437

Batch 184990, train_perplexity=689.8928, train_loss=6.536536

Batch 185000, train_perplexity=729.8063, train_loss=6.592779

Batch 185010, train_perplexity=756.4817, train_loss=6.6286783

Batch 185020, train_perplexity=808.0639, train_loss=6.694641

Batch 185030, train_perplexity=805.2717, train_loss=6.6911798

Batch 185040, train_perplexity=787.3359, train_loss=6.668655

Batch 185050, train_perplexity=717.5851, train_loss=6.5758915

Batch 185060, train_perplexity=775.1656, train_loss=6.6530766

Batch 185070, train_perplexity=739.67413, train_loss=6.6062098

Batch 185080, train_perplexity=716.5149, train_loss=6.574399

Batch 185090, train_perplexity=826.6091, train_loss=6.717332

Batch 185100, train_perplexity=817.37427, train_loss=6.706097

Batch 185110, train_perplexity=756.0486, train_loss=6.6281056

Batch 185120, train_perplexity=716.8498, train_loss=6.5748663
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 185130, train_perplexity=737.6366, train_loss=6.6034513

Batch 185140, train_perplexity=745.11194, train_loss=6.6135345

Batch 185150, train_perplexity=726.94824, train_loss=6.5888553

Batch 185160, train_perplexity=739.8075, train_loss=6.60639

Batch 185170, train_perplexity=755.3445, train_loss=6.627174

Batch 185180, train_perplexity=706.8609, train_loss=6.560834

Batch 185190, train_perplexity=743.49426, train_loss=6.611361

Batch 185200, train_perplexity=751.0626, train_loss=6.621489

Batch 185210, train_perplexity=731.0072, train_loss=6.5944233

Batch 185220, train_perplexity=695.06036, train_loss=6.5439987

Batch 185230, train_perplexity=707.28235, train_loss=6.56143

Batch 185240, train_perplexity=763.3851, train_loss=6.6377625

Batch 185250, train_perplexity=783.4033, train_loss=6.6636477

Batch 185260, train_perplexity=741.64484, train_loss=6.6088705

Batch 185270, train_perplexity=773.0354, train_loss=6.650325

Batch 185280, train_perplexity=725.37445, train_loss=6.586688

Batch 185290, train_perplexity=809.7313, train_loss=6.6967025

Batch 185300, train_perplexity=785.8097, train_loss=6.6667147

Batch 185310, train_perplexity=750.1968, train_loss=6.6203356

Batch 185320, train_perplexity=747.907, train_loss=6.6172786

Batch 185330, train_perplexity=780.2881, train_loss=6.659663

Batch 185340, train_perplexity=776.31964, train_loss=6.6545644

Batch 185350, train_perplexity=798.85535, train_loss=6.68318

Batch 185360, train_perplexity=735.0184, train_loss=6.5998955

Batch 185370, train_perplexity=836.38, train_loss=6.729083

Batch 185380, train_perplexity=815.2385, train_loss=6.7034807

Batch 185390, train_perplexity=810.1647, train_loss=6.6972375

Batch 185400, train_perplexity=692.698, train_loss=6.540594

Batch 185410, train_perplexity=694.7781, train_loss=6.5435925

Batch 185420, train_perplexity=697.26654, train_loss=6.547168

Batch 185430, train_perplexity=823.72375, train_loss=6.7138352

Batch 185440, train_perplexity=811.7118, train_loss=6.6991453

Batch 185450, train_perplexity=733.8039, train_loss=6.598242

Batch 185460, train_perplexity=818.70325, train_loss=6.7077217

Batch 185470, train_perplexity=773.6645, train_loss=6.6511383

Batch 185480, train_perplexity=799.3385, train_loss=6.6837845

Batch 185490, train_perplexity=783.76013, train_loss=6.664103

Batch 185500, train_perplexity=705.5207, train_loss=6.558936

Batch 185510, train_perplexity=730.1955, train_loss=6.5933123

Batch 185520, train_perplexity=847.2884, train_loss=6.742041

Batch 185530, train_perplexity=784.5435, train_loss=6.665102

Batch 185540, train_perplexity=777.07263, train_loss=6.655534

Batch 185550, train_perplexity=731.5728, train_loss=6.5951967

Batch 185560, train_perplexity=732.55786, train_loss=6.5965424

Batch 185570, train_perplexity=817.7025, train_loss=6.7064986

Batch 185580, train_perplexity=792.7051, train_loss=6.6754513

Batch 185590, train_perplexity=744.1071, train_loss=6.612185

Batch 185600, train_perplexity=796.20624, train_loss=6.679858

Batch 185610, train_perplexity=782.2398, train_loss=6.6621614

Batch 185620, train_perplexity=796.68585, train_loss=6.6804605

Batch 185630, train_perplexity=835.4903, train_loss=6.7280188

Batch 185640, train_perplexity=752.25507, train_loss=6.6230755

Batch 185650, train_perplexity=835.90955, train_loss=6.7285204

Batch 185660, train_perplexity=669.4212, train_loss=6.5064135

Batch 185670, train_perplexity=713.7691, train_loss=6.5705595

Batch 185680, train_perplexity=689.42053, train_loss=6.5358515

Batch 185690, train_perplexity=667.8207, train_loss=6.5040197

Batch 185700, train_perplexity=771.7375, train_loss=6.6486444

Batch 185710, train_perplexity=698.723, train_loss=6.5492544

Batch 185720, train_perplexity=654.7615, train_loss=6.484271

Batch 185730, train_perplexity=799.43756, train_loss=6.6839085

Batch 185740, train_perplexity=758.65234, train_loss=6.6315436

Batch 185750, train_perplexity=742.7703, train_loss=6.610387

Batch 185760, train_perplexity=695.998, train_loss=6.5453467

Batch 185770, train_perplexity=753.8207, train_loss=6.6251545

Batch 185780, train_perplexity=761.6038, train_loss=6.6354265

Batch 185790, train_perplexity=697.7395, train_loss=6.547846

Batch 185800, train_perplexity=835.7457, train_loss=6.7283244

Batch 185810, train_perplexity=765.6018, train_loss=6.640662

Batch 185820, train_perplexity=812.00366, train_loss=6.699505

Batch 185830, train_perplexity=761.4056, train_loss=6.635166

Batch 185840, train_perplexity=869.55493, train_loss=6.7679815

Batch 185850, train_perplexity=732.94434, train_loss=6.5970697

Batch 185860, train_perplexity=748.6298, train_loss=6.6182446

Batch 185870, train_perplexity=698.655, train_loss=6.549157

Batch 185880, train_perplexity=799.0092, train_loss=6.6833725

Batch 185890, train_perplexity=768.46783, train_loss=6.6443987

Batch 185900, train_perplexity=814.7562, train_loss=6.702889

Batch 185910, train_perplexity=804.3507, train_loss=6.6900353

Batch 185920, train_perplexity=710.2565, train_loss=6.565626

Batch 185930, train_perplexity=679.10956, train_loss=6.5207825

Batch 185940, train_perplexity=792.6885, train_loss=6.6754303

Batch 185950, train_perplexity=688.87177, train_loss=6.535055

Batch 185960, train_perplexity=834.5582, train_loss=6.7269025

Batch 185970, train_perplexity=789.2288, train_loss=6.6710563

Batch 185980, train_perplexity=746.06366, train_loss=6.614811

Batch 185990, train_perplexity=717.9886, train_loss=6.5764537

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Batch 186000, train_perplexity=739.03955, train_loss=6.6053514

Batch 186010, train_perplexity=726.10846, train_loss=6.5876994

Batch 186020, train_perplexity=819.27264, train_loss=6.708417

Batch 186030, train_perplexity=744.60156, train_loss=6.612849

Batch 186040, train_perplexity=752.5471, train_loss=6.6234636

Batch 186050, train_perplexity=645.4605, train_loss=6.469964

Batch 186060, train_perplexity=726.6831, train_loss=6.5884905

Batch 186070, train_perplexity=768.7764, train_loss=6.6448

Batch 186080, train_perplexity=704.87976, train_loss=6.5580273

Batch 186090, train_perplexity=728.6783, train_loss=6.5912323

Batch 186100, train_perplexity=779.77814, train_loss=6.6590095

Batch 186110, train_perplexity=729.36725, train_loss=6.5921774

Batch 186120, train_perplexity=758.461, train_loss=6.6312914

Batch 186130, train_perplexity=813.79486, train_loss=6.7017083

Batch 186140, train_perplexity=724.02576, train_loss=6.584827

Batch 186150, train_perplexity=742.3355, train_loss=6.6098013

Batch 186160, train_perplexity=723.2269, train_loss=6.583723

Batch 186170, train_perplexity=726.1919, train_loss=6.5878143

Batch 186180, train_perplexity=799.5939, train_loss=6.684104

Batch 186190, train_perplexity=625.9428, train_loss=6.439259

Batch 186200, train_perplexity=780.0753, train_loss=6.6593904

Batch 186210, train_perplexity=692.48334, train_loss=6.540284

Batch 186220, train_perplexity=821.21106, train_loss=6.71078

Batch 186230, train_perplexity=665.1023, train_loss=6.499941

Batch 186240, train_perplexity=692.9147, train_loss=6.540907

Batch 186250, train_perplexity=777.1845, train_loss=6.655678

Batch 186260, train_perplexity=751.25354, train_loss=6.621743

Batch 186270, train_perplexity=777.7154, train_loss=6.6563606

Batch 186280, train_perplexity=787.8406, train_loss=6.669296

Batch 186290, train_perplexity=758.98126, train_loss=6.631977

Batch 186300, train_perplexity=834.80457, train_loss=6.7271976

Batch 186310, train_perplexity=781.4789, train_loss=6.661188

Batch 186320, train_perplexity=714.6733, train_loss=6.5718255

Batch 186330, train_perplexity=830.8152, train_loss=6.7224073

Batch 186340, train_perplexity=748.04785, train_loss=6.617467

Batch 186350, train_perplexity=717.56934, train_loss=6.5758696

Batch 186360, train_perplexity=769.8575, train_loss=6.6462054

Batch 186370, train_perplexity=810.24347, train_loss=6.697335
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 186380, train_perplexity=693.31793, train_loss=6.5414886

Batch 186390, train_perplexity=698.8077, train_loss=6.5493755

Batch 186400, train_perplexity=772.17737, train_loss=6.6492143

Batch 186410, train_perplexity=780.6256, train_loss=6.6600957

Batch 186420, train_perplexity=747.23785, train_loss=6.6163836

Batch 186430, train_perplexity=766.31696, train_loss=6.641596

Batch 186440, train_perplexity=789.1938, train_loss=6.671012

Batch 186450, train_perplexity=729.3693, train_loss=6.5921803

Batch 186460, train_perplexity=769.88354, train_loss=6.6462393

Batch 186470, train_perplexity=754.264, train_loss=6.6257424

Batch 186480, train_perplexity=790.3869, train_loss=6.6725225

Batch 186490, train_perplexity=819.4133, train_loss=6.7085886

Batch 186500, train_perplexity=714.3009, train_loss=6.5713043

Batch 186510, train_perplexity=715.9155, train_loss=6.573562

Batch 186520, train_perplexity=737.97186, train_loss=6.6039057

Batch 186530, train_perplexity=678.28076, train_loss=6.5195613

Batch 186540, train_perplexity=774.3288, train_loss=6.6519966

Batch 186550, train_perplexity=667.86975, train_loss=6.504093

Batch 186560, train_perplexity=707.94165, train_loss=6.5623617

Batch 186570, train_perplexity=770.5226, train_loss=6.647069

Batch 186580, train_perplexity=720.85956, train_loss=6.5804443

Batch 186590, train_perplexity=798.1471, train_loss=6.682293

Batch 186600, train_perplexity=813.545, train_loss=6.701401

Batch 186610, train_perplexity=800.3461, train_loss=6.6850443

Batch 186620, train_perplexity=736.4174, train_loss=6.601797

Batch 186630, train_perplexity=694.5753, train_loss=6.5433006

Batch 186640, train_perplexity=697.0126, train_loss=6.5468035

Batch 186650, train_perplexity=747.25244, train_loss=6.616403

Batch 186660, train_perplexity=817.8254, train_loss=6.706649

Batch 186670, train_perplexity=834.0088, train_loss=6.726244

Batch 186680, train_perplexity=680.26917, train_loss=6.5224886

Batch 186690, train_perplexity=770.7563, train_loss=6.6473722

Batch 186700, train_perplexity=767.7305, train_loss=6.643439

Batch 186710, train_perplexity=730.41797, train_loss=6.593617

Batch 186720, train_perplexity=822.28235, train_loss=6.712084

Batch 186730, train_perplexity=723.6678, train_loss=6.5843325

Batch 186740, train_perplexity=758.7895, train_loss=6.6317244

Batch 186750, train_perplexity=729.15826, train_loss=6.591891

Batch 186760, train_perplexity=745.4758, train_loss=6.6140227

Batch 186770, train_perplexity=774.26385, train_loss=6.6519127

Batch 186780, train_perplexity=776.276, train_loss=6.654508

Batch 186790, train_perplexity=778.4255, train_loss=6.6572733

Batch 186800, train_perplexity=787.847, train_loss=6.669304

Batch 186810, train_perplexity=814.5911, train_loss=6.7026863

Batch 186820, train_perplexity=794.32, train_loss=6.6774864

Batch 186830, train_perplexity=777.70575, train_loss=6.656348

Batch 186840, train_perplexity=693.5914, train_loss=6.541883

Batch 186850, train_perplexity=713.7759, train_loss=6.570569

Batch 186860, train_perplexity=650.84686, train_loss=6.4782743

Batch 186870, train_perplexity=697.36865, train_loss=6.547314

Batch 186880, train_perplexity=786.0803, train_loss=6.667059

Batch 186890, train_perplexity=734.9826, train_loss=6.599847

Batch 186900, train_perplexity=732.9359, train_loss=6.5970583

Batch 186910, train_perplexity=773.75006, train_loss=6.651249

Batch 186920, train_perplexity=758.809, train_loss=6.63175

Batch 186930, train_perplexity=753.63306, train_loss=6.6249056

Batch 186940, train_perplexity=728.6081, train_loss=6.591136

Batch 186950, train_perplexity=768.91864, train_loss=6.644985

Batch 186960, train_perplexity=724.65955, train_loss=6.585702

Batch 186970, train_perplexity=679.4182, train_loss=6.521237

Batch 186980, train_perplexity=807.27747, train_loss=6.6936674

Batch 186990, train_perplexity=691.04846, train_loss=6.53821

Batch 187000, train_perplexity=729.0404, train_loss=6.591729

Batch 187010, train_perplexity=740.29517, train_loss=6.607049

Batch 187020, train_perplexity=818.98126, train_loss=6.708061

Batch 187030, train_perplexity=689.82965, train_loss=6.5364447

Batch 187040, train_perplexity=711.52594, train_loss=6.567412

Batch 187050, train_perplexity=760.5906, train_loss=6.634095

Batch 187060, train_perplexity=717.1199, train_loss=6.575243

Batch 187070, train_perplexity=698.82166, train_loss=6.5493956

Batch 187080, train_perplexity=735.87823, train_loss=6.6010647

Batch 187090, train_perplexity=756.82086, train_loss=6.6291265

Batch 187100, train_perplexity=732.25824, train_loss=6.596133

Batch 187110, train_perplexity=744.9681, train_loss=6.6133413

Batch 187120, train_perplexity=703.5675, train_loss=6.556164

Batch 187130, train_perplexity=689.44885, train_loss=6.5358925

Batch 187140, train_perplexity=707.6528, train_loss=6.5619535

Batch 187150, train_perplexity=800.6923, train_loss=6.685477

Batch 187160, train_perplexity=761.26764, train_loss=6.634985

Batch 187170, train_perplexity=761.17035, train_loss=6.634857

Batch 187180, train_perplexity=741.5925, train_loss=6.6088

Batch 187190, train_perplexity=816.5683, train_loss=6.7051105

Batch 187200, train_perplexity=718.65753, train_loss=6.577385

Batch 187210, train_perplexity=780.3078, train_loss=6.6596885

Batch 187220, train_perplexity=738.85455, train_loss=6.605101

Batch 187230, train_perplexity=726.24664, train_loss=6.5878897

Batch 187240, train_perplexity=825.41724, train_loss=6.715889

Batch 187250, train_perplexity=832.6562, train_loss=6.724621

Batch 187260, train_perplexity=733.3988, train_loss=6.5976896

Batch 187270, train_perplexity=735.66284, train_loss=6.600772

Batch 187280, train_perplexity=769.90924, train_loss=6.6462727

Batch 187290, train_perplexity=768.26886, train_loss=6.64414

Batch 187300, train_perplexity=673.33453, train_loss=6.5122423

Batch 187310, train_perplexity=798.1152, train_loss=6.682253

Batch 187320, train_perplexity=837.0822, train_loss=6.7299223

Batch 187330, train_perplexity=806.72485, train_loss=6.6929827

Batch 187340, train_perplexity=748.572, train_loss=6.6181674

Batch 187350, train_perplexity=791.4315, train_loss=6.6738434

Batch 187360, train_perplexity=797.8625, train_loss=6.6819363

Batch 187370, train_perplexity=720.17035, train_loss=6.579488

Batch 187380, train_perplexity=702.88306, train_loss=6.5551906

Batch 187390, train_perplexity=781.48224, train_loss=6.6611924

Batch 187400, train_perplexity=820.2452, train_loss=6.7096033

Batch 187410, train_perplexity=807.97797, train_loss=6.694535

Batch 187420, train_perplexity=745.57715, train_loss=6.6141586

Batch 187430, train_perplexity=790.80383, train_loss=6.67305

Batch 187440, train_perplexity=715.0479, train_loss=6.5723495

Batch 187450, train_perplexity=785.5261, train_loss=6.6663537

Batch 187460, train_perplexity=784.69763, train_loss=6.6652985

Batch 187470, train_perplexity=720.7719, train_loss=6.5803227

Batch 187480, train_perplexity=743.7698, train_loss=6.6117315

Batch 187490, train_perplexity=746.6124, train_loss=6.615546

Batch 187500, train_perplexity=763.0564, train_loss=6.637332

Batch 187510, train_perplexity=780.9447, train_loss=6.6605043

Batch 187520, train_perplexity=691.02075, train_loss=6.53817

Batch 187530, train_perplexity=814.99286, train_loss=6.7031794

Batch 187540, train_perplexity=809.6502, train_loss=6.6966023

Batch 187550, train_perplexity=741.8613, train_loss=6.6091623

Batch 187560, train_perplexity=720.5307, train_loss=6.579988

Batch 187570, train_perplexity=811.242, train_loss=6.6985664

Batch 187580, train_perplexity=822.69025, train_loss=6.7125797

Batch 187590, train_perplexity=840.28033, train_loss=6.7337356

Batch 187600, train_perplexity=760.9374, train_loss=6.634551

Batch 187610, train_perplexity=756.7923, train_loss=6.629089

Batch 187620, train_perplexity=712.7066, train_loss=6.56907

Batch 187630, train_perplexity=718.5486, train_loss=6.5772333

Batch 187640, train_perplexity=710.25446, train_loss=6.5656233

Batch 187650, train_perplexity=769.281, train_loss=6.6454563

Batch 187660, train_perplexity=743.5382, train_loss=6.61142

Batch 187670, train_perplexity=721.72833, train_loss=6.581649

Batch 187680, train_perplexity=834.8364, train_loss=6.727236
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 187690, train_perplexity=851.0955, train_loss=6.7465243

Batch 187700, train_perplexity=738.34705, train_loss=6.604414

Batch 187710, train_perplexity=688.18463, train_loss=6.534057

Batch 187720, train_perplexity=751.5595, train_loss=6.6221504

Batch 187730, train_perplexity=717.6836, train_loss=6.576029

Batch 187740, train_perplexity=722.2, train_loss=6.582302

Batch 187750, train_perplexity=869.06915, train_loss=6.7674227

Batch 187760, train_perplexity=731.48663, train_loss=6.595079

Batch 187770, train_perplexity=798.50494, train_loss=6.682741

Batch 187780, train_perplexity=743.1458, train_loss=6.6108923

Batch 187790, train_perplexity=707.834, train_loss=6.5622096

Batch 187800, train_perplexity=723.4667, train_loss=6.5840545

Batch 187810, train_perplexity=784.6475, train_loss=6.6652346

Batch 187820, train_perplexity=785.9049, train_loss=6.666836

Batch 187830, train_perplexity=720.05945, train_loss=6.579334

Batch 187840, train_perplexity=775.9877, train_loss=6.6541367

Batch 187850, train_perplexity=784.60895, train_loss=6.6651855

Batch 187860, train_perplexity=765.8786, train_loss=6.6410236

Batch 187870, train_perplexity=705.9104, train_loss=6.5594883

Batch 187880, train_perplexity=718.06903, train_loss=6.5765657

Batch 187890, train_perplexity=756.09656, train_loss=6.628169

Batch 187900, train_perplexity=718.904, train_loss=6.577728

Batch 187910, train_perplexity=801.73914, train_loss=6.6867833

Batch 187920, train_perplexity=793.0265, train_loss=6.6758566

Batch 187930, train_perplexity=679.5358, train_loss=6.52141

Batch 187940, train_perplexity=681.19525, train_loss=6.523849

Batch 187950, train_perplexity=670.3038, train_loss=6.507731

Batch 187960, train_perplexity=747.3708, train_loss=6.6165614

Batch 187970, train_perplexity=762.85925, train_loss=6.6370735

Batch 187980, train_perplexity=743.56305, train_loss=6.6114535

Batch 187990, train_perplexity=727.7702, train_loss=6.5899854

Batch 188000, train_perplexity=781.9545, train_loss=6.6617966

Batch 188010, train_perplexity=748.68805, train_loss=6.6183224

Batch 188020, train_perplexity=709.1137, train_loss=6.564016

Batch 188030, train_perplexity=725.535, train_loss=6.5869093

Batch 188040, train_perplexity=722.98004, train_loss=6.5833817

Batch 188050, train_perplexity=755.2411, train_loss=6.627037

Batch 188060, train_perplexity=798.583, train_loss=6.682839

Batch 188070, train_perplexity=786.53925, train_loss=6.6676426

Batch 188080, train_perplexity=786.40796, train_loss=6.6674757

Batch 188090, train_perplexity=765.77704, train_loss=6.640891

Batch 188100, train_perplexity=748.04675, train_loss=6.6174655

Batch 188110, train_perplexity=856.3432, train_loss=6.7526712

Batch 188120, train_perplexity=790.6794, train_loss=6.6728926

Batch 188130, train_perplexity=760.785, train_loss=6.634351

Batch 188140, train_perplexity=644.7373, train_loss=6.468843

Batch 188150, train_perplexity=802.8949, train_loss=6.688224

Batch 188160, train_perplexity=753.69916, train_loss=6.6249933

Batch 188170, train_perplexity=752.2816, train_loss=6.623111

Batch 188180, train_perplexity=777.2983, train_loss=6.655824

Batch 188190, train_perplexity=747.83887, train_loss=6.6171875

Batch 188200, train_perplexity=694.60876, train_loss=6.543349

Batch 188210, train_perplexity=818.1694, train_loss=6.7070694

Batch 188220, train_perplexity=716.06604, train_loss=6.5737724

Batch 188230, train_perplexity=822.7271, train_loss=6.7126245

Batch 188240, train_perplexity=775.09607, train_loss=6.652987

Batch 188250, train_perplexity=764.41846, train_loss=6.6391153

Batch 188260, train_perplexity=828.15283, train_loss=6.7191978

Batch 188270, train_perplexity=709.1445, train_loss=6.5640593

Batch 188280, train_perplexity=830.67615, train_loss=6.72224

Batch 188290, train_perplexity=783.1452, train_loss=6.663318

Batch 188300, train_perplexity=660.12787, train_loss=6.4924335

Batch 188310, train_perplexity=727.8521, train_loss=6.590098

Batch 188320, train_perplexity=741.43695, train_loss=6.60859

Batch 188330, train_perplexity=728.7485, train_loss=6.5913286

Batch 188340, train_perplexity=702.2121, train_loss=6.5542355

Batch 188350, train_perplexity=775.5771, train_loss=6.6536074

Batch 188360, train_perplexity=685.2402, train_loss=6.5297694

Batch 188370, train_perplexity=752.50946, train_loss=6.6234136

Batch 188380, train_perplexity=617.526, train_loss=6.425721

Batch 188390, train_perplexity=671.9201, train_loss=6.5101395

Batch 188400, train_perplexity=781.82367, train_loss=6.661629

Batch 188410, train_perplexity=751.91296, train_loss=6.6226206

Batch 188420, train_perplexity=790.23425, train_loss=6.6723294

Batch 188430, train_perplexity=852.55084, train_loss=6.748233

Batch 188440, train_perplexity=715.3351, train_loss=6.572751

Batch 188450, train_perplexity=695.2526, train_loss=6.5442753

Batch 188460, train_perplexity=752.216, train_loss=6.6230235

Batch 188470, train_perplexity=795.01044, train_loss=6.678355

Batch 188480, train_perplexity=714.37177, train_loss=6.5714035

Batch 188490, train_perplexity=816.0234, train_loss=6.704443

Batch 188500, train_perplexity=767.3371, train_loss=6.642926

Batch 188510, train_perplexity=745.1535, train_loss=6.6135902

Batch 188520, train_perplexity=719.4691, train_loss=6.5785136

Batch 188530, train_perplexity=717.7418, train_loss=6.57611

Batch 188540, train_perplexity=743.4861, train_loss=6.61135

Batch 188550, train_perplexity=851.225, train_loss=6.7466764

Batch 188560, train_perplexity=715.2757, train_loss=6.572668

Batch 188570, train_perplexity=706.7032, train_loss=6.560611

Batch 188580, train_perplexity=715.6339, train_loss=6.5731688

Batch 188590, train_perplexity=780.11993, train_loss=6.6594477

Batch 188600, train_perplexity=682.0472, train_loss=6.525099

Batch 188610, train_perplexity=767.41174, train_loss=6.6430235

Batch 188620, train_perplexity=683.9837, train_loss=6.527934

Batch 188630, train_perplexity=664.0084, train_loss=6.498295

Batch 188640, train_perplexity=769.5349, train_loss=6.6457863

Batch 188650, train_perplexity=766.9592, train_loss=6.6424336

Batch 188660, train_perplexity=769.6927, train_loss=6.6459913

Batch 188670, train_perplexity=707.6433, train_loss=6.56194

Batch 188680, train_perplexity=697.44116, train_loss=6.547418

Batch 188690, train_perplexity=762.0027, train_loss=6.63595

Batch 188700, train_perplexity=703.9708, train_loss=6.556737

Batch 188710, train_perplexity=805.4311, train_loss=6.6913776

Batch 188720, train_perplexity=791.05273, train_loss=6.6733646

Batch 188730, train_perplexity=829.2415, train_loss=6.7205114

Batch 188740, train_perplexity=789.2269, train_loss=6.671054

Batch 188750, train_perplexity=757.2103, train_loss=6.629641

Batch 188760, train_perplexity=704.47284, train_loss=6.55745

Batch 188770, train_perplexity=799.2173, train_loss=6.683633

Batch 188780, train_perplexity=773.38635, train_loss=6.650779

Batch 188790, train_perplexity=686.4502, train_loss=6.5315337

Batch 188800, train_perplexity=711.6675, train_loss=6.5676107

Batch 188810, train_perplexity=732.87964, train_loss=6.5969815

Batch 188820, train_perplexity=810.3868, train_loss=6.6975117

Batch 188830, train_perplexity=742.33514, train_loss=6.609801

Batch 188840, train_perplexity=784.81287, train_loss=6.6654453

Batch 188850, train_perplexity=722.6189, train_loss=6.582882

Batch 188860, train_perplexity=746.97925, train_loss=6.6160374

Batch 188870, train_perplexity=729.09814, train_loss=6.5918083

Batch 188880, train_perplexity=682.69794, train_loss=6.5260525

Batch 188890, train_perplexity=752.6584, train_loss=6.6236115

Batch 188900, train_perplexity=803.33685, train_loss=6.688774

Batch 188910, train_perplexity=745.78235, train_loss=6.614434

Batch 188920, train_perplexity=760.5485, train_loss=6.63404

Batch 188930, train_perplexity=751.97784, train_loss=6.622707

Batch 188940, train_perplexity=692.74225, train_loss=6.540658

Batch 188950, train_perplexity=739.34235, train_loss=6.605761

Batch 188960, train_perplexity=733.7416, train_loss=6.598157

Batch 188970, train_perplexity=776.6496, train_loss=6.6549892

Batch 188980, train_perplexity=706.9897, train_loss=6.561016

Batch 188990, train_perplexity=737.7101, train_loss=6.603551
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 189000, train_perplexity=793.61206, train_loss=6.6765947

Batch 189010, train_perplexity=731.11176, train_loss=6.5945663

Batch 189020, train_perplexity=714.0544, train_loss=6.570959

Batch 189030, train_perplexity=723.01935, train_loss=6.583436

Batch 189040, train_perplexity=730.25604, train_loss=6.593395

Batch 189050, train_perplexity=762.1953, train_loss=6.636203

Batch 189060, train_perplexity=667.9242, train_loss=6.5041747

Batch 189070, train_perplexity=731.8659, train_loss=6.5955973

Batch 189080, train_perplexity=782.90515, train_loss=6.6630116

Batch 189090, train_perplexity=738.0127, train_loss=6.603961

Batch 189100, train_perplexity=700.2406, train_loss=6.551424

Batch 189110, train_perplexity=706.87573, train_loss=6.560855

Batch 189120, train_perplexity=755.708, train_loss=6.627655

Batch 189130, train_perplexity=723.1897, train_loss=6.5836716

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Batch 189140, train_perplexity=764.84247, train_loss=6.63967

Batch 189150, train_perplexity=770.3095, train_loss=6.6467924

Batch 189160, train_perplexity=790.0361, train_loss=6.6720786

Batch 189170, train_perplexity=707.79315, train_loss=6.562152

Batch 189180, train_perplexity=775.444, train_loss=6.6534357

Batch 189190, train_perplexity=747.5258, train_loss=6.616769

Batch 189200, train_perplexity=673.13745, train_loss=6.5119495

Batch 189210, train_perplexity=768.1879, train_loss=6.6440344

Batch 189220, train_perplexity=748.2712, train_loss=6.6177654

Batch 189230, train_perplexity=748.768, train_loss=6.618429

Batch 189240, train_perplexity=719.2273, train_loss=6.5781775

Batch 189250, train_perplexity=677.39484, train_loss=6.5182543

Batch 189260, train_perplexity=691.0731, train_loss=6.5382457

Batch 189270, train_perplexity=687.249, train_loss=6.5326967

Batch 189280, train_perplexity=691.4644, train_loss=6.5388117

Batch 189290, train_perplexity=765.06464, train_loss=6.6399603

Batch 189300, train_perplexity=727.28314, train_loss=6.589316

Batch 189310, train_perplexity=789.283, train_loss=6.671125

Batch 189320, train_perplexity=777.2698, train_loss=6.6557875

Batch 189330, train_perplexity=774.85254, train_loss=6.652673

Batch 189340, train_perplexity=766.1189, train_loss=6.6413374

Batch 189350, train_perplexity=748.07635, train_loss=6.617505

Batch 189360, train_perplexity=867.2824, train_loss=6.7653646

Batch 189370, train_perplexity=760.44336, train_loss=6.6339016

Batch 189380, train_perplexity=813.38715, train_loss=6.701207

Batch 189390, train_perplexity=767.6855, train_loss=6.64338

Batch 189400, train_perplexity=808.96136, train_loss=6.695751

Batch 189410, train_perplexity=765.9242, train_loss=6.6410832

Batch 189420, train_perplexity=871.0992, train_loss=6.769756

Batch 189430, train_perplexity=711.25964, train_loss=6.5670376

Batch 189440, train_perplexity=768.2751, train_loss=6.644148

Batch 189450, train_perplexity=754.10394, train_loss=6.6255302

Batch 189460, train_perplexity=803.0067, train_loss=6.688363

Batch 189470, train_perplexity=739.6886, train_loss=6.6062293

Batch 189480, train_perplexity=656.41925, train_loss=6.4867997

Batch 189490, train_perplexity=775.6362, train_loss=6.6536837

Batch 189500, train_perplexity=730.2108, train_loss=6.5933332

Batch 189510, train_perplexity=782.5237, train_loss=6.662524

Batch 189520, train_perplexity=736.099, train_loss=6.6013646

Batch 189530, train_perplexity=687.6398, train_loss=6.533265

Batch 189540, train_perplexity=780.1735, train_loss=6.6595163

Batch 189550, train_perplexity=709.58856, train_loss=6.5646853

Batch 189560, train_perplexity=700.76904, train_loss=6.5521784

Batch 189570, train_perplexity=765.2638, train_loss=6.6402206

Batch 189580, train_perplexity=829.6556, train_loss=6.7210107

Batch 189590, train_perplexity=670.2414, train_loss=6.507638

Batch 189600, train_perplexity=799.3754, train_loss=6.6838307

Batch 189610, train_perplexity=746.725, train_loss=6.615697

Batch 189620, train_perplexity=714.02374, train_loss=6.570916

Batch 189630, train_perplexity=741.04535, train_loss=6.608062

Batch 189640, train_perplexity=758.93604, train_loss=6.6319175

Batch 189650, train_perplexity=816.65045, train_loss=6.705211

Batch 189660, train_perplexity=757.2067, train_loss=6.6296363

Batch 189670, train_perplexity=839.7496, train_loss=6.7331038

Batch 189680, train_perplexity=736.63727, train_loss=6.6020956

Batch 189690, train_perplexity=724.5662, train_loss=6.585573

Batch 189700, train_perplexity=684.01044, train_loss=6.527973

Batch 189710, train_perplexity=673.4119, train_loss=6.512357

Batch 189720, train_perplexity=742.1097, train_loss=6.609497

Batch 189730, train_perplexity=701.35004, train_loss=6.553007

Batch 189740, train_perplexity=761.8581, train_loss=6.6357603

Batch 189750, train_perplexity=775.05725, train_loss=6.652937

Batch 189760, train_perplexity=750.6305, train_loss=6.6209135

Batch 189770, train_perplexity=718.7347, train_loss=6.577492

Batch 189780, train_perplexity=805.5532, train_loss=6.6915293

Batch 189790, train_perplexity=677.4084, train_loss=6.5182743

Batch 189800, train_perplexity=801.2538, train_loss=6.6861777

Batch 189810, train_perplexity=772.45764, train_loss=6.649577

Batch 189820, train_perplexity=757.67194, train_loss=6.6302505

Batch 189830, train_perplexity=631.8984, train_loss=6.4487286

Batch 189840, train_perplexity=745.23737, train_loss=6.613703

Batch 189850, train_perplexity=789.42865, train_loss=6.6713095

Batch 189860, train_perplexity=676.2517, train_loss=6.5165653

Batch 189870, train_perplexity=759.7326, train_loss=6.6329665

Batch 189880, train_perplexity=786.5475, train_loss=6.667653

Batch 189890, train_perplexity=789.3899, train_loss=6.6712604

Batch 189900, train_perplexity=801.1323, train_loss=6.686026

Batch 189910, train_perplexity=749.8542, train_loss=6.619879

Batch 189920, train_perplexity=697.56854, train_loss=6.5476007

Batch 189930, train_perplexity=651.95135, train_loss=6.47997

Batch 189940, train_perplexity=708.1622, train_loss=6.562673

Batch 189950, train_perplexity=753.93317, train_loss=6.6253037

Batch 189960, train_perplexity=681.72784, train_loss=6.5246305

Batch 189970, train_perplexity=773.9578, train_loss=6.6515174

Batch 189980, train_perplexity=713.4043, train_loss=6.5700483

Batch 189990, train_perplexity=739.2658, train_loss=6.6056576

Batch 190000, train_perplexity=763.4411, train_loss=6.637836

Batch 190010, train_perplexity=719.39264, train_loss=6.5784073

Batch 190020, train_perplexity=737.1858, train_loss=6.60284

Batch 190030, train_perplexity=715.9541, train_loss=6.573616

Batch 190040, train_perplexity=687.1494, train_loss=6.532552

Batch 190050, train_perplexity=761.1776, train_loss=6.6348667

Batch 190060, train_perplexity=703.64935, train_loss=6.55628

Batch 190070, train_perplexity=841.16547, train_loss=6.7347884

Batch 190080, train_perplexity=736.671, train_loss=6.6021414

Batch 190090, train_perplexity=788.534, train_loss=6.6701756

Batch 190100, train_perplexity=750.7572, train_loss=6.6210823

Batch 190110, train_perplexity=718.6401, train_loss=6.5773606

Batch 190120, train_perplexity=757.49457, train_loss=6.6300163

Batch 190130, train_perplexity=744.9034, train_loss=6.6132545

Batch 190140, train_perplexity=741.397, train_loss=6.6085362

Batch 190150, train_perplexity=708.9044, train_loss=6.5637207

Batch 190160, train_perplexity=825.1567, train_loss=6.7155733

Batch 190170, train_perplexity=763.54486, train_loss=6.637972

Batch 190180, train_perplexity=739.495, train_loss=6.6059675

Batch 190190, train_perplexity=765.0453, train_loss=6.639935

Batch 190200, train_perplexity=801.7273, train_loss=6.6867685

Batch 190210, train_perplexity=705.08954, train_loss=6.558325

Batch 190220, train_perplexity=752.4833, train_loss=6.6233788

Batch 190230, train_perplexity=701.2902, train_loss=6.552922

Batch 190240, train_perplexity=714.68933, train_loss=6.571848
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 190250, train_perplexity=778.44366, train_loss=6.6572967

Batch 190260, train_perplexity=786.36444, train_loss=6.6674204

Batch 190270, train_perplexity=773.5033, train_loss=6.65093

Batch 190280, train_perplexity=749.377, train_loss=6.619242

Batch 190290, train_perplexity=817.9334, train_loss=6.706781

Batch 190300, train_perplexity=766.0422, train_loss=6.6412373

Batch 190310, train_perplexity=752.6448, train_loss=6.6235933

Batch 190320, train_perplexity=679.8314, train_loss=6.521845

Batch 190330, train_perplexity=777.04, train_loss=6.655492

Batch 190340, train_perplexity=740.8577, train_loss=6.6078086

Batch 190350, train_perplexity=717.92664, train_loss=6.5763674

Batch 190360, train_perplexity=718.9437, train_loss=6.577783

Batch 190370, train_perplexity=713.3662, train_loss=6.569995

Batch 190380, train_perplexity=725.5945, train_loss=6.5869913

Batch 190390, train_perplexity=773.9257, train_loss=6.651476

Batch 190400, train_perplexity=708.7053, train_loss=6.56344

Batch 190410, train_perplexity=753.6963, train_loss=6.6249895

Batch 190420, train_perplexity=741.804, train_loss=6.609085

Batch 190430, train_perplexity=753.383, train_loss=6.6245737

Batch 190440, train_perplexity=796.1591, train_loss=6.679799

Batch 190450, train_perplexity=700.872, train_loss=6.5523252

Batch 190460, train_perplexity=792.76636, train_loss=6.6755285

Batch 190470, train_perplexity=742.2838, train_loss=6.6097317

Batch 190480, train_perplexity=701.6745, train_loss=6.5534697

Batch 190490, train_perplexity=828.949, train_loss=6.7201586

Batch 190500, train_perplexity=697.65436, train_loss=6.547724

Batch 190510, train_perplexity=687.8838, train_loss=6.53362

Batch 190520, train_perplexity=741.6293, train_loss=6.6088495

Batch 190530, train_perplexity=841.22485, train_loss=6.734859

Batch 190540, train_perplexity=769.8755, train_loss=6.646229

Batch 190550, train_perplexity=792.08844, train_loss=6.674673

Batch 190560, train_perplexity=713.337, train_loss=6.569954

Batch 190570, train_perplexity=753.3183, train_loss=6.624488

Batch 190580, train_perplexity=737.5082, train_loss=6.603277

Batch 190590, train_perplexity=791.2119, train_loss=6.673566

Batch 190600, train_perplexity=726.56775, train_loss=6.5883317

Batch 190610, train_perplexity=695.01764, train_loss=6.543937

Batch 190620, train_perplexity=740.5091, train_loss=6.607338

Batch 190630, train_perplexity=712.53534, train_loss=6.5688295

Batch 190640, train_perplexity=737.5546, train_loss=6.60334

Batch 190650, train_perplexity=747.5336, train_loss=6.6167793

Batch 190660, train_perplexity=687.5919, train_loss=6.5331955

Batch 190670, train_perplexity=713.35803, train_loss=6.5699835

Batch 190680, train_perplexity=724.2194, train_loss=6.5850945

Batch 190690, train_perplexity=751.9011, train_loss=6.622605

Batch 190700, train_perplexity=828.828, train_loss=6.7200127

Batch 190710, train_perplexity=761.58057, train_loss=6.635396

Batch 190720, train_perplexity=675.7856, train_loss=6.515876

Batch 190730, train_perplexity=786.0968, train_loss=6.66708

Batch 190740, train_perplexity=724.1552, train_loss=6.5850058

Batch 190750, train_perplexity=661.8387, train_loss=6.495022

Batch 190760, train_perplexity=727.5263, train_loss=6.58965

Batch 190770, train_perplexity=752.52203, train_loss=6.6234303

Batch 190780, train_perplexity=716.5128, train_loss=6.574396

Batch 190790, train_perplexity=742.504, train_loss=6.6100283

Batch 190800, train_perplexity=749.23193, train_loss=6.6190486

Batch 190810, train_perplexity=731.3813, train_loss=6.594935

Batch 190820, train_perplexity=707.1326, train_loss=6.5612183

Batch 190830, train_perplexity=766.49603, train_loss=6.6418295

Batch 190840, train_perplexity=745.90076, train_loss=6.6145926

Batch 190850, train_perplexity=736.527, train_loss=6.601946

Batch 190860, train_perplexity=728.392, train_loss=6.5908394

Batch 190870, train_perplexity=741.3952, train_loss=6.608534

Batch 190880, train_perplexity=765.447, train_loss=6.64046

Batch 190890, train_perplexity=808.245, train_loss=6.694865

Batch 190900, train_perplexity=807.37177, train_loss=6.693784

Batch 190910, train_perplexity=759.69965, train_loss=6.632923

Batch 190920, train_perplexity=715.5254, train_loss=6.573017

Batch 190930, train_perplexity=747.5896, train_loss=6.616854

Batch 190940, train_perplexity=709.56256, train_loss=6.5646486

Batch 190950, train_perplexity=684.573, train_loss=6.5287952

Batch 190960, train_perplexity=748.6052, train_loss=6.6182117

Batch 190970, train_perplexity=671.45184, train_loss=6.5094423

Batch 190980, train_perplexity=785.0116, train_loss=6.6656985

Batch 190990, train_perplexity=792.27203, train_loss=6.674905

Batch 191000, train_perplexity=702.9317, train_loss=6.5552597

Batch 191010, train_perplexity=721.58966, train_loss=6.5814567

Batch 191020, train_perplexity=804.07153, train_loss=6.689688

Batch 191030, train_perplexity=671.9829, train_loss=6.510233

Batch 191040, train_perplexity=866.2657, train_loss=6.7641916

Batch 191050, train_perplexity=690.6182, train_loss=6.537587

Batch 191060, train_perplexity=773.23224, train_loss=6.6505795

Batch 191070, train_perplexity=732.50757, train_loss=6.5964737

Batch 191080, train_perplexity=731.7989, train_loss=6.5955057

Batch 191090, train_perplexity=712.30707, train_loss=6.568509

Batch 191100, train_perplexity=680.5618, train_loss=6.5229187

Batch 191110, train_perplexity=765.0828, train_loss=6.639984

Batch 191120, train_perplexity=757.93427, train_loss=6.6305966

Batch 191130, train_perplexity=732.72943, train_loss=6.5967765

Batch 191140, train_perplexity=738.0556, train_loss=6.604019

Batch 191150, train_perplexity=706.7268, train_loss=6.560644

Batch 191160, train_perplexity=771.4685, train_loss=6.648296

Batch 191170, train_perplexity=736.9675, train_loss=6.602544

Batch 191180, train_perplexity=799.0206, train_loss=6.683387

Batch 191190, train_perplexity=666.5319, train_loss=6.502088

Batch 191200, train_perplexity=763.661, train_loss=6.638124

Batch 191210, train_perplexity=720.8362, train_loss=6.580412

Batch 191220, train_perplexity=717.64496, train_loss=6.575975

Batch 191230, train_perplexity=749.6325, train_loss=6.619583

Batch 191240, train_perplexity=853.56696, train_loss=6.749424

Batch 191250, train_perplexity=734.3426, train_loss=6.5989757

Batch 191260, train_perplexity=757.76117, train_loss=6.630368

Batch 191270, train_perplexity=820.00275, train_loss=6.7093077

Batch 191280, train_perplexity=805.1408, train_loss=6.691017

Batch 191290, train_perplexity=661.15295, train_loss=6.493985

Batch 191300, train_perplexity=684.9707, train_loss=6.529376

Batch 191310, train_perplexity=843.1287, train_loss=6.7371197

Batch 191320, train_perplexity=737.7333, train_loss=6.6035824

Batch 191330, train_perplexity=806.7556, train_loss=6.693021

Batch 191340, train_perplexity=642.4924, train_loss=6.465355

Batch 191350, train_perplexity=742.9552, train_loss=6.6106358

Batch 191360, train_perplexity=786.96387, train_loss=6.6681824

Batch 191370, train_perplexity=708.3219, train_loss=6.5628986

Batch 191380, train_perplexity=774.92126, train_loss=6.6527615

Batch 191390, train_perplexity=765.90265, train_loss=6.641055

Batch 191400, train_perplexity=753.1337, train_loss=6.624243

Batch 191410, train_perplexity=707.70135, train_loss=6.562022

Batch 191420, train_perplexity=634.03467, train_loss=6.4521036

Batch 191430, train_perplexity=739.574, train_loss=6.6060743

Batch 191440, train_perplexity=747.75323, train_loss=6.617073

Batch 191450, train_perplexity=755.91595, train_loss=6.62793

Batch 191460, train_perplexity=713.0601, train_loss=6.569566

Batch 191470, train_perplexity=678.96643, train_loss=6.5205717

Batch 191480, train_perplexity=736.0095, train_loss=6.601243

Batch 191490, train_perplexity=772.78033, train_loss=6.649995

Batch 191500, train_perplexity=815.071, train_loss=6.703275

Batch 191510, train_perplexity=704.60187, train_loss=6.557633

Batch 191520, train_perplexity=787.2394, train_loss=6.6685324

Batch 191530, train_perplexity=687.5939, train_loss=6.5331984

Batch 191540, train_perplexity=779.3793, train_loss=6.658498

Batch 191550, train_perplexity=675.2235, train_loss=6.5150437
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 191560, train_perplexity=713.58734, train_loss=6.570305

Batch 191570, train_perplexity=847.5692, train_loss=6.7423725

Batch 191580, train_perplexity=745.4645, train_loss=6.6140075

Batch 191590, train_perplexity=740.2966, train_loss=6.607051

Batch 191600, train_perplexity=723.39734, train_loss=6.5839586

Batch 191610, train_perplexity=809.1558, train_loss=6.6959915

Batch 191620, train_perplexity=736.34827, train_loss=6.601703

Batch 191630, train_perplexity=774.0737, train_loss=6.651667

Batch 191640, train_perplexity=681.9688, train_loss=6.524984

Batch 191650, train_perplexity=692.6141, train_loss=6.540473

Batch 191660, train_perplexity=748.2269, train_loss=6.6177063

Batch 191670, train_perplexity=708.46783, train_loss=6.5631046

Batch 191680, train_perplexity=756.91394, train_loss=6.6292496

Batch 191690, train_perplexity=755.2238, train_loss=6.627014

Batch 191700, train_perplexity=776.46704, train_loss=6.654754

Batch 191710, train_perplexity=749.55817, train_loss=6.619484

Batch 191720, train_perplexity=756.2671, train_loss=6.6283946

Batch 191730, train_perplexity=688.2476, train_loss=6.5341487

Batch 191740, train_perplexity=755.655, train_loss=6.627585

Batch 191750, train_perplexity=756.473, train_loss=6.628667

Batch 191760, train_perplexity=658.95105, train_loss=6.490649

Batch 191770, train_perplexity=728.49207, train_loss=6.5909767

Batch 191780, train_perplexity=711.79846, train_loss=6.567795

Batch 191790, train_perplexity=752.62646, train_loss=6.623569

Batch 191800, train_perplexity=689.08203, train_loss=6.5353603

Batch 191810, train_perplexity=746.76373, train_loss=6.615749

Batch 191820, train_perplexity=754.0594, train_loss=6.625471

Batch 191830, train_perplexity=768.23883, train_loss=6.6441007

Batch 191840, train_perplexity=731.9064, train_loss=6.5956526

Batch 191850, train_perplexity=763.3683, train_loss=6.6377406

Batch 191860, train_perplexity=703.1375, train_loss=6.5555525

Batch 191870, train_perplexity=805.84595, train_loss=6.6918926

Batch 191880, train_perplexity=675.79974, train_loss=6.515897

Batch 191890, train_perplexity=665.5947, train_loss=6.500681

Batch 191900, train_perplexity=725.0771, train_loss=6.586278

Batch 191910, train_perplexity=696.21936, train_loss=6.545665

Batch 191920, train_perplexity=761.37146, train_loss=6.6351213

Batch 191930, train_perplexity=780.58954, train_loss=6.6600494

Batch 191940, train_perplexity=800.09045, train_loss=6.684725

Batch 191950, train_perplexity=659.5398, train_loss=6.4915423

Batch 191960, train_perplexity=764.503, train_loss=6.639226

Batch 191970, train_perplexity=730.07294, train_loss=6.5931444

Batch 191980, train_perplexity=675.7637, train_loss=6.5158434

Batch 191990, train_perplexity=740.72876, train_loss=6.6076345

Batch 192000, train_perplexity=799.30304, train_loss=6.68374

Batch 192010, train_perplexity=730.9535, train_loss=6.59435

Batch 192020, train_perplexity=691.80804, train_loss=6.5393085

Batch 192030, train_perplexity=752.3204, train_loss=6.6231623

Batch 192040, train_perplexity=824.2149, train_loss=6.7144313

Batch 192050, train_perplexity=741.3977, train_loss=6.608537

Batch 192060, train_perplexity=760.26855, train_loss=6.6336718

Batch 192070, train_perplexity=732.3218, train_loss=6.59622

Batch 192080, train_perplexity=851.0634, train_loss=6.7464867

Batch 192090, train_perplexity=752.3943, train_loss=6.6232605

Batch 192100, train_perplexity=737.86066, train_loss=6.603755

Batch 192110, train_perplexity=752.5773, train_loss=6.6235037

Batch 192120, train_perplexity=760.5746, train_loss=6.634074

Batch 192130, train_perplexity=641.3335, train_loss=6.4635496

Batch 192140, train_perplexity=731.90564, train_loss=6.5956516

Batch 192150, train_perplexity=732.201, train_loss=6.596055

Batch 192160, train_perplexity=738.1253, train_loss=6.6041136

Batch 192170, train_perplexity=701.0648, train_loss=6.5526004

Batch 192180, train_perplexity=765.7676, train_loss=6.6408787

Batch 192190, train_perplexity=701.4002, train_loss=6.5530787

Batch 192200, train_perplexity=716.1992, train_loss=6.5739584

Batch 192210, train_perplexity=720.71484, train_loss=6.5802436

Batch 192220, train_perplexity=672.7755, train_loss=6.5114117

Batch 192230, train_perplexity=811.71954, train_loss=6.699155

Batch 192240, train_perplexity=694.474, train_loss=6.5431547

Batch 192250, train_perplexity=716.3618, train_loss=6.5741854

Batch 192260, train_perplexity=739.085, train_loss=6.605413

Batch 192270, train_perplexity=741.71136, train_loss=6.60896

Batch 192280, train_perplexity=730.0217, train_loss=6.5930743

Batch 192290, train_perplexity=659.81726, train_loss=6.491963

Batch 192300, train_perplexity=736.3191, train_loss=6.6016636

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Batch 192310, train_perplexity=748.33966, train_loss=6.617857

Batch 192320, train_perplexity=811.9851, train_loss=6.699482

Batch 192330, train_perplexity=728.22253, train_loss=6.5906067

Batch 192340, train_perplexity=680.60046, train_loss=6.5229754

Batch 192350, train_perplexity=821.0525, train_loss=6.710587

Batch 192360, train_perplexity=710.6806, train_loss=6.566223

Batch 192370, train_perplexity=730.6866, train_loss=6.5939846

Batch 192380, train_perplexity=712.19055, train_loss=6.5683455

Batch 192390, train_perplexity=697.48004, train_loss=6.547474

Batch 192400, train_perplexity=716.1214, train_loss=6.5738497

Batch 192410, train_perplexity=774.59546, train_loss=6.652341

Batch 192420, train_perplexity=706.82117, train_loss=6.5607777

Batch 192430, train_perplexity=699.271, train_loss=6.5500383

Batch 192440, train_perplexity=688.269, train_loss=6.5341797

Batch 192450, train_perplexity=748.1684, train_loss=6.617628

Batch 192460, train_perplexity=727.46735, train_loss=6.589569

Batch 192470, train_perplexity=727.42224, train_loss=6.589507

Batch 192480, train_perplexity=729.72833, train_loss=6.5926723

Batch 192490, train_perplexity=691.35065, train_loss=6.538647

Batch 192500, train_perplexity=829.90686, train_loss=6.7213135

Batch 192510, train_perplexity=740.4325, train_loss=6.6072345

Batch 192520, train_perplexity=777.57965, train_loss=6.656186

Batch 192530, train_perplexity=605.87024, train_loss=6.406666

Batch 192540, train_perplexity=727.4663, train_loss=6.5895677

Batch 192550, train_perplexity=720.05084, train_loss=6.579322

Batch 192560, train_perplexity=765.21857, train_loss=6.6401615

Batch 192570, train_perplexity=768.59204, train_loss=6.6445603

Batch 192580, train_perplexity=849.0149, train_loss=6.7440767

Batch 192590, train_perplexity=769.4571, train_loss=6.645685

Batch 192600, train_perplexity=796.70184, train_loss=6.6804805

Batch 192610, train_perplexity=727.6377, train_loss=6.589803

Batch 192620, train_perplexity=742.8479, train_loss=6.6104913

Batch 192630, train_perplexity=776.3589, train_loss=6.654615

Batch 192640, train_perplexity=718.20123, train_loss=6.57675

Batch 192650, train_perplexity=743.983, train_loss=6.612018

Batch 192660, train_perplexity=685.1124, train_loss=6.529583

Batch 192670, train_perplexity=783.89246, train_loss=6.664272

Batch 192680, train_perplexity=781.344, train_loss=6.6610155

Batch 192690, train_perplexity=725.3862, train_loss=6.5867043

Batch 192700, train_perplexity=727.54987, train_loss=6.5896826

Batch 192710, train_perplexity=766.77203, train_loss=6.6421895

Batch 192720, train_perplexity=712.7644, train_loss=6.569151

Batch 192730, train_perplexity=673.50665, train_loss=6.512498

Batch 192740, train_perplexity=668.4375, train_loss=6.504943

Batch 192750, train_perplexity=773.61066, train_loss=6.6510687

Batch 192760, train_perplexity=708.42523, train_loss=6.5630445

Batch 192770, train_perplexity=706.96106, train_loss=6.5609756

Batch 192780, train_perplexity=689.224, train_loss=6.5355663

Batch 192790, train_perplexity=685.24054, train_loss=6.52977
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 192800, train_perplexity=680.5352, train_loss=6.5228796

Batch 192810, train_perplexity=692.39813, train_loss=6.540161

Batch 192820, train_perplexity=745.48755, train_loss=6.6140385

Batch 192830, train_perplexity=731.17694, train_loss=6.5946555

Batch 192840, train_perplexity=802.86005, train_loss=6.6881804

Batch 192850, train_perplexity=856.1946, train_loss=6.7524977

Batch 192860, train_perplexity=732.6438, train_loss=6.5966597

Batch 192870, train_perplexity=839.8241, train_loss=6.7331924

Batch 192880, train_perplexity=803.5146, train_loss=6.6889954

Batch 192890, train_perplexity=756.5113, train_loss=6.6287174

Batch 192900, train_perplexity=712.82764, train_loss=6.5692396

Batch 192910, train_perplexity=742.7529, train_loss=6.6103635

Batch 192920, train_perplexity=786.2486, train_loss=6.667273

Batch 192930, train_perplexity=702.74603, train_loss=6.5549955

Batch 192940, train_perplexity=714.6382, train_loss=6.5717764

Batch 192950, train_perplexity=751.6678, train_loss=6.6222944

Batch 192960, train_perplexity=783.3443, train_loss=6.6635723

Batch 192970, train_perplexity=712.7039, train_loss=6.569066

Batch 192980, train_perplexity=809.8989, train_loss=6.6969094

Batch 192990, train_perplexity=789.2307, train_loss=6.6710587

Batch 193000, train_perplexity=793.3385, train_loss=6.67625

Batch 193010, train_perplexity=750.3517, train_loss=6.620542

Batch 193020, train_perplexity=841.32715, train_loss=6.7349806

Batch 193030, train_perplexity=861.16, train_loss=6.7582803

Batch 193040, train_perplexity=725.8077, train_loss=6.587285

Batch 193050, train_perplexity=705.78284, train_loss=6.5593076

Batch 193060, train_perplexity=784.2641, train_loss=6.664746

Batch 193070, train_perplexity=684.4855, train_loss=6.5286674

Batch 193080, train_perplexity=762.94073, train_loss=6.6371803

Batch 193090, train_perplexity=774.3576, train_loss=6.652034

Batch 193100, train_perplexity=851.229, train_loss=6.746681

Batch 193110, train_perplexity=726.8044, train_loss=6.5886574

Batch 193120, train_perplexity=784.2652, train_loss=6.664747

Batch 193130, train_perplexity=709.23846, train_loss=6.564192

Batch 193140, train_perplexity=742.03894, train_loss=6.6094017

Batch 193150, train_perplexity=699.12494, train_loss=6.5498295

Batch 193160, train_perplexity=770.57367, train_loss=6.6471353

Batch 193170, train_perplexity=708.32153, train_loss=6.562898

Batch 193180, train_perplexity=724.10895, train_loss=6.584942

Batch 193190, train_perplexity=729.9671, train_loss=6.5929995

Batch 193200, train_perplexity=712.7933, train_loss=6.5691915

Batch 193210, train_perplexity=743.4145, train_loss=6.6112537

Batch 193220, train_perplexity=719.0706, train_loss=6.5779595

Batch 193230, train_perplexity=747.3469, train_loss=6.6165295

Batch 193240, train_perplexity=696.57465, train_loss=6.546175

Batch 193250, train_perplexity=707.16974, train_loss=6.5612707

Batch 193260, train_perplexity=770.9096, train_loss=6.647571

Batch 193270, train_perplexity=665.65625, train_loss=6.5007734

Batch 193280, train_perplexity=694.9954, train_loss=6.5439053

Batch 193290, train_perplexity=768.2348, train_loss=6.6440954

Batch 193300, train_perplexity=662.1335, train_loss=6.495467

Batch 193310, train_perplexity=740.00116, train_loss=6.606652

Batch 193320, train_perplexity=686.7537, train_loss=6.5319757

Batch 193330, train_perplexity=815.33997, train_loss=6.703605

Batch 193340, train_perplexity=692.7383, train_loss=6.5406523

Batch 193350, train_perplexity=749.6436, train_loss=6.619598

Batch 193360, train_perplexity=614.4214, train_loss=6.420681

Batch 193370, train_perplexity=732.16815, train_loss=6.59601

Batch 193380, train_perplexity=714.96606, train_loss=6.572235

Batch 193390, train_perplexity=770.6049, train_loss=6.647176

Batch 193400, train_perplexity=863.8946, train_loss=6.761451

Batch 193410, train_perplexity=732.6571, train_loss=6.596678

Batch 193420, train_perplexity=759.47144, train_loss=6.6326227

Batch 193430, train_perplexity=674.8086, train_loss=6.514429

Batch 193440, train_perplexity=741.9243, train_loss=6.609247

Batch 193450, train_perplexity=735.3156, train_loss=6.6003

Batch 193460, train_perplexity=716.3242, train_loss=6.574133

Batch 193470, train_perplexity=822.9139, train_loss=6.7128515

Batch 193480, train_perplexity=733.3268, train_loss=6.5975914

Batch 193490, train_perplexity=710.4848, train_loss=6.5659475

Batch 193500, train_perplexity=786.18604, train_loss=6.6671934

Batch 193510, train_perplexity=774.92426, train_loss=6.6527653

Batch 193520, train_perplexity=728.9855, train_loss=6.591654

Batch 193530, train_perplexity=751.39685, train_loss=6.621934

Batch 193540, train_perplexity=752.51697, train_loss=6.6234236

Batch 193550, train_perplexity=728.19165, train_loss=6.5905643

Batch 193560, train_perplexity=703.9762, train_loss=6.5567446

Batch 193570, train_perplexity=807.4276, train_loss=6.6938534

Batch 193580, train_perplexity=666.6308, train_loss=6.5022364

Batch 193590, train_perplexity=739.4343, train_loss=6.6058855

Batch 193600, train_perplexity=719.6445, train_loss=6.5787573

Batch 193610, train_perplexity=701.98376, train_loss=6.5539103

Batch 193620, train_perplexity=685.73145, train_loss=6.530486

Batch 193630, train_perplexity=763.5463, train_loss=6.637974

Batch 193640, train_perplexity=672.9584, train_loss=6.5116835

Batch 193650, train_perplexity=739.238, train_loss=6.60562

Batch 193660, train_perplexity=703.47925, train_loss=6.5560384

Batch 193670, train_perplexity=730.3661, train_loss=6.593546

Batch 193680, train_perplexity=738.2647, train_loss=6.6043024

Batch 193690, train_perplexity=719.59503, train_loss=6.5786886

Batch 193700, train_perplexity=751.9879, train_loss=6.6227202

Batch 193710, train_perplexity=744.18304, train_loss=6.612287

Batch 193720, train_perplexity=738.2432, train_loss=6.6042733

Batch 193730, train_perplexity=739.8519, train_loss=6.60645

Batch 193740, train_perplexity=819.7889, train_loss=6.709047

Batch 193750, train_perplexity=795.1894, train_loss=6.6785803

Batch 193760, train_perplexity=697.9931, train_loss=6.548209

Batch 193770, train_perplexity=687.1321, train_loss=6.5325265

Batch 193780, train_perplexity=758.0069, train_loss=6.6306925

Batch 193790, train_perplexity=833.27264, train_loss=6.725361

Batch 193800, train_perplexity=705.1578, train_loss=6.5584216

Batch 193810, train_perplexity=730.9392, train_loss=6.5943303

Batch 193820, train_perplexity=712.76166, train_loss=6.569147

Batch 193830, train_perplexity=775.89886, train_loss=6.654022

Batch 193840, train_perplexity=689.3019, train_loss=6.5356793

Batch 193850, train_perplexity=865.41516, train_loss=6.7632093

Batch 193860, train_perplexity=776.0506, train_loss=6.6542177

Batch 193870, train_perplexity=751.86566, train_loss=6.6225576

Batch 193880, train_perplexity=733.459, train_loss=6.5977716

Batch 193890, train_perplexity=771.2011, train_loss=6.647949

Batch 193900, train_perplexity=796.1743, train_loss=6.679818

Batch 193910, train_perplexity=747.7818, train_loss=6.617111

Batch 193920, train_perplexity=659.6131, train_loss=6.4916534

Batch 193930, train_perplexity=673.99756, train_loss=6.5132265

Batch 193940, train_perplexity=743.70306, train_loss=6.611642

Batch 193950, train_perplexity=690.06616, train_loss=6.5367875

Batch 193960, train_perplexity=687.9294, train_loss=6.533686

Batch 193970, train_perplexity=690.4796, train_loss=6.5373864

Batch 193980, train_perplexity=687.2825, train_loss=6.5327454

Batch 193990, train_perplexity=803.1591, train_loss=6.688553

Batch 194000, train_perplexity=786.09155, train_loss=6.6670732

Batch 194010, train_perplexity=772.85406, train_loss=6.65009

Batch 194020, train_perplexity=717.80615, train_loss=6.5761995

Batch 194030, train_perplexity=714.5329, train_loss=6.571629

Batch 194040, train_perplexity=739.51825, train_loss=6.605999

Batch 194050, train_perplexity=749.7262, train_loss=6.619708

Batch 194060, train_perplexity=704.19916, train_loss=6.557061

Batch 194070, train_perplexity=734.04114, train_loss=6.598565

Batch 194080, train_perplexity=710.09393, train_loss=6.5653973

Batch 194090, train_perplexity=718.15533, train_loss=6.576686

Batch 194100, train_perplexity=690.9789, train_loss=6.5381093
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 194110, train_perplexity=804.6595, train_loss=6.690419

Batch 194120, train_perplexity=739.8943, train_loss=6.6065073

Batch 194130, train_perplexity=759.0768, train_loss=6.632103

Batch 194140, train_perplexity=679.9073, train_loss=6.5219564

Batch 194150, train_perplexity=635.7024, train_loss=6.4547305

Batch 194160, train_perplexity=700.04333, train_loss=6.551142

Batch 194170, train_perplexity=719.52094, train_loss=6.5785856

Batch 194180, train_perplexity=806.44336, train_loss=6.6926336

Batch 194190, train_perplexity=762.53845, train_loss=6.636653

Batch 194200, train_perplexity=784.61945, train_loss=6.665199

Batch 194210, train_perplexity=773.86816, train_loss=6.6514015

Batch 194220, train_perplexity=751.5108, train_loss=6.6220856

Batch 194230, train_perplexity=668.19147, train_loss=6.504575

Batch 194240, train_perplexity=844.2673, train_loss=6.738469

Batch 194250, train_perplexity=680.77734, train_loss=6.5232353

Batch 194260, train_perplexity=700.69086, train_loss=6.552067

Batch 194270, train_perplexity=756.95294, train_loss=6.629301

Batch 194280, train_perplexity=714.49475, train_loss=6.5715756

Batch 194290, train_perplexity=625.6036, train_loss=6.438717

Batch 194300, train_perplexity=826.91736, train_loss=6.717705

Batch 194310, train_perplexity=822.122, train_loss=6.711889

Batch 194320, train_perplexity=773.0593, train_loss=6.650356

Batch 194330, train_perplexity=724.2112, train_loss=6.585083

Batch 194340, train_perplexity=748.61163, train_loss=6.6182203

Batch 194350, train_perplexity=701.1441, train_loss=6.5527134

Batch 194360, train_perplexity=776.6059, train_loss=6.654933

Batch 194370, train_perplexity=777.9995, train_loss=6.656726

Batch 194380, train_perplexity=755.19934, train_loss=6.6269817

Batch 194390, train_perplexity=670.744, train_loss=6.5083876

Batch 194400, train_perplexity=643.92224, train_loss=6.467578

Batch 194410, train_perplexity=732.78046, train_loss=6.596846

Batch 194420, train_perplexity=717.61346, train_loss=6.575931

Batch 194430, train_perplexity=658.40265, train_loss=6.4898167

Batch 194440, train_perplexity=740.1628, train_loss=6.60687

Batch 194450, train_perplexity=803.3131, train_loss=6.6887445

Batch 194460, train_perplexity=739.45374, train_loss=6.6059117

Batch 194470, train_perplexity=775.0861, train_loss=6.652974

Batch 194480, train_perplexity=717.3449, train_loss=6.5755568

Batch 194490, train_perplexity=691.4766, train_loss=6.5388293

Batch 194500, train_perplexity=736.84454, train_loss=6.602377

Batch 194510, train_perplexity=739.0995, train_loss=6.6054325

Batch 194520, train_perplexity=787.135, train_loss=6.6684

Batch 194530, train_perplexity=755.726, train_loss=6.627679

Batch 194540, train_perplexity=647.3209, train_loss=6.472842

Batch 194550, train_perplexity=741.7863, train_loss=6.6090612

Batch 194560, train_perplexity=793.61395, train_loss=6.676597

Batch 194570, train_perplexity=759.62067, train_loss=6.632819

Batch 194580, train_perplexity=712.3825, train_loss=6.568615

Batch 194590, train_perplexity=661.60486, train_loss=6.4946685

Batch 194600, train_perplexity=770.311, train_loss=6.6467943

Batch 194610, train_perplexity=707.69257, train_loss=6.56201

Batch 194620, train_perplexity=744.133, train_loss=6.61222

Batch 194630, train_perplexity=686.1779, train_loss=6.531137

Batch 194640, train_perplexity=698.56445, train_loss=6.5490274

Batch 194650, train_perplexity=725.8194, train_loss=6.5873013

Batch 194660, train_perplexity=709.70667, train_loss=6.5648518

Batch 194670, train_perplexity=710.96094, train_loss=6.5666175

Batch 194680, train_perplexity=743.7414, train_loss=6.6116934

Batch 194690, train_perplexity=800.0058, train_loss=6.684619

Batch 194700, train_perplexity=701.85956, train_loss=6.5537333

Batch 194710, train_perplexity=697.7961, train_loss=6.547927

Batch 194720, train_perplexity=599.25323, train_loss=6.3956842

Batch 194730, train_perplexity=782.0723, train_loss=6.6619473

Batch 194740, train_perplexity=760.13336, train_loss=6.633494

Batch 194750, train_perplexity=715.2757, train_loss=6.572668

Batch 194760, train_perplexity=735.8207, train_loss=6.6009865

Batch 194770, train_perplexity=789.4234, train_loss=6.671303

Batch 194780, train_perplexity=736.1885, train_loss=6.601486

Batch 194790, train_perplexity=713.1472, train_loss=6.569688

Batch 194800, train_perplexity=678.17017, train_loss=6.519398

Batch 194810, train_perplexity=687.6103, train_loss=6.533222

Batch 194820, train_perplexity=758.0503, train_loss=6.6307497

Batch 194830, train_perplexity=761.2829, train_loss=6.635005

Batch 194840, train_perplexity=729.6591, train_loss=6.5925775

Batch 194850, train_perplexity=738.66504, train_loss=6.6048446

Batch 194860, train_perplexity=746.2967, train_loss=6.6151233

Batch 194870, train_perplexity=674.6413, train_loss=6.514181

Batch 194880, train_perplexity=732.8632, train_loss=6.596959

Batch 194890, train_perplexity=726.32697, train_loss=6.5880003

Batch 194900, train_perplexity=667.30884, train_loss=6.503253

Batch 194910, train_perplexity=748.8358, train_loss=6.61852

Batch 194920, train_perplexity=714.72034, train_loss=6.5718913

Batch 194930, train_perplexity=683.2786, train_loss=6.5269027

Batch 194940, train_perplexity=673.81824, train_loss=6.5129604

Batch 194950, train_perplexity=675.7727, train_loss=6.5158567

Batch 194960, train_perplexity=788.234, train_loss=6.669795

Batch 194970, train_perplexity=743.55664, train_loss=6.611445

Batch 194980, train_perplexity=744.19904, train_loss=6.6123085

Batch 194990, train_perplexity=735.95544, train_loss=6.6011696

Batch 195000, train_perplexity=760.2048, train_loss=6.633588

Batch 195010, train_perplexity=669.1911, train_loss=6.5060697

Batch 195020, train_perplexity=708.73846, train_loss=6.5634866

Batch 195030, train_perplexity=729.07794, train_loss=6.5917807

Batch 195040, train_perplexity=739.29083, train_loss=6.6056914

Batch 195050, train_perplexity=719.12445, train_loss=6.5780344

Batch 195060, train_perplexity=776.8129, train_loss=6.6551995

Batch 195070, train_perplexity=697.1891, train_loss=6.5470567

Batch 195080, train_perplexity=761.95764, train_loss=6.635891

Batch 195090, train_perplexity=745.99713, train_loss=6.614722

Batch 195100, train_perplexity=817.2695, train_loss=6.705969

Batch 195110, train_perplexity=669.19653, train_loss=6.506078

Batch 195120, train_perplexity=756.5755, train_loss=6.6288023

Batch 195130, train_perplexity=718.6925, train_loss=6.5774336

Batch 195140, train_perplexity=681.17224, train_loss=6.523815

Batch 195150, train_perplexity=778.7901, train_loss=6.6577415

Batch 195160, train_perplexity=713.99646, train_loss=6.570878

Batch 195170, train_perplexity=768.2692, train_loss=6.6441402

Batch 195180, train_perplexity=725.6575, train_loss=6.587078

Batch 195190, train_perplexity=784.8615, train_loss=6.6655073

Batch 195200, train_perplexity=717.27515, train_loss=6.5754595

Batch 195210, train_perplexity=730.5667, train_loss=6.5938206

Batch 195220, train_perplexity=752.31537, train_loss=6.6231556

Batch 195230, train_perplexity=779.56586, train_loss=6.658737

Batch 195240, train_perplexity=765.7208, train_loss=6.6408176

Batch 195250, train_perplexity=788.6709, train_loss=6.670349

Batch 195260, train_perplexity=668.75214, train_loss=6.5054135

Batch 195270, train_perplexity=756.6736, train_loss=6.628932

Batch 195280, train_perplexity=797.6982, train_loss=6.6817303

Batch 195290, train_perplexity=721.0583, train_loss=6.58072

Batch 195300, train_perplexity=703.2569, train_loss=6.555722

Batch 195310, train_perplexity=720.96063, train_loss=6.5805845

Batch 195320, train_perplexity=750.3335, train_loss=6.6205177

Batch 195330, train_perplexity=786.65924, train_loss=6.667795

Batch 195340, train_perplexity=784.57007, train_loss=6.665136

Batch 195350, train_perplexity=732.4, train_loss=6.596327

Batch 195360, train_perplexity=664.82837, train_loss=6.499529

Batch 195370, train_perplexity=738.633, train_loss=6.604801

Batch 195380, train_perplexity=746.7278, train_loss=6.6157007

Batch 195390, train_perplexity=772.1652, train_loss=6.6491985

Batch 195400, train_perplexity=754.2514, train_loss=6.6257257

Batch 195410, train_perplexity=738.571, train_loss=6.6047173
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 195420, train_perplexity=741.228, train_loss=6.6083083

Batch 195430, train_perplexity=815.8448, train_loss=6.704224

Batch 195440, train_perplexity=702.78955, train_loss=6.5550575

Batch 195450, train_perplexity=773.708, train_loss=6.6511946

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Batch 195460, train_perplexity=773.146, train_loss=6.650468

Batch 195470, train_perplexity=856.6136, train_loss=6.752987

Batch 195480, train_perplexity=675.61676, train_loss=6.515626

Batch 195490, train_perplexity=775.66583, train_loss=6.653722

Batch 195500, train_perplexity=728.2128, train_loss=6.5905933

Batch 195510, train_perplexity=728.10516, train_loss=6.5904455

Batch 195520, train_perplexity=733.51, train_loss=6.5978413

Batch 195530, train_perplexity=742.1868, train_loss=6.609601

Batch 195540, train_perplexity=662.6275, train_loss=6.496213

Batch 195550, train_perplexity=727.49927, train_loss=6.589613

Batch 195560, train_perplexity=759.89453, train_loss=6.6331797

Batch 195570, train_perplexity=650.2357, train_loss=6.477335

Batch 195580, train_perplexity=742.5359, train_loss=6.610071

Batch 195590, train_perplexity=727.6807, train_loss=6.5898623

Batch 195600, train_perplexity=747.8, train_loss=6.6171355

Batch 195610, train_perplexity=702.16187, train_loss=6.554164

Batch 195620, train_perplexity=745.06396, train_loss=6.61347

Batch 195630, train_perplexity=741.7563, train_loss=6.6090207

Batch 195640, train_perplexity=780.22784, train_loss=6.659586

Batch 195650, train_perplexity=732.9482, train_loss=6.597075

Batch 195660, train_perplexity=747.2254, train_loss=6.616367

Batch 195670, train_perplexity=742.33765, train_loss=6.609804

Batch 195680, train_perplexity=748.91724, train_loss=6.6186285

Batch 195690, train_perplexity=734.2344, train_loss=6.5988283

Batch 195700, train_perplexity=698.6164, train_loss=6.549102

Batch 195710, train_perplexity=733.4289, train_loss=6.5977306

Batch 195720, train_perplexity=763.57104, train_loss=6.638006

Batch 195730, train_perplexity=731.65375, train_loss=6.5953074

Batch 195740, train_perplexity=726.9437, train_loss=6.588849

Batch 195750, train_perplexity=710.04114, train_loss=6.565323

Batch 195760, train_perplexity=707.53064, train_loss=6.561781

Batch 195770, train_perplexity=725.9821, train_loss=6.5875254

Batch 195780, train_perplexity=783.9078, train_loss=6.6642914

Batch 195790, train_perplexity=703.50305, train_loss=6.556072

Batch 195800, train_perplexity=650.2252, train_loss=6.477319

Batch 195810, train_perplexity=649.6122, train_loss=6.4763756

Batch 195820, train_perplexity=677.2876, train_loss=6.518096

Batch 195830, train_perplexity=639.2004, train_loss=6.460218

Batch 195840, train_perplexity=771.4042, train_loss=6.6482124

Batch 195850, train_perplexity=743.31384, train_loss=6.6111183

Batch 195860, train_perplexity=785.64185, train_loss=6.666501

Batch 195870, train_perplexity=724.28644, train_loss=6.585187

Batch 195880, train_perplexity=725.8007, train_loss=6.5872755

Batch 195890, train_perplexity=741.96533, train_loss=6.6093025

Batch 195900, train_perplexity=785.15314, train_loss=6.665879

Batch 195910, train_perplexity=705.6095, train_loss=6.559062

Batch 195920, train_perplexity=780.0162, train_loss=6.6593146

Batch 195930, train_perplexity=749.5028, train_loss=6.61941

Batch 195940, train_perplexity=680.27924, train_loss=6.5225034

Batch 195950, train_perplexity=638.0261, train_loss=6.4583793

Batch 195960, train_perplexity=757.2663, train_loss=6.629715

Batch 195970, train_perplexity=729.60486, train_loss=6.592503

Batch 195980, train_perplexity=787.54126, train_loss=6.6689157

Batch 195990, train_perplexity=741.7998, train_loss=6.6090794

Batch 196000, train_perplexity=719.25165, train_loss=6.5782113

Batch 196010, train_perplexity=778.3149, train_loss=6.657131

Batch 196020, train_perplexity=837.4216, train_loss=6.7303276

Batch 196030, train_perplexity=747.898, train_loss=6.6172667

Batch 196040, train_perplexity=746.97284, train_loss=6.616029

Batch 196050, train_perplexity=675.9435, train_loss=6.5161095

Batch 196060, train_perplexity=766.84406, train_loss=6.6422834

Batch 196070, train_perplexity=811.0773, train_loss=6.6983633

Batch 196080, train_perplexity=667.9242, train_loss=6.5041747

Batch 196090, train_perplexity=730.4528, train_loss=6.5936646

Batch 196100, train_perplexity=710.0682, train_loss=6.565361

Batch 196110, train_perplexity=803.82886, train_loss=6.6893864

Batch 196120, train_perplexity=798.5743, train_loss=6.682828

Batch 196130, train_perplexity=724.5345, train_loss=6.5855293

Batch 196140, train_perplexity=755.36066, train_loss=6.6271954

Batch 196150, train_perplexity=745.20966, train_loss=6.6136656

Batch 196160, train_perplexity=718.60614, train_loss=6.5773134

Batch 196170, train_perplexity=827.9432, train_loss=6.7189445

Batch 196180, train_perplexity=710.5159, train_loss=6.5659914

Batch 196190, train_perplexity=700.0547, train_loss=6.5511584

Batch 196200, train_perplexity=750.0176, train_loss=6.6200967

Batch 196210, train_perplexity=701.07086, train_loss=6.552609

Batch 196220, train_perplexity=733.68494, train_loss=6.5980797

Batch 196230, train_perplexity=704.7346, train_loss=6.5578213

Batch 196240, train_perplexity=744.6097, train_loss=6.61286

Batch 196250, train_perplexity=728.9368, train_loss=6.591587

Batch 196260, train_perplexity=739.5585, train_loss=6.6060534

Batch 196270, train_perplexity=702.868, train_loss=6.555169

Batch 196280, train_perplexity=766.14154, train_loss=6.641367

Batch 196290, train_perplexity=686.23, train_loss=6.531213

Batch 196300, train_perplexity=783.71716, train_loss=6.664048

Batch 196310, train_perplexity=805.02484, train_loss=6.690873

Batch 196320, train_perplexity=690.46606, train_loss=6.537367

Batch 196330, train_perplexity=706.0383, train_loss=6.5596695

Batch 196340, train_perplexity=711.8426, train_loss=6.567857

Batch 196350, train_perplexity=731.9273, train_loss=6.595681

Batch 196360, train_perplexity=729.48553, train_loss=6.5923395

Batch 196370, train_perplexity=734.67706, train_loss=6.599431

Batch 196380, train_perplexity=626.1999, train_loss=6.4396696

Batch 196390, train_perplexity=725.3779, train_loss=6.586693

Batch 196400, train_perplexity=787.678, train_loss=6.6690893

Batch 196410, train_perplexity=672.1316, train_loss=6.510454

Batch 196420, train_perplexity=811.7907, train_loss=6.6992426

Batch 196430, train_perplexity=701.04645, train_loss=6.552574

Batch 196440, train_perplexity=747.575, train_loss=6.6168346

Batch 196450, train_perplexity=704.45807, train_loss=6.557429

Batch 196460, train_perplexity=654.66785, train_loss=6.484128

Batch 196470, train_perplexity=720.7001, train_loss=6.580223

Batch 196480, train_perplexity=728.5011, train_loss=6.590989

Batch 196490, train_perplexity=722.6764, train_loss=6.5829616

Batch 196500, train_perplexity=741.53314, train_loss=6.60872

Batch 196510, train_perplexity=733.81683, train_loss=6.5982594

Batch 196520, train_perplexity=749.65753, train_loss=6.6196165

Batch 196530, train_perplexity=734.8803, train_loss=6.5997076

Batch 196540, train_perplexity=683.45355, train_loss=6.5271587

Batch 196550, train_perplexity=775.2968, train_loss=6.653246

Batch 196560, train_perplexity=698.9903, train_loss=6.549637

Batch 196570, train_perplexity=710.662, train_loss=6.566197

Batch 196580, train_perplexity=756.0814, train_loss=6.628149

Batch 196590, train_perplexity=687.65845, train_loss=6.5332923

Batch 196600, train_perplexity=690.8369, train_loss=6.537904

Batch 196610, train_perplexity=750.3818, train_loss=6.620582

Batch 196620, train_perplexity=753.3463, train_loss=6.624525

Batch 196630, train_perplexity=722.38806, train_loss=6.5825624

Batch 196640, train_perplexity=698.46985, train_loss=6.548892

Batch 196650, train_perplexity=744.31226, train_loss=6.6124606

Batch 196660, train_perplexity=785.9851, train_loss=6.666938
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 196670, train_perplexity=811.57477, train_loss=6.6989765

Batch 196680, train_perplexity=716.049, train_loss=6.5737486

Batch 196690, train_perplexity=717.83386, train_loss=6.576238

Batch 196700, train_perplexity=766.65906, train_loss=6.642042

Batch 196710, train_perplexity=681.84357, train_loss=6.5248003

Batch 196720, train_perplexity=679.93774, train_loss=6.5220013

Batch 196730, train_perplexity=808.4728, train_loss=6.695147

Batch 196740, train_perplexity=742.0396, train_loss=6.6094027

Batch 196750, train_perplexity=670.96027, train_loss=6.50871

Batch 196760, train_perplexity=771.35046, train_loss=6.648143

Batch 196770, train_perplexity=700.8459, train_loss=6.552288

Batch 196780, train_perplexity=771.79266, train_loss=6.648716

Batch 196790, train_perplexity=722.0474, train_loss=6.582091

Batch 196800, train_perplexity=688.45935, train_loss=6.5344563

Batch 196810, train_perplexity=714.6566, train_loss=6.571802

Batch 196820, train_perplexity=590.2373, train_loss=6.3805246

Batch 196830, train_perplexity=759.47144, train_loss=6.6326227

Batch 196840, train_perplexity=785.43774, train_loss=6.666241

Batch 196850, train_perplexity=819.33984, train_loss=6.708499

Batch 196860, train_perplexity=765.2923, train_loss=6.640258

Batch 196870, train_perplexity=725.37933, train_loss=6.5866947

Batch 196880, train_perplexity=837.93805, train_loss=6.730944

Batch 196890, train_perplexity=649.71136, train_loss=6.476528

Batch 196900, train_perplexity=663.20154, train_loss=6.497079

Batch 196910, train_perplexity=665.23236, train_loss=6.5001364

Batch 196920, train_perplexity=746.7495, train_loss=6.61573

Batch 196930, train_perplexity=683.37537, train_loss=6.5270443

Batch 196940, train_perplexity=732.231, train_loss=6.596096

Batch 196950, train_perplexity=795.01117, train_loss=6.678356

Batch 196960, train_perplexity=791.012, train_loss=6.673313

Batch 196970, train_perplexity=701.1498, train_loss=6.5527215

Batch 196980, train_perplexity=811.01807, train_loss=6.6982903

Batch 196990, train_perplexity=673.61456, train_loss=6.512658

Batch 197000, train_perplexity=710.2101, train_loss=6.565561

Batch 197010, train_perplexity=700.9308, train_loss=6.552409

Batch 197020, train_perplexity=657.57776, train_loss=6.488563

Batch 197030, train_perplexity=689.69806, train_loss=6.536254

Batch 197040, train_perplexity=770.6233, train_loss=6.6471996

Batch 197050, train_perplexity=649.8923, train_loss=6.4768066

Batch 197060, train_perplexity=716.9578, train_loss=6.575017

Batch 197070, train_perplexity=684.7519, train_loss=6.5290565

Batch 197080, train_perplexity=755.64386, train_loss=6.62757

Batch 197090, train_perplexity=700.5339, train_loss=6.5518427

Batch 197100, train_perplexity=772.00287, train_loss=6.6489882

Batch 197110, train_perplexity=672.71967, train_loss=6.5113287

Batch 197120, train_perplexity=777.45544, train_loss=6.6560264

Batch 197130, train_perplexity=673.99567, train_loss=6.5132236

Batch 197140, train_perplexity=730.76324, train_loss=6.5940895

Batch 197150, train_perplexity=676.29553, train_loss=6.51663

Batch 197160, train_perplexity=702.1913, train_loss=6.554206

Batch 197170, train_perplexity=704.7679, train_loss=6.5578685

Batch 197180, train_perplexity=725.37756, train_loss=6.5866923

Batch 197190, train_perplexity=723.8818, train_loss=6.584628

Batch 197200, train_perplexity=667.2627, train_loss=6.503184

Batch 197210, train_perplexity=720.1796, train_loss=6.5795007

Batch 197220, train_perplexity=775.5168, train_loss=6.6535296

Batch 197230, train_perplexity=725.30115, train_loss=6.586587

Batch 197240, train_perplexity=718.6048, train_loss=6.5773115

Batch 197250, train_perplexity=705.482, train_loss=6.5588813

Batch 197260, train_perplexity=742.4268, train_loss=6.6099243

Batch 197270, train_perplexity=694.4568, train_loss=6.54313

Batch 197280, train_perplexity=736.5804, train_loss=6.6020184

Batch 197290, train_perplexity=746.5291, train_loss=6.6154346

Batch 197300, train_perplexity=741.9427, train_loss=6.609272

Batch 197310, train_perplexity=767.3993, train_loss=6.6430073

Batch 197320, train_perplexity=755.1345, train_loss=6.626896

Batch 197330, train_perplexity=785.56696, train_loss=6.6664057

Batch 197340, train_perplexity=734.1024, train_loss=6.5986485

Batch 197350, train_perplexity=731.4797, train_loss=6.5950694

Batch 197360, train_perplexity=694.0238, train_loss=6.542506

Batch 197370, train_perplexity=699.70795, train_loss=6.550663

Batch 197380, train_perplexity=697.6883, train_loss=6.5477724

Batch 197390, train_perplexity=732.2736, train_loss=6.596154

Batch 197400, train_perplexity=752.169, train_loss=6.622961

Batch 197410, train_perplexity=804.8752, train_loss=6.690687

Batch 197420, train_perplexity=696.7594, train_loss=6.54644

Batch 197430, train_perplexity=721.1972, train_loss=6.5809126

Batch 197440, train_perplexity=709.2094, train_loss=6.564151

Batch 197450, train_perplexity=699.2293, train_loss=6.5499787

Batch 197460, train_perplexity=742.00885, train_loss=6.609361

Batch 197470, train_perplexity=675.0136, train_loss=6.514733

Batch 197480, train_perplexity=730.1882, train_loss=6.5933022

Batch 197490, train_perplexity=760.12396, train_loss=6.6334815

Batch 197500, train_perplexity=712.54285, train_loss=6.56884

Batch 197510, train_perplexity=774.3015, train_loss=6.6519613

Batch 197520, train_perplexity=742.1826, train_loss=6.6095953

Batch 197530, train_perplexity=712.7311, train_loss=6.569104

Batch 197540, train_perplexity=660.6323, train_loss=6.4931974

Batch 197550, train_perplexity=771.5414, train_loss=6.6483903

Batch 197560, train_perplexity=702.4592, train_loss=6.5545874

Batch 197570, train_perplexity=693.5229, train_loss=6.5417843

Batch 197580, train_perplexity=694.31836, train_loss=6.5429306

Batch 197590, train_perplexity=774.4828, train_loss=6.6521955

Batch 197600, train_perplexity=738.2379, train_loss=6.604266

Batch 197610, train_perplexity=704.70905, train_loss=6.557785

Batch 197620, train_perplexity=785.45905, train_loss=6.6662683

Batch 197630, train_perplexity=744.50287, train_loss=6.6127167

Batch 197640, train_perplexity=728.6456, train_loss=6.5911875

Batch 197650, train_perplexity=763.75244, train_loss=6.6382437

Batch 197660, train_perplexity=819.71735, train_loss=6.7089596

Batch 197670, train_perplexity=658.24634, train_loss=6.489579

Batch 197680, train_perplexity=674.2361, train_loss=6.5135803

Batch 197690, train_perplexity=766.7051, train_loss=6.6421022

Batch 197700, train_perplexity=680.1414, train_loss=6.5223007

Batch 197710, train_perplexity=684.61707, train_loss=6.5288596

Batch 197720, train_perplexity=783.2412, train_loss=6.6634407

Batch 197730, train_perplexity=699.212, train_loss=6.549954

Batch 197740, train_perplexity=677.16064, train_loss=6.5179086

Batch 197750, train_perplexity=746.3786, train_loss=6.615233

Batch 197760, train_perplexity=662.4423, train_loss=6.4959335

Batch 197770, train_perplexity=767.55853, train_loss=6.6432147

Batch 197780, train_perplexity=690.2399, train_loss=6.5370393

Batch 197790, train_perplexity=721.83813, train_loss=6.581801

Batch 197800, train_perplexity=686.6856, train_loss=6.5318766

Batch 197810, train_perplexity=777.5619, train_loss=6.656163

Batch 197820, train_perplexity=662.51056, train_loss=6.4960365

Batch 197830, train_perplexity=824.20667, train_loss=6.7144213

Batch 197840, train_perplexity=727.0935, train_loss=6.589055

Batch 197850, train_perplexity=710.4123, train_loss=6.5658455

Batch 197860, train_perplexity=697.84863, train_loss=6.5480022

Batch 197870, train_perplexity=727.9191, train_loss=6.59019

Batch 197880, train_perplexity=705.4548, train_loss=6.5588427

Batch 197890, train_perplexity=774.18555, train_loss=6.6518116

Batch 197900, train_perplexity=749.76196, train_loss=6.6197557

Batch 197910, train_perplexity=784.8181, train_loss=6.665452

Batch 197920, train_perplexity=706.70154, train_loss=6.5606084

Batch 197930, train_perplexity=686.3023, train_loss=6.531318

Batch 197940, train_perplexity=684.61505, train_loss=6.5288568

Batch 197950, train_perplexity=738.0225, train_loss=6.6039743

Batch 197960, train_perplexity=675.50977, train_loss=6.5154676

Batch 197970, train_perplexity=710.0567, train_loss=6.565345
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 197980, train_perplexity=712.7844, train_loss=6.569179

Batch 197990, train_perplexity=647.4537, train_loss=6.4730473

Batch 198000, train_perplexity=701.9486, train_loss=6.55386

Batch 198010, train_perplexity=722.1008, train_loss=6.582165

Batch 198020, train_perplexity=758.94507, train_loss=6.6319294

Batch 198030, train_perplexity=738.46606, train_loss=6.604575

Batch 198040, train_perplexity=721.25977, train_loss=6.5809994

Batch 198050, train_perplexity=748.98724, train_loss=6.618722

Batch 198060, train_perplexity=705.58057, train_loss=6.559021

Batch 198070, train_perplexity=662.5934, train_loss=6.4961615

Batch 198080, train_perplexity=769.0015, train_loss=6.645093

Batch 198090, train_perplexity=680.7426, train_loss=6.5231843

Batch 198100, train_perplexity=695.7886, train_loss=6.545046

Batch 198110, train_perplexity=707.8505, train_loss=6.562233

Batch 198120, train_perplexity=751.0379, train_loss=6.621456

Batch 198130, train_perplexity=807.8085, train_loss=6.694325

Batch 198140, train_perplexity=766.8389, train_loss=6.642277

Batch 198150, train_perplexity=684.7708, train_loss=6.529084

Batch 198160, train_perplexity=704.4104, train_loss=6.557361

Batch 198170, train_perplexity=665.8245, train_loss=6.501026

Batch 198180, train_perplexity=728.0781, train_loss=6.5904083

Batch 198190, train_perplexity=704.19244, train_loss=6.5570517

Batch 198200, train_perplexity=741.3387, train_loss=6.6084576

Batch 198210, train_perplexity=689.80695, train_loss=6.536412

Batch 198220, train_perplexity=663.41626, train_loss=6.4974027

Batch 198230, train_perplexity=732.8709, train_loss=6.5969696

Batch 198240, train_perplexity=732.437, train_loss=6.5963774

Batch 198250, train_perplexity=716.4247, train_loss=6.574273

Batch 198260, train_perplexity=730.16, train_loss=6.5932636

Batch 198270, train_perplexity=742.6492, train_loss=6.610224

Batch 198280, train_perplexity=704.71643, train_loss=6.5577955

Batch 198290, train_perplexity=735.1768, train_loss=6.600111

Batch 198300, train_perplexity=678.96063, train_loss=6.520563

Batch 198310, train_perplexity=729.70746, train_loss=6.5926437

Batch 198320, train_perplexity=663.2765, train_loss=6.497192

Batch 198330, train_perplexity=766.5786, train_loss=6.6419373

Batch 198340, train_perplexity=753.25256, train_loss=6.6244006

Batch 198350, train_perplexity=692.17694, train_loss=6.5398417

Batch 198360, train_perplexity=687.6034, train_loss=6.533212

Batch 198370, train_perplexity=695.43066, train_loss=6.5445313

Batch 198380, train_perplexity=821.85156, train_loss=6.71156

Batch 198390, train_perplexity=698.80896, train_loss=6.5493774

Batch 198400, train_perplexity=718.343, train_loss=6.576947

Batch 198410, train_perplexity=734.9763, train_loss=6.5998383

Batch 198420, train_perplexity=736.192, train_loss=6.601491

Batch 198430, train_perplexity=747.4716, train_loss=6.6166964

Batch 198440, train_perplexity=731.8251, train_loss=6.5955415

Batch 198450, train_perplexity=745.2857, train_loss=6.6137676

Batch 198460, train_perplexity=694.5071, train_loss=6.5432024

Batch 198470, train_perplexity=679.6635, train_loss=6.521598

Batch 198480, train_perplexity=727.06506, train_loss=6.589016

Batch 198490, train_perplexity=721.1047, train_loss=6.5807843

Batch 198500, train_perplexity=753.49146, train_loss=6.6247177

Batch 198510, train_perplexity=841.1386, train_loss=6.7347565

Batch 198520, train_perplexity=708.5394, train_loss=6.5632057

Batch 198530, train_perplexity=724.2806, train_loss=6.585179

Batch 198540, train_perplexity=795.65515, train_loss=6.679166

Batch 198550, train_perplexity=721.51843, train_loss=6.581358

Batch 198560, train_perplexity=711.586, train_loss=6.5674963

Batch 198570, train_perplexity=702.6361, train_loss=6.554839

Batch 198580, train_perplexity=728.96326, train_loss=6.5916233

Batch 198590, train_perplexity=776.50256, train_loss=6.6548

Batch 198600, train_perplexity=743.7538, train_loss=6.61171

Batch 198610, train_perplexity=724.5918, train_loss=6.5856085

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Batch 198620, train_perplexity=766.9084, train_loss=6.6423674

Batch 198630, train_perplexity=779.84845, train_loss=6.6590996

Batch 198640, train_perplexity=708.2297, train_loss=6.5627685

Batch 198650, train_perplexity=730.0444, train_loss=6.5931053

Batch 198660, train_perplexity=763.7364, train_loss=6.6382227

Batch 198670, train_perplexity=733.06805, train_loss=6.5972385

Batch 198680, train_perplexity=694.1707, train_loss=6.542718

Batch 198690, train_perplexity=737.23956, train_loss=6.602913

Batch 198700, train_perplexity=777.7432, train_loss=6.6563964

Batch 198710, train_perplexity=626.3946, train_loss=6.4399805

Batch 198720, train_perplexity=750.3499, train_loss=6.6205397

Batch 198730, train_perplexity=772.37695, train_loss=6.6494727

Batch 198740, train_perplexity=698.70703, train_loss=6.5492315

Batch 198750, train_perplexity=668.12744, train_loss=6.504479

Batch 198760, train_perplexity=720.74475, train_loss=6.580285

Batch 198770, train_perplexity=719.1824, train_loss=6.578115

Batch 198780, train_perplexity=853.89264, train_loss=6.7498055

Batch 198790, train_perplexity=765.22516, train_loss=6.64017

Batch 198800, train_perplexity=726.53687, train_loss=6.5882893

Batch 198810, train_perplexity=728.359, train_loss=6.590794

Batch 198820, train_perplexity=761.752, train_loss=6.635621

Batch 198830, train_perplexity=671.1529, train_loss=6.508997

Batch 198840, train_perplexity=781.2009, train_loss=6.6608324

Batch 198850, train_perplexity=788.81683, train_loss=6.670534

Batch 198860, train_perplexity=716.9434, train_loss=6.574997

Batch 198870, train_perplexity=688.12524, train_loss=6.533971

Batch 198880, train_perplexity=771.28864, train_loss=6.6480627

Batch 198890, train_perplexity=678.7787, train_loss=6.520295

Batch 198900, train_perplexity=693.9894, train_loss=6.5424566

Batch 198910, train_perplexity=745.00714, train_loss=6.613394

Batch 198920, train_perplexity=730.71796, train_loss=6.5940275

Batch 198930, train_perplexity=793.1516, train_loss=6.6760144

Batch 198940, train_perplexity=671.88715, train_loss=6.5100904

Batch 198950, train_perplexity=798.41547, train_loss=6.682629

Batch 198960, train_perplexity=677.8627, train_loss=6.5189447

Batch 198970, train_perplexity=655.67004, train_loss=6.4856577

Batch 198980, train_perplexity=740.82556, train_loss=6.607765

Batch 198990, train_perplexity=712.50476, train_loss=6.5687866

Batch 199000, train_perplexity=757.98016, train_loss=6.630657

Batch 199010, train_perplexity=722.87665, train_loss=6.5832386

Batch 199020, train_perplexity=698.3846, train_loss=6.54877

Batch 199030, train_perplexity=816.4881, train_loss=6.7050123

Batch 199040, train_perplexity=738.7443, train_loss=6.604952

Batch 199050, train_perplexity=688.61334, train_loss=6.53468

Batch 199060, train_perplexity=731.32166, train_loss=6.5948534

Batch 199070, train_perplexity=769.8329, train_loss=6.6461735

Batch 199080, train_perplexity=729.84247, train_loss=6.5928288

Batch 199090, train_perplexity=736.34436, train_loss=6.601698

Batch 199100, train_perplexity=725.7814, train_loss=6.587249

Batch 199110, train_perplexity=666.3886, train_loss=6.501873

Batch 199120, train_perplexity=704.701, train_loss=6.5577736

Batch 199130, train_perplexity=737.76776, train_loss=6.603629

Batch 199140, train_perplexity=724.4401, train_loss=6.585399

Batch 199150, train_perplexity=796.8633, train_loss=6.680683

Batch 199160, train_perplexity=698.1342, train_loss=6.5484114

Batch 199170, train_perplexity=591.05347, train_loss=6.3819065

Batch 199180, train_perplexity=744.23804, train_loss=6.612361

Batch 199190, train_perplexity=695.9167, train_loss=6.54523

Batch 199200, train_perplexity=649.08026, train_loss=6.4755564

Batch 199210, train_perplexity=744.1153, train_loss=6.612196

Batch 199220, train_perplexity=716.9055, train_loss=6.574944
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 199230, train_perplexity=751.44806, train_loss=6.622002

Batch 199240, train_perplexity=770.4021, train_loss=6.6469126

Batch 199250, train_perplexity=674.8318, train_loss=6.5144634

Batch 199260, train_perplexity=691.01086, train_loss=6.5381556

Batch 199270, train_perplexity=686.81824, train_loss=6.5320697

Batch 199280, train_perplexity=717.4191, train_loss=6.57566

Batch 199290, train_perplexity=686.0893, train_loss=6.531008

Batch 199300, train_perplexity=690.584, train_loss=6.5375376

Batch 199310, train_perplexity=690.0093, train_loss=6.536705

Batch 199320, train_perplexity=763.5208, train_loss=6.6379404

Batch 199330, train_perplexity=789.6892, train_loss=6.6716394

Batch 199340, train_perplexity=773.56934, train_loss=6.6510153

Batch 199350, train_perplexity=765.3091, train_loss=6.64028

Batch 199360, train_perplexity=688.9539, train_loss=6.5351744

Batch 199370, train_perplexity=736.1896, train_loss=6.6014876

Batch 199380, train_perplexity=731.0922, train_loss=6.5945396

Batch 199390, train_perplexity=805.5663, train_loss=6.6915455

Batch 199400, train_perplexity=718.6027, train_loss=6.5773087

Batch 199410, train_perplexity=706.4074, train_loss=6.560192

Batch 199420, train_perplexity=693.1956, train_loss=6.541312

Batch 199430, train_perplexity=670.9577, train_loss=6.508706

Batch 199440, train_perplexity=667.291, train_loss=6.5032263

Batch 199450, train_perplexity=768.02527, train_loss=6.6438227

Batch 199460, train_perplexity=743.7024, train_loss=6.611641

Batch 199470, train_perplexity=722.0664, train_loss=6.582117

Batch 199480, train_perplexity=748.62164, train_loss=6.6182337

Batch 199490, train_perplexity=733.96625, train_loss=6.598463

Batch 199500, train_perplexity=708.8534, train_loss=6.5636487

Batch 199510, train_perplexity=705.5705, train_loss=6.5590067

Batch 199520, train_perplexity=775.5364, train_loss=6.653555

Batch 199530, train_perplexity=733.8893, train_loss=6.598358

Batch 199540, train_perplexity=687.5919, train_loss=6.5331955

Batch 199550, train_perplexity=717.7623, train_loss=6.5761385

Batch 199560, train_perplexity=753.31476, train_loss=6.624483

Batch 199570, train_perplexity=726.3342, train_loss=6.5880103

Batch 199580, train_perplexity=714.18713, train_loss=6.571145

Batch 199590, train_perplexity=768.5873, train_loss=6.644554

Batch 199600, train_perplexity=737.21423, train_loss=6.6028786

Batch 199610, train_perplexity=820.0372, train_loss=6.7093496

Batch 199620, train_perplexity=764.04895, train_loss=6.638632

Batch 199630, train_perplexity=697.3357, train_loss=6.547267

Batch 199640, train_perplexity=729.52515, train_loss=6.592394

Batch 199650, train_perplexity=697.7987, train_loss=6.5479307

Batch 199660, train_perplexity=682.85223, train_loss=6.5262785

Batch 199670, train_perplexity=736.4722, train_loss=6.6018715

Batch 199680, train_perplexity=740.35095, train_loss=6.6071243

Batch 199690, train_perplexity=844.2947, train_loss=6.7385015

Batch 199700, train_perplexity=786.49457, train_loss=6.667586

Batch 199710, train_perplexity=700.37823, train_loss=6.5516205

Batch 199720, train_perplexity=711.5575, train_loss=6.5674562

Batch 199730, train_perplexity=698.4419, train_loss=6.548852

Batch 199740, train_perplexity=699.0163, train_loss=6.549674

Batch 199750, train_perplexity=659.59106, train_loss=6.49162

Batch 199760, train_perplexity=720.46783, train_loss=6.5799007

Batch 199770, train_perplexity=703.01215, train_loss=6.555374

Batch 199780, train_perplexity=747.43066, train_loss=6.6166415

Batch 199790, train_perplexity=703.9379, train_loss=6.55669

Batch 199800, train_perplexity=809.15967, train_loss=6.6959963

Batch 199810, train_perplexity=734.78815, train_loss=6.599582

Batch 199820, train_perplexity=750.1017, train_loss=6.6202087

Batch 199830, train_perplexity=753.54395, train_loss=6.6247873

Batch 199840, train_perplexity=705.61896, train_loss=6.5590754

Batch 199850, train_perplexity=721.08374, train_loss=6.580755

Batch 199860, train_perplexity=721.0638, train_loss=6.5807276

Batch 199870, train_perplexity=765.1354, train_loss=6.640053

Batch 199880, train_perplexity=762.1124, train_loss=6.636094

Batch 199890, train_perplexity=702.62537, train_loss=6.554824

Batch 199900, train_perplexity=706.5711, train_loss=6.560424

Batch 199910, train_perplexity=734.7086, train_loss=6.599474

Batch 199920, train_perplexity=721.8626, train_loss=6.581835

Batch 199930, train_perplexity=724.2084, train_loss=6.585079

Batch 199940, train_perplexity=712.5734, train_loss=6.568883

Batch 199950, train_perplexity=726.5362, train_loss=6.5882883

Batch 199960, train_perplexity=702.2951, train_loss=6.5543537

Batch 199970, train_perplexity=796.5761, train_loss=6.6803226

Batch 199980, train_perplexity=704.0366, train_loss=6.5568304

Batch 199990, train_perplexity=683.8617, train_loss=6.5277557

Batch 200000, train_perplexity=673.5356, train_loss=6.512541

Batch 200010, train_perplexity=775.8715, train_loss=6.653987

Batch 200020, train_perplexity=668.5389, train_loss=6.5050945

Batch 200030, train_perplexity=739.66144, train_loss=6.6061926

Batch 200040, train_perplexity=734.3086, train_loss=6.5989294

Batch 200050, train_perplexity=681.03906, train_loss=6.5236197

Batch 200060, train_perplexity=735.0471, train_loss=6.5999346

Batch 200070, train_perplexity=696.77734, train_loss=6.546466

Batch 200080, train_perplexity=737.9258, train_loss=6.603843

Batch 200090, train_perplexity=765.7522, train_loss=6.6408587

Batch 200100, train_perplexity=682.2342, train_loss=6.525373

Batch 200110, train_perplexity=765.17847, train_loss=6.640109

Batch 200120, train_perplexity=696.72815, train_loss=6.5463953

Batch 200130, train_perplexity=733.588, train_loss=6.5979476

Batch 200140, train_perplexity=744.26855, train_loss=6.612402

Batch 200150, train_perplexity=768.11615, train_loss=6.643941

Batch 200160, train_perplexity=726.5618, train_loss=6.5883236

Batch 200170, train_perplexity=730.6448, train_loss=6.5939274

Batch 200180, train_perplexity=670.11676, train_loss=6.507452

Batch 200190, train_perplexity=720.9221, train_loss=6.580531

Batch 200200, train_perplexity=667.1084, train_loss=6.5029526

Batch 200210, train_perplexity=643.2863, train_loss=6.46659

Batch 200220, train_perplexity=772.7745, train_loss=6.649987

Batch 200230, train_perplexity=717.3791, train_loss=6.5756044

Batch 200240, train_perplexity=744.8128, train_loss=6.613133

Batch 200250, train_perplexity=802.24207, train_loss=6.6874104

Batch 200260, train_perplexity=727.5735, train_loss=6.589715

Batch 200270, train_perplexity=687.41785, train_loss=6.5329423

Batch 200280, train_perplexity=756.4615, train_loss=6.6286516

Batch 200290, train_perplexity=657.69635, train_loss=6.4887433

Batch 200300, train_perplexity=719.0291, train_loss=6.577902

Batch 200310, train_perplexity=772.0732, train_loss=6.6490793

Batch 200320, train_perplexity=675.0838, train_loss=6.514837

Batch 200330, train_perplexity=713.7773, train_loss=6.570571

Batch 200340, train_perplexity=725.7433, train_loss=6.5871964

Batch 200350, train_perplexity=718.52594, train_loss=6.577202

Batch 200360, train_perplexity=692.2829, train_loss=6.5399947

Batch 200370, train_perplexity=700.1919, train_loss=6.5513544

Batch 200380, train_perplexity=738.76825, train_loss=6.6049843

Batch 200390, train_perplexity=740.3972, train_loss=6.607187

Batch 200400, train_perplexity=768.88605, train_loss=6.6449428

Batch 200410, train_perplexity=686.7858, train_loss=6.5320225

Batch 200420, train_perplexity=732.6529, train_loss=6.596672

Batch 200430, train_perplexity=775.85486, train_loss=6.6539655

Batch 200440, train_perplexity=723.16144, train_loss=6.5836325

Batch 200450, train_perplexity=687.0931, train_loss=6.5324697

Batch 200460, train_perplexity=704.66974, train_loss=6.5577292

Batch 200470, train_perplexity=744.4624, train_loss=6.6126623

Batch 200480, train_perplexity=683.61456, train_loss=6.5273943

Batch 200490, train_perplexity=679.0393, train_loss=6.520679

Batch 200500, train_perplexity=746.516, train_loss=6.615417

Batch 200510, train_perplexity=674.7539, train_loss=6.514348

Batch 200520, train_perplexity=763.7743, train_loss=6.6382723

Batch 200530, train_perplexity=769.5672, train_loss=6.6458282
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 200540, train_perplexity=682.1102, train_loss=6.5251913

Batch 200550, train_perplexity=758.4422, train_loss=6.6312666

Batch 200560, train_perplexity=727.4063, train_loss=6.589485

Batch 200570, train_perplexity=686.722, train_loss=6.5319295

Batch 200580, train_perplexity=758.22815, train_loss=6.6309843

Batch 200590, train_perplexity=737.5096, train_loss=6.603279

Batch 200600, train_perplexity=711.36005, train_loss=6.5671787

Batch 200610, train_perplexity=723.09143, train_loss=6.5835357

Batch 200620, train_perplexity=786.62775, train_loss=6.667755

Batch 200630, train_perplexity=775.7021, train_loss=6.6537685

Batch 200640, train_perplexity=800.2484, train_loss=6.684922

Batch 200650, train_perplexity=729.6793, train_loss=6.592605

Batch 200660, train_perplexity=712.60297, train_loss=6.5689244

Batch 200670, train_perplexity=724.2319, train_loss=6.5851116

Batch 200680, train_perplexity=787.2856, train_loss=6.668591

Batch 200690, train_perplexity=722.056, train_loss=6.582103

Batch 200700, train_perplexity=735.34827, train_loss=6.600344

Batch 200710, train_perplexity=760.8191, train_loss=6.6343956

Batch 200720, train_perplexity=663.33813, train_loss=6.497285

Batch 200730, train_perplexity=743.6719, train_loss=6.6116

Batch 200740, train_perplexity=694.79895, train_loss=6.5436225

Batch 200750, train_perplexity=751.0451, train_loss=6.6214657

Batch 200760, train_perplexity=753.2594, train_loss=6.6244097

Batch 200770, train_perplexity=758.9588, train_loss=6.6319475

Batch 200780, train_perplexity=603.8363, train_loss=6.403303

Batch 200790, train_perplexity=713.6636, train_loss=6.5704117

Batch 200800, train_perplexity=718.3033, train_loss=6.576892

Batch 200810, train_perplexity=698.8836, train_loss=6.5494843

Batch 200820, train_perplexity=656.75366, train_loss=6.487309

Batch 200830, train_perplexity=746.0366, train_loss=6.6147747

Batch 200840, train_perplexity=758.8937, train_loss=6.6318617

Batch 200850, train_perplexity=719.6726, train_loss=6.5787964

Batch 200860, train_perplexity=760.7959, train_loss=6.634365

Batch 200870, train_perplexity=662.0101, train_loss=6.4952807

Batch 200880, train_perplexity=679.2216, train_loss=6.5209475

Batch 200890, train_perplexity=783.75867, train_loss=6.664101

Batch 200900, train_perplexity=707.0817, train_loss=6.5611463

Batch 200910, train_perplexity=760.888, train_loss=6.634486

Batch 200920, train_perplexity=748.69586, train_loss=6.618333

Batch 200930, train_perplexity=730.31177, train_loss=6.5934715

Batch 200940, train_perplexity=711.4381, train_loss=6.5672884

Batch 200950, train_perplexity=750.4172, train_loss=6.6206293

Batch 200960, train_perplexity=762.7472, train_loss=6.6369267

Batch 200970, train_perplexity=676.7633, train_loss=6.5173216

Batch 200980, train_perplexity=737.2262, train_loss=6.602895

Batch 200990, train_perplexity=727.1039, train_loss=6.5890694

Batch 201000, train_perplexity=688.44916, train_loss=6.5344415

Batch 201010, train_perplexity=821.6466, train_loss=6.7113104

Batch 201020, train_perplexity=760.3451, train_loss=6.6337724

Batch 201030, train_perplexity=793.64233, train_loss=6.676633

Batch 201040, train_perplexity=810.32574, train_loss=6.6974363

Batch 201050, train_perplexity=723.9985, train_loss=6.5847893

Batch 201060, train_perplexity=819.62354, train_loss=6.708845

Batch 201070, train_perplexity=787.5826, train_loss=6.668968

Batch 201080, train_perplexity=788.37085, train_loss=6.6699686

Batch 201090, train_perplexity=769.2186, train_loss=6.6453753

Batch 201100, train_perplexity=760.54706, train_loss=6.634038

Batch 201110, train_perplexity=652.26636, train_loss=6.480453

Batch 201120, train_perplexity=633.6922, train_loss=6.4515634

Batch 201130, train_perplexity=821.065, train_loss=6.7106023

Batch 201140, train_perplexity=800.1828, train_loss=6.68484

Batch 201150, train_perplexity=749.0173, train_loss=6.618762

Batch 201160, train_perplexity=663.89886, train_loss=6.49813

Batch 201170, train_perplexity=783.9059, train_loss=6.664289

Batch 201180, train_perplexity=750.39856, train_loss=6.6206045

Batch 201190, train_perplexity=723.87695, train_loss=6.5846214

Batch 201200, train_perplexity=757.1771, train_loss=6.629597

Batch 201210, train_perplexity=655.9027, train_loss=6.4860125

Batch 201220, train_perplexity=734.06494, train_loss=6.5985975

Batch 201230, train_perplexity=794.28064, train_loss=6.677437

Batch 201240, train_perplexity=724.27606, train_loss=6.5851727

Batch 201250, train_perplexity=700.63275, train_loss=6.551984

Batch 201260, train_perplexity=672.3653, train_loss=6.510802

Batch 201270, train_perplexity=735.6376, train_loss=6.6007376

Batch 201280, train_perplexity=710.12646, train_loss=6.565443

Batch 201290, train_perplexity=746.86597, train_loss=6.6158857

Batch 201300, train_perplexity=717.70825, train_loss=6.576063

Batch 201310, train_perplexity=673.0745, train_loss=6.511856

Batch 201320, train_perplexity=790.742, train_loss=6.6729717

Batch 201330, train_perplexity=737.74384, train_loss=6.6035967

Batch 201340, train_perplexity=749.8781, train_loss=6.6199107

Batch 201350, train_perplexity=642.6882, train_loss=6.4656596

Batch 201360, train_perplexity=659.00824, train_loss=6.490736

Batch 201370, train_perplexity=737.3148, train_loss=6.603015

Batch 201380, train_perplexity=697.17615, train_loss=6.547038

Batch 201390, train_perplexity=734.6841, train_loss=6.5994406

Batch 201400, train_perplexity=778.8926, train_loss=6.657873

Batch 201410, train_perplexity=704.52563, train_loss=6.5575247

Batch 201420, train_perplexity=710.44855, train_loss=6.5658965

Batch 201430, train_perplexity=723.86523, train_loss=6.584605

Batch 201440, train_perplexity=687.5368, train_loss=6.5331154

Batch 201450, train_perplexity=738.21783, train_loss=6.604239

Batch 201460, train_perplexity=698.71967, train_loss=6.5492496

Batch 201470, train_perplexity=737.0807, train_loss=6.6026974

Batch 201480, train_perplexity=691.57587, train_loss=6.538973

Batch 201490, train_perplexity=718.99756, train_loss=6.577858

Batch 201500, train_perplexity=719.46124, train_loss=6.5785027

Batch 201510, train_perplexity=737.96765, train_loss=6.6039

Batch 201520, train_perplexity=690.7546, train_loss=6.5377846

Batch 201530, train_perplexity=698.1695, train_loss=6.548462

Batch 201540, train_perplexity=674.7073, train_loss=6.514279

Batch 201550, train_perplexity=716.34644, train_loss=6.574164

Batch 201560, train_perplexity=703.0175, train_loss=6.555382

Batch 201570, train_perplexity=774.1387, train_loss=6.651751

Batch 201580, train_perplexity=711.64166, train_loss=6.5675745

Batch 201590, train_perplexity=742.5649, train_loss=6.6101103

Batch 201600, train_perplexity=652.35504, train_loss=6.480589

Batch 201610, train_perplexity=761.5138, train_loss=6.6353083

Batch 201620, train_perplexity=704.61633, train_loss=6.5576534

Batch 201630, train_perplexity=728.2955, train_loss=6.590707

Batch 201640, train_perplexity=756.9966, train_loss=6.629359

Batch 201650, train_perplexity=743.69104, train_loss=6.6116257

Batch 201660, train_perplexity=684.31903, train_loss=6.5284243

Batch 201670, train_perplexity=754.02484, train_loss=6.6254253

Batch 201680, train_perplexity=824.4818, train_loss=6.714755

Batch 201690, train_perplexity=708.64014, train_loss=6.563348

Batch 201700, train_perplexity=723.2256, train_loss=6.583721

Batch 201710, train_perplexity=720.01483, train_loss=6.579272

Batch 201720, train_perplexity=788.99774, train_loss=6.6707635

Batch 201730, train_perplexity=690.90546, train_loss=6.538003

Batch 201740, train_perplexity=703.04, train_loss=6.5554137

Batch 201750, train_perplexity=715.43195, train_loss=6.5728865

Batch 201760, train_perplexity=771.62634, train_loss=6.6485004

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Batch 201770, train_perplexity=730.89703, train_loss=6.5942726
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 201780, train_perplexity=731.6593, train_loss=6.595315

Batch 201790, train_perplexity=704.1273, train_loss=6.556959

Batch 201800, train_perplexity=700.0086, train_loss=6.5510926

Batch 201810, train_perplexity=771.8523, train_loss=6.648793

Batch 201820, train_perplexity=691.5185, train_loss=6.53889

Batch 201830, train_perplexity=737.43365, train_loss=6.603176

Batch 201840, train_perplexity=728.87286, train_loss=6.5914993

Batch 201850, train_perplexity=680.52386, train_loss=6.522863

Batch 201860, train_perplexity=759.48157, train_loss=6.632636

Batch 201870, train_perplexity=709.9883, train_loss=6.5652485

Batch 201880, train_perplexity=741.1761, train_loss=6.608238

Batch 201890, train_perplexity=743.9975, train_loss=6.6120377

Batch 201900, train_perplexity=695.0902, train_loss=6.5440416

Batch 201910, train_perplexity=697.3158, train_loss=6.5472383

Batch 201920, train_perplexity=715.17883, train_loss=6.5725327

Batch 201930, train_perplexity=687.0295, train_loss=6.5323772

Batch 201940, train_perplexity=794.3655, train_loss=6.6775436

Batch 201950, train_perplexity=747.2446, train_loss=6.6163926

Batch 201960, train_perplexity=653.3437, train_loss=6.4821033

Batch 201970, train_perplexity=678.38525, train_loss=6.5197153

Batch 201980, train_perplexity=691.3929, train_loss=6.538708

Batch 201990, train_perplexity=675.3185, train_loss=6.5151844

Batch 202000, train_perplexity=720.7991, train_loss=6.5803604

Batch 202010, train_perplexity=724.9333, train_loss=6.5860796

Batch 202020, train_perplexity=723.58154, train_loss=6.5842133

Batch 202030, train_perplexity=682.931, train_loss=6.526394

Batch 202040, train_perplexity=743.61053, train_loss=6.6115174

Batch 202050, train_perplexity=678.9917, train_loss=6.520609

Batch 202060, train_perplexity=707.70105, train_loss=6.5620217

Batch 202070, train_perplexity=667.15356, train_loss=6.5030203

Batch 202080, train_perplexity=719.8291, train_loss=6.579014

Batch 202090, train_perplexity=798.6611, train_loss=6.6829367

Batch 202100, train_perplexity=716.4657, train_loss=6.5743303

Batch 202110, train_perplexity=747.7083, train_loss=6.617013

Batch 202120, train_perplexity=777.68646, train_loss=6.6563234

Batch 202130, train_perplexity=702.26965, train_loss=6.5543175

Batch 202140, train_perplexity=746.4039, train_loss=6.615267

Batch 202150, train_perplexity=714.97766, train_loss=6.5722513

Batch 202160, train_perplexity=787.93115, train_loss=6.6694107

Batch 202170, train_perplexity=714.67773, train_loss=6.5718317

Batch 202180, train_perplexity=792.7981, train_loss=6.6755686

Batch 202190, train_perplexity=727.1926, train_loss=6.5891914

Batch 202200, train_perplexity=725.1559, train_loss=6.5863867

Batch 202210, train_perplexity=701.87463, train_loss=6.553755

Batch 202220, train_perplexity=821.95734, train_loss=6.7116885

Batch 202230, train_perplexity=691.0867, train_loss=6.538265

Batch 202240, train_perplexity=840.87317, train_loss=6.734441

Batch 202250, train_perplexity=637.9425, train_loss=6.458248

Batch 202260, train_perplexity=685.87604, train_loss=6.530697

Batch 202270, train_perplexity=714.29205, train_loss=6.571292

Batch 202280, train_perplexity=774.557, train_loss=6.6522913

Batch 202290, train_perplexity=697.99976, train_loss=6.5482187

Batch 202300, train_perplexity=691.63586, train_loss=6.5390596

Batch 202310, train_perplexity=717.81433, train_loss=6.576211

Batch 202320, train_perplexity=749.4256, train_loss=6.619307

Batch 202330, train_perplexity=655.76324, train_loss=6.4858

Batch 202340, train_perplexity=724.8697, train_loss=6.585992

Batch 202350, train_perplexity=653.33405, train_loss=6.4820886

Batch 202360, train_perplexity=644.49664, train_loss=6.4684696

Batch 202370, train_perplexity=780.74774, train_loss=6.660252

Batch 202380, train_perplexity=703.1191, train_loss=6.5555263

Batch 202390, train_perplexity=709.1948, train_loss=6.5641303

Batch 202400, train_perplexity=657.6515, train_loss=6.488675

Batch 202410, train_perplexity=746.4786, train_loss=6.615367

Batch 202420, train_perplexity=765.70984, train_loss=6.6408033

Batch 202430, train_perplexity=694.54254, train_loss=6.5432534

Batch 202440, train_perplexity=707.05005, train_loss=6.5611014

Batch 202450, train_perplexity=671.5966, train_loss=6.509658

Batch 202460, train_perplexity=647.6649, train_loss=6.4733734

Batch 202470, train_perplexity=659.6879, train_loss=6.491767

Batch 202480, train_perplexity=721.70496, train_loss=6.5816164

Batch 202490, train_perplexity=742.90594, train_loss=6.6105695

Batch 202500, train_perplexity=711.5657, train_loss=6.5674677

Batch 202510, train_perplexity=773.2256, train_loss=6.650571

Batch 202520, train_perplexity=760.3879, train_loss=6.6338286

Batch 202530, train_perplexity=714.6511, train_loss=6.5717945

Batch 202540, train_perplexity=752.8569, train_loss=6.623875

Batch 202550, train_perplexity=691.46375, train_loss=6.5388107

Batch 202560, train_perplexity=749.7044, train_loss=6.619679

Batch 202570, train_perplexity=673.89923, train_loss=6.5130806

Batch 202580, train_perplexity=806.0835, train_loss=6.6921873

Batch 202590, train_perplexity=684.4189, train_loss=6.52857

Batch 202600, train_perplexity=784.8915, train_loss=6.6655455

Batch 202610, train_perplexity=692.4939, train_loss=6.5402994

Batch 202620, train_perplexity=738.8183, train_loss=6.605052

Batch 202630, train_perplexity=741.7368, train_loss=6.6089945

Batch 202640, train_perplexity=756.86414, train_loss=6.629184

Batch 202650, train_perplexity=693.65656, train_loss=6.541977

Batch 202660, train_perplexity=758.2896, train_loss=6.6310654

Batch 202670, train_perplexity=676.92017, train_loss=6.5175533

Batch 202680, train_perplexity=774.99115, train_loss=6.6528516

Batch 202690, train_perplexity=687.13367, train_loss=6.532529

Batch 202700, train_perplexity=736.4469, train_loss=6.601837

Batch 202710, train_perplexity=729.68695, train_loss=6.5926156

Batch 202720, train_perplexity=735.9674, train_loss=6.601186

Batch 202730, train_perplexity=703.77313, train_loss=6.556456

Batch 202740, train_perplexity=696.8647, train_loss=6.5465913

Batch 202750, train_perplexity=747.1997, train_loss=6.6163325

Batch 202760, train_perplexity=677.2311, train_loss=6.5180125

Batch 202770, train_perplexity=708.2969, train_loss=6.5628633

Batch 202780, train_perplexity=805.1454, train_loss=6.691023

Batch 202790, train_perplexity=751.8133, train_loss=6.622488

Batch 202800, train_perplexity=729.2247, train_loss=6.591982

Batch 202810, train_perplexity=675.8855, train_loss=6.5160236

Batch 202820, train_perplexity=673.98566, train_loss=6.513209

Batch 202830, train_perplexity=632.51575, train_loss=6.449705

Batch 202840, train_perplexity=700.6949, train_loss=6.5520725

Batch 202850, train_perplexity=727.8976, train_loss=6.5901604

Batch 202860, train_perplexity=714.7172, train_loss=6.571887

Batch 202870, train_perplexity=761.66156, train_loss=6.6355023

Batch 202880, train_perplexity=699.9352, train_loss=6.5509877

Batch 202890, train_perplexity=683.06586, train_loss=6.5265913

Batch 202900, train_perplexity=742.6269, train_loss=6.6101937

Batch 202910, train_perplexity=607.1491, train_loss=6.4087744

Batch 202920, train_perplexity=815.9942, train_loss=6.704407

Batch 202930, train_perplexity=641.9102, train_loss=6.4644485

Batch 202940, train_perplexity=776.83923, train_loss=6.6552334

Batch 202950, train_perplexity=715.6353, train_loss=6.5731707

Batch 202960, train_perplexity=672.42334, train_loss=6.510888

Batch 202970, train_perplexity=750.6645, train_loss=6.620959

Batch 202980, train_perplexity=804.73083, train_loss=6.690508

Batch 202990, train_perplexity=788.3949, train_loss=6.669999

Batch 203000, train_perplexity=753.619, train_loss=6.624887

Batch 203010, train_perplexity=705.35657, train_loss=6.5587034

Batch 203020, train_perplexity=609.3766, train_loss=6.4124365

Batch 203030, train_perplexity=750.93335, train_loss=6.621317

Batch 203040, train_perplexity=701.06085, train_loss=6.5525947

Batch 203050, train_perplexity=754.7432, train_loss=6.6263776

Batch 203060, train_perplexity=752.2257, train_loss=6.6230364

Batch 203070, train_perplexity=752.08295, train_loss=6.6228466

Batch 203080, train_perplexity=620.9926, train_loss=6.431319
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 203090, train_perplexity=765.81213, train_loss=6.640937

Batch 203100, train_perplexity=694.5571, train_loss=6.5432744

Batch 203110, train_perplexity=715.1543, train_loss=6.5724983

Batch 203120, train_perplexity=822.1106, train_loss=6.711875

Batch 203130, train_perplexity=698.4978, train_loss=6.548932

Batch 203140, train_perplexity=693.4548, train_loss=6.541686

Batch 203150, train_perplexity=729.7614, train_loss=6.5927176

Batch 203160, train_perplexity=727.4212, train_loss=6.5895057

Batch 203170, train_perplexity=658.4548, train_loss=6.489896

Batch 203180, train_perplexity=729.5158, train_loss=6.592381

Batch 203190, train_perplexity=737.177, train_loss=6.602828

Batch 203200, train_perplexity=721.08954, train_loss=6.5807633

Batch 203210, train_perplexity=757.5455, train_loss=6.6300836

Batch 203220, train_perplexity=691.01154, train_loss=6.5381565

Batch 203230, train_perplexity=721.9311, train_loss=6.5819297

Batch 203240, train_perplexity=696.6112, train_loss=6.5462275

Batch 203250, train_perplexity=753.8976, train_loss=6.6252565

Batch 203260, train_perplexity=707.7915, train_loss=6.5621495

Batch 203270, train_perplexity=788.9541, train_loss=6.670708

Batch 203280, train_perplexity=689.5106, train_loss=6.535982

Batch 203290, train_perplexity=723.2883, train_loss=6.583808

Batch 203300, train_perplexity=718.3389, train_loss=6.5769415

Batch 203310, train_perplexity=750.90826, train_loss=6.6212835

Batch 203320, train_perplexity=710.4689, train_loss=6.565925

Batch 203330, train_perplexity=742.4948, train_loss=6.610016

Batch 203340, train_perplexity=680.33344, train_loss=6.522583

Batch 203350, train_perplexity=789.443, train_loss=6.6713276

Batch 203360, train_perplexity=708.26447, train_loss=6.5628176

Batch 203370, train_perplexity=752.83856, train_loss=6.623851

Batch 203380, train_perplexity=688.50006, train_loss=6.5345154

Batch 203390, train_perplexity=739.33777, train_loss=6.605755

Batch 203400, train_perplexity=797.4494, train_loss=6.6814184

Batch 203410, train_perplexity=791.51605, train_loss=6.67395

Batch 203420, train_perplexity=754.57336, train_loss=6.6261525

Batch 203430, train_perplexity=725.4298, train_loss=6.5867643

Batch 203440, train_perplexity=717.38525, train_loss=6.575613

Batch 203450, train_perplexity=792.848, train_loss=6.6756315

Batch 203460, train_perplexity=678.1498, train_loss=6.519368

Batch 203470, train_perplexity=704.2294, train_loss=6.557104

Batch 203480, train_perplexity=755.58545, train_loss=6.627493

Batch 203490, train_perplexity=670.7095, train_loss=6.508336

Batch 203500, train_perplexity=737.17804, train_loss=6.6028295

Batch 203510, train_perplexity=660.0813, train_loss=6.492363

Batch 203520, train_perplexity=734.17, train_loss=6.5987406

Batch 203530, train_perplexity=709.4103, train_loss=6.564434

Batch 203540, train_perplexity=683.93085, train_loss=6.527857

Batch 203550, train_perplexity=668.0577, train_loss=6.5043745

Batch 203560, train_perplexity=751.5509, train_loss=6.622139

Batch 203570, train_perplexity=723.3146, train_loss=6.583844

Batch 203580, train_perplexity=682.3933, train_loss=6.525606

Batch 203590, train_perplexity=678.56573, train_loss=6.5199814

Batch 203600, train_perplexity=675.0609, train_loss=6.514803

Batch 203610, train_perplexity=763.3694, train_loss=6.637742

Batch 203620, train_perplexity=761.2945, train_loss=6.6350203

Batch 203630, train_perplexity=725.7114, train_loss=6.5871525

Batch 203640, train_perplexity=706.4589, train_loss=6.560265

Batch 203650, train_perplexity=754.3061, train_loss=6.625798

Batch 203660, train_perplexity=663.76404, train_loss=6.4979267

Batch 203670, train_perplexity=674.9592, train_loss=6.5146523

Batch 203680, train_perplexity=707.0885, train_loss=6.561156

Batch 203690, train_perplexity=684.45874, train_loss=6.5286283

Batch 203700, train_perplexity=701.9871, train_loss=6.553915

Batch 203710, train_perplexity=740.69415, train_loss=6.607588

Batch 203720, train_perplexity=716.57465, train_loss=6.5744824

Batch 203730, train_perplexity=688.18164, train_loss=6.534053

Batch 203740, train_perplexity=734.5552, train_loss=6.599265

Batch 203750, train_perplexity=763.1081, train_loss=6.6373997

Batch 203760, train_perplexity=681.99384, train_loss=6.5250206

Batch 203770, train_perplexity=751.2679, train_loss=6.6217623

Batch 203780, train_perplexity=746.93506, train_loss=6.6159782

Batch 203790, train_perplexity=730.98346, train_loss=6.594391

Batch 203800, train_perplexity=767.70746, train_loss=6.643409

Batch 203810, train_perplexity=740.09576, train_loss=6.6067796

Batch 203820, train_perplexity=731.4022, train_loss=6.5949636

Batch 203830, train_perplexity=767.91473, train_loss=6.6436787

Batch 203840, train_perplexity=700.84155, train_loss=6.552282

Batch 203850, train_perplexity=717.7668, train_loss=6.5761447

Batch 203860, train_perplexity=695.98303, train_loss=6.5453253

Batch 203870, train_perplexity=728.10205, train_loss=6.590441

Batch 203880, train_perplexity=749.915, train_loss=6.61996

Batch 203890, train_perplexity=745.7841, train_loss=6.614436

Batch 203900, train_perplexity=732.18176, train_loss=6.596029

Batch 203910, train_perplexity=685.18365, train_loss=6.529687

Batch 203920, train_perplexity=743.5776, train_loss=6.611473

Batch 203930, train_perplexity=679.0351, train_loss=6.520673

Batch 203940, train_perplexity=713.82935, train_loss=6.570644

Batch 203950, train_perplexity=734.87256, train_loss=6.599697

Batch 203960, train_perplexity=846.1635, train_loss=6.7407126

Batch 203970, train_perplexity=793.0613, train_loss=6.6759005

Batch 203980, train_perplexity=759.2245, train_loss=6.6322975

Batch 203990, train_perplexity=799.3499, train_loss=6.683799

Batch 204000, train_perplexity=673.0142, train_loss=6.5117664

Batch 204010, train_perplexity=721.50745, train_loss=6.5813427

Batch 204020, train_perplexity=678.07214, train_loss=6.5192537

Batch 204030, train_perplexity=712.3305, train_loss=6.568542

Batch 204040, train_perplexity=769.69635, train_loss=6.645996

Batch 204050, train_perplexity=711.9498, train_loss=6.5680075

Batch 204060, train_perplexity=698.55444, train_loss=6.549013

Batch 204070, train_perplexity=737.16016, train_loss=6.602805

Batch 204080, train_perplexity=771.99255, train_loss=6.648975

Batch 204090, train_perplexity=677.2443, train_loss=6.518032

Batch 204100, train_perplexity=734.13916, train_loss=6.5986986

Batch 204110, train_perplexity=663.8945, train_loss=6.498123

Batch 204120, train_perplexity=714.8066, train_loss=6.572012

Batch 204130, train_perplexity=634.62024, train_loss=6.453027

Batch 204140, train_perplexity=747.8089, train_loss=6.6171474

Batch 204150, train_perplexity=623.38806, train_loss=6.435169

Batch 204160, train_perplexity=721.321, train_loss=6.5810843

Batch 204170, train_perplexity=756.92584, train_loss=6.6292653

Batch 204180, train_perplexity=817.63275, train_loss=6.7064133

Batch 204190, train_perplexity=787.11554, train_loss=6.668375

Batch 204200, train_perplexity=710.0167, train_loss=6.5652885

Batch 204210, train_perplexity=650.082, train_loss=6.4770985

Batch 204220, train_perplexity=689.387, train_loss=6.535803

Batch 204230, train_perplexity=682.98444, train_loss=6.526472

Batch 204240, train_perplexity=784.72644, train_loss=6.665335

Batch 204250, train_perplexity=720.98334, train_loss=6.580616

Batch 204260, train_perplexity=725.544, train_loss=6.5869217

Batch 204270, train_perplexity=804.503, train_loss=6.6902246

Batch 204280, train_perplexity=758.707, train_loss=6.6316156

Batch 204290, train_perplexity=760.6649, train_loss=6.634193

Batch 204300, train_perplexity=682.79755, train_loss=6.5261984

Batch 204310, train_perplexity=721.5656, train_loss=6.5814233

Batch 204320, train_perplexity=684.71497, train_loss=6.5290027

Batch 204330, train_perplexity=734.6522, train_loss=6.599397

Batch 204340, train_perplexity=622.95184, train_loss=6.434469

Batch 204350, train_perplexity=764.08826, train_loss=6.6386833

Batch 204360, train_perplexity=678.8078, train_loss=6.520338

Batch 204370, train_perplexity=677.2889, train_loss=6.518098

Batch 204380, train_perplexity=630.7048, train_loss=6.446838

Batch 204390, train_perplexity=710.662, train_loss=6.566197
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 204400, train_perplexity=675.1414, train_loss=6.514922

Batch 204410, train_perplexity=700.5643, train_loss=6.551886

Batch 204420, train_perplexity=760.0221, train_loss=6.6333475

Batch 204430, train_perplexity=719.3206, train_loss=6.578307

Batch 204440, train_perplexity=765.1901, train_loss=6.6401243

Batch 204450, train_perplexity=671.45795, train_loss=6.5094514

Batch 204460, train_perplexity=761.3631, train_loss=6.6351104

Batch 204470, train_perplexity=643.3225, train_loss=6.466646

Batch 204480, train_perplexity=695.46515, train_loss=6.544581

Batch 204490, train_perplexity=650.16815, train_loss=6.477231

Batch 204500, train_perplexity=656.60364, train_loss=6.4870806

Batch 204510, train_perplexity=687.0026, train_loss=6.532338

Batch 204520, train_perplexity=765.1007, train_loss=6.6400075

Batch 204530, train_perplexity=666.6022, train_loss=6.5021935

Batch 204540, train_perplexity=688.36707, train_loss=6.5343223

Batch 204550, train_perplexity=630.0029, train_loss=6.4457245

Batch 204560, train_perplexity=757.5856, train_loss=6.6301365

Batch 204570, train_perplexity=679.6266, train_loss=6.5215435

Batch 204580, train_perplexity=710.8806, train_loss=6.5665045

Batch 204590, train_perplexity=736.09796, train_loss=6.601363

Batch 204600, train_perplexity=753.79694, train_loss=6.625123

Batch 204610, train_perplexity=745.94556, train_loss=6.6146526

Batch 204620, train_perplexity=742.7002, train_loss=6.6102924

Batch 204630, train_perplexity=693.7822, train_loss=6.542158

Batch 204640, train_perplexity=735.13965, train_loss=6.6000605

Batch 204650, train_perplexity=712.64545, train_loss=6.568984

Batch 204660, train_perplexity=708.2442, train_loss=6.562789

Batch 204670, train_perplexity=795.75226, train_loss=6.679288

Batch 204680, train_perplexity=704.33417, train_loss=6.557253

Batch 204690, train_perplexity=676.7065, train_loss=6.5172377

Batch 204700, train_perplexity=762.80176, train_loss=6.636998

Batch 204710, train_perplexity=695.0339, train_loss=6.5439606

Batch 204720, train_perplexity=760.892, train_loss=6.6344914

Batch 204730, train_perplexity=726.2757, train_loss=6.5879297

Batch 204740, train_perplexity=754.6299, train_loss=6.6262274

Batch 204750, train_perplexity=689.14777, train_loss=6.5354557

Batch 204760, train_perplexity=692.4787, train_loss=6.5402775

Batch 204770, train_perplexity=747.6969, train_loss=6.6169977

Batch 204780, train_perplexity=707.16907, train_loss=6.5612698

Batch 204790, train_perplexity=781.9321, train_loss=6.661768

Batch 204800, train_perplexity=715.96704, train_loss=6.573634

Batch 204810, train_perplexity=729.4198, train_loss=6.5922494

Batch 204820, train_perplexity=678.2736, train_loss=6.519551

Batch 204830, train_perplexity=745.7962, train_loss=6.6144524

Batch 204840, train_perplexity=724.85724, train_loss=6.5859747

Batch 204850, train_perplexity=724.80054, train_loss=6.5858965

Batch 204860, train_perplexity=652.76514, train_loss=6.4812174

Batch 204870, train_perplexity=686.8408, train_loss=6.5321026

Batch 204880, train_perplexity=734.0114, train_loss=6.5985246

Batch 204890, train_perplexity=702.5276, train_loss=6.5546846

Batch 204900, train_perplexity=697.43414, train_loss=6.547408

Batch 204910, train_perplexity=654.01074, train_loss=6.483124

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Batch 204920, train_perplexity=695.4171, train_loss=6.544512

Batch 204930, train_perplexity=697.852, train_loss=6.548007

Batch 204940, train_perplexity=687.5021, train_loss=6.533065

Batch 204950, train_perplexity=748.9012, train_loss=6.618607

Batch 204960, train_perplexity=777.75323, train_loss=6.6564093

Batch 204970, train_perplexity=745.89575, train_loss=6.614586

Batch 204980, train_perplexity=678.6069, train_loss=6.520042

Batch 204990, train_perplexity=680.9085, train_loss=6.523428

Batch 205000, train_perplexity=651.5567, train_loss=6.4793644

Batch 205010, train_perplexity=728.2576, train_loss=6.590655

Batch 205020, train_perplexity=704.18304, train_loss=6.5570383

Batch 205030, train_perplexity=713.98694, train_loss=6.5708647

Batch 205040, train_perplexity=712.58295, train_loss=6.5688963

Batch 205050, train_perplexity=671.5639, train_loss=6.509609

Batch 205060, train_perplexity=749.9915, train_loss=6.620062

Batch 205070, train_perplexity=754.414, train_loss=6.6259413

Batch 205080, train_perplexity=660.19775, train_loss=6.4925394

Batch 205090, train_perplexity=743.4024, train_loss=6.6112375

Batch 205100, train_perplexity=793.21027, train_loss=6.6760883

Batch 205110, train_perplexity=730.11676, train_loss=6.5932045

Batch 205120, train_perplexity=745.441, train_loss=6.613976

Batch 205130, train_perplexity=753.94073, train_loss=6.6253138

Batch 205140, train_perplexity=742.3397, train_loss=6.609807

Batch 205150, train_perplexity=780.3979, train_loss=6.659804

Batch 205160, train_perplexity=711.58466, train_loss=6.5674944

Batch 205170, train_perplexity=765.67114, train_loss=6.640753

Batch 205180, train_perplexity=766.64, train_loss=6.6420174

Batch 205190, train_perplexity=732.14197, train_loss=6.5959744

Batch 205200, train_perplexity=746.40314, train_loss=6.615266

Batch 205210, train_perplexity=710.8582, train_loss=6.566473

Batch 205220, train_perplexity=722.1886, train_loss=6.5822864

Batch 205230, train_perplexity=679.0639, train_loss=6.520715

Batch 205240, train_perplexity=695.4065, train_loss=6.5444965

Batch 205250, train_perplexity=707.9744, train_loss=6.562408

Batch 205260, train_perplexity=715.6687, train_loss=6.5732174

Batch 205270, train_perplexity=692.558, train_loss=6.540392

Batch 205280, train_perplexity=767.6079, train_loss=6.643279

Batch 205290, train_perplexity=679.2083, train_loss=6.520928

Batch 205300, train_perplexity=657.2737, train_loss=6.4881005

Batch 205310, train_perplexity=736.57825, train_loss=6.6020155

Batch 205320, train_perplexity=793.961, train_loss=6.6770344

Batch 205330, train_perplexity=653.31036, train_loss=6.4820523

Batch 205340, train_perplexity=760.10657, train_loss=6.6334586

Batch 205350, train_perplexity=686.62274, train_loss=6.531785

Batch 205360, train_perplexity=728.36633, train_loss=6.590804

Batch 205370, train_perplexity=686.3831, train_loss=6.531436

Batch 205380, train_perplexity=797.42017, train_loss=6.6813817

Batch 205390, train_perplexity=688.7072, train_loss=6.5348163

Batch 205400, train_perplexity=764.00195, train_loss=6.6385703

Batch 205410, train_perplexity=720.38226, train_loss=6.579782

Batch 205420, train_perplexity=707.40314, train_loss=6.5616007

Batch 205430, train_perplexity=831.3367, train_loss=6.723035

Batch 205440, train_perplexity=630.7941, train_loss=6.4469795

Batch 205450, train_perplexity=724.4319, train_loss=6.5853877

Batch 205460, train_perplexity=703.8866, train_loss=6.5566173

Batch 205470, train_perplexity=698.1955, train_loss=6.548499

Batch 205480, train_perplexity=676.4942, train_loss=6.516924

Batch 205490, train_perplexity=709.3863, train_loss=6.5644

Batch 205500, train_perplexity=745.62195, train_loss=6.6142187

Batch 205510, train_perplexity=759.7239, train_loss=6.632955

Batch 205520, train_perplexity=785.70105, train_loss=6.6665764

Batch 205530, train_perplexity=726.82935, train_loss=6.5886917

Batch 205540, train_perplexity=718.21765, train_loss=6.5767727

Batch 205550, train_perplexity=704.0162, train_loss=6.5568013

Batch 205560, train_perplexity=667.7761, train_loss=6.503953

Batch 205570, train_perplexity=733.2212, train_loss=6.5974474

Batch 205580, train_perplexity=733.7745, train_loss=6.5982018

Batch 205590, train_perplexity=729.83344, train_loss=6.5928164

Batch 205600, train_perplexity=705.5611, train_loss=6.5589933

Batch 205610, train_perplexity=733.28937, train_loss=6.5975404

Batch 205620, train_perplexity=745.17303, train_loss=6.6136165

Batch 205630, train_perplexity=709.95306, train_loss=6.565199
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 205640, train_perplexity=745.99677, train_loss=6.6147213

Batch 205650, train_perplexity=676.0444, train_loss=6.5162587

Batch 205660, train_perplexity=743.27234, train_loss=6.6110625

Batch 205670, train_perplexity=755.2782, train_loss=6.627086

Batch 205680, train_perplexity=722.98004, train_loss=6.5833817

Batch 205690, train_perplexity=769.3808, train_loss=6.645586

Batch 205700, train_perplexity=665.6709, train_loss=6.5007954

Batch 205710, train_perplexity=697.6563, train_loss=6.5477266

Batch 205720, train_perplexity=693.58575, train_loss=6.541875

Batch 205730, train_perplexity=635.24, train_loss=6.454003

Batch 205740, train_perplexity=739.3159, train_loss=6.6057253

Batch 205750, train_perplexity=794.6595, train_loss=6.6779137

Batch 205760, train_perplexity=709.98627, train_loss=6.5652456

Batch 205770, train_perplexity=678.3403, train_loss=6.519649

Batch 205780, train_perplexity=661.7683, train_loss=6.4949155

Batch 205790, train_perplexity=690.56354, train_loss=6.537508

Batch 205800, train_perplexity=710.91113, train_loss=6.5665474

Batch 205810, train_perplexity=696.1463, train_loss=6.54556

Batch 205820, train_perplexity=684.48157, train_loss=6.5286617

Batch 205830, train_perplexity=741.45605, train_loss=6.608616

Batch 205840, train_perplexity=691.46936, train_loss=6.538819

Batch 205850, train_perplexity=721.2013, train_loss=6.5809183

Batch 205860, train_perplexity=720.8149, train_loss=6.5803823

Batch 205870, train_perplexity=737.4442, train_loss=6.6031904

Batch 205880, train_perplexity=735.9751, train_loss=6.6011963

Batch 205890, train_perplexity=765.4726, train_loss=6.6404934

Batch 205900, train_perplexity=649.8266, train_loss=6.4767056

Batch 205910, train_perplexity=702.342, train_loss=6.5544205

Batch 205920, train_perplexity=745.984, train_loss=6.614704

Batch 205930, train_perplexity=605.90894, train_loss=6.4067297

Batch 205940, train_perplexity=765.89246, train_loss=6.6410418

Batch 205950, train_perplexity=762.07135, train_loss=6.63604

Batch 205960, train_perplexity=709.58826, train_loss=6.564685

Batch 205970, train_perplexity=789.49115, train_loss=6.6713886

Batch 205980, train_perplexity=775.8234, train_loss=6.653925

Batch 205990, train_perplexity=704.275, train_loss=6.557169

Batch 206000, train_perplexity=737.93665, train_loss=6.603858

Batch 206010, train_perplexity=739.3737, train_loss=6.6058035

Batch 206020, train_perplexity=740.90155, train_loss=6.6078677

Batch 206030, train_perplexity=771.74927, train_loss=6.6486597

Batch 206040, train_perplexity=745.16345, train_loss=6.6136036

Batch 206050, train_perplexity=710.99414, train_loss=6.566664

Batch 206060, train_perplexity=765.5945, train_loss=6.6406527

Batch 206070, train_perplexity=704.4386, train_loss=6.557401

Batch 206080, train_perplexity=699.15094, train_loss=6.5498667

Batch 206090, train_perplexity=648.49805, train_loss=6.474659

Batch 206100, train_perplexity=690.3732, train_loss=6.5372324

Batch 206110, train_perplexity=735.3072, train_loss=6.6002884

Batch 206120, train_perplexity=662.4597, train_loss=6.4959598

Batch 206130, train_perplexity=737.20935, train_loss=6.602872

Batch 206140, train_perplexity=639.9808, train_loss=6.461438

Batch 206150, train_perplexity=738.3865, train_loss=6.6044674

Batch 206160, train_perplexity=724.1269, train_loss=6.5849667

Batch 206170, train_perplexity=660.3467, train_loss=6.492765

Batch 206180, train_perplexity=684.5622, train_loss=6.5287795

Batch 206190, train_perplexity=785.9326, train_loss=6.666871

Batch 206200, train_perplexity=773.07227, train_loss=6.6503725

Batch 206210, train_perplexity=758.1056, train_loss=6.6308227

Batch 206220, train_perplexity=615.95734, train_loss=6.4231777

Batch 206230, train_perplexity=753.0274, train_loss=6.6241016

Batch 206240, train_perplexity=715.8783, train_loss=6.57351

Batch 206250, train_perplexity=695.32324, train_loss=6.544377

Batch 206260, train_perplexity=766.1741, train_loss=6.6414094

Batch 206270, train_perplexity=662.19824, train_loss=6.495565

Batch 206280, train_perplexity=823.54114, train_loss=6.7136135

Batch 206290, train_perplexity=741.2835, train_loss=6.608383

Batch 206300, train_perplexity=745.4307, train_loss=6.613962

Batch 206310, train_perplexity=728.7672, train_loss=6.5913544

Batch 206320, train_perplexity=708.7219, train_loss=6.563463

Batch 206330, train_perplexity=704.68756, train_loss=6.5577545

Batch 206340, train_perplexity=677.08673, train_loss=6.5177994

Batch 206350, train_perplexity=710.4204, train_loss=6.565857

Batch 206360, train_perplexity=724.2395, train_loss=6.585122

Batch 206370, train_perplexity=679.79315, train_loss=6.5217886

Batch 206380, train_perplexity=732.8961, train_loss=6.597004

Batch 206390, train_perplexity=670.5969, train_loss=6.508168

Batch 206400, train_perplexity=736.3809, train_loss=6.6017475

Batch 206410, train_perplexity=822.4227, train_loss=6.7122545

Batch 206420, train_perplexity=674.9505, train_loss=6.5146394

Batch 206430, train_perplexity=807.30554, train_loss=6.693702

Batch 206440, train_perplexity=676.8879, train_loss=6.5175056

Batch 206450, train_perplexity=805.85364, train_loss=6.691902

Batch 206460, train_perplexity=730.5298, train_loss=6.59377

Batch 206470, train_perplexity=698.5651, train_loss=6.5490284

Batch 206480, train_perplexity=820.21313, train_loss=6.709564

Batch 206490, train_perplexity=689.96124, train_loss=6.5366354

Batch 206500, train_perplexity=739.4499, train_loss=6.6059065

Batch 206510, train_perplexity=730.3146, train_loss=6.5934753

Batch 206520, train_perplexity=680.4444, train_loss=6.522746

Batch 206530, train_perplexity=673.0643, train_loss=6.511841

Batch 206540, train_perplexity=680.68585, train_loss=6.523101

Batch 206550, train_perplexity=742.32947, train_loss=6.609793

Batch 206560, train_perplexity=756.05615, train_loss=6.6281157

Batch 206570, train_perplexity=717.1828, train_loss=6.5753307

Batch 206580, train_perplexity=691.915, train_loss=6.539463

Batch 206590, train_perplexity=781.80237, train_loss=6.661602

Batch 206600, train_perplexity=714.9671, train_loss=6.5722365

Batch 206610, train_perplexity=708.36646, train_loss=6.5629616

Batch 206620, train_perplexity=746.5188, train_loss=6.615421

Batch 206630, train_perplexity=710.5112, train_loss=6.5659847

Batch 206640, train_perplexity=649.22046, train_loss=6.4757724

Batch 206650, train_perplexity=698.03467, train_loss=6.548269

Batch 206660, train_perplexity=694.0668, train_loss=6.542568

Batch 206670, train_perplexity=691.5218, train_loss=6.5388947

Batch 206680, train_perplexity=657.1418, train_loss=6.4879

Batch 206690, train_perplexity=743.48895, train_loss=6.611354

Batch 206700, train_perplexity=762.3792, train_loss=6.636444

Batch 206710, train_perplexity=722.54236, train_loss=6.582776

Batch 206720, train_perplexity=714.07275, train_loss=6.570985

Batch 206730, train_perplexity=699.5191, train_loss=6.550393

Batch 206740, train_perplexity=674.2978, train_loss=6.513672

Batch 206750, train_perplexity=775.9455, train_loss=6.6540823

Batch 206760, train_perplexity=727.3279, train_loss=6.5893774

Batch 206770, train_perplexity=662.9846, train_loss=6.496752

Batch 206780, train_perplexity=676.01953, train_loss=6.516222

Batch 206790, train_perplexity=749.41235, train_loss=6.6192894

Batch 206800, train_perplexity=682.4076, train_loss=6.525627

Batch 206810, train_perplexity=694.9709, train_loss=6.54387

Batch 206820, train_perplexity=781.7655, train_loss=6.661555

Batch 206830, train_perplexity=735.1155, train_loss=6.6000276

Batch 206840, train_perplexity=719.44135, train_loss=6.578475

Batch 206850, train_perplexity=737.29865, train_loss=6.602993

Batch 206860, train_perplexity=663.62604, train_loss=6.497719

Batch 206870, train_perplexity=815.4162, train_loss=6.7036986

Batch 206880, train_perplexity=723.9819, train_loss=6.5847664

Batch 206890, train_perplexity=702.5162, train_loss=6.5546684

Batch 206900, train_perplexity=725.1303, train_loss=6.5863514

Batch 206910, train_perplexity=757.781, train_loss=6.6303945

Batch 206920, train_perplexity=671.47046, train_loss=6.50947

Batch 206930, train_perplexity=680.567, train_loss=6.5229263

Batch 206940, train_perplexity=779.0623, train_loss=6.658091
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 206950, train_perplexity=709.7355, train_loss=6.5648923

Batch 206960, train_perplexity=610.4955, train_loss=6.414271

Batch 206970, train_perplexity=712.8113, train_loss=6.5692167

Batch 206980, train_perplexity=735.3798, train_loss=6.600387

Batch 206990, train_perplexity=690.9581, train_loss=6.5380793

Batch 207000, train_perplexity=674.57245, train_loss=6.514079

Batch 207010, train_perplexity=656.7809, train_loss=6.4873505

Batch 207020, train_perplexity=653.4017, train_loss=6.482192

Batch 207030, train_perplexity=761.82465, train_loss=6.6357164

Batch 207040, train_perplexity=709.6363, train_loss=6.5647526

Batch 207050, train_perplexity=715.3235, train_loss=6.572735

Batch 207060, train_perplexity=666.29395, train_loss=6.501731

Batch 207070, train_perplexity=678.0275, train_loss=6.519188

Batch 207080, train_perplexity=705.0875, train_loss=6.558322

Batch 207090, train_perplexity=703.6628, train_loss=6.556299

Batch 207100, train_perplexity=741.2199, train_loss=6.6082973

Batch 207110, train_perplexity=727.59393, train_loss=6.589743

Batch 207120, train_perplexity=721.2261, train_loss=6.5809526

Batch 207130, train_perplexity=712.0048, train_loss=6.5680847

Batch 207140, train_perplexity=682.90173, train_loss=6.526351

Batch 207150, train_perplexity=681.1527, train_loss=6.5237865

Batch 207160, train_perplexity=671.50116, train_loss=6.509516

Batch 207170, train_perplexity=714.2151, train_loss=6.571184

Batch 207180, train_perplexity=633.7004, train_loss=6.451576

Batch 207190, train_perplexity=775.0761, train_loss=6.6529613

Batch 207200, train_perplexity=698.4046, train_loss=6.5487986

Batch 207210, train_perplexity=765.29486, train_loss=6.640261

Batch 207220, train_perplexity=707.9316, train_loss=6.5623474

Batch 207230, train_perplexity=672.51666, train_loss=6.511027

Batch 207240, train_perplexity=769.2186, train_loss=6.6453753

Batch 207250, train_perplexity=741.2284, train_loss=6.608309

Batch 207260, train_perplexity=756.0151, train_loss=6.6280613

Batch 207270, train_perplexity=703.846, train_loss=6.5565596

Batch 207280, train_perplexity=701.92786, train_loss=6.5538306

Batch 207290, train_perplexity=680.24457, train_loss=6.5224524

Batch 207300, train_perplexity=657.6665, train_loss=6.488698

Batch 207310, train_perplexity=668.8373, train_loss=6.505541

Batch 207320, train_perplexity=729.79065, train_loss=6.5927577

Batch 207330, train_perplexity=701.0999, train_loss=6.5526505

Batch 207340, train_perplexity=760.9606, train_loss=6.6345816

Batch 207350, train_perplexity=734.9132, train_loss=6.5997524

Batch 207360, train_perplexity=701.60895, train_loss=6.553376

Batch 207370, train_perplexity=736.8259, train_loss=6.6023517

Batch 207380, train_perplexity=722.8746, train_loss=6.5832357

Batch 207390, train_perplexity=736.2327, train_loss=6.6015463

Batch 207400, train_perplexity=632.35834, train_loss=6.449456

Batch 207410, train_perplexity=664.0749, train_loss=6.498395

Batch 207420, train_perplexity=651.1057, train_loss=6.478672

Batch 207430, train_perplexity=700.1819, train_loss=6.55134

Batch 207440, train_perplexity=728.14685, train_loss=6.5905027

Batch 207450, train_perplexity=721.7772, train_loss=6.5817165

Batch 207460, train_perplexity=679.1834, train_loss=6.520891

Batch 207470, train_perplexity=702.2827, train_loss=6.554336

Batch 207480, train_perplexity=706.79724, train_loss=6.560744

Batch 207490, train_perplexity=821.5714, train_loss=6.711219

Batch 207500, train_perplexity=746.4793, train_loss=6.615368

Batch 207510, train_perplexity=734.76605, train_loss=6.599552

Batch 207520, train_perplexity=679.04285, train_loss=6.5206842

Batch 207530, train_perplexity=677.1484, train_loss=6.5178905

Batch 207540, train_perplexity=737.39355, train_loss=6.6031218

Batch 207550, train_perplexity=723.4166, train_loss=6.5839853

Batch 207560, train_perplexity=706.73486, train_loss=6.5606556

Batch 207570, train_perplexity=734.4648, train_loss=6.599142

Batch 207580, train_perplexity=716.0367, train_loss=6.5737314

Batch 207590, train_perplexity=792.24786, train_loss=6.6748743

Batch 207600, train_perplexity=696.30566, train_loss=6.545789

Batch 207610, train_perplexity=659.56464, train_loss=6.49158

Batch 207620, train_perplexity=747.1025, train_loss=6.6162024

Batch 207630, train_perplexity=704.34625, train_loss=6.55727

Batch 207640, train_perplexity=779.23956, train_loss=6.6583185

Batch 207650, train_perplexity=807.2224, train_loss=6.693599

Batch 207660, train_perplexity=777.90454, train_loss=6.656604

Batch 207670, train_perplexity=753.46204, train_loss=6.6246786

Batch 207680, train_perplexity=668.42413, train_loss=6.504923

Batch 207690, train_perplexity=686.7714, train_loss=6.5320015

Batch 207700, train_perplexity=686.07556, train_loss=6.5309877

Batch 207710, train_perplexity=655.7379, train_loss=6.485761

Batch 207720, train_perplexity=629.0193, train_loss=6.444162

Batch 207730, train_perplexity=658.2118, train_loss=6.4895267

Batch 207740, train_perplexity=757.1074, train_loss=6.629505

Batch 207750, train_perplexity=712.9024, train_loss=6.5693445

Batch 207760, train_perplexity=679.4406, train_loss=6.52127

Batch 207770, train_perplexity=682.3809, train_loss=6.525588

Batch 207780, train_perplexity=639.6751, train_loss=6.4609604

Batch 207790, train_perplexity=780.0939, train_loss=6.6594143

Batch 207800, train_perplexity=633.623, train_loss=6.451454

Batch 207810, train_perplexity=710.8813, train_loss=6.5665054

Batch 207820, train_perplexity=683.87445, train_loss=6.5277743

Batch 207830, train_perplexity=714.4651, train_loss=6.571534

Batch 207840, train_perplexity=678.3684, train_loss=6.5196905

Batch 207850, train_perplexity=691.76385, train_loss=6.5392447

Batch 207860, train_perplexity=758.71674, train_loss=6.6316285

Batch 207870, train_perplexity=632.631, train_loss=6.4498873

Batch 207880, train_perplexity=724.03784, train_loss=6.5848436

Batch 207890, train_perplexity=743.29114, train_loss=6.611088

Batch 207900, train_perplexity=694.9928, train_loss=6.5439014

Batch 207910, train_perplexity=713.6588, train_loss=6.570405

Batch 207920, train_perplexity=734.4592, train_loss=6.5991344

Batch 207930, train_perplexity=731.71027, train_loss=6.5953846

Batch 207940, train_perplexity=721.3086, train_loss=6.581067

Batch 207950, train_perplexity=678.25323, train_loss=6.5195208

Batch 207960, train_perplexity=717.0419, train_loss=6.5751343

Batch 207970, train_perplexity=690.4075, train_loss=6.537282

Batch 207980, train_perplexity=710.6661, train_loss=6.5662026

Batch 207990, train_perplexity=687.2851, train_loss=6.532749

Batch 208000, train_perplexity=771.49097, train_loss=6.648325

Batch 208010, train_perplexity=734.9756, train_loss=6.5998373

Batch 208020, train_perplexity=758.2697, train_loss=6.631039

Batch 208030, train_perplexity=632.3044, train_loss=6.449371

Batch 208040, train_perplexity=672.9045, train_loss=6.5116034

Batch 208050, train_perplexity=818.0465, train_loss=6.706919

Batch 208060, train_perplexity=717.78217, train_loss=6.576166

Batch 208070, train_perplexity=673.1926, train_loss=6.5120316

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Batch 208080, train_perplexity=713.68774, train_loss=6.5704455

Batch 208090, train_perplexity=740.7987, train_loss=6.607729

Batch 208100, train_perplexity=696.8664, train_loss=6.5465937

Batch 208110, train_perplexity=730.77893, train_loss=6.594111

Batch 208120, train_perplexity=747.28955, train_loss=6.6164527

Batch 208130, train_perplexity=786.3502, train_loss=6.6674023

Batch 208140, train_perplexity=642.8751, train_loss=6.4659505

Batch 208150, train_perplexity=746.37964, train_loss=6.6152344

Batch 208160, train_perplexity=730.9828, train_loss=6.59439

Batch 208170, train_perplexity=759.61523, train_loss=6.632812

Batch 208180, train_perplexity=730.3943, train_loss=6.5935845

Batch 208190, train_perplexity=676.853, train_loss=6.517454
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 208200, train_perplexity=660.44586, train_loss=6.492915

Batch 208210, train_perplexity=723.0028, train_loss=6.583413

Batch 208220, train_perplexity=643.28906, train_loss=6.466594

Batch 208230, train_perplexity=678.59875, train_loss=6.52003

Batch 208240, train_perplexity=736.66394, train_loss=6.602132

Batch 208250, train_perplexity=715.4538, train_loss=6.572917

Batch 208260, train_perplexity=757.2482, train_loss=6.629691

Batch 208270, train_perplexity=728.78564, train_loss=6.5913796

Batch 208280, train_perplexity=680.5638, train_loss=6.5229216

Batch 208290, train_perplexity=694.0046, train_loss=6.5424786

Batch 208300, train_perplexity=763.4185, train_loss=6.6378064

Batch 208310, train_perplexity=647.3271, train_loss=6.4728518

Batch 208320, train_perplexity=758.9096, train_loss=6.6318827

Batch 208330, train_perplexity=614.64526, train_loss=6.4210453

Batch 208340, train_perplexity=689.8846, train_loss=6.5365243

Batch 208350, train_perplexity=693.1996, train_loss=6.541318

Batch 208360, train_perplexity=656.2046, train_loss=6.4864726

Batch 208370, train_perplexity=758.43567, train_loss=6.631258

Batch 208380, train_perplexity=719.429, train_loss=6.578458

Batch 208390, train_perplexity=667.8548, train_loss=6.5040708

Batch 208400, train_perplexity=672.40247, train_loss=6.510857

Batch 208410, train_perplexity=691.2119, train_loss=6.5384464

Batch 208420, train_perplexity=738.738, train_loss=6.6049433

Batch 208430, train_perplexity=671.18616, train_loss=6.5090466

Batch 208440, train_perplexity=676.8162, train_loss=6.5174

Batch 208450, train_perplexity=719.92175, train_loss=6.5791426

Batch 208460, train_perplexity=748.42175, train_loss=6.6179667

Batch 208470, train_perplexity=661.08295, train_loss=6.4938793

Batch 208480, train_perplexity=723.1142, train_loss=6.583567

Batch 208490, train_perplexity=705.75555, train_loss=6.559269

Batch 208500, train_perplexity=687.0931, train_loss=6.5324697

Batch 208510, train_perplexity=689.86316, train_loss=6.5364933

Batch 208520, train_perplexity=763.01495, train_loss=6.6372776

Batch 208530, train_perplexity=688.8839, train_loss=6.535073

Batch 208540, train_perplexity=709.82886, train_loss=6.565024

Batch 208550, train_perplexity=665.0192, train_loss=6.499816

Batch 208560, train_perplexity=720.61694, train_loss=6.5801077

Batch 208570, train_perplexity=721.03766, train_loss=6.5806913

Batch 208580, train_perplexity=676.3839, train_loss=6.516761

Batch 208590, train_perplexity=773.8563, train_loss=6.6513863

Batch 208600, train_perplexity=676.6465, train_loss=6.517149

Batch 208610, train_perplexity=700.584, train_loss=6.551914

Batch 208620, train_perplexity=745.3902, train_loss=6.613908

Batch 208630, train_perplexity=666.7497, train_loss=6.5024147

Batch 208640, train_perplexity=698.781, train_loss=6.5493374

Batch 208650, train_perplexity=662.37067, train_loss=6.4958253

Batch 208660, train_perplexity=645.93097, train_loss=6.4706926

Batch 208670, train_perplexity=684.62256, train_loss=6.5288677

Batch 208680, train_perplexity=767.18567, train_loss=6.642729

Batch 208690, train_perplexity=799.53973, train_loss=6.6840363

Batch 208700, train_perplexity=751.15576, train_loss=6.621613

Batch 208710, train_perplexity=728.6946, train_loss=6.5912547

Batch 208720, train_perplexity=752.0514, train_loss=6.6228046

Batch 208730, train_perplexity=686.5258, train_loss=6.531644

Batch 208740, train_perplexity=693.7766, train_loss=6.54215

Batch 208750, train_perplexity=691.3708, train_loss=6.5386763

Batch 208760, train_perplexity=752.2357, train_loss=6.6230497

Batch 208770, train_perplexity=807.4176, train_loss=6.693841

Batch 208780, train_perplexity=704.7245, train_loss=6.557807

Batch 208790, train_perplexity=697.8819, train_loss=6.54805

Batch 208800, train_perplexity=717.1835, train_loss=6.5753317

Batch 208810, train_perplexity=687.86475, train_loss=6.533592

Batch 208820, train_perplexity=696.8966, train_loss=6.546637

Batch 208830, train_perplexity=699.1066, train_loss=6.5498033

Batch 208840, train_perplexity=748.9444, train_loss=6.6186647

Batch 208850, train_perplexity=751.9112, train_loss=6.622618

Batch 208860, train_perplexity=708.1861, train_loss=6.562707

Batch 208870, train_perplexity=758.03186, train_loss=6.6307254

Batch 208880, train_perplexity=667.475, train_loss=6.503502

Batch 208890, train_perplexity=740.53314, train_loss=6.6073704

Batch 208900, train_perplexity=725.2306, train_loss=6.5864897

Batch 208910, train_perplexity=710.2809, train_loss=6.5656605

Batch 208920, train_perplexity=720.4541, train_loss=6.5798817

Batch 208930, train_perplexity=736.7286, train_loss=6.6022196

Batch 208940, train_perplexity=717.40063, train_loss=6.5756345

Batch 208950, train_perplexity=698.85095, train_loss=6.5494375

Batch 208960, train_perplexity=773.3178, train_loss=6.65069

Batch 208970, train_perplexity=722.9346, train_loss=6.5833187

Batch 208980, train_perplexity=694.477, train_loss=6.543159

Batch 208990, train_perplexity=765.9085, train_loss=6.6410627

Batch 209000, train_perplexity=666.6181, train_loss=6.5022173

Batch 209010, train_perplexity=687.68964, train_loss=6.5333376

Batch 209020, train_perplexity=750.0616, train_loss=6.6201553

Batch 209030, train_perplexity=727.33136, train_loss=6.589382

Batch 209040, train_perplexity=636.58875, train_loss=6.456124

Batch 209050, train_perplexity=695.4635, train_loss=6.5445786

Batch 209060, train_perplexity=718.06976, train_loss=6.5765667

Batch 209070, train_perplexity=776.5825, train_loss=6.654903

Batch 209080, train_perplexity=643.2645, train_loss=6.466556

Batch 209090, train_perplexity=708.99194, train_loss=6.563844

Batch 209100, train_perplexity=693.1771, train_loss=6.5412855

Batch 209110, train_perplexity=716.49915, train_loss=6.574377

Batch 209120, train_perplexity=697.8187, train_loss=6.5479593

Batch 209130, train_perplexity=692.05615, train_loss=6.539667

Batch 209140, train_perplexity=666.4026, train_loss=6.501894

Batch 209150, train_perplexity=775.5649, train_loss=6.6535916

Batch 209160, train_perplexity=646.9186, train_loss=6.4722204

Batch 209170, train_perplexity=692.1212, train_loss=6.539761

Batch 209180, train_perplexity=699.0446, train_loss=6.5497146

Batch 209190, train_perplexity=756.8053, train_loss=6.629106

Batch 209200, train_perplexity=735.42926, train_loss=6.6004543

Batch 209210, train_perplexity=651.6008, train_loss=6.479432

Batch 209220, train_perplexity=679.12573, train_loss=6.5208063

Batch 209230, train_perplexity=673.5619, train_loss=6.51258

Batch 209240, train_perplexity=705.1047, train_loss=6.5583463

Batch 209250, train_perplexity=706.96, train_loss=6.560974

Batch 209260, train_perplexity=777.5174, train_loss=6.656106

Batch 209270, train_perplexity=698.72534, train_loss=6.5492578

Batch 209280, train_perplexity=666.64417, train_loss=6.5022564

Batch 209290, train_perplexity=723.397, train_loss=6.583958

Batch 209300, train_perplexity=632.8603, train_loss=6.4502497

Batch 209310, train_perplexity=713.8739, train_loss=6.5707064

Batch 209320, train_perplexity=658.2984, train_loss=6.4896584

Batch 209330, train_perplexity=676.3949, train_loss=6.516777

Batch 209340, train_perplexity=704.30457, train_loss=6.557211

Batch 209350, train_perplexity=743.14014, train_loss=6.6108847

Batch 209360, train_perplexity=779.33954, train_loss=6.658447

Batch 209370, train_perplexity=773.5719, train_loss=6.6510186

Batch 209380, train_perplexity=688.17444, train_loss=6.5340424

Batch 209390, train_perplexity=696.68896, train_loss=6.546339

Batch 209400, train_perplexity=636.8417, train_loss=6.456521

Batch 209410, train_perplexity=728.6911, train_loss=6.59125

Batch 209420, train_perplexity=683.5468, train_loss=6.527295

Batch 209430, train_perplexity=642.6045, train_loss=6.4655294

Batch 209440, train_perplexity=705.53687, train_loss=6.558959

Batch 209450, train_perplexity=688.89215, train_loss=6.5350847

Batch 209460, train_perplexity=690.6996, train_loss=6.537705

Batch 209470, train_perplexity=722.18, train_loss=6.5822744

Batch 209480, train_perplexity=754.39386, train_loss=6.6259146

Batch 209490, train_perplexity=615.4424, train_loss=6.4223413

Batch 209500, train_perplexity=656.50757, train_loss=6.486934
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 209510, train_perplexity=677.69073, train_loss=6.518691

Batch 209520, train_perplexity=797.9797, train_loss=6.682083

Batch 209530, train_perplexity=714.15717, train_loss=6.571103

Batch 209540, train_perplexity=698.0813, train_loss=6.5483356

Batch 209550, train_perplexity=689.176, train_loss=6.5354967

Batch 209560, train_perplexity=680.48785, train_loss=6.52281

Batch 209570, train_perplexity=639.06964, train_loss=6.4600134

Batch 209580, train_perplexity=715.7626, train_loss=6.5733485

Batch 209590, train_perplexity=695.38855, train_loss=6.544471

Batch 209600, train_perplexity=680.58264, train_loss=6.522949

Batch 209610, train_perplexity=695.8775, train_loss=6.5451736

Batch 209620, train_perplexity=639.6348, train_loss=6.4608974

Batch 209630, train_perplexity=689.4183, train_loss=6.535848

Batch 209640, train_perplexity=630.95087, train_loss=6.447228

Batch 209650, train_perplexity=721.7139, train_loss=6.581629

Batch 209660, train_perplexity=740.7005, train_loss=6.6075964

Batch 209670, train_perplexity=764.95374, train_loss=6.6398153

Batch 209680, train_perplexity=726.2757, train_loss=6.5879297

Batch 209690, train_perplexity=649.1211, train_loss=6.4756193

Batch 209700, train_perplexity=700.1131, train_loss=6.551242

Batch 209710, train_perplexity=716.71716, train_loss=6.5746813

Batch 209720, train_perplexity=724.8593, train_loss=6.5859776

Batch 209730, train_perplexity=694.127, train_loss=6.542655

Batch 209740, train_perplexity=749.476, train_loss=6.6193743

Batch 209750, train_perplexity=698.93097, train_loss=6.549552

Batch 209760, train_perplexity=633.45715, train_loss=6.4511924

Batch 209770, train_perplexity=761.8399, train_loss=6.6357365

Batch 209780, train_perplexity=723.467, train_loss=6.584055

Batch 209790, train_perplexity=693.8815, train_loss=6.542301

Batch 209800, train_perplexity=648.79803, train_loss=6.4751215

Batch 209810, train_perplexity=714.5816, train_loss=6.571697

Batch 209820, train_perplexity=767.4293, train_loss=6.6430464

Batch 209830, train_perplexity=731.5637, train_loss=6.5951843

Batch 209840, train_perplexity=641.00574, train_loss=6.4630384

Batch 209850, train_perplexity=763.5849, train_loss=6.6380243

Batch 209860, train_perplexity=662.12463, train_loss=6.495454

Batch 209870, train_perplexity=763.8249, train_loss=6.6383386

Batch 209880, train_perplexity=761.81085, train_loss=6.6356983

Batch 209890, train_perplexity=684.32623, train_loss=6.5284348

Batch 209900, train_perplexity=671.6475, train_loss=6.5097337

Batch 209910, train_perplexity=736.6499, train_loss=6.602113

Batch 209920, train_perplexity=664.3682, train_loss=6.4988365

Batch 209930, train_perplexity=735.72455, train_loss=6.600856

Batch 209940, train_perplexity=715.4517, train_loss=6.572914

Batch 209950, train_perplexity=708.55396, train_loss=6.563226

Batch 209960, train_perplexity=735.21606, train_loss=6.6001644

Batch 209970, train_perplexity=679.5851, train_loss=6.5214825

Batch 209980, train_perplexity=781.4856, train_loss=6.6611967

Batch 209990, train_perplexity=733.02576, train_loss=6.597181

Batch 210000, train_perplexity=810.8089, train_loss=6.6980324

Batch 210010, train_perplexity=644.7047, train_loss=6.4687924

Batch 210020, train_perplexity=734.3394, train_loss=6.5989714

Batch 210030, train_perplexity=774.82153, train_loss=6.6526327

Batch 210040, train_perplexity=727.97705, train_loss=6.5902696

Batch 210050, train_perplexity=692.33606, train_loss=6.5400715

Batch 210060, train_perplexity=736.77075, train_loss=6.602277

Batch 210070, train_perplexity=663.1016, train_loss=6.496928

Batch 210080, train_perplexity=719.6047, train_loss=6.578702

Batch 210090, train_perplexity=775.4188, train_loss=6.6534033

Batch 210100, train_perplexity=727.9611, train_loss=6.5902476

Batch 210110, train_perplexity=700.60535, train_loss=6.5519447

Batch 210120, train_perplexity=745.5793, train_loss=6.6141615

Batch 210130, train_perplexity=739.0406, train_loss=6.605353

Batch 210140, train_perplexity=703.70874, train_loss=6.5563645

Batch 210150, train_perplexity=673.74115, train_loss=6.512846

Batch 210160, train_perplexity=682.9317, train_loss=6.526395

Batch 210170, train_perplexity=776.09204, train_loss=6.654271

Batch 210180, train_perplexity=708.53674, train_loss=6.563202

Batch 210190, train_perplexity=748.1049, train_loss=6.617543

Batch 210200, train_perplexity=662.32043, train_loss=6.4957495

Batch 210210, train_perplexity=684.85736, train_loss=6.5292106

Batch 210220, train_perplexity=736.54456, train_loss=6.6019697

Batch 210230, train_perplexity=688.17346, train_loss=6.534041

Batch 210240, train_perplexity=684.5743, train_loss=6.528797

Batch 210250, train_perplexity=756.7472, train_loss=6.6290293

Batch 210260, train_perplexity=647.836, train_loss=6.4736376

Batch 210270, train_perplexity=744.87354, train_loss=6.6132145

Batch 210280, train_perplexity=698.1136, train_loss=6.548382

Batch 210290, train_perplexity=746.38104, train_loss=6.6152363

Batch 210300, train_perplexity=691.8318, train_loss=6.539343

Batch 210310, train_perplexity=744.53906, train_loss=6.6127653

Batch 210320, train_perplexity=720.3242, train_loss=6.5797014

Batch 210330, train_perplexity=688.37335, train_loss=6.5343313

Batch 210340, train_perplexity=657.57245, train_loss=6.488555

Batch 210350, train_perplexity=688.8547, train_loss=6.5350304

Batch 210360, train_perplexity=706.89056, train_loss=6.560876

Batch 210370, train_perplexity=689.16876, train_loss=6.535486

Batch 210380, train_perplexity=696.84906, train_loss=6.546569

Batch 210390, train_perplexity=770.4785, train_loss=6.6470118

Batch 210400, train_perplexity=724.09033, train_loss=6.584916

Batch 210410, train_perplexity=704.865, train_loss=6.5580063

Batch 210420, train_perplexity=676.2159, train_loss=6.5165124

Batch 210430, train_perplexity=688.8127, train_loss=6.5349693

Batch 210440, train_perplexity=742.2895, train_loss=6.6097393

Batch 210450, train_perplexity=731.4835, train_loss=6.5950747

Batch 210460, train_perplexity=664.58875, train_loss=6.4991684

Batch 210470, train_perplexity=769.04224, train_loss=6.645146

Batch 210480, train_perplexity=657.0208, train_loss=6.4877157

Batch 210490, train_perplexity=685.3529, train_loss=6.529934

Batch 210500, train_perplexity=679.02734, train_loss=6.5206614

Batch 210510, train_perplexity=761.88495, train_loss=6.6357956

Batch 210520, train_perplexity=690.5715, train_loss=6.5375195

Batch 210530, train_perplexity=690.67816, train_loss=6.537674

Batch 210540, train_perplexity=699.822, train_loss=6.550826

Batch 210550, train_perplexity=610.69086, train_loss=6.414591

Batch 210560, train_perplexity=659.9233, train_loss=6.4921236

Batch 210570, train_perplexity=669.7961, train_loss=6.5069733

Batch 210580, train_perplexity=716.4377, train_loss=6.574291

Batch 210590, train_perplexity=722.95734, train_loss=6.58335

Batch 210600, train_perplexity=725.1002, train_loss=6.58631

Batch 210610, train_perplexity=688.79626, train_loss=6.5349455

Batch 210620, train_perplexity=650.1446, train_loss=6.477195

Batch 210630, train_perplexity=662.05804, train_loss=6.495353

Batch 210640, train_perplexity=762.4163, train_loss=6.6364927

Batch 210650, train_perplexity=685.9761, train_loss=6.530843

Batch 210660, train_perplexity=714.36224, train_loss=6.57139

Batch 210670, train_perplexity=672.00665, train_loss=6.510268

Batch 210680, train_perplexity=761.07745, train_loss=6.634735

Batch 210690, train_perplexity=729.1544, train_loss=6.5918856

Batch 210700, train_perplexity=752.84216, train_loss=6.6238556

Batch 210710, train_perplexity=712.1719, train_loss=6.5683193

Batch 210720, train_perplexity=738.98883, train_loss=6.605283

Batch 210730, train_perplexity=697.1389, train_loss=6.5469847

Batch 210740, train_perplexity=663.515, train_loss=6.4975514

Batch 210750, train_perplexity=693.7472, train_loss=6.5421076

Batch 210760, train_perplexity=722.24133, train_loss=6.5823593

Batch 210770, train_perplexity=661.70233, train_loss=6.494816

Batch 210780, train_perplexity=701.7629, train_loss=6.5535955

Batch 210790, train_perplexity=702.4599, train_loss=6.5545883

Batch 210800, train_perplexity=714.3091, train_loss=6.571316

Batch 210810, train_perplexity=737.5539, train_loss=6.603339
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 210820, train_perplexity=725.8897, train_loss=6.587398

Batch 210830, train_perplexity=694.6777, train_loss=6.543448

Batch 210840, train_perplexity=743.3386, train_loss=6.6111517

Batch 210850, train_perplexity=796.923, train_loss=6.680758

Batch 210860, train_perplexity=706.6678, train_loss=6.5605607

Batch 210870, train_perplexity=679.17175, train_loss=6.520874

Batch 210880, train_perplexity=696.5707, train_loss=6.5461693

Batch 210890, train_perplexity=676.1485, train_loss=6.5164127

Batch 210900, train_perplexity=695.6612, train_loss=6.5448627

Batch 210910, train_perplexity=696.068, train_loss=6.5454473

Batch 210920, train_perplexity=758.88715, train_loss=6.631853

Batch 210930, train_perplexity=599.63965, train_loss=6.396329

Batch 210940, train_perplexity=677.3506, train_loss=6.518189

Batch 210950, train_perplexity=693.5249, train_loss=6.541787

Batch 210960, train_perplexity=709.8299, train_loss=6.5650253

Batch 210970, train_perplexity=829.5785, train_loss=6.7209177

Batch 210980, train_perplexity=659.11316, train_loss=6.4908953

Batch 210990, train_perplexity=679.2216, train_loss=6.5209475

Batch 211000, train_perplexity=718.75934, train_loss=6.5775266

Batch 211010, train_perplexity=706.4236, train_loss=6.560215

Batch 211020, train_perplexity=666.3549, train_loss=6.5018225

Batch 211030, train_perplexity=704.2059, train_loss=6.5570707

Batch 211040, train_perplexity=717.7086, train_loss=6.5760636

Batch 211050, train_perplexity=734.97736, train_loss=6.5998397

Batch 211060, train_perplexity=663.3578, train_loss=6.4973145

Batch 211070, train_perplexity=657.1136, train_loss=6.487857

Batch 211080, train_perplexity=637.9565, train_loss=6.45827

Batch 211090, train_perplexity=749.3884, train_loss=6.6192575

Batch 211100, train_perplexity=767.24927, train_loss=6.642812

Batch 211110, train_perplexity=695.60547, train_loss=6.5447826

Batch 211120, train_perplexity=708.12366, train_loss=6.5626187

Batch 211130, train_perplexity=728.5327, train_loss=6.5910325

Batch 211140, train_perplexity=626.52155, train_loss=6.440183

Batch 211150, train_perplexity=784.6363, train_loss=6.6652203

Batch 211160, train_perplexity=685.92737, train_loss=6.5307717

Batch 211170, train_perplexity=735.03516, train_loss=6.5999184

Batch 211180, train_perplexity=738.8627, train_loss=6.605112

Batch 211190, train_perplexity=708.7787, train_loss=6.5635433

Batch 211200, train_perplexity=739.7969, train_loss=6.6063757

Batch 211210, train_perplexity=728.0687, train_loss=6.5903955

Batch 211220, train_perplexity=710.4245, train_loss=6.5658627

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Batch 211230, train_perplexity=704.66675, train_loss=6.557725

Batch 211240, train_perplexity=715.17, train_loss=6.5725203

Batch 211250, train_perplexity=710.10004, train_loss=6.565406

Batch 211260, train_perplexity=726.47174, train_loss=6.5881996

Batch 211270, train_perplexity=708.09564, train_loss=6.562579

Batch 211280, train_perplexity=684.1742, train_loss=6.5282125

Batch 211290, train_perplexity=770.5329, train_loss=6.6470823

Batch 211300, train_perplexity=812.5436, train_loss=6.7001696

Batch 211310, train_perplexity=704.56323, train_loss=6.557578

Batch 211320, train_perplexity=670.4204, train_loss=6.507905

Batch 211330, train_perplexity=729.83795, train_loss=6.5928226

Batch 211340, train_perplexity=671.4032, train_loss=6.50937

Batch 211350, train_perplexity=804.42316, train_loss=6.6901255

Batch 211360, train_perplexity=718.0581, train_loss=6.5765505

Batch 211370, train_perplexity=712.5999, train_loss=6.56892

Batch 211380, train_perplexity=758.4733, train_loss=6.6313076

Batch 211390, train_perplexity=674.0933, train_loss=6.5133686

Batch 211400, train_perplexity=668.0491, train_loss=6.5043616

Batch 211410, train_perplexity=687.20514, train_loss=6.532633

Batch 211420, train_perplexity=704.4853, train_loss=6.5574675

Batch 211430, train_perplexity=699.41437, train_loss=6.5502434

Batch 211440, train_perplexity=779.4387, train_loss=6.658574

Batch 211450, train_perplexity=682.8301, train_loss=6.526246

Batch 211460, train_perplexity=678.1485, train_loss=6.5193663

Batch 211470, train_perplexity=614.8692, train_loss=6.4214096

Batch 211480, train_perplexity=722.888, train_loss=6.5832543

Batch 211490, train_perplexity=610.3948, train_loss=6.414106

Batch 211500, train_perplexity=689.6297, train_loss=6.5361547

Batch 211510, train_perplexity=698.025, train_loss=6.548255

Batch 211520, train_perplexity=667.92773, train_loss=6.50418

Batch 211530, train_perplexity=745.5935, train_loss=6.6141806

Batch 211540, train_perplexity=847.62823, train_loss=6.742442

Batch 211550, train_perplexity=710.78906, train_loss=6.5663757

Batch 211560, train_perplexity=658.5791, train_loss=6.4900846

Batch 211570, train_perplexity=705.3377, train_loss=6.5586767

Batch 211580, train_perplexity=690.2235, train_loss=6.5370154

Batch 211590, train_perplexity=723.7679, train_loss=6.5844707

Batch 211600, train_perplexity=723.6844, train_loss=6.5843554

Batch 211610, train_perplexity=615.0416, train_loss=6.42169

Batch 211620, train_perplexity=695.47546, train_loss=6.5445957

Batch 211630, train_perplexity=689.435, train_loss=6.5358725

Batch 211640, train_perplexity=726.4458, train_loss=6.588164

Batch 211650, train_perplexity=670.0554, train_loss=6.5073605

Batch 211660, train_perplexity=770.23236, train_loss=6.6466923

Batch 211670, train_perplexity=739.92957, train_loss=6.606555

Batch 211680, train_perplexity=603.6843, train_loss=6.4030514

Batch 211690, train_perplexity=683.09973, train_loss=6.526641

Batch 211700, train_perplexity=629.8065, train_loss=6.4454126

Batch 211710, train_perplexity=601.03375, train_loss=6.398651

Batch 211720, train_perplexity=723.67126, train_loss=6.584337

Batch 211730, train_perplexity=662.01355, train_loss=6.495286

Batch 211740, train_perplexity=673.6313, train_loss=6.512683

Batch 211750, train_perplexity=705.0922, train_loss=6.5583286

Batch 211760, train_perplexity=682.7077, train_loss=6.526067

Batch 211770, train_perplexity=698.72235, train_loss=6.5492535

Batch 211780, train_perplexity=742.5469, train_loss=6.610086

Batch 211790, train_perplexity=662.6657, train_loss=6.4962707

Batch 211800, train_perplexity=738.7309, train_loss=6.6049337

Batch 211810, train_perplexity=701.4183, train_loss=6.5531044

Batch 211820, train_perplexity=677.05444, train_loss=6.5177517

Batch 211830, train_perplexity=662.45306, train_loss=6.4959497

Batch 211840, train_perplexity=738.3801, train_loss=6.604459

Batch 211850, train_perplexity=704.0279, train_loss=6.556818

Batch 211860, train_perplexity=665.562, train_loss=6.500632

Batch 211870, train_perplexity=726.92224, train_loss=6.5888195

Batch 211880, train_perplexity=692.1862, train_loss=6.539855

Batch 211890, train_perplexity=689.5626, train_loss=6.5360575

Batch 211900, train_perplexity=577.97437, train_loss=6.3595295

Batch 211910, train_perplexity=673.3018, train_loss=6.5121937

Batch 211920, train_perplexity=690.87085, train_loss=6.537953

Batch 211930, train_perplexity=647.61365, train_loss=6.4732943

Batch 211940, train_perplexity=758.6701, train_loss=6.631567

Batch 211950, train_perplexity=771.5263, train_loss=6.6483707

Batch 211960, train_perplexity=711.41095, train_loss=6.5672503

Batch 211970, train_perplexity=735.5127, train_loss=6.600568

Batch 211980, train_perplexity=736.13794, train_loss=6.6014175

Batch 211990, train_perplexity=752.6146, train_loss=6.6235533

Batch 212000, train_perplexity=717.2806, train_loss=6.575467

Batch 212010, train_perplexity=721.28314, train_loss=6.581032

Batch 212020, train_perplexity=638.03375, train_loss=6.458391

Batch 212030, train_perplexity=689.9764, train_loss=6.5366573

Batch 212040, train_perplexity=684.9295, train_loss=6.529316

Batch 212050, train_perplexity=772.2293, train_loss=6.6492815

Batch 212060, train_perplexity=729.6407, train_loss=6.592552
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 212070, train_perplexity=672.03674, train_loss=6.510313

Batch 212080, train_perplexity=732.0348, train_loss=6.595828

Batch 212090, train_perplexity=746.7513, train_loss=6.615732

Batch 212100, train_perplexity=719.8078, train_loss=6.5789843

Batch 212110, train_perplexity=721.01184, train_loss=6.5806556

Batch 212120, train_perplexity=636.0435, train_loss=6.455267

Batch 212130, train_perplexity=689.9964, train_loss=6.5366864

Batch 212140, train_perplexity=785.1217, train_loss=6.6658387

Batch 212150, train_perplexity=688.15216, train_loss=6.53401

Batch 212160, train_perplexity=643.8657, train_loss=6.46749

Batch 212170, train_perplexity=720.5949, train_loss=6.580077

Batch 212180, train_perplexity=754.80585, train_loss=6.6264606

Batch 212190, train_perplexity=736.6239, train_loss=6.6020775

Batch 212200, train_perplexity=701.30457, train_loss=6.5529423

Batch 212210, train_perplexity=687.32574, train_loss=6.5328083

Batch 212220, train_perplexity=776.0939, train_loss=6.6542735

Batch 212230, train_perplexity=701.07117, train_loss=6.5526094

Batch 212240, train_perplexity=679.80225, train_loss=6.521802

Batch 212250, train_perplexity=701.8666, train_loss=6.5537434

Batch 212260, train_perplexity=708.1726, train_loss=6.562688

Batch 212270, train_perplexity=654.85144, train_loss=6.4844084

Batch 212280, train_perplexity=645.3362, train_loss=6.4697714

Batch 212290, train_perplexity=664.4306, train_loss=6.4989305

Batch 212300, train_perplexity=654.7315, train_loss=6.4842253

Batch 212310, train_perplexity=706.95734, train_loss=6.5609703

Batch 212320, train_perplexity=745.8225, train_loss=6.6144876

Batch 212330, train_perplexity=689.4666, train_loss=6.535918

Batch 212340, train_perplexity=660.148, train_loss=6.492464

Batch 212350, train_perplexity=745.85345, train_loss=6.614529

Batch 212360, train_perplexity=698.2617, train_loss=6.548594

Batch 212370, train_perplexity=699.73193, train_loss=6.5506973

Batch 212380, train_perplexity=696.4621, train_loss=6.5460134

Batch 212390, train_perplexity=701.4417, train_loss=6.553138

Batch 212400, train_perplexity=700.9883, train_loss=6.552491

Batch 212410, train_perplexity=699.5031, train_loss=6.55037

Batch 212420, train_perplexity=675.7894, train_loss=6.5158815

Batch 212430, train_perplexity=710.47833, train_loss=6.5659385

Batch 212440, train_perplexity=668.4279, train_loss=6.5049286

Batch 212450, train_perplexity=645.63074, train_loss=6.4702277

Batch 212460, train_perplexity=704.8576, train_loss=6.557996

Batch 212470, train_perplexity=695.8951, train_loss=6.545199

Batch 212480, train_perplexity=667.61694, train_loss=6.5037146

Batch 212490, train_perplexity=735.7642, train_loss=6.6009097

Batch 212500, train_perplexity=731.63556, train_loss=6.5952826

Batch 212510, train_perplexity=717.86707, train_loss=6.5762844

Batch 212520, train_perplexity=688.0032, train_loss=6.5337934

Batch 212530, train_perplexity=703.12476, train_loss=6.5555344

Batch 212540, train_perplexity=703.41315, train_loss=6.5559444

Batch 212550, train_perplexity=682.7454, train_loss=6.526122

Batch 212560, train_perplexity=692.3895, train_loss=6.5401487

Batch 212570, train_perplexity=676.42645, train_loss=6.516824

Batch 212580, train_perplexity=690.87445, train_loss=6.537958

Batch 212590, train_perplexity=661.66766, train_loss=6.4947634

Batch 212600, train_perplexity=675.4396, train_loss=6.5153637

Batch 212610, train_perplexity=741.8394, train_loss=6.609133

Batch 212620, train_perplexity=666.40643, train_loss=6.5018997

Batch 212630, train_perplexity=763.92505, train_loss=6.6384697

Batch 212640, train_perplexity=632.69434, train_loss=6.4499874

Batch 212650, train_perplexity=714.2403, train_loss=6.5712194

Batch 212660, train_perplexity=691.07416, train_loss=6.538247

Batch 212670, train_perplexity=668.89026, train_loss=6.50562

Batch 212680, train_perplexity=724.216, train_loss=6.5850897

Batch 212690, train_perplexity=636.6103, train_loss=6.4561577

Batch 212700, train_perplexity=701.75684, train_loss=6.553587

Batch 212710, train_perplexity=680.569, train_loss=6.522929

Batch 212720, train_perplexity=733.4971, train_loss=6.5978236

Batch 212730, train_perplexity=678.96545, train_loss=6.5205703

Batch 212740, train_perplexity=695.8052, train_loss=6.5450697

Batch 212750, train_perplexity=619.0319, train_loss=6.428157

Batch 212760, train_perplexity=698.2601, train_loss=6.5485916

Batch 212770, train_perplexity=740.6917, train_loss=6.6075845

Batch 212780, train_perplexity=675.3584, train_loss=6.5152435

Batch 212790, train_perplexity=682.3848, train_loss=6.5255938

Batch 212800, train_perplexity=704.2294, train_loss=6.557104

Batch 212810, train_perplexity=693.2022, train_loss=6.5413218

Batch 212820, train_perplexity=727.64355, train_loss=6.5898113

Batch 212830, train_perplexity=782.5349, train_loss=6.6625385

Batch 212840, train_perplexity=727.78894, train_loss=6.590011

Batch 212850, train_perplexity=786.06903, train_loss=6.6670446

Batch 212860, train_perplexity=630.87836, train_loss=6.447113

Batch 212870, train_perplexity=679.1138, train_loss=6.5207887

Batch 212880, train_perplexity=702.70514, train_loss=6.5549374

Batch 212890, train_perplexity=771.9859, train_loss=6.6489663

Batch 212900, train_perplexity=699.3737, train_loss=6.550185

Batch 212910, train_perplexity=737.5596, train_loss=6.603347

Batch 212920, train_perplexity=670.6826, train_loss=6.508296

Batch 212930, train_perplexity=686.2031, train_loss=6.5311737

Batch 212940, train_perplexity=712.48846, train_loss=6.5687637

Batch 212950, train_perplexity=689.24634, train_loss=6.5355988

Batch 212960, train_perplexity=634.6756, train_loss=6.453114

Batch 212970, train_perplexity=651.5219, train_loss=6.479311

Batch 212980, train_perplexity=714.4937, train_loss=6.571574

Batch 212990, train_perplexity=727.50964, train_loss=6.5896273

Batch 213000, train_perplexity=727.68384, train_loss=6.5898666

Batch 213010, train_perplexity=722.3784, train_loss=6.582549

Batch 213020, train_perplexity=604.8412, train_loss=6.404966

Batch 213030, train_perplexity=687.78406, train_loss=6.533475

Batch 213040, train_perplexity=733.85535, train_loss=6.598312

Batch 213050, train_perplexity=617.66, train_loss=6.425938

Batch 213060, train_perplexity=657.6797, train_loss=6.488718

Batch 213070, train_perplexity=766.6667, train_loss=6.642052

Batch 213080, train_perplexity=626.35754, train_loss=6.4399214

Batch 213090, train_perplexity=702.548, train_loss=6.5547137

Batch 213100, train_perplexity=810.91095, train_loss=6.6981583

Batch 213110, train_perplexity=693.5226, train_loss=6.541784

Batch 213120, train_perplexity=613.24884, train_loss=6.418771

Batch 213130, train_perplexity=714.62897, train_loss=6.5717635

Batch 213140, train_perplexity=757.6412, train_loss=6.63021

Batch 213150, train_perplexity=657.1346, train_loss=6.487889

Batch 213160, train_perplexity=597.4483, train_loss=6.392668

Batch 213170, train_perplexity=687.35126, train_loss=6.5328455

Batch 213180, train_perplexity=740.12714, train_loss=6.606822

Batch 213190, train_perplexity=650.7947, train_loss=6.478194

Batch 213200, train_perplexity=780.68854, train_loss=6.6601763

Batch 213210, train_perplexity=679.09595, train_loss=6.5207624

Batch 213220, train_perplexity=695.981, train_loss=6.5453224

Batch 213230, train_perplexity=667.7182, train_loss=6.503866

Batch 213240, train_perplexity=642.2063, train_loss=6.4649096

Batch 213250, train_perplexity=762.0935, train_loss=6.6360693

Batch 213260, train_perplexity=720.24524, train_loss=6.5795918

Batch 213270, train_perplexity=699.62115, train_loss=6.550539

Batch 213280, train_perplexity=662.23895, train_loss=6.4956264

Batch 213290, train_perplexity=619.78394, train_loss=6.429371

Batch 213300, train_perplexity=734.42035, train_loss=6.5990815

Batch 213310, train_perplexity=704.44196, train_loss=6.557406

Batch 213320, train_perplexity=688.7785, train_loss=6.5349197

Batch 213330, train_perplexity=776.9807, train_loss=6.6554155

Batch 213340, train_perplexity=679.6875, train_loss=6.521633

Batch 213350, train_perplexity=711.7856, train_loss=6.5677767

Batch 213360, train_perplexity=637.2463, train_loss=6.457156

Batch 213370, train_perplexity=681.1215, train_loss=6.523741
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 213380, train_perplexity=744.62, train_loss=6.612874

Batch 213390, train_perplexity=661.85443, train_loss=6.4950457

Batch 213400, train_perplexity=686.77405, train_loss=6.5320053

Batch 213410, train_perplexity=718.9636, train_loss=6.577811

Batch 213420, train_perplexity=759.8489, train_loss=6.6331196

Batch 213430, train_perplexity=758.4856, train_loss=6.631324

Batch 213440, train_perplexity=750.5314, train_loss=6.6207814

Batch 213450, train_perplexity=729.0244, train_loss=6.591707

Batch 213460, train_perplexity=691.6339, train_loss=6.539057

Batch 213470, train_perplexity=721.58966, train_loss=6.5814567

Batch 213480, train_perplexity=736.046, train_loss=6.6012926

Batch 213490, train_perplexity=714.38196, train_loss=6.571418

Batch 213500, train_perplexity=718.69934, train_loss=6.577443

Batch 213510, train_perplexity=729.023, train_loss=6.5917053

Batch 213520, train_perplexity=713.88074, train_loss=6.570716

Batch 213530, train_perplexity=687.465, train_loss=6.533011

Batch 213540, train_perplexity=626.86816, train_loss=6.4407363

Batch 213550, train_perplexity=674.558, train_loss=6.5140576

Batch 213560, train_perplexity=679.1018, train_loss=6.520771

Batch 213570, train_perplexity=668.56085, train_loss=6.5051274

Batch 213580, train_perplexity=716.2238, train_loss=6.5739927

Batch 213590, train_perplexity=687.0023, train_loss=6.5323377

Batch 213600, train_perplexity=759.871, train_loss=6.6331487

Batch 213610, train_perplexity=712.30707, train_loss=6.568509

Batch 213620, train_perplexity=709.1634, train_loss=6.564086

Batch 213630, train_perplexity=690.48846, train_loss=6.5373993

Batch 213640, train_perplexity=678.80457, train_loss=6.5203333

Batch 213650, train_perplexity=712.38586, train_loss=6.5686197

Batch 213660, train_perplexity=672.9725, train_loss=6.5117044

Batch 213670, train_perplexity=632.11176, train_loss=6.449066

Batch 213680, train_perplexity=697.2456, train_loss=6.5471377

Batch 213690, train_perplexity=682.5944, train_loss=6.525901

Batch 213700, train_perplexity=626.31213, train_loss=6.439849

Batch 213710, train_perplexity=673.6313, train_loss=6.512683

Batch 213720, train_perplexity=688.3129, train_loss=6.5342436

Batch 213730, train_perplexity=722.07983, train_loss=6.5821357

Batch 213740, train_perplexity=738.22986, train_loss=6.604255

Batch 213750, train_perplexity=725.5952, train_loss=6.5869923

Batch 213760, train_perplexity=738.29987, train_loss=6.60435

Batch 213770, train_perplexity=716.3191, train_loss=6.574126

Batch 213780, train_perplexity=683.21765, train_loss=6.5268135

Batch 213790, train_perplexity=680.5132, train_loss=6.522847

Batch 213800, train_perplexity=690.4154, train_loss=6.5372934

Batch 213810, train_perplexity=689.7122, train_loss=6.5362744

Batch 213820, train_perplexity=694.7635, train_loss=6.5435715

Batch 213830, train_perplexity=698.5451, train_loss=6.549

Batch 213840, train_perplexity=734.82214, train_loss=6.5996284

Batch 213850, train_perplexity=762.8265, train_loss=6.6370306

Batch 213860, train_perplexity=715.3364, train_loss=6.572753

Batch 213870, train_perplexity=705.47766, train_loss=6.558875

Batch 213880, train_perplexity=654.08496, train_loss=6.4832373

Batch 213890, train_perplexity=762.7468, train_loss=6.636926

Batch 213900, train_perplexity=686.36346, train_loss=6.5314074

Batch 213910, train_perplexity=649.97784, train_loss=6.4769382

Batch 213920, train_perplexity=673.14386, train_loss=6.511959

Batch 213930, train_perplexity=753.9192, train_loss=6.625285

Batch 213940, train_perplexity=712.5109, train_loss=6.568795

Batch 213950, train_perplexity=765.3799, train_loss=6.6403723

Batch 213960, train_perplexity=832.5847, train_loss=6.724535

Batch 213970, train_perplexity=628.6022, train_loss=6.4434986

Batch 213980, train_perplexity=736.55365, train_loss=6.601982

Batch 213990, train_perplexity=686.44666, train_loss=6.5315285

Batch 214000, train_perplexity=665.3256, train_loss=6.5002766

Batch 214010, train_perplexity=686.09485, train_loss=6.531016

Batch 214020, train_perplexity=741.116, train_loss=6.608157

Batch 214030, train_perplexity=682.8581, train_loss=6.526287

Batch 214040, train_perplexity=678.84796, train_loss=6.520397

Batch 214050, train_perplexity=689.51984, train_loss=6.5359955

Batch 214060, train_perplexity=656.9713, train_loss=6.4876404

Batch 214070, train_perplexity=723.30316, train_loss=6.5838284

Batch 214080, train_perplexity=647.70135, train_loss=6.4734297

Batch 214090, train_perplexity=772.258, train_loss=6.6493187

Batch 214100, train_perplexity=710.07025, train_loss=6.565364

Batch 214110, train_perplexity=676.8582, train_loss=6.517462

Batch 214120, train_perplexity=706.7399, train_loss=6.5606627

Batch 214130, train_perplexity=740.70264, train_loss=6.6075993

Batch 214140, train_perplexity=671.7933, train_loss=6.5099506

Batch 214150, train_perplexity=718.759, train_loss=6.577526

Batch 214160, train_perplexity=652.4291, train_loss=6.4807024

Batch 214170, train_perplexity=680.5898, train_loss=6.5229597

Batch 214180, train_perplexity=660.4956, train_loss=6.4929905

Batch 214190, train_perplexity=672.3707, train_loss=6.51081

Batch 214200, train_perplexity=656.43964, train_loss=6.4868307

Batch 214210, train_perplexity=669.4442, train_loss=6.506448

Batch 214220, train_perplexity=626.5015, train_loss=6.440151

Batch 214230, train_perplexity=669.91614, train_loss=6.5071526

Batch 214240, train_perplexity=686.79663, train_loss=6.532038

Batch 214250, train_perplexity=741.98724, train_loss=6.609332

Batch 214260, train_perplexity=643.2416, train_loss=6.4665203

Batch 214270, train_perplexity=671.2249, train_loss=6.5091043

Batch 214280, train_perplexity=647.312, train_loss=6.4728284

Batch 214290, train_perplexity=661.5115, train_loss=6.4945273

Batch 214300, train_perplexity=666.3549, train_loss=6.5018225

Batch 214310, train_perplexity=730.7298, train_loss=6.5940437

Batch 214320, train_perplexity=707.9093, train_loss=6.562316

Batch 214330, train_perplexity=706.8363, train_loss=6.560799

Batch 214340, train_perplexity=665.7004, train_loss=6.5008397

Batch 214350, train_perplexity=690.7773, train_loss=6.5378175

Batch 214360, train_perplexity=680.9806, train_loss=6.523534

Batch 214370, train_perplexity=698.9676, train_loss=6.5496044

Batch 214380, train_perplexity=756.32947, train_loss=6.628477

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Batch 214390, train_perplexity=657.57495, train_loss=6.488559

Batch 214400, train_perplexity=780.95886, train_loss=6.6605225

Batch 214410, train_perplexity=692.9018, train_loss=6.5408883

Batch 214420, train_perplexity=674.27277, train_loss=6.5136347

Batch 214430, train_perplexity=709.83154, train_loss=6.5650277

Batch 214440, train_perplexity=762.2389, train_loss=6.63626

Batch 214450, train_perplexity=654.94574, train_loss=6.4845524

Batch 214460, train_perplexity=686.09973, train_loss=6.531023

Batch 214470, train_perplexity=680.74524, train_loss=6.523188

Batch 214480, train_perplexity=714.18036, train_loss=6.5711355

Batch 214490, train_perplexity=723.9757, train_loss=6.584758

Batch 214500, train_perplexity=768.041, train_loss=6.643843

Batch 214510, train_perplexity=774.4551, train_loss=6.6521597

Batch 214520, train_perplexity=654.3143, train_loss=6.4835877

Batch 214530, train_perplexity=668.22205, train_loss=6.5046206

Batch 214540, train_perplexity=708.16046, train_loss=6.5626707

Batch 214550, train_perplexity=730.70435, train_loss=6.594009

Batch 214560, train_perplexity=720.76575, train_loss=6.580314

Batch 214570, train_perplexity=775.4129, train_loss=6.6533957

Batch 214580, train_perplexity=721.61237, train_loss=6.581488

Batch 214590, train_perplexity=639.719, train_loss=6.461029

Batch 214600, train_perplexity=635.90826, train_loss=6.4550543

Batch 214610, train_perplexity=750.4029, train_loss=6.62061

Batch 214620, train_perplexity=678.8467, train_loss=6.5203953
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 214630, train_perplexity=734.6042, train_loss=6.599332

Batch 214640, train_perplexity=722.2131, train_loss=6.58232

Batch 214650, train_perplexity=677.37994, train_loss=6.5182323

Batch 214660, train_perplexity=644.804, train_loss=6.4689465

Batch 214670, train_perplexity=762.12115, train_loss=6.6361055

Batch 214680, train_perplexity=647.0839, train_loss=6.472476

Batch 214690, train_perplexity=672.5035, train_loss=6.5110073

Batch 214700, train_perplexity=723.5891, train_loss=6.5842237

Batch 214710, train_perplexity=671.75995, train_loss=6.509901

Batch 214720, train_perplexity=673.3821, train_loss=6.512313

Batch 214730, train_perplexity=735.5562, train_loss=6.600627

Batch 214740, train_perplexity=672.6247, train_loss=6.5111876

Batch 214750, train_perplexity=729.592, train_loss=6.5924854

Batch 214760, train_perplexity=634.7667, train_loss=6.4532576

Batch 214770, train_perplexity=638.5165, train_loss=6.4591475

Batch 214780, train_perplexity=679.6862, train_loss=6.5216312

Batch 214790, train_perplexity=703.61176, train_loss=6.5562267

Batch 214800, train_perplexity=686.8356, train_loss=6.532095

Batch 214810, train_perplexity=742.65204, train_loss=6.6102276

Batch 214820, train_perplexity=659.8088, train_loss=6.49195

Batch 214830, train_perplexity=714.5721, train_loss=6.571684

Batch 214840, train_perplexity=674.8627, train_loss=6.514509

Batch 214850, train_perplexity=665.5325, train_loss=6.5005875

Batch 214860, train_perplexity=706.7005, train_loss=6.560607

Batch 214870, train_perplexity=652.5174, train_loss=6.480838

Batch 214880, train_perplexity=634.2826, train_loss=6.4524946

Batch 214890, train_perplexity=788.55505, train_loss=6.6702023

Batch 214900, train_perplexity=725.4115, train_loss=6.586739

Batch 214910, train_perplexity=731.69977, train_loss=6.5953703

Batch 214920, train_perplexity=733.3911, train_loss=6.597679

Batch 214930, train_perplexity=720.7774, train_loss=6.5803304

Batch 214940, train_perplexity=631.57904, train_loss=6.448223

Batch 214950, train_perplexity=755.0143, train_loss=6.6267366

Batch 214960, train_perplexity=726.2868, train_loss=6.587945

Batch 214970, train_perplexity=662.1284, train_loss=6.4954596

Batch 214980, train_perplexity=618.10254, train_loss=6.4266543

Batch 214990, train_perplexity=625.6668, train_loss=6.438818

Batch 215000, train_perplexity=663.67004, train_loss=6.497785

Batch 215010, train_perplexity=665.9655, train_loss=6.501238

Batch 215020, train_perplexity=719.1361, train_loss=6.5780506

Batch 215030, train_perplexity=755.1025, train_loss=6.6268535

Batch 215040, train_perplexity=753.52954, train_loss=6.6247683

Batch 215050, train_perplexity=674.98364, train_loss=6.5146885

Batch 215060, train_perplexity=725.9873, train_loss=6.5875325

Batch 215070, train_perplexity=690.9654, train_loss=6.5380898

Batch 215080, train_perplexity=724.24915, train_loss=6.5851355

Batch 215090, train_perplexity=766.98157, train_loss=6.6424627

Batch 215100, train_perplexity=639.91125, train_loss=6.4613295

Batch 215110, train_perplexity=662.68054, train_loss=6.496293

Batch 215120, train_perplexity=644.9719, train_loss=6.469207

Batch 215130, train_perplexity=716.1156, train_loss=6.5738416

Batch 215140, train_perplexity=732.2736, train_loss=6.596154

Batch 215150, train_perplexity=747.4139, train_loss=6.616619

Batch 215160, train_perplexity=722.96423, train_loss=6.5833597

Batch 215170, train_perplexity=621.12, train_loss=6.4315243

Batch 215180, train_perplexity=722.82324, train_loss=6.5831647

Batch 215190, train_perplexity=650.69574, train_loss=6.478042

Batch 215200, train_perplexity=661.53925, train_loss=6.4945693

Batch 215210, train_perplexity=740.435, train_loss=6.607238

Batch 215220, train_perplexity=697.41785, train_loss=6.5473847

Batch 215230, train_perplexity=616.5471, train_loss=6.4241347

Batch 215240, train_perplexity=579.2485, train_loss=6.3617315

Batch 215250, train_perplexity=720.69116, train_loss=6.5802107

Batch 215260, train_perplexity=669.69037, train_loss=6.5068154

Batch 215270, train_perplexity=725.22644, train_loss=6.586484

Batch 215280, train_perplexity=615.3591, train_loss=6.422206

Batch 215290, train_perplexity=719.7591, train_loss=6.5789165

Batch 215300, train_perplexity=678.5172, train_loss=6.51991

Batch 215310, train_perplexity=698.56445, train_loss=6.5490274

Batch 215320, train_perplexity=661.3806, train_loss=6.4943295

Batch 215330, train_perplexity=685.0203, train_loss=6.5294485

Batch 215340, train_perplexity=703.5715, train_loss=6.5561695

Batch 215350, train_perplexity=740.66064, train_loss=6.6075425

Batch 215360, train_perplexity=741.0764, train_loss=6.6081038

Batch 215370, train_perplexity=768.3044, train_loss=6.644186

Batch 215380, train_perplexity=721.0579, train_loss=6.5807195

Batch 215390, train_perplexity=712.967, train_loss=6.569435

Batch 215400, train_perplexity=660.0196, train_loss=6.4922695

Batch 215410, train_perplexity=697.34705, train_loss=6.547283

Batch 215420, train_perplexity=799.5169, train_loss=6.6840076

Batch 215430, train_perplexity=705.445, train_loss=6.558829

Batch 215440, train_perplexity=643.32684, train_loss=6.466653

Batch 215450, train_perplexity=708.5874, train_loss=6.5632734

Batch 215460, train_perplexity=701.41626, train_loss=6.5531015

Batch 215470, train_perplexity=682.04456, train_loss=6.525095

Batch 215480, train_perplexity=765.32, train_loss=6.640294

Batch 215490, train_perplexity=732.57324, train_loss=6.5965633

Batch 215500, train_perplexity=722.31366, train_loss=6.5824594

Batch 215510, train_perplexity=742.54297, train_loss=6.6100807

Batch 215520, train_perplexity=711.87244, train_loss=6.5678988

Batch 215530, train_perplexity=674.9154, train_loss=6.5145874

Batch 215540, train_perplexity=688.2056, train_loss=6.5340877

Batch 215550, train_perplexity=682.3019, train_loss=6.525472

Batch 215560, train_perplexity=649.9084, train_loss=6.4768314

Batch 215570, train_perplexity=652.0086, train_loss=6.4800577

Batch 215580, train_perplexity=702.3413, train_loss=6.5544195

Batch 215590, train_perplexity=707.46454, train_loss=6.5616875

Batch 215600, train_perplexity=697.7881, train_loss=6.5479155

Batch 215610, train_perplexity=710.7613, train_loss=6.5663366

Batch 215620, train_perplexity=702.7728, train_loss=6.5550337

Batch 215630, train_perplexity=678.4593, train_loss=6.5198245

Batch 215640, train_perplexity=686.353, train_loss=6.531392

Batch 215650, train_perplexity=694.41174, train_loss=6.543065

Batch 215660, train_perplexity=760.9918, train_loss=6.6346226

Batch 215670, train_perplexity=722.07947, train_loss=6.582135

Batch 215680, train_perplexity=636.92914, train_loss=6.4566584

Batch 215690, train_perplexity=673.0052, train_loss=6.511753

Batch 215700, train_perplexity=726.6495, train_loss=6.588444

Batch 215710, train_perplexity=634.636, train_loss=6.4530516

Batch 215720, train_perplexity=681.13196, train_loss=6.523756

Batch 215730, train_perplexity=716.06775, train_loss=6.573775

Batch 215740, train_perplexity=684.1344, train_loss=6.5281544

Batch 215750, train_perplexity=746.12915, train_loss=6.6148987

Batch 215760, train_perplexity=718.3358, train_loss=6.576937

Batch 215770, train_perplexity=699.05426, train_loss=6.5497284

Batch 215780, train_perplexity=691.0643, train_loss=6.538233

Batch 215790, train_perplexity=749.0655, train_loss=6.6188264

Batch 215800, train_perplexity=714.5721, train_loss=6.571684

Batch 215810, train_perplexity=682.2924, train_loss=6.5254583

Batch 215820, train_perplexity=714.47943, train_loss=6.571554

Batch 215830, train_perplexity=802.0898, train_loss=6.6872206

Batch 215840, train_perplexity=692.8533, train_loss=6.540818

Batch 215850, train_perplexity=770.1259, train_loss=6.646554

Batch 215860, train_perplexity=769.38666, train_loss=6.6455936

Batch 215870, train_perplexity=709.6823, train_loss=6.5648174

Batch 215880, train_perplexity=674.0117, train_loss=6.5132475

Batch 215890, train_perplexity=722.95593, train_loss=6.5833483

Batch 215900, train_perplexity=694.02905, train_loss=6.542514

Batch 215910, train_perplexity=682.2794, train_loss=6.5254393

Batch 215920, train_perplexity=628.29596, train_loss=6.4430113

Batch 215930, train_perplexity=753.22455, train_loss=6.6243634
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 215940, train_perplexity=754.3219, train_loss=6.625819

Batch 215950, train_perplexity=699.88745, train_loss=6.5509195

Batch 215960, train_perplexity=682.73505, train_loss=6.526107

Batch 215970, train_perplexity=730.94586, train_loss=6.5943394

Batch 215980, train_perplexity=662.8114, train_loss=6.4964905

Batch 215990, train_perplexity=729.8056, train_loss=6.592778

Batch 216000, train_perplexity=651.60767, train_loss=6.4794426

Batch 216010, train_perplexity=673.4418, train_loss=6.5124016

Batch 216020, train_perplexity=733.6842, train_loss=6.5980787

Batch 216030, train_perplexity=744.43506, train_loss=6.6126256

Batch 216040, train_perplexity=599.45154, train_loss=6.396015

Batch 216050, train_perplexity=747.8182, train_loss=6.61716

Batch 216060, train_perplexity=652.46826, train_loss=6.4807625

Batch 216070, train_perplexity=720.81006, train_loss=6.5803757

Batch 216080, train_perplexity=663.3954, train_loss=6.497371

Batch 216090, train_perplexity=653.7326, train_loss=6.4826984

Batch 216100, train_perplexity=631.2018, train_loss=6.4476256

Batch 216110, train_perplexity=735.99194, train_loss=6.601219

Batch 216120, train_perplexity=703.88495, train_loss=6.556615

Batch 216130, train_perplexity=659.44104, train_loss=6.4913926

Batch 216140, train_perplexity=614.93787, train_loss=6.421521

Batch 216150, train_perplexity=737.81, train_loss=6.6036863

Batch 216160, train_perplexity=680.1103, train_loss=6.522255

Batch 216170, train_perplexity=648.8575, train_loss=6.475213

Batch 216180, train_perplexity=711.3933, train_loss=6.5672255

Batch 216190, train_perplexity=713.49927, train_loss=6.5701814

Batch 216200, train_perplexity=732.9223, train_loss=6.5970397

Batch 216210, train_perplexity=626.8243, train_loss=6.440666

Batch 216220, train_perplexity=705.36597, train_loss=6.558717

Batch 216230, train_perplexity=617.0506, train_loss=6.424951

Batch 216240, train_perplexity=744.18695, train_loss=6.6122923

Batch 216250, train_perplexity=675.09247, train_loss=6.5148497

Batch 216260, train_perplexity=651.334, train_loss=6.4790225

Batch 216270, train_perplexity=694.0284, train_loss=6.542513

Batch 216280, train_perplexity=702.29913, train_loss=6.5543594

Batch 216290, train_perplexity=695.4509, train_loss=6.5445604

Batch 216300, train_perplexity=642.2369, train_loss=6.464957

Batch 216310, train_perplexity=683.89105, train_loss=6.5277987

Batch 216320, train_perplexity=677.3961, train_loss=6.518256

Batch 216330, train_perplexity=696.3299, train_loss=6.5458236

Batch 216340, train_perplexity=644.9621, train_loss=6.4691916

Batch 216350, train_perplexity=665.64233, train_loss=6.5007524

Batch 216360, train_perplexity=603.9463, train_loss=6.4034853

Batch 216370, train_perplexity=617.24664, train_loss=6.4252687

Batch 216380, train_perplexity=682.5156, train_loss=6.5257854

Batch 216390, train_perplexity=675.209, train_loss=6.5150223

Batch 216400, train_perplexity=706.8353, train_loss=6.5607977

Batch 216410, train_perplexity=617.82556, train_loss=6.426206

Batch 216420, train_perplexity=711.50085, train_loss=6.5673766

Batch 216430, train_perplexity=746.9892, train_loss=6.6160507

Batch 216440, train_perplexity=683.1141, train_loss=6.526662

Batch 216450, train_perplexity=658.1823, train_loss=6.489482

Batch 216460, train_perplexity=700.072, train_loss=6.551183

Batch 216470, train_perplexity=578.2972, train_loss=6.360088

Batch 216480, train_perplexity=735.17926, train_loss=6.6001143

Batch 216490, train_perplexity=722.4459, train_loss=6.5826426

Batch 216500, train_perplexity=676.04596, train_loss=6.516261

Batch 216510, train_perplexity=701.1601, train_loss=6.5527363

Batch 216520, train_perplexity=700.37555, train_loss=6.5516167

Batch 216530, train_perplexity=734.39966, train_loss=6.5990534

Batch 216540, train_perplexity=653.0204, train_loss=6.4816084

Batch 216550, train_perplexity=723.45184, train_loss=6.584034

Batch 216560, train_perplexity=639.91125, train_loss=6.4613295

Batch 216570, train_perplexity=669.71686, train_loss=6.506855

Batch 216580, train_perplexity=705.22974, train_loss=6.5585237

Batch 216590, train_perplexity=627.20184, train_loss=6.4412684

Batch 216600, train_perplexity=693.64795, train_loss=6.5419645

Batch 216610, train_perplexity=649.1657, train_loss=6.475688

Batch 216620, train_perplexity=749.0322, train_loss=6.618782

Batch 216630, train_perplexity=714.874, train_loss=6.5721064

Batch 216640, train_perplexity=692.24036, train_loss=6.539933

Batch 216650, train_perplexity=619.739, train_loss=6.4292984

Batch 216660, train_perplexity=702.2201, train_loss=6.554247

Batch 216670, train_perplexity=725.5723, train_loss=6.586961

Batch 216680, train_perplexity=681.3041, train_loss=6.5240088

Batch 216690, train_perplexity=656.68506, train_loss=6.4872046

Batch 216700, train_perplexity=710.42993, train_loss=6.5658703

Batch 216710, train_perplexity=669.42053, train_loss=6.5064125

Batch 216720, train_perplexity=727.53357, train_loss=6.58966

Batch 216730, train_perplexity=741.21497, train_loss=6.6082907

Batch 216740, train_perplexity=697.1535, train_loss=6.5470057

Batch 216750, train_perplexity=758.2675, train_loss=6.6310363

Batch 216760, train_perplexity=686.1521, train_loss=6.5310993

Batch 216770, train_perplexity=704.81995, train_loss=6.5579424

Batch 216780, train_perplexity=637.15027, train_loss=6.4570055

Batch 216790, train_perplexity=727.01447, train_loss=6.5889463

Batch 216800, train_perplexity=668.83795, train_loss=6.505542

Batch 216810, train_perplexity=693.68036, train_loss=6.5420113

Batch 216820, train_perplexity=707.3876, train_loss=6.5615788

Batch 216830, train_perplexity=681.52765, train_loss=6.524337

Batch 216840, train_perplexity=673.3541, train_loss=6.5122714

Batch 216850, train_perplexity=650.5753, train_loss=6.477857

Batch 216860, train_perplexity=664.5266, train_loss=6.499075

Batch 216870, train_perplexity=661.1132, train_loss=6.493925

Batch 216880, train_perplexity=622.467, train_loss=6.4336905

Batch 216890, train_perplexity=734.3573, train_loss=6.5989957

Batch 216900, train_perplexity=648.47577, train_loss=6.4746246

Batch 216910, train_perplexity=643.09375, train_loss=6.4662905

Batch 216920, train_perplexity=692.3704, train_loss=6.540121

Batch 216930, train_perplexity=695.8556, train_loss=6.545142

Batch 216940, train_perplexity=652.5908, train_loss=6.4809504

Batch 216950, train_perplexity=613.0535, train_loss=6.4184523

Batch 216960, train_perplexity=757.4595, train_loss=6.62997

Batch 216970, train_perplexity=724.6934, train_loss=6.5857487

Batch 216980, train_perplexity=691.93475, train_loss=6.5394917

Batch 216990, train_perplexity=639.45184, train_loss=6.4606113

Batch 217000, train_perplexity=646.92474, train_loss=6.47223

Batch 217010, train_perplexity=757.0258, train_loss=6.6293974

Batch 217020, train_perplexity=691.26465, train_loss=6.5385227

Batch 217030, train_perplexity=720.16077, train_loss=6.5794744

Batch 217040, train_perplexity=740.567, train_loss=6.607416

Batch 217050, train_perplexity=676.1379, train_loss=6.516397

Batch 217060, train_perplexity=735.5064, train_loss=6.600559

Batch 217070, train_perplexity=665.1239, train_loss=6.4999733

Batch 217080, train_perplexity=741.0637, train_loss=6.6080866

Batch 217090, train_perplexity=656.9507, train_loss=6.487609

Batch 217100, train_perplexity=687.5437, train_loss=6.5331254

Batch 217110, train_perplexity=743.059, train_loss=6.6107755

Batch 217120, train_perplexity=631.4062, train_loss=6.4479494

Batch 217130, train_perplexity=825.82666, train_loss=6.716385

Batch 217140, train_perplexity=713.6724, train_loss=6.570424

Batch 217150, train_perplexity=700.2961, train_loss=6.551503

Batch 217160, train_perplexity=624.38824, train_loss=6.4367723

Batch 217170, train_perplexity=701.5979, train_loss=6.5533605

Batch 217180, train_perplexity=673.32556, train_loss=6.512229

Batch 217190, train_perplexity=691.8341, train_loss=6.539346

Batch 217200, train_perplexity=652.5187, train_loss=6.4808397

Batch 217210, train_perplexity=678.9645, train_loss=6.520569

Batch 217220, train_perplexity=681.167, train_loss=6.5238075

Batch 217230, train_perplexity=760.4687, train_loss=6.633935

Batch 217240, train_perplexity=727.7636, train_loss=6.5899763
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 217250, train_perplexity=751.8843, train_loss=6.6225824

Batch 217260, train_perplexity=625.25555, train_loss=6.4381604

Batch 217270, train_perplexity=663.05194, train_loss=6.4968534

Batch 217280, train_perplexity=636.2765, train_loss=6.455633

Batch 217290, train_perplexity=626.6874, train_loss=6.440448

Batch 217300, train_perplexity=690.4954, train_loss=6.5374093

Batch 217310, train_perplexity=648.63965, train_loss=6.4748774

Batch 217320, train_perplexity=676.1804, train_loss=6.51646

Batch 217330, train_perplexity=670.7472, train_loss=6.5083923

Batch 217340, train_perplexity=753.8692, train_loss=6.625219

Batch 217350, train_perplexity=694.80786, train_loss=6.5436354

Batch 217360, train_perplexity=745.74713, train_loss=6.6143866

Batch 217370, train_perplexity=664.18225, train_loss=6.4985566

Batch 217380, train_perplexity=680.4486, train_loss=6.5227523

Batch 217390, train_perplexity=706.39526, train_loss=6.560175

Batch 217400, train_perplexity=770.3169, train_loss=6.646802

Batch 217410, train_perplexity=652.5236, train_loss=6.4808474

Batch 217420, train_perplexity=770.38666, train_loss=6.6468925

Batch 217430, train_perplexity=700.9803, train_loss=6.5524797

Batch 217440, train_perplexity=792.583, train_loss=6.6752973

Batch 217450, train_perplexity=695.05505, train_loss=6.543991

Batch 217460, train_perplexity=686.0088, train_loss=6.5308905

Batch 217470, train_perplexity=779.3503, train_loss=6.6584606

Batch 217480, train_perplexity=613.7725, train_loss=6.4196243

Batch 217490, train_perplexity=747.3451, train_loss=6.616527

Batch 217500, train_perplexity=742.92725, train_loss=6.610598

Batch 217510, train_perplexity=631.3502, train_loss=6.4478607

Batch 217520, train_perplexity=675.16907, train_loss=6.514963

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Batch 217530, train_perplexity=674.2792, train_loss=6.513644

Batch 217540, train_perplexity=700.26166, train_loss=6.551454

Batch 217550, train_perplexity=649.1737, train_loss=6.4757004

Batch 217560, train_perplexity=718.33136, train_loss=6.576931

Batch 217570, train_perplexity=795.2933, train_loss=6.678711

Batch 217580, train_perplexity=640.0904, train_loss=6.4616094

Batch 217590, train_perplexity=713.7344, train_loss=6.570511

Batch 217600, train_perplexity=646.31116, train_loss=6.471281

Batch 217610, train_perplexity=612.65375, train_loss=6.4178

Batch 217620, train_perplexity=711.6512, train_loss=6.567588

Batch 217630, train_perplexity=648.5073, train_loss=6.4746733

Batch 217640, train_perplexity=679.9248, train_loss=6.521982

Batch 217650, train_perplexity=651.24915, train_loss=6.4788923

Batch 217660, train_perplexity=684.9481, train_loss=6.529343

Batch 217670, train_perplexity=706.3114, train_loss=6.560056

Batch 217680, train_perplexity=724.74524, train_loss=6.58582

Batch 217690, train_perplexity=663.56433, train_loss=6.497626

Batch 217700, train_perplexity=679.2663, train_loss=6.5210133

Batch 217710, train_perplexity=631.5977, train_loss=6.4482527

Batch 217720, train_perplexity=654.3645, train_loss=6.4836645

Batch 217730, train_perplexity=763.27655, train_loss=6.6376204

Batch 217740, train_perplexity=691.78033, train_loss=6.5392685

Batch 217750, train_perplexity=687.02167, train_loss=6.532366

Batch 217760, train_perplexity=675.57166, train_loss=6.515559

Batch 217770, train_perplexity=675.10986, train_loss=6.5148754

Batch 217780, train_perplexity=716.5709, train_loss=6.574477

Batch 217790, train_perplexity=708.3482, train_loss=6.562936

Batch 217800, train_perplexity=800.3759, train_loss=6.6850815

Batch 217810, train_perplexity=728.7752, train_loss=6.5913653

Batch 217820, train_perplexity=651.374, train_loss=6.479084

Batch 217830, train_perplexity=749.59467, train_loss=6.6195326

Batch 217840, train_perplexity=624.94763, train_loss=6.437668

Batch 217850, train_perplexity=634.1668, train_loss=6.452312

Batch 217860, train_perplexity=718.4951, train_loss=6.577159

Batch 217870, train_perplexity=621.65393, train_loss=6.4323835

Batch 217880, train_perplexity=717.43726, train_loss=6.5756855

Batch 217890, train_perplexity=602.7788, train_loss=6.4015503

Batch 217900, train_perplexity=702.9746, train_loss=6.5553207

Batch 217910, train_perplexity=717.18896, train_loss=6.5753393

Batch 217920, train_perplexity=731.8617, train_loss=6.5955915

Batch 217930, train_perplexity=668.4553, train_loss=6.5049696

Batch 217940, train_perplexity=654.2469, train_loss=6.4834847

Batch 217950, train_perplexity=695.55005, train_loss=6.544703

Batch 217960, train_perplexity=737.183, train_loss=6.602836

Batch 217970, train_perplexity=690.7216, train_loss=6.537737

Batch 217980, train_perplexity=594.17456, train_loss=6.387173

Batch 217990, train_perplexity=707.2432, train_loss=6.5613747

Batch 218000, train_perplexity=597.4466, train_loss=6.392665

Batch 218010, train_perplexity=737.9258, train_loss=6.603843

Batch 218020, train_perplexity=668.34186, train_loss=6.5048

Batch 218030, train_perplexity=695.762, train_loss=6.5450077

Batch 218040, train_perplexity=772.381, train_loss=6.649478

Batch 218050, train_perplexity=633.8001, train_loss=6.4517336

Batch 218060, train_perplexity=724.7563, train_loss=6.5858355

Batch 218070, train_perplexity=747.2243, train_loss=6.6163654

Batch 218080, train_perplexity=778.3787, train_loss=6.657213

Batch 218090, train_perplexity=710.7738, train_loss=6.5663543

Batch 218100, train_perplexity=748.37463, train_loss=6.6179037

Batch 218110, train_perplexity=710.28394, train_loss=6.565665

Batch 218120, train_perplexity=728.83325, train_loss=6.591445

Batch 218130, train_perplexity=771.555, train_loss=6.648408

Batch 218140, train_perplexity=696.7481, train_loss=6.546424

Batch 218150, train_perplexity=728.442, train_loss=6.590908

Batch 218160, train_perplexity=748.8394, train_loss=6.6185246

Batch 218170, train_perplexity=696.6956, train_loss=6.5463486

Batch 218180, train_perplexity=708.6712, train_loss=6.5633917

Batch 218190, train_perplexity=672.943, train_loss=6.5116606

Batch 218200, train_perplexity=685.67914, train_loss=6.53041

Batch 218210, train_perplexity=683.21765, train_loss=6.5268135

Batch 218220, train_perplexity=681.1521, train_loss=6.5237856

Batch 218230, train_perplexity=628.9068, train_loss=6.443983

Batch 218240, train_perplexity=682.47235, train_loss=6.525722

Batch 218250, train_perplexity=632.6768, train_loss=6.4499598

Batch 218260, train_perplexity=605.09875, train_loss=6.4053917

Batch 218270, train_perplexity=672.27203, train_loss=6.510663

Batch 218280, train_perplexity=694.8493, train_loss=6.543695

Batch 218290, train_perplexity=749.72473, train_loss=6.619706

Batch 218300, train_perplexity=759.60614, train_loss=6.6328

Batch 218310, train_perplexity=759.03845, train_loss=6.6320524

Batch 218320, train_perplexity=650.4162, train_loss=6.4776125

Batch 218330, train_perplexity=710.543, train_loss=6.5660295

Batch 218340, train_perplexity=638.05383, train_loss=6.4584227

Batch 218350, train_perplexity=662.6098, train_loss=6.4961863

Batch 218360, train_perplexity=668.2527, train_loss=6.5046663

Batch 218370, train_perplexity=681.7844, train_loss=6.5247135

Batch 218380, train_perplexity=624.46985, train_loss=6.436903

Batch 218390, train_perplexity=722.4501, train_loss=6.5826483

Batch 218400, train_perplexity=651.74994, train_loss=6.479661

Batch 218410, train_perplexity=660.4396, train_loss=6.4929056

Batch 218420, train_perplexity=670.2162, train_loss=6.5076003

Batch 218430, train_perplexity=655.0085, train_loss=6.484648

Batch 218440, train_perplexity=757.5357, train_loss=6.6300707

Batch 218450, train_perplexity=728.1024, train_loss=6.5904417

Batch 218460, train_perplexity=702.11127, train_loss=6.554092

Batch 218470, train_perplexity=745.8887, train_loss=6.6145763

Batch 218480, train_perplexity=662.60095, train_loss=6.496173

Batch 218490, train_perplexity=740.2175, train_loss=6.606944
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 218500, train_perplexity=677.2317, train_loss=6.5180135

Batch 218510, train_perplexity=662.0817, train_loss=6.495389

Batch 218520, train_perplexity=769.3089, train_loss=6.6454926

Batch 218530, train_perplexity=686.8955, train_loss=6.532182

Batch 218540, train_perplexity=740.507, train_loss=6.607335

Batch 218550, train_perplexity=667.06165, train_loss=6.5028825

Batch 218560, train_perplexity=647.3722, train_loss=6.4729214

Batch 218570, train_perplexity=718.17316, train_loss=6.5767107

Batch 218580, train_perplexity=712.84595, train_loss=6.5692654

Batch 218590, train_perplexity=645.9716, train_loss=6.4707556

Batch 218600, train_perplexity=627.9515, train_loss=6.442463

Batch 218610, train_perplexity=676.5736, train_loss=6.517041

Batch 218620, train_perplexity=739.2334, train_loss=6.6056137

Batch 218630, train_perplexity=633.57166, train_loss=6.451373

Batch 218640, train_perplexity=672.2188, train_loss=6.510584

Batch 218650, train_perplexity=727.17737, train_loss=6.5891705

Batch 218660, train_perplexity=701.18085, train_loss=6.552766

Batch 218670, train_perplexity=622.1162, train_loss=6.433127

Batch 218680, train_perplexity=667.3305, train_loss=6.5032854

Batch 218690, train_perplexity=700.5038, train_loss=6.5518

Batch 218700, train_perplexity=688.183, train_loss=6.5340548

Batch 218710, train_perplexity=692.3929, train_loss=6.5401535

Batch 218720, train_perplexity=741.6491, train_loss=6.608876

Batch 218730, train_perplexity=661.05145, train_loss=6.4938316

Batch 218740, train_perplexity=713.60846, train_loss=6.5703344

Batch 218750, train_perplexity=616.56506, train_loss=6.424164

Batch 218760, train_perplexity=679.184, train_loss=6.520892

Batch 218770, train_perplexity=622.1361, train_loss=6.433159

Batch 218780, train_perplexity=655.83826, train_loss=6.485914

Batch 218790, train_perplexity=767.7642, train_loss=6.6434827

Batch 218800, train_perplexity=727.82715, train_loss=6.5900636

Batch 218810, train_perplexity=617.22455, train_loss=6.425233

Batch 218820, train_perplexity=636.1075, train_loss=6.4553676

Batch 218830, train_perplexity=682.85876, train_loss=6.526288

Batch 218840, train_perplexity=700.88336, train_loss=6.5523415

Batch 218850, train_perplexity=627.91797, train_loss=6.4424095

Batch 218860, train_perplexity=678.3451, train_loss=6.519656

Batch 218870, train_perplexity=695.04114, train_loss=6.543971

Batch 218880, train_perplexity=700.3104, train_loss=6.5515237

Batch 218890, train_perplexity=657.99963, train_loss=6.4892044

Batch 218900, train_perplexity=661.67206, train_loss=6.49477

Batch 218910, train_perplexity=615.8804, train_loss=6.423053

Batch 218920, train_perplexity=677.93896, train_loss=6.5190573

Batch 218930, train_perplexity=703.8923, train_loss=6.5566254

Batch 218940, train_perplexity=700.88336, train_loss=6.5523415

Batch 218950, train_perplexity=724.2039, train_loss=6.585073

Batch 218960, train_perplexity=758.0723, train_loss=6.630779

Batch 218970, train_perplexity=612.1661, train_loss=6.4170036

Batch 218980, train_perplexity=690.0639, train_loss=6.536784

Batch 218990, train_perplexity=608.25775, train_loss=6.4105988

Batch 219000, train_perplexity=704.78094, train_loss=6.557887

Batch 219010, train_perplexity=628.42175, train_loss=6.4432116

Batch 219020, train_perplexity=637.91754, train_loss=6.458209

Batch 219030, train_perplexity=783.03284, train_loss=6.6631746

Batch 219040, train_perplexity=753.5975, train_loss=6.6248584

Batch 219050, train_perplexity=705.00586, train_loss=6.558206

Batch 219060, train_perplexity=703.38434, train_loss=6.5559034

Batch 219070, train_perplexity=789.03424, train_loss=6.6708097

Batch 219080, train_perplexity=685.59937, train_loss=6.5302935

Batch 219090, train_perplexity=724.34863, train_loss=6.585273

Batch 219100, train_perplexity=709.8218, train_loss=6.565014

Batch 219110, train_perplexity=708.76044, train_loss=6.5635176

Batch 219120, train_perplexity=682.7822, train_loss=6.526176

Batch 219130, train_perplexity=688.93616, train_loss=6.5351486

Batch 219140, train_perplexity=608.585, train_loss=6.4111366

Batch 219150, train_perplexity=571.6113, train_loss=6.3484592

Batch 219160, train_perplexity=675.6799, train_loss=6.5157194

Batch 219170, train_perplexity=719.5199, train_loss=6.578584

Batch 219180, train_perplexity=635.7767, train_loss=6.4548473

Batch 219190, train_perplexity=704.4403, train_loss=6.5574036

Batch 219200, train_perplexity=703.546, train_loss=6.5561333

Batch 219210, train_perplexity=667.914, train_loss=6.5041595

Batch 219220, train_perplexity=626.3782, train_loss=6.4399543

Batch 219230, train_perplexity=671.82465, train_loss=6.5099974

Batch 219240, train_perplexity=672.15564, train_loss=6.51049

Batch 219250, train_perplexity=736.5438, train_loss=6.601969

Batch 219260, train_perplexity=663.92896, train_loss=6.498175

Batch 219270, train_perplexity=666.60474, train_loss=6.5021973

Batch 219280, train_perplexity=671.4467, train_loss=6.5094347

Batch 219290, train_perplexity=625.742, train_loss=6.438938

Batch 219300, train_perplexity=642.9665, train_loss=6.4660926

Batch 219310, train_perplexity=645.43036, train_loss=6.4699173

Batch 219320, train_perplexity=679.88525, train_loss=6.521924

Batch 219330, train_perplexity=792.24786, train_loss=6.6748743

Batch 219340, train_perplexity=628.5698, train_loss=6.443447

Batch 219350, train_perplexity=758.9892, train_loss=6.6319876

Batch 219360, train_perplexity=808.2057, train_loss=6.6948166

Batch 219370, train_perplexity=688.833, train_loss=6.534999

Batch 219380, train_perplexity=741.84436, train_loss=6.6091394

Batch 219390, train_perplexity=720.5001, train_loss=6.5799456

Batch 219400, train_perplexity=780.84564, train_loss=6.6603775

Batch 219410, train_perplexity=617.1136, train_loss=6.425053

Batch 219420, train_perplexity=773.0029, train_loss=6.650283

Batch 219430, train_perplexity=662.1505, train_loss=6.495493

Batch 219440, train_perplexity=765.4791, train_loss=6.640502

Batch 219450, train_perplexity=764.53656, train_loss=6.63927

Batch 219460, train_perplexity=674.4972, train_loss=6.5139675

Batch 219470, train_perplexity=687.5958, train_loss=6.533201

Batch 219480, train_perplexity=750.5095, train_loss=6.6207523

Batch 219490, train_perplexity=731.35864, train_loss=6.594904

Batch 219500, train_perplexity=708.6124, train_loss=6.5633087

Batch 219510, train_perplexity=707.7459, train_loss=6.562085

Batch 219520, train_perplexity=684.1455, train_loss=6.5281706

Batch 219530, train_perplexity=669.7731, train_loss=6.506939

Batch 219540, train_perplexity=645.71234, train_loss=6.470354

Batch 219550, train_perplexity=692.7528, train_loss=6.5406733

Batch 219560, train_perplexity=730.83954, train_loss=6.594194

Batch 219570, train_perplexity=735.6618, train_loss=6.6007705

Batch 219580, train_perplexity=764.1105, train_loss=6.6387124

Batch 219590, train_perplexity=731.47375, train_loss=6.5950613

Batch 219600, train_perplexity=606.92126, train_loss=6.408399

Batch 219610, train_perplexity=736.8396, train_loss=6.6023703

Batch 219620, train_perplexity=710.9097, train_loss=6.5665455

Batch 219630, train_perplexity=695.3017, train_loss=6.544346

Batch 219640, train_perplexity=760.748, train_loss=6.634302

Batch 219650, train_perplexity=752.55536, train_loss=6.6234746

Batch 219660, train_perplexity=729.3697, train_loss=6.5921807

Batch 219670, train_perplexity=636.58057, train_loss=6.456111

Batch 219680, train_perplexity=668.35657, train_loss=6.504822

Batch 219690, train_perplexity=599.5599, train_loss=6.396196

Batch 219700, train_perplexity=716.96124, train_loss=6.5750217

Batch 219710, train_perplexity=677.34247, train_loss=6.518177

Batch 219720, train_perplexity=648.1833, train_loss=6.4741735

Batch 219730, train_perplexity=710.86536, train_loss=6.566483

Batch 219740, train_perplexity=692.95966, train_loss=6.5409718

Batch 219750, train_perplexity=719.0332, train_loss=6.5779076

Batch 219760, train_perplexity=643.09314, train_loss=6.4662895

Batch 219770, train_perplexity=687.4719, train_loss=6.533021

Batch 219780, train_perplexity=766.7365, train_loss=6.6421432

Batch 219790, train_perplexity=687.56433, train_loss=6.5331554

Batch 219800, train_perplexity=772.6315, train_loss=6.649802
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 219810, train_perplexity=702.22314, train_loss=6.554251

Batch 219820, train_perplexity=708.705, train_loss=6.5634394

Batch 219830, train_perplexity=668.96106, train_loss=6.505726

Batch 219840, train_perplexity=687.76965, train_loss=6.533454

Batch 219850, train_perplexity=628.7356, train_loss=6.443711

Batch 219860, train_perplexity=657.7628, train_loss=6.4888444

Batch 219870, train_perplexity=686.4165, train_loss=6.5314846

Batch 219880, train_perplexity=667.0543, train_loss=6.5028715

Batch 219890, train_perplexity=748.3989, train_loss=6.617936

Batch 219900, train_perplexity=671.8195, train_loss=6.5099897

Batch 219910, train_perplexity=674.918, train_loss=6.514591

Batch 219920, train_perplexity=682.9773, train_loss=6.5264616

Batch 219930, train_perplexity=708.1274, train_loss=6.562624

Batch 219940, train_perplexity=684.1546, train_loss=6.528184

Batch 219950, train_perplexity=720.1868, train_loss=6.5795107

Batch 219960, train_perplexity=691.44464, train_loss=6.538783

Batch 219970, train_perplexity=634.96625, train_loss=6.453572

Batch 219980, train_perplexity=786.7335, train_loss=6.6678896

Batch 219990, train_perplexity=688.05927, train_loss=6.533875

Batch 220000, train_perplexity=697.33307, train_loss=6.547263

Batch 220010, train_perplexity=712.54486, train_loss=6.568843

Batch 220020, train_perplexity=653.9278, train_loss=6.482997

Batch 220030, train_perplexity=684.5263, train_loss=6.528727

Batch 220040, train_perplexity=727.5825, train_loss=6.5897274

Batch 220050, train_perplexity=751.9545, train_loss=6.622676

Batch 220060, train_perplexity=690.8567, train_loss=6.5379324

Batch 220070, train_perplexity=718.0054, train_loss=6.576477

Batch 220080, train_perplexity=689.535, train_loss=6.5360174

Batch 220090, train_perplexity=694.1863, train_loss=6.5427403

Batch 220100, train_perplexity=692.9365, train_loss=6.5409384

Batch 220110, train_perplexity=603.75684, train_loss=6.4031715

Batch 220120, train_perplexity=722.614, train_loss=6.5828753

Batch 220130, train_perplexity=649.6723, train_loss=6.476468

Batch 220140, train_perplexity=629.81854, train_loss=6.4454317

Batch 220150, train_perplexity=666.2463, train_loss=6.5016594

Batch 220160, train_perplexity=726.4787, train_loss=6.588209

Batch 220170, train_perplexity=722.9697, train_loss=6.5833673

Batch 220180, train_perplexity=633.4898, train_loss=6.451244

Batch 220190, train_perplexity=705.53955, train_loss=6.558963

Batch 220200, train_perplexity=656.3839, train_loss=6.486746

Batch 220210, train_perplexity=690.5405, train_loss=6.5374746

Batch 220220, train_perplexity=764.9486, train_loss=6.6398087

Batch 220230, train_perplexity=672.0906, train_loss=6.510393

Batch 220240, train_perplexity=644.2198, train_loss=6.46804

Batch 220250, train_perplexity=706.98566, train_loss=6.5610104

Batch 220260, train_perplexity=697.4089, train_loss=6.547372

Batch 220270, train_perplexity=695.987, train_loss=6.545331

Batch 220280, train_perplexity=606.1086, train_loss=6.407059

Batch 220290, train_perplexity=825.39594, train_loss=6.715863

Batch 220300, train_perplexity=646.94324, train_loss=6.4722586

Batch 220310, train_perplexity=672.7062, train_loss=6.5113087

Batch 220320, train_perplexity=710.32556, train_loss=6.5657234

Batch 220330, train_perplexity=747.9223, train_loss=6.617299

Batch 220340, train_perplexity=721.9889, train_loss=6.58201

Batch 220350, train_perplexity=570.5395, train_loss=6.3465824

Batch 220360, train_perplexity=810.76324, train_loss=6.697976

Batch 220370, train_perplexity=665.1251, train_loss=6.499975

Batch 220380, train_perplexity=721.04486, train_loss=6.5807014

Batch 220390, train_perplexity=654.19385, train_loss=6.4834037

Batch 220400, train_perplexity=695.4854, train_loss=6.54461

Batch 220410, train_perplexity=686.6817, train_loss=6.531871

Batch 220420, train_perplexity=619.0945, train_loss=6.428258

Batch 220430, train_perplexity=719.6376, train_loss=6.5787477

Batch 220440, train_perplexity=632.51636, train_loss=6.449706

Batch 220450, train_perplexity=742.65625, train_loss=6.6102333

Batch 220460, train_perplexity=697.0219, train_loss=6.546817

Batch 220470, train_perplexity=702.18964, train_loss=6.5542035

Batch 220480, train_perplexity=634.9269, train_loss=6.45351

Batch 220490, train_perplexity=658.69403, train_loss=6.490259

Batch 220500, train_perplexity=709.4306, train_loss=6.5644627

Batch 220510, train_perplexity=629.0538, train_loss=6.4442167

Batch 220520, train_perplexity=745.7578, train_loss=6.614401

Batch 220530, train_perplexity=691.34705, train_loss=6.538642

Batch 220540, train_perplexity=633.64355, train_loss=6.4514866

Batch 220550, train_perplexity=714.4617, train_loss=6.5715294

Batch 220560, train_perplexity=715.5817, train_loss=6.573096

Batch 220570, train_perplexity=690.06287, train_loss=6.5367827

Batch 220580, train_perplexity=621.60144, train_loss=6.432299

Batch 220590, train_perplexity=741.92285, train_loss=6.6092453

Batch 220600, train_perplexity=724.7259, train_loss=6.5857935

Batch 220610, train_perplexity=674.8981, train_loss=6.5145617

Batch 220620, train_perplexity=663.4761, train_loss=6.497493

Batch 220630, train_perplexity=685.53204, train_loss=6.530195

Batch 220640, train_perplexity=694.9812, train_loss=6.5438848

Batch 220650, train_perplexity=667.8672, train_loss=6.5040894

Batch 220660, train_perplexity=634.3894, train_loss=6.452663

Batch 220670, train_perplexity=663.0229, train_loss=6.4968095

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Batch 220680, train_perplexity=711.4313, train_loss=6.567279

Batch 220690, train_perplexity=709.4922, train_loss=6.5645494

Batch 220700, train_perplexity=606.3899, train_loss=6.407523

Batch 220710, train_perplexity=652.88715, train_loss=6.4814043

Batch 220720, train_perplexity=674.0146, train_loss=6.513252

Batch 220730, train_perplexity=659.8415, train_loss=6.4919996

Batch 220740, train_perplexity=670.847, train_loss=6.508541

Batch 220750, train_perplexity=700.1515, train_loss=6.5512967

Batch 220760, train_perplexity=782.1007, train_loss=6.6619835

Batch 220770, train_perplexity=697.7881, train_loss=6.5479155

Batch 220780, train_perplexity=635.3463, train_loss=6.45417

Batch 220790, train_perplexity=786.72455, train_loss=6.667878

Batch 220800, train_perplexity=663.42163, train_loss=6.497411

Batch 220810, train_perplexity=658.5678, train_loss=6.4900675

Batch 220820, train_perplexity=626.6243, train_loss=6.440347

Batch 220830, train_perplexity=721.57697, train_loss=6.581439

Batch 220840, train_perplexity=735.48956, train_loss=6.6005363

Batch 220850, train_perplexity=696.47437, train_loss=6.546031

Batch 220860, train_perplexity=719.7876, train_loss=6.578956

Batch 220870, train_perplexity=635.236, train_loss=6.4539967

Batch 220880, train_perplexity=667.2293, train_loss=6.503134

Batch 220890, train_perplexity=651.54205, train_loss=6.479342

Batch 220900, train_perplexity=712.61285, train_loss=6.5689383

Batch 220910, train_perplexity=674.8324, train_loss=6.5144644

Batch 220920, train_perplexity=713.63434, train_loss=6.5703707

Batch 220930, train_perplexity=701.8974, train_loss=6.553787

Batch 220940, train_perplexity=782.39685, train_loss=6.662362

Batch 220950, train_perplexity=688.4679, train_loss=6.5344687

Batch 220960, train_perplexity=670.2401, train_loss=6.507636

Batch 220970, train_perplexity=673.41614, train_loss=6.5123634

Batch 220980, train_perplexity=701.62366, train_loss=6.553397

Batch 220990, train_perplexity=695.46484, train_loss=6.5445805

Batch 221000, train_perplexity=702.62573, train_loss=6.5548244

Batch 221010, train_perplexity=659.9862, train_loss=6.492219

Batch 221020, train_perplexity=689.4603, train_loss=6.535909

Batch 221030, train_perplexity=763.19904, train_loss=6.637519

Batch 221040, train_perplexity=721.4435, train_loss=6.581254

Batch 221050, train_perplexity=660.22296, train_loss=6.4925776
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 221060, train_perplexity=693.20355, train_loss=6.5413237

Batch 221070, train_perplexity=667.7889, train_loss=6.503972

Batch 221080, train_perplexity=639.60614, train_loss=6.4608526

Batch 221090, train_perplexity=689.5455, train_loss=6.5360327

Batch 221100, train_perplexity=638.3971, train_loss=6.4589605

Batch 221110, train_perplexity=649.984, train_loss=6.476948

Batch 221120, train_perplexity=675.83905, train_loss=6.515955

Batch 221130, train_perplexity=702.3889, train_loss=6.554487

Batch 221140, train_perplexity=736.3163, train_loss=6.60166

Batch 221150, train_perplexity=733.0268, train_loss=6.5971823

Batch 221160, train_perplexity=680.0279, train_loss=6.522134

Batch 221170, train_perplexity=662.1054, train_loss=6.4954247

Batch 221180, train_perplexity=611.9828, train_loss=6.416704

Batch 221190, train_perplexity=688.181, train_loss=6.534052

Batch 221200, train_perplexity=750.4694, train_loss=6.620699

Batch 221210, train_perplexity=679.9919, train_loss=6.522081

Batch 221220, train_perplexity=658.225, train_loss=6.489547

Batch 221230, train_perplexity=673.1227, train_loss=6.5119276

Batch 221240, train_perplexity=686.76685, train_loss=6.531995

Batch 221250, train_perplexity=613.8287, train_loss=6.419716

Batch 221260, train_perplexity=732.29456, train_loss=6.596183

Batch 221270, train_perplexity=642.6278, train_loss=6.4655657

Batch 221280, train_perplexity=655.16907, train_loss=6.4848933

Batch 221290, train_perplexity=747.8503, train_loss=6.6172028

Batch 221300, train_perplexity=768.0458, train_loss=6.6438494

Batch 221310, train_perplexity=688.74567, train_loss=6.534872

Batch 221320, train_perplexity=651.9831, train_loss=6.4800186

Batch 221330, train_perplexity=763.7353, train_loss=6.6382213

Batch 221340, train_perplexity=616.9494, train_loss=6.424787

Batch 221350, train_perplexity=684.96674, train_loss=6.5293703

Batch 221360, train_perplexity=657.9526, train_loss=6.489133

Batch 221370, train_perplexity=634.01654, train_loss=6.452075

Batch 221380, train_perplexity=630.1676, train_loss=6.445986

Batch 221390, train_perplexity=680.95105, train_loss=6.5234904

Batch 221400, train_perplexity=726.9097, train_loss=6.5888023

Batch 221410, train_perplexity=700.1358, train_loss=6.5512743

Batch 221420, train_perplexity=678.91174, train_loss=6.520491

Batch 221430, train_perplexity=688.3103, train_loss=6.53424

Batch 221440, train_perplexity=691.18024, train_loss=6.5384007

Batch 221450, train_perplexity=710.6874, train_loss=6.5662327

Batch 221460, train_perplexity=694.28723, train_loss=6.542886

Batch 221470, train_perplexity=654.1711, train_loss=6.483369

Batch 221480, train_perplexity=610.8592, train_loss=6.4148664

Batch 221490, train_perplexity=681.57184, train_loss=6.5244017

Batch 221500, train_perplexity=665.3491, train_loss=6.500312

Batch 221510, train_perplexity=715.5435, train_loss=6.5730424

Batch 221520, train_perplexity=657.6477, train_loss=6.4886694

Batch 221530, train_perplexity=727.0436, train_loss=6.5889864

Batch 221540, train_perplexity=679.8463, train_loss=6.521867

Batch 221550, train_perplexity=664.42334, train_loss=6.4989195

Batch 221560, train_perplexity=643.45996, train_loss=6.46686

Batch 221570, train_perplexity=700.051, train_loss=6.551153

Batch 221580, train_perplexity=715.3596, train_loss=6.5727854

Batch 221590, train_perplexity=634.3292, train_loss=6.452568

Batch 221600, train_perplexity=664.5165, train_loss=6.4990597

Batch 221610, train_perplexity=692.6293, train_loss=6.540495

Batch 221620, train_perplexity=728.8864, train_loss=6.591518

Batch 221630, train_perplexity=600.57886, train_loss=6.397894

Batch 221640, train_perplexity=630.173, train_loss=6.4459944

Batch 221650, train_perplexity=656.68097, train_loss=6.4871984

Batch 221660, train_perplexity=757.1807, train_loss=6.629602

Batch 221670, train_perplexity=630.7809, train_loss=6.4469585

Batch 221680, train_perplexity=690.6113, train_loss=6.537577

Batch 221690, train_perplexity=694.19324, train_loss=6.5427504

Batch 221700, train_perplexity=695.99036, train_loss=6.545336

Batch 221710, train_perplexity=681.473, train_loss=6.5242567

Batch 221720, train_perplexity=595.2792, train_loss=6.3890305

Batch 221730, train_perplexity=702.61804, train_loss=6.5548134

Batch 221740, train_perplexity=681.23914, train_loss=6.5239134

Batch 221750, train_perplexity=642.0752, train_loss=6.4647055

Batch 221760, train_perplexity=699.0336, train_loss=6.549699

Batch 221770, train_perplexity=685.81586, train_loss=6.530609

Batch 221780, train_perplexity=704.2824, train_loss=6.5571795

Batch 221790, train_perplexity=679.9384, train_loss=6.522002

Batch 221800, train_perplexity=768.118, train_loss=6.6439433

Batch 221810, train_perplexity=705.1084, train_loss=6.5583515

Batch 221820, train_perplexity=626.754, train_loss=6.440554

Batch 221830, train_perplexity=637.2724, train_loss=6.457197

Batch 221840, train_perplexity=726.3069, train_loss=6.5879726

Batch 221850, train_perplexity=712.7311, train_loss=6.569104

Batch 221860, train_perplexity=632.7459, train_loss=6.450069

Batch 221870, train_perplexity=682.7507, train_loss=6.5261297

Batch 221880, train_perplexity=719.5614, train_loss=6.578642

Batch 221890, train_perplexity=733.49255, train_loss=6.5978174

Batch 221900, train_perplexity=723.31177, train_loss=6.5838404

Batch 221910, train_perplexity=685.9895, train_loss=6.5308623

Batch 221920, train_perplexity=622.937, train_loss=6.4344454

Batch 221930, train_perplexity=692.7224, train_loss=6.5406294

Batch 221940, train_perplexity=668.4168, train_loss=6.504912

Batch 221950, train_perplexity=708.05615, train_loss=6.5625234

Batch 221960, train_perplexity=768.9553, train_loss=6.645033

Batch 221970, train_perplexity=604.51215, train_loss=6.404422

Batch 221980, train_perplexity=673.0688, train_loss=6.5118475

Batch 221990, train_perplexity=697.5236, train_loss=6.5475364

Batch 222000, train_perplexity=691.3576, train_loss=6.538657

Batch 222010, train_perplexity=636.50134, train_loss=6.4559865

Batch 222020, train_perplexity=680.5813, train_loss=6.5229473

Batch 222030, train_perplexity=675.07764, train_loss=6.5148277

Batch 222040, train_perplexity=755.19934, train_loss=6.6269817

Batch 222050, train_perplexity=744.319, train_loss=6.6124697

Batch 222060, train_perplexity=731.64185, train_loss=6.595291

Batch 222070, train_perplexity=657.4759, train_loss=6.488408

Batch 222080, train_perplexity=675.90967, train_loss=6.5160594

Batch 222090, train_perplexity=649.5995, train_loss=6.476356

Batch 222100, train_perplexity=622.1447, train_loss=6.4331727

Batch 222110, train_perplexity=626.8311, train_loss=6.440677

Batch 222120, train_perplexity=651.93146, train_loss=6.4799395

Batch 222130, train_perplexity=698.6604, train_loss=6.549165

Batch 222140, train_perplexity=724.9485, train_loss=6.5861006

Batch 222150, train_perplexity=693.88745, train_loss=6.5423098

Batch 222160, train_perplexity=615.4691, train_loss=6.4223847

Batch 222170, train_perplexity=666.03217, train_loss=6.501338

Batch 222180, train_perplexity=657.8751, train_loss=6.489015

Batch 222190, train_perplexity=717.81366, train_loss=6.57621

Batch 222200, train_perplexity=672.32074, train_loss=6.5107355

Batch 222210, train_perplexity=724.072, train_loss=6.584891

Batch 222220, train_perplexity=667.73474, train_loss=6.503891

Batch 222230, train_perplexity=663.17303, train_loss=6.497036

Batch 222240, train_perplexity=708.0612, train_loss=6.5625305

Batch 222250, train_perplexity=775.22473, train_loss=6.653153

Batch 222260, train_perplexity=714.4617, train_loss=6.5715294

Batch 222270, train_perplexity=702.42236, train_loss=6.554535

Batch 222280, train_perplexity=653.5319, train_loss=6.4823914

Batch 222290, train_perplexity=716.87885, train_loss=6.574907

Batch 222300, train_perplexity=590.0223, train_loss=6.3801603

Batch 222310, train_perplexity=624.24536, train_loss=6.4365435

Batch 222320, train_perplexity=628.6798, train_loss=6.443622

Batch 222330, train_perplexity=716.29865, train_loss=6.574097

Batch 222340, train_perplexity=633.4209, train_loss=6.451135

Batch 222350, train_perplexity=682.4668, train_loss=6.525714

Batch 222360, train_perplexity=685.3193, train_loss=6.529885
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 222370, train_perplexity=684.95105, train_loss=6.5293474

Batch 222380, train_perplexity=706.1097, train_loss=6.5597706

Batch 222390, train_perplexity=699.087, train_loss=6.549775

Batch 222400, train_perplexity=634.1169, train_loss=6.4522333

Batch 222410, train_perplexity=649.153, train_loss=6.4756684

Batch 222420, train_perplexity=693.6271, train_loss=6.5419345

Batch 222430, train_perplexity=670.8048, train_loss=6.508478

Batch 222440, train_perplexity=656.5949, train_loss=6.487067

Batch 222450, train_perplexity=648.79895, train_loss=6.475123

Batch 222460, train_perplexity=747.8538, train_loss=6.6172075

Batch 222470, train_perplexity=715.12976, train_loss=6.572464

Batch 222480, train_perplexity=664.6201, train_loss=6.4992156

Batch 222490, train_perplexity=645.64087, train_loss=6.4702435

Batch 222500, train_perplexity=687.9195, train_loss=6.533672

Batch 222510, train_perplexity=630.6386, train_loss=6.446733

Batch 222520, train_perplexity=689.20856, train_loss=6.535544

Batch 222530, train_perplexity=708.4087, train_loss=6.563021

Batch 222540, train_perplexity=736.70544, train_loss=6.602188

Batch 222550, train_perplexity=793.2549, train_loss=6.6761446

Batch 222560, train_perplexity=704.2287, train_loss=6.557103

Batch 222570, train_perplexity=745.90216, train_loss=6.6145945

Batch 222580, train_perplexity=718.2958, train_loss=6.5768814

Batch 222590, train_perplexity=776.9374, train_loss=6.6553597

Batch 222600, train_perplexity=634.6136, train_loss=6.4530163

Batch 222610, train_perplexity=588.29877, train_loss=6.377235

Batch 222620, train_perplexity=699.2103, train_loss=6.5499516

Batch 222630, train_perplexity=738.2154, train_loss=6.6042356

Batch 222640, train_perplexity=690.67883, train_loss=6.537675

Batch 222650, train_perplexity=747.9929, train_loss=6.6173935

Batch 222660, train_perplexity=651.47095, train_loss=6.479233

Batch 222670, train_perplexity=678.3891, train_loss=6.519721

Batch 222680, train_perplexity=688.6669, train_loss=6.5347576

Batch 222690, train_perplexity=674.7272, train_loss=6.5143085

Batch 222700, train_perplexity=629.52124, train_loss=6.4449596

Batch 222710, train_perplexity=711.527, train_loss=6.5674133

Batch 222720, train_perplexity=704.74805, train_loss=6.5578403

Batch 222730, train_perplexity=688.4587, train_loss=6.5344553

Batch 222740, train_perplexity=681.85077, train_loss=6.524811

Batch 222750, train_perplexity=688.7614, train_loss=6.534895

Batch 222760, train_perplexity=614.3722, train_loss=6.420601

Batch 222770, train_perplexity=626.2775, train_loss=6.4397936

Batch 222780, train_perplexity=701.86395, train_loss=6.5537395

Batch 222790, train_perplexity=661.4014, train_loss=6.494361

Batch 222800, train_perplexity=686.14294, train_loss=6.531086

Batch 222810, train_perplexity=672.75653, train_loss=6.5113835

Batch 222820, train_perplexity=682.31195, train_loss=6.525487

Batch 222830, train_perplexity=716.4452, train_loss=6.5743017

Batch 222840, train_perplexity=626.8798, train_loss=6.440755

Batch 222850, train_perplexity=635.10645, train_loss=6.4537926

Batch 222860, train_perplexity=704.9255, train_loss=6.558092

Batch 222870, train_perplexity=627.6168, train_loss=6.44193

Batch 222880, train_perplexity=645.79297, train_loss=6.470479

Batch 222890, train_perplexity=702.78357, train_loss=6.555049

Batch 222900, train_perplexity=703.9178, train_loss=6.5566616

Batch 222910, train_perplexity=670.9052, train_loss=6.508628

Batch 222920, train_perplexity=668.4917, train_loss=6.505024

Batch 222930, train_perplexity=628.54645, train_loss=6.44341

Batch 222940, train_perplexity=702.7115, train_loss=6.5549464

Batch 222950, train_perplexity=759.00336, train_loss=6.632006

Batch 222960, train_perplexity=609.39636, train_loss=6.412469

Batch 222970, train_perplexity=639.9363, train_loss=6.4613686

Batch 222980, train_perplexity=644.25635, train_loss=6.4680967

Batch 222990, train_perplexity=672.19635, train_loss=6.5105505

Batch 223000, train_perplexity=717.7647, train_loss=6.576142

Batch 223010, train_perplexity=690.9522, train_loss=6.5380707

Batch 223020, train_perplexity=708.097, train_loss=6.562581

Batch 223030, train_perplexity=695.5139, train_loss=6.544651

Batch 223040, train_perplexity=698.40326, train_loss=6.5487967

Batch 223050, train_perplexity=684.87006, train_loss=6.529229

Batch 223060, train_perplexity=673.9195, train_loss=6.5131106

Batch 223070, train_perplexity=690.23566, train_loss=6.537033

Batch 223080, train_perplexity=658.83417, train_loss=6.490472

Batch 223090, train_perplexity=671.5037, train_loss=6.5095196

Batch 223100, train_perplexity=699.9048, train_loss=6.5509443

Batch 223110, train_perplexity=655.6469, train_loss=6.4856224

Batch 223120, train_perplexity=667.2109, train_loss=6.503106

Batch 223130, train_perplexity=740.5543, train_loss=6.607399

Batch 223140, train_perplexity=695.3103, train_loss=6.5443583

Batch 223150, train_perplexity=666.1529, train_loss=6.501519

Batch 223160, train_perplexity=651.5728, train_loss=6.479389

Batch 223170, train_perplexity=757.45984, train_loss=6.6299706

Batch 223180, train_perplexity=702.675, train_loss=6.5548944

Batch 223190, train_perplexity=681.853, train_loss=6.524814

Batch 223200, train_perplexity=693.0766, train_loss=6.5411406

Batch 223210, train_perplexity=630.08466, train_loss=6.445854

Batch 223220, train_perplexity=622.19305, train_loss=6.4332504

Batch 223230, train_perplexity=688.66223, train_loss=6.534751

Batch 223240, train_perplexity=682.8978, train_loss=6.5263453

Batch 223250, train_perplexity=682.64386, train_loss=6.5259733

Batch 223260, train_perplexity=712.13525, train_loss=6.568268

Batch 223270, train_perplexity=656.12665, train_loss=6.486354

Batch 223280, train_perplexity=724.9118, train_loss=6.58605

Batch 223290, train_perplexity=744.0124, train_loss=6.6120577

Batch 223300, train_perplexity=641.7924, train_loss=6.464265

Batch 223310, train_perplexity=627.0072, train_loss=6.440958

Batch 223320, train_perplexity=718.83405, train_loss=6.5776305

Batch 223330, train_perplexity=711.3109, train_loss=6.5671096

Batch 223340, train_perplexity=722.81976, train_loss=6.58316

Batch 223350, train_perplexity=679.9423, train_loss=6.522008

Batch 223360, train_perplexity=740.3446, train_loss=6.6071157

Batch 223370, train_perplexity=633.58014, train_loss=6.4513865

Batch 223380, train_perplexity=711.2522, train_loss=6.567027

Batch 223390, train_perplexity=722.3946, train_loss=6.5825715

Batch 223400, train_perplexity=678.7292, train_loss=6.520222

Batch 223410, train_perplexity=655.15564, train_loss=6.484873

Batch 223420, train_perplexity=683.72284, train_loss=6.5275526

Batch 223430, train_perplexity=683.7626, train_loss=6.527611

Batch 223440, train_perplexity=660.81757, train_loss=6.493478

Batch 223450, train_perplexity=654.45624, train_loss=6.4838047

Batch 223460, train_perplexity=698.76, train_loss=6.5493073

Batch 223470, train_perplexity=646.04553, train_loss=6.47087

Batch 223480, train_perplexity=668.00354, train_loss=6.5042934

Batch 223490, train_perplexity=654.67535, train_loss=6.4841394

Batch 223500, train_perplexity=632.36316, train_loss=6.449464

Batch 223510, train_perplexity=733.3729, train_loss=6.5976543

Batch 223520, train_perplexity=642.30304, train_loss=6.46506

Batch 223530, train_perplexity=726.5341, train_loss=6.5882854

Batch 223540, train_perplexity=710.9294, train_loss=6.566573

Batch 223550, train_perplexity=692.482, train_loss=6.5402822

Batch 223560, train_perplexity=759.28674, train_loss=6.6323795

Batch 223570, train_perplexity=645.6344, train_loss=6.4702334

Batch 223580, train_perplexity=673.92303, train_loss=6.513116

Batch 223590, train_perplexity=693.7412, train_loss=6.542099

Batch 223600, train_perplexity=717.0077, train_loss=6.5750866

Batch 223610, train_perplexity=696.4624, train_loss=6.546014

Batch 223620, train_perplexity=737.30426, train_loss=6.6030006

Batch 223630, train_perplexity=711.3336, train_loss=6.5671415

Batch 223640, train_perplexity=677.76184, train_loss=6.518796

Batch 223650, train_perplexity=618.8047, train_loss=6.4277897

Batch 223660, train_perplexity=721.89496, train_loss=6.5818796

Batch 223670, train_perplexity=636.0526, train_loss=6.4552813
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 223680, train_perplexity=690.341, train_loss=6.5371857

Batch 223690, train_perplexity=706.18243, train_loss=6.5598736

Batch 223700, train_perplexity=669.2067, train_loss=6.506093

Batch 223710, train_perplexity=675.9596, train_loss=6.5161333

Batch 223720, train_perplexity=761.17395, train_loss=6.634862

Batch 223730, train_perplexity=700.73566, train_loss=6.5521307

Batch 223740, train_perplexity=690.289, train_loss=6.5371103

Batch 223750, train_perplexity=672.90894, train_loss=6.51161

Batch 223760, train_perplexity=699.37836, train_loss=6.550192

Batch 223770, train_perplexity=637.70953, train_loss=6.457883

Batch 223780, train_perplexity=687.6306, train_loss=6.533252

Batch 223790, train_perplexity=602.75525, train_loss=6.401511

Batch 223800, train_perplexity=677.7179, train_loss=6.518731

Batch 223810, train_perplexity=661.5673, train_loss=6.4946117

Batch 223820, train_perplexity=700.12714, train_loss=6.551262

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Batch 223830, train_perplexity=699.63184, train_loss=6.5505543

Batch 223840, train_perplexity=679.5592, train_loss=6.5214443

Batch 223850, train_perplexity=700.1738, train_loss=6.5513287

Batch 223860, train_perplexity=616.88794, train_loss=6.4246874

Batch 223870, train_perplexity=648.8674, train_loss=6.4752283

Batch 223880, train_perplexity=686.2244, train_loss=6.5312047

Batch 223890, train_perplexity=661.7282, train_loss=6.494855

Batch 223900, train_perplexity=694.57764, train_loss=6.543304

Batch 223910, train_perplexity=652.1684, train_loss=6.480303

Batch 223920, train_perplexity=702.1612, train_loss=6.554163

Batch 223930, train_perplexity=754.5514, train_loss=6.6261234

Batch 223940, train_perplexity=634.3661, train_loss=6.452626

Batch 223950, train_perplexity=724.77496, train_loss=6.585861

Batch 223960, train_perplexity=666.1834, train_loss=6.501565

Batch 223970, train_perplexity=676.762, train_loss=6.5173197

Batch 223980, train_perplexity=648.32367, train_loss=6.47439

Batch 223990, train_perplexity=715.90045, train_loss=6.573541

Batch 224000, train_perplexity=730.5228, train_loss=6.5937605

Batch 224010, train_perplexity=710.2765, train_loss=6.5656543

Batch 224020, train_perplexity=654.1009, train_loss=6.4832616

Batch 224030, train_perplexity=615.29565, train_loss=6.422103

Batch 224040, train_perplexity=623.8832, train_loss=6.435963

Batch 224050, train_perplexity=661.20087, train_loss=6.4940577

Batch 224060, train_perplexity=656.67816, train_loss=6.487194

Batch 224070, train_perplexity=731.59863, train_loss=6.595232

Batch 224080, train_perplexity=642.7737, train_loss=6.4657927

Batch 224090, train_perplexity=677.4662, train_loss=6.5183597

Batch 224100, train_perplexity=675.70306, train_loss=6.5157537

Batch 224110, train_perplexity=753.37146, train_loss=6.6245584

Batch 224120, train_perplexity=732.34766, train_loss=6.5962553

Batch 224130, train_perplexity=627.8138, train_loss=6.4422436

Batch 224140, train_perplexity=678.1685, train_loss=6.519396

Batch 224150, train_perplexity=677.3199, train_loss=6.5181437

Batch 224160, train_perplexity=713.16693, train_loss=6.5697155

Batch 224170, train_perplexity=652.09375, train_loss=6.4801884

Batch 224180, train_perplexity=699.2556, train_loss=6.5500164

Batch 224190, train_perplexity=768.79474, train_loss=6.644824

Batch 224200, train_perplexity=702.38556, train_loss=6.5544825

Batch 224210, train_perplexity=665.8378, train_loss=6.501046

Batch 224220, train_perplexity=647.30676, train_loss=6.4728203

Batch 224230, train_perplexity=696.68964, train_loss=6.54634

Batch 224240, train_perplexity=612.84546, train_loss=6.4181128

Batch 224250, train_perplexity=746.51276, train_loss=6.6154127

Batch 224260, train_perplexity=698.14386, train_loss=6.548425

Batch 224270, train_perplexity=702.02997, train_loss=6.553976

Batch 224280, train_perplexity=656.3088, train_loss=6.4866314

Batch 224290, train_perplexity=672.67377, train_loss=6.5112605

Batch 224300, train_perplexity=669.7293, train_loss=6.5068736

Batch 224310, train_perplexity=643.1692, train_loss=6.466408

Batch 224320, train_perplexity=649.6066, train_loss=6.476367

Batch 224330, train_perplexity=675.295, train_loss=6.5151496

Batch 224340, train_perplexity=618.81647, train_loss=6.427809

Batch 224350, train_perplexity=683.3943, train_loss=6.527072

Batch 224360, train_perplexity=660.0259, train_loss=6.492279

Batch 224370, train_perplexity=677.9739, train_loss=6.519109

Batch 224380, train_perplexity=672.2255, train_loss=6.510594

Batch 224390, train_perplexity=733.2072, train_loss=6.5974283

Batch 224400, train_perplexity=652.64874, train_loss=6.481039

Batch 224410, train_perplexity=702.411, train_loss=6.5545187

Batch 224420, train_perplexity=687.626, train_loss=6.533245

Batch 224430, train_perplexity=744.89417, train_loss=6.613242

Batch 224440, train_perplexity=676.1982, train_loss=6.516486

Batch 224450, train_perplexity=692.53613, train_loss=6.5403605

Batch 224460, train_perplexity=695.57825, train_loss=6.5447435

Batch 224470, train_perplexity=670.43384, train_loss=6.507925

Batch 224480, train_perplexity=702.0989, train_loss=6.5540743

Batch 224490, train_perplexity=650.3461, train_loss=6.4775047

Batch 224500, train_perplexity=668.6932, train_loss=6.5053253

Batch 224510, train_perplexity=689.94183, train_loss=6.5366073

Batch 224520, train_perplexity=655.5453, train_loss=6.4854674

Batch 224530, train_perplexity=694.48926, train_loss=6.5431767

Batch 224540, train_perplexity=784.4541, train_loss=6.664988

Batch 224550, train_perplexity=712.92755, train_loss=6.56938

Batch 224560, train_perplexity=638.2641, train_loss=6.458752

Batch 224570, train_perplexity=732.6599, train_loss=6.5966816

Batch 224580, train_perplexity=701.8606, train_loss=6.553735

Batch 224590, train_perplexity=640.7295, train_loss=6.4626074

Batch 224600, train_perplexity=751.70215, train_loss=6.62234

Batch 224610, train_perplexity=645.3817, train_loss=6.469842

Batch 224620, train_perplexity=652.2993, train_loss=6.4805036

Batch 224630, train_perplexity=706.3424, train_loss=6.5601

Batch 224640, train_perplexity=683.788, train_loss=6.527648

Batch 224650, train_perplexity=664.32513, train_loss=6.4987717

Batch 224660, train_perplexity=704.448, train_loss=6.5574145

Batch 224670, train_perplexity=702.37213, train_loss=6.5544634

Batch 224680, train_perplexity=698.09064, train_loss=6.548349

Batch 224690, train_perplexity=636.24976, train_loss=6.455591

Batch 224700, train_perplexity=652.24115, train_loss=6.4804144

Batch 224710, train_perplexity=622.4423, train_loss=6.433651

Batch 224720, train_perplexity=677.5066, train_loss=6.5184193

Batch 224730, train_perplexity=702.9518, train_loss=6.5552883

Batch 224740, train_perplexity=727.09174, train_loss=6.5890527

Batch 224750, train_perplexity=743.707, train_loss=6.611647

Batch 224760, train_perplexity=649.3533, train_loss=6.475977

Batch 224770, train_perplexity=748.4457, train_loss=6.6179986

Batch 224780, train_perplexity=744.5731, train_loss=6.612811

Batch 224790, train_perplexity=658.06995, train_loss=6.489311

Batch 224800, train_perplexity=680.6761, train_loss=6.5230865

Batch 224810, train_perplexity=658.1202, train_loss=6.4893875

Batch 224820, train_perplexity=714.08093, train_loss=6.5709963

Batch 224830, train_perplexity=785.69135, train_loss=6.666564

Batch 224840, train_perplexity=672.7299, train_loss=6.511344

Batch 224850, train_perplexity=654.08246, train_loss=6.4832335

Batch 224860, train_perplexity=723.80585, train_loss=6.584523

Batch 224870, train_perplexity=654.0987, train_loss=6.4832582

Batch 224880, train_perplexity=650.3297, train_loss=6.4774795

Batch 224890, train_perplexity=668.0959, train_loss=6.5044317

Batch 224900, train_perplexity=642.00543, train_loss=6.4645967

Batch 224910, train_perplexity=704.78503, train_loss=6.557893

Batch 224920, train_perplexity=737.2937, train_loss=6.6029863
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 224930, train_perplexity=662.6932, train_loss=6.496312

Batch 224940, train_perplexity=704.9383, train_loss=6.55811

Batch 224950, train_perplexity=648.0535, train_loss=6.4739733

Batch 224960, train_perplexity=689.7599, train_loss=6.5363436

Batch 224970, train_perplexity=646.42395, train_loss=6.4714556

Batch 224980, train_perplexity=683.44025, train_loss=6.527139

Batch 224990, train_perplexity=677.0499, train_loss=6.517745

Batch 225000, train_perplexity=668.9387, train_loss=6.5056925

Batch 225010, train_perplexity=682.44275, train_loss=6.5256786

Batch 225020, train_perplexity=627.7084, train_loss=6.4420757

Batch 225030, train_perplexity=701.5213, train_loss=6.5532513

Batch 225040, train_perplexity=666.5297, train_loss=6.5020847

Batch 225050, train_perplexity=738.75555, train_loss=6.604967

Batch 225060, train_perplexity=686.8153, train_loss=6.5320654

Batch 225070, train_perplexity=717.3415, train_loss=6.575552

Batch 225080, train_perplexity=652.2959, train_loss=6.4804983

Batch 225090, train_perplexity=749.6783, train_loss=6.619644

Batch 225100, train_perplexity=696.1543, train_loss=6.5455713

Batch 225110, train_perplexity=672.7168, train_loss=6.5113244

Batch 225120, train_perplexity=724.00885, train_loss=6.5848036

Batch 225130, train_perplexity=709.4962, train_loss=6.564555

Batch 225140, train_perplexity=681.0241, train_loss=6.5235977

Batch 225150, train_perplexity=682.7754, train_loss=6.526166

Batch 225160, train_perplexity=663.29956, train_loss=6.4972267

Batch 225170, train_perplexity=636.4813, train_loss=6.455955

Batch 225180, train_perplexity=631.17444, train_loss=6.4475822

Batch 225190, train_perplexity=693.7409, train_loss=6.5420985

Batch 225200, train_perplexity=660.17285, train_loss=6.4925017

Batch 225210, train_perplexity=709.7185, train_loss=6.5648685

Batch 225220, train_perplexity=657.92, train_loss=6.4890833

Batch 225230, train_perplexity=680.4496, train_loss=6.5227537

Batch 225240, train_perplexity=670.4105, train_loss=6.50789

Batch 225250, train_perplexity=665.1169, train_loss=6.499963

Batch 225260, train_perplexity=607.1931, train_loss=6.408847

Batch 225270, train_perplexity=666.87244, train_loss=6.502599

Batch 225280, train_perplexity=682.2033, train_loss=6.5253277

Batch 225290, train_perplexity=706.402, train_loss=6.5601845

Batch 225300, train_perplexity=635.824, train_loss=6.4549217

Batch 225310, train_perplexity=658.9906, train_loss=6.4907093

Batch 225320, train_perplexity=697.2759, train_loss=6.547181

Batch 225330, train_perplexity=689.952, train_loss=6.536622

Batch 225340, train_perplexity=680.0824, train_loss=6.522214

Batch 225350, train_perplexity=715.7885, train_loss=6.573385

Batch 225360, train_perplexity=687.5217, train_loss=6.5330935

Batch 225370, train_perplexity=651.5604, train_loss=6.47937

Batch 225380, train_perplexity=591.84033, train_loss=6.383237

Batch 225390, train_perplexity=668.4197, train_loss=6.504916

Batch 225400, train_perplexity=715.3596, train_loss=6.5727854

Batch 225410, train_perplexity=737.0329, train_loss=6.6026325

Batch 225420, train_perplexity=669.65424, train_loss=6.5067616

Batch 225430, train_perplexity=688.7102, train_loss=6.5348206

Batch 225440, train_perplexity=670.5924, train_loss=6.5081615

Batch 225450, train_perplexity=676.4542, train_loss=6.516865

Batch 225460, train_perplexity=630.5932, train_loss=6.446661

Batch 225470, train_perplexity=634.51404, train_loss=6.4528594

Batch 225480, train_perplexity=707.57684, train_loss=6.5618463

Batch 225490, train_perplexity=627.7431, train_loss=6.442131

Batch 225500, train_perplexity=663.0959, train_loss=6.4969196

Batch 225510, train_perplexity=599.825, train_loss=6.396638

Batch 225520, train_perplexity=699.9529, train_loss=6.551013

Batch 225530, train_perplexity=684.7914, train_loss=6.5291142

Batch 225540, train_perplexity=586.6161, train_loss=6.3743706

Batch 225550, train_perplexity=661.2393, train_loss=6.494116

Batch 225560, train_perplexity=600.6848, train_loss=6.3980703

Batch 225570, train_perplexity=606.90765, train_loss=6.4083767

Batch 225580, train_perplexity=641.6253, train_loss=6.4640045

Batch 225590, train_perplexity=654.23004, train_loss=6.483459

Batch 225600, train_perplexity=675.65607, train_loss=6.515684

Batch 225610, train_perplexity=682.9851, train_loss=6.526473

Batch 225620, train_perplexity=725.49866, train_loss=6.586859

Batch 225630, train_perplexity=718.6781, train_loss=6.5774136

Batch 225640, train_perplexity=687.4211, train_loss=6.532947

Batch 225650, train_perplexity=695.1708, train_loss=6.5441575

Batch 225660, train_perplexity=673.16956, train_loss=6.511997

Batch 225670, train_perplexity=614.8716, train_loss=6.4214134

Batch 225680, train_perplexity=639.1135, train_loss=6.460082

Batch 225690, train_perplexity=699.42035, train_loss=6.550252

Batch 225700, train_perplexity=696.7707, train_loss=6.5464563

Batch 225710, train_perplexity=652.5218, train_loss=6.4808445

Batch 225720, train_perplexity=710.3452, train_loss=6.565751

Batch 225730, train_perplexity=742.60034, train_loss=6.610158

Batch 225740, train_perplexity=767.5175, train_loss=6.6431613

Batch 225750, train_perplexity=731.2994, train_loss=6.594823

Batch 225760, train_perplexity=727.62555, train_loss=6.5897865

Batch 225770, train_perplexity=675.6934, train_loss=6.5157394

Batch 225780, train_perplexity=783.13293, train_loss=6.6633024

Batch 225790, train_perplexity=753.64453, train_loss=6.624921

Batch 225800, train_perplexity=691.30615, train_loss=6.538583

Batch 225810, train_perplexity=738.9289, train_loss=6.6052017

Batch 225820, train_perplexity=690.0053, train_loss=6.5366993

Batch 225830, train_perplexity=639.33844, train_loss=6.460434

Batch 225840, train_perplexity=599.813, train_loss=6.396618

Batch 225850, train_perplexity=672.4349, train_loss=6.5109053

Batch 225860, train_perplexity=635.83606, train_loss=6.454941

Batch 225870, train_perplexity=707.0049, train_loss=6.5610375

Batch 225880, train_perplexity=722.62573, train_loss=6.5828915

Batch 225890, train_perplexity=631.05347, train_loss=6.4473906

Batch 225900, train_perplexity=686.93713, train_loss=6.532243

Batch 225910, train_perplexity=701.965, train_loss=6.5538836

Batch 225920, train_perplexity=681.1664, train_loss=6.5238066

Batch 225930, train_perplexity=712.17596, train_loss=6.568325

Batch 225940, train_perplexity=696.8803, train_loss=6.5466137

Batch 225950, train_perplexity=762.11426, train_loss=6.6360965

Batch 225960, train_perplexity=679.5184, train_loss=6.5213842

Batch 225970, train_perplexity=740.1522, train_loss=6.606856

Batch 225980, train_perplexity=716.6037, train_loss=6.574523

Batch 225990, train_perplexity=671.8868, train_loss=6.51009

Batch 226000, train_perplexity=644.8606, train_loss=6.469034

Batch 226010, train_perplexity=665.5261, train_loss=6.500578

Batch 226020, train_perplexity=626.02313, train_loss=6.4393873

Batch 226030, train_perplexity=649.28455, train_loss=6.475871

Batch 226040, train_perplexity=721.928, train_loss=6.5819254

Batch 226050, train_perplexity=685.97906, train_loss=6.530847

Batch 226060, train_perplexity=603.5646, train_loss=6.402853

Batch 226070, train_perplexity=684.08777, train_loss=6.528086

Batch 226080, train_perplexity=681.07605, train_loss=6.523674

Batch 226090, train_perplexity=761.1838, train_loss=6.634875

Batch 226100, train_perplexity=675.1021, train_loss=6.514864

Batch 226110, train_perplexity=663.28467, train_loss=6.4972043

Batch 226120, train_perplexity=650.0882, train_loss=6.477108

Batch 226130, train_perplexity=708.35297, train_loss=6.5629425

Batch 226140, train_perplexity=651.0033, train_loss=6.4785147

Batch 226150, train_perplexity=608.19916, train_loss=6.4105024

Batch 226160, train_perplexity=596.14667, train_loss=6.3904867

Batch 226170, train_perplexity=648.6564, train_loss=6.474903

Batch 226180, train_perplexity=710.30695, train_loss=6.565697

Batch 226190, train_perplexity=707.90656, train_loss=6.562312

Batch 226200, train_perplexity=628.5731, train_loss=6.4434524

Batch 226210, train_perplexity=654.6391, train_loss=6.484084

Batch 226220, train_perplexity=608.08026, train_loss=6.410307

Batch 226230, train_perplexity=637.8008, train_loss=6.458026
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 226240, train_perplexity=681.39215, train_loss=6.524138

Batch 226250, train_perplexity=693.9298, train_loss=6.542371

Batch 226260, train_perplexity=582.6052, train_loss=6.36751

Batch 226270, train_perplexity=677.9765, train_loss=6.5191126

Batch 226280, train_perplexity=618.5492, train_loss=6.4273767

Batch 226290, train_perplexity=741.0877, train_loss=6.608119

Batch 226300, train_perplexity=726.8273, train_loss=6.588689

Batch 226310, train_perplexity=665.98517, train_loss=6.5012674

Batch 226320, train_perplexity=698.55676, train_loss=6.5490165

Batch 226330, train_perplexity=671.83044, train_loss=6.510006

Batch 226340, train_perplexity=690.79376, train_loss=6.5378413

Batch 226350, train_perplexity=691.6774, train_loss=6.5391197

Batch 226360, train_perplexity=723.3411, train_loss=6.583881

Batch 226370, train_perplexity=663.17114, train_loss=6.497033

Batch 226380, train_perplexity=650.82263, train_loss=6.478237

Batch 226390, train_perplexity=632.36584, train_loss=6.449468

Batch 226400, train_perplexity=631.33215, train_loss=6.447832

Batch 226410, train_perplexity=716.3447, train_loss=6.5741615

Batch 226420, train_perplexity=718.4766, train_loss=6.577133

Batch 226430, train_perplexity=661.73645, train_loss=6.4948673

Batch 226440, train_perplexity=669.6118, train_loss=6.506698

Batch 226450, train_perplexity=697.246, train_loss=6.547138

Batch 226460, train_perplexity=696.3193, train_loss=6.5458083

Batch 226470, train_perplexity=675.4222, train_loss=6.515338

Batch 226480, train_perplexity=664.1303, train_loss=6.4984784

Batch 226490, train_perplexity=590.80383, train_loss=6.381484

Batch 226500, train_perplexity=754.7457, train_loss=6.626381

Batch 226510, train_perplexity=636.58356, train_loss=6.4561157

Batch 226520, train_perplexity=666.8915, train_loss=6.5026274

Batch 226530, train_perplexity=704.4987, train_loss=6.5574865

Batch 226540, train_perplexity=696.20013, train_loss=6.545637

Batch 226550, train_perplexity=633.25726, train_loss=6.4508767

Batch 226560, train_perplexity=656.2102, train_loss=6.486481

Batch 226570, train_perplexity=630.68823, train_loss=6.4468117

Batch 226580, train_perplexity=715.449, train_loss=6.5729103

Batch 226590, train_perplexity=670.48694, train_loss=6.508004

Batch 226600, train_perplexity=628.56024, train_loss=6.443432

Batch 226610, train_perplexity=736.2868, train_loss=6.6016197

Batch 226620, train_perplexity=685.29834, train_loss=6.5298543

Batch 226630, train_perplexity=733.1547, train_loss=6.597357

Batch 226640, train_perplexity=729.6852, train_loss=6.592613

Batch 226650, train_perplexity=771.9826, train_loss=6.648962

Batch 226660, train_perplexity=691.80676, train_loss=6.5393066

Batch 226670, train_perplexity=685.58044, train_loss=6.530266

Batch 226680, train_perplexity=761.6336, train_loss=6.6354656

Batch 226690, train_perplexity=729.32935, train_loss=6.5921254

Batch 226700, train_perplexity=753.50726, train_loss=6.6247387

Batch 226710, train_perplexity=705.586, train_loss=6.5590286

Batch 226720, train_perplexity=668.06055, train_loss=6.504379

Batch 226730, train_perplexity=735.2497, train_loss=6.60021

Batch 226740, train_perplexity=629.8558, train_loss=6.445491

Batch 226750, train_perplexity=723.8204, train_loss=6.584543

Batch 226760, train_perplexity=725.77893, train_loss=6.5872455

Batch 226770, train_perplexity=691.3622, train_loss=6.538664

Batch 226780, train_perplexity=618.50525, train_loss=6.4273057

Batch 226790, train_perplexity=658.9733, train_loss=6.490683

Batch 226800, train_perplexity=643.0361, train_loss=6.466201

Batch 226810, train_perplexity=610.5784, train_loss=6.414407

Batch 226820, train_perplexity=745.5882, train_loss=6.6141734

Batch 226830, train_perplexity=641.19836, train_loss=6.463339

Batch 226840, train_perplexity=702.33936, train_loss=6.5544167

Batch 226850, train_perplexity=634.26263, train_loss=6.452463

Batch 226860, train_perplexity=662.5968, train_loss=6.4961667

Batch 226870, train_perplexity=658.53516, train_loss=6.490018

Batch 226880, train_perplexity=594.9707, train_loss=6.388512

Batch 226890, train_perplexity=619.99023, train_loss=6.4297037

Batch 226900, train_perplexity=616.8338, train_loss=6.4245996

Batch 226910, train_perplexity=674.32324, train_loss=6.5137095

Batch 226920, train_perplexity=636.37537, train_loss=6.4557886

Batch 226930, train_perplexity=624.9667, train_loss=6.4376984

Batch 226940, train_perplexity=704.79645, train_loss=6.557909

Batch 226950, train_perplexity=625.4174, train_loss=6.4384193

Batch 226960, train_perplexity=710.6647, train_loss=6.5662007

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Batch 226970, train_perplexity=638.15576, train_loss=6.4585824

Batch 226980, train_perplexity=662.6913, train_loss=6.4963093

Batch 226990, train_perplexity=649.70636, train_loss=6.4765205

Batch 227000, train_perplexity=660.5139, train_loss=6.493018

Batch 227010, train_perplexity=698.46185, train_loss=6.5488806

Batch 227020, train_perplexity=705.3989, train_loss=6.5587635

Batch 227030, train_perplexity=632.81866, train_loss=6.450184

Batch 227040, train_perplexity=663.0396, train_loss=6.4968348

Batch 227050, train_perplexity=780.5583, train_loss=6.6600094

Batch 227060, train_perplexity=657.0045, train_loss=6.487691

Batch 227070, train_perplexity=735.8846, train_loss=6.6010733

Batch 227080, train_perplexity=695.6154, train_loss=6.544797

Batch 227090, train_perplexity=686.6175, train_loss=6.5317774

Batch 227100, train_perplexity=694.52203, train_loss=6.543224

Batch 227110, train_perplexity=788.5378, train_loss=6.6701803

Batch 227120, train_perplexity=666.11224, train_loss=6.501458

Batch 227130, train_perplexity=713.54584, train_loss=6.5702467

Batch 227140, train_perplexity=631.70374, train_loss=6.4484205

Batch 227150, train_perplexity=683.4595, train_loss=6.5271673

Batch 227160, train_perplexity=612.37744, train_loss=6.417349

Batch 227170, train_perplexity=704.1461, train_loss=6.556986

Batch 227180, train_perplexity=645.2845, train_loss=6.4696913

Batch 227190, train_perplexity=681.02155, train_loss=6.523594

Batch 227200, train_perplexity=621.3045, train_loss=6.4318213

Batch 227210, train_perplexity=620.5392, train_loss=6.4305887

Batch 227220, train_perplexity=727.2367, train_loss=6.589252

Batch 227230, train_perplexity=664.7177, train_loss=6.4993625

Batch 227240, train_perplexity=610.4055, train_loss=6.4141235

Batch 227250, train_perplexity=641.11395, train_loss=6.4632072

Batch 227260, train_perplexity=668.65936, train_loss=6.505275

Batch 227270, train_perplexity=674.98627, train_loss=6.5146923

Batch 227280, train_perplexity=669.9014, train_loss=6.5071306

Batch 227290, train_perplexity=640.472, train_loss=6.4622054

Batch 227300, train_perplexity=658.7154, train_loss=6.4902916

Batch 227310, train_perplexity=712.50037, train_loss=6.5687804

Batch 227320, train_perplexity=675.7991, train_loss=6.515896

Batch 227330, train_perplexity=631.4959, train_loss=6.4480915

Batch 227340, train_perplexity=654.86017, train_loss=6.4844217

Batch 227350, train_perplexity=670.72034, train_loss=6.5083523

Batch 227360, train_perplexity=676.05725, train_loss=6.516278

Batch 227370, train_perplexity=617.92426, train_loss=6.426366

Batch 227380, train_perplexity=689.07416, train_loss=6.535349

Batch 227390, train_perplexity=639.31647, train_loss=6.4603996

Batch 227400, train_perplexity=608.8904, train_loss=6.4116383

Batch 227410, train_perplexity=677.83746, train_loss=6.5189075

Batch 227420, train_perplexity=742.8114, train_loss=6.610442

Batch 227430, train_perplexity=692.17926, train_loss=6.539845

Batch 227440, train_perplexity=641.7673, train_loss=6.464226

Batch 227450, train_perplexity=653.66907, train_loss=6.482601

Batch 227460, train_perplexity=616.7803, train_loss=6.424513

Batch 227470, train_perplexity=675.25024, train_loss=6.5150833
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 227480, train_perplexity=651.7021, train_loss=6.4795876

Batch 227490, train_perplexity=653.36615, train_loss=6.4821377

Batch 227500, train_perplexity=735.687, train_loss=6.600805

Batch 227510, train_perplexity=583.38336, train_loss=6.3688445

Batch 227520, train_perplexity=600.33887, train_loss=6.3974943

Batch 227530, train_perplexity=722.49274, train_loss=6.5827074

Batch 227540, train_perplexity=635.42084, train_loss=6.4542875

Batch 227550, train_perplexity=777.2727, train_loss=6.6557913

Batch 227560, train_perplexity=630.4639, train_loss=6.446456

Batch 227570, train_perplexity=637.32983, train_loss=6.4572873

Batch 227580, train_perplexity=674.25793, train_loss=6.5136127

Batch 227590, train_perplexity=690.7579, train_loss=6.5377893

Batch 227600, train_perplexity=708.5989, train_loss=6.5632896

Batch 227610, train_perplexity=654.16046, train_loss=6.4833527

Batch 227620, train_perplexity=689.5175, train_loss=6.535992

Batch 227630, train_perplexity=672.4817, train_loss=6.510975

Batch 227640, train_perplexity=690.7994, train_loss=6.5378494

Batch 227650, train_perplexity=702.59186, train_loss=6.554776

Batch 227660, train_perplexity=693.65125, train_loss=6.5419693

Batch 227670, train_perplexity=578.6028, train_loss=6.360616

Batch 227680, train_perplexity=686.19366, train_loss=6.53116

Batch 227690, train_perplexity=700.1979, train_loss=6.551363

Batch 227700, train_perplexity=755.7368, train_loss=6.627693

Batch 227710, train_perplexity=738.41534, train_loss=6.6045065

Batch 227720, train_perplexity=674.2772, train_loss=6.5136414

Batch 227730, train_perplexity=676.04083, train_loss=6.5162535

Batch 227740, train_perplexity=685.82043, train_loss=6.530616

Batch 227750, train_perplexity=618.2001, train_loss=6.426812

Batch 227760, train_perplexity=679.29224, train_loss=6.5210514

Batch 227770, train_perplexity=671.4554, train_loss=6.5094476

Batch 227780, train_perplexity=629.62067, train_loss=6.4451175

Batch 227790, train_perplexity=686.84937, train_loss=6.532115

Batch 227800, train_perplexity=668.68555, train_loss=6.505314

Batch 227810, train_perplexity=653.5001, train_loss=6.4823427

Batch 227820, train_perplexity=715.1584, train_loss=6.572504

Batch 227830, train_perplexity=691.8681, train_loss=6.5393953

Batch 227840, train_perplexity=682.2072, train_loss=6.5253334

Batch 227850, train_perplexity=676.7485, train_loss=6.5172997

Batch 227860, train_perplexity=635.7445, train_loss=6.454797

Batch 227870, train_perplexity=630.801, train_loss=6.4469905

Batch 227880, train_perplexity=720.4293, train_loss=6.5798473

Batch 227890, train_perplexity=666.5424, train_loss=6.502104

Batch 227900, train_perplexity=688.4639, train_loss=6.534463

Batch 227910, train_perplexity=613.81757, train_loss=6.4196978

Batch 227920, train_perplexity=681.80164, train_loss=6.524739

Batch 227930, train_perplexity=648.4674, train_loss=6.4746118

Batch 227940, train_perplexity=707.3849, train_loss=6.561575

Batch 227950, train_perplexity=717.86053, train_loss=6.5762753

Batch 227960, train_perplexity=699.42004, train_loss=6.5502515

Batch 227970, train_perplexity=622.29663, train_loss=6.433417

Batch 227980, train_perplexity=617.9596, train_loss=6.426423

Batch 227990, train_perplexity=598.56555, train_loss=6.394536

Batch 228000, train_perplexity=728.5108, train_loss=6.5910025

Batch 228010, train_perplexity=650.0888, train_loss=6.477109

Batch 228020, train_perplexity=677.0609, train_loss=6.517761

Batch 228030, train_perplexity=697.4348, train_loss=6.547409

Batch 228040, train_perplexity=640.7356, train_loss=6.462617

Batch 228050, train_perplexity=696.47107, train_loss=6.546026

Batch 228060, train_perplexity=689.61786, train_loss=6.5361376

Batch 228070, train_perplexity=762.72644, train_loss=6.6368995

Batch 228080, train_perplexity=635.79846, train_loss=6.4548817

Batch 228090, train_perplexity=768.8802, train_loss=6.644935

Batch 228100, train_perplexity=653.3244, train_loss=6.482074

Batch 228110, train_perplexity=699.5188, train_loss=6.5503926

Batch 228120, train_perplexity=629.5375, train_loss=6.4449854

Batch 228130, train_perplexity=626.731, train_loss=6.4405174

Batch 228140, train_perplexity=649.1094, train_loss=6.475601

Batch 228150, train_perplexity=669.9973, train_loss=6.5072737

Batch 228160, train_perplexity=669.8286, train_loss=6.507022

Batch 228170, train_perplexity=685.283, train_loss=6.529832

Batch 228180, train_perplexity=738.81757, train_loss=6.605051

Batch 228190, train_perplexity=681.75616, train_loss=6.524672

Batch 228200, train_perplexity=714.8045, train_loss=6.572009

Batch 228210, train_perplexity=696.21735, train_loss=6.545662

Batch 228220, train_perplexity=705.35455, train_loss=6.5587006

Batch 228230, train_perplexity=633.4375, train_loss=6.4511614

Batch 228240, train_perplexity=724.2312, train_loss=6.5851107

Batch 228250, train_perplexity=696.1878, train_loss=6.5456195

Batch 228260, train_perplexity=584.2006, train_loss=6.3702445

Batch 228270, train_perplexity=663.46783, train_loss=6.4974804

Batch 228280, train_perplexity=707.3937, train_loss=6.5615873

Batch 228290, train_perplexity=634.8733, train_loss=6.4534254

Batch 228300, train_perplexity=627.8524, train_loss=6.442305

Batch 228310, train_perplexity=706.836, train_loss=6.5607986

Batch 228320, train_perplexity=714.3043, train_loss=6.571309

Batch 228330, train_perplexity=646.79736, train_loss=6.472033

Batch 228340, train_perplexity=675.92413, train_loss=6.516081

Batch 228350, train_perplexity=743.36273, train_loss=6.611184

Batch 228360, train_perplexity=673.1824, train_loss=6.5120163

Batch 228370, train_perplexity=667.2414, train_loss=6.503152

Batch 228380, train_perplexity=692.97156, train_loss=6.540989

Batch 228390, train_perplexity=698.2654, train_loss=6.5485992

Batch 228400, train_perplexity=731.1602, train_loss=6.5946326

Batch 228410, train_perplexity=696.1005, train_loss=6.545494

Batch 228420, train_perplexity=663.25653, train_loss=6.497162

Batch 228430, train_perplexity=685.62225, train_loss=6.530327

Batch 228440, train_perplexity=651.65796, train_loss=6.47952

Batch 228450, train_perplexity=695.8244, train_loss=6.5450974

Batch 228460, train_perplexity=667.3222, train_loss=6.503273

Batch 228470, train_perplexity=693.82227, train_loss=6.542216

Batch 228480, train_perplexity=679.50604, train_loss=6.521366

Batch 228490, train_perplexity=682.6914, train_loss=6.526043

Batch 228500, train_perplexity=665.95404, train_loss=6.5012207

Batch 228510, train_perplexity=628.9086, train_loss=6.443986

Batch 228520, train_perplexity=699.7106, train_loss=6.550667

Batch 228530, train_perplexity=720.92554, train_loss=6.580536

Batch 228540, train_perplexity=736.8772, train_loss=6.6024213

Batch 228550, train_perplexity=643.30963, train_loss=6.466626

Batch 228560, train_perplexity=593.119, train_loss=6.385395

Batch 228570, train_perplexity=651.7885, train_loss=6.47972

Batch 228580, train_perplexity=705.66034, train_loss=6.559134

Batch 228590, train_perplexity=662.0457, train_loss=6.4953346

Batch 228600, train_perplexity=661.91315, train_loss=6.4951344

Batch 228610, train_perplexity=582.73083, train_loss=6.3677254

Batch 228620, train_perplexity=696.672, train_loss=6.5463147

Batch 228630, train_perplexity=635.70483, train_loss=6.4547343

Batch 228640, train_perplexity=654.62756, train_loss=6.4840665

Batch 228650, train_perplexity=689.8724, train_loss=6.5365067

Batch 228660, train_perplexity=704.81055, train_loss=6.557929

Batch 228670, train_perplexity=658.90894, train_loss=6.4905853

Batch 228680, train_perplexity=636.94305, train_loss=6.4566803

Batch 228690, train_perplexity=649.99335, train_loss=6.476962

Batch 228700, train_perplexity=813.88995, train_loss=6.701825

Batch 228710, train_perplexity=733.97534, train_loss=6.5984755

Batch 228720, train_perplexity=573.3004, train_loss=6.35141

Batch 228730, train_perplexity=675.9145, train_loss=6.5160666

Batch 228740, train_perplexity=591.69556, train_loss=6.3829923

Batch 228750, train_perplexity=737.89233, train_loss=6.603798

Batch 228760, train_perplexity=692.4371, train_loss=6.5402174

Batch 228770, train_perplexity=784.6221, train_loss=6.665202

Batch 228780, train_perplexity=659.1779, train_loss=6.4909935
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 228790, train_perplexity=722.2, train_loss=6.582302

Batch 228800, train_perplexity=696.0587, train_loss=6.545434

Batch 228810, train_perplexity=744.5412, train_loss=6.612768

Batch 228820, train_perplexity=627.01855, train_loss=6.440976

Batch 228830, train_perplexity=703.8138, train_loss=6.556514

Batch 228840, train_perplexity=724.8216, train_loss=6.5859256

Batch 228850, train_perplexity=709.6174, train_loss=6.564726

Batch 228860, train_perplexity=629.3406, train_loss=6.4446726

Batch 228870, train_perplexity=647.95337, train_loss=6.473819

Batch 228880, train_perplexity=654.8789, train_loss=6.4844503

Batch 228890, train_perplexity=774.10583, train_loss=6.6517086

Batch 228900, train_perplexity=629.1528, train_loss=6.444374

Batch 228910, train_perplexity=745.2253, train_loss=6.6136866

Batch 228920, train_perplexity=739.54895, train_loss=6.6060405

Batch 228930, train_perplexity=686.7871, train_loss=6.5320244

Batch 228940, train_perplexity=739.78986, train_loss=6.606366

Batch 228950, train_perplexity=716.19855, train_loss=6.5739574

Batch 228960, train_perplexity=597.80225, train_loss=6.39326

Batch 228970, train_perplexity=633.67163, train_loss=6.451531

Batch 228980, train_perplexity=720.37646, train_loss=6.579774

Batch 228990, train_perplexity=751.90436, train_loss=6.622609

Batch 229000, train_perplexity=737.2575, train_loss=6.602937

Batch 229010, train_perplexity=644.9824, train_loss=6.469223

Batch 229020, train_perplexity=726.11884, train_loss=6.5877137

Batch 229030, train_perplexity=728.3552, train_loss=6.590789

Batch 229040, train_perplexity=703.9856, train_loss=6.556758

Batch 229050, train_perplexity=689.0019, train_loss=6.535244

Batch 229060, train_perplexity=667.35754, train_loss=6.503326

Batch 229070, train_perplexity=694.31244, train_loss=6.542922

Batch 229080, train_perplexity=667.98444, train_loss=6.504265

Batch 229090, train_perplexity=718.74115, train_loss=6.5775013

Batch 229100, train_perplexity=680.49725, train_loss=6.522824

Batch 229110, train_perplexity=766.86597, train_loss=6.642312

Batch 229120, train_perplexity=715.69806, train_loss=6.5732584

Batch 229130, train_perplexity=718.1194, train_loss=6.576636

Batch 229140, train_perplexity=680.7167, train_loss=6.523146

Batch 229150, train_perplexity=724.7532, train_loss=6.585831

Batch 229160, train_perplexity=668.9764, train_loss=6.5057487

Batch 229170, train_perplexity=716.0155, train_loss=6.573702

Batch 229180, train_perplexity=702.4133, train_loss=6.554522

Batch 229190, train_perplexity=679.5786, train_loss=6.521473

Batch 229200, train_perplexity=719.8957, train_loss=6.5791063

Batch 229210, train_perplexity=667.77484, train_loss=6.503951

Batch 229220, train_perplexity=661.6818, train_loss=6.494785

Batch 229230, train_perplexity=664.3638, train_loss=6.49883

Batch 229240, train_perplexity=685.10724, train_loss=6.5295753

Batch 229250, train_perplexity=694.34753, train_loss=6.5429726

Batch 229260, train_perplexity=677.97906, train_loss=6.5191164

Batch 229270, train_perplexity=632.7227, train_loss=6.450032

Batch 229280, train_perplexity=661.0754, train_loss=6.493868

Batch 229290, train_perplexity=631.73566, train_loss=6.448471

Batch 229300, train_perplexity=731.4242, train_loss=6.5949936

Batch 229310, train_perplexity=632.30164, train_loss=6.4493666

Batch 229320, train_perplexity=695.565, train_loss=6.5447245

Batch 229330, train_perplexity=583.761, train_loss=6.3694916

Batch 229340, train_perplexity=702.2489, train_loss=6.554288

Batch 229350, train_perplexity=723.62915, train_loss=6.584279

Batch 229360, train_perplexity=697.218, train_loss=6.547098

Batch 229370, train_perplexity=680.1972, train_loss=6.5223827

Batch 229380, train_perplexity=624.48706, train_loss=6.4369307

Batch 229390, train_perplexity=673.54266, train_loss=6.5125513

Batch 229400, train_perplexity=626.624, train_loss=6.4403467

Batch 229410, train_perplexity=686.478, train_loss=6.5315742

Batch 229420, train_perplexity=648.709, train_loss=6.474984

Batch 229430, train_perplexity=708.8983, train_loss=6.563712

Batch 229440, train_perplexity=735.6814, train_loss=6.600797

Batch 229450, train_perplexity=697.8653, train_loss=6.548026

Batch 229460, train_perplexity=685.58044, train_loss=6.530266

Batch 229470, train_perplexity=680.8027, train_loss=6.5232725

Batch 229480, train_perplexity=703.8225, train_loss=6.556526

Batch 229490, train_perplexity=686.5553, train_loss=6.531687

Batch 229500, train_perplexity=684.94257, train_loss=6.529335

Batch 229510, train_perplexity=662.3457, train_loss=6.4957876

Batch 229520, train_perplexity=674.74615, train_loss=6.5143366

Batch 229530, train_perplexity=653.03723, train_loss=6.481634

Batch 229540, train_perplexity=670.0986, train_loss=6.507425

Batch 229550, train_perplexity=673.3853, train_loss=6.5123177

Batch 229560, train_perplexity=595.4864, train_loss=6.3893785

Batch 229570, train_perplexity=658.27673, train_loss=6.4896255

Batch 229580, train_perplexity=647.8941, train_loss=6.473727

Batch 229590, train_perplexity=616.01227, train_loss=6.423267

Batch 229600, train_perplexity=714.46716, train_loss=6.571537

Batch 229610, train_perplexity=646.4844, train_loss=6.471549

Batch 229620, train_perplexity=694.68665, train_loss=6.543461

Batch 229630, train_perplexity=722.8312, train_loss=6.5831757

Batch 229640, train_perplexity=681.9873, train_loss=6.525011

Batch 229650, train_perplexity=651.23553, train_loss=6.4788713

Batch 229660, train_perplexity=746.8838, train_loss=6.6159096

Batch 229670, train_perplexity=694.1125, train_loss=6.542634

Batch 229680, train_perplexity=645.94696, train_loss=6.4707174

Batch 229690, train_perplexity=714.6409, train_loss=6.57178

Batch 229700, train_perplexity=680.778, train_loss=6.5232363

Batch 229710, train_perplexity=702.23486, train_loss=6.554268

Batch 229720, train_perplexity=667.20605, train_loss=6.503099

Batch 229730, train_perplexity=662.22284, train_loss=6.495602

Batch 229740, train_perplexity=687.60504, train_loss=6.5332146

Batch 229750, train_perplexity=687.3378, train_loss=6.532826

Batch 229760, train_perplexity=609.33246, train_loss=6.412364

Batch 229770, train_perplexity=721.9552, train_loss=6.581963

Batch 229780, train_perplexity=759.49896, train_loss=6.632659

Batch 229790, train_perplexity=655.10474, train_loss=6.484795

Batch 229800, train_perplexity=679.3648, train_loss=6.521158

Batch 229810, train_perplexity=649.59796, train_loss=6.4763536

Batch 229820, train_perplexity=682.5293, train_loss=6.5258055

Batch 229830, train_perplexity=656.1733, train_loss=6.486425

Batch 229840, train_perplexity=658.2714, train_loss=6.4896173

Batch 229850, train_perplexity=664.6781, train_loss=6.499303

Batch 229860, train_perplexity=607.5322, train_loss=6.409405

Batch 229870, train_perplexity=640.30133, train_loss=6.461939

Batch 229880, train_perplexity=726.93054, train_loss=6.588831

Batch 229890, train_perplexity=615.76587, train_loss=6.422867

Batch 229900, train_perplexity=664.4987, train_loss=6.499033

Batch 229910, train_perplexity=698.43256, train_loss=6.5488386

Batch 229920, train_perplexity=723.80066, train_loss=6.584516

Batch 229930, train_perplexity=652.09656, train_loss=6.4801927

Batch 229940, train_perplexity=696.6803, train_loss=6.5463266

Batch 229950, train_perplexity=669.5093, train_loss=6.506545

Batch 229960, train_perplexity=620.039, train_loss=6.4297824

Batch 229970, train_perplexity=661.5424, train_loss=6.494574

Batch 229980, train_perplexity=668.0564, train_loss=6.5043726

Batch 229990, train_perplexity=623.581, train_loss=6.4354787

Batch 230000, train_perplexity=689.6711, train_loss=6.536215

Batch 230010, train_perplexity=712.7467, train_loss=6.569126

Batch 230020, train_perplexity=685.9277, train_loss=6.530772

Batch 230030, train_perplexity=675.81134, train_loss=6.515914

Batch 230040, train_perplexity=765.1638, train_loss=6.64009

Batch 230050, train_perplexity=613.776, train_loss=6.41963

Batch 230060, train_perplexity=667.0143, train_loss=6.5028114

Batch 230070, train_perplexity=729.7492, train_loss=6.592701

Batch 230080, train_perplexity=646.45044, train_loss=6.4714966

Batch 230090, train_perplexity=624.5526, train_loss=6.4370356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 230100, train_perplexity=657.9984, train_loss=6.4892025

Batch 230110, train_perplexity=719.5607, train_loss=6.578641

Batch 230120, train_perplexity=738.4569, train_loss=6.6045628

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Batch 230130, train_perplexity=683.3226, train_loss=6.526967

Batch 230140, train_perplexity=651.4442, train_loss=6.479192

Batch 230150, train_perplexity=692.29346, train_loss=6.54001

Batch 230160, train_perplexity=709.51685, train_loss=6.5645843

Batch 230170, train_perplexity=751.32733, train_loss=6.6218414

Batch 230180, train_perplexity=632.9146, train_loss=6.4503355

Batch 230190, train_perplexity=679.2255, train_loss=6.520953

Batch 230200, train_perplexity=715.9561, train_loss=6.573619

Batch 230210, train_perplexity=625.1807, train_loss=6.4380407

Batch 230220, train_perplexity=626.1581, train_loss=6.439603

Batch 230230, train_perplexity=678.43634, train_loss=6.5197906

Batch 230240, train_perplexity=659.7364, train_loss=6.4918404

Batch 230250, train_perplexity=680.79877, train_loss=6.523267

Batch 230260, train_perplexity=600.73175, train_loss=6.3981485

Batch 230270, train_perplexity=723.87006, train_loss=6.584612

Batch 230280, train_perplexity=648.8927, train_loss=6.4752674

Batch 230290, train_perplexity=714.4297, train_loss=6.5714846

Batch 230300, train_perplexity=752.2099, train_loss=6.6230154

Batch 230310, train_perplexity=750.25836, train_loss=6.6204176

Batch 230320, train_perplexity=719.93896, train_loss=6.5791664

Batch 230330, train_perplexity=701.43567, train_loss=6.553129

Batch 230340, train_perplexity=636.319, train_loss=6.4557

Batch 230350, train_perplexity=650.028, train_loss=6.4770155

Batch 230360, train_perplexity=669.97205, train_loss=6.507236

Batch 230370, train_perplexity=720.88086, train_loss=6.580474

Batch 230380, train_perplexity=603.98895, train_loss=6.403556

Batch 230390, train_perplexity=676.5162, train_loss=6.5169563

Batch 230400, train_perplexity=753.5375, train_loss=6.6247787

Batch 230410, train_perplexity=601.6164, train_loss=6.39962

Batch 230420, train_perplexity=682.7012, train_loss=6.5260572

Batch 230430, train_perplexity=651.3563, train_loss=6.479057

Batch 230440, train_perplexity=635.90704, train_loss=6.4550524

Batch 230450, train_perplexity=623.94775, train_loss=6.4360666

Batch 230460, train_perplexity=686.5363, train_loss=6.531659

Batch 230470, train_perplexity=703.79126, train_loss=6.556482

Batch 230480, train_perplexity=701.8124, train_loss=6.553666

Batch 230490, train_perplexity=698.95294, train_loss=6.5495834

Batch 230500, train_perplexity=716.54767, train_loss=6.574445

Batch 230510, train_perplexity=692.22314, train_loss=6.5399084

Batch 230520, train_perplexity=639.55005, train_loss=6.460765

Batch 230530, train_perplexity=636.1269, train_loss=6.455398

Batch 230540, train_perplexity=699.35504, train_loss=6.5501585

Batch 230550, train_perplexity=598.7448, train_loss=6.3948355

Batch 230560, train_perplexity=651.3563, train_loss=6.479057

Batch 230570, train_perplexity=681.7893, train_loss=6.5247207

Batch 230580, train_perplexity=688.202, train_loss=6.5340824

Batch 230590, train_perplexity=741.13934, train_loss=6.6081886

Batch 230600, train_perplexity=673.9256, train_loss=6.5131197

Batch 230610, train_perplexity=673.8057, train_loss=6.512942

Batch 230620, train_perplexity=690.4905, train_loss=6.537402

Batch 230630, train_perplexity=637.05914, train_loss=6.4568624

Batch 230640, train_perplexity=687.0128, train_loss=6.532353

Batch 230650, train_perplexity=634.5876, train_loss=6.4529753

Batch 230660, train_perplexity=623.3417, train_loss=6.435095

Batch 230670, train_perplexity=655.7901, train_loss=6.485841

Batch 230680, train_perplexity=727.2252, train_loss=6.5892363

Batch 230690, train_perplexity=649.9682, train_loss=6.4769235

Batch 230700, train_perplexity=648.6459, train_loss=6.474887

Batch 230710, train_perplexity=662.67395, train_loss=6.496283

Batch 230720, train_perplexity=769.9669, train_loss=6.6463475

Batch 230730, train_perplexity=686.8638, train_loss=6.532136

Batch 230740, train_perplexity=685.86096, train_loss=6.530675

Batch 230750, train_perplexity=630.8561, train_loss=6.4470778

Batch 230760, train_perplexity=614.00726, train_loss=6.4200068

Batch 230770, train_perplexity=672.66736, train_loss=6.511251

Batch 230780, train_perplexity=718.7761, train_loss=6.57755

Batch 230790, train_perplexity=634.8497, train_loss=6.453388

Batch 230800, train_perplexity=592.2322, train_loss=6.3838987

Batch 230810, train_perplexity=661.4645, train_loss=6.4944563

Batch 230820, train_perplexity=648.1298, train_loss=6.474091

Batch 230830, train_perplexity=593.1224, train_loss=6.385401

Batch 230840, train_perplexity=650.41187, train_loss=6.477606

Batch 230850, train_perplexity=670.96344, train_loss=6.5087147

Batch 230860, train_perplexity=618.52826, train_loss=6.427343

Batch 230870, train_perplexity=666.07855, train_loss=6.5014076

Batch 230880, train_perplexity=701.27313, train_loss=6.5528975

Batch 230890, train_perplexity=677.01056, train_loss=6.517687

Batch 230900, train_perplexity=681.05853, train_loss=6.5236483

Batch 230910, train_perplexity=644.7222, train_loss=6.4688196

Batch 230920, train_perplexity=607.78174, train_loss=6.409816

Batch 230930, train_perplexity=665.5649, train_loss=6.500636

Batch 230940, train_perplexity=668.73206, train_loss=6.5053835

Batch 230950, train_perplexity=661.9037, train_loss=6.49512

Batch 230960, train_perplexity=678.11487, train_loss=6.5193167

Batch 230970, train_perplexity=670.9052, train_loss=6.508628

Batch 230980, train_perplexity=659.5225, train_loss=6.491516

Batch 230990, train_perplexity=683.01373, train_loss=6.526515

Batch 231000, train_perplexity=630.5481, train_loss=6.4465895

Batch 231010, train_perplexity=647.0577, train_loss=6.4724355

Batch 231020, train_perplexity=663.90424, train_loss=6.498138

Batch 231030, train_perplexity=661.4828, train_loss=6.494484

Batch 231040, train_perplexity=720.4407, train_loss=6.579863

Batch 231050, train_perplexity=612.5258, train_loss=6.417591

Batch 231060, train_perplexity=601.8112, train_loss=6.399944

Batch 231070, train_perplexity=667.18, train_loss=6.50306

Batch 231080, train_perplexity=678.8904, train_loss=6.5204597

Batch 231090, train_perplexity=670.0245, train_loss=6.507314

Batch 231100, train_perplexity=719.94305, train_loss=6.579172

Batch 231110, train_perplexity=706.03357, train_loss=6.559663

Batch 231120, train_perplexity=635.67145, train_loss=6.454682

Batch 231130, train_perplexity=707.4507, train_loss=6.561668

Batch 231140, train_perplexity=641.82056, train_loss=6.4643087

Batch 231150, train_perplexity=684.72577, train_loss=6.5290184

Batch 231160, train_perplexity=668.9113, train_loss=6.5056515

Batch 231170, train_perplexity=737.7657, train_loss=6.6036263

Batch 231180, train_perplexity=670.8742, train_loss=6.5085816

Batch 231190, train_perplexity=580.93134, train_loss=6.3646326

Batch 231200, train_perplexity=630.7857, train_loss=6.446966

Batch 231210, train_perplexity=738.77814, train_loss=6.6049976

Batch 231220, train_perplexity=661.117, train_loss=6.493931

Batch 231230, train_perplexity=639.4964, train_loss=6.460681

Batch 231240, train_perplexity=623.58606, train_loss=6.435487

Batch 231250, train_perplexity=678.93604, train_loss=6.520527

Batch 231260, train_perplexity=653.4895, train_loss=6.4823265

Batch 231270, train_perplexity=666.79803, train_loss=6.502487

Batch 231280, train_perplexity=618.1285, train_loss=6.4266963

Batch 231290, train_perplexity=631.9255, train_loss=6.4487715

Batch 231300, train_perplexity=666.8438, train_loss=6.502556

Batch 231310, train_perplexity=675.8204, train_loss=6.5159273

Batch 231320, train_perplexity=726.30237, train_loss=6.5879664

Batch 231330, train_perplexity=636.0802, train_loss=6.4553246

Batch 231340, train_perplexity=754.64355, train_loss=6.6262455
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 231350, train_perplexity=663.51245, train_loss=6.4975476

Batch 231360, train_perplexity=686.48065, train_loss=6.531578

Batch 231370, train_perplexity=718.8968, train_loss=6.577718

Batch 231380, train_perplexity=645.07526, train_loss=6.469367

Batch 231390, train_perplexity=768.87396, train_loss=6.644927

Batch 231400, train_perplexity=644.95776, train_loss=6.469185

Batch 231410, train_perplexity=715.04315, train_loss=6.572343

Batch 231420, train_perplexity=572.831, train_loss=6.3505907

Batch 231430, train_perplexity=632.4337, train_loss=6.4495754

Batch 231440, train_perplexity=733.89276, train_loss=6.598363

Batch 231450, train_perplexity=597.1393, train_loss=6.3921504

Batch 231460, train_perplexity=713.48254, train_loss=6.570158

Batch 231470, train_perplexity=662.54, train_loss=6.496081

Batch 231480, train_perplexity=690.2903, train_loss=6.537112

Batch 231490, train_perplexity=706.4222, train_loss=6.560213

Batch 231500, train_perplexity=755.56024, train_loss=6.6274595

Batch 231510, train_perplexity=663.9701, train_loss=6.498237

Batch 231520, train_perplexity=760.72766, train_loss=6.6342754

Batch 231530, train_perplexity=706.6405, train_loss=6.560522

Batch 231540, train_perplexity=586.1321, train_loss=6.373545

Batch 231550, train_perplexity=670.42236, train_loss=6.507908

Batch 231560, train_perplexity=696.8853, train_loss=6.546621

Batch 231570, train_perplexity=610.50073, train_loss=6.4142795

Batch 231580, train_perplexity=651.4408, train_loss=6.4791865

Batch 231590, train_perplexity=620.0396, train_loss=6.4297833

Batch 231600, train_perplexity=615.413, train_loss=6.4222937

Batch 231610, train_perplexity=680.45703, train_loss=6.5227647

Batch 231620, train_perplexity=722.45557, train_loss=6.582656

Batch 231630, train_perplexity=654.54517, train_loss=6.4839406

Batch 231640, train_perplexity=639.16406, train_loss=6.460161

Batch 231650, train_perplexity=655.7248, train_loss=6.485741

Batch 231660, train_perplexity=664.8258, train_loss=6.499525

Batch 231670, train_perplexity=722.4656, train_loss=6.5826697

Batch 231680, train_perplexity=589.5172, train_loss=6.379304

Batch 231690, train_perplexity=640.34375, train_loss=6.462005

Batch 231700, train_perplexity=638.8953, train_loss=6.4597406

Batch 231710, train_perplexity=735.599, train_loss=6.600685

Batch 231720, train_perplexity=745.02277, train_loss=6.613415

Batch 231730, train_perplexity=703.28235, train_loss=6.5557585

Batch 231740, train_perplexity=614.52924, train_loss=6.4208565

Batch 231750, train_perplexity=602.4949, train_loss=6.401079

Batch 231760, train_perplexity=672.8294, train_loss=6.511492

Batch 231770, train_perplexity=657.73364, train_loss=6.4888

Batch 231780, train_perplexity=684.461, train_loss=6.5286317

Batch 231790, train_perplexity=653.0721, train_loss=6.4816875

Batch 231800, train_perplexity=648.8339, train_loss=6.475177

Batch 231810, train_perplexity=705.81616, train_loss=6.559355

Batch 231820, train_perplexity=598.1666, train_loss=6.3938694

Batch 231830, train_perplexity=678.0424, train_loss=6.51921

Batch 231840, train_perplexity=731.2945, train_loss=6.594816

Batch 231850, train_perplexity=722.9742, train_loss=6.5833735

Batch 231860, train_perplexity=668.2399, train_loss=6.5046473

Batch 231870, train_perplexity=636.4786, train_loss=6.4559507

Batch 231880, train_perplexity=708.22394, train_loss=6.5627604

Batch 231890, train_perplexity=586.3868, train_loss=6.3739796

Batch 231900, train_perplexity=674.1622, train_loss=6.5134706

Batch 231910, train_perplexity=695.1194, train_loss=6.5440836

Batch 231920, train_perplexity=667.88824, train_loss=6.504121

Batch 231930, train_perplexity=664.66064, train_loss=6.4992766

Batch 231940, train_perplexity=668.7805, train_loss=6.505456

Batch 231950, train_perplexity=709.7642, train_loss=6.564933

Batch 231960, train_perplexity=683.789, train_loss=6.5276494

Batch 231970, train_perplexity=700.0497, train_loss=6.5511513

Batch 231980, train_perplexity=661.7711, train_loss=6.49492

Batch 231990, train_perplexity=709.6972, train_loss=6.5648384

Batch 232000, train_perplexity=689.1195, train_loss=6.5354147

Batch 232010, train_perplexity=706.77124, train_loss=6.560707

Batch 232020, train_perplexity=692.08984, train_loss=6.539716

Batch 232030, train_perplexity=709.8468, train_loss=6.565049

Batch 232040, train_perplexity=631.7423, train_loss=6.4484816

Batch 232050, train_perplexity=748.3425, train_loss=6.617861

Batch 232060, train_perplexity=666.1415, train_loss=6.501502

Batch 232070, train_perplexity=711.082, train_loss=6.5667877

Batch 232080, train_perplexity=641.41547, train_loss=6.4636774

Batch 232090, train_perplexity=651.41003, train_loss=6.4791393

Batch 232100, train_perplexity=665.22064, train_loss=6.5001187

Batch 232110, train_perplexity=592.8029, train_loss=6.384862

Batch 232120, train_perplexity=635.9622, train_loss=6.455139

Batch 232130, train_perplexity=669.70087, train_loss=6.506831

Batch 232140, train_perplexity=673.3535, train_loss=6.5122705

Batch 232150, train_perplexity=688.6898, train_loss=6.534791

Batch 232160, train_perplexity=624.4913, train_loss=6.4369373

Batch 232170, train_perplexity=655.44, train_loss=6.4853067

Batch 232180, train_perplexity=645.8038, train_loss=6.4704957

Batch 232190, train_perplexity=656.9256, train_loss=6.487571

Batch 232200, train_perplexity=678.5806, train_loss=6.5200033

Batch 232210, train_perplexity=629.03906, train_loss=6.4441934

Batch 232220, train_perplexity=699.54376, train_loss=6.5504284

Batch 232230, train_perplexity=668.23193, train_loss=6.5046353

Batch 232240, train_perplexity=726.69006, train_loss=6.5885

Batch 232250, train_perplexity=605.2918, train_loss=6.4057107

Batch 232260, train_perplexity=623.82104, train_loss=6.4358635

Batch 232270, train_perplexity=720.52625, train_loss=6.579982

Batch 232280, train_perplexity=663.80457, train_loss=6.4979877

Batch 232290, train_perplexity=719.6139, train_loss=6.578715

Batch 232300, train_perplexity=668.9139, train_loss=6.5056553

Batch 232310, train_perplexity=731.94617, train_loss=6.595707

Batch 232320, train_perplexity=610.1704, train_loss=6.4137383

Batch 232330, train_perplexity=657.1562, train_loss=6.4879217

Batch 232340, train_perplexity=649.24554, train_loss=6.475811

Batch 232350, train_perplexity=675.90674, train_loss=6.516055

Batch 232360, train_perplexity=637.8002, train_loss=6.458025

Batch 232370, train_perplexity=742.1511, train_loss=6.609553

Batch 232380, train_perplexity=668.1112, train_loss=6.5044546

Batch 232390, train_perplexity=693.3725, train_loss=6.5415673

Batch 232400, train_perplexity=647.63336, train_loss=6.473325

Batch 232410, train_perplexity=668.187, train_loss=6.504568

Batch 232420, train_perplexity=623.31287, train_loss=6.4350486

Batch 232430, train_perplexity=624.53534, train_loss=6.437008

Batch 232440, train_perplexity=699.3407, train_loss=6.550138

Batch 232450, train_perplexity=676.651, train_loss=6.5171556

Batch 232460, train_perplexity=673.05365, train_loss=6.511825

Batch 232470, train_perplexity=694.74426, train_loss=6.543544

Batch 232480, train_perplexity=616.5862, train_loss=6.424198

Batch 232490, train_perplexity=660.61975, train_loss=6.4931784

Batch 232500, train_perplexity=628.52454, train_loss=6.443375

Batch 232510, train_perplexity=689.86053, train_loss=6.5364895

Batch 232520, train_perplexity=587.1002, train_loss=6.3751955

Batch 232530, train_perplexity=695.9774, train_loss=6.545317

Batch 232540, train_perplexity=646.3247, train_loss=6.471302

Batch 232550, train_perplexity=669.4381, train_loss=6.5064387

Batch 232560, train_perplexity=669.24695, train_loss=6.506153

Batch 232570, train_perplexity=655.39526, train_loss=6.4852386

Batch 232580, train_perplexity=651.34174, train_loss=6.4790344

Batch 232590, train_perplexity=665.2552, train_loss=6.5001707

Batch 232600, train_perplexity=558.38434, train_loss=6.3250475

Batch 232610, train_perplexity=636.2279, train_loss=6.455557

Batch 232620, train_perplexity=708.5894, train_loss=6.5632763

Batch 232630, train_perplexity=772.41455, train_loss=6.6495214

Batch 232640, train_perplexity=645.8364, train_loss=6.4705462

Batch 232650, train_perplexity=696.8032, train_loss=6.546503
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 232660, train_perplexity=624.80225, train_loss=6.437435

Batch 232670, train_perplexity=658.3948, train_loss=6.4898047

Batch 232680, train_perplexity=620.2667, train_loss=6.4301496

Batch 232690, train_perplexity=623.6509, train_loss=6.4355907

Batch 232700, train_perplexity=593.4279, train_loss=6.3859158

Batch 232710, train_perplexity=682.52997, train_loss=6.5258064

Batch 232720, train_perplexity=655.2487, train_loss=6.485015

Batch 232730, train_perplexity=629.44025, train_loss=6.444831

Batch 232740, train_perplexity=686.9067, train_loss=6.5321984

Batch 232750, train_perplexity=679.0059, train_loss=6.52063

Batch 232760, train_perplexity=667.69653, train_loss=6.503834

Batch 232770, train_perplexity=671.33344, train_loss=6.509266

Batch 232780, train_perplexity=723.40076, train_loss=6.5839634

Batch 232790, train_perplexity=685.1464, train_loss=6.5296326

Batch 232800, train_perplexity=687.0859, train_loss=6.5324593

Batch 232810, train_perplexity=666.84827, train_loss=6.5025625

Batch 232820, train_perplexity=616.71765, train_loss=6.4244113

Batch 232830, train_perplexity=748.5431, train_loss=6.618129

Batch 232840, train_perplexity=629.8636, train_loss=6.445503

Batch 232850, train_perplexity=673.05884, train_loss=6.5118327

Batch 232860, train_perplexity=652.83673, train_loss=6.481327

Batch 232870, train_perplexity=660.5278, train_loss=6.493039

Batch 232880, train_perplexity=628.5992, train_loss=6.443494

Batch 232890, train_perplexity=645.6526, train_loss=6.4702616

Batch 232900, train_perplexity=672.2191, train_loss=6.5105844

Batch 232910, train_perplexity=669.13654, train_loss=6.505988

Batch 232920, train_perplexity=687.71814, train_loss=6.533379

Batch 232930, train_perplexity=706.1602, train_loss=6.559842

Batch 232940, train_perplexity=642.074, train_loss=6.4647036

Batch 232950, train_perplexity=708.6776, train_loss=6.5634007

Batch 232960, train_perplexity=749.3795, train_loss=6.6192455

Batch 232970, train_perplexity=706.7214, train_loss=6.5606365

Batch 232980, train_perplexity=641.1372, train_loss=6.4632435

Batch 232990, train_perplexity=667.90735, train_loss=6.5041494

Batch 233000, train_perplexity=653.9262, train_loss=6.4829946

Batch 233010, train_perplexity=655.38745, train_loss=6.4852266

Batch 233020, train_perplexity=677.82294, train_loss=6.518886

Batch 233030, train_perplexity=676.6465, train_loss=6.517149

Batch 233040, train_perplexity=668.0672, train_loss=6.504389

Batch 233050, train_perplexity=810.26935, train_loss=6.6973667

Batch 233060, train_perplexity=689.0222, train_loss=6.5352736

Batch 233070, train_perplexity=656.1301, train_loss=6.486359

Batch 233080, train_perplexity=631.09406, train_loss=6.447455

Batch 233090, train_perplexity=605.62067, train_loss=6.406254

Batch 233100, train_perplexity=651.8488, train_loss=6.4798126

Batch 233110, train_perplexity=660.06616, train_loss=6.49234

Batch 233120, train_perplexity=726.408, train_loss=6.588112

Batch 233130, train_perplexity=647.43823, train_loss=6.4730234

Batch 233140, train_perplexity=702.4498, train_loss=6.554574

Batch 233150, train_perplexity=696.87964, train_loss=6.5466127

Batch 233160, train_perplexity=667.3585, train_loss=6.5033274

Batch 233170, train_perplexity=631.1338, train_loss=6.447518

Batch 233180, train_perplexity=618.10284, train_loss=6.426655

Batch 233190, train_perplexity=660.2538, train_loss=6.4926243

Batch 233200, train_perplexity=665.2774, train_loss=6.500204

Batch 233210, train_perplexity=633.4611, train_loss=6.4511986

Batch 233220, train_perplexity=689.2207, train_loss=6.5355616

Batch 233230, train_perplexity=654.53174, train_loss=6.48392

Batch 233240, train_perplexity=704.0397, train_loss=6.5568347

Batch 233250, train_perplexity=609.26764, train_loss=6.4122577

Batch 233260, train_perplexity=624.7, train_loss=6.4372716

Batch 233270, train_perplexity=644.03217, train_loss=6.4677486

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Batch 233280, train_perplexity=677.1636, train_loss=6.517913

Batch 233290, train_perplexity=626.99554, train_loss=6.4409394

Batch 233300, train_perplexity=641.6758, train_loss=6.464083

Batch 233310, train_perplexity=653.42255, train_loss=6.482224

Batch 233320, train_perplexity=627.28534, train_loss=6.4414015

Batch 233330, train_perplexity=677.2714, train_loss=6.518072

Batch 233340, train_perplexity=693.21045, train_loss=6.5413337

Batch 233350, train_perplexity=660.57623, train_loss=6.4931126

Batch 233360, train_perplexity=700.54553, train_loss=6.5518594

Batch 233370, train_perplexity=715.5558, train_loss=6.5730596

Batch 233380, train_perplexity=722.5052, train_loss=6.5827246

Batch 233390, train_perplexity=648.98804, train_loss=6.4754143

Batch 233400, train_perplexity=705.5974, train_loss=6.559045

Batch 233410, train_perplexity=652.9494, train_loss=6.4814997

Batch 233420, train_perplexity=659.0918, train_loss=6.490863

Batch 233430, train_perplexity=713.909, train_loss=6.5707555

Batch 233440, train_perplexity=721.40564, train_loss=6.5812016

Batch 233450, train_perplexity=669.23737, train_loss=6.506139

Batch 233460, train_perplexity=791.79846, train_loss=6.674307

Batch 233470, train_perplexity=592.0766, train_loss=6.383636

Batch 233480, train_perplexity=592.59656, train_loss=6.384514

Batch 233490, train_perplexity=581.3304, train_loss=6.3653193

Batch 233500, train_perplexity=692.8896, train_loss=6.5408707

Batch 233510, train_perplexity=648.21606, train_loss=6.474224

Batch 233520, train_perplexity=656.9369, train_loss=6.487588

Batch 233530, train_perplexity=710.72675, train_loss=6.566288

Batch 233540, train_perplexity=680.6949, train_loss=6.523114

Batch 233550, train_perplexity=679.83405, train_loss=6.5218487

Batch 233560, train_perplexity=624.1691, train_loss=6.4364214

Batch 233570, train_perplexity=678.67834, train_loss=6.5201473

Batch 233580, train_perplexity=711.27936, train_loss=6.5670652

Batch 233590, train_perplexity=712.06836, train_loss=6.568174

Batch 233600, train_perplexity=679.88654, train_loss=6.521926

Batch 233610, train_perplexity=630.06665, train_loss=6.4458256

Batch 233620, train_perplexity=682.5033, train_loss=6.5257673

Batch 233630, train_perplexity=692.6128, train_loss=6.540471

Batch 233640, train_perplexity=725.46375, train_loss=6.586811

Batch 233650, train_perplexity=610.9297, train_loss=6.414982

Batch 233660, train_perplexity=621.1283, train_loss=6.4315376

Batch 233670, train_perplexity=672.41626, train_loss=6.5108776

Batch 233680, train_perplexity=628.3223, train_loss=6.4430532

Batch 233690, train_perplexity=686.8336, train_loss=6.532092

Batch 233700, train_perplexity=670.05225, train_loss=6.5073557

Batch 233710, train_perplexity=690.56354, train_loss=6.537508

Batch 233720, train_perplexity=700.261, train_loss=6.551453

Batch 233730, train_perplexity=656.7984, train_loss=6.487377

Batch 233740, train_perplexity=706.13934, train_loss=6.5598125

Batch 233750, train_perplexity=616.99854, train_loss=6.4248667

Batch 233760, train_perplexity=741.5568, train_loss=6.608752

Batch 233770, train_perplexity=708.05206, train_loss=6.5625176

Batch 233780, train_perplexity=645.60547, train_loss=6.4701886

Batch 233790, train_perplexity=597.301, train_loss=6.3924212

Batch 233800, train_perplexity=696.137, train_loss=6.5455465

Batch 233810, train_perplexity=708.9943, train_loss=6.5638475

Batch 233820, train_perplexity=672.9109, train_loss=6.511613

Batch 233830, train_perplexity=655.9565, train_loss=6.4860945

Batch 233840, train_perplexity=747.1766, train_loss=6.6163015

Batch 233850, train_perplexity=671.12537, train_loss=6.508956

Batch 233860, train_perplexity=614.3956, train_loss=6.420639

Batch 233870, train_perplexity=693.16455, train_loss=6.5412674

Batch 233880, train_perplexity=697.3134, train_loss=6.547235

Batch 233890, train_perplexity=611.3467, train_loss=6.415664
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 233900, train_perplexity=634.3425, train_loss=6.452589

Batch 233910, train_perplexity=684.79236, train_loss=6.5291157

Batch 233920, train_perplexity=669.5317, train_loss=6.5065784

Batch 233930, train_perplexity=672.6709, train_loss=6.511256

Batch 233940, train_perplexity=737.09686, train_loss=6.6027193

Batch 233950, train_perplexity=656.3407, train_loss=6.48668

Batch 233960, train_perplexity=668.029, train_loss=6.5043316

Batch 233970, train_perplexity=657.2226, train_loss=6.488023

Batch 233980, train_perplexity=651.87646, train_loss=6.479855

Batch 233990, train_perplexity=775.2406, train_loss=6.6531734

Batch 234000, train_perplexity=744.67365, train_loss=6.612946

Batch 234010, train_perplexity=619.3514, train_loss=6.428673

Batch 234020, train_perplexity=670.3968, train_loss=6.5078697

Batch 234030, train_perplexity=553.9378, train_loss=6.3170524

Batch 234040, train_perplexity=724.5248, train_loss=6.585516

Batch 234050, train_perplexity=716.5887, train_loss=6.574502

Batch 234060, train_perplexity=650.0677, train_loss=6.4770765

Batch 234070, train_perplexity=610.5077, train_loss=6.414291

Batch 234080, train_perplexity=728.034, train_loss=6.590348

Batch 234090, train_perplexity=662.3817, train_loss=6.495842

Batch 234100, train_perplexity=662.94415, train_loss=6.4966908

Batch 234110, train_perplexity=652.6058, train_loss=6.4809732

Batch 234120, train_perplexity=686.6735, train_loss=6.531859

Batch 234130, train_perplexity=688.7749, train_loss=6.5349145

Batch 234140, train_perplexity=643.2262, train_loss=6.4664965

Batch 234150, train_perplexity=689.5238, train_loss=6.536001

Batch 234160, train_perplexity=768.4664, train_loss=6.644397

Batch 234170, train_perplexity=632.55554, train_loss=6.449768

Batch 234180, train_perplexity=737.66504, train_loss=6.60349

Batch 234190, train_perplexity=732.6061, train_loss=6.596608

Batch 234200, train_perplexity=743.635, train_loss=6.6115503

Batch 234210, train_perplexity=694.66016, train_loss=6.5434227

Batch 234220, train_perplexity=613.79443, train_loss=6.41966

Batch 234230, train_perplexity=625.90314, train_loss=6.4391956

Batch 234240, train_perplexity=697.71356, train_loss=6.5478086

Batch 234250, train_perplexity=617.36615, train_loss=6.4254622

Batch 234260, train_perplexity=672.3522, train_loss=6.5107822

Batch 234270, train_perplexity=667.23315, train_loss=6.5031395

Batch 234280, train_perplexity=660.7539, train_loss=6.4933815

Batch 234290, train_perplexity=612.256, train_loss=6.4171505

Batch 234300, train_perplexity=672.8467, train_loss=6.5115175

Batch 234310, train_perplexity=638.6678, train_loss=6.4593844

Batch 234320, train_perplexity=729.44794, train_loss=6.592288

Batch 234330, train_perplexity=623.0285, train_loss=6.4345922

Batch 234340, train_perplexity=604.6673, train_loss=6.4046783

Batch 234350, train_perplexity=713.6003, train_loss=6.570323

Batch 234360, train_perplexity=646.75696, train_loss=6.4719706

Batch 234370, train_perplexity=636.0805, train_loss=6.455325

Batch 234380, train_perplexity=661.33453, train_loss=6.49426

Batch 234390, train_perplexity=704.05206, train_loss=6.5568523

Batch 234400, train_perplexity=637.6177, train_loss=6.457739

Batch 234410, train_perplexity=678.6337, train_loss=6.5200815

Batch 234420, train_perplexity=673.5343, train_loss=6.512539

Batch 234430, train_perplexity=662.5185, train_loss=6.4960485

Batch 234440, train_perplexity=717.5272, train_loss=6.575811

Batch 234450, train_perplexity=713.1594, train_loss=6.569705

Batch 234460, train_perplexity=706.59906, train_loss=6.5604634

Batch 234470, train_perplexity=664.07367, train_loss=6.498393

Batch 234480, train_perplexity=635.0035, train_loss=6.4536304

Batch 234490, train_perplexity=711.18506, train_loss=6.5669327

Batch 234500, train_perplexity=772.84265, train_loss=6.6500754

Batch 234510, train_perplexity=719.28973, train_loss=6.578264

Batch 234520, train_perplexity=743.9606, train_loss=6.611988

Batch 234530, train_perplexity=679.39685, train_loss=6.5212054

Batch 234540, train_perplexity=712.68384, train_loss=6.569038

Batch 234550, train_perplexity=625.2964, train_loss=6.4382257

Batch 234560, train_perplexity=677.26984, train_loss=6.5180697

Batch 234570, train_perplexity=712.6648, train_loss=6.569011

Batch 234580, train_perplexity=660.3785, train_loss=6.492813

Batch 234590, train_perplexity=626.2441, train_loss=6.43974

Batch 234600, train_perplexity=741.98584, train_loss=6.60933

Batch 234610, train_perplexity=683.3304, train_loss=6.5269785

Batch 234620, train_perplexity=642.6327, train_loss=6.4655733

Batch 234630, train_perplexity=603.982, train_loss=6.4035444

Batch 234640, train_perplexity=657.1722, train_loss=6.487946

Batch 234650, train_perplexity=579.70715, train_loss=6.362523

Batch 234660, train_perplexity=662.1644, train_loss=6.495514

Batch 234670, train_perplexity=711.7469, train_loss=6.5677223

Batch 234680, train_perplexity=667.1307, train_loss=6.502986

Batch 234690, train_perplexity=606.11237, train_loss=6.4070654

Batch 234700, train_perplexity=659.8094, train_loss=6.491951

Batch 234710, train_perplexity=681.1716, train_loss=6.523814

Batch 234720, train_perplexity=628.7803, train_loss=6.443782

Batch 234730, train_perplexity=716.6597, train_loss=6.574601

Batch 234740, train_perplexity=692.3572, train_loss=6.540102

Batch 234750, train_perplexity=752.3771, train_loss=6.6232376

Batch 234760, train_perplexity=641.0051, train_loss=6.4630375

Batch 234770, train_perplexity=650.4342, train_loss=6.47764

Batch 234780, train_perplexity=596.8979, train_loss=6.391746

Batch 234790, train_perplexity=669.8127, train_loss=6.506998

Batch 234800, train_perplexity=646.4301, train_loss=6.471465

Batch 234810, train_perplexity=666.7977, train_loss=6.5024867

Batch 234820, train_perplexity=632.377, train_loss=6.449486

Batch 234830, train_perplexity=654.7774, train_loss=6.4842954

Batch 234840, train_perplexity=666.7853, train_loss=6.502468

Batch 234850, train_perplexity=611.68024, train_loss=6.4162097

Batch 234860, train_perplexity=705.727, train_loss=6.5592284

Batch 234870, train_perplexity=635.68146, train_loss=6.4546976

Batch 234880, train_perplexity=672.8576, train_loss=6.5115337

Batch 234890, train_perplexity=667.8016, train_loss=6.503991

Batch 234900, train_perplexity=689.0939, train_loss=6.5353775

Batch 234910, train_perplexity=713.1023, train_loss=6.569625

Batch 234920, train_perplexity=565.21375, train_loss=6.337204

Batch 234930, train_perplexity=590.28174, train_loss=6.3806

Batch 234940, train_perplexity=694.2399, train_loss=6.5428176

Batch 234950, train_perplexity=679.6791, train_loss=6.5216208

Batch 234960, train_perplexity=630.7478, train_loss=6.446906

Batch 234970, train_perplexity=648.2328, train_loss=6.47425

Batch 234980, train_perplexity=694.58856, train_loss=6.5433197

Batch 234990, train_perplexity=672.1156, train_loss=6.5104303

Batch 235000, train_perplexity=640.38495, train_loss=6.4620695

Batch 235010, train_perplexity=644.85693, train_loss=6.4690285

Batch 235020, train_perplexity=664.7488, train_loss=6.499409

Batch 235030, train_perplexity=647.51917, train_loss=6.4731483

Batch 235040, train_perplexity=633.2491, train_loss=6.450864

Batch 235050, train_perplexity=712.99554, train_loss=6.569475

Batch 235060, train_perplexity=639.1062, train_loss=6.4600706

Batch 235070, train_perplexity=722.8122, train_loss=6.5831494

Batch 235080, train_perplexity=635.0262, train_loss=6.453666

Batch 235090, train_perplexity=653.0369, train_loss=6.4816337

Batch 235100, train_perplexity=644.95776, train_loss=6.469185

Batch 235110, train_perplexity=718.07794, train_loss=6.576578

Batch 235120, train_perplexity=665.83344, train_loss=6.5010395

Batch 235130, train_perplexity=668.14813, train_loss=6.50451

Batch 235140, train_perplexity=640.85785, train_loss=6.4628077

Batch 235150, train_perplexity=626.9519, train_loss=6.44087

Batch 235160, train_perplexity=673.8578, train_loss=6.513019

Batch 235170, train_perplexity=697.4022, train_loss=6.5473623

Batch 235180, train_perplexity=720.81696, train_loss=6.580385

Batch 235190, train_perplexity=674.93024, train_loss=6.5146093

Batch 235200, train_perplexity=688.4846, train_loss=6.534493
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 235210, train_perplexity=660.839, train_loss=6.4935102

Batch 235220, train_perplexity=769.1083, train_loss=6.6452317

Batch 235230, train_perplexity=703.8044, train_loss=6.5565004

Batch 235240, train_perplexity=669.0855, train_loss=6.505912

Batch 235250, train_perplexity=709.771, train_loss=6.5649424

Batch 235260, train_perplexity=720.305, train_loss=6.5796747

Batch 235270, train_perplexity=640.45276, train_loss=6.4621754

Batch 235280, train_perplexity=662.20044, train_loss=6.4955683

Batch 235290, train_perplexity=681.46295, train_loss=6.524242

Batch 235300, train_perplexity=712.1064, train_loss=6.5682273

Batch 235310, train_perplexity=777.1934, train_loss=6.6556892

Batch 235320, train_perplexity=764.7823, train_loss=6.639591

Batch 235330, train_perplexity=604.65894, train_loss=6.4046645

Batch 235340, train_perplexity=638.7674, train_loss=6.4595404

Batch 235350, train_perplexity=600.0942, train_loss=6.3970866

Batch 235360, train_perplexity=665.1727, train_loss=6.5000467

Batch 235370, train_perplexity=702.43646, train_loss=6.554555

Batch 235380, train_perplexity=687.33295, train_loss=6.532819

Batch 235390, train_perplexity=703.12646, train_loss=6.5555367

Batch 235400, train_perplexity=715.9015, train_loss=6.5735426

Batch 235410, train_perplexity=693.0733, train_loss=6.541136

Batch 235420, train_perplexity=705.23816, train_loss=6.5585356

Batch 235430, train_perplexity=700.78375, train_loss=6.5521994

Batch 235440, train_perplexity=665.6956, train_loss=6.5008326

Batch 235450, train_perplexity=623.3494, train_loss=6.435107

Batch 235460, train_perplexity=682.55664, train_loss=6.5258455

Batch 235470, train_perplexity=615.8842, train_loss=6.423059

Batch 235480, train_perplexity=807.94366, train_loss=6.6944923

Batch 235490, train_perplexity=689.62573, train_loss=6.536149

Batch 235500, train_perplexity=659.7018, train_loss=6.491788

Batch 235510, train_perplexity=550.30304, train_loss=6.310469

Batch 235520, train_perplexity=739.46326, train_loss=6.6059246

Batch 235530, train_perplexity=693.73755, train_loss=6.5420938

Batch 235540, train_perplexity=690.1132, train_loss=6.5368557

Batch 235550, train_perplexity=621.4358, train_loss=6.4320326

Batch 235560, train_perplexity=573.5933, train_loss=6.3519206

Batch 235570, train_perplexity=735.72736, train_loss=6.6008596

Batch 235580, train_perplexity=600.05756, train_loss=6.3970256

Batch 235590, train_perplexity=651.64246, train_loss=6.479496

Batch 235600, train_perplexity=671.0908, train_loss=6.5089045

Batch 235610, train_perplexity=638.40564, train_loss=6.458974

Batch 235620, train_perplexity=714.3588, train_loss=6.5713854

Batch 235630, train_perplexity=664.14014, train_loss=6.498493

Batch 235640, train_perplexity=641.3647, train_loss=6.4635983

Batch 235650, train_perplexity=688.75385, train_loss=6.534884

Batch 235660, train_perplexity=692.14526, train_loss=6.539796

Batch 235670, train_perplexity=628.28876, train_loss=6.443

Batch 235680, train_perplexity=652.5731, train_loss=6.480923

Batch 235690, train_perplexity=702.9585, train_loss=6.555298

Batch 235700, train_perplexity=679.6013, train_loss=6.5215063

Batch 235710, train_perplexity=676.92596, train_loss=6.517562

Batch 235720, train_perplexity=634.98804, train_loss=6.453606

Batch 235730, train_perplexity=686.58246, train_loss=6.5317264

Batch 235740, train_perplexity=640.11206, train_loss=6.461643

Batch 235750, train_perplexity=711.2665, train_loss=6.567047

Batch 235760, train_perplexity=727.5006, train_loss=6.589615

Batch 235770, train_perplexity=636.6406, train_loss=6.4562054

Batch 235780, train_perplexity=668.96295, train_loss=6.5057287

Batch 235790, train_perplexity=675.75525, train_loss=6.515831

Batch 235800, train_perplexity=653.1288, train_loss=6.4817743

Batch 235810, train_perplexity=570.01685, train_loss=6.345666

Batch 235820, train_perplexity=638.7552, train_loss=6.4595213

Batch 235830, train_perplexity=612.6552, train_loss=6.4178023

Batch 235840, train_perplexity=643.0002, train_loss=6.466145

Batch 235850, train_perplexity=614.2125, train_loss=6.420341

Batch 235860, train_perplexity=620.7617, train_loss=6.4309473

Batch 235870, train_perplexity=663.1417, train_loss=6.496989

Batch 235880, train_perplexity=636.2822, train_loss=6.455642

Batch 235890, train_perplexity=657.39813, train_loss=6.48829

Batch 235900, train_perplexity=758.33514, train_loss=6.6311255

Batch 235910, train_perplexity=648.13293, train_loss=6.474096

Batch 235920, train_perplexity=716.17975, train_loss=6.573931

Batch 235930, train_perplexity=684.4914, train_loss=6.528676

Batch 235940, train_perplexity=707.37683, train_loss=6.5615635

Batch 235950, train_perplexity=667.3935, train_loss=6.50338

Batch 235960, train_perplexity=737.5061, train_loss=6.6032743

Batch 235970, train_perplexity=632.48285, train_loss=6.449653

Batch 235980, train_perplexity=623.94806, train_loss=6.436067

Batch 235990, train_perplexity=714.0959, train_loss=6.5710173

Batch 236000, train_perplexity=706.64825, train_loss=6.560533

Batch 236010, train_perplexity=639.4549, train_loss=6.460616

Batch 236020, train_perplexity=668.26605, train_loss=6.5046864

Batch 236030, train_perplexity=697.12695, train_loss=6.5469675

Batch 236040, train_perplexity=658.0354, train_loss=6.489259

Batch 236050, train_perplexity=583.847, train_loss=6.369639

Batch 236060, train_perplexity=648.10236, train_loss=6.4740486

Batch 236070, train_perplexity=658.6554, train_loss=6.4902005

Batch 236080, train_perplexity=638.2577, train_loss=6.458742

Batch 236090, train_perplexity=637.1776, train_loss=6.4570484

Batch 236100, train_perplexity=654.9851, train_loss=6.4846125

Batch 236110, train_perplexity=658.4642, train_loss=6.48991

Batch 236120, train_perplexity=660.23114, train_loss=6.49259

Batch 236130, train_perplexity=660.13513, train_loss=6.4924445

Batch 236140, train_perplexity=710.1904, train_loss=6.565533

Batch 236150, train_perplexity=574.9419, train_loss=6.354269

Batch 236160, train_perplexity=662.69006, train_loss=6.4963074

Batch 236170, train_perplexity=587.76184, train_loss=6.376322

Batch 236180, train_perplexity=624.04175, train_loss=6.4362173

Batch 236190, train_perplexity=669.04175, train_loss=6.5058465

Batch 236200, train_perplexity=613.1102, train_loss=6.418545

Batch 236210, train_perplexity=638.32556, train_loss=6.4588485

Batch 236220, train_perplexity=690.64685, train_loss=6.5376287

Batch 236230, train_perplexity=695.5345, train_loss=6.5446806

Batch 236240, train_perplexity=665.65753, train_loss=6.5007753

Batch 236250, train_perplexity=632.1527, train_loss=6.449131

Batch 236260, train_perplexity=651.46783, train_loss=6.479228

Batch 236270, train_perplexity=700.4978, train_loss=6.551791

Batch 236280, train_perplexity=633.24664, train_loss=6.45086

Batch 236290, train_perplexity=617.02295, train_loss=6.4249063

Batch 236300, train_perplexity=651.01135, train_loss=6.478527

Batch 236310, train_perplexity=636.7378, train_loss=6.456358

Batch 236320, train_perplexity=673.2915, train_loss=6.5121784

Batch 236330, train_perplexity=678.3936, train_loss=6.5197277

Batch 236340, train_perplexity=645.5818, train_loss=6.470152

Batch 236350, train_perplexity=610.6128, train_loss=6.414463

Batch 236360, train_perplexity=572.0907, train_loss=6.3492975

Batch 236370, train_perplexity=651.7866, train_loss=6.4797173

Batch 236380, train_perplexity=728.5987, train_loss=6.591123

Batch 236390, train_perplexity=671.77466, train_loss=6.509923

Batch 236400, train_perplexity=679.30585, train_loss=6.5210714

Batch 236410, train_perplexity=678.27167, train_loss=6.519548

Batch 236420, train_perplexity=686.7976, train_loss=6.5320396

Batch 236430, train_perplexity=615.76294, train_loss=6.422862

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Batch 236440, train_perplexity=702.2228, train_loss=6.5542507
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 236450, train_perplexity=615.86865, train_loss=6.4230337

Batch 236460, train_perplexity=616.4198, train_loss=6.4239283

Batch 236470, train_perplexity=615.4242, train_loss=6.422312

Batch 236480, train_perplexity=697.82404, train_loss=6.547967

Batch 236490, train_perplexity=636.49255, train_loss=6.4559727

Batch 236500, train_perplexity=645.64764, train_loss=6.470254

Batch 236510, train_perplexity=624.57465, train_loss=6.437071

Batch 236520, train_perplexity=713.354, train_loss=6.5699778

Batch 236530, train_perplexity=662.70013, train_loss=6.4963226

Batch 236540, train_perplexity=597.3919, train_loss=6.3925734

Batch 236550, train_perplexity=583.8812, train_loss=6.3696976

Batch 236560, train_perplexity=674.39655, train_loss=6.5138183

Batch 236570, train_perplexity=656.07227, train_loss=6.486271

Batch 236580, train_perplexity=609.85394, train_loss=6.4132195

Batch 236590, train_perplexity=647.4827, train_loss=6.473092

Batch 236600, train_perplexity=700.5756, train_loss=6.5519023

Batch 236610, train_perplexity=656.8896, train_loss=6.487516

Batch 236620, train_perplexity=704.8667, train_loss=6.5580087

Batch 236630, train_perplexity=687.32214, train_loss=6.532803

Batch 236640, train_perplexity=726.14136, train_loss=6.5877447

Batch 236650, train_perplexity=630.0828, train_loss=6.4458513

Batch 236660, train_perplexity=644.6061, train_loss=6.4686394

Batch 236670, train_perplexity=673.76074, train_loss=6.512875

Batch 236680, train_perplexity=603.3177, train_loss=6.402444

Batch 236690, train_perplexity=623.2323, train_loss=6.4349194

Batch 236700, train_perplexity=676.9834, train_loss=6.517647

Batch 236710, train_perplexity=632.69885, train_loss=6.4499946

Batch 236720, train_perplexity=693.27295, train_loss=6.541424

Batch 236730, train_perplexity=660.07465, train_loss=6.492353

Batch 236740, train_perplexity=686.57166, train_loss=6.5317106

Batch 236750, train_perplexity=683.2568, train_loss=6.5268707

Batch 236760, train_perplexity=627.4935, train_loss=6.4417334

Batch 236770, train_perplexity=671.46277, train_loss=6.5094585

Batch 236780, train_perplexity=632.0852, train_loss=6.449024

Batch 236790, train_perplexity=765.6456, train_loss=6.6407194

Batch 236800, train_perplexity=654.3676, train_loss=6.4836693

Batch 236810, train_perplexity=701.31494, train_loss=6.552957

Batch 236820, train_perplexity=611.1074, train_loss=6.4152727

Batch 236830, train_perplexity=732.1601, train_loss=6.5959992

Batch 236840, train_perplexity=708.54755, train_loss=6.563217

Batch 236850, train_perplexity=695.7136, train_loss=6.544938

Batch 236860, train_perplexity=601.7363, train_loss=6.3998194

Batch 236870, train_perplexity=670.5707, train_loss=6.508129

Batch 236880, train_perplexity=705.7071, train_loss=6.5592003

Batch 236890, train_perplexity=654.77277, train_loss=6.484288

Batch 236900, train_perplexity=669.0555, train_loss=6.505867

Batch 236910, train_perplexity=698.4072, train_loss=6.5488024

Batch 236920, train_perplexity=666.1907, train_loss=6.501576

Batch 236930, train_perplexity=680.1057, train_loss=6.5222483

Batch 236940, train_perplexity=619.6657, train_loss=6.42918

Batch 236950, train_perplexity=718.953, train_loss=6.577796

Batch 236960, train_perplexity=655.4678, train_loss=6.485349

Batch 236970, train_perplexity=677.769, train_loss=6.5188065

Batch 236980, train_perplexity=669.7319, train_loss=6.5068774

Batch 236990, train_perplexity=654.80273, train_loss=6.484334

Batch 237000, train_perplexity=662.65906, train_loss=6.4962606

Batch 237010, train_perplexity=594.3069, train_loss=6.387396

Batch 237020, train_perplexity=720.39703, train_loss=6.5798025

Batch 237030, train_perplexity=642.4231, train_loss=6.465247

Batch 237040, train_perplexity=638.9928, train_loss=6.459893

Batch 237050, train_perplexity=596.2436, train_loss=6.3906493

Batch 237060, train_perplexity=748.40674, train_loss=6.6179466

Batch 237070, train_perplexity=671.9682, train_loss=6.510211

Batch 237080, train_perplexity=713.8263, train_loss=6.5706396

Batch 237090, train_perplexity=563.51996, train_loss=6.334203

Batch 237100, train_perplexity=697.66034, train_loss=6.5477324

Batch 237110, train_perplexity=641.87744, train_loss=6.4643974

Batch 237120, train_perplexity=654.9882, train_loss=6.484617

Batch 237130, train_perplexity=682.85974, train_loss=6.5262895

Batch 237140, train_perplexity=604.3476, train_loss=6.4041495

Batch 237150, train_perplexity=623.54565, train_loss=6.435422

Batch 237160, train_perplexity=651.54926, train_loss=6.479353

Batch 237170, train_perplexity=666.22723, train_loss=6.501631

Batch 237180, train_perplexity=672.6446, train_loss=6.511217

Batch 237190, train_perplexity=680.38275, train_loss=6.5226555

Batch 237200, train_perplexity=735.14734, train_loss=6.600071

Batch 237210, train_perplexity=694.0681, train_loss=6.54257

Batch 237220, train_perplexity=643.12805, train_loss=6.466344

Batch 237230, train_perplexity=629.84375, train_loss=6.445472

Batch 237240, train_perplexity=654.30084, train_loss=6.483567

Batch 237250, train_perplexity=736.82697, train_loss=6.602353

Batch 237260, train_perplexity=842.84534, train_loss=6.7367835

Batch 237270, train_perplexity=636.32166, train_loss=6.455704

Batch 237280, train_perplexity=655.9759, train_loss=6.486124

Batch 237290, train_perplexity=631.3186, train_loss=6.4478106

Batch 237300, train_perplexity=711.05383, train_loss=6.566748

Batch 237310, train_perplexity=743.9904, train_loss=6.612028

Batch 237320, train_perplexity=716.1955, train_loss=6.573953

Batch 237330, train_perplexity=690.5939, train_loss=6.537552

Batch 237340, train_perplexity=648.46497, train_loss=6.474608

Batch 237350, train_perplexity=676.9208, train_loss=6.5175543

Batch 237360, train_perplexity=635.13214, train_loss=6.453833

Batch 237370, train_perplexity=667.3152, train_loss=6.5032625

Batch 237380, train_perplexity=617.241, train_loss=6.4252596

Batch 237390, train_perplexity=757.98627, train_loss=6.6306653

Batch 237400, train_perplexity=654.61664, train_loss=6.48405

Batch 237410, train_perplexity=678.0686, train_loss=6.5192485

Batch 237420, train_perplexity=715.5022, train_loss=6.5729847

Batch 237430, train_perplexity=628.8192, train_loss=6.443844

Batch 237440, train_perplexity=653.63727, train_loss=6.4825525

Batch 237450, train_perplexity=647.6408, train_loss=6.473336

Batch 237460, train_perplexity=668.9936, train_loss=6.5057745

Batch 237470, train_perplexity=629.92096, train_loss=6.4455943

Batch 237480, train_perplexity=698.34467, train_loss=6.5487127

Batch 237490, train_perplexity=637.04816, train_loss=6.4568453

Batch 237500, train_perplexity=690.92523, train_loss=6.5380316

Batch 237510, train_perplexity=606.0869, train_loss=6.4070234

Batch 237520, train_perplexity=658.1233, train_loss=6.4893923

Batch 237530, train_perplexity=603.7413, train_loss=6.403146

Batch 237540, train_perplexity=664.3872, train_loss=6.498865

Batch 237550, train_perplexity=733.3932, train_loss=6.597682

Batch 237560, train_perplexity=660.7813, train_loss=6.493423

Batch 237570, train_perplexity=621.0791, train_loss=6.4314585

Batch 237580, train_perplexity=649.3378, train_loss=6.475953

Batch 237590, train_perplexity=654.72406, train_loss=6.484214

Batch 237600, train_perplexity=616.183, train_loss=6.423544

Batch 237610, train_perplexity=739.0163, train_loss=6.60532

Batch 237620, train_perplexity=717.9047, train_loss=6.576337

Batch 237630, train_perplexity=663.0055, train_loss=6.4967833

Batch 237640, train_perplexity=685.10266, train_loss=6.5295687

Batch 237650, train_perplexity=729.5196, train_loss=6.5923862

Batch 237660, train_perplexity=673.74115, train_loss=6.512846

Batch 237670, train_perplexity=613.2567, train_loss=6.4187837

Batch 237680, train_perplexity=630.52405, train_loss=6.4465513

Batch 237690, train_perplexity=640.30133, train_loss=6.461939

Batch 237700, train_perplexity=614.156, train_loss=6.420249

Batch 237710, train_perplexity=741.0983, train_loss=6.6081333

Batch 237720, train_perplexity=775.2994, train_loss=6.6532493

Batch 237730, train_perplexity=657.076, train_loss=6.4877996

Batch 237740, train_perplexity=660.54285, train_loss=6.493062

Batch 237750, train_perplexity=674.4354, train_loss=6.513876
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 237760, train_perplexity=657.6308, train_loss=6.4886436

Batch 237770, train_perplexity=653.3259, train_loss=6.482076

Batch 237780, train_perplexity=702.51685, train_loss=6.5546694

Batch 237790, train_perplexity=697.5273, train_loss=6.5475416

Batch 237800, train_perplexity=664.5279, train_loss=6.499077

Batch 237810, train_perplexity=681.907, train_loss=6.5248933

Batch 237820, train_perplexity=603.43506, train_loss=6.4026384

Batch 237830, train_perplexity=732.4611, train_loss=6.5964103

Batch 237840, train_perplexity=658.9988, train_loss=6.4907217

Batch 237850, train_perplexity=644.8013, train_loss=6.468942

Batch 237860, train_perplexity=735.94916, train_loss=6.601161

Batch 237870, train_perplexity=663.9388, train_loss=6.49819

Batch 237880, train_perplexity=700.6882, train_loss=6.552063

Batch 237890, train_perplexity=691.7091, train_loss=6.5391655

Batch 237900, train_perplexity=700.96356, train_loss=6.552456

Batch 237910, train_perplexity=598.28845, train_loss=6.394073

Batch 237920, train_perplexity=637.6727, train_loss=6.457825

Batch 237930, train_perplexity=653.46893, train_loss=6.482295

Batch 237940, train_perplexity=762.4418, train_loss=6.636526

Batch 237950, train_perplexity=617.82526, train_loss=6.4262056

Batch 237960, train_perplexity=731.2205, train_loss=6.594715

Batch 237970, train_perplexity=609.8795, train_loss=6.4132614

Batch 237980, train_perplexity=659.1773, train_loss=6.4909925

Batch 237990, train_perplexity=639.3074, train_loss=6.4603853

Batch 238000, train_perplexity=607.9333, train_loss=6.410065

Batch 238010, train_perplexity=624.25964, train_loss=6.4365664

Batch 238020, train_perplexity=640.0369, train_loss=6.461526

Batch 238030, train_perplexity=593.40356, train_loss=6.3858747

Batch 238040, train_perplexity=618.0321, train_loss=6.4265404

Batch 238050, train_perplexity=682.61005, train_loss=6.5259237

Batch 238060, train_perplexity=658.60046, train_loss=6.490117

Batch 238070, train_perplexity=627.5459, train_loss=6.441817

Batch 238080, train_perplexity=673.0752, train_loss=6.511857

Batch 238090, train_perplexity=698.58844, train_loss=6.549062

Batch 238100, train_perplexity=634.054, train_loss=6.452134

Batch 238110, train_perplexity=638.24005, train_loss=6.4587145

Batch 238120, train_perplexity=680.816, train_loss=6.523292

Batch 238130, train_perplexity=653.33875, train_loss=6.4820957

Batch 238140, train_perplexity=654.94415, train_loss=6.48455

Batch 238150, train_perplexity=659.46277, train_loss=6.4914255

Batch 238160, train_perplexity=727.3175, train_loss=6.589363

Batch 238170, train_perplexity=628.2216, train_loss=6.442893

Batch 238180, train_perplexity=684.60364, train_loss=6.52884

Batch 238190, train_perplexity=635.2909, train_loss=6.454083

Batch 238200, train_perplexity=716.82654, train_loss=6.574834

Batch 238210, train_perplexity=584.3391, train_loss=6.3704815

Batch 238220, train_perplexity=733.7255, train_loss=6.598135

Batch 238230, train_perplexity=626.3534, train_loss=6.4399147

Batch 238240, train_perplexity=653.7613, train_loss=6.4827423

Batch 238250, train_perplexity=629.76086, train_loss=6.44534

Batch 238260, train_perplexity=611.0165, train_loss=6.415124

Batch 238270, train_perplexity=741.3811, train_loss=6.608515

Batch 238280, train_perplexity=666.46234, train_loss=6.5019836

Batch 238290, train_perplexity=643.3735, train_loss=6.4667253

Batch 238300, train_perplexity=807.8897, train_loss=6.6944256

Batch 238310, train_perplexity=657.0064, train_loss=6.487694

Batch 238320, train_perplexity=656.17426, train_loss=6.4864264

Batch 238330, train_perplexity=675.4531, train_loss=6.5153837

Batch 238340, train_perplexity=652.01416, train_loss=6.4800663

Batch 238350, train_perplexity=710.74976, train_loss=6.5663204

Batch 238360, train_perplexity=705.6839, train_loss=6.5591674

Batch 238370, train_perplexity=633.4085, train_loss=6.4511156

Batch 238380, train_perplexity=707.16736, train_loss=6.5612674

Batch 238390, train_perplexity=628.902, train_loss=6.4439754

Batch 238400, train_perplexity=656.77496, train_loss=6.4873414

Batch 238410, train_perplexity=636.4312, train_loss=6.4558764

Batch 238420, train_perplexity=637.0102, train_loss=6.4567857

Batch 238430, train_perplexity=628.4589, train_loss=6.4432707

Batch 238440, train_perplexity=672.0326, train_loss=6.510307

Batch 238450, train_perplexity=645.5048, train_loss=6.4700327

Batch 238460, train_perplexity=619.4651, train_loss=6.4288564

Batch 238470, train_perplexity=710.7504, train_loss=6.5663214

Batch 238480, train_perplexity=675.4318, train_loss=6.5153522

Batch 238490, train_perplexity=613.5656, train_loss=6.419287

Batch 238500, train_perplexity=745.46124, train_loss=6.614003

Batch 238510, train_perplexity=644.6611, train_loss=6.4687247

Batch 238520, train_perplexity=622.3064, train_loss=6.4334326

Batch 238530, train_perplexity=706.8471, train_loss=6.5608144

Batch 238540, train_perplexity=694.8841, train_loss=6.543745

Batch 238550, train_perplexity=637.5569, train_loss=6.4576435

Batch 238560, train_perplexity=689.8928, train_loss=6.536536

Batch 238570, train_perplexity=698.4552, train_loss=6.548871

Batch 238580, train_perplexity=661.05585, train_loss=6.4938383

Batch 238590, train_perplexity=672.8098, train_loss=6.5114627

Batch 238600, train_perplexity=666.66736, train_loss=6.502291

Batch 238610, train_perplexity=529.9943, train_loss=6.2728662

Batch 238620, train_perplexity=755.51196, train_loss=6.6273956

Batch 238630, train_perplexity=631.14343, train_loss=6.447533

Batch 238640, train_perplexity=714.6423, train_loss=6.571782

Batch 238650, train_perplexity=656.62024, train_loss=6.487106

Batch 238660, train_perplexity=675.84326, train_loss=6.515961

Batch 238670, train_perplexity=719.96265, train_loss=6.5791993

Batch 238680, train_perplexity=608.1519, train_loss=6.4104247

Batch 238690, train_perplexity=708.254, train_loss=6.562803

Batch 238700, train_perplexity=606.7841, train_loss=6.408173

Batch 238710, train_perplexity=627.59406, train_loss=6.4418936

Batch 238720, train_perplexity=682.1789, train_loss=6.525292

Batch 238730, train_perplexity=720.69183, train_loss=6.5802116

Batch 238740, train_perplexity=643.77057, train_loss=6.4673424

Batch 238750, train_perplexity=631.9273, train_loss=6.4487743

Batch 238760, train_perplexity=695.7703, train_loss=6.5450196

Batch 238770, train_perplexity=665.4151, train_loss=6.500411

Batch 238780, train_perplexity=649.99335, train_loss=6.476962

Batch 238790, train_perplexity=719.1985, train_loss=6.5781374

Batch 238800, train_perplexity=648.4065, train_loss=6.474518

Batch 238810, train_perplexity=687.57684, train_loss=6.5331736

Batch 238820, train_perplexity=642.69824, train_loss=6.4656754

Batch 238830, train_perplexity=717.7117, train_loss=6.576068

Batch 238840, train_perplexity=642.2277, train_loss=6.464943

Batch 238850, train_perplexity=620.32086, train_loss=6.430237

Batch 238860, train_perplexity=712.8147, train_loss=6.5692215

Batch 238870, train_perplexity=725.3786, train_loss=6.586694

Batch 238880, train_perplexity=582.81476, train_loss=6.3678694

Batch 238890, train_perplexity=573.7265, train_loss=6.352153

Batch 238900, train_perplexity=606.3095, train_loss=6.4073906

Batch 238910, train_perplexity=603.6417, train_loss=6.402981

Batch 238920, train_perplexity=604.1992, train_loss=6.403904

Batch 238930, train_perplexity=641.98096, train_loss=6.4645586

Batch 238940, train_perplexity=707.1279, train_loss=6.5612116

Batch 238950, train_perplexity=677.8433, train_loss=6.518916

Batch 238960, train_perplexity=666.1567, train_loss=6.501525

Batch 238970, train_perplexity=629.2422, train_loss=6.444516

Batch 238980, train_perplexity=692.57416, train_loss=6.5404153

Batch 238990, train_perplexity=656.2205, train_loss=6.486497

Batch 239000, train_perplexity=678.3095, train_loss=6.5196037

Batch 239010, train_perplexity=689.3975, train_loss=6.535818

Batch 239020, train_perplexity=695.39124, train_loss=6.5444746

Batch 239030, train_perplexity=670.6548, train_loss=6.5082545

Batch 239040, train_perplexity=629.13416, train_loss=6.4443445

Batch 239050, train_perplexity=650.9288, train_loss=6.4784

Batch 239060, train_perplexity=655.0204, train_loss=6.4846663
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 239070, train_perplexity=770.08844, train_loss=6.6465054

Batch 239080, train_perplexity=763.22015, train_loss=6.6375465

Batch 239090, train_perplexity=608.98914, train_loss=6.4118004

Batch 239100, train_perplexity=595.8091, train_loss=6.38992

Batch 239110, train_perplexity=633.5623, train_loss=6.4513583

Batch 239120, train_perplexity=691.7863, train_loss=6.539277

Batch 239130, train_perplexity=631.8146, train_loss=6.448596

Batch 239140, train_perplexity=668.62366, train_loss=6.5052214

Batch 239150, train_perplexity=644.98944, train_loss=6.469234

Batch 239160, train_perplexity=586.40186, train_loss=6.3740053

Batch 239170, train_perplexity=688.7936, train_loss=6.5349417

Batch 239180, train_perplexity=643.8123, train_loss=6.467407

Batch 239190, train_perplexity=697.6204, train_loss=6.547675

Batch 239200, train_perplexity=661.8336, train_loss=6.495014

Batch 239210, train_perplexity=650.7159, train_loss=6.478073

Batch 239220, train_perplexity=651.83264, train_loss=6.479788

Batch 239230, train_perplexity=594.3505, train_loss=6.3874693

Batch 239240, train_perplexity=642.8583, train_loss=6.4659243

Batch 239250, train_perplexity=662.2276, train_loss=6.4956093

Batch 239260, train_perplexity=723.9401, train_loss=6.5847087

Batch 239270, train_perplexity=691.26465, train_loss=6.5385227

Batch 239280, train_perplexity=644.8449, train_loss=6.46901

Batch 239290, train_perplexity=773.8563, train_loss=6.6513863

Batch 239300, train_perplexity=703.53864, train_loss=6.556123

Batch 239310, train_perplexity=644.0487, train_loss=6.4677744

Batch 239320, train_perplexity=661.2002, train_loss=6.4940567

Batch 239330, train_perplexity=635.86395, train_loss=6.4549847

Batch 239340, train_perplexity=634.0044, train_loss=6.452056

Batch 239350, train_perplexity=721.98236, train_loss=6.5820007

Batch 239360, train_perplexity=651.0008, train_loss=6.478511

Batch 239370, train_perplexity=663.3375, train_loss=6.497284

Batch 239380, train_perplexity=660.87555, train_loss=6.4935656

Batch 239390, train_perplexity=595.5568, train_loss=6.389497

Batch 239400, train_perplexity=650.1576, train_loss=6.477215

Batch 239410, train_perplexity=701.17017, train_loss=6.5527506

Batch 239420, train_perplexity=612.8828, train_loss=6.418174

Batch 239430, train_perplexity=618.7752, train_loss=6.427742

Batch 239440, train_perplexity=685.4549, train_loss=6.5300827

Batch 239450, train_perplexity=611.86084, train_loss=6.416505

Batch 239460, train_perplexity=709.56323, train_loss=6.5646496

Batch 239470, train_perplexity=612.31, train_loss=6.4172387

Batch 239480, train_perplexity=662.72986, train_loss=6.4963675

Batch 239490, train_perplexity=719.5628, train_loss=6.578644

Batch 239500, train_perplexity=725.07153, train_loss=6.5862703

Batch 239510, train_perplexity=619.9701, train_loss=6.4296713

Batch 239520, train_perplexity=685.6543, train_loss=6.5303736

Batch 239530, train_perplexity=565.0262, train_loss=6.336872

Batch 239540, train_perplexity=621.2257, train_loss=6.4316945

Batch 239550, train_perplexity=713.3744, train_loss=6.5700064

Batch 239560, train_perplexity=668.94574, train_loss=6.505703

Batch 239570, train_perplexity=642.9309, train_loss=6.4660373

Batch 239580, train_perplexity=695.4834, train_loss=6.544607

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Batch 239590, train_perplexity=640.61584, train_loss=6.46243

Batch 239600, train_perplexity=682.89, train_loss=6.526334

Batch 239610, train_perplexity=636.77057, train_loss=6.4564095

Batch 239620, train_perplexity=704.8586, train_loss=6.557997

Batch 239630, train_perplexity=634.2705, train_loss=6.4524755

Batch 239640, train_perplexity=654.58545, train_loss=6.484002

Batch 239650, train_perplexity=643.98975, train_loss=6.467683

Batch 239660, train_perplexity=605.64923, train_loss=6.406301

Batch 239670, train_perplexity=653.15247, train_loss=6.4818106

Batch 239680, train_perplexity=582.862, train_loss=6.3679504

Batch 239690, train_perplexity=705.4797, train_loss=6.558878

Batch 239700, train_perplexity=635.22455, train_loss=6.4539785

Batch 239710, train_perplexity=686.663, train_loss=6.5318437

Batch 239720, train_perplexity=695.069, train_loss=6.544011

Batch 239730, train_perplexity=678.41113, train_loss=6.5197535

Batch 239740, train_perplexity=611.26886, train_loss=6.415537

Batch 239750, train_perplexity=651.9856, train_loss=6.4800224

Batch 239760, train_perplexity=646.99384, train_loss=6.472337

Batch 239770, train_perplexity=651.50073, train_loss=6.4792786

Batch 239780, train_perplexity=605.33484, train_loss=6.4057817

Batch 239790, train_perplexity=665.8629, train_loss=6.501084

Batch 239800, train_perplexity=667.9341, train_loss=6.5041895

Batch 239810, train_perplexity=672.5965, train_loss=6.5111456

Batch 239820, train_perplexity=655.1788, train_loss=6.484908

Batch 239830, train_perplexity=630.7803, train_loss=6.4469576

Batch 239840, train_perplexity=605.0324, train_loss=6.405282

Batch 239850, train_perplexity=660.79456, train_loss=6.493443

Batch 239860, train_perplexity=745.4627, train_loss=6.614005

Batch 239870, train_perplexity=797.94165, train_loss=6.6820354

Batch 239880, train_perplexity=663.9033, train_loss=6.4981365

Batch 239890, train_perplexity=637.29486, train_loss=6.4572325

Batch 239900, train_perplexity=717.5625, train_loss=6.57586

Batch 239910, train_perplexity=610.7639, train_loss=6.4147105

Batch 239920, train_perplexity=681.0517, train_loss=6.5236382

Batch 239930, train_perplexity=628.7803, train_loss=6.443782

Batch 239940, train_perplexity=584.59076, train_loss=6.370912

Batch 239950, train_perplexity=648.67834, train_loss=6.474937

Batch 239960, train_perplexity=667.90796, train_loss=6.5041504

Batch 239970, train_perplexity=652.76575, train_loss=6.4812183

Batch 239980, train_perplexity=665.61755, train_loss=6.5007153

Batch 239990, train_perplexity=658.17474, train_loss=6.4894705

Batch 240000, train_perplexity=671.167, train_loss=6.509018

Batch 240010, train_perplexity=675.13525, train_loss=6.514913

Batch 240020, train_perplexity=728.96185, train_loss=6.5916214

Batch 240030, train_perplexity=691.64575, train_loss=6.539074

Batch 240040, train_perplexity=677.11255, train_loss=6.5178375

Batch 240050, train_perplexity=631.87067, train_loss=6.4486847

Batch 240060, train_perplexity=608.2543, train_loss=6.410593

Batch 240070, train_perplexity=636.146, train_loss=6.455428

Batch 240080, train_perplexity=664.07715, train_loss=6.4983983

Batch 240090, train_perplexity=680.11804, train_loss=6.5222664

Batch 240100, train_perplexity=606.4136, train_loss=6.4075623

Batch 240110, train_perplexity=643.74475, train_loss=6.4673023

Batch 240120, train_perplexity=651.0343, train_loss=6.4785624

Batch 240130, train_perplexity=686.6751, train_loss=6.5318613

Batch 240140, train_perplexity=655.13, train_loss=6.4848337

Batch 240150, train_perplexity=653.33435, train_loss=6.482089

Batch 240160, train_perplexity=620.1493, train_loss=6.4299603

Batch 240170, train_perplexity=654.4634, train_loss=6.4838157

Batch 240180, train_perplexity=673.1548, train_loss=6.5119753

Batch 240190, train_perplexity=678.20734, train_loss=6.519453

Batch 240200, train_perplexity=655.53625, train_loss=6.4854536

Batch 240210, train_perplexity=613.93994, train_loss=6.419897

Batch 240220, train_perplexity=622.66705, train_loss=6.434012

Batch 240230, train_perplexity=591.7162, train_loss=6.383027

Batch 240240, train_perplexity=600.6794, train_loss=6.3980613

Batch 240250, train_perplexity=658.8668, train_loss=6.4905214

Batch 240260, train_perplexity=660.3331, train_loss=6.4927444

Batch 240270, train_perplexity=654.1433, train_loss=6.4833264

Batch 240280, train_perplexity=659.704, train_loss=6.4917912

Batch 240290, train_perplexity=699.0499, train_loss=6.549722

Batch 240300, train_perplexity=628.53894, train_loss=6.443398
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 240310, train_perplexity=570.75037, train_loss=6.346952

Batch 240320, train_perplexity=625.84106, train_loss=6.4390965

Batch 240330, train_perplexity=652.3214, train_loss=6.4805374

Batch 240340, train_perplexity=675.7794, train_loss=6.5158668

Batch 240350, train_perplexity=660.0548, train_loss=6.492323

Batch 240360, train_perplexity=719.09735, train_loss=6.5779967

Batch 240370, train_perplexity=653.1369, train_loss=6.4817867

Batch 240380, train_perplexity=618.1435, train_loss=6.4267206

Batch 240390, train_perplexity=666.40515, train_loss=6.501898

Batch 240400, train_perplexity=671.489, train_loss=6.5094976

Batch 240410, train_perplexity=690.6268, train_loss=6.5375996

Batch 240420, train_perplexity=573.6198, train_loss=6.351967

Batch 240430, train_perplexity=657.7866, train_loss=6.4888806

Batch 240440, train_perplexity=642.187, train_loss=6.4648795

Batch 240450, train_perplexity=712.8483, train_loss=6.5692687

Batch 240460, train_perplexity=653.2409, train_loss=6.481946

Batch 240470, train_perplexity=607.4094, train_loss=6.409203

Batch 240480, train_perplexity=584.60834, train_loss=6.370942

Batch 240490, train_perplexity=701.2043, train_loss=6.552799

Batch 240500, train_perplexity=709.2351, train_loss=6.564187

Batch 240510, train_perplexity=668.00385, train_loss=6.504294

Batch 240520, train_perplexity=752.40576, train_loss=6.6232758

Batch 240530, train_perplexity=565.45526, train_loss=6.337631

Batch 240540, train_perplexity=652.8439, train_loss=6.481338

Batch 240550, train_perplexity=697.7711, train_loss=6.547891

Batch 240560, train_perplexity=667.52716, train_loss=6.50358

Batch 240570, train_perplexity=658.4604, train_loss=6.4899044

Batch 240580, train_perplexity=633.76654, train_loss=6.4516807

Batch 240590, train_perplexity=634.2811, train_loss=6.452492

Batch 240600, train_perplexity=632.73895, train_loss=6.450058

Batch 240610, train_perplexity=765.0617, train_loss=6.6399565

Batch 240620, train_perplexity=712.0449, train_loss=6.568141

Batch 240630, train_perplexity=727.8695, train_loss=6.5901217

Batch 240640, train_perplexity=810.9187, train_loss=6.698168

Batch 240650, train_perplexity=604.1485, train_loss=6.40382

Batch 240660, train_perplexity=778.9026, train_loss=6.657886

Batch 240670, train_perplexity=649.5329, train_loss=6.4762535

Batch 240680, train_perplexity=733.4999, train_loss=6.5978274

Batch 240690, train_perplexity=703.26965, train_loss=6.5557404

Batch 240700, train_perplexity=669.18695, train_loss=6.5060635

Batch 240710, train_perplexity=654.6766, train_loss=6.4841413

Batch 240720, train_perplexity=720.81696, train_loss=6.580385

Batch 240730, train_perplexity=638.22485, train_loss=6.4586906

Batch 240740, train_perplexity=699.7156, train_loss=6.550674

Batch 240750, train_perplexity=643.61035, train_loss=6.4670935

Batch 240760, train_perplexity=649.03076, train_loss=6.47548

Batch 240770, train_perplexity=605.426, train_loss=6.4059324

Batch 240780, train_perplexity=586.0935, train_loss=6.3734794

Batch 240790, train_perplexity=665.6499, train_loss=6.500764

Batch 240800, train_perplexity=664.8905, train_loss=6.4996223

Batch 240810, train_perplexity=604.2274, train_loss=6.4039507

Batch 240820, train_perplexity=682.05304, train_loss=6.5251074

Batch 240830, train_perplexity=666.1821, train_loss=6.501563

Batch 240840, train_perplexity=669.4458, train_loss=6.50645

Batch 240850, train_perplexity=638.276, train_loss=6.4587708

Batch 240860, train_perplexity=615.7876, train_loss=6.422902

Batch 240870, train_perplexity=734.8039, train_loss=6.5996037

Batch 240880, train_perplexity=703.67584, train_loss=6.556318

Batch 240890, train_perplexity=631.82544, train_loss=6.448613

Batch 240900, train_perplexity=627.88385, train_loss=6.442355

Batch 240910, train_perplexity=570.9744, train_loss=6.3473444

Batch 240920, train_perplexity=592.25476, train_loss=6.383937

Batch 240930, train_perplexity=703.9947, train_loss=6.556771

Batch 240940, train_perplexity=640.0308, train_loss=6.4615164

Batch 240950, train_perplexity=661.89105, train_loss=6.495101

Batch 240960, train_perplexity=647.10675, train_loss=6.4725113

Batch 240970, train_perplexity=708.7226, train_loss=6.563464

Batch 240980, train_perplexity=595.7176, train_loss=6.3897667

Batch 240990, train_perplexity=734.74786, train_loss=6.5995274

Batch 241000, train_perplexity=588.97943, train_loss=6.3783913

Batch 241010, train_perplexity=664.19684, train_loss=6.4985785

Batch 241020, train_perplexity=599.415, train_loss=6.395954

Batch 241030, train_perplexity=570.4775, train_loss=6.3464737

Batch 241040, train_perplexity=757.86163, train_loss=6.630501

Batch 241050, train_perplexity=694.5525, train_loss=6.5432677

Batch 241060, train_perplexity=620.9731, train_loss=6.431288

Batch 241070, train_perplexity=602.26483, train_loss=6.400697

Batch 241080, train_perplexity=697.04517, train_loss=6.54685

Batch 241090, train_perplexity=642.3493, train_loss=6.465132

Batch 241100, train_perplexity=658.17163, train_loss=6.4894657

Batch 241110, train_perplexity=645.64954, train_loss=6.470257

Batch 241120, train_perplexity=659.75653, train_loss=6.491871

Batch 241130, train_perplexity=612.50244, train_loss=6.417553

Batch 241140, train_perplexity=644.94147, train_loss=6.4691596

Batch 241150, train_perplexity=666.19293, train_loss=6.5015793

Batch 241160, train_perplexity=675.5704, train_loss=6.5155573

Batch 241170, train_perplexity=623.6875, train_loss=6.4356494

Batch 241180, train_perplexity=671.1011, train_loss=6.5089197

Batch 241190, train_perplexity=616.3052, train_loss=6.4237423

Batch 241200, train_perplexity=699.6579, train_loss=6.5505915

Batch 241210, train_perplexity=632.95807, train_loss=6.450404

Batch 241220, train_perplexity=665.8677, train_loss=6.501091

Batch 241230, train_perplexity=613.0009, train_loss=6.4183664

Batch 241240, train_perplexity=719.00305, train_loss=6.5778656

Batch 241250, train_perplexity=626.22437, train_loss=6.4397087

Batch 241260, train_perplexity=714.03937, train_loss=6.570938

Batch 241270, train_perplexity=683.9452, train_loss=6.527878

Batch 241280, train_perplexity=706.8235, train_loss=6.560781

Batch 241290, train_perplexity=702.22314, train_loss=6.554251

Batch 241300, train_perplexity=669.5374, train_loss=6.506587

Batch 241310, train_perplexity=595.90735, train_loss=6.390085

Batch 241320, train_perplexity=630.7448, train_loss=6.4469013

Batch 241330, train_perplexity=558.6016, train_loss=6.3254366

Batch 241340, train_perplexity=647.9231, train_loss=6.473772

Batch 241350, train_perplexity=697.4677, train_loss=6.5474563

Batch 241360, train_perplexity=754.04895, train_loss=6.6254573

Batch 241370, train_perplexity=673.72125, train_loss=6.5128164

Batch 241380, train_perplexity=643.81995, train_loss=6.467419

Batch 241390, train_perplexity=690.38477, train_loss=6.537249

Batch 241400, train_perplexity=636.24554, train_loss=6.4555845

Batch 241410, train_perplexity=639.2019, train_loss=6.4602203

Batch 241420, train_perplexity=676.28326, train_loss=6.516612

Batch 241430, train_perplexity=652.1068, train_loss=6.4802084

Batch 241440, train_perplexity=609.2116, train_loss=6.4121656

Batch 241450, train_perplexity=672.9969, train_loss=6.5117407

Batch 241460, train_perplexity=711.70544, train_loss=6.567664

Batch 241470, train_perplexity=737.1777, train_loss=6.602829

Batch 241480, train_perplexity=654.99194, train_loss=6.484623

Batch 241490, train_perplexity=711.4897, train_loss=6.567361

Batch 241500, train_perplexity=634.6366, train_loss=6.4530525

Batch 241510, train_perplexity=683.1506, train_loss=6.5267153

Batch 241520, train_perplexity=637.1527, train_loss=6.4570093

Batch 241530, train_perplexity=619.53925, train_loss=6.428976

Batch 241540, train_perplexity=687.46765, train_loss=6.533015

Batch 241550, train_perplexity=663.3717, train_loss=6.4973354

Batch 241560, train_perplexity=649.8251, train_loss=6.476703

Batch 241570, train_perplexity=746.28357, train_loss=6.6151056

Batch 241580, train_perplexity=662.11835, train_loss=6.4954443

Batch 241590, train_perplexity=617.817, train_loss=6.4261923

Batch 241600, train_perplexity=672.1207, train_loss=6.510438

Batch 241610, train_perplexity=689.1001, train_loss=6.5353866
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 241620, train_perplexity=655.9778, train_loss=6.486127

Batch 241630, train_perplexity=647.1296, train_loss=6.4725466

Batch 241640, train_perplexity=628.80005, train_loss=6.4438133

Batch 241650, train_perplexity=655.6569, train_loss=6.4856377

Batch 241660, train_perplexity=642.40533, train_loss=6.4652195

Batch 241670, train_perplexity=650.68427, train_loss=6.4780245

Batch 241680, train_perplexity=613.9188, train_loss=6.4198627

Batch 241690, train_perplexity=626.64465, train_loss=6.4403796

Batch 241700, train_perplexity=583.56335, train_loss=6.369153

Batch 241710, train_perplexity=627.1854, train_loss=6.441242

Batch 241720, train_perplexity=690.0165, train_loss=6.5367155

Batch 241730, train_perplexity=694.3574, train_loss=6.542987

Batch 241740, train_perplexity=629.3226, train_loss=6.444644

Batch 241750, train_perplexity=646.5756, train_loss=6.47169

Batch 241760, train_perplexity=686.832, train_loss=6.5320897

Batch 241770, train_perplexity=599.3841, train_loss=6.3959026

Batch 241780, train_perplexity=618.73267, train_loss=6.4276733

Batch 241790, train_perplexity=618.2231, train_loss=6.4268494

Batch 241800, train_perplexity=673.9378, train_loss=6.513138

Batch 241810, train_perplexity=686.53796, train_loss=6.5316615

Batch 241820, train_perplexity=658.9341, train_loss=6.4906235

Batch 241830, train_perplexity=595.52277, train_loss=6.3894396

Batch 241840, train_perplexity=670.2184, train_loss=6.5076036

Batch 241850, train_perplexity=590.3037, train_loss=6.380637

Batch 241860, train_perplexity=640.9642, train_loss=6.4629736

Batch 241870, train_perplexity=651.0014, train_loss=6.478512

Batch 241880, train_perplexity=557.10565, train_loss=6.322755

Batch 241890, train_perplexity=609.82776, train_loss=6.4131765

Batch 241900, train_perplexity=769.46185, train_loss=6.6456914

Batch 241910, train_perplexity=703.1328, train_loss=6.555546

Batch 241920, train_perplexity=615.5225, train_loss=6.4224715

Batch 241930, train_perplexity=665.54584, train_loss=6.5006075

Batch 241940, train_perplexity=693.2739, train_loss=6.541425

Batch 241950, train_perplexity=625.74854, train_loss=6.4389486

Batch 241960, train_perplexity=689.3364, train_loss=6.5357294

Batch 241970, train_perplexity=618.03357, train_loss=6.4265428

Batch 241980, train_perplexity=655.4969, train_loss=6.4853935

Batch 241990, train_perplexity=623.27094, train_loss=6.4349813

Batch 242000, train_perplexity=652.74896, train_loss=6.4811926

Batch 242010, train_perplexity=683.6205, train_loss=6.527403

Batch 242020, train_perplexity=607.516, train_loss=6.4093785

Batch 242030, train_perplexity=639.842, train_loss=6.461221

Batch 242040, train_perplexity=654.0737, train_loss=6.48322

Batch 242050, train_perplexity=673.5041, train_loss=6.512494

Batch 242060, train_perplexity=669.77563, train_loss=6.5069427

Batch 242070, train_perplexity=672.5173, train_loss=6.511028

Batch 242080, train_perplexity=643.18085, train_loss=6.466426

Batch 242090, train_perplexity=667.4409, train_loss=6.503451

Batch 242100, train_perplexity=660.7051, train_loss=6.4933076

Batch 242110, train_perplexity=708.43604, train_loss=6.56306

Batch 242120, train_perplexity=642.7614, train_loss=6.4657736

Batch 242130, train_perplexity=700.36053, train_loss=6.551595

Batch 242140, train_perplexity=650.0141, train_loss=6.476994

Batch 242150, train_perplexity=627.06165, train_loss=6.441045

Batch 242160, train_perplexity=689.4271, train_loss=6.535861

Batch 242170, train_perplexity=609.48065, train_loss=6.412607

Batch 242180, train_perplexity=639.96277, train_loss=6.46141

Batch 242190, train_perplexity=668.1526, train_loss=6.5045166

Batch 242200, train_perplexity=678.1698, train_loss=6.5193977

Batch 242210, train_perplexity=643.1701, train_loss=6.466409

Batch 242220, train_perplexity=672.5683, train_loss=6.5111036

Batch 242230, train_perplexity=669.8331, train_loss=6.5070286

Batch 242240, train_perplexity=641.9748, train_loss=6.464549

Batch 242250, train_perplexity=655.90643, train_loss=6.486018

Batch 242260, train_perplexity=666.402, train_loss=6.501893

Batch 242270, train_perplexity=590.6945, train_loss=6.381299

Batch 242280, train_perplexity=761.0651, train_loss=6.634719

Batch 242290, train_perplexity=581.6468, train_loss=6.3658633

Batch 242300, train_perplexity=629.96747, train_loss=6.445668

Batch 242310, train_perplexity=666.14496, train_loss=6.5015073

Batch 242320, train_perplexity=714.41125, train_loss=6.571459

Batch 242330, train_perplexity=678.014, train_loss=6.519168

Batch 242340, train_perplexity=644.11475, train_loss=6.467877

Batch 242350, train_perplexity=648.24976, train_loss=6.474276

Batch 242360, train_perplexity=619.9864, train_loss=6.4296975

Batch 242370, train_perplexity=678.3192, train_loss=6.519618

Batch 242380, train_perplexity=585.11536, train_loss=6.371809

Batch 242390, train_perplexity=724.7642, train_loss=6.5858464

Batch 242400, train_perplexity=660.886, train_loss=6.4935813

Batch 242410, train_perplexity=626.8637, train_loss=6.440729

Batch 242420, train_perplexity=601.04755, train_loss=6.398674

Batch 242430, train_perplexity=573.7881, train_loss=6.35226

Batch 242440, train_perplexity=578.432, train_loss=6.360321

Batch 242450, train_perplexity=715.1509, train_loss=6.5724936

Batch 242460, train_perplexity=690.1909, train_loss=6.536968

Batch 242470, train_perplexity=614.9683, train_loss=6.421571

Batch 242480, train_perplexity=664.19116, train_loss=6.49857

Batch 242490, train_perplexity=610.62384, train_loss=6.414481

Batch 242500, train_perplexity=676.13306, train_loss=6.51639

Batch 242510, train_perplexity=637.2663, train_loss=6.4571877

Batch 242520, train_perplexity=651.4057, train_loss=6.4791327

Batch 242530, train_perplexity=604.77686, train_loss=6.4048595

Batch 242540, train_perplexity=606.3413, train_loss=6.407443

Batch 242550, train_perplexity=623.5367, train_loss=6.4354076

Batch 242560, train_perplexity=649.1484, train_loss=6.4756613

Batch 242570, train_perplexity=642.04584, train_loss=6.4646597

Batch 242580, train_perplexity=634.9048, train_loss=6.453475

Batch 242590, train_perplexity=574.0768, train_loss=6.352763

Batch 242600, train_perplexity=775.5297, train_loss=6.6535463

Batch 242610, train_perplexity=642.94836, train_loss=6.4660645

Batch 242620, train_perplexity=603.4236, train_loss=6.4026194

Batch 242630, train_perplexity=682.92194, train_loss=6.5263805

Batch 242640, train_perplexity=575.89703, train_loss=6.355929

Batch 242650, train_perplexity=579.26117, train_loss=6.3617535

Batch 242660, train_perplexity=662.2513, train_loss=6.495645

Batch 242670, train_perplexity=671.4775, train_loss=6.5094805

Batch 242680, train_perplexity=673.26874, train_loss=6.5121446

Batch 242690, train_perplexity=696.8082, train_loss=6.54651

Batch 242700, train_perplexity=733.7479, train_loss=6.5981655

Batch 242710, train_perplexity=635.4136, train_loss=6.454276

Batch 242720, train_perplexity=661.27905, train_loss=6.494176

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Batch 242730, train_perplexity=711.0484, train_loss=6.5667405

Batch 242740, train_perplexity=657.5229, train_loss=6.4884796

Batch 242750, train_perplexity=686.39197, train_loss=6.531449

Batch 242760, train_perplexity=713.62616, train_loss=6.570359

Batch 242770, train_perplexity=642.13153, train_loss=6.464793

Batch 242780, train_perplexity=587.5063, train_loss=6.375887

Batch 242790, train_perplexity=624.94794, train_loss=6.4376683

Batch 242800, train_perplexity=637.8312, train_loss=6.4580736

Batch 242810, train_perplexity=677.2443, train_loss=6.518032

Batch 242820, train_perplexity=671.85284, train_loss=6.5100393

Batch 242830, train_perplexity=672.3573, train_loss=6.51079

Batch 242840, train_perplexity=622.9453, train_loss=6.4344587

Batch 242850, train_perplexity=624.11615, train_loss=6.4363365

Batch 242860, train_perplexity=746.9714, train_loss=6.616027
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 242870, train_perplexity=712.2378, train_loss=6.568412

Batch 242880, train_perplexity=635.8676, train_loss=6.4549904

Batch 242890, train_perplexity=671.5274, train_loss=6.509555

Batch 242900, train_perplexity=666.00806, train_loss=6.501302

Batch 242910, train_perplexity=650.72644, train_loss=6.4780893

Batch 242920, train_perplexity=656.5648, train_loss=6.4870214

Batch 242930, train_perplexity=643.4244, train_loss=6.4668045

Batch 242940, train_perplexity=645.22754, train_loss=6.469603

Batch 242950, train_perplexity=589.139, train_loss=6.378662

Batch 242960, train_perplexity=601.6801, train_loss=6.399726

Batch 242970, train_perplexity=690.46606, train_loss=6.537367

Batch 242980, train_perplexity=635.084, train_loss=6.4537573

Batch 242990, train_perplexity=592.4423, train_loss=6.3842535

Batch 243000, train_perplexity=631.7339, train_loss=6.448468

Batch 243010, train_perplexity=691.9559, train_loss=6.539522

Batch 243020, train_perplexity=610.3619, train_loss=6.414052

Batch 243030, train_perplexity=693.5408, train_loss=6.54181

Batch 243040, train_perplexity=659.9447, train_loss=6.492156

Batch 243050, train_perplexity=625.6859, train_loss=6.4388485

Batch 243060, train_perplexity=637.1919, train_loss=6.457071

Batch 243070, train_perplexity=684.58734, train_loss=6.528816

Batch 243080, train_perplexity=649.90533, train_loss=6.4768267

Batch 243090, train_perplexity=761.4782, train_loss=6.6352615

Batch 243100, train_perplexity=625.7074, train_loss=6.438883

Batch 243110, train_perplexity=657.8428, train_loss=6.488966

Batch 243120, train_perplexity=679.74713, train_loss=6.521721

Batch 243130, train_perplexity=662.0085, train_loss=6.4952784

Batch 243140, train_perplexity=672.0182, train_loss=6.5102854

Batch 243150, train_perplexity=638.8606, train_loss=6.4596863

Batch 243160, train_perplexity=695.82837, train_loss=6.545103

Batch 243170, train_perplexity=670.4658, train_loss=6.5079727

Batch 243180, train_perplexity=594.24854, train_loss=6.3872976

Batch 243190, train_perplexity=616.5365, train_loss=6.4241176

Batch 243200, train_perplexity=706.55054, train_loss=6.560395

Batch 243210, train_perplexity=644.64233, train_loss=6.4686956

Batch 243220, train_perplexity=645.6563, train_loss=6.4702673

Batch 243230, train_perplexity=698.3453, train_loss=6.5487137

Batch 243240, train_perplexity=656.3229, train_loss=6.486653

Batch 243250, train_perplexity=629.418, train_loss=6.4447956

Batch 243260, train_perplexity=662.1689, train_loss=6.4955206

Batch 243270, train_perplexity=662.49225, train_loss=6.496009

Batch 243280, train_perplexity=697.5672, train_loss=6.547599

Batch 243290, train_perplexity=721.66986, train_loss=6.581568

Batch 243300, train_perplexity=656.04034, train_loss=6.4862223

Batch 243310, train_perplexity=575.4883, train_loss=6.355219

Batch 243320, train_perplexity=618.4439, train_loss=6.4272065

Batch 243330, train_perplexity=684.8779, train_loss=6.5292406

Batch 243340, train_perplexity=645.57806, train_loss=6.470146

Batch 243350, train_perplexity=646.10223, train_loss=6.4709578

Batch 243360, train_perplexity=647.8113, train_loss=6.4735994

Batch 243370, train_perplexity=661.2166, train_loss=6.4940815

Batch 243380, train_perplexity=611.1622, train_loss=6.4153624

Batch 243390, train_perplexity=648.2306, train_loss=6.4742465

Batch 243400, train_perplexity=647.0728, train_loss=6.472459

Batch 243410, train_perplexity=634.0909, train_loss=6.4521923

Batch 243420, train_perplexity=625.87866, train_loss=6.4391565

Batch 243430, train_perplexity=651.28955, train_loss=6.4789543

Batch 243440, train_perplexity=639.99664, train_loss=6.461463

Batch 243450, train_perplexity=722.59924, train_loss=6.5828547

Batch 243460, train_perplexity=645.9621, train_loss=6.470741

Batch 243470, train_perplexity=665.3044, train_loss=6.5002446

Batch 243480, train_perplexity=668.8896, train_loss=6.505619

Batch 243490, train_perplexity=632.292, train_loss=6.4493513

Batch 243500, train_perplexity=673.5789, train_loss=6.512605

Batch 243510, train_perplexity=629.49756, train_loss=6.444922

Batch 243520, train_perplexity=602.907, train_loss=6.401763

Batch 243530, train_perplexity=704.09906, train_loss=6.556919

Batch 243540, train_perplexity=594.6615, train_loss=6.3879924

Batch 243550, train_perplexity=688.2443, train_loss=6.534144

Batch 243560, train_perplexity=655.5538, train_loss=6.4854803

Batch 243570, train_perplexity=665.4665, train_loss=6.5004883

Batch 243580, train_perplexity=697.57587, train_loss=6.547611

Batch 243590, train_perplexity=656.29877, train_loss=6.486616

Batch 243600, train_perplexity=605.336, train_loss=6.4057837

Batch 243610, train_perplexity=641.75323, train_loss=6.464204

Batch 243620, train_perplexity=606.08575, train_loss=6.4070215

Batch 243630, train_perplexity=698.1462, train_loss=6.5484285

Batch 243640, train_perplexity=662.654, train_loss=6.496253

Batch 243650, train_perplexity=614.0254, train_loss=6.4200363

Batch 243660, train_perplexity=651.06226, train_loss=6.4786053

Batch 243670, train_perplexity=639.82336, train_loss=6.461192

Batch 243680, train_perplexity=629.3823, train_loss=6.444739

Batch 243690, train_perplexity=652.1877, train_loss=6.4803324

Batch 243700, train_perplexity=694.80255, train_loss=6.5436277

Batch 243710, train_perplexity=626.76416, train_loss=6.4405704

Batch 243720, train_perplexity=638.9645, train_loss=6.459849

Batch 243730, train_perplexity=598.046, train_loss=6.3936677

Batch 243740, train_perplexity=638.09033, train_loss=6.45848

Batch 243750, train_perplexity=693.2386, train_loss=6.541374

Batch 243760, train_perplexity=592.0557, train_loss=6.3836007

Batch 243770, train_perplexity=642.1303, train_loss=6.4647913

Batch 243780, train_perplexity=658.33386, train_loss=6.489712

Batch 243790, train_perplexity=634.2923, train_loss=6.45251

Batch 243800, train_perplexity=649.63696, train_loss=6.4764137

Batch 243810, train_perplexity=624.6348, train_loss=6.437167

Batch 243820, train_perplexity=611.8702, train_loss=6.41652

Batch 243830, train_perplexity=629.10895, train_loss=6.4443045

Batch 243840, train_perplexity=587.9513, train_loss=6.376644

Batch 243850, train_perplexity=634.8672, train_loss=6.453416

Batch 243860, train_perplexity=698.85065, train_loss=6.549437

Batch 243870, train_perplexity=655.3206, train_loss=6.4851246

Batch 243880, train_perplexity=610.0313, train_loss=6.4135103

Batch 243890, train_perplexity=665.86487, train_loss=6.5010867

Batch 243900, train_perplexity=656.2021, train_loss=6.486469

Batch 243910, train_perplexity=617.45624, train_loss=6.425608

Batch 243920, train_perplexity=669.3995, train_loss=6.506381

Batch 243930, train_perplexity=576.0236, train_loss=6.3561487

Batch 243940, train_perplexity=619.58325, train_loss=6.429047

Batch 243950, train_perplexity=651.7046, train_loss=6.4795914

Batch 243960, train_perplexity=697.4245, train_loss=6.5473943

Batch 243970, train_perplexity=651.71423, train_loss=6.479606

Batch 243980, train_perplexity=619.3366, train_loss=6.428649

Batch 243990, train_perplexity=672.4342, train_loss=6.5109043

Batch 244000, train_perplexity=648.5886, train_loss=6.4747987

Batch 244010, train_perplexity=640.6904, train_loss=6.4625463

Batch 244020, train_perplexity=616.2779, train_loss=6.423698

Batch 244030, train_perplexity=664.11005, train_loss=6.498448

Batch 244040, train_perplexity=656.2112, train_loss=6.4864826

Batch 244050, train_perplexity=668.3814, train_loss=6.504859

Batch 244060, train_perplexity=671.085, train_loss=6.508896

Batch 244070, train_perplexity=645.4673, train_loss=6.4699745

Batch 244080, train_perplexity=649.2412, train_loss=6.4758043

Batch 244090, train_perplexity=779.0961, train_loss=6.6581345

Batch 244100, train_perplexity=710.6481, train_loss=6.5661774

Batch 244110, train_perplexity=653.39166, train_loss=6.482177

Batch 244120, train_perplexity=692.8807, train_loss=6.540858

Batch 244130, train_perplexity=692.8354, train_loss=6.5407925

Batch 244140, train_perplexity=665.24756, train_loss=6.5001593

Batch 244150, train_perplexity=622.64716, train_loss=6.43398

Batch 244160, train_perplexity=637.0716, train_loss=6.456882

Batch 244170, train_perplexity=627.60004, train_loss=6.441903
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 244180, train_perplexity=567.86224, train_loss=6.341879

Batch 244190, train_perplexity=669.5885, train_loss=6.5066633

Batch 244200, train_perplexity=654.6466, train_loss=6.4840956

Batch 244210, train_perplexity=633.0468, train_loss=6.4505444

Batch 244220, train_perplexity=581.6515, train_loss=6.3658714

Batch 244230, train_perplexity=675.4373, train_loss=6.5153604

Batch 244240, train_perplexity=641.93866, train_loss=6.464493

Batch 244250, train_perplexity=709.59094, train_loss=6.5646887

Batch 244260, train_perplexity=650.82697, train_loss=6.478244

Batch 244270, train_perplexity=562.985, train_loss=6.333253

Batch 244280, train_perplexity=615.6499, train_loss=6.4226785

Batch 244290, train_perplexity=674.42834, train_loss=6.5138655

Batch 244300, train_perplexity=652.1939, train_loss=6.480342

Batch 244310, train_perplexity=647.78845, train_loss=6.473564

Batch 244320, train_perplexity=620.2522, train_loss=6.430126

Batch 244330, train_perplexity=669.41034, train_loss=6.5063972

Batch 244340, train_perplexity=642.36066, train_loss=6.46515

Batch 244350, train_perplexity=651.29297, train_loss=6.4789596

Batch 244360, train_perplexity=659.5571, train_loss=6.4915686

Batch 244370, train_perplexity=553.27545, train_loss=6.315856

Batch 244380, train_perplexity=678.5962, train_loss=6.520026

Batch 244390, train_perplexity=699.9238, train_loss=6.5509715

Batch 244400, train_perplexity=716.82587, train_loss=6.574833

Batch 244410, train_perplexity=583.7359, train_loss=6.3694487

Batch 244420, train_perplexity=638.99493, train_loss=6.4598966

Batch 244430, train_perplexity=621.1982, train_loss=6.43165

Batch 244440, train_perplexity=730.0611, train_loss=6.593128

Batch 244450, train_perplexity=697.6221, train_loss=6.5476775

Batch 244460, train_perplexity=693.15393, train_loss=6.541252

Batch 244470, train_perplexity=627.6533, train_loss=6.441988

Batch 244480, train_perplexity=687.31854, train_loss=6.532798

Batch 244490, train_perplexity=728.4163, train_loss=6.590873

Batch 244500, train_perplexity=679.6648, train_loss=6.5216

Batch 244510, train_perplexity=642.1285, train_loss=6.4647884

Batch 244520, train_perplexity=611.17676, train_loss=6.415386

Batch 244530, train_perplexity=752.6408, train_loss=6.623588

Batch 244540, train_perplexity=738.2654, train_loss=6.6043034

Batch 244550, train_perplexity=639.45184, train_loss=6.4606113

Batch 244560, train_perplexity=640.3276, train_loss=6.46198

Batch 244570, train_perplexity=695.55237, train_loss=6.5447063

Batch 244580, train_perplexity=642.0091, train_loss=6.4646025

Batch 244590, train_perplexity=635.2909, train_loss=6.454083

Batch 244600, train_perplexity=655.45874, train_loss=6.4853354

Batch 244610, train_perplexity=639.8356, train_loss=6.461211

Batch 244620, train_perplexity=635.43115, train_loss=6.4543037

Batch 244630, train_perplexity=676.1914, train_loss=6.516476

Batch 244640, train_perplexity=726.8231, train_loss=6.588683

Batch 244650, train_perplexity=607.962, train_loss=6.4101124

Batch 244660, train_perplexity=715.0745, train_loss=6.5723867

Batch 244670, train_perplexity=640.5713, train_loss=6.4623604

Batch 244680, train_perplexity=665.10516, train_loss=6.499945

Batch 244690, train_perplexity=710.8209, train_loss=6.5664206

Batch 244700, train_perplexity=614.40936, train_loss=6.4206614

Batch 244710, train_perplexity=645.6372, train_loss=6.4702377

Batch 244720, train_perplexity=606.2234, train_loss=6.4072485

Batch 244730, train_perplexity=642.5156, train_loss=6.465391

Batch 244740, train_perplexity=624.4823, train_loss=6.436923

Batch 244750, train_perplexity=665.3653, train_loss=6.500336

Batch 244760, train_perplexity=666.4878, train_loss=6.502022

Batch 244770, train_perplexity=700.035, train_loss=6.5511303

Batch 244780, train_perplexity=560.2614, train_loss=6.3284035

Batch 244790, train_perplexity=663.06555, train_loss=6.496874

Batch 244800, train_perplexity=664.85944, train_loss=6.4995756

Batch 244810, train_perplexity=636.3584, train_loss=6.455762

Batch 244820, train_perplexity=537.72864, train_loss=6.287354

Batch 244830, train_perplexity=619.8312, train_loss=6.429447

Batch 244840, train_perplexity=582.73694, train_loss=6.367736

Batch 244850, train_perplexity=639.9015, train_loss=6.461314

Batch 244860, train_perplexity=618.44244, train_loss=6.427204

Batch 244870, train_perplexity=681.33136, train_loss=6.524049

Batch 244880, train_perplexity=585.78253, train_loss=6.3729486

Batch 244890, train_perplexity=600.03723, train_loss=6.3969917

Batch 244900, train_perplexity=635.0301, train_loss=6.4536724

Batch 244910, train_perplexity=669.4981, train_loss=6.5065284

Batch 244920, train_perplexity=630.76495, train_loss=6.4469333

Batch 244930, train_perplexity=662.0716, train_loss=6.4953737

Batch 244940, train_perplexity=623.2906, train_loss=6.435013

Batch 244950, train_perplexity=587.6413, train_loss=6.3761168

Batch 244960, train_perplexity=668.2523, train_loss=6.504666

Batch 244970, train_perplexity=617.4059, train_loss=6.4255266

Batch 244980, train_perplexity=606.09155, train_loss=6.407031

Batch 244990, train_perplexity=697.59015, train_loss=6.5476317

Batch 245000, train_perplexity=691.52313, train_loss=6.5388966

Batch 245010, train_perplexity=649.8681, train_loss=6.4767694

Batch 245020, train_perplexity=650.50397, train_loss=6.4777474

Batch 245030, train_perplexity=656.16327, train_loss=6.4864097

Batch 245040, train_perplexity=650.19696, train_loss=6.4772754

Batch 245050, train_perplexity=636.19244, train_loss=6.455501

Batch 245060, train_perplexity=616.0931, train_loss=6.423398

Batch 245070, train_perplexity=604.05975, train_loss=6.403673

Batch 245080, train_perplexity=649.1471, train_loss=6.4756594

Batch 245090, train_perplexity=665.7661, train_loss=6.5009384

Batch 245100, train_perplexity=695.44495, train_loss=6.544552

Batch 245110, train_perplexity=684.45544, train_loss=6.5286236

Batch 245120, train_perplexity=688.92566, train_loss=6.5351334

Batch 245130, train_perplexity=627.96704, train_loss=6.4424877

Batch 245140, train_perplexity=569.2965, train_loss=6.3444014

Batch 245150, train_perplexity=639.3171, train_loss=6.4604006

Batch 245160, train_perplexity=638.48663, train_loss=6.4591007

Batch 245170, train_perplexity=649.0035, train_loss=6.475438

Batch 245180, train_perplexity=635.6136, train_loss=6.454591

Batch 245190, train_perplexity=598.7842, train_loss=6.3949013

Batch 245200, train_perplexity=663.84, train_loss=6.498041

Batch 245210, train_perplexity=658.6815, train_loss=6.49024

Batch 245220, train_perplexity=629.61316, train_loss=6.4451056

Batch 245230, train_perplexity=620.8926, train_loss=6.431158

Batch 245240, train_perplexity=790.3258, train_loss=6.6724453

Batch 245250, train_perplexity=592.1681, train_loss=6.3837905

Batch 245260, train_perplexity=606.81104, train_loss=6.4082174

Batch 245270, train_perplexity=630.31726, train_loss=6.4462233

Batch 245280, train_perplexity=659.63824, train_loss=6.4916916

Batch 245290, train_perplexity=678.31244, train_loss=6.519608

Batch 245300, train_perplexity=768.97626, train_loss=6.64506

Batch 245310, train_perplexity=790.6809, train_loss=6.6728945

Batch 245320, train_perplexity=638.2273, train_loss=6.4586945

Batch 245330, train_perplexity=638.89594, train_loss=6.4597416

Batch 245340, train_perplexity=609.1372, train_loss=6.4120436

Batch 245350, train_perplexity=654.3217, train_loss=6.483599

Batch 245360, train_perplexity=643.95416, train_loss=6.4676275

Batch 245370, train_perplexity=699.85675, train_loss=6.5508757

Batch 245380, train_perplexity=648.49, train_loss=6.4746466

Batch 245390, train_perplexity=639.5226, train_loss=6.460722

Batch 245400, train_perplexity=713.0183, train_loss=6.569507

Batch 245410, train_perplexity=638.6401, train_loss=6.459341

Batch 245420, train_perplexity=700.2433, train_loss=6.551428

Batch 245430, train_perplexity=677.928, train_loss=6.519041

Batch 245440, train_perplexity=607.21625, train_loss=6.408885

Batch 245450, train_perplexity=640.78876, train_loss=6.4627

Batch 245460, train_perplexity=682.8044, train_loss=6.5262084

Batch 245470, train_perplexity=647.51294, train_loss=6.473139

Batch 245480, train_perplexity=745.8502, train_loss=6.614525
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 245490, train_perplexity=625.03107, train_loss=6.4378014

Batch 245500, train_perplexity=642.4893, train_loss=6.46535

Batch 245510, train_perplexity=639.55554, train_loss=6.4607735

Batch 245520, train_perplexity=684.47437, train_loss=6.528651

Batch 245530, train_perplexity=683.29974, train_loss=6.5269337

Batch 245540, train_perplexity=575.44244, train_loss=6.3551393

Batch 245550, train_perplexity=679.7945, train_loss=6.5217905

Batch 245560, train_perplexity=623.37646, train_loss=6.4351506

Batch 245570, train_perplexity=605.75146, train_loss=6.40647

Batch 245580, train_perplexity=586.3697, train_loss=6.3739505

Batch 245590, train_perplexity=563.6764, train_loss=6.3344803

Batch 245600, train_perplexity=758.3055, train_loss=6.6310863

Batch 245610, train_perplexity=666.5208, train_loss=6.5020714

Batch 245620, train_perplexity=664.82074, train_loss=6.4995174

Batch 245630, train_perplexity=623.1815, train_loss=6.434838

Batch 245640, train_perplexity=657.6013, train_loss=6.488599

Batch 245650, train_perplexity=629.3268, train_loss=6.4446507

Batch 245660, train_perplexity=644.2865, train_loss=6.4681435

Batch 245670, train_perplexity=549.1788, train_loss=6.308424

Batch 245680, train_perplexity=649.6407, train_loss=6.4764194

Batch 245690, train_perplexity=614.5925, train_loss=6.4209595

Batch 245700, train_perplexity=596.52686, train_loss=6.3911242

Batch 245710, train_perplexity=710.4807, train_loss=6.565942

Batch 245720, train_perplexity=582.49774, train_loss=6.3673253

Batch 245730, train_perplexity=655.9005, train_loss=6.486009

Batch 245740, train_perplexity=577.41846, train_loss=6.358567

Batch 245750, train_perplexity=703.9433, train_loss=6.556698

Batch 245760, train_perplexity=683.60614, train_loss=6.527382

Batch 245770, train_perplexity=632.0328, train_loss=6.448941

Batch 245780, train_perplexity=596.5786, train_loss=6.391211

Batch 245790, train_perplexity=654.48834, train_loss=6.483854

Batch 245800, train_perplexity=582.1648, train_loss=6.3667536

Batch 245810, train_perplexity=655.49, train_loss=6.485383

Batch 245820, train_perplexity=665.42303, train_loss=6.500423

Batch 245830, train_perplexity=755.3996, train_loss=6.627247

Batch 245840, train_perplexity=693.51697, train_loss=6.5417757

Batch 245850, train_perplexity=712.14136, train_loss=6.5682764

Batch 245860, train_perplexity=613.14825, train_loss=6.4186068

Batch 245870, train_perplexity=635.044, train_loss=6.4536943

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Batch 245880, train_perplexity=716.3765, train_loss=6.574206

Batch 245890, train_perplexity=631.73083, train_loss=6.4484634

Batch 245900, train_perplexity=621.39374, train_loss=6.431965

Batch 245910, train_perplexity=645.362, train_loss=6.4698114

Batch 245920, train_perplexity=611.9647, train_loss=6.4166746

Batch 245930, train_perplexity=739.3737, train_loss=6.6058035

Batch 245940, train_perplexity=676.4839, train_loss=6.5169086

Batch 245950, train_perplexity=695.1618, train_loss=6.5441446

Batch 245960, train_perplexity=616.1025, train_loss=6.4234133

Batch 245970, train_perplexity=615.4938, train_loss=6.422425

Batch 245980, train_perplexity=660.3344, train_loss=6.4927464

Batch 245990, train_perplexity=674.3628, train_loss=6.513768

Batch 246000, train_perplexity=668.42413, train_loss=6.504923

Batch 246010, train_perplexity=674.79956, train_loss=6.5144157

Batch 246020, train_perplexity=679.79285, train_loss=6.521788

Batch 246030, train_perplexity=646.9377, train_loss=6.47225

Batch 246040, train_perplexity=644.8357, train_loss=6.4689956

Batch 246050, train_perplexity=635.4669, train_loss=6.45436

Batch 246060, train_perplexity=645.3196, train_loss=6.4697456

Batch 246070, train_perplexity=614.1938, train_loss=6.4203105

Batch 246080, train_perplexity=686.0232, train_loss=6.5309114

Batch 246090, train_perplexity=624.5931, train_loss=6.4371004

Batch 246100, train_perplexity=641.62195, train_loss=6.4639993

Batch 246110, train_perplexity=694.64124, train_loss=6.5433955

Batch 246120, train_perplexity=635.9228, train_loss=6.455077

Batch 246130, train_perplexity=676.0795, train_loss=6.5163107

Batch 246140, train_perplexity=580.7768, train_loss=6.3643665

Batch 246150, train_perplexity=589.9638, train_loss=6.380061

Batch 246160, train_perplexity=650.2835, train_loss=6.4774084

Batch 246170, train_perplexity=671.2105, train_loss=6.509083

Batch 246180, train_perplexity=711.20917, train_loss=6.5669665

Batch 246190, train_perplexity=687.0249, train_loss=6.5323706

Batch 246200, train_perplexity=603.8807, train_loss=6.4033766

Batch 246210, train_perplexity=673.0405, train_loss=6.5118055

Batch 246220, train_perplexity=725.73083, train_loss=6.587179

Batch 246230, train_perplexity=624.0504, train_loss=6.436231

Batch 246240, train_perplexity=702.7527, train_loss=6.555005

Batch 246250, train_perplexity=700.0143, train_loss=6.5511007

Batch 246260, train_perplexity=658.4943, train_loss=6.489956

Batch 246270, train_perplexity=649.39667, train_loss=6.4760437

Batch 246280, train_perplexity=720.5647, train_loss=6.580035

Batch 246290, train_perplexity=670.1778, train_loss=6.507543

Batch 246300, train_perplexity=622.472, train_loss=6.4336987

Batch 246310, train_perplexity=653.8037, train_loss=6.482807

Batch 246320, train_perplexity=664.0382, train_loss=6.4983397

Batch 246330, train_perplexity=644.5655, train_loss=6.4685764

Batch 246340, train_perplexity=688.3333, train_loss=6.534273

Batch 246350, train_perplexity=681.02673, train_loss=6.5236015

Batch 246360, train_perplexity=614.9296, train_loss=6.421508

Batch 246370, train_perplexity=630.057, train_loss=6.4458103

Batch 246380, train_perplexity=633.43994, train_loss=6.451165

Batch 246390, train_perplexity=655.125, train_loss=6.484826

Batch 246400, train_perplexity=637.95953, train_loss=6.458275

Batch 246410, train_perplexity=667.8449, train_loss=6.504056

Batch 246420, train_perplexity=592.7667, train_loss=6.384801

Batch 246430, train_perplexity=618.5091, train_loss=6.427312

Batch 246440, train_perplexity=676.1556, train_loss=6.516423

Batch 246450, train_perplexity=575.0187, train_loss=6.3544025

Batch 246460, train_perplexity=686.1979, train_loss=6.531166

Batch 246470, train_perplexity=677.4562, train_loss=6.518345

Batch 246480, train_perplexity=697.1475, train_loss=6.546997

Batch 246490, train_perplexity=738.9811, train_loss=6.6052723

Batch 246500, train_perplexity=647.84344, train_loss=6.473649

Batch 246510, train_perplexity=671.6491, train_loss=6.509736

Batch 246520, train_perplexity=670.4575, train_loss=6.5079603

Batch 246530, train_perplexity=655.9058, train_loss=6.486017

Batch 246540, train_perplexity=675.4499, train_loss=6.515379

Batch 246550, train_perplexity=655.1909, train_loss=6.4849267

Batch 246560, train_perplexity=696.3342, train_loss=6.54583

Batch 246570, train_perplexity=639.1836, train_loss=6.4601917

Batch 246580, train_perplexity=638.42847, train_loss=6.4590096

Batch 246590, train_perplexity=645.688, train_loss=6.4703164

Batch 246600, train_perplexity=665.3624, train_loss=6.500332

Batch 246610, train_perplexity=644.3264, train_loss=6.4682055

Batch 246620, train_perplexity=642.1254, train_loss=6.4647837

Batch 246630, train_perplexity=661.245, train_loss=6.4941244

Batch 246640, train_perplexity=640.3819, train_loss=6.4620647

Batch 246650, train_perplexity=708.1861, train_loss=6.562707

Batch 246660, train_perplexity=590.37604, train_loss=6.3807597

Batch 246670, train_perplexity=603.6713, train_loss=6.40303

Batch 246680, train_perplexity=608.06464, train_loss=6.410281

Batch 246690, train_perplexity=638.1536, train_loss=6.458579

Batch 246700, train_perplexity=621.3451, train_loss=6.4318867

Batch 246710, train_perplexity=610.5365, train_loss=6.414338

Batch 246720, train_perplexity=661.2683, train_loss=6.4941597

Batch 246730, train_perplexity=660.7839, train_loss=6.493427
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 246740, train_perplexity=663.156, train_loss=6.49701

Batch 246750, train_perplexity=677.53406, train_loss=6.51846

Batch 246760, train_perplexity=613.4416, train_loss=6.419085

Batch 246770, train_perplexity=655.0363, train_loss=6.4846907

Batch 246780, train_perplexity=616.80206, train_loss=6.424548

Batch 246790, train_perplexity=612.9021, train_loss=6.4182053

Batch 246800, train_perplexity=676.8543, train_loss=6.517456

Batch 246810, train_perplexity=640.91284, train_loss=6.4628935

Batch 246820, train_perplexity=657.9021, train_loss=6.489056

Batch 246830, train_perplexity=644.1215, train_loss=6.4678874

Batch 246840, train_perplexity=587.42786, train_loss=6.3757534

Batch 246850, train_perplexity=666.6305, train_loss=6.502236

Batch 246860, train_perplexity=588.5303, train_loss=6.3776283

Batch 246870, train_perplexity=630.0564, train_loss=6.4458094

Batch 246880, train_perplexity=710.86707, train_loss=6.5664854

Batch 246890, train_perplexity=765.84937, train_loss=6.6409855

Batch 246900, train_perplexity=714.82086, train_loss=6.572032

Batch 246910, train_perplexity=705.0095, train_loss=6.5582113

Batch 246920, train_perplexity=629.313, train_loss=6.4446287

Batch 246930, train_perplexity=691.001, train_loss=6.5381413

Batch 246940, train_perplexity=653.732, train_loss=6.4826975

Batch 246950, train_perplexity=703.20996, train_loss=6.5556555

Batch 246960, train_perplexity=694.4194, train_loss=6.543076

Batch 246970, train_perplexity=638.89105, train_loss=6.459734

Batch 246980, train_perplexity=657.395, train_loss=6.488285

Batch 246990, train_perplexity=632.7366, train_loss=6.450054

Batch 247000, train_perplexity=556.3268, train_loss=6.321356

Batch 247010, train_perplexity=579.7912, train_loss=6.362668

Batch 247020, train_perplexity=673.92816, train_loss=6.5131235

Batch 247030, train_perplexity=751.7362, train_loss=6.6223855

Batch 247040, train_perplexity=618.2484, train_loss=6.4268904

Batch 247050, train_perplexity=753.75885, train_loss=6.6250725

Batch 247060, train_perplexity=633.74084, train_loss=6.45164

Batch 247070, train_perplexity=669.9219, train_loss=6.507161

Batch 247080, train_perplexity=684.97723, train_loss=6.5293856

Batch 247090, train_perplexity=653.2123, train_loss=6.481902

Batch 247100, train_perplexity=663.71277, train_loss=6.4978495

Batch 247110, train_perplexity=590.18604, train_loss=6.380438

Batch 247120, train_perplexity=604.77716, train_loss=6.40486

Batch 247130, train_perplexity=618.3118, train_loss=6.426993

Batch 247140, train_perplexity=699.233, train_loss=6.549984

Batch 247150, train_perplexity=745.558, train_loss=6.614133

Batch 247160, train_perplexity=672.4945, train_loss=6.510994

Batch 247170, train_perplexity=589.9891, train_loss=6.380104

Batch 247180, train_perplexity=633.0263, train_loss=6.450512

Batch 247190, train_perplexity=672.65936, train_loss=6.511239

Batch 247200, train_perplexity=651.50885, train_loss=6.479291

Batch 247210, train_perplexity=672.81396, train_loss=6.511469

Batch 247220, train_perplexity=648.56946, train_loss=6.474769

Batch 247230, train_perplexity=738.9726, train_loss=6.605261

Batch 247240, train_perplexity=623.9132, train_loss=6.4360113

Batch 247250, train_perplexity=690.6353, train_loss=6.537612

Batch 247260, train_perplexity=695.4509, train_loss=6.5445604

Batch 247270, train_perplexity=620.1978, train_loss=6.4300385

Batch 247280, train_perplexity=637.8366, train_loss=6.458082

Batch 247290, train_perplexity=634.1577, train_loss=6.4522977

Batch 247300, train_perplexity=672.2406, train_loss=6.5106163

Batch 247310, train_perplexity=641.89984, train_loss=6.4644322

Batch 247320, train_perplexity=657.28845, train_loss=6.488123

Batch 247330, train_perplexity=610.33215, train_loss=6.4140034

Batch 247340, train_perplexity=631.87335, train_loss=6.448689

Batch 247350, train_perplexity=581.4956, train_loss=6.3656034

Batch 247360, train_perplexity=688.9848, train_loss=6.535219

Batch 247370, train_perplexity=600.9134, train_loss=6.398451

Batch 247380, train_perplexity=614.44073, train_loss=6.4207125

Batch 247390, train_perplexity=712.7623, train_loss=6.569148

Batch 247400, train_perplexity=601.18054, train_loss=6.3988953

Batch 247410, train_perplexity=615.90216, train_loss=6.423088

Batch 247420, train_perplexity=682.70966, train_loss=6.5260696

Batch 247430, train_perplexity=643.69226, train_loss=6.467221

Batch 247440, train_perplexity=720.7664, train_loss=6.580315

Batch 247450, train_perplexity=670.0075, train_loss=6.507289

Batch 247460, train_perplexity=685.2732, train_loss=6.5298176

Batch 247470, train_perplexity=689.1202, train_loss=6.5354156

Batch 247480, train_perplexity=634.22516, train_loss=6.452404

Batch 247490, train_perplexity=682.46814, train_loss=6.525716

Batch 247500, train_perplexity=652.05524, train_loss=6.4801292

Batch 247510, train_perplexity=605.25977, train_loss=6.405658

Batch 247520, train_perplexity=607.882, train_loss=6.409981

Batch 247530, train_perplexity=662.6537, train_loss=6.4962525

Batch 247540, train_perplexity=679.83405, train_loss=6.5218487

Batch 247550, train_perplexity=645.4125, train_loss=6.4698896

Batch 247560, train_perplexity=736.76935, train_loss=6.602275

Batch 247570, train_perplexity=722.20685, train_loss=6.5823116

Batch 247580, train_perplexity=644.7696, train_loss=6.468893

Batch 247590, train_perplexity=666.32635, train_loss=6.5017796

Batch 247600, train_perplexity=684.571, train_loss=6.5287924

Batch 247610, train_perplexity=602.93115, train_loss=6.401803

Batch 247620, train_perplexity=658.6526, train_loss=6.490196

Batch 247630, train_perplexity=641.5519, train_loss=6.46389

Batch 247640, train_perplexity=615.90155, train_loss=6.423087

Batch 247650, train_perplexity=698.06696, train_loss=6.548315

Batch 247660, train_perplexity=691.9183, train_loss=6.539468

Batch 247670, train_perplexity=695.2649, train_loss=6.544293

Batch 247680, train_perplexity=666.3775, train_loss=6.5018563

Batch 247690, train_perplexity=673.7187, train_loss=6.5128126

Batch 247700, train_perplexity=687.9615, train_loss=6.533733

Batch 247710, train_perplexity=720.83826, train_loss=6.580415

Batch 247720, train_perplexity=657.6976, train_loss=6.488745

Batch 247730, train_perplexity=597.7891, train_loss=6.393238

Batch 247740, train_perplexity=654.6423, train_loss=6.484089

Batch 247750, train_perplexity=637.85547, train_loss=6.458112

Batch 247760, train_perplexity=693.9126, train_loss=6.542346

Batch 247770, train_perplexity=694.5227, train_loss=6.543225

Batch 247780, train_perplexity=683.2496, train_loss=6.52686

Batch 247790, train_perplexity=679.6117, train_loss=6.5215216

Batch 247800, train_perplexity=626.49677, train_loss=6.4401436

Batch 247810, train_perplexity=632.7616, train_loss=6.4500937

Batch 247820, train_perplexity=661.5137, train_loss=6.4945307

Batch 247830, train_perplexity=611.39685, train_loss=6.415746

Batch 247840, train_perplexity=641.7826, train_loss=6.4642496

Batch 247850, train_perplexity=671.8477, train_loss=6.5100317

Batch 247860, train_perplexity=672.4551, train_loss=6.5109353

Batch 247870, train_perplexity=638.7796, train_loss=6.4595594

Batch 247880, train_perplexity=597.2207, train_loss=6.392287

Batch 247890, train_perplexity=607.77826, train_loss=6.40981

Batch 247900, train_perplexity=578.5578, train_loss=6.3605385

Batch 247910, train_perplexity=635.65906, train_loss=6.4546623

Batch 247920, train_perplexity=702.9853, train_loss=6.555336

Batch 247930, train_perplexity=683.9645, train_loss=6.527906

Batch 247940, train_perplexity=590.38367, train_loss=6.3807726

Batch 247950, train_perplexity=681.6505, train_loss=6.524517

Batch 247960, train_perplexity=642.089, train_loss=6.464727

Batch 247970, train_perplexity=680.06683, train_loss=6.522191

Batch 247980, train_perplexity=662.4673, train_loss=6.495971

Batch 247990, train_perplexity=624.08044, train_loss=6.4362793

Batch 248000, train_perplexity=649.62085, train_loss=6.476389

Batch 248010, train_perplexity=641.63696, train_loss=6.4640226

Batch 248020, train_perplexity=591.73425, train_loss=6.3830576

Batch 248030, train_perplexity=593.48706, train_loss=6.3860154

Batch 248040, train_perplexity=572.9744, train_loss=6.350841
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 248050, train_perplexity=615.0223, train_loss=6.4216585

Batch 248060, train_perplexity=637.46936, train_loss=6.457506

Batch 248070, train_perplexity=581.69696, train_loss=6.3659496

Batch 248080, train_perplexity=696.4621, train_loss=6.5460134

Batch 248090, train_perplexity=674.4908, train_loss=6.513958

Batch 248100, train_perplexity=677.00793, train_loss=6.517683

Batch 248110, train_perplexity=653.25464, train_loss=6.481967

Batch 248120, train_perplexity=581.60266, train_loss=6.3657875

Batch 248130, train_perplexity=619.5203, train_loss=6.4289455

Batch 248140, train_perplexity=752.751, train_loss=6.6237345

Batch 248150, train_perplexity=681.50714, train_loss=6.524307

Batch 248160, train_perplexity=664.39166, train_loss=6.498872

Batch 248170, train_perplexity=649.3716, train_loss=6.476005

Batch 248180, train_perplexity=581.99384, train_loss=6.36646

Batch 248190, train_perplexity=615.0601, train_loss=6.42172

Batch 248200, train_perplexity=659.0937, train_loss=6.4908657

Batch 248210, train_perplexity=651.70337, train_loss=6.4795895

Batch 248220, train_perplexity=654.2865, train_loss=6.4835453

Batch 248230, train_perplexity=623.79065, train_loss=6.435815

Batch 248240, train_perplexity=607.1201, train_loss=6.4087267

Batch 248250, train_perplexity=639.93506, train_loss=6.4613667

Batch 248260, train_perplexity=635.6221, train_loss=6.454604

Batch 248270, train_perplexity=663.25214, train_loss=6.497155

Batch 248280, train_perplexity=692.7132, train_loss=6.540616

Batch 248290, train_perplexity=604.20264, train_loss=6.4039097

Batch 248300, train_perplexity=674.5313, train_loss=6.514018

Batch 248310, train_perplexity=591.87756, train_loss=6.3833

Batch 248320, train_perplexity=685.27386, train_loss=6.5298185

Batch 248330, train_perplexity=607.642, train_loss=6.409586

Batch 248340, train_perplexity=697.5073, train_loss=6.547513

Batch 248350, train_perplexity=636.6355, train_loss=6.4561973

Batch 248360, train_perplexity=607.88605, train_loss=6.4099874

Batch 248370, train_perplexity=648.9336, train_loss=6.4753304

Batch 248380, train_perplexity=679.51605, train_loss=6.521381

Batch 248390, train_perplexity=608.71967, train_loss=6.411358

Batch 248400, train_perplexity=576.7415, train_loss=6.357394

Batch 248410, train_perplexity=706.4094, train_loss=6.560195

Batch 248420, train_perplexity=705.218, train_loss=6.558507

Batch 248430, train_perplexity=654.8121, train_loss=6.4843483

Batch 248440, train_perplexity=676.99506, train_loss=6.517664

Batch 248450, train_perplexity=639.3708, train_loss=6.4604845

Batch 248460, train_perplexity=585.9681, train_loss=6.3732653

Batch 248470, train_perplexity=620.144, train_loss=6.4299517

Batch 248480, train_perplexity=641.5029, train_loss=6.463814

Batch 248490, train_perplexity=636.2179, train_loss=6.455541

Batch 248500, train_perplexity=711.8789, train_loss=6.567908

Batch 248510, train_perplexity=664.0892, train_loss=6.4984164

Batch 248520, train_perplexity=622.5355, train_loss=6.4338007

Batch 248530, train_perplexity=633.8763, train_loss=6.4518538

Batch 248540, train_perplexity=630.38214, train_loss=6.4463263

Batch 248550, train_perplexity=599.35236, train_loss=6.3958497

Batch 248560, train_perplexity=656.7972, train_loss=6.4873753

Batch 248570, train_perplexity=638.2419, train_loss=6.4587173

Batch 248580, train_perplexity=668.4197, train_loss=6.504916

Batch 248590, train_perplexity=614.41144, train_loss=6.420665

Batch 248600, train_perplexity=629.84735, train_loss=6.4454775

Batch 248610, train_perplexity=588.32654, train_loss=6.377282

Batch 248620, train_perplexity=674.53577, train_loss=6.5140247

Batch 248630, train_perplexity=637.88104, train_loss=6.458152

Batch 248640, train_perplexity=605.9153, train_loss=6.40674

Batch 248650, train_perplexity=699.7413, train_loss=6.5507107

Batch 248660, train_perplexity=637.8424, train_loss=6.4580913

Batch 248670, train_perplexity=658.3643, train_loss=6.4897585

Batch 248680, train_perplexity=702.6187, train_loss=6.5548143

Batch 248690, train_perplexity=665.83405, train_loss=6.5010405

Batch 248700, train_perplexity=586.911, train_loss=6.374873

Batch 248710, train_perplexity=730.6106, train_loss=6.5938807

Batch 248720, train_perplexity=673.13104, train_loss=6.51194

Batch 248730, train_perplexity=671.4916, train_loss=6.5095015

Batch 248740, train_perplexity=655.8001, train_loss=6.485856

Batch 248750, train_perplexity=637.9726, train_loss=6.4582953

Batch 248760, train_perplexity=588.6195, train_loss=6.37778

Batch 248770, train_perplexity=572.3772, train_loss=6.349798

Batch 248780, train_perplexity=707.76074, train_loss=6.562106

Batch 248790, train_perplexity=625.6677, train_loss=6.4388194

Batch 248800, train_perplexity=718.85394, train_loss=6.577658

Batch 248810, train_perplexity=660.7281, train_loss=6.4933424

Batch 248820, train_perplexity=713.42, train_loss=6.5700703

Batch 248830, train_perplexity=658.1239, train_loss=6.489393

Batch 248840, train_perplexity=661.4979, train_loss=6.494507

Batch 248850, train_perplexity=674.0744, train_loss=6.5133405

Batch 248860, train_perplexity=664.0718, train_loss=6.49839

Batch 248870, train_perplexity=637.8683, train_loss=6.458132

Batch 248880, train_perplexity=641.9038, train_loss=6.4644384

Batch 248890, train_perplexity=613.5074, train_loss=6.4191923

Batch 248900, train_perplexity=620.5504, train_loss=6.430607

Batch 248910, train_perplexity=641.9044, train_loss=6.4644394

Batch 248920, train_perplexity=616.36426, train_loss=6.423838

Batch 248930, train_perplexity=660.48303, train_loss=6.4929714

Batch 248940, train_perplexity=724.9951, train_loss=6.586165

Batch 248950, train_perplexity=602.9286, train_loss=6.4017987

Batch 248960, train_perplexity=668.2756, train_loss=6.5047007

Batch 248970, train_perplexity=621.4607, train_loss=6.4320726

Batch 248980, train_perplexity=615.80347, train_loss=6.422928

Batch 248990, train_perplexity=631.02997, train_loss=6.4473534

Batch 249000, train_perplexity=678.63434, train_loss=6.5200825

Batch 249010, train_perplexity=694.88477, train_loss=6.543746

Batch 249020, train_perplexity=579.4711, train_loss=6.362116

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Batch 249030, train_perplexity=660.0844, train_loss=6.4923677

Batch 249040, train_perplexity=621.71735, train_loss=6.4324856

Batch 249050, train_perplexity=664.12463, train_loss=6.49847

Batch 249060, train_perplexity=653.41724, train_loss=6.482216

Batch 249070, train_perplexity=651.2554, train_loss=6.478902

Batch 249080, train_perplexity=611.2327, train_loss=6.4154778

Batch 249090, train_perplexity=669.5297, train_loss=6.5065756

Batch 249100, train_perplexity=719.83014, train_loss=6.5790153

Batch 249110, train_perplexity=695.73883, train_loss=6.5449743

Batch 249120, train_perplexity=678.9431, train_loss=6.5205374

Batch 249130, train_perplexity=625.6155, train_loss=6.438736

Batch 249140, train_perplexity=692.29346, train_loss=6.54001

Batch 249150, train_perplexity=725.1739, train_loss=6.5864115

Batch 249160, train_perplexity=671.5415, train_loss=6.509576

Batch 249170, train_perplexity=703.5185, train_loss=6.556094

Batch 249180, train_perplexity=675.638, train_loss=6.5156574

Batch 249190, train_perplexity=618.63354, train_loss=6.427513

Batch 249200, train_perplexity=660.20026, train_loss=6.492543

Batch 249210, train_perplexity=582.212, train_loss=6.3668346

Batch 249220, train_perplexity=645.61383, train_loss=6.4702015

Batch 249230, train_perplexity=593.83923, train_loss=6.3866086

Batch 249240, train_perplexity=630.68646, train_loss=6.446809

Batch 249250, train_perplexity=604.23035, train_loss=6.4039555

Batch 249260, train_perplexity=696.89294, train_loss=6.546632

Batch 249270, train_perplexity=692.0106, train_loss=6.5396013

Batch 249280, train_perplexity=659.15344, train_loss=6.4909563

Batch 249290, train_perplexity=579.6049, train_loss=6.3623466
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 249300, train_perplexity=681.1657, train_loss=6.5238056

Batch 249310, train_perplexity=733.0569, train_loss=6.5972233

Batch 249320, train_perplexity=593.25195, train_loss=6.385619

Batch 249330, train_perplexity=674.5873, train_loss=6.514101

Batch 249340, train_perplexity=628.1153, train_loss=6.4427238

Batch 249350, train_perplexity=641.20996, train_loss=6.463357

Batch 249360, train_perplexity=685.7998, train_loss=6.530586

Batch 249370, train_perplexity=648.82465, train_loss=6.4751625

Batch 249380, train_perplexity=672.47754, train_loss=6.5109687

Batch 249390, train_perplexity=604.5191, train_loss=6.4044333

Batch 249400, train_perplexity=655.9787, train_loss=6.4861283

Batch 249410, train_perplexity=570.558, train_loss=6.346615

Batch 249420, train_perplexity=703.67755, train_loss=6.55632

Batch 249430, train_perplexity=587.4427, train_loss=6.3757787

Batch 249440, train_perplexity=621.71027, train_loss=6.432474

Batch 249450, train_perplexity=606.07367, train_loss=6.4070015

Batch 249460, train_perplexity=643.4164, train_loss=6.466792

Batch 249470, train_perplexity=697.13525, train_loss=6.5469794

Batch 249480, train_perplexity=634.12415, train_loss=6.4522448

Batch 249490, train_perplexity=696.7355, train_loss=6.546406

Batch 249500, train_perplexity=699.5651, train_loss=6.550459

Batch 249510, train_perplexity=630.64825, train_loss=6.4467483

Batch 249520, train_perplexity=615.67957, train_loss=6.4227266

Batch 249530, train_perplexity=585.54156, train_loss=6.372537

Batch 249540, train_perplexity=647.8675, train_loss=6.473686

Batch 249550, train_perplexity=646.8118, train_loss=6.4720554

Batch 249560, train_perplexity=668.54205, train_loss=6.5050993

Batch 249570, train_perplexity=657.0932, train_loss=6.487826

Batch 249580, train_perplexity=625.30054, train_loss=6.4382324

Batch 249590, train_perplexity=641.7263, train_loss=6.464162

Batch 249600, train_perplexity=622.96106, train_loss=6.434484

Batch 249610, train_perplexity=694.6906, train_loss=6.5434666

Batch 249620, train_perplexity=669.7322, train_loss=6.506878

Batch 249630, train_perplexity=655.5919, train_loss=6.4855385

Batch 249640, train_perplexity=625.5105, train_loss=6.438568

Batch 249650, train_perplexity=690.55304, train_loss=6.5374928

Batch 249660, train_perplexity=674.34735, train_loss=6.5137453

Batch 249670, train_perplexity=621.5075, train_loss=6.432148

Batch 249680, train_perplexity=553.8768, train_loss=6.316942

Batch 249690, train_perplexity=634.5582, train_loss=6.452929

Batch 249700, train_perplexity=630.04047, train_loss=6.445784

Batch 249710, train_perplexity=634.41815, train_loss=6.4527082

Batch 249720, train_perplexity=623.12415, train_loss=6.434746

Batch 249730, train_perplexity=599.67883, train_loss=6.3963943

Batch 249740, train_perplexity=626.2835, train_loss=6.439803

Batch 249750, train_perplexity=677.80646, train_loss=6.518862

Batch 249760, train_perplexity=608.4147, train_loss=6.4108567

Batch 249770, train_perplexity=649.3747, train_loss=6.47601

Batch 249780, train_perplexity=656.1442, train_loss=6.4863806

Batch 249790, train_perplexity=619.3638, train_loss=6.428693

Batch 249800, train_perplexity=654.1673, train_loss=6.483363

Batch 249810, train_perplexity=666.74207, train_loss=6.5024033

Batch 249820, train_perplexity=600.3778, train_loss=6.397559

Batch 249830, train_perplexity=600.39215, train_loss=6.397583

Batch 249840, train_perplexity=651.7242, train_loss=6.4796214

Batch 249850, train_perplexity=653.3263, train_loss=6.4820766

Batch 249860, train_perplexity=592.9861, train_loss=6.385171

Batch 249870, train_perplexity=659.11945, train_loss=6.490905

Batch 249880, train_perplexity=748.2098, train_loss=6.6176834

Batch 249890, train_perplexity=658.4309, train_loss=6.4898596

Batch 249900, train_perplexity=646.76587, train_loss=6.4719844

Batch 249910, train_perplexity=622.54474, train_loss=6.4338155

Batch 249920, train_perplexity=681.58905, train_loss=6.524427

Batch 249930, train_perplexity=613.4015, train_loss=6.4190197

Batch 249940, train_perplexity=559.0109, train_loss=6.326169

Batch 249950, train_perplexity=655.09973, train_loss=6.4847875

Batch 249960, train_perplexity=620.8091, train_loss=6.4310236

Batch 249970, train_perplexity=598.8733, train_loss=6.39505

Batch 249980, train_perplexity=718.53485, train_loss=6.5772142

Batch 249990, train_perplexity=652.8694, train_loss=6.481377

Batch 250000, train_perplexity=667.8736, train_loss=6.504099

Batch 250010, train_perplexity=663.4982, train_loss=6.497526

Batch 250020, train_perplexity=660.17883, train_loss=6.492511

Batch 250030, train_perplexity=630.3362, train_loss=6.4462533

Batch 250040, train_perplexity=697.33374, train_loss=6.547264

Batch 250050, train_perplexity=644.51324, train_loss=6.4684954

Batch 250060, train_perplexity=627.31824, train_loss=6.441454

Batch 250070, train_perplexity=680.1203, train_loss=6.5222697

Batch 250080, train_perplexity=669.9053, train_loss=6.5071363

Batch 250090, train_perplexity=562.8918, train_loss=6.3330874

Batch 250100, train_perplexity=688.05304, train_loss=6.533866

Batch 250110, train_perplexity=680.349, train_loss=6.522606

Batch 250120, train_perplexity=643.5361, train_loss=6.466978

Batch 250130, train_perplexity=571.079, train_loss=6.3475275

Batch 250140, train_perplexity=697.9558, train_loss=6.548156

Batch 250150, train_perplexity=612.4931, train_loss=6.4175377

Batch 250160, train_perplexity=591.8189, train_loss=6.3832006

Batch 250170, train_perplexity=658.1195, train_loss=6.4893866

Batch 250180, train_perplexity=621.66815, train_loss=6.4324064

Batch 250190, train_perplexity=639.46466, train_loss=6.4606314

Batch 250200, train_perplexity=639.6159, train_loss=6.460868

Batch 250210, train_perplexity=627.1229, train_loss=6.4411426

Batch 250220, train_perplexity=658.25226, train_loss=6.4895883

Batch 250230, train_perplexity=621.79297, train_loss=6.432607

Batch 250240, train_perplexity=615.43884, train_loss=6.4223356

Batch 250250, train_perplexity=639.8954, train_loss=6.4613047

Batch 250260, train_perplexity=732.00964, train_loss=6.5957937

Batch 250270, train_perplexity=718.5335, train_loss=6.5772123

Batch 250280, train_perplexity=552.1517, train_loss=6.3138227

Batch 250290, train_perplexity=685.2402, train_loss=6.5297694

Batch 250300, train_perplexity=610.7642, train_loss=6.414711

Batch 250310, train_perplexity=632.7043, train_loss=6.450003

Batch 250320, train_perplexity=597.43066, train_loss=6.392638

Batch 250330, train_perplexity=691.6438, train_loss=6.539071

Batch 250340, train_perplexity=600.3658, train_loss=6.397539

Batch 250350, train_perplexity=707.478, train_loss=6.5617065

Batch 250360, train_perplexity=611.9542, train_loss=6.4166574

Batch 250370, train_perplexity=603.0033, train_loss=6.4019227

Batch 250380, train_perplexity=667.78314, train_loss=6.5039635

Batch 250390, train_perplexity=636.24854, train_loss=6.4555893

Batch 250400, train_perplexity=617.3252, train_loss=6.425396

Batch 250410, train_perplexity=647.97314, train_loss=6.4738493

Batch 250420, train_perplexity=639.7623, train_loss=6.461097

Batch 250430, train_perplexity=641.7232, train_loss=6.464157

Batch 250440, train_perplexity=613.16, train_loss=6.418626

Batch 250450, train_perplexity=623.3132, train_loss=6.435049

Batch 250460, train_perplexity=617.7566, train_loss=6.4260945

Batch 250470, train_perplexity=626.8703, train_loss=6.4407396

Batch 250480, train_perplexity=659.46027, train_loss=6.4914217

Batch 250490, train_perplexity=643.5532, train_loss=6.467005

Batch 250500, train_perplexity=599.5751, train_loss=6.396221

Batch 250510, train_perplexity=675.69855, train_loss=6.515747

Batch 250520, train_perplexity=676.04596, train_loss=6.516261

Batch 250530, train_perplexity=670.5093, train_loss=6.5080376

Batch 250540, train_perplexity=620.2919, train_loss=6.43019

Batch 250550, train_perplexity=658.78516, train_loss=6.4903975

Batch 250560, train_perplexity=620.3093, train_loss=6.430218

Batch 250570, train_perplexity=669.2958, train_loss=6.506226

Batch 250580, train_perplexity=651.3312, train_loss=6.479018

Batch 250590, train_perplexity=604.8957, train_loss=6.405056

Batch 250600, train_perplexity=624.94226, train_loss=6.4376593
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 250610, train_perplexity=707.5509, train_loss=6.5618095

Batch 250620, train_perplexity=606.7595, train_loss=6.4081326

Batch 250630, train_perplexity=659.6458, train_loss=6.491703

Batch 250640, train_perplexity=601.6893, train_loss=6.399741

Batch 250650, train_perplexity=614.7396, train_loss=6.421199

Batch 250660, train_perplexity=702.0788, train_loss=6.5540457

Batch 250670, train_perplexity=614.08923, train_loss=6.4201403

Batch 250680, train_perplexity=582.75775, train_loss=6.3677716

Batch 250690, train_perplexity=578.32776, train_loss=6.360141

Batch 250700, train_perplexity=628.5231, train_loss=6.4433727

Batch 250710, train_perplexity=616.6115, train_loss=6.424239

Batch 250720, train_perplexity=701.5206, train_loss=6.5532503

Batch 250730, train_perplexity=621.0661, train_loss=6.4314375

Batch 250740, train_perplexity=649.1267, train_loss=6.475628

Batch 250750, train_perplexity=656.78436, train_loss=6.4873557

Batch 250760, train_perplexity=544.8499, train_loss=6.3005104

Batch 250770, train_perplexity=614.63354, train_loss=6.421026

Batch 250780, train_perplexity=655.21564, train_loss=6.4849644

Batch 250790, train_perplexity=627.633, train_loss=6.4419556

Batch 250800, train_perplexity=681.02344, train_loss=6.523597

Batch 250810, train_perplexity=631.9381, train_loss=6.4487915

Batch 250820, train_perplexity=618.5221, train_loss=6.427333

Batch 250830, train_perplexity=638.0657, train_loss=6.4584413

Batch 250840, train_perplexity=585.69403, train_loss=6.3727975

Batch 250850, train_perplexity=545.6096, train_loss=6.3019037

Batch 250860, train_perplexity=614.5298, train_loss=6.4208574

Batch 250870, train_perplexity=655.1725, train_loss=6.4848986

Batch 250880, train_perplexity=614.2234, train_loss=6.4203587

Batch 250890, train_perplexity=615.7623, train_loss=6.422861

Batch 250900, train_perplexity=664.7434, train_loss=6.499401

Batch 250910, train_perplexity=661.3399, train_loss=6.494268

Batch 250920, train_perplexity=655.83575, train_loss=6.4859104

Batch 250930, train_perplexity=655.2487, train_loss=6.485015

Batch 250940, train_perplexity=752.8393, train_loss=6.623852

Batch 250950, train_perplexity=641.24664, train_loss=6.463414

Batch 250960, train_perplexity=621.10815, train_loss=6.431505

Batch 250970, train_perplexity=639.36896, train_loss=6.4604816

Batch 250980, train_perplexity=606.6557, train_loss=6.4079614

Batch 250990, train_perplexity=683.37213, train_loss=6.5270395

Batch 251000, train_perplexity=648.417, train_loss=6.474534

Batch 251010, train_perplexity=605.6097, train_loss=6.4062357

Batch 251020, train_perplexity=626.8078, train_loss=6.44064

Batch 251030, train_perplexity=636.3092, train_loss=6.4556847

Batch 251040, train_perplexity=663.75867, train_loss=6.4979186

Batch 251050, train_perplexity=677.0845, train_loss=6.517796

Batch 251060, train_perplexity=691.05304, train_loss=6.5382166

Batch 251070, train_perplexity=609.4998, train_loss=6.4126387

Batch 251080, train_perplexity=662.46796, train_loss=6.495972

Batch 251090, train_perplexity=699.64386, train_loss=6.5505714

Batch 251100, train_perplexity=562.19653, train_loss=6.3318515

Batch 251110, train_perplexity=626.00226, train_loss=6.439354

Batch 251120, train_perplexity=717.8171, train_loss=6.576215

Batch 251130, train_perplexity=590.2359, train_loss=6.3805223

Batch 251140, train_perplexity=655.58124, train_loss=6.4855223

Batch 251150, train_perplexity=642.48193, train_loss=6.4653387

Batch 251160, train_perplexity=570.30505, train_loss=6.3461714

Batch 251170, train_perplexity=652.3799, train_loss=6.480627

Batch 251180, train_perplexity=583.51105, train_loss=6.3690634

Batch 251190, train_perplexity=602.7144, train_loss=6.4014435

Batch 251200, train_perplexity=587.3898, train_loss=6.3756886

Batch 251210, train_perplexity=605.0307, train_loss=6.405279

Batch 251220, train_perplexity=719.5573, train_loss=6.578636

Batch 251230, train_perplexity=650.8478, train_loss=6.478276

Batch 251240, train_perplexity=681.5738, train_loss=6.5244045

Batch 251250, train_perplexity=540.0523, train_loss=6.291666

Batch 251260, train_perplexity=611.2126, train_loss=6.415445

Batch 251270, train_perplexity=598.20374, train_loss=6.3939314

Batch 251280, train_perplexity=608.80505, train_loss=6.411498

Batch 251290, train_perplexity=602.6604, train_loss=6.401354

Batch 251300, train_perplexity=536.83887, train_loss=6.285698

Batch 251310, train_perplexity=619.9778, train_loss=6.4296837

Batch 251320, train_perplexity=667.19714, train_loss=6.5030856

Batch 251330, train_perplexity=599.63336, train_loss=6.3963184

Batch 251340, train_perplexity=656.8388, train_loss=6.4874387

Batch 251350, train_perplexity=628.60516, train_loss=6.4435034

Batch 251360, train_perplexity=686.0674, train_loss=6.530976

Batch 251370, train_perplexity=655.60065, train_loss=6.485552

Batch 251380, train_perplexity=641.1494, train_loss=6.4632626

Batch 251390, train_perplexity=648.6669, train_loss=6.4749193

Batch 251400, train_perplexity=630.21265, train_loss=6.4460573

Batch 251410, train_perplexity=629.95966, train_loss=6.445656

Batch 251420, train_perplexity=630.98425, train_loss=6.447281

Batch 251430, train_perplexity=668.639, train_loss=6.5052443

Batch 251440, train_perplexity=676.7891, train_loss=6.5173597

Batch 251450, train_perplexity=647.8224, train_loss=6.4736166

Batch 251460, train_perplexity=531.9547, train_loss=6.2765584

Batch 251470, train_perplexity=693.29645, train_loss=6.5414577

Batch 251480, train_perplexity=629.13477, train_loss=6.4443455

Batch 251490, train_perplexity=681.308, train_loss=6.5240145

Batch 251500, train_perplexity=682.7839, train_loss=6.5261784

Batch 251510, train_perplexity=619.99615, train_loss=6.4297132

Batch 251520, train_perplexity=614.4466, train_loss=6.420722

Batch 251530, train_perplexity=606.3616, train_loss=6.4074764

Batch 251540, train_perplexity=631.33154, train_loss=6.447831

Batch 251550, train_perplexity=645.4667, train_loss=6.4699736

Batch 251560, train_perplexity=636.0939, train_loss=6.455346

Batch 251570, train_perplexity=629.18365, train_loss=6.444423

Batch 251580, train_perplexity=696.3827, train_loss=6.5458994

Batch 251590, train_perplexity=592.8679, train_loss=6.3849716

Batch 251600, train_perplexity=723.00146, train_loss=6.583411

Batch 251610, train_perplexity=647.8694, train_loss=6.473689

Batch 251620, train_perplexity=597.1572, train_loss=6.3921804

Batch 251630, train_perplexity=585.7669, train_loss=6.372922

Batch 251640, train_perplexity=590.41235, train_loss=6.380821

Batch 251650, train_perplexity=638.7942, train_loss=6.4595823

Batch 251660, train_perplexity=643.5981, train_loss=6.4670744

Batch 251670, train_perplexity=626.55115, train_loss=6.4402304

Batch 251680, train_perplexity=555.0239, train_loss=6.319011

Batch 251690, train_perplexity=658.6237, train_loss=6.4901524

Batch 251700, train_perplexity=614.64764, train_loss=6.421049

Batch 251710, train_perplexity=645.3202, train_loss=6.4697466

Batch 251720, train_perplexity=654.2297, train_loss=6.4834585

Batch 251730, train_perplexity=628.53143, train_loss=6.443386

Batch 251740, train_perplexity=628.7329, train_loss=6.4437065

Batch 251750, train_perplexity=673.0251, train_loss=6.5117826

Batch 251760, train_perplexity=656.5085, train_loss=6.4869356

Batch 251770, train_perplexity=691.60156, train_loss=6.53901

Batch 251780, train_perplexity=692.0139, train_loss=6.539606

Batch 251790, train_perplexity=657.0616, train_loss=6.4877777

Batch 251800, train_perplexity=600.73004, train_loss=6.3981457

Batch 251810, train_perplexity=657.0196, train_loss=6.487714

Batch 251820, train_perplexity=740.6744, train_loss=6.607561

Batch 251830, train_perplexity=671.4224, train_loss=6.5093985

Batch 251840, train_perplexity=654.3171, train_loss=6.483592

Batch 251850, train_perplexity=634.4199, train_loss=6.452711

Batch 251860, train_perplexity=585.2153, train_loss=6.3719797

Batch 251870, train_perplexity=647.9466, train_loss=6.4738083

Batch 251880, train_perplexity=688.40027, train_loss=6.5343704

Batch 251890, train_perplexity=633.0921, train_loss=6.450616

Batch 251900, train_perplexity=697.74615, train_loss=6.5478554

Batch 251910, train_perplexity=650.2965, train_loss=6.4774284
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 251920, train_perplexity=671.59467, train_loss=6.509655

Batch 251930, train_perplexity=581.51556, train_loss=6.365638

Batch 251940, train_perplexity=655.0263, train_loss=6.4846754

Batch 251950, train_perplexity=606.60706, train_loss=6.4078813

Batch 251960, train_perplexity=663.0902, train_loss=6.496911

Batch 251970, train_perplexity=665.7312, train_loss=6.500886

Batch 251980, train_perplexity=698.6607, train_loss=6.5491652

Batch 251990, train_perplexity=620.9305, train_loss=6.431219

Batch 252000, train_perplexity=673.0354, train_loss=6.511798

Batch 252010, train_perplexity=621.7802, train_loss=6.4325867

Batch 252020, train_perplexity=639.04584, train_loss=6.459976

Batch 252030, train_perplexity=646.18665, train_loss=6.4710884

Batch 252040, train_perplexity=619.68164, train_loss=6.429206

Batch 252050, train_perplexity=621.0424, train_loss=6.4313993

Batch 252060, train_perplexity=693.4122, train_loss=6.5416245

Batch 252070, train_perplexity=654.9735, train_loss=6.484595

Batch 252080, train_perplexity=619.9814, train_loss=6.4296894

Batch 252090, train_perplexity=685.2196, train_loss=6.5297394

Batch 252100, train_perplexity=607.17255, train_loss=6.408813

Batch 252110, train_perplexity=641.8897, train_loss=6.4644165

Batch 252120, train_perplexity=600.16144, train_loss=6.3971987

Batch 252130, train_perplexity=649.12604, train_loss=6.475627

Batch 252140, train_perplexity=615.09564, train_loss=6.4217777

Batch 252150, train_perplexity=650.24817, train_loss=6.477354

Batch 252160, train_perplexity=595.74286, train_loss=6.389809

Batch 252170, train_perplexity=646.3716, train_loss=6.4713745

Batch 252180, train_perplexity=710.118, train_loss=6.565431

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Batch 252190, train_perplexity=638.5597, train_loss=6.459215

Batch 252200, train_perplexity=626.5263, train_loss=6.440191

Batch 252210, train_perplexity=542.8073, train_loss=6.2967544

Batch 252220, train_perplexity=637.5414, train_loss=6.457619

Batch 252230, train_perplexity=645.6009, train_loss=6.4701815

Batch 252240, train_perplexity=653.2954, train_loss=6.4820294

Batch 252250, train_perplexity=618.1011, train_loss=6.426652

Batch 252260, train_perplexity=628.4553, train_loss=6.443265

Batch 252270, train_perplexity=637.50946, train_loss=6.457569

Batch 252280, train_perplexity=630.0423, train_loss=6.445787

Batch 252290, train_perplexity=698.88794, train_loss=6.5494905

Batch 252300, train_perplexity=626.63837, train_loss=6.4403696

Batch 252310, train_perplexity=631.57544, train_loss=6.4482174

Batch 252320, train_perplexity=632.3674, train_loss=6.4494705

Batch 252330, train_perplexity=663.7264, train_loss=6.49787

Batch 252340, train_perplexity=629.99335, train_loss=6.445709

Batch 252350, train_perplexity=625.4258, train_loss=6.4384327

Batch 252360, train_perplexity=670.1008, train_loss=6.507428

Batch 252370, train_perplexity=628.21954, train_loss=6.4428897

Batch 252380, train_perplexity=644.09296, train_loss=6.467843

Batch 252390, train_perplexity=642.8046, train_loss=6.465841

Batch 252400, train_perplexity=618.5191, train_loss=6.427328

Batch 252410, train_perplexity=652.4073, train_loss=6.480669

Batch 252420, train_perplexity=688.87146, train_loss=6.5350547

Batch 252430, train_perplexity=602.50867, train_loss=6.401102

Batch 252440, train_perplexity=738.0197, train_loss=6.6039705

Batch 252450, train_perplexity=627.8683, train_loss=6.4423304

Batch 252460, train_perplexity=599.36237, train_loss=6.3958664

Batch 252470, train_perplexity=642.42926, train_loss=6.4652567

Batch 252480, train_perplexity=643.3888, train_loss=6.466749

Batch 252490, train_perplexity=616.3284, train_loss=6.42378

Batch 252500, train_perplexity=668.6275, train_loss=6.505227

Batch 252510, train_perplexity=634.0833, train_loss=6.4521804

Batch 252520, train_perplexity=607.3854, train_loss=6.4091635

Batch 252530, train_perplexity=646.89514, train_loss=6.472184

Batch 252540, train_perplexity=658.5665, train_loss=6.4900656

Batch 252550, train_perplexity=685.71576, train_loss=6.530463

Batch 252560, train_perplexity=662.46826, train_loss=6.4959726

Batch 252570, train_perplexity=616.5586, train_loss=6.4241533

Batch 252580, train_perplexity=581.22064, train_loss=6.3651304

Batch 252590, train_perplexity=760.62103, train_loss=6.6341352

Batch 252600, train_perplexity=710.22327, train_loss=6.5655794

Batch 252610, train_perplexity=662.6518, train_loss=6.4962497

Batch 252620, train_perplexity=694.88574, train_loss=6.5437474

Batch 252630, train_perplexity=712.43616, train_loss=6.5686903

Batch 252640, train_perplexity=625.28625, train_loss=6.4382095

Batch 252650, train_perplexity=585.82166, train_loss=6.3730154

Batch 252660, train_perplexity=650.08136, train_loss=6.4770975

Batch 252670, train_perplexity=582.63135, train_loss=6.3675547

Batch 252680, train_perplexity=614.8575, train_loss=6.4213905

Batch 252690, train_perplexity=629.2707, train_loss=6.4445615

Batch 252700, train_perplexity=647.90643, train_loss=6.4737463

Batch 252710, train_perplexity=633.7052, train_loss=6.451584

Batch 252720, train_perplexity=687.5191, train_loss=6.5330896

Batch 252730, train_perplexity=625.2433, train_loss=6.438141

Batch 252740, train_perplexity=640.85724, train_loss=6.4628067

Batch 252750, train_perplexity=633.5233, train_loss=6.451297

Batch 252760, train_perplexity=682.5062, train_loss=6.5257716

Batch 252770, train_perplexity=635.30695, train_loss=6.454108

Batch 252780, train_perplexity=646.78345, train_loss=6.4720116

Batch 252790, train_perplexity=685.23334, train_loss=6.5297594

Batch 252800, train_perplexity=603.38617, train_loss=6.4025574

Batch 252810, train_perplexity=607.587, train_loss=6.4094954

Batch 252820, train_perplexity=593.4568, train_loss=6.3859644

Batch 252830, train_perplexity=677.04346, train_loss=6.5177355

Batch 252840, train_perplexity=616.4392, train_loss=6.4239597

Batch 252850, train_perplexity=669.83563, train_loss=6.5070324

Batch 252860, train_perplexity=612.81506, train_loss=6.418063

Batch 252870, train_perplexity=661.5452, train_loss=6.4945784

Batch 252880, train_perplexity=629.4991, train_loss=6.4449244

Batch 252890, train_perplexity=623.9695, train_loss=6.4361014

Batch 252900, train_perplexity=612.56116, train_loss=6.417649

Batch 252910, train_perplexity=654.75616, train_loss=6.484263

Batch 252920, train_perplexity=640.53156, train_loss=6.4622984

Batch 252930, train_perplexity=642.6281, train_loss=6.465566

Batch 252940, train_perplexity=693.4356, train_loss=6.5416584

Batch 252950, train_perplexity=652.2953, train_loss=6.4804974

Batch 252960, train_perplexity=654.7999, train_loss=6.4843297

Batch 252970, train_perplexity=646.1953, train_loss=6.4711018

Batch 252980, train_perplexity=669.0871, train_loss=6.505914

Batch 252990, train_perplexity=632.51276, train_loss=6.4497004

Batch 253000, train_perplexity=670.44885, train_loss=6.5079474

Batch 253010, train_perplexity=599.813, train_loss=6.396618

Batch 253020, train_perplexity=615.5281, train_loss=6.4224806

Batch 253030, train_perplexity=630.56494, train_loss=6.446616

Batch 253040, train_perplexity=655.60724, train_loss=6.485562

Batch 253050, train_perplexity=627.2883, train_loss=6.4414062

Batch 253060, train_perplexity=641.169, train_loss=6.463293

Batch 253070, train_perplexity=664.0597, train_loss=6.498372

Batch 253080, train_perplexity=618.0563, train_loss=6.4265795

Batch 253090, train_perplexity=687.9881, train_loss=6.5337715

Batch 253100, train_perplexity=605.876, train_loss=6.4066753

Batch 253110, train_perplexity=653.0279, train_loss=6.48162

Batch 253120, train_perplexity=660.5255, train_loss=6.493036

Batch 253130, train_perplexity=694.8897, train_loss=6.543753

Batch 253140, train_perplexity=686.1066, train_loss=6.531033

Batch 253150, train_perplexity=640.49615, train_loss=6.462243
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 253160, train_perplexity=740.5197, train_loss=6.6073523

Batch 253170, train_perplexity=690.3025, train_loss=6.53713

Batch 253180, train_perplexity=637.7591, train_loss=6.4579606

Batch 253190, train_perplexity=693.6949, train_loss=6.5420322

Batch 253200, train_perplexity=672.29126, train_loss=6.5106916

Batch 253210, train_perplexity=639.9988, train_loss=6.4614663

Batch 253220, train_perplexity=675.3761, train_loss=6.5152698

Batch 253230, train_perplexity=626.09656, train_loss=6.4395046

Batch 253240, train_perplexity=612.4394, train_loss=6.41745

Batch 253250, train_perplexity=635.00525, train_loss=6.4536333

Batch 253260, train_perplexity=598.226, train_loss=6.3939686

Batch 253270, train_perplexity=668.76904, train_loss=6.505439

Batch 253280, train_perplexity=672.4073, train_loss=6.5108643

Batch 253290, train_perplexity=635.51416, train_loss=6.4544344

Batch 253300, train_perplexity=633.8696, train_loss=6.4518433

Batch 253310, train_perplexity=658.5477, train_loss=6.490037

Batch 253320, train_perplexity=653.5805, train_loss=6.4824657

Batch 253330, train_perplexity=664.32544, train_loss=6.498772

Batch 253340, train_perplexity=709.32574, train_loss=6.564315

Batch 253350, train_perplexity=729.00494, train_loss=6.5916805

Batch 253360, train_perplexity=634.6006, train_loss=6.452996

Batch 253370, train_perplexity=692.1387, train_loss=6.5397863

Batch 253380, train_perplexity=612.9366, train_loss=6.4182615

Batch 253390, train_perplexity=599.02124, train_loss=6.395297

Batch 253400, train_perplexity=644.60114, train_loss=6.4686317

Batch 253410, train_perplexity=723.8335, train_loss=6.5845613

Batch 253420, train_perplexity=673.9815, train_loss=6.5132027

Batch 253430, train_perplexity=652.1709, train_loss=6.4803066

Batch 253440, train_perplexity=621.58813, train_loss=6.4322777

Batch 253450, train_perplexity=661.5011, train_loss=6.4945116

Batch 253460, train_perplexity=595.3229, train_loss=6.389104

Batch 253470, train_perplexity=620.5386, train_loss=6.430588

Batch 253480, train_perplexity=667.9424, train_loss=6.504202

Batch 253490, train_perplexity=669.0504, train_loss=6.5058594

Batch 253500, train_perplexity=633.7134, train_loss=6.4515967

Batch 253510, train_perplexity=664.1199, train_loss=6.4984627

Batch 253520, train_perplexity=641.46716, train_loss=6.463758

Batch 253530, train_perplexity=680.21375, train_loss=6.522407

Batch 253540, train_perplexity=606.9152, train_loss=6.408389

Batch 253550, train_perplexity=580.0412, train_loss=6.363099

Batch 253560, train_perplexity=652.5187, train_loss=6.4808397

Batch 253570, train_perplexity=619.0006, train_loss=6.4281063

Batch 253580, train_perplexity=664.2789, train_loss=6.498702

Batch 253590, train_perplexity=611.67883, train_loss=6.4162073

Batch 253600, train_perplexity=607.2704, train_loss=6.408974

Batch 253610, train_perplexity=633.872, train_loss=6.451847

Batch 253620, train_perplexity=585.0431, train_loss=6.3716855

Batch 253630, train_perplexity=611.7587, train_loss=6.416338

Batch 253640, train_perplexity=621.2269, train_loss=6.4316964

Batch 253650, train_perplexity=656.9005, train_loss=6.4875326

Batch 253660, train_perplexity=673.9134, train_loss=6.5131016

Batch 253670, train_perplexity=626.5529, train_loss=6.440233

Batch 253680, train_perplexity=692.9147, train_loss=6.540907

Batch 253690, train_perplexity=585.8211, train_loss=6.3730145

Batch 253700, train_perplexity=645.22784, train_loss=6.4696035

Batch 253710, train_perplexity=580.884, train_loss=6.364551

Batch 253720, train_perplexity=615.85425, train_loss=6.4230103

Batch 253730, train_perplexity=718.0307, train_loss=6.5765123

Batch 253740, train_perplexity=699.1873, train_loss=6.5499187

Batch 253750, train_perplexity=615.15607, train_loss=6.421876

Batch 253760, train_perplexity=583.8862, train_loss=6.369706

Batch 253770, train_perplexity=602.05493, train_loss=6.4003487

Batch 253780, train_perplexity=590.1627, train_loss=6.3803983

Batch 253790, train_perplexity=613.24097, train_loss=6.418758

Batch 253800, train_perplexity=707.5485, train_loss=6.561806

Batch 253810, train_perplexity=648.4077, train_loss=6.4745197

Batch 253820, train_perplexity=728.1927, train_loss=6.5905657

Batch 253830, train_perplexity=610.78925, train_loss=6.414752

Batch 253840, train_perplexity=635.92126, train_loss=6.455075

Batch 253850, train_perplexity=658.1076, train_loss=6.4893684

Batch 253860, train_perplexity=706.19653, train_loss=6.5598936

Batch 253870, train_perplexity=682.04193, train_loss=6.525091

Batch 253880, train_perplexity=704.3204, train_loss=6.5572333

Batch 253890, train_perplexity=640.03784, train_loss=6.4615273

Batch 253900, train_perplexity=679.06226, train_loss=6.520713

Batch 253910, train_perplexity=662.8481, train_loss=6.496546

Batch 253920, train_perplexity=710.7593, train_loss=6.566334

Batch 253930, train_perplexity=624.5651, train_loss=6.4370556

Batch 253940, train_perplexity=593.4936, train_loss=6.3860264

Batch 253950, train_perplexity=596.2635, train_loss=6.3906827

Batch 253960, train_perplexity=645.53375, train_loss=6.4700775

Batch 253970, train_perplexity=607.8014, train_loss=6.409848

Batch 253980, train_perplexity=561.33105, train_loss=6.330311

Batch 253990, train_perplexity=631.99176, train_loss=6.4488764

Batch 254000, train_perplexity=618.7418, train_loss=6.427688

Batch 254010, train_perplexity=607.71735, train_loss=6.40971

Batch 254020, train_perplexity=653.25336, train_loss=6.481965

Batch 254030, train_perplexity=659.15405, train_loss=6.4909573

Batch 254040, train_perplexity=586.4427, train_loss=6.374075

Batch 254050, train_perplexity=599.2069, train_loss=6.395607

Batch 254060, train_perplexity=613.0313, train_loss=6.418416

Batch 254070, train_perplexity=590.6106, train_loss=6.381157

Batch 254080, train_perplexity=662.11804, train_loss=6.495444

Batch 254090, train_perplexity=722.48724, train_loss=6.5827

Batch 254100, train_perplexity=606.69995, train_loss=6.4080343

Batch 254110, train_perplexity=593.1478, train_loss=6.3854437

Batch 254120, train_perplexity=639.5238, train_loss=6.460724

Batch 254130, train_perplexity=654.435, train_loss=6.4837723

Batch 254140, train_perplexity=603.3315, train_loss=6.402467

Batch 254150, train_perplexity=624.67236, train_loss=6.4372272

Batch 254160, train_perplexity=654.8361, train_loss=6.484385

Batch 254170, train_perplexity=650.25464, train_loss=6.477364

Batch 254180, train_perplexity=680.5054, train_loss=6.5228357

Batch 254190, train_perplexity=646.66473, train_loss=6.471828

Batch 254200, train_perplexity=624.5151, train_loss=6.4369755

Batch 254210, train_perplexity=637.4821, train_loss=6.457526

Batch 254220, train_perplexity=655.15, train_loss=6.484864

Batch 254230, train_perplexity=574.45496, train_loss=6.3534217

Batch 254240, train_perplexity=659.3159, train_loss=6.491203

Batch 254250, train_perplexity=605.28723, train_loss=6.405703

Batch 254260, train_perplexity=660.6065, train_loss=6.4931583

Batch 254270, train_perplexity=654.37604, train_loss=6.483682

Batch 254280, train_perplexity=594.0825, train_loss=6.387018

Batch 254290, train_perplexity=620.2546, train_loss=6.43013

Batch 254300, train_perplexity=655.6191, train_loss=6.48558

Batch 254310, train_perplexity=627.7695, train_loss=6.442173

Batch 254320, train_perplexity=610.0959, train_loss=6.413616

Batch 254330, train_perplexity=585.5795, train_loss=6.372602

Batch 254340, train_perplexity=668.98914, train_loss=6.505768

Batch 254350, train_perplexity=629.863, train_loss=6.4455023

Batch 254360, train_perplexity=627.064, train_loss=6.4410486

Batch 254370, train_perplexity=706.2781, train_loss=6.560009

Batch 254380, train_perplexity=661.99457, train_loss=6.4952574

Batch 254390, train_perplexity=684.7826, train_loss=6.5291014

Batch 254400, train_perplexity=694.6568, train_loss=6.543418

Batch 254410, train_perplexity=653.7669, train_loss=6.482751

Batch 254420, train_perplexity=696.4644, train_loss=6.5460167

Batch 254430, train_perplexity=657.1982, train_loss=6.4879856

Batch 254440, train_perplexity=661.8387, train_loss=6.495022

Batch 254450, train_perplexity=624.4591, train_loss=6.436886

Batch 254460, train_perplexity=592.1791, train_loss=6.383809
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 254470, train_perplexity=626.03357, train_loss=6.439404

Batch 254480, train_perplexity=686.03827, train_loss=6.5309334

Batch 254490, train_perplexity=628.67535, train_loss=6.443615

Batch 254500, train_perplexity=654.9345, train_loss=6.484535

Batch 254510, train_perplexity=567.15216, train_loss=6.3406277

Batch 254520, train_perplexity=639.194, train_loss=6.460208

Batch 254530, train_perplexity=613.1231, train_loss=6.4185658

Batch 254540, train_perplexity=567.865, train_loss=6.3418837

Batch 254550, train_perplexity=573.6313, train_loss=6.351987

Batch 254560, train_perplexity=731.5871, train_loss=6.5952163

Batch 254570, train_perplexity=616.7215, train_loss=6.4244175

Batch 254580, train_perplexity=612.59534, train_loss=6.4177046

Batch 254590, train_perplexity=577.9644, train_loss=6.3595123

Batch 254600, train_perplexity=667.1402, train_loss=6.5030003

Batch 254610, train_perplexity=715.318, train_loss=6.572727

Batch 254620, train_perplexity=603.29065, train_loss=6.402399

Batch 254630, train_perplexity=635.60754, train_loss=6.4545813

Batch 254640, train_perplexity=640.4088, train_loss=6.4621067

Batch 254650, train_perplexity=699.20966, train_loss=6.5499506

Batch 254660, train_perplexity=675.0101, train_loss=6.5147276

Batch 254670, train_perplexity=581.4452, train_loss=6.3655167

Batch 254680, train_perplexity=655.5669, train_loss=6.4855003

Batch 254690, train_perplexity=597.75323, train_loss=6.393178

Batch 254700, train_perplexity=649.7987, train_loss=6.4766626

Batch 254710, train_perplexity=638.332, train_loss=6.4588585

Batch 254720, train_perplexity=665.80865, train_loss=6.5010023

Batch 254730, train_perplexity=591.3596, train_loss=6.3824244

Batch 254740, train_perplexity=622.0554, train_loss=6.433029

Batch 254750, train_perplexity=567.86554, train_loss=6.3418846

Batch 254760, train_perplexity=656.091, train_loss=6.4862995

Batch 254770, train_perplexity=621.6003, train_loss=6.432297

Batch 254780, train_perplexity=666.87177, train_loss=6.502598

Batch 254790, train_perplexity=644.6497, train_loss=6.468707

Batch 254800, train_perplexity=607.7072, train_loss=6.4096932

Batch 254810, train_perplexity=682.7624, train_loss=6.526147

Batch 254820, train_perplexity=674.8607, train_loss=6.5145063

Batch 254830, train_perplexity=652.3435, train_loss=6.4805713

Batch 254840, train_perplexity=680.81274, train_loss=6.5232873

Batch 254850, train_perplexity=687.70374, train_loss=6.533358

Batch 254860, train_perplexity=642.6262, train_loss=6.4655633

Batch 254870, train_perplexity=615.2211, train_loss=6.421982

Batch 254880, train_perplexity=628.0311, train_loss=6.4425898

Batch 254890, train_perplexity=684.33734, train_loss=6.528451

Batch 254900, train_perplexity=683.54095, train_loss=6.5272865

Batch 254910, train_perplexity=683.5272, train_loss=6.5272665

Batch 254920, train_perplexity=630.9936, train_loss=6.4472957

Batch 254930, train_perplexity=663.94574, train_loss=6.4982004

Batch 254940, train_perplexity=639.6168, train_loss=6.4608693

Batch 254950, train_perplexity=628.36426, train_loss=6.44312

Batch 254960, train_perplexity=676.43036, train_loss=6.5168295

Batch 254970, train_perplexity=649.3883, train_loss=6.476031

Batch 254980, train_perplexity=659.9145, train_loss=6.4921103

Batch 254990, train_perplexity=660.701, train_loss=6.4933014

Batch 255000, train_perplexity=669.501, train_loss=6.5065327

Batch 255010, train_perplexity=621.47845, train_loss=6.4321012

Batch 255020, train_perplexity=674.39044, train_loss=6.513809

Batch 255030, train_perplexity=642.8509, train_loss=6.465913

Batch 255040, train_perplexity=690.85864, train_loss=6.5379353

Batch 255050, train_perplexity=637.45715, train_loss=6.457487

Batch 255060, train_perplexity=676.2314, train_loss=6.5165353

Batch 255070, train_perplexity=610.56274, train_loss=6.414381

Batch 255080, train_perplexity=633.86414, train_loss=6.4518347

Batch 255090, train_perplexity=634.42993, train_loss=6.452727

Batch 255100, train_perplexity=655.58624, train_loss=6.48553

Batch 255110, train_perplexity=637.8582, train_loss=6.458116

Batch 255120, train_perplexity=685.54803, train_loss=6.5302186

Batch 255130, train_perplexity=582.68274, train_loss=6.367643

Batch 255140, train_perplexity=648.17834, train_loss=6.474166

Batch 255150, train_perplexity=665.92737, train_loss=6.5011806

Batch 255160, train_perplexity=561.32465, train_loss=6.3302994

Batch 255170, train_perplexity=634.54126, train_loss=6.4529023

Batch 255180, train_perplexity=702.70984, train_loss=6.554944

Batch 255190, train_perplexity=629.5885, train_loss=6.4450665

Batch 255200, train_perplexity=637.951, train_loss=6.4582615

Batch 255210, train_perplexity=629.1423, train_loss=6.4443574

Batch 255220, train_perplexity=627.3296, train_loss=6.441472

Batch 255230, train_perplexity=623.6896, train_loss=6.4356527

Batch 255240, train_perplexity=664.39166, train_loss=6.498872

Batch 255250, train_perplexity=605.02, train_loss=6.4052615

Batch 255260, train_perplexity=599.0778, train_loss=6.3953915

Batch 255270, train_perplexity=569.0658, train_loss=6.343996

Batch 255280, train_perplexity=742.5734, train_loss=6.6101217

Batch 255290, train_perplexity=664.7982, train_loss=6.4994836

Batch 255300, train_perplexity=628.9476, train_loss=6.444048

Batch 255310, train_perplexity=629.7089, train_loss=6.4452577

Batch 255320, train_perplexity=654.4874, train_loss=6.4838524

Batch 255330, train_perplexity=621.14813, train_loss=6.4315696

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Batch 255340, train_perplexity=649.5078, train_loss=6.476215

Batch 255350, train_perplexity=655.4209, train_loss=6.4852777

Batch 255360, train_perplexity=653.1319, train_loss=6.481779

Batch 255370, train_perplexity=587.03973, train_loss=6.3750925

Batch 255380, train_perplexity=698.4202, train_loss=6.548821

Batch 255390, train_perplexity=639.6733, train_loss=6.4609575

Batch 255400, train_perplexity=671.1132, train_loss=6.508938

Batch 255410, train_perplexity=633.8732, train_loss=6.451849

Batch 255420, train_perplexity=610.66174, train_loss=6.414543

Batch 255430, train_perplexity=648.85376, train_loss=6.4752073

Batch 255440, train_perplexity=523.5255, train_loss=6.260586

Batch 255450, train_perplexity=616.0983, train_loss=6.4234066

Batch 255460, train_perplexity=612.72095, train_loss=6.4179096

Batch 255470, train_perplexity=604.51105, train_loss=6.40442

Batch 255480, train_perplexity=646.81305, train_loss=6.4720573

Batch 255490, train_perplexity=685.14056, train_loss=6.529624

Batch 255500, train_perplexity=565.2073, train_loss=6.3371925

Batch 255510, train_perplexity=625.71277, train_loss=6.4388914

Batch 255520, train_perplexity=580.1745, train_loss=6.363329

Batch 255530, train_perplexity=713.52203, train_loss=6.5702133

Batch 255540, train_perplexity=712.7168, train_loss=6.569084

Batch 255550, train_perplexity=620.0712, train_loss=6.4298344

Batch 255560, train_perplexity=626.7397, train_loss=6.4405313

Batch 255570, train_perplexity=601.7053, train_loss=6.399768

Batch 255580, train_perplexity=640.74603, train_loss=6.462633

Batch 255590, train_perplexity=613.4986, train_loss=6.419178

Batch 255600, train_perplexity=586.1066, train_loss=6.373502

Batch 255610, train_perplexity=638.2644, train_loss=6.4587526

Batch 255620, train_perplexity=600.3647, train_loss=6.397537

Batch 255630, train_perplexity=620.75964, train_loss=6.430944

Batch 255640, train_perplexity=624.0549, train_loss=6.4362383

Batch 255650, train_perplexity=634.1689, train_loss=6.4523153

Batch 255660, train_perplexity=683.9204, train_loss=6.5278416

Batch 255670, train_perplexity=625.69366, train_loss=6.438861

Batch 255680, train_perplexity=609.1413, train_loss=6.4120502

Batch 255690, train_perplexity=628.1839, train_loss=6.442833

Batch 255700, train_perplexity=641.9307, train_loss=6.4644804
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 255710, train_perplexity=573.2674, train_loss=6.351352

Batch 255720, train_perplexity=685.5752, train_loss=6.530258

Batch 255730, train_perplexity=617.8258, train_loss=6.4262066

Batch 255740, train_perplexity=622.54504, train_loss=6.433816

Batch 255750, train_perplexity=636.1651, train_loss=6.455458

Batch 255760, train_perplexity=608.6358, train_loss=6.41122

Batch 255770, train_perplexity=674.918, train_loss=6.514591

Batch 255780, train_perplexity=603.3827, train_loss=6.4025517

Batch 255790, train_perplexity=628.7212, train_loss=6.443688

Batch 255800, train_perplexity=631.1079, train_loss=6.447477

Batch 255810, train_perplexity=609.89984, train_loss=6.413295

Batch 255820, train_perplexity=609.60913, train_loss=6.412818

Batch 255830, train_perplexity=557.5069, train_loss=6.323475

Batch 255840, train_perplexity=599.5493, train_loss=6.3961782

Batch 255850, train_perplexity=681.01373, train_loss=6.5235825

Batch 255860, train_perplexity=612.0902, train_loss=6.4168797

Batch 255870, train_perplexity=675.2824, train_loss=6.515131

Batch 255880, train_perplexity=589.2446, train_loss=6.3788414

Batch 255890, train_perplexity=665.1251, train_loss=6.499975

Batch 255900, train_perplexity=665.7086, train_loss=6.500852

Batch 255910, train_perplexity=622.94916, train_loss=6.434465

Batch 255920, train_perplexity=552.5541, train_loss=6.3145514

Batch 255930, train_perplexity=592.0221, train_loss=6.383544

Batch 255940, train_perplexity=706.0821, train_loss=6.5597315

Batch 255950, train_perplexity=596.7092, train_loss=6.39143

Batch 255960, train_perplexity=679.7867, train_loss=6.521779

Batch 255970, train_perplexity=642.8074, train_loss=6.465845

Batch 255980, train_perplexity=613.50653, train_loss=6.419191

Batch 255990, train_perplexity=694.19257, train_loss=6.5427494

Batch 256000, train_perplexity=633.08484, train_loss=6.4506044

Batch 256010, train_perplexity=651.4094, train_loss=6.4791384

Batch 256020, train_perplexity=609.59717, train_loss=6.4127984

Batch 256030, train_perplexity=703.6131, train_loss=6.5562286

Batch 256040, train_perplexity=576.6552, train_loss=6.3572445

Batch 256050, train_perplexity=548.7332, train_loss=6.3076124

Batch 256060, train_perplexity=561.5436, train_loss=6.3306894

Batch 256070, train_perplexity=591.3613, train_loss=6.382427

Batch 256080, train_perplexity=644.1439, train_loss=6.467922

Batch 256090, train_perplexity=600.8699, train_loss=6.3983784

Batch 256100, train_perplexity=650.3399, train_loss=6.477495

Batch 256110, train_perplexity=628.1396, train_loss=6.4427624

Batch 256120, train_perplexity=639.19763, train_loss=6.4602137

Batch 256130, train_perplexity=589.8192, train_loss=6.379816

Batch 256140, train_perplexity=628.80066, train_loss=6.4438143

Batch 256150, train_perplexity=635.31116, train_loss=6.454115

Batch 256160, train_perplexity=633.45447, train_loss=6.451188

Batch 256170, train_perplexity=653.2434, train_loss=6.48195

Batch 256180, train_perplexity=550.4136, train_loss=6.31067

Batch 256190, train_perplexity=601.4942, train_loss=6.399417

Batch 256200, train_perplexity=600.3933, train_loss=6.397585

Batch 256210, train_perplexity=639.16315, train_loss=6.46016

Batch 256220, train_perplexity=639.1406, train_loss=6.4601245

Batch 256230, train_perplexity=657.6264, train_loss=6.488637

Batch 256240, train_perplexity=607.5012, train_loss=6.409354

Batch 256250, train_perplexity=647.3413, train_loss=6.4728737

Batch 256260, train_perplexity=679.0927, train_loss=6.5207577

Batch 256270, train_perplexity=554.7027, train_loss=6.3184323

Batch 256280, train_perplexity=628.3109, train_loss=6.443035

Batch 256290, train_perplexity=683.05414, train_loss=6.526574

Batch 256300, train_perplexity=631.4797, train_loss=6.4480658

Batch 256310, train_perplexity=691.01086, train_loss=6.5381556

Batch 256320, train_perplexity=672.71643, train_loss=6.511324

Batch 256330, train_perplexity=610.87085, train_loss=6.4148855

Batch 256340, train_perplexity=657.6772, train_loss=6.488714

Batch 256350, train_perplexity=660.4695, train_loss=6.492951

Batch 256360, train_perplexity=628.6591, train_loss=6.443589

Batch 256370, train_perplexity=637.8835, train_loss=6.4581556

Batch 256380, train_perplexity=629.5891, train_loss=6.4450674

Batch 256390, train_perplexity=649.10254, train_loss=6.4755907

Batch 256400, train_perplexity=662.48944, train_loss=6.4960046

Batch 256410, train_perplexity=629.25714, train_loss=6.44454

Batch 256420, train_perplexity=632.9219, train_loss=6.450347

Batch 256430, train_perplexity=636.3138, train_loss=6.455692

Batch 256440, train_perplexity=642.5999, train_loss=6.4655223

Batch 256450, train_perplexity=648.51843, train_loss=6.4746904

Batch 256460, train_perplexity=611.20703, train_loss=6.415436

Batch 256470, train_perplexity=609.84576, train_loss=6.413206

Batch 256480, train_perplexity=645.17096, train_loss=6.4695153

Batch 256490, train_perplexity=637.7393, train_loss=6.4579296

Batch 256500, train_perplexity=684.2985, train_loss=6.528394

Batch 256510, train_perplexity=650.4606, train_loss=6.4776807

Batch 256520, train_perplexity=590.5636, train_loss=6.3810773

Batch 256530, train_perplexity=611.6193, train_loss=6.41611

Batch 256540, train_perplexity=633.86053, train_loss=6.451829

Batch 256550, train_perplexity=663.60767, train_loss=6.497691

Batch 256560, train_perplexity=650.18115, train_loss=6.477251

Batch 256570, train_perplexity=633.24725, train_loss=6.450861

Batch 256580, train_perplexity=578.4469, train_loss=6.360347

Batch 256590, train_perplexity=584.411, train_loss=6.3706045

Batch 256600, train_perplexity=701.4912, train_loss=6.5532084

Batch 256610, train_perplexity=662.48785, train_loss=6.496002

Batch 256620, train_perplexity=693.02405, train_loss=6.5410647

Batch 256630, train_perplexity=658.1437, train_loss=6.4894233

Batch 256640, train_perplexity=622.15625, train_loss=6.4331913

Batch 256650, train_perplexity=620.7339, train_loss=6.4309025

Batch 256660, train_perplexity=648.7922, train_loss=6.4751124

Batch 256670, train_perplexity=647.0864, train_loss=6.47248

Batch 256680, train_perplexity=638.6005, train_loss=6.459279

Batch 256690, train_perplexity=590.1399, train_loss=6.3803596

Batch 256700, train_perplexity=636.80945, train_loss=6.4564705

Batch 256710, train_perplexity=598.39746, train_loss=6.394255

Batch 256720, train_perplexity=545.20233, train_loss=6.301157

Batch 256730, train_perplexity=694.46405, train_loss=6.5431404

Batch 256740, train_perplexity=587.736, train_loss=6.376278

Batch 256750, train_perplexity=676.05273, train_loss=6.516271

Batch 256760, train_perplexity=633.39343, train_loss=6.451092

Batch 256770, train_perplexity=630.8645, train_loss=6.447091

Batch 256780, train_perplexity=549.95966, train_loss=6.309845

Batch 256790, train_perplexity=609.8327, train_loss=6.4131846

Batch 256800, train_perplexity=601.656, train_loss=6.399686

Batch 256810, train_perplexity=615.86804, train_loss=6.4230328

Batch 256820, train_perplexity=606.25195, train_loss=6.4072957

Batch 256830, train_perplexity=718.1057, train_loss=6.576617

Batch 256840, train_perplexity=660.3816, train_loss=6.492818

Batch 256850, train_perplexity=593.87463, train_loss=6.386668

Batch 256860, train_perplexity=634.7352, train_loss=6.453208

Batch 256870, train_perplexity=664.8537, train_loss=6.499567

Batch 256880, train_perplexity=644.82434, train_loss=6.468978

Batch 256890, train_perplexity=628.414, train_loss=6.443199

Batch 256900, train_perplexity=600.83777, train_loss=6.398325

Batch 256910, train_perplexity=630.6657, train_loss=6.446776

Batch 256920, train_perplexity=700.6441, train_loss=6.552

Batch 256930, train_perplexity=620.0328, train_loss=6.4297724

Batch 256940, train_perplexity=571.12256, train_loss=6.347604

Batch 256950, train_perplexity=640.13464, train_loss=6.4616785

Batch 256960, train_perplexity=589.3747, train_loss=6.379062

Batch 256970, train_perplexity=654.45215, train_loss=6.4837985

Batch 256980, train_perplexity=628.33905, train_loss=6.44308

Batch 256990, train_perplexity=626.20886, train_loss=6.439684

Batch 257000, train_perplexity=638.03314, train_loss=6.45839

Batch 257010, train_perplexity=635.8367, train_loss=6.4549417
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 257020, train_perplexity=641.06476, train_loss=6.4631305

Batch 257030, train_perplexity=648.0158, train_loss=6.473915

Batch 257040, train_perplexity=639.4967, train_loss=6.4606814

Batch 257050, train_perplexity=632.717, train_loss=6.450023

Batch 257060, train_perplexity=650.47394, train_loss=6.477701

Batch 257070, train_perplexity=649.97687, train_loss=6.476937

Batch 257080, train_perplexity=668.4471, train_loss=6.504957

Batch 257090, train_perplexity=634.4605, train_loss=6.452775

Batch 257100, train_perplexity=658.0241, train_loss=6.4892416

Batch 257110, train_perplexity=609.04834, train_loss=6.4118977

Batch 257120, train_perplexity=655.1769, train_loss=6.4849052

Batch 257130, train_perplexity=668.2944, train_loss=6.504729

Batch 257140, train_perplexity=594.1726, train_loss=6.38717

Batch 257150, train_perplexity=649.48, train_loss=6.476172

Batch 257160, train_perplexity=582.79193, train_loss=6.3678303

Batch 257170, train_perplexity=636.3541, train_loss=6.455755

Batch 257180, train_perplexity=752.64764, train_loss=6.623597

Batch 257190, train_perplexity=668.55225, train_loss=6.5051146

Batch 257200, train_perplexity=605.23236, train_loss=6.4056125

Batch 257210, train_perplexity=633.61755, train_loss=6.4514456

Batch 257220, train_perplexity=557.6332, train_loss=6.3237014

Batch 257230, train_perplexity=557.2188, train_loss=6.322958

Batch 257240, train_perplexity=640.83734, train_loss=6.4627757

Batch 257250, train_perplexity=595.0726, train_loss=6.3886833

Batch 257260, train_perplexity=588.8382, train_loss=6.3781514

Batch 257270, train_perplexity=673.3596, train_loss=6.5122795

Batch 257280, train_perplexity=668.1787, train_loss=6.5045557

Batch 257290, train_perplexity=710.1556, train_loss=6.565484

Batch 257300, train_perplexity=590.7376, train_loss=6.381372

Batch 257310, train_perplexity=747.7119, train_loss=6.6170177

Batch 257320, train_perplexity=628.417, train_loss=6.443204

Batch 257330, train_perplexity=683.77826, train_loss=6.5276337

Batch 257340, train_perplexity=626.4131, train_loss=6.44001

Batch 257350, train_perplexity=585.2789, train_loss=6.3720884

Batch 257360, train_perplexity=667.3133, train_loss=6.5032597

Batch 257370, train_perplexity=662.6616, train_loss=6.4962645

Batch 257380, train_perplexity=641.2968, train_loss=6.4634924

Batch 257390, train_perplexity=604.213, train_loss=6.403927

Batch 257400, train_perplexity=646.0246, train_loss=6.4708376

Batch 257410, train_perplexity=659.7219, train_loss=6.4918184

Batch 257420, train_perplexity=581.1231, train_loss=6.3649626

Batch 257430, train_perplexity=625.2591, train_loss=6.438166

Batch 257440, train_perplexity=596.6193, train_loss=6.391279

Batch 257450, train_perplexity=645.3442, train_loss=6.469784

Batch 257460, train_perplexity=674.1017, train_loss=6.513381

Batch 257470, train_perplexity=599.6946, train_loss=6.3964205

Batch 257480, train_perplexity=614.6394, train_loss=6.421036

Batch 257490, train_perplexity=597.05133, train_loss=6.392003

Batch 257500, train_perplexity=662.03186, train_loss=6.4953136

Batch 257510, train_perplexity=728.8562, train_loss=6.5914764

Batch 257520, train_perplexity=610.37585, train_loss=6.414075

Batch 257530, train_perplexity=629.86896, train_loss=6.445512

Batch 257540, train_perplexity=645.1122, train_loss=6.4694242

Batch 257550, train_perplexity=636.8331, train_loss=6.4565077

Batch 257560, train_perplexity=610.92474, train_loss=6.4149737

Batch 257570, train_perplexity=658.15344, train_loss=6.489438

Batch 257580, train_perplexity=616.7941, train_loss=6.4245353

Batch 257590, train_perplexity=644.35834, train_loss=6.468255

Batch 257600, train_perplexity=694.32367, train_loss=6.542938

Batch 257610, train_perplexity=601.22235, train_loss=6.398965

Batch 257620, train_perplexity=588.54706, train_loss=6.377657

Batch 257630, train_perplexity=626.31335, train_loss=6.439851

Batch 257640, train_perplexity=643.1204, train_loss=6.466332

Batch 257650, train_perplexity=662.9675, train_loss=6.496726

Batch 257660, train_perplexity=582.08875, train_loss=6.366623

Batch 257670, train_perplexity=581.7713, train_loss=6.3660774

Batch 257680, train_perplexity=577.43884, train_loss=6.3586025

Batch 257690, train_perplexity=617.133, train_loss=6.4250846

Batch 257700, train_perplexity=571.6495, train_loss=6.348526

Batch 257710, train_perplexity=611.4324, train_loss=6.4158044

Batch 257720, train_perplexity=662.4657, train_loss=6.495969

Batch 257730, train_perplexity=692.3721, train_loss=6.5401235

Batch 257740, train_perplexity=625.1187, train_loss=6.4379416

Batch 257750, train_perplexity=617.76514, train_loss=6.4261084

Batch 257760, train_perplexity=728.65326, train_loss=6.591198

Batch 257770, train_perplexity=655.6407, train_loss=6.485613

Batch 257780, train_perplexity=661.0842, train_loss=6.493881

Batch 257790, train_perplexity=625.22424, train_loss=6.4381104

Batch 257800, train_perplexity=632.21875, train_loss=6.4492354

Batch 257810, train_perplexity=660.5822, train_loss=6.4931216

Batch 257820, train_perplexity=634.6505, train_loss=6.4530745

Batch 257830, train_perplexity=557.71216, train_loss=6.323843

Batch 257840, train_perplexity=687.71094, train_loss=6.5333686

Batch 257850, train_perplexity=628.6561, train_loss=6.4435844

Batch 257860, train_perplexity=599.0672, train_loss=6.395374

Batch 257870, train_perplexity=604.0252, train_loss=6.403616

Batch 257880, train_perplexity=637.363, train_loss=6.4573393

Batch 257890, train_perplexity=605.30225, train_loss=6.405728

Batch 257900, train_perplexity=567.6587, train_loss=6.3415203

Batch 257910, train_perplexity=600.2702, train_loss=6.39738

Batch 257920, train_perplexity=613.29095, train_loss=6.4188395

Batch 257930, train_perplexity=652.24304, train_loss=6.4804173

Batch 257940, train_perplexity=603.4132, train_loss=6.402602

Batch 257950, train_perplexity=685.92114, train_loss=6.5307627

Batch 257960, train_perplexity=670.99164, train_loss=6.5087566

Batch 257970, train_perplexity=617.3694, train_loss=6.4254675

Batch 257980, train_perplexity=688.4922, train_loss=6.534504

Batch 257990, train_perplexity=622.002, train_loss=6.4329433

Batch 258000, train_perplexity=641.7238, train_loss=6.464158

Batch 258010, train_perplexity=608.16785, train_loss=6.410451

Batch 258020, train_perplexity=682.35944, train_loss=6.5255566

Batch 258030, train_perplexity=663.2205, train_loss=6.4971075

Batch 258040, train_perplexity=725.31323, train_loss=6.5866036

Batch 258050, train_perplexity=566.4813, train_loss=6.339444

Batch 258060, train_perplexity=651.9619, train_loss=6.479986

Batch 258070, train_perplexity=651.30725, train_loss=6.4789815

Batch 258080, train_perplexity=588.9258, train_loss=6.3783

Batch 258090, train_perplexity=631.667, train_loss=6.4483624

Batch 258100, train_perplexity=663.7707, train_loss=6.4979367

Batch 258110, train_perplexity=643.65515, train_loss=6.467163

Batch 258120, train_perplexity=627.10767, train_loss=6.4411182

Batch 258130, train_perplexity=616.9327, train_loss=6.42476

Batch 258140, train_perplexity=654.02386, train_loss=6.483144

Batch 258150, train_perplexity=635.9656, train_loss=6.4551444

Batch 258160, train_perplexity=648.5784, train_loss=6.474783

Batch 258170, train_perplexity=653.9677, train_loss=6.483058

Batch 258180, train_perplexity=652.5442, train_loss=6.480879

Batch 258190, train_perplexity=653.0229, train_loss=6.481612

Batch 258200, train_perplexity=623.4288, train_loss=6.4352345

Batch 258210, train_perplexity=617.53186, train_loss=6.4257307

Batch 258220, train_perplexity=610.26465, train_loss=6.4138927

Batch 258230, train_perplexity=621.549, train_loss=6.4322147

Batch 258240, train_perplexity=707.1104, train_loss=6.561187

Batch 258250, train_perplexity=599.50385, train_loss=6.3961024

Batch 258260, train_perplexity=635.16394, train_loss=6.453883

Batch 258270, train_perplexity=684.96936, train_loss=6.529374

Batch 258280, train_perplexity=651.2579, train_loss=6.4789057

Batch 258290, train_perplexity=587.801, train_loss=6.3763885

Batch 258300, train_perplexity=601.4449, train_loss=6.399335

Batch 258310, train_perplexity=643.01245, train_loss=6.466164

Batch 258320, train_perplexity=563.55945, train_loss=6.334273
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 258330, train_perplexity=588.1431, train_loss=6.3769703

Batch 258340, train_perplexity=585.9608, train_loss=6.373253

Batch 258350, train_perplexity=707.5455, train_loss=6.561802

Batch 258360, train_perplexity=665.9947, train_loss=6.5012817

Batch 258370, train_perplexity=599.0532, train_loss=6.3953505

Batch 258380, train_perplexity=575.52203, train_loss=6.3552775

Batch 258390, train_perplexity=704.60455, train_loss=6.5576367

Batch 258400, train_perplexity=632.40204, train_loss=6.4495254

Batch 258410, train_perplexity=681.26056, train_loss=6.523945

Batch 258420, train_perplexity=565.92786, train_loss=6.3384666

Batch 258430, train_perplexity=620.3776, train_loss=6.4303284

Batch 258440, train_perplexity=604.1992, train_loss=6.403904

Batch 258450, train_perplexity=647.596, train_loss=6.473267

Batch 258460, train_perplexity=672.0329, train_loss=6.5103073

Batch 258470, train_perplexity=650.99457, train_loss=6.4785013

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Batch 258480, train_perplexity=593.4166, train_loss=6.3858967

Batch 258490, train_perplexity=621.5872, train_loss=6.4322762

Batch 258500, train_perplexity=638.20355, train_loss=6.4586573

Batch 258510, train_perplexity=609.55884, train_loss=6.4127355

Batch 258520, train_perplexity=639.24884, train_loss=6.460294

Batch 258530, train_perplexity=596.4165, train_loss=6.390939

Batch 258540, train_perplexity=653.93744, train_loss=6.4830117

Batch 258550, train_perplexity=677.70044, train_loss=6.5187054

Batch 258560, train_perplexity=585.85516, train_loss=6.3730726

Batch 258570, train_perplexity=639.30066, train_loss=6.460375

Batch 258580, train_perplexity=585.8342, train_loss=6.373037

Batch 258590, train_perplexity=640.0424, train_loss=6.4615345

Batch 258600, train_perplexity=643.36975, train_loss=6.4667196

Batch 258610, train_perplexity=645.0796, train_loss=6.4693737

Batch 258620, train_perplexity=636.60785, train_loss=6.456154

Batch 258630, train_perplexity=601.1542, train_loss=6.3988514

Batch 258640, train_perplexity=637.46356, train_loss=6.457497

Batch 258650, train_perplexity=674.6278, train_loss=6.514161

Batch 258660, train_perplexity=667.8468, train_loss=6.504059

Batch 258670, train_perplexity=683.5005, train_loss=6.5272274

Batch 258680, train_perplexity=578.37494, train_loss=6.3602223

Batch 258690, train_perplexity=651.4619, train_loss=6.479219

Batch 258700, train_perplexity=630.7448, train_loss=6.4469013

Batch 258710, train_perplexity=638.90753, train_loss=6.4597597

Batch 258720, train_perplexity=664.70154, train_loss=6.499338

Batch 258730, train_perplexity=715.39984, train_loss=6.5728416

Batch 258740, train_perplexity=619.0496, train_loss=6.4281855

Batch 258750, train_perplexity=668.1201, train_loss=6.504468

Batch 258760, train_perplexity=630.43, train_loss=6.446402

Batch 258770, train_perplexity=692.4998, train_loss=6.540308

Batch 258780, train_perplexity=649.62213, train_loss=6.476391

Batch 258790, train_perplexity=626.69244, train_loss=6.440456

Batch 258800, train_perplexity=612.86035, train_loss=6.418137

Batch 258810, train_perplexity=652.151, train_loss=6.480276

Batch 258820, train_perplexity=622.4664, train_loss=6.4336896

Batch 258830, train_perplexity=589.8712, train_loss=6.3799043

Batch 258840, train_perplexity=617.09564, train_loss=6.425024

Batch 258850, train_perplexity=560.4573, train_loss=6.328753

Batch 258860, train_perplexity=621.80865, train_loss=6.4326324

Batch 258870, train_perplexity=676.6555, train_loss=6.5171623

Batch 258880, train_perplexity=634.1816, train_loss=6.4523354

Batch 258890, train_perplexity=614.4199, train_loss=6.4206786

Batch 258900, train_perplexity=693.94965, train_loss=6.5423994

Batch 258910, train_perplexity=616.3778, train_loss=6.42386

Batch 258920, train_perplexity=677.3044, train_loss=6.518121

Batch 258930, train_perplexity=623.14435, train_loss=6.434778

Batch 258940, train_perplexity=624.18164, train_loss=6.4364414

Batch 258950, train_perplexity=586.3479, train_loss=6.3739133

Batch 258960, train_perplexity=595.2973, train_loss=6.389061

Batch 258970, train_perplexity=663.68933, train_loss=6.497814

Batch 258980, train_perplexity=618.8345, train_loss=6.427838

Batch 258990, train_perplexity=663.65515, train_loss=6.4977627

Batch 259000, train_perplexity=610.3834, train_loss=6.4140873

Batch 259010, train_perplexity=565.2588, train_loss=6.3372836

Batch 259020, train_perplexity=624.7766, train_loss=6.437394

Batch 259030, train_perplexity=593.3051, train_loss=6.385709

Batch 259040, train_perplexity=639.16284, train_loss=6.4601593

Batch 259050, train_perplexity=620.1487, train_loss=6.4299593

Batch 259060, train_perplexity=665.66644, train_loss=6.5007887

Batch 259070, train_perplexity=575.1931, train_loss=6.354706

Batch 259080, train_perplexity=630.7619, train_loss=6.4469285

Batch 259090, train_perplexity=542.7276, train_loss=6.2966075

Batch 259100, train_perplexity=630.4516, train_loss=6.4464364

Batch 259110, train_perplexity=677.6455, train_loss=6.5186243

Batch 259120, train_perplexity=685.93915, train_loss=6.530789

Batch 259130, train_perplexity=716.927, train_loss=6.574974

Batch 259140, train_perplexity=620.32086, train_loss=6.430237

Batch 259150, train_perplexity=604.98224, train_loss=6.405199

Batch 259160, train_perplexity=571.5023, train_loss=6.3482685

Batch 259170, train_perplexity=688.79395, train_loss=6.534942

Batch 259180, train_perplexity=680.1472, train_loss=6.5223093

Batch 259190, train_perplexity=674.9087, train_loss=6.5145774

Batch 259200, train_perplexity=563.7807, train_loss=6.3346653

Batch 259210, train_perplexity=589.86475, train_loss=6.3798933

Batch 259220, train_perplexity=654.1489, train_loss=6.483335

Batch 259230, train_perplexity=563.01666, train_loss=6.333309

Batch 259240, train_perplexity=634.02014, train_loss=6.4520807

Batch 259250, train_perplexity=620.31195, train_loss=6.4302225

Batch 259260, train_perplexity=603.3721, train_loss=6.402534

Batch 259270, train_perplexity=650.25995, train_loss=6.477372

Batch 259280, train_perplexity=592.0949, train_loss=6.383667

Batch 259290, train_perplexity=647.24036, train_loss=6.472718

Batch 259300, train_perplexity=644.3633, train_loss=6.4682627

Batch 259310, train_perplexity=602.1571, train_loss=6.4005184

Batch 259320, train_perplexity=627.34393, train_loss=6.441495

Batch 259330, train_perplexity=634.5821, train_loss=6.4529667

Batch 259340, train_perplexity=578.8194, train_loss=6.3609905

Batch 259350, train_perplexity=673.39044, train_loss=6.5123253

Batch 259360, train_perplexity=603.1261, train_loss=6.4021263

Batch 259370, train_perplexity=628.29266, train_loss=6.443006

Batch 259380, train_perplexity=628.5548, train_loss=6.4434233

Batch 259390, train_perplexity=691.48553, train_loss=6.538842

Batch 259400, train_perplexity=683.7939, train_loss=6.5276566

Batch 259410, train_perplexity=651.09143, train_loss=6.47865

Batch 259420, train_perplexity=612.5007, train_loss=6.41755

Batch 259430, train_perplexity=602.31854, train_loss=6.4007864

Batch 259440, train_perplexity=650.63055, train_loss=6.477942

Batch 259450, train_perplexity=587.7198, train_loss=6.3762503

Batch 259460, train_perplexity=637.90295, train_loss=6.458186

Batch 259470, train_perplexity=607.306, train_loss=6.409033

Batch 259480, train_perplexity=563.0881, train_loss=6.333436

Batch 259490, train_perplexity=681.4256, train_loss=6.524187

Batch 259500, train_perplexity=584.8275, train_loss=6.371317

Batch 259510, train_perplexity=620.2445, train_loss=6.430114

Batch 259520, train_perplexity=608.1957, train_loss=6.4104967

Batch 259530, train_perplexity=608.0777, train_loss=6.4103026

Batch 259540, train_perplexity=706.305, train_loss=6.560047

Batch 259550, train_perplexity=609.40564, train_loss=6.412484

Batch 259560, train_perplexity=642.9545, train_loss=6.466074

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 259570, train_perplexity=602.7107, train_loss=6.4014373

Batch 259580, train_perplexity=583.8829, train_loss=6.3697004

Batch 259590, train_perplexity=652.9547, train_loss=6.481508

Batch 259600, train_perplexity=692.5216, train_loss=6.5403395

Batch 259610, train_perplexity=563.495, train_loss=6.3341584

Batch 259620, train_perplexity=640.5896, train_loss=6.462389

Batch 259630, train_perplexity=562.93396, train_loss=6.3331623

Batch 259640, train_perplexity=641.37695, train_loss=6.4636173

Batch 259650, train_perplexity=613.0991, train_loss=6.4185266

Batch 259660, train_perplexity=637.06854, train_loss=6.456877

Batch 259670, train_perplexity=631.4499, train_loss=6.4480186

Batch 259680, train_perplexity=588.9182, train_loss=6.3782873

Batch 259690, train_perplexity=597.68054, train_loss=6.3930564

Batch 259700, train_perplexity=632.39575, train_loss=6.4495153

Batch 259710, train_perplexity=603.9924, train_loss=6.4035616

Batch 259720, train_perplexity=648.6347, train_loss=6.4748697

Batch 259730, train_perplexity=578.2379, train_loss=6.3599854

Batch 259740, train_perplexity=665.88293, train_loss=6.501114

Batch 259750, train_perplexity=599.3712, train_loss=6.395881

Batch 259760, train_perplexity=616.2702, train_loss=6.4236856

Batch 259770, train_perplexity=588.82385, train_loss=6.378127

Batch 259780, train_perplexity=640.45917, train_loss=6.4621854

Batch 259790, train_perplexity=645.70496, train_loss=6.4703426

Batch 259800, train_perplexity=632.0162, train_loss=6.448915

Batch 259810, train_perplexity=684.825, train_loss=6.5291634

Batch 259820, train_perplexity=604.4194, train_loss=6.4042683

Batch 259830, train_perplexity=670.9852, train_loss=6.508747

Batch 259840, train_perplexity=759.02686, train_loss=6.632037

Batch 259850, train_perplexity=622.9685, train_loss=6.434496

Batch 259860, train_perplexity=623.3453, train_loss=6.4351006

Batch 259870, train_perplexity=691.47626, train_loss=6.538829

Batch 259880, train_perplexity=615.5128, train_loss=6.422456

Batch 259890, train_perplexity=623.5144, train_loss=6.435372

Batch 259900, train_perplexity=673.4322, train_loss=6.5123873

Batch 259910, train_perplexity=604.49, train_loss=6.404385

Batch 259920, train_perplexity=621.962, train_loss=6.432879

Batch 259930, train_perplexity=614.33905, train_loss=6.420547

Batch 259940, train_perplexity=642.3769, train_loss=6.465175

Batch 259950, train_perplexity=655.16064, train_loss=6.4848804

Batch 259960, train_perplexity=646.1848, train_loss=6.4710855

Batch 259970, train_perplexity=612.2484, train_loss=6.417138

Batch 259980, train_perplexity=581.91504, train_loss=6.3663244

Batch 259990, train_perplexity=621.60114, train_loss=6.4322987

Batch 260000, train_perplexity=640.9379, train_loss=6.4629326

Batch 260010, train_perplexity=657.5135, train_loss=6.4884653

Batch 260020, train_perplexity=616.9515, train_loss=6.4247904

Batch 260030, train_perplexity=647.14874, train_loss=6.472576

Batch 260040, train_perplexity=599.48645, train_loss=6.3960733

Batch 260050, train_perplexity=662.2901, train_loss=6.4957037

Batch 260060, train_perplexity=587.3186, train_loss=6.3755674

Batch 260070, train_perplexity=603.5985, train_loss=6.4029093

Batch 260080, train_perplexity=577.4482, train_loss=6.3586187

Batch 260090, train_perplexity=630.4417, train_loss=6.4464207

Batch 260100, train_perplexity=562.28046, train_loss=6.3320007

Batch 260110, train_perplexity=676.6994, train_loss=6.517227

Batch 260120, train_perplexity=574.9112, train_loss=6.3542156

Batch 260130, train_perplexity=605.407, train_loss=6.405901

Batch 260140, train_perplexity=653.5266, train_loss=6.4823833

Batch 260150, train_perplexity=668.7732, train_loss=6.505445

Batch 260160, train_perplexity=602.9714, train_loss=6.40187

Batch 260170, train_perplexity=623.08374, train_loss=6.434681

Batch 260180, train_perplexity=656.2174, train_loss=6.486492

Batch 260190, train_perplexity=529.35455, train_loss=6.2716584

Batch 260200, train_perplexity=562.2201, train_loss=6.3318934

Batch 260210, train_perplexity=562.7622, train_loss=6.332857

Batch 260220, train_perplexity=639.4104, train_loss=6.4605465

Batch 260230, train_perplexity=715.63324, train_loss=6.573168

Batch 260240, train_perplexity=601.12604, train_loss=6.3988047

Batch 260250, train_perplexity=610.2271, train_loss=6.413831

Batch 260260, train_perplexity=614.1859, train_loss=6.4202976

Batch 260270, train_perplexity=670.06696, train_loss=6.5073776

Batch 260280, train_perplexity=679.28314, train_loss=6.521038

Batch 260290, train_perplexity=637.3636, train_loss=6.4573402

Batch 260300, train_perplexity=643.8043, train_loss=6.467395

Batch 260310, train_perplexity=598.3515, train_loss=6.3941784

Batch 260320, train_perplexity=595.00244, train_loss=6.3885655

Batch 260330, train_perplexity=626.59357, train_loss=6.440298

Batch 260340, train_perplexity=609.9505, train_loss=6.413378

Batch 260350, train_perplexity=590.5205, train_loss=6.3810043

Batch 260360, train_perplexity=577.3714, train_loss=6.3584857

Batch 260370, train_perplexity=718.4643, train_loss=6.577116

Batch 260380, train_perplexity=602.2496, train_loss=6.400672

Batch 260390, train_perplexity=577.16547, train_loss=6.358129

Batch 260400, train_perplexity=702.8378, train_loss=6.555126

Batch 260410, train_perplexity=624.88416, train_loss=6.4375663

Batch 260420, train_perplexity=604.17847, train_loss=6.4038696

Batch 260430, train_perplexity=612.6903, train_loss=6.4178596

Batch 260440, train_perplexity=597.8239, train_loss=6.3932962

Batch 260450, train_perplexity=601.6606, train_loss=6.3996935

Batch 260460, train_perplexity=585.0612, train_loss=6.3717165

Batch 260470, train_perplexity=674.03613, train_loss=6.5132837

Batch 260480, train_perplexity=698.5501, train_loss=6.549007

Batch 260490, train_perplexity=564.76355, train_loss=6.336407

Batch 260500, train_perplexity=624.19745, train_loss=6.4364667

Batch 260510, train_perplexity=719.5803, train_loss=6.578668

Batch 260520, train_perplexity=625.7104, train_loss=6.4388876

Batch 260530, train_perplexity=667.4301, train_loss=6.5034347

Batch 260540, train_perplexity=635.1131, train_loss=6.453803

Batch 260550, train_perplexity=635.3451, train_loss=6.4541683

Batch 260560, train_perplexity=599.3144, train_loss=6.3957863

Batch 260570, train_perplexity=613.3246, train_loss=6.4188943

Batch 260580, train_perplexity=569.4173, train_loss=6.3446136

Batch 260590, train_perplexity=639.6672, train_loss=6.460948

Batch 260600, train_perplexity=653.9356, train_loss=6.483009

Batch 260610, train_perplexity=625.50214, train_loss=6.438555

Batch 260620, train_perplexity=592.1192, train_loss=6.383708

Batch 260630, train_perplexity=596.3269, train_loss=6.390789

Batch 260640, train_perplexity=672.78156, train_loss=6.5114207

Batch 260650, train_perplexity=665.24506, train_loss=6.5001554

Batch 260660, train_perplexity=618.58167, train_loss=6.427429

Batch 260670, train_perplexity=620.7043, train_loss=6.430855

Batch 260680, train_perplexity=657.039, train_loss=6.4877434

Batch 260690, train_perplexity=659.6917, train_loss=6.4917727

Batch 260700, train_perplexity=588.8286, train_loss=6.378135

Batch 260710, train_perplexity=652.146, train_loss=6.4802685

Batch 260720, train_perplexity=686.6208, train_loss=6.531782

Batch 260730, train_perplexity=670.3322, train_loss=6.5077734

Batch 260740, train_perplexity=610.6705, train_loss=6.4145575

Batch 260750, train_perplexity=559.3661, train_loss=6.326804

Batch 260760, train_perplexity=617.873, train_loss=6.426283

Batch 260770, train_perplexity=666.78467, train_loss=6.502467

Batch 260780, train_perplexity=694.6962, train_loss=6.5434747

Batch 260790, train_perplexity=611.0975, train_loss=6.4152565

Batch 260800, train_perplexity=629.92334, train_loss=6.445598

Batch 260810, train_perplexity=616.2526, train_loss=6.423657

Batch 260820, train_perplexity=578.7609, train_loss=6.3608894

Batch 260830, train_perplexity=619.85364, train_loss=6.4294834

Batch 260840, train_perplexity=614.0465, train_loss=6.4200706

Batch 260850, train_perplexity=625.29846, train_loss=6.438229

Batch 260860, train_perplexity=654.9626, train_loss=6.484578

Batch 260870, train_perplexity=708.76654, train_loss=6.563526
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 260880, train_perplexity=663.0927, train_loss=6.496915

Batch 260890, train_perplexity=592.70593, train_loss=6.3846984

Batch 260900, train_perplexity=668.49426, train_loss=6.505028

Batch 260910, train_perplexity=683.67944, train_loss=6.527489

Batch 260920, train_perplexity=621.01044, train_loss=6.431348

Batch 260930, train_perplexity=660.3498, train_loss=6.4927697

Batch 260940, train_perplexity=617.31696, train_loss=6.4253826

Batch 260950, train_perplexity=674.58984, train_loss=6.514105

Batch 260960, train_perplexity=656.73016, train_loss=6.487273

Batch 260970, train_perplexity=617.16095, train_loss=6.42513

Batch 260980, train_perplexity=673.6361, train_loss=6.51269

Batch 260990, train_perplexity=570.3986, train_loss=6.3463354

Batch 261000, train_perplexity=661.5269, train_loss=6.4945507

Batch 261010, train_perplexity=618.4973, train_loss=6.427293

Batch 261020, train_perplexity=677.77576, train_loss=6.5188165

Batch 261030, train_perplexity=678.284, train_loss=6.519566

Batch 261040, train_perplexity=694.47266, train_loss=6.543153

Batch 261050, train_perplexity=665.6474, train_loss=6.50076

Batch 261060, train_perplexity=516.82904, train_loss=6.247712

Batch 261070, train_perplexity=693.01843, train_loss=6.5410566

Batch 261080, train_perplexity=613.3512, train_loss=6.4189377

Batch 261090, train_perplexity=625.6519, train_loss=6.438794

Batch 261100, train_perplexity=557.3554, train_loss=6.323203

Batch 261110, train_perplexity=567.9438, train_loss=6.3420224

Batch 261120, train_perplexity=663.3552, train_loss=6.4973106

Batch 261130, train_perplexity=668.0959, train_loss=6.5044317

Batch 261140, train_perplexity=654.15546, train_loss=6.483345

Batch 261150, train_perplexity=619.19135, train_loss=6.4284143

Batch 261160, train_perplexity=646.89606, train_loss=6.4721856

Batch 261170, train_perplexity=620.22296, train_loss=6.430079

Batch 261180, train_perplexity=623.66876, train_loss=6.4356194

Batch 261190, train_perplexity=599.9938, train_loss=6.3969193

Batch 261200, train_perplexity=622.66943, train_loss=6.4340158

Batch 261210, train_perplexity=629.5267, train_loss=6.444968

Batch 261220, train_perplexity=594.66406, train_loss=6.3879967

Batch 261230, train_perplexity=719.7213, train_loss=6.578864

Batch 261240, train_perplexity=594.55804, train_loss=6.3878183

Batch 261250, train_perplexity=668.5816, train_loss=6.5051584

Batch 261260, train_perplexity=611.81885, train_loss=6.416436

Batch 261270, train_perplexity=596.3235, train_loss=6.3907833

Batch 261280, train_perplexity=608.4939, train_loss=6.410987

Batch 261290, train_perplexity=604.52167, train_loss=6.4044375

Batch 261300, train_perplexity=611.61346, train_loss=6.4161005

Batch 261310, train_perplexity=662.673, train_loss=6.4962816

Batch 261320, train_perplexity=681.462, train_loss=6.5242405

Batch 261330, train_perplexity=611.8086, train_loss=6.4164195

Batch 261340, train_perplexity=567.91156, train_loss=6.3419657

Batch 261350, train_perplexity=714.7772, train_loss=6.571971

Batch 261360, train_perplexity=621.8229, train_loss=6.4326553

Batch 261370, train_perplexity=682.02637, train_loss=6.5250683

Batch 261380, train_perplexity=660.4446, train_loss=6.4929132

Batch 261390, train_perplexity=581.8207, train_loss=6.3661623

Batch 261400, train_perplexity=562.50836, train_loss=6.332406

Batch 261410, train_perplexity=690.5267, train_loss=6.5374546

Batch 261420, train_perplexity=664.84296, train_loss=6.499551

Batch 261430, train_perplexity=584.78314, train_loss=6.371241

Batch 261440, train_perplexity=659.3188, train_loss=6.491207

Batch 261450, train_perplexity=642.0232, train_loss=6.4646244

Batch 261460, train_perplexity=601.6646, train_loss=6.3997

Batch 261470, train_perplexity=682.7493, train_loss=6.526128

Batch 261480, train_perplexity=642.6321, train_loss=6.4655724

Batch 261490, train_perplexity=633.6251, train_loss=6.4514575

Batch 261500, train_perplexity=679.40594, train_loss=6.521219

Batch 261510, train_perplexity=625.46906, train_loss=6.438502

Batch 261520, train_perplexity=662.38104, train_loss=6.495841

Batch 261530, train_perplexity=686.7878, train_loss=6.5320253

Batch 261540, train_perplexity=591.83356, train_loss=6.3832254

Batch 261550, train_perplexity=571.12964, train_loss=6.347616

Batch 261560, train_perplexity=713.7514, train_loss=6.5705347

Batch 261570, train_perplexity=583.1528, train_loss=6.368449

Batch 261580, train_perplexity=646.28467, train_loss=6.47124

Batch 261590, train_perplexity=574.31665, train_loss=6.353181

Batch 261600, train_perplexity=650.3573, train_loss=6.477522

Batch 261610, train_perplexity=618.7256, train_loss=6.427662

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Batch 261620, train_perplexity=658.3898, train_loss=6.489797

Batch 261630, train_perplexity=600.7736, train_loss=6.398218

Batch 261640, train_perplexity=612.93396, train_loss=6.418257

Batch 261650, train_perplexity=583.1928, train_loss=6.368518

Batch 261660, train_perplexity=596.9414, train_loss=6.391819

Batch 261670, train_perplexity=620.1304, train_loss=6.4299297

Batch 261680, train_perplexity=551.26953, train_loss=6.312224

Batch 261690, train_perplexity=604.2165, train_loss=6.4039326

Batch 261700, train_perplexity=665.3815, train_loss=6.5003605

Batch 261710, train_perplexity=552.2541, train_loss=6.314008

Batch 261720, train_perplexity=597.0411, train_loss=6.391986

Batch 261730, train_perplexity=735.7372, train_loss=6.600873

Batch 261740, train_perplexity=639.0891, train_loss=6.460044

Batch 261750, train_perplexity=584.182, train_loss=6.3702126

Batch 261760, train_perplexity=678.2429, train_loss=6.5195055

Batch 261770, train_perplexity=589.1241, train_loss=6.378637

Batch 261780, train_perplexity=736.1281, train_loss=6.601404

Batch 261790, train_perplexity=580.66547, train_loss=6.364175

Batch 261800, train_perplexity=633.083, train_loss=6.4506016

Batch 261810, train_perplexity=650.6371, train_loss=6.477952

Batch 261820, train_perplexity=698.06396, train_loss=6.5483108

Batch 261830, train_perplexity=603.8965, train_loss=6.403403

Batch 261840, train_perplexity=609.25226, train_loss=6.4122324

Batch 261850, train_perplexity=659.81, train_loss=6.491952

Batch 261860, train_perplexity=645.9775, train_loss=6.4707646

Batch 261870, train_perplexity=654.9776, train_loss=6.484601

Batch 261880, train_perplexity=675.8687, train_loss=6.515999

Batch 261890, train_perplexity=593.9596, train_loss=6.3868113

Batch 261900, train_perplexity=648.29333, train_loss=6.4743433

Batch 261910, train_perplexity=602.9234, train_loss=6.40179

Batch 261920, train_perplexity=609.76697, train_loss=6.413077

Batch 261930, train_perplexity=718.3016, train_loss=6.5768895

Batch 261940, train_perplexity=640.0296, train_loss=6.4615145

Batch 261950, train_perplexity=643.8399, train_loss=6.46745

Batch 261960, train_perplexity=623.74963, train_loss=6.435749

Batch 261970, train_perplexity=617.94543, train_loss=6.4264

Batch 261980, train_perplexity=651.39294, train_loss=6.479113

Batch 261990, train_perplexity=598.98236, train_loss=6.395232

Batch 262000, train_perplexity=630.1928, train_loss=6.446026

Batch 262010, train_perplexity=636.0362, train_loss=6.4552555

Batch 262020, train_perplexity=612.9717, train_loss=6.4183187

Batch 262030, train_perplexity=619.8643, train_loss=6.4295006

Batch 262040, train_perplexity=603.1152, train_loss=6.402108

Batch 262050, train_perplexity=650.4838, train_loss=6.4777164

Batch 262060, train_perplexity=680.9371, train_loss=6.52347

Batch 262070, train_perplexity=568.2041, train_loss=6.3424807

Batch 262080, train_perplexity=618.379, train_loss=6.4271016

Batch 262090, train_perplexity=621.23846, train_loss=6.431715

Batch 262100, train_perplexity=597.3748, train_loss=6.3925447

Batch 262110, train_perplexity=616.4398, train_loss=6.4239607

Batch 262120, train_perplexity=680.7923, train_loss=6.5232573
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 262130, train_perplexity=640.6947, train_loss=6.462553

Batch 262140, train_perplexity=679.40106, train_loss=6.5212116

Batch 262150, train_perplexity=635.1258, train_loss=6.453823

Batch 262160, train_perplexity=699.67053, train_loss=6.5506096

Batch 262170, train_perplexity=680.5722, train_loss=6.522934

Batch 262180, train_perplexity=684.4294, train_loss=6.5285854

Batch 262190, train_perplexity=614.1583, train_loss=6.420253

Batch 262200, train_perplexity=616.7203, train_loss=6.4244156

Batch 262210, train_perplexity=616.4921, train_loss=6.4240456

Batch 262220, train_perplexity=614.248, train_loss=6.4203987

Batch 262230, train_perplexity=593.2542, train_loss=6.385623

Batch 262240, train_perplexity=690.7279, train_loss=6.537746

Batch 262250, train_perplexity=610.9294, train_loss=6.4149814

Batch 262260, train_perplexity=687.3159, train_loss=6.532794

Batch 262270, train_perplexity=610.8347, train_loss=6.4148264

Batch 262280, train_perplexity=675.323, train_loss=6.515191

Batch 262290, train_perplexity=610.73047, train_loss=6.4146557

Batch 262300, train_perplexity=648.14624, train_loss=6.4741163

Batch 262310, train_perplexity=635.0804, train_loss=6.4537516

Batch 262320, train_perplexity=588.0284, train_loss=6.3767753

Batch 262330, train_perplexity=609.82336, train_loss=6.4131694

Batch 262340, train_perplexity=604.4672, train_loss=6.4043474

Batch 262350, train_perplexity=607.06024, train_loss=6.408628

Batch 262360, train_perplexity=623.8436, train_loss=6.4358997

Batch 262370, train_perplexity=608.03564, train_loss=6.4102335

Batch 262380, train_perplexity=645.0642, train_loss=6.46935

Batch 262390, train_perplexity=604.84863, train_loss=6.4049783

Batch 262400, train_perplexity=700.24866, train_loss=6.5514355

Batch 262410, train_perplexity=622.9994, train_loss=6.4345455

Batch 262420, train_perplexity=653.5715, train_loss=6.482452

Batch 262430, train_perplexity=641.89734, train_loss=6.4644284

Batch 262440, train_perplexity=582.79865, train_loss=6.3678417

Batch 262450, train_perplexity=628.7569, train_loss=6.4437447

Batch 262460, train_perplexity=638.08636, train_loss=6.4584737

Batch 262470, train_perplexity=699.403, train_loss=6.550227

Batch 262480, train_perplexity=606.5203, train_loss=6.407738

Batch 262490, train_perplexity=634.70166, train_loss=6.453155

Batch 262500, train_perplexity=566.14923, train_loss=6.3388577

Batch 262510, train_perplexity=644.4841, train_loss=6.46845

Batch 262520, train_perplexity=586.7767, train_loss=6.3746443

Batch 262530, train_perplexity=618.14703, train_loss=6.4267263

Batch 262540, train_perplexity=600.01294, train_loss=6.396951

Batch 262550, train_perplexity=582.2747, train_loss=6.3669424

Batch 262560, train_perplexity=637.1709, train_loss=6.457038

Batch 262570, train_perplexity=657.2496, train_loss=6.488064

Batch 262580, train_perplexity=667.2605, train_loss=6.5031805

Batch 262590, train_perplexity=602.67474, train_loss=6.4013777

Batch 262600, train_perplexity=667.0216, train_loss=6.5028224

Batch 262610, train_perplexity=644.59717, train_loss=6.4686255

Batch 262620, train_perplexity=606.9355, train_loss=6.4084225

Batch 262630, train_perplexity=661.35474, train_loss=6.4942904

Batch 262640, train_perplexity=661.5118, train_loss=6.494528

Batch 262650, train_perplexity=668.5398, train_loss=6.505096

Batch 262660, train_perplexity=608.1458, train_loss=6.4104147

Batch 262670, train_perplexity=660.9651, train_loss=6.493701

Batch 262680, train_perplexity=611.701, train_loss=6.4162436

Batch 262690, train_perplexity=605.5932, train_loss=6.4062085

Batch 262700, train_perplexity=592.39825, train_loss=6.384179

Batch 262710, train_perplexity=660.2519, train_loss=6.4926214

Batch 262720, train_perplexity=632.437, train_loss=6.4495807

Batch 262730, train_perplexity=618.2785, train_loss=6.426939

Batch 262740, train_perplexity=632.66235, train_loss=6.449937

Batch 262750, train_perplexity=627.797, train_loss=6.442217

Batch 262760, train_perplexity=636.8234, train_loss=6.4564924

Batch 262770, train_perplexity=689.7461, train_loss=6.5363235

Batch 262780, train_perplexity=688.06683, train_loss=6.533886

Batch 262790, train_perplexity=646.79395, train_loss=6.472028

Batch 262800, train_perplexity=583.57, train_loss=6.3691645

Batch 262810, train_perplexity=648.2872, train_loss=6.474334

Batch 262820, train_perplexity=600.3681, train_loss=6.397543

Batch 262830, train_perplexity=681.7217, train_loss=6.5246215

Batch 262840, train_perplexity=614.81793, train_loss=6.421326

Batch 262850, train_perplexity=624.2367, train_loss=6.4365296

Batch 262860, train_perplexity=636.5077, train_loss=6.4559965

Batch 262870, train_perplexity=639.27075, train_loss=6.460328

Batch 262880, train_perplexity=536.2946, train_loss=6.2846837

Batch 262890, train_perplexity=590.6078, train_loss=6.381152

Batch 262900, train_perplexity=659.1333, train_loss=6.490926

Batch 262910, train_perplexity=662.18085, train_loss=6.4955387

Batch 262920, train_perplexity=605.55884, train_loss=6.406152

Batch 262930, train_perplexity=640.0162, train_loss=6.4614935

Batch 262940, train_perplexity=655.4368, train_loss=6.485302

Batch 262950, train_perplexity=655.5019, train_loss=6.485401

Batch 262960, train_perplexity=650.5012, train_loss=6.477743

Batch 262970, train_perplexity=658.33545, train_loss=6.4897146

Batch 262980, train_perplexity=611.3088, train_loss=6.415602

Batch 262990, train_perplexity=687.94574, train_loss=6.53371

Batch 263000, train_perplexity=669.79156, train_loss=6.5069666

Batch 263010, train_perplexity=666.11224, train_loss=6.501458

Batch 263020, train_perplexity=608.8242, train_loss=6.4115295

Batch 263030, train_perplexity=606.9528, train_loss=6.408451

Batch 263040, train_perplexity=643.3765, train_loss=6.46673

Batch 263050, train_perplexity=595.59656, train_loss=6.3895636

Batch 263060, train_perplexity=639.65314, train_loss=6.460926

Batch 263070, train_perplexity=631.3996, train_loss=6.447939

Batch 263080, train_perplexity=594.08984, train_loss=6.3870306

Batch 263090, train_perplexity=630.6912, train_loss=6.4468164

Batch 263100, train_perplexity=643.2793, train_loss=6.466579

Batch 263110, train_perplexity=628.3178, train_loss=6.443046

Batch 263120, train_perplexity=650.3874, train_loss=6.477568

Batch 263130, train_perplexity=620.1203, train_loss=6.4299135

Batch 263140, train_perplexity=659.02014, train_loss=6.490754

Batch 263150, train_perplexity=599.38354, train_loss=6.3959017

Batch 263160, train_perplexity=627.4268, train_loss=6.441627

Batch 263170, train_perplexity=698.7507, train_loss=6.549294

Batch 263180, train_perplexity=651.0896, train_loss=6.478647

Batch 263190, train_perplexity=611.12665, train_loss=6.415304

Batch 263200, train_perplexity=625.1944, train_loss=6.4380627

Batch 263210, train_perplexity=624.0203, train_loss=6.436183

Batch 263220, train_perplexity=613.51965, train_loss=6.4192123

Batch 263230, train_perplexity=649.9388, train_loss=6.476878

Batch 263240, train_perplexity=663.0396, train_loss=6.4968348

Batch 263250, train_perplexity=641.2442, train_loss=6.4634104

Batch 263260, train_perplexity=668.072, train_loss=6.504396

Batch 263270, train_perplexity=570.6867, train_loss=6.3468404

Batch 263280, train_perplexity=685.86426, train_loss=6.5306797

Batch 263290, train_perplexity=613.04944, train_loss=6.4184456

Batch 263300, train_perplexity=638.8551, train_loss=6.4596777

Batch 263310, train_perplexity=665.6741, train_loss=6.5008

Batch 263320, train_perplexity=693.5963, train_loss=6.54189

Batch 263330, train_perplexity=685.9084, train_loss=6.530744

Batch 263340, train_perplexity=575.89075, train_loss=6.355918

Batch 263350, train_perplexity=659.996, train_loss=6.4922338

Batch 263360, train_perplexity=632.7426, train_loss=6.4500637

Batch 263370, train_perplexity=605.7209, train_loss=6.4064193

Batch 263380, train_perplexity=649.54285, train_loss=6.476269

Batch 263390, train_perplexity=624.6008, train_loss=6.437113

Batch 263400, train_perplexity=617.1233, train_loss=6.425069

Batch 263410, train_perplexity=665.9655, train_loss=6.501238

Batch 263420, train_perplexity=642.4565, train_loss=6.465299

Batch 263430, train_perplexity=613.5059, train_loss=6.41919
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 263440, train_perplexity=633.28534, train_loss=6.450921

Batch 263450, train_perplexity=691.5775, train_loss=6.5389752

Batch 263460, train_perplexity=655.1381, train_loss=6.484846

Batch 263470, train_perplexity=620.3658, train_loss=6.4303093

Batch 263480, train_perplexity=631.3487, train_loss=6.4478583

Batch 263490, train_perplexity=557.75793, train_loss=6.323925

Batch 263500, train_perplexity=656.58923, train_loss=6.4870586

Batch 263510, train_perplexity=575.39777, train_loss=6.3550615

Batch 263520, train_perplexity=586.6788, train_loss=6.3744774

Batch 263530, train_perplexity=623.46655, train_loss=6.435295

Batch 263540, train_perplexity=649.24744, train_loss=6.475814

Batch 263550, train_perplexity=608.9441, train_loss=6.4117265

Batch 263560, train_perplexity=697.33704, train_loss=6.547269

Batch 263570, train_perplexity=701.8934, train_loss=6.5537815

Batch 263580, train_perplexity=642.6572, train_loss=6.4656115

Batch 263590, train_perplexity=636.8313, train_loss=6.456505

Batch 263600, train_perplexity=589.2356, train_loss=6.378826

Batch 263610, train_perplexity=623.69727, train_loss=6.435665

Batch 263620, train_perplexity=575.00116, train_loss=6.354372

Batch 263630, train_perplexity=590.1813, train_loss=6.3804297

Batch 263640, train_perplexity=585.1159, train_loss=6.37181

Batch 263650, train_perplexity=624.18524, train_loss=6.436447

Batch 263660, train_perplexity=629.8933, train_loss=6.4455504

Batch 263670, train_perplexity=601.6764, train_loss=6.3997197

Batch 263680, train_perplexity=591.8855, train_loss=6.383313

Batch 263690, train_perplexity=607.108, train_loss=6.4087067

Batch 263700, train_perplexity=658.9897, train_loss=6.490708

Batch 263710, train_perplexity=605.38995, train_loss=6.405873

Batch 263720, train_perplexity=599.9165, train_loss=6.3967905

Batch 263730, train_perplexity=535.89124, train_loss=6.2839313

Batch 263740, train_perplexity=660.0234, train_loss=6.492275

Batch 263750, train_perplexity=631.8993, train_loss=6.44873

Batch 263760, train_perplexity=612.2604, train_loss=6.4171576

Batch 263770, train_perplexity=629.7104, train_loss=6.44526

Batch 263780, train_perplexity=613.8673, train_loss=6.419779

Batch 263790, train_perplexity=631.6983, train_loss=6.448412

Batch 263800, train_perplexity=611.46155, train_loss=6.415852

Batch 263810, train_perplexity=651.41254, train_loss=6.479143

Batch 263820, train_perplexity=605.40466, train_loss=6.405897

Batch 263830, train_perplexity=623.68835, train_loss=6.435651

Batch 263840, train_perplexity=632.1916, train_loss=6.4491925

Batch 263850, train_perplexity=615.4597, train_loss=6.4223695

Batch 263860, train_perplexity=590.4315, train_loss=6.3808537

Batch 263870, train_perplexity=615.12756, train_loss=6.4218297

Batch 263880, train_perplexity=652.7371, train_loss=6.4811745

Batch 263890, train_perplexity=719.66296, train_loss=6.578783

Batch 263900, train_perplexity=655.2456, train_loss=6.48501

Batch 263910, train_perplexity=637.20496, train_loss=6.4570913

Batch 263920, train_perplexity=598.5935, train_loss=6.3945827

Batch 263930, train_perplexity=534.60876, train_loss=6.281535

Batch 263940, train_perplexity=658.1487, train_loss=6.489431

Batch 263950, train_perplexity=604.4505, train_loss=6.40432

Batch 263960, train_perplexity=692.1618, train_loss=6.5398197

Batch 263970, train_perplexity=698.4156, train_loss=6.5488143

Batch 263980, train_perplexity=671.4039, train_loss=6.509371

Batch 263990, train_perplexity=719.6115, train_loss=6.5787115

Batch 264000, train_perplexity=660.6651, train_loss=6.493247

Batch 264010, train_perplexity=659.82135, train_loss=6.491969

Batch 264020, train_perplexity=636.6537, train_loss=6.456226

Batch 264030, train_perplexity=649.46106, train_loss=6.476143

Batch 264040, train_perplexity=594.19696, train_loss=6.387211

Batch 264050, train_perplexity=611.8363, train_loss=6.416465

Batch 264060, train_perplexity=622.5118, train_loss=6.4337626

Batch 264070, train_perplexity=738.2492, train_loss=6.6042814

Batch 264080, train_perplexity=585.5932, train_loss=6.3726254

Batch 264090, train_perplexity=676.15106, train_loss=6.5164165

Batch 264100, train_perplexity=685.1092, train_loss=6.529578

Batch 264110, train_perplexity=563.5893, train_loss=6.334326

Batch 264120, train_perplexity=652.91797, train_loss=6.4814515

Batch 264130, train_perplexity=593.58527, train_loss=6.386181

Batch 264140, train_perplexity=562.49603, train_loss=6.332384

Batch 264150, train_perplexity=643.86237, train_loss=6.467485

Batch 264160, train_perplexity=627.44476, train_loss=6.4416556

Batch 264170, train_perplexity=680.1654, train_loss=6.522336

Batch 264180, train_perplexity=649.89044, train_loss=6.476804

Batch 264190, train_perplexity=683.75476, train_loss=6.5275993

Batch 264200, train_perplexity=677.6714, train_loss=6.5186625

Batch 264210, train_perplexity=636.91455, train_loss=6.4566355

Batch 264220, train_perplexity=612.31964, train_loss=6.4172544

Batch 264230, train_perplexity=627.5866, train_loss=6.4418817

Batch 264240, train_perplexity=642.7893, train_loss=6.465817

Batch 264250, train_perplexity=609.53094, train_loss=6.4126897

Batch 264260, train_perplexity=640.3575, train_loss=6.4620266

Batch 264270, train_perplexity=688.25256, train_loss=6.534156

Batch 264280, train_perplexity=704.62134, train_loss=6.5576606

Batch 264290, train_perplexity=645.3122, train_loss=6.469734

Batch 264300, train_perplexity=619.658, train_loss=6.4291677

Batch 264310, train_perplexity=670.9359, train_loss=6.5086737

Batch 264320, train_perplexity=613.93964, train_loss=6.4198966

Batch 264330, train_perplexity=614.48, train_loss=6.4207764

Batch 264340, train_perplexity=597.1894, train_loss=6.3922343

Batch 264350, train_perplexity=583.1133, train_loss=6.3683815

Batch 264360, train_perplexity=568.11847, train_loss=6.34233

Batch 264370, train_perplexity=637.5985, train_loss=6.457709

Batch 264380, train_perplexity=616.9933, train_loss=6.424858

Batch 264390, train_perplexity=722.8084, train_loss=6.583144

Batch 264400, train_perplexity=656.521, train_loss=6.4869547

Batch 264410, train_perplexity=604.86163, train_loss=6.4049997

Batch 264420, train_perplexity=618.4855, train_loss=6.4272738

Batch 264430, train_perplexity=557.8978, train_loss=6.324176

Batch 264440, train_perplexity=574.02423, train_loss=6.3526716

Batch 264450, train_perplexity=690.1468, train_loss=6.5369043

Batch 264460, train_perplexity=646.3364, train_loss=6.47132

Batch 264470, train_perplexity=626.6581, train_loss=6.440401

Batch 264480, train_perplexity=572.7512, train_loss=6.3504515

Batch 264490, train_perplexity=608.729, train_loss=6.411373

Batch 264500, train_perplexity=611.32776, train_loss=6.415633

Batch 264510, train_perplexity=615.5475, train_loss=6.422512

Batch 264520, train_perplexity=599.9188, train_loss=6.3967943

Batch 264530, train_perplexity=621.0513, train_loss=6.4314137

Batch 264540, train_perplexity=680.59753, train_loss=6.522971

Batch 264550, train_perplexity=670.6858, train_loss=6.508301

Batch 264560, train_perplexity=641.0929, train_loss=6.4631743

Batch 264570, train_perplexity=630.6756, train_loss=6.4467916

Batch 264580, train_perplexity=605.4965, train_loss=6.406049

Batch 264590, train_perplexity=642.9229, train_loss=6.466025

Batch 264600, train_perplexity=679.3771, train_loss=6.5211763

Batch 264610, train_perplexity=627.7479, train_loss=6.4421387

Batch 264620, train_perplexity=674.39624, train_loss=6.513818

Batch 264630, train_perplexity=593.9757, train_loss=6.3868384

Batch 264640, train_perplexity=644.03644, train_loss=6.4677553

Batch 264650, train_perplexity=660.92316, train_loss=6.4936376

Batch 264660, train_perplexity=634.2185, train_loss=6.4523935

Batch 264670, train_perplexity=595.8903, train_loss=6.3900566

Batch 264680, train_perplexity=667.69495, train_loss=6.5038314

Batch 264690, train_perplexity=591.71814, train_loss=6.3830304

Batch 264700, train_perplexity=672.78894, train_loss=6.5114317

Batch 264710, train_perplexity=643.97687, train_loss=6.467663

Batch 264720, train_perplexity=672.6908, train_loss=6.511286

Batch 264730, train_perplexity=637.8336, train_loss=6.4580774

Batch 264740, train_perplexity=603.0999, train_loss=6.402083
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 264750, train_perplexity=587.15674, train_loss=6.375292

Batch 264760, train_perplexity=573.6899, train_loss=6.352089

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Batch 264770, train_perplexity=588.99963, train_loss=6.3784256

Batch 264780, train_perplexity=668.8411, train_loss=6.5055466

Batch 264790, train_perplexity=651.6869, train_loss=6.479564

Batch 264800, train_perplexity=629.0334, train_loss=6.4441843

Batch 264810, train_perplexity=581.7358, train_loss=6.3660164

Batch 264820, train_perplexity=552.7217, train_loss=6.3148546

Batch 264830, train_perplexity=615.6857, train_loss=6.4227366

Batch 264840, train_perplexity=546.83844, train_loss=6.3041534

Batch 264850, train_perplexity=671.27, train_loss=6.5091715

Batch 264860, train_perplexity=601.8879, train_loss=6.400071

Batch 264870, train_perplexity=650.00476, train_loss=6.4769797

Batch 264880, train_perplexity=588.97833, train_loss=6.3783894

Batch 264890, train_perplexity=618.9481, train_loss=6.4280214

Batch 264900, train_perplexity=697.9964, train_loss=6.548214

Batch 264910, train_perplexity=605.7301, train_loss=6.4064345

Batch 264920, train_perplexity=617.59314, train_loss=6.42583

Batch 264930, train_perplexity=617.49304, train_loss=6.425668

Batch 264940, train_perplexity=567.0629, train_loss=6.3404703

Batch 264950, train_perplexity=624.48706, train_loss=6.4369307

Batch 264960, train_perplexity=535.3835, train_loss=6.2829833

Batch 264970, train_perplexity=637.7448, train_loss=6.457938

Batch 264980, train_perplexity=617.28754, train_loss=6.425335

Batch 264990, train_perplexity=568.85175, train_loss=6.34362

Batch 265000, train_perplexity=575.9067, train_loss=6.3559456

Batch 265010, train_perplexity=691.70746, train_loss=6.539163

Batch 265020, train_perplexity=618.04443, train_loss=6.4265604

Batch 265030, train_perplexity=590.8613, train_loss=6.3815813

Batch 265040, train_perplexity=558.7759, train_loss=6.3257484

Batch 265050, train_perplexity=593.10144, train_loss=6.3853655

Batch 265060, train_perplexity=658.4243, train_loss=6.4898496

Batch 265070, train_perplexity=608.05273, train_loss=6.4102616

Batch 265080, train_perplexity=609.3853, train_loss=6.412451

Batch 265090, train_perplexity=611.9904, train_loss=6.4167166

Batch 265100, train_perplexity=622.9483, train_loss=6.4344635

Batch 265110, train_perplexity=634.7474, train_loss=6.453227

Batch 265120, train_perplexity=687.9884, train_loss=6.533772

Batch 265130, train_perplexity=676.8934, train_loss=6.5175138

Batch 265140, train_perplexity=640.05524, train_loss=6.4615545

Batch 265150, train_perplexity=640.35565, train_loss=6.4620237

Batch 265160, train_perplexity=635.2421, train_loss=6.454006

Batch 265170, train_perplexity=669.99664, train_loss=6.5072727

Batch 265180, train_perplexity=621.0436, train_loss=6.4314013

Batch 265190, train_perplexity=634.8058, train_loss=6.453319

Batch 265200, train_perplexity=612.981, train_loss=6.418334

Batch 265210, train_perplexity=620.27704, train_loss=6.4301662

Batch 265220, train_perplexity=599.8956, train_loss=6.3967557

Batch 265230, train_perplexity=676.9938, train_loss=6.517662

Batch 265240, train_perplexity=654.8727, train_loss=6.484441

Batch 265250, train_perplexity=665.5884, train_loss=6.5006714

Batch 265260, train_perplexity=548.65216, train_loss=6.3074646

Batch 265270, train_perplexity=595.6622, train_loss=6.3896737

Batch 265280, train_perplexity=655.8057, train_loss=6.4858646

Batch 265290, train_perplexity=627.332, train_loss=6.441476

Batch 265300, train_perplexity=674.7661, train_loss=6.514366

Batch 265310, train_perplexity=674.18884, train_loss=6.51351

Batch 265320, train_perplexity=664.56146, train_loss=6.4991274

Batch 265330, train_perplexity=656.60614, train_loss=6.4870844

Batch 265340, train_perplexity=663.0643, train_loss=6.496872

Batch 265350, train_perplexity=678.48035, train_loss=6.5198555

Batch 265360, train_perplexity=663.695, train_loss=6.4978228

Batch 265370, train_perplexity=658.94696, train_loss=6.490643

Batch 265380, train_perplexity=659.92957, train_loss=6.492133

Batch 265390, train_perplexity=577.38434, train_loss=6.358508

Batch 265400, train_perplexity=660.0123, train_loss=6.4922585

Batch 265410, train_perplexity=614.057, train_loss=6.420088

Batch 265420, train_perplexity=664.95264, train_loss=6.499716

Batch 265430, train_perplexity=607.7, train_loss=6.4096813

Batch 265440, train_perplexity=639.8621, train_loss=6.4612527

Batch 265450, train_perplexity=640.72766, train_loss=6.4626045

Batch 265460, train_perplexity=565.5281, train_loss=6.33776

Batch 265470, train_perplexity=644.0893, train_loss=6.4678373

Batch 265480, train_perplexity=601.5906, train_loss=6.399577

Batch 265490, train_perplexity=640.64246, train_loss=6.4624715

Batch 265500, train_perplexity=611.168, train_loss=6.415372

Batch 265510, train_perplexity=603.87, train_loss=6.403359

Batch 265520, train_perplexity=722.4549, train_loss=6.582655

Batch 265530, train_perplexity=599.85614, train_loss=6.39669

Batch 265540, train_perplexity=644.8557, train_loss=6.4690266

Batch 265550, train_perplexity=631.6218, train_loss=6.448291

Batch 265560, train_perplexity=605.53577, train_loss=6.4061136

Batch 265570, train_perplexity=570.4704, train_loss=6.3464613

Batch 265580, train_perplexity=581.35675, train_loss=6.3653646

Batch 265590, train_perplexity=636.0981, train_loss=6.455353

Batch 265600, train_perplexity=581.1303, train_loss=6.364975

Batch 265610, train_perplexity=581.76935, train_loss=6.366074

Batch 265620, train_perplexity=598.0109, train_loss=6.393609

Batch 265630, train_perplexity=639.6678, train_loss=6.460949

Batch 265640, train_perplexity=651.6996, train_loss=6.4795837

Batch 265650, train_perplexity=689.6662, train_loss=6.5362077

Batch 265660, train_perplexity=721.6795, train_loss=6.581581

Batch 265670, train_perplexity=650.232, train_loss=6.4773293

Batch 265680, train_perplexity=678.4826, train_loss=6.519859

Batch 265690, train_perplexity=646.9001, train_loss=6.472192

Batch 265700, train_perplexity=602.1448, train_loss=6.400498

Batch 265710, train_perplexity=584.2221, train_loss=6.370281

Batch 265720, train_perplexity=545.5045, train_loss=6.301711

Batch 265730, train_perplexity=615.76, train_loss=6.4228573

Batch 265740, train_perplexity=576.29785, train_loss=6.3566246

Batch 265750, train_perplexity=645.52423, train_loss=6.4700627

Batch 265760, train_perplexity=554.7035, train_loss=6.318434

Batch 265770, train_perplexity=637.3292, train_loss=6.4572864

Batch 265780, train_perplexity=606.38007, train_loss=6.407507

Batch 265790, train_perplexity=691.35956, train_loss=6.53866

Batch 265800, train_perplexity=691.81793, train_loss=6.539323

Batch 265810, train_perplexity=549.77563, train_loss=6.30951

Batch 265820, train_perplexity=644.5108, train_loss=6.4684916

Batch 265830, train_perplexity=710.3561, train_loss=6.5657663

Batch 265840, train_perplexity=609.4458, train_loss=6.41255

Batch 265850, train_perplexity=606.9968, train_loss=6.4085236

Batch 265860, train_perplexity=607.1028, train_loss=6.408698

Batch 265870, train_perplexity=599.6774, train_loss=6.396392

Batch 265880, train_perplexity=638.62, train_loss=6.4593096

Batch 265890, train_perplexity=533.1709, train_loss=6.278842

Batch 265900, train_perplexity=650.1635, train_loss=6.477224

Batch 265910, train_perplexity=653.91095, train_loss=6.482971

Batch 265920, train_perplexity=586.1128, train_loss=6.3735123

Batch 265930, train_perplexity=637.273, train_loss=6.457198

Batch 265940, train_perplexity=558.26215, train_loss=6.3248286

Batch 265950, train_perplexity=615.3919, train_loss=6.4222593

Batch 265960, train_perplexity=614.6318, train_loss=6.4210234

Batch 265970, train_perplexity=578.9867, train_loss=6.3612795

Batch 265980, train_perplexity=625.8118, train_loss=6.4390497

Batch 265990, train_perplexity=542.47015, train_loss=6.296133
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 266000, train_perplexity=614.8519, train_loss=6.4213815

Batch 266010, train_perplexity=637.2663, train_loss=6.4571877

Batch 266020, train_perplexity=603.0893, train_loss=6.4020653

Batch 266030, train_perplexity=602.3245, train_loss=6.4007964

Batch 266040, train_perplexity=601.5522, train_loss=6.3995132

Batch 266050, train_perplexity=681.0644, train_loss=6.523657

Batch 266060, train_perplexity=577.80133, train_loss=6.35923

Batch 266070, train_perplexity=652.34784, train_loss=6.480578

Batch 266080, train_perplexity=645.12787, train_loss=6.4694486

Batch 266090, train_perplexity=601.3181, train_loss=6.399124

Batch 266100, train_perplexity=585.3947, train_loss=6.3722863

Batch 266110, train_perplexity=663.953, train_loss=6.4982114

Batch 266120, train_perplexity=698.64703, train_loss=6.5491457

Batch 266130, train_perplexity=603.08813, train_loss=6.4020634

Batch 266140, train_perplexity=529.94073, train_loss=6.272765

Batch 266150, train_perplexity=671.39105, train_loss=6.5093517

Batch 266160, train_perplexity=646.9599, train_loss=6.4722843

Batch 266170, train_perplexity=576.05115, train_loss=6.3561964

Batch 266180, train_perplexity=659.16223, train_loss=6.4909697

Batch 266190, train_perplexity=610.3892, train_loss=6.414097

Batch 266200, train_perplexity=655.2416, train_loss=6.485004

Batch 266210, train_perplexity=665.9309, train_loss=6.501186

Batch 266220, train_perplexity=604.5926, train_loss=6.404555

Batch 266230, train_perplexity=605.33136, train_loss=6.405776

Batch 266240, train_perplexity=622.532, train_loss=6.433795

Batch 266250, train_perplexity=623.5902, train_loss=6.4354935

Batch 266260, train_perplexity=579.47943, train_loss=6.36213

Batch 266270, train_perplexity=621.0418, train_loss=6.4313984

Batch 266280, train_perplexity=611.59247, train_loss=6.416066

Batch 266290, train_perplexity=687.10944, train_loss=6.5324936

Batch 266300, train_perplexity=657.54767, train_loss=6.4885173

Batch 266310, train_perplexity=586.33813, train_loss=6.3738966

Batch 266320, train_perplexity=619.1211, train_loss=6.428301

Batch 266330, train_perplexity=558.51215, train_loss=6.3252764

Batch 266340, train_perplexity=636.3226, train_loss=6.4557056

Batch 266350, train_perplexity=619.5115, train_loss=6.428931

Batch 266360, train_perplexity=642.4176, train_loss=6.4652386

Batch 266370, train_perplexity=608.1035, train_loss=6.410345

Batch 266380, train_perplexity=680.14075, train_loss=6.5223

Batch 266390, train_perplexity=595.45856, train_loss=6.389332

Batch 266400, train_perplexity=627.1645, train_loss=6.441209

Batch 266410, train_perplexity=593.92957, train_loss=6.3867607

Batch 266420, train_perplexity=616.00934, train_loss=6.423262

Batch 266430, train_perplexity=572.1526, train_loss=6.349406

Batch 266440, train_perplexity=646.85345, train_loss=6.47212

Batch 266450, train_perplexity=640.5963, train_loss=6.4623995

Batch 266460, train_perplexity=589.65076, train_loss=6.3795304

Batch 266470, train_perplexity=581.36115, train_loss=6.365372

Batch 266480, train_perplexity=671.3392, train_loss=6.5092745

Batch 266490, train_perplexity=606.78906, train_loss=6.408181

Batch 266500, train_perplexity=675.3291, train_loss=6.5152

Batch 266510, train_perplexity=632.04663, train_loss=6.448963

Batch 266520, train_perplexity=742.4863, train_loss=6.6100044

Batch 266530, train_perplexity=608.9885, train_loss=6.4117994

Batch 266540, train_perplexity=616.15063, train_loss=6.4234915

Batch 266550, train_perplexity=620.7878, train_loss=6.4309893

Batch 266560, train_perplexity=640.60364, train_loss=6.462411

Batch 266570, train_perplexity=575.4581, train_loss=6.3551664

Batch 266580, train_perplexity=656.9817, train_loss=6.487656

Batch 266590, train_perplexity=548.155, train_loss=6.306558

Batch 266600, train_perplexity=630.7526, train_loss=6.4469137

Batch 266610, train_perplexity=625.00244, train_loss=6.4377556

Batch 266620, train_perplexity=644.7204, train_loss=6.4688168

Batch 266630, train_perplexity=656.41205, train_loss=6.4867887

Batch 266640, train_perplexity=665.3294, train_loss=6.5002823

Batch 266650, train_perplexity=624.24713, train_loss=6.4365463

Batch 266660, train_perplexity=532.1222, train_loss=6.276873

Batch 266670, train_perplexity=573.0277, train_loss=6.350934

Batch 266680, train_perplexity=677.10254, train_loss=6.5178227

Batch 266690, train_perplexity=694.69653, train_loss=6.543475

Batch 266700, train_perplexity=641.3011, train_loss=6.463499

Batch 266710, train_perplexity=662.8316, train_loss=6.496521

Batch 266720, train_perplexity=665.7452, train_loss=6.500907

Batch 266730, train_perplexity=621.91956, train_loss=6.432811

Batch 266740, train_perplexity=638.23456, train_loss=6.458706

Batch 266750, train_perplexity=619.65, train_loss=6.429155

Batch 266760, train_perplexity=710.3981, train_loss=6.5658255

Batch 266770, train_perplexity=635.6357, train_loss=6.4546256

Batch 266780, train_perplexity=676.40326, train_loss=6.5167894

Batch 266790, train_perplexity=652.5871, train_loss=6.4809446

Batch 266800, train_perplexity=623.06476, train_loss=6.4346504

Batch 266810, train_perplexity=596.56354, train_loss=6.3911858

Batch 266820, train_perplexity=595.9358, train_loss=6.390133

Batch 266830, train_perplexity=619.4855, train_loss=6.4288893

Batch 266840, train_perplexity=607.9884, train_loss=6.410156

Batch 266850, train_perplexity=596.0625, train_loss=6.3903456

Batch 266860, train_perplexity=600.5791, train_loss=6.3978944

Batch 266870, train_perplexity=596.26154, train_loss=6.3906794

Batch 266880, train_perplexity=649.7569, train_loss=6.4765983

Batch 266890, train_perplexity=633.4587, train_loss=6.451195

Batch 266900, train_perplexity=643.06244, train_loss=6.466242

Batch 266910, train_perplexity=685.0582, train_loss=6.529504

Batch 266920, train_perplexity=587.30237, train_loss=6.37554

Batch 266930, train_perplexity=659.737, train_loss=6.4918413

Batch 266940, train_perplexity=622.5462, train_loss=6.433818

Batch 266950, train_perplexity=567.00024, train_loss=6.3403597

Batch 266960, train_perplexity=614.68866, train_loss=6.421116

Batch 266970, train_perplexity=581.7483, train_loss=6.366038

Batch 266980, train_perplexity=609.2331, train_loss=6.412201

Batch 266990, train_perplexity=622.016, train_loss=6.4329658

Batch 267000, train_perplexity=622.56757, train_loss=6.433852

Batch 267010, train_perplexity=698.8223, train_loss=6.5493965

Batch 267020, train_perplexity=662.1278, train_loss=6.4954586

Batch 267030, train_perplexity=576.67773, train_loss=6.3572836

Batch 267040, train_perplexity=594.7364, train_loss=6.3881183

Batch 267050, train_perplexity=634.14954, train_loss=6.452285

Batch 267060, train_perplexity=680.0292, train_loss=6.5221357

Batch 267070, train_perplexity=584.5888, train_loss=6.3709087

Batch 267080, train_perplexity=610.76306, train_loss=6.414709

Batch 267090, train_perplexity=615.0912, train_loss=6.4217706

Batch 267100, train_perplexity=659.6389, train_loss=6.4916925

Batch 267110, train_perplexity=576.6362, train_loss=6.3572116

Batch 267120, train_perplexity=607.2035, train_loss=6.408864

Batch 267130, train_perplexity=569.74945, train_loss=6.3451967

Batch 267140, train_perplexity=633.833, train_loss=6.4517856

Batch 267150, train_perplexity=639.3979, train_loss=6.460527

Batch 267160, train_perplexity=618.2083, train_loss=6.4268255

Batch 267170, train_perplexity=595.2701, train_loss=6.389015

Batch 267180, train_perplexity=652.5912, train_loss=6.480951

Batch 267190, train_perplexity=619.76733, train_loss=6.429344

Batch 267200, train_perplexity=580.8798, train_loss=6.364544

Batch 267210, train_perplexity=667.92993, train_loss=6.5041833

Batch 267220, train_perplexity=597.3771, train_loss=6.3925486

Batch 267230, train_perplexity=623.8549, train_loss=6.435918

Batch 267240, train_perplexity=552.0685, train_loss=6.313672

Batch 267250, train_perplexity=691.6563, train_loss=6.539089

Batch 267260, train_perplexity=657.0164, train_loss=6.487709

Batch 267270, train_perplexity=640.05707, train_loss=6.4615574

Batch 267280, train_perplexity=576.4754, train_loss=6.3569326

Batch 267290, train_perplexity=639.46405, train_loss=6.4606304

Batch 267300, train_perplexity=663.10065, train_loss=6.496927
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 267310, train_perplexity=658.32385, train_loss=6.489697

Batch 267320, train_perplexity=640.1569, train_loss=6.4617133

Batch 267330, train_perplexity=600.22327, train_loss=6.3973017

Batch 267340, train_perplexity=612.562, train_loss=6.41765

Batch 267350, train_perplexity=599.41095, train_loss=6.3959475

Batch 267360, train_perplexity=666.1878, train_loss=6.5015717

Batch 267370, train_perplexity=669.20544, train_loss=6.506091

Batch 267380, train_perplexity=605.3395, train_loss=6.4057894

Batch 267390, train_perplexity=609.78937, train_loss=6.4131136

Batch 267400, train_perplexity=671.31934, train_loss=6.509245

Batch 267410, train_perplexity=660.97705, train_loss=6.493719

Batch 267420, train_perplexity=669.2007, train_loss=6.506084

Batch 267430, train_perplexity=628.0413, train_loss=6.442606

Batch 267440, train_perplexity=533.84705, train_loss=6.2801094

Batch 267450, train_perplexity=549.83856, train_loss=6.3096247

Batch 267460, train_perplexity=593.878, train_loss=6.386674

Batch 267470, train_perplexity=619.3886, train_loss=6.428733

Batch 267480, train_perplexity=617.99347, train_loss=6.426478

Batch 267490, train_perplexity=732.0341, train_loss=6.595827

Batch 267500, train_perplexity=585.8328, train_loss=6.3730345

Batch 267510, train_perplexity=605.0416, train_loss=6.4052973

Batch 267520, train_perplexity=612.2303, train_loss=6.4171085

Batch 267530, train_perplexity=623.88495, train_loss=6.435966

Batch 267540, train_perplexity=655.1141, train_loss=6.4848094

Batch 267550, train_perplexity=661.2693, train_loss=6.494161

Batch 267560, train_perplexity=668.2711, train_loss=6.504694

Batch 267570, train_perplexity=629.5609, train_loss=6.4450226

Batch 267580, train_perplexity=641.4106, train_loss=6.46367

Batch 267590, train_perplexity=662.16034, train_loss=6.4955077

Batch 267600, train_perplexity=650.0733, train_loss=6.477085

Batch 267610, train_perplexity=643.39307, train_loss=6.466756

Batch 267620, train_perplexity=617.45123, train_loss=6.4256

Batch 267630, train_perplexity=617.13446, train_loss=6.425087

Batch 267640, train_perplexity=614.68134, train_loss=6.421104

Batch 267650, train_perplexity=623.8514, train_loss=6.435912

Batch 267660, train_perplexity=616.7494, train_loss=6.424463

Batch 267670, train_perplexity=682.9115, train_loss=6.5263653

Batch 267680, train_perplexity=570.7528, train_loss=6.3469563

Batch 267690, train_perplexity=582.43, train_loss=6.367209

Batch 267700, train_perplexity=653.3534, train_loss=6.482118

Batch 267710, train_perplexity=636.7685, train_loss=6.456406

Batch 267720, train_perplexity=691.1147, train_loss=6.5383058

Batch 267730, train_perplexity=617.10095, train_loss=6.4250326

Batch 267740, train_perplexity=597.87354, train_loss=6.393379

Batch 267750, train_perplexity=599.4727, train_loss=6.3960505

Batch 267760, train_perplexity=655.7095, train_loss=6.485718

Batch 267770, train_perplexity=657.88794, train_loss=6.4890347

Batch 267780, train_perplexity=649.2158, train_loss=6.475765

Batch 267790, train_perplexity=676.96277, train_loss=6.5176163

Batch 267800, train_perplexity=610.9859, train_loss=6.415074

Batch 267810, train_perplexity=602.1232, train_loss=6.400462

Batch 267820, train_perplexity=584.7773, train_loss=6.371231

Batch 267830, train_perplexity=642.6324, train_loss=6.465573

Batch 267840, train_perplexity=617.1357, train_loss=6.425089

Batch 267850, train_perplexity=601.4882, train_loss=6.399407

Batch 267860, train_perplexity=651.00946, train_loss=6.478524

Batch 267870, train_perplexity=627.29785, train_loss=6.4414215

Batch 267880, train_perplexity=630.0162, train_loss=6.4457455

Batch 267890, train_perplexity=626.8213, train_loss=6.4406614

Batch 267900, train_perplexity=639.9722, train_loss=6.461425

Batch 267910, train_perplexity=659.89185, train_loss=6.492076

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Batch 267920, train_perplexity=685.20264, train_loss=6.5297146

Batch 267930, train_perplexity=625.1616, train_loss=6.43801

Batch 267940, train_perplexity=609.13403, train_loss=6.4120383

Batch 267950, train_perplexity=607.8333, train_loss=6.4099007

Batch 267960, train_perplexity=701.9188, train_loss=6.5538177

Batch 267970, train_perplexity=598.2776, train_loss=6.394055

Batch 267980, train_perplexity=688.44653, train_loss=6.5344377

Batch 267990, train_perplexity=644.37006, train_loss=6.468273

Batch 268000, train_perplexity=613.30206, train_loss=6.4188576

Batch 268010, train_perplexity=626.87537, train_loss=6.4407477

Batch 268020, train_perplexity=549.3836, train_loss=6.308797

Batch 268030, train_perplexity=578.024, train_loss=6.3596153

Batch 268040, train_perplexity=615.5727, train_loss=6.422553

Batch 268050, train_perplexity=689.84314, train_loss=6.536464

Batch 268060, train_perplexity=627.37506, train_loss=6.4415445

Batch 268070, train_perplexity=563.07355, train_loss=6.3334103

Batch 268080, train_perplexity=666.0392, train_loss=6.5013485

Batch 268090, train_perplexity=711.37225, train_loss=6.567196

Batch 268100, train_perplexity=696.26715, train_loss=6.5457335

Batch 268110, train_perplexity=648.14343, train_loss=6.474112

Batch 268120, train_perplexity=634.96893, train_loss=6.453576

Batch 268130, train_perplexity=657.0471, train_loss=6.487756

Batch 268140, train_perplexity=585.6714, train_loss=6.372759

Batch 268150, train_perplexity=626.7519, train_loss=6.440551

Batch 268160, train_perplexity=569.00964, train_loss=6.3438973

Batch 268170, train_perplexity=629.76355, train_loss=6.4453444

Batch 268180, train_perplexity=603.0801, train_loss=6.40205

Batch 268190, train_perplexity=655.4959, train_loss=6.485392

Batch 268200, train_perplexity=650.3458, train_loss=6.4775043

Batch 268210, train_perplexity=652.1995, train_loss=6.4803505

Batch 268220, train_perplexity=577.67017, train_loss=6.359003

Batch 268230, train_perplexity=571.6718, train_loss=6.348565

Batch 268240, train_perplexity=620.53766, train_loss=6.4305863

Batch 268250, train_perplexity=675.9151, train_loss=6.5160675

Batch 268260, train_perplexity=619.72656, train_loss=6.4292784

Batch 268270, train_perplexity=636.4804, train_loss=6.4559536

Batch 268280, train_perplexity=585.53235, train_loss=6.3725214

Batch 268290, train_perplexity=628.0072, train_loss=6.4425516

Batch 268300, train_perplexity=593.01013, train_loss=6.3852115

Batch 268310, train_perplexity=623.45465, train_loss=6.435276

Batch 268320, train_perplexity=606.184, train_loss=6.4071836

Batch 268330, train_perplexity=610.81726, train_loss=6.414798

Batch 268340, train_perplexity=673.8639, train_loss=6.513028

Batch 268350, train_perplexity=648.6112, train_loss=6.4748335

Batch 268360, train_perplexity=631.3948, train_loss=6.4479313

Batch 268370, train_perplexity=616.7588, train_loss=6.424478

Batch 268380, train_perplexity=635.3269, train_loss=6.4541397

Batch 268390, train_perplexity=606.28784, train_loss=6.407355

Batch 268400, train_perplexity=639.26434, train_loss=6.460318

Batch 268410, train_perplexity=656.49567, train_loss=6.486916

Batch 268420, train_perplexity=579.9101, train_loss=6.362873

Batch 268430, train_perplexity=604.027, train_loss=6.403619

Batch 268440, train_perplexity=645.54205, train_loss=6.4700904

Batch 268450, train_perplexity=606.0852, train_loss=6.4070206

Batch 268460, train_perplexity=633.4381, train_loss=6.4511623

Batch 268470, train_perplexity=632.4011, train_loss=6.449524

Batch 268480, train_perplexity=585.963, train_loss=6.3732567

Batch 268490, train_perplexity=592.9861, train_loss=6.385171

Batch 268500, train_perplexity=715.6837, train_loss=6.5732384

Batch 268510, train_perplexity=590.4304, train_loss=6.3808517

Batch 268520, train_perplexity=609.5818, train_loss=6.412773

Batch 268530, train_perplexity=635.88885, train_loss=6.455024

Batch 268540, train_perplexity=654.8215, train_loss=6.4843626

Batch 268550, train_perplexity=634.50525, train_loss=6.4528456
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 268560, train_perplexity=606.91986, train_loss=6.4083967

Batch 268570, train_perplexity=632.5906, train_loss=6.4498234

Batch 268580, train_perplexity=631.5694, train_loss=6.448208

Batch 268590, train_perplexity=559.2424, train_loss=6.326583

Batch 268600, train_perplexity=609.769, train_loss=6.41308

Batch 268610, train_perplexity=681.1748, train_loss=6.523819

Batch 268620, train_perplexity=658.89575, train_loss=6.4905653

Batch 268630, train_perplexity=568.86176, train_loss=6.3436375

Batch 268640, train_perplexity=619.6418, train_loss=6.4291415

Batch 268650, train_perplexity=558.786, train_loss=6.3257666

Batch 268660, train_perplexity=629.7789, train_loss=6.445369

Batch 268670, train_perplexity=707.0787, train_loss=6.561142

Batch 268680, train_perplexity=600.67706, train_loss=6.3980575

Batch 268690, train_perplexity=640.84686, train_loss=6.4627905

Batch 268700, train_perplexity=691.02075, train_loss=6.53817

Batch 268710, train_perplexity=649.7107, train_loss=6.476527

Batch 268720, train_perplexity=591.3196, train_loss=6.3823566

Batch 268730, train_perplexity=578.97894, train_loss=6.361266

Batch 268740, train_perplexity=643.87494, train_loss=6.4675045

Batch 268750, train_perplexity=602.0802, train_loss=6.4003906

Batch 268760, train_perplexity=627.93774, train_loss=6.442441

Batch 268770, train_perplexity=568.77496, train_loss=6.343485

Batch 268780, train_perplexity=640.7054, train_loss=6.4625697

Batch 268790, train_perplexity=737.73895, train_loss=6.60359

Batch 268800, train_perplexity=648.0535, train_loss=6.4739733

Batch 268810, train_perplexity=677.55536, train_loss=6.5184913

Batch 268820, train_perplexity=610.80994, train_loss=6.414786

Batch 268830, train_perplexity=659.5442, train_loss=6.491549

Batch 268840, train_perplexity=622.3405, train_loss=6.4334874

Batch 268850, train_perplexity=634.0824, train_loss=6.452179

Batch 268860, train_perplexity=637.63104, train_loss=6.45776

Batch 268870, train_perplexity=633.22284, train_loss=6.4508224

Batch 268880, train_perplexity=619.676, train_loss=6.429197

Batch 268890, train_perplexity=614.3341, train_loss=6.420539

Batch 268900, train_perplexity=553.94916, train_loss=6.317073

Batch 268910, train_perplexity=664.25165, train_loss=6.498661

Batch 268920, train_perplexity=567.2842, train_loss=6.3408604

Batch 268930, train_perplexity=655.75446, train_loss=6.4857864

Batch 268940, train_perplexity=660.37787, train_loss=6.492812

Batch 268950, train_perplexity=614.8229, train_loss=6.4213343

Batch 268960, train_perplexity=619.13434, train_loss=6.4283223

Batch 268970, train_perplexity=649.5763, train_loss=6.4763203

Batch 268980, train_perplexity=534.8321, train_loss=6.281953

Batch 268990, train_perplexity=648.81354, train_loss=6.4751453

Batch 269000, train_perplexity=601.57227, train_loss=6.3995466

Batch 269010, train_perplexity=568.16833, train_loss=6.3424177

Batch 269020, train_perplexity=624.8064, train_loss=6.437442

Batch 269030, train_perplexity=628.4137, train_loss=6.4431987

Batch 269040, train_perplexity=603.33954, train_loss=6.40248

Batch 269050, train_perplexity=659.3232, train_loss=6.491214

Batch 269060, train_perplexity=573.4787, train_loss=6.351721

Batch 269070, train_perplexity=675.0513, train_loss=6.5147886

Batch 269080, train_perplexity=612.41016, train_loss=6.4174023

Batch 269090, train_perplexity=616.8388, train_loss=6.4246078

Batch 269100, train_perplexity=672.58014, train_loss=6.5111213

Batch 269110, train_perplexity=639.54333, train_loss=6.4607544

Batch 269120, train_perplexity=618.7038, train_loss=6.4276266

Batch 269130, train_perplexity=710.47595, train_loss=6.565935

Batch 269140, train_perplexity=580.83606, train_loss=6.3644686

Batch 269150, train_perplexity=661.07477, train_loss=6.493867

Batch 269160, train_perplexity=698.4689, train_loss=6.5488906

Batch 269170, train_perplexity=674.2792, train_loss=6.513644

Batch 269180, train_perplexity=613.30994, train_loss=6.4188704

Batch 269190, train_perplexity=624.95386, train_loss=6.437678

Batch 269200, train_perplexity=711.09247, train_loss=6.5668025

Batch 269210, train_perplexity=666.9109, train_loss=6.5026565

Batch 269220, train_perplexity=615.84454, train_loss=6.4229946

Batch 269230, train_perplexity=547.8475, train_loss=6.305997

Batch 269240, train_perplexity=642.17596, train_loss=6.4648623

Batch 269250, train_perplexity=623.4499, train_loss=6.4352684

Batch 269260, train_perplexity=617.4059, train_loss=6.4255266

Batch 269270, train_perplexity=605.52655, train_loss=6.4060984

Batch 269280, train_perplexity=614.64526, train_loss=6.4210453

Batch 269290, train_perplexity=698.95496, train_loss=6.5495863

Batch 269300, train_perplexity=656.5761, train_loss=6.4870386

Batch 269310, train_perplexity=634.106, train_loss=6.452216

Batch 269320, train_perplexity=591.46735, train_loss=6.3826065

Batch 269330, train_perplexity=641.45184, train_loss=6.463734

Batch 269340, train_perplexity=629.76025, train_loss=6.445339

Batch 269350, train_perplexity=597.1977, train_loss=6.392248

Batch 269360, train_perplexity=645.49896, train_loss=6.4700236

Batch 269370, train_perplexity=698.9063, train_loss=6.5495167

Batch 269380, train_perplexity=602.96045, train_loss=6.4018517

Batch 269390, train_perplexity=641.8273, train_loss=6.464319

Batch 269400, train_perplexity=659.0296, train_loss=6.4907684

Batch 269410, train_perplexity=629.6077, train_loss=6.445097

Batch 269420, train_perplexity=583.8197, train_loss=6.369592

Batch 269430, train_perplexity=597.4925, train_loss=6.3927417

Batch 269440, train_perplexity=605.61487, train_loss=6.4062443

Batch 269450, train_perplexity=619.4397, train_loss=6.4288154

Batch 269460, train_perplexity=651.9862, train_loss=6.4800234

Batch 269470, train_perplexity=630.7911, train_loss=6.4469748

Batch 269480, train_perplexity=608.07855, train_loss=6.410304

Batch 269490, train_perplexity=649.7033, train_loss=6.476516

Batch 269500, train_perplexity=609.329, train_loss=6.4123583

Batch 269510, train_perplexity=642.2458, train_loss=6.464971

Batch 269520, train_perplexity=622.5456, train_loss=6.433817

Batch 269530, train_perplexity=624.07153, train_loss=6.436265

Batch 269540, train_perplexity=642.39496, train_loss=6.4652033

Batch 269550, train_perplexity=573.4984, train_loss=6.351755

Batch 269560, train_perplexity=563.67664, train_loss=6.334481

Batch 269570, train_perplexity=579.46674, train_loss=6.362108

Batch 269580, train_perplexity=680.90204, train_loss=6.5234184

Batch 269590, train_perplexity=625.0162, train_loss=6.4377775

Batch 269600, train_perplexity=614.68604, train_loss=6.4211116

Batch 269610, train_perplexity=588.88, train_loss=6.3782225

Batch 269620, train_perplexity=607.50323, train_loss=6.4093575

Batch 269630, train_perplexity=614.4284, train_loss=6.4206924

Batch 269640, train_perplexity=564.8662, train_loss=6.336589

Batch 269650, train_perplexity=622.74927, train_loss=6.434144

Batch 269660, train_perplexity=600.77704, train_loss=6.398224

Batch 269670, train_perplexity=568.02527, train_loss=6.342166

Batch 269680, train_perplexity=670.5867, train_loss=6.508153

Batch 269690, train_perplexity=662.71313, train_loss=6.496342

Batch 269700, train_perplexity=644.7518, train_loss=6.4688654

Batch 269710, train_perplexity=633.5765, train_loss=6.4513807

Batch 269720, train_perplexity=640.3715, train_loss=6.4620485

Batch 269730, train_perplexity=598.6683, train_loss=6.3947077

Batch 269740, train_perplexity=596.29706, train_loss=6.390739

Batch 269750, train_perplexity=620.25134, train_loss=6.4301248

Batch 269760, train_perplexity=585.4022, train_loss=6.372299

Batch 269770, train_perplexity=602.7753, train_loss=6.4015446

Batch 269780, train_perplexity=561.38513, train_loss=6.330407

Batch 269790, train_perplexity=523.6933, train_loss=6.260906

Batch 269800, train_perplexity=653.8929, train_loss=6.4829435

Batch 269810, train_perplexity=636.58295, train_loss=6.456115

Batch 269820, train_perplexity=606.1843, train_loss=6.407184

Batch 269830, train_perplexity=604.22485, train_loss=6.4039464

Batch 269840, train_perplexity=616.9297, train_loss=6.424755

Batch 269850, train_perplexity=591.631, train_loss=6.382883

Batch 269860, train_perplexity=614.8701, train_loss=6.421411
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 269870, train_perplexity=614.715, train_loss=6.421159

Batch 269880, train_perplexity=742.3546, train_loss=6.609827

Batch 269890, train_perplexity=631.65076, train_loss=6.4483366

Batch 269900, train_perplexity=620.56696, train_loss=6.4306335

Batch 269910, train_perplexity=594.57275, train_loss=6.387843

Batch 269920, train_perplexity=677.3034, train_loss=6.5181193

Batch 269930, train_perplexity=655.5706, train_loss=6.485506

Batch 269940, train_perplexity=679.4856, train_loss=6.521336

Batch 269950, train_perplexity=705.6506, train_loss=6.55912

Batch 269960, train_perplexity=606.2471, train_loss=6.4072876

Batch 269970, train_perplexity=645.7499, train_loss=6.4704123

Batch 269980, train_perplexity=710.5498, train_loss=6.566039

Batch 269990, train_perplexity=627.76465, train_loss=6.4421654

Batch 270000, train_perplexity=612.4756, train_loss=6.417509

Batch 270010, train_perplexity=576.5188, train_loss=6.357008

Batch 270020, train_perplexity=596.8871, train_loss=6.391728

Batch 270030, train_perplexity=639.67175, train_loss=6.460955

Batch 270040, train_perplexity=673.0495, train_loss=6.511819

Batch 270050, train_perplexity=562.9683, train_loss=6.3332233

Batch 270060, train_perplexity=612.3214, train_loss=6.4172573

Batch 270070, train_perplexity=649.4796, train_loss=6.4761715

Batch 270080, train_perplexity=616.1665, train_loss=6.423517

Batch 270090, train_perplexity=628.2438, train_loss=6.4429283

Batch 270100, train_perplexity=659.15845, train_loss=6.490964

Batch 270110, train_perplexity=680.8416, train_loss=6.5233297

Batch 270120, train_perplexity=585.58984, train_loss=6.3726196

Batch 270130, train_perplexity=638.11835, train_loss=6.4585238

Batch 270140, train_perplexity=675.8023, train_loss=6.5159006

Batch 270150, train_perplexity=552.1806, train_loss=6.313875

Batch 270160, train_perplexity=595.0019, train_loss=6.3885646

Batch 270170, train_perplexity=573.2638, train_loss=6.351346

Batch 270180, train_perplexity=630.8666, train_loss=6.4470944

Batch 270190, train_perplexity=630.43146, train_loss=6.4464045

Batch 270200, train_perplexity=599.47125, train_loss=6.396048

Batch 270210, train_perplexity=584.1213, train_loss=6.3701086

Batch 270220, train_perplexity=586.4424, train_loss=6.3740745

Batch 270230, train_perplexity=594.32336, train_loss=6.3874235

Batch 270240, train_perplexity=601.1146, train_loss=6.3987856

Batch 270250, train_perplexity=597.47394, train_loss=6.3927107

Batch 270260, train_perplexity=578.1221, train_loss=6.359785

Batch 270270, train_perplexity=596.30164, train_loss=6.3907466

Batch 270280, train_perplexity=649.549, train_loss=6.4762783

Batch 270290, train_perplexity=581.8912, train_loss=6.3662834

Batch 270300, train_perplexity=610.1227, train_loss=6.41366

Batch 270310, train_perplexity=638.6797, train_loss=6.459403

Batch 270320, train_perplexity=605.6394, train_loss=6.406285

Batch 270330, train_perplexity=622.5314, train_loss=6.433794

Batch 270340, train_perplexity=605.9199, train_loss=6.406748

Batch 270350, train_perplexity=587.97595, train_loss=6.376686

Batch 270360, train_perplexity=598.6977, train_loss=6.394757

Batch 270370, train_perplexity=669.2361, train_loss=6.506137

Batch 270380, train_perplexity=620.9272, train_loss=6.431214

Batch 270390, train_perplexity=594.061, train_loss=6.386982

Batch 270400, train_perplexity=624.0346, train_loss=6.436206

Batch 270410, train_perplexity=663.6846, train_loss=6.497807

Batch 270420, train_perplexity=633.2237, train_loss=6.450824

Batch 270430, train_perplexity=600.2467, train_loss=6.397341

Batch 270440, train_perplexity=632.17566, train_loss=6.4491673

Batch 270450, train_perplexity=591.2914, train_loss=6.382309

Batch 270460, train_perplexity=665.6487, train_loss=6.500762

Batch 270470, train_perplexity=639.005, train_loss=6.4599123

Batch 270480, train_perplexity=626.45496, train_loss=6.440077

Batch 270490, train_perplexity=625.782, train_loss=6.439002

Batch 270500, train_perplexity=604.808, train_loss=6.404911

Batch 270510, train_perplexity=644.2072, train_loss=6.4680204

Batch 270520, train_perplexity=552.83295, train_loss=6.315056

Batch 270530, train_perplexity=607.4097, train_loss=6.4092035

Batch 270540, train_perplexity=556.3849, train_loss=6.3214602

Batch 270550, train_perplexity=658.32477, train_loss=6.4896984

Batch 270560, train_perplexity=597.6962, train_loss=6.3930826

Batch 270570, train_perplexity=609.9708, train_loss=6.413411

Batch 270580, train_perplexity=580.3112, train_loss=6.3635645

Batch 270590, train_perplexity=601.4182, train_loss=6.3992906

Batch 270600, train_perplexity=665.56995, train_loss=6.5006437

Batch 270610, train_perplexity=613.54456, train_loss=6.419253

Batch 270620, train_perplexity=663.1547, train_loss=6.4970083

Batch 270630, train_perplexity=599.54016, train_loss=6.396163

Batch 270640, train_perplexity=596.662, train_loss=6.3913507

Batch 270650, train_perplexity=607.2096, train_loss=6.408874

Batch 270660, train_perplexity=596.9443, train_loss=6.391824

Batch 270670, train_perplexity=641.3384, train_loss=6.4635572

Batch 270680, train_perplexity=653.2627, train_loss=6.4819794

Batch 270690, train_perplexity=601.07764, train_loss=6.398724

Batch 270700, train_perplexity=670.9116, train_loss=6.5086374

Batch 270710, train_perplexity=651.30756, train_loss=6.478982

Batch 270720, train_perplexity=614.6846, train_loss=6.421109

Batch 270730, train_perplexity=582.9382, train_loss=6.368081

Batch 270740, train_perplexity=574.51, train_loss=6.3535175

Batch 270750, train_perplexity=635.18695, train_loss=6.4539194

Batch 270760, train_perplexity=682.56903, train_loss=6.5258636

Batch 270770, train_perplexity=661.41656, train_loss=6.494384

Batch 270780, train_perplexity=670.9644, train_loss=6.508716

Batch 270790, train_perplexity=638.8079, train_loss=6.459604

Batch 270800, train_perplexity=587.2514, train_loss=6.375453

Batch 270810, train_perplexity=660.9682, train_loss=6.4937057

Batch 270820, train_perplexity=626.52185, train_loss=6.4401836

Batch 270830, train_perplexity=591.92725, train_loss=6.3833838

Batch 270840, train_perplexity=619.41315, train_loss=6.4287724

Batch 270850, train_perplexity=576.09674, train_loss=6.3562756

Batch 270860, train_perplexity=631.6019, train_loss=6.4482594

Batch 270870, train_perplexity=591.03656, train_loss=6.381878

Batch 270880, train_perplexity=659.5263, train_loss=6.491522

Batch 270890, train_perplexity=578.3937, train_loss=6.360255

Batch 270900, train_perplexity=622.57294, train_loss=6.433861

Batch 270910, train_perplexity=604.78143, train_loss=6.404867

Batch 270920, train_perplexity=587.04083, train_loss=6.3750944

Batch 270930, train_perplexity=639.8655, train_loss=6.461258

Batch 270940, train_perplexity=657.7857, train_loss=6.488879

Batch 270950, train_perplexity=636.6006, train_loss=6.4561424

Batch 270960, train_perplexity=587.78705, train_loss=6.3763647

Batch 270970, train_perplexity=602.9133, train_loss=6.4017735

Batch 270980, train_perplexity=624.61456, train_loss=6.4371347

Batch 270990, train_perplexity=688.8435, train_loss=6.535014

Batch 271000, train_perplexity=683.4412, train_loss=6.5271406

Batch 271010, train_perplexity=708.2273, train_loss=6.562765

Batch 271020, train_perplexity=611.0401, train_loss=6.4151626

Batch 271030, train_perplexity=581.5791, train_loss=6.365747

Batch 271040, train_perplexity=562.207, train_loss=6.33187

Batch 271050, train_perplexity=560.534, train_loss=6.32889

Batch 271060, train_perplexity=618.957, train_loss=6.4280357

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Batch 271070, train_perplexity=684.35657, train_loss=6.528479

Batch 271080, train_perplexity=623.0861, train_loss=6.4346848

Batch 271090, train_perplexity=683.2965, train_loss=6.526929

Batch 271100, train_perplexity=626.4597, train_loss=6.4400845

Batch 271110, train_perplexity=650.1613, train_loss=6.4772205
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 271120, train_perplexity=555.39935, train_loss=6.3196874

Batch 271130, train_perplexity=663.7071, train_loss=6.497841

Batch 271140, train_perplexity=644.89813, train_loss=6.4690924

Batch 271150, train_perplexity=537.1236, train_loss=6.286228

Batch 271160, train_perplexity=646.0807, train_loss=6.4709244

Batch 271170, train_perplexity=651.87427, train_loss=6.4798517

Batch 271180, train_perplexity=613.56006, train_loss=6.419278

Batch 271190, train_perplexity=619.1742, train_loss=6.4283867

Batch 271200, train_perplexity=674.75195, train_loss=6.514345

Batch 271210, train_perplexity=608.98505, train_loss=6.4117937

Batch 271220, train_perplexity=608.8056, train_loss=6.411499

Batch 271230, train_perplexity=570.4881, train_loss=6.3464923

Batch 271240, train_perplexity=605.8656, train_loss=6.406658

Batch 271250, train_perplexity=596.0992, train_loss=6.390407

Batch 271260, train_perplexity=627.86945, train_loss=6.4423323

Batch 271270, train_perplexity=594.7191, train_loss=6.388089

Batch 271280, train_perplexity=639.36285, train_loss=6.460472

Batch 271290, train_perplexity=647.97626, train_loss=6.473854

Batch 271300, train_perplexity=608.40454, train_loss=6.41084

Batch 271310, train_perplexity=598.5407, train_loss=6.3944945

Batch 271320, train_perplexity=627.073, train_loss=6.441063

Batch 271330, train_perplexity=679.12964, train_loss=6.520812

Batch 271340, train_perplexity=706.2016, train_loss=6.5599008

Batch 271350, train_perplexity=645.2854, train_loss=6.4696927

Batch 271360, train_perplexity=674.61426, train_loss=6.514141

Batch 271370, train_perplexity=682.14215, train_loss=6.525238

Batch 271380, train_perplexity=615.1191, train_loss=6.421816

Batch 271390, train_perplexity=621.2148, train_loss=6.431677

Batch 271400, train_perplexity=639.5638, train_loss=6.4607863

Batch 271410, train_perplexity=676.31036, train_loss=6.516652

Batch 271420, train_perplexity=614.96716, train_loss=6.421569

Batch 271430, train_perplexity=638.7729, train_loss=6.459549

Batch 271440, train_perplexity=636.27527, train_loss=6.4556313

Batch 271450, train_perplexity=615.61206, train_loss=6.422617

Batch 271460, train_perplexity=620.641, train_loss=6.4307528

Batch 271470, train_perplexity=573.45135, train_loss=6.351673

Batch 271480, train_perplexity=585.82837, train_loss=6.373027

Batch 271490, train_perplexity=605.02203, train_loss=6.405265

Batch 271500, train_perplexity=582.7758, train_loss=6.3678026

Batch 271510, train_perplexity=633.1295, train_loss=6.450675

Batch 271520, train_perplexity=585.84625, train_loss=6.3730574

Batch 271530, train_perplexity=620.48706, train_loss=6.430505

Batch 271540, train_perplexity=602.25104, train_loss=6.4006743

Batch 271550, train_perplexity=628.50806, train_loss=6.443349

Batch 271560, train_perplexity=660.4988, train_loss=6.4929953

Batch 271570, train_perplexity=711.122, train_loss=6.566844

Batch 271580, train_perplexity=545.7267, train_loss=6.3021183

Batch 271590, train_perplexity=626.1939, train_loss=6.43966

Batch 271600, train_perplexity=619.7623, train_loss=6.429336

Batch 271610, train_perplexity=621.3756, train_loss=6.431936

Batch 271620, train_perplexity=660.546, train_loss=6.493067

Batch 271630, train_perplexity=575.9536, train_loss=6.356027

Batch 271640, train_perplexity=627.3323, train_loss=6.4414763

Batch 271650, train_perplexity=638.5219, train_loss=6.459156

Batch 271660, train_perplexity=652.7813, train_loss=6.481242

Batch 271670, train_perplexity=715.1161, train_loss=6.572445

Batch 271680, train_perplexity=682.3653, train_loss=6.525565

Batch 271690, train_perplexity=574.63275, train_loss=6.353731

Batch 271700, train_perplexity=636.69165, train_loss=6.4562855

Batch 271710, train_perplexity=557.8276, train_loss=6.32405

Batch 271720, train_perplexity=660.3325, train_loss=6.4927435

Batch 271730, train_perplexity=614.33527, train_loss=6.420541

Batch 271740, train_perplexity=685.9813, train_loss=6.5308504

Batch 271750, train_perplexity=637.79895, train_loss=6.458023

Batch 271760, train_perplexity=552.7043, train_loss=6.314823

Batch 271770, train_perplexity=615.9597, train_loss=6.4231815

Batch 271780, train_perplexity=595.73376, train_loss=6.389794

Batch 271790, train_perplexity=632.8162, train_loss=6.45018

Batch 271800, train_perplexity=586.7098, train_loss=6.3745303

Batch 271810, train_perplexity=691.5508, train_loss=6.5389366

Batch 271820, train_perplexity=632.9315, train_loss=6.450362

Batch 271830, train_perplexity=638.70526, train_loss=6.459443

Batch 271840, train_perplexity=658.3543, train_loss=6.489743

Batch 271850, train_perplexity=576.12915, train_loss=6.356332

Batch 271860, train_perplexity=559.9599, train_loss=6.327865

Batch 271870, train_perplexity=645.4205, train_loss=6.469902

Batch 271880, train_perplexity=545.76025, train_loss=6.30218

Batch 271890, train_perplexity=544.56445, train_loss=6.2999864

Batch 271900, train_perplexity=626.2793, train_loss=6.4397964

Batch 271910, train_perplexity=606.1332, train_loss=6.4070997

Batch 271920, train_perplexity=701.80835, train_loss=6.5536604

Batch 271930, train_perplexity=633.62726, train_loss=6.451461

Batch 271940, train_perplexity=674.4911, train_loss=6.5139585

Batch 271950, train_perplexity=623.9912, train_loss=6.4361362

Batch 271960, train_perplexity=629.298, train_loss=6.444605

Batch 271970, train_perplexity=537.513, train_loss=6.286953

Batch 271980, train_perplexity=581.5816, train_loss=6.3657513

Batch 271990, train_perplexity=645.5621, train_loss=6.4701214

Batch 272000, train_perplexity=612.0301, train_loss=6.4167814

Batch 272010, train_perplexity=667.3254, train_loss=6.503278

Batch 272020, train_perplexity=644.7616, train_loss=6.4688807

Batch 272030, train_perplexity=627.26587, train_loss=6.4413705

Batch 272040, train_perplexity=657.2414, train_loss=6.4880514

Batch 272050, train_perplexity=564.14966, train_loss=6.3353195

Batch 272060, train_perplexity=548.55505, train_loss=6.3072877

Batch 272070, train_perplexity=584.5687, train_loss=6.3708744

Batch 272080, train_perplexity=681.19366, train_loss=6.5238466

Batch 272090, train_perplexity=653.3188, train_loss=6.482065

Batch 272100, train_perplexity=624.60175, train_loss=6.4371142

Batch 272110, train_perplexity=551.3971, train_loss=6.312455

Batch 272120, train_perplexity=691.50134, train_loss=6.538865

Batch 272130, train_perplexity=564.83545, train_loss=6.3365345

Batch 272140, train_perplexity=638.4793, train_loss=6.4590893

Batch 272150, train_perplexity=622.75165, train_loss=6.434148

Batch 272160, train_perplexity=677.0612, train_loss=6.5177617

Batch 272170, train_perplexity=572.85504, train_loss=6.3506327

Batch 272180, train_perplexity=630.13696, train_loss=6.445937

Batch 272190, train_perplexity=602.7472, train_loss=6.401498

Batch 272200, train_perplexity=678.3691, train_loss=6.5196915

Batch 272210, train_perplexity=608.2827, train_loss=6.41064

Batch 272220, train_perplexity=646.1749, train_loss=6.4710703

Batch 272230, train_perplexity=638.49335, train_loss=6.459111

Batch 272240, train_perplexity=577.82336, train_loss=6.359268

Batch 272250, train_perplexity=681.04553, train_loss=6.523629

Batch 272260, train_perplexity=686.5412, train_loss=6.5316663

Batch 272270, train_perplexity=614.9589, train_loss=6.4215555

Batch 272280, train_perplexity=612.1308, train_loss=6.416946

Batch 272290, train_perplexity=629.5609, train_loss=6.4450226

Batch 272300, train_perplexity=636.6643, train_loss=6.4562426

Batch 272310, train_perplexity=628.47064, train_loss=6.4432893

Batch 272320, train_perplexity=624.06555, train_loss=6.4362555

Batch 272330, train_perplexity=630.42725, train_loss=6.446398

Batch 272340, train_perplexity=603.15485, train_loss=6.402174

Batch 272350, train_perplexity=602.74316, train_loss=6.401491

Batch 272360, train_perplexity=570.88214, train_loss=6.3471828

Batch 272370, train_perplexity=657.1706, train_loss=6.4879436

Batch 272380, train_perplexity=666.4976, train_loss=6.5020366

Batch 272390, train_perplexity=623.169, train_loss=6.434818

Batch 272400, train_perplexity=652.05707, train_loss=6.480132

Batch 272410, train_perplexity=650.58, train_loss=6.4778643

Batch 272420, train_perplexity=707.76416, train_loss=6.562111
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 272430, train_perplexity=610.8673, train_loss=6.41488

Batch 272440, train_perplexity=608.43933, train_loss=6.4108973

Batch 272450, train_perplexity=608.1418, train_loss=6.410408

Batch 272460, train_perplexity=622.1557, train_loss=6.4331903

Batch 272470, train_perplexity=649.26447, train_loss=6.47584

Batch 272480, train_perplexity=636.50525, train_loss=6.4559927

Batch 272490, train_perplexity=577.7313, train_loss=6.359109

Batch 272500, train_perplexity=701.7796, train_loss=6.5536194

Batch 272510, train_perplexity=580.09375, train_loss=6.3631897

Batch 272520, train_perplexity=643.522, train_loss=6.466956

Batch 272530, train_perplexity=599.9434, train_loss=6.3968353

Batch 272540, train_perplexity=590.72943, train_loss=6.381358

Batch 272550, train_perplexity=624.0114, train_loss=6.4361687

Batch 272560, train_perplexity=688.1006, train_loss=6.533935

Batch 272570, train_perplexity=659.4175, train_loss=6.491357

Batch 272580, train_perplexity=557.90796, train_loss=6.324194

Batch 272590, train_perplexity=650.47144, train_loss=6.4776974

Batch 272600, train_perplexity=621.6942, train_loss=6.4324484

Batch 272610, train_perplexity=671.0428, train_loss=6.508833

Batch 272620, train_perplexity=576.4248, train_loss=6.356845

Batch 272630, train_perplexity=642.27765, train_loss=6.4650207

Batch 272640, train_perplexity=617.3161, train_loss=6.425381

Batch 272650, train_perplexity=540.45056, train_loss=6.292403

Batch 272660, train_perplexity=623.9055, train_loss=6.435999

Batch 272670, train_perplexity=625.2409, train_loss=6.438137

Batch 272680, train_perplexity=688.46985, train_loss=6.5344715

Batch 272690, train_perplexity=604.43665, train_loss=6.404297

Batch 272700, train_perplexity=631.73206, train_loss=6.4484653

Batch 272710, train_perplexity=618.62, train_loss=6.427491

Batch 272720, train_perplexity=675.1517, train_loss=6.5149374

Batch 272730, train_perplexity=593.6974, train_loss=6.3863697

Batch 272740, train_perplexity=586.2414, train_loss=6.3737316

Batch 272750, train_perplexity=677.67267, train_loss=6.5186644

Batch 272760, train_perplexity=654.76746, train_loss=6.48428

Batch 272770, train_perplexity=591.699, train_loss=6.382998

Batch 272780, train_perplexity=592.63556, train_loss=6.3845797

Batch 272790, train_perplexity=560.84894, train_loss=6.3294516

Batch 272800, train_perplexity=572.6562, train_loss=6.3502855

Batch 272810, train_perplexity=544.7793, train_loss=6.3003807

Batch 272820, train_perplexity=588.50946, train_loss=6.377593

Batch 272830, train_perplexity=598.3995, train_loss=6.3942585

Batch 272840, train_perplexity=559.71454, train_loss=6.327427

Batch 272850, train_perplexity=570.4571, train_loss=6.346438

Batch 272860, train_perplexity=632.3161, train_loss=6.4493895

Batch 272870, train_perplexity=642.08167, train_loss=6.4647155

Batch 272880, train_perplexity=568.435, train_loss=6.342887

Batch 272890, train_perplexity=529.7916, train_loss=6.272484

Batch 272900, train_perplexity=610.57697, train_loss=6.4144044

Batch 272910, train_perplexity=575.81274, train_loss=6.3557825

Batch 272920, train_perplexity=611.65344, train_loss=6.416166

Batch 272930, train_perplexity=606.7971, train_loss=6.4081945

Batch 272940, train_perplexity=674.18304, train_loss=6.5135016

Batch 272950, train_perplexity=637.2274, train_loss=6.4571266

Batch 272960, train_perplexity=654.4244, train_loss=6.483756

Batch 272970, train_perplexity=619.58887, train_loss=6.429056

Batch 272980, train_perplexity=682.1669, train_loss=6.5252743

Batch 272990, train_perplexity=655.5853, train_loss=6.4855285

Batch 273000, train_perplexity=549.5266, train_loss=6.309057

Batch 273010, train_perplexity=585.30566, train_loss=6.372134

Batch 273020, train_perplexity=648.7065, train_loss=6.4749804

Batch 273030, train_perplexity=628.5701, train_loss=6.4434476

Batch 273040, train_perplexity=573.1086, train_loss=6.351075

Batch 273050, train_perplexity=618.5958, train_loss=6.427452

Batch 273060, train_perplexity=578.8023, train_loss=6.360961

Batch 273070, train_perplexity=647.0438, train_loss=6.472414

Batch 273080, train_perplexity=627.6461, train_loss=6.4419765

Batch 273090, train_perplexity=580.75494, train_loss=6.364329

Batch 273100, train_perplexity=558.0734, train_loss=6.3244905

Batch 273110, train_perplexity=572.7638, train_loss=6.3504734

Batch 273120, train_perplexity=569.9744, train_loss=6.3455915

Batch 273130, train_perplexity=621.00684, train_loss=6.431342

Batch 273140, train_perplexity=662.90717, train_loss=6.496635

Batch 273150, train_perplexity=675.8265, train_loss=6.5159364

Batch 273160, train_perplexity=710.1467, train_loss=6.5654716

Batch 273170, train_perplexity=624.3945, train_loss=6.4367824

Batch 273180, train_perplexity=607.8565, train_loss=6.409939

Batch 273190, train_perplexity=618.5274, train_loss=6.4273415

Batch 273200, train_perplexity=536.3558, train_loss=6.2847977

Batch 273210, train_perplexity=634.3364, train_loss=6.4525795

Batch 273220, train_perplexity=610.4984, train_loss=6.4142756

Batch 273230, train_perplexity=618.10284, train_loss=6.426655

Batch 273240, train_perplexity=610.9259, train_loss=6.4149756

Batch 273250, train_perplexity=561.2355, train_loss=6.3301406

Batch 273260, train_perplexity=658.90894, train_loss=6.4905853

Batch 273270, train_perplexity=652.06146, train_loss=6.480139

Batch 273280, train_perplexity=648.1169, train_loss=6.474071

Batch 273290, train_perplexity=622.6059, train_loss=6.4339137

Batch 273300, train_perplexity=658.424, train_loss=6.489849

Batch 273310, train_perplexity=647.61456, train_loss=6.4732957

Batch 273320, train_perplexity=571.2042, train_loss=6.347747

Batch 273330, train_perplexity=648.4294, train_loss=6.474553

Batch 273340, train_perplexity=574.87994, train_loss=6.3541613

Batch 273350, train_perplexity=644.26373, train_loss=6.468108

Batch 273360, train_perplexity=620.91565, train_loss=6.4311953

Batch 273370, train_perplexity=568.18677, train_loss=6.34245

Batch 273380, train_perplexity=626.6808, train_loss=6.4404373

Batch 273390, train_perplexity=583.7623, train_loss=6.369494

Batch 273400, train_perplexity=570.86444, train_loss=6.3471518

Batch 273410, train_perplexity=607.08136, train_loss=6.408663

Batch 273420, train_perplexity=607.3182, train_loss=6.409053

Batch 273430, train_perplexity=606.81885, train_loss=6.4082303

Batch 273440, train_perplexity=628.62976, train_loss=6.4435425

Batch 273450, train_perplexity=617.9251, train_loss=6.4263673

Batch 273460, train_perplexity=644.1353, train_loss=6.467909

Batch 273470, train_perplexity=624.8091, train_loss=6.437446

Batch 273480, train_perplexity=685.04254, train_loss=6.529481

Batch 273490, train_perplexity=662.5921, train_loss=6.4961596

Batch 273500, train_perplexity=529.31165, train_loss=6.2715774

Batch 273510, train_perplexity=589.1443, train_loss=6.378671

Batch 273520, train_perplexity=603.44946, train_loss=6.4026623

Batch 273530, train_perplexity=585.3355, train_loss=6.372185

Batch 273540, train_perplexity=554.5784, train_loss=6.318208

Batch 273550, train_perplexity=570.779, train_loss=6.347002

Batch 273560, train_perplexity=649.06354, train_loss=6.4755306

Batch 273570, train_perplexity=554.69507, train_loss=6.3184185

Batch 273580, train_perplexity=612.28375, train_loss=6.417196

Batch 273590, train_perplexity=593.7648, train_loss=6.386483

Batch 273600, train_perplexity=552.9996, train_loss=6.315357

Batch 273610, train_perplexity=560.2841, train_loss=6.328444

Batch 273620, train_perplexity=557.49414, train_loss=6.323452

Batch 273630, train_perplexity=649.9825, train_loss=6.4769454

Batch 273640, train_perplexity=619.79395, train_loss=6.429387

Batch 273650, train_perplexity=676.29425, train_loss=6.5166283

Batch 273660, train_perplexity=650.78107, train_loss=6.4781733

Batch 273670, train_perplexity=589.14233, train_loss=6.378668

Batch 273680, train_perplexity=631.5312, train_loss=6.4481473

Batch 273690, train_perplexity=610.2664, train_loss=6.4138956

Batch 273700, train_perplexity=630.26135, train_loss=6.4461346

Batch 273710, train_perplexity=598.28815, train_loss=6.3940725

Batch 273720, train_perplexity=593.0839, train_loss=6.385336

Batch 273730, train_perplexity=614.95953, train_loss=6.4215565
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 273740, train_perplexity=650.66626, train_loss=6.477997

Batch 273750, train_perplexity=537.73425, train_loss=6.2873645

Batch 273760, train_perplexity=571.7918, train_loss=6.348775

Batch 273770, train_perplexity=549.8147, train_loss=6.3095813

Batch 273780, train_perplexity=656.63184, train_loss=6.4871235

Batch 273790, train_perplexity=648.2578, train_loss=6.4742885

Batch 273800, train_perplexity=571.6184, train_loss=6.3484716

Batch 273810, train_perplexity=646.6151, train_loss=6.471751

Batch 273820, train_perplexity=561.5356, train_loss=6.330675

Batch 273830, train_perplexity=634.8594, train_loss=6.4534035

Batch 273840, train_perplexity=630.1778, train_loss=6.446002

Batch 273850, train_perplexity=572.71844, train_loss=6.3503942

Batch 273860, train_perplexity=604.5168, train_loss=6.4044294

Batch 273870, train_perplexity=613.3465, train_loss=6.41893

Batch 273880, train_perplexity=598.2465, train_loss=6.394003

Batch 273890, train_perplexity=598.6803, train_loss=6.3947277

Batch 273900, train_perplexity=560.1172, train_loss=6.328146

Batch 273910, train_perplexity=624.4567, train_loss=6.436882

Batch 273920, train_perplexity=638.8956, train_loss=6.459741

Batch 273930, train_perplexity=623.43976, train_loss=6.435252

Batch 273940, train_perplexity=646.3152, train_loss=6.4712873

Batch 273950, train_perplexity=668.7072, train_loss=6.5053463

Batch 273960, train_perplexity=626.5774, train_loss=6.4402723

Batch 273970, train_perplexity=712.77563, train_loss=6.5691667

Batch 273980, train_perplexity=663.4849, train_loss=6.497506

Batch 273990, train_perplexity=540.06573, train_loss=6.291691

Batch 274000, train_perplexity=590.87427, train_loss=6.3816032

Batch 274010, train_perplexity=600.582, train_loss=6.397899

Batch 274020, train_perplexity=648.362, train_loss=6.474449

Batch 274030, train_perplexity=600.4199, train_loss=6.3976293

Batch 274040, train_perplexity=557.8412, train_loss=6.3240743

Batch 274050, train_perplexity=632.5869, train_loss=6.4498177

Batch 274060, train_perplexity=685.9296, train_loss=6.530775

Batch 274070, train_perplexity=643.56647, train_loss=6.4670253

Batch 274080, train_perplexity=560.9891, train_loss=6.3297014

Batch 274090, train_perplexity=588.9463, train_loss=6.378335

Batch 274100, train_perplexity=591.1076, train_loss=6.381998

Batch 274110, train_perplexity=554.6527, train_loss=6.318342

Batch 274120, train_perplexity=622.11176, train_loss=6.43312

Batch 274130, train_perplexity=589.1935, train_loss=6.3787546

Batch 274140, train_perplexity=637.29333, train_loss=6.45723

Batch 274150, train_perplexity=591.484, train_loss=6.3826346

Batch 274160, train_perplexity=639.5958, train_loss=6.4608364

Batch 274170, train_perplexity=594.7321, train_loss=6.388111

Batch 274180, train_perplexity=600.55365, train_loss=6.397852

Batch 274190, train_perplexity=609.27203, train_loss=6.412265

Batch 274200, train_perplexity=666.12305, train_loss=6.5014744

Batch 274210, train_perplexity=654.06067, train_loss=6.4832

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Batch 274220, train_perplexity=667.1835, train_loss=6.503065

Batch 274230, train_perplexity=646.3324, train_loss=6.471314

Batch 274240, train_perplexity=618.6728, train_loss=6.4275765

Batch 274250, train_perplexity=641.6492, train_loss=6.4640417

Batch 274260, train_perplexity=645.9285, train_loss=6.470689

Batch 274270, train_perplexity=607.34686, train_loss=6.4091

Batch 274280, train_perplexity=554.47845, train_loss=6.318028

Batch 274290, train_perplexity=629.46246, train_loss=6.444866

Batch 274300, train_perplexity=571.3846, train_loss=6.3480625

Batch 274310, train_perplexity=614.5779, train_loss=6.4209356

Batch 274320, train_perplexity=565.8216, train_loss=6.338279

Batch 274330, train_perplexity=691.8833, train_loss=6.5394173

Batch 274340, train_perplexity=598.21, train_loss=6.393942

Batch 274350, train_perplexity=581.42633, train_loss=6.365484

Batch 274360, train_perplexity=644.87354, train_loss=6.469054

Batch 274370, train_perplexity=651.3902, train_loss=6.479109

Batch 274380, train_perplexity=578.62897, train_loss=6.3606615

Batch 274390, train_perplexity=621.3931, train_loss=6.431964

Batch 274400, train_perplexity=506.1272, train_loss=6.226788

Batch 274410, train_perplexity=624.365, train_loss=6.436735

Batch 274420, train_perplexity=591.80927, train_loss=6.3831844

Batch 274430, train_perplexity=679.29517, train_loss=6.5210557

Batch 274440, train_perplexity=649.80493, train_loss=6.476672

Batch 274450, train_perplexity=644.3086, train_loss=6.468178

Batch 274460, train_perplexity=625.9891, train_loss=6.439333

Batch 274470, train_perplexity=639.0233, train_loss=6.459941

Batch 274480, train_perplexity=645.66614, train_loss=6.4702826

Batch 274490, train_perplexity=565.086, train_loss=6.336978

Batch 274500, train_perplexity=706.87036, train_loss=6.5608473

Batch 274510, train_perplexity=595.2281, train_loss=6.3889446

Batch 274520, train_perplexity=589.6215, train_loss=6.379481

Batch 274530, train_perplexity=567.8119, train_loss=6.34179

Batch 274540, train_perplexity=661.02747, train_loss=6.4937954

Batch 274550, train_perplexity=673.8825, train_loss=6.513056

Batch 274560, train_perplexity=620.39417, train_loss=6.430355

Batch 274570, train_perplexity=642.7602, train_loss=6.4657717

Batch 274580, train_perplexity=639.58636, train_loss=6.4608216

Batch 274590, train_perplexity=644.6884, train_loss=6.468767

Batch 274600, train_perplexity=658.47424, train_loss=6.4899254

Batch 274610, train_perplexity=627.3305, train_loss=6.4414735

Batch 274620, train_perplexity=641.98553, train_loss=6.4645658

Batch 274630, train_perplexity=640.1444, train_loss=6.461694

Batch 274640, train_perplexity=606.5955, train_loss=6.407862

Batch 274650, train_perplexity=638.17737, train_loss=6.4586163

Batch 274660, train_perplexity=540.6496, train_loss=6.2927713

Batch 274670, train_perplexity=633.03534, train_loss=6.450526

Batch 274680, train_perplexity=534.92596, train_loss=6.2821283

Batch 274690, train_perplexity=565.3873, train_loss=6.337511

Batch 274700, train_perplexity=626.1321, train_loss=6.4395614

Batch 274710, train_perplexity=615.4893, train_loss=6.4224176

Batch 274720, train_perplexity=683.63934, train_loss=6.5274305

Batch 274730, train_perplexity=632.1802, train_loss=6.4491744

Batch 274740, train_perplexity=616.9838, train_loss=6.424843

Batch 274750, train_perplexity=600.74554, train_loss=6.3981714

Batch 274760, train_perplexity=619.37976, train_loss=6.4287186

Batch 274770, train_perplexity=557.4176, train_loss=6.3233147

Batch 274780, train_perplexity=598.2254, train_loss=6.3939676

Batch 274790, train_perplexity=581.1575, train_loss=6.3650217

Batch 274800, train_perplexity=678.1533, train_loss=6.5193734

Batch 274810, train_perplexity=569.1, train_loss=6.344056

Batch 274820, train_perplexity=725.19464, train_loss=6.58644

Batch 274830, train_perplexity=628.8366, train_loss=6.4438715

Batch 274840, train_perplexity=579.7486, train_loss=6.3625946

Batch 274850, train_perplexity=653.35583, train_loss=6.482122

Batch 274860, train_perplexity=574.55, train_loss=6.353587

Batch 274870, train_perplexity=599.4153, train_loss=6.3959546

Batch 274880, train_perplexity=596.42017, train_loss=6.3909454

Batch 274890, train_perplexity=589.8732, train_loss=6.3799076

Batch 274900, train_perplexity=577.6922, train_loss=6.359041

Batch 274910, train_perplexity=611.7955, train_loss=6.416398

Batch 274920, train_perplexity=579.67816, train_loss=6.362473

Batch 274930, train_perplexity=618.0386, train_loss=6.426551

Batch 274940, train_perplexity=621.2829, train_loss=6.4317865

Batch 274950, train_perplexity=618.879, train_loss=6.42791

Batch 274960, train_perplexity=600.4998, train_loss=6.3977623

Batch 274970, train_perplexity=579.5706, train_loss=6.3622875

Batch 274980, train_perplexity=523.6459, train_loss=6.2608156
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 274990, train_perplexity=624.3382, train_loss=6.436692

Batch 275000, train_perplexity=583.1517, train_loss=6.3684473

Batch 275010, train_perplexity=630.4447, train_loss=6.4464254

Batch 275020, train_perplexity=604.6791, train_loss=6.404698

Batch 275030, train_perplexity=625.464, train_loss=6.4384937

Batch 275040, train_perplexity=577.0447, train_loss=6.3579197

Batch 275050, train_perplexity=616.6033, train_loss=6.424226

Batch 275060, train_perplexity=572.2997, train_loss=6.349663

Batch 275070, train_perplexity=619.6202, train_loss=6.4291067

Batch 275080, train_perplexity=625.63043, train_loss=6.43876

Batch 275090, train_perplexity=592.9573, train_loss=6.3851223

Batch 275100, train_perplexity=597.7464, train_loss=6.3931665

Batch 275110, train_perplexity=586.9054, train_loss=6.3748636

Batch 275120, train_perplexity=590.73285, train_loss=6.381364

Batch 275130, train_perplexity=643.4192, train_loss=6.4667964

Batch 275140, train_perplexity=664.05023, train_loss=6.498358

Batch 275150, train_perplexity=635.19604, train_loss=6.4539337

Batch 275160, train_perplexity=649.3192, train_loss=6.4759245

Batch 275170, train_perplexity=579.12006, train_loss=6.36151

Batch 275180, train_perplexity=637.9468, train_loss=6.458255

Batch 275190, train_perplexity=594.8757, train_loss=6.3883524

Batch 275200, train_perplexity=669.13495, train_loss=6.5059857

Batch 275210, train_perplexity=685.85834, train_loss=6.530671

Batch 275220, train_perplexity=625.93774, train_loss=6.439251

Batch 275230, train_perplexity=624.9196, train_loss=6.437623

Batch 275240, train_perplexity=532.0141, train_loss=6.27667

Batch 275250, train_perplexity=604.6021, train_loss=6.4045706

Batch 275260, train_perplexity=647.2685, train_loss=6.472761

Batch 275270, train_perplexity=608.0989, train_loss=6.4103374

Batch 275280, train_perplexity=639.2625, train_loss=6.460315

Batch 275290, train_perplexity=578.01953, train_loss=6.3596077

Batch 275300, train_perplexity=661.59064, train_loss=6.494647

Batch 275310, train_perplexity=618.4973, train_loss=6.427293

Batch 275320, train_perplexity=560.2197, train_loss=6.328329

Batch 275330, train_perplexity=618.6684, train_loss=6.4275694

Batch 275340, train_perplexity=650.0854, train_loss=6.4771037

Batch 275350, train_perplexity=619.51917, train_loss=6.4289436

Batch 275360, train_perplexity=614.62445, train_loss=6.4210114

Batch 275370, train_perplexity=614.0957, train_loss=6.4201508

Batch 275380, train_perplexity=654.37384, train_loss=6.483679

Batch 275390, train_perplexity=574.772, train_loss=6.3539734

Batch 275400, train_perplexity=594.5192, train_loss=6.387753

Batch 275410, train_perplexity=599.24664, train_loss=6.3956733

Batch 275420, train_perplexity=555.5249, train_loss=6.3199134

Batch 275430, train_perplexity=644.49634, train_loss=6.468469

Batch 275440, train_perplexity=624.6208, train_loss=6.4371448

Batch 275450, train_perplexity=563.247, train_loss=6.3337183

Batch 275460, train_perplexity=587.92773, train_loss=6.376604

Batch 275470, train_perplexity=620.8467, train_loss=6.431084

Batch 275480, train_perplexity=704.8166, train_loss=6.5579376

Batch 275490, train_perplexity=572.0034, train_loss=6.349145

Batch 275500, train_perplexity=612.55505, train_loss=6.417639

Batch 275510, train_perplexity=638.1214, train_loss=6.4585285

Batch 275520, train_perplexity=559.4125, train_loss=6.326887

Batch 275530, train_perplexity=599.06805, train_loss=6.3953753

Batch 275540, train_perplexity=581.15216, train_loss=6.3650126

Batch 275550, train_perplexity=616.1359, train_loss=6.4234676

Batch 275560, train_perplexity=645.6372, train_loss=6.4702377

Batch 275570, train_perplexity=620.49536, train_loss=6.430518

Batch 275580, train_perplexity=623.7199, train_loss=6.4357014

Batch 275590, train_perplexity=658.2083, train_loss=6.4895215

Batch 275600, train_perplexity=583.4248, train_loss=6.3689156

Batch 275610, train_perplexity=630.5307, train_loss=6.446562

Batch 275620, train_perplexity=608.34796, train_loss=6.410747

Batch 275630, train_perplexity=623.71216, train_loss=6.435689

Batch 275640, train_perplexity=598.1986, train_loss=6.393923

Batch 275650, train_perplexity=602.29987, train_loss=6.4007554

Batch 275660, train_perplexity=601.0298, train_loss=6.3986444

Batch 275670, train_perplexity=655.0844, train_loss=6.484764

Batch 275680, train_perplexity=685.0961, train_loss=6.529559

Batch 275690, train_perplexity=646.0234, train_loss=6.4708357

Batch 275700, train_perplexity=642.2721, train_loss=6.465012

Batch 275710, train_perplexity=609.0222, train_loss=6.4118547

Batch 275720, train_perplexity=600.18176, train_loss=6.3972325

Batch 275730, train_perplexity=648.7711, train_loss=6.47508

Batch 275740, train_perplexity=621.4358, train_loss=6.4320326

Batch 275750, train_perplexity=593.7772, train_loss=6.386504

Batch 275760, train_perplexity=572.89026, train_loss=6.350694

Batch 275770, train_perplexity=542.0973, train_loss=6.2954454

Batch 275780, train_perplexity=580.4734, train_loss=6.363844

Batch 275790, train_perplexity=651.0542, train_loss=6.478593

Batch 275800, train_perplexity=737.14044, train_loss=6.6027784

Batch 275810, train_perplexity=656.8586, train_loss=6.4874687

Batch 275820, train_perplexity=595.1208, train_loss=6.3887644

Batch 275830, train_perplexity=635.8197, train_loss=6.454915

Batch 275840, train_perplexity=639.8661, train_loss=6.461259

Batch 275850, train_perplexity=632.51935, train_loss=6.449711

Batch 275860, train_perplexity=600.0979, train_loss=6.397093

Batch 275870, train_perplexity=558.08673, train_loss=6.3245144

Batch 275880, train_perplexity=611.3347, train_loss=6.4156446

Batch 275890, train_perplexity=605.19055, train_loss=6.4055433

Batch 275900, train_perplexity=584.9513, train_loss=6.3715286

Batch 275910, train_perplexity=624.449, train_loss=6.4368696

Batch 275920, train_perplexity=722.3529, train_loss=6.582514

Batch 275930, train_perplexity=629.3091, train_loss=6.4446225

Batch 275940, train_perplexity=577.3397, train_loss=6.358431

Batch 275950, train_perplexity=613.7324, train_loss=6.419559

Batch 275960, train_perplexity=622.9869, train_loss=6.4345255

Batch 275970, train_perplexity=614.5163, train_loss=6.4208355

Batch 275980, train_perplexity=597.0786, train_loss=6.392049

Batch 275990, train_perplexity=611.15753, train_loss=6.4153547

Batch 276000, train_perplexity=575.6225, train_loss=6.355452

Batch 276010, train_perplexity=645.2771, train_loss=6.46968

Batch 276020, train_perplexity=638.5865, train_loss=6.459257

Batch 276030, train_perplexity=628.5821, train_loss=6.4434667

Batch 276040, train_perplexity=599.13495, train_loss=6.395487

Batch 276050, train_perplexity=649.00476, train_loss=6.47544

Batch 276060, train_perplexity=610.6565, train_loss=6.4145346

Batch 276070, train_perplexity=586.5607, train_loss=6.374276

Batch 276080, train_perplexity=596.6281, train_loss=6.391294

Batch 276090, train_perplexity=553.11743, train_loss=6.3155704

Batch 276100, train_perplexity=667.71375, train_loss=6.5038595

Batch 276110, train_perplexity=577.07605, train_loss=6.357974

Batch 276120, train_perplexity=602.29553, train_loss=6.4007483

Batch 276130, train_perplexity=564.98285, train_loss=6.3367953

Batch 276140, train_perplexity=579.0019, train_loss=6.3613057

Batch 276150, train_perplexity=543.3783, train_loss=6.297806

Batch 276160, train_perplexity=626.8383, train_loss=6.4406886

Batch 276170, train_perplexity=635.41174, train_loss=6.454273

Batch 276180, train_perplexity=632.51904, train_loss=6.4497104

Batch 276190, train_perplexity=555.815, train_loss=6.3204355

Batch 276200, train_perplexity=659.4914, train_loss=6.491469

Batch 276210, train_perplexity=609.6667, train_loss=6.4129124

Batch 276220, train_perplexity=580.26855, train_loss=6.363491

Batch 276230, train_perplexity=648.96545, train_loss=6.4753795

Batch 276240, train_perplexity=638.6681, train_loss=6.459385

Batch 276250, train_perplexity=608.07043, train_loss=6.4102907

Batch 276260, train_perplexity=612.863, train_loss=6.4181414

Batch 276270, train_perplexity=679.2783, train_loss=6.521031

Batch 276280, train_perplexity=612.98865, train_loss=6.4183464

Batch 276290, train_perplexity=623.3928, train_loss=6.435177
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 276300, train_perplexity=565.28973, train_loss=6.3373384

Batch 276310, train_perplexity=599.32294, train_loss=6.3958006

Batch 276320, train_perplexity=616.5709, train_loss=6.4241734

Batch 276330, train_perplexity=588.9803, train_loss=6.3783927

Batch 276340, train_perplexity=596.43243, train_loss=6.390966

Batch 276350, train_perplexity=608.9426, train_loss=6.411724

Batch 276360, train_perplexity=636.0435, train_loss=6.455267

Batch 276370, train_perplexity=676.3968, train_loss=6.51678

Batch 276380, train_perplexity=560.6094, train_loss=6.3290243

Batch 276390, train_perplexity=594.1573, train_loss=6.387144

Batch 276400, train_perplexity=619.84717, train_loss=6.429473

Batch 276410, train_perplexity=695.85394, train_loss=6.54514

Batch 276420, train_perplexity=633.391, train_loss=6.451088

Batch 276430, train_perplexity=606.0251, train_loss=6.4069214

Batch 276440, train_perplexity=647.69543, train_loss=6.4734206

Batch 276450, train_perplexity=589.366, train_loss=6.3790474

Batch 276460, train_perplexity=611.73773, train_loss=6.4163036

Batch 276470, train_perplexity=552.5088, train_loss=6.3144693

Batch 276480, train_perplexity=623.8576, train_loss=6.435922

Batch 276490, train_perplexity=603.7966, train_loss=6.4032373

Batch 276500, train_perplexity=625.594, train_loss=6.4387016

Batch 276510, train_perplexity=554.7027, train_loss=6.3184323

Batch 276520, train_perplexity=598.88873, train_loss=6.395076

Batch 276530, train_perplexity=592.7865, train_loss=6.3848343

Batch 276540, train_perplexity=626.5036, train_loss=6.4401546

Batch 276550, train_perplexity=603.997, train_loss=6.403569

Batch 276560, train_perplexity=569.60876, train_loss=6.3449497

Batch 276570, train_perplexity=565.79297, train_loss=6.338228

Batch 276580, train_perplexity=623.7425, train_loss=6.4357376

Batch 276590, train_perplexity=639.7739, train_loss=6.461115

Batch 276600, train_perplexity=581.13, train_loss=6.3649745

Batch 276610, train_perplexity=651.8867, train_loss=6.479871

Batch 276620, train_perplexity=553.0539, train_loss=6.3154554

Batch 276630, train_perplexity=632.0623, train_loss=6.448988

Batch 276640, train_perplexity=700.0897, train_loss=6.5512085

Batch 276650, train_perplexity=661.4317, train_loss=6.4944067

Batch 276660, train_perplexity=546.77277, train_loss=6.3040333

Batch 276670, train_perplexity=675.9467, train_loss=6.516114

Batch 276680, train_perplexity=619.8714, train_loss=6.429512

Batch 276690, train_perplexity=646.0557, train_loss=6.4708858

Batch 276700, train_perplexity=611.51434, train_loss=6.4159384

Batch 276710, train_perplexity=625.76825, train_loss=6.43898

Batch 276720, train_perplexity=562.4073, train_loss=6.3322263

Batch 276730, train_perplexity=672.8313, train_loss=6.5114946

Batch 276740, train_perplexity=622.17676, train_loss=6.433224

Batch 276750, train_perplexity=570.8484, train_loss=6.3471236

Batch 276760, train_perplexity=617.40497, train_loss=6.425525

Batch 276770, train_perplexity=615.5991, train_loss=6.422596

Batch 276780, train_perplexity=528.42065, train_loss=6.2698927

Batch 276790, train_perplexity=673.19525, train_loss=6.5120354

Batch 276800, train_perplexity=632.0496, train_loss=6.448968

Batch 276810, train_perplexity=673.9124, train_loss=6.5131

Batch 276820, train_perplexity=682.37115, train_loss=6.5255737

Batch 276830, train_perplexity=651.66327, train_loss=6.479528

Batch 276840, train_perplexity=648.8222, train_loss=6.4751587

Batch 276850, train_perplexity=591.5802, train_loss=6.3827972

Batch 276860, train_perplexity=563.03864, train_loss=6.3333483

Batch 276870, train_perplexity=601.5269, train_loss=6.3994713

Batch 276880, train_perplexity=584.67413, train_loss=6.3710546

Batch 276890, train_perplexity=582.0027, train_loss=6.366475

Batch 276900, train_perplexity=600.22955, train_loss=6.397312

Batch 276910, train_perplexity=590.6534, train_loss=6.3812294

Batch 276920, train_perplexity=637.7643, train_loss=6.4579687

Batch 276930, train_perplexity=647.4559, train_loss=6.4730506

Batch 276940, train_perplexity=651.92523, train_loss=6.47993

Batch 276950, train_perplexity=698.1802, train_loss=6.548477

Batch 276960, train_perplexity=547.3691, train_loss=6.3051233

Batch 276970, train_perplexity=650.719, train_loss=6.478078

Batch 276980, train_perplexity=598.3361, train_loss=6.3941526

Batch 276990, train_perplexity=580.92804, train_loss=6.364627

Batch 277000, train_perplexity=585.8211, train_loss=6.3730145

Batch 277010, train_perplexity=576.00385, train_loss=6.3561144

Batch 277020, train_perplexity=622.8396, train_loss=6.434289

Batch 277030, train_perplexity=587.96643, train_loss=6.37667

Batch 277040, train_perplexity=573.1955, train_loss=6.351227

Batch 277050, train_perplexity=644.63495, train_loss=6.468684

Batch 277060, train_perplexity=626.2441, train_loss=6.43974

Batch 277070, train_perplexity=645.76556, train_loss=6.4704366

Batch 277080, train_perplexity=626.66046, train_loss=6.440405

Batch 277090, train_perplexity=619.8613, train_loss=6.429496

Batch 277100, train_perplexity=606.822, train_loss=6.4082355

Batch 277110, train_perplexity=638.31555, train_loss=6.4588327

Batch 277120, train_perplexity=554.3254, train_loss=6.317752

Batch 277130, train_perplexity=615.33203, train_loss=6.422162

Batch 277140, train_perplexity=614.70154, train_loss=6.421137

Batch 277150, train_perplexity=619.27936, train_loss=6.4285564

Batch 277160, train_perplexity=619.24036, train_loss=6.4284935

Batch 277170, train_perplexity=670.0545, train_loss=6.507359

Batch 277180, train_perplexity=601.9206, train_loss=6.4001255

Batch 277190, train_perplexity=522.0134, train_loss=6.2576933

Batch 277200, train_perplexity=574.63715, train_loss=6.353739

Batch 277210, train_perplexity=638.69977, train_loss=6.4594345

Batch 277220, train_perplexity=682.8457, train_loss=6.526269

Batch 277230, train_perplexity=531.28705, train_loss=6.2753024

Batch 277240, train_perplexity=609.8935, train_loss=6.4132843

Batch 277250, train_perplexity=635.39417, train_loss=6.4542456

Batch 277260, train_perplexity=637.7171, train_loss=6.457895

Batch 277270, train_perplexity=662.4837, train_loss=6.495996

Batch 277280, train_perplexity=593.7101, train_loss=6.386391

Batch 277290, train_perplexity=594.65045, train_loss=6.387974

Batch 277300, train_perplexity=630.05164, train_loss=6.4458017

Batch 277310, train_perplexity=592.3641, train_loss=6.3841214

Batch 277320, train_perplexity=647.1333, train_loss=6.4725523

Batch 277330, train_perplexity=574.07025, train_loss=6.3527517

Batch 277340, train_perplexity=613.25586, train_loss=6.418782

Batch 277350, train_perplexity=576.9952, train_loss=6.357834

Batch 277360, train_perplexity=551.7053, train_loss=6.313014

Batch 277370, train_perplexity=584.6323, train_loss=6.370983

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Batch 277380, train_perplexity=582.1501, train_loss=6.3667283

Batch 277390, train_perplexity=626.12463, train_loss=6.4395494

Batch 277400, train_perplexity=632.602, train_loss=6.4498415

Batch 277410, train_perplexity=607.7626, train_loss=6.4097843

Batch 277420, train_perplexity=697.7974, train_loss=6.547929

Batch 277430, train_perplexity=606.0352, train_loss=6.406938

Batch 277440, train_perplexity=591.8155, train_loss=6.383195

Batch 277450, train_perplexity=594.2389, train_loss=6.3872814

Batch 277460, train_perplexity=603.56226, train_loss=6.402849

Batch 277470, train_perplexity=635.3121, train_loss=6.4541163

Batch 277480, train_perplexity=644.7216, train_loss=6.4688187

Batch 277490, train_perplexity=643.8479, train_loss=6.4674625

Batch 277500, train_perplexity=624.7876, train_loss=6.437412

Batch 277510, train_perplexity=642.57446, train_loss=6.4654827

Batch 277520, train_perplexity=681.293, train_loss=6.5239925

Batch 277530, train_perplexity=563.1498, train_loss=6.3335457

Batch 277540, train_perplexity=661.6929, train_loss=6.4948015
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 277550, train_perplexity=609.8795, train_loss=6.4132614

Batch 277560, train_perplexity=676.32007, train_loss=6.5166664

Batch 277570, train_perplexity=593.7987, train_loss=6.3865404

Batch 277580, train_perplexity=604.7287, train_loss=6.40478

Batch 277590, train_perplexity=635.1767, train_loss=6.453903

Batch 277600, train_perplexity=564.0168, train_loss=6.335084

Batch 277610, train_perplexity=607.6994, train_loss=6.4096804

Batch 277620, train_perplexity=588.8328, train_loss=6.3781424

Batch 277630, train_perplexity=680.00977, train_loss=6.522107

Batch 277640, train_perplexity=626.3378, train_loss=6.43989

Batch 277650, train_perplexity=658.3044, train_loss=6.4896674

Batch 277660, train_perplexity=627.18243, train_loss=6.4412374

Batch 277670, train_perplexity=555.2934, train_loss=6.3194966

Batch 277680, train_perplexity=624.7474, train_loss=6.4373474

Batch 277690, train_perplexity=595.50146, train_loss=6.389404

Batch 277700, train_perplexity=621.1715, train_loss=6.4316072

Batch 277710, train_perplexity=635.08673, train_loss=6.4537616

Batch 277720, train_perplexity=572.98126, train_loss=6.350853

Batch 277730, train_perplexity=609.2026, train_loss=6.412151

Batch 277740, train_perplexity=568.5092, train_loss=6.3430176

Batch 277750, train_perplexity=535.7027, train_loss=6.2835793

Batch 277760, train_perplexity=620.51404, train_loss=6.430548

Batch 277770, train_perplexity=596.369, train_loss=6.3908596

Batch 277780, train_perplexity=703.5252, train_loss=6.5561037

Batch 277790, train_perplexity=625.77094, train_loss=6.4389844

Batch 277800, train_perplexity=582.7069, train_loss=6.3676844

Batch 277810, train_perplexity=630.4555, train_loss=6.4464426

Batch 277820, train_perplexity=605.43066, train_loss=6.40594

Batch 277830, train_perplexity=668.6192, train_loss=6.5052147

Batch 277840, train_perplexity=625.29517, train_loss=6.438224

Batch 277850, train_perplexity=627.8922, train_loss=6.4423685

Batch 277860, train_perplexity=612.96936, train_loss=6.418315

Batch 277870, train_perplexity=597.22327, train_loss=6.392291

Batch 277880, train_perplexity=656.1004, train_loss=6.486314

Batch 277890, train_perplexity=590.1523, train_loss=6.3803806

Batch 277900, train_perplexity=605.3193, train_loss=6.405756

Batch 277910, train_perplexity=622.885, train_loss=6.434362

Batch 277920, train_perplexity=640.76373, train_loss=6.462661

Batch 277930, train_perplexity=615.98553, train_loss=6.4232235

Batch 277940, train_perplexity=691.12885, train_loss=6.5383263

Batch 277950, train_perplexity=600.8252, train_loss=6.398304

Batch 277960, train_perplexity=680.2536, train_loss=6.5224657

Batch 277970, train_perplexity=643.9855, train_loss=6.467676

Batch 277980, train_perplexity=732.6819, train_loss=6.5967116

Batch 277990, train_perplexity=583.3747, train_loss=6.3688297

Batch 278000, train_perplexity=614.0421, train_loss=6.4200635

Batch 278010, train_perplexity=647.0537, train_loss=6.4724293

Batch 278020, train_perplexity=614.7877, train_loss=6.421277

Batch 278030, train_perplexity=636.2446, train_loss=6.455583

Batch 278040, train_perplexity=587.83246, train_loss=6.376442

Batch 278050, train_perplexity=622.21027, train_loss=6.433278

Batch 278060, train_perplexity=601.5195, train_loss=6.399459

Batch 278070, train_perplexity=665.77405, train_loss=6.5009503

Batch 278080, train_perplexity=588.5805, train_loss=6.3777137

Batch 278090, train_perplexity=598.8539, train_loss=6.3950176

Batch 278100, train_perplexity=596.39087, train_loss=6.3908963

Batch 278110, train_perplexity=612.95355, train_loss=6.418289

Batch 278120, train_perplexity=532.85016, train_loss=6.27824

Batch 278130, train_perplexity=576.0033, train_loss=6.3561134

Batch 278140, train_perplexity=654.5654, train_loss=6.4839716

Batch 278150, train_perplexity=676.5355, train_loss=6.516985

Batch 278160, train_perplexity=702.82776, train_loss=6.555112

Batch 278170, train_perplexity=587.2724, train_loss=6.3754888

Batch 278180, train_perplexity=558.7037, train_loss=6.325619

Batch 278190, train_perplexity=613.2301, train_loss=6.4187403

Batch 278200, train_perplexity=646.70886, train_loss=6.471896

Batch 278210, train_perplexity=598.851, train_loss=6.395013

Batch 278220, train_perplexity=507.06906, train_loss=6.228647

Batch 278230, train_perplexity=643.0511, train_loss=6.466224

Batch 278240, train_perplexity=626.5756, train_loss=6.4402695

Batch 278250, train_perplexity=608.4858, train_loss=6.4109735

Batch 278260, train_perplexity=578.5002, train_loss=6.360439

Batch 278270, train_perplexity=645.5147, train_loss=6.470048

Batch 278280, train_perplexity=630.2631, train_loss=6.4461374

Batch 278290, train_perplexity=579.70026, train_loss=6.362511

Batch 278300, train_perplexity=635.52026, train_loss=6.454444

Batch 278310, train_perplexity=552.8667, train_loss=6.315117

Batch 278320, train_perplexity=613.2649, train_loss=6.418797

Batch 278330, train_perplexity=642.4991, train_loss=6.4653654

Batch 278340, train_perplexity=635.7169, train_loss=6.4547534

Batch 278350, train_perplexity=568.2729, train_loss=6.342602

Batch 278360, train_perplexity=564.9879, train_loss=6.3368044

Batch 278370, train_perplexity=643.1293, train_loss=6.466346

Batch 278380, train_perplexity=579.90015, train_loss=6.362856

Batch 278390, train_perplexity=555.1242, train_loss=6.319192

Batch 278400, train_perplexity=633.21436, train_loss=6.450809

Batch 278410, train_perplexity=616.1806, train_loss=6.42354

Batch 278420, train_perplexity=649.36444, train_loss=6.475994

Batch 278430, train_perplexity=603.7137, train_loss=6.4031

Batch 278440, train_perplexity=607.9359, train_loss=6.4100695

Batch 278450, train_perplexity=518.25964, train_loss=6.2504764

Batch 278460, train_perplexity=636.83374, train_loss=6.4565086

Batch 278470, train_perplexity=629.74945, train_loss=6.445322

Batch 278480, train_perplexity=624.3168, train_loss=6.436658

Batch 278490, train_perplexity=628.3187, train_loss=6.4430475

Batch 278500, train_perplexity=616.5206, train_loss=6.424092

Batch 278510, train_perplexity=571.52686, train_loss=6.3483114

Batch 278520, train_perplexity=633.6254, train_loss=6.451458

Batch 278530, train_perplexity=677.26886, train_loss=6.5180683

Batch 278540, train_perplexity=628.5902, train_loss=6.4434795

Batch 278550, train_perplexity=596.7983, train_loss=6.391579

Batch 278560, train_perplexity=541.5953, train_loss=6.294519

Batch 278570, train_perplexity=532.4879, train_loss=6.27756

Batch 278580, train_perplexity=632.6274, train_loss=6.4498816

Batch 278590, train_perplexity=593.1829, train_loss=6.385503

Batch 278600, train_perplexity=601.196, train_loss=6.398921

Batch 278610, train_perplexity=550.05383, train_loss=6.310016

Batch 278620, train_perplexity=621.74286, train_loss=6.4325266

Batch 278630, train_perplexity=640.86884, train_loss=6.462825

Batch 278640, train_perplexity=582.3125, train_loss=6.3670073

Batch 278650, train_perplexity=617.22864, train_loss=6.4252396

Batch 278660, train_perplexity=606.0497, train_loss=6.406962

Batch 278670, train_perplexity=637.11285, train_loss=6.456947

Batch 278680, train_perplexity=612.4563, train_loss=6.4174776

Batch 278690, train_perplexity=653.287, train_loss=6.4820166

Batch 278700, train_perplexity=603.2757, train_loss=6.4023743

Batch 278710, train_perplexity=630.1339, train_loss=6.4459324

Batch 278720, train_perplexity=676.2059, train_loss=6.5164976

Batch 278730, train_perplexity=657.1217, train_loss=6.4878693

Batch 278740, train_perplexity=596.9306, train_loss=6.391801

Batch 278750, train_perplexity=695.62634, train_loss=6.5448127

Batch 278760, train_perplexity=600.37897, train_loss=6.397561

Batch 278770, train_perplexity=563.0687, train_loss=6.3334017

Batch 278780, train_perplexity=586.99493, train_loss=6.375016

Batch 278790, train_perplexity=583.0032, train_loss=6.3681927

Batch 278800, train_perplexity=616.69, train_loss=6.4243665

Batch 278810, train_perplexity=589.2946, train_loss=6.3789263

Batch 278820, train_perplexity=689.1392, train_loss=6.5354433

Batch 278830, train_perplexity=610.8522, train_loss=6.414855

Batch 278840, train_perplexity=593.61755, train_loss=6.386235

Batch 278850, train_perplexity=596.5385, train_loss=6.391144
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 278860, train_perplexity=608.1763, train_loss=6.410465

Batch 278870, train_perplexity=670.6398, train_loss=6.508232

Batch 278880, train_perplexity=549.8844, train_loss=6.309708

Batch 278890, train_perplexity=574.4782, train_loss=6.353462

Batch 278900, train_perplexity=592.4632, train_loss=6.384289

Batch 278910, train_perplexity=604.3854, train_loss=6.404212

Batch 278920, train_perplexity=556.2854, train_loss=6.3212814

Batch 278930, train_perplexity=619.026, train_loss=6.4281473

Batch 278940, train_perplexity=598.1153, train_loss=6.3937836

Batch 278950, train_perplexity=580.48663, train_loss=6.363867

Batch 278960, train_perplexity=620.92633, train_loss=6.4312124

Batch 278970, train_perplexity=645.7801, train_loss=6.470459

Batch 278980, train_perplexity=567.6987, train_loss=6.341591

Batch 278990, train_perplexity=590.4045, train_loss=6.380808

Batch 279000, train_perplexity=647.7819, train_loss=6.473554

Batch 279010, train_perplexity=619.23267, train_loss=6.428481

Batch 279020, train_perplexity=590.38025, train_loss=6.380767

Batch 279030, train_perplexity=655.1794, train_loss=6.484909

Batch 279040, train_perplexity=652.7819, train_loss=6.481243

Batch 279050, train_perplexity=557.4675, train_loss=6.3234043

Batch 279060, train_perplexity=600.85785, train_loss=6.3983583

Batch 279070, train_perplexity=590.3521, train_loss=6.380719

Batch 279080, train_perplexity=612.45074, train_loss=6.4174685

Batch 279090, train_perplexity=628.04315, train_loss=6.442609

Batch 279100, train_perplexity=632.1075, train_loss=6.4490595

Batch 279110, train_perplexity=545.1532, train_loss=6.301067

Batch 279120, train_perplexity=594.86035, train_loss=6.3883266

Batch 279130, train_perplexity=591.8539, train_loss=6.38326

Batch 279140, train_perplexity=642.8727, train_loss=6.4659467

Batch 279150, train_perplexity=620.92633, train_loss=6.4312124

Batch 279160, train_perplexity=649.8867, train_loss=6.476798

Batch 279170, train_perplexity=600.86615, train_loss=6.398372

Batch 279180, train_perplexity=673.6496, train_loss=6.51271

Batch 279190, train_perplexity=590.14667, train_loss=6.380371

Batch 279200, train_perplexity=648.63135, train_loss=6.4748645

Batch 279210, train_perplexity=600.32715, train_loss=6.397475

Batch 279220, train_perplexity=612.8238, train_loss=6.4180775

Batch 279230, train_perplexity=588.3569, train_loss=6.3773336

Batch 279240, train_perplexity=536.37317, train_loss=6.28483

Batch 279250, train_perplexity=552.9579, train_loss=6.315282

Batch 279260, train_perplexity=612.9869, train_loss=6.4183435

Batch 279270, train_perplexity=589.82117, train_loss=6.3798194

Batch 279280, train_perplexity=666.37146, train_loss=6.5018473

Batch 279290, train_perplexity=650.7792, train_loss=6.4781704

Batch 279300, train_perplexity=568.42957, train_loss=6.3428774

Batch 279310, train_perplexity=635.60236, train_loss=6.454573

Batch 279320, train_perplexity=634.44836, train_loss=6.452756

Batch 279330, train_perplexity=632.78424, train_loss=6.4501295

Batch 279340, train_perplexity=588.96313, train_loss=6.3783636

Batch 279350, train_perplexity=604.01776, train_loss=6.4036036

Batch 279360, train_perplexity=604.61914, train_loss=6.4045987

Batch 279370, train_perplexity=695.5995, train_loss=6.544774

Batch 279380, train_perplexity=635.3954, train_loss=6.4542475

Batch 279390, train_perplexity=606.6589, train_loss=6.4079666

Batch 279400, train_perplexity=594.5155, train_loss=6.387747

Batch 279410, train_perplexity=607.4462, train_loss=6.4092636

Batch 279420, train_perplexity=551.5583, train_loss=6.3127475

Batch 279430, train_perplexity=587.9367, train_loss=6.3766193

Batch 279440, train_perplexity=616.10657, train_loss=6.42342

Batch 279450, train_perplexity=601.2539, train_loss=6.3990173

Batch 279460, train_perplexity=640.3654, train_loss=6.462039

Batch 279470, train_perplexity=666.89026, train_loss=6.5026255

Batch 279480, train_perplexity=595.1395, train_loss=6.388796

Batch 279490, train_perplexity=555.63055, train_loss=6.3201036

Batch 279500, train_perplexity=589.47815, train_loss=6.3792377

Batch 279510, train_perplexity=698.1748, train_loss=6.5484695

Batch 279520, train_perplexity=640.8096, train_loss=6.4627323

Batch 279530, train_perplexity=590.23785, train_loss=6.3805256

Batch 279540, train_perplexity=599.82556, train_loss=6.396639

Batch 279550, train_perplexity=557.609, train_loss=6.323658

Batch 279560, train_perplexity=621.3558, train_loss=6.431904

Batch 279570, train_perplexity=595.3325, train_loss=6.38912

Batch 279580, train_perplexity=618.30505, train_loss=6.426982

Batch 279590, train_perplexity=600.8303, train_loss=6.3983126

Batch 279600, train_perplexity=601.12317, train_loss=6.3988

Batch 279610, train_perplexity=638.5731, train_loss=6.459236

Batch 279620, train_perplexity=576.9858, train_loss=6.3578176

Batch 279630, train_perplexity=629.20795, train_loss=6.444462

Batch 279640, train_perplexity=612.4972, train_loss=6.4175444

Batch 279650, train_perplexity=594.9795, train_loss=6.388527

Batch 279660, train_perplexity=603.4115, train_loss=6.4025993

Batch 279670, train_perplexity=595.58606, train_loss=6.389546

Batch 279680, train_perplexity=622.8508, train_loss=6.434307

Batch 279690, train_perplexity=614.7323, train_loss=6.421187

Batch 279700, train_perplexity=665.03125, train_loss=6.499834

Batch 279710, train_perplexity=629.62573, train_loss=6.4451256

Batch 279720, train_perplexity=609.46204, train_loss=6.4125767

Batch 279730, train_perplexity=571.32465, train_loss=6.3479576

Batch 279740, train_perplexity=669.22107, train_loss=6.5061145

Batch 279750, train_perplexity=575.73914, train_loss=6.3556547

Batch 279760, train_perplexity=634.08185, train_loss=6.452178

Batch 279770, train_perplexity=595.7738, train_loss=6.389861

Batch 279780, train_perplexity=558.0032, train_loss=6.3243647

Batch 279790, train_perplexity=581.6537, train_loss=6.3658752

Batch 279800, train_perplexity=651.3119, train_loss=6.4789886

Batch 279810, train_perplexity=550.4088, train_loss=6.3106613

Batch 279820, train_perplexity=605.4532, train_loss=6.4059772

Batch 279830, train_perplexity=594.4736, train_loss=6.3876762

Batch 279840, train_perplexity=627.909, train_loss=6.442395

Batch 279850, train_perplexity=609.9499, train_loss=6.413377

Batch 279860, train_perplexity=558.2129, train_loss=6.3247404

Batch 279870, train_perplexity=591.21356, train_loss=6.3821774

Batch 279880, train_perplexity=558.07983, train_loss=6.324502

Batch 279890, train_perplexity=582.0907, train_loss=6.3666263

Batch 279900, train_perplexity=723.33453, train_loss=6.583872

Batch 279910, train_perplexity=615.44476, train_loss=6.422345

Batch 279920, train_perplexity=586.4519, train_loss=6.3740907

Batch 279930, train_perplexity=578.9729, train_loss=6.3612556

Batch 279940, train_perplexity=630.12164, train_loss=6.445913

Batch 279950, train_perplexity=580.003, train_loss=6.3630333

Batch 279960, train_perplexity=596.53796, train_loss=6.391143

Batch 279970, train_perplexity=614.98004, train_loss=6.42159

Batch 279980, train_perplexity=631.50946, train_loss=6.448113

Batch 279990, train_perplexity=588.8188, train_loss=6.3781185

Batch 280000, train_perplexity=584.5256, train_loss=6.3708005

Batch 280010, train_perplexity=625.3834, train_loss=6.438365

Batch 280020, train_perplexity=544.7177, train_loss=6.3002677

Batch 280030, train_perplexity=571.4432, train_loss=6.348165

Batch 280040, train_perplexity=557.14386, train_loss=6.3228235

Batch 280050, train_perplexity=565.1458, train_loss=6.337084

Batch 280060, train_perplexity=685.4739, train_loss=6.5301104

Batch 280070, train_perplexity=701.25073, train_loss=6.5528655

Batch 280080, train_perplexity=620.7647, train_loss=6.430952

Batch 280090, train_perplexity=609.8856, train_loss=6.4132714

Batch 280100, train_perplexity=622.7695, train_loss=6.4341764

Batch 280110, train_perplexity=608.21136, train_loss=6.4105225

Batch 280120, train_perplexity=587.42896, train_loss=6.3757553

Batch 280130, train_perplexity=602.8696, train_loss=6.401701

Batch 280140, train_perplexity=683.6576, train_loss=6.527457

Batch 280150, train_perplexity=663.8159, train_loss=6.498005

Batch 280160, train_perplexity=559.7476, train_loss=6.327486
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 280170, train_perplexity=623.9183, train_loss=6.4360194

Batch 280180, train_perplexity=708.34015, train_loss=6.5629244

Batch 280190, train_perplexity=703.4494, train_loss=6.555996

Batch 280200, train_perplexity=605.8988, train_loss=6.406713

Batch 280210, train_perplexity=564.0953, train_loss=6.335223

Batch 280220, train_perplexity=684.1409, train_loss=6.528164

Batch 280230, train_perplexity=562.08124, train_loss=6.3316464

Batch 280240, train_perplexity=623.2817, train_loss=6.4349985

Batch 280250, train_perplexity=554.7503, train_loss=6.318518

Batch 280260, train_perplexity=568.31354, train_loss=6.3426733

Batch 280270, train_perplexity=584.69086, train_loss=6.3710833

Batch 280280, train_perplexity=564.7547, train_loss=6.3363914

Batch 280290, train_perplexity=576.9454, train_loss=6.3577476

Batch 280300, train_perplexity=642.4274, train_loss=6.465254

Batch 280310, train_perplexity=615.76, train_loss=6.4228573

Batch 280320, train_perplexity=614.4943, train_loss=6.4207997

Batch 280330, train_perplexity=529.7805, train_loss=6.272463

Batch 280340, train_perplexity=730.8643, train_loss=6.594228

Batch 280350, train_perplexity=562.20215, train_loss=6.3318615

Batch 280360, train_perplexity=667.16406, train_loss=6.503036

Batch 280370, train_perplexity=580.95905, train_loss=6.3646803

Batch 280380, train_perplexity=658.9938, train_loss=6.490714

Batch 280390, train_perplexity=613.262, train_loss=6.4187922

Batch 280400, train_perplexity=635.4475, train_loss=6.4543295

Batch 280410, train_perplexity=641.5262, train_loss=6.46385

Batch 280420, train_perplexity=661.1195, train_loss=6.4939346

Batch 280430, train_perplexity=570.9096, train_loss=6.347231

Batch 280440, train_perplexity=575.7496, train_loss=6.355673

Batch 280450, train_perplexity=605.0538, train_loss=6.4053173

Batch 280460, train_perplexity=619.5641, train_loss=6.429016

Batch 280470, train_perplexity=635.9853, train_loss=6.4551754

Batch 280480, train_perplexity=658.521, train_loss=6.4899964

Batch 280490, train_perplexity=669.4078, train_loss=6.5063934

Batch 280500, train_perplexity=673.5452, train_loss=6.512555

Batch 280510, train_perplexity=535.82404, train_loss=6.283806

Batch 280520, train_perplexity=590.2083, train_loss=6.3804755

Batch 280530, train_perplexity=612.89453, train_loss=6.418193

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Batch 280540, train_perplexity=756.34424, train_loss=6.6284966

Batch 280550, train_perplexity=593.30566, train_loss=6.38571

Batch 280560, train_perplexity=671.02875, train_loss=6.508812

Batch 280570, train_perplexity=683.88196, train_loss=6.5277853

Batch 280580, train_perplexity=654.89075, train_loss=6.4844685

Batch 280590, train_perplexity=644.4847, train_loss=6.468451

Batch 280600, train_perplexity=587.26483, train_loss=6.375476

Batch 280610, train_perplexity=626.64197, train_loss=6.4403753

Batch 280620, train_perplexity=574.23285, train_loss=6.353035

Batch 280630, train_perplexity=590.76215, train_loss=6.3814135

Batch 280640, train_perplexity=612.38916, train_loss=6.417368

Batch 280650, train_perplexity=664.14014, train_loss=6.498493

Batch 280660, train_perplexity=554.09155, train_loss=6.31733

Batch 280670, train_perplexity=540.5774, train_loss=6.292638

Batch 280680, train_perplexity=678.37744, train_loss=6.519704

Batch 280690, train_perplexity=637.42194, train_loss=6.457432

Batch 280700, train_perplexity=585.8222, train_loss=6.3730164

Batch 280710, train_perplexity=570.2664, train_loss=6.3461037

Batch 280720, train_perplexity=597.6483, train_loss=6.3930025

Batch 280730, train_perplexity=593.4528, train_loss=6.3859577

Batch 280740, train_perplexity=574.06915, train_loss=6.35275

Batch 280750, train_perplexity=613.46875, train_loss=6.4191294

Batch 280760, train_perplexity=493.34515, train_loss=6.201209

Batch 280770, train_perplexity=633.7608, train_loss=6.4516716

Batch 280780, train_perplexity=598.61865, train_loss=6.3946247

Batch 280790, train_perplexity=596.1043, train_loss=6.3904157

Batch 280800, train_perplexity=668.5172, train_loss=6.505062

Batch 280810, train_perplexity=624.0016, train_loss=6.436153

Batch 280820, train_perplexity=719.29456, train_loss=6.578271

Batch 280830, train_perplexity=580.3881, train_loss=6.363697

Batch 280840, train_perplexity=515.5319, train_loss=6.245199

Batch 280850, train_perplexity=608.0148, train_loss=6.410199

Batch 280860, train_perplexity=651.35443, train_loss=6.479054

Batch 280870, train_perplexity=622.5762, train_loss=6.433866

Batch 280880, train_perplexity=629.217, train_loss=6.444476

Batch 280890, train_perplexity=608.4022, train_loss=6.410836

Batch 280900, train_perplexity=566.1487, train_loss=6.3388567

Batch 280910, train_perplexity=560.28174, train_loss=6.3284397

Batch 280920, train_perplexity=620.18835, train_loss=6.430023

Batch 280930, train_perplexity=601.4621, train_loss=6.3993635

Batch 280940, train_perplexity=650.60046, train_loss=6.4778957

Batch 280950, train_perplexity=527.5601, train_loss=6.268263

Batch 280960, train_perplexity=620.5741, train_loss=6.430645

Batch 280970, train_perplexity=569.5664, train_loss=6.3448753

Batch 280980, train_perplexity=593.28534, train_loss=6.3856754

Batch 280990, train_perplexity=639.4479, train_loss=6.460605

Batch 281000, train_perplexity=600.2513, train_loss=6.3973484

Batch 281010, train_perplexity=644.16974, train_loss=6.4679623

Batch 281020, train_perplexity=531.68823, train_loss=6.2760572

Batch 281030, train_perplexity=693.23065, train_loss=6.541363

Batch 281040, train_perplexity=602.24384, train_loss=6.4006624

Batch 281050, train_perplexity=628.4445, train_loss=6.443248

Batch 281060, train_perplexity=626.9976, train_loss=6.440943

Batch 281070, train_perplexity=680.9923, train_loss=6.523551

Batch 281080, train_perplexity=608.86194, train_loss=6.4115915

Batch 281090, train_perplexity=601.9648, train_loss=6.400199

Batch 281100, train_perplexity=623.9986, train_loss=6.436148

Batch 281110, train_perplexity=605.8884, train_loss=6.406696

Batch 281120, train_perplexity=604.8939, train_loss=6.405053

Batch 281130, train_perplexity=616.35693, train_loss=6.423826

Batch 281140, train_perplexity=683.1291, train_loss=6.526684

Batch 281150, train_perplexity=549.70905, train_loss=6.309389

Batch 281160, train_perplexity=580.4313, train_loss=6.3637714

Batch 281170, train_perplexity=581.4238, train_loss=6.36548

Batch 281180, train_perplexity=666.1859, train_loss=6.501569

Batch 281190, train_perplexity=586.13293, train_loss=6.3735466

Batch 281200, train_perplexity=597.6751, train_loss=6.3930473

Batch 281210, train_perplexity=614.2573, train_loss=6.420414

Batch 281220, train_perplexity=657.484, train_loss=6.4884205

Batch 281230, train_perplexity=585.0919, train_loss=6.371769

Batch 281240, train_perplexity=618.0515, train_loss=6.426572

Batch 281250, train_perplexity=637.0409, train_loss=6.456834

Batch 281260, train_perplexity=567.2463, train_loss=6.3407936

Batch 281270, train_perplexity=624.52136, train_loss=6.4369855

Batch 281280, train_perplexity=599.55676, train_loss=6.3961906

Batch 281290, train_perplexity=591.8956, train_loss=6.3833303

Batch 281300, train_perplexity=567.2793, train_loss=6.340852

Batch 281310, train_perplexity=577.3755, train_loss=6.358493

Batch 281320, train_perplexity=647.9355, train_loss=6.473791

Batch 281330, train_perplexity=697.8483, train_loss=6.548002

Batch 281340, train_perplexity=617.5301, train_loss=6.425728

Batch 281350, train_perplexity=667.6717, train_loss=6.5037966

Batch 281360, train_perplexity=556.642, train_loss=6.3219223

Batch 281370, train_perplexity=607.43146, train_loss=6.4092393

Batch 281380, train_perplexity=616.9232, train_loss=6.4247446

Batch 281390, train_perplexity=584.14465, train_loss=6.3701487

Batch 281400, train_perplexity=581.0732, train_loss=6.3648767

Batch 281410, train_perplexity=601.8744, train_loss=6.4000487
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 281420, train_perplexity=597.66656, train_loss=6.393033

Batch 281430, train_perplexity=623.4124, train_loss=6.4352083

Batch 281440, train_perplexity=623.4526, train_loss=6.4352727

Batch 281450, train_perplexity=588.97296, train_loss=6.3783803

Batch 281460, train_perplexity=603.7666, train_loss=6.4031878

Batch 281470, train_perplexity=610.7153, train_loss=6.414631

Batch 281480, train_perplexity=623.8216, train_loss=6.4358644

Batch 281490, train_perplexity=611.7494, train_loss=6.4163227

Batch 281500, train_perplexity=645.769, train_loss=6.470442

Batch 281510, train_perplexity=618.4486, train_loss=6.427214

Batch 281520, train_perplexity=612.5743, train_loss=6.4176702

Batch 281530, train_perplexity=590.5492, train_loss=6.381053

Batch 281540, train_perplexity=658.3656, train_loss=6.4897604

Batch 281550, train_perplexity=612.6891, train_loss=6.4178576

Batch 281560, train_perplexity=604.91675, train_loss=6.405091

Batch 281570, train_perplexity=583.31354, train_loss=6.368725

Batch 281580, train_perplexity=590.94586, train_loss=6.3817244

Batch 281590, train_perplexity=555.59717, train_loss=6.3200436

Batch 281600, train_perplexity=638.56396, train_loss=6.459222

Batch 281610, train_perplexity=632.9466, train_loss=6.450386

Batch 281620, train_perplexity=685.65625, train_loss=6.5303764

Batch 281630, train_perplexity=542.939, train_loss=6.296997

Batch 281640, train_perplexity=613.6982, train_loss=6.419503

Batch 281650, train_perplexity=628.76825, train_loss=6.443763

Batch 281660, train_perplexity=593.76416, train_loss=6.3864822

Batch 281670, train_perplexity=601.40015, train_loss=6.3992605

Batch 281680, train_perplexity=621.51996, train_loss=6.432168

Batch 281690, train_perplexity=588.2615, train_loss=6.3771715

Batch 281700, train_perplexity=623.42554, train_loss=6.4352293

Batch 281710, train_perplexity=567.8184, train_loss=6.3418016

Batch 281720, train_perplexity=614.91675, train_loss=6.421487

Batch 281730, train_perplexity=595.68915, train_loss=6.389719

Batch 281740, train_perplexity=634.9589, train_loss=6.4535604

Batch 281750, train_perplexity=537.04443, train_loss=6.286081

Batch 281760, train_perplexity=615.6942, train_loss=6.4227505

Batch 281770, train_perplexity=565.1873, train_loss=6.3371572

Batch 281780, train_perplexity=618.9555, train_loss=6.4280334

Batch 281790, train_perplexity=674.6149, train_loss=6.514142

Batch 281800, train_perplexity=657.30475, train_loss=6.4881477

Batch 281810, train_perplexity=684.48615, train_loss=6.5286684

Batch 281820, train_perplexity=636.6534, train_loss=6.4562254

Batch 281830, train_perplexity=625.2439, train_loss=6.438142

Batch 281840, train_perplexity=599.7292, train_loss=6.396478

Batch 281850, train_perplexity=656.4249, train_loss=6.4868083

Batch 281860, train_perplexity=591.23724, train_loss=6.3822174

Batch 281870, train_perplexity=690.499, train_loss=6.5374146

Batch 281880, train_perplexity=534.72906, train_loss=6.28176

Batch 281890, train_perplexity=636.3244, train_loss=6.4557085

Batch 281900, train_perplexity=642.5276, train_loss=6.4654098

Batch 281910, train_perplexity=641.15186, train_loss=6.4632664

Batch 281920, train_perplexity=598.4285, train_loss=6.394307

Batch 281930, train_perplexity=561.1774, train_loss=6.330037

Batch 281940, train_perplexity=681.2326, train_loss=6.523904

Batch 281950, train_perplexity=597.7994, train_loss=6.393255

Batch 281960, train_perplexity=607.8304, train_loss=6.409896

Batch 281970, train_perplexity=619.8826, train_loss=6.42953

Batch 281980, train_perplexity=539.64124, train_loss=6.2909045

Batch 281990, train_perplexity=583.8495, train_loss=6.369643

Batch 282000, train_perplexity=632.5272, train_loss=6.4497232

Batch 282010, train_perplexity=586.86816, train_loss=6.3748

Batch 282020, train_perplexity=619.5794, train_loss=6.429041

Batch 282030, train_perplexity=696.2346, train_loss=6.5456867

Batch 282040, train_perplexity=565.7806, train_loss=6.3382063

Batch 282050, train_perplexity=551.1379, train_loss=6.311985

Batch 282060, train_perplexity=698.71204, train_loss=6.5492387

Batch 282070, train_perplexity=602.77997, train_loss=6.401552

Batch 282080, train_perplexity=677.3354, train_loss=6.5181665

Batch 282090, train_perplexity=661.768, train_loss=6.494915

Batch 282100, train_perplexity=647.37524, train_loss=6.472926

Batch 282110, train_perplexity=612.92725, train_loss=6.4182463

Batch 282120, train_perplexity=582.272, train_loss=6.3669376

Batch 282130, train_perplexity=556.43713, train_loss=6.321554

Batch 282140, train_perplexity=612.4668, train_loss=6.417495

Batch 282150, train_perplexity=717.16296, train_loss=6.575303

Batch 282160, train_perplexity=591.02246, train_loss=6.381854

Batch 282170, train_perplexity=600.41046, train_loss=6.3976135

Batch 282180, train_perplexity=584.47205, train_loss=6.370709

Batch 282190, train_perplexity=596.14124, train_loss=6.3904777

Batch 282200, train_perplexity=544.10065, train_loss=6.2991343

Batch 282210, train_perplexity=659.771, train_loss=6.491893

Batch 282220, train_perplexity=598.75134, train_loss=6.3948464

Batch 282230, train_perplexity=615.9427, train_loss=6.423154

Batch 282240, train_perplexity=611.15985, train_loss=6.4153585

Batch 282250, train_perplexity=612.3319, train_loss=6.4172745

Batch 282260, train_perplexity=598.42456, train_loss=6.3943005

Batch 282270, train_perplexity=597.23553, train_loss=6.3923116

Batch 282280, train_perplexity=636.1985, train_loss=6.4555106

Batch 282290, train_perplexity=602.6164, train_loss=6.401281

Batch 282300, train_perplexity=626.6653, train_loss=6.4404125

Batch 282310, train_perplexity=608.3123, train_loss=6.4106884

Batch 282320, train_perplexity=577.4554, train_loss=6.358631

Batch 282330, train_perplexity=661.57996, train_loss=6.494631

Batch 282340, train_perplexity=617.46564, train_loss=6.4256234

Batch 282350, train_perplexity=592.5887, train_loss=6.3845005

Batch 282360, train_perplexity=657.6151, train_loss=6.48862

Batch 282370, train_perplexity=558.5303, train_loss=6.325309

Batch 282380, train_perplexity=651.8973, train_loss=6.479887

Batch 282390, train_perplexity=572.37885, train_loss=6.349801

Batch 282400, train_perplexity=592.67145, train_loss=6.38464

Batch 282410, train_perplexity=646.6243, train_loss=6.4717655

Batch 282420, train_perplexity=613.4331, train_loss=6.419071

Batch 282430, train_perplexity=597.46686, train_loss=6.392699

Batch 282440, train_perplexity=605.3654, train_loss=6.4058323

Batch 282450, train_perplexity=607.3764, train_loss=6.4091487

Batch 282460, train_perplexity=594.71454, train_loss=6.3880816

Batch 282470, train_perplexity=586.66754, train_loss=6.3744583

Batch 282480, train_perplexity=579.3739, train_loss=6.361948

Batch 282490, train_perplexity=581.076, train_loss=6.3648815

Batch 282500, train_perplexity=635.9152, train_loss=6.4550653

Batch 282510, train_perplexity=620.88574, train_loss=6.431147

Batch 282520, train_perplexity=634.7383, train_loss=6.4532127

Batch 282530, train_perplexity=600.2536, train_loss=6.397352

Batch 282540, train_perplexity=651.6244, train_loss=6.4794683

Batch 282550, train_perplexity=615.95355, train_loss=6.4231715

Batch 282560, train_perplexity=631.59564, train_loss=6.4482493

Batch 282570, train_perplexity=561.361, train_loss=6.330364

Batch 282580, train_perplexity=574.1258, train_loss=6.3528485

Batch 282590, train_perplexity=599.35754, train_loss=6.3958583

Batch 282600, train_perplexity=652.74677, train_loss=6.4811893

Batch 282610, train_perplexity=658.62494, train_loss=6.4901543

Batch 282620, train_perplexity=601.2952, train_loss=6.399086

Batch 282630, train_perplexity=710.71045, train_loss=6.566265

Batch 282640, train_perplexity=647.95276, train_loss=6.473818

Batch 282650, train_perplexity=663.74, train_loss=6.4978905

Batch 282660, train_perplexity=565.5734, train_loss=6.33784

Batch 282670, train_perplexity=593.5547, train_loss=6.3861294

Batch 282680, train_perplexity=618.7197, train_loss=6.4276524

Batch 282690, train_perplexity=562.8239, train_loss=6.332967

Batch 282700, train_perplexity=645.05743, train_loss=6.4693394

Batch 282710, train_perplexity=633.9222, train_loss=6.451926

Batch 282720, train_perplexity=702.2258, train_loss=6.554255
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 282730, train_perplexity=558.19293, train_loss=6.3247046

Batch 282740, train_perplexity=631.52423, train_loss=6.4481363

Batch 282750, train_perplexity=625.3298, train_loss=6.438279

Batch 282760, train_perplexity=648.34467, train_loss=6.4744225

Batch 282770, train_perplexity=575.5607, train_loss=6.355345

Batch 282780, train_perplexity=600.1188, train_loss=6.3971276

Batch 282790, train_perplexity=598.56213, train_loss=6.3945303

Batch 282800, train_perplexity=586.1245, train_loss=6.3735323

Batch 282810, train_perplexity=603.91235, train_loss=6.403429

Batch 282820, train_perplexity=595.54205, train_loss=6.389472

Batch 282830, train_perplexity=609.62103, train_loss=6.4128375

Batch 282840, train_perplexity=640.9177, train_loss=6.462901

Batch 282850, train_perplexity=657.1014, train_loss=6.4878383

Batch 282860, train_perplexity=580.9682, train_loss=6.364696

Batch 282870, train_perplexity=621.6972, train_loss=6.432453

Batch 282880, train_perplexity=644.4389, train_loss=6.46838

Batch 282890, train_perplexity=713.6044, train_loss=6.5703287

Batch 282900, train_perplexity=577.733, train_loss=6.359112

Batch 282910, train_perplexity=597.4437, train_loss=6.39266

Batch 282920, train_perplexity=661.7301, train_loss=6.494858

Batch 282930, train_perplexity=571.7471, train_loss=6.3486967

Batch 282940, train_perplexity=608.245, train_loss=6.410578

Batch 282950, train_perplexity=647.145, train_loss=6.4725704

Batch 282960, train_perplexity=618.00146, train_loss=6.426491

Batch 282970, train_perplexity=572.8468, train_loss=6.3506184

Batch 282980, train_perplexity=585.9655, train_loss=6.373261

Batch 282990, train_perplexity=543.6349, train_loss=6.298278

Batch 283000, train_perplexity=662.0858, train_loss=6.495395

Batch 283010, train_perplexity=643.39984, train_loss=6.4667664

Batch 283020, train_perplexity=627.4618, train_loss=6.441683

Batch 283030, train_perplexity=558.2006, train_loss=6.3247185

Batch 283040, train_perplexity=623.83527, train_loss=6.4358864

Batch 283050, train_perplexity=539.28906, train_loss=6.2902517

Batch 283060, train_perplexity=661.8879, train_loss=6.495096

Batch 283070, train_perplexity=640.8676, train_loss=6.462823

Batch 283080, train_perplexity=597.168, train_loss=6.3921986

Batch 283090, train_perplexity=598.4542, train_loss=6.39435

Batch 283100, train_perplexity=629.4282, train_loss=6.444812

Batch 283110, train_perplexity=596.37866, train_loss=6.390876

Batch 283120, train_perplexity=605.62933, train_loss=6.406268

Batch 283130, train_perplexity=562.15094, train_loss=6.3317704

Batch 283140, train_perplexity=525.11664, train_loss=6.2636204

Batch 283150, train_perplexity=641.40753, train_loss=6.463665

Batch 283160, train_perplexity=568.74243, train_loss=6.3434277

Batch 283170, train_perplexity=655.3884, train_loss=6.485228

Batch 283180, train_perplexity=633.5475, train_loss=6.451335

Batch 283190, train_perplexity=568.3092, train_loss=6.3426657

Batch 283200, train_perplexity=598.41144, train_loss=6.3942785

Batch 283210, train_perplexity=608.9508, train_loss=6.4117374

Batch 283220, train_perplexity=696.82184, train_loss=6.54653

Batch 283230, train_perplexity=558.54785, train_loss=6.3253403

Batch 283240, train_perplexity=639.1854, train_loss=6.4601946

Batch 283250, train_perplexity=687.2241, train_loss=6.5326605

Batch 283260, train_perplexity=606.73926, train_loss=6.408099

Batch 283270, train_perplexity=590.41406, train_loss=6.380824

Batch 283280, train_perplexity=629.5279, train_loss=6.44497

Batch 283290, train_perplexity=603.94916, train_loss=6.40349

Batch 283300, train_perplexity=644.1661, train_loss=6.4679565

Batch 283310, train_perplexity=534.52026, train_loss=6.2813697

Batch 283320, train_perplexity=681.4659, train_loss=6.524246

Batch 283330, train_perplexity=550.58575, train_loss=6.3109827

Batch 283340, train_perplexity=536.1793, train_loss=6.2844687

Batch 283350, train_perplexity=619.2817, train_loss=6.4285603

Batch 283360, train_perplexity=651.8519, train_loss=6.4798174

Batch 283370, train_perplexity=654.86206, train_loss=6.4844246

Batch 283380, train_perplexity=582.9003, train_loss=6.3680162

Batch 283390, train_perplexity=602.48456, train_loss=6.401062

Batch 283400, train_perplexity=579.2651, train_loss=6.36176

Batch 283410, train_perplexity=605.21246, train_loss=6.4055796

Batch 283420, train_perplexity=592.45306, train_loss=6.3842716

Batch 283430, train_perplexity=576.6233, train_loss=6.357189

Batch 283440, train_perplexity=634.266, train_loss=6.4524684

Batch 283450, train_perplexity=601.57684, train_loss=6.3995543

Batch 283460, train_perplexity=599.6803, train_loss=6.3963966

Batch 283470, train_perplexity=639.38446, train_loss=6.460506

Batch 283480, train_perplexity=568.6394, train_loss=6.3432465

Batch 283490, train_perplexity=607.0692, train_loss=6.408643

Batch 283500, train_perplexity=578.06696, train_loss=6.3596897

Batch 283510, train_perplexity=580.13605, train_loss=6.3632627

Batch 283520, train_perplexity=567.83655, train_loss=6.3418336

Batch 283530, train_perplexity=682.3568, train_loss=6.5255527

Batch 283540, train_perplexity=638.35236, train_loss=6.4588904

Batch 283550, train_perplexity=591.24176, train_loss=6.382225

Batch 283560, train_perplexity=663.02856, train_loss=6.496818

Batch 283570, train_perplexity=582.40857, train_loss=6.3671722

Batch 283580, train_perplexity=622.74927, train_loss=6.434144

Batch 283590, train_perplexity=605.79913, train_loss=6.4065485

Batch 283600, train_perplexity=580.49, train_loss=6.3638725

Batch 283610, train_perplexity=609.3353, train_loss=6.412369

Batch 283620, train_perplexity=538.8613, train_loss=6.2894583

Batch 283630, train_perplexity=591.631, train_loss=6.382883

Batch 283640, train_perplexity=698.8546, train_loss=6.549443

Batch 283650, train_perplexity=539.78815, train_loss=6.291177

Batch 283660, train_perplexity=595.3913, train_loss=6.389219

Batch 283670, train_perplexity=625.11664, train_loss=6.437938

Batch 283680, train_perplexity=652.94696, train_loss=6.481496

Batch 283690, train_perplexity=615.6802, train_loss=6.4227276

Batch 283700, train_perplexity=515.12695, train_loss=6.2444134

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Batch 283710, train_perplexity=549.84454, train_loss=6.3096356

Batch 283720, train_perplexity=559.332, train_loss=6.326743

Batch 283730, train_perplexity=628.36755, train_loss=6.4431252

Batch 283740, train_perplexity=613.8357, train_loss=6.4197273

Batch 283750, train_perplexity=547.90857, train_loss=6.3061085

Batch 283760, train_perplexity=572.3537, train_loss=6.349757

Batch 283770, train_perplexity=619.73364, train_loss=6.42929

Batch 283780, train_perplexity=633.6907, train_loss=6.451561

Batch 283790, train_perplexity=591.0845, train_loss=6.381959

Batch 283800, train_perplexity=612.58716, train_loss=6.417691

Batch 283810, train_perplexity=611.7223, train_loss=6.4162784

Batch 283820, train_perplexity=574.7314, train_loss=6.353903

Batch 283830, train_perplexity=639.41833, train_loss=6.460559

Batch 283840, train_perplexity=518.5266, train_loss=6.2509913

Batch 283850, train_perplexity=594.5994, train_loss=6.387888

Batch 283860, train_perplexity=560.8896, train_loss=6.329524

Batch 283870, train_perplexity=563.8382, train_loss=6.3347673

Batch 283880, train_perplexity=628.9518, train_loss=6.4440546

Batch 283890, train_perplexity=687.5414, train_loss=6.533122

Batch 283900, train_perplexity=521.733, train_loss=6.257156

Batch 283910, train_perplexity=579.6165, train_loss=6.3623667

Batch 283920, train_perplexity=617.7593, train_loss=6.426099

Batch 283930, train_perplexity=646.7628, train_loss=6.4719796

Batch 283940, train_perplexity=571.8987, train_loss=6.348962

Batch 283950, train_perplexity=618.6052, train_loss=6.4274673

Batch 283960, train_perplexity=586.22156, train_loss=6.3736978

Batch 283970, train_perplexity=630.3437, train_loss=6.446265
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 283980, train_perplexity=624.0436, train_loss=6.43622

Batch 283990, train_perplexity=662.62305, train_loss=6.4962063

Batch 284000, train_perplexity=677.57056, train_loss=6.5185137

Batch 284010, train_perplexity=618.73267, train_loss=6.4276733

Batch 284020, train_perplexity=652.899, train_loss=6.4814224

Batch 284030, train_perplexity=680.02856, train_loss=6.522135

Batch 284040, train_perplexity=677.9622, train_loss=6.5190916

Batch 284050, train_perplexity=604.7959, train_loss=6.404891

Batch 284060, train_perplexity=564.4833, train_loss=6.335911

Batch 284070, train_perplexity=614.05676, train_loss=6.4200873

Batch 284080, train_perplexity=595.71844, train_loss=6.389768

Batch 284090, train_perplexity=637.3581, train_loss=6.4573317

Batch 284100, train_perplexity=598.11816, train_loss=6.3937883

Batch 284110, train_perplexity=628.50867, train_loss=6.44335

Batch 284120, train_perplexity=630.64435, train_loss=6.446742

Batch 284130, train_perplexity=675.5762, train_loss=6.515566

Batch 284140, train_perplexity=610.8679, train_loss=6.4148808

Batch 284150, train_perplexity=584.2745, train_loss=6.370371

Batch 284160, train_perplexity=624.7733, train_loss=6.437389

Batch 284170, train_perplexity=589.8721, train_loss=6.3799057

Batch 284180, train_perplexity=628.2276, train_loss=6.4429026

Batch 284190, train_perplexity=586.61163, train_loss=6.374363

Batch 284200, train_perplexity=540.8012, train_loss=6.2930517

Batch 284210, train_perplexity=598.0663, train_loss=6.3937016

Batch 284220, train_perplexity=579.5341, train_loss=6.3622246

Batch 284230, train_perplexity=648.46466, train_loss=6.4746075

Batch 284240, train_perplexity=548.03503, train_loss=6.3063393

Batch 284250, train_perplexity=582.9963, train_loss=6.3681808

Batch 284260, train_perplexity=606.1959, train_loss=6.407203

Batch 284270, train_perplexity=601.1381, train_loss=6.3988247

Batch 284280, train_perplexity=561.03455, train_loss=6.3297825

Batch 284290, train_perplexity=600.9839, train_loss=6.398568

Batch 284300, train_perplexity=645.9072, train_loss=6.470656

Batch 284310, train_perplexity=573.4853, train_loss=6.3517323

Batch 284320, train_perplexity=610.1105, train_loss=6.41364

Batch 284330, train_perplexity=552.1048, train_loss=6.313738

Batch 284340, train_perplexity=579.48883, train_loss=6.3621464

Batch 284350, train_perplexity=552.7275, train_loss=6.314865

Batch 284360, train_perplexity=600.8934, train_loss=6.3984175

Batch 284370, train_perplexity=579.0212, train_loss=6.361339

Batch 284380, train_perplexity=599.3732, train_loss=6.3958845

Batch 284390, train_perplexity=629.1498, train_loss=6.4443693

Batch 284400, train_perplexity=563.33405, train_loss=6.333873

Batch 284410, train_perplexity=592.0018, train_loss=6.3835096

Batch 284420, train_perplexity=648.696, train_loss=6.474964

Batch 284430, train_perplexity=580.10065, train_loss=6.3632016

Batch 284440, train_perplexity=620.69836, train_loss=6.4308453

Batch 284450, train_perplexity=594.9619, train_loss=6.3884974

Batch 284460, train_perplexity=633.4683, train_loss=6.45121

Batch 284470, train_perplexity=626.39606, train_loss=6.439983

Batch 284480, train_perplexity=685.72687, train_loss=6.5304794

Batch 284490, train_perplexity=603.53406, train_loss=6.4028025

Batch 284500, train_perplexity=616.0337, train_loss=6.4233017

Batch 284510, train_perplexity=621.9291, train_loss=6.432826

Batch 284520, train_perplexity=576.03577, train_loss=6.3561697

Batch 284530, train_perplexity=586.47174, train_loss=6.3741245

Batch 284540, train_perplexity=634.60266, train_loss=6.452999

Batch 284550, train_perplexity=562.5926, train_loss=6.332556

Batch 284560, train_perplexity=582.00134, train_loss=6.3664727

Batch 284570, train_perplexity=613.1687, train_loss=6.41864

Batch 284580, train_perplexity=684.81525, train_loss=6.529149

Batch 284590, train_perplexity=669.93884, train_loss=6.5071864

Batch 284600, train_perplexity=576.0484, train_loss=6.3561916

Batch 284610, train_perplexity=634.1949, train_loss=6.4523563

Batch 284620, train_perplexity=599.61993, train_loss=6.396296

Batch 284630, train_perplexity=643.2805, train_loss=6.466581

Batch 284640, train_perplexity=558.50415, train_loss=6.325262

Batch 284650, train_perplexity=653.0995, train_loss=6.4817295

Batch 284660, train_perplexity=562.2531, train_loss=6.331952

Batch 284670, train_perplexity=558.44824, train_loss=6.325162

Batch 284680, train_perplexity=605.653, train_loss=6.406307

Batch 284690, train_perplexity=597.52606, train_loss=6.392798

Batch 284700, train_perplexity=657.4822, train_loss=6.4884176

Batch 284710, train_perplexity=601.53094, train_loss=6.399478

Batch 284720, train_perplexity=574.69165, train_loss=6.3538337

Batch 284730, train_perplexity=550.01056, train_loss=6.3099375

Batch 284740, train_perplexity=582.4858, train_loss=6.367305

Batch 284750, train_perplexity=604.7578, train_loss=6.404828

Batch 284760, train_perplexity=594.1975, train_loss=6.387212

Batch 284770, train_perplexity=659.26404, train_loss=6.491124

Batch 284780, train_perplexity=640.14136, train_loss=6.461689

Batch 284790, train_perplexity=641.37756, train_loss=6.4636183

Batch 284800, train_perplexity=550.7175, train_loss=6.311222

Batch 284810, train_perplexity=607.463, train_loss=6.4092913

Batch 284820, train_perplexity=590.03467, train_loss=6.3801813

Batch 284830, train_perplexity=630.9854, train_loss=6.447283

Batch 284840, train_perplexity=646.99445, train_loss=6.4723377

Batch 284850, train_perplexity=531.3813, train_loss=6.27548

Batch 284860, train_perplexity=678.5664, train_loss=6.5199823

Batch 284870, train_perplexity=620.66345, train_loss=6.430789

Batch 284880, train_perplexity=644.8459, train_loss=6.4690113

Batch 284890, train_perplexity=638.4623, train_loss=6.4590626

Batch 284900, train_perplexity=611.30963, train_loss=6.4156036

Batch 284910, train_perplexity=611.0812, train_loss=6.41523

Batch 284920, train_perplexity=662.45374, train_loss=6.4959507

Batch 284930, train_perplexity=574.97015, train_loss=6.354318

Batch 284940, train_perplexity=611.06104, train_loss=6.415197

Batch 284950, train_perplexity=607.5279, train_loss=6.409398

Batch 284960, train_perplexity=623.708, train_loss=6.4356823

Batch 284970, train_perplexity=576.537, train_loss=6.3570395

Batch 284980, train_perplexity=575.0132, train_loss=6.354393

Batch 284990, train_perplexity=637.4213, train_loss=6.457431

Batch 285000, train_perplexity=570.2191, train_loss=6.3460207

Batch 285010, train_perplexity=674.87036, train_loss=6.5145206

Batch 285020, train_perplexity=586.2595, train_loss=6.3737626

Batch 285030, train_perplexity=645.80005, train_loss=6.47049

Batch 285040, train_perplexity=625.2454, train_loss=6.438144

Batch 285050, train_perplexity=618.9655, train_loss=6.4280496

Batch 285060, train_perplexity=559.67773, train_loss=6.327361

Batch 285070, train_perplexity=593.6107, train_loss=6.386224

Batch 285080, train_perplexity=546.1271, train_loss=6.3028517

Batch 285090, train_perplexity=654.76776, train_loss=6.4842806

Batch 285100, train_perplexity=714.3711, train_loss=6.5714025

Batch 285110, train_perplexity=631.1726, train_loss=6.4475794

Batch 285120, train_perplexity=639.76294, train_loss=6.4610977

Batch 285130, train_perplexity=687.11273, train_loss=6.5324984

Batch 285140, train_perplexity=615.482, train_loss=6.4224057

Batch 285150, train_perplexity=557.3384, train_loss=6.3231726

Batch 285160, train_perplexity=566.32526, train_loss=6.3391685

Batch 285170, train_perplexity=636.52167, train_loss=6.4560184

Batch 285180, train_perplexity=587.2444, train_loss=6.375441

Batch 285190, train_perplexity=579.89325, train_loss=6.362844

Batch 285200, train_perplexity=563.81995, train_loss=6.334735

Batch 285210, train_perplexity=601.8985, train_loss=6.400089

Batch 285220, train_perplexity=584.6702, train_loss=6.371048

Batch 285230, train_perplexity=593.8862, train_loss=6.3866878

Batch 285240, train_perplexity=597.3036, train_loss=6.3924255

Batch 285250, train_perplexity=651.5374, train_loss=6.479335

Batch 285260, train_perplexity=621.42957, train_loss=6.4320226

Batch 285270, train_perplexity=592.7158, train_loss=6.384715

Batch 285280, train_perplexity=625.6602, train_loss=6.4388075
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 285290, train_perplexity=602.50665, train_loss=6.4010987

Batch 285300, train_perplexity=555.9979, train_loss=6.3207645

Batch 285310, train_perplexity=594.4781, train_loss=6.387684

Batch 285320, train_perplexity=643.28296, train_loss=6.4665847

Batch 285330, train_perplexity=602.23694, train_loss=6.400651

Batch 285340, train_perplexity=737.5852, train_loss=6.6033816

Batch 285350, train_perplexity=622.56104, train_loss=6.4338417

Batch 285360, train_perplexity=647.4234, train_loss=6.4730005

Batch 285370, train_perplexity=609.67944, train_loss=6.4129333

Batch 285380, train_perplexity=605.03705, train_loss=6.4052896

Batch 285390, train_perplexity=495.04468, train_loss=6.204648

Batch 285400, train_perplexity=536.30206, train_loss=6.2846975

Batch 285410, train_perplexity=525.96466, train_loss=6.265234

Batch 285420, train_perplexity=610.3112, train_loss=6.413969

Batch 285430, train_perplexity=554.1779, train_loss=6.317486

Batch 285440, train_perplexity=626.6315, train_loss=6.4403586

Batch 285450, train_perplexity=631.40137, train_loss=6.447942

Batch 285460, train_perplexity=584.6669, train_loss=6.3710423

Batch 285470, train_perplexity=570.89545, train_loss=6.347206

Batch 285480, train_perplexity=656.0866, train_loss=6.486293

Batch 285490, train_perplexity=629.3727, train_loss=6.4447236

Batch 285500, train_perplexity=608.22296, train_loss=6.4105415

Batch 285510, train_perplexity=565.82806, train_loss=6.33829

Batch 285520, train_perplexity=590.12415, train_loss=6.380333

Batch 285530, train_perplexity=576.00275, train_loss=6.3561125

Batch 285540, train_perplexity=610.5284, train_loss=6.4143248

Batch 285550, train_perplexity=635.50934, train_loss=6.454427

Batch 285560, train_perplexity=608.0878, train_loss=6.4103193

Batch 285570, train_perplexity=606.83246, train_loss=6.4082527

Batch 285580, train_perplexity=600.54675, train_loss=6.3978405

Batch 285590, train_perplexity=595.4123, train_loss=6.389254

Batch 285600, train_perplexity=633.60126, train_loss=6.45142

Batch 285610, train_perplexity=566.2326, train_loss=6.339005

Batch 285620, train_perplexity=620.118, train_loss=6.4299097

Batch 285630, train_perplexity=583.32355, train_loss=6.368742

Batch 285640, train_perplexity=551.5007, train_loss=6.312643

Batch 285650, train_perplexity=685.81683, train_loss=6.5306106

Batch 285660, train_perplexity=607.5375, train_loss=6.409414

Batch 285670, train_perplexity=611.771, train_loss=6.416358

Batch 285680, train_perplexity=608.4077, train_loss=6.4108453

Batch 285690, train_perplexity=581.4149, train_loss=6.3654647

Batch 285700, train_perplexity=519.02014, train_loss=6.2519426

Batch 285710, train_perplexity=593.6787, train_loss=6.386338

Batch 285720, train_perplexity=607.355, train_loss=6.4091134

Batch 285730, train_perplexity=594.19867, train_loss=6.3872137

Batch 285740, train_perplexity=589.705, train_loss=6.3796225

Batch 285750, train_perplexity=567.1565, train_loss=6.3406353

Batch 285760, train_perplexity=680.98, train_loss=6.523533

Batch 285770, train_perplexity=616.8385, train_loss=6.4246073

Batch 285780, train_perplexity=566.22644, train_loss=6.338994

Batch 285790, train_perplexity=589.18054, train_loss=6.3787327

Batch 285800, train_perplexity=621.1976, train_loss=6.431649

Batch 285810, train_perplexity=540.69086, train_loss=6.2928476

Batch 285820, train_perplexity=611.1301, train_loss=6.41531

Batch 285830, train_perplexity=611.14, train_loss=6.415326

Batch 285840, train_perplexity=592.4779, train_loss=6.3843136

Batch 285850, train_perplexity=570.03754, train_loss=6.345702

Batch 285860, train_perplexity=662.112, train_loss=6.4954348

Batch 285870, train_perplexity=561.77203, train_loss=6.331096

Batch 285880, train_perplexity=663.68555, train_loss=6.4978085

Batch 285890, train_perplexity=554.1198, train_loss=6.317381

Batch 285900, train_perplexity=635.3687, train_loss=6.4542055

Batch 285910, train_perplexity=633.29254, train_loss=6.4509325

Batch 285920, train_perplexity=551.9979, train_loss=6.3135443

Batch 285930, train_perplexity=596.5476, train_loss=6.391159

Batch 285940, train_perplexity=608.60126, train_loss=6.4111633

Batch 285950, train_perplexity=634.66925, train_loss=6.453104

Batch 285960, train_perplexity=611.7138, train_loss=6.4162645

Batch 285970, train_perplexity=602.13074, train_loss=6.4004745

Batch 285980, train_perplexity=662.72986, train_loss=6.4963675

Batch 285990, train_perplexity=665.2894, train_loss=6.500222

Batch 286000, train_perplexity=620.2812, train_loss=6.430173

Batch 286010, train_perplexity=608.23254, train_loss=6.4105573

Batch 286020, train_perplexity=655.3156, train_loss=6.485117

Batch 286030, train_perplexity=566.22046, train_loss=6.3389835

Batch 286040, train_perplexity=613.91125, train_loss=6.4198503

Batch 286050, train_perplexity=608.70197, train_loss=6.411329

Batch 286060, train_perplexity=635.0543, train_loss=6.4537106

Batch 286070, train_perplexity=592.6192, train_loss=6.384552

Batch 286080, train_perplexity=571.0316, train_loss=6.3474445

Batch 286090, train_perplexity=626.3632, train_loss=6.4399304

Batch 286100, train_perplexity=576.416, train_loss=6.3568296

Batch 286110, train_perplexity=651.2128, train_loss=6.4788365

Batch 286120, train_perplexity=602.711, train_loss=6.4014378

Batch 286130, train_perplexity=569.64734, train_loss=6.3450174

Batch 286140, train_perplexity=568.25555, train_loss=6.3425713

Batch 286150, train_perplexity=530.546, train_loss=6.2739067

Batch 286160, train_perplexity=562.956, train_loss=6.3332014

Batch 286170, train_perplexity=579.2789, train_loss=6.361784

Batch 286180, train_perplexity=629.12244, train_loss=6.444326

Batch 286190, train_perplexity=598.20404, train_loss=6.393932

Batch 286200, train_perplexity=583.0755, train_loss=6.3683167

Batch 286210, train_perplexity=634.7401, train_loss=6.4532156

Batch 286220, train_perplexity=665.5604, train_loss=6.5006294

Batch 286230, train_perplexity=621.48083, train_loss=6.432105

Batch 286240, train_perplexity=581.4363, train_loss=6.3655014

Batch 286250, train_perplexity=660.7407, train_loss=6.4933615

Batch 286260, train_perplexity=628.8888, train_loss=6.4439545

Batch 286270, train_perplexity=540.80066, train_loss=6.293051

Batch 286280, train_perplexity=533.7768, train_loss=6.279978

Batch 286290, train_perplexity=574.1337, train_loss=6.3528624

Batch 286300, train_perplexity=593.959, train_loss=6.3868103

Batch 286310, train_perplexity=585.0601, train_loss=6.3717146

Batch 286320, train_perplexity=599.0855, train_loss=6.3954043

Batch 286330, train_perplexity=611.8194, train_loss=6.416437

Batch 286340, train_perplexity=544.8125, train_loss=6.3004417

Batch 286350, train_perplexity=629.51166, train_loss=6.4449444

Batch 286360, train_perplexity=541.7838, train_loss=6.294867

Batch 286370, train_perplexity=517.5193, train_loss=6.249047

Batch 286380, train_perplexity=670.50867, train_loss=6.5080366

Batch 286390, train_perplexity=589.99585, train_loss=6.3801155

Batch 286400, train_perplexity=632.4328, train_loss=6.449574

Batch 286410, train_perplexity=553.1462, train_loss=6.3156223

Batch 286420, train_perplexity=561.21246, train_loss=6.3300996

Batch 286430, train_perplexity=722.8946, train_loss=6.5832634

Batch 286440, train_perplexity=601.5576, train_loss=6.3995223

Batch 286450, train_perplexity=578.7452, train_loss=6.3608623

Batch 286460, train_perplexity=703.1781, train_loss=6.55561

Batch 286470, train_perplexity=564.4634, train_loss=6.3358755

Batch 286480, train_perplexity=596.06995, train_loss=6.390358

Batch 286490, train_perplexity=585.40784, train_loss=6.3723087

Batch 286500, train_perplexity=569.8299, train_loss=6.345338

Batch 286510, train_perplexity=561.4547, train_loss=6.330531

Batch 286520, train_perplexity=686.8998, train_loss=6.5321884

Batch 286530, train_perplexity=583.7059, train_loss=6.369397

Batch 286540, train_perplexity=540.279, train_loss=6.2920856

Batch 286550, train_perplexity=602.5035, train_loss=6.4010935

Batch 286560, train_perplexity=608.66223, train_loss=6.4112635

Batch 286570, train_perplexity=583.355, train_loss=6.368796

Batch 286580, train_perplexity=585.0559, train_loss=6.3717074

Batch 286590, train_perplexity=584.7371, train_loss=6.3711624
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 286600, train_perplexity=609.82336, train_loss=6.4131694

Batch 286610, train_perplexity=597.68024, train_loss=6.393056

Batch 286620, train_perplexity=622.4673, train_loss=6.433691

Batch 286630, train_perplexity=586.02057, train_loss=6.373355

Batch 286640, train_perplexity=723.12555, train_loss=6.583583

Batch 286650, train_perplexity=560.45966, train_loss=6.3287573

Batch 286660, train_perplexity=567.5978, train_loss=6.341413

Batch 286670, train_perplexity=629.95215, train_loss=6.445644

Batch 286680, train_perplexity=539.7864, train_loss=6.2911735

Batch 286690, train_perplexity=608.8451, train_loss=6.411564

Batch 286700, train_perplexity=595.49805, train_loss=6.389398

Batch 286710, train_perplexity=580.4053, train_loss=6.3637266

Batch 286720, train_perplexity=607.8985, train_loss=6.410008

Batch 286730, train_perplexity=595.0652, train_loss=6.388671

Batch 286740, train_perplexity=589.84564, train_loss=6.379861

Batch 286750, train_perplexity=624.78046, train_loss=6.4374003

Batch 286760, train_perplexity=660.4698, train_loss=6.4929514

Batch 286770, train_perplexity=624.1882, train_loss=6.436452

Batch 286780, train_perplexity=596.72626, train_loss=6.3914585

Batch 286790, train_perplexity=611.7984, train_loss=6.416403

Batch 286800, train_perplexity=572.86707, train_loss=6.3506536

Batch 286810, train_perplexity=537.6945, train_loss=6.2872906

Batch 286820, train_perplexity=575.6535, train_loss=6.355506

Batch 286830, train_perplexity=561.4445, train_loss=6.330513

Batch 286840, train_perplexity=593.79193, train_loss=6.386529

Batch 286850, train_perplexity=587.50793, train_loss=6.37589

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Batch 286860, train_perplexity=588.71716, train_loss=6.377946

Batch 286870, train_perplexity=535.36383, train_loss=6.2829466

Batch 286880, train_perplexity=640.735, train_loss=6.462616

Batch 286890, train_perplexity=656.2919, train_loss=6.4866056

Batch 286900, train_perplexity=582.10236, train_loss=6.3666463

Batch 286910, train_perplexity=637.30945, train_loss=6.4572554

Batch 286920, train_perplexity=583.967, train_loss=6.3698444

Batch 286930, train_perplexity=633.63116, train_loss=6.451467

Batch 286940, train_perplexity=637.3389, train_loss=6.4573016

Batch 286950, train_perplexity=583.64264, train_loss=6.369289

Batch 286960, train_perplexity=575.09906, train_loss=6.3545423

Batch 286970, train_perplexity=582.2931, train_loss=6.366974

Batch 286980, train_perplexity=671.7859, train_loss=6.5099397

Batch 286990, train_perplexity=604.9138, train_loss=6.405086

Batch 287000, train_perplexity=628.4035, train_loss=6.4431825

Batch 287010, train_perplexity=531.2179, train_loss=6.275172

Batch 287020, train_perplexity=617.80524, train_loss=6.426173

Batch 287030, train_perplexity=565.52783, train_loss=6.3377595

Batch 287040, train_perplexity=604.05804, train_loss=6.4036703

Batch 287050, train_perplexity=572.535, train_loss=6.350074

Batch 287060, train_perplexity=531.8586, train_loss=6.2763777

Batch 287070, train_perplexity=583.8993, train_loss=6.3697286

Batch 287080, train_perplexity=527.7576, train_loss=6.268637

Batch 287090, train_perplexity=561.56744, train_loss=6.330732

Batch 287100, train_perplexity=641.0284, train_loss=6.4630737

Batch 287110, train_perplexity=566.0998, train_loss=6.3387704

Batch 287120, train_perplexity=650.88995, train_loss=6.4783406

Batch 287130, train_perplexity=583.19727, train_loss=6.3685255

Batch 287140, train_perplexity=569.0994, train_loss=6.344055

Batch 287150, train_perplexity=567.91455, train_loss=6.341971

Batch 287160, train_perplexity=599.14294, train_loss=6.3955

Batch 287170, train_perplexity=635.1664, train_loss=6.453887

Batch 287180, train_perplexity=622.96075, train_loss=6.4344835

Batch 287190, train_perplexity=504.3935, train_loss=6.2233567

Batch 287200, train_perplexity=593.91766, train_loss=6.3867407

Batch 287210, train_perplexity=662.6082, train_loss=6.496184

Batch 287220, train_perplexity=551.0404, train_loss=6.311808

Batch 287230, train_perplexity=599.9022, train_loss=6.3967667

Batch 287240, train_perplexity=599.5722, train_loss=6.3962164

Batch 287250, train_perplexity=592.19714, train_loss=6.3838396

Batch 287260, train_perplexity=643.2477, train_loss=6.46653

Batch 287270, train_perplexity=625.0594, train_loss=6.4378467

Batch 287280, train_perplexity=602.40125, train_loss=6.4009237

Batch 287290, train_perplexity=570.22675, train_loss=6.346034

Batch 287300, train_perplexity=542.73456, train_loss=6.2966204

Batch 287310, train_perplexity=549.66235, train_loss=6.309304

Batch 287320, train_perplexity=572.60376, train_loss=6.350194

Batch 287330, train_perplexity=586.26544, train_loss=6.3737726

Batch 287340, train_perplexity=595.37994, train_loss=6.3891997

Batch 287350, train_perplexity=559.9318, train_loss=6.327815

Batch 287360, train_perplexity=635.5257, train_loss=6.4544525

Batch 287370, train_perplexity=608.51013, train_loss=6.4110136

Batch 287380, train_perplexity=630.07263, train_loss=6.445835

Batch 287390, train_perplexity=642.6174, train_loss=6.4655495

Batch 287400, train_perplexity=596.9181, train_loss=6.39178

Batch 287410, train_perplexity=651.57904, train_loss=6.4793987

Batch 287420, train_perplexity=568.05615, train_loss=6.3422203

Batch 287430, train_perplexity=579.78125, train_loss=6.362651

Batch 287440, train_perplexity=621.3756, train_loss=6.431936

Batch 287450, train_perplexity=588.5224, train_loss=6.377615

Batch 287460, train_perplexity=566.072, train_loss=6.3387213

Batch 287470, train_perplexity=600.6321, train_loss=6.3979826

Batch 287480, train_perplexity=640.86273, train_loss=6.4628153

Batch 287490, train_perplexity=625.63074, train_loss=6.4387603

Batch 287500, train_perplexity=638.9916, train_loss=6.4598913

Batch 287510, train_perplexity=622.76294, train_loss=6.434166

Batch 287520, train_perplexity=667.4915, train_loss=6.5035267

Batch 287530, train_perplexity=663.36786, train_loss=6.4973297

Batch 287540, train_perplexity=554.7556, train_loss=6.3185277

Batch 287550, train_perplexity=584.9379, train_loss=6.3715057

Batch 287560, train_perplexity=556.4578, train_loss=6.3215914

Batch 287570, train_perplexity=593.68097, train_loss=6.386342

Batch 287580, train_perplexity=566.61536, train_loss=6.3396807

Batch 287590, train_perplexity=629.9903, train_loss=6.4457045

Batch 287600, train_perplexity=570.32654, train_loss=6.346209

Batch 287610, train_perplexity=618.11993, train_loss=6.4266825

Batch 287620, train_perplexity=599.2395, train_loss=6.3956614

Batch 287630, train_perplexity=602.3248, train_loss=6.400797

Batch 287640, train_perplexity=604.58826, train_loss=6.4045477

Batch 287650, train_perplexity=592.7102, train_loss=6.3847055

Batch 287660, train_perplexity=564.05066, train_loss=6.335144

Batch 287670, train_perplexity=560.5468, train_loss=6.3289127

Batch 287680, train_perplexity=623.3096, train_loss=6.4350433

Batch 287690, train_perplexity=557.416, train_loss=6.323312

Batch 287700, train_perplexity=597.61017, train_loss=6.3929386

Batch 287710, train_perplexity=544.08563, train_loss=6.2991066

Batch 287720, train_perplexity=568.8577, train_loss=6.3436303

Batch 287730, train_perplexity=586.4846, train_loss=6.3741465

Batch 287740, train_perplexity=578.25275, train_loss=6.360011

Batch 287750, train_perplexity=583.94305, train_loss=6.3698034

Batch 287760, train_perplexity=507.06714, train_loss=6.2286434

Batch 287770, train_perplexity=667.2102, train_loss=6.503105

Batch 287780, train_perplexity=617.29755, train_loss=6.425351

Batch 287790, train_perplexity=582.32526, train_loss=6.367029

Batch 287800, train_perplexity=613.5255, train_loss=6.419222

Batch 287810, train_perplexity=519.28845, train_loss=6.2524595

Batch 287820, train_perplexity=621.7749, train_loss=6.432578

Batch 287830, train_perplexity=627.2865, train_loss=6.4414034
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 287840, train_perplexity=613.0734, train_loss=6.4184847

Batch 287850, train_perplexity=693.81964, train_loss=6.542212

Batch 287860, train_perplexity=650.5713, train_loss=6.477851

Batch 287870, train_perplexity=657.4859, train_loss=6.4884233

Batch 287880, train_perplexity=561.55273, train_loss=6.3307056

Batch 287890, train_perplexity=534.1264, train_loss=6.2806325

Batch 287900, train_perplexity=551.54144, train_loss=6.312717

Batch 287910, train_perplexity=666.5847, train_loss=6.502167

Batch 287920, train_perplexity=611.60645, train_loss=6.416089

Batch 287930, train_perplexity=635.76483, train_loss=6.4548287

Batch 287940, train_perplexity=659.05475, train_loss=6.4908066

Batch 287950, train_perplexity=636.87537, train_loss=6.456574

Batch 287960, train_perplexity=634.0576, train_loss=6.45214

Batch 287970, train_perplexity=606.0742, train_loss=6.4070024

Batch 287980, train_perplexity=584.42773, train_loss=6.370633

Batch 287990, train_perplexity=591.49304, train_loss=6.38265

Batch 288000, train_perplexity=620.76056, train_loss=6.4309454

Batch 288010, train_perplexity=594.1176, train_loss=6.3870773

Batch 288020, train_perplexity=614.92316, train_loss=6.4214973

Batch 288030, train_perplexity=539.0325, train_loss=6.289776

Batch 288040, train_perplexity=635.2397, train_loss=6.4540024

Batch 288050, train_perplexity=571.00385, train_loss=6.347396

Batch 288060, train_perplexity=630.8146, train_loss=6.447012

Batch 288070, train_perplexity=567.6354, train_loss=6.3414793

Batch 288080, train_perplexity=711.2726, train_loss=6.5670557

Batch 288090, train_perplexity=648.4343, train_loss=6.4745607

Batch 288100, train_perplexity=563.89276, train_loss=6.334864

Batch 288110, train_perplexity=572.90967, train_loss=6.350728

Batch 288120, train_perplexity=602.5541, train_loss=6.4011774

Batch 288130, train_perplexity=583.928, train_loss=6.3697777

Batch 288140, train_perplexity=562.7107, train_loss=6.3327656

Batch 288150, train_perplexity=605.9251, train_loss=6.4067564

Batch 288160, train_perplexity=608.54785, train_loss=6.4110756

Batch 288170, train_perplexity=601.0604, train_loss=6.3986955

Batch 288180, train_perplexity=587.46704, train_loss=6.37582

Batch 288190, train_perplexity=594.9463, train_loss=6.388471

Batch 288200, train_perplexity=596.63525, train_loss=6.391306

Batch 288210, train_perplexity=627.1746, train_loss=6.441225

Batch 288220, train_perplexity=606.1395, train_loss=6.40711

Batch 288230, train_perplexity=627.92334, train_loss=6.442418

Batch 288240, train_perplexity=656.7696, train_loss=6.4873333

Batch 288250, train_perplexity=689.3837, train_loss=6.535798

Batch 288260, train_perplexity=649.6401, train_loss=6.4764185

Batch 288270, train_perplexity=648.5395, train_loss=6.474723

Batch 288280, train_perplexity=633.5526, train_loss=6.451343

Batch 288290, train_perplexity=585.5418, train_loss=6.3725376

Batch 288300, train_perplexity=598.7816, train_loss=6.394897

Batch 288310, train_perplexity=587.4284, train_loss=6.3757544

Batch 288320, train_perplexity=693.5811, train_loss=6.541868

Batch 288330, train_perplexity=605.9598, train_loss=6.4068136

Batch 288340, train_perplexity=582.8881, train_loss=6.3679953

Batch 288350, train_perplexity=610.1349, train_loss=6.41368

Batch 288360, train_perplexity=528.6034, train_loss=6.2702384

Batch 288370, train_perplexity=571.29034, train_loss=6.3478975

Batch 288380, train_perplexity=605.5173, train_loss=6.406083

Batch 288390, train_perplexity=653.65094, train_loss=6.4825735

Batch 288400, train_perplexity=617.30225, train_loss=6.425359

Batch 288410, train_perplexity=634.51526, train_loss=6.4528613

Batch 288420, train_perplexity=673.56445, train_loss=6.5125837

Batch 288430, train_perplexity=597.1584, train_loss=6.3921824

Batch 288440, train_perplexity=561.1908, train_loss=6.330061

Batch 288450, train_perplexity=619.09686, train_loss=6.4282618

Batch 288460, train_perplexity=654.58795, train_loss=6.484006

Batch 288470, train_perplexity=580.0708, train_loss=6.36315

Batch 288480, train_perplexity=631.5923, train_loss=6.448244

Batch 288490, train_perplexity=620.4454, train_loss=6.4304376

Batch 288500, train_perplexity=597.7116, train_loss=6.3931084

Batch 288510, train_perplexity=561.9577, train_loss=6.3314266

Batch 288520, train_perplexity=590.68774, train_loss=6.3812876

Batch 288530, train_perplexity=657.7556, train_loss=6.4888334

Batch 288540, train_perplexity=575.92065, train_loss=6.35597

Batch 288550, train_perplexity=576.8623, train_loss=6.3576035

Batch 288560, train_perplexity=633.52814, train_loss=6.4513044

Batch 288570, train_perplexity=584.24774, train_loss=6.370325

Batch 288580, train_perplexity=603.1365, train_loss=6.4021435

Batch 288590, train_perplexity=533.6786, train_loss=6.2797937

Batch 288600, train_perplexity=675.93475, train_loss=6.5160966

Batch 288610, train_perplexity=618.43274, train_loss=6.4271884

Batch 288620, train_perplexity=544.03503, train_loss=6.2990136

Batch 288630, train_perplexity=636.91754, train_loss=6.4566402

Batch 288640, train_perplexity=602.4127, train_loss=6.400943

Batch 288650, train_perplexity=620.9583, train_loss=6.431264

Batch 288660, train_perplexity=643.5201, train_loss=6.4669533

Batch 288670, train_perplexity=578.2522, train_loss=6.36001

Batch 288680, train_perplexity=606.9528, train_loss=6.408451

Batch 288690, train_perplexity=526.36005, train_loss=6.2659855

Batch 288700, train_perplexity=600.035, train_loss=6.396988

Batch 288710, train_perplexity=609.8446, train_loss=6.413204

Batch 288720, train_perplexity=610.0325, train_loss=6.413512

Batch 288730, train_perplexity=580.76904, train_loss=6.364353

Batch 288740, train_perplexity=624.6005, train_loss=6.4371123

Batch 288750, train_perplexity=648.44794, train_loss=6.4745817

Batch 288760, train_perplexity=590.3189, train_loss=6.380663

Batch 288770, train_perplexity=557.61725, train_loss=6.323673

Batch 288780, train_perplexity=629.46246, train_loss=6.444866

Batch 288790, train_perplexity=637.7028, train_loss=6.4578724

Batch 288800, train_perplexity=585.951, train_loss=6.373236

Batch 288810, train_perplexity=538.8778, train_loss=6.289489

Batch 288820, train_perplexity=643.7279, train_loss=6.467276

Batch 288830, train_perplexity=587.37714, train_loss=6.375667

Batch 288840, train_perplexity=622.0738, train_loss=6.4330587

Batch 288850, train_perplexity=689.4015, train_loss=6.535824

Batch 288860, train_perplexity=584.7394, train_loss=6.371166

Batch 288870, train_perplexity=609.28424, train_loss=6.412285

Batch 288880, train_perplexity=588.13495, train_loss=6.3769565

Batch 288890, train_perplexity=638.3186, train_loss=6.4588375

Batch 288900, train_perplexity=621.68744, train_loss=6.4324374

Batch 288910, train_perplexity=555.7694, train_loss=6.3203535

Batch 288920, train_perplexity=530.54047, train_loss=6.273896

Batch 288930, train_perplexity=629.391, train_loss=6.4447527

Batch 288940, train_perplexity=588.4354, train_loss=6.377467

Batch 288950, train_perplexity=532.459, train_loss=6.277506

Batch 288960, train_perplexity=595.6625, train_loss=6.389674

Batch 288970, train_perplexity=600.2573, train_loss=6.3973584

Batch 288980, train_perplexity=581.50616, train_loss=6.3656216

Batch 288990, train_perplexity=584.8052, train_loss=6.371279

Batch 289000, train_perplexity=600.62836, train_loss=6.3979764

Batch 289010, train_perplexity=547.85846, train_loss=6.306017

Batch 289020, train_perplexity=614.4237, train_loss=6.420685

Batch 289030, train_perplexity=598.33154, train_loss=6.394145

Batch 289040, train_perplexity=571.7285, train_loss=6.3486643

Batch 289050, train_perplexity=695.703, train_loss=6.544923

Batch 289060, train_perplexity=602.95416, train_loss=6.401841

Batch 289070, train_perplexity=605.3747, train_loss=6.4058475

Batch 289080, train_perplexity=601.2998, train_loss=6.3990936

Batch 289090, train_perplexity=641.08215, train_loss=6.4631577

Batch 289100, train_perplexity=592.0094, train_loss=6.3835225

Batch 289110, train_perplexity=604.5482, train_loss=6.4044814

Batch 289120, train_perplexity=642.2393, train_loss=6.464961

Batch 289130, train_perplexity=636.27466, train_loss=6.4556303

Batch 289140, train_perplexity=618.625, train_loss=6.4274993
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 289150, train_perplexity=628.962, train_loss=6.444071

Batch 289160, train_perplexity=598.5949, train_loss=6.394585

Batch 289170, train_perplexity=622.3064, train_loss=6.4334326

Batch 289180, train_perplexity=643.34094, train_loss=6.466675

Batch 289190, train_perplexity=652.4471, train_loss=6.48073

Batch 289200, train_perplexity=567.9817, train_loss=6.342089

Batch 289210, train_perplexity=554.9919, train_loss=6.3189535

Batch 289220, train_perplexity=559.54645, train_loss=6.3271265

Batch 289230, train_perplexity=606.697, train_loss=6.4080296

Batch 289240, train_perplexity=543.22235, train_loss=6.2975187

Batch 289250, train_perplexity=542.1133, train_loss=6.295475

Batch 289260, train_perplexity=628.8855, train_loss=6.443949

Batch 289270, train_perplexity=610.2388, train_loss=6.4138503

Batch 289280, train_perplexity=651.02594, train_loss=6.4785495

Batch 289290, train_perplexity=575.8523, train_loss=6.355851

Batch 289300, train_perplexity=625.28265, train_loss=6.438204

Batch 289310, train_perplexity=668.95917, train_loss=6.505723

Batch 289320, train_perplexity=600.7432, train_loss=6.3981676

Batch 289330, train_perplexity=577.4702, train_loss=6.358657

Batch 289340, train_perplexity=551.19727, train_loss=6.312093

Batch 289350, train_perplexity=573.5952, train_loss=6.351924

Batch 289360, train_perplexity=589.40955, train_loss=6.3791213

Batch 289370, train_perplexity=574.54755, train_loss=6.353583

Batch 289380, train_perplexity=562.9726, train_loss=6.333231

Batch 289390, train_perplexity=585.9166, train_loss=6.3731775

Batch 289400, train_perplexity=628.78687, train_loss=6.4437923

Batch 289410, train_perplexity=622.77484, train_loss=6.434185

Batch 289420, train_perplexity=572.99, train_loss=6.350868

Batch 289430, train_perplexity=591.8883, train_loss=6.383318

Batch 289440, train_perplexity=597.64233, train_loss=6.3929925

Batch 289450, train_perplexity=582.2431, train_loss=6.366888

Batch 289460, train_perplexity=636.8635, train_loss=6.4565554

Batch 289470, train_perplexity=627.29816, train_loss=6.441422

Batch 289480, train_perplexity=581.2805, train_loss=6.3652334

Batch 289490, train_perplexity=611.0019, train_loss=6.4151

Batch 289500, train_perplexity=590.6385, train_loss=6.381204

Batch 289510, train_perplexity=563.49634, train_loss=6.334161

Batch 289520, train_perplexity=640.86334, train_loss=6.462816

Batch 289530, train_perplexity=575.5023, train_loss=6.355243

Batch 289540, train_perplexity=519.0701, train_loss=6.252039

Batch 289550, train_perplexity=572.7354, train_loss=6.350424

Batch 289560, train_perplexity=631.0438, train_loss=6.4473753

Batch 289570, train_perplexity=600.28394, train_loss=6.397403

Batch 289580, train_perplexity=636.07416, train_loss=6.455315

Batch 289590, train_perplexity=566.40735, train_loss=6.3393135

Batch 289600, train_perplexity=621.0205, train_loss=6.431364

Batch 289610, train_perplexity=553.3704, train_loss=6.3160276

Batch 289620, train_perplexity=621.4008, train_loss=6.4319763

Batch 289630, train_perplexity=655.0232, train_loss=6.4846706

Batch 289640, train_perplexity=590.1416, train_loss=6.3803625

Batch 289650, train_perplexity=566.275, train_loss=6.33908

Batch 289660, train_perplexity=599.46985, train_loss=6.3960457

Batch 289670, train_perplexity=637.66754, train_loss=6.457817

Batch 289680, train_perplexity=615.7277, train_loss=6.422805

Batch 289690, train_perplexity=593.394, train_loss=6.3858585

Batch 289700, train_perplexity=541.4731, train_loss=6.2942934

Batch 289710, train_perplexity=529.7447, train_loss=6.272395

Batch 289720, train_perplexity=561.3316, train_loss=6.330312

Batch 289730, train_perplexity=553.713, train_loss=6.3166466

Batch 289740, train_perplexity=622.03107, train_loss=6.43299

Batch 289750, train_perplexity=630.1273, train_loss=6.445922

Batch 289760, train_perplexity=595.06744, train_loss=6.3886747

Batch 289770, train_perplexity=534.4538, train_loss=6.281245

Batch 289780, train_perplexity=642.90607, train_loss=6.4659986

Batch 289790, train_perplexity=629.0934, train_loss=6.4442797

Batch 289800, train_perplexity=610.88885, train_loss=6.414915

Batch 289810, train_perplexity=628.6831, train_loss=6.4436274

Batch 289820, train_perplexity=577.76575, train_loss=6.3591685

Batch 289830, train_perplexity=608.64276, train_loss=6.4112315

Batch 289840, train_perplexity=623.8308, train_loss=6.435879

Batch 289850, train_perplexity=585.1198, train_loss=6.3718166

Batch 289860, train_perplexity=581.4485, train_loss=6.3655224

Batch 289870, train_perplexity=632.0207, train_loss=6.448922

Batch 289880, train_perplexity=594.09045, train_loss=6.3870316

Batch 289890, train_perplexity=612.80133, train_loss=6.4180408

Batch 289900, train_perplexity=544.5642, train_loss=6.299986

Batch 289910, train_perplexity=604.379, train_loss=6.4042015

Batch 289920, train_perplexity=558.4781, train_loss=6.3252153

Batch 289930, train_perplexity=596.0182, train_loss=6.390271

Batch 289940, train_perplexity=662.3091, train_loss=6.4957323

Batch 289950, train_perplexity=543.37885, train_loss=6.2978067

Batch 289960, train_perplexity=562.14105, train_loss=6.331753

Batch 289970, train_perplexity=617.46594, train_loss=6.425624

Batch 289980, train_perplexity=577.30176, train_loss=6.358365

Batch 289990, train_perplexity=588.1574, train_loss=6.3769946

Batch 290000, train_perplexity=587.578, train_loss=6.376009

Batch 290010, train_perplexity=632.2486, train_loss=6.4492826

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00017-of-00100
Loaded 306553 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00017-of-00100
Loaded 306553 sentences.
Finished loading
Batch 290020, train_perplexity=623.4939, train_loss=6.435339

Batch 290030, train_perplexity=565.2499, train_loss=6.337268

Batch 290040, train_perplexity=585.1575, train_loss=6.371881

Batch 290050, train_perplexity=553.16754, train_loss=6.315661

Batch 290060, train_perplexity=595.0646, train_loss=6.38867

Batch 290070, train_perplexity=649.7405, train_loss=6.476573

Batch 290080, train_perplexity=591.151, train_loss=6.3820715

Batch 290090, train_perplexity=630.37555, train_loss=6.446316

Batch 290100, train_perplexity=629.18274, train_loss=6.444422

Batch 290110, train_perplexity=651.69714, train_loss=6.47958

Batch 290120, train_perplexity=585.656, train_loss=6.3727326

Batch 290130, train_perplexity=614.0213, train_loss=6.4200296

Batch 290140, train_perplexity=615.1261, train_loss=6.4218273

Batch 290150, train_perplexity=600.4763, train_loss=6.397723

Batch 290160, train_perplexity=597.5742, train_loss=6.3928785

Batch 290170, train_perplexity=622.5005, train_loss=6.4337444

Batch 290180, train_perplexity=600.9942, train_loss=6.3985853

Batch 290190, train_perplexity=586.76294, train_loss=6.374621

Batch 290200, train_perplexity=594.7676, train_loss=6.3881707

Batch 290210, train_perplexity=549.11224, train_loss=6.308303

Batch 290220, train_perplexity=689.30225, train_loss=6.53568

Batch 290230, train_perplexity=644.316, train_loss=6.4681892

Batch 290240, train_perplexity=623.70917, train_loss=6.435684

Batch 290250, train_perplexity=569.6082, train_loss=6.344949

Batch 290260, train_perplexity=588.1894, train_loss=6.377049

Batch 290270, train_perplexity=579.9477, train_loss=6.362938

Batch 290280, train_perplexity=630.1412, train_loss=6.445944

Batch 290290, train_perplexity=562.84674, train_loss=6.3330073

Batch 290300, train_perplexity=586.76465, train_loss=6.374624

Batch 290310, train_perplexity=576.72284, train_loss=6.357362

Batch 290320, train_perplexity=572.59424, train_loss=6.3501773

Batch 290330, train_perplexity=564.39935, train_loss=6.335762

Batch 290340, train_perplexity=587.12427, train_loss=6.3752365

Batch 290350, train_perplexity=580.5946, train_loss=6.364053

Batch 290360, train_perplexity=640.35535, train_loss=6.4620233

Batch 290370, train_perplexity=594.43445, train_loss=6.3876104

Batch 290380, train_perplexity=603.8326, train_loss=6.403297

Batch 290390, train_perplexity=592.41516, train_loss=6.3842077
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 290400, train_perplexity=541.6252, train_loss=6.2945743

Batch 290410, train_perplexity=615.7991, train_loss=6.4229207

Batch 290420, train_perplexity=586.0237, train_loss=6.37336

Batch 290430, train_perplexity=648.3515, train_loss=6.474433

Batch 290440, train_perplexity=643.5361, train_loss=6.466978

Batch 290450, train_perplexity=673.44824, train_loss=6.512411

Batch 290460, train_perplexity=620.406, train_loss=6.430374

Batch 290470, train_perplexity=528.462, train_loss=6.269971

Batch 290480, train_perplexity=574.6015, train_loss=6.353677

Batch 290490, train_perplexity=599.0858, train_loss=6.395405

Batch 290500, train_perplexity=558.4629, train_loss=6.325188

Batch 290510, train_perplexity=568.5575, train_loss=6.3431025

Batch 290520, train_perplexity=615.67633, train_loss=6.4227214

Batch 290530, train_perplexity=571.10345, train_loss=6.3475704

Batch 290540, train_perplexity=597.043, train_loss=6.391989

Batch 290550, train_perplexity=587.9367, train_loss=6.3766193

Batch 290560, train_perplexity=592.0718, train_loss=6.383628

Batch 290570, train_perplexity=560.83044, train_loss=6.3294187

Batch 290580, train_perplexity=588.629, train_loss=6.377796

Batch 290590, train_perplexity=637.6256, train_loss=6.4577513

Batch 290600, train_perplexity=616.6297, train_loss=6.4242687

Batch 290610, train_perplexity=612.3424, train_loss=6.4172916

Batch 290620, train_perplexity=563.34265, train_loss=6.333888

Batch 290630, train_perplexity=616.23145, train_loss=6.4236226

Batch 290640, train_perplexity=617.03503, train_loss=6.424926

Batch 290650, train_perplexity=607.0842, train_loss=6.4086676

Batch 290660, train_perplexity=566.2745, train_loss=6.339079

Batch 290670, train_perplexity=542.05383, train_loss=6.2953653

Batch 290680, train_perplexity=532.00726, train_loss=6.276657

Batch 290690, train_perplexity=539.5136, train_loss=6.290668

Batch 290700, train_perplexity=632.0249, train_loss=6.448929

Batch 290710, train_perplexity=609.01117, train_loss=6.4118366

Batch 290720, train_perplexity=604.0488, train_loss=6.403655

Batch 290730, train_perplexity=598.4348, train_loss=6.3943176

Batch 290740, train_perplexity=616.1956, train_loss=6.4235644

Batch 290750, train_perplexity=560.341, train_loss=6.3285456

Batch 290760, train_perplexity=649.9121, train_loss=6.476837

Batch 290770, train_perplexity=626.84753, train_loss=6.4407034

Batch 290780, train_perplexity=564.0566, train_loss=6.3351545

Batch 290790, train_perplexity=606.22565, train_loss=6.4072523

Batch 290800, train_perplexity=648.7838, train_loss=6.4750996

Batch 290810, train_perplexity=618.4537, train_loss=6.4272223

Batch 290820, train_perplexity=593.1654, train_loss=6.3854733

Batch 290830, train_perplexity=595.9423, train_loss=6.390144

Batch 290840, train_perplexity=618.9711, train_loss=6.4280586

Batch 290850, train_perplexity=550.6136, train_loss=6.3110332

Batch 290860, train_perplexity=656.30817, train_loss=6.4866304

Batch 290870, train_perplexity=655.0775, train_loss=6.4847536

Batch 290880, train_perplexity=592.0035, train_loss=6.3835125

Batch 290890, train_perplexity=642.2274, train_loss=6.4649425

Batch 290900, train_perplexity=559.7647, train_loss=6.3275166

Batch 290910, train_perplexity=573.1914, train_loss=6.3512197

Batch 290920, train_perplexity=606.8724, train_loss=6.4083185

Batch 290930, train_perplexity=674.3859, train_loss=6.5138025

Batch 290940, train_perplexity=523.4891, train_loss=6.260516

Batch 290950, train_perplexity=623.89984, train_loss=6.43599

Batch 290960, train_perplexity=602.2516, train_loss=6.4006753

Batch 290970, train_perplexity=577.2566, train_loss=6.358287

Batch 290980, train_perplexity=586.1452, train_loss=6.3735676

Batch 290990, train_perplexity=666.51575, train_loss=6.5020638

Batch 291000, train_perplexity=592.9759, train_loss=6.385154

Batch 291010, train_perplexity=633.40674, train_loss=6.4511127

Batch 291020, train_perplexity=554.562, train_loss=6.3181787

Batch 291030, train_perplexity=577.4592, train_loss=6.358638

Batch 291040, train_perplexity=565.1593, train_loss=6.3371077

Batch 291050, train_perplexity=721.0693, train_loss=6.580735

Batch 291060, train_perplexity=545.50037, train_loss=6.3017035

Batch 291070, train_perplexity=532.0019, train_loss=6.276647

Batch 291080, train_perplexity=586.408, train_loss=6.374016

Batch 291090, train_perplexity=644.6909, train_loss=6.468771

Batch 291100, train_perplexity=662.65436, train_loss=6.4962535

Batch 291110, train_perplexity=570.64996, train_loss=6.346776

Batch 291120, train_perplexity=602.03424, train_loss=6.4003143

Batch 291130, train_perplexity=610.5837, train_loss=6.4144154

Batch 291140, train_perplexity=732.5544, train_loss=6.5965376

Batch 291150, train_perplexity=585.5999, train_loss=6.372637

Batch 291160, train_perplexity=596.1239, train_loss=6.3904486

Batch 291170, train_perplexity=553.78644, train_loss=6.316779

Batch 291180, train_perplexity=596.00116, train_loss=6.3902426

Batch 291190, train_perplexity=661.64746, train_loss=6.494733

Batch 291200, train_perplexity=560.881, train_loss=6.329509

Batch 291210, train_perplexity=606.4188, train_loss=6.407571

Batch 291220, train_perplexity=672.6517, train_loss=6.5112276

Batch 291230, train_perplexity=663.23944, train_loss=6.497136

Batch 291240, train_perplexity=586.6986, train_loss=6.3745112

Batch 291250, train_perplexity=559.3554, train_loss=6.326785

Batch 291260, train_perplexity=636.38995, train_loss=6.4558115

Batch 291270, train_perplexity=546.84656, train_loss=6.304168

Batch 291280, train_perplexity=567.66626, train_loss=6.3415337

Batch 291290, train_perplexity=567.59644, train_loss=6.3414106

Batch 291300, train_perplexity=599.41785, train_loss=6.395959

Batch 291310, train_perplexity=616.9662, train_loss=6.424814

Batch 291320, train_perplexity=590.0375, train_loss=6.380186

Batch 291330, train_perplexity=626.9026, train_loss=6.440791

Batch 291340, train_perplexity=611.37585, train_loss=6.415712

Batch 291350, train_perplexity=574.64594, train_loss=6.353754

Batch 291360, train_perplexity=535.2403, train_loss=6.282716

Batch 291370, train_perplexity=642.7167, train_loss=6.465704

Batch 291380, train_perplexity=592.498, train_loss=6.3843474

Batch 291390, train_perplexity=632.60114, train_loss=6.44984

Batch 291400, train_perplexity=625.2973, train_loss=6.438227

Batch 291410, train_perplexity=604.443, train_loss=6.4043074

Batch 291420, train_perplexity=556.5608, train_loss=6.3217764

Batch 291430, train_perplexity=609.5876, train_loss=6.4127827

Batch 291440, train_perplexity=621.2557, train_loss=6.4317427

Batch 291450, train_perplexity=607.0472, train_loss=6.4086065

Batch 291460, train_perplexity=721.3351, train_loss=6.581104

Batch 291470, train_perplexity=599.0447, train_loss=6.395336

Batch 291480, train_perplexity=538.9906, train_loss=6.289698

Batch 291490, train_perplexity=599.7369, train_loss=6.396491

Batch 291500, train_perplexity=594.7514, train_loss=6.3881435

Batch 291510, train_perplexity=631.8239, train_loss=6.448611

Batch 291520, train_perplexity=653.2154, train_loss=6.481907

Batch 291530, train_perplexity=629.52484, train_loss=6.4449654

Batch 291540, train_perplexity=571.6307, train_loss=6.348493

Batch 291550, train_perplexity=524.5665, train_loss=6.2625723

Batch 291560, train_perplexity=568.935, train_loss=6.343766

Batch 291570, train_perplexity=601.2677, train_loss=6.39904

Batch 291580, train_perplexity=608.9804, train_loss=6.411786

Batch 291590, train_perplexity=545.97345, train_loss=6.3025703

Batch 291600, train_perplexity=634.99133, train_loss=6.4536114

Batch 291610, train_perplexity=587.8008, train_loss=6.376388

Batch 291620, train_perplexity=640.3761, train_loss=6.4620557

Batch 291630, train_perplexity=618.79346, train_loss=6.4277716

Batch 291640, train_perplexity=636.1976, train_loss=6.455509

Batch 291650, train_perplexity=609.8627, train_loss=6.4132338

Batch 291660, train_perplexity=550.8636, train_loss=6.311487

Batch 291670, train_perplexity=635.61206, train_loss=6.4545884

Batch 291680, train_perplexity=586.022, train_loss=6.3733573

Batch 291690, train_perplexity=591.7105, train_loss=6.3830175

Batch 291700, train_perplexity=553.2166, train_loss=6.3157496
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 291710, train_perplexity=593.43384, train_loss=6.385926

Batch 291720, train_perplexity=605.5161, train_loss=6.406081

Batch 291730, train_perplexity=556.83844, train_loss=6.322275

Batch 291740, train_perplexity=570.7131, train_loss=6.3468866

Batch 291750, train_perplexity=556.6043, train_loss=6.3218546

Batch 291760, train_perplexity=503.56512, train_loss=6.221713

Batch 291770, train_perplexity=578.30023, train_loss=6.360093

Batch 291780, train_perplexity=643.5913, train_loss=6.467064

Batch 291790, train_perplexity=609.5097, train_loss=6.412655

Batch 291800, train_perplexity=563.9285, train_loss=6.3349276

Batch 291810, train_perplexity=689.802, train_loss=6.5364046

Batch 291820, train_perplexity=540.22076, train_loss=6.291978

Batch 291830, train_perplexity=632.8805, train_loss=6.4502816

Batch 291840, train_perplexity=654.1062, train_loss=6.4832697

Batch 291850, train_perplexity=599.07837, train_loss=6.3953924

Batch 291860, train_perplexity=617.72565, train_loss=6.4260445

Batch 291870, train_perplexity=654.6853, train_loss=6.4841547

Batch 291880, train_perplexity=610.2554, train_loss=6.4138775

Batch 291890, train_perplexity=692.06244, train_loss=6.539676

Batch 291900, train_perplexity=620.4699, train_loss=6.430477

Batch 291910, train_perplexity=603.3522, train_loss=6.402501

Batch 291920, train_perplexity=593.83386, train_loss=6.3865995

Batch 291930, train_perplexity=703.167, train_loss=6.5555944

Batch 291940, train_perplexity=569.50854, train_loss=6.344774

Batch 291950, train_perplexity=587.4603, train_loss=6.3758087

Batch 291960, train_perplexity=625.4828, train_loss=6.438524

Batch 291970, train_perplexity=601.2138, train_loss=6.3989506

Batch 291980, train_perplexity=568.0058, train_loss=6.3421316

Batch 291990, train_perplexity=637.60156, train_loss=6.4577136

Batch 292000, train_perplexity=629.75244, train_loss=6.445327

Batch 292010, train_perplexity=570.06903, train_loss=6.3457575

Batch 292020, train_perplexity=676.7788, train_loss=6.5173445

Batch 292030, train_perplexity=601.68585, train_loss=6.3997355

Batch 292040, train_perplexity=648.1326, train_loss=6.4740953

Batch 292050, train_perplexity=540.8151, train_loss=6.2930775

Batch 292060, train_perplexity=561.2497, train_loss=6.330166

Batch 292070, train_perplexity=549.5303, train_loss=6.309064

Batch 292080, train_perplexity=654.9788, train_loss=6.484603

Batch 292090, train_perplexity=650.84436, train_loss=6.4782705

Batch 292100, train_perplexity=602.91504, train_loss=6.4017763

Batch 292110, train_perplexity=599.831, train_loss=6.396648

Batch 292120, train_perplexity=615.1912, train_loss=6.421933

Batch 292130, train_perplexity=572.25684, train_loss=6.349588

Batch 292140, train_perplexity=550.58734, train_loss=6.3109856

Batch 292150, train_perplexity=600.49695, train_loss=6.3977575

Batch 292160, train_perplexity=574.7506, train_loss=6.353936

Batch 292170, train_perplexity=575.09247, train_loss=6.354531

Batch 292180, train_perplexity=605.71075, train_loss=6.4064026

Batch 292190, train_perplexity=601.6732, train_loss=6.3997145

Batch 292200, train_perplexity=568.1228, train_loss=6.3423376

Batch 292210, train_perplexity=662.5753, train_loss=6.4961343

Batch 292220, train_perplexity=622.5124, train_loss=6.4337635

Batch 292230, train_perplexity=578.4009, train_loss=6.360267

Batch 292240, train_perplexity=574.2816, train_loss=6.35312

Batch 292250, train_perplexity=620.9814, train_loss=6.431301

Batch 292260, train_perplexity=562.6857, train_loss=6.332721

Batch 292270, train_perplexity=591.67017, train_loss=6.3829494

Batch 292280, train_perplexity=682.13727, train_loss=6.525231

Batch 292290, train_perplexity=603.5263, train_loss=6.4027896

Batch 292300, train_perplexity=656.3614, train_loss=6.4867115

Batch 292310, train_perplexity=650.00385, train_loss=6.4769783

Batch 292320, train_perplexity=635.8943, train_loss=6.4550323

Batch 292330, train_perplexity=623.36816, train_loss=6.4351373

Batch 292340, train_perplexity=638.4233, train_loss=6.4590015

Batch 292350, train_perplexity=591.99554, train_loss=6.383499

Batch 292360, train_perplexity=593.2103, train_loss=6.385549

Batch 292370, train_perplexity=591.49554, train_loss=6.382654

Batch 292380, train_perplexity=602.36273, train_loss=6.40086

Batch 292390, train_perplexity=677.7179, train_loss=6.518731

Batch 292400, train_perplexity=563.7546, train_loss=6.334619

Batch 292410, train_perplexity=661.13873, train_loss=6.4939637

Batch 292420, train_perplexity=599.12177, train_loss=6.395465

Batch 292430, train_perplexity=593.0093, train_loss=6.38521

Batch 292440, train_perplexity=627.2572, train_loss=6.4413567

Batch 292450, train_perplexity=562.289, train_loss=6.332016

Batch 292460, train_perplexity=571.0436, train_loss=6.3474655

Batch 292470, train_perplexity=666.4509, train_loss=6.5019665

Batch 292480, train_perplexity=542.8787, train_loss=6.296886

Batch 292490, train_perplexity=597.36597, train_loss=6.39253

Batch 292500, train_perplexity=600.1105, train_loss=6.397114

Batch 292510, train_perplexity=636.498, train_loss=6.4559813

Batch 292520, train_perplexity=609.68994, train_loss=6.4129505

Batch 292530, train_perplexity=537.77246, train_loss=6.2874355

Batch 292540, train_perplexity=630.0859, train_loss=6.445856

Batch 292550, train_perplexity=537.25726, train_loss=6.286477

Batch 292560, train_perplexity=670.7939, train_loss=6.508462

Batch 292570, train_perplexity=524.96014, train_loss=6.2633224

Batch 292580, train_perplexity=686.20544, train_loss=6.531177

Batch 292590, train_perplexity=571.76263, train_loss=6.348724

Batch 292600, train_perplexity=578.4003, train_loss=6.360266

Batch 292610, train_perplexity=587.5858, train_loss=6.3760223

Batch 292620, train_perplexity=548.11554, train_loss=6.306486

Batch 292630, train_perplexity=671.11224, train_loss=6.5089364

Batch 292640, train_perplexity=588.23676, train_loss=6.3771296

Batch 292650, train_perplexity=582.5083, train_loss=6.3673434

Batch 292660, train_perplexity=549.7515, train_loss=6.3094664

Batch 292670, train_perplexity=626.4812, train_loss=6.440119

Batch 292680, train_perplexity=607.7301, train_loss=6.409731

Batch 292690, train_perplexity=534.6671, train_loss=6.2816443

Batch 292700, train_perplexity=591.4595, train_loss=6.382593

Batch 292710, train_perplexity=590.6174, train_loss=6.3811684

Batch 292720, train_perplexity=597.00977, train_loss=6.3919334

Batch 292730, train_perplexity=602.97833, train_loss=6.401881

Batch 292740, train_perplexity=595.4625, train_loss=6.3893385

Batch 292750, train_perplexity=587.0151, train_loss=6.3750505

Batch 292760, train_perplexity=509.13766, train_loss=6.2327185

Batch 292770, train_perplexity=694.8314, train_loss=6.543669

Batch 292780, train_perplexity=578.7468, train_loss=6.360865

Batch 292790, train_perplexity=633.4943, train_loss=6.451251

Batch 292800, train_perplexity=603.925, train_loss=6.40345

Batch 292810, train_perplexity=604.77106, train_loss=6.40485

Batch 292820, train_perplexity=664.57477, train_loss=6.4991474

Batch 292830, train_perplexity=554.7284, train_loss=6.3184786

Batch 292840, train_perplexity=605.26556, train_loss=6.4056673

Batch 292850, train_perplexity=546.93494, train_loss=6.30433

Batch 292860, train_perplexity=600.6527, train_loss=6.398017

Batch 292870, train_perplexity=599.5625, train_loss=6.3962

Batch 292880, train_perplexity=598.06854, train_loss=6.3937054

Batch 292890, train_perplexity=557.976, train_loss=6.324316

Batch 292900, train_perplexity=618.5315, train_loss=6.427348

Batch 292910, train_perplexity=587.0361, train_loss=6.3750863

Batch 292920, train_perplexity=590.262, train_loss=6.3805666

Batch 292930, train_perplexity=622.96936, train_loss=6.4344974

Batch 292940, train_perplexity=670.9961, train_loss=6.5087633

Batch 292950, train_perplexity=558.6576, train_loss=6.3255367

Batch 292960, train_perplexity=589.69183, train_loss=6.3796

Batch 292970, train_perplexity=604.1554, train_loss=6.4038315

Batch 292980, train_perplexity=611.85266, train_loss=6.4164915

Batch 292990, train_perplexity=588.93054, train_loss=6.3783083

Batch 293000, train_perplexity=591.6733, train_loss=6.3829546

Batch 293010, train_perplexity=604.98047, train_loss=6.405196
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 293020, train_perplexity=621.4171, train_loss=6.4320025

Batch 293030, train_perplexity=604.02924, train_loss=6.4036226

Batch 293040, train_perplexity=521.51483, train_loss=6.2567377

Batch 293050, train_perplexity=602.98175, train_loss=6.401887

Batch 293060, train_perplexity=573.1121, train_loss=6.3510814

Batch 293070, train_perplexity=613.61914, train_loss=6.4193745

Batch 293080, train_perplexity=568.5217, train_loss=6.3430395

Batch 293090, train_perplexity=667.61566, train_loss=6.5037127

Batch 293100, train_perplexity=579.94714, train_loss=6.362937

Batch 293110, train_perplexity=649.7885, train_loss=6.476647

Batch 293120, train_perplexity=545.7725, train_loss=6.302202

Batch 293130, train_perplexity=531.8994, train_loss=6.2764544

Batch 293140, train_perplexity=584.53253, train_loss=6.3708124

Batch 293150, train_perplexity=630.0162, train_loss=6.4457455

Batch 293160, train_perplexity=642.4513, train_loss=6.465291

Batch 293170, train_perplexity=607.9742, train_loss=6.4101324

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00055-of-00100
Loaded 306641 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00055-of-00100
Loaded 306641 sentences.
Finished loading
Batch 293180, train_perplexity=552.6458, train_loss=6.3147173

Batch 293190, train_perplexity=534.86115, train_loss=6.282007

Batch 293200, train_perplexity=580.3217, train_loss=6.3635826

Batch 293210, train_perplexity=635.3736, train_loss=6.454213

Batch 293220, train_perplexity=635.2136, train_loss=6.4539614

Batch 293230, train_perplexity=556.6261, train_loss=6.3218937

Batch 293240, train_perplexity=553.8155, train_loss=6.3168316

Batch 293250, train_perplexity=578.6991, train_loss=6.3607826

Batch 293260, train_perplexity=605.1051, train_loss=6.405402

Batch 293270, train_perplexity=560.91656, train_loss=6.329572

Batch 293280, train_perplexity=641.3592, train_loss=6.4635897

Batch 293290, train_perplexity=549.06775, train_loss=6.308222

Batch 293300, train_perplexity=498.10394, train_loss=6.2108088

Batch 293310, train_perplexity=654.7852, train_loss=6.4843073

Batch 293320, train_perplexity=568.2583, train_loss=6.342576

Batch 293330, train_perplexity=626.13477, train_loss=6.4395657

Batch 293340, train_perplexity=591.2914, train_loss=6.382309

Batch 293350, train_perplexity=621.15466, train_loss=6.43158

Batch 293360, train_perplexity=559.16766, train_loss=6.3264494

Batch 293370, train_perplexity=577.96606, train_loss=6.359515

Batch 293380, train_perplexity=618.65125, train_loss=6.4275417

Batch 293390, train_perplexity=600.2716, train_loss=6.3973823

Batch 293400, train_perplexity=586.78, train_loss=6.37465

Batch 293410, train_perplexity=647.1031, train_loss=6.4725056

Batch 293420, train_perplexity=660.4005, train_loss=6.4928465

Batch 293430, train_perplexity=562.2681, train_loss=6.331979

Batch 293440, train_perplexity=642.7734, train_loss=6.465792

Batch 293450, train_perplexity=516.80536, train_loss=6.2476664

Batch 293460, train_perplexity=562.1732, train_loss=6.33181

Batch 293470, train_perplexity=596.0264, train_loss=6.390285

Batch 293480, train_perplexity=620.686, train_loss=6.430825

Batch 293490, train_perplexity=620.1156, train_loss=6.429906

Batch 293500, train_perplexity=554.9403, train_loss=6.3188605

Batch 293510, train_perplexity=612.8028, train_loss=6.418043

Batch 293520, train_perplexity=590.17676, train_loss=6.380422

Batch 293530, train_perplexity=588.2334, train_loss=6.377124

Batch 293540, train_perplexity=579.77295, train_loss=6.3626366

Batch 293550, train_perplexity=605.9523, train_loss=6.406801

Batch 293560, train_perplexity=588.2735, train_loss=6.377192

Batch 293570, train_perplexity=608.22296, train_loss=6.4105415

Batch 293580, train_perplexity=617.80756, train_loss=6.426177

Batch 293590, train_perplexity=614.06903, train_loss=6.4201074

Batch 293600, train_perplexity=660.2878, train_loss=6.492676

Batch 293610, train_perplexity=536.6453, train_loss=6.2853374

Batch 293620, train_perplexity=612.6356, train_loss=6.4177704

Batch 293630, train_perplexity=623.6081, train_loss=6.435522

Batch 293640, train_perplexity=642.6462, train_loss=6.4655943

Batch 293650, train_perplexity=546.05286, train_loss=6.302716

Batch 293660, train_perplexity=516.5393, train_loss=6.2471514

Batch 293670, train_perplexity=579.9593, train_loss=6.362958

Batch 293680, train_perplexity=667.0133, train_loss=6.50281

Batch 293690, train_perplexity=621.7802, train_loss=6.4325867

Batch 293700, train_perplexity=597.225, train_loss=6.392294

Batch 293710, train_perplexity=665.33765, train_loss=6.5002947

Batch 293720, train_perplexity=639.64764, train_loss=6.4609175

Batch 293730, train_perplexity=652.9799, train_loss=6.4815464

Batch 293740, train_perplexity=657.1897, train_loss=6.4879727

Batch 293750, train_perplexity=588.76825, train_loss=6.3780327

Batch 293760, train_perplexity=640.08765, train_loss=6.461605

Batch 293770, train_perplexity=611.2945, train_loss=6.415579

Batch 293780, train_perplexity=634.022, train_loss=6.4520836

Batch 293790, train_perplexity=584.22546, train_loss=6.370287

Batch 293800, train_perplexity=575.61237, train_loss=6.3554344

Batch 293810, train_perplexity=546.80145, train_loss=6.3040857

Batch 293820, train_perplexity=609.85944, train_loss=6.4132285

Batch 293830, train_perplexity=611.6677, train_loss=6.416189

Batch 293840, train_perplexity=613.29474, train_loss=6.4188457

Batch 293850, train_perplexity=630.51025, train_loss=6.4465294

Batch 293860, train_perplexity=638.27295, train_loss=6.458766

Batch 293870, train_perplexity=573.14026, train_loss=6.3511305

Batch 293880, train_perplexity=606.76447, train_loss=6.4081407

Batch 293890, train_perplexity=654.05316, train_loss=6.4831886

Batch 293900, train_perplexity=558.53186, train_loss=6.3253117

Batch 293910, train_perplexity=599.0818, train_loss=6.395398

Batch 293920, train_perplexity=578.0526, train_loss=6.359665

Batch 293930, train_perplexity=618.1031, train_loss=6.4266553

Batch 293940, train_perplexity=688.502, train_loss=6.5345182

Batch 293950, train_perplexity=579.4993, train_loss=6.3621645

Batch 293960, train_perplexity=627.77814, train_loss=6.442187

Batch 293970, train_perplexity=586.44604, train_loss=6.3740807

Batch 293980, train_perplexity=613.1249, train_loss=6.4185686

Batch 293990, train_perplexity=614.54974, train_loss=6.42089

Batch 294000, train_perplexity=583.2209, train_loss=6.368566

Batch 294010, train_perplexity=618.9859, train_loss=6.4280825

Batch 294020, train_perplexity=654.7946, train_loss=6.4843216

Batch 294030, train_perplexity=574.6388, train_loss=6.3537416

Batch 294040, train_perplexity=565.9246, train_loss=6.338461

Batch 294050, train_perplexity=622.2521, train_loss=6.4333453

Batch 294060, train_perplexity=630.99146, train_loss=6.4472923

Batch 294070, train_perplexity=602.71155, train_loss=6.4014387

Batch 294080, train_perplexity=586.3521, train_loss=6.3739204

Batch 294090, train_perplexity=597.0661, train_loss=6.392028

Batch 294100, train_perplexity=565.2957, train_loss=6.337349

Batch 294110, train_perplexity=611.9682, train_loss=6.4166803

Batch 294120, train_perplexity=576.52185, train_loss=6.357013

Batch 294130, train_perplexity=623.64197, train_loss=6.4355764

Batch 294140, train_perplexity=622.8188, train_loss=6.4342556

Batch 294150, train_perplexity=550.01447, train_loss=6.3099446

Batch 294160, train_perplexity=627.26556, train_loss=6.44137

Batch 294170, train_perplexity=610.31006, train_loss=6.413967

Batch 294180, train_perplexity=592.95105, train_loss=6.385112

Batch 294190, train_perplexity=611.70447, train_loss=6.4162493

Batch 294200, train_perplexity=590.1182, train_loss=6.380323

Batch 294210, train_perplexity=549.72424, train_loss=6.309417

Batch 294220, train_perplexity=682.8829, train_loss=6.5263233

Batch 294230, train_perplexity=614.7071, train_loss=6.421146

Batch 294240, train_perplexity=608.7519, train_loss=6.411411

Batch 294250, train_perplexity=700.597, train_loss=6.551933
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 294260, train_perplexity=622.07733, train_loss=6.4330645

Batch 294270, train_perplexity=583.3007, train_loss=6.368703

Batch 294280, train_perplexity=593.0282, train_loss=6.385242

Batch 294290, train_perplexity=571.49084, train_loss=6.3482485

Batch 294300, train_perplexity=539.088, train_loss=6.289879

Batch 294310, train_perplexity=542.6114, train_loss=6.2963934

Batch 294320, train_perplexity=549.7657, train_loss=6.309492

Batch 294330, train_perplexity=596.4128, train_loss=6.390933

Batch 294340, train_perplexity=567.24524, train_loss=6.3407917

Batch 294350, train_perplexity=549.60394, train_loss=6.309198

Batch 294360, train_perplexity=635.7345, train_loss=6.454781

Batch 294370, train_perplexity=537.1602, train_loss=6.2862964

Batch 294380, train_perplexity=619.29645, train_loss=6.428584

Batch 294390, train_perplexity=617.41174, train_loss=6.425536

Batch 294400, train_perplexity=660.5819, train_loss=6.493121

Batch 294410, train_perplexity=623.0077, train_loss=6.434559

Batch 294420, train_perplexity=655.15875, train_loss=6.4848776

Batch 294430, train_perplexity=614.87274, train_loss=6.4214153

Batch 294440, train_perplexity=600.1094, train_loss=6.397112

Batch 294450, train_perplexity=521.97406, train_loss=6.257618

Batch 294460, train_perplexity=558.379, train_loss=6.325038

Batch 294470, train_perplexity=626.4627, train_loss=6.440089

Batch 294480, train_perplexity=560.9463, train_loss=6.329625

Batch 294490, train_perplexity=618.0645, train_loss=6.426593

Batch 294500, train_perplexity=565.7039, train_loss=6.338071

Batch 294510, train_perplexity=579.9405, train_loss=6.3629255

Batch 294520, train_perplexity=554.483, train_loss=6.318036

Batch 294530, train_perplexity=529.67346, train_loss=6.2722607

Batch 294540, train_perplexity=580.63806, train_loss=6.3641276

Batch 294550, train_perplexity=679.67065, train_loss=6.5216084

Batch 294560, train_perplexity=625.3083, train_loss=6.438245

Batch 294570, train_perplexity=626.6049, train_loss=6.440316

Batch 294580, train_perplexity=525.86584, train_loss=6.265046

Batch 294590, train_perplexity=585.1717, train_loss=6.3719053

Batch 294600, train_perplexity=624.6884, train_loss=6.437253

Batch 294610, train_perplexity=617.8756, train_loss=6.426287

Batch 294620, train_perplexity=597.33923, train_loss=6.392485

Batch 294630, train_perplexity=660.36053, train_loss=6.492786

Batch 294640, train_perplexity=581.7108, train_loss=6.3659735

Batch 294650, train_perplexity=631.30835, train_loss=6.4477944

Batch 294660, train_perplexity=639.70807, train_loss=6.461012

Batch 294670, train_perplexity=613.7579, train_loss=6.4196005

Batch 294680, train_perplexity=601.64166, train_loss=6.399662

Batch 294690, train_perplexity=627.5456, train_loss=6.4418163

Batch 294700, train_perplexity=599.2492, train_loss=6.3956776

Batch 294710, train_perplexity=590.543, train_loss=6.3810425

Batch 294720, train_perplexity=596.946, train_loss=6.3918266

Batch 294730, train_perplexity=607.5934, train_loss=6.409506

Batch 294740, train_perplexity=587.6719, train_loss=6.3761687

Batch 294750, train_perplexity=671.3856, train_loss=6.5093436

Batch 294760, train_perplexity=603.76605, train_loss=6.403187

Batch 294770, train_perplexity=634.3014, train_loss=6.452524

Batch 294780, train_perplexity=631.908, train_loss=6.448744

Batch 294790, train_perplexity=688.141, train_loss=6.5339937

Batch 294800, train_perplexity=598.82587, train_loss=6.394971

Batch 294810, train_perplexity=577.4306, train_loss=6.358588

Batch 294820, train_perplexity=663.2142, train_loss=6.497098

Batch 294830, train_perplexity=600.2284, train_loss=6.3973103

Batch 294840, train_perplexity=614.1329, train_loss=6.4202113

Batch 294850, train_perplexity=602.0796, train_loss=6.4003897

Batch 294860, train_perplexity=639.3714, train_loss=6.4604855

Batch 294870, train_perplexity=601.0401, train_loss=6.3986616

Batch 294880, train_perplexity=597.4802, train_loss=6.392721

Batch 294890, train_perplexity=619.5487, train_loss=6.4289913

Batch 294900, train_perplexity=638.79785, train_loss=6.459588

Batch 294910, train_perplexity=526.96124, train_loss=6.267127

Batch 294920, train_perplexity=627.19977, train_loss=6.441265

Batch 294930, train_perplexity=589.3708, train_loss=6.3790555

Batch 294940, train_perplexity=572.46234, train_loss=6.349947

Batch 294950, train_perplexity=594.69727, train_loss=6.3880525

Batch 294960, train_perplexity=592.296, train_loss=6.3840065

Batch 294970, train_perplexity=569.2091, train_loss=6.344248

Batch 294980, train_perplexity=625.9297, train_loss=6.439238

Batch 294990, train_perplexity=578.43756, train_loss=6.3603306

Batch 295000, train_perplexity=592.1424, train_loss=6.383747

Batch 295010, train_perplexity=628.9584, train_loss=6.444065

Batch 295020, train_perplexity=598.6063, train_loss=6.394604

Batch 295030, train_perplexity=618.5622, train_loss=6.4273977

Batch 295040, train_perplexity=604.16235, train_loss=6.403843

Batch 295050, train_perplexity=578.4122, train_loss=6.3602867

Batch 295060, train_perplexity=643.10474, train_loss=6.4663076

Batch 295070, train_perplexity=616.80145, train_loss=6.424547

Batch 295080, train_perplexity=591.3461, train_loss=6.3824015

Batch 295090, train_perplexity=617.6417, train_loss=6.4259086

Batch 295100, train_perplexity=570.2599, train_loss=6.346092

Batch 295110, train_perplexity=640.9119, train_loss=6.462892

Batch 295120, train_perplexity=584.60583, train_loss=6.370938

Batch 295130, train_perplexity=566.2083, train_loss=6.338962

Batch 295140, train_perplexity=556.60486, train_loss=6.3218555

Batch 295150, train_perplexity=571.8422, train_loss=6.348863

Batch 295160, train_perplexity=578.2991, train_loss=6.360091

Batch 295170, train_perplexity=607.08655, train_loss=6.4086714

Batch 295180, train_perplexity=588.1131, train_loss=6.3769193

Batch 295190, train_perplexity=592.0481, train_loss=6.383588

Batch 295200, train_perplexity=563.25696, train_loss=6.333736

Batch 295210, train_perplexity=593.0616, train_loss=6.3852983

Batch 295220, train_perplexity=553.4417, train_loss=6.3161564

Batch 295230, train_perplexity=618.9971, train_loss=6.4281006

Batch 295240, train_perplexity=596.0455, train_loss=6.390317

Batch 295250, train_perplexity=622.08984, train_loss=6.4330845

Batch 295260, train_perplexity=527.32526, train_loss=6.2678175

Batch 295270, train_perplexity=598.9304, train_loss=6.3951454

Batch 295280, train_perplexity=658.79144, train_loss=6.490407

Batch 295290, train_perplexity=609.9499, train_loss=6.413377

Batch 295300, train_perplexity=557.3307, train_loss=6.3231587

Batch 295310, train_perplexity=669.2699, train_loss=6.5061874

Batch 295320, train_perplexity=658.1735, train_loss=6.4894686

Batch 295330, train_perplexity=568.9491, train_loss=6.343791

Batch 295340, train_perplexity=576.46497, train_loss=6.3569145

Batch 295350, train_perplexity=578.1169, train_loss=6.359776

Batch 295360, train_perplexity=536.89417, train_loss=6.285801

Batch 295370, train_perplexity=527.97766, train_loss=6.269054

Batch 295380, train_perplexity=575.9105, train_loss=6.3559523

Batch 295390, train_perplexity=580.402, train_loss=6.363721

Batch 295400, train_perplexity=568.7674, train_loss=6.3434715

Batch 295410, train_perplexity=548.73615, train_loss=6.3076177

Batch 295420, train_perplexity=565.42615, train_loss=6.3375797

Batch 295430, train_perplexity=567.4021, train_loss=6.3410683

Batch 295440, train_perplexity=599.1035, train_loss=6.3954344

Batch 295450, train_perplexity=539.3058, train_loss=6.2902827

Batch 295460, train_perplexity=560.0958, train_loss=6.328108

Batch 295470, train_perplexity=581.8237, train_loss=6.3661675

Batch 295480, train_perplexity=661.7301, train_loss=6.494858

Batch 295490, train_perplexity=636.01074, train_loss=6.4552155

Batch 295500, train_perplexity=572.2036, train_loss=6.349495

Batch 295510, train_perplexity=607.5806, train_loss=6.409485

Batch 295520, train_perplexity=587.20325, train_loss=6.375371

Batch 295530, train_perplexity=633.00336, train_loss=6.4504757

Batch 295540, train_perplexity=654.5174, train_loss=6.483898

Batch 295550, train_perplexity=568.20734, train_loss=6.3424864

Batch 295560, train_perplexity=586.30035, train_loss=6.373832
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 295570, train_perplexity=542.8438, train_loss=6.2968216

Batch 295580, train_perplexity=584.6284, train_loss=6.3709764

Batch 295590, train_perplexity=577.00146, train_loss=6.357845

Batch 295600, train_perplexity=611.5814, train_loss=6.416048

Batch 295610, train_perplexity=585.45416, train_loss=6.372388

Batch 295620, train_perplexity=633.70764, train_loss=6.4515877

Batch 295630, train_perplexity=542.54205, train_loss=6.2962656

Batch 295640, train_perplexity=605.3086, train_loss=6.4057384

Batch 295650, train_perplexity=562.4295, train_loss=6.332266

Batch 295660, train_perplexity=573.4049, train_loss=6.351592

Batch 295670, train_perplexity=576.32587, train_loss=6.3566732

Batch 295680, train_perplexity=576.85046, train_loss=6.357583

Batch 295690, train_perplexity=636.1257, train_loss=6.455396

Batch 295700, train_perplexity=604.30176, train_loss=6.4040737

Batch 295710, train_perplexity=690.5477, train_loss=6.537485

Batch 295720, train_perplexity=610.3514, train_loss=6.414035

Batch 295730, train_perplexity=577.67896, train_loss=6.3590183

Batch 295740, train_perplexity=634.5782, train_loss=6.4529605

Batch 295750, train_perplexity=587.1271, train_loss=6.3752413

Batch 295760, train_perplexity=545.8477, train_loss=6.30234

Batch 295770, train_perplexity=626.1811, train_loss=6.4396396

Batch 295780, train_perplexity=577.4782, train_loss=6.3586707

Batch 295790, train_perplexity=618.7368, train_loss=6.42768

Batch 295800, train_perplexity=586.065, train_loss=6.3734307

Batch 295810, train_perplexity=584.50635, train_loss=6.3707676

Batch 295820, train_perplexity=550.76324, train_loss=6.311305

Batch 295830, train_perplexity=559.7017, train_loss=6.327404

Batch 295840, train_perplexity=685.2059, train_loss=6.5297194

Batch 295850, train_perplexity=580.2727, train_loss=6.363498

Batch 295860, train_perplexity=565.8777, train_loss=6.338378

Batch 295870, train_perplexity=611.48456, train_loss=6.4158897

Batch 295880, train_perplexity=572.54645, train_loss=6.350094

Batch 295890, train_perplexity=524.4455, train_loss=6.2623415

Batch 295900, train_perplexity=612.59125, train_loss=6.417698

Batch 295910, train_perplexity=621.7559, train_loss=6.4325476

Batch 295920, train_perplexity=619.6811, train_loss=6.429205

Batch 295930, train_perplexity=637.9288, train_loss=6.4582267

Batch 295940, train_perplexity=583.286, train_loss=6.3686776

Batch 295950, train_perplexity=618.843, train_loss=6.4278517

Batch 295960, train_perplexity=569.86194, train_loss=6.345394

Batch 295970, train_perplexity=545.4707, train_loss=6.301649

Batch 295980, train_perplexity=609.6405, train_loss=6.4128695

Batch 295990, train_perplexity=548.2638, train_loss=6.3067565

Batch 296000, train_perplexity=599.15436, train_loss=6.3955193

Batch 296010, train_perplexity=599.9277, train_loss=6.396809

Batch 296020, train_perplexity=510.8146, train_loss=6.2360067

Batch 296030, train_perplexity=627.77966, train_loss=6.442189

Batch 296040, train_perplexity=542.6419, train_loss=6.2964497

Batch 296050, train_perplexity=575.19806, train_loss=6.3547144

Batch 296060, train_perplexity=660.9436, train_loss=6.4936686

Batch 296070, train_perplexity=596.7257, train_loss=6.3914576

Batch 296080, train_perplexity=612.73145, train_loss=6.417927

Batch 296090, train_perplexity=615.3256, train_loss=6.4221516

Batch 296100, train_perplexity=646.9516, train_loss=6.4722714

Batch 296110, train_perplexity=606.60535, train_loss=6.4078784

Batch 296120, train_perplexity=540.54803, train_loss=6.2925835

Batch 296130, train_perplexity=569.5767, train_loss=6.3448935

Batch 296140, train_perplexity=583.7938, train_loss=6.369548

Batch 296150, train_perplexity=634.54034, train_loss=6.452901

Batch 296160, train_perplexity=621.7227, train_loss=6.432494

Batch 296170, train_perplexity=602.88513, train_loss=6.4017267

Batch 296180, train_perplexity=587.9499, train_loss=6.3766418

Batch 296190, train_perplexity=608.0954, train_loss=6.4103317

Batch 296200, train_perplexity=555.0427, train_loss=6.319045

Batch 296210, train_perplexity=560.1797, train_loss=6.3282576

Batch 296220, train_perplexity=573.62994, train_loss=6.3519845

Batch 296230, train_perplexity=488.56198, train_loss=6.1914663

Batch 296240, train_perplexity=623.3435, train_loss=6.4350977

Batch 296250, train_perplexity=599.21924, train_loss=6.3956275

Batch 296260, train_perplexity=593.04376, train_loss=6.385268

Batch 296270, train_perplexity=595.54376, train_loss=6.389475

Batch 296280, train_perplexity=615.8798, train_loss=6.423052

Batch 296290, train_perplexity=525.8009, train_loss=6.2649226

Batch 296300, train_perplexity=573.4924, train_loss=6.3517447

Batch 296310, train_perplexity=565.20245, train_loss=6.337184

Batch 296320, train_perplexity=562.34503, train_loss=6.3321157

Batch 296330, train_perplexity=604.7561, train_loss=6.404825

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00025-of-00100
Loaded 306273 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00025-of-00100
Loaded 306273 sentences.
Finished loading
Batch 296340, train_perplexity=556.1565, train_loss=6.3210497

Batch 296350, train_perplexity=585.1695, train_loss=6.3719015

Batch 296360, train_perplexity=605.3083, train_loss=6.405738

Batch 296370, train_perplexity=641.22894, train_loss=6.4633865

Batch 296380, train_perplexity=541.7786, train_loss=6.2948575

Batch 296390, train_perplexity=653.2995, train_loss=6.4820356

Batch 296400, train_perplexity=550.0263, train_loss=6.309966

Batch 296410, train_perplexity=552.0674, train_loss=6.31367

Batch 296420, train_perplexity=552.1896, train_loss=6.3138914

Batch 296430, train_perplexity=593.53265, train_loss=6.386092

Batch 296440, train_perplexity=631.6929, train_loss=6.4484034

Batch 296450, train_perplexity=668.7168, train_loss=6.5053606

Batch 296460, train_perplexity=609.81177, train_loss=6.4131503

Batch 296470, train_perplexity=624.9712, train_loss=6.4377055

Batch 296480, train_perplexity=649.8997, train_loss=6.476818

Batch 296490, train_perplexity=598.3898, train_loss=6.3942423

Batch 296500, train_perplexity=552.4445, train_loss=6.314353

Batch 296510, train_perplexity=562.41425, train_loss=6.3322387

Batch 296520, train_perplexity=550.08136, train_loss=6.310066

Batch 296530, train_perplexity=589.42163, train_loss=6.379142

Batch 296540, train_perplexity=583.3903, train_loss=6.3688564

Batch 296550, train_perplexity=630.1634, train_loss=6.445979

Batch 296560, train_perplexity=647.6062, train_loss=6.473283

Batch 296570, train_perplexity=736.4933, train_loss=6.6019

Batch 296580, train_perplexity=626.3818, train_loss=6.43996

Batch 296590, train_perplexity=601.6577, train_loss=6.3996887

Batch 296600, train_perplexity=601.67664, train_loss=6.39972

Batch 296610, train_perplexity=600.1663, train_loss=6.397207

Batch 296620, train_perplexity=563.18713, train_loss=6.333612

Batch 296630, train_perplexity=639.871, train_loss=6.4612665

Batch 296640, train_perplexity=606.1705, train_loss=6.407161

Batch 296650, train_perplexity=597.0083, train_loss=6.391931

Batch 296660, train_perplexity=595.7497, train_loss=6.3898206

Batch 296670, train_perplexity=604.10126, train_loss=6.403742

Batch 296680, train_perplexity=593.92615, train_loss=6.386755

Batch 296690, train_perplexity=617.62555, train_loss=6.4258823

Batch 296700, train_perplexity=647.8502, train_loss=6.4736595

Batch 296710, train_perplexity=552.18274, train_loss=6.313879

Batch 296720, train_perplexity=576.0775, train_loss=6.356242

Batch 296730, train_perplexity=627.14417, train_loss=6.4411764

Batch 296740, train_perplexity=634.72675, train_loss=6.4531946

Batch 296750, train_perplexity=551.39575, train_loss=6.312453

Batch 296760, train_perplexity=594.7622, train_loss=6.3881617

Batch 296770, train_perplexity=586.5143, train_loss=6.374197

Batch 296780, train_perplexity=572.5385, train_loss=6.35008

Batch 296790, train_perplexity=612.9705, train_loss=6.418317

Batch 296800, train_perplexity=653.1923, train_loss=6.4818716

Batch 296810, train_perplexity=590.8534, train_loss=6.381568
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 296820, train_perplexity=587.35, train_loss=6.375621

Batch 296830, train_perplexity=594.7665, train_loss=6.388169

Batch 296840, train_perplexity=598.38806, train_loss=6.3942394

Batch 296850, train_perplexity=593.1232, train_loss=6.385402

Batch 296860, train_perplexity=556.80975, train_loss=6.3222237

Batch 296870, train_perplexity=569.2395, train_loss=6.344301

Batch 296880, train_perplexity=555.1454, train_loss=6.31923

Batch 296890, train_perplexity=680.4483, train_loss=6.522752

Batch 296900, train_perplexity=542.29016, train_loss=6.295801

Batch 296910, train_perplexity=597.7367, train_loss=6.3931503

Batch 296920, train_perplexity=590.1219, train_loss=6.380329

Batch 296930, train_perplexity=655.883, train_loss=6.4859824

Batch 296940, train_perplexity=649.49603, train_loss=6.476197

Batch 296950, train_perplexity=584.8537, train_loss=6.3713617

Batch 296960, train_perplexity=601.9085, train_loss=6.4001055

Batch 296970, train_perplexity=565.1343, train_loss=6.3370633

Batch 296980, train_perplexity=569.1863, train_loss=6.344208

Batch 296990, train_perplexity=587.61523, train_loss=6.3760724

Batch 297000, train_perplexity=661.1971, train_loss=6.494052

Batch 297010, train_perplexity=561.1541, train_loss=6.3299956

Batch 297020, train_perplexity=615.3714, train_loss=6.422226

Batch 297030, train_perplexity=625.70264, train_loss=6.438875

Batch 297040, train_perplexity=554.6527, train_loss=6.318342

Batch 297050, train_perplexity=631.6447, train_loss=6.448327

Batch 297060, train_perplexity=549.5308, train_loss=6.309065

Batch 297070, train_perplexity=614.9795, train_loss=6.421589

Batch 297080, train_perplexity=598.1666, train_loss=6.3938694

Batch 297090, train_perplexity=600.0893, train_loss=6.3970785

Batch 297100, train_perplexity=550.8068, train_loss=6.311384

Batch 297110, train_perplexity=544.0098, train_loss=6.2989674

Batch 297120, train_perplexity=605.321, train_loss=6.405759

Batch 297130, train_perplexity=567.37695, train_loss=6.341024

Batch 297140, train_perplexity=574.6418, train_loss=6.353747

Batch 297150, train_perplexity=577.72253, train_loss=6.3590937

Batch 297160, train_perplexity=573.0178, train_loss=6.350917

Batch 297170, train_perplexity=568.03394, train_loss=6.342181

Batch 297180, train_perplexity=576.1418, train_loss=6.3563538

Batch 297190, train_perplexity=662.49225, train_loss=6.496009

Batch 297200, train_perplexity=595.7221, train_loss=6.3897743

Batch 297210, train_perplexity=619.11633, train_loss=6.428293

Batch 297220, train_perplexity=545.3495, train_loss=6.301427

Batch 297230, train_perplexity=591.17694, train_loss=6.3821154

Batch 297240, train_perplexity=597.9374, train_loss=6.393486

Batch 297250, train_perplexity=569.01697, train_loss=6.34391

Batch 297260, train_perplexity=545.1251, train_loss=6.3010154

Batch 297270, train_perplexity=519.31964, train_loss=6.2525196

Batch 297280, train_perplexity=580.6555, train_loss=6.3641577

Batch 297290, train_perplexity=620.509, train_loss=6.43054

Batch 297300, train_perplexity=583.2543, train_loss=6.3686233

Batch 297310, train_perplexity=548.8541, train_loss=6.3078327

Batch 297320, train_perplexity=570.2006, train_loss=6.3459883

Batch 297330, train_perplexity=630.0678, train_loss=6.4458275

Batch 297340, train_perplexity=614.817, train_loss=6.4213247

Batch 297350, train_perplexity=630.197, train_loss=6.4460325

Batch 297360, train_perplexity=592.8221, train_loss=6.3848944

Batch 297370, train_perplexity=605.97656, train_loss=6.4068413

Batch 297380, train_perplexity=533.5829, train_loss=6.2796144

Batch 297390, train_perplexity=515.70154, train_loss=6.245528

Batch 297400, train_perplexity=635.0495, train_loss=6.453703

Batch 297410, train_perplexity=571.21924, train_loss=6.347773

Batch 297420, train_perplexity=642.74054, train_loss=6.465741

Batch 297430, train_perplexity=548.63354, train_loss=6.3074307

Batch 297440, train_perplexity=630.2634, train_loss=6.446138

Batch 297450, train_perplexity=674.56024, train_loss=6.514061

Batch 297460, train_perplexity=566.82666, train_loss=6.3400536

Batch 297470, train_perplexity=609.6905, train_loss=6.4129515

Batch 297480, train_perplexity=584.11707, train_loss=6.3701015

Batch 297490, train_perplexity=678.6415, train_loss=6.520093

Batch 297500, train_perplexity=591.1121, train_loss=6.3820057

Batch 297510, train_perplexity=583.42816, train_loss=6.3689213

Batch 297520, train_perplexity=618.2508, train_loss=6.426894

Batch 297530, train_perplexity=577.3119, train_loss=6.3583827

Batch 297540, train_perplexity=559.17755, train_loss=6.326467

Batch 297550, train_perplexity=588.22217, train_loss=6.3771048

Batch 297560, train_perplexity=665.5995, train_loss=6.500688

Batch 297570, train_perplexity=604.41876, train_loss=6.4042673

Batch 297580, train_perplexity=574.2126, train_loss=6.3529997

Batch 297590, train_perplexity=611.7567, train_loss=6.4163346

Batch 297600, train_perplexity=592.6056, train_loss=6.384529

Batch 297610, train_perplexity=575.04584, train_loss=6.3544497

Batch 297620, train_perplexity=611.6677, train_loss=6.416189

Batch 297630, train_perplexity=575.6384, train_loss=6.3554797

Batch 297640, train_perplexity=646.5192, train_loss=6.471603

Batch 297650, train_perplexity=564.42706, train_loss=6.335811

Batch 297660, train_perplexity=599.8859, train_loss=6.3967395

Batch 297670, train_perplexity=639.26404, train_loss=6.4603176

Batch 297680, train_perplexity=593.42566, train_loss=6.385912

Batch 297690, train_perplexity=653.98456, train_loss=6.4830837

Batch 297700, train_perplexity=551.1757, train_loss=6.3120537

Batch 297710, train_perplexity=580.6699, train_loss=6.3641825

Batch 297720, train_perplexity=583.923, train_loss=6.369769

Batch 297730, train_perplexity=656.5554, train_loss=6.487007

Batch 297740, train_perplexity=573.65594, train_loss=6.35203

Batch 297750, train_perplexity=656.4155, train_loss=6.486794

Batch 297760, train_perplexity=586.0066, train_loss=6.373331

Batch 297770, train_perplexity=691.6906, train_loss=6.539139

Batch 297780, train_perplexity=605.0434, train_loss=6.4053

Batch 297790, train_perplexity=586.51764, train_loss=6.3742027

Batch 297800, train_perplexity=535.0147, train_loss=6.2822943

Batch 297810, train_perplexity=592.83344, train_loss=6.3849134

Batch 297820, train_perplexity=660.301, train_loss=6.492696

Batch 297830, train_perplexity=587.45026, train_loss=6.3757915

Batch 297840, train_perplexity=667.7271, train_loss=6.5038795

Batch 297850, train_perplexity=563.11975, train_loss=6.3334923

Batch 297860, train_perplexity=600.7868, train_loss=6.39824

Batch 297870, train_perplexity=640.67084, train_loss=6.462516

Batch 297880, train_perplexity=618.9263, train_loss=6.427986

Batch 297890, train_perplexity=568.58026, train_loss=6.3431425

Batch 297900, train_perplexity=567.1517, train_loss=6.3406267

Batch 297910, train_perplexity=595.152, train_loss=6.388817

Batch 297920, train_perplexity=559.7092, train_loss=6.3274174

Batch 297930, train_perplexity=567.55634, train_loss=6.34134

Batch 297940, train_perplexity=601.4891, train_loss=6.3994083

Batch 297950, train_perplexity=565.6187, train_loss=6.33792

Batch 297960, train_perplexity=547.0787, train_loss=6.3045926

Batch 297970, train_perplexity=554.4322, train_loss=6.3179445

Batch 297980, train_perplexity=611.3044, train_loss=6.415595

Batch 297990, train_perplexity=538.5428, train_loss=6.288867

Batch 298000, train_perplexity=539.66876, train_loss=6.2909555

Batch 298010, train_perplexity=610.0657, train_loss=6.4135666

Batch 298020, train_perplexity=607.64264, train_loss=6.409587

Batch 298030, train_perplexity=656.3798, train_loss=6.4867396

Batch 298040, train_perplexity=547.8051, train_loss=6.3059196

Batch 298050, train_perplexity=624.6217, train_loss=6.437146

Batch 298060, train_perplexity=624.7703, train_loss=6.437384

Batch 298070, train_perplexity=554.5068, train_loss=6.318079

Batch 298080, train_perplexity=616.925, train_loss=6.4247475

Batch 298090, train_perplexity=605.295, train_loss=6.405716

Batch 298100, train_perplexity=625.9055, train_loss=6.4391994

Batch 298110, train_perplexity=596.6404, train_loss=6.3913145

Batch 298120, train_perplexity=575.73206, train_loss=6.3556423
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 298130, train_perplexity=608.4791, train_loss=6.4109626

Batch 298140, train_perplexity=542.70844, train_loss=6.296572

Batch 298150, train_perplexity=561.2248, train_loss=6.3301215

Batch 298160, train_perplexity=650.1142, train_loss=6.477148

Batch 298170, train_perplexity=603.75946, train_loss=6.403176

Batch 298180, train_perplexity=615.1742, train_loss=6.4219055

Batch 298190, train_perplexity=533.8061, train_loss=6.2800326

Batch 298200, train_perplexity=582.4125, train_loss=6.367179

Batch 298210, train_perplexity=574.28705, train_loss=6.3531294

Batch 298220, train_perplexity=605.44885, train_loss=6.40597

Batch 298230, train_perplexity=626.28467, train_loss=6.439805

Batch 298240, train_perplexity=586.79877, train_loss=6.374682

Batch 298250, train_perplexity=622.8874, train_loss=6.4343657

Batch 298260, train_perplexity=541.4127, train_loss=6.294182

Batch 298270, train_perplexity=618.6955, train_loss=6.4276133

Batch 298280, train_perplexity=588.00146, train_loss=6.3767295

Batch 298290, train_perplexity=502.28214, train_loss=6.219162

Batch 298300, train_perplexity=570.1027, train_loss=6.3458166

Batch 298310, train_perplexity=565.2984, train_loss=6.3373537

Batch 298320, train_perplexity=554.90247, train_loss=6.3187923

Batch 298330, train_perplexity=582.6297, train_loss=6.367552

Batch 298340, train_perplexity=615.6945, train_loss=6.422751

Batch 298350, train_perplexity=603.131, train_loss=6.4021344

Batch 298360, train_perplexity=615.85864, train_loss=6.4230175

Batch 298370, train_perplexity=629.7146, train_loss=6.4452667

Batch 298380, train_perplexity=587.09906, train_loss=6.3751936

Batch 298390, train_perplexity=590.2176, train_loss=6.3804913

Batch 298400, train_perplexity=570.9085, train_loss=6.347229

Batch 298410, train_perplexity=535.00964, train_loss=6.2822847

Batch 298420, train_perplexity=662.97925, train_loss=6.4967437

Batch 298430, train_perplexity=563.72906, train_loss=6.3345737

Batch 298440, train_perplexity=622.82025, train_loss=6.434258

Batch 298450, train_perplexity=568.2453, train_loss=6.342553

Batch 298460, train_perplexity=677.23785, train_loss=6.5180225

Batch 298470, train_perplexity=543.3988, train_loss=6.2978435

Batch 298480, train_perplexity=598.20087, train_loss=6.3939266

Batch 298490, train_perplexity=572.7256, train_loss=6.3504066

Batch 298500, train_perplexity=580.9297, train_loss=6.3646297

Batch 298510, train_perplexity=541.1809, train_loss=6.2937536

Batch 298520, train_perplexity=536.0812, train_loss=6.2842855

Batch 298530, train_perplexity=624.5681, train_loss=6.4370604

Batch 298540, train_perplexity=561.39954, train_loss=6.330433

Batch 298550, train_perplexity=650.1353, train_loss=6.4771805

Batch 298560, train_perplexity=545.4642, train_loss=6.301637

Batch 298570, train_perplexity=617.0424, train_loss=6.4249377

Batch 298580, train_perplexity=591.10114, train_loss=6.381987

Batch 298590, train_perplexity=568.5575, train_loss=6.3431025

Batch 298600, train_perplexity=585.3598, train_loss=6.3722267

Batch 298610, train_perplexity=590.9086, train_loss=6.3816614

Batch 298620, train_perplexity=578.0479, train_loss=6.359657

Batch 298630, train_perplexity=625.5132, train_loss=6.4385724

Batch 298640, train_perplexity=537.23987, train_loss=6.2864447

Batch 298650, train_perplexity=601.1057, train_loss=6.398771

Batch 298660, train_perplexity=612.29803, train_loss=6.417219

Batch 298670, train_perplexity=523.31537, train_loss=6.2601843

Batch 298680, train_perplexity=571.6762, train_loss=6.3485727

Batch 298690, train_perplexity=547.7628, train_loss=6.3058424

Batch 298700, train_perplexity=583.8606, train_loss=6.3696623

Batch 298710, train_perplexity=653.1805, train_loss=6.4818535

Batch 298720, train_perplexity=641.488, train_loss=6.4637904

Batch 298730, train_perplexity=611.6479, train_loss=6.416157

Batch 298740, train_perplexity=552.50195, train_loss=6.314457

Batch 298750, train_perplexity=596.3508, train_loss=6.390829

Batch 298760, train_perplexity=566.4784, train_loss=6.339439

Batch 298770, train_perplexity=535.552, train_loss=6.283298

Batch 298780, train_perplexity=609.85333, train_loss=6.4132185

Batch 298790, train_perplexity=662.8297, train_loss=6.496518

Batch 298800, train_perplexity=580.6716, train_loss=6.3641853

Batch 298810, train_perplexity=598.843, train_loss=6.3949995

Batch 298820, train_perplexity=595.08813, train_loss=6.3887095

Batch 298830, train_perplexity=584.3575, train_loss=6.370513

Batch 298840, train_perplexity=567.2723, train_loss=6.3408394

Batch 298850, train_perplexity=581.99603, train_loss=6.3664637

Batch 298860, train_perplexity=523.4826, train_loss=6.260504

Batch 298870, train_perplexity=548.8309, train_loss=6.3077903

Batch 298880, train_perplexity=579.2543, train_loss=6.3617415

Batch 298890, train_perplexity=575.451, train_loss=6.355154

Batch 298900, train_perplexity=559.0104, train_loss=6.326168

Batch 298910, train_perplexity=556.3719, train_loss=6.321437

Batch 298920, train_perplexity=600.25476, train_loss=6.397354

Batch 298930, train_perplexity=546.19324, train_loss=6.302973

Batch 298940, train_perplexity=602.1261, train_loss=6.400467

Batch 298950, train_perplexity=583.97034, train_loss=6.36985

Batch 298960, train_perplexity=634.1387, train_loss=6.4522676

Batch 298970, train_perplexity=634.2917, train_loss=6.452509

Batch 298980, train_perplexity=597.55005, train_loss=6.392838

Batch 298990, train_perplexity=606.17194, train_loss=6.4071636

Batch 299000, train_perplexity=560.0456, train_loss=6.328018

Batch 299010, train_perplexity=614.1739, train_loss=6.420278

Batch 299020, train_perplexity=651.8115, train_loss=6.4797554

Batch 299030, train_perplexity=550.7559, train_loss=6.3112917

Batch 299040, train_perplexity=580.93304, train_loss=6.3646355

Batch 299050, train_perplexity=581.10284, train_loss=6.364928

Batch 299060, train_perplexity=617.95575, train_loss=6.426417

Batch 299070, train_perplexity=522.3996, train_loss=6.258433

Batch 299080, train_perplexity=594.37067, train_loss=6.387503

Batch 299090, train_perplexity=510.0742, train_loss=6.234556

Batch 299100, train_perplexity=574.6892, train_loss=6.3538294

Batch 299110, train_perplexity=570.6867, train_loss=6.3468404

Batch 299120, train_perplexity=583.86115, train_loss=6.3696632

Batch 299130, train_perplexity=596.4964, train_loss=6.391073

Batch 299140, train_perplexity=614.4331, train_loss=6.4207

Batch 299150, train_perplexity=619.48694, train_loss=6.4288917

Batch 299160, train_perplexity=625.071, train_loss=6.4378653

Batch 299170, train_perplexity=541.5516, train_loss=6.2944384

Batch 299180, train_perplexity=570.4971, train_loss=6.346508

Batch 299190, train_perplexity=598.5033, train_loss=6.394432

Batch 299200, train_perplexity=586.76605, train_loss=6.374626

Batch 299210, train_perplexity=614.6031, train_loss=6.4209766

Batch 299220, train_perplexity=535.0374, train_loss=6.2823367

Batch 299230, train_perplexity=681.9528, train_loss=6.5249605

Batch 299240, train_perplexity=632.05444, train_loss=6.4489756

Batch 299250, train_perplexity=521.57153, train_loss=6.2568464

Batch 299260, train_perplexity=596.9244, train_loss=6.3917904

Batch 299270, train_perplexity=635.7445, train_loss=6.454797

Batch 299280, train_perplexity=628.28516, train_loss=6.442994

Batch 299290, train_perplexity=675.485, train_loss=6.515431

Batch 299300, train_perplexity=581.20984, train_loss=6.365112

Batch 299310, train_perplexity=558.9443, train_loss=6.32605

Batch 299320, train_perplexity=586.4102, train_loss=6.3740196

Batch 299330, train_perplexity=575.17664, train_loss=6.354677

Batch 299340, train_perplexity=547.8059, train_loss=6.305921

Batch 299350, train_perplexity=626.7005, train_loss=6.440469

Batch 299360, train_perplexity=661.4648, train_loss=6.494457

Batch 299370, train_perplexity=570.0397, train_loss=6.345706

Batch 299380, train_perplexity=593.03357, train_loss=6.385251

Batch 299390, train_perplexity=681.7766, train_loss=6.524702

Batch 299400, train_perplexity=601.9378, train_loss=6.400154

Batch 299410, train_perplexity=623.6904, train_loss=6.435654

Batch 299420, train_perplexity=589.3643, train_loss=6.3790445

Batch 299430, train_perplexity=611.7581, train_loss=6.416337
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 299440, train_perplexity=572.7895, train_loss=6.350518

Batch 299450, train_perplexity=623.0392, train_loss=6.4346094

Batch 299460, train_perplexity=679.9313, train_loss=6.5219917

Batch 299470, train_perplexity=555.22375, train_loss=6.319371

Batch 299480, train_perplexity=653.41473, train_loss=6.482212

Batch 299490, train_perplexity=519.6124, train_loss=6.253083

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00070-of-00100
Loaded 305476 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00070-of-00100
Loaded 305476 sentences.
Finished loading
Batch 299500, train_perplexity=600.7429, train_loss=6.398167

Batch 299510, train_perplexity=591.69244, train_loss=6.382987

Batch 299520, train_perplexity=584.17584, train_loss=6.370202

Batch 299530, train_perplexity=643.68555, train_loss=6.4672103

Batch 299540, train_perplexity=614.8194, train_loss=6.4213285

Batch 299550, train_perplexity=595.55396, train_loss=6.389492

Batch 299560, train_perplexity=590.99713, train_loss=6.381811

Batch 299570, train_perplexity=626.38324, train_loss=6.4399624

Batch 299580, train_perplexity=574.7034, train_loss=6.353854

Batch 299590, train_perplexity=642.64, train_loss=6.4655848

Batch 299600, train_perplexity=585.682, train_loss=6.372777

Batch 299610, train_perplexity=552.3492, train_loss=6.3141804

Batch 299620, train_perplexity=617.25867, train_loss=6.425288

Batch 299630, train_perplexity=586.16113, train_loss=6.3735948

Batch 299640, train_perplexity=567.98926, train_loss=6.3421025

Batch 299650, train_perplexity=536.88544, train_loss=6.2857847

Batch 299660, train_perplexity=621.0963, train_loss=6.431486

Batch 299670, train_perplexity=583.3018, train_loss=6.368705

Batch 299680, train_perplexity=500.92358, train_loss=6.2164536

Batch 299690, train_perplexity=600.78107, train_loss=6.3982306

Batch 299700, train_perplexity=525.056, train_loss=6.263505

Batch 299710, train_perplexity=521.9452, train_loss=6.2575626

Batch 299720, train_perplexity=579.3424, train_loss=6.3618937

Batch 299730, train_perplexity=582.1296, train_loss=6.366693

Batch 299740, train_perplexity=597.3868, train_loss=6.392565

Batch 299750, train_perplexity=652.6036, train_loss=6.48097

Batch 299760, train_perplexity=676.58, train_loss=6.5170507

Batch 299770, train_perplexity=636.5374, train_loss=6.4560432

Batch 299780, train_perplexity=550.5374, train_loss=6.310895

Batch 299790, train_perplexity=600.4285, train_loss=6.3976436

Batch 299800, train_perplexity=659.20624, train_loss=6.4910364

Batch 299810, train_perplexity=543.3643, train_loss=6.29778

Batch 299820, train_perplexity=620.5096, train_loss=6.430541

Batch 299830, train_perplexity=523.9805, train_loss=6.2614546

Batch 299840, train_perplexity=664.577, train_loss=6.4991508

Batch 299850, train_perplexity=520.33844, train_loss=6.2544794

Batch 299860, train_perplexity=608.3651, train_loss=6.410775

Batch 299870, train_perplexity=583.85785, train_loss=6.3696575

Batch 299880, train_perplexity=597.6438, train_loss=6.392995

Batch 299890, train_perplexity=575.17114, train_loss=6.3546677

Batch 299900, train_perplexity=617.9496, train_loss=6.426407

Batch 299910, train_perplexity=547.2715, train_loss=6.304945

Batch 299920, train_perplexity=639.9381, train_loss=6.4613714

Batch 299930, train_perplexity=590.4895, train_loss=6.380952

Batch 299940, train_perplexity=586.00604, train_loss=6.37333

Batch 299950, train_perplexity=532.09784, train_loss=6.2768273

Batch 299960, train_perplexity=548.52185, train_loss=6.307227

Batch 299970, train_perplexity=590.3952, train_loss=6.380792

Batch 299980, train_perplexity=605.7457, train_loss=6.4064603

Batch 299990, train_perplexity=587.0694, train_loss=6.375143

Batch 300000, train_perplexity=606.8295, train_loss=6.408248

Batch 300010, train_perplexity=530.48303, train_loss=6.273788

Batch 300020, train_perplexity=629.0574, train_loss=6.4442225

Batch 300030, train_perplexity=586.25616, train_loss=6.373757

Batch 300040, train_perplexity=631.3791, train_loss=6.4479065

Batch 300050, train_perplexity=626.0345, train_loss=6.4394054

Batch 300060, train_perplexity=592.0413, train_loss=6.3835764

Batch 300070, train_perplexity=616.46216, train_loss=6.423997

Batch 300080, train_perplexity=634.3107, train_loss=6.452539

Batch 300090, train_perplexity=555.5797, train_loss=6.320012

Batch 300100, train_perplexity=564.55273, train_loss=6.336034

Batch 300110, train_perplexity=648.9203, train_loss=6.47531

Batch 300120, train_perplexity=531.4277, train_loss=6.275567

Batch 300130, train_perplexity=598.9281, train_loss=6.3951416

Batch 300140, train_perplexity=602.2091, train_loss=6.4006047

Batch 300150, train_perplexity=569.86084, train_loss=6.345392

Batch 300160, train_perplexity=614.43665, train_loss=6.420706

Batch 300170, train_perplexity=553.26434, train_loss=6.315836

Batch 300180, train_perplexity=579.2656, train_loss=6.361761

Batch 300190, train_perplexity=530.76996, train_loss=6.2743287

Batch 300200, train_perplexity=569.52295, train_loss=6.344799

Batch 300210, train_perplexity=652.5641, train_loss=6.4809093

Batch 300220, train_perplexity=583.64294, train_loss=6.3692894

Batch 300230, train_perplexity=642.606, train_loss=6.465532

Batch 300240, train_perplexity=560.0701, train_loss=6.328062

Batch 300250, train_perplexity=573.9646, train_loss=6.3525677

Batch 300260, train_perplexity=539.17694, train_loss=6.290044

Batch 300270, train_perplexity=568.61554, train_loss=6.3432045

Batch 300280, train_perplexity=647.75323, train_loss=6.47351

Batch 300290, train_perplexity=557.86694, train_loss=6.3241205

Batch 300300, train_perplexity=549.3385, train_loss=6.308715

Batch 300310, train_perplexity=562.08905, train_loss=6.3316603

Batch 300320, train_perplexity=561.76587, train_loss=6.331085

Batch 300330, train_perplexity=541.25525, train_loss=6.293891

Batch 300340, train_perplexity=635.486, train_loss=6.45439

Batch 300350, train_perplexity=657.58185, train_loss=6.4885693

Batch 300360, train_perplexity=519.33997, train_loss=6.2525587

Batch 300370, train_perplexity=600.47003, train_loss=6.3977127

Batch 300380, train_perplexity=557.86163, train_loss=6.324111

Batch 300390, train_perplexity=583.8055, train_loss=6.369568

Batch 300400, train_perplexity=603.4377, train_loss=6.4026427

Batch 300410, train_perplexity=610.0389, train_loss=6.4135227

Batch 300420, train_perplexity=633.65204, train_loss=6.4515

Batch 300430, train_perplexity=588.0834, train_loss=6.3768687

Batch 300440, train_perplexity=600.3131, train_loss=6.3974514

Batch 300450, train_perplexity=562.2351, train_loss=6.33192

Batch 300460, train_perplexity=498.8893, train_loss=6.212384

Batch 300470, train_perplexity=577.69855, train_loss=6.359052

Batch 300480, train_perplexity=583.5878, train_loss=6.369195

Batch 300490, train_perplexity=608.97345, train_loss=6.4117746

Batch 300500, train_perplexity=603.9106, train_loss=6.403426

Batch 300510, train_perplexity=501.31403, train_loss=6.2172327

Batch 300520, train_perplexity=594.431, train_loss=6.3876047

Batch 300530, train_perplexity=521.3766, train_loss=6.2564726

Batch 300540, train_perplexity=578.4607, train_loss=6.3603706

Batch 300550, train_perplexity=526.18085, train_loss=6.265645

Batch 300560, train_perplexity=591.2863, train_loss=6.3823004

Batch 300570, train_perplexity=630.43054, train_loss=6.446403

Batch 300580, train_perplexity=609.2772, train_loss=6.4122734

Batch 300590, train_perplexity=572.2844, train_loss=6.349636

Batch 300600, train_perplexity=513.9534, train_loss=6.2421327

Batch 300610, train_perplexity=574.84436, train_loss=6.3540993

Batch 300620, train_perplexity=548.0523, train_loss=6.3063707

Batch 300630, train_perplexity=656.1711, train_loss=6.4864216

Batch 300640, train_perplexity=609.78296, train_loss=6.413103

Batch 300650, train_perplexity=587.46454, train_loss=6.375816

Batch 300660, train_perplexity=604.7209, train_loss=6.404767

Batch 300670, train_perplexity=587.5741, train_loss=6.3760023

Batch 300680, train_perplexity=607.9006, train_loss=6.4100113
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 300690, train_perplexity=567.70197, train_loss=6.3415966

Batch 300700, train_perplexity=613.6648, train_loss=6.419449

Batch 300710, train_perplexity=565.85046, train_loss=6.33833

Batch 300720, train_perplexity=553.15094, train_loss=6.315631

Batch 300730, train_perplexity=563.7608, train_loss=6.33463

Batch 300740, train_perplexity=561.2098, train_loss=6.330095

Batch 300750, train_perplexity=587.5029, train_loss=6.375881

Batch 300760, train_perplexity=601.08997, train_loss=6.3987446

Batch 300770, train_perplexity=565.33075, train_loss=6.337411

Batch 300780, train_perplexity=661.85284, train_loss=6.4950433

Batch 300790, train_perplexity=527.88226, train_loss=6.268873

Batch 300800, train_perplexity=601.48047, train_loss=6.399394

Batch 300810, train_perplexity=648.881, train_loss=6.4752493

Batch 300820, train_perplexity=595.03705, train_loss=6.3886237

Batch 300830, train_perplexity=582.38025, train_loss=6.3671236

Batch 300840, train_perplexity=616.0255, train_loss=6.4232883

Batch 300850, train_perplexity=597.3167, train_loss=6.3924475

Batch 300860, train_perplexity=550.01056, train_loss=6.3099375

Batch 300870, train_perplexity=565.2501, train_loss=6.3372684

Batch 300880, train_perplexity=590.9802, train_loss=6.3817825

Batch 300890, train_perplexity=577.9542, train_loss=6.3594947

Batch 300900, train_perplexity=589.1915, train_loss=6.3787513

Batch 300910, train_perplexity=593.3855, train_loss=6.385844

Batch 300920, train_perplexity=660.8031, train_loss=6.493456

Batch 300930, train_perplexity=594.06665, train_loss=6.3869915

Batch 300940, train_perplexity=568.76685, train_loss=6.3434706

Batch 300950, train_perplexity=546.08356, train_loss=6.302772

Batch 300960, train_perplexity=649.89325, train_loss=6.476808

Batch 300970, train_perplexity=623.95605, train_loss=6.43608

Batch 300980, train_perplexity=615.48114, train_loss=6.4224043

Batch 300990, train_perplexity=654.37695, train_loss=6.4836836

Batch 301000, train_perplexity=537.533, train_loss=6.28699

Batch 301010, train_perplexity=602.2312, train_loss=6.4006414

Batch 301020, train_perplexity=587.76465, train_loss=6.3763266

Batch 301030, train_perplexity=630.60046, train_loss=6.4466724

Batch 301040, train_perplexity=595.49664, train_loss=6.3893957

Batch 301050, train_perplexity=629.01056, train_loss=6.444148

Batch 301060, train_perplexity=569.7701, train_loss=6.345233

Batch 301070, train_perplexity=579.1203, train_loss=6.3615103

Batch 301080, train_perplexity=629.4231, train_loss=6.4448037

Batch 301090, train_perplexity=593.93463, train_loss=6.3867693

Batch 301100, train_perplexity=586.09937, train_loss=6.3734894

Batch 301110, train_perplexity=522.0836, train_loss=6.2578278

Batch 301120, train_perplexity=620.7854, train_loss=6.4309855

Batch 301130, train_perplexity=556.63934, train_loss=6.3219175

Batch 301140, train_perplexity=559.4445, train_loss=6.3269444

Batch 301150, train_perplexity=642.2648, train_loss=6.4650006

Batch 301160, train_perplexity=566.64453, train_loss=6.339732

Batch 301170, train_perplexity=607.9982, train_loss=6.410172

Batch 301180, train_perplexity=642.45685, train_loss=6.4652996

Batch 301190, train_perplexity=610.04474, train_loss=6.4135323

Batch 301200, train_perplexity=561.22076, train_loss=6.3301144

Batch 301210, train_perplexity=543.39514, train_loss=6.297837

Batch 301220, train_perplexity=624.45404, train_loss=6.4368777

Batch 301230, train_perplexity=535.3138, train_loss=6.282853

Batch 301240, train_perplexity=571.7612, train_loss=6.3487215

Batch 301250, train_perplexity=586.95746, train_loss=6.3749523

Batch 301260, train_perplexity=603.76373, train_loss=6.403183

Batch 301270, train_perplexity=568.3878, train_loss=6.342804

Batch 301280, train_perplexity=644.5268, train_loss=6.4685163

Batch 301290, train_perplexity=658.7707, train_loss=6.4903755

Batch 301300, train_perplexity=658.2899, train_loss=6.4896455

Batch 301310, train_perplexity=620.0328, train_loss=6.4297724

Batch 301320, train_perplexity=664.61346, train_loss=6.4992056

Batch 301330, train_perplexity=588.14453, train_loss=6.3769727

Batch 301340, train_perplexity=539.4866, train_loss=6.290618

Batch 301350, train_perplexity=562.8875, train_loss=6.33308

Batch 301360, train_perplexity=610.75726, train_loss=6.4146996

Batch 301370, train_perplexity=562.59344, train_loss=6.332557

Batch 301380, train_perplexity=609.9476, train_loss=6.413373

Batch 301390, train_perplexity=564.3907, train_loss=6.335747

Batch 301400, train_perplexity=599.4641, train_loss=6.396036

Batch 301410, train_perplexity=577.05237, train_loss=6.357933

Batch 301420, train_perplexity=612.3109, train_loss=6.41724

Batch 301430, train_perplexity=567.2615, train_loss=6.3408203

Batch 301440, train_perplexity=597.1478, train_loss=6.3921647

Batch 301450, train_perplexity=555.44885, train_loss=6.3197765

Batch 301460, train_perplexity=574.7133, train_loss=6.3538713

Batch 301470, train_perplexity=523.5979, train_loss=6.260724

Batch 301480, train_perplexity=523.59143, train_loss=6.2607117

Batch 301490, train_perplexity=567.4346, train_loss=6.3411255

Batch 301500, train_perplexity=533.7656, train_loss=6.279957

Batch 301510, train_perplexity=643.6668, train_loss=6.467181

Batch 301520, train_perplexity=598.0965, train_loss=6.393752

Batch 301530, train_perplexity=563.6121, train_loss=6.3343663

Batch 301540, train_perplexity=634.8672, train_loss=6.453416

Batch 301550, train_perplexity=590.1484, train_loss=6.380374

Batch 301560, train_perplexity=609.8795, train_loss=6.4132614

Batch 301570, train_perplexity=647.85205, train_loss=6.4736624

Batch 301580, train_perplexity=589.57623, train_loss=6.379404

Batch 301590, train_perplexity=563.2258, train_loss=6.3336806

Batch 301600, train_perplexity=597.5004, train_loss=6.392755

Batch 301610, train_perplexity=588.89404, train_loss=6.3782463

Batch 301620, train_perplexity=514.07086, train_loss=6.242361

Batch 301630, train_perplexity=601.8864, train_loss=6.4000688

Batch 301640, train_perplexity=624.8755, train_loss=6.4375525

Batch 301650, train_perplexity=624.8889, train_loss=6.437574

Batch 301660, train_perplexity=693.2416, train_loss=6.5413785

Batch 301670, train_perplexity=574.4366, train_loss=6.3533897

Batch 301680, train_perplexity=536.23047, train_loss=6.284564

Batch 301690, train_perplexity=564.8226, train_loss=6.3365116

Batch 301700, train_perplexity=573.31573, train_loss=6.3514366

Batch 301710, train_perplexity=612.99567, train_loss=6.418358

Batch 301720, train_perplexity=630.2283, train_loss=6.446082

Batch 301730, train_perplexity=591.32916, train_loss=6.382373

Batch 301740, train_perplexity=608.9299, train_loss=6.411703

Batch 301750, train_perplexity=561.97864, train_loss=6.331464

Batch 301760, train_perplexity=615.5709, train_loss=6.42255

Batch 301770, train_perplexity=566.71313, train_loss=6.3398533

Batch 301780, train_perplexity=619.4323, train_loss=6.4288034

Batch 301790, train_perplexity=542.53894, train_loss=6.29626

Batch 301800, train_perplexity=614.2371, train_loss=6.420381

Batch 301810, train_perplexity=583.3964, train_loss=6.368867

Batch 301820, train_perplexity=602.04, train_loss=6.400324

Batch 301830, train_perplexity=502.09247, train_loss=6.2187843

Batch 301840, train_perplexity=550.71466, train_loss=6.311217

Batch 301850, train_perplexity=599.8707, train_loss=6.396714

Batch 301860, train_perplexity=533.91833, train_loss=6.280243

Batch 301870, train_perplexity=518.985, train_loss=6.251875

Batch 301880, train_perplexity=606.8417, train_loss=6.408268

Batch 301890, train_perplexity=597.6757, train_loss=6.3930483

Batch 301900, train_perplexity=583.8982, train_loss=6.3697267

Batch 301910, train_perplexity=623.6039, train_loss=6.4355154

Batch 301920, train_perplexity=530.7935, train_loss=6.274373

Batch 301930, train_perplexity=570.6734, train_loss=6.346817

Batch 301940, train_perplexity=533.0537, train_loss=6.278622

Batch 301950, train_perplexity=630.46576, train_loss=6.446459

Batch 301960, train_perplexity=634.6892, train_loss=6.4531355

Batch 301970, train_perplexity=533.844, train_loss=6.2801037

Batch 301980, train_perplexity=592.5485, train_loss=6.384433

Batch 301990, train_perplexity=615.63464, train_loss=6.4226537
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 302000, train_perplexity=566.978, train_loss=6.3403206

Batch 302010, train_perplexity=532.11304, train_loss=6.276856

Batch 302020, train_perplexity=632.55554, train_loss=6.449768

Batch 302030, train_perplexity=574.8657, train_loss=6.3541365

Batch 302040, train_perplexity=548.6935, train_loss=6.30754

Batch 302050, train_perplexity=578.8078, train_loss=6.3609705

Batch 302060, train_perplexity=584.10815, train_loss=6.370086

Batch 302070, train_perplexity=552.0232, train_loss=6.31359

Batch 302080, train_perplexity=643.1207, train_loss=6.4663324

Batch 302090, train_perplexity=641.44727, train_loss=6.463727

Batch 302100, train_perplexity=523.9681, train_loss=6.2614307

Batch 302110, train_perplexity=586.5361, train_loss=6.374234

Batch 302120, train_perplexity=652.39764, train_loss=6.4806542

Batch 302130, train_perplexity=584.8827, train_loss=6.3714113

Batch 302140, train_perplexity=580.3156, train_loss=6.363572

Batch 302150, train_perplexity=568.46857, train_loss=6.342946

Batch 302160, train_perplexity=593.6634, train_loss=6.3863125

Batch 302170, train_perplexity=611.8194, train_loss=6.416437

Batch 302180, train_perplexity=609.988, train_loss=6.4134393

Batch 302190, train_perplexity=522.28656, train_loss=6.2582164

Batch 302200, train_perplexity=600.05927, train_loss=6.3970284

Batch 302210, train_perplexity=589.1491, train_loss=6.3786793

Batch 302220, train_perplexity=555.7742, train_loss=6.320362

Batch 302230, train_perplexity=606.7746, train_loss=6.4081573

Batch 302240, train_perplexity=651.2622, train_loss=6.4789124

Batch 302250, train_perplexity=513.3477, train_loss=6.2409534

Batch 302260, train_perplexity=612.41656, train_loss=6.4174128

Batch 302270, train_perplexity=552.561, train_loss=6.3145638

Batch 302280, train_perplexity=591.45667, train_loss=6.3825884

Batch 302290, train_perplexity=563.44257, train_loss=6.3340654

Batch 302300, train_perplexity=487.46222, train_loss=6.189213

Batch 302310, train_perplexity=552.53986, train_loss=6.3145256

Batch 302320, train_perplexity=651.3526, train_loss=6.479051

Batch 302330, train_perplexity=534.0678, train_loss=6.280523

Batch 302340, train_perplexity=607.1563, train_loss=6.4087863

Batch 302350, train_perplexity=653.62445, train_loss=6.482533

Batch 302360, train_perplexity=629.82874, train_loss=6.445448

Batch 302370, train_perplexity=565.50006, train_loss=6.3377104

Batch 302380, train_perplexity=599.99835, train_loss=6.396927

Batch 302390, train_perplexity=536.04767, train_loss=6.284223

Batch 302400, train_perplexity=604.5857, train_loss=6.4045434

Batch 302410, train_perplexity=481.58008, train_loss=6.1770725

Batch 302420, train_perplexity=585.59375, train_loss=6.3726263

Batch 302430, train_perplexity=474.4531, train_loss=6.162163

Batch 302440, train_perplexity=578.24725, train_loss=6.3600016

Batch 302450, train_perplexity=629.17554, train_loss=6.4444103

Batch 302460, train_perplexity=600.4042, train_loss=6.397603

Batch 302470, train_perplexity=539.28754, train_loss=6.290249

Batch 302480, train_perplexity=571.2282, train_loss=6.347789

Batch 302490, train_perplexity=571.65576, train_loss=6.348537

Batch 302500, train_perplexity=598.15497, train_loss=6.39385

Batch 302510, train_perplexity=545.92816, train_loss=6.3024874

Batch 302520, train_perplexity=578.25775, train_loss=6.3600197

Batch 302530, train_perplexity=648.8707, train_loss=6.4752336

Batch 302540, train_perplexity=588.12964, train_loss=6.3769474

Batch 302550, train_perplexity=551.23804, train_loss=6.3121667

Batch 302560, train_perplexity=588.27014, train_loss=6.3771863

Batch 302570, train_perplexity=609.27954, train_loss=6.412277

Batch 302580, train_perplexity=627.69434, train_loss=6.4420533

Batch 302590, train_perplexity=557.6247, train_loss=6.323686

Batch 302600, train_perplexity=604.5658, train_loss=6.4045105

Batch 302610, train_perplexity=599.24176, train_loss=6.395665

Batch 302620, train_perplexity=534.49915, train_loss=6.28133

Batch 302630, train_perplexity=593.5901, train_loss=6.386189

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00068-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00068-of-00100
Loaded 306324 sentences.
Finished loading
Batch 302640, train_perplexity=598.01495, train_loss=6.3936157

Batch 302650, train_perplexity=618.4094, train_loss=6.4271507

Batch 302660, train_perplexity=619.6834, train_loss=6.4292088

Batch 302670, train_perplexity=609.74225, train_loss=6.4130363

Batch 302680, train_perplexity=563.283, train_loss=6.333782

Batch 302690, train_perplexity=557.93054, train_loss=6.3242345

Batch 302700, train_perplexity=577.2792, train_loss=6.358326

Batch 302710, train_perplexity=578.23236, train_loss=6.359976

Batch 302720, train_perplexity=562.6747, train_loss=6.3327017

Batch 302730, train_perplexity=578.1466, train_loss=6.3598275

Batch 302740, train_perplexity=599.8676, train_loss=6.396709

Batch 302750, train_perplexity=555.7848, train_loss=6.320381

Batch 302760, train_perplexity=602.7943, train_loss=6.401576

Batch 302770, train_perplexity=558.1192, train_loss=6.3245726

Batch 302780, train_perplexity=605.61664, train_loss=6.406247

Batch 302790, train_perplexity=661.9504, train_loss=6.4951906

Batch 302800, train_perplexity=641.9681, train_loss=6.4645386

Batch 302810, train_perplexity=674.0461, train_loss=6.5132985

Batch 302820, train_perplexity=561.3551, train_loss=6.3303537

Batch 302830, train_perplexity=562.25525, train_loss=6.331956

Batch 302840, train_perplexity=532.7061, train_loss=6.27797

Batch 302850, train_perplexity=642.6633, train_loss=6.465621

Batch 302860, train_perplexity=588.7071, train_loss=6.3779287

Batch 302870, train_perplexity=553.01697, train_loss=6.3153887

Batch 302880, train_perplexity=612.65845, train_loss=6.4178076

Batch 302890, train_perplexity=629.1534, train_loss=6.444375

Batch 302900, train_perplexity=648.96515, train_loss=6.475379

Batch 302910, train_perplexity=605.2009, train_loss=6.4055605

Batch 302920, train_perplexity=594.73444, train_loss=6.388115

Batch 302930, train_perplexity=558.2358, train_loss=6.3247814

Batch 302940, train_perplexity=552.8524, train_loss=6.315091

Batch 302950, train_perplexity=626.1049, train_loss=6.439518

Batch 302960, train_perplexity=581.5533, train_loss=6.3657026

Batch 302970, train_perplexity=612.0645, train_loss=6.4168377

Batch 302980, train_perplexity=625.3894, train_loss=6.4383745

Batch 302990, train_perplexity=579.0389, train_loss=6.3613696

Batch 303000, train_perplexity=626.7812, train_loss=6.4405975

Batch 303010, train_perplexity=602.1037, train_loss=6.4004297

Batch 303020, train_perplexity=558.9965, train_loss=6.3261433

Batch 303030, train_perplexity=639.8655, train_loss=6.461258

Batch 303040, train_perplexity=678.76056, train_loss=6.5202684

Batch 303050, train_perplexity=596.28485, train_loss=6.3907185

Batch 303060, train_perplexity=591.57965, train_loss=6.3827963

Batch 303070, train_perplexity=623.24866, train_loss=6.4349456

Batch 303080, train_perplexity=584.08, train_loss=6.370038

Batch 303090, train_perplexity=607.83997, train_loss=6.4099116

Batch 303100, train_perplexity=568.22577, train_loss=6.342519

Batch 303110, train_perplexity=640.8157, train_loss=6.462742

Batch 303120, train_perplexity=549.76276, train_loss=6.309487

Batch 303130, train_perplexity=683.4122, train_loss=6.527098

Batch 303140, train_perplexity=637.57086, train_loss=6.4576654

Batch 303150, train_perplexity=597.80536, train_loss=6.3932652

Batch 303160, train_perplexity=631.0366, train_loss=6.447364

Batch 303170, train_perplexity=600.7275, train_loss=6.3981414

Batch 303180, train_perplexity=594.8819, train_loss=6.388363

Batch 303190, train_perplexity=561.2502, train_loss=6.330167

Batch 303200, train_perplexity=606.8411, train_loss=6.408267

Batch 303210, train_perplexity=535.3837, train_loss=6.282984

Batch 303220, train_perplexity=581.11365, train_loss=6.3649464

Batch 303230, train_perplexity=502.11212, train_loss=6.2188234
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 303240, train_perplexity=664.6147, train_loss=6.4992075

Batch 303250, train_perplexity=558.17084, train_loss=6.324665

Batch 303260, train_perplexity=552.60126, train_loss=6.3146367

Batch 303270, train_perplexity=641.258, train_loss=6.463432

Batch 303280, train_perplexity=601.016, train_loss=6.3986216

Batch 303290, train_perplexity=547.532, train_loss=6.305421

Batch 303300, train_perplexity=530.7254, train_loss=6.274245

Batch 303310, train_perplexity=565.95514, train_loss=6.338515

Batch 303320, train_perplexity=609.69604, train_loss=6.4129605

Batch 303330, train_perplexity=585.5239, train_loss=6.372507

Batch 303340, train_perplexity=539.86743, train_loss=6.2913237

Batch 303350, train_perplexity=658.7732, train_loss=6.4903793

Batch 303360, train_perplexity=670.6647, train_loss=6.5082693

Batch 303370, train_perplexity=609.46204, train_loss=6.4125767

Batch 303380, train_perplexity=566.4811, train_loss=6.3394437

Batch 303390, train_perplexity=516.2389, train_loss=6.2465696

Batch 303400, train_perplexity=606.139, train_loss=6.4071093

Batch 303410, train_perplexity=617.83673, train_loss=6.426224

Batch 303420, train_perplexity=618.91534, train_loss=6.4279685

Batch 303430, train_perplexity=591.365, train_loss=6.3824334

Batch 303440, train_perplexity=540.2378, train_loss=6.2920094

Batch 303450, train_perplexity=616.93854, train_loss=6.4247694

Batch 303460, train_perplexity=534.93567, train_loss=6.2821465

Batch 303470, train_perplexity=636.99927, train_loss=6.4567685

Batch 303480, train_perplexity=564.112, train_loss=6.335253

Batch 303490, train_perplexity=598.13385, train_loss=6.3938146

Batch 303500, train_perplexity=596.5209, train_loss=6.391114

Batch 303510, train_perplexity=568.0816, train_loss=6.342265

Batch 303520, train_perplexity=563.48236, train_loss=6.334136

Batch 303530, train_perplexity=547.5837, train_loss=6.3055153

Batch 303540, train_perplexity=560.2513, train_loss=6.3283854

Batch 303550, train_perplexity=594.57025, train_loss=6.387839

Batch 303560, train_perplexity=611.64294, train_loss=6.4161487

Batch 303570, train_perplexity=576.02527, train_loss=6.3561516

Batch 303580, train_perplexity=562.49817, train_loss=6.332388

Batch 303590, train_perplexity=508.5185, train_loss=6.2315016

Batch 303600, train_perplexity=547.6066, train_loss=6.3055573

Batch 303610, train_perplexity=586.1531, train_loss=6.373581

Batch 303620, train_perplexity=573.9364, train_loss=6.3525186

Batch 303630, train_perplexity=539.2356, train_loss=6.2901525

Batch 303640, train_perplexity=500.65137, train_loss=6.21591

Batch 303650, train_perplexity=644.48065, train_loss=6.468445

Batch 303660, train_perplexity=625.7041, train_loss=6.4388776

Batch 303670, train_perplexity=626.86194, train_loss=6.4407263

Batch 303680, train_perplexity=622.1219, train_loss=6.433136

Batch 303690, train_perplexity=641.02045, train_loss=6.4630613

Batch 303700, train_perplexity=561.08325, train_loss=6.3298693

Batch 303710, train_perplexity=529.67145, train_loss=6.272257

Batch 303720, train_perplexity=550.2671, train_loss=6.310404

Batch 303730, train_perplexity=611.2222, train_loss=6.4154606

Batch 303740, train_perplexity=615.60675, train_loss=6.4226084

Batch 303750, train_perplexity=577.5206, train_loss=6.358744

Batch 303760, train_perplexity=548.75336, train_loss=6.307649

Batch 303770, train_perplexity=537.6786, train_loss=6.287261

Batch 303780, train_perplexity=600.8229, train_loss=6.3983

Batch 303790, train_perplexity=581.7497, train_loss=6.36604

Batch 303800, train_perplexity=620.76825, train_loss=6.430958

Batch 303810, train_perplexity=617.5849, train_loss=6.4258165

Batch 303820, train_perplexity=546.8296, train_loss=6.304137

Batch 303830, train_perplexity=547.2044, train_loss=6.3048224

Batch 303840, train_perplexity=520.3764, train_loss=6.2545524

Batch 303850, train_perplexity=537.42944, train_loss=6.2867975

Batch 303860, train_perplexity=508.1949, train_loss=6.230865

Batch 303870, train_perplexity=499.66208, train_loss=6.213932

Batch 303880, train_perplexity=552.6126, train_loss=6.314657

Batch 303890, train_perplexity=615.0657, train_loss=6.421729

Batch 303900, train_perplexity=577.06396, train_loss=6.357953

Batch 303910, train_perplexity=559.13464, train_loss=6.3263903

Batch 303920, train_perplexity=581.7641, train_loss=6.366065

Batch 303930, train_perplexity=598.33466, train_loss=6.3941503

Batch 303940, train_perplexity=570.91724, train_loss=6.3472443

Batch 303950, train_perplexity=594.1145, train_loss=6.387072

Batch 303960, train_perplexity=601.951, train_loss=6.400176

Batch 303970, train_perplexity=510.29996, train_loss=6.2349987

Batch 303980, train_perplexity=612.9457, train_loss=6.4182763

Batch 303990, train_perplexity=636.0186, train_loss=6.455228

Batch 304000, train_perplexity=534.18445, train_loss=6.280741

Batch 304010, train_perplexity=570.1664, train_loss=6.345928

Batch 304020, train_perplexity=548.93317, train_loss=6.3079767

Batch 304030, train_perplexity=601.9292, train_loss=6.40014

Batch 304040, train_perplexity=589.20105, train_loss=6.3787675

Batch 304050, train_perplexity=604.96, train_loss=6.4051623

Batch 304060, train_perplexity=630.4405, train_loss=6.446419

Batch 304070, train_perplexity=578.23236, train_loss=6.359976

Batch 304080, train_perplexity=582.08044, train_loss=6.3666086

Batch 304090, train_perplexity=611.0031, train_loss=6.415102

Batch 304100, train_perplexity=572.31604, train_loss=6.3496914

Batch 304110, train_perplexity=611.71674, train_loss=6.4162693

Batch 304120, train_perplexity=549.2125, train_loss=6.3084855

Batch 304130, train_perplexity=587.26794, train_loss=6.375481

Batch 304140, train_perplexity=564.5937, train_loss=6.3361063

Batch 304150, train_perplexity=568.603, train_loss=6.3431826

Batch 304160, train_perplexity=583.0866, train_loss=6.3683357

Batch 304170, train_perplexity=489.48447, train_loss=6.1933527

Batch 304180, train_perplexity=543.0952, train_loss=6.2972846

Batch 304190, train_perplexity=591.3901, train_loss=6.382476

Batch 304200, train_perplexity=571.44946, train_loss=6.348176

Batch 304210, train_perplexity=682.1405, train_loss=6.5252357

Batch 304220, train_perplexity=553.0808, train_loss=6.315504

Batch 304230, train_perplexity=603.9858, train_loss=6.4035506

Batch 304240, train_perplexity=628.39453, train_loss=6.443168

Batch 304250, train_perplexity=583.43036, train_loss=6.368925

Batch 304260, train_perplexity=575.7534, train_loss=6.3556795

Batch 304270, train_perplexity=540.60474, train_loss=6.2926884

Batch 304280, train_perplexity=616.64087, train_loss=6.424287

Batch 304290, train_perplexity=587.9272, train_loss=6.376603

Batch 304300, train_perplexity=568.38617, train_loss=6.342801

Batch 304310, train_perplexity=586.6788, train_loss=6.3744774

Batch 304320, train_perplexity=575.5292, train_loss=6.35529

Batch 304330, train_perplexity=655.38184, train_loss=6.485218

Batch 304340, train_perplexity=588.34, train_loss=6.377305

Batch 304350, train_perplexity=549.5822, train_loss=6.3091583

Batch 304360, train_perplexity=568.12744, train_loss=6.3423457

Batch 304370, train_perplexity=567.38184, train_loss=6.3410325

Batch 304380, train_perplexity=533.4104, train_loss=6.279291

Batch 304390, train_perplexity=579.5297, train_loss=6.362217

Batch 304400, train_perplexity=554.66437, train_loss=6.318363

Batch 304410, train_perplexity=586.5134, train_loss=6.3741956

Batch 304420, train_perplexity=559.95, train_loss=6.3278475

Batch 304430, train_perplexity=564.8207, train_loss=6.3365083

Batch 304440, train_perplexity=588.8873, train_loss=6.378235

Batch 304450, train_perplexity=597.407, train_loss=6.3925986

Batch 304460, train_perplexity=581.189, train_loss=6.365076

Batch 304470, train_perplexity=633.62274, train_loss=6.4514537

Batch 304480, train_perplexity=556.2647, train_loss=6.3212442

Batch 304490, train_perplexity=535.4644, train_loss=6.2831345

Batch 304500, train_perplexity=603.9757, train_loss=6.403534

Batch 304510, train_perplexity=568.9887, train_loss=6.3438606

Batch 304520, train_perplexity=554.00964, train_loss=6.317182

Batch 304530, train_perplexity=610.8321, train_loss=6.414822

Batch 304540, train_perplexity=599.88104, train_loss=6.3967314
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

nohup: ignoring input
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-29 06:20:17.758520: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-29 06:20:17.954069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0b:00.0
totalMemory: 22.38GiB freeMemory: 21.32GiB
2020-10-29 06:20:17.954148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-29 06:20:18.550975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 06:20:18.551031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-29 06:20:18.551042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-29 06:20:18.552422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20683 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:0b:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 51 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Found 51 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/add_39:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(1, 512) dtype=float32>)
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/add_39:0' shape=(1, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(1, 512) dtype=float32>)
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 384324440 batches
Batch 0, train_perplexity=150255.95, train_loss=11.920095

Batch 10, train_perplexity=104785.375, train_loss=11.5596695

Batch 20, train_perplexity=63420.945, train_loss=11.057549

Batch 30, train_perplexity=30812.162, train_loss=10.335665

Batch 40, train_perplexity=11527.134, train_loss=9.352459

Batch 50, train_perplexity=3129.7485, train_loss=8.048708

Batch 60, train_perplexity=691.74475, train_loss=6.539217

Batch 70, train_perplexity=154.04779, train_loss=5.037263

Batch 80, train_perplexity=56.4042, train_loss=4.0325437

Batch 90, train_perplexity=38.00683, train_loss=3.637766

Batch 100, train_perplexity=30.3319, train_loss=3.4122

Batch 110, train_perplexity=25.41677, train_loss=3.2354093

Batch 120, train_perplexity=22.341316, train_loss=3.1064377

Batch 130, train_perplexity=20.630579, train_loss=3.0267744

Batch 140, train_perplexity=19.732412, train_loss=2.9822626

Batch 150, train_perplexity=19.263432, train_loss=2.9582086

Batch 160, train_perplexity=18.9715, train_loss=2.9429379

Batch 170, train_perplexity=18.752401, train_loss=2.9313219

Batch 180, train_perplexity=18.597853, train_loss=2.923046

Batch 190, train_perplexity=18.473671, train_loss=2.9163465

Batch 200, train_perplexity=18.350101, train_loss=2.909635

Batch 210, train_perplexity=18.237442, train_loss=2.9034767

Batch 220, train_perplexity=18.145498, train_loss=2.8984225

Batch 230, train_perplexity=18.053553, train_loss=2.8933425

Batch 240, train_perplexity=17.960623, train_loss=2.8881817

Batch 250, train_perplexity=17.84941, train_loss=2.8819704

Batch 260, train_perplexity=17.792908, train_loss=2.8788

Batch 270, train_perplexity=17.695896, train_loss=2.8733327

Batch 280, train_perplexity=17.599459, train_loss=2.8678682

Batch 290, train_perplexity=17.511309, train_loss=2.8628469

Batch 300, train_perplexity=17.410322, train_loss=2.8570633

Batch 310, train_perplexity=17.307241, train_loss=2.851125

Batch 320, train_perplexity=17.219988, train_loss=2.8460708

Batch 330, train_perplexity=17.085777, train_loss=2.8382463

Batch 340, train_perplexity=16.988153, train_loss=2.8325162

Batch 350, train_perplexity=16.85253, train_loss=2.8245008

Batch 360, train_perplexity=16.7081, train_loss=2.8158937

Batch 370, train_perplexity=16.553677, train_loss=2.8066082

Batch 380, train_perplexity=16.401575, train_loss=2.7973773

Batch 390, train_perplexity=16.229212, train_loss=2.7868128

Batch 400, train_perplexity=16.049067, train_loss=2.7756507

Batch 410, train_perplexity=15.861572, train_loss=2.7638993

Batch 420, train_perplexity=15.664726, train_loss=2.7514114

Batch 430, train_perplexity=15.456446, train_loss=2.7380261

Batch 440, train_perplexity=15.236056, train_loss=2.7236648

Batch 450, train_perplexity=15.00161, train_loss=2.7081575

Batch 460, train_perplexity=14.837951, train_loss=2.6971881

Batch 470, train_perplexity=14.512505, train_loss=2.6750107

Batch 480, train_perplexity=14.257795, train_loss=2.6573038

Batch 490, train_perplexity=14.057429, train_loss=2.643151

Batch 500, train_perplexity=13.894232, train_loss=2.6314738

Batch 510, train_perplexity=13.5000105, train_loss=2.6026905

Batch 520, train_perplexity=13.301678, train_loss=2.5878901

Batch 530, train_perplexity=12.925223, train_loss=2.5591807

Batch 540, train_perplexity=12.363488, train_loss=2.5147476

Batch 550, train_perplexity=11.99903, train_loss=2.4848258

Batch 560, train_perplexity=11.633265, train_loss=2.4538686

Batch 570, train_perplexity=11.223641, train_loss=2.4180224

Batch 580, train_perplexity=11.060547, train_loss=2.4033844

Batch 590, train_perplexity=10.496324, train_loss=2.351025

Batch 600, train_perplexity=10.097413, train_loss=2.3122792

Batch 610, train_perplexity=9.504785, train_loss=2.2517953

Batch 620, train_perplexity=9.957353, train_loss=2.2983112

Batch 630, train_perplexity=8.839014, train_loss=2.1791754

Batch 640, train_perplexity=8.436533, train_loss=2.1325715

Batch 650, train_perplexity=7.8593125, train_loss=2.0616992

Batch 660, train_perplexity=7.8065424, train_loss=2.0549622

Batch 670, train_perplexity=7.5597177, train_loss=2.0228338

Batch 680, train_perplexity=6.747275, train_loss=1.9091387

Batch 690, train_perplexity=6.2439027, train_loss=1.8316054

Batch 700, train_perplexity=6.675814, train_loss=1.8984911

Batch 710, train_perplexity=5.7165713, train_loss=1.7433692

Batch 720, train_perplexity=5.389606, train_loss=1.6844723

Batch 730, train_perplexity=5.273048, train_loss=1.6626085

Batch 740, train_perplexity=4.9003963, train_loss=1.5893161

Batch 750, train_perplexity=4.6034517, train_loss=1.5268064

Batch 760, train_perplexity=4.4238653, train_loss=1.4870138

Batch 770, train_perplexity=4.2491755, train_loss=1.446725

Batch 780, train_perplexity=4.023435, train_loss=1.3921361

Batch 790, train_perplexity=3.8236718, train_loss=1.3412112

Batch 800, train_perplexity=3.5672953, train_loss=1.2718077

Batch 810, train_perplexity=3.3442965, train_loss=1.2072563

Batch 820, train_perplexity=3.3395166, train_loss=1.205826

Batch 830, train_perplexity=3.141539, train_loss=1.1447128

Batch 840, train_perplexity=2.9990947, train_loss=1.0983105

Batch 850, train_perplexity=2.9031255, train_loss=1.0657879

Batch 860, train_perplexity=2.7254403, train_loss=1.00263

Batch 870, train_perplexity=2.606474, train_loss=0.9579983
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 880, train_perplexity=2.526419, train_loss=0.9268028

Batch 890, train_perplexity=2.3837, train_loss=0.8686539

Batch 900, train_perplexity=2.4193535, train_loss=0.8835004

Batch 910, train_perplexity=2.3217943, train_loss=0.84234023

Batch 920, train_perplexity=2.1585639, train_loss=0.76944315

Batch 930, train_perplexity=2.112355, train_loss=0.74780345

Batch 940, train_perplexity=2.0437515, train_loss=0.71478707

Batch 950, train_perplexity=2.0078897, train_loss=0.6970843

Batch 960, train_perplexity=1.9235511, train_loss=0.654173

Batch 970, train_perplexity=1.8942674, train_loss=0.6388322

Batch 980, train_perplexity=1.819801, train_loss=0.5987271

Batch 990, train_perplexity=1.7922016, train_loss=0.58344483

Batch 1000, train_perplexity=1.7339762, train_loss=0.5504172

Batch 1010, train_perplexity=1.691671, train_loss=0.52571684

Batch 1020, train_perplexity=1.6561648, train_loss=0.50450456

Batch 1030, train_perplexity=1.6172259, train_loss=0.4807123

Batch 1040, train_perplexity=1.5861901, train_loss=0.46133497

Batch 1050, train_perplexity=1.5560515, train_loss=0.44215155

Batch 1060, train_perplexity=1.5286462, train_loss=0.4243825

Batch 1070, train_perplexity=1.5049406, train_loss=0.40875348

Batch 1080, train_perplexity=1.4863178, train_loss=0.39630172

Batch 1090, train_perplexity=1.4571093, train_loss=0.3764546

Batch 1100, train_perplexity=1.4392947, train_loss=0.36415324

Batch 1110, train_perplexity=1.4384109, train_loss=0.36353892

Batch 1120, train_perplexity=1.4023833, train_loss=0.33817315

Batch 1130, train_perplexity=1.3855146, train_loss=0.32607165

Batch 1140, train_perplexity=1.3705292, train_loss=0.31519693

Batch 1150, train_perplexity=1.3567994, train_loss=0.3051285

Batch 1160, train_perplexity=1.3434652, train_loss=0.29525223

Batch 1170, train_perplexity=1.3306135, train_loss=0.28564012

Batch 1180, train_perplexity=1.3188055, train_loss=0.27672637

Batch 1190, train_perplexity=1.3078115, train_loss=0.26835513

Batch 1200, train_perplexity=1.297066, train_loss=0.26010478

Batch 1210, train_perplexity=1.28682, train_loss=0.25217408

Batch 1220, train_perplexity=1.2775803, train_loss=0.24496785

Batch 1230, train_perplexity=1.2687546, train_loss=0.2380358

Batch 1240, train_perplexity=1.2596912, train_loss=0.23086664

Batch 1250, train_perplexity=1.251852, train_loss=0.2246241

Batch 1260, train_perplexity=1.2432644, train_loss=0.21774057

Batch 1270, train_perplexity=1.2361407, train_loss=0.2119942

Batch 1280, train_perplexity=1.2285861, train_loss=0.20586401

Batch 1290, train_perplexity=1.221926, train_loss=0.20042823

Batch 1300, train_perplexity=1.2152314, train_loss=0.19493449

Batch 1310, train_perplexity=1.2088972, train_loss=0.18970856

Batch 1320, train_perplexity=1.2032455, train_loss=0.18502253

Batch 1330, train_perplexity=1.1970507, train_loss=0.17986077

Batch 1340, train_perplexity=1.1916493, train_loss=0.1753383

Batch 1350, train_perplexity=1.186143, train_loss=0.17070687

Batch 1360, train_perplexity=1.1810296, train_loss=0.16638651

Batch 1370, train_perplexity=1.1763024, train_loss=0.16237602

Batch 1380, train_perplexity=1.1714115, train_loss=0.15820944

Batch 1390, train_perplexity=1.1676918, train_loss=0.155029

Batch 1400, train_perplexity=1.1629353, train_loss=0.15094717

Batch 1410, train_perplexity=1.1591117, train_loss=0.14765394

Batch 1420, train_perplexity=1.155085, train_loss=0.1441739

Batch 1430, train_perplexity=1.1510949, train_loss=0.1407136

Batch 1440, train_perplexity=1.1480132, train_loss=0.1380328

Batch 1450, train_perplexity=1.1441711, train_loss=0.13468044

Batch 1460, train_perplexity=1.1406667, train_loss=0.1316129

Batch 1470, train_perplexity=1.1380059, train_loss=0.12927748

Batch 1480, train_perplexity=1.134513, train_loss=0.12620355

Batch 1490, train_perplexity=1.1313825, train_loss=0.12344034

Batch 1500, train_perplexity=1.1290286, train_loss=0.121357575

Batch 1510, train_perplexity=1.1259205, train_loss=0.11860095

Batch 1520, train_perplexity=1.1231462, train_loss=0.116133854

Batch 1530, train_perplexity=1.1210033, train_loss=0.114224076

Batch 1540, train_perplexity=1.1183513, train_loss=0.11185562

Batch 1550, train_perplexity=1.1157895, train_loss=0.10956226

Batch 1560, train_perplexity=1.1134384, train_loss=0.10745287

Batch 1570, train_perplexity=1.1115147, train_loss=0.105723694

Batch 1580, train_perplexity=1.1090181, train_loss=0.103474975

Batch 1590, train_perplexity=1.106802, train_loss=0.10147475

Batch 1600, train_perplexity=1.1048396, train_loss=0.09970009

Batch 1610, train_perplexity=1.103093, train_loss=0.09811804

Batch 1620, train_perplexity=1.1009016, train_loss=0.09612952

Batch 1630, train_perplexity=1.0989658, train_loss=0.09436953

Batch 1640, train_perplexity=1.0971075, train_loss=0.09267719

Batch 1650, train_perplexity=1.0956491, train_loss=0.09134698

Batch 1660, train_perplexity=1.0938396, train_loss=0.08969413

Batch 1670, train_perplexity=1.092047, train_loss=0.0880539

Batch 1680, train_perplexity=1.0903707, train_loss=0.08651769

Batch 1690, train_perplexity=1.0888256, train_loss=0.08509967

Batch 1700, train_perplexity=1.0875436, train_loss=0.08392157

Batch 1710, train_perplexity=1.0860187, train_loss=0.082518466

Batch 1720, train_perplexity=1.0844434, train_loss=0.08106694

Batch 1730, train_perplexity=1.0830208, train_loss=0.079754174

Batch 1740, train_perplexity=1.0815954, train_loss=0.07843721

Batch 1750, train_perplexity=1.0803643, train_loss=0.07729831

Batch 1760, train_perplexity=1.0792401, train_loss=0.076257125

Batch 1770, train_perplexity=1.0779653, train_loss=0.075075276

Batch 1780, train_perplexity=1.0766342, train_loss=0.073839664

Batch 1790, train_perplexity=1.0754155, train_loss=0.07270706

Batch 1800, train_perplexity=1.0742344, train_loss=0.071608156

Batch 1810, train_perplexity=1.0730681, train_loss=0.07052201

Batch 1820, train_perplexity=1.072016, train_loss=0.06954099

Batch 1830, train_perplexity=1.0710591, train_loss=0.06864798

Batch 1840, train_perplexity=1.070073, train_loss=0.067726925

Batch 1850, train_perplexity=1.0690243, train_loss=0.0667464

Batch 1860, train_perplexity=1.0679668, train_loss=0.065756634

Batch 1870, train_perplexity=1.0669632, train_loss=0.06481652

Batch 1880, train_perplexity=1.0660098, train_loss=0.0639225

Batch 1890, train_perplexity=1.0650774, train_loss=0.06304754

Batch 1900, train_perplexity=1.0641574, train_loss=0.062183227

Batch 1910, train_perplexity=1.0632657, train_loss=0.061345026

Batch 1920, train_perplexity=1.062418, train_loss=0.060547397

Batch 1930, train_perplexity=1.0616134, train_loss=0.059789903

Batch 1940, train_perplexity=1.0608369, train_loss=0.059058167

Batch 1950, train_perplexity=1.0600715, train_loss=0.05833637

Batch 1960, train_perplexity=1.0593096, train_loss=0.05761741

Batch 1970, train_perplexity=1.0585511, train_loss=0.05690101

Batch 1980, train_perplexity=1.0577989, train_loss=0.056190223

Batch 1990, train_perplexity=1.0570573, train_loss=0.05548884

Batch 2000, train_perplexity=1.0563296, train_loss=0.05480028

Batch 2010, train_perplexity=1.0556186, train_loss=0.054127015

Batch 2020, train_perplexity=1.054925, train_loss=0.05346968

Batch 2030, train_perplexity=1.0542485, train_loss=0.052828133

Batch 2040, train_perplexity=1.0535884, train_loss=0.052201807

Batch 2050, train_perplexity=1.0529438, train_loss=0.05158994

Batch 2060, train_perplexity=1.0523143, train_loss=0.050991856

Batch 2070, train_perplexity=1.051699, train_loss=0.050406948

Batch 2080, train_perplexity=1.0510974, train_loss=0.049834777

Batch 2090, train_perplexity=1.0505091, train_loss=0.049274873

Batch 2100, train_perplexity=1.0499336, train_loss=0.048726935

Batch 2110, train_perplexity=1.0493706, train_loss=0.048190612

Batch 2120, train_perplexity=1.0488195, train_loss=0.047665242

Batch 2130, train_perplexity=1.0482796, train_loss=0.047150403

Batch 2140, train_perplexity=1.0477498, train_loss=0.046644777

Batch 2150, train_perplexity=1.0472285, train_loss=0.046147116

Batch 2160, train_perplexity=1.0467144, train_loss=0.045656092

Batch 2170, train_perplexity=1.0462059, train_loss=0.045170132

Batch 2180, train_perplexity=1.0457027, train_loss=0.044689044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 2190, train_perplexity=1.0452056, train_loss=0.044213567

Batch 2200, train_perplexity=1.0447172, train_loss=0.043746233

Batch 2210, train_perplexity=1.0442408, train_loss=0.04329016

Batch 2220, train_perplexity=1.0437785, train_loss=0.04284732

Batch 2230, train_perplexity=1.04333, train_loss=0.042417422

Batch 2240, train_perplexity=1.0428923, train_loss=0.041997977

Batch 2250, train_perplexity=1.0424633, train_loss=0.04158649

Batch 2260, train_perplexity=1.0420426, train_loss=0.04118287

Batch 2270, train_perplexity=1.0416315, train_loss=0.040788155

Batch 2280, train_perplexity=1.0412285, train_loss=0.040401336

Batch 2290, train_perplexity=1.0408285, train_loss=0.04001704

Batch 2300, train_perplexity=1.040427, train_loss=0.03963121

Batch 2310, train_perplexity=1.0400264, train_loss=0.039246082

Batch 2320, train_perplexity=1.0396345, train_loss=0.03886921

Batch 2330, train_perplexity=1.0392562, train_loss=0.038505234

Batch 2340, train_perplexity=1.0388889, train_loss=0.03815178

Batch 2350, train_perplexity=1.0385298, train_loss=0.037806

Batch 2360, train_perplexity=1.0381778, train_loss=0.037467152

Batch 2370, train_perplexity=1.0378294, train_loss=0.03713142

Batch 2380, train_perplexity=1.0374795, train_loss=0.036794223

Batch 2390, train_perplexity=1.0371315, train_loss=0.036458783

Batch 2400, train_perplexity=1.0367931, train_loss=0.036132433

Batch 2410, train_perplexity=1.036465, train_loss=0.03581588

Batch 2420, train_perplexity=1.0361444, train_loss=0.0355065

Batch 2430, train_perplexity=1.0358288, train_loss=0.035201922

Batch 2440, train_perplexity=1.0355139, train_loss=0.03489783

Batch 2450, train_perplexity=1.0352002, train_loss=0.03459483

Batch 2460, train_perplexity=1.0348934, train_loss=0.03429839

Batch 2470, train_perplexity=1.0345948, train_loss=0.034009837

Batch 2480, train_perplexity=1.0343022, train_loss=0.033727042

Batch 2490, train_perplexity=1.0340126, train_loss=0.03344689

Batch 2500, train_perplexity=1.0337242, train_loss=0.033167996

Batch 2510, train_perplexity=1.0334402, train_loss=0.032893285

Batch 2520, train_perplexity=1.033163, train_loss=0.032624938

Batch 2530, train_perplexity=1.0328909, train_loss=0.03236153

Batch 2540, train_perplexity=1.0326215, train_loss=0.03210072

Batch 2550, train_perplexity=1.0323545, train_loss=0.031842142

Batch 2560, train_perplexity=1.0320921, train_loss=0.03158786

Batch 2570, train_perplexity=1.0318347, train_loss=0.0313385

Batch 2580, train_perplexity=1.0315809, train_loss=0.031092547

Batch 2590, train_perplexity=1.0313299, train_loss=0.030849062

Batch 2600, train_perplexity=1.0310822, train_loss=0.030608865

Batch 2610, train_perplexity=1.0308388, train_loss=0.03037291

Batch 2620, train_perplexity=1.030599, train_loss=0.030140195

Batch 2630, train_perplexity=1.0303619, train_loss=0.029910136

Batch 2640, train_perplexity=1.0301281, train_loss=0.029683162

Batch 2650, train_perplexity=1.0298979, train_loss=0.029459734

Batch 2660, train_perplexity=1.0296708, train_loss=0.029239226

Batch 2670, train_perplexity=1.0294466, train_loss=0.029021427

Batch 2680, train_perplexity=1.0292256, train_loss=0.028806688

Batch 2690, train_perplexity=1.0290077, train_loss=0.028594859

Batch 2700, train_perplexity=1.0287925, train_loss=0.028385751

Batch 2710, train_perplexity=1.0285801, train_loss=0.028179308

Batch 2720, train_perplexity=1.0283707, train_loss=0.027975714

Batch 2730, train_perplexity=1.028164, train_loss=0.027774658

Batch 2740, train_perplexity=1.02796, train_loss=0.027576206

Batch 2750, train_perplexity=1.0277586, train_loss=0.027380275

Batch 2760, train_perplexity=1.02756, train_loss=0.027187008

Batch 2770, train_perplexity=1.0273638, train_loss=0.026996024

Batch 2780, train_perplexity=1.0271701, train_loss=0.026807485

Batch 2790, train_perplexity=1.0269789, train_loss=0.0266213

Batch 2800, train_perplexity=1.02679, train_loss=0.026437482

Batch 2810, train_perplexity=1.0266036, train_loss=0.026255913

Batch 2820, train_perplexity=1.0264196, train_loss=0.026076665

Batch 2830, train_perplexity=1.0262378, train_loss=0.025899585

Batch 2840, train_perplexity=1.0260583, train_loss=0.025724623

Batch 2850, train_perplexity=1.025881, train_loss=0.025551785

Batch 2860, train_perplexity=1.0257059, train_loss=0.02538107

Batch 2870, train_perplexity=1.025533, train_loss=0.0252124

Batch 2880, train_perplexity=1.025362, train_loss=0.025045738

Batch 2890, train_perplexity=1.0251932, train_loss=0.02488109

Batch 2900, train_perplexity=1.0250264, train_loss=0.024718372

Batch 2910, train_perplexity=1.0248616, train_loss=0.024557557

Batch 2920, train_perplexity=1.0246987, train_loss=0.02439866

Batch 2930, train_perplexity=1.0245378, train_loss=0.024241578

Batch 2940, train_perplexity=1.0243789, train_loss=0.024086418

Batch 2950, train_perplexity=1.0242217, train_loss=0.023932977

Batch 2960, train_perplexity=1.0240662, train_loss=0.023781236

Batch 2970, train_perplexity=1.0239128, train_loss=0.023631312

Batch 2980, train_perplexity=1.0237609, train_loss=0.02348305

Batch 2990, train_perplexity=1.023611, train_loss=0.02333647

Batch 3000, train_perplexity=1.0234627, train_loss=0.023191603

Batch 3010, train_perplexity=1.023316, train_loss=0.02304833

Batch 3020, train_perplexity=1.0231711, train_loss=0.022906676

Batch 3030, train_perplexity=1.0230277, train_loss=0.022766553

Batch 3040, train_perplexity=1.0228859, train_loss=0.022627983

Batch 3050, train_perplexity=1.0227457, train_loss=0.022490893

Batch 3060, train_perplexity=1.0226072, train_loss=0.022355415

Batch 3070, train_perplexity=1.02247, train_loss=0.022221228

Batch 3080, train_perplexity=1.0223345, train_loss=0.022088692

Batch 3090, train_perplexity=1.0222002, train_loss=0.021957396

Batch 3100, train_perplexity=1.0220677, train_loss=0.021827653

Batch 3110, train_perplexity=1.0219364, train_loss=0.02169925

Batch 3120, train_perplexity=1.0218065, train_loss=0.021572175

Batch 3130, train_perplexity=1.0216781, train_loss=0.021446481

Batch 3140, train_perplexity=1.0215509, train_loss=0.021322008

Batch 3150, train_perplexity=1.0214252, train_loss=0.021198997

Batch 3160, train_perplexity=1.0213008, train_loss=0.021077074

Batch 3170, train_perplexity=1.0211776, train_loss=0.020956561

Batch 3180, train_perplexity=1.0210558, train_loss=0.020837216

Batch 3190, train_perplexity=1.0209353, train_loss=0.020719148

Batch 3200, train_perplexity=1.020816, train_loss=0.020602241

Batch 3210, train_perplexity=1.0206978, train_loss=0.020486591

Batch 3220, train_perplexity=1.0205809, train_loss=0.020371962

Batch 3230, train_perplexity=1.0204653, train_loss=0.020258628

Batch 3240, train_perplexity=1.0203506, train_loss=0.020146307

Batch 3250, train_perplexity=1.0202372, train_loss=0.020035192

Batch 3260, train_perplexity=1.0201249, train_loss=0.019925129

Batch 3270, train_perplexity=1.0200138, train_loss=0.019816123

Batch 3280, train_perplexity=1.0199038, train_loss=0.019708324

Batch 3290, train_perplexity=1.019795, train_loss=0.019601537

Batch 3300, train_perplexity=1.0196872, train_loss=0.019495834

Batch 3310, train_perplexity=1.0195804, train_loss=0.019391147

Batch 3320, train_perplexity=1.0194746, train_loss=0.019287396

Batch 3330, train_perplexity=1.01937, train_loss=0.019184768

Batch 3340, train_perplexity=1.0192664, train_loss=0.019083064

Batch 3350, train_perplexity=1.0191637, train_loss=0.018982388

Batch 3360, train_perplexity=1.0190622, train_loss=0.018882737

Batch 3370, train_perplexity=1.0189615, train_loss=0.018784005

Batch 3380, train_perplexity=1.018862, train_loss=0.018686272

Batch 3390, train_perplexity=1.0187633, train_loss=0.018589415

Batch 3400, train_perplexity=1.0186656, train_loss=0.018493515

Batch 3410, train_perplexity=1.0185688, train_loss=0.018398503

Batch 3420, train_perplexity=1.0184729, train_loss=0.018304402

Batch 3430, train_perplexity=1.018378, train_loss=0.018211192

Batch 3440, train_perplexity=1.0182841, train_loss=0.018118888

Batch 3450, train_perplexity=1.0181909, train_loss=0.018027423

Batch 3460, train_perplexity=1.0180986, train_loss=0.017936721

Batch 3470, train_perplexity=1.0180072, train_loss=0.017846992
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3480, train_perplexity=1.0179166, train_loss=0.01775794

Batch 3490, train_perplexity=1.0178269, train_loss=0.017669838

Batch 3500, train_perplexity=1.017738, train_loss=0.017582461

Batch 3510, train_perplexity=1.0176499, train_loss=0.017495964

Batch 3520, train_perplexity=1.0175626, train_loss=0.017410174

Batch 3530, train_perplexity=1.0174762, train_loss=0.017325217

Batch 3540, train_perplexity=1.0173904, train_loss=0.017240917

Batch 3550, train_perplexity=1.0173055, train_loss=0.017157413

Batch 3560, train_perplexity=1.0172212, train_loss=0.017074578

Batch 3570, train_perplexity=1.0171378, train_loss=0.016992513

Batch 3580, train_perplexity=1.017055, train_loss=0.016911196

Batch 3590, train_perplexity=1.016973, train_loss=0.01683059

Batch 3600, train_perplexity=1.0168918, train_loss=0.016750757

Batch 3610, train_perplexity=1.0168113, train_loss=0.016671527

Batch 3620, train_perplexity=1.0167314, train_loss=0.016593006

Batch 3630, train_perplexity=1.0166522, train_loss=0.016515113

Batch 3640, train_perplexity=1.0165738, train_loss=0.016437925

Batch 3650, train_perplexity=1.016496, train_loss=0.016361345

Batch 3660, train_perplexity=1.0164188, train_loss=0.016285535

Batch 3670, train_perplexity=1.0163423, train_loss=0.01621024

Batch 3680, train_perplexity=1.0162666, train_loss=0.01613567

Batch 3690, train_perplexity=1.0161914, train_loss=0.016061734

Batch 3700, train_perplexity=1.0161169, train_loss=0.015988365

Batch 3710, train_perplexity=1.016043, train_loss=0.015915634

Batch 3720, train_perplexity=1.0159696, train_loss=0.015843425

Batch 3730, train_perplexity=1.015897, train_loss=0.01577196

Batch 3740, train_perplexity=1.0158249, train_loss=0.015701026

Batch 3750, train_perplexity=1.0157534, train_loss=0.015630642

Batch 3760, train_perplexity=1.0156826, train_loss=0.015560833

Batch 3770, train_perplexity=1.0156122, train_loss=0.015491606

Batch 3780, train_perplexity=1.0155425, train_loss=0.01542292

Batch 3790, train_perplexity=1.0154734, train_loss=0.015354827

Batch 3800, train_perplexity=1.0154047, train_loss=0.015287295

Batch 3810, train_perplexity=1.0153368, train_loss=0.015220325

Batch 3820, train_perplexity=1.0152692, train_loss=0.015153774

Batch 3830, train_perplexity=1.0152023, train_loss=0.015087895

Batch 3840, train_perplexity=1.0151359, train_loss=0.015022444

Batch 3850, train_perplexity=1.0150701, train_loss=0.014957603

Batch 3860, train_perplexity=1.0150046, train_loss=0.014893217

Batch 3870, train_perplexity=1.0149399, train_loss=0.014829354

Batch 3880, train_perplexity=1.0148755, train_loss=0.014765941

Batch 3890, train_perplexity=1.0148118, train_loss=0.014703095

Batch 3900, train_perplexity=1.0147485, train_loss=0.014640724

Batch 3910, train_perplexity=1.0146855, train_loss=0.014578728

Batch 3920, train_perplexity=1.0146233, train_loss=0.014517414

Batch 3930, train_perplexity=1.0145614, train_loss=0.014456388

Batch 3940, train_perplexity=1.0145001, train_loss=0.014395972

Batch 3950, train_perplexity=1.0144391, train_loss=0.014335852

Batch 3960, train_perplexity=1.0143787, train_loss=0.014276326

Batch 3970, train_perplexity=1.0143187, train_loss=0.014217206

Batch 3980, train_perplexity=1.0142593, train_loss=0.014158611

Batch 3990, train_perplexity=1.0142002, train_loss=0.014100366

Batch 4000, train_perplexity=1.0141416, train_loss=0.0140425395

Batch 4010, train_perplexity=1.0140835, train_loss=0.013985202

Batch 4020, train_perplexity=1.0140258, train_loss=0.013928301

Batch 4030, train_perplexity=1.0139685, train_loss=0.013871803

Batch 4040, train_perplexity=1.0139117, train_loss=0.013815818

Batch 4050, train_perplexity=1.0138552, train_loss=0.013760144

Batch 4060, train_perplexity=1.0137992, train_loss=0.013704905

Batch 4070, train_perplexity=1.0137436, train_loss=0.013650017

Batch 4080, train_perplexity=1.0136884, train_loss=0.013595636

Batch 4090, train_perplexity=1.0136337, train_loss=0.013541584

Batch 4100, train_perplexity=1.0135794, train_loss=0.013488007

Batch 4110, train_perplexity=1.0135254, train_loss=0.013434686

Batch 4120, train_perplexity=1.0134717, train_loss=0.013381774

Batch 4130, train_perplexity=1.0134186, train_loss=0.013329312

Batch 4140, train_perplexity=1.0133657, train_loss=0.013277235

Batch 4150, train_perplexity=1.0133133, train_loss=0.013225505

Batch 4160, train_perplexity=1.0132613, train_loss=0.013174169

Batch 4170, train_perplexity=1.0132097, train_loss=0.013123188

Batch 4180, train_perplexity=1.0131583, train_loss=0.013072535

Batch 4190, train_perplexity=1.0131074, train_loss=0.013022238

Batch 4200, train_perplexity=1.0130568, train_loss=0.0129722785

Batch 4210, train_perplexity=1.0130066, train_loss=0.012922754

Batch 4220, train_perplexity=1.0129567, train_loss=0.012873576

Batch 4230, train_perplexity=1.0129073, train_loss=0.012824658

Batch 4240, train_perplexity=1.012858, train_loss=0.012776099

Batch 4250, train_perplexity=1.0128093, train_loss=0.012727907

Batch 4260, train_perplexity=1.0127608, train_loss=0.012680054

Batch 4270, train_perplexity=1.0127126, train_loss=0.012632517

Batch 4280, train_perplexity=1.0126649, train_loss=0.012585361

Batch 4290, train_perplexity=1.0126175, train_loss=0.012538506

Batch 4300, train_perplexity=1.0125703, train_loss=0.012491904

Batch 4310, train_perplexity=1.0125234, train_loss=0.012445633

Batch 4320, train_perplexity=1.0124769, train_loss=0.012399703

Batch 4330, train_perplexity=1.0124308, train_loss=0.012354149

Batch 4340, train_perplexity=1.0123849, train_loss=0.012308869

Batch 4350, train_perplexity=1.0123394, train_loss=0.0122639

Batch 4360, train_perplexity=1.0122942, train_loss=0.01221917

Batch 4370, train_perplexity=1.0122492, train_loss=0.012174768

Batch 4380, train_perplexity=1.0122045, train_loss=0.012130612

Batch 4390, train_perplexity=1.0121602, train_loss=0.0120868385

Batch 4400, train_perplexity=1.0121162, train_loss=0.012043372

Batch 4410, train_perplexity=1.0120724, train_loss=0.01200017

Batch 4420, train_perplexity=1.0120289, train_loss=0.011957187

Batch 4430, train_perplexity=1.0119858, train_loss=0.011914542

Batch 4440, train_perplexity=1.011943, train_loss=0.011872275

Batch 4450, train_perplexity=1.0119004, train_loss=0.011830134

Batch 4460, train_perplexity=1.0118581, train_loss=0.01178832

Batch 4470, train_perplexity=1.011816, train_loss=0.011746801

Batch 4480, train_perplexity=1.0117743, train_loss=0.011705564

Batch 4490, train_perplexity=1.0117328, train_loss=0.011664571

Batch 4500, train_perplexity=1.0116917, train_loss=0.011623859

Batch 4510, train_perplexity=1.0116507, train_loss=0.011583393

Batch 4520, train_perplexity=1.01161, train_loss=0.011543194

Batch 4530, train_perplexity=1.0115696, train_loss=0.011503229

Batch 4540, train_perplexity=1.0115296, train_loss=0.011463573

Batch 4550, train_perplexity=1.0114896, train_loss=0.011424114

Batch 4560, train_perplexity=1.0114499, train_loss=0.011384916

Batch 4570, train_perplexity=1.0114106, train_loss=0.011345994

Batch 4580, train_perplexity=1.0113715, train_loss=0.011307338

Batch 4590, train_perplexity=1.0113326, train_loss=0.011268863

Batch 4600, train_perplexity=1.0112939, train_loss=0.01123062

Batch 4610, train_perplexity=1.0112555, train_loss=0.011192683

Batch 4620, train_perplexity=1.0112174, train_loss=0.011154933

Batch 4630, train_perplexity=1.0111796, train_loss=0.011117487

Batch 4640, train_perplexity=1.0111418, train_loss=0.011080151

Batch 4650, train_perplexity=1.0111043, train_loss=0.011043159

Batch 4660, train_perplexity=1.0110672, train_loss=0.011006365

Batch 4670, train_perplexity=1.0110302, train_loss=0.010969756

Batch 4680, train_perplexity=1.0109934, train_loss=0.010933373

Batch 4690, train_perplexity=1.0109569, train_loss=0.010897307

Batch 4700, train_perplexity=1.0109206, train_loss=0.010861403

Batch 4710, train_perplexity=1.0108845, train_loss=0.010825758

Batch 4720, train_perplexity=1.0108488, train_loss=0.01079029

Batch 4730, train_perplexity=1.0108131, train_loss=0.010755021

Batch 4740, train_perplexity=1.0107776, train_loss=0.010719977

Batch 4750, train_perplexity=1.0107424, train_loss=0.010685174

Batch 4760, train_perplexity=1.0107075, train_loss=0.010650544
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4770, train_perplexity=1.0106727, train_loss=0.01061615

Batch 4780, train_perplexity=1.0106381, train_loss=0.010581952

Batch 4790, train_perplexity=1.0106038, train_loss=0.010547984

Batch 4800, train_perplexity=1.0105696, train_loss=0.010514127

Batch 4810, train_perplexity=1.0105357, train_loss=0.010480579

Batch 4820, train_perplexity=1.010502, train_loss=0.010447175

Batch 4830, train_perplexity=1.0104684, train_loss=0.010413991

Batch 4840, train_perplexity=1.0104351, train_loss=0.010381017

Batch 4850, train_perplexity=1.010402, train_loss=0.01034819

Batch 4860, train_perplexity=1.010369, train_loss=0.010315577

Batch 4870, train_perplexity=1.0103363, train_loss=0.0102831945

Batch 4880, train_perplexity=1.0103036, train_loss=0.010250917

Batch 4890, train_perplexity=1.0102713, train_loss=0.0102188755

Batch 4900, train_perplexity=1.0102391, train_loss=0.010187011

Batch 4910, train_perplexity=1.010207, train_loss=0.010155337

Batch 4920, train_perplexity=1.0101753, train_loss=0.010123888

Batch 4930, train_perplexity=1.0101436, train_loss=0.010092528

Batch 4940, train_perplexity=1.0101122, train_loss=0.010061395

Batch 4950, train_perplexity=1.0100809, train_loss=0.010030513

Batch 4960, train_perplexity=1.0100499, train_loss=0.009999756

Batch 4970, train_perplexity=1.0100191, train_loss=0.009969168

Batch 4980, train_perplexity=1.0099883, train_loss=0.0099387225

Batch 4990, train_perplexity=1.0099578, train_loss=0.0099085225

Batch 5000, train_perplexity=1.0099274, train_loss=0.009878442

Batch 5010, train_perplexity=1.0098972, train_loss=0.009848548

Batch 5020, train_perplexity=1.0098672, train_loss=0.009818865

Batch 5030, train_perplexity=1.0098374, train_loss=0.009789288

Batch 5040, train_perplexity=1.0098077, train_loss=0.0097598685

Batch 5050, train_perplexity=1.0097781, train_loss=0.009730667

Batch 5060, train_perplexity=1.0097488, train_loss=0.009701628

Batch 5070, train_perplexity=1.0097196, train_loss=0.009672642

Batch 5080, train_perplexity=1.0096905, train_loss=0.00964392

Batch 5090, train_perplexity=1.0096617, train_loss=0.009615336

Batch 5100, train_perplexity=1.009633, train_loss=0.009586899

Batch 5110, train_perplexity=1.0096045, train_loss=0.009558568

Batch 5120, train_perplexity=1.0095761, train_loss=0.009530494

Batch 5130, train_perplexity=1.0095477, train_loss=0.009502454

Batch 5140, train_perplexity=1.0095197, train_loss=0.009474611

Batch 5150, train_perplexity=1.0094917, train_loss=0.00944694

Batch 5160, train_perplexity=1.0094639, train_loss=0.009419414

Batch 5170, train_perplexity=1.0094362, train_loss=0.009392034

Batch 5180, train_perplexity=1.0094087, train_loss=0.009364756

Batch 5190, train_perplexity=1.0093814, train_loss=0.009337716

Batch 5200, train_perplexity=1.0093542, train_loss=0.009310744

Batch 5210, train_perplexity=1.0093272, train_loss=0.009283947

Batch 5220, train_perplexity=1.0093002, train_loss=0.009257279

Batch 5230, train_perplexity=1.0092735, train_loss=0.00923074

Batch 5240, train_perplexity=1.009247, train_loss=0.009204421

Batch 5250, train_perplexity=1.0092204, train_loss=0.009178149

Batch 5260, train_perplexity=1.009194, train_loss=0.009152044

Batch 5270, train_perplexity=1.0091679, train_loss=0.00912614

Batch 5280, train_perplexity=1.0091418, train_loss=0.009100281

Batch 5290, train_perplexity=1.0091158, train_loss=0.009074536

Batch 5300, train_perplexity=1.0090901, train_loss=0.009048976

Batch 5310, train_perplexity=1.0090643, train_loss=0.009023534

Batch 5320, train_perplexity=1.0090389, train_loss=0.008998295

Batch 5330, train_perplexity=1.0090135, train_loss=0.008973113

Batch 5340, train_perplexity=1.0089881, train_loss=0.008948026

Batch 5350, train_perplexity=1.008963, train_loss=0.00892311

Batch 5360, train_perplexity=1.008938, train_loss=0.008898288

Batch 5370, train_perplexity=1.0089132, train_loss=0.0088736415

Batch 5380, train_perplexity=1.0088884, train_loss=0.008849088

Batch 5390, train_perplexity=1.0088637, train_loss=0.008824633

Batch 5400, train_perplexity=1.0088391, train_loss=0.008800309

Batch 5410, train_perplexity=1.0088147, train_loss=0.008776074

Batch 5420, train_perplexity=1.0087905, train_loss=0.008752114

Batch 5430, train_perplexity=1.0087663, train_loss=0.008728104

Batch 5440, train_perplexity=1.0087423, train_loss=0.008704305

Batch 5450, train_perplexity=1.0087184, train_loss=0.00868061

Batch 5460, train_perplexity=1.0086945, train_loss=0.008656998

Batch 5470, train_perplexity=1.0086709, train_loss=0.008633519

Batch 5480, train_perplexity=1.0086473, train_loss=0.008610204

Batch 5490, train_perplexity=1.008624, train_loss=0.008586941

Batch 5500, train_perplexity=1.0086006, train_loss=0.008563828

Batch 5510, train_perplexity=1.0085773, train_loss=0.0085407905

Batch 5520, train_perplexity=1.0085542, train_loss=0.008517899

Batch 5530, train_perplexity=1.0085313, train_loss=0.008495146

Batch 5540, train_perplexity=1.0085083, train_loss=0.0084723765

Batch 5550, train_perplexity=1.0084857, train_loss=0.00844988

Batch 5560, train_perplexity=1.0084631, train_loss=0.008427478

Batch 5570, train_perplexity=1.0084405, train_loss=0.008405029

Batch 5580, train_perplexity=1.0084181, train_loss=0.0083828

Batch 5590, train_perplexity=1.0083958, train_loss=0.008360757

Batch 5600, train_perplexity=1.0083735, train_loss=0.008338684

Batch 5610, train_perplexity=1.0083514, train_loss=0.008316753

Batch 5620, train_perplexity=1.0083295, train_loss=0.008294957

Batch 5630, train_perplexity=1.0083076, train_loss=0.008273207

Batch 5640, train_perplexity=1.0082858, train_loss=0.008251565

Batch 5650, train_perplexity=1.0082641, train_loss=0.008230055

Batch 5660, train_perplexity=1.0082424, train_loss=0.008208621

Batch 5670, train_perplexity=1.0082209, train_loss=0.008187309

Batch 5680, train_perplexity=1.0081996, train_loss=0.008166097

Batch 5690, train_perplexity=1.0081782, train_loss=0.008144969

Batch 5700, train_perplexity=1.008157, train_loss=0.008123921

Batch 5710, train_perplexity=1.0081359, train_loss=0.008103022

Batch 5720, train_perplexity=1.0081149, train_loss=0.0080822315

Batch 5730, train_perplexity=1.0080941, train_loss=0.008061472

Batch 5740, train_perplexity=1.0080733, train_loss=0.008040903

Batch 5750, train_perplexity=1.0080526, train_loss=0.008020369

Batch 5760, train_perplexity=1.008032, train_loss=0.007999914

Batch 5770, train_perplexity=1.0080115, train_loss=0.007979569

Batch 5780, train_perplexity=1.0079912, train_loss=0.007959381

Batch 5790, train_perplexity=1.0079709, train_loss=0.007939273

Batch 5800, train_perplexity=1.0079507, train_loss=0.007919166

Batch 5810, train_perplexity=1.0079305, train_loss=0.007899243

Batch 5820, train_perplexity=1.0079105, train_loss=0.007879379

Batch 5830, train_perplexity=1.0078906, train_loss=0.007859599

Batch 5840, train_perplexity=1.0078707, train_loss=0.0078399135

Batch 5850, train_perplexity=1.007851, train_loss=0.007820307

Batch 5860, train_perplexity=1.0078313, train_loss=0.0078008072

Batch 5870, train_perplexity=1.0078118, train_loss=0.007781445

Batch 5880, train_perplexity=1.0077922, train_loss=0.0077620675

Batch 5890, train_perplexity=1.0077729, train_loss=0.0077428706

Batch 5900, train_perplexity=1.0077537, train_loss=0.0077237673

Batch 5910, train_perplexity=1.0077344, train_loss=0.007704663

Batch 5920, train_perplexity=1.0077153, train_loss=0.007685718

Batch 5930, train_perplexity=1.0076964, train_loss=0.007666871

Batch 5940, train_perplexity=1.0076773, train_loss=0.0076479875

Batch 5950, train_perplexity=1.0076585, train_loss=0.007629292

Batch 5960, train_perplexity=1.0076398, train_loss=0.007610674

Batch 5970, train_perplexity=1.007621, train_loss=0.007592108

Batch 5980, train_perplexity=1.0076025, train_loss=0.007573661

Batch 5990, train_perplexity=1.007584, train_loss=0.007555308

Batch 6000, train_perplexity=1.0075654, train_loss=0.0075369356

Batch 6010, train_perplexity=1.007547, train_loss=0.0075186845

Batch 6020, train_perplexity=1.0075288, train_loss=0.0075005833

Batch 6030, train_perplexity=1.0075105, train_loss=0.0074825296

Batch 6040, train_perplexity=1.0074924, train_loss=0.0074645495

Batch 6050, train_perplexity=1.0074744, train_loss=0.0074465717
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 6060, train_perplexity=1.0074564, train_loss=0.0074288035

Batch 6070, train_perplexity=1.0074385, train_loss=0.007411023

Batch 6080, train_perplexity=1.0074208, train_loss=0.0073933695

Batch 6090, train_perplexity=1.0074031, train_loss=0.0073758154

Batch 6100, train_perplexity=1.0073855, train_loss=0.0073583033

Batch 6110, train_perplexity=1.0073678, train_loss=0.0073408578

Batch 6120, train_perplexity=1.0073503, train_loss=0.007323486

Batch 6130, train_perplexity=1.0073329, train_loss=0.007306179

Batch 6140, train_perplexity=1.0073156, train_loss=0.007288999

Batch 6150, train_perplexity=1.0072984, train_loss=0.00727186

Batch 6160, train_perplexity=1.0072813, train_loss=0.007254868

Batch 6170, train_perplexity=1.0072641, train_loss=0.0072378376

Batch 6180, train_perplexity=1.0072471, train_loss=0.0072209397

Batch 6190, train_perplexity=1.00723, train_loss=0.0072040185

Batch 6200, train_perplexity=1.0072132, train_loss=0.007187294

Batch 6210, train_perplexity=1.0071964, train_loss=0.007170652

Batch 6220, train_perplexity=1.0071797, train_loss=0.007154043

Batch 6230, train_perplexity=1.007163, train_loss=0.0071374923

Batch 6240, train_perplexity=1.0071464, train_loss=0.0071210004

Batch 6250, train_perplexity=1.0071299, train_loss=0.0071045915

Batch 6260, train_perplexity=1.0071135, train_loss=0.0070883175

Batch 6270, train_perplexity=1.0070971, train_loss=0.007072008

Batch 6280, train_perplexity=1.0070808, train_loss=0.007055852

Batch 6290, train_perplexity=1.0070646, train_loss=0.007039737

Batch 6300, train_perplexity=1.0070485, train_loss=0.007023708

Batch 6310, train_perplexity=1.0070323, train_loss=0.0070077134

Batch 6320, train_perplexity=1.0070163, train_loss=0.0069917627

Batch 6330, train_perplexity=1.0070003, train_loss=0.006975948

Batch 6340, train_perplexity=1.0069845, train_loss=0.0069601806

Batch 6350, train_perplexity=1.0069686, train_loss=0.006944419

Batch 6360, train_perplexity=1.0069529, train_loss=0.0069287745

Batch 6370, train_perplexity=1.0069371, train_loss=0.0069132373

Batch 6380, train_perplexity=1.0069215, train_loss=0.006897658

Batch 6390, train_perplexity=1.006906, train_loss=0.0068822494

Batch 6400, train_perplexity=1.0068905, train_loss=0.006866897

Batch 6410, train_perplexity=1.0068752, train_loss=0.006851574

Batch 6420, train_perplexity=1.0068597, train_loss=0.0068362774

Batch 6430, train_perplexity=1.0068444, train_loss=0.0068210987

Batch 6440, train_perplexity=1.0068291, train_loss=0.0068059345

Batch 6450, train_perplexity=1.006814, train_loss=0.0067908643

Batch 6460, train_perplexity=1.0067989, train_loss=0.0067758895

Batch 6470, train_perplexity=1.0067838, train_loss=0.0067608785

Batch 6480, train_perplexity=1.0067688, train_loss=0.0067459964

Batch 6490, train_perplexity=1.0067539, train_loss=0.006731162

Batch 6500, train_perplexity=1.006739, train_loss=0.006716389

Batch 6510, train_perplexity=1.0067242, train_loss=0.0067016766

Batch 6520, train_perplexity=1.0067095, train_loss=0.006687049

Batch 6530, train_perplexity=1.0066948, train_loss=0.0066724494

Batch 6540, train_perplexity=1.0066801, train_loss=0.006657936

Batch 6550, train_perplexity=1.0066656, train_loss=0.006643438

Batch 6560, train_perplexity=1.006651, train_loss=0.0066290395

Batch 6570, train_perplexity=1.0066366, train_loss=0.006614691

Batch 6580, train_perplexity=1.0066222, train_loss=0.006600414

Batch 6590, train_perplexity=1.0066079, train_loss=0.0065861987

Batch 6600, train_perplexity=1.0065937, train_loss=0.006572027

Batch 6610, train_perplexity=1.0065795, train_loss=0.0065579466

Batch 6620, train_perplexity=1.0065653, train_loss=0.0065438906

Batch 6630, train_perplexity=1.0065513, train_loss=0.0065299403

Batch 6640, train_perplexity=1.0065372, train_loss=0.0065159476

Batch 6650, train_perplexity=1.0065233, train_loss=0.0065020835

Batch 6660, train_perplexity=1.0065094, train_loss=0.0064882888

Batch 6670, train_perplexity=1.0064955, train_loss=0.0064744735

Batch 6680, train_perplexity=1.0064818, train_loss=0.0064608008

Batch 6690, train_perplexity=1.0064679, train_loss=0.0064470912

Batch 6700, train_perplexity=1.0064542, train_loss=0.0064335032

Batch 6710, train_perplexity=1.0064406, train_loss=0.006419939

Batch 6720, train_perplexity=1.0064269, train_loss=0.006406422

Batch 6730, train_perplexity=1.0064135, train_loss=0.0063929604

Batch 6740, train_perplexity=1.0064, train_loss=0.0063795727

Batch 6750, train_perplexity=1.0063865, train_loss=0.0063661635

Batch 6760, train_perplexity=1.0063732, train_loss=0.0063529387

Batch 6770, train_perplexity=1.0063598, train_loss=0.0063396916

Batch 6780, train_perplexity=1.0063466, train_loss=0.006326514

Batch 6790, train_perplexity=1.0063334, train_loss=0.0063134003

Batch 6800, train_perplexity=1.0063202, train_loss=0.006300316

Batch 6810, train_perplexity=1.0063071, train_loss=0.006287368

Batch 6820, train_perplexity=1.0062941, train_loss=0.0062743695

Batch 6830, train_perplexity=1.0062811, train_loss=0.0062614856

Batch 6840, train_perplexity=1.0062681, train_loss=0.0062486166

Batch 6850, train_perplexity=1.0062553, train_loss=0.006235792

Batch 6860, train_perplexity=1.0062424, train_loss=0.0062230295

Batch 6870, train_perplexity=1.0062296, train_loss=0.006210305

Batch 6880, train_perplexity=1.0062169, train_loss=0.006197678

Batch 6890, train_perplexity=1.0062041, train_loss=0.0061850147

Batch 6900, train_perplexity=1.0061915, train_loss=0.0061724083

Batch 6910, train_perplexity=1.006179, train_loss=0.0061599347

Batch 6920, train_perplexity=1.0061663, train_loss=0.006147372

Batch 6930, train_perplexity=1.0061538, train_loss=0.006134957

Batch 6940, train_perplexity=1.0061414, train_loss=0.006122628

Batch 6950, train_perplexity=1.0061289, train_loss=0.006110204

Batch 6960, train_perplexity=1.0061165, train_loss=0.006097922

Batch 6970, train_perplexity=1.0061042, train_loss=0.0060856985

Batch 6980, train_perplexity=1.006092, train_loss=0.0060734516

Batch 6990, train_perplexity=1.0060798, train_loss=0.006061334

Batch 7000, train_perplexity=1.0060675, train_loss=0.0060492

Batch 7010, train_perplexity=1.0060554, train_loss=0.006037112

Batch 7020, train_perplexity=1.0060433, train_loss=0.006025104

Batch 7030, train_perplexity=1.0060313, train_loss=0.006013102

Batch 7040, train_perplexity=1.0060192, train_loss=0.006001182

Batch 7050, train_perplexity=1.0060073, train_loss=0.0059893425

Batch 7060, train_perplexity=1.0059954, train_loss=0.005977473

Batch 7070, train_perplexity=1.0059835, train_loss=0.0059656827

Batch 7080, train_perplexity=1.0059717, train_loss=0.0059539024

Batch 7090, train_perplexity=1.0059599, train_loss=0.005942204

Batch 7100, train_perplexity=1.0059482, train_loss=0.005930588

Batch 7110, train_perplexity=1.0059364, train_loss=0.005918892

Batch 7120, train_perplexity=1.0059248, train_loss=0.005907327

Batch 7130, train_perplexity=1.0059133, train_loss=0.0058958293

Batch 7140, train_perplexity=1.0059017, train_loss=0.005884314

Batch 7150, train_perplexity=1.0058903, train_loss=0.0058729313

Batch 7160, train_perplexity=1.0058787, train_loss=0.0058615077

Batch 7170, train_perplexity=1.0058674, train_loss=0.0058501633

Batch 7180, train_perplexity=1.0058559, train_loss=0.005838845

Batch 7190, train_perplexity=1.0058446, train_loss=0.005827542

Batch 7200, train_perplexity=1.0058333, train_loss=0.005816334

Batch 7210, train_perplexity=1.0058221, train_loss=0.0058051394

Batch 7220, train_perplexity=1.0058109, train_loss=0.005793984

Batch 7230, train_perplexity=1.0057997, train_loss=0.0057829563

Batch 7240, train_perplexity=1.0057886, train_loss=0.005771818

Batch 7250, train_perplexity=1.0057774, train_loss=0.005760789

Batch 7260, train_perplexity=1.0057663, train_loss=0.0057497127

Batch 7270, train_perplexity=1.0057554, train_loss=0.005738885

Batch 7280, train_perplexity=1.0057443, train_loss=0.005727877

Batch 7290, train_perplexity=1.0057334, train_loss=0.00571704

Batch 7300, train_perplexity=1.0057225, train_loss=0.0057061585

Batch 7310, train_perplexity=1.0057117, train_loss=0.005695387

Batch 7320, train_perplexity=1.0057008, train_loss=0.0056846207

Batch 7330, train_perplexity=1.00569, train_loss=0.0056738723
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7340, train_perplexity=1.0056792, train_loss=0.005663204

Batch 7350, train_perplexity=1.0056686, train_loss=0.005652597

Batch 7360, train_perplexity=1.0056579, train_loss=0.005641922

Batch 7370, train_perplexity=1.0056472, train_loss=0.0056313416

Batch 7380, train_perplexity=1.0056367, train_loss=0.005620826

Batch 7390, train_perplexity=1.0056261, train_loss=0.0056103137

Batch 7400, train_perplexity=1.0056156, train_loss=0.005599932

Batch 7410, train_perplexity=1.0056051, train_loss=0.005589446

Batch 7420, train_perplexity=1.0055947, train_loss=0.005579093

Batch 7430, train_perplexity=1.0055844, train_loss=0.005568796

Batch 7440, train_perplexity=1.0055739, train_loss=0.0055584433

Batch 7450, train_perplexity=1.0055637, train_loss=0.005548265

Batch 7460, train_perplexity=1.0055534, train_loss=0.0055379616

Batch 7470, train_perplexity=1.005543, train_loss=0.0055277264

Batch 7480, train_perplexity=1.0055329, train_loss=0.005517648

Batch 7490, train_perplexity=1.0055226, train_loss=0.0055074487

Batch 7500, train_perplexity=1.0055126, train_loss=0.0054974384

Batch 7510, train_perplexity=1.0055025, train_loss=0.0054873805

Batch 7520, train_perplexity=1.0054923, train_loss=0.0054773316

Batch 7530, train_perplexity=1.0054824, train_loss=0.005467454

Batch 7540, train_perplexity=1.0054723, train_loss=0.005457431

Batch 7550, train_perplexity=1.0054623, train_loss=0.0054474417

Batch 7560, train_perplexity=1.0054524, train_loss=0.0054375487

Batch 7570, train_perplexity=1.0054425, train_loss=0.0054277037

Batch 7580, train_perplexity=1.0054326, train_loss=0.005417867

Batch 7590, train_perplexity=1.0054228, train_loss=0.0054081725

Batch 7600, train_perplexity=1.005413, train_loss=0.0053983983

Batch 7610, train_perplexity=1.0054032, train_loss=0.0053886743

Batch 7620, train_perplexity=1.0053935, train_loss=0.005378994

Batch 7630, train_perplexity=1.0053837, train_loss=0.005369323

Batch 7640, train_perplexity=1.0053741, train_loss=0.0053596846

Batch 7650, train_perplexity=1.0053644, train_loss=0.0053501017

Batch 7660, train_perplexity=1.0053549, train_loss=0.0053405697

Batch 7670, train_perplexity=1.0053452, train_loss=0.005330966

Batch 7680, train_perplexity=1.0053357, train_loss=0.0053215045

Batch 7690, train_perplexity=1.0053262, train_loss=0.0053120586

Batch 7700, train_perplexity=1.0053166, train_loss=0.0053025875

Batch 7710, train_perplexity=1.0053072, train_loss=0.0052931732

Batch 7720, train_perplexity=1.0052978, train_loss=0.005283842

Batch 7730, train_perplexity=1.0052885, train_loss=0.005274522

Batch 7740, train_perplexity=1.0052792, train_loss=0.005265259

Batch 7750, train_perplexity=1.0052698, train_loss=0.0052559683

Batch 7760, train_perplexity=1.0052606, train_loss=0.0052467557

Batch 7770, train_perplexity=1.0052513, train_loss=0.0052374974

Batch 7780, train_perplexity=1.005242, train_loss=0.0052283164

Batch 7790, train_perplexity=1.0052328, train_loss=0.005219192

Batch 7800, train_perplexity=1.0052236, train_loss=0.0052100783

Batch 7810, train_perplexity=1.0052145, train_loss=0.0052009635

Batch 7820, train_perplexity=1.0052054, train_loss=0.0051919203

Batch 7830, train_perplexity=1.0051963, train_loss=0.00518292

Batch 7840, train_perplexity=1.0051874, train_loss=0.005173999

Batch 7850, train_perplexity=1.0051783, train_loss=0.00516498

Batch 7860, train_perplexity=1.0051694, train_loss=0.0051560504

Batch 7870, train_perplexity=1.0051603, train_loss=0.00514707

Batch 7880, train_perplexity=1.0051514, train_loss=0.005138208

Batch 7890, train_perplexity=1.0051425, train_loss=0.0051293285

Batch 7900, train_perplexity=1.0051337, train_loss=0.00512057

Batch 7910, train_perplexity=1.0051248, train_loss=0.0051117465

Batch 7920, train_perplexity=1.0051161, train_loss=0.005103047

Batch 7930, train_perplexity=1.0051073, train_loss=0.005094259

Batch 7940, train_perplexity=1.0050986, train_loss=0.005085622

Batch 7950, train_perplexity=1.0050898, train_loss=0.005076901

Batch 7960, train_perplexity=1.0050812, train_loss=0.005068252

Batch 7970, train_perplexity=1.0050725, train_loss=0.0050596087

Batch 7980, train_perplexity=1.0050638, train_loss=0.005051042

Batch 7990, train_perplexity=1.0050552, train_loss=0.0050424663

Batch 8000, train_perplexity=1.0050466, train_loss=0.0050338795

Batch 8010, train_perplexity=1.005038, train_loss=0.005025434

Batch 8020, train_perplexity=1.0050296, train_loss=0.005016939

Batch 8030, train_perplexity=1.0050211, train_loss=0.0050084777

Batch 8040, train_perplexity=1.0050126, train_loss=0.0050001037

Batch 8050, train_perplexity=1.0050042, train_loss=0.0049916464

Batch 8060, train_perplexity=1.0049957, train_loss=0.004983293

Batch 8070, train_perplexity=1.0049874, train_loss=0.00497498

Batch 8080, train_perplexity=1.0049789, train_loss=0.0049665812

Batch 8090, train_perplexity=1.0049706, train_loss=0.0049582636

Batch 8100, train_perplexity=1.0049623, train_loss=0.0049500363

Batch 8110, train_perplexity=1.0049541, train_loss=0.0049418425

Batch 8120, train_perplexity=1.0049458, train_loss=0.004933577

Batch 8130, train_perplexity=1.0049375, train_loss=0.0049253856

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 8140, train_perplexity=1.0049293, train_loss=0.004917209

Batch 8150, train_perplexity=1.0049212, train_loss=0.004909095

Batch 8160, train_perplexity=1.004913, train_loss=0.004900924

Batch 8170, train_perplexity=1.0049049, train_loss=0.004892854

Batch 8180, train_perplexity=1.0048968, train_loss=0.004884813

Batch 8190, train_perplexity=1.0048887, train_loss=0.00487679

Batch 8200, train_perplexity=1.0048807, train_loss=0.0048687523

Batch 8210, train_perplexity=1.0048726, train_loss=0.004860771

Batch 8220, train_perplexity=1.0048646, train_loss=0.004852795

Batch 8230, train_perplexity=1.0048566, train_loss=0.0048448546

Batch 8240, train_perplexity=1.0048487, train_loss=0.0048369803

Batch 8250, train_perplexity=1.0048407, train_loss=0.0048290426

Batch 8260, train_perplexity=1.0048329, train_loss=0.0048212325

Batch 8270, train_perplexity=1.004825, train_loss=0.0048133507

Batch 8280, train_perplexity=1.0048171, train_loss=0.004805544

Batch 8290, train_perplexity=1.0048093, train_loss=0.0047977515

Batch 8300, train_perplexity=1.0048014, train_loss=0.0047899326

Batch 8310, train_perplexity=1.0047936, train_loss=0.004782223

Batch 8320, train_perplexity=1.0047859, train_loss=0.00477451

Batch 8330, train_perplexity=1.0047781, train_loss=0.004766818

Batch 8340, train_perplexity=1.0047704, train_loss=0.004759091

Batch 8350, train_perplexity=1.0047628, train_loss=0.0047515025

Batch 8360, train_perplexity=1.0047551, train_loss=0.004743899

Batch 8370, train_perplexity=1.0047475, train_loss=0.0047363015

Batch 8380, train_perplexity=1.0047399, train_loss=0.0047286507

Batch 8390, train_perplexity=1.0047323, train_loss=0.004721098

Batch 8400, train_perplexity=1.0047246, train_loss=0.004713474

Batch 8410, train_perplexity=1.0047171, train_loss=0.0047060363

Batch 8420, train_perplexity=1.0047096, train_loss=0.0046985457

Batch 8430, train_perplexity=1.0047021, train_loss=0.004691096

Batch 8440, train_perplexity=1.0046946, train_loss=0.0046836464

Batch 8450, train_perplexity=1.0046871, train_loss=0.004676137

Batch 8460, train_perplexity=1.0046797, train_loss=0.004668792

Batch 8470, train_perplexity=1.0046722, train_loss=0.0046613477

Batch 8480, train_perplexity=1.0046649, train_loss=0.0046540373

Batch 8490, train_perplexity=1.0046575, train_loss=0.0046466906

Batch 8500, train_perplexity=1.0046501, train_loss=0.0046393657

Batch 8510, train_perplexity=1.0046428, train_loss=0.0046321116

Batch 8520, train_perplexity=1.0046356, train_loss=0.0046248157

Batch 8530, train_perplexity=1.0046283, train_loss=0.004617596

Batch 8540, train_perplexity=1.004621, train_loss=0.0046104044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 8550, train_perplexity=1.0046138, train_loss=0.0046031554

Batch 8560, train_perplexity=1.0046065, train_loss=0.0045959395

Batch 8570, train_perplexity=1.0045993, train_loss=0.0045887735

Batch 8580, train_perplexity=1.0045922, train_loss=0.0045816614

Batch 8590, train_perplexity=1.004585, train_loss=0.004574566

Batch 8600, train_perplexity=1.004578, train_loss=0.0045675104

Batch 8610, train_perplexity=1.0045708, train_loss=0.0045603737

Batch 8620, train_perplexity=1.0045637, train_loss=0.004553294

Batch 8630, train_perplexity=1.0045565, train_loss=0.004546229

Batch 8640, train_perplexity=1.0045496, train_loss=0.004539258

Batch 8650, train_perplexity=1.0045426, train_loss=0.004532314

Batch 8660, train_perplexity=1.0045356, train_loss=0.0045253285

Batch 8670, train_perplexity=1.0045286, train_loss=0.00451837

Batch 8680, train_perplexity=1.0045216, train_loss=0.004511405

Batch 8690, train_perplexity=1.0045147, train_loss=0.0045045083

Batch 8700, train_perplexity=1.0045077, train_loss=0.00449757

Batch 8710, train_perplexity=1.0045009, train_loss=0.0044907713

Batch 8720, train_perplexity=1.004494, train_loss=0.004483836

Batch 8730, train_perplexity=1.0044872, train_loss=0.004477096

Batch 8740, train_perplexity=1.0044802, train_loss=0.0044702347

Batch 8750, train_perplexity=1.0044734, train_loss=0.0044635097

Batch 8760, train_perplexity=1.0044667, train_loss=0.0044567017

Batch 8770, train_perplexity=1.0044599, train_loss=0.0044499347

Batch 8780, train_perplexity=1.0044531, train_loss=0.0044431565

Batch 8790, train_perplexity=1.0044464, train_loss=0.004436517

Batch 8800, train_perplexity=1.0044396, train_loss=0.004429786

Batch 8810, train_perplexity=1.0044329, train_loss=0.004423144

Batch 8820, train_perplexity=1.0044262, train_loss=0.004416504

Batch 8830, train_perplexity=1.0044196, train_loss=0.004409844

Batch 8840, train_perplexity=1.0044129, train_loss=0.004403228

Batch 8850, train_perplexity=1.0044063, train_loss=0.004396648

Batch 8860, train_perplexity=1.0043997, train_loss=0.0043899873

Batch 8870, train_perplexity=1.0043931, train_loss=0.0043834876

Batch 8880, train_perplexity=1.0043865, train_loss=0.004376946

Batch 8890, train_perplexity=1.00438, train_loss=0.0043704305

Batch 8900, train_perplexity=1.0043734, train_loss=0.004363885

Batch 8910, train_perplexity=1.0043669, train_loss=0.0043573882

Batch 8920, train_perplexity=1.0043604, train_loss=0.0043508993

Batch 8930, train_perplexity=1.0043539, train_loss=0.0043444373

Batch 8940, train_perplexity=1.0043474, train_loss=0.004338026

Batch 8950, train_perplexity=1.004341, train_loss=0.004331614

Batch 8960, train_perplexity=1.0043346, train_loss=0.0043251514

Batch 8970, train_perplexity=1.0043281, train_loss=0.0043188203

Batch 8980, train_perplexity=1.0043217, train_loss=0.0043124408

Batch 8990, train_perplexity=1.0043154, train_loss=0.0043061087

Batch 9000, train_perplexity=1.004309, train_loss=0.004299801

Batch 9010, train_perplexity=1.0043027, train_loss=0.004293508

Batch 9020, train_perplexity=1.0042963, train_loss=0.0042871344

Batch 9030, train_perplexity=1.0042901, train_loss=0.0042808913

Batch 9040, train_perplexity=1.0042838, train_loss=0.0042746067

Batch 9050, train_perplexity=1.0042775, train_loss=0.0042683454

Batch 9060, train_perplexity=1.0042713, train_loss=0.0042622006

Batch 9070, train_perplexity=1.0042651, train_loss=0.0042560045

Batch 9080, train_perplexity=1.0042589, train_loss=0.0042498596

Batch 9090, train_perplexity=1.0042527, train_loss=0.0042436067

Batch 9100, train_perplexity=1.0042465, train_loss=0.0042374856

Batch 9110, train_perplexity=1.0042403, train_loss=0.0042313077

Batch 9120, train_perplexity=1.0042342, train_loss=0.0042252806

Batch 9130, train_perplexity=1.004228, train_loss=0.0042190906

Batch 9140, train_perplexity=1.004222, train_loss=0.0042130994

Batch 9150, train_perplexity=1.0042158, train_loss=0.0042069415

Batch 9160, train_perplexity=1.0042099, train_loss=0.0042010034

Batch 9170, train_perplexity=1.0042037, train_loss=0.0041948813

Batch 9180, train_perplexity=1.0041977, train_loss=0.004188899

Batch 9190, train_perplexity=1.0041916, train_loss=0.004182916

Batch 9200, train_perplexity=1.0041857, train_loss=0.0041769305

Batch 9210, train_perplexity=1.0041796, train_loss=0.0041709235

Batch 9220, train_perplexity=1.0041736, train_loss=0.00416498

Batch 9230, train_perplexity=1.0041677, train_loss=0.004159062

Batch 9240, train_perplexity=1.0041617, train_loss=0.004153109

Batch 9250, train_perplexity=1.0041558, train_loss=0.0041471706

Batch 9260, train_perplexity=1.0041499, train_loss=0.004141318

Batch 9270, train_perplexity=1.0041441, train_loss=0.0041354867

Batch 9280, train_perplexity=1.0041381, train_loss=0.004129548

Batch 9290, train_perplexity=1.0041323, train_loss=0.004123767

Batch 9300, train_perplexity=1.0041264, train_loss=0.0041179108

Batch 9310, train_perplexity=1.0041206, train_loss=0.004112091

Batch 9320, train_perplexity=1.0041147, train_loss=0.0041062552

Batch 9330, train_perplexity=1.0041089, train_loss=0.0041004773

Batch 9340, train_perplexity=1.0041031, train_loss=0.0040947283

Batch 9350, train_perplexity=1.0040973, train_loss=0.0040889615

Batch 9360, train_perplexity=1.0040916, train_loss=0.0040832213

Batch 9370, train_perplexity=1.0040858, train_loss=0.004077511

Batch 9380, train_perplexity=1.00408, train_loss=0.0040718005

Batch 9390, train_perplexity=1.0040743, train_loss=0.004066051

Batch 9400, train_perplexity=1.0040686, train_loss=0.0040603406

Batch 9410, train_perplexity=1.004063, train_loss=0.0040547214

Batch 9420, train_perplexity=1.0040573, train_loss=0.004049035

Batch 9430, train_perplexity=1.0040516, train_loss=0.0040433956

Batch 9440, train_perplexity=1.004046, train_loss=0.0040377527

Batch 9450, train_perplexity=1.0040402, train_loss=0.0040321276

Batch 9460, train_perplexity=1.0040348, train_loss=0.0040266104

Batch 9470, train_perplexity=1.0040292, train_loss=0.0040210206

Batch 9480, train_perplexity=1.0040236, train_loss=0.0040154876

Batch 9490, train_perplexity=1.0040181, train_loss=0.004009961

Batch 9500, train_perplexity=1.0040125, train_loss=0.0040043984

Batch 9510, train_perplexity=1.0040069, train_loss=0.003998833

Batch 9520, train_perplexity=1.0040014, train_loss=0.0039933706

Batch 9530, train_perplexity=1.0039959, train_loss=0.0039879237

Batch 9540, train_perplexity=1.0039903, train_loss=0.00398237

Batch 9550, train_perplexity=1.0039848, train_loss=0.003976902

Batch 9560, train_perplexity=1.0039794, train_loss=0.00397149

Batch 9570, train_perplexity=1.003974, train_loss=0.0039660493

Batch 9580, train_perplexity=1.0039685, train_loss=0.0039606406

Batch 9590, train_perplexity=1.0039631, train_loss=0.0039552436

Batch 9600, train_perplexity=1.0039576, train_loss=0.003949811

Batch 9610, train_perplexity=1.0039523, train_loss=0.0039444678

Batch 9620, train_perplexity=1.0039468, train_loss=0.003939056

Batch 9630, train_perplexity=1.0039415, train_loss=0.0039337417

Batch 9640, train_perplexity=1.003936, train_loss=0.003928369

Batch 9650, train_perplexity=1.0039308, train_loss=0.0039230906

Batch 9660, train_perplexity=1.0039254, train_loss=0.0039177886

Batch 9670, train_perplexity=1.0039201, train_loss=0.0039124684

Batch 9680, train_perplexity=1.003915, train_loss=0.003907255

Batch 9690, train_perplexity=1.0039096, train_loss=0.0039020157

Batch 9700, train_perplexity=1.0039043, train_loss=0.0038966981

Batch 9710, train_perplexity=1.0038991, train_loss=0.003891482

Batch 9720, train_perplexity=1.0038939, train_loss=0.0038862482

Batch 9730, train_perplexity=1.0038886, train_loss=0.0038810382

Batch 9740, train_perplexity=1.0038834, train_loss=0.0038758628

Batch 9750, train_perplexity=1.003878, train_loss=0.0038705426

Batch 9760, train_perplexity=1.003873, train_loss=0.0038654688

Batch 9770, train_perplexity=1.0038677, train_loss=0.003860291

Batch 9780, train_perplexity=1.0038625, train_loss=0.0038550687

Batch 9790, train_perplexity=1.0038574, train_loss=0.0038499413

Batch 9800, train_perplexity=1.0038522, train_loss=0.0038448703

Batch 9810, train_perplexity=1.0038471, train_loss=0.0038397987

Batch 9820, train_perplexity=1.0038421, train_loss=0.003834707
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9830, train_perplexity=1.0038369, train_loss=0.0038295616

Batch 9840, train_perplexity=1.0038319, train_loss=0.0038245707

Batch 9850, train_perplexity=1.0038267, train_loss=0.0038194547

Batch 9860, train_perplexity=1.0038217, train_loss=0.0038144845

Batch 9870, train_perplexity=1.0038166, train_loss=0.0038093417

Batch 9880, train_perplexity=1.0038116, train_loss=0.0038043447

Batch 9890, train_perplexity=1.0038066, train_loss=0.0037993921

Batch 9900, train_perplexity=1.0038016, train_loss=0.0037943742

Batch 9910, train_perplexity=1.0037966, train_loss=0.0037893471

Batch 9920, train_perplexity=1.0037916, train_loss=0.0037844211

Batch 9930, train_perplexity=1.0037866, train_loss=0.0037794474

Batch 9940, train_perplexity=1.0037817, train_loss=0.0037745037

Batch 9950, train_perplexity=1.0037767, train_loss=0.0037695388

Batch 9960, train_perplexity=1.0037718, train_loss=0.0037646662

Batch 9970, train_perplexity=1.0037669, train_loss=0.0037597637

Batch 9980, train_perplexity=1.0037619, train_loss=0.0037548554

Batch 9990, train_perplexity=1.003757, train_loss=0.003749988

Batch 10000, train_perplexity=1.0037521, train_loss=0.0037451098

Batch 10010, train_perplexity=1.0037472, train_loss=0.00374021

Batch 10020, train_perplexity=1.0037425, train_loss=0.0037354676

Batch 10030, train_perplexity=1.0037376, train_loss=0.0037305534

Batch 10040, train_perplexity=1.0037327, train_loss=0.0037257513

Batch 10050, train_perplexity=1.0037279, train_loss=0.003720944

Batch 10060, train_perplexity=1.003723, train_loss=0.003716142

Batch 10070, train_perplexity=1.0037183, train_loss=0.0037113107

Batch 10080, train_perplexity=1.0037135, train_loss=0.0037066007

Batch 10090, train_perplexity=1.0037087, train_loss=0.0037018554

Batch 10100, train_perplexity=1.0037038, train_loss=0.0036970442

Batch 10110, train_perplexity=1.0036992, train_loss=0.003692379

Batch 10120, train_perplexity=1.0036944, train_loss=0.0036876483

Batch 10130, train_perplexity=1.0036896, train_loss=0.0036829058

Batch 10140, train_perplexity=1.003685, train_loss=0.0036782492

Batch 10150, train_perplexity=1.0036803, train_loss=0.003673539

Batch 10160, train_perplexity=1.0036756, train_loss=0.0036688352

Batch 10170, train_perplexity=1.0036709, train_loss=0.0036641662

Batch 10180, train_perplexity=1.0036662, train_loss=0.0036595159

Batch 10190, train_perplexity=1.0036616, train_loss=0.0036549033

Batch 10200, train_perplexity=1.0036569, train_loss=0.003650223

Batch 10210, train_perplexity=1.0036522, train_loss=0.003645596

Batch 10220, train_perplexity=1.0036476, train_loss=0.0036409807

Batch 10230, train_perplexity=1.0036429, train_loss=0.0036363564

Batch 10240, train_perplexity=1.0036384, train_loss=0.0036318302

Batch 10250, train_perplexity=1.0036339, train_loss=0.003627218

Batch 10260, train_perplexity=1.0036292, train_loss=0.0036226828

Batch 10270, train_perplexity=1.0036247, train_loss=0.0036180853

Batch 10280, train_perplexity=1.00362, train_loss=0.0036135472

Batch 10290, train_perplexity=1.0036156, train_loss=0.003609104

Batch 10300, train_perplexity=1.003611, train_loss=0.0036045122

Batch 10310, train_perplexity=1.0036066, train_loss=0.0036000155

Batch 10320, train_perplexity=1.003602, train_loss=0.003595522

Batch 10330, train_perplexity=1.0035975, train_loss=0.0035910103

Batch 10340, train_perplexity=1.003593, train_loss=0.0035864986

Batch 10350, train_perplexity=1.0035884, train_loss=0.0035820492

Batch 10360, train_perplexity=1.003584, train_loss=0.0035776058

Batch 10370, train_perplexity=1.0035795, train_loss=0.0035731567

Batch 10380, train_perplexity=1.0035751, train_loss=0.003568713

Batch 10390, train_perplexity=1.0035707, train_loss=0.0035642637

Batch 10400, train_perplexity=1.0035663, train_loss=0.0035598972

Batch 10410, train_perplexity=1.0035617, train_loss=0.003555424

Batch 10420, train_perplexity=1.0035573, train_loss=0.003551016

Batch 10430, train_perplexity=1.0035529, train_loss=0.0035466466

Batch 10440, train_perplexity=1.0035485, train_loss=0.0035422596

Batch 10450, train_perplexity=1.0035441, train_loss=0.0035378546

Batch 10460, train_perplexity=1.0035398, train_loss=0.0035335207

Batch 10470, train_perplexity=1.0035354, train_loss=0.0035291812

Batch 10480, train_perplexity=1.0035311, train_loss=0.0035249006

Batch 10490, train_perplexity=1.0035267, train_loss=0.0035205015

Batch 10500, train_perplexity=1.0035224, train_loss=0.003516159

Batch 10510, train_perplexity=1.0035181, train_loss=0.003511887

Batch 10520, train_perplexity=1.0035137, train_loss=0.003507592

Batch 10530, train_perplexity=1.0035094, train_loss=0.0035033051

Batch 10540, train_perplexity=1.0035051, train_loss=0.0034990306

Batch 10550, train_perplexity=1.0035009, train_loss=0.003494771

Batch 10560, train_perplexity=1.0034966, train_loss=0.0034905113

Batch 10570, train_perplexity=1.0034924, train_loss=0.0034862426

Batch 10580, train_perplexity=1.0034881, train_loss=0.0034820242

Batch 10590, train_perplexity=1.0034839, train_loss=0.0034778118

Batch 10600, train_perplexity=1.0034796, train_loss=0.0034736171

Batch 10610, train_perplexity=1.0034754, train_loss=0.003469405

Batch 10620, train_perplexity=1.0034711, train_loss=0.0034651598

Batch 10630, train_perplexity=1.003467, train_loss=0.0034609828

Batch 10640, train_perplexity=1.0034628, train_loss=0.003456803

Batch 10650, train_perplexity=1.0034586, train_loss=0.0034526084

Batch 10660, train_perplexity=1.0034544, train_loss=0.0034484405

Batch 10670, train_perplexity=1.0034502, train_loss=0.0034442758

Batch 10680, train_perplexity=1.0034461, train_loss=0.003440161

Batch 10690, train_perplexity=1.0034419, train_loss=0.0034360313

Batch 10700, train_perplexity=1.0034378, train_loss=0.0034318753

Batch 10710, train_perplexity=1.0034336, train_loss=0.0034277579

Batch 10720, train_perplexity=1.0034295, train_loss=0.003423723

Batch 10730, train_perplexity=1.0034255, train_loss=0.0034195995

Batch 10740, train_perplexity=1.0034213, train_loss=0.0034154877

Batch 10750, train_perplexity=1.0034173, train_loss=0.00341143

Batch 10760, train_perplexity=1.0034131, train_loss=0.0034073265

Batch 10770, train_perplexity=1.003409, train_loss=0.0034032888

Batch 10780, train_perplexity=1.0034051, train_loss=0.0033992545

Batch 10790, train_perplexity=1.0034009, train_loss=0.0033951392

Batch 10800, train_perplexity=1.003397, train_loss=0.0033911853

Batch 10810, train_perplexity=1.0033929, train_loss=0.0033871357

Batch 10820, train_perplexity=1.0033889, train_loss=0.0033831247

Batch 10830, train_perplexity=1.0033848, train_loss=0.0033791438

Batch 10840, train_perplexity=1.0033808, train_loss=0.0033751

Batch 10850, train_perplexity=1.0033768, train_loss=0.0033711037

Batch 10860, train_perplexity=1.0033728, train_loss=0.0033671255

Batch 10870, train_perplexity=1.003369, train_loss=0.0033632778

Batch 10880, train_perplexity=1.0033649, train_loss=0.003359225

Batch 10890, train_perplexity=1.003361, train_loss=0.003355309

Batch 10900, train_perplexity=1.0033569, train_loss=0.0033513424

Batch 10910, train_perplexity=1.003353, train_loss=0.0033473815

Batch 10920, train_perplexity=1.0033492, train_loss=0.0033435489

Batch 10930, train_perplexity=1.0033451, train_loss=0.0033395584

Batch 10940, train_perplexity=1.0033412, train_loss=0.0033356184

Batch 10950, train_perplexity=1.0033373, train_loss=0.0033317558

Batch 10960, train_perplexity=1.0033333, train_loss=0.0033277948

Batch 10970, train_perplexity=1.0033295, train_loss=0.003323974

Batch 10980, train_perplexity=1.0033257, train_loss=0.0033201375

Batch 10990, train_perplexity=1.0033218, train_loss=0.0033162364

Batch 11000, train_perplexity=1.0033178, train_loss=0.0033123915

Batch 11010, train_perplexity=1.003314, train_loss=0.0033085109

Batch 11020, train_perplexity=1.0033101, train_loss=0.0033046687

Batch 11030, train_perplexity=1.0033063, train_loss=0.003300797

Batch 11040, train_perplexity=1.0033025, train_loss=0.003297041

Batch 11050, train_perplexity=1.0032986, train_loss=0.0032932167

Batch 11060, train_perplexity=1.0032948, train_loss=0.0032893775

Batch 11070, train_perplexity=1.0032909, train_loss=0.0032855384

Batch 11080, train_perplexity=1.0032872, train_loss=0.0032817821
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11090, train_perplexity=1.0032834, train_loss=0.0032779789

Batch 11100, train_perplexity=1.0032796, train_loss=0.0032741842

Batch 11110, train_perplexity=1.0032758, train_loss=0.0032704398

Batch 11120, train_perplexity=1.003272, train_loss=0.0032667373

Batch 11130, train_perplexity=1.0032682, train_loss=0.0032629692

Batch 11140, train_perplexity=1.0032645, train_loss=0.0032592188

Batch 11150, train_perplexity=1.0032607, train_loss=0.0032553766

Batch 11160, train_perplexity=1.003257, train_loss=0.003251754

Batch 11170, train_perplexity=1.0032532, train_loss=0.0032479381

Batch 11180, train_perplexity=1.0032495, train_loss=0.003244313

Batch 11190, train_perplexity=1.0032458, train_loss=0.0032405448

Batch 11200, train_perplexity=1.003242, train_loss=0.0032368242

Batch 11210, train_perplexity=1.0032383, train_loss=0.0032331152

Batch 11220, train_perplexity=1.0032347, train_loss=0.0032295224

Batch 11230, train_perplexity=1.0032309, train_loss=0.0032257186

Batch 11240, train_perplexity=1.0032274, train_loss=0.0032221433

Batch 11250, train_perplexity=1.0032237, train_loss=0.0032184673

Batch 11260, train_perplexity=1.0032201, train_loss=0.0032148683

Batch 11270, train_perplexity=1.0032163, train_loss=0.0032111416

Batch 11280, train_perplexity=1.0032127, train_loss=0.0032075814

Batch 11290, train_perplexity=1.0032091, train_loss=0.0032039168

Batch 11300, train_perplexity=1.0032054, train_loss=0.0032002823

Batch 11310, train_perplexity=1.0032017, train_loss=0.0031966623

Batch 11320, train_perplexity=1.0031981, train_loss=0.0031930038

Batch 11330, train_perplexity=1.0031946, train_loss=0.0031895088

Batch 11340, train_perplexity=1.003191, train_loss=0.003185877

Batch 11350, train_perplexity=1.0031873, train_loss=0.003182269

Batch 11360, train_perplexity=1.0031838, train_loss=0.003178753

Batch 11370, train_perplexity=1.0031803, train_loss=0.0031752102

Batch 11380, train_perplexity=1.0031766, train_loss=0.0031715902

Batch 11390, train_perplexity=1.0031731, train_loss=0.0031680893

Batch 11400, train_perplexity=1.0031695, train_loss=0.0031645019

Batch 11410, train_perplexity=1.0031661, train_loss=0.0031610394

Batch 11420, train_perplexity=1.0031624, train_loss=0.00315744

Batch 11430, train_perplexity=1.0031589, train_loss=0.0031539358

Batch 11440, train_perplexity=1.0031554, train_loss=0.003150393

Batch 11450, train_perplexity=1.0031519, train_loss=0.0031469897

Batch 11460, train_perplexity=1.0031483, train_loss=0.0031434142

Batch 11470, train_perplexity=1.003145, train_loss=0.0031400318

Batch 11480, train_perplexity=1.0031414, train_loss=0.0031364565

Batch 11490, train_perplexity=1.003138, train_loss=0.0031329845

Batch 11500, train_perplexity=1.0031344, train_loss=0.0031295014

Batch 11510, train_perplexity=1.003131, train_loss=0.003126083

Batch 11520, train_perplexity=1.0031276, train_loss=0.0031226915

Batch 11530, train_perplexity=1.003124, train_loss=0.0031191576

Batch 11540, train_perplexity=1.0031207, train_loss=0.0031158018

Batch 11550, train_perplexity=1.0031172, train_loss=0.003112324

Batch 11560, train_perplexity=1.0031137, train_loss=0.0031088642

Batch 11570, train_perplexity=1.0031103, train_loss=0.0031054993

Batch 11580, train_perplexity=1.0031068, train_loss=0.003102072

Batch 11590, train_perplexity=1.0031035, train_loss=0.0030986955

Batch 11600, train_perplexity=1.0031002, train_loss=0.0030953307

Batch 11610, train_perplexity=1.0030967, train_loss=0.0030919244

Batch 11620, train_perplexity=1.0030932, train_loss=0.003088491

Batch 11630, train_perplexity=1.0030899, train_loss=0.0030851676

Batch 11640, train_perplexity=1.0030866, train_loss=0.0030817613

Batch 11650, train_perplexity=1.0030831, train_loss=0.003078426

Batch 11660, train_perplexity=1.0030798, train_loss=0.0030750553

Batch 11670, train_perplexity=1.0030764, train_loss=0.003071738

Batch 11680, train_perplexity=1.0030731, train_loss=0.0030683582

Batch 11690, train_perplexity=1.0030698, train_loss=0.0030650673

Batch 11700, train_perplexity=1.0030664, train_loss=0.0030617737

Batch 11710, train_perplexity=1.0030631, train_loss=0.0030584172

Batch 11720, train_perplexity=1.0030597, train_loss=0.0030551031

Batch 11730, train_perplexity=1.0030565, train_loss=0.003051836

Batch 11740, train_perplexity=1.0030532, train_loss=0.0030484884

Batch 11750, train_perplexity=1.0030499, train_loss=0.00304521

Batch 11760, train_perplexity=1.0030466, train_loss=0.0030419459

Batch 11770, train_perplexity=1.0030433, train_loss=0.003038652

Batch 11780, train_perplexity=1.00304, train_loss=0.0030353433

Batch 11790, train_perplexity=1.0030367, train_loss=0.003032106

Batch 11800, train_perplexity=1.0030334, train_loss=0.003028839

Batch 11810, train_perplexity=1.0030302, train_loss=0.0030255718

Batch 11820, train_perplexity=1.003027, train_loss=0.0030223848

Batch 11830, train_perplexity=1.0030236, train_loss=0.003019079

Batch 11840, train_perplexity=1.0030204, train_loss=0.0030158595

Batch 11850, train_perplexity=1.0030172, train_loss=0.003012649

Batch 11860, train_perplexity=1.003014, train_loss=0.0030094497

Batch 11870, train_perplexity=1.0030107, train_loss=0.0030062008

Batch 11880, train_perplexity=1.0030075, train_loss=0.0030029782

Batch 11890, train_perplexity=1.0030042, train_loss=0.0029997465

Batch 11900, train_perplexity=1.0030012, train_loss=0.0029966664

Batch 11910, train_perplexity=1.0029979, train_loss=0.0029934142

Batch 11920, train_perplexity=1.0029948, train_loss=0.0029902658

Batch 11930, train_perplexity=1.0029916, train_loss=0.0029870844

Batch 11940, train_perplexity=1.0029883, train_loss=0.0029838914

Batch 11950, train_perplexity=1.0029852, train_loss=0.0029807552

Batch 11960, train_perplexity=1.002982, train_loss=0.0029775738

Batch 11970, train_perplexity=1.0029788, train_loss=0.0029743807

Batch 11980, train_perplexity=1.0029757, train_loss=0.0029712976

Batch 11990, train_perplexity=1.0029725, train_loss=0.0029680599

Batch 12000, train_perplexity=1.0029694, train_loss=0.002964992

Batch 12010, train_perplexity=1.0029663, train_loss=0.0029619234

Batch 12020, train_perplexity=1.0029631, train_loss=0.0029587124

Batch 12030, train_perplexity=1.00296, train_loss=0.002955576

Batch 12040, train_perplexity=1.0029569, train_loss=0.0029524597

Batch 12050, train_perplexity=1.0029538, train_loss=0.0029494092

Batch 12060, train_perplexity=1.0029507, train_loss=0.002946326

Batch 12070, train_perplexity=1.0029476, train_loss=0.0029432727

Batch 12080, train_perplexity=1.0029446, train_loss=0.0029402194

Batch 12090, train_perplexity=1.0029415, train_loss=0.002937127

Batch 12100, train_perplexity=1.0029384, train_loss=0.0029340582

Batch 12110, train_perplexity=1.0029353, train_loss=0.0029309662

Batch 12120, train_perplexity=1.0029322, train_loss=0.0029279124

Batch 12130, train_perplexity=1.0029292, train_loss=0.002924871

Batch 12140, train_perplexity=1.0029261, train_loss=0.0029218323

Batch 12150, train_perplexity=1.0029231, train_loss=0.002918811

Batch 12160, train_perplexity=1.00292, train_loss=0.0029157544

Batch 12170, train_perplexity=1.0029169, train_loss=0.002912692

Batch 12180, train_perplexity=1.002914, train_loss=0.0029096594

Batch 12190, train_perplexity=1.0029109, train_loss=0.002906665

Batch 12200, train_perplexity=1.0029079, train_loss=0.0029036407

Batch 12210, train_perplexity=1.0029049, train_loss=0.0029006288

Batch 12220, train_perplexity=1.0029019, train_loss=0.0028976877

Batch 12230, train_perplexity=1.0028989, train_loss=0.0028947056

Batch 12240, train_perplexity=1.0028958, train_loss=0.0028916816

Batch 12250, train_perplexity=1.002893, train_loss=0.0028887736

Batch 12260, train_perplexity=1.00289, train_loss=0.0028858029

Batch 12270, train_perplexity=1.0028869, train_loss=0.0028827493

Batch 12280, train_perplexity=1.0028839, train_loss=0.0028797993

Batch 12290, train_perplexity=1.002881, train_loss=0.0028768615

Batch 12300, train_perplexity=1.0028781, train_loss=0.0028738966

Batch 12310, train_perplexity=1.0028751, train_loss=0.002870935

Batch 12320, train_perplexity=1.0028721, train_loss=0.002868006

Batch 12330, train_perplexity=1.0028691, train_loss=0.002865059
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 12340, train_perplexity=1.0028661, train_loss=0.0028621003

Batch 12350, train_perplexity=1.0028633, train_loss=0.0028592278

Batch 12360, train_perplexity=1.0028603, train_loss=0.0028562183

Batch 12370, train_perplexity=1.0028574, train_loss=0.002853393

Batch 12380, train_perplexity=1.0028546, train_loss=0.0028505027

Batch 12390, train_perplexity=1.0028517, train_loss=0.002847642

Batch 12400, train_perplexity=1.0028487, train_loss=0.0028446354

Batch 12410, train_perplexity=1.0028458, train_loss=0.002841751

Batch 12420, train_perplexity=1.0028429, train_loss=0.002838911

Batch 12430, train_perplexity=1.00284, train_loss=0.0028360323

Batch 12440, train_perplexity=1.0028372, train_loss=0.0028331392

Batch 12450, train_perplexity=1.0028343, train_loss=0.0028303494

Batch 12460, train_perplexity=1.0028315, train_loss=0.002827438

Batch 12470, train_perplexity=1.0028285, train_loss=0.0028245058

Batch 12480, train_perplexity=1.0028257, train_loss=0.0028217046

Batch 12490, train_perplexity=1.0028229, train_loss=0.002818888

Batch 12500, train_perplexity=1.0028201, train_loss=0.0028161076

Batch 12510, train_perplexity=1.0028172, train_loss=0.0028131693

Batch 12520, train_perplexity=1.0028144, train_loss=0.0028104512

Batch 12530, train_perplexity=1.0028116, train_loss=0.0028075662

Batch 12540, train_perplexity=1.0028086, train_loss=0.0028046789

Batch 12550, train_perplexity=1.0028058, train_loss=0.002801877

Batch 12560, train_perplexity=1.0028031, train_loss=0.0027991352

Batch 12570, train_perplexity=1.0028002, train_loss=0.0027962741

Batch 12580, train_perplexity=1.0027975, train_loss=0.0027935258

Batch 12590, train_perplexity=1.0027946, train_loss=0.0027907514

Batch 12600, train_perplexity=1.0027918, train_loss=0.0027879318

Batch 12610, train_perplexity=1.002789, train_loss=0.0027851362

Batch 12620, train_perplexity=1.0027863, train_loss=0.0027823912

Batch 12630, train_perplexity=1.0027835, train_loss=0.0027796342

Batch 12640, train_perplexity=1.0027807, train_loss=0.0027768682

Batch 12650, train_perplexity=1.0027779, train_loss=0.0027740665

Batch 12660, train_perplexity=1.0027752, train_loss=0.0027713333

Batch 12670, train_perplexity=1.0027725, train_loss=0.0027685938

Batch 12680, train_perplexity=1.0027696, train_loss=0.0027657836

Batch 12690, train_perplexity=1.002767, train_loss=0.0027630918

Batch 12700, train_perplexity=1.0027642, train_loss=0.0027603584

Batch 12710, train_perplexity=1.0027615, train_loss=0.0027576666

Batch 12720, train_perplexity=1.0027587, train_loss=0.0027548978

Batch 12730, train_perplexity=1.002756, train_loss=0.0027521586

Batch 12740, train_perplexity=1.0027533, train_loss=0.0027495082

Batch 12750, train_perplexity=1.0027505, train_loss=0.0027467571

Batch 12760, train_perplexity=1.0027478, train_loss=0.0027440563

Batch 12770, train_perplexity=1.0027452, train_loss=0.0027413794

Batch 12780, train_perplexity=1.0027424, train_loss=0.0027386816

Batch 12790, train_perplexity=1.0027398, train_loss=0.002736005

Batch 12800, train_perplexity=1.002737, train_loss=0.00273334

Batch 12810, train_perplexity=1.0027344, train_loss=0.0027306601

Batch 12820, train_perplexity=1.0027318, train_loss=0.0027280184

Batch 12830, train_perplexity=1.002729, train_loss=0.0027253237

Batch 12840, train_perplexity=1.0027264, train_loss=0.0027226708

Batch 12850, train_perplexity=1.0027237, train_loss=0.002719955

Batch 12860, train_perplexity=1.0027211, train_loss=0.0027173879

Batch 12870, train_perplexity=1.0027183, train_loss=0.002714693

Batch 12880, train_perplexity=1.0027158, train_loss=0.0027120963

Batch 12890, train_perplexity=1.0027131, train_loss=0.002709461

Batch 12900, train_perplexity=1.0027105, train_loss=0.0027068255

Batch 12910, train_perplexity=1.0027078, train_loss=0.0027042197

Batch 12920, train_perplexity=1.0027052, train_loss=0.0027015368

Batch 12930, train_perplexity=1.0027026, train_loss=0.0026989665

Batch 12940, train_perplexity=1.0027, train_loss=0.0026963488

Batch 12950, train_perplexity=1.0026973, train_loss=0.002693663

Batch 12960, train_perplexity=1.0026947, train_loss=0.0026911343

Batch 12970, train_perplexity=1.0026922, train_loss=0.0026885585

Batch 12980, train_perplexity=1.0026896, train_loss=0.0026859585

Batch 12990, train_perplexity=1.002687, train_loss=0.0026834244

Batch 13000, train_perplexity=1.0026845, train_loss=0.0026808272

Batch 13010, train_perplexity=1.0026817, train_loss=0.002678156

Batch 13020, train_perplexity=1.0026792, train_loss=0.0026756602

Batch 13030, train_perplexity=1.0026767, train_loss=0.0026731612

Batch 13040, train_perplexity=1.0026741, train_loss=0.002670505

Batch 13050, train_perplexity=1.0026716, train_loss=0.002667997

Batch 13060, train_perplexity=1.002669, train_loss=0.0026653884

Batch 13070, train_perplexity=1.0026665, train_loss=0.0026628776

Batch 13080, train_perplexity=1.0026639, train_loss=0.0026603253

Batch 13090, train_perplexity=1.0026613, train_loss=0.0026578116

Batch 13100, train_perplexity=1.0026588, train_loss=0.002655271

Batch 13110, train_perplexity=1.0026562, train_loss=0.0026527215

Batch 13120, train_perplexity=1.0026537, train_loss=0.0026502167

Batch 13130, train_perplexity=1.0026512, train_loss=0.002647709

Batch 13140, train_perplexity=1.0026486, train_loss=0.0026451387

Batch 13150, train_perplexity=1.0026462, train_loss=0.0026426902

Batch 13160, train_perplexity=1.0026437, train_loss=0.0026401915

Batch 13170, train_perplexity=1.0026412, train_loss=0.0026376806

Batch 13180, train_perplexity=1.0026387, train_loss=0.0026351726

Batch 13190, train_perplexity=1.0026362, train_loss=0.0026327153

Batch 13200, train_perplexity=1.0026337, train_loss=0.0026302044

Batch 13210, train_perplexity=1.0026311, train_loss=0.0026276638

Batch 13220, train_perplexity=1.0026287, train_loss=0.0026252866

Batch 13230, train_perplexity=1.0026262, train_loss=0.002622758

Batch 13240, train_perplexity=1.0026238, train_loss=0.0026203091

Batch 13250, train_perplexity=1.0026213, train_loss=0.002617855

Batch 13260, train_perplexity=1.0026188, train_loss=0.002615326

Batch 13270, train_perplexity=1.0026164, train_loss=0.0026129875

Batch 13280, train_perplexity=1.0026139, train_loss=0.0026104471

Batch 13290, train_perplexity=1.0026113, train_loss=0.0026079302

Batch 13300, train_perplexity=1.0026089, train_loss=0.0026055202

Batch 13310, train_perplexity=1.0026065, train_loss=0.0026031192

Batch 13320, train_perplexity=1.002604, train_loss=0.0026006498

Batch 13330, train_perplexity=1.0026016, train_loss=0.0025981984

Batch 13340, train_perplexity=1.0025991, train_loss=0.0025957527

Batch 13350, train_perplexity=1.0025967, train_loss=0.0025933427

Batch 13360, train_perplexity=1.0025944, train_loss=0.0025909804

Batch 13370, train_perplexity=1.0025918, train_loss=0.0025884637

Batch 13380, train_perplexity=1.0025895, train_loss=0.0025861012

Batch 13390, train_perplexity=1.0025871, train_loss=0.0025837298

Batch 13400, train_perplexity=1.0025846, train_loss=0.0025812904

Batch 13410, train_perplexity=1.0025822, train_loss=0.002578892

Batch 13420, train_perplexity=1.0025798, train_loss=0.002576488

Batch 13430, train_perplexity=1.0025774, train_loss=0.002574072

Batch 13440, train_perplexity=1.002575, train_loss=0.0025716946

Batch 13450, train_perplexity=1.0025727, train_loss=0.0025693532

Batch 13460, train_perplexity=1.0025703, train_loss=0.0025669192

Batch 13470, train_perplexity=1.0025678, train_loss=0.002564536

Batch 13480, train_perplexity=1.0025655, train_loss=0.0025621853

Batch 13490, train_perplexity=1.0025631, train_loss=0.0025598498

Batch 13500, train_perplexity=1.0025607, train_loss=0.0025575021

Batch 13510, train_perplexity=1.0025584, train_loss=0.0025550802

Batch 13520, train_perplexity=1.0025561, train_loss=0.002552813

Batch 13530, train_perplexity=1.0025537, train_loss=0.0025504443

Batch 13540, train_perplexity=1.0025514, train_loss=0.0025481798

Batch 13550, train_perplexity=1.0025489, train_loss=0.0025457134

Batch 13560, train_perplexity=1.0025465, train_loss=0.0025433686

Batch 13570, train_perplexity=1.0025444, train_loss=0.0025411248

Batch 13580, train_perplexity=1.002542, train_loss=0.0025387625
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13590, train_perplexity=1.0025396, train_loss=0.0025363998

Batch 13600, train_perplexity=1.0025374, train_loss=0.0025341145

Batch 13610, train_perplexity=1.002535, train_loss=0.0025317934

Batch 13620, train_perplexity=1.0025326, train_loss=0.0025294519

Batch 13630, train_perplexity=1.0025303, train_loss=0.00252711

Batch 13640, train_perplexity=1.0025281, train_loss=0.002524875

Batch 13650, train_perplexity=1.0025258, train_loss=0.00252262

Batch 13660, train_perplexity=1.0025234, train_loss=0.0025202928

Batch 13670, train_perplexity=1.0025212, train_loss=0.0025180106

Batch 13680, train_perplexity=1.0025189, train_loss=0.00251574

Batch 13690, train_perplexity=1.0025166, train_loss=0.0025134666

Batch 13700, train_perplexity=1.0025144, train_loss=0.0025111514

Batch 13710, train_perplexity=1.002512, train_loss=0.0025088782

Batch 13720, train_perplexity=1.0025097, train_loss=0.0025066193

Batch 13730, train_perplexity=1.0025074, train_loss=0.002504349

Batch 13740, train_perplexity=1.0025052, train_loss=0.0025020638

Batch 13750, train_perplexity=1.0025029, train_loss=0.0024998349

Batch 13760, train_perplexity=1.0025007, train_loss=0.0024974956

Batch 13770, train_perplexity=1.0024984, train_loss=0.0024952549

Batch 13780, train_perplexity=1.0024961, train_loss=0.002493032

Batch 13790, train_perplexity=1.002494, train_loss=0.0024908744

Batch 13800, train_perplexity=1.0024916, train_loss=0.0024885447

Batch 13810, train_perplexity=1.0024893, train_loss=0.0024862562

Batch 13820, train_perplexity=1.0024872, train_loss=0.0024841074

Batch 13830, train_perplexity=1.002485, train_loss=0.0024819467

Batch 13840, train_perplexity=1.0024828, train_loss=0.0024796675

Batch 13850, train_perplexity=1.0024805, train_loss=0.002477489

Batch 13860, train_perplexity=1.0024782, train_loss=0.0024751944

Batch 13870, train_perplexity=1.002476, train_loss=0.0024729092

Batch 13880, train_perplexity=1.002474, train_loss=0.0024708435

Batch 13890, train_perplexity=1.0024718, train_loss=0.0024687154

Batch 13900, train_perplexity=1.0024695, train_loss=0.0024664688

Batch 13910, train_perplexity=1.0024673, train_loss=0.0024642306

Batch 13920, train_perplexity=1.002465, train_loss=0.0024620136

Batch 13930, train_perplexity=1.0024629, train_loss=0.002459844

Batch 13940, train_perplexity=1.0024607, train_loss=0.0024576536

Batch 13950, train_perplexity=1.0024585, train_loss=0.0024554217

Batch 13960, train_perplexity=1.0024563, train_loss=0.002453264

Batch 13970, train_perplexity=1.0024542, train_loss=0.0024511511

Batch 13980, train_perplexity=1.0024519, train_loss=0.0024489013

Batch 13990, train_perplexity=1.0024498, train_loss=0.0024467404

Batch 14000, train_perplexity=1.0024476, train_loss=0.0024446184

Batch 14010, train_perplexity=1.0024455, train_loss=0.0024424163

Batch 14020, train_perplexity=1.0024432, train_loss=0.002440223

Batch 14030, train_perplexity=1.0024409, train_loss=0.0024379757

Batch 14040, train_perplexity=1.002439, train_loss=0.0024360232

Batch 14050, train_perplexity=1.0024369, train_loss=0.0024338835

Batch 14060, train_perplexity=1.0024347, train_loss=0.0024318085

Batch 14070, train_perplexity=1.0024325, train_loss=0.002429547

Batch 14080, train_perplexity=1.0024304, train_loss=0.0024274962

Batch 14090, train_perplexity=1.0024282, train_loss=0.0024252404

Batch 14100, train_perplexity=1.0024261, train_loss=0.0024232075

Batch 14110, train_perplexity=1.002424, train_loss=0.0024210466

Batch 14120, train_perplexity=1.0024217, train_loss=0.0024187847

Batch 14130, train_perplexity=1.0024197, train_loss=0.00241677

Batch 14140, train_perplexity=1.0024176, train_loss=0.0024146447

Batch 14150, train_perplexity=1.0024154, train_loss=0.0024125255

Batch 14160, train_perplexity=1.0024133, train_loss=0.0024103737

Batch 14170, train_perplexity=1.0024112, train_loss=0.0024083052

Batch 14180, train_perplexity=1.0024091, train_loss=0.0024062302

Batch 14190, train_perplexity=1.002407, train_loss=0.0024040993

Batch 14200, train_perplexity=1.0024049, train_loss=0.0024020248

Batch 14210, train_perplexity=1.0024028, train_loss=0.0023999112

Batch 14220, train_perplexity=1.0024008, train_loss=0.0023978578

Batch 14230, train_perplexity=1.0023986, train_loss=0.0023957116

Batch 14240, train_perplexity=1.0023966, train_loss=0.0023936788

Batch 14250, train_perplexity=1.0023944, train_loss=0.002391607

Batch 14260, train_perplexity=1.0023924, train_loss=0.0023895532

Batch 14270, train_perplexity=1.0023903, train_loss=0.002387449

Batch 14280, train_perplexity=1.0023882, train_loss=0.0023853625

Batch 14290, train_perplexity=1.0023862, train_loss=0.0023833322

Batch 14300, train_perplexity=1.0023841, train_loss=0.0023812787

Batch 14310, train_perplexity=1.002382, train_loss=0.0023791534

Batch 14320, train_perplexity=1.0023799, train_loss=0.0023771261

Batch 14330, train_perplexity=1.0023779, train_loss=0.0023750812

Batch 14340, train_perplexity=1.0023758, train_loss=0.0023730244

Batch 14350, train_perplexity=1.0023738, train_loss=0.0023710001

Batch 14360, train_perplexity=1.0023718, train_loss=0.0023689256

Batch 14370, train_perplexity=1.0023698, train_loss=0.002366919

Batch 14380, train_perplexity=1.0023677, train_loss=0.002364934

Batch 14390, train_perplexity=1.0023656, train_loss=0.002362844

Batch 14400, train_perplexity=1.0023636, train_loss=0.0023608198

Batch 14410, train_perplexity=1.0023615, train_loss=0.0023587928

Batch 14420, train_perplexity=1.0023595, train_loss=0.0023567479

Batch 14430, train_perplexity=1.0023576, train_loss=0.0023547772

Batch 14440, train_perplexity=1.0023556, train_loss=0.0023527858

Batch 14450, train_perplexity=1.0023535, train_loss=0.0023507555

Batch 14460, train_perplexity=1.0023515, train_loss=0.0023487492

Batch 14470, train_perplexity=1.0023495, train_loss=0.0023467457

Batch 14480, train_perplexity=1.0023475, train_loss=0.0023447066

Batch 14490, train_perplexity=1.0023456, train_loss=0.0023427955

Batch 14500, train_perplexity=1.0023434, train_loss=0.0023407205

Batch 14510, train_perplexity=1.0023415, train_loss=0.0023387233

Batch 14520, train_perplexity=1.0023396, train_loss=0.0023368238

Batch 14530, train_perplexity=1.0023376, train_loss=0.0023348709

Batch 14540, train_perplexity=1.0023355, train_loss=0.0023328261

Batch 14550, train_perplexity=1.0023336, train_loss=0.0023309027

Batch 14560, train_perplexity=1.0023316, train_loss=0.0023288638

Batch 14570, train_perplexity=1.0023296, train_loss=0.0023268838

Batch 14580, train_perplexity=1.0023276, train_loss=0.0023249162

Batch 14590, train_perplexity=1.0023257, train_loss=0.0023229248

Batch 14600, train_perplexity=1.0023237, train_loss=0.0023210226

Batch 14610, train_perplexity=1.0023217, train_loss=0.0023190663

Batch 14620, train_perplexity=1.0023198, train_loss=0.0023170689

Batch 14630, train_perplexity=1.0023179, train_loss=0.0023151932

Batch 14640, train_perplexity=1.0023159, train_loss=0.0023131808

Batch 14650, train_perplexity=1.002314, train_loss=0.0023112488

Batch 14660, train_perplexity=1.002312, train_loss=0.0023093047

Batch 14670, train_perplexity=1.00231, train_loss=0.0023073428

Batch 14680, train_perplexity=1.0023081, train_loss=0.0023054136

Batch 14690, train_perplexity=1.0023062, train_loss=0.002303541

Batch 14700, train_perplexity=1.0023042, train_loss=0.0023015672

Batch 14710, train_perplexity=1.0023023, train_loss=0.0022996054

Batch 14720, train_perplexity=1.0023004, train_loss=0.002297685

Batch 14730, train_perplexity=1.0022984, train_loss=0.0022957618

Batch 14740, train_perplexity=1.0022966, train_loss=0.0022938894

Batch 14750, train_perplexity=1.0022947, train_loss=0.0022919984

Batch 14760, train_perplexity=1.0022928, train_loss=0.0022900724

Batch 14770, train_perplexity=1.0022908, train_loss=0.0022881967

Batch 14780, train_perplexity=1.0022888, train_loss=0.0022862377

Batch 14790, train_perplexity=1.002287, train_loss=0.0022843652

Batch 14800, train_perplexity=1.002285, train_loss=0.0022824

Batch 14810, train_perplexity=1.0022831, train_loss=0.002280551

Batch 14820, train_perplexity=1.0022813, train_loss=0.0022786604

Batch 14830, train_perplexity=1.0022794, train_loss=0.0022767491
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 14840, train_perplexity=1.0022775, train_loss=0.0022749207

Batch 14850, train_perplexity=1.0022756, train_loss=0.0022730124

Batch 14860, train_perplexity=1.0022737, train_loss=0.0022711488

Batch 14870, train_perplexity=1.0022719, train_loss=0.0022692848

Batch 14880, train_perplexity=1.0022699, train_loss=0.0022673435

Batch 14890, train_perplexity=1.0022681, train_loss=0.0022654827

Batch 14900, train_perplexity=1.0022662, train_loss=0.002263589

Batch 14910, train_perplexity=1.0022643, train_loss=0.002261755

Batch 14920, train_perplexity=1.0022624, train_loss=0.0022598407

Batch 14930, train_perplexity=1.0022606, train_loss=0.0022579767

Batch 14940, train_perplexity=1.0022588, train_loss=0.0022562137

Batch 14950, train_perplexity=1.0022569, train_loss=0.0022543708

Batch 14960, train_perplexity=1.0022551, train_loss=0.0022525247

Batch 14970, train_perplexity=1.0022532, train_loss=0.0022506313

Batch 14980, train_perplexity=1.0022514, train_loss=0.002248797

Batch 14990, train_perplexity=1.0022495, train_loss=0.002246921

Batch 15000, train_perplexity=1.0022476, train_loss=0.0022450634

Batch 15010, train_perplexity=1.0022458, train_loss=0.0022432825

Batch 15020, train_perplexity=1.002244, train_loss=0.0022414573

Batch 15030, train_perplexity=1.0022422, train_loss=0.0022396499

Batch 15040, train_perplexity=1.0022403, train_loss=0.00223778

Batch 15050, train_perplexity=1.0022384, train_loss=0.0022359428

Batch 15060, train_perplexity=1.0022366, train_loss=0.0022340938

Batch 15070, train_perplexity=1.0022347, train_loss=0.0022322417

Batch 15080, train_perplexity=1.0022329, train_loss=0.0022304342

Batch 15090, train_perplexity=1.002231, train_loss=0.0022285702

Batch 15100, train_perplexity=1.0022293, train_loss=0.0022268668

Batch 15110, train_perplexity=1.0022275, train_loss=0.0022250474

Batch 15120, train_perplexity=1.0022256, train_loss=0.0022232102

Batch 15130, train_perplexity=1.0022238, train_loss=0.0022213731

Batch 15140, train_perplexity=1.0022221, train_loss=0.0022196104

Batch 15150, train_perplexity=1.0022203, train_loss=0.0022178027

Batch 15160, train_perplexity=1.0022185, train_loss=0.002216001

Batch 15170, train_perplexity=1.0022167, train_loss=0.0022141878

Batch 15180, train_perplexity=1.0022149, train_loss=0.0022124099

Batch 15190, train_perplexity=1.0022131, train_loss=0.002210662

Batch 15200, train_perplexity=1.0022112, train_loss=0.0022087977

Batch 15210, train_perplexity=1.0022095, train_loss=0.002207059

Batch 15220, train_perplexity=1.0022078, train_loss=0.0022053286

Batch 15230, train_perplexity=1.002206, train_loss=0.002203518

Batch 15240, train_perplexity=1.0022042, train_loss=0.0022017255

Batch 15250, train_perplexity=1.0022024, train_loss=0.0021999537

Batch 15260, train_perplexity=1.0022006, train_loss=0.0021982116

Batch 15270, train_perplexity=1.0021989, train_loss=0.0021964933

Batch 15280, train_perplexity=1.002197, train_loss=0.002194665

Batch 15290, train_perplexity=1.0021952, train_loss=0.0021928693

Batch 15300, train_perplexity=1.0021936, train_loss=0.0021911717

Batch 15310, train_perplexity=1.0021918, train_loss=0.0021894001

Batch 15320, train_perplexity=1.00219, train_loss=0.0021875743

Batch 15330, train_perplexity=1.0021883, train_loss=0.0021859244

Batch 15340, train_perplexity=1.0021864, train_loss=0.0021840902

Batch 15350, train_perplexity=1.0021847, train_loss=0.0021824134

Batch 15360, train_perplexity=1.0021831, train_loss=0.0021806476

Batch 15370, train_perplexity=1.0021813, train_loss=0.002178947

Batch 15380, train_perplexity=1.0021795, train_loss=0.0021771751

Batch 15390, train_perplexity=1.0021778, train_loss=0.002175421

Batch 15400, train_perplexity=1.002176, train_loss=0.0021736762

Batch 15410, train_perplexity=1.0021744, train_loss=0.002171999

Batch 15420, train_perplexity=1.0021726, train_loss=0.0021702868

Batch 15430, train_perplexity=1.0021709, train_loss=0.0021685325

Batch 15440, train_perplexity=1.0021691, train_loss=0.0021668351

Batch 15450, train_perplexity=1.0021675, train_loss=0.002165063

Batch 15460, train_perplexity=1.0021658, train_loss=0.002163398

Batch 15470, train_perplexity=1.002164, train_loss=0.0021616798

Batch 15480, train_perplexity=1.0021622, train_loss=0.0021599317

Batch 15490, train_perplexity=1.0021605, train_loss=0.0021582074

Batch 15500, train_perplexity=1.0021589, train_loss=0.0021565126

Batch 15510, train_perplexity=1.0021572, train_loss=0.0021548835

Batch 15520, train_perplexity=1.0021554, train_loss=0.0021530879

Batch 15530, train_perplexity=1.0021538, train_loss=0.0021514078

Batch 15540, train_perplexity=1.002152, train_loss=0.0021497072

Batch 15550, train_perplexity=1.0021504, train_loss=0.0021480632

Batch 15560, train_perplexity=1.0021486, train_loss=0.0021463446

Batch 15570, train_perplexity=1.0021468, train_loss=0.0021445965

Batch 15580, train_perplexity=1.0021453, train_loss=0.0021429467

Batch 15590, train_perplexity=1.0021436, train_loss=0.002141377

Batch 15600, train_perplexity=1.002142, train_loss=0.0021396377

Batch 15610, train_perplexity=1.0021402, train_loss=0.0021378952

Batch 15620, train_perplexity=1.0021385, train_loss=0.0021362333

Batch 15630, train_perplexity=1.0021368, train_loss=0.0021345804

Batch 15640, train_perplexity=1.0021352, train_loss=0.0021329182

Batch 15650, train_perplexity=1.0021335, train_loss=0.0021312505

Batch 15660, train_perplexity=1.0021318, train_loss=0.002129544

Batch 15670, train_perplexity=1.0021302, train_loss=0.0021279412

Batch 15680, train_perplexity=1.0021285, train_loss=0.0021262108

Batch 15690, train_perplexity=1.0021269, train_loss=0.0021246294

Batch 15700, train_perplexity=1.0021251, train_loss=0.0021229344

Batch 15710, train_perplexity=1.0021235, train_loss=0.0021212162

Batch 15720, train_perplexity=1.0021219, train_loss=0.002119667

Batch 15730, train_perplexity=1.0021201, train_loss=0.002117937

Batch 15740, train_perplexity=1.0021186, train_loss=0.002116364

Batch 15750, train_perplexity=1.0021169, train_loss=0.0021146932

Batch 15760, train_perplexity=1.0021152, train_loss=0.0021130694

Batch 15770, train_perplexity=1.0021137, train_loss=0.0021114345

Batch 15780, train_perplexity=1.002112, train_loss=0.0021098168

Batch 15790, train_perplexity=1.0021104, train_loss=0.0021081907

Batch 15800, train_perplexity=1.0021088, train_loss=0.0021065525

Batch 15810, train_perplexity=1.0021071, train_loss=0.0021048724

Batch 15820, train_perplexity=1.0021056, train_loss=0.002103347

Batch 15830, train_perplexity=1.0021039, train_loss=0.0021017385

Batch 15840, train_perplexity=1.0021023, train_loss=0.00210005

Batch 15850, train_perplexity=1.0021007, train_loss=0.002098522

Batch 15860, train_perplexity=1.002099, train_loss=0.0020968448

Batch 15870, train_perplexity=1.0020975, train_loss=0.0020952423

Batch 15880, train_perplexity=1.0020958, train_loss=0.0020935978

Batch 15890, train_perplexity=1.0020941, train_loss=0.0020919833

Batch 15900, train_perplexity=1.0020926, train_loss=0.002090396

Batch 15910, train_perplexity=1.0020909, train_loss=0.0020887873

Batch 15920, train_perplexity=1.0020894, train_loss=0.0020872084

Batch 15930, train_perplexity=1.0020878, train_loss=0.002085615

Batch 15940, train_perplexity=1.0020862, train_loss=0.0020839793

Batch 15950, train_perplexity=1.0020846, train_loss=0.0020824275

Batch 15960, train_perplexity=1.002083, train_loss=0.0020808012

Batch 15970, train_perplexity=1.0020814, train_loss=0.002079249

Batch 15980, train_perplexity=1.0020798, train_loss=0.0020776433

Batch 15990, train_perplexity=1.0020783, train_loss=0.0020761387

Batch 16000, train_perplexity=1.0020766, train_loss=0.0020745127

Batch 16010, train_perplexity=1.0020751, train_loss=0.0020729369

Batch 16020, train_perplexity=1.0020735, train_loss=0.002071322

Batch 16030, train_perplexity=1.002072, train_loss=0.002069788

Batch 16040, train_perplexity=1.0020703, train_loss=0.0020681971

Batch 16050, train_perplexity=1.0020688, train_loss=0.0020665885

Batch 16060, train_perplexity=1.0020672, train_loss=0.0020650635

Batch 16070, train_perplexity=1.0020657, train_loss=0.0020635112

Batch 16080, train_perplexity=1.0020641, train_loss=0.002061992
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 16090, train_perplexity=1.0020626, train_loss=0.0020604248

Batch 16100, train_perplexity=1.002061, train_loss=0.0020588906

Batch 16110, train_perplexity=1.0020593, train_loss=0.0020572525

Batch 16120, train_perplexity=1.0020578, train_loss=0.0020557358

Batch 16130, train_perplexity=1.0020564, train_loss=0.0020542135

Batch 16140, train_perplexity=1.0020547, train_loss=0.0020526168

Batch 16150, train_perplexity=1.0020531, train_loss=0.0020510615

Batch 16160, train_perplexity=1.0020516, train_loss=0.0020495453

Batch 16170, train_perplexity=1.0020502, train_loss=0.002048014

Batch 16180, train_perplexity=1.0020486, train_loss=0.0020465362

Batch 16190, train_perplexity=1.0020471, train_loss=0.0020449127

Batch 16200, train_perplexity=1.0020455, train_loss=0.0020434766

Batch 16210, train_perplexity=1.002044, train_loss=0.0020419098

Batch 16220, train_perplexity=1.0020424, train_loss=0.0020403187

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 16230, train_perplexity=1.0020409, train_loss=0.0020388025

Batch 16240, train_perplexity=1.0020393, train_loss=0.002037271

Batch 16250, train_perplexity=1.0020379, train_loss=0.002035811

Batch 16260, train_perplexity=1.0020363, train_loss=0.0020342143

Batch 16270, train_perplexity=1.0020348, train_loss=0.0020327128

Batch 16280, train_perplexity=1.0020332, train_loss=0.0020311903

Batch 16290, train_perplexity=1.0020317, train_loss=0.00202965

Batch 16300, train_perplexity=1.0020303, train_loss=0.0020281812

Batch 16310, train_perplexity=1.0020287, train_loss=0.0020266587

Batch 16320, train_perplexity=1.0020272, train_loss=0.0020251481

Batch 16330, train_perplexity=1.0020257, train_loss=0.002023694

Batch 16340, train_perplexity=1.0020242, train_loss=0.0020221658

Batch 16350, train_perplexity=1.0020227, train_loss=0.0020207504

Batch 16360, train_perplexity=1.0020212, train_loss=0.0020192072

Batch 16370, train_perplexity=1.0020198, train_loss=0.0020176787

Batch 16380, train_perplexity=1.0020182, train_loss=0.002016168

Batch 16390, train_perplexity=1.0020168, train_loss=0.0020146964

Batch 16400, train_perplexity=1.0020152, train_loss=0.002013168

Batch 16410, train_perplexity=1.0020137, train_loss=0.002011684

Batch 16420, train_perplexity=1.0020123, train_loss=0.0020101885

Batch 16430, train_perplexity=1.0020108, train_loss=0.0020087492

Batch 16440, train_perplexity=1.0020093, train_loss=0.0020072297

Batch 16450, train_perplexity=1.0020078, train_loss=0.0020057787

Batch 16460, train_perplexity=1.0020063, train_loss=0.0020043335

Batch 16470, train_perplexity=1.0020049, train_loss=0.0020028618

Batch 16480, train_perplexity=1.0020033, train_loss=0.0020013303

Batch 16490, train_perplexity=1.0020019, train_loss=0.0019998793

Batch 16500, train_perplexity=1.0020003, train_loss=0.0019983659

Batch 16510, train_perplexity=1.0019989, train_loss=0.0019969116

Batch 16520, train_perplexity=1.0019975, train_loss=0.0019954604

Batch 16530, train_perplexity=1.0019959, train_loss=0.0019939886

Batch 16540, train_perplexity=1.0019946, train_loss=0.001992594

Batch 16550, train_perplexity=1.0019931, train_loss=0.00199111

Batch 16560, train_perplexity=1.0019916, train_loss=0.0019896084

Batch 16570, train_perplexity=1.0019901, train_loss=0.0019881246

Batch 16580, train_perplexity=1.0019885, train_loss=0.0019865872

Batch 16590, train_perplexity=1.0019872, train_loss=0.0019852165

Batch 16600, train_perplexity=1.0019857, train_loss=0.0019837504

Batch 16610, train_perplexity=1.0019842, train_loss=0.0019822964

Batch 16620, train_perplexity=1.0019828, train_loss=0.0019808155

Batch 16630, train_perplexity=1.0019814, train_loss=0.0019793673

Batch 16640, train_perplexity=1.0019798, train_loss=0.001977928

Batch 16650, train_perplexity=1.0019785, train_loss=0.0019765482

Batch 16660, train_perplexity=1.0019771, train_loss=0.0019751121

Batch 16670, train_perplexity=1.0019757, train_loss=0.0019736965

Batch 16680, train_perplexity=1.0019742, train_loss=0.0019722604

Batch 16690, train_perplexity=1.0019728, train_loss=0.001970794

Batch 16700, train_perplexity=1.0019712, train_loss=0.0019693342

Batch 16710, train_perplexity=1.0019699, train_loss=0.0019679782

Batch 16720, train_perplexity=1.0019684, train_loss=0.0019664883

Batch 16730, train_perplexity=1.001967, train_loss=0.001965067

Batch 16740, train_perplexity=1.0019655, train_loss=0.001963601

Batch 16750, train_perplexity=1.0019641, train_loss=0.0019622152

Batch 16760, train_perplexity=1.0019627, train_loss=0.0019607402

Batch 16770, train_perplexity=1.0019612, train_loss=0.0019593574

Batch 16780, train_perplexity=1.0019599, train_loss=0.0019579867

Batch 16790, train_perplexity=1.0019585, train_loss=0.0019565471

Batch 16800, train_perplexity=1.001957, train_loss=0.001955126

Batch 16810, train_perplexity=1.0019556, train_loss=0.001953761

Batch 16820, train_perplexity=1.0019543, train_loss=0.0019523841

Batch 16830, train_perplexity=1.0019526, train_loss=0.0019507962

Batch 16840, train_perplexity=1.0019513, train_loss=0.0019494847

Batch 16850, train_perplexity=1.0019499, train_loss=0.0019480305

Batch 16860, train_perplexity=1.0019486, train_loss=0.0019466835

Batch 16870, train_perplexity=1.001947, train_loss=0.0019451787

Batch 16880, train_perplexity=1.0019457, train_loss=0.0019438703

Batch 16890, train_perplexity=1.0019444, train_loss=0.0019424874

Batch 16900, train_perplexity=1.001943, train_loss=0.0019410779

Batch 16910, train_perplexity=1.0019416, train_loss=0.0019396893

Batch 16920, train_perplexity=1.0019403, train_loss=0.0019383183

Batch 16930, train_perplexity=1.0019387, train_loss=0.001936876

Batch 16940, train_perplexity=1.0019374, train_loss=0.001935523

Batch 16950, train_perplexity=1.001936, train_loss=0.0019340864

Batch 16960, train_perplexity=1.0019346, train_loss=0.0019327514

Batch 16970, train_perplexity=1.0019332, train_loss=0.0019313508

Batch 16980, train_perplexity=1.0019318, train_loss=0.0019299202

Batch 16990, train_perplexity=1.0019305, train_loss=0.001928588

Batch 17000, train_perplexity=1.0019292, train_loss=0.0019272589

Batch 17010, train_perplexity=1.0019277, train_loss=0.0019259175

Batch 17020, train_perplexity=1.0019264, train_loss=0.0019245138

Batch 17030, train_perplexity=1.001925, train_loss=0.001923131

Batch 17040, train_perplexity=1.0019236, train_loss=0.0019217602

Batch 17050, train_perplexity=1.0019222, train_loss=0.0019203713

Batch 17060, train_perplexity=1.0019208, train_loss=0.001918938

Batch 17070, train_perplexity=1.0019195, train_loss=0.0019176889

Batch 17080, train_perplexity=1.0019181, train_loss=0.0019162465

Batch 17090, train_perplexity=1.0019168, train_loss=0.0019149412

Batch 17100, train_perplexity=1.0019155, train_loss=0.001913591

Batch 17110, train_perplexity=1.0019141, train_loss=0.0019122588

Batch 17120, train_perplexity=1.0019127, train_loss=0.0019109293

Batch 17130, train_perplexity=1.0019113, train_loss=0.0019094246

Batch 17140, train_perplexity=1.00191, train_loss=0.0019081547

Batch 17150, train_perplexity=1.0019087, train_loss=0.0019068373

Batch 17160, train_perplexity=1.0019072, train_loss=0.0019053742

Batch 17170, train_perplexity=1.0019059, train_loss=0.0019041102

Batch 17180, train_perplexity=1.0019047, train_loss=0.0019028732

Batch 17190, train_perplexity=1.0019033, train_loss=0.0019014752

Batch 17200, train_perplexity=1.001902, train_loss=0.0019001223

Batch 17210, train_perplexity=1.0019006, train_loss=0.0018987453

Batch 17220, train_perplexity=1.0018992, train_loss=0.0018974933

Batch 17230, train_perplexity=1.0018978, train_loss=0.001896072

Batch 17240, train_perplexity=1.0018966, train_loss=0.0018948496

Batch 17250, train_perplexity=1.0018952, train_loss=0.0018933775

Batch 17260, train_perplexity=1.0018939, train_loss=0.0018921015

Batch 17270, train_perplexity=1.0018926, train_loss=0.0018907544
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17280, train_perplexity=1.0018913, train_loss=0.0018894995

Batch 17290, train_perplexity=1.0018898, train_loss=0.0018880481

Batch 17300, train_perplexity=1.0018886, train_loss=0.0018868913

Batch 17310, train_perplexity=1.0018872, train_loss=0.0018854667

Batch 17320, train_perplexity=1.001886, train_loss=0.0018841999

Batch 17330, train_perplexity=1.0018846, train_loss=0.0018828376

Batch 17340, train_perplexity=1.0018834, train_loss=0.0018816094

Batch 17350, train_perplexity=1.001882, train_loss=0.0018801998

Batch 17360, train_perplexity=1.0018808, train_loss=0.0018790103

Batch 17370, train_perplexity=1.0018795, train_loss=0.0018776839

Batch 17380, train_perplexity=1.0018781, train_loss=0.0018763605

Batch 17390, train_perplexity=1.0018767, train_loss=0.0018749775

Batch 17400, train_perplexity=1.0018755, train_loss=0.0018737225

Batch 17410, train_perplexity=1.0018742, train_loss=0.0018724408

Batch 17420, train_perplexity=1.0018729, train_loss=0.0018711768

Batch 17430, train_perplexity=1.0018716, train_loss=0.0018698652

Batch 17440, train_perplexity=1.0018703, train_loss=0.0018685686

Batch 17450, train_perplexity=1.0018691, train_loss=0.0018672838

Batch 17460, train_perplexity=1.0018677, train_loss=0.0018659425

Batch 17470, train_perplexity=1.0018663, train_loss=0.0018645506

Batch 17480, train_perplexity=1.0018651, train_loss=0.0018634235

Batch 17490, train_perplexity=1.0018638, train_loss=0.0018620763

Batch 17500, train_perplexity=1.0018625, train_loss=0.0018607618

Batch 17510, train_perplexity=1.0018612, train_loss=0.0018594918

Batch 17520, train_perplexity=1.0018599, train_loss=0.0018582338

Batch 17530, train_perplexity=1.0018587, train_loss=0.0018569966

Batch 17540, train_perplexity=1.0018574, train_loss=0.0018556584

Batch 17550, train_perplexity=1.0018561, train_loss=0.0018543438

Batch 17560, train_perplexity=1.0018549, train_loss=0.0018531571

Batch 17570, train_perplexity=1.0018535, train_loss=0.001851774

Batch 17580, train_perplexity=1.0018523, train_loss=0.0018505428

Batch 17590, train_perplexity=1.001851, train_loss=0.0018492581

Batch 17600, train_perplexity=1.0018498, train_loss=0.0018480448

Batch 17610, train_perplexity=1.0018485, train_loss=0.0018467004

Batch 17620, train_perplexity=1.0018473, train_loss=0.0018455079

Batch 17630, train_perplexity=1.0018458, train_loss=0.0018441754

Batch 17640, train_perplexity=1.0018446, train_loss=0.0018429292

Batch 17650, train_perplexity=1.0018433, train_loss=0.0018416799

Batch 17660, train_perplexity=1.0018421, train_loss=0.0018404042

Batch 17670, train_perplexity=1.0018408, train_loss=0.0018391046

Batch 17680, train_perplexity=1.0018396, train_loss=0.0018379001

Batch 17690, train_perplexity=1.0018383, train_loss=0.0018366063

Batch 17700, train_perplexity=1.001837, train_loss=0.0018353097

Batch 17710, train_perplexity=1.0018358, train_loss=0.0018341257

Batch 17720, train_perplexity=1.0018345, train_loss=0.0018327995

Batch 17730, train_perplexity=1.0018333, train_loss=0.0018316216

Batch 17740, train_perplexity=1.001832, train_loss=0.0018303783

Batch 17750, train_perplexity=1.0018308, train_loss=0.0018291441

Batch 17760, train_perplexity=1.0018295, train_loss=0.0018278861

Batch 17770, train_perplexity=1.0018283, train_loss=0.0018266577

Batch 17780, train_perplexity=1.001827, train_loss=0.0018253907

Batch 17790, train_perplexity=1.0018258, train_loss=0.0018241655

Batch 17800, train_perplexity=1.0018245, train_loss=0.0018228658

Batch 17810, train_perplexity=1.0018234, train_loss=0.0018217177

Batch 17820, train_perplexity=1.0018221, train_loss=0.0018204567

Batch 17830, train_perplexity=1.0018209, train_loss=0.0018192164

Batch 17840, train_perplexity=1.0018196, train_loss=0.0018180149

Batch 17850, train_perplexity=1.0018184, train_loss=0.001816751

Batch 17860, train_perplexity=1.0018171, train_loss=0.0018155167

Batch 17870, train_perplexity=1.0018159, train_loss=0.0018143001

Batch 17880, train_perplexity=1.0018147, train_loss=0.0018130897

Batch 17890, train_perplexity=1.0018135, train_loss=0.0018119118

Batch 17900, train_perplexity=1.0018122, train_loss=0.0018106061

Batch 17910, train_perplexity=1.001811, train_loss=0.0018093687

Batch 17920, train_perplexity=1.0018098, train_loss=0.0018082117

Batch 17930, train_perplexity=1.0018086, train_loss=0.0018069955

Batch 17940, train_perplexity=1.0018075, train_loss=0.0018057849

Batch 17950, train_perplexity=1.0018061, train_loss=0.0018045625

Batch 17960, train_perplexity=1.001805, train_loss=0.001803349

Batch 17970, train_perplexity=1.0018038, train_loss=0.0018021562

Batch 17980, train_perplexity=1.0018026, train_loss=0.0018009578

Batch 17990, train_perplexity=1.0018013, train_loss=0.0017996818

Batch 18000, train_perplexity=1.0018002, train_loss=0.0017985217

Batch 18010, train_perplexity=1.0017989, train_loss=0.0017973054

Batch 18020, train_perplexity=1.0017978, train_loss=0.0017961366

Batch 18030, train_perplexity=1.0017965, train_loss=0.0017949231

Batch 18040, train_perplexity=1.0017953, train_loss=0.0017937273

Batch 18050, train_perplexity=1.0017942, train_loss=0.0017925556

Batch 18060, train_perplexity=1.0017929, train_loss=0.001791345

Batch 18070, train_perplexity=1.0017917, train_loss=0.0017901226

Batch 18080, train_perplexity=1.0017906, train_loss=0.0017889953

Batch 18090, train_perplexity=1.0017893, train_loss=0.0017877549

Batch 18100, train_perplexity=1.0017881, train_loss=0.0017865712

Batch 18110, train_perplexity=1.001787, train_loss=0.0017853786

Batch 18120, train_perplexity=1.0017859, train_loss=0.0017842633

Batch 18130, train_perplexity=1.0017847, train_loss=0.0017830438

Batch 18140, train_perplexity=1.0017835, train_loss=0.0017818422

Batch 18150, train_perplexity=1.0017823, train_loss=0.001780703

Batch 18160, train_perplexity=1.001781, train_loss=0.0017794298

Batch 18170, train_perplexity=1.0017799, train_loss=0.0017783234

Batch 18180, train_perplexity=1.0017787, train_loss=0.0017770982

Batch 18190, train_perplexity=1.0017775, train_loss=0.0017759262

Batch 18200, train_perplexity=1.0017763, train_loss=0.0017748137

Batch 18210, train_perplexity=1.0017751, train_loss=0.0017735765

Batch 18220, train_perplexity=1.0017741, train_loss=0.0017724463

Batch 18230, train_perplexity=1.0017729, train_loss=0.001771316

Batch 18240, train_perplexity=1.0017717, train_loss=0.0017701501

Batch 18250, train_perplexity=1.0017705, train_loss=0.0017689692

Batch 18260, train_perplexity=1.0017693, train_loss=0.0017677528

Batch 18270, train_perplexity=1.0017682, train_loss=0.0017666165

Batch 18280, train_perplexity=1.001767, train_loss=0.0017654625

Batch 18290, train_perplexity=1.0017658, train_loss=0.0017642549

Batch 18300, train_perplexity=1.0017648, train_loss=0.0017631634

Batch 18310, train_perplexity=1.0017636, train_loss=0.0017619974

Batch 18320, train_perplexity=1.0017624, train_loss=0.0017608016

Batch 18330, train_perplexity=1.0017612, train_loss=0.0017595971

Batch 18340, train_perplexity=1.00176, train_loss=0.0017584816

Batch 18350, train_perplexity=1.0017588, train_loss=0.0017573098

Batch 18360, train_perplexity=1.0017577, train_loss=0.0017561824

Batch 18370, train_perplexity=1.0017565, train_loss=0.0017550462

Batch 18380, train_perplexity=1.0017554, train_loss=0.0017538654

Batch 18390, train_perplexity=1.0017543, train_loss=0.0017526995

Batch 18400, train_perplexity=1.0017531, train_loss=0.0017515244

Batch 18410, train_perplexity=1.001752, train_loss=0.0017504329

Batch 18420, train_perplexity=1.0017508, train_loss=0.0017492787

Batch 18430, train_perplexity=1.0017498, train_loss=0.0017482021

Batch 18440, train_perplexity=1.0017486, train_loss=0.0017470422

Batch 18450, train_perplexity=1.0017475, train_loss=0.0017459297

Batch 18460, train_perplexity=1.0017463, train_loss=0.0017447816

Batch 18470, train_perplexity=1.0017451, train_loss=0.0017436068

Batch 18480, train_perplexity=1.001744, train_loss=0.0017425478

Batch 18490, train_perplexity=1.001743, train_loss=0.0017414025

Batch 18500, train_perplexity=1.0017418, train_loss=0.0017402337

Batch 18510, train_perplexity=1.0017407, train_loss=0.0017391241

Batch 18520, train_perplexity=1.0017395, train_loss=0.0017379969
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 18530, train_perplexity=1.0017383, train_loss=0.0017368577

Batch 18540, train_perplexity=1.0017374, train_loss=0.001735894

Batch 18550, train_perplexity=1.001736, train_loss=0.0017345583

Batch 18560, train_perplexity=1.0017351, train_loss=0.0017335294

Batch 18570, train_perplexity=1.0017339, train_loss=0.0017323782

Batch 18580, train_perplexity=1.0017327, train_loss=0.0017311885

Batch 18590, train_perplexity=1.0017315, train_loss=0.0017300372

Batch 18600, train_perplexity=1.0017304, train_loss=0.0017289931

Batch 18610, train_perplexity=1.0017295, train_loss=0.0017279431

Batch 18620, train_perplexity=1.0017282, train_loss=0.001726679

Batch 18630, train_perplexity=1.0017271, train_loss=0.0017255994

Batch 18640, train_perplexity=1.0017259, train_loss=0.001724472

Batch 18650, train_perplexity=1.001725, train_loss=0.0017234487

Batch 18660, train_perplexity=1.0017238, train_loss=0.0017223065

Batch 18670, train_perplexity=1.0017227, train_loss=0.0017211703

Batch 18680, train_perplexity=1.0017216, train_loss=0.0017201381

Batch 18690, train_perplexity=1.0017204, train_loss=0.0017189544

Batch 18700, train_perplexity=1.0017194, train_loss=0.0017179311

Batch 18710, train_perplexity=1.0017183, train_loss=0.0017168304

Batch 18720, train_perplexity=1.0017172, train_loss=0.0017157032

Batch 18730, train_perplexity=1.001716, train_loss=0.0017145697

Batch 18740, train_perplexity=1.001715, train_loss=0.001713481

Batch 18750, train_perplexity=1.0017139, train_loss=0.0017123716

Batch 18760, train_perplexity=1.0017128, train_loss=0.0017113099

Batch 18770, train_perplexity=1.0017117, train_loss=0.0017103072

Batch 18780, train_perplexity=1.0017105, train_loss=0.0017091174

Batch 18790, train_perplexity=1.0017095, train_loss=0.0017080466

Batch 18800, train_perplexity=1.0017084, train_loss=0.00170694

Batch 18810, train_perplexity=1.0017073, train_loss=0.0017058335

Batch 18820, train_perplexity=1.0017062, train_loss=0.0017047834

Batch 18830, train_perplexity=1.0017052, train_loss=0.001703677

Batch 18840, train_perplexity=1.0017041, train_loss=0.0017025883

Batch 18850, train_perplexity=1.001703, train_loss=0.001701565

Batch 18860, train_perplexity=1.0017018, train_loss=0.0017004376

Batch 18870, train_perplexity=1.0017009, train_loss=0.0016993787

Batch 18880, train_perplexity=1.0016998, train_loss=0.0016983049

Batch 18890, train_perplexity=1.0016986, train_loss=0.0016971417

Batch 18900, train_perplexity=1.0016975, train_loss=0.0016961275

Batch 18910, train_perplexity=1.0016965, train_loss=0.0016950597

Batch 18920, train_perplexity=1.0016954, train_loss=0.00169395

Batch 18930, train_perplexity=1.0016943, train_loss=0.0016929447

Batch 18940, train_perplexity=1.0016932, train_loss=0.0016918024

Batch 18950, train_perplexity=1.0016923, train_loss=0.0016908387

Batch 18960, train_perplexity=1.0016912, train_loss=0.0016897649

Batch 18970, train_perplexity=1.00169, train_loss=0.0016886226

Batch 18980, train_perplexity=1.001689, train_loss=0.0016875844

Batch 18990, train_perplexity=1.0016879, train_loss=0.0016864333

Batch 19000, train_perplexity=1.0016868, train_loss=0.0016854368

Batch 19010, train_perplexity=1.0016859, train_loss=0.0016844105

Batch 19020, train_perplexity=1.0016848, train_loss=0.0016833188

Batch 19030, train_perplexity=1.0016837, train_loss=0.0016822896

Batch 19040, train_perplexity=1.0016826, train_loss=0.0016812218

Batch 19050, train_perplexity=1.0016816, train_loss=0.0016801984

Batch 19060, train_perplexity=1.0016805, train_loss=0.0016790621

Batch 19070, train_perplexity=1.0016794, train_loss=0.0016779853

Batch 19080, train_perplexity=1.0016783, train_loss=0.0016769739

Batch 19090, train_perplexity=1.0016774, train_loss=0.0016759359

Batch 19100, train_perplexity=1.0016763, train_loss=0.0016748976

Batch 19110, train_perplexity=1.0016751, train_loss=0.001673785

Batch 19120, train_perplexity=1.0016743, train_loss=0.0016728571

Batch 19130, train_perplexity=1.0016732, train_loss=0.0016717652

Batch 19140, train_perplexity=1.001672, train_loss=0.0016706797

Batch 19150, train_perplexity=1.001671, train_loss=0.0016696147

Batch 19160, train_perplexity=1.00167, train_loss=0.0016686211

Batch 19170, train_perplexity=1.0016689, train_loss=0.0016675799

Batch 19180, train_perplexity=1.0016679, train_loss=0.001666506

Batch 19190, train_perplexity=1.0016668, train_loss=0.0016654381

Batch 19200, train_perplexity=1.001666, train_loss=0.001664516

Batch 19210, train_perplexity=1.0016648, train_loss=0.0016633825

Batch 19220, train_perplexity=1.0016637, train_loss=0.0016623265

Batch 19230, train_perplexity=1.0016627, train_loss=0.0016612944

Batch 19240, train_perplexity=1.0016617, train_loss=0.0016602948

Batch 19250, train_perplexity=1.0016606, train_loss=0.0016592537

Batch 19260, train_perplexity=1.0016596, train_loss=0.001658263

Batch 19270, train_perplexity=1.0016586, train_loss=0.0016572131

Batch 19280, train_perplexity=1.0016576, train_loss=0.0016562492

Batch 19290, train_perplexity=1.0016565, train_loss=0.0016551844

Batch 19300, train_perplexity=1.0016555, train_loss=0.0016541372

Batch 19310, train_perplexity=1.0016544, train_loss=0.0016530603

Batch 19320, train_perplexity=1.0016534, train_loss=0.0016520459

Batch 19330, train_perplexity=1.0016525, train_loss=0.0016510673

Batch 19340, train_perplexity=1.0016514, train_loss=0.001650041

Batch 19350, train_perplexity=1.0016503, train_loss=0.0016489879

Batch 19360, train_perplexity=1.0016493, train_loss=0.0016479586

Batch 19370, train_perplexity=1.0016483, train_loss=0.0016469413

Batch 19380, train_perplexity=1.0016474, train_loss=0.0016459834

Batch 19390, train_perplexity=1.0016463, train_loss=0.0016449661

Batch 19400, train_perplexity=1.0016453, train_loss=0.0016439337

Batch 19410, train_perplexity=1.0016443, train_loss=0.0016429045

Batch 19420, train_perplexity=1.0016432, train_loss=0.001641884

Batch 19430, train_perplexity=1.0016422, train_loss=0.0016408637

Batch 19440, train_perplexity=1.0016412, train_loss=0.0016398286

Batch 19450, train_perplexity=1.0016401, train_loss=0.0016387666

Batch 19460, train_perplexity=1.0016392, train_loss=0.0016378682

Batch 19470, train_perplexity=1.001638, train_loss=0.0016367526

Batch 19480, train_perplexity=1.0016372, train_loss=0.0016358483

Batch 19490, train_perplexity=1.0016361, train_loss=0.001634825

Batch 19500, train_perplexity=1.0016352, train_loss=0.001633876

Batch 19510, train_perplexity=1.0016341, train_loss=0.0016328439

Batch 19520, train_perplexity=1.0016332, train_loss=0.0016318234

Batch 19530, train_perplexity=1.0016322, train_loss=0.0016308597

Batch 19540, train_perplexity=1.0016311, train_loss=0.0016298334

Batch 19550, train_perplexity=1.0016302, train_loss=0.001628825

Batch 19560, train_perplexity=1.0016291, train_loss=0.0016278191

Batch 19570, train_perplexity=1.0016282, train_loss=0.0016268436

Batch 19580, train_perplexity=1.0016272, train_loss=0.00162585

Batch 19590, train_perplexity=1.0016261, train_loss=0.0016248148

Batch 19600, train_perplexity=1.0016252, train_loss=0.0016238687

Batch 19610, train_perplexity=1.0016242, train_loss=0.0016229019

Batch 19620, train_perplexity=1.0016233, train_loss=0.0016219232

Batch 19630, train_perplexity=1.0016222, train_loss=0.0016208611

Batch 19640, train_perplexity=1.0016212, train_loss=0.001619945

Batch 19650, train_perplexity=1.0016203, train_loss=0.0016189989

Batch 19660, train_perplexity=1.0016193, train_loss=0.0016179755

Batch 19670, train_perplexity=1.0016183, train_loss=0.0016169818

Batch 19680, train_perplexity=1.0016173, train_loss=0.0016159825

Batch 19690, train_perplexity=1.0016164, train_loss=0.0016150451

Batch 19700, train_perplexity=1.0016153, train_loss=0.0016139625

Batch 19710, train_perplexity=1.0016143, train_loss=0.0016130671

Batch 19720, train_perplexity=1.0016134, train_loss=0.0016120824

Batch 19730, train_perplexity=1.0016124, train_loss=0.001611181

Batch 19740, train_perplexity=1.0016115, train_loss=0.0016101992

Batch 19750, train_perplexity=1.0016105, train_loss=0.0016092205

Batch 19760, train_perplexity=1.0016096, train_loss=0.0016082656

Batch 19770, train_perplexity=1.0016086, train_loss=0.001607284
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19780, train_perplexity=1.0016077, train_loss=0.0016063259

Batch 19790, train_perplexity=1.0016066, train_loss=0.00160524

Batch 19800, train_perplexity=1.0016056, train_loss=0.0016043715

Batch 19810, train_perplexity=1.0016048, train_loss=0.001603476

Batch 19820, train_perplexity=1.0016037, train_loss=0.0016024052

Batch 19830, train_perplexity=1.0016026, train_loss=0.0016013727

Batch 19840, train_perplexity=1.0016017, train_loss=0.0016004654

Batch 19850, train_perplexity=1.0016009, train_loss=0.0015995224

Batch 19860, train_perplexity=1.0015999, train_loss=0.0015985912

Batch 19870, train_perplexity=1.001599, train_loss=0.0015976185

Batch 19880, train_perplexity=1.0015979, train_loss=0.0015966576

Batch 19890, train_perplexity=1.0015969, train_loss=0.001595652

Batch 19900, train_perplexity=1.001596, train_loss=0.0015947416

Batch 19910, train_perplexity=1.0015951, train_loss=0.0015938581

Batch 19920, train_perplexity=1.0015941, train_loss=0.0015927753

Batch 19930, train_perplexity=1.0015931, train_loss=0.0015918767

Batch 19940, train_perplexity=1.0015922, train_loss=0.001590901

Batch 19950, train_perplexity=1.0015913, train_loss=0.0015900235

Batch 19960, train_perplexity=1.0015903, train_loss=0.0015890388

Batch 19970, train_perplexity=1.0015893, train_loss=0.0015880896

Batch 19980, train_perplexity=1.0015883, train_loss=0.0015871407

Batch 19990, train_perplexity=1.0015874, train_loss=0.0015861827

Batch 20000, train_perplexity=1.0015864, train_loss=0.0015852218

Batch 20010, train_perplexity=1.0015856, train_loss=0.0015843055

Batch 20020, train_perplexity=1.0015846, train_loss=0.001583419

Batch 20030, train_perplexity=1.0015836, train_loss=0.001582369

Batch 20040, train_perplexity=1.0015827, train_loss=0.0015814586

Batch 20050, train_perplexity=1.0015818, train_loss=0.001580572

Batch 20060, train_perplexity=1.0015807, train_loss=0.001579483

Batch 20070, train_perplexity=1.0015799, train_loss=0.0015786353

Batch 20080, train_perplexity=1.001579, train_loss=0.0015777487

Batch 20090, train_perplexity=1.001578, train_loss=0.0015767522

Batch 20100, train_perplexity=1.001577, train_loss=0.0015758297

Batch 20110, train_perplexity=1.0015762, train_loss=0.0015748868

Batch 20120, train_perplexity=1.0015752, train_loss=0.0015739496

Batch 20130, train_perplexity=1.0015743, train_loss=0.0015730185

Batch 20140, train_perplexity=1.0015733, train_loss=0.0015720367

Batch 20150, train_perplexity=1.0015724, train_loss=0.0015711769

Batch 20160, train_perplexity=1.0015714, train_loss=0.001570222

Batch 20170, train_perplexity=1.0015706, train_loss=0.0015693144

Batch 20180, train_perplexity=1.0015695, train_loss=0.0015683179

Batch 20190, train_perplexity=1.0015687, train_loss=0.0015674314

Batch 20200, train_perplexity=1.0015678, train_loss=0.0015665895

Batch 20210, train_perplexity=1.0015669, train_loss=0.0015656077

Batch 20220, train_perplexity=1.0015658, train_loss=0.0015646378

Batch 20230, train_perplexity=1.001565, train_loss=0.001563778

Batch 20240, train_perplexity=1.001564, train_loss=0.0015627755

Batch 20250, train_perplexity=1.0015632, train_loss=0.0015619484

Batch 20260, train_perplexity=1.0015622, train_loss=0.0015610023

Batch 20270, train_perplexity=1.0015613, train_loss=0.0015600771

Batch 20280, train_perplexity=1.0015603, train_loss=0.0015591311

Batch 20290, train_perplexity=1.0015595, train_loss=0.0015582979

Batch 20300, train_perplexity=1.0015585, train_loss=0.0015572836

Batch 20310, train_perplexity=1.0015576, train_loss=0.0015564297

Batch 20320, train_perplexity=1.0015568, train_loss=0.0015555104

Batch 20330, train_perplexity=1.0015558, train_loss=0.0015546118

Batch 20340, train_perplexity=1.0015548, train_loss=0.0015536599

Batch 20350, train_perplexity=1.001554, train_loss=0.0015527941

Batch 20360, train_perplexity=1.001553, train_loss=0.0015518987

Batch 20370, train_perplexity=1.0015522, train_loss=0.0015510062

Batch 20380, train_perplexity=1.0015514, train_loss=0.0015501762

Batch 20390, train_perplexity=1.0015504, train_loss=0.0015492032

Batch 20400, train_perplexity=1.0015494, train_loss=0.0015482067

Batch 20410, train_perplexity=1.0015485, train_loss=0.0015473526

Batch 20420, train_perplexity=1.0015477, train_loss=0.0015464751

Batch 20430, train_perplexity=1.0015467, train_loss=0.001545517

Batch 20440, train_perplexity=1.0015459, train_loss=0.001544696

Batch 20450, train_perplexity=1.001545, train_loss=0.0015437081

Batch 20460, train_perplexity=1.001544, train_loss=0.0015428662

Batch 20470, train_perplexity=1.0015432, train_loss=0.0015420034

Batch 20480, train_perplexity=1.0015422, train_loss=0.0015410217

Batch 20490, train_perplexity=1.0015414, train_loss=0.0015401589

Batch 20500, train_perplexity=1.0015404, train_loss=0.0015392366

Batch 20510, train_perplexity=1.0015396, train_loss=0.0015383947

Batch 20520, train_perplexity=1.0015386, train_loss=0.0015374753

Batch 20530, train_perplexity=1.0015377, train_loss=0.001536553

Batch 20540, train_perplexity=1.0015368, train_loss=0.0015356694

Batch 20550, train_perplexity=1.001536, train_loss=0.0015348572

Batch 20560, train_perplexity=1.0015352, train_loss=0.0015339439

Batch 20570, train_perplexity=1.0015341, train_loss=0.0015329858

Batch 20580, train_perplexity=1.0015334, train_loss=0.0015321559

Batch 20590, train_perplexity=1.0015323, train_loss=0.001531183

Batch 20600, train_perplexity=1.0015315, train_loss=0.001530332

Batch 20610, train_perplexity=1.0015308, train_loss=0.0015295616

Batch 20620, train_perplexity=1.0015298, train_loss=0.0015286126

Batch 20630, train_perplexity=1.001529, train_loss=0.0015277914

Batch 20640, train_perplexity=1.001528, train_loss=0.0015268808

Batch 20650, train_perplexity=1.0015272, train_loss=0.0015260419

Batch 20660, train_perplexity=1.0015262, train_loss=0.0015250719

Batch 20670, train_perplexity=1.0015254, train_loss=0.0015242093

Batch 20680, train_perplexity=1.0015246, train_loss=0.0015233732

Batch 20690, train_perplexity=1.0015235, train_loss=0.0015223855

Batch 20700, train_perplexity=1.0015228, train_loss=0.0015215911

Batch 20710, train_perplexity=1.0015218, train_loss=0.0015206628

Batch 20720, train_perplexity=1.001521, train_loss=0.0015198148

Batch 20730, train_perplexity=1.0015202, train_loss=0.0015189671

Batch 20740, train_perplexity=1.0015192, train_loss=0.0015180774

Batch 20750, train_perplexity=1.0015184, train_loss=0.0015172088

Batch 20760, train_perplexity=1.0015175, train_loss=0.0015164113

Batch 20770, train_perplexity=1.0015166, train_loss=0.0015154325

Batch 20780, train_perplexity=1.0015157, train_loss=0.0015145666

Batch 20790, train_perplexity=1.0015149, train_loss=0.001513799

Batch 20800, train_perplexity=1.0015141, train_loss=0.0015129035

Batch 20810, train_perplexity=1.0015131, train_loss=0.0015119811

Batch 20820, train_perplexity=1.0015123, train_loss=0.0015111156

Batch 20830, train_perplexity=1.0015115, train_loss=0.0015103477

Batch 20840, train_perplexity=1.0015106, train_loss=0.001509476

Batch 20850, train_perplexity=1.0015098, train_loss=0.0015086429

Batch 20860, train_perplexity=1.0015088, train_loss=0.0015076671

Batch 20870, train_perplexity=1.001508, train_loss=0.0015069025

Batch 20880, train_perplexity=1.0015072, train_loss=0.0015060545

Batch 20890, train_perplexity=1.0015063, train_loss=0.0015052364

Batch 20900, train_perplexity=1.0015054, train_loss=0.001504201

Batch 20910, train_perplexity=1.0015045, train_loss=0.0015034184

Batch 20920, train_perplexity=1.0015037, train_loss=0.0015026003

Batch 20930, train_perplexity=1.0015029, train_loss=0.0015017136

Batch 20940, train_perplexity=1.0015019, train_loss=0.001500827

Batch 20950, train_perplexity=1.0015011, train_loss=0.0014999553

Batch 20960, train_perplexity=1.0015002, train_loss=0.001499155

Batch 20970, train_perplexity=1.0014995, train_loss=0.0014983636

Batch 20980, train_perplexity=1.0014986, train_loss=0.001497465

Batch 20990, train_perplexity=1.0014976, train_loss=0.0014964892

Batch 21000, train_perplexity=1.0014969, train_loss=0.0014957571

Batch 21010, train_perplexity=1.0014961, train_loss=0.0014949122

Batch 21020, train_perplexity=1.0014951, train_loss=0.0014940494
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21030, train_perplexity=1.0014943, train_loss=0.0014931837

Batch 21040, train_perplexity=1.0014936, train_loss=0.0014924427

Batch 21050, train_perplexity=1.0014926, train_loss=0.0014915235

Batch 21060, train_perplexity=1.0014919, train_loss=0.0014907825

Batch 21070, train_perplexity=1.001491, train_loss=0.001489893

Batch 21080, train_perplexity=1.0014901, train_loss=0.0014890272

Batch 21090, train_perplexity=1.0014893, train_loss=0.0014881792

Batch 21100, train_perplexity=1.0014884, train_loss=0.0014873638

Batch 21110, train_perplexity=1.0014876, train_loss=0.001486528

Batch 21120, train_perplexity=1.0014868, train_loss=0.001485656

Batch 21130, train_perplexity=1.001486, train_loss=0.0014848261

Batch 21140, train_perplexity=1.0014851, train_loss=0.0014840019

Batch 21150, train_perplexity=1.0014842, train_loss=0.0014830853

Batch 21160, train_perplexity=1.0014833, train_loss=0.0014822732

Batch 21170, train_perplexity=1.0014826, train_loss=0.0014815413

Batch 21180, train_perplexity=1.0014818, train_loss=0.0014806755

Batch 21190, train_perplexity=1.0014809, train_loss=0.0014798574

Batch 21200, train_perplexity=1.0014801, train_loss=0.0014789528

Batch 21210, train_perplexity=1.0014793, train_loss=0.0014781198

Batch 21220, train_perplexity=1.0014784, train_loss=0.0014773431

Batch 21230, train_perplexity=1.0014777, train_loss=0.0014765991

Batch 21240, train_perplexity=1.0014768, train_loss=0.0014756799

Batch 21250, train_perplexity=1.0014759, train_loss=0.0014748854

Batch 21260, train_perplexity=1.0014751, train_loss=0.0014740166

Batch 21270, train_perplexity=1.0014743, train_loss=0.0014731598

Batch 21280, train_perplexity=1.0014735, train_loss=0.0014724336

Batch 21290, train_perplexity=1.0014727, train_loss=0.0014716513

Batch 21300, train_perplexity=1.0014718, train_loss=0.0014706932

Batch 21310, train_perplexity=1.001471, train_loss=0.0014699702

Batch 21320, train_perplexity=1.0014702, train_loss=0.0014691133

Batch 21330, train_perplexity=1.0014694, train_loss=0.0014683069

Batch 21340, train_perplexity=1.0014685, train_loss=0.0014675155

Batch 21350, train_perplexity=1.0014677, train_loss=0.0014666527

Batch 21360, train_perplexity=1.001467, train_loss=0.0014658553

Batch 21370, train_perplexity=1.001466, train_loss=0.0014649597

Batch 21380, train_perplexity=1.0014653, train_loss=0.0014642545

Batch 21390, train_perplexity=1.0014644, train_loss=0.0014633351

Batch 21400, train_perplexity=1.0014637, train_loss=0.0014626242

Batch 21410, train_perplexity=1.0014628, train_loss=0.0014617998

Batch 21420, train_perplexity=1.0014622, train_loss=0.0014611125

Batch 21430, train_perplexity=1.0014613, train_loss=0.0014601455

Batch 21440, train_perplexity=1.0014604, train_loss=0.0014593541

Batch 21450, train_perplexity=1.0014596, train_loss=0.0014585268

Batch 21460, train_perplexity=1.0014589, train_loss=0.001457786

Batch 21470, train_perplexity=1.001458, train_loss=0.0014569352

Batch 21480, train_perplexity=1.0014572, train_loss=0.0014562091

Batch 21490, train_perplexity=1.0014564, train_loss=0.0014552658

Batch 21500, train_perplexity=1.0014555, train_loss=0.001454507

Batch 21510, train_perplexity=1.0014548, train_loss=0.0014537664

Batch 21520, train_perplexity=1.0014539, train_loss=0.0014528558

Batch 21530, train_perplexity=1.0014532, train_loss=0.0014521417

Batch 21540, train_perplexity=1.0014524, train_loss=0.001451371

Batch 21550, train_perplexity=1.0014516, train_loss=0.001450541

Batch 21560, train_perplexity=1.0014508, train_loss=0.0014497644

Batch 21570, train_perplexity=1.00145, train_loss=0.0014488539

Batch 21580, train_perplexity=1.0014491, train_loss=0.0014480683

Batch 21590, train_perplexity=1.0014484, train_loss=0.001447384

Batch 21600, train_perplexity=1.0014476, train_loss=0.0014464764

Batch 21610, train_perplexity=1.0014467, train_loss=0.0014457058

Batch 21620, train_perplexity=1.0014461, train_loss=0.0014450364

Batch 21630, train_perplexity=1.0014452, train_loss=0.0014440991

Batch 21640, train_perplexity=1.0014443, train_loss=0.0014433551

Batch 21650, train_perplexity=1.0014436, train_loss=0.0014426203

Batch 21660, train_perplexity=1.0014429, train_loss=0.0014418378

Batch 21670, train_perplexity=1.001442, train_loss=0.0014409691

Batch 21680, train_perplexity=1.0014412, train_loss=0.0014402162

Batch 21690, train_perplexity=1.0014405, train_loss=0.0014395111

Batch 21700, train_perplexity=1.0014397, train_loss=0.0014386124

Batch 21710, train_perplexity=1.0014389, train_loss=0.0014378269

Batch 21720, train_perplexity=1.0014381, train_loss=0.0014371306

Batch 21730, train_perplexity=1.0014373, train_loss=0.0014362738

Batch 21740, train_perplexity=1.0014366, train_loss=0.0014355357

Batch 21750, train_perplexity=1.0014356, train_loss=0.001434643

Batch 21760, train_perplexity=1.001435, train_loss=0.0014339618

Batch 21770, train_perplexity=1.0014342, train_loss=0.0014331405

Batch 21780, train_perplexity=1.0014334, train_loss=0.0014323937

Batch 21790, train_perplexity=1.0014327, train_loss=0.0014316319

Batch 21800, train_perplexity=1.0014318, train_loss=0.0014307809

Batch 21810, train_perplexity=1.001431, train_loss=0.0014300162

Batch 21820, train_perplexity=1.0014303, train_loss=0.0014292456

Batch 21830, train_perplexity=1.0014296, train_loss=0.0014285701

Batch 21840, train_perplexity=1.0014287, train_loss=0.0014277518

Batch 21850, train_perplexity=1.001428, train_loss=0.0014269783

Batch 21860, train_perplexity=1.0014272, train_loss=0.0014261659

Batch 21870, train_perplexity=1.0014265, train_loss=0.0014254964

Batch 21880, train_perplexity=1.0014256, train_loss=0.0014246574

Batch 21890, train_perplexity=1.0014249, train_loss=0.0014238837

Batch 21900, train_perplexity=1.0014241, train_loss=0.001423009

Batch 21910, train_perplexity=1.0014234, train_loss=0.0014223095

Batch 21920, train_perplexity=1.0014225, train_loss=0.001421542

Batch 21930, train_perplexity=1.0014218, train_loss=0.0014207802

Batch 21940, train_perplexity=1.001421, train_loss=0.0014199412

Batch 21950, train_perplexity=1.0014203, train_loss=0.001419239

Batch 21960, train_perplexity=1.0014195, train_loss=0.0014185039

Batch 21970, train_perplexity=1.0014187, train_loss=0.0014177333

Batch 21980, train_perplexity=1.001418, train_loss=0.0014170043

Batch 21990, train_perplexity=1.0014172, train_loss=0.0014161414

Batch 22000, train_perplexity=1.0014164, train_loss=0.0014154243

Batch 22010, train_perplexity=1.0014156, train_loss=0.0014145852

Batch 22020, train_perplexity=1.0014149, train_loss=0.001413889

Batch 22030, train_perplexity=1.0014141, train_loss=0.0014131063

Batch 22040, train_perplexity=1.0014133, train_loss=0.0014123893

Batch 22050, train_perplexity=1.0014126, train_loss=0.0014116275

Batch 22060, train_perplexity=1.0014118, train_loss=0.001410848

Batch 22070, train_perplexity=1.0014111, train_loss=0.0014100594

Batch 22080, train_perplexity=1.0014104, train_loss=0.0014093185

Batch 22090, train_perplexity=1.0014095, train_loss=0.0014084913

Batch 22100, train_perplexity=1.0014087, train_loss=0.0014077562

Batch 22110, train_perplexity=1.001408, train_loss=0.0014069946

Batch 22120, train_perplexity=1.0014073, train_loss=0.0014062806

Batch 22130, train_perplexity=1.0014064, train_loss=0.001405486

Batch 22140, train_perplexity=1.0014058, train_loss=0.0014048016

Batch 22150, train_perplexity=1.001405, train_loss=0.00140401

Batch 22160, train_perplexity=1.0014042, train_loss=0.0014032393

Batch 22170, train_perplexity=1.0014036, train_loss=0.0014025759

Batch 22180, train_perplexity=1.0014027, train_loss=0.0014018051

Batch 22190, train_perplexity=1.001402, train_loss=0.0014010911

Batch 22200, train_perplexity=1.0014013, train_loss=0.0014002726

Batch 22210, train_perplexity=1.0014005, train_loss=0.001399514

Batch 22220, train_perplexity=1.0013998, train_loss=0.0013988058

Batch 22230, train_perplexity=1.0013989, train_loss=0.0013979726

Batch 22240, train_perplexity=1.0013983, train_loss=0.001397321

Batch 22250, train_perplexity=1.0013975, train_loss=0.0013965056

Batch 22260, train_perplexity=1.0013968, train_loss=0.0013958183

Batch 22270, train_perplexity=1.0013961, train_loss=0.0013950625
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22280, train_perplexity=1.0013952, train_loss=0.0013942828

Batch 22290, train_perplexity=1.0013945, train_loss=0.0013935419

Batch 22300, train_perplexity=1.0013937, train_loss=0.0013927295

Batch 22310, train_perplexity=1.001393, train_loss=0.0013920125

Batch 22320, train_perplexity=1.0013922, train_loss=0.0013913101

Batch 22330, train_perplexity=1.0013916, train_loss=0.0013906526

Batch 22340, train_perplexity=1.0013908, train_loss=0.0013898016

Batch 22350, train_perplexity=1.0013901, train_loss=0.0013891647

Batch 22360, train_perplexity=1.0013894, train_loss=0.0013883971

Batch 22370, train_perplexity=1.0013885, train_loss=0.0013876413

Batch 22380, train_perplexity=1.0013878, train_loss=0.0013868704

Batch 22390, train_perplexity=1.0013871, train_loss=0.001386204

Batch 22400, train_perplexity=1.0013864, train_loss=0.0013854036

Batch 22410, train_perplexity=1.0013857, train_loss=0.0013847519

Batch 22420, train_perplexity=1.001385, train_loss=0.0013840675

Batch 22430, train_perplexity=1.0013843, train_loss=0.0013832671

Batch 22440, train_perplexity=1.0013834, train_loss=0.0013825113

Batch 22450, train_perplexity=1.0013827, train_loss=0.001381797

Batch 22460, train_perplexity=1.001382, train_loss=0.0013810412

Batch 22470, train_perplexity=1.0013813, train_loss=0.0013802974

Batch 22480, train_perplexity=1.0013806, train_loss=0.0013796249

Batch 22490, train_perplexity=1.0013798, train_loss=0.0013788899

Batch 22500, train_perplexity=1.0013791, train_loss=0.0013781697

Batch 22510, train_perplexity=1.0013783, train_loss=0.0013774021

Batch 22520, train_perplexity=1.0013777, train_loss=0.0013767384

Batch 22530, train_perplexity=1.0013769, train_loss=0.0013759648

Batch 22540, train_perplexity=1.0013763, train_loss=0.0013752864

Batch 22550, train_perplexity=1.0013756, train_loss=0.0013746318

Batch 22560, train_perplexity=1.0013748, train_loss=0.0013738789

Batch 22570, train_perplexity=1.001374, train_loss=0.0013730635

Batch 22580, train_perplexity=1.0013734, train_loss=0.0013724088

Batch 22590, train_perplexity=1.0013727, train_loss=0.0013716978

Batch 22600, train_perplexity=1.0013719, train_loss=0.0013708823

Batch 22610, train_perplexity=1.0013711, train_loss=0.0013702426

Batch 22620, train_perplexity=1.0013705, train_loss=0.0013695701

Batch 22630, train_perplexity=1.0013698, train_loss=0.0013688856

Batch 22640, train_perplexity=1.001369, train_loss=0.0013681119

Batch 22650, train_perplexity=1.0013683, train_loss=0.0013673888

Batch 22660, train_perplexity=1.0013676, train_loss=0.0013666211

Batch 22670, train_perplexity=1.0013669, train_loss=0.0013659725

Batch 22680, train_perplexity=1.0013661, train_loss=0.0013652314

Batch 22690, train_perplexity=1.0013654, train_loss=0.0013645263

Batch 22700, train_perplexity=1.0013647, train_loss=0.001363821

Batch 22710, train_perplexity=1.001364, train_loss=0.0013630772

Batch 22720, train_perplexity=1.0013634, train_loss=0.0013624373

Batch 22730, train_perplexity=1.0013627, train_loss=0.0013617201

Batch 22740, train_perplexity=1.001362, train_loss=0.001360991

Batch 22750, train_perplexity=1.0013611, train_loss=0.0013602264

Batch 22760, train_perplexity=1.0013605, train_loss=0.0013595806

Batch 22770, train_perplexity=1.0013598, train_loss=0.0013589141

Batch 22780, train_perplexity=1.0013591, train_loss=0.001358185

Batch 22790, train_perplexity=1.0013584, train_loss=0.0013574768

Batch 22800, train_perplexity=1.0013576, train_loss=0.0013566762

Batch 22810, train_perplexity=1.001357, train_loss=0.0013560901

Batch 22820, train_perplexity=1.0013562, train_loss=0.0013553075

Batch 22830, train_perplexity=1.0013556, train_loss=0.0013547332

Batch 22840, train_perplexity=1.0013549, train_loss=0.0013539654

Batch 22850, train_perplexity=1.0013541, train_loss=0.0013532215

Batch 22860, train_perplexity=1.0013534, train_loss=0.0013525013

Batch 22870, train_perplexity=1.0013527, train_loss=0.0013517991

Batch 22880, train_perplexity=1.0013521, train_loss=0.0013511681

Batch 22890, train_perplexity=1.0013515, train_loss=0.0013505135

Batch 22900, train_perplexity=1.0013506, train_loss=0.0013497547

Batch 22910, train_perplexity=1.00135, train_loss=0.001349115

Batch 22920, train_perplexity=1.0013493, train_loss=0.0013484336

Batch 22930, train_perplexity=1.0013486, train_loss=0.001347749

Batch 22940, train_perplexity=1.0013479, train_loss=0.0013469396

Batch 22950, train_perplexity=1.0013472, train_loss=0.001346282

Batch 22960, train_perplexity=1.0013465, train_loss=0.0013455916

Batch 22970, train_perplexity=1.0013459, train_loss=0.0013449191

Batch 22980, train_perplexity=1.0013452, train_loss=0.0013442615

Batch 22990, train_perplexity=1.0013444, train_loss=0.0013435888

Batch 23000, train_perplexity=1.0013438, train_loss=0.0013429134

Batch 23010, train_perplexity=1.0013431, train_loss=0.0013421753

Batch 23020, train_perplexity=1.0013424, train_loss=0.0013415

Batch 23030, train_perplexity=1.0013417, train_loss=0.0013407529

Batch 23040, train_perplexity=1.0013411, train_loss=0.0013401608

Batch 23050, train_perplexity=1.0013403, train_loss=0.0013394167

Batch 23060, train_perplexity=1.0013397, train_loss=0.0013387413

Batch 23070, train_perplexity=1.001339, train_loss=0.0013380301

Batch 23080, train_perplexity=1.0013384, train_loss=0.0013374409

Batch 23090, train_perplexity=1.0013376, train_loss=0.0013367536

Batch 23100, train_perplexity=1.0013369, train_loss=0.0013360633

Batch 23110, train_perplexity=1.0013362, train_loss=0.0013353638

Batch 23120, train_perplexity=1.0013356, train_loss=0.0013347092

Batch 23130, train_perplexity=1.0013349, train_loss=0.0013340604

Batch 23140, train_perplexity=1.0013342, train_loss=0.0013333164

Batch 23150, train_perplexity=1.0013335, train_loss=0.0013325755

Batch 23160, train_perplexity=1.0013329, train_loss=0.0013320071

Batch 23170, train_perplexity=1.0013322, train_loss=0.0013312781

Batch 23180, train_perplexity=1.0013314, train_loss=0.0013305609

Batch 23190, train_perplexity=1.0013309, train_loss=0.0013299836

Batch 23200, train_perplexity=1.0013301, train_loss=0.0013292962

Batch 23210, train_perplexity=1.0013294, train_loss=0.0013285731

Batch 23220, train_perplexity=1.0013288, train_loss=0.001327975

Batch 23230, train_perplexity=1.0013281, train_loss=0.0013272666

Batch 23240, train_perplexity=1.0013275, train_loss=0.0013265823

Batch 23250, train_perplexity=1.0013268, train_loss=0.0013259127

Batch 23260, train_perplexity=1.0013261, train_loss=0.0013252341

Batch 23270, train_perplexity=1.0013255, train_loss=0.0013245647

Batch 23280, train_perplexity=1.0013248, train_loss=0.0013239426

Batch 23290, train_perplexity=1.001324, train_loss=0.0013231778

Batch 23300, train_perplexity=1.0013235, train_loss=0.0013225588

Batch 23310, train_perplexity=1.0013229, train_loss=0.0013219339

Batch 23320, train_perplexity=1.001322, train_loss=0.0013212108

Batch 23330, train_perplexity=1.0013214, train_loss=0.0013205709

Batch 23340, train_perplexity=1.0013207, train_loss=0.0013199016

Batch 23350, train_perplexity=1.0013201, train_loss=0.0013192379

Batch 23360, train_perplexity=1.0013194, train_loss=0.0013185891

Batch 23370, train_perplexity=1.0013187, train_loss=0.0013178808

Batch 23380, train_perplexity=1.0013181, train_loss=0.0013171934

Batch 23390, train_perplexity=1.0013175, train_loss=0.0013165952

Batch 23400, train_perplexity=1.0013169, train_loss=0.001316003

Batch 23410, train_perplexity=1.0013161, train_loss=0.0013152442

Batch 23420, train_perplexity=1.0013155, train_loss=0.0013146074

Batch 23430, train_perplexity=1.0013148, train_loss=0.001313911

Batch 23440, train_perplexity=1.0013142, train_loss=0.0013132712

Batch 23450, train_perplexity=1.0013136, train_loss=0.0013126612

Batch 23460, train_perplexity=1.0013129, train_loss=0.0013120333

Batch 23470, train_perplexity=1.0013123, train_loss=0.0013113489

Batch 23480, train_perplexity=1.0013117, train_loss=0.0013107655

Batch 23490, train_perplexity=1.0013108, train_loss=0.0013099888

Batch 23500, train_perplexity=1.0013103, train_loss=0.0013094652

Batch 23510, train_perplexity=1.0013095, train_loss=0.0013086705

Batch 23520, train_perplexity=1.0013089, train_loss=0.001308111
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 23530, train_perplexity=1.0013083, train_loss=0.0013074356

Batch 23540, train_perplexity=1.0013076, train_loss=0.0013067897

Batch 23550, train_perplexity=1.001307, train_loss=0.0013061292

Batch 23560, train_perplexity=1.0013063, train_loss=0.0013054416

Batch 23570, train_perplexity=1.0013056, train_loss=0.0013047662

Batch 23580, train_perplexity=1.001305, train_loss=0.0013041411

Batch 23590, train_perplexity=1.0013044, train_loss=0.0013035014

Batch 23600, train_perplexity=1.0013037, train_loss=0.001302802

Batch 23610, train_perplexity=1.0013031, train_loss=0.0013021801

Batch 23620, train_perplexity=1.0013025, train_loss=0.001301579

Batch 23630, train_perplexity=1.0013018, train_loss=0.0013009212

Batch 23640, train_perplexity=1.001301, train_loss=0.0013002576

Batch 23650, train_perplexity=1.0013005, train_loss=0.0012996505

Batch 23660, train_perplexity=1.0012999, train_loss=0.001298966

Batch 23670, train_perplexity=1.0012991, train_loss=0.0012982875

Batch 23680, train_perplexity=1.0012985, train_loss=0.001297713

Batch 23690, train_perplexity=1.001298, train_loss=0.0012970823

Batch 23700, train_perplexity=1.0012971, train_loss=0.0012963323

Batch 23710, train_perplexity=1.0012966, train_loss=0.001295773

Batch 23720, train_perplexity=1.001296, train_loss=0.0012951479

Batch 23730, train_perplexity=1.0012952, train_loss=0.0012943919

Batch 23740, train_perplexity=1.0012947, train_loss=0.0012938564

Batch 23750, train_perplexity=1.0012941, train_loss=0.0012932403

Batch 23760, train_perplexity=1.0012933, train_loss=0.0012925202

Batch 23770, train_perplexity=1.0012928, train_loss=0.001291937

Batch 23780, train_perplexity=1.0012921, train_loss=0.0012912435

Batch 23790, train_perplexity=1.0012915, train_loss=0.0012906662

Batch 23800, train_perplexity=1.0012908, train_loss=0.0012899847

Batch 23810, train_perplexity=1.0012902, train_loss=0.0012894103

Batch 23820, train_perplexity=1.0012896, train_loss=0.0012887199

Batch 23830, train_perplexity=1.0012889, train_loss=0.0012881039

Batch 23840, train_perplexity=1.0012883, train_loss=0.0012874166

Batch 23850, train_perplexity=1.0012877, train_loss=0.0012868599

Batch 23860, train_perplexity=1.0012871, train_loss=0.0012862647

Batch 23870, train_perplexity=1.0012865, train_loss=0.0012856277

Batch 23880, train_perplexity=1.0012858, train_loss=0.0012849404

Batch 23890, train_perplexity=1.0012851, train_loss=0.0012842976

Batch 23900, train_perplexity=1.0012845, train_loss=0.0012836666

Batch 23910, train_perplexity=1.0012839, train_loss=0.0012830774

Batch 23920, train_perplexity=1.0012833, train_loss=0.0012824109

Batch 23930, train_perplexity=1.0012826, train_loss=0.0012817918

Batch 23940, train_perplexity=1.001282, train_loss=0.0012811283

Batch 23950, train_perplexity=1.0012814, train_loss=0.0012805031

Batch 23960, train_perplexity=1.0012807, train_loss=0.0012798693

Batch 23970, train_perplexity=1.0012801, train_loss=0.0012792592

Batch 23980, train_perplexity=1.0012795, train_loss=0.0012786731

Batch 23990, train_perplexity=1.0012789, train_loss=0.001278054

Batch 24000, train_perplexity=1.0012782, train_loss=0.0012773783

Batch 24010, train_perplexity=1.0012777, train_loss=0.0012768755

Batch 24020, train_perplexity=1.0012769, train_loss=0.001276084

Batch 24030, train_perplexity=1.0012764, train_loss=0.0012755243

Batch 24040, train_perplexity=1.0012758, train_loss=0.0012749768

Batch 24050, train_perplexity=1.0012751, train_loss=0.0012743042

Batch 24060, train_perplexity=1.0012745, train_loss=0.0012736821

Batch 24070, train_perplexity=1.0012739, train_loss=0.0012730721

Batch 24080, train_perplexity=1.0012733, train_loss=0.0012724443

Batch 24090, train_perplexity=1.0012727, train_loss=0.0012718401

Batch 24100, train_perplexity=1.001272, train_loss=0.0012711942

Batch 24110, train_perplexity=1.0012714, train_loss=0.0012705454

Batch 24120, train_perplexity=1.0012707, train_loss=0.0012698551

Batch 24130, train_perplexity=1.0012702, train_loss=0.0012693222

Batch 24140, train_perplexity=1.0012696, train_loss=0.0012687212

Batch 24150, train_perplexity=1.0012689, train_loss=0.001268117

Batch 24160, train_perplexity=1.0012683, train_loss=0.0012674981

Batch 24170, train_perplexity=1.0012677, train_loss=0.0012668641

Batch 24180, train_perplexity=1.0012672, train_loss=0.0012663463

Batch 24190, train_perplexity=1.0012665, train_loss=0.0012656886

Batch 24200, train_perplexity=1.0012658, train_loss=0.0012649593

Batch 24210, train_perplexity=1.0012653, train_loss=0.0012644625

Batch 24220, train_perplexity=1.0012646, train_loss=0.0012637869

Batch 24230, train_perplexity=1.001264, train_loss=0.0012631232

Batch 24240, train_perplexity=1.0012633, train_loss=0.0012625102

Batch 24250, train_perplexity=1.0012628, train_loss=0.0012620073

Batch 24260, train_perplexity=1.0012622, train_loss=0.0012613465

Batch 24270, train_perplexity=1.0012616, train_loss=0.0012607782

Batch 24280, train_perplexity=1.0012609, train_loss=0.0012600371

Batch 24290, train_perplexity=1.0012604, train_loss=0.0012595581

Batch 24300, train_perplexity=1.0012597, train_loss=0.0012588855

Batch 24310, train_perplexity=1.0012591, train_loss=0.0012583378

Batch 24320, train_perplexity=1.0012585, train_loss=0.001257692

Batch 24330, train_perplexity=1.0012579, train_loss=0.0012571235

Batch 24340, train_perplexity=1.0012572, train_loss=0.0012564182

Batch 24350, train_perplexity=1.0012567, train_loss=0.0012558855

Batch 24360, train_perplexity=1.001256, train_loss=0.0012552277

Batch 24370, train_perplexity=1.0012555, train_loss=0.0012547248

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 24380, train_perplexity=1.0012548, train_loss=0.0012540641

Batch 24390, train_perplexity=1.0012542, train_loss=0.0012534094

Batch 24400, train_perplexity=1.0012536, train_loss=0.0012528738

Batch 24410, train_perplexity=1.001253, train_loss=0.0012522219

Batch 24420, train_perplexity=1.0012524, train_loss=0.0012516803

Batch 24430, train_perplexity=1.0012518, train_loss=0.0012510285

Batch 24440, train_perplexity=1.0012512, train_loss=0.0012504065

Batch 24450, train_perplexity=1.0012506, train_loss=0.0012498291

Batch 24460, train_perplexity=1.00125, train_loss=0.0012492875

Batch 24470, train_perplexity=1.0012494, train_loss=0.0012486387

Batch 24480, train_perplexity=1.0012488, train_loss=0.0012480703

Batch 24490, train_perplexity=1.0012482, train_loss=0.0012474691

Batch 24500, train_perplexity=1.0012475, train_loss=0.0012468025

Batch 24510, train_perplexity=1.001247, train_loss=0.0012462459

Batch 24520, train_perplexity=1.0012465, train_loss=0.001245633

Batch 24530, train_perplexity=1.0012459, train_loss=0.0012450704

Batch 24540, train_perplexity=1.0012453, train_loss=0.0012444484

Batch 24550, train_perplexity=1.0012447, train_loss=0.0012438563

Batch 24560, train_perplexity=1.0012441, train_loss=0.0012432609

Batch 24570, train_perplexity=1.0012434, train_loss=0.001242615

Batch 24580, train_perplexity=1.0012428, train_loss=0.0012420109

Batch 24590, train_perplexity=1.0012423, train_loss=0.0012415049

Batch 24600, train_perplexity=1.0012417, train_loss=0.0012408949

Batch 24610, train_perplexity=1.0012411, train_loss=0.0012403263

Batch 24620, train_perplexity=1.0012405, train_loss=0.0012397044

Batch 24630, train_perplexity=1.0012398, train_loss=0.0012390616

Batch 24640, train_perplexity=1.0012393, train_loss=0.0012385081

Batch 24650, train_perplexity=1.0012387, train_loss=0.0012379515

Batch 24660, train_perplexity=1.0012381, train_loss=0.0012373445

Batch 24670, train_perplexity=1.0012375, train_loss=0.0012367163

Batch 24680, train_perplexity=1.001237, train_loss=0.0012362255

Batch 24690, train_perplexity=1.0012363, train_loss=0.0012355259

Batch 24700, train_perplexity=1.0012357, train_loss=0.0012349873

Batch 24710, train_perplexity=1.0012351, train_loss=0.0012343919
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24720, train_perplexity=1.0012345, train_loss=0.0012337908

Batch 24730, train_perplexity=1.0012339, train_loss=0.0012331868

Batch 24740, train_perplexity=1.0012333, train_loss=0.0012326094

Batch 24750, train_perplexity=1.0012329, train_loss=0.0012320826

Batch 24760, train_perplexity=1.0012323, train_loss=0.0012314843

Batch 24770, train_perplexity=1.0012316, train_loss=0.0012308356

Batch 24780, train_perplexity=1.0012311, train_loss=0.0012303058

Batch 24790, train_perplexity=1.0012306, train_loss=0.0012298386

Batch 24800, train_perplexity=1.0012299, train_loss=0.0012291749

Batch 24810, train_perplexity=1.0012293, train_loss=0.0012285558

Batch 24820, train_perplexity=1.0012288, train_loss=0.0012280024

Batch 24830, train_perplexity=1.0012281, train_loss=0.0012273772

Batch 24840, train_perplexity=1.0012276, train_loss=0.0012268089

Batch 24850, train_perplexity=1.001227, train_loss=0.0012262462

Batch 24860, train_perplexity=1.0012264, train_loss=0.0012256689

Batch 24870, train_perplexity=1.0012258, train_loss=0.0012250587

Batch 24880, train_perplexity=1.0012252, train_loss=0.0012244903

Batch 24890, train_perplexity=1.0012248, train_loss=0.0012239725

Batch 24900, train_perplexity=1.001224, train_loss=0.0012233236

Batch 24910, train_perplexity=1.0012236, train_loss=0.001222794

Batch 24920, train_perplexity=1.001223, train_loss=0.0012222046

Batch 24930, train_perplexity=1.0012224, train_loss=0.0012216184

Batch 24940, train_perplexity=1.0012219, train_loss=0.0012211124

Batch 24950, train_perplexity=1.0012213, train_loss=0.001220553

Batch 24960, train_perplexity=1.0012207, train_loss=0.0012199606

Batch 24970, train_perplexity=1.0012201, train_loss=0.0012193446

Batch 24980, train_perplexity=1.0012195, train_loss=0.0012188058

Batch 24990, train_perplexity=1.0012189, train_loss=0.0012182256

Batch 25000, train_perplexity=1.0012184, train_loss=0.0012176838

Batch 25010, train_perplexity=1.0012178, train_loss=0.0012170617

Batch 25020, train_perplexity=1.0012172, train_loss=0.0012164813

Batch 25030, train_perplexity=1.0012167, train_loss=0.0012159576

Batch 25040, train_perplexity=1.0012162, train_loss=0.0012154069

Batch 25050, train_perplexity=1.0012156, train_loss=0.001214788

Batch 25060, train_perplexity=1.001215, train_loss=0.0012142523

Batch 25070, train_perplexity=1.0012144, train_loss=0.001213636

Batch 25080, train_perplexity=1.0012139, train_loss=0.001213151

Batch 25090, train_perplexity=1.0012133, train_loss=0.001212538

Batch 25100, train_perplexity=1.0012127, train_loss=0.0012120111

Batch 25110, train_perplexity=1.0012121, train_loss=0.001211404

Batch 25120, train_perplexity=1.0012115, train_loss=0.0012108206

Batch 25130, train_perplexity=1.001211, train_loss=0.0012102552

Batch 25140, train_perplexity=1.0012105, train_loss=0.0012096599

Batch 25150, train_perplexity=1.0012099, train_loss=0.0012091034

Batch 25160, train_perplexity=1.0012093, train_loss=0.0012085319

Batch 25170, train_perplexity=1.0012088, train_loss=0.0012080141

Batch 25180, train_perplexity=1.0012082, train_loss=0.0012074397

Batch 25190, train_perplexity=1.0012077, train_loss=0.0012069456

Batch 25200, train_perplexity=1.0012071, train_loss=0.0012063623

Batch 25210, train_perplexity=1.0012065, train_loss=0.00120577

Batch 25220, train_perplexity=1.0012059, train_loss=0.0012052255

Batch 25230, train_perplexity=1.0012053, train_loss=0.0012046332

Batch 25240, train_perplexity=1.0012048, train_loss=0.0012040855

Batch 25250, train_perplexity=1.0012043, train_loss=0.0012035081

Batch 25260, train_perplexity=1.0012038, train_loss=0.0012030051

Batch 25270, train_perplexity=1.0012032, train_loss=0.0012024068

Batch 25280, train_perplexity=1.0012026, train_loss=0.0012018443

Batch 25290, train_perplexity=1.001202, train_loss=0.0012012343

Batch 25300, train_perplexity=1.0012015, train_loss=0.001200743

Batch 25310, train_perplexity=1.0012009, train_loss=0.0012001656

Batch 25320, train_perplexity=1.0012003, train_loss=0.001199618

Batch 25330, train_perplexity=1.0011997, train_loss=0.0011990436

Batch 25340, train_perplexity=1.0011992, train_loss=0.0011984931

Batch 25350, train_perplexity=1.0011988, train_loss=0.001197993

Batch 25360, train_perplexity=1.0011982, train_loss=0.0011974394

Batch 25370, train_perplexity=1.0011976, train_loss=0.0011968294

Batch 25380, train_perplexity=1.0011971, train_loss=0.0011963353

Batch 25390, train_perplexity=1.0011965, train_loss=0.0011958026

Batch 25400, train_perplexity=1.0011959, train_loss=0.0011951745

Batch 25410, train_perplexity=1.0011954, train_loss=0.0011946715

Batch 25420, train_perplexity=1.0011948, train_loss=0.0011940971

Batch 25430, train_perplexity=1.0011944, train_loss=0.0011935972

Batch 25440, train_perplexity=1.0011938, train_loss=0.0011930733

Batch 25450, train_perplexity=1.0011932, train_loss=0.0011924661

Batch 25460, train_perplexity=1.0011927, train_loss=0.0011919244

Batch 25470, train_perplexity=1.001192, train_loss=0.0011913201

Batch 25480, train_perplexity=1.0011916, train_loss=0.0011908559

Batch 25490, train_perplexity=1.001191, train_loss=0.0011903291

Batch 25500, train_perplexity=1.0011904, train_loss=0.0011896862

Batch 25510, train_perplexity=1.00119, train_loss=0.0011892279

Batch 25520, train_perplexity=1.0011894, train_loss=0.0011886742

Batch 25530, train_perplexity=1.0011888, train_loss=0.001188079

Batch 25540, train_perplexity=1.0011883, train_loss=0.0011875224

Batch 25550, train_perplexity=1.0011877, train_loss=0.0011870104

Batch 25560, train_perplexity=1.0011872, train_loss=0.0011865224

Batch 25570, train_perplexity=1.0011866, train_loss=0.0011859243

Batch 25580, train_perplexity=1.0011861, train_loss=0.0011854153

Batch 25590, train_perplexity=1.0011855, train_loss=0.001184814

Batch 25600, train_perplexity=1.0011849, train_loss=0.0011842962

Batch 25610, train_perplexity=1.0011845, train_loss=0.0011837543

Batch 25620, train_perplexity=1.001184, train_loss=0.0011832456

Batch 25630, train_perplexity=1.0011834, train_loss=0.0011826353

Batch 25640, train_perplexity=1.0011829, train_loss=0.0011821561

Batch 25650, train_perplexity=1.0011822, train_loss=0.0011815312

Batch 25660, train_perplexity=1.0011817, train_loss=0.0011809984

Batch 25670, train_perplexity=1.0011811, train_loss=0.0011804537

Batch 25680, train_perplexity=1.0011808, train_loss=0.00118004

Batch 25690, train_perplexity=1.00118, train_loss=0.0011794149

Batch 25700, train_perplexity=1.0011796, train_loss=0.0011788823

Batch 25710, train_perplexity=1.001179, train_loss=0.0011782721

Batch 25720, train_perplexity=1.0011785, train_loss=0.0011778376

Batch 25730, train_perplexity=1.001178, train_loss=0.0011773107

Batch 25740, train_perplexity=1.0011774, train_loss=0.0011767452

Batch 25750, train_perplexity=1.0011768, train_loss=0.0011761857

Batch 25760, train_perplexity=1.0011764, train_loss=0.0011756469

Batch 25770, train_perplexity=1.0011758, train_loss=0.0011750993

Batch 25780, train_perplexity=1.0011753, train_loss=0.0011745517

Batch 25790, train_perplexity=1.0011748, train_loss=0.0011740904

Batch 25800, train_perplexity=1.0011743, train_loss=0.0011735873

Batch 25810, train_perplexity=1.0011737, train_loss=0.0011730546

Batch 25820, train_perplexity=1.0011731, train_loss=0.0011724116

Batch 25830, train_perplexity=1.0011727, train_loss=0.0011720187

Batch 25840, train_perplexity=1.0011722, train_loss=0.0011714651

Batch 25850, train_perplexity=1.0011715, train_loss=0.0011708103

Batch 25860, train_perplexity=1.0011711, train_loss=0.0011703967

Batch 25870, train_perplexity=1.0011705, train_loss=0.0011697747

Batch 25880, train_perplexity=1.00117, train_loss=0.0011693132

Batch 25890, train_perplexity=1.0011694, train_loss=0.0011688013

Batch 25900, train_perplexity=1.001169, train_loss=0.0011682507

Batch 25910, train_perplexity=1.0011685, train_loss=0.0011677537

Batch 25920, train_perplexity=1.0011679, train_loss=0.0011671584

Batch 25930, train_perplexity=1.0011674, train_loss=0.0011666762

Batch 25940, train_perplexity=1.0011668, train_loss=0.0011661047

Batch 25950, train_perplexity=1.0011663, train_loss=0.0011656731

Batch 25960, train_perplexity=1.0011657, train_loss=0.001165075
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 25970, train_perplexity=1.0011653, train_loss=0.0011645451

Batch 25980, train_perplexity=1.0011647, train_loss=0.0011640302

Batch 25990, train_perplexity=1.0011642, train_loss=0.0011634796

Batch 26000, train_perplexity=1.0011637, train_loss=0.0011629856

Batch 26010, train_perplexity=1.0011632, train_loss=0.001162539

Batch 26020, train_perplexity=1.0011626, train_loss=0.0011619735

Batch 26030, train_perplexity=1.0011622, train_loss=0.0011614736

Batch 26040, train_perplexity=1.0011616, train_loss=0.0011608663

Batch 26050, train_perplexity=1.0011611, train_loss=0.0011604645

Batch 26060, train_perplexity=1.0011606, train_loss=0.0011599406

Batch 26070, train_perplexity=1.00116, train_loss=0.001159393

Batch 26080, train_perplexity=1.0011595, train_loss=0.0011588722

Batch 26090, train_perplexity=1.001159, train_loss=0.0011583215

Batch 26100, train_perplexity=1.0011585, train_loss=0.0011577767

Batch 26110, train_perplexity=1.0011579, train_loss=0.0011572412

Batch 26120, train_perplexity=1.0011574, train_loss=0.0011567709

Batch 26130, train_perplexity=1.0011569, train_loss=0.0011562172

Batch 26140, train_perplexity=1.0011563, train_loss=0.0011557112

Batch 26150, train_perplexity=1.0011559, train_loss=0.0011552052

Batch 26160, train_perplexity=1.0011554, train_loss=0.0011546814

Batch 26170, train_perplexity=1.0011549, train_loss=0.001154217

Batch 26180, train_perplexity=1.0011542, train_loss=0.0011535741

Batch 26190, train_perplexity=1.0011538, train_loss=0.001153202

Batch 26200, train_perplexity=1.0011533, train_loss=0.0011526335

Batch 26210, train_perplexity=1.0011529, train_loss=0.0011521752

Batch 26220, train_perplexity=1.0011523, train_loss=0.0011515829

Batch 26230, train_perplexity=1.0011517, train_loss=0.0011510709

Batch 26240, train_perplexity=1.0011513, train_loss=0.0011506126

Batch 26250, train_perplexity=1.0011507, train_loss=0.0011500858

Batch 26260, train_perplexity=1.0011503, train_loss=0.0011495916

Batch 26270, train_perplexity=1.0011498, train_loss=0.0011491214

Batch 26280, train_perplexity=1.0011492, train_loss=0.0011484636

Batch 26290, train_perplexity=1.0011487, train_loss=0.0011480707

Batch 26300, train_perplexity=1.0011482, train_loss=0.0011475974

Batch 26310, train_perplexity=1.0011476, train_loss=0.0011470141

Batch 26320, train_perplexity=1.0011472, train_loss=0.0011465081

Batch 26330, train_perplexity=1.0011468, train_loss=0.0011460885

Batch 26340, train_perplexity=1.0011462, train_loss=0.0011454842

Batch 26350, train_perplexity=1.0011456, train_loss=0.0011449901

Batch 26360, train_perplexity=1.0011451, train_loss=0.0011444811

Batch 26370, train_perplexity=1.0011446, train_loss=0.0011440318

Batch 26380, train_perplexity=1.0011443, train_loss=0.0011436061

Batch 26390, train_perplexity=1.0011437, train_loss=0.001142981

Batch 26400, train_perplexity=1.0011431, train_loss=0.0011424513

Batch 26410, train_perplexity=1.0011426, train_loss=0.0011419303

Batch 26420, train_perplexity=1.001142, train_loss=0.0011414184

Batch 26430, train_perplexity=1.0011415, train_loss=0.0011409272

Batch 26440, train_perplexity=1.0011411, train_loss=0.0011404331

Batch 26450, train_perplexity=1.0011406, train_loss=0.0011399777

Batch 26460, train_perplexity=1.0011401, train_loss=0.0011395046

Batch 26470, train_perplexity=1.0011396, train_loss=0.0011389509

Batch 26480, train_perplexity=1.0011392, train_loss=0.0011384687

Batch 26490, train_perplexity=1.0011386, train_loss=0.0011379418

Batch 26500, train_perplexity=1.0011381, train_loss=0.0011374566

Batch 26510, train_perplexity=1.0011376, train_loss=0.0011369358

Batch 26520, train_perplexity=1.0011371, train_loss=0.0011364536

Batch 26530, train_perplexity=1.0011367, train_loss=0.0011359655

Batch 26540, train_perplexity=1.0011361, train_loss=0.0011353851

Batch 26550, train_perplexity=1.0011356, train_loss=0.0011349653

Batch 26560, train_perplexity=1.0011351, train_loss=0.0011344594

Batch 26570, train_perplexity=1.0011345, train_loss=0.0011339267

Batch 26580, train_perplexity=1.001134, train_loss=0.0011334415

Batch 26590, train_perplexity=1.0011336, train_loss=0.0011329682

Batch 26600, train_perplexity=1.0011332, train_loss=0.0011325157

Batch 26610, train_perplexity=1.0011326, train_loss=0.0011319591

Batch 26620, train_perplexity=1.0011321, train_loss=0.0011314829

Batch 26630, train_perplexity=1.0011317, train_loss=0.0011309918

Batch 26640, train_perplexity=1.0011312, train_loss=0.0011305215

Batch 26650, train_perplexity=1.0011306, train_loss=0.0011299469

Batch 26660, train_perplexity=1.0011301, train_loss=0.0011294411

Batch 26670, train_perplexity=1.0011296, train_loss=0.0011289499

Batch 26680, train_perplexity=1.0011292, train_loss=0.0011285392

Batch 26690, train_perplexity=1.0011287, train_loss=0.0011280035

Batch 26700, train_perplexity=1.0011281, train_loss=0.0011274915

Batch 26710, train_perplexity=1.0011276, train_loss=0.0011269349

Batch 26720, train_perplexity=1.0011271, train_loss=0.0011265359

Batch 26730, train_perplexity=1.0011266, train_loss=0.0011259824

Batch 26740, train_perplexity=1.001126, train_loss=0.0011254614

Batch 26750, train_perplexity=1.0011257, train_loss=0.0011250805

Batch 26760, train_perplexity=1.0011252, train_loss=0.0011245596

Batch 26770, train_perplexity=1.0011247, train_loss=0.0011240775

Batch 26780, train_perplexity=1.0011241, train_loss=0.0011235683

Batch 26790, train_perplexity=1.0011238, train_loss=0.001123116

Batch 26800, train_perplexity=1.0011232, train_loss=0.0011226069

Batch 26810, train_perplexity=1.0011227, train_loss=0.0011221189

Batch 26820, train_perplexity=1.0011222, train_loss=0.0011215978

Batch 26830, train_perplexity=1.0011218, train_loss=0.0011211454

Batch 26840, train_perplexity=1.0011213, train_loss=0.0011206544

Batch 26850, train_perplexity=1.0011207, train_loss=0.0011200889

Batch 26860, train_perplexity=1.0011203, train_loss=0.001119684

Batch 26870, train_perplexity=1.0011199, train_loss=0.001119172

Batch 26880, train_perplexity=1.0011194, train_loss=0.0011187226

Batch 26890, train_perplexity=1.0011188, train_loss=0.0011181601

Batch 26900, train_perplexity=1.0011183, train_loss=0.0011177225

Batch 26910, train_perplexity=1.001118, train_loss=0.0011172611

Batch 26920, train_perplexity=1.0011173, train_loss=0.0011167193

Batch 26930, train_perplexity=1.0011169, train_loss=0.0011162134

Batch 26940, train_perplexity=1.0011164, train_loss=0.001115749

Batch 26950, train_perplexity=1.001116, train_loss=0.0011153622

Batch 26960, train_perplexity=1.0011154, train_loss=0.0011148532

Batch 26970, train_perplexity=1.001115, train_loss=0.0011143321

Batch 26980, train_perplexity=1.0011145, train_loss=0.0011138291

Batch 26990, train_perplexity=1.0011139, train_loss=0.0011133291

Batch 27000, train_perplexity=1.0011134, train_loss=0.0011128499

Batch 27010, train_perplexity=1.0011129, train_loss=0.0011123677

Batch 27020, train_perplexity=1.0011126, train_loss=0.001111933

Batch 27030, train_perplexity=1.0011121, train_loss=0.0011114927

Batch 27040, train_perplexity=1.0011116, train_loss=0.0011110283

Batch 27050, train_perplexity=1.0011111, train_loss=0.0011104865

Batch 27060, train_perplexity=1.0011107, train_loss=0.0011100161

Batch 27070, train_perplexity=1.0011101, train_loss=0.0011094983

Batch 27080, train_perplexity=1.0011097, train_loss=0.0011090667

Batch 27090, train_perplexity=1.0011091, train_loss=0.001108522

Batch 27100, train_perplexity=1.0011088, train_loss=0.001108147

Batch 27110, train_perplexity=1.0011082, train_loss=0.0011075546

Batch 27120, train_perplexity=1.0011078, train_loss=0.0011071558

Batch 27130, train_perplexity=1.0011073, train_loss=0.0011066794

Batch 27140, train_perplexity=1.0011069, train_loss=0.0011062032

Batch 27150, train_perplexity=1.0011064, train_loss=0.0011057449

Batch 27160, train_perplexity=1.0011059, train_loss=0.0011052417

Batch 27170, train_perplexity=1.0011053, train_loss=0.0011047507

Batch 27180, train_perplexity=1.001105, train_loss=0.0011043102

Batch 27190, train_perplexity=1.0011044, train_loss=0.0011037475

Batch 27200, train_perplexity=1.001104, train_loss=0.0011033755

Batch 27210, train_perplexity=1.0011035, train_loss=0.0011028904
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 27220, train_perplexity=1.001103, train_loss=0.0011024617

Batch 27230, train_perplexity=1.0011026, train_loss=0.001101908

Batch 27240, train_perplexity=1.0011021, train_loss=0.0011014704

Batch 27250, train_perplexity=1.0011016, train_loss=0.0011010419

Batch 27260, train_perplexity=1.0011011, train_loss=0.001100512

Batch 27270, train_perplexity=1.0011007, train_loss=0.0011000535

Batch 27280, train_perplexity=1.0011002, train_loss=0.0010995803

Batch 27290, train_perplexity=1.0010997, train_loss=0.001099113

Batch 27300, train_perplexity=1.0010992, train_loss=0.0010986188

Batch 27310, train_perplexity=1.0010988, train_loss=0.0010981546

Batch 27320, train_perplexity=1.0010983, train_loss=0.001097705

Batch 27330, train_perplexity=1.0010979, train_loss=0.0010972585

Batch 27340, train_perplexity=1.0010974, train_loss=0.0010968449

Batch 27350, train_perplexity=1.0010968, train_loss=0.0010962794

Batch 27360, train_perplexity=1.0010965, train_loss=0.0010958833

Batch 27370, train_perplexity=1.001096, train_loss=0.0010953564

Batch 27380, train_perplexity=1.0010954, train_loss=0.0010948714

Batch 27390, train_perplexity=1.001095, train_loss=0.0010944308

Batch 27400, train_perplexity=1.0010946, train_loss=0.0010940172

Batch 27410, train_perplexity=1.0010941, train_loss=0.00109352

Batch 27420, train_perplexity=1.0010936, train_loss=0.0010930378

Batch 27430, train_perplexity=1.0010931, train_loss=0.0010925436

Batch 27440, train_perplexity=1.0010927, train_loss=0.001092121

Batch 27450, train_perplexity=1.0010923, train_loss=0.0010916893

Batch 27460, train_perplexity=1.0010917, train_loss=0.0010911773

Batch 27470, train_perplexity=1.0010914, train_loss=0.0010907128

Batch 27480, train_perplexity=1.0010909, train_loss=0.0010902308

Batch 27490, train_perplexity=1.0010903, train_loss=0.0010897366

Batch 27500, train_perplexity=1.0010899, train_loss=0.0010893289

Batch 27510, train_perplexity=1.0010895, train_loss=0.001088805

Batch 27520, train_perplexity=1.001089, train_loss=0.0010883554

Batch 27530, train_perplexity=1.0010885, train_loss=0.001087903

Batch 27540, train_perplexity=1.001088, train_loss=0.0010874625

Batch 27550, train_perplexity=1.0010875, train_loss=0.0010870071

Batch 27560, train_perplexity=1.0010872, train_loss=0.0010865844

Batch 27570, train_perplexity=1.0010867, train_loss=0.0010861319

Batch 27580, train_perplexity=1.0010862, train_loss=0.0010856199

Batch 27590, train_perplexity=1.0010858, train_loss=0.0010852121

Batch 27600, train_perplexity=1.0010853, train_loss=0.0010847328

Batch 27610, train_perplexity=1.0010848, train_loss=0.0010842418

Batch 27620, train_perplexity=1.0010844, train_loss=0.0010838608

Batch 27630, train_perplexity=1.001084, train_loss=0.0010833608

Batch 27640, train_perplexity=1.0010835, train_loss=0.0010828606

Batch 27650, train_perplexity=1.001083, train_loss=0.0010824855

Batch 27660, train_perplexity=1.0010825, train_loss=0.001081917

Batch 27670, train_perplexity=1.0010821, train_loss=0.001081536

Batch 27680, train_perplexity=1.0010816, train_loss=0.0010809973

Batch 27690, train_perplexity=1.0010812, train_loss=0.0010806192

Batch 27700, train_perplexity=1.0010808, train_loss=0.0010801312

Batch 27710, train_perplexity=1.0010804, train_loss=0.0010797707

Batch 27720, train_perplexity=1.0010798, train_loss=0.0010791963

Batch 27730, train_perplexity=1.0010794, train_loss=0.0010788005

Batch 27740, train_perplexity=1.001079, train_loss=0.001078336

Batch 27750, train_perplexity=1.0010785, train_loss=0.0010778627

Batch 27760, train_perplexity=1.001078, train_loss=0.001077455

Batch 27770, train_perplexity=1.0010775, train_loss=0.0010769549

Batch 27780, train_perplexity=1.001077, train_loss=0.0010765025

Batch 27790, train_perplexity=1.0010766, train_loss=0.0010760351

Batch 27800, train_perplexity=1.0010762, train_loss=0.0010756243

Batch 27810, train_perplexity=1.0010757, train_loss=0.00107516

Batch 27820, train_perplexity=1.0010753, train_loss=0.0010747225

Batch 27830, train_perplexity=1.0010749, train_loss=0.0010743265

Batch 27840, train_perplexity=1.0010744, train_loss=0.0010738145

Batch 27850, train_perplexity=1.001074, train_loss=0.0010733977

Batch 27860, train_perplexity=1.0010735, train_loss=0.0010729304

Batch 27870, train_perplexity=1.001073, train_loss=0.0010724394

Batch 27880, train_perplexity=1.0010726, train_loss=0.0010720255

Batch 27890, train_perplexity=1.0010722, train_loss=0.00107157

Batch 27900, train_perplexity=1.0010717, train_loss=0.0010710701

Batch 27910, train_perplexity=1.0010712, train_loss=0.0010706562

Batch 27920, train_perplexity=1.0010709, train_loss=0.0010702366

Batch 27930, train_perplexity=1.0010704, train_loss=0.0010697842

Batch 27940, train_perplexity=1.0010699, train_loss=0.0010693584

Batch 27950, train_perplexity=1.0010694, train_loss=0.0010688673

Batch 27960, train_perplexity=1.001069, train_loss=0.0010683909

Batch 27970, train_perplexity=1.0010686, train_loss=0.0010680219

Batch 27980, train_perplexity=1.0010681, train_loss=0.0010675753

Batch 27990, train_perplexity=1.0010678, train_loss=0.0010671646

Batch 28000, train_perplexity=1.0010673, train_loss=0.0010666733

Batch 28010, train_perplexity=1.0010668, train_loss=0.0010662507

Batch 28020, train_perplexity=1.0010664, train_loss=0.0010658639

Batch 28030, train_perplexity=1.001066, train_loss=0.0010653993

Batch 28040, train_perplexity=1.0010655, train_loss=0.0010649648

Batch 28050, train_perplexity=1.001065, train_loss=0.001064426

Batch 28060, train_perplexity=1.0010647, train_loss=0.0010640718

Batch 28070, train_perplexity=1.0010642, train_loss=0.0010636313

Batch 28080, train_perplexity=1.0010638, train_loss=0.0010632264

Batch 28090, train_perplexity=1.0010632, train_loss=0.0010627024

Batch 28100, train_perplexity=1.0010628, train_loss=0.0010622262

Batch 28110, train_perplexity=1.0010624, train_loss=0.0010618749

Batch 28120, train_perplexity=1.0010619, train_loss=0.001061369

Batch 28130, train_perplexity=1.0010614, train_loss=0.0010609254

Batch 28140, train_perplexity=1.0010611, train_loss=0.0010605354

Batch 28150, train_perplexity=1.0010606, train_loss=0.0010600949

Batch 28160, train_perplexity=1.0010602, train_loss=0.0010596514

Batch 28170, train_perplexity=1.0010598, train_loss=0.0010591841

Batch 28180, train_perplexity=1.0010594, train_loss=0.0010588119

Batch 28190, train_perplexity=1.0010589, train_loss=0.0010583863

Batch 28200, train_perplexity=1.0010585, train_loss=0.001057901

Batch 28210, train_perplexity=1.0010581, train_loss=0.00105752

Batch 28220, train_perplexity=1.0010576, train_loss=0.0010570824

Batch 28230, train_perplexity=1.0010571, train_loss=0.0010565764

Batch 28240, train_perplexity=1.0010568, train_loss=0.0010562013

Batch 28250, train_perplexity=1.0010562, train_loss=0.0010556537

Batch 28260, train_perplexity=1.0010558, train_loss=0.0010552397

Batch 28270, train_perplexity=1.0010554, train_loss=0.001054838

Batch 28280, train_perplexity=1.001055, train_loss=0.0010544481

Batch 28290, train_perplexity=1.0010545, train_loss=0.0010539627

Batch 28300, train_perplexity=1.0010542, train_loss=0.0010535908

Batch 28310, train_perplexity=1.0010537, train_loss=0.001053177

Batch 28320, train_perplexity=1.0010533, train_loss=0.0010527454

Batch 28330, train_perplexity=1.0010529, train_loss=0.0010522512

Batch 28340, train_perplexity=1.0010525, train_loss=0.0010518939

Batch 28350, train_perplexity=1.0010519, train_loss=0.001051391

Batch 28360, train_perplexity=1.0010515, train_loss=0.0010510456

Batch 28370, train_perplexity=1.0010511, train_loss=0.0010505724

Batch 28380, train_perplexity=1.0010507, train_loss=0.0010501109

Batch 28390, train_perplexity=1.0010502, train_loss=0.0010496436

Batch 28400, train_perplexity=1.0010498, train_loss=0.0010492179

Batch 28410, train_perplexity=1.0010494, train_loss=0.0010487952

Batch 28420, train_perplexity=1.0010489, train_loss=0.0010484082

Batch 28430, train_perplexity=1.0010486, train_loss=0.0010479706

Batch 28440, train_perplexity=1.001048, train_loss=0.0010474764

Batch 28450, train_perplexity=1.0010476, train_loss=0.0010470984

Batch 28460, train_perplexity=1.0010473, train_loss=0.0010466846
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28470, train_perplexity=1.0010469, train_loss=0.0010462946

Batch 28480, train_perplexity=1.0010464, train_loss=0.0010458333

Batch 28490, train_perplexity=1.001046, train_loss=0.0010454075

Batch 28500, train_perplexity=1.0010455, train_loss=0.001044973

Batch 28510, train_perplexity=1.0010451, train_loss=0.0010445742

Batch 28520, train_perplexity=1.0010446, train_loss=0.001044074

Batch 28530, train_perplexity=1.0010443, train_loss=0.0010437078

Batch 28540, train_perplexity=1.0010438, train_loss=0.001043303

Batch 28550, train_perplexity=1.0010434, train_loss=0.0010428893

Batch 28560, train_perplexity=1.001043, train_loss=0.0010424308

Batch 28570, train_perplexity=1.0010426, train_loss=0.0010420915

Batch 28580, train_perplexity=1.0010421, train_loss=0.0010416212

Batch 28590, train_perplexity=1.0010417, train_loss=0.0010411121

Batch 28600, train_perplexity=1.0010413, train_loss=0.0010407728

Batch 28610, train_perplexity=1.0010408, train_loss=0.0010403024

Batch 28620, train_perplexity=1.0010405, train_loss=0.0010399094

Batch 28630, train_perplexity=1.00104, train_loss=0.0010394421

Batch 28640, train_perplexity=1.0010396, train_loss=0.0010390582

Batch 28650, train_perplexity=1.0010391, train_loss=0.0010386116

Batch 28660, train_perplexity=1.0010388, train_loss=0.0010382276

Batch 28670, train_perplexity=1.0010383, train_loss=0.0010377513

Batch 28680, train_perplexity=1.001038, train_loss=0.0010373822

Batch 28690, train_perplexity=1.0010375, train_loss=0.0010369804

Batch 28700, train_perplexity=1.0010371, train_loss=0.0010365546

Batch 28710, train_perplexity=1.0010366, train_loss=0.0010361141

Batch 28720, train_perplexity=1.0010363, train_loss=0.0010356973

Batch 28730, train_perplexity=1.0010359, train_loss=0.0010353669

Batch 28740, train_perplexity=1.0010355, train_loss=0.0010348668

Batch 28750, train_perplexity=1.001035, train_loss=0.001034447

Batch 28760, train_perplexity=1.0010345, train_loss=0.0010340004

Batch 28770, train_perplexity=1.0010341, train_loss=0.0010335867

Batch 28780, train_perplexity=1.0010337, train_loss=0.0010331848

Batch 28790, train_perplexity=1.0010333, train_loss=0.0010328068

Batch 28800, train_perplexity=1.001033, train_loss=0.0010323812

Batch 28810, train_perplexity=1.0010325, train_loss=0.0010318989

Batch 28820, train_perplexity=1.0010321, train_loss=0.0010315448

Batch 28830, train_perplexity=1.0010316, train_loss=0.0010311457

Batch 28840, train_perplexity=1.0010313, train_loss=0.0010307321

Batch 28850, train_perplexity=1.0010308, train_loss=0.0010303124

Batch 28860, train_perplexity=1.0010304, train_loss=0.0010298865

Batch 28870, train_perplexity=1.0010301, train_loss=0.0010295026

Batch 28880, train_perplexity=1.0010296, train_loss=0.0010290709

Batch 28890, train_perplexity=1.0010291, train_loss=0.0010286452

Batch 28900, train_perplexity=1.0010288, train_loss=0.0010281987

Batch 28910, train_perplexity=1.0010284, train_loss=0.0010278474

Batch 28920, train_perplexity=1.001028, train_loss=0.0010274098

Batch 28930, train_perplexity=1.0010276, train_loss=0.0010270019

Batch 28940, train_perplexity=1.0010271, train_loss=0.0010266328

Batch 28950, train_perplexity=1.0010267, train_loss=0.0010261715

Batch 28960, train_perplexity=1.0010263, train_loss=0.0010256921

Batch 28970, train_perplexity=1.0010259, train_loss=0.001025335

Batch 28980, train_perplexity=1.0010254, train_loss=0.0010249182

Batch 28990, train_perplexity=1.0010251, train_loss=0.0010245044

Batch 29000, train_perplexity=1.0010246, train_loss=0.0010241116

Batch 29010, train_perplexity=1.0010242, train_loss=0.0010237037

Batch 29020, train_perplexity=1.0010238, train_loss=0.0010232928

Batch 29030, train_perplexity=1.0010234, train_loss=0.0010229178

Batch 29040, train_perplexity=1.001023, train_loss=0.0010225219

Batch 29050, train_perplexity=1.0010227, train_loss=0.001022117

Batch 29060, train_perplexity=1.0010221, train_loss=0.0010216169

Batch 29070, train_perplexity=1.0010217, train_loss=0.001021215

Batch 29080, train_perplexity=1.0010214, train_loss=0.001020828

Batch 29090, train_perplexity=1.001021, train_loss=0.0010204588

Batch 29100, train_perplexity=1.0010206, train_loss=0.0010200182

Batch 29110, train_perplexity=1.0010202, train_loss=0.001019658

Batch 29120, train_perplexity=1.0010197, train_loss=0.0010192116

Batch 29130, train_perplexity=1.0010194, train_loss=0.0010188215

Batch 29140, train_perplexity=1.0010189, train_loss=0.0010184079

Batch 29150, train_perplexity=1.0010185, train_loss=0.0010180001

Batch 29160, train_perplexity=1.001018, train_loss=0.0010175743

Batch 29170, train_perplexity=1.0010177, train_loss=0.001017214

Batch 29180, train_perplexity=1.0010173, train_loss=0.0010167825

Batch 29190, train_perplexity=1.0010169, train_loss=0.00101633

Batch 29200, train_perplexity=1.0010165, train_loss=0.0010159579

Batch 29210, train_perplexity=1.001016, train_loss=0.0010155588

Batch 29220, train_perplexity=1.0010157, train_loss=0.0010151481

Batch 29230, train_perplexity=1.0010153, train_loss=0.0010148088

Batch 29240, train_perplexity=1.0010148, train_loss=0.0010143444

Batch 29250, train_perplexity=1.0010145, train_loss=0.0010139872

Batch 29260, train_perplexity=1.001014, train_loss=0.0010135049

Batch 29270, train_perplexity=1.0010138, train_loss=0.0010131864

Batch 29280, train_perplexity=1.0010133, train_loss=0.0010128024

Batch 29290, train_perplexity=1.0010129, train_loss=0.0010124183

Batch 29300, train_perplexity=1.0010126, train_loss=0.0010120105

Batch 29310, train_perplexity=1.0010121, train_loss=0.0010115372

Batch 29320, train_perplexity=1.0010116, train_loss=0.0010111502

Batch 29330, train_perplexity=1.0010113, train_loss=0.0010107781

Batch 29340, train_perplexity=1.0010109, train_loss=0.0010103524

Batch 29350, train_perplexity=1.0010105, train_loss=0.0010099981

Batch 29360, train_perplexity=1.0010102, train_loss=0.001009623

Batch 29370, train_perplexity=1.0010097, train_loss=0.0010091616

Batch 29380, train_perplexity=1.0010092, train_loss=0.0010087567

Batch 29390, train_perplexity=1.0010089, train_loss=0.00100834

Batch 29400, train_perplexity=1.0010085, train_loss=0.0010079679

Batch 29410, train_perplexity=1.001008, train_loss=0.0010075392

Batch 29420, train_perplexity=1.0010077, train_loss=0.0010072088

Batch 29430, train_perplexity=1.0010073, train_loss=0.0010067921

Batch 29440, train_perplexity=1.0010068, train_loss=0.0010063603

Batch 29450, train_perplexity=1.0010065, train_loss=0.0010060002

Batch 29460, train_perplexity=1.0010061, train_loss=0.0010056458

Batch 29470, train_perplexity=1.0010058, train_loss=0.0010052678

Batch 29480, train_perplexity=1.0010053, train_loss=0.0010047498

Batch 29490, train_perplexity=1.0010049, train_loss=0.0010043867

Batch 29500, train_perplexity=1.0010045, train_loss=0.0010039669

Batch 29510, train_perplexity=1.0010042, train_loss=0.0010037108

Batch 29520, train_perplexity=1.0010037, train_loss=0.0010032584

Batch 29530, train_perplexity=1.0010034, train_loss=0.0010028298

Batch 29540, train_perplexity=1.0010029, train_loss=0.0010024516

Batch 29550, train_perplexity=1.0010026, train_loss=0.001002026

Batch 29560, train_perplexity=1.0010022, train_loss=0.0010016926

Batch 29570, train_perplexity=1.0010018, train_loss=0.0010013056

Batch 29580, train_perplexity=1.0010014, train_loss=0.0010008798

Batch 29590, train_perplexity=1.001001, train_loss=0.001000481

Batch 29600, train_perplexity=1.0010005, train_loss=0.0010000791

Batch 29610, train_perplexity=1.0010003, train_loss=0.0009997278

Batch 29620, train_perplexity=1.0009998, train_loss=0.0009993616

Batch 29630, train_perplexity=1.0009995, train_loss=0.000998921

Batch 29640, train_perplexity=1.0009991, train_loss=0.000998537

Batch 29650, train_perplexity=1.0009986, train_loss=0.0009981737

Batch 29660, train_perplexity=1.0009983, train_loss=0.0009977957

Batch 29670, train_perplexity=1.0009978, train_loss=0.0009973195

Batch 29680, train_perplexity=1.0009975, train_loss=0.0009969949

Batch 29690, train_perplexity=1.0009971, train_loss=0.000996593

Batch 29700, train_perplexity=1.0009967, train_loss=0.000996212

Batch 29710, train_perplexity=1.0009964, train_loss=0.0009958191
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 29720, train_perplexity=1.000996, train_loss=0.0009954676

Batch 29730, train_perplexity=1.0009955, train_loss=0.0009950212

Batch 29740, train_perplexity=1.000995, train_loss=0.0009946015

Batch 29750, train_perplexity=1.0009948, train_loss=0.0009942859

Batch 29760, train_perplexity=1.0009943, train_loss=0.0009938751

Batch 29770, train_perplexity=1.000994, train_loss=0.0009934613

Batch 29780, train_perplexity=1.0009936, train_loss=0.0009930921

Batch 29790, train_perplexity=1.0009931, train_loss=0.0009926932

Batch 29800, train_perplexity=1.0009929, train_loss=0.0009924104

Batch 29810, train_perplexity=1.0009924, train_loss=0.0009918895

Batch 29820, train_perplexity=1.0009921, train_loss=0.0009915532

Batch 29830, train_perplexity=1.0009916, train_loss=0.0009911305

Batch 29840, train_perplexity=1.0009913, train_loss=0.0009907999

Batch 29850, train_perplexity=1.0009909, train_loss=0.0009903621

Batch 29860, train_perplexity=1.0009905, train_loss=0.0009900171

Batch 29870, train_perplexity=1.00099, train_loss=0.0009896002

Batch 29880, train_perplexity=1.0009897, train_loss=0.0009892341

Batch 29890, train_perplexity=1.0009893, train_loss=0.0009888529

Batch 29900, train_perplexity=1.000989, train_loss=0.0009884362

Batch 29910, train_perplexity=1.0009886, train_loss=0.0009880671

Batch 29920, train_perplexity=1.0009882, train_loss=0.0009877575

Batch 29930, train_perplexity=1.0009878, train_loss=0.0009873139

Batch 29940, train_perplexity=1.0009874, train_loss=0.0009869805

Batch 29950, train_perplexity=1.000987, train_loss=0.0009866173

Batch 29960, train_perplexity=1.0009867, train_loss=0.0009861499

Batch 29970, train_perplexity=1.0009863, train_loss=0.0009858283

Batch 29980, train_perplexity=1.0009859, train_loss=0.0009853938

Batch 29990, train_perplexity=1.0009855, train_loss=0.0009850663

Batch 30000, train_perplexity=1.0009851, train_loss=0.0009846494

Batch 30010, train_perplexity=1.0009848, train_loss=0.0009842834

Batch 30020, train_perplexity=1.0009844, train_loss=0.0009839202

Batch 30030, train_perplexity=1.0009841, train_loss=0.0009835421

Batch 30040, train_perplexity=1.0009836, train_loss=0.0009831253

Batch 30050, train_perplexity=1.0009832, train_loss=0.000982774

Batch 30060, train_perplexity=1.0009829, train_loss=0.0009824109

Batch 30070, train_perplexity=1.0009824, train_loss=0.0009819464

Batch 30080, train_perplexity=1.0009822, train_loss=0.0009816517

Batch 30090, train_perplexity=1.0009817, train_loss=0.00098122

Batch 30100, train_perplexity=1.0009813, train_loss=0.0009808419

Batch 30110, train_perplexity=1.000981, train_loss=0.0009804936

Batch 30120, train_perplexity=1.0009806, train_loss=0.0009801126

Batch 30130, train_perplexity=1.0009803, train_loss=0.0009797465

Batch 30140, train_perplexity=1.0009799, train_loss=0.0009794219

Batch 30150, train_perplexity=1.0009794, train_loss=0.0009789933

Batch 30160, train_perplexity=1.0009791, train_loss=0.0009785824

Batch 30170, train_perplexity=1.0009787, train_loss=0.0009782104

Batch 30180, train_perplexity=1.0009784, train_loss=0.0009778948

Batch 30190, train_perplexity=1.000978, train_loss=0.0009774779

Batch 30200, train_perplexity=1.0009776, train_loss=0.0009771057

Batch 30210, train_perplexity=1.0009772, train_loss=0.0009767187

Batch 30220, train_perplexity=1.0009768, train_loss=0.0009763675

Batch 30230, train_perplexity=1.0009766, train_loss=0.0009760609

Batch 30240, train_perplexity=1.0009761, train_loss=0.0009756441

Batch 30250, train_perplexity=1.0009757, train_loss=0.0009752452

Batch 30260, train_perplexity=1.0009754, train_loss=0.000974876

Batch 30270, train_perplexity=1.000975, train_loss=0.0009745277

Batch 30280, train_perplexity=1.0009747, train_loss=0.0009741585

Batch 30290, train_perplexity=1.0009742, train_loss=0.0009737596

Batch 30300, train_perplexity=1.0009739, train_loss=0.0009734351

Batch 30310, train_perplexity=1.0009735, train_loss=0.00097304815

Batch 30320, train_perplexity=1.0009731, train_loss=0.00097262237

Batch 30330, train_perplexity=1.0009727, train_loss=0.0009722532

Batch 30340, train_perplexity=1.0009724, train_loss=0.0009719466

Batch 30350, train_perplexity=1.000972, train_loss=0.0009715418

Batch 30360, train_perplexity=1.0009716, train_loss=0.00097113976

Batch 30370, train_perplexity=1.0009713, train_loss=0.00097087194

Batch 30380, train_perplexity=1.000971, train_loss=0.00097052066

Batch 30390, train_perplexity=1.0009706, train_loss=0.00097009784

Batch 30400, train_perplexity=1.0009702, train_loss=0.0009697168

Batch 30410, train_perplexity=1.0009699, train_loss=0.0009693864

Batch 30420, train_perplexity=1.0009694, train_loss=0.0009689249

Batch 30430, train_perplexity=1.000969, train_loss=0.0009686332

Batch 30440, train_perplexity=1.0009687, train_loss=0.00096826704

Batch 30450, train_perplexity=1.0009683, train_loss=0.00096787116

Batch 30460, train_perplexity=1.000968, train_loss=0.0009674692

Batch 30470, train_perplexity=1.0009676, train_loss=0.0009671447

Batch 30480, train_perplexity=1.0009673, train_loss=0.00096677546

Batch 30490, train_perplexity=1.0009668, train_loss=0.000966317

Batch 30500, train_perplexity=1.0009664, train_loss=0.00096601044

Batch 30510, train_perplexity=1.0009661, train_loss=0.0009656085

Batch 30520, train_perplexity=1.0009657, train_loss=0.0009652543

Batch 30530, train_perplexity=1.0009654, train_loss=0.00096487906

Batch 30540, train_perplexity=1.000965, train_loss=0.00096449204

Batch 30550, train_perplexity=1.0009648, train_loss=0.0009642778

Batch 30560, train_perplexity=1.0009642, train_loss=0.00096371805

Batch 30570, train_perplexity=1.0009639, train_loss=0.0009634085

Batch 30580, train_perplexity=1.0009636, train_loss=0.00096309296

Batch 30590, train_perplexity=1.0009632, train_loss=0.0009627803

Batch 30600, train_perplexity=1.0009629, train_loss=0.00096240523

Batch 30610, train_perplexity=1.0009625, train_loss=0.0009620123

Batch 30620, train_perplexity=1.0009621, train_loss=0.0009616282

Batch 30630, train_perplexity=1.0009618, train_loss=0.000961265

Batch 30640, train_perplexity=1.0009613, train_loss=0.00096087495

Batch 30650, train_perplexity=1.000961, train_loss=0.00096053263

Batch 30660, train_perplexity=1.0009606, train_loss=0.00096018135

Batch 30670, train_perplexity=1.0009604, train_loss=0.0009598449

Batch 30680, train_perplexity=1.0009599, train_loss=0.00095945504

Batch 30690, train_perplexity=1.0009596, train_loss=0.0009591543

Batch 30700, train_perplexity=1.0009592, train_loss=0.0009587404

Batch 30710, train_perplexity=1.0009588, train_loss=0.0009583236

Batch 30720, train_perplexity=1.0009584, train_loss=0.0009580229

Batch 30730, train_perplexity=1.0009581, train_loss=0.00095761515

Batch 30740, train_perplexity=1.0009577, train_loss=0.0009572966

Batch 30750, train_perplexity=1.0009574, train_loss=0.00095687684

Batch 30760, train_perplexity=1.0009571, train_loss=0.00095668336

Batch 30770, train_perplexity=1.0009567, train_loss=0.0009562308

Batch 30780, train_perplexity=1.0009563, train_loss=0.00095589436

Batch 30790, train_perplexity=1.0009559, train_loss=0.00095551333

Batch 30800, train_perplexity=1.0009557, train_loss=0.0009552126

Batch 30810, train_perplexity=1.0009552, train_loss=0.0009547929

Batch 30820, train_perplexity=1.0009549, train_loss=0.0009544296

Batch 30830, train_perplexity=1.0009545, train_loss=0.0009540664

Batch 30840, train_perplexity=1.0009542, train_loss=0.00095368235

Batch 30850, train_perplexity=1.0009538, train_loss=0.00095329236

Batch 30860, train_perplexity=1.0009536, train_loss=0.0009530424

Batch 30870, train_perplexity=1.0009531, train_loss=0.0009525957

Batch 30880, train_perplexity=1.0009526, train_loss=0.000952182

Batch 30890, train_perplexity=1.0009524, train_loss=0.00095195865

Batch 30900, train_perplexity=1.0009521, train_loss=0.00095166697

Batch 30910, train_perplexity=1.0009518, train_loss=0.00095128285

Batch 30920, train_perplexity=1.0009513, train_loss=0.0009508572

Batch 30930, train_perplexity=1.0009509, train_loss=0.00095049385

Batch 30940, train_perplexity=1.0009506, train_loss=0.00095018133

Batch 30950, train_perplexity=1.0009503, train_loss=0.00094988965
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 30960, train_perplexity=1.00095, train_loss=0.00094949664

Batch 30970, train_perplexity=1.0009495, train_loss=0.0009490262

Batch 30980, train_perplexity=1.0009493, train_loss=0.0009487851

Batch 30990, train_perplexity=1.0009488, train_loss=0.0009483802

Batch 31000, train_perplexity=1.0009485, train_loss=0.00094806764

Batch 31010, train_perplexity=1.0009482, train_loss=0.00094770145

Batch 31020, train_perplexity=1.0009478, train_loss=0.0009473383

Batch 31030, train_perplexity=1.0009474, train_loss=0.00094696606

Batch 31040, train_perplexity=1.0009471, train_loss=0.0009466356

Batch 31050, train_perplexity=1.0009468, train_loss=0.00094626937

Batch 31060, train_perplexity=1.0009463, train_loss=0.0009458913

Batch 31070, train_perplexity=1.000946, train_loss=0.00094559963

Batch 31080, train_perplexity=1.0009457, train_loss=0.00094518275

Batch 31090, train_perplexity=1.0009454, train_loss=0.0009449506

Batch 31100, train_perplexity=1.0009449, train_loss=0.0009444414

Batch 31110, train_perplexity=1.0009447, train_loss=0.0009442569

Batch 31120, train_perplexity=1.0009443, train_loss=0.00094379846

Batch 31130, train_perplexity=1.0009439, train_loss=0.000943471

Batch 31140, train_perplexity=1.0009435, train_loss=0.00094306906

Batch 31150, train_perplexity=1.0009433, train_loss=0.0009428398

Batch 31160, train_perplexity=1.0009428, train_loss=0.0009424408

Batch 31170, train_perplexity=1.0009426, train_loss=0.0009420925

Batch 31180, train_perplexity=1.0009421, train_loss=0.0009417174

Batch 31190, train_perplexity=1.0009419, train_loss=0.00094144954

Batch 31200, train_perplexity=1.0009415, train_loss=0.00094107434

Batch 31210, train_perplexity=1.0009412, train_loss=0.0009406963

Batch 31220, train_perplexity=1.0009408, train_loss=0.00094032113

Batch 31230, train_perplexity=1.0009404, train_loss=0.00094003836

Batch 31240, train_perplexity=1.0009401, train_loss=0.0009396334

Batch 31250, train_perplexity=1.0009397, train_loss=0.0009392762

Batch 31260, train_perplexity=1.0009395, train_loss=0.00093899923

Batch 31270, train_perplexity=1.000939, train_loss=0.00093860924

Batch 31280, train_perplexity=1.0009387, train_loss=0.000938255

Batch 31290, train_perplexity=1.0009384, train_loss=0.0009379514

Batch 31300, train_perplexity=1.0009379, train_loss=0.00093754346

Batch 31310, train_perplexity=1.0009377, train_loss=0.00093723985

Batch 31320, train_perplexity=1.0009373, train_loss=0.0009368707

Batch 31330, train_perplexity=1.000937, train_loss=0.0009365313

Batch 31340, train_perplexity=1.0009366, train_loss=0.0009361651

Batch 31350, train_perplexity=1.0009362, train_loss=0.0009357274

Batch 31360, train_perplexity=1.0009359, train_loss=0.0009354774

Batch 31370, train_perplexity=1.0009356, train_loss=0.000935138

Batch 31380, train_perplexity=1.0009353, train_loss=0.00093486405

Batch 31390, train_perplexity=1.000935, train_loss=0.0009344681

Batch 31400, train_perplexity=1.0009345, train_loss=0.00093409297

Batch 31410, train_perplexity=1.0009342, train_loss=0.00093382207

Batch 31420, train_perplexity=1.0009339, train_loss=0.00093348866

Batch 31430, train_perplexity=1.0009335, train_loss=0.00093305693

Batch 31440, train_perplexity=1.0009332, train_loss=0.00093268475

Batch 31450, train_perplexity=1.0009328, train_loss=0.00093241094

Batch 31460, train_perplexity=1.0009325, train_loss=0.0009320328

Batch 31470, train_perplexity=1.0009321, train_loss=0.00093169336

Batch 31480, train_perplexity=1.0009319, train_loss=0.0009313868

Batch 31490, train_perplexity=1.0009314, train_loss=0.00093098776

Batch 31500, train_perplexity=1.0009311, train_loss=0.0009306752

Batch 31510, train_perplexity=1.0009308, train_loss=0.0009303478

Batch 31520, train_perplexity=1.0009304, train_loss=0.0009300232

Batch 31530, train_perplexity=1.0009301, train_loss=0.00092963915

Batch 31540, train_perplexity=1.0009297, train_loss=0.00092931464

Batch 31550, train_perplexity=1.0009294, train_loss=0.00092897227

Batch 31560, train_perplexity=1.0009291, train_loss=0.0009286984

Batch 31570, train_perplexity=1.0009288, train_loss=0.00092834706

Batch 31580, train_perplexity=1.0009284, train_loss=0.0009279452

Batch 31590, train_perplexity=1.0009282, train_loss=0.0009276951

Batch 31600, train_perplexity=1.0009277, train_loss=0.0009272634

Batch 31610, train_perplexity=1.0009274, train_loss=0.00092698354

Batch 31620, train_perplexity=1.0009271, train_loss=0.00092665304

Batch 31630, train_perplexity=1.0009267, train_loss=0.0009262898

Batch 31640, train_perplexity=1.0009264, train_loss=0.0009259088

Batch 31650, train_perplexity=1.0009261, train_loss=0.0009256586

Batch 31660, train_perplexity=1.0009257, train_loss=0.00092522695

Batch 31670, train_perplexity=1.0009254, train_loss=0.00092493824

Batch 31680, train_perplexity=1.000925, train_loss=0.00092450355

Batch 31690, train_perplexity=1.0009247, train_loss=0.00092431006

Batch 31700, train_perplexity=1.0009243, train_loss=0.0009239438

Batch 31710, train_perplexity=1.000924, train_loss=0.0009235777

Batch 31720, train_perplexity=1.0009236, train_loss=0.0009231936

Batch 31730, train_perplexity=1.0009233, train_loss=0.0009229048

Batch 31740, train_perplexity=1.000923, train_loss=0.00092261605

Batch 31750, train_perplexity=1.0009227, train_loss=0.00092224684

Batch 31760, train_perplexity=1.0009223, train_loss=0.0009218688

Batch 31770, train_perplexity=1.000922, train_loss=0.0009215531

Batch 31780, train_perplexity=1.0009216, train_loss=0.0009212346

Batch 31790, train_perplexity=1.0009214, train_loss=0.00092095777

Batch 31800, train_perplexity=1.000921, train_loss=0.00092056475

Batch 31810, train_perplexity=1.0009207, train_loss=0.0009202253

Batch 31820, train_perplexity=1.0009203, train_loss=0.00091984717

Batch 31830, train_perplexity=1.00092, train_loss=0.0009195823

Batch 31840, train_perplexity=1.0009197, train_loss=0.00091924286

Batch 31850, train_perplexity=1.0009193, train_loss=0.00091889454

Batch 31860, train_perplexity=1.000919, train_loss=0.00091853435

Batch 31870, train_perplexity=1.0009186, train_loss=0.00091816514

Batch 31880, train_perplexity=1.0009183, train_loss=0.00091790606

Batch 31890, train_perplexity=1.000918, train_loss=0.00091756065

Batch 31900, train_perplexity=1.0009177, train_loss=0.0009172541

Batch 31910, train_perplexity=1.0009173, train_loss=0.00091692066

Batch 31920, train_perplexity=1.000917, train_loss=0.0009165872

Batch 31930, train_perplexity=1.0009167, train_loss=0.0009162657

Batch 31940, train_perplexity=1.0009164, train_loss=0.0009159619

Batch 31950, train_perplexity=1.000916, train_loss=0.0009155571

Batch 31960, train_perplexity=1.0009156, train_loss=0.0009152831

Batch 31970, train_perplexity=1.0009153, train_loss=0.0009148932

Batch 31980, train_perplexity=1.0009149, train_loss=0.00091455074

Batch 31990, train_perplexity=1.0009147, train_loss=0.0009142858

Batch 32000, train_perplexity=1.0009143, train_loss=0.00091391365

Batch 32010, train_perplexity=1.000914, train_loss=0.00091350876

Batch 32020, train_perplexity=1.0009136, train_loss=0.000913208

Batch 32030, train_perplexity=1.0009134, train_loss=0.00091295794

Batch 32040, train_perplexity=1.0009129, train_loss=0.00091246073

Batch 32050, train_perplexity=1.0009127, train_loss=0.00091219874

Batch 32060, train_perplexity=1.0009123, train_loss=0.00091194274

Batch 32070, train_perplexity=1.000912, train_loss=0.0009115944

Batch 32080, train_perplexity=1.0009117, train_loss=0.00091130263

Batch 32090, train_perplexity=1.0009114, train_loss=0.0009109721

Batch 32100, train_perplexity=1.000911, train_loss=0.00091055536

Batch 32110, train_perplexity=1.0009106, train_loss=0.0009102785

Batch 32120, train_perplexity=1.0009104, train_loss=0.00090996287

Batch 32130, train_perplexity=1.00091, train_loss=0.00090963533

Batch 32140, train_perplexity=1.0009097, train_loss=0.00090929004

Batch 32150, train_perplexity=1.0009094, train_loss=0.0009089953

Batch 32160, train_perplexity=1.0009091, train_loss=0.00090864697

Batch 32170, train_perplexity=1.0009089, train_loss=0.00090839685

Batch 32180, train_perplexity=1.0009084, train_loss=0.0009080039

Batch 32190, train_perplexity=1.0009081, train_loss=0.0009077181
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 32200, train_perplexity=1.0009078, train_loss=0.0009074174

Batch 32210, train_perplexity=1.0009075, train_loss=0.00090714946

Batch 32220, train_perplexity=1.0009071, train_loss=0.00090670877

Batch 32230, train_perplexity=1.0009067, train_loss=0.00090635155

Batch 32240, train_perplexity=1.0009063, train_loss=0.00090597343

Batch 32250, train_perplexity=1.0009061, train_loss=0.0009057382

Batch 32260, train_perplexity=1.0009058, train_loss=0.00090538396

Batch 32270, train_perplexity=1.0009055, train_loss=0.0009051011

Batch 32280, train_perplexity=1.0009052, train_loss=0.0009048034

Batch 32290, train_perplexity=1.0009049, train_loss=0.0009044491

Batch 32300, train_perplexity=1.0009046, train_loss=0.00090415735

Batch 32310, train_perplexity=1.0009042, train_loss=0.0009037941

Batch 32320, train_perplexity=1.000904, train_loss=0.00090351416

Batch 32330, train_perplexity=1.0009035, train_loss=0.00090311514

Batch 32340, train_perplexity=1.0009032, train_loss=0.0009028681

Batch 32350, train_perplexity=1.0009029, train_loss=0.0009025316

Batch 32360, train_perplexity=1.0009027, train_loss=0.00090219534

Batch 32370, train_perplexity=1.0009022, train_loss=0.0009018261

Batch 32380, train_perplexity=1.0009019, train_loss=0.0009015105

Batch 32390, train_perplexity=1.0009017, train_loss=0.00090125145

Batch 32400, train_perplexity=1.0009012, train_loss=0.00090085855

Batch 32410, train_perplexity=1.000901, train_loss=0.00090055785

Batch 32420, train_perplexity=1.0009007, train_loss=0.0009003107

Batch 32430, train_perplexity=1.0009004, train_loss=0.0008999236

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 32440, train_perplexity=1.0009, train_loss=0.0008996229

Batch 32450, train_perplexity=1.0008997, train_loss=0.00089930126

Batch 32460, train_perplexity=1.0008994, train_loss=0.00089904235

Batch 32470, train_perplexity=1.0008991, train_loss=0.00089863746

Batch 32480, train_perplexity=1.0008987, train_loss=0.0008983129

Batch 32490, train_perplexity=1.0008984, train_loss=0.0008980033

Batch 32500, train_perplexity=1.0008981, train_loss=0.0008976966

Batch 32510, train_perplexity=1.0008978, train_loss=0.00089738105

Batch 32520, train_perplexity=1.0008975, train_loss=0.0008970803

Batch 32530, train_perplexity=1.0008972, train_loss=0.0008968124

Batch 32540, train_perplexity=1.0008968, train_loss=0.0008964372

Batch 32550, train_perplexity=1.0008965, train_loss=0.00089609483

Batch 32560, train_perplexity=1.0008962, train_loss=0.0008957733

Batch 32570, train_perplexity=1.0008959, train_loss=0.00089547853

Batch 32580, train_perplexity=1.0008955, train_loss=0.00089511834

Batch 32590, train_perplexity=1.0008953, train_loss=0.0008948086

Batch 32600, train_perplexity=1.0008949, train_loss=0.00089448714

Batch 32610, train_perplexity=1.0008947, train_loss=0.0008942251

Batch 32620, train_perplexity=1.0008943, train_loss=0.0008938589

Batch 32630, train_perplexity=1.000894, train_loss=0.00089359086

Batch 32640, train_perplexity=1.0008936, train_loss=0.0008932337

Batch 32650, train_perplexity=1.0008934, train_loss=0.000892924

Batch 32660, train_perplexity=1.000893, train_loss=0.0008925727

Batch 32670, train_perplexity=1.0008926, train_loss=0.0008922928

Batch 32680, train_perplexity=1.0008924, train_loss=0.00089199515

Batch 32690, train_perplexity=1.000892, train_loss=0.0008916676

Batch 32700, train_perplexity=1.0008917, train_loss=0.0008912955

Batch 32710, train_perplexity=1.0008914, train_loss=0.0008910572

Batch 32720, train_perplexity=1.0008911, train_loss=0.00089073274

Batch 32730, train_perplexity=1.0008907, train_loss=0.0008903933

Batch 32740, train_perplexity=1.0008905, train_loss=0.00089005986

Batch 32750, train_perplexity=1.0008901, train_loss=0.00088975613

Batch 32760, train_perplexity=1.0008899, train_loss=0.0008894644

Batch 32770, train_perplexity=1.0008895, train_loss=0.0008891161

Batch 32780, train_perplexity=1.0008893, train_loss=0.00088885403

Batch 32790, train_perplexity=1.000889, train_loss=0.000888607

Batch 32800, train_perplexity=1.0008887, train_loss=0.0008882973

Batch 32810, train_perplexity=1.0008883, train_loss=0.0008879282

Batch 32820, train_perplexity=1.000888, train_loss=0.0008875619

Batch 32830, train_perplexity=1.0008876, train_loss=0.00088726415

Batch 32840, train_perplexity=1.0008874, train_loss=0.0008870022

Batch 32850, train_perplexity=1.000887, train_loss=0.00088664796

Batch 32860, train_perplexity=1.0008868, train_loss=0.00088637695

Batch 32870, train_perplexity=1.0008864, train_loss=0.0008859929

Batch 32880, train_perplexity=1.0008862, train_loss=0.0008857696

Batch 32890, train_perplexity=1.0008858, train_loss=0.00088542415

Batch 32900, train_perplexity=1.0008855, train_loss=0.00088514434

Batch 32910, train_perplexity=1.0008852, train_loss=0.000884805

Batch 32920, train_perplexity=1.0008849, train_loss=0.0008845578

Batch 32930, train_perplexity=1.0008845, train_loss=0.0008841886

Batch 32940, train_perplexity=1.0008843, train_loss=0.00088387006

Batch 32950, train_perplexity=1.0008839, train_loss=0.00088353956

Batch 32960, train_perplexity=1.0008836, train_loss=0.00088320905

Batch 32970, train_perplexity=1.0008833, train_loss=0.0008829054

Batch 32980, train_perplexity=1.0008831, train_loss=0.0008826583

Batch 32990, train_perplexity=1.0008827, train_loss=0.00088233076

Batch 33000, train_perplexity=1.0008824, train_loss=0.0008820181

Batch 33010, train_perplexity=1.0008821, train_loss=0.00088176806

Batch 33020, train_perplexity=1.0008818, train_loss=0.0008814346

Batch 33030, train_perplexity=1.0008814, train_loss=0.0008810981

Batch 33040, train_perplexity=1.0008812, train_loss=0.0008807647

Batch 33050, train_perplexity=1.000881, train_loss=0.0008805741

Batch 33060, train_perplexity=1.0008806, train_loss=0.0008801663

Batch 33070, train_perplexity=1.0008802, train_loss=0.00087980897

Batch 33080, train_perplexity=1.00088, train_loss=0.00087963336

Batch 33090, train_perplexity=1.0008796, train_loss=0.0008792522

Batch 33100, train_perplexity=1.0008793, train_loss=0.0008789545

Batch 33110, train_perplexity=1.000879, train_loss=0.0008786449

Batch 33120, train_perplexity=1.0008788, train_loss=0.00087838876

Batch 33130, train_perplexity=1.0008785, train_loss=0.0008780523

Batch 33140, train_perplexity=1.0008781, train_loss=0.00087766827

Batch 33150, train_perplexity=1.0008779, train_loss=0.0008774182

Batch 33160, train_perplexity=1.0008776, train_loss=0.00087721576

Batch 33170, train_perplexity=1.0008773, train_loss=0.00087685254

Batch 33180, train_perplexity=1.0008769, train_loss=0.000876528

Batch 33190, train_perplexity=1.0008767, train_loss=0.0008762629

Batch 33200, train_perplexity=1.0008763, train_loss=0.00087596825

Batch 33210, train_perplexity=1.0008761, train_loss=0.00087563763

Batch 33220, train_perplexity=1.0008757, train_loss=0.0008752864

Batch 33230, train_perplexity=1.0008754, train_loss=0.0008749946

Batch 33240, train_perplexity=1.0008751, train_loss=0.00087470585

Batch 33250, train_perplexity=1.0008748, train_loss=0.0008743276

Batch 33260, train_perplexity=1.0008745, train_loss=0.0008741104

Batch 33270, train_perplexity=1.0008742, train_loss=0.00087383937

Batch 33280, train_perplexity=1.0008739, train_loss=0.00087348215

Batch 33290, train_perplexity=1.0008736, train_loss=0.0008731873

Batch 33300, train_perplexity=1.0008733, train_loss=0.0008729313

Batch 33310, train_perplexity=1.000873, train_loss=0.0008726277

Batch 33320, train_perplexity=1.0008727, train_loss=0.00087235076

Batch 33330, train_perplexity=1.0008724, train_loss=0.0008720262

Batch 33340, train_perplexity=1.000872, train_loss=0.0008716868

Batch 33350, train_perplexity=1.0008717, train_loss=0.0008712937

Batch 33360, train_perplexity=1.0008715, train_loss=0.0008711687

Batch 33370, train_perplexity=1.0008712, train_loss=0.0008707906
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33380, train_perplexity=1.000871, train_loss=0.0008705078

Batch 33390, train_perplexity=1.0008707, train_loss=0.0008702696

Batch 33400, train_perplexity=1.0008703, train_loss=0.00086991524

Batch 33410, train_perplexity=1.0008701, train_loss=0.00086969195

Batch 33420, train_perplexity=1.0008696, train_loss=0.0008692781

Batch 33430, train_perplexity=1.0008693, train_loss=0.0008689268

Batch 33440, train_perplexity=1.0008692, train_loss=0.0008687272

Batch 33450, train_perplexity=1.0008688, train_loss=0.0008683968

Batch 33460, train_perplexity=1.0008684, train_loss=0.0008680604

Batch 33470, train_perplexity=1.0008682, train_loss=0.00086784596

Batch 33480, train_perplexity=1.0008678, train_loss=0.00086751254

Batch 33490, train_perplexity=1.0008676, train_loss=0.0008672386

Batch 33500, train_perplexity=1.0008674, train_loss=0.0008669468

Batch 33510, train_perplexity=1.000867, train_loss=0.00086665206

Batch 33520, train_perplexity=1.0008668, train_loss=0.0008663543

Batch 33530, train_perplexity=1.0008664, train_loss=0.0008660059

Batch 33540, train_perplexity=1.000866, train_loss=0.0008656904

Batch 33550, train_perplexity=1.0008658, train_loss=0.00086542545

Batch 33560, train_perplexity=1.0008655, train_loss=0.0008651366

Batch 33570, train_perplexity=1.0008652, train_loss=0.00086488953

Batch 33580, train_perplexity=1.0008649, train_loss=0.0008644995

Batch 33590, train_perplexity=1.0008646, train_loss=0.0008642463

Batch 33600, train_perplexity=1.0008643, train_loss=0.0008638652

Batch 33610, train_perplexity=1.000864, train_loss=0.00086363894

Batch 33620, train_perplexity=1.0008638, train_loss=0.0008633978

Batch 33630, train_perplexity=1.0008634, train_loss=0.00086304056

Batch 33640, train_perplexity=1.0008631, train_loss=0.00086272194

Batch 33650, train_perplexity=1.0008628, train_loss=0.00086243014

Batch 33660, train_perplexity=1.0008625, train_loss=0.00086212344

Batch 33670, train_perplexity=1.0008622, train_loss=0.0008618941

Batch 33680, train_perplexity=1.000862, train_loss=0.0008615816

Batch 33690, train_perplexity=1.0008616, train_loss=0.0008612898

Batch 33700, train_perplexity=1.0008613, train_loss=0.0008609503

Batch 33710, train_perplexity=1.000861, train_loss=0.00086068537

Batch 33720, train_perplexity=1.0008608, train_loss=0.00086040545

Batch 33730, train_perplexity=1.0008603, train_loss=0.00086000055

Batch 33740, train_perplexity=1.0008601, train_loss=0.00085975043

Batch 33750, train_perplexity=1.0008599, train_loss=0.00085951225

Batch 33760, train_perplexity=1.0008596, train_loss=0.0008592681

Batch 33770, train_perplexity=1.0008593, train_loss=0.00085888105

Batch 33780, train_perplexity=1.0008589, train_loss=0.0008585922

Batch 33790, train_perplexity=1.0008587, train_loss=0.0008583124

Batch 33800, train_perplexity=1.0008584, train_loss=0.00085805333

Batch 33810, train_perplexity=1.0008582, train_loss=0.0008577914

Batch 33820, train_perplexity=1.0008578, train_loss=0.0008574936

Batch 33830, train_perplexity=1.0008576, train_loss=0.00085721374

Batch 33840, train_perplexity=1.0008572, train_loss=0.0008569129

Batch 33850, train_perplexity=1.000857, train_loss=0.0008566213

Batch 33860, train_perplexity=1.0008566, train_loss=0.0008562937

Batch 33870, train_perplexity=1.0008563, train_loss=0.00085592153

Batch 33880, train_perplexity=1.0008562, train_loss=0.00085576077

Batch 33890, train_perplexity=1.0008558, train_loss=0.0008553796

Batch 33900, train_perplexity=1.0008556, train_loss=0.00085514446

Batch 33910, train_perplexity=1.0008552, train_loss=0.00085484073

Batch 33920, train_perplexity=1.0008548, train_loss=0.0008545132

Batch 33930, train_perplexity=1.0008546, train_loss=0.000854284

Batch 33940, train_perplexity=1.0008543, train_loss=0.0008539267

Batch 33950, train_perplexity=1.000854, train_loss=0.0008536795

Batch 33960, train_perplexity=1.0008538, train_loss=0.00085339375

Batch 33970, train_perplexity=1.0008534, train_loss=0.000853096

Batch 33980, train_perplexity=1.0008532, train_loss=0.00085284293

Batch 33990, train_perplexity=1.000853, train_loss=0.00085253024

Batch 34000, train_perplexity=1.0008526, train_loss=0.0008522176

Batch 34010, train_perplexity=1.0008523, train_loss=0.0008519854

Batch 34020, train_perplexity=1.000852, train_loss=0.0008516727

Batch 34030, train_perplexity=1.0008518, train_loss=0.00085141975

Batch 34040, train_perplexity=1.0008515, train_loss=0.00085113087

Batch 34050, train_perplexity=1.0008512, train_loss=0.0008508301

Batch 34060, train_perplexity=1.0008509, train_loss=0.0008505115

Batch 34070, train_perplexity=1.0008506, train_loss=0.00085018703

Batch 34080, train_perplexity=1.0008503, train_loss=0.0008499369

Batch 34090, train_perplexity=1.00085, train_loss=0.00084960647

Batch 34100, train_perplexity=1.0008497, train_loss=0.0008494188

Batch 34110, train_perplexity=1.0008494, train_loss=0.0008490585

Batch 34120, train_perplexity=1.0008491, train_loss=0.0008487578

Batch 34130, train_perplexity=1.0008489, train_loss=0.0008485435

Batch 34140, train_perplexity=1.0008485, train_loss=0.0008481981

Batch 34150, train_perplexity=1.0008483, train_loss=0.00084798364

Batch 34160, train_perplexity=1.0008479, train_loss=0.00084760855

Batch 34170, train_perplexity=1.0008477, train_loss=0.000847296

Batch 34180, train_perplexity=1.0008475, train_loss=0.00084706955

Batch 34190, train_perplexity=1.0008472, train_loss=0.0008468165

Batch 34200, train_perplexity=1.0008469, train_loss=0.0008465069

Batch 34210, train_perplexity=1.0008465, train_loss=0.0008462032

Batch 34220, train_perplexity=1.0008463, train_loss=0.0008459382

Batch 34230, train_perplexity=1.000846, train_loss=0.00084569707

Batch 34240, train_perplexity=1.0008458, train_loss=0.0008453904

Batch 34250, train_perplexity=1.0008454, train_loss=0.0008451224

Batch 34260, train_perplexity=1.0008451, train_loss=0.0008447621

Batch 34270, train_perplexity=1.0008448, train_loss=0.00084452395

Batch 34280, train_perplexity=1.0008446, train_loss=0.00084420235

Batch 34290, train_perplexity=1.0008442, train_loss=0.00084393146

Batch 34300, train_perplexity=1.000844, train_loss=0.0008436664

Batch 34310, train_perplexity=1.0008438, train_loss=0.0008433865

Batch 34320, train_perplexity=1.0008435, train_loss=0.0008431156

Batch 34330, train_perplexity=1.0008432, train_loss=0.00084284466

Batch 34340, train_perplexity=1.0008429, train_loss=0.0008425439

Batch 34350, train_perplexity=1.0008426, train_loss=0.00084224326

Batch 34360, train_perplexity=1.0008423, train_loss=0.00084202585

Batch 34370, train_perplexity=1.0008421, train_loss=0.0008417192

Batch 34380, train_perplexity=1.0008417, train_loss=0.000841347

Batch 34390, train_perplexity=1.0008415, train_loss=0.00084116834

Batch 34400, train_perplexity=1.0008411, train_loss=0.00084083783

Batch 34410, train_perplexity=1.0008408, train_loss=0.0008404627

Batch 34420, train_perplexity=1.0008407, train_loss=0.00084035844

Batch 34430, train_perplexity=1.0008403, train_loss=0.00084000116

Batch 34440, train_perplexity=1.0008401, train_loss=0.00083973916

Batch 34450, train_perplexity=1.0008398, train_loss=0.0008395248

Batch 34460, train_perplexity=1.0008395, train_loss=0.0008391675

Batch 34470, train_perplexity=1.0008392, train_loss=0.0008389263

Batch 34480, train_perplexity=1.000839, train_loss=0.0008386256

Batch 34490, train_perplexity=1.0008388, train_loss=0.00083835167

Batch 34500, train_perplexity=1.0008384, train_loss=0.00083808077

Batch 34510, train_perplexity=1.0008382, train_loss=0.00083785143

Batch 34520, train_perplexity=1.0008378, train_loss=0.0008374376

Batch 34530, train_perplexity=1.0008376, train_loss=0.0008372322

Batch 34540, train_perplexity=1.0008373, train_loss=0.00083694933

Batch 34550, train_perplexity=1.000837, train_loss=0.00083659496

Batch 34560, train_perplexity=1.0008367, train_loss=0.00083641626

Batch 34570, train_perplexity=1.0008365, train_loss=0.00083616027

Batch 34580, train_perplexity=1.0008363, train_loss=0.00083584763

Batch 34590, train_perplexity=1.0008359, train_loss=0.00083552906

Batch 34600, train_perplexity=1.0008357, train_loss=0.0008352521

Batch 34610, train_perplexity=1.0008354, train_loss=0.0008350169
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34620, train_perplexity=1.0008351, train_loss=0.00083476084

Batch 34630, train_perplexity=1.0008347, train_loss=0.00083438563

Batch 34640, train_perplexity=1.0008345, train_loss=0.00083413854

Batch 34650, train_perplexity=1.0008342, train_loss=0.0008338676

Batch 34660, train_perplexity=1.000834, train_loss=0.0008335907

Batch 34670, train_perplexity=1.0008337, train_loss=0.0008333436

Batch 34680, train_perplexity=1.0008334, train_loss=0.0008330995

Batch 34690, train_perplexity=1.0008332, train_loss=0.00083280157

Batch 34700, train_perplexity=1.0008329, train_loss=0.00083253667

Batch 34710, train_perplexity=1.0008326, train_loss=0.0008322032

Batch 34720, train_perplexity=1.0008323, train_loss=0.000831965

Batch 34730, train_perplexity=1.000832, train_loss=0.00083165825

Batch 34740, train_perplexity=1.0008317, train_loss=0.00083138735

Batch 34750, train_perplexity=1.0008315, train_loss=0.0008311431

Batch 34760, train_perplexity=1.0008311, train_loss=0.0008307918

Batch 34770, train_perplexity=1.0008309, train_loss=0.0008305744

Batch 34780, train_perplexity=1.0008307, train_loss=0.0008303095

Batch 34790, train_perplexity=1.0008304, train_loss=0.00083002064

Batch 34800, train_perplexity=1.00083, train_loss=0.00082974974

Batch 34810, train_perplexity=1.0008299, train_loss=0.0008295502

Batch 34820, train_perplexity=1.0008296, train_loss=0.0008292822

Batch 34830, train_perplexity=1.0008292, train_loss=0.0008289011

Batch 34840, train_perplexity=1.000829, train_loss=0.0008286837

Batch 34850, train_perplexity=1.0008287, train_loss=0.00082844857

Batch 34860, train_perplexity=1.0008285, train_loss=0.00082815974

Batch 34870, train_perplexity=1.0008281, train_loss=0.0008277816

Batch 34880, train_perplexity=1.0008279, train_loss=0.00082756416

Batch 34890, train_perplexity=1.0008277, train_loss=0.0008273319

Batch 34900, train_perplexity=1.0008274, train_loss=0.00082703127

Batch 34910, train_perplexity=1.0008271, train_loss=0.0008267067

Batch 34920, train_perplexity=1.0008267, train_loss=0.00082642084

Batch 34930, train_perplexity=1.0008265, train_loss=0.00082619453

Batch 34940, train_perplexity=1.0008262, train_loss=0.00082592363

Batch 34950, train_perplexity=1.0008259, train_loss=0.00082559313

Batch 34960, train_perplexity=1.0008256, train_loss=0.00082531024

Batch 34970, train_perplexity=1.0008254, train_loss=0.00082508987

Batch 34980, train_perplexity=1.0008252, train_loss=0.0008248071

Batch 34990, train_perplexity=1.0008249, train_loss=0.00082461943

Batch 35000, train_perplexity=1.0008247, train_loss=0.0008243039

Batch 35010, train_perplexity=1.0008243, train_loss=0.0008240269

Batch 35020, train_perplexity=1.0008241, train_loss=0.0008237411

Batch 35030, train_perplexity=1.0008239, train_loss=0.000823485

Batch 35040, train_perplexity=1.0008235, train_loss=0.00082320813

Batch 35050, train_perplexity=1.0008234, train_loss=0.0008230057

Batch 35060, train_perplexity=1.0008229, train_loss=0.0008225888

Batch 35070, train_perplexity=1.0008228, train_loss=0.0008224012

Batch 35080, train_perplexity=1.0008225, train_loss=0.00082214817

Batch 35090, train_perplexity=1.0008222, train_loss=0.00082186825

Batch 35100, train_perplexity=1.0008221, train_loss=0.0008216747

Batch 35110, train_perplexity=1.0008217, train_loss=0.0008213263

Batch 35120, train_perplexity=1.0008214, train_loss=0.00082102563

Batch 35130, train_perplexity=1.0008211, train_loss=0.0008207696

Batch 35140, train_perplexity=1.0008209, train_loss=0.00082051347

Batch 35150, train_perplexity=1.0008206, train_loss=0.0008202663

Batch 35160, train_perplexity=1.0008204, train_loss=0.0008200847

Batch 35170, train_perplexity=1.0008202, train_loss=0.0008197662

Batch 35180, train_perplexity=1.0008199, train_loss=0.00081953686

Batch 35190, train_perplexity=1.0008196, train_loss=0.0008192093

Batch 35200, train_perplexity=1.0008192, train_loss=0.0008188669

Batch 35210, train_perplexity=1.000819, train_loss=0.0008186555

Batch 35220, train_perplexity=1.0008186, train_loss=0.00081831904

Batch 35230, train_perplexity=1.0008185, train_loss=0.0008181493

Batch 35240, train_perplexity=1.0008183, train_loss=0.0008178784

Batch 35250, train_perplexity=1.0008179, train_loss=0.00081761635

Batch 35260, train_perplexity=1.0008177, train_loss=0.0008173365

Batch 35270, train_perplexity=1.0008174, train_loss=0.000817125

Batch 35280, train_perplexity=1.0008171, train_loss=0.0008167707

Batch 35290, train_perplexity=1.000817, train_loss=0.00081656233

Batch 35300, train_perplexity=1.0008167, train_loss=0.00081635086

Batch 35310, train_perplexity=1.0008163, train_loss=0.0008160322

Batch 35320, train_perplexity=1.0008161, train_loss=0.00081578514

Batch 35330, train_perplexity=1.0008159, train_loss=0.00081552006

Batch 35340, train_perplexity=1.0008155, train_loss=0.0008151777

Batch 35350, train_perplexity=1.0008153, train_loss=0.0008149723

Batch 35360, train_perplexity=1.000815, train_loss=0.0008147162

Batch 35370, train_perplexity=1.0008147, train_loss=0.00081439165

Batch 35380, train_perplexity=1.0008144, train_loss=0.0008141594

Batch 35390, train_perplexity=1.0008142, train_loss=0.00081386766

Batch 35400, train_perplexity=1.0008138, train_loss=0.00081354304

Batch 35410, train_perplexity=1.0008136, train_loss=0.0008133317

Batch 35420, train_perplexity=1.0008135, train_loss=0.0008131559

Batch 35430, train_perplexity=1.0008132, train_loss=0.00081289094

Batch 35440, train_perplexity=1.0008129, train_loss=0.0008125783

Batch 35450, train_perplexity=1.0008126, train_loss=0.0008122657

Batch 35460, train_perplexity=1.0008124, train_loss=0.00081209897

Batch 35470, train_perplexity=1.000812, train_loss=0.00081175653

Batch 35480, train_perplexity=1.0008118, train_loss=0.0008115004

Batch 35490, train_perplexity=1.0008116, train_loss=0.00081124133

Batch 35500, train_perplexity=1.0008113, train_loss=0.0008110181

Batch 35510, train_perplexity=1.000811, train_loss=0.00081068755

Batch 35520, train_perplexity=1.0008107, train_loss=0.0008104553

Batch 35530, train_perplexity=1.0008106, train_loss=0.00081024086

Batch 35540, train_perplexity=1.0008103, train_loss=0.00080994016

Batch 35550, train_perplexity=1.00081, train_loss=0.000809705

Batch 35560, train_perplexity=1.0008097, train_loss=0.0008093745

Batch 35570, train_perplexity=1.0008094, train_loss=0.00080910645

Batch 35580, train_perplexity=1.0008092, train_loss=0.00080885633

Batch 35590, train_perplexity=1.0008091, train_loss=0.0008087164

Batch 35600, train_perplexity=1.0008087, train_loss=0.000808374

Batch 35610, train_perplexity=1.0008084, train_loss=0.0008080703

Batch 35620, train_perplexity=1.0008081, train_loss=0.00080782315

Batch 35630, train_perplexity=1.0008079, train_loss=0.0008075759

Batch 35640, train_perplexity=1.0008076, train_loss=0.0008073318

Batch 35650, train_perplexity=1.0008074, train_loss=0.0008070489

Batch 35660, train_perplexity=1.0008072, train_loss=0.00080684654

Batch 35670, train_perplexity=1.0008069, train_loss=0.0008065696

Batch 35680, train_perplexity=1.0008066, train_loss=0.0008062599

Batch 35690, train_perplexity=1.0008063, train_loss=0.0008059949

Batch 35700, train_perplexity=1.0008061, train_loss=0.0008057209

Batch 35710, train_perplexity=1.0008057, train_loss=0.000805447

Batch 35720, train_perplexity=1.0008056, train_loss=0.00080530706

Batch 35730, train_perplexity=1.0008054, train_loss=0.00080509565

Batch 35740, train_perplexity=1.0008051, train_loss=0.00080476224

Batch 35750, train_perplexity=1.0008048, train_loss=0.0008044942

Batch 35760, train_perplexity=1.0008045, train_loss=0.0008042739

Batch 35770, train_perplexity=1.0008043, train_loss=0.00080400886

Batch 35780, train_perplexity=1.0008041, train_loss=0.00080373493

Batch 35790, train_perplexity=1.0008038, train_loss=0.00080349075

Batch 35800, train_perplexity=1.0008036, train_loss=0.00080322876

Batch 35810, train_perplexity=1.0008032, train_loss=0.000802934

Batch 35820, train_perplexity=1.0008031, train_loss=0.0008027374

Batch 35830, train_perplexity=1.0008029, train_loss=0.00080251705

Batch 35840, train_perplexity=1.0008025, train_loss=0.0008021806

Batch 35850, train_perplexity=1.0008022, train_loss=0.0008018739
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 35860, train_perplexity=1.000802, train_loss=0.00080168043

Batch 35870, train_perplexity=1.0008017, train_loss=0.0008013885

Batch 35880, train_perplexity=1.0008014, train_loss=0.0008011444

Batch 35890, train_perplexity=1.0008013, train_loss=0.00080098666

Batch 35900, train_perplexity=1.0008011, train_loss=0.0008007127

Batch 35910, train_perplexity=1.0008006, train_loss=0.00080032856

Batch 35920, train_perplexity=1.0008004, train_loss=0.0008001023

Batch 35930, train_perplexity=1.0008001, train_loss=0.0007998611

Batch 35940, train_perplexity=1.0008, train_loss=0.00079967047

Batch 35950, train_perplexity=1.0007997, train_loss=0.0007993757

Batch 35960, train_perplexity=1.0007994, train_loss=0.0007991316

Batch 35970, train_perplexity=1.0007992, train_loss=0.00079890224

Batch 35980, train_perplexity=1.0007988, train_loss=0.000798536

Batch 35990, train_perplexity=1.0007987, train_loss=0.0007983306

Batch 36000, train_perplexity=1.0007983, train_loss=0.0007980537

Batch 36010, train_perplexity=1.0007981, train_loss=0.00079783035

Batch 36020, train_perplexity=1.0007979, train_loss=0.0007975415

Batch 36030, train_perplexity=1.0007977, train_loss=0.0007973956

Batch 36040, train_perplexity=1.0007974, train_loss=0.000797071

Batch 36050, train_perplexity=1.0007972, train_loss=0.00079689245

Batch 36060, train_perplexity=1.0007968, train_loss=0.00079648447

Batch 36070, train_perplexity=1.0007966, train_loss=0.0007962969

Batch 36080, train_perplexity=1.0007964, train_loss=0.0007960706

Batch 36090, train_perplexity=1.0007961, train_loss=0.00079580257

Batch 36100, train_perplexity=1.0007958, train_loss=0.00079547206

Batch 36110, train_perplexity=1.0007956, train_loss=0.0007952488

Batch 36120, train_perplexity=1.0007954, train_loss=0.0007950582

Batch 36130, train_perplexity=1.0007951, train_loss=0.00079480815

Batch 36140, train_perplexity=1.0007949, train_loss=0.0007945818

Batch 36150, train_perplexity=1.0007946, train_loss=0.0007942929

Batch 36160, train_perplexity=1.0007943, train_loss=0.00079401897

Batch 36170, train_perplexity=1.000794, train_loss=0.00079375994

Batch 36180, train_perplexity=1.0007938, train_loss=0.0007935307

Batch 36190, train_perplexity=1.0007936, train_loss=0.0007932597

Batch 36200, train_perplexity=1.0007932, train_loss=0.000792959

Batch 36210, train_perplexity=1.0007931, train_loss=0.0007927446

Batch 36220, train_perplexity=1.0007927, train_loss=0.00079247955

Batch 36230, train_perplexity=1.0007926, train_loss=0.000792286

Batch 36240, train_perplexity=1.0007924, train_loss=0.000792027

Batch 36250, train_perplexity=1.0007921, train_loss=0.0007918275

Batch 36260, train_perplexity=1.0007918, train_loss=0.00079148205

Batch 36270, train_perplexity=1.0007915, train_loss=0.00079127366

Batch 36280, train_perplexity=1.0007913, train_loss=0.0007909908

Batch 36290, train_perplexity=1.0007911, train_loss=0.00079077645

Batch 36300, train_perplexity=1.0007908, train_loss=0.0007904816

Batch 36310, train_perplexity=1.0007905, train_loss=0.0007902166

Batch 36320, train_perplexity=1.0007902, train_loss=0.0007899754

Batch 36330, train_perplexity=1.0007901, train_loss=0.000789776

Batch 36340, train_perplexity=1.0007898, train_loss=0.0007894841

Batch 36350, train_perplexity=1.0007895, train_loss=0.00078918634

Batch 36360, train_perplexity=1.0007893, train_loss=0.0007890315

Batch 36370, train_perplexity=1.000789, train_loss=0.00078875164

Batch 36380, train_perplexity=1.0007888, train_loss=0.0007884836

Batch 36390, train_perplexity=1.0007887, train_loss=0.0007883407

Batch 36400, train_perplexity=1.0007883, train_loss=0.0007880549

Batch 36410, train_perplexity=1.0007881, train_loss=0.0007877452

Batch 36420, train_perplexity=1.0007879, train_loss=0.00078752486

Batch 36430, train_perplexity=1.0007876, train_loss=0.00078731345

Batch 36440, train_perplexity=1.0007874, train_loss=0.00078704837

Batch 36450, train_perplexity=1.0007871, train_loss=0.0007868548

Batch 36460, train_perplexity=1.0007869, train_loss=0.00078654813

Batch 36470, train_perplexity=1.0007867, train_loss=0.0007863635

Batch 36480, train_perplexity=1.0007863, train_loss=0.00078605383

Batch 36490, train_perplexity=1.0007861, train_loss=0.0007857918

Batch 36500, train_perplexity=1.0007858, train_loss=0.0007855119

Batch 36510, train_perplexity=1.0007857, train_loss=0.0007853601

Batch 36520, train_perplexity=1.0007854, train_loss=0.00078505033

Batch 36530, train_perplexity=1.0007851, train_loss=0.0007848301

Batch 36540, train_perplexity=1.0007849, train_loss=0.000784568

Batch 36550, train_perplexity=1.0007846, train_loss=0.0007843477

Batch 36560, train_perplexity=1.0007844, train_loss=0.00078409753

Batch 36570, train_perplexity=1.0007842, train_loss=0.0007838355

Batch 36580, train_perplexity=1.0007839, train_loss=0.0007836033

Batch 36590, train_perplexity=1.0007837, train_loss=0.0007833501

Batch 36600, train_perplexity=1.0007834, train_loss=0.0007831447

Batch 36610, train_perplexity=1.0007831, train_loss=0.0007827963

Batch 36620, train_perplexity=1.0007828, train_loss=0.00078257895

Batch 36630, train_perplexity=1.0007827, train_loss=0.0007824331

Batch 36640, train_perplexity=1.0007824, train_loss=0.0007820638

Batch 36650, train_perplexity=1.0007821, train_loss=0.00078184943

Batch 36660, train_perplexity=1.0007819, train_loss=0.0007815636

Batch 36670, train_perplexity=1.0007817, train_loss=0.0007813908

Batch 36680, train_perplexity=1.0007814, train_loss=0.00078112585

Batch 36690, train_perplexity=1.0007812, train_loss=0.00078089954

Batch 36700, train_perplexity=1.0007809, train_loss=0.0007806762

Batch 36710, train_perplexity=1.0007807, train_loss=0.00078036654

Batch 36720, train_perplexity=1.0007805, train_loss=0.00078019977

Batch 36730, train_perplexity=1.0007802, train_loss=0.00077991985

Batch 36740, train_perplexity=1.00078, train_loss=0.000779637

Batch 36750, train_perplexity=1.0007796, train_loss=0.00077938393

Batch 36760, train_perplexity=1.0007795, train_loss=0.0007791963

Batch 36770, train_perplexity=1.0007793, train_loss=0.0007789164

Batch 36780, train_perplexity=1.000779, train_loss=0.00077872886

Batch 36790, train_perplexity=1.0007788, train_loss=0.00077849056

Batch 36800, train_perplexity=1.0007786, train_loss=0.00077824946

Batch 36810, train_perplexity=1.0007783, train_loss=0.0007779725

Batch 36820, train_perplexity=1.000778, train_loss=0.00077769253

Batch 36830, train_perplexity=1.0007778, train_loss=0.0007775646

Batch 36840, train_perplexity=1.0007776, train_loss=0.00077727274

Batch 36850, train_perplexity=1.0007774, train_loss=0.0007770227

Batch 36860, train_perplexity=1.000777, train_loss=0.00077670993

Batch 36870, train_perplexity=1.0007769, train_loss=0.00077657297

Batch 36880, train_perplexity=1.0007765, train_loss=0.0007762216

Batch 36890, train_perplexity=1.0007763, train_loss=0.00077604

Batch 36900, train_perplexity=1.000776, train_loss=0.00077577797

Batch 36910, train_perplexity=1.0007758, train_loss=0.00077556947

Batch 36920, train_perplexity=1.0007756, train_loss=0.00077531935

Batch 36930, train_perplexity=1.0007755, train_loss=0.00077514077

Batch 36940, train_perplexity=1.0007751, train_loss=0.00077484886

Batch 36950, train_perplexity=1.000775, train_loss=0.000774703

Batch 36960, train_perplexity=1.0007746, train_loss=0.0007743427

Batch 36970, train_perplexity=1.0007744, train_loss=0.0007741314

Batch 36980, train_perplexity=1.0007741, train_loss=0.0007738157

Batch 36990, train_perplexity=1.0007739, train_loss=0.00077355956

Batch 37000, train_perplexity=1.0007738, train_loss=0.0007734315

Batch 37010, train_perplexity=1.0007734, train_loss=0.0007731397

Batch 37020, train_perplexity=1.0007733, train_loss=0.00077295804

Batch 37030, train_perplexity=1.000773, train_loss=0.0007726067

Batch 37040, train_perplexity=1.0007727, train_loss=0.00077246083

Batch 37050, train_perplexity=1.0007725, train_loss=0.0007721868

Batch 37060, train_perplexity=1.0007722, train_loss=0.0007719367

Batch 37070, train_perplexity=1.000772, train_loss=0.0007716984

Batch 37080, train_perplexity=1.0007718, train_loss=0.0007714901

Batch 37090, train_perplexity=1.0007715, train_loss=0.00077126676
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37100, train_perplexity=1.0007713, train_loss=0.00077098387

Batch 37110, train_perplexity=1.000771, train_loss=0.00077078136

Batch 37120, train_perplexity=1.0007708, train_loss=0.000770558

Batch 37130, train_perplexity=1.0007707, train_loss=0.00077037944

Batch 37140, train_perplexity=1.0007703, train_loss=0.000770028

Batch 37150, train_perplexity=1.0007701, train_loss=0.00076979573

Batch 37160, train_perplexity=1.0007699, train_loss=0.00076957245

Batch 37170, train_perplexity=1.0007696, train_loss=0.00076933124

Batch 37180, train_perplexity=1.0007695, train_loss=0.0007692122

Batch 37190, train_perplexity=1.0007691, train_loss=0.00076884584

Batch 37200, train_perplexity=1.0007689, train_loss=0.00076860166

Batch 37210, train_perplexity=1.0007687, train_loss=0.0007683784

Batch 37220, train_perplexity=1.0007684, train_loss=0.00076816994

Batch 37230, train_perplexity=1.0007682, train_loss=0.00076787814

Batch 37240, train_perplexity=1.0007681, train_loss=0.0007677203

Batch 37250, train_perplexity=1.0007677, train_loss=0.0007674583

Batch 37260, train_perplexity=1.0007676, train_loss=0.0007673034

Batch 37270, train_perplexity=1.0007674, train_loss=0.00076699676

Batch 37280, train_perplexity=1.0007671, train_loss=0.0007667972

Batch 37290, train_perplexity=1.0007668, train_loss=0.00076649047

Batch 37300, train_perplexity=1.0007666, train_loss=0.0007663446

Batch 37310, train_perplexity=1.0007663, train_loss=0.0007660319

Batch 37320, train_perplexity=1.0007662, train_loss=0.0007658503

Batch 37330, train_perplexity=1.0007659, train_loss=0.00076557347

Batch 37340, train_perplexity=1.0007657, train_loss=0.0007654126

Batch 37350, train_perplexity=1.0007653, train_loss=0.0007650136

Batch 37360, train_perplexity=1.0007652, train_loss=0.0007649242

Batch 37370, train_perplexity=1.000765, train_loss=0.00076467113

Batch 37380, train_perplexity=1.0007647, train_loss=0.00076441205

Batch 37390, train_perplexity=1.0007645, train_loss=0.0007642036

Batch 37400, train_perplexity=1.0007644, train_loss=0.000764025

Batch 37410, train_perplexity=1.0007641, train_loss=0.00076379866

Batch 37420, train_perplexity=1.0007638, train_loss=0.0007634861

Batch 37430, train_perplexity=1.0007635, train_loss=0.00076320604

Batch 37440, train_perplexity=1.0007633, train_loss=0.00076300965

Batch 37450, train_perplexity=1.000763, train_loss=0.0007627356

Batch 37460, train_perplexity=1.0007628, train_loss=0.0007625659

Batch 37470, train_perplexity=1.0007626, train_loss=0.0007623366

Batch 37480, train_perplexity=1.0007623, train_loss=0.00076208054

Batch 37490, train_perplexity=1.0007621, train_loss=0.0007618721

Batch 37500, train_perplexity=1.000762, train_loss=0.0007616518

Batch 37510, train_perplexity=1.0007617, train_loss=0.0007614016

Batch 37520, train_perplexity=1.0007614, train_loss=0.00076114846

Batch 37530, train_perplexity=1.0007613, train_loss=0.0007609966

Batch 37540, train_perplexity=1.000761, train_loss=0.0007606988

Batch 37550, train_perplexity=1.0007608, train_loss=0.0007605291

Batch 37560, train_perplexity=1.0007604, train_loss=0.0007602016

Batch 37570, train_perplexity=1.0007603, train_loss=0.0007600289

Batch 37580, train_perplexity=1.0007601, train_loss=0.0007597549

Batch 37590, train_perplexity=1.0007598, train_loss=0.00075958227

Batch 37600, train_perplexity=1.0007596, train_loss=0.0007593082

Batch 37610, train_perplexity=1.0007594, train_loss=0.0007590492

Batch 37620, train_perplexity=1.0007591, train_loss=0.00075887644

Batch 37630, train_perplexity=1.0007589, train_loss=0.0007586323

Batch 37640, train_perplexity=1.0007588, train_loss=0.0007584238

Batch 37650, train_perplexity=1.0007585, train_loss=0.00075826905

Batch 37660, train_perplexity=1.0007582, train_loss=0.0007579385

Batch 37670, train_perplexity=1.000758, train_loss=0.0007577181

Batch 37680, train_perplexity=1.0007578, train_loss=0.00075748295

Batch 37690, train_perplexity=1.0007576, train_loss=0.0007572923

Batch 37700, train_perplexity=1.0007572, train_loss=0.00075698266

Batch 37710, train_perplexity=1.0007571, train_loss=0.00075685757

Batch 37720, train_perplexity=1.0007569, train_loss=0.0007565479

Batch 37730, train_perplexity=1.0007566, train_loss=0.00075634243

Batch 37740, train_perplexity=1.0007564, train_loss=0.0007561132

Batch 37750, train_perplexity=1.0007561, train_loss=0.0007559107

Batch 37760, train_perplexity=1.0007559, train_loss=0.00075558905

Batch 37770, train_perplexity=1.0007558, train_loss=0.00075544906

Batch 37780, train_perplexity=1.0007554, train_loss=0.0007551573

Batch 37790, train_perplexity=1.0007552, train_loss=0.0007549637

Batch 37800, train_perplexity=1.0007551, train_loss=0.0007547672

Batch 37810, train_perplexity=1.0007548, train_loss=0.00075449323

Batch 37820, train_perplexity=1.0007546, train_loss=0.0007542699

Batch 37830, train_perplexity=1.0007545, train_loss=0.00075414777

Batch 37840, train_perplexity=1.0007541, train_loss=0.0007538679

Batch 37850, train_perplexity=1.0007539, train_loss=0.000753585

Batch 37860, train_perplexity=1.0007536, train_loss=0.00075334683

Batch 37870, train_perplexity=1.0007535, train_loss=0.00075320987

Batch 37880, train_perplexity=1.0007532, train_loss=0.000752921

Batch 37890, train_perplexity=1.0007529, train_loss=0.00075268577

Batch 37900, train_perplexity=1.0007528, train_loss=0.0007524714

Batch 37910, train_perplexity=1.0007526, train_loss=0.00075228675

Batch 37920, train_perplexity=1.0007523, train_loss=0.0007519919

Batch 37930, train_perplexity=1.0007521, train_loss=0.0007517657

Batch 37940, train_perplexity=1.0007519, train_loss=0.00075157505

Batch 37950, train_perplexity=1.0007515, train_loss=0.00075124454

Batch 37960, train_perplexity=1.0007514, train_loss=0.0007511076

Batch 37970, train_perplexity=1.0007511, train_loss=0.00075090205

Batch 37980, train_perplexity=1.000751, train_loss=0.00075069955

Batch 37990, train_perplexity=1.0007507, train_loss=0.0007504256

Batch 38000, train_perplexity=1.0007504, train_loss=0.0007501249

Batch 38010, train_perplexity=1.0007502, train_loss=0.0007499283

Batch 38020, train_perplexity=1.0007501, train_loss=0.0007497348

Batch 38030, train_perplexity=1.0007498, train_loss=0.0007495025

Batch 38040, train_perplexity=1.0007496, train_loss=0.00074936554

Batch 38050, train_perplexity=1.0007493, train_loss=0.000749029

Batch 38060, train_perplexity=1.0007491, train_loss=0.00074884447

Batch 38070, train_perplexity=1.0007489, train_loss=0.0007485467

Batch 38080, train_perplexity=1.0007488, train_loss=0.0007484514

Batch 38090, train_perplexity=1.0007485, train_loss=0.00074818335

Batch 38100, train_perplexity=1.0007483, train_loss=0.00074803154

Batch 38110, train_perplexity=1.000748, train_loss=0.0007477129

Batch 38120, train_perplexity=1.0007478, train_loss=0.00074748066

Batch 38130, train_perplexity=1.0007476, train_loss=0.00074731384

Batch 38140, train_perplexity=1.0007473, train_loss=0.0007470667

Batch 38150, train_perplexity=1.0007471, train_loss=0.0007468017

Batch 38160, train_perplexity=1.0007468, train_loss=0.00074658135

Batch 38170, train_perplexity=1.0007467, train_loss=0.0007464086

Batch 38180, train_perplexity=1.0007464, train_loss=0.0007461406

Batch 38190, train_perplexity=1.0007463, train_loss=0.00074594107

Batch 38200, train_perplexity=1.000746, train_loss=0.0007457058

Batch 38210, train_perplexity=1.0007458, train_loss=0.0007454498

Batch 38220, train_perplexity=1.0007455, train_loss=0.000745286

Batch 38230, train_perplexity=1.0007453, train_loss=0.000745018

Batch 38240, train_perplexity=1.0007452, train_loss=0.00074484234

Batch 38250, train_perplexity=1.0007449, train_loss=0.0007446755

Batch 38260, train_perplexity=1.0007447, train_loss=0.0007443688

Batch 38270, train_perplexity=1.0007443, train_loss=0.00074412464

Batch 38280, train_perplexity=1.0007442, train_loss=0.00074393116

Batch 38290, train_perplexity=1.0007441, train_loss=0.00074380013

Batch 38300, train_perplexity=1.0007437, train_loss=0.00074352016

Batch 38310, train_perplexity=1.0007436, train_loss=0.0007433981

Batch 38320, train_perplexity=1.0007434, train_loss=0.0007430586

Batch 38330, train_perplexity=1.0007432, train_loss=0.0007428562
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38340, train_perplexity=1.0007429, train_loss=0.00074268645

Batch 38350, train_perplexity=1.0007427, train_loss=0.00074243627

Batch 38360, train_perplexity=1.0007424, train_loss=0.00074219215

Batch 38370, train_perplexity=1.0007422, train_loss=0.0007419658

Batch 38380, train_perplexity=1.0007421, train_loss=0.00074182585

Batch 38390, train_perplexity=1.0007418, train_loss=0.00074156676

Batch 38400, train_perplexity=1.0007416, train_loss=0.0007413672

Batch 38410, train_perplexity=1.0007414, train_loss=0.0007411201

Batch 38420, train_perplexity=1.0007411, train_loss=0.0007408372

Batch 38430, train_perplexity=1.0007409, train_loss=0.00074065255

Batch 38440, train_perplexity=1.0007408, train_loss=0.0007404382

Batch 38450, train_perplexity=1.0007405, train_loss=0.0007402267

Batch 38460, train_perplexity=1.0007402, train_loss=0.0007399409

Batch 38470, train_perplexity=1.00074, train_loss=0.00073978

Batch 38480, train_perplexity=1.0007398, train_loss=0.0007395657

Batch 38490, train_perplexity=1.0007396, train_loss=0.0007392828

Batch 38500, train_perplexity=1.0007393, train_loss=0.0007390833

Batch 38510, train_perplexity=1.0007391, train_loss=0.0007388629

Batch 38520, train_perplexity=1.000739, train_loss=0.0007386694

Batch 38530, train_perplexity=1.0007387, train_loss=0.00073843414

Batch 38540, train_perplexity=1.0007385, train_loss=0.0007382793

Batch 38550, train_perplexity=1.0007384, train_loss=0.0007380708

Batch 38560, train_perplexity=1.000738, train_loss=0.0007377671

Batch 38570, train_perplexity=1.0007378, train_loss=0.0007375438

Batch 38580, train_perplexity=1.0007375, train_loss=0.0007373085

Batch 38590, train_perplexity=1.0007374, train_loss=0.00073716557

Batch 38600, train_perplexity=1.0007373, train_loss=0.00073699286

Batch 38610, train_perplexity=1.000737, train_loss=0.00073668314

Batch 38620, train_perplexity=1.0007368, train_loss=0.0007365879

Batch 38630, train_perplexity=1.0007365, train_loss=0.0007362603

Batch 38640, train_perplexity=1.0007364, train_loss=0.00073605485

Batch 38650, train_perplexity=1.0007361, train_loss=0.00073588215

Batch 38660, train_perplexity=1.0007359, train_loss=0.00073565287

Batch 38670, train_perplexity=1.0007358, train_loss=0.0007354563

Batch 38680, train_perplexity=1.0007354, train_loss=0.0007351764

Batch 38690, train_perplexity=1.0007353, train_loss=0.00073502754

Batch 38700, train_perplexity=1.000735, train_loss=0.00073478033

Batch 38710, train_perplexity=1.0007349, train_loss=0.0007346255

Batch 38720, train_perplexity=1.0007347, train_loss=0.0007344141

Batch 38730, train_perplexity=1.0007344, train_loss=0.0007341878

Batch 38740, train_perplexity=1.0007342, train_loss=0.00073395553

Batch 38750, train_perplexity=1.0007339, train_loss=0.0007336339

Batch 38760, train_perplexity=1.0007339, train_loss=0.0007335624

Batch 38770, train_perplexity=1.0007335, train_loss=0.00073328253

Batch 38780, train_perplexity=1.0007333, train_loss=0.0007330234

Batch 38790, train_perplexity=1.0007331, train_loss=0.0007328298

Batch 38800, train_perplexity=1.0007329, train_loss=0.00073259464

Batch 38810, train_perplexity=1.0007327, train_loss=0.0007323564

Batch 38820, train_perplexity=1.0007324, train_loss=0.000732151

Batch 38830, train_perplexity=1.0007322, train_loss=0.00073195144

Batch 38840, train_perplexity=1.000732, train_loss=0.00073168933

Batch 38850, train_perplexity=1.000732, train_loss=0.0007316209

Batch 38860, train_perplexity=1.0007317, train_loss=0.0007314214

Batch 38870, train_perplexity=1.0007313, train_loss=0.00073111465

Batch 38880, train_perplexity=1.0007311, train_loss=0.00073085557

Batch 38890, train_perplexity=1.0007309, train_loss=0.00073063513

Batch 38900, train_perplexity=1.0007309, train_loss=0.00073056377

Batch 38910, train_perplexity=1.0007305, train_loss=0.0007302958

Batch 38920, train_perplexity=1.0007304, train_loss=0.00073010515

Batch 38930, train_perplexity=1.00073, train_loss=0.00072981336

Batch 38940, train_perplexity=1.0007298, train_loss=0.00072956027

Batch 38950, train_perplexity=1.0007297, train_loss=0.00072945

Batch 38960, train_perplexity=1.0007294, train_loss=0.0007291939

Batch 38970, train_perplexity=1.0007293, train_loss=0.00072903314

Batch 38980, train_perplexity=1.0007291, train_loss=0.000728777

Batch 38990, train_perplexity=1.0007288, train_loss=0.0007285597

Batch 39000, train_perplexity=1.0007286, train_loss=0.00072834233

Batch 39010, train_perplexity=1.0007284, train_loss=0.0007281041

Batch 39020, train_perplexity=1.0007281, train_loss=0.0007278777

Batch 39030, train_perplexity=1.0007279, train_loss=0.00072763354

Batch 39040, train_perplexity=1.0007278, train_loss=0.00072749064

Batch 39050, train_perplexity=1.0007275, train_loss=0.0007272494

Batch 39060, train_perplexity=1.0007273, train_loss=0.0007270409

Batch 39070, train_perplexity=1.000727, train_loss=0.0007268295

Batch 39080, train_perplexity=1.0007268, train_loss=0.0007265883

Batch 39090, train_perplexity=1.0007267, train_loss=0.0007264216

Batch 39100, train_perplexity=1.0007266, train_loss=0.00072626676

Batch 39110, train_perplexity=1.0007262, train_loss=0.0007259749

Batch 39120, train_perplexity=1.0007261, train_loss=0.000725823

Batch 39130, train_perplexity=1.0007259, train_loss=0.00072558183

Batch 39140, train_perplexity=1.0007256, train_loss=0.0007253734

Batch 39150, train_perplexity=1.0007254, train_loss=0.00072512624

Batch 39160, train_perplexity=1.0007252, train_loss=0.00072492077

Batch 39170, train_perplexity=1.000725, train_loss=0.00072476

Batch 39180, train_perplexity=1.0007248, train_loss=0.0007244949

Batch 39190, train_perplexity=1.0007246, train_loss=0.0007243074

Batch 39200, train_perplexity=1.0007243, train_loss=0.00072411075

Batch 39210, train_perplexity=1.0007242, train_loss=0.00072388747

Batch 39220, train_perplexity=1.000724, train_loss=0.0007237266

Batch 39230, train_perplexity=1.0007237, train_loss=0.00072344375

Batch 39240, train_perplexity=1.0007235, train_loss=0.000723277

Batch 39250, train_perplexity=1.0007234, train_loss=0.0007230507

Batch 39260, train_perplexity=1.0007231, train_loss=0.00072285114

Batch 39270, train_perplexity=1.0007229, train_loss=0.0007225801

Batch 39280, train_perplexity=1.0007226, train_loss=0.0007224372

Batch 39290, train_perplexity=1.0007225, train_loss=0.00072220794

Batch 39300, train_perplexity=1.0007223, train_loss=0.00072198757

Batch 39310, train_perplexity=1.000722, train_loss=0.00072179997

Batch 39320, train_perplexity=1.0007218, train_loss=0.0007215737

Batch 39330, train_perplexity=1.0007216, train_loss=0.0007212669

Batch 39340, train_perplexity=1.0007213, train_loss=0.0007210793

Batch 39350, train_perplexity=1.0007212, train_loss=0.0007209631

Batch 39360, train_perplexity=1.0007211, train_loss=0.0007207905

Batch 39370, train_perplexity=1.0007207, train_loss=0.00072053436

Batch 39380, train_perplexity=1.0007206, train_loss=0.0007203914

Batch 39390, train_perplexity=1.0007203, train_loss=0.00072005787

Batch 39400, train_perplexity=1.0007203, train_loss=0.0007199566

Batch 39410, train_perplexity=1.00072, train_loss=0.000719781

Batch 39420, train_perplexity=1.0007198, train_loss=0.0007195219

Batch 39430, train_perplexity=1.0007195, train_loss=0.000719233

Batch 39440, train_perplexity=1.0007193, train_loss=0.00071906333

Batch 39450, train_perplexity=1.0007192, train_loss=0.0007189412

Batch 39460, train_perplexity=1.000719, train_loss=0.0007186851

Batch 39470, train_perplexity=1.0007187, train_loss=0.00071848265

Batch 39480, train_perplexity=1.0007185, train_loss=0.0007182295

Batch 39490, train_perplexity=1.0007182, train_loss=0.00071802107

Batch 39500, train_perplexity=1.000718, train_loss=0.00071779476

Batch 39510, train_perplexity=1.0007178, train_loss=0.00071755354

Batch 39520, train_perplexity=1.0007176, train_loss=0.00071734213

Batch 39530, train_perplexity=1.0007175, train_loss=0.0007172349

Batch 39540, train_perplexity=1.0007173, train_loss=0.0007170116

Batch 39550, train_perplexity=1.000717, train_loss=0.0007167912

Batch 39560, train_perplexity=1.0007168, train_loss=0.0007166036

Batch 39570, train_perplexity=1.0007167, train_loss=0.0007164249
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 39580, train_perplexity=1.0007164, train_loss=0.0007161569

Batch 39590, train_perplexity=1.0007162, train_loss=0.0007159306

Batch 39600, train_perplexity=1.000716, train_loss=0.0007157519

Batch 39610, train_perplexity=1.0007157, train_loss=0.0007155315

Batch 39620, train_perplexity=1.0007156, train_loss=0.00071538566

Batch 39630, train_perplexity=1.0007154, train_loss=0.0007151504

Batch 39640, train_perplexity=1.0007153, train_loss=0.0007149717

Batch 39650, train_perplexity=1.000715, train_loss=0.0007147662

Batch 39660, train_perplexity=1.0007148, train_loss=0.0007145131

Batch 39670, train_perplexity=1.0007145, train_loss=0.0007143434

Batch 39680, train_perplexity=1.0007144, train_loss=0.00071412895

Batch 39690, train_perplexity=1.0007142, train_loss=0.0007139116

Batch 39700, train_perplexity=1.000714, train_loss=0.0007136555

Batch 39710, train_perplexity=1.0007137, train_loss=0.0007134798

Batch 39720, train_perplexity=1.0007135, train_loss=0.0007132564

Batch 39730, train_perplexity=1.0007133, train_loss=0.00071305095

Batch 39740, train_perplexity=1.0007131, train_loss=0.0007128604

Batch 39750, train_perplexity=1.0007129, train_loss=0.00071259233

Batch 39760, train_perplexity=1.0007128, train_loss=0.0007125239

Batch 39770, train_perplexity=1.0007125, train_loss=0.0007122618

Batch 39780, train_perplexity=1.0007124, train_loss=0.00071211293

Batch 39790, train_perplexity=1.0007122, train_loss=0.0007119015

Batch 39800, train_perplexity=1.0007119, train_loss=0.0007116692

Batch 39810, train_perplexity=1.0007117, train_loss=0.000711434

Batch 39820, train_perplexity=1.0007116, train_loss=0.00071125827

Batch 39830, train_perplexity=1.0007113, train_loss=0.0007110766

Batch 39840, train_perplexity=1.0007111, train_loss=0.0007108861

Batch 39850, train_perplexity=1.000711, train_loss=0.0007106598

Batch 39860, train_perplexity=1.0007107, train_loss=0.00071041554

Batch 39870, train_perplexity=1.0007105, train_loss=0.000710216

Batch 39880, train_perplexity=1.0007104, train_loss=0.0007100582

Batch 39890, train_perplexity=1.00071, train_loss=0.00070979615

Batch 39900, train_perplexity=1.0007098, train_loss=0.00070957874

Batch 39910, train_perplexity=1.0007097, train_loss=0.000709409

Batch 39920, train_perplexity=1.0007094, train_loss=0.0007092125

Batch 39930, train_perplexity=1.0007093, train_loss=0.000709001

Batch 39940, train_perplexity=1.000709, train_loss=0.0007087509

Batch 39950, train_perplexity=1.0007088, train_loss=0.0007086139

Batch 39960, train_perplexity=1.0007086, train_loss=0.0007083578

Batch 39970, train_perplexity=1.0007085, train_loss=0.000708197

Batch 39980, train_perplexity=1.0007082, train_loss=0.0007079796

Batch 39990, train_perplexity=1.0007081, train_loss=0.00070782175

Batch 40000, train_perplexity=1.0007079, train_loss=0.00070762227

Batch 40010, train_perplexity=1.0007076, train_loss=0.000707387

Batch 40020, train_perplexity=1.0007074, train_loss=0.0007071905

Batch 40030, train_perplexity=1.0007073, train_loss=0.0007069879

Batch 40040, train_perplexity=1.000707, train_loss=0.0007067855

Batch 40050, train_perplexity=1.0007068, train_loss=0.00070658297

Batch 40060, train_perplexity=1.0007066, train_loss=0.00070636265

Batch 40070, train_perplexity=1.0007063, train_loss=0.0007061125

Batch 40080, train_perplexity=1.0007062, train_loss=0.00070597255

Batch 40090, train_perplexity=1.000706, train_loss=0.00070576114

Batch 40100, train_perplexity=1.0007057, train_loss=0.00070550793

Batch 40110, train_perplexity=1.0007056, train_loss=0.000705368

Batch 40120, train_perplexity=1.0007056, train_loss=0.00070530846

Batch 40130, train_perplexity=1.0007051, train_loss=0.00070487964

Batch 40140, train_perplexity=1.000705, train_loss=0.0007047545

Batch 40150, train_perplexity=1.0007049, train_loss=0.0007046235

Batch 40160, train_perplexity=1.0007046, train_loss=0.0007043942

Batch 40170, train_perplexity=1.0007044, train_loss=0.000704153

Batch 40180, train_perplexity=1.0007042, train_loss=0.0007039654

Batch 40190, train_perplexity=1.0007039, train_loss=0.0007037361

Batch 40200, train_perplexity=1.0007038, train_loss=0.0007035723

Batch 40210, train_perplexity=1.0007037, train_loss=0.0007034354

Batch 40220, train_perplexity=1.0007035, train_loss=0.00070320605

Batch 40230, train_perplexity=1.0007032, train_loss=0.0007030243

Batch 40240, train_perplexity=1.0007031, train_loss=0.000702807

Batch 40250, train_perplexity=1.0007029, train_loss=0.0007025836

Batch 40260, train_perplexity=1.0007026, train_loss=0.00070236623

Batch 40270, train_perplexity=1.0007024, train_loss=0.0007021101

Batch 40280, train_perplexity=1.0007021, train_loss=0.0007019285

Batch 40290, train_perplexity=1.000702, train_loss=0.00070173794

Batch 40300, train_perplexity=1.0007018, train_loss=0.0007015235

Batch 40310, train_perplexity=1.0007017, train_loss=0.0007013775

Batch 40320, train_perplexity=1.0007015, train_loss=0.0007012883

Batch 40330, train_perplexity=1.0007012, train_loss=0.0007009547

Batch 40340, train_perplexity=1.000701, train_loss=0.00070073735

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 40350, train_perplexity=1.000701, train_loss=0.00070065394

Batch 40360, train_perplexity=1.0007006, train_loss=0.00070035015

Batch 40370, train_perplexity=1.0007004, train_loss=0.0007001596

Batch 40380, train_perplexity=1.0007002, train_loss=0.00069999875

Batch 40390, train_perplexity=1.0007001, train_loss=0.0006998291

Batch 40400, train_perplexity=1.0006999, train_loss=0.00069962954

Batch 40410, train_perplexity=1.0006996, train_loss=0.00069942407

Batch 40420, train_perplexity=1.0006994, train_loss=0.000699165

Batch 40430, train_perplexity=1.0006992, train_loss=0.00069892977

Batch 40440, train_perplexity=1.0006992, train_loss=0.000698894

Batch 40450, train_perplexity=1.0006989, train_loss=0.0006986498

Batch 40460, train_perplexity=1.0006987, train_loss=0.0006984711

Batch 40470, train_perplexity=1.0006986, train_loss=0.0006983192

Batch 40480, train_perplexity=1.0006982, train_loss=0.00069802144

Batch 40490, train_perplexity=1.0006981, train_loss=0.0006978011

Batch 40500, train_perplexity=1.0006979, train_loss=0.0006976403

Batch 40510, train_perplexity=1.0006977, train_loss=0.00069747947

Batch 40520, train_perplexity=1.0006975, train_loss=0.0006972502

Batch 40530, train_perplexity=1.0006973, train_loss=0.0006970655

Batch 40540, train_perplexity=1.0006971, train_loss=0.0006968601

Batch 40550, train_perplexity=1.000697, train_loss=0.0006967171

Batch 40560, train_perplexity=1.0006967, train_loss=0.00069641933

Batch 40570, train_perplexity=1.0006965, train_loss=0.0006962585

Batch 40580, train_perplexity=1.0006963, train_loss=0.00069610064

Batch 40590, train_perplexity=1.0006961, train_loss=0.00069587736

Batch 40600, train_perplexity=1.000696, train_loss=0.00069573737

Batch 40610, train_perplexity=1.0006957, train_loss=0.00069549016

Batch 40620, train_perplexity=1.0006955, train_loss=0.0006952668

Batch 40630, train_perplexity=1.0006953, train_loss=0.0006950941

Batch 40640, train_perplexity=1.0006951, train_loss=0.00069487974

Batch 40650, train_perplexity=1.000695, train_loss=0.00069470704

Batch 40660, train_perplexity=1.0006949, train_loss=0.00069459685

Batch 40670, train_perplexity=1.0006946, train_loss=0.0006943437

Batch 40680, train_perplexity=1.0006944, train_loss=0.0006941054

Batch 40690, train_perplexity=1.0006942, train_loss=0.0006939625

Batch 40700, train_perplexity=1.0006939, train_loss=0.00069367664

Batch 40710, train_perplexity=1.0006938, train_loss=0.00069355755

Batch 40720, train_perplexity=1.0006936, train_loss=0.0006932955

Batch 40730, train_perplexity=1.0006933, train_loss=0.0006931197

Batch 40740, train_perplexity=1.0006932, train_loss=0.0006929113

Batch 40750, train_perplexity=1.000693, train_loss=0.00069273263
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40760, train_perplexity=1.0006928, train_loss=0.0006925599

Batch 40770, train_perplexity=1.0006926, train_loss=0.00069237826

Batch 40780, train_perplexity=1.0006924, train_loss=0.00069217576

Batch 40790, train_perplexity=1.0006922, train_loss=0.00069196726

Batch 40800, train_perplexity=1.000692, train_loss=0.000691732

Batch 40810, train_perplexity=1.0006918, train_loss=0.0006915146

Batch 40820, train_perplexity=1.0006917, train_loss=0.0006913866

Batch 40830, train_perplexity=1.0006914, train_loss=0.0006911871

Batch 40840, train_perplexity=1.0006912, train_loss=0.00069098163

Batch 40850, train_perplexity=1.000691, train_loss=0.00069086836

Batch 40860, train_perplexity=1.0006909, train_loss=0.0006906898

Batch 40870, train_perplexity=1.0006907, train_loss=0.00069041876

Batch 40880, train_perplexity=1.0006905, train_loss=0.00069027283

Batch 40890, train_perplexity=1.0006903, train_loss=0.0006900554

Batch 40900, train_perplexity=1.0006901, train_loss=0.00068990956

Batch 40910, train_perplexity=1.0006899, train_loss=0.0006895879

Batch 40920, train_perplexity=1.0006897, train_loss=0.0006894539

Batch 40930, train_perplexity=1.0006895, train_loss=0.0006892901

Batch 40940, train_perplexity=1.0006893, train_loss=0.0006890757

Batch 40950, train_perplexity=1.0006891, train_loss=0.0006888583

Batch 40960, train_perplexity=1.0006889, train_loss=0.00068870344

Batch 40970, train_perplexity=1.0006888, train_loss=0.0006885069

Batch 40980, train_perplexity=1.0006886, train_loss=0.0006882865

Batch 40990, train_perplexity=1.0006883, train_loss=0.0006880989

Batch 41000, train_perplexity=1.0006882, train_loss=0.00068796193

Batch 41010, train_perplexity=1.000688, train_loss=0.0006877475

Batch 41020, train_perplexity=1.0006877, train_loss=0.00068753306

Batch 41030, train_perplexity=1.0006876, train_loss=0.00068735145

Batch 41040, train_perplexity=1.0006875, train_loss=0.000687271

Batch 41050, train_perplexity=1.0006872, train_loss=0.00068696134

Batch 41060, train_perplexity=1.000687, train_loss=0.0006867796

Batch 41070, train_perplexity=1.0006869, train_loss=0.0006866367

Batch 41080, train_perplexity=1.0006868, train_loss=0.00068646995

Batch 41090, train_perplexity=1.0006864, train_loss=0.00068618404

Batch 41100, train_perplexity=1.0006862, train_loss=0.00068597565

Batch 41110, train_perplexity=1.0006862, train_loss=0.0006858774

Batch 41120, train_perplexity=1.0006859, train_loss=0.00068567786

Batch 41130, train_perplexity=1.0006857, train_loss=0.00068544556

Batch 41140, train_perplexity=1.0006855, train_loss=0.00068527577

Batch 41150, train_perplexity=1.0006853, train_loss=0.00068506727

Batch 41160, train_perplexity=1.0006852, train_loss=0.0006849214

Batch 41170, train_perplexity=1.000685, train_loss=0.00068470405

Batch 41180, train_perplexity=1.0006847, train_loss=0.0006844896

Batch 41190, train_perplexity=1.0006845, train_loss=0.0006843139

Batch 41200, train_perplexity=1.0006844, train_loss=0.00068409945

Batch 41210, train_perplexity=1.0006841, train_loss=0.00068388804

Batch 41220, train_perplexity=1.000684, train_loss=0.00068376295

Batch 41230, train_perplexity=1.0006838, train_loss=0.00068349496

Batch 41240, train_perplexity=1.0006835, train_loss=0.000683355

Batch 41250, train_perplexity=1.0006834, train_loss=0.0006831465

Batch 41260, train_perplexity=1.0006832, train_loss=0.0006829082

Batch 41270, train_perplexity=1.0006831, train_loss=0.0006827981

Batch 41280, train_perplexity=1.000683, train_loss=0.00068266405

Batch 41290, train_perplexity=1.0006826, train_loss=0.00068234245

Batch 41300, train_perplexity=1.0006825, train_loss=0.00068222627

Batch 41310, train_perplexity=1.0006822, train_loss=0.00068201195

Batch 41320, train_perplexity=1.0006821, train_loss=0.0006818451

Batch 41330, train_perplexity=1.0006819, train_loss=0.000681595

Batch 41340, train_perplexity=1.0006816, train_loss=0.0006813597

Batch 41350, train_perplexity=1.0006815, train_loss=0.0006812465

Batch 41360, train_perplexity=1.0006813, train_loss=0.00068105594

Batch 41370, train_perplexity=1.000681, train_loss=0.00068084756

Batch 41380, train_perplexity=1.0006809, train_loss=0.0006806748

Batch 41390, train_perplexity=1.0006807, train_loss=0.00068050204

Batch 41400, train_perplexity=1.0006806, train_loss=0.0006802907

Batch 41410, train_perplexity=1.0006803, train_loss=0.0006800762

Batch 41420, train_perplexity=1.0006801, train_loss=0.0006799124

Batch 41430, train_perplexity=1.00068, train_loss=0.0006797427

Batch 41440, train_perplexity=1.0006799, train_loss=0.0006796295

Batch 41450, train_perplexity=1.0006796, train_loss=0.00067941204

Batch 41460, train_perplexity=1.0006795, train_loss=0.00067920366

Batch 41470, train_perplexity=1.0006793, train_loss=0.0006790339

Batch 41480, train_perplexity=1.000679, train_loss=0.0006787957

Batch 41490, train_perplexity=1.0006789, train_loss=0.0006786318

Batch 41500, train_perplexity=1.0006787, train_loss=0.0006783996

Batch 41510, train_perplexity=1.0006784, train_loss=0.00067823875

Batch 41520, train_perplexity=1.0006782, train_loss=0.0006780035

Batch 41530, train_perplexity=1.0006781, train_loss=0.0006778665

Batch 41540, train_perplexity=1.000678, train_loss=0.0006777236

Batch 41550, train_perplexity=1.0006777, train_loss=0.0006774913

Batch 41560, train_perplexity=1.0006776, train_loss=0.0006773454

Batch 41570, train_perplexity=1.0006772, train_loss=0.00067705056

Batch 41580, train_perplexity=1.0006771, train_loss=0.0006769225

Batch 41590, train_perplexity=1.000677, train_loss=0.00067677954

Batch 41600, train_perplexity=1.0006768, train_loss=0.00067655323

Batch 41610, train_perplexity=1.0006766, train_loss=0.00067636266

Batch 41620, train_perplexity=1.0006764, train_loss=0.0006761631

Batch 41630, train_perplexity=1.0006762, train_loss=0.0006759606

Batch 41640, train_perplexity=1.000676, train_loss=0.000675782

Batch 41650, train_perplexity=1.0006759, train_loss=0.00067566277

Batch 41660, train_perplexity=1.0006757, train_loss=0.0006754186

Batch 41670, train_perplexity=1.0006754, train_loss=0.0006752638

Batch 41680, train_perplexity=1.0006753, train_loss=0.00067510887

Batch 41690, train_perplexity=1.0006751, train_loss=0.0006748885

Batch 41700, train_perplexity=1.000675, train_loss=0.0006747426

Batch 41710, train_perplexity=1.0006747, train_loss=0.00067454606

Batch 41720, train_perplexity=1.0006745, train_loss=0.0006742751

Batch 41730, train_perplexity=1.0006744, train_loss=0.0006741679

Batch 41740, train_perplexity=1.0006742, train_loss=0.00067398325

Batch 41750, train_perplexity=1.000674, train_loss=0.0006737717

Batch 41760, train_perplexity=1.0006739, train_loss=0.000673614

Batch 41770, train_perplexity=1.0006737, train_loss=0.0006734442

Batch 41780, train_perplexity=1.0006735, train_loss=0.00067325355

Batch 41790, train_perplexity=1.0006733, train_loss=0.0006730452

Batch 41800, train_perplexity=1.000673, train_loss=0.0006728814

Batch 41810, train_perplexity=1.0006729, train_loss=0.0006726788

Batch 41820, train_perplexity=1.0006727, train_loss=0.00067248824

Batch 41830, train_perplexity=1.0006726, train_loss=0.0006723424

Batch 41840, train_perplexity=1.0006723, train_loss=0.0006721637

Batch 41850, train_perplexity=1.0006722, train_loss=0.00067196414

Batch 41860, train_perplexity=1.000672, train_loss=0.0006718122

Batch 41870, train_perplexity=1.0006719, train_loss=0.0006716514

Batch 41880, train_perplexity=1.0006716, train_loss=0.0006714549

Batch 41890, train_perplexity=1.0006715, train_loss=0.0006712881

Batch 41900, train_perplexity=1.0006713, train_loss=0.0006710439

Batch 41910, train_perplexity=1.0006711, train_loss=0.00067088613

Batch 41920, train_perplexity=1.0006709, train_loss=0.00067068357

Batch 41930, train_perplexity=1.0006707, train_loss=0.0006704959

Batch 41940, train_perplexity=1.0006706, train_loss=0.00067037385

Batch 41950, train_perplexity=1.0006703, train_loss=0.00067014445

Batch 41960, train_perplexity=1.0006702, train_loss=0.00066992117

Batch 41970, train_perplexity=1.0006701, train_loss=0.00066979614

Batch 41980, train_perplexity=1.0006698, train_loss=0.0006695698

Batch 41990, train_perplexity=1.0006696, train_loss=0.00066937914
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42000, train_perplexity=1.0006695, train_loss=0.00066924514

Batch 42010, train_perplexity=1.0006692, train_loss=0.00066906057

Batch 42020, train_perplexity=1.0006691, train_loss=0.0006688908

Batch 42030, train_perplexity=1.000669, train_loss=0.00066872704

Batch 42040, train_perplexity=1.0006688, train_loss=0.0006685185

Batch 42050, train_perplexity=1.0006685, train_loss=0.00066836074

Batch 42060, train_perplexity=1.0006683, train_loss=0.0006680838

Batch 42070, train_perplexity=1.0006682, train_loss=0.00066799147

Batch 42080, train_perplexity=1.000668, train_loss=0.00066780974

Batch 42090, train_perplexity=1.0006678, train_loss=0.0006676162

Batch 42100, train_perplexity=1.0006676, train_loss=0.0006673571

Batch 42110, train_perplexity=1.0006675, train_loss=0.0006672767

Batch 42120, train_perplexity=1.0006672, train_loss=0.0006670384

Batch 42130, train_perplexity=1.0006671, train_loss=0.000666833

Batch 42140, train_perplexity=1.0006669, train_loss=0.00066666916

Batch 42150, train_perplexity=1.0006667, train_loss=0.00066656794

Batch 42160, train_perplexity=1.0006665, train_loss=0.00066631474

Batch 42170, train_perplexity=1.0006665, train_loss=0.00066622545

Batch 42180, train_perplexity=1.0006661, train_loss=0.00066595443

Batch 42190, train_perplexity=1.000666, train_loss=0.00066582044

Batch 42200, train_perplexity=1.0006658, train_loss=0.0006655643

Batch 42210, train_perplexity=1.0006655, train_loss=0.00066534686

Batch 42220, train_perplexity=1.0006655, train_loss=0.00066528434

Batch 42230, train_perplexity=1.0006653, train_loss=0.0006650848

Batch 42240, train_perplexity=1.0006651, train_loss=0.0006648407

Batch 42250, train_perplexity=1.000665, train_loss=0.0006647126

Batch 42260, train_perplexity=1.0006647, train_loss=0.0006644684

Batch 42270, train_perplexity=1.0006645, train_loss=0.0006642659

Batch 42280, train_perplexity=1.0006644, train_loss=0.00066414976

Batch 42290, train_perplexity=1.0006642, train_loss=0.00066396804

Batch 42300, train_perplexity=1.000664, train_loss=0.00066380727

Batch 42310, train_perplexity=1.0006639, train_loss=0.00066364056

Batch 42320, train_perplexity=1.0006636, train_loss=0.00066347374

Batch 42330, train_perplexity=1.0006635, train_loss=0.0006632563

Batch 42340, train_perplexity=1.0006633, train_loss=0.00066310144

Batch 42350, train_perplexity=1.0006632, train_loss=0.00066292874

Batch 42360, train_perplexity=1.0006629, train_loss=0.0006627322

Batch 42370, train_perplexity=1.0006628, train_loss=0.00066258933

Batch 42380, train_perplexity=1.0006627, train_loss=0.0006624761

Batch 42390, train_perplexity=1.0006624, train_loss=0.00066220504

Batch 42400, train_perplexity=1.0006622, train_loss=0.0006620204

Batch 42410, train_perplexity=1.0006621, train_loss=0.0006619162

Batch 42420, train_perplexity=1.0006618, train_loss=0.00066168094

Batch 42430, train_perplexity=1.0006617, train_loss=0.000661544

Batch 42440, train_perplexity=1.0006615, train_loss=0.0006613117

Batch 42450, train_perplexity=1.0006614, train_loss=0.0006611926

Batch 42460, train_perplexity=1.0006611, train_loss=0.00066091854

Batch 42470, train_perplexity=1.000661, train_loss=0.00066078163

Batch 42480, train_perplexity=1.0006608, train_loss=0.00066057604

Batch 42490, train_perplexity=1.0006607, train_loss=0.0006604451

Batch 42500, train_perplexity=1.0006604, train_loss=0.0006602515

Batch 42510, train_perplexity=1.0006603, train_loss=0.0006601085

Batch 42520, train_perplexity=1.0006602, train_loss=0.0006599507

Batch 42530, train_perplexity=1.0006598, train_loss=0.00065965287

Batch 42540, train_perplexity=1.0006598, train_loss=0.0006595784

Batch 42550, train_perplexity=1.0006597, train_loss=0.0006594355

Batch 42560, train_perplexity=1.0006595, train_loss=0.00065926276

Batch 42570, train_perplexity=1.0006592, train_loss=0.0006590036

Batch 42580, train_perplexity=1.000659, train_loss=0.0006588041

Batch 42590, train_perplexity=1.0006589, train_loss=0.0006586284

Batch 42600, train_perplexity=1.0006588, train_loss=0.0006585034

Batch 42610, train_perplexity=1.0006585, train_loss=0.0006582889

Batch 42620, train_perplexity=1.0006584, train_loss=0.000658137

Batch 42630, train_perplexity=1.0006582, train_loss=0.0006579792

Batch 42640, train_perplexity=1.000658, train_loss=0.0006577707

Batch 42650, train_perplexity=1.0006578, train_loss=0.00065761595

Batch 42660, train_perplexity=1.0006577, train_loss=0.000657473

Batch 42670, train_perplexity=1.0006574, train_loss=0.0006572645

Batch 42680, train_perplexity=1.0006573, train_loss=0.00065714534

Batch 42690, train_perplexity=1.0006571, train_loss=0.0006569071

Batch 42700, train_perplexity=1.000657, train_loss=0.0006568029

Batch 42710, train_perplexity=1.0006567, train_loss=0.0006564753

Batch 42720, train_perplexity=1.0006566, train_loss=0.0006564247

Batch 42730, train_perplexity=1.0006564, train_loss=0.0006561715

Batch 42740, train_perplexity=1.0006562, train_loss=0.00065606134

Batch 42750, train_perplexity=1.0006561, train_loss=0.00065587077

Batch 42760, train_perplexity=1.000656, train_loss=0.00065577845

Batch 42770, train_perplexity=1.0006558, train_loss=0.0006555313

Batch 42780, train_perplexity=1.0006555, train_loss=0.0006553258

Batch 42790, train_perplexity=1.0006553, train_loss=0.0006551113

Batch 42800, train_perplexity=1.0006552, train_loss=0.0006549803

Batch 42810, train_perplexity=1.000655, train_loss=0.0006548165

Batch 42820, train_perplexity=1.0006549, train_loss=0.0006546705

Batch 42830, train_perplexity=1.0006547, train_loss=0.000654483

Batch 42840, train_perplexity=1.0006546, train_loss=0.0006543222

Batch 42850, train_perplexity=1.0006543, train_loss=0.00065416726

Batch 42860, train_perplexity=1.0006542, train_loss=0.0006539677

Batch 42870, train_perplexity=1.000654, train_loss=0.0006537891

Batch 42880, train_perplexity=1.0006537, train_loss=0.0006535865

Batch 42890, train_perplexity=1.0006537, train_loss=0.0006534733

Batch 42900, train_perplexity=1.0006534, train_loss=0.0006532262

Batch 42910, train_perplexity=1.0006534, train_loss=0.00065312494

Batch 42920, train_perplexity=1.0006531, train_loss=0.0006529552

Batch 42930, train_perplexity=1.000653, train_loss=0.0006527676

Batch 42940, train_perplexity=1.0006528, train_loss=0.00065256807

Batch 42950, train_perplexity=1.0006527, train_loss=0.0006524102

Batch 42960, train_perplexity=1.0006524, train_loss=0.000652169

Batch 42970, train_perplexity=1.0006522, train_loss=0.00065202906

Batch 42980, train_perplexity=1.0006521, train_loss=0.0006518414

Batch 42990, train_perplexity=1.000652, train_loss=0.00065170444

Batch 43000, train_perplexity=1.0006517, train_loss=0.000651487

Batch 43010, train_perplexity=1.0006516, train_loss=0.0006513351

Batch 43020, train_perplexity=1.0006515, train_loss=0.0006512726

Batch 43030, train_perplexity=1.0006512, train_loss=0.0006510135

Batch 43040, train_perplexity=1.0006511, train_loss=0.0006508556

Batch 43050, train_perplexity=1.0006509, train_loss=0.0006506472

Batch 43060, train_perplexity=1.0006508, train_loss=0.0006505251

Batch 43070, train_perplexity=1.0006505, train_loss=0.00065035536

Batch 43080, train_perplexity=1.0006504, train_loss=0.00065020344

Batch 43090, train_perplexity=1.0006502, train_loss=0.00065000687

Batch 43100, train_perplexity=1.00065, train_loss=0.0006497835

Batch 43110, train_perplexity=1.0006498, train_loss=0.00064963463

Batch 43120, train_perplexity=1.0006497, train_loss=0.0006495215

Batch 43130, train_perplexity=1.0006496, train_loss=0.000649313

Batch 43140, train_perplexity=1.0006493, train_loss=0.00064911344

Batch 43150, train_perplexity=1.0006492, train_loss=0.0006489437

Batch 43160, train_perplexity=1.000649, train_loss=0.00064882165

Batch 43170, train_perplexity=1.0006489, train_loss=0.0006486012

Batch 43180, train_perplexity=1.0006486, train_loss=0.00064846425

Batch 43190, train_perplexity=1.0006485, train_loss=0.00064830633

Batch 43200, train_perplexity=1.0006484, train_loss=0.0006481337

Batch 43210, train_perplexity=1.0006481, train_loss=0.00064790435

Batch 43220, train_perplexity=1.0006479, train_loss=0.00064771675

Batch 43230, train_perplexity=1.0006478, train_loss=0.0006475678
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 43240, train_perplexity=1.0006477, train_loss=0.00064745464

Batch 43250, train_perplexity=1.0006475, train_loss=0.0006472998

Batch 43260, train_perplexity=1.0006472, train_loss=0.0006470288

Batch 43270, train_perplexity=1.0006471, train_loss=0.0006468382

Batch 43280, train_perplexity=1.0006471, train_loss=0.0006468203

Batch 43290, train_perplexity=1.0006468, train_loss=0.0006465791

Batch 43300, train_perplexity=1.0006466, train_loss=0.00064638257

Batch 43310, train_perplexity=1.0006465, train_loss=0.00064626645

Batch 43320, train_perplexity=1.0006462, train_loss=0.0006460728

Batch 43330, train_perplexity=1.0006462, train_loss=0.00064599543

Batch 43340, train_perplexity=1.000646, train_loss=0.0006457363

Batch 43350, train_perplexity=1.0006459, train_loss=0.00064561714

Batch 43360, train_perplexity=1.0006456, train_loss=0.00064547424

Batch 43370, train_perplexity=1.0006455, train_loss=0.0006452658

Batch 43380, train_perplexity=1.0006453, train_loss=0.0006450513

Batch 43390, train_perplexity=1.0006452, train_loss=0.0006449203

Batch 43400, train_perplexity=1.0006449, train_loss=0.0006447237

Batch 43410, train_perplexity=1.0006448, train_loss=0.00064459274

Batch 43420, train_perplexity=1.0006446, train_loss=0.0006443366

Batch 43430, train_perplexity=1.0006444, train_loss=0.0006442502

Batch 43440, train_perplexity=1.0006442, train_loss=0.00064403284

Batch 43450, train_perplexity=1.0006441, train_loss=0.0006438631

Batch 43460, train_perplexity=1.000644, train_loss=0.00064373505

Batch 43470, train_perplexity=1.0006437, train_loss=0.0006435325

Batch 43480, train_perplexity=1.0006435, train_loss=0.00064333

Batch 43490, train_perplexity=1.0006434, train_loss=0.0006432198

Batch 43500, train_perplexity=1.0006433, train_loss=0.0006430917

Batch 43510, train_perplexity=1.000643, train_loss=0.00064281176

Batch 43520, train_perplexity=1.0006429, train_loss=0.00064269267

Batch 43530, train_perplexity=1.0006427, train_loss=0.00064246636

Batch 43540, train_perplexity=1.0006425, train_loss=0.000642371

Batch 43550, train_perplexity=1.0006424, train_loss=0.0006422042

Batch 43560, train_perplexity=1.0006423, train_loss=0.00064208213

Batch 43570, train_perplexity=1.0006422, train_loss=0.0006419243

Batch 43580, train_perplexity=1.000642, train_loss=0.0006417099

Batch 43590, train_perplexity=1.0006418, train_loss=0.00064158475

Batch 43600, train_perplexity=1.0006416, train_loss=0.0006413972

Batch 43610, train_perplexity=1.0006415, train_loss=0.0006412363

Batch 43620, train_perplexity=1.0006412, train_loss=0.0006410071

Batch 43630, train_perplexity=1.0006411, train_loss=0.00064088195

Batch 43640, train_perplexity=1.0006409, train_loss=0.00064070034

Batch 43650, train_perplexity=1.0006407, train_loss=0.0006405186

Batch 43660, train_perplexity=1.0006405, train_loss=0.0006403638

Batch 43670, train_perplexity=1.0006404, train_loss=0.0006401553

Batch 43680, train_perplexity=1.0006403, train_loss=0.0006400749

Batch 43690, train_perplexity=1.0006402, train_loss=0.00063989323

Batch 43700, train_perplexity=1.0006399, train_loss=0.0006397622

Batch 43710, train_perplexity=1.0006398, train_loss=0.0006395716

Batch 43720, train_perplexity=1.0006394, train_loss=0.00063926185

Batch 43730, train_perplexity=1.0006394, train_loss=0.0006391844

Batch 43740, train_perplexity=1.0006392, train_loss=0.0006390177

Batch 43750, train_perplexity=1.0006391, train_loss=0.0006388747

Batch 43760, train_perplexity=1.0006388, train_loss=0.00063868705

Batch 43770, train_perplexity=1.0006387, train_loss=0.00063856796

Batch 43780, train_perplexity=1.0006386, train_loss=0.000638425

Batch 43790, train_perplexity=1.0006385, train_loss=0.0006382374

Batch 43800, train_perplexity=1.0006382, train_loss=0.00063802896

Batch 43810, train_perplexity=1.0006381, train_loss=0.00063789485

Batch 43820, train_perplexity=1.0006379, train_loss=0.00063769834

Batch 43830, train_perplexity=1.0006378, train_loss=0.00063755835

Batch 43840, train_perplexity=1.0006375, train_loss=0.00063730526

Batch 43850, train_perplexity=1.0006374, train_loss=0.0006371891

Batch 43860, train_perplexity=1.0006373, train_loss=0.000637058

Batch 43870, train_perplexity=1.000637, train_loss=0.0006368347

Batch 43880, train_perplexity=1.0006368, train_loss=0.00063667085

Batch 43890, train_perplexity=1.0006367, train_loss=0.0006365309

Batch 43900, train_perplexity=1.0006366, train_loss=0.00063641183

Batch 43910, train_perplexity=1.0006365, train_loss=0.0006362122

Batch 43920, train_perplexity=1.0006363, train_loss=0.0006360783

Batch 43930, train_perplexity=1.0006361, train_loss=0.0006359324

Batch 43940, train_perplexity=1.000636, train_loss=0.00063575956

Batch 43950, train_perplexity=1.0006359, train_loss=0.00063564937

Batch 43960, train_perplexity=1.0006356, train_loss=0.00063543196

Batch 43970, train_perplexity=1.0006354, train_loss=0.0006352235

Batch 43980, train_perplexity=1.0006353, train_loss=0.00063511624

Batch 43990, train_perplexity=1.0006351, train_loss=0.0006349435

Batch 44000, train_perplexity=1.000635, train_loss=0.00063478574

Batch 44010, train_perplexity=1.0006348, train_loss=0.0006345713

Batch 44020, train_perplexity=1.0006347, train_loss=0.00063441345

Batch 44030, train_perplexity=1.0006346, train_loss=0.00063438364

Batch 44040, train_perplexity=1.0006343, train_loss=0.0006341156

Batch 44050, train_perplexity=1.0006342, train_loss=0.00063396676

Batch 44060, train_perplexity=1.000634, train_loss=0.0006337195

Batch 44070, train_perplexity=1.0006338, train_loss=0.0006336064

Batch 44080, train_perplexity=1.0006337, train_loss=0.000633523

Batch 44090, train_perplexity=1.0006335, train_loss=0.0006332847

Batch 44100, train_perplexity=1.0006334, train_loss=0.00063315965

Batch 44110, train_perplexity=1.0006331, train_loss=0.000632972

Batch 44120, train_perplexity=1.000633, train_loss=0.00063278736

Batch 44130, train_perplexity=1.0006328, train_loss=0.0006325491

Batch 44140, train_perplexity=1.0006326, train_loss=0.0006324181

Batch 44150, train_perplexity=1.0006325, train_loss=0.00063229597

Batch 44160, train_perplexity=1.0006324, train_loss=0.00063215604

Batch 44170, train_perplexity=1.000632, train_loss=0.00063189096

Batch 44180, train_perplexity=1.000632, train_loss=0.00063186715

Batch 44190, train_perplexity=1.0006319, train_loss=0.00063168246

Batch 44200, train_perplexity=1.0006317, train_loss=0.000631477

Batch 44210, train_perplexity=1.0006316, train_loss=0.0006313191

Batch 44220, train_perplexity=1.0006315, train_loss=0.00063121493

Batch 44230, train_perplexity=1.0006312, train_loss=0.00063100946

Batch 44240, train_perplexity=1.000631, train_loss=0.0006307741

Batch 44250, train_perplexity=1.0006309, train_loss=0.00063069374

Batch 44260, train_perplexity=1.0006307, train_loss=0.00063058356

Batch 44270, train_perplexity=1.0006306, train_loss=0.0006304078

Batch 44280, train_perplexity=1.0006304, train_loss=0.0006301487

Batch 44290, train_perplexity=1.0006301, train_loss=0.0006299492

Batch 44300, train_perplexity=1.00063, train_loss=0.00062982703

Batch 44310, train_perplexity=1.0006299, train_loss=0.00062975264

Batch 44320, train_perplexity=1.0006298, train_loss=0.00062957697

Batch 44330, train_perplexity=1.0006295, train_loss=0.0006293893

Batch 44340, train_perplexity=1.0006294, train_loss=0.00062926125

Batch 44350, train_perplexity=1.0006293, train_loss=0.00062911527

Batch 44360, train_perplexity=1.0006291, train_loss=0.00062889495

Batch 44370, train_perplexity=1.000629, train_loss=0.0006287371

Batch 44380, train_perplexity=1.0006288, train_loss=0.00062864483

Batch 44390, train_perplexity=1.0006287, train_loss=0.000628466

Batch 44400, train_perplexity=1.0006285, train_loss=0.0006283083

Batch 44410, train_perplexity=1.0006282, train_loss=0.00062807894

Batch 44420, train_perplexity=1.0006282, train_loss=0.00062799256

Batch 44430, train_perplexity=1.000628, train_loss=0.0006278466

Batch 44440, train_perplexity=1.0006278, train_loss=0.00062758755

Batch 44450, train_perplexity=1.0006278, train_loss=0.0006275071

Batch 44460, train_perplexity=1.0006275, train_loss=0.00062731944

Batch 44470, train_perplexity=1.0006274, train_loss=0.0006271944
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44480, train_perplexity=1.0006272, train_loss=0.00062701275

Batch 44490, train_perplexity=1.000627, train_loss=0.000626834

Batch 44500, train_perplexity=1.0006268, train_loss=0.0006266136

Batch 44510, train_perplexity=1.0006268, train_loss=0.0006265511

Batch 44520, train_perplexity=1.0006266, train_loss=0.0006264171

Batch 44530, train_perplexity=1.0006263, train_loss=0.00062617287

Batch 44540, train_perplexity=1.0006263, train_loss=0.0006260836

Batch 44550, train_perplexity=1.0006261, train_loss=0.000625875

Batch 44560, train_perplexity=1.000626, train_loss=0.0006257202

Batch 44570, train_perplexity=1.0006257, train_loss=0.00062558916

Batch 44580, train_perplexity=1.0006255, train_loss=0.00062533

Batch 44590, train_perplexity=1.0006255, train_loss=0.00062525563

Batch 44600, train_perplexity=1.0006254, train_loss=0.0006251811

Batch 44610, train_perplexity=1.0006251, train_loss=0.00062492804

Batch 44620, train_perplexity=1.0006249, train_loss=0.00062474335

Batch 44630, train_perplexity=1.0006249, train_loss=0.00062464504

Batch 44640, train_perplexity=1.0006247, train_loss=0.00062443066

Batch 44650, train_perplexity=1.0006244, train_loss=0.0006242311

Batch 44660, train_perplexity=1.0006244, train_loss=0.00062419544

Batch 44670, train_perplexity=1.0006242, train_loss=0.00062393036

Batch 44680, train_perplexity=1.000624, train_loss=0.0006237576

Batch 44690, train_perplexity=1.0006238, train_loss=0.00062361767

Batch 44700, train_perplexity=1.0006236, train_loss=0.00062342995

Batch 44710, train_perplexity=1.0006236, train_loss=0.00062339124

Batch 44720, train_perplexity=1.0006233, train_loss=0.000623147

Batch 44730, train_perplexity=1.0006232, train_loss=0.00062301895

Batch 44740, train_perplexity=1.0006231, train_loss=0.000622879

Batch 44750, train_perplexity=1.0006229, train_loss=0.00062264665

Batch 44760, train_perplexity=1.0006227, train_loss=0.00062261394

Batch 44770, train_perplexity=1.0006225, train_loss=0.0006223608

Batch 44780, train_perplexity=1.0006224, train_loss=0.0006221642

Batch 44790, train_perplexity=1.0006223, train_loss=0.00062211056

Batch 44800, train_perplexity=1.0006222, train_loss=0.0006219677

Batch 44810, train_perplexity=1.0006219, train_loss=0.0006217265

Batch 44820, train_perplexity=1.0006218, train_loss=0.00062161626

Batch 44830, train_perplexity=1.0006217, train_loss=0.0006214614

Batch 44840, train_perplexity=1.0006214, train_loss=0.00062128867

Batch 44850, train_perplexity=1.0006213, train_loss=0.0006211696

Batch 44860, train_perplexity=1.0006212, train_loss=0.000620967

Batch 44870, train_perplexity=1.0006211, train_loss=0.00062083895

Batch 44880, train_perplexity=1.000621, train_loss=0.00062071683

Batch 44890, train_perplexity=1.0006207, train_loss=0.0006204875

Batch 44900, train_perplexity=1.0006206, train_loss=0.0006204101

Batch 44910, train_perplexity=1.0006204, train_loss=0.0006202165

Batch 44920, train_perplexity=1.0006202, train_loss=0.00062004384

Batch 44930, train_perplexity=1.00062, train_loss=0.0006198383

Batch 44940, train_perplexity=1.0006199, train_loss=0.00061968644

Batch 44950, train_perplexity=1.0006198, train_loss=0.00061958807

Batch 44960, train_perplexity=1.0006196, train_loss=0.00061943324

Batch 44970, train_perplexity=1.0006194, train_loss=0.0006192784

Batch 44980, train_perplexity=1.0006193, train_loss=0.00061904907

Batch 44990, train_perplexity=1.000619, train_loss=0.000618918

Batch 45000, train_perplexity=1.0006189, train_loss=0.00061872444

Batch 45010, train_perplexity=1.0006188, train_loss=0.0006186827

Batch 45020, train_perplexity=1.0006187, train_loss=0.0006184683

Batch 45030, train_perplexity=1.0006186, train_loss=0.00061836105

Batch 45040, train_perplexity=1.0006183, train_loss=0.0006181854

Batch 45050, train_perplexity=1.0006182, train_loss=0.0006180246

Batch 45060, train_perplexity=1.000618, train_loss=0.00061782205

Batch 45070, train_perplexity=1.000618, train_loss=0.0006177506

Batch 45080, train_perplexity=1.0006176, train_loss=0.0006174795

Batch 45090, train_perplexity=1.0006175, train_loss=0.00061737234

Batch 45100, train_perplexity=1.0006174, train_loss=0.0006172204

Batch 45110, train_perplexity=1.0006173, train_loss=0.0006170983

Batch 45120, train_perplexity=1.0006171, train_loss=0.0006169375

Batch 45130, train_perplexity=1.0006169, train_loss=0.0006167439

Batch 45140, train_perplexity=1.0006168, train_loss=0.00061656226

Batch 45150, train_perplexity=1.0006167, train_loss=0.0006164521

Batch 45160, train_perplexity=1.0006166, train_loss=0.000616321

Batch 45170, train_perplexity=1.0006163, train_loss=0.0006160828

Batch 45180, train_perplexity=1.0006163, train_loss=0.0006160649

Batch 45190, train_perplexity=1.0006161, train_loss=0.0006158445

Batch 45200, train_perplexity=1.0006158, train_loss=0.00061565393

Batch 45210, train_perplexity=1.0006157, train_loss=0.0006154842

Batch 45220, train_perplexity=1.0006156, train_loss=0.00061538885

Batch 45230, train_perplexity=1.0006154, train_loss=0.0006151536

Batch 45240, train_perplexity=1.0006154, train_loss=0.00061512087

Batch 45250, train_perplexity=1.0006151, train_loss=0.0006149898

Batch 45260, train_perplexity=1.000615, train_loss=0.0006148409

Batch 45270, train_perplexity=1.0006149, train_loss=0.0006146592

Batch 45280, train_perplexity=1.0006146, train_loss=0.00061443285

Batch 45290, train_perplexity=1.0006144, train_loss=0.00061423925

Batch 45300, train_perplexity=1.0006143, train_loss=0.00061414985

Batch 45310, train_perplexity=1.0006142, train_loss=0.0006139474

Batch 45320, train_perplexity=1.000614, train_loss=0.0006138908

Batch 45330, train_perplexity=1.0006139, train_loss=0.000613727

Batch 45340, train_perplexity=1.0006138, train_loss=0.00061358407

Batch 45350, train_perplexity=1.0006137, train_loss=0.00061347085

Batch 45360, train_perplexity=1.0006136, train_loss=0.0006133309

Batch 45370, train_perplexity=1.0006132, train_loss=0.00061298546

Batch 45380, train_perplexity=1.0006131, train_loss=0.0006129437

Batch 45390, train_perplexity=1.000613, train_loss=0.00061278883

Batch 45400, train_perplexity=1.0006127, train_loss=0.00061256555

Batch 45410, train_perplexity=1.0006126, train_loss=0.00061247917

Batch 45420, train_perplexity=1.0006125, train_loss=0.0006122945

Batch 45430, train_perplexity=1.0006123, train_loss=0.00061208004

Batch 45440, train_perplexity=1.0006121, train_loss=0.00061198475

Batch 45450, train_perplexity=1.000612, train_loss=0.0006118537

Batch 45460, train_perplexity=1.0006119, train_loss=0.0006117405

Batch 45470, train_perplexity=1.0006117, train_loss=0.00061152014

Batch 45480, train_perplexity=1.0006115, train_loss=0.0006113385

Batch 45490, train_perplexity=1.0006114, train_loss=0.00061128184

Batch 45500, train_perplexity=1.0006113, train_loss=0.00061113003

Batch 45510, train_perplexity=1.0006112, train_loss=0.0006109483

Batch 45520, train_perplexity=1.0006111, train_loss=0.0006108262

Batch 45530, train_perplexity=1.0006108, train_loss=0.0006106893

Batch 45540, train_perplexity=1.0006107, train_loss=0.0006105313

Batch 45550, train_perplexity=1.0006106, train_loss=0.00061035866

Batch 45560, train_perplexity=1.0006104, train_loss=0.00061016507

Batch 45570, train_perplexity=1.0006102, train_loss=0.0006100548

Batch 45580, train_perplexity=1.0006101, train_loss=0.0006098702

Batch 45590, train_perplexity=1.0006099, train_loss=0.0006096737

Batch 45600, train_perplexity=1.0006098, train_loss=0.00060956343

Batch 45610, train_perplexity=1.0006096, train_loss=0.0006094265

Batch 45620, train_perplexity=1.0006094, train_loss=0.0006092537

Batch 45630, train_perplexity=1.0006093, train_loss=0.0006091197

Batch 45640, train_perplexity=1.0006092, train_loss=0.00060896185

Batch 45650, train_perplexity=1.000609, train_loss=0.00060884864

Batch 45660, train_perplexity=1.0006088, train_loss=0.00060865213

Batch 45670, train_perplexity=1.0006087, train_loss=0.0006085241

Batch 45680, train_perplexity=1.0006084, train_loss=0.0006082739

Batch 45690, train_perplexity=1.0006084, train_loss=0.0006082739

Batch 45700, train_perplexity=1.0006082, train_loss=0.0006080743

Batch 45710, train_perplexity=1.0006081, train_loss=0.00060793734
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 45720, train_perplexity=1.000608, train_loss=0.00060773187

Batch 45730, train_perplexity=1.0006077, train_loss=0.000607571

Batch 45740, train_perplexity=1.0006076, train_loss=0.0006074668

Batch 45750, train_perplexity=1.0006074, train_loss=0.00060719275

Batch 45760, train_perplexity=1.0006074, train_loss=0.0006072195

Batch 45770, train_perplexity=1.0006073, train_loss=0.00060707075

Batch 45780, train_perplexity=1.0006071, train_loss=0.00060689193

Batch 45790, train_perplexity=1.000607, train_loss=0.0006068086

Batch 45800, train_perplexity=1.0006068, train_loss=0.00060656737

Batch 45810, train_perplexity=1.0006067, train_loss=0.00060643035

Batch 45820, train_perplexity=1.0006064, train_loss=0.00060623675

Batch 45830, train_perplexity=1.0006063, train_loss=0.0006061385

Batch 45840, train_perplexity=1.0006062, train_loss=0.0006060015

Batch 45850, train_perplexity=1.000606, train_loss=0.00060578407

Batch 45860, train_perplexity=1.0006057, train_loss=0.0006055577

Batch 45870, train_perplexity=1.0006056, train_loss=0.0006054415

Batch 45880, train_perplexity=1.0006056, train_loss=0.00060541474

Batch 45890, train_perplexity=1.0006053, train_loss=0.0006051467

Batch 45900, train_perplexity=1.0006052, train_loss=0.0006050693

Batch 45910, train_perplexity=1.0006051, train_loss=0.0006048966

Batch 45920, train_perplexity=1.0006049, train_loss=0.0006046881

Batch 45930, train_perplexity=1.0006047, train_loss=0.00060458085

Batch 45940, train_perplexity=1.0006046, train_loss=0.0006044379

Batch 45950, train_perplexity=1.0006045, train_loss=0.0006043813

Batch 45960, train_perplexity=1.0006044, train_loss=0.00060416386

Batch 45970, train_perplexity=1.0006043, train_loss=0.0006040418

Batch 45980, train_perplexity=1.0006042, train_loss=0.0006039465

Batch 45990, train_perplexity=1.0006039, train_loss=0.0006036755

Batch 46000, train_perplexity=1.0006038, train_loss=0.00060360995

Batch 46010, train_perplexity=1.0006037, train_loss=0.00060350576

Batch 46020, train_perplexity=1.0006034, train_loss=0.00060330913

Batch 46030, train_perplexity=1.0006033, train_loss=0.0006030918

Batch 46040, train_perplexity=1.0006031, train_loss=0.00060292194

Batch 46050, train_perplexity=1.0006031, train_loss=0.0006028684

Batch 46060, train_perplexity=1.0006028, train_loss=0.00060262415

Batch 46070, train_perplexity=1.0006027, train_loss=0.00060252886

Batch 46080, train_perplexity=1.0006025, train_loss=0.00060234417

Batch 46090, train_perplexity=1.0006025, train_loss=0.0006022548

Batch 46100, train_perplexity=1.0006022, train_loss=0.0006020612

Batch 46110, train_perplexity=1.0006021, train_loss=0.0006019868

Batch 46120, train_perplexity=1.0006019, train_loss=0.0006017485

Batch 46130, train_perplexity=1.0006018, train_loss=0.0006016354

Batch 46140, train_perplexity=1.0006018, train_loss=0.000601546

Batch 46150, train_perplexity=1.0006016, train_loss=0.0006014358

Batch 46160, train_perplexity=1.0006014, train_loss=0.0006012214

Batch 46170, train_perplexity=1.0006013, train_loss=0.0006011082

Batch 46180, train_perplexity=1.0006012, train_loss=0.00060100097

Batch 46190, train_perplexity=1.000601, train_loss=0.0006008432

Batch 46200, train_perplexity=1.0006008, train_loss=0.0006006585

Batch 46210, train_perplexity=1.0006006, train_loss=0.0006004232

Batch 46220, train_perplexity=1.0006006, train_loss=0.0006003994

Batch 46230, train_perplexity=1.0006003, train_loss=0.0006001582

Batch 46240, train_perplexity=1.0006002, train_loss=0.00060005987

Batch 46250, train_perplexity=1.0006, train_loss=0.0005998127

Batch 46260, train_perplexity=1.0005999, train_loss=0.00059972034

Batch 46270, train_perplexity=1.0005997, train_loss=0.00059953565

Batch 46280, train_perplexity=1.0005996, train_loss=0.0005993898

Batch 46290, train_perplexity=1.0005996, train_loss=0.0005994196

Batch 46300, train_perplexity=1.0005993, train_loss=0.000599083

Batch 46310, train_perplexity=1.0005993, train_loss=0.0005990532

Batch 46320, train_perplexity=1.0005991, train_loss=0.00059890724

Batch 46330, train_perplexity=1.0005989, train_loss=0.00059870176

Batch 46340, train_perplexity=1.0005988, train_loss=0.0005985916

Batch 46350, train_perplexity=1.0005985, train_loss=0.0005983712

Batch 46360, train_perplexity=1.0005985, train_loss=0.0005983206

Batch 46370, train_perplexity=1.0005983, train_loss=0.0005981091

Batch 46380, train_perplexity=1.0005981, train_loss=0.00059790956

Batch 46390, train_perplexity=1.000598, train_loss=0.0005978083

Batch 46400, train_perplexity=1.0005977, train_loss=0.0005975879

Batch 46410, train_perplexity=1.0005977, train_loss=0.0005975015

Batch 46420, train_perplexity=1.0005976, train_loss=0.00059738837

Batch 46430, train_perplexity=1.0005975, train_loss=0.00059725734

Batch 46440, train_perplexity=1.0005975, train_loss=0.0005972454

Batch 46450, train_perplexity=1.0005971, train_loss=0.0005969356

Batch 46460, train_perplexity=1.000597, train_loss=0.00059684634

Batch 46470, train_perplexity=1.0005969, train_loss=0.0005966438

Batch 46480, train_perplexity=1.0005968, train_loss=0.00059657823

Batch 46490, train_perplexity=1.0005966, train_loss=0.0005964056

Batch 46500, train_perplexity=1.0005964, train_loss=0.0005962745

Batch 46510, train_perplexity=1.0005963, train_loss=0.0005960839

Batch 46520, train_perplexity=1.0005962, train_loss=0.0005959558

Batch 46530, train_perplexity=1.0005959, train_loss=0.00059577415

Batch 46540, train_perplexity=1.0005957, train_loss=0.0005955657

Batch 46550, train_perplexity=1.0005956, train_loss=0.00059541676

Batch 46560, train_perplexity=1.0005956, train_loss=0.00059537205

Batch 46570, train_perplexity=1.0005955, train_loss=0.0005952469

Batch 46580, train_perplexity=1.0005952, train_loss=0.00059506233

Batch 46590, train_perplexity=1.0005951, train_loss=0.00059492234

Batch 46600, train_perplexity=1.0005949, train_loss=0.000594696

Batch 46610, train_perplexity=1.0005949, train_loss=0.00059469003

Batch 46620, train_perplexity=1.0005946, train_loss=0.0005944577

Batch 46630, train_perplexity=1.0005946, train_loss=0.0005943893

Batch 46640, train_perplexity=1.0005944, train_loss=0.0005942046

Batch 46650, train_perplexity=1.0005943, train_loss=0.0005941093

Batch 46660, train_perplexity=1.0005941, train_loss=0.00059395144

Batch 46670, train_perplexity=1.0005939, train_loss=0.00059375487

Batch 46680, train_perplexity=1.0005938, train_loss=0.00059365365

Batch 46690, train_perplexity=1.0005937, train_loss=0.0005934987

Batch 46700, train_perplexity=1.0005935, train_loss=0.0005933826

Batch 46710, train_perplexity=1.0005934, train_loss=0.0005932426

Batch 46720, train_perplexity=1.0005933, train_loss=0.0005930996

Batch 46730, train_perplexity=1.0005931, train_loss=0.00059292396

Batch 46740, train_perplexity=1.000593, train_loss=0.0005927959

Batch 46750, train_perplexity=1.0005928, train_loss=0.00059265294

Batch 46760, train_perplexity=1.0005926, train_loss=0.00059246225

Batch 46770, train_perplexity=1.0005925, train_loss=0.0005922895

Batch 46780, train_perplexity=1.0005924, train_loss=0.00059222407

Batch 46790, train_perplexity=1.0005922, train_loss=0.00059209

Batch 46800, train_perplexity=1.0005921, train_loss=0.00059193815

Batch 46810, train_perplexity=1.000592, train_loss=0.0005917803

Batch 46820, train_perplexity=1.0005919, train_loss=0.00059165515

Batch 46830, train_perplexity=1.0005916, train_loss=0.0005914854

Batch 46840, train_perplexity=1.0005914, train_loss=0.0005912769

Batch 46850, train_perplexity=1.0005913, train_loss=0.00059116085

Batch 46860, train_perplexity=1.0005912, train_loss=0.00059102976

Batch 46870, train_perplexity=1.000591, train_loss=0.00059089274

Batch 46880, train_perplexity=1.0005909, train_loss=0.00059075275

Batch 46890, train_perplexity=1.0005908, train_loss=0.0005906753

Batch 46900, train_perplexity=1.0005907, train_loss=0.0005904758

Batch 46910, train_perplexity=1.0005904, train_loss=0.00059031195

Batch 46920, train_perplexity=1.0005903, train_loss=0.0005901303

Batch 46930, train_perplexity=1.0005903, train_loss=0.0005901124

Batch 46940, train_perplexity=1.0005901, train_loss=0.0005899158

Batch 46950, train_perplexity=1.0005898, train_loss=0.00058973127
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46960, train_perplexity=1.0005897, train_loss=0.0005896002

Batch 46970, train_perplexity=1.0005896, train_loss=0.000589484

Batch 46980, train_perplexity=1.0005895, train_loss=0.000589344

Batch 46990, train_perplexity=1.0005894, train_loss=0.0005892487

Batch 47000, train_perplexity=1.0005891, train_loss=0.0005889806

Batch 47010, train_perplexity=1.000589, train_loss=0.0005888705

Batch 47020, train_perplexity=1.000589, train_loss=0.0005888258

Batch 47030, train_perplexity=1.0005888, train_loss=0.00058863225

Batch 47040, train_perplexity=1.0005887, train_loss=0.00058849517

Batch 47050, train_perplexity=1.0005885, train_loss=0.0005883522

Batch 47060, train_perplexity=1.0005883, train_loss=0.0005881557

Batch 47070, train_perplexity=1.0005882, train_loss=0.00058803055

Batch 47080, train_perplexity=1.0005882, train_loss=0.00058795017

Batch 47090, train_perplexity=1.0005879, train_loss=0.0005877268

Batch 47100, train_perplexity=1.0005877, train_loss=0.00058754813

Batch 47110, train_perplexity=1.0005877, train_loss=0.0005874975

Batch 47120, train_perplexity=1.0005876, train_loss=0.0005873664

Batch 47130, train_perplexity=1.0005875, train_loss=0.00058723544

Batch 47140, train_perplexity=1.0005873, train_loss=0.0005871162

Batch 47150, train_perplexity=1.000587, train_loss=0.0005868542

Batch 47160, train_perplexity=1.000587, train_loss=0.00058677077

Batch 47170, train_perplexity=1.0005867, train_loss=0.0005866159

Batch 47180, train_perplexity=1.0005866, train_loss=0.0005864878

Batch 47190, train_perplexity=1.0005865, train_loss=0.00058637466

Batch 47200, train_perplexity=1.0005864, train_loss=0.0005861989

Batch 47210, train_perplexity=1.0005864, train_loss=0.0005861692

Batch 47220, train_perplexity=1.0005862, train_loss=0.00058601424

Batch 47230, train_perplexity=1.000586, train_loss=0.0005858564

Batch 47240, train_perplexity=1.0005858, train_loss=0.00058563903

Batch 47250, train_perplexity=1.0005857, train_loss=0.00058550795

Batch 47260, train_perplexity=1.0005856, train_loss=0.000585356

Batch 47270, train_perplexity=1.0005854, train_loss=0.0005852161

Batch 47280, train_perplexity=1.0005853, train_loss=0.00058510294

Batch 47290, train_perplexity=1.0005851, train_loss=0.0005849123

Batch 47300, train_perplexity=1.000585, train_loss=0.00058482296

Batch 47310, train_perplexity=1.0005848, train_loss=0.0005847068

Batch 47320, train_perplexity=1.0005847, train_loss=0.0005845579

Batch 47330, train_perplexity=1.0005846, train_loss=0.00058439106

Batch 47340, train_perplexity=1.0005845, train_loss=0.0005843345

Batch 47350, train_perplexity=1.0005842, train_loss=0.00058410515

Batch 47360, train_perplexity=1.0005841, train_loss=0.00058394735

Batch 47370, train_perplexity=1.000584, train_loss=0.00058384007

Batch 47380, train_perplexity=1.0005839, train_loss=0.00058375974

Batch 47390, train_perplexity=1.0005838, train_loss=0.000583575

Batch 47400, train_perplexity=1.0005836, train_loss=0.0005834737

Batch 47410, train_perplexity=1.0005834, train_loss=0.0005832653

Batch 47420, train_perplexity=1.0005833, train_loss=0.00058309257

Batch 47430, train_perplexity=1.0005832, train_loss=0.00058303296

Batch 47440, train_perplexity=1.000583, train_loss=0.00058283046

Batch 47450, train_perplexity=1.0005828, train_loss=0.0005826756

Batch 47460, train_perplexity=1.0005828, train_loss=0.0005826219

Batch 47470, train_perplexity=1.0005827, train_loss=0.0005824879

Batch 47480, train_perplexity=1.0005826, train_loss=0.00058238674

Batch 47490, train_perplexity=1.0005823, train_loss=0.0005821693

Batch 47500, train_perplexity=1.0005822, train_loss=0.0005820114

Batch 47510, train_perplexity=1.000582, train_loss=0.00058185356

Batch 47520, train_perplexity=1.000582, train_loss=0.0005817731

Batch 47530, train_perplexity=1.0005817, train_loss=0.00058156764

Batch 47540, train_perplexity=1.0005816, train_loss=0.0005814992

Batch 47550, train_perplexity=1.0005815, train_loss=0.00058134133

Batch 47560, train_perplexity=1.0005814, train_loss=0.00058121915

Batch 47570, train_perplexity=1.0005813, train_loss=0.0005810881

Batch 47580, train_perplexity=1.0005811, train_loss=0.00058098393

Batch 47590, train_perplexity=1.000581, train_loss=0.00058082904

Batch 47600, train_perplexity=1.0005808, train_loss=0.00058056694

Batch 47610, train_perplexity=1.0005807, train_loss=0.00058055203

Batch 47620, train_perplexity=1.0005805, train_loss=0.00058034057

Batch 47630, train_perplexity=1.0005804, train_loss=0.0005802453

Batch 47640, train_perplexity=1.0005803, train_loss=0.00058009336

Batch 47650, train_perplexity=1.0005802, train_loss=0.0005800189

Batch 47660, train_perplexity=1.0005801, train_loss=0.00057988195

Batch 47670, train_perplexity=1.0005798, train_loss=0.0005796615

Batch 47680, train_perplexity=1.0005797, train_loss=0.0005795811

Batch 47690, train_perplexity=1.0005795, train_loss=0.00057935773

Batch 47700, train_perplexity=1.0005794, train_loss=0.0005792058

Batch 47710, train_perplexity=1.0005792, train_loss=0.0005791135

Batch 47720, train_perplexity=1.0005791, train_loss=0.00057895563

Batch 47730, train_perplexity=1.000579, train_loss=0.00057883945

Batch 47740, train_perplexity=1.0005789, train_loss=0.0005787412

Batch 47750, train_perplexity=1.0005788, train_loss=0.00057857146

Batch 47760, train_perplexity=1.0005785, train_loss=0.000578357

Batch 47770, train_perplexity=1.0005784, train_loss=0.0005782944

Batch 47780, train_perplexity=1.0005784, train_loss=0.00057820807

Batch 47790, train_perplexity=1.0005782, train_loss=0.0005780413

Batch 47800, train_perplexity=1.000578, train_loss=0.0005778805

Batch 47810, train_perplexity=1.0005779, train_loss=0.00057775236

Batch 47820, train_perplexity=1.0005778, train_loss=0.00057762733

Batch 47830, train_perplexity=1.0005776, train_loss=0.0005774307

Batch 47840, train_perplexity=1.0005776, train_loss=0.0005773593

Batch 47850, train_perplexity=1.0005773, train_loss=0.0005771775

Batch 47860, train_perplexity=1.0005773, train_loss=0.00057715975

Batch 47870, train_perplexity=1.0005771, train_loss=0.0005768797

Batch 47880, train_perplexity=1.0005769, train_loss=0.0005767159

Batch 47890, train_perplexity=1.0005769, train_loss=0.00057669217

Batch 47900, train_perplexity=1.0005766, train_loss=0.0005764271

Batch 47910, train_perplexity=1.0005765, train_loss=0.0005763734

Batch 47920, train_perplexity=1.0005764, train_loss=0.0005762632

Batch 47930, train_perplexity=1.0005763, train_loss=0.0005761024

Batch 47940, train_perplexity=1.0005761, train_loss=0.00057598925

Batch 47950, train_perplexity=1.0005759, train_loss=0.00057578966

Batch 47960, train_perplexity=1.0005759, train_loss=0.000575745

Batch 47970, train_perplexity=1.0005758, train_loss=0.00057556934

Batch 47980, train_perplexity=1.0005755, train_loss=0.00057542336

Batch 47990, train_perplexity=1.0005754, train_loss=0.0005752506

Batch 48000, train_perplexity=1.0005754, train_loss=0.0005752179

Batch 48010, train_perplexity=1.0005752, train_loss=0.0005749706

Batch 48020, train_perplexity=1.000575, train_loss=0.0005748217

Batch 48030, train_perplexity=1.0005748, train_loss=0.0005746996

Batch 48040, train_perplexity=1.0005747, train_loss=0.0005745656

Batch 48050, train_perplexity=1.0005746, train_loss=0.00057447026

Batch 48060, train_perplexity=1.0005744, train_loss=0.0005742052

Batch 48070, train_perplexity=1.0005744, train_loss=0.000574241

Batch 48080, train_perplexity=1.0005742, train_loss=0.00057402946

Batch 48090, train_perplexity=1.000574, train_loss=0.0005738478

Batch 48100, train_perplexity=1.0005739, train_loss=0.00057373755

Batch 48110, train_perplexity=1.0005738, train_loss=0.0005735351

Batch 48120, train_perplexity=1.0005736, train_loss=0.00057348737

Batch 48130, train_perplexity=1.0005735, train_loss=0.00057334144

Batch 48140, train_perplexity=1.0005734, train_loss=0.0005731747

Batch 48150, train_perplexity=1.0005733, train_loss=0.0005731568

Batch 48160, train_perplexity=1.0005732, train_loss=0.0005729513

Batch 48170, train_perplexity=1.000573, train_loss=0.0005728322

Batch 48180, train_perplexity=1.0005729, train_loss=0.00057272497

Batch 48190, train_perplexity=1.0005727, train_loss=0.0005724897
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48200, train_perplexity=1.0005726, train_loss=0.0005724182

Batch 48210, train_perplexity=1.0005724, train_loss=0.00057232287

Batch 48220, train_perplexity=1.0005723, train_loss=0.00057212927

Batch 48230, train_perplexity=1.0005722, train_loss=0.00057209353

Batch 48240, train_perplexity=1.0005721, train_loss=0.00057188503

Batch 48250, train_perplexity=1.0005718, train_loss=0.00057170936

Batch 48260, train_perplexity=1.0005717, train_loss=0.0005715813

Batch 48270, train_perplexity=1.0005716, train_loss=0.0005714651

Batch 48280, train_perplexity=1.0005715, train_loss=0.0005713162

Batch 48290, train_perplexity=1.0005714, train_loss=0.0005711911

Batch 48300, train_perplexity=1.0005713, train_loss=0.00057108386

Batch 48310, train_perplexity=1.000571, train_loss=0.00057090214

Batch 48320, train_perplexity=1.000571, train_loss=0.00057080993

Batch 48330, train_perplexity=1.0005708, train_loss=0.00057063415

Batch 48340, train_perplexity=1.0005708, train_loss=0.00057055376

Batch 48350, train_perplexity=1.0005707, train_loss=0.0005704584

Batch 48360, train_perplexity=1.0005705, train_loss=0.0005703184

Batch 48370, train_perplexity=1.0005703, train_loss=0.0005701427

Batch 48380, train_perplexity=1.0005702, train_loss=0.00057005044

Batch 48390, train_perplexity=1.00057, train_loss=0.0005698806

Batch 48400, train_perplexity=1.0005699, train_loss=0.00056975853

Batch 48410, train_perplexity=1.0005697, train_loss=0.00056957087

Batch 48420, train_perplexity=1.0005696, train_loss=0.00056942494

Batch 48430, train_perplexity=1.0005695, train_loss=0.00056929386

Batch 48440, train_perplexity=1.0005693, train_loss=0.0005691867

Batch 48450, train_perplexity=1.0005692, train_loss=0.0005690169

Batch 48460, train_perplexity=1.000569, train_loss=0.0005688472

Batch 48470, train_perplexity=1.0005689, train_loss=0.00056873995

Batch 48480, train_perplexity=1.0005687, train_loss=0.0005685612

Batch 48490, train_perplexity=1.0005686, train_loss=0.0005684748

Batch 48500, train_perplexity=1.0005685, train_loss=0.0005683348

Batch 48510, train_perplexity=1.0005684, train_loss=0.0005682336

Batch 48520, train_perplexity=1.0005683, train_loss=0.0005681085

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 48530, train_perplexity=1.0005682, train_loss=0.0005679595

Batch 48540, train_perplexity=1.000568, train_loss=0.0005678226

Batch 48550, train_perplexity=1.0005679, train_loss=0.0005676944

Batch 48560, train_perplexity=1.0005677, train_loss=0.000567492

Batch 48570, train_perplexity=1.0005677, train_loss=0.000567489

Batch 48580, train_perplexity=1.0005674, train_loss=0.0005672895

Batch 48590, train_perplexity=1.0005673, train_loss=0.00056720304

Batch 48600, train_perplexity=1.0005672, train_loss=0.00056698266

Batch 48610, train_perplexity=1.0005671, train_loss=0.0005668844

Batch 48620, train_perplexity=1.000567, train_loss=0.00056679203

Batch 48630, train_perplexity=1.0005667, train_loss=0.0005666163

Batch 48640, train_perplexity=1.0005667, train_loss=0.0005665061

Batch 48650, train_perplexity=1.0005666, train_loss=0.0005664376

Batch 48660, train_perplexity=1.0005665, train_loss=0.0005663214

Batch 48670, train_perplexity=1.0005662, train_loss=0.0005661338

Batch 48680, train_perplexity=1.0005661, train_loss=0.0005659551

Batch 48690, train_perplexity=1.000566, train_loss=0.000565839

Batch 48700, train_perplexity=1.0005659, train_loss=0.0005657288

Batch 48710, train_perplexity=1.0005658, train_loss=0.00056561857

Batch 48720, train_perplexity=1.0005655, train_loss=0.00056541007

Batch 48730, train_perplexity=1.0005654, train_loss=0.00056528195

Batch 48740, train_perplexity=1.0005654, train_loss=0.00056529394

Batch 48750, train_perplexity=1.0005652, train_loss=0.0005650229

Batch 48760, train_perplexity=1.000565, train_loss=0.0005648978

Batch 48770, train_perplexity=1.000565, train_loss=0.00056486507

Batch 48780, train_perplexity=1.0005648, train_loss=0.0005646536

Batch 48790, train_perplexity=1.0005647, train_loss=0.00056448084

Batch 48800, train_perplexity=1.0005646, train_loss=0.000564454

Batch 48810, train_perplexity=1.0005645, train_loss=0.0005642902

Batch 48820, train_perplexity=1.0005642, train_loss=0.0005640758

Batch 48830, train_perplexity=1.0005642, train_loss=0.00056400726

Batch 48840, train_perplexity=1.000564, train_loss=0.0005638554

Batch 48850, train_perplexity=1.0005639, train_loss=0.00056368555

Batch 48860, train_perplexity=1.0005637, train_loss=0.00056364096

Batch 48870, train_perplexity=1.0005637, train_loss=0.0005635248

Batch 48880, train_perplexity=1.0005635, train_loss=0.00056339963

Batch 48890, train_perplexity=1.0005634, train_loss=0.0005631971

Batch 48900, train_perplexity=1.0005633, train_loss=0.0005631227

Batch 48910, train_perplexity=1.000563, train_loss=0.00056289934

Batch 48920, train_perplexity=1.000563, train_loss=0.0005628249

Batch 48930, train_perplexity=1.0005628, train_loss=0.00056264916

Batch 48940, train_perplexity=1.0005628, train_loss=0.0005626194

Batch 48950, train_perplexity=1.0005627, train_loss=0.0005624585

Batch 48960, train_perplexity=1.0005624, train_loss=0.00056227087

Batch 48970, train_perplexity=1.0005623, train_loss=0.00056213984

Batch 48980, train_perplexity=1.0005623, train_loss=0.0005621428

Batch 48990, train_perplexity=1.0005621, train_loss=0.000561973

Batch 49000, train_perplexity=1.000562, train_loss=0.0005617883

Batch 49010, train_perplexity=1.0005618, train_loss=0.00056162756

Batch 49020, train_perplexity=1.0005617, train_loss=0.00056152337

Batch 49030, train_perplexity=1.0005615, train_loss=0.0005612761

Batch 49040, train_perplexity=1.0005615, train_loss=0.00056133565

Batch 49050, train_perplexity=1.0005614, train_loss=0.0005612285

Batch 49060, train_perplexity=1.000561, train_loss=0.0005609008

Batch 49070, train_perplexity=1.000561, train_loss=0.0005608591

Batch 49080, train_perplexity=1.0005609, train_loss=0.0005607697

Batch 49090, train_perplexity=1.0005608, train_loss=0.00056055834

Batch 49100, train_perplexity=1.0005608, train_loss=0.0005605583

Batch 49110, train_perplexity=1.0005605, train_loss=0.0005603618

Batch 49120, train_perplexity=1.0005604, train_loss=0.00056024257

Batch 49130, train_perplexity=1.0005603, train_loss=0.00056015316

Batch 49140, train_perplexity=1.0005602, train_loss=0.00056001026

Batch 49150, train_perplexity=1.00056, train_loss=0.0005598315

Batch 49160, train_perplexity=1.0005599, train_loss=0.00055973325

Batch 49170, train_perplexity=1.0005597, train_loss=0.0005595813

Batch 49180, train_perplexity=1.0005597, train_loss=0.0005594831

Batch 49190, train_perplexity=1.0005594, train_loss=0.0005593014

Batch 49200, train_perplexity=1.0005593, train_loss=0.0005591853

Batch 49210, train_perplexity=1.0005593, train_loss=0.00055911974

Batch 49220, train_perplexity=1.0005591, train_loss=0.000558944

Batch 49230, train_perplexity=1.0005591, train_loss=0.000558938

Batch 49240, train_perplexity=1.0005589, train_loss=0.0005587295

Batch 49250, train_perplexity=1.0005587, train_loss=0.0005585747

Batch 49260, train_perplexity=1.0005587, train_loss=0.00055853894

Batch 49270, train_perplexity=1.0005584, train_loss=0.00055825594

Batch 49280, train_perplexity=1.0005583, train_loss=0.000558119

Batch 49290, train_perplexity=1.0005583, train_loss=0.00055809517

Batch 49300, train_perplexity=1.0005581, train_loss=0.0005579939

Batch 49310, train_perplexity=1.0005579, train_loss=0.0005577378

Batch 49320, train_perplexity=1.0005578, train_loss=0.0005576365

Batch 49330, train_perplexity=1.0005577, train_loss=0.00055755605

Batch 49340, train_perplexity=1.0005577, train_loss=0.0005574905

Batch 49350, train_perplexity=1.0005574, train_loss=0.00055728504

Batch 49360, train_perplexity=1.0005573, train_loss=0.00055715104

Batch 49370, train_perplexity=1.0005571, train_loss=0.0005569395
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 49380, train_perplexity=1.0005572, train_loss=0.0005570259

Batch 49390, train_perplexity=1.000557, train_loss=0.00055675785

Batch 49400, train_perplexity=1.0005568, train_loss=0.0005566447

Batch 49410, train_perplexity=1.0005566, train_loss=0.000556466

Batch 49420, train_perplexity=1.0005565, train_loss=0.00055633194

Batch 49430, train_perplexity=1.0005563, train_loss=0.0005562366

Batch 49440, train_perplexity=1.0005562, train_loss=0.00055612647

Batch 49450, train_perplexity=1.0005561, train_loss=0.00055599835

Batch 49460, train_perplexity=1.0005561, train_loss=0.0005559716

Batch 49470, train_perplexity=1.0005559, train_loss=0.0005557601

Batch 49480, train_perplexity=1.0005558, train_loss=0.0005555725

Batch 49490, train_perplexity=1.0005556, train_loss=0.00055550993

Batch 49500, train_perplexity=1.0005556, train_loss=0.0005554354

Batch 49510, train_perplexity=1.0005555, train_loss=0.0005553074

Batch 49520, train_perplexity=1.0005553, train_loss=0.000555081

Batch 49530, train_perplexity=1.0005552, train_loss=0.00055497675

Batch 49540, train_perplexity=1.000555, train_loss=0.00055487547

Batch 49550, train_perplexity=1.0005549, train_loss=0.0005547802

Batch 49560, train_perplexity=1.0005547, train_loss=0.0005545241

Batch 49570, train_perplexity=1.0005547, train_loss=0.0005545062

Batch 49580, train_perplexity=1.0005546, train_loss=0.00055445556

Batch 49590, train_perplexity=1.0005543, train_loss=0.0005542113

Batch 49600, train_perplexity=1.0005543, train_loss=0.0005541637

Batch 49610, train_perplexity=1.0005542, train_loss=0.0005541071

Batch 49620, train_perplexity=1.0005538, train_loss=0.00055374665

Batch 49630, train_perplexity=1.0005538, train_loss=0.00055371097

Batch 49640, train_perplexity=1.0005537, train_loss=0.00055361865

Batch 49650, train_perplexity=1.0005536, train_loss=0.0005534906

Batch 49660, train_perplexity=1.0005535, train_loss=0.00055328204

Batch 49670, train_perplexity=1.0005534, train_loss=0.0005532195

Batch 49680, train_perplexity=1.0005533, train_loss=0.0005530467

Batch 49690, train_perplexity=1.0005531, train_loss=0.0005529395

Batch 49700, train_perplexity=1.000553, train_loss=0.00055290083

Batch 49710, train_perplexity=1.0005529, train_loss=0.00055272505

Batch 49720, train_perplexity=1.0005527, train_loss=0.00055249874

Batch 49730, train_perplexity=1.0005527, train_loss=0.0005524808

Batch 49740, train_perplexity=1.0005524, train_loss=0.00055229914

Batch 49750, train_perplexity=1.0005524, train_loss=0.0005522098

Batch 49760, train_perplexity=1.0005523, train_loss=0.0005521056

Batch 49770, train_perplexity=1.000552, train_loss=0.00055193575

Batch 49780, train_perplexity=1.000552, train_loss=0.0005518941

Batch 49790, train_perplexity=1.0005518, train_loss=0.00055169757

Batch 49800, train_perplexity=1.0005517, train_loss=0.0005515933

Batch 49810, train_perplexity=1.0005516, train_loss=0.0005514712

Batch 49820, train_perplexity=1.0005515, train_loss=0.0005512865

Batch 49830, train_perplexity=1.0005512, train_loss=0.00055110484

Batch 49840, train_perplexity=1.0005513, train_loss=0.0005511346

Batch 49850, train_perplexity=1.0005512, train_loss=0.0005510155

Batch 49860, train_perplexity=1.0005511, train_loss=0.0005509261

Batch 49870, train_perplexity=1.0005509, train_loss=0.0005507682

Batch 49880, train_perplexity=1.0005507, train_loss=0.00055055384

Batch 49890, train_perplexity=1.0005506, train_loss=0.0005504406

Batch 49900, train_perplexity=1.0005505, train_loss=0.0005503454

Batch 49910, train_perplexity=1.0005504, train_loss=0.0005502172

Batch 49920, train_perplexity=1.0005502, train_loss=0.00055005046

Batch 49930, train_perplexity=1.0005502, train_loss=0.0005500058

Batch 49940, train_perplexity=1.00055, train_loss=0.00054991647

Batch 49950, train_perplexity=1.0005499, train_loss=0.00054971396

Batch 49960, train_perplexity=1.0005498, train_loss=0.00054959184

Batch 49970, train_perplexity=1.0005496, train_loss=0.00054941606

Batch 49980, train_perplexity=1.0005496, train_loss=0.00054937135

Batch 49990, train_perplexity=1.0005493, train_loss=0.0005491808

Batch 50000, train_perplexity=1.0005493, train_loss=0.0005492046

Batch 50010, train_perplexity=1.0005491, train_loss=0.0005489246

Batch 50020, train_perplexity=1.000549, train_loss=0.00054885016

Batch 50030, train_perplexity=1.000549, train_loss=0.00054881145

Batch 50040, train_perplexity=1.0005488, train_loss=0.0005486298

Batch 50050, train_perplexity=1.0005486, train_loss=0.0005483975

Batch 50060, train_perplexity=1.0005485, train_loss=0.0005483439

Batch 50070, train_perplexity=1.0005484, train_loss=0.0005482456

Batch 50080, train_perplexity=1.0005482, train_loss=0.0005481234

Batch 50090, train_perplexity=1.0005481, train_loss=0.0005480251

Batch 50100, train_perplexity=1.0005481, train_loss=0.0005479269

Batch 50110, train_perplexity=1.0005479, train_loss=0.0005476916

Batch 50120, train_perplexity=1.0005476, train_loss=0.000547492

Batch 50130, train_perplexity=1.0005476, train_loss=0.0005475278

Batch 50140, train_perplexity=1.0005474, train_loss=0.0005473163

Batch 50150, train_perplexity=1.0005474, train_loss=0.00054720603

Batch 50160, train_perplexity=1.0005473, train_loss=0.00054712564

Batch 50170, train_perplexity=1.0005473, train_loss=0.00054710184

Batch 50180, train_perplexity=1.000547, train_loss=0.0005469112

Batch 50190, train_perplexity=1.0005469, train_loss=0.0005467861

Batch 50200, train_perplexity=1.0005468, train_loss=0.000546664

Batch 50210, train_perplexity=1.0005467, train_loss=0.0005465895

Batch 50220, train_perplexity=1.0005465, train_loss=0.00054630055

Batch 50230, train_perplexity=1.0005463, train_loss=0.0005462053

Batch 50240, train_perplexity=1.0005462, train_loss=0.00054611

Batch 50250, train_perplexity=1.0005461, train_loss=0.000545973

Batch 50260, train_perplexity=1.0005462, train_loss=0.00054602063

Batch 50270, train_perplexity=1.000546, train_loss=0.0005457734

Batch 50280, train_perplexity=1.0005457, train_loss=0.0005456364

Batch 50290, train_perplexity=1.0005457, train_loss=0.00054556795

Batch 50300, train_perplexity=1.0005455, train_loss=0.00054539216

Batch 50310, train_perplexity=1.0005454, train_loss=0.00054526713

Batch 50320, train_perplexity=1.0005453, train_loss=0.0005450914

Batch 50330, train_perplexity=1.0005451, train_loss=0.00054500205

Batch 50340, train_perplexity=1.000545, train_loss=0.00054489484

Batch 50350, train_perplexity=1.0005449, train_loss=0.0005447548

Batch 50360, train_perplexity=1.0005449, train_loss=0.000544728

Batch 50370, train_perplexity=1.0005447, train_loss=0.0005445314

Batch 50380, train_perplexity=1.0005447, train_loss=0.00054449274

Batch 50390, train_perplexity=1.0005444, train_loss=0.00054427824

Batch 50400, train_perplexity=1.0005443, train_loss=0.00054415315

Batch 50410, train_perplexity=1.0005442, train_loss=0.00054401613

Batch 50420, train_perplexity=1.0005441, train_loss=0.00054394465

Batch 50430, train_perplexity=1.000544, train_loss=0.0005437868

Batch 50440, train_perplexity=1.0005438, train_loss=0.0005436826

Batch 50450, train_perplexity=1.0005438, train_loss=0.00054362894

Batch 50460, train_perplexity=1.0005436, train_loss=0.00054346217

Batch 50470, train_perplexity=1.0005436, train_loss=0.0005434383

Batch 50480, train_perplexity=1.0005434, train_loss=0.0005431792

Batch 50490, train_perplexity=1.0005431, train_loss=0.00054300646

Batch 50500, train_perplexity=1.0005431, train_loss=0.000542926

Batch 50510, train_perplexity=1.000543, train_loss=0.0005428635

Batch 50520, train_perplexity=1.0005429, train_loss=0.00054275326

Batch 50530, train_perplexity=1.0005428, train_loss=0.00054257456

Batch 50540, train_perplexity=1.0005425, train_loss=0.0005424138

Batch 50550, train_perplexity=1.0005425, train_loss=0.0005423512

Batch 50560, train_perplexity=1.0005424, train_loss=0.0005422946

Batch 50570, train_perplexity=1.0005423, train_loss=0.0005421308

Batch 50580, train_perplexity=1.0005422, train_loss=0.000541967

Batch 50590, train_perplexity=1.0005422, train_loss=0.00054205634

Batch 50600, train_perplexity=1.0005419, train_loss=0.0005417793

Batch 50610, train_perplexity=1.0005417, train_loss=0.00054154405
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 50620, train_perplexity=1.0005417, train_loss=0.00054149044

Batch 50630, train_perplexity=1.0005416, train_loss=0.0005414279

Batch 50640, train_perplexity=1.0005414, train_loss=0.0005413117

Batch 50650, train_perplexity=1.0005413, train_loss=0.00054123136

Batch 50660, train_perplexity=1.0005412, train_loss=0.00054103177

Batch 50670, train_perplexity=1.000541, train_loss=0.00054085004

Batch 50680, train_perplexity=1.000541, train_loss=0.00054079643

Batch 50690, train_perplexity=1.0005409, train_loss=0.0005406654

Batch 50700, train_perplexity=1.0005406, train_loss=0.0005405046

Batch 50710, train_perplexity=1.0005407, train_loss=0.00054057303

Batch 50720, train_perplexity=1.0005405, train_loss=0.00054031395

Batch 50730, train_perplexity=1.0005404, train_loss=0.00054018584

Batch 50740, train_perplexity=1.0005403, train_loss=0.0005400786

Batch 50750, train_perplexity=1.0005401, train_loss=0.0005399833

Batch 50760, train_perplexity=1.00054, train_loss=0.000539894

Batch 50770, train_perplexity=1.0005399, train_loss=0.00053978374

Batch 50780, train_perplexity=1.0005397, train_loss=0.0005395604

Batch 50790, train_perplexity=1.0005397, train_loss=0.0005394591

Batch 50800, train_perplexity=1.0005394, train_loss=0.0005393191

Batch 50810, train_perplexity=1.0005394, train_loss=0.00053923874

Batch 50820, train_perplexity=1.0005393, train_loss=0.00053913146

Batch 50830, train_perplexity=1.0005392, train_loss=0.0005390034

Batch 50840, train_perplexity=1.000539, train_loss=0.0005388486

Batch 50850, train_perplexity=1.0005388, train_loss=0.0005386996

Batch 50860, train_perplexity=1.0005388, train_loss=0.000538655

Batch 50870, train_perplexity=1.0005386, train_loss=0.0005384673

Batch 50880, train_perplexity=1.0005386, train_loss=0.00053842855

Batch 50890, train_perplexity=1.0005383, train_loss=0.0005382439

Batch 50900, train_perplexity=1.0005383, train_loss=0.00053816056

Batch 50910, train_perplexity=1.0005382, train_loss=0.000538095

Batch 50920, train_perplexity=1.000538, train_loss=0.0005378567

Batch 50930, train_perplexity=1.000538, train_loss=0.000537812

Batch 50940, train_perplexity=1.0005379, train_loss=0.0005376899

Batch 50950, train_perplexity=1.0005376, train_loss=0.00053754397

Batch 50960, train_perplexity=1.0005376, train_loss=0.00053744565

Batch 50970, train_perplexity=1.0005375, train_loss=0.00053735636

Batch 50980, train_perplexity=1.0005374, train_loss=0.0005372223

Batch 50990, train_perplexity=1.0005373, train_loss=0.0005371329

Batch 51000, train_perplexity=1.0005372, train_loss=0.0005370287

Batch 51010, train_perplexity=1.0005369, train_loss=0.0005367517

Batch 51020, train_perplexity=1.0005368, train_loss=0.0005366445

Batch 51030, train_perplexity=1.0005368, train_loss=0.0005366296

Batch 51040, train_perplexity=1.0005367, train_loss=0.0005365134

Batch 51050, train_perplexity=1.0005364, train_loss=0.00053634064

Batch 51060, train_perplexity=1.0005363, train_loss=0.00053620664

Batch 51070, train_perplexity=1.0005363, train_loss=0.00053617684

Batch 51080, train_perplexity=1.0005361, train_loss=0.00053599523

Batch 51090, train_perplexity=1.0005361, train_loss=0.00053595053

Batch 51100, train_perplexity=1.000536, train_loss=0.0005357837

Batch 51110, train_perplexity=1.000536, train_loss=0.0005357688

Batch 51120, train_perplexity=1.0005356, train_loss=0.0005354829

Batch 51130, train_perplexity=1.0005356, train_loss=0.000535468

Batch 51140, train_perplexity=1.0005355, train_loss=0.00053533394

Batch 51150, train_perplexity=1.0005354, train_loss=0.0005352148

Batch 51160, train_perplexity=1.0005352, train_loss=0.00053510757

Batch 51170, train_perplexity=1.0005351, train_loss=0.0005349795

Batch 51180, train_perplexity=1.000535, train_loss=0.0005348901

Batch 51190, train_perplexity=1.0005349, train_loss=0.0005347561

Batch 51200, train_perplexity=1.0005348, train_loss=0.0005346102

Batch 51210, train_perplexity=1.0005347, train_loss=0.00053445826

Batch 51220, train_perplexity=1.0005345, train_loss=0.0005343987

Batch 51230, train_perplexity=1.0005343, train_loss=0.0005341754

Batch 51240, train_perplexity=1.0005343, train_loss=0.0005341366

Batch 51250, train_perplexity=1.000534, train_loss=0.0005339519

Batch 51260, train_perplexity=1.0005339, train_loss=0.0005338268

Batch 51270, train_perplexity=1.000534, train_loss=0.0005338745

Batch 51280, train_perplexity=1.0005338, train_loss=0.0005336958

Batch 51290, train_perplexity=1.0005337, train_loss=0.0005336035

Batch 51300, train_perplexity=1.0005336, train_loss=0.0005334575

Batch 51310, train_perplexity=1.0005335, train_loss=0.00053335325

Batch 51320, train_perplexity=1.0005335, train_loss=0.00053328474

Batch 51330, train_perplexity=1.0005332, train_loss=0.00053311803

Batch 51340, train_perplexity=1.0005331, train_loss=0.000532978

Batch 51350, train_perplexity=1.000533, train_loss=0.00053281116

Batch 51360, train_perplexity=1.0005329, train_loss=0.0005327456

Batch 51370, train_perplexity=1.0005327, train_loss=0.00053265627

Batch 51380, train_perplexity=1.0005326, train_loss=0.00053254014

Batch 51390, train_perplexity=1.0005325, train_loss=0.0005323972

Batch 51400, train_perplexity=1.0005325, train_loss=0.0005323674

Batch 51410, train_perplexity=1.0005323, train_loss=0.0005321142

Batch 51420, train_perplexity=1.0005322, train_loss=0.0005319712

Batch 51430, train_perplexity=1.0005319, train_loss=0.0005317776

Batch 51440, train_perplexity=1.0005319, train_loss=0.0005317627

Batch 51450, train_perplexity=1.0005318, train_loss=0.0005317002

Batch 51460, train_perplexity=1.0005317, train_loss=0.00053154235

Batch 51470, train_perplexity=1.0005316, train_loss=0.0005314649

Batch 51480, train_perplexity=1.0005316, train_loss=0.0005313726

Batch 51490, train_perplexity=1.0005313, train_loss=0.0005312207

Batch 51500, train_perplexity=1.0005311, train_loss=0.0005309704

Batch 51510, train_perplexity=1.0005311, train_loss=0.00053090195

Batch 51520, train_perplexity=1.000531, train_loss=0.0005308632

Batch 51530, train_perplexity=1.000531, train_loss=0.00053083943

Batch 51540, train_perplexity=1.0005307, train_loss=0.00053060113

Batch 51550, train_perplexity=1.0005306, train_loss=0.0005304522

Batch 51560, train_perplexity=1.0005306, train_loss=0.0005305118

Batch 51570, train_perplexity=1.0005302, train_loss=0.00053016027

Batch 51580, train_perplexity=1.0005304, train_loss=0.0005301811

Batch 51590, train_perplexity=1.0005301, train_loss=0.0005299846

Batch 51600, train_perplexity=1.00053, train_loss=0.00052988634

Batch 51610, train_perplexity=1.00053, train_loss=0.0005299101

Batch 51620, train_perplexity=1.0005298, train_loss=0.00052963605

Batch 51630, train_perplexity=1.0005298, train_loss=0.00052962714

Batch 51640, train_perplexity=1.0005296, train_loss=0.00052954076

Batch 51650, train_perplexity=1.0005294, train_loss=0.0005292549

Batch 51660, train_perplexity=1.0005294, train_loss=0.00052926084

Batch 51670, train_perplexity=1.0005293, train_loss=0.00052918633

Batch 51680, train_perplexity=1.0005292, train_loss=0.00052899873

Batch 51690, train_perplexity=1.0005289, train_loss=0.0005288319

Batch 51700, train_perplexity=1.0005289, train_loss=0.00052882

Batch 51710, train_perplexity=1.0005287, train_loss=0.0005285817

Batch 51720, train_perplexity=1.0005286, train_loss=0.0005284834

Batch 51730, train_perplexity=1.0005286, train_loss=0.0005284894

Batch 51740, train_perplexity=1.0005286, train_loss=0.0005284477

Batch 51750, train_perplexity=1.0005283, train_loss=0.00052817067

Batch 51760, train_perplexity=1.0005282, train_loss=0.0005280664

Batch 51770, train_perplexity=1.0005281, train_loss=0.00052798307

Batch 51780, train_perplexity=1.000528, train_loss=0.000527852

Batch 51790, train_perplexity=1.0005277, train_loss=0.0005275809

Batch 51800, train_perplexity=1.0005279, train_loss=0.00052767026

Batch 51810, train_perplexity=1.0005276, train_loss=0.0005275303

Batch 51820, train_perplexity=1.0005275, train_loss=0.00052736053

Batch 51830, train_perplexity=1.0005274, train_loss=0.0005272622

Batch 51840, train_perplexity=1.0005273, train_loss=0.00052717887

Batch 51850, train_perplexity=1.0005273, train_loss=0.00052708946
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 51860, train_perplexity=1.0005271, train_loss=0.0005269643

Batch 51870, train_perplexity=1.0005269, train_loss=0.00052676484

Batch 51880, train_perplexity=1.0005268, train_loss=0.0005266904

Batch 51890, train_perplexity=1.0005267, train_loss=0.00052655634

Batch 51900, train_perplexity=1.0005267, train_loss=0.00052653253

Batch 51910, train_perplexity=1.0005265, train_loss=0.00052635674

Batch 51920, train_perplexity=1.0005263, train_loss=0.0005261751

Batch 51930, train_perplexity=1.0005263, train_loss=0.0005261274

Batch 51940, train_perplexity=1.0005262, train_loss=0.0005260024

Batch 51950, train_perplexity=1.0005261, train_loss=0.00052588317

Batch 51960, train_perplexity=1.0005258, train_loss=0.0005257402

Batch 51970, train_perplexity=1.0005258, train_loss=0.00052568363

Batch 51980, train_perplexity=1.0005258, train_loss=0.0005256389

Batch 51990, train_perplexity=1.0005256, train_loss=0.00052548107

Batch 52000, train_perplexity=1.0005255, train_loss=0.0005253262

Batch 52010, train_perplexity=1.0005254, train_loss=0.0005252309

Batch 52020, train_perplexity=1.0005252, train_loss=0.00052513555

Batch 52030, train_perplexity=1.0005251, train_loss=0.0005249658

Batch 52040, train_perplexity=1.0005249, train_loss=0.0005247841

Batch 52050, train_perplexity=1.0005249, train_loss=0.0005247543

Batch 52060, train_perplexity=1.0005249, train_loss=0.00052470074

Batch 52070, train_perplexity=1.0005246, train_loss=0.00052445946

Batch 52080, train_perplexity=1.0005246, train_loss=0.00052447437

Batch 52090, train_perplexity=1.0005244, train_loss=0.00052423903

Batch 52100, train_perplexity=1.0005243, train_loss=0.00052415265

Batch 52110, train_perplexity=1.0005243, train_loss=0.000524108

Batch 52120, train_perplexity=1.0005242, train_loss=0.000523968

Batch 52130, train_perplexity=1.0005239, train_loss=0.00052383397

Batch 52140, train_perplexity=1.0005238, train_loss=0.000523697

Batch 52150, train_perplexity=1.0005237, train_loss=0.00052359875

Batch 52160, train_perplexity=1.0005237, train_loss=0.00052350934

Batch 52170, train_perplexity=1.0005236, train_loss=0.0005233872

Batch 52180, train_perplexity=1.0005234, train_loss=0.0005232651

Batch 52190, train_perplexity=1.0005233, train_loss=0.0005231936

Batch 52200, train_perplexity=1.0005232, train_loss=0.0005230596

Batch 52210, train_perplexity=1.0005231, train_loss=0.0005229106

Batch 52220, train_perplexity=1.000523, train_loss=0.0005228213

Batch 52230, train_perplexity=1.0005229, train_loss=0.00052266935

Batch 52240, train_perplexity=1.0005227, train_loss=0.0005226128

Batch 52250, train_perplexity=1.0005226, train_loss=0.0005224639

Batch 52260, train_perplexity=1.0005225, train_loss=0.00052234175

Batch 52270, train_perplexity=1.0005224, train_loss=0.0005222405

Batch 52280, train_perplexity=1.0005223, train_loss=0.00052209455

Batch 52290, train_perplexity=1.0005223, train_loss=0.0005220677

Batch 52300, train_perplexity=1.000522, train_loss=0.0005218771

Batch 52310, train_perplexity=1.0005219, train_loss=0.0005217133

Batch 52320, train_perplexity=1.0005219, train_loss=0.00052172213

Batch 52330, train_perplexity=1.0005217, train_loss=0.00052155246

Batch 52340, train_perplexity=1.0005215, train_loss=0.0005214184

Batch 52350, train_perplexity=1.0005214, train_loss=0.0005212814

Batch 52360, train_perplexity=1.0005213, train_loss=0.00052113243

Batch 52370, train_perplexity=1.0005212, train_loss=0.0005210788

Batch 52380, train_perplexity=1.0005211, train_loss=0.0005209865

Batch 52390, train_perplexity=1.0005211, train_loss=0.00052096264

Batch 52400, train_perplexity=1.0005208, train_loss=0.00052068866

Batch 52410, train_perplexity=1.0005208, train_loss=0.000520638

Batch 52420, train_perplexity=1.0005207, train_loss=0.00052054273

Batch 52430, train_perplexity=1.0005205, train_loss=0.0005203908

Batch 52440, train_perplexity=1.0005205, train_loss=0.00052036403

Batch 52450, train_perplexity=1.0005203, train_loss=0.0005201674

Batch 52460, train_perplexity=1.0005202, train_loss=0.0005201227

Batch 52470, train_perplexity=1.0005201, train_loss=0.00051997683

Batch 52480, train_perplexity=1.0005201, train_loss=0.00051995

Batch 52490, train_perplexity=1.0005199, train_loss=0.0005197326

Batch 52500, train_perplexity=1.0005199, train_loss=0.00051968195

Batch 52510, train_perplexity=1.0005196, train_loss=0.0005195568

Batch 52520, train_perplexity=1.0005195, train_loss=0.0005194258

Batch 52530, train_perplexity=1.0005194, train_loss=0.00051930966

Batch 52540, train_perplexity=1.0005193, train_loss=0.0005191637

Batch 52550, train_perplexity=1.0005193, train_loss=0.00051911006

Batch 52560, train_perplexity=1.000519, train_loss=0.0005189135

Batch 52570, train_perplexity=1.000519, train_loss=0.0005188926

Batch 52580, train_perplexity=1.0005188, train_loss=0.00051870197

Batch 52590, train_perplexity=1.0005188, train_loss=0.00051862746

Batch 52600, train_perplexity=1.0005187, train_loss=0.00051854114

Batch 52610, train_perplexity=1.0005186, train_loss=0.0005183744

Batch 52620, train_perplexity=1.0005183, train_loss=0.00051823433

Batch 52630, train_perplexity=1.0005183, train_loss=0.0005181807

Batch 52640, train_perplexity=1.0005182, train_loss=0.00051809737

Batch 52650, train_perplexity=1.0005181, train_loss=0.00051795435

Batch 52660, train_perplexity=1.000518, train_loss=0.0005178263

Batch 52670, train_perplexity=1.0005178, train_loss=0.0005177667

Batch 52680, train_perplexity=1.0005177, train_loss=0.00051761186

Batch 52690, train_perplexity=1.0005177, train_loss=0.00051754934

Batch 52700, train_perplexity=1.0005176, train_loss=0.00051741523

Batch 52710, train_perplexity=1.0005174, train_loss=0.0005172484

Batch 52720, train_perplexity=1.0005174, train_loss=0.00051723956

Batch 52730, train_perplexity=1.0005172, train_loss=0.0005170638

Batch 52740, train_perplexity=1.0005171, train_loss=0.00051698636

Batch 52750, train_perplexity=1.000517, train_loss=0.0005169059

Batch 52760, train_perplexity=1.0005169, train_loss=0.000516757

Batch 52770, train_perplexity=1.0005168, train_loss=0.0005165902

Batch 52780, train_perplexity=1.0005167, train_loss=0.0005165336

Batch 52790, train_perplexity=1.0005165, train_loss=0.00051638763

Batch 52800, train_perplexity=1.0005164, train_loss=0.00051623874

Batch 52810, train_perplexity=1.0005163, train_loss=0.00051616726

Batch 52820, train_perplexity=1.000516, train_loss=0.00051597366

Batch 52830, train_perplexity=1.000516, train_loss=0.0005159498

Batch 52840, train_perplexity=1.0005159, train_loss=0.0005158158

Batch 52850, train_perplexity=1.0005158, train_loss=0.000515655

Batch 52860, train_perplexity=1.0005157, train_loss=0.0005155626

Batch 52870, train_perplexity=1.0005156, train_loss=0.0005154852

Batch 52880, train_perplexity=1.0005156, train_loss=0.00051540777

Batch 52890, train_perplexity=1.0005155, train_loss=0.0005152826

Batch 52900, train_perplexity=1.0005153, train_loss=0.0005151933

Batch 52910, train_perplexity=1.0005152, train_loss=0.0005151039

Batch 52920, train_perplexity=1.000515, train_loss=0.00051488355

Batch 52930, train_perplexity=1.000515, train_loss=0.0005148329

Batch 52940, train_perplexity=1.0005149, train_loss=0.0005147197

Batch 52950, train_perplexity=1.0005147, train_loss=0.00051459763

Batch 52960, train_perplexity=1.0005147, train_loss=0.0005145916

Batch 52970, train_perplexity=1.0005146, train_loss=0.0005144606

Batch 52980, train_perplexity=1.0005144, train_loss=0.00051423715

Batch 52990, train_perplexity=1.0005143, train_loss=0.0005141955

Batch 53000, train_perplexity=1.0005143, train_loss=0.0005140912

Batch 53010, train_perplexity=1.000514, train_loss=0.0005138976

Batch 53020, train_perplexity=1.000514, train_loss=0.00051385886

Batch 53030, train_perplexity=1.0005139, train_loss=0.00051372487

Batch 53040, train_perplexity=1.0005138, train_loss=0.0005136236

Batch 53050, train_perplexity=1.0005136, train_loss=0.0005134687

Batch 53060, train_perplexity=1.0005136, train_loss=0.0005133883

Batch 53070, train_perplexity=1.0005134, train_loss=0.0005133108

Batch 53080, train_perplexity=1.0005132, train_loss=0.0005131023

Batch 53090, train_perplexity=1.0005132, train_loss=0.0005131053
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53100, train_perplexity=1.0005131, train_loss=0.0005129088

Batch 53110, train_perplexity=1.0005131, train_loss=0.00051290274

Batch 53120, train_perplexity=1.0005128, train_loss=0.0005126913

Batch 53130, train_perplexity=1.0005127, train_loss=0.00051262573

Batch 53140, train_perplexity=1.0005127, train_loss=0.0005125633

Batch 53150, train_perplexity=1.0005126, train_loss=0.0005124381

Batch 53160, train_perplexity=1.0005125, train_loss=0.0005123249

Batch 53170, train_perplexity=1.0005124, train_loss=0.0005122832

Batch 53180, train_perplexity=1.0005122, train_loss=0.00051205384

Batch 53190, train_perplexity=1.0005121, train_loss=0.0005119675

Batch 53200, train_perplexity=1.0005121, train_loss=0.0005119466

Batch 53210, train_perplexity=1.0005119, train_loss=0.0005117381

Batch 53220, train_perplexity=1.0005118, train_loss=0.00051168754

Batch 53230, train_perplexity=1.0005118, train_loss=0.000511628

Batch 53240, train_perplexity=1.0005116, train_loss=0.0005114701

Batch 53250, train_perplexity=1.0005114, train_loss=0.0005113033

Batch 53260, train_perplexity=1.0005113, train_loss=0.0005111365

Batch 53270, train_perplexity=1.0005112, train_loss=0.0005110978

Batch 53280, train_perplexity=1.000511, train_loss=0.0005109548

Batch 53290, train_perplexity=1.0005109, train_loss=0.0005108565

Batch 53300, train_perplexity=1.0005109, train_loss=0.00051082077

Batch 53310, train_perplexity=1.0005108, train_loss=0.0005106956

Batch 53320, train_perplexity=1.0005107, train_loss=0.00051051995

Batch 53330, train_perplexity=1.0005106, train_loss=0.00051042764

Batch 53340, train_perplexity=1.0005105, train_loss=0.0005103412

Batch 53350, train_perplexity=1.0005105, train_loss=0.0005102876

Batch 53360, train_perplexity=1.0005102, train_loss=0.0005101208

Batch 53370, train_perplexity=1.0005102, train_loss=0.00051007915

Batch 53380, train_perplexity=1.0005101, train_loss=0.0005099212

Batch 53390, train_perplexity=1.00051, train_loss=0.00050979614

Batch 53400, train_perplexity=1.0005099, train_loss=0.00050974253

Batch 53410, train_perplexity=1.0005097, train_loss=0.0005096055

Batch 53420, train_perplexity=1.0005096, train_loss=0.00050944765

Batch 53430, train_perplexity=1.0005095, train_loss=0.000509385

Batch 53440, train_perplexity=1.0005094, train_loss=0.0005092451

Batch 53450, train_perplexity=1.0005094, train_loss=0.0005092422

Batch 53460, train_perplexity=1.0005091, train_loss=0.0005090276

Batch 53470, train_perplexity=1.000509, train_loss=0.00050889957

Batch 53480, train_perplexity=1.0005089, train_loss=0.00050879235

Batch 53490, train_perplexity=1.0005088, train_loss=0.0005086881

Batch 53500, train_perplexity=1.0005088, train_loss=0.000508697

Batch 53510, train_perplexity=1.0005087, train_loss=0.00050848257

Batch 53520, train_perplexity=1.0005085, train_loss=0.00050847663

Batch 53530, train_perplexity=1.0005084, train_loss=0.00050831283

Batch 53540, train_perplexity=1.0005082, train_loss=0.00050807453

Batch 53550, train_perplexity=1.0005082, train_loss=0.0005081162

Batch 53560, train_perplexity=1.0005081, train_loss=0.0005079405

Batch 53570, train_perplexity=1.0005081, train_loss=0.0005079316

Batch 53580, train_perplexity=1.0005078, train_loss=0.0005077558

Batch 53590, train_perplexity=1.0005077, train_loss=0.0005076069

Batch 53600, train_perplexity=1.0005076, train_loss=0.00050751155

Batch 53610, train_perplexity=1.0005076, train_loss=0.00050748774

Batch 53620, train_perplexity=1.0005075, train_loss=0.00050731795

Batch 53630, train_perplexity=1.0005074, train_loss=0.00050724344

Batch 53640, train_perplexity=1.0005072, train_loss=0.0005071035

Batch 53650, train_perplexity=1.0005072, train_loss=0.0005070857

Batch 53660, train_perplexity=1.000507, train_loss=0.0005068891

Batch 53670, train_perplexity=1.000507, train_loss=0.00050682353

Batch 53680, train_perplexity=1.0005068, train_loss=0.0005066656

Batch 53690, train_perplexity=1.0005068, train_loss=0.0005066389

Batch 53700, train_perplexity=1.0005066, train_loss=0.0005065376

Batch 53710, train_perplexity=1.0005065, train_loss=0.00050639757

Batch 53720, train_perplexity=1.0005064, train_loss=0.0005063171

Batch 53730, train_perplexity=1.0005063, train_loss=0.00050614437

Batch 53740, train_perplexity=1.0005062, train_loss=0.0005059925

Batch 53750, train_perplexity=1.000506, train_loss=0.0005058972

Batch 53760, train_perplexity=1.0005059, train_loss=0.00050579885

Batch 53770, train_perplexity=1.0005058, train_loss=0.00050573336

Batch 53780, train_perplexity=1.0005058, train_loss=0.0005056947

Batch 53790, train_perplexity=1.0005056, train_loss=0.00050546526

Batch 53800, train_perplexity=1.0005054, train_loss=0.0005053432

Batch 53810, train_perplexity=1.0005054, train_loss=0.00050530146

Batch 53820, train_perplexity=1.0005053, train_loss=0.0005051674

Batch 53830, train_perplexity=1.0005052, train_loss=0.00050511386

Batch 53840, train_perplexity=1.0005052, train_loss=0.0005050572

Batch 53850, train_perplexity=1.000505, train_loss=0.00050483085

Batch 53860, train_perplexity=1.0005049, train_loss=0.00050476234

Batch 53870, train_perplexity=1.0005047, train_loss=0.0005046432

Batch 53880, train_perplexity=1.0005047, train_loss=0.0005045538

Batch 53890, train_perplexity=1.0005046, train_loss=0.0005044645

Batch 53900, train_perplexity=1.0005045, train_loss=0.0005043543

Batch 53910, train_perplexity=1.0005043, train_loss=0.0005041875

Batch 53920, train_perplexity=1.0005043, train_loss=0.0005040773

Batch 53930, train_perplexity=1.0005041, train_loss=0.00050399685

Batch 53940, train_perplexity=1.0005041, train_loss=0.0005040058

Batch 53950, train_perplexity=1.0005039, train_loss=0.00050378236

Batch 53960, train_perplexity=1.0005038, train_loss=0.00050369004

Batch 53970, train_perplexity=1.0005037, train_loss=0.0005035709

Batch 53980, train_perplexity=1.0005037, train_loss=0.0005035322

Batch 53990, train_perplexity=1.0005035, train_loss=0.0005034309

Batch 54000, train_perplexity=1.0005034, train_loss=0.0005033297

Batch 54010, train_perplexity=1.0005033, train_loss=0.00050320756

Batch 54020, train_perplexity=1.0005032, train_loss=0.0005030675

Batch 54030, train_perplexity=1.0005032, train_loss=0.00050300197

Batch 54040, train_perplexity=1.000503, train_loss=0.0005028441

Batch 54050, train_perplexity=1.000503, train_loss=0.00050279347

Batch 54060, train_perplexity=1.0005028, train_loss=0.0005026476

Batch 54070, train_perplexity=1.0005027, train_loss=0.00050253735

Batch 54080, train_perplexity=1.0005026, train_loss=0.0005024986

Batch 54090, train_perplexity=1.0005023, train_loss=0.00050226925

Batch 54100, train_perplexity=1.0005023, train_loss=0.0005022723

Batch 54110, train_perplexity=1.0005022, train_loss=0.00050215307

Batch 54120, train_perplexity=1.0005022, train_loss=0.0005020995

Batch 54130, train_perplexity=1.0005021, train_loss=0.0005019506

Batch 54140, train_perplexity=1.0005019, train_loss=0.00050180464

Batch 54150, train_perplexity=1.0005019, train_loss=0.0005017511

Batch 54160, train_perplexity=1.0005016, train_loss=0.0005015455

Batch 54170, train_perplexity=1.0005016, train_loss=0.00050149485

Batch 54180, train_perplexity=1.0005016, train_loss=0.000501471

Batch 54190, train_perplexity=1.0005015, train_loss=0.00050140254

Batch 54200, train_perplexity=1.0005014, train_loss=0.00050122675

Batch 54210, train_perplexity=1.0005013, train_loss=0.00050112547

Batch 54220, train_perplexity=1.0005012, train_loss=0.0005010451

Batch 54230, train_perplexity=1.000501, train_loss=0.00050095277

Batch 54240, train_perplexity=1.0005009, train_loss=0.0005008277

Batch 54250, train_perplexity=1.0005008, train_loss=0.00050066086

Batch 54260, train_perplexity=1.0005007, train_loss=0.00050058635

Batch 54270, train_perplexity=1.0005006, train_loss=0.00050046423

Batch 54280, train_perplexity=1.0005006, train_loss=0.0005003868

Batch 54290, train_perplexity=1.0005004, train_loss=0.00050030346

Batch 54300, train_perplexity=1.0005003, train_loss=0.0005001575

Batch 54310, train_perplexity=1.0005001, train_loss=0.00049999065

Batch 54320, train_perplexity=1.0005001, train_loss=0.000499937

Batch 54330, train_perplexity=1.0005, train_loss=0.00049980596
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 54340, train_perplexity=1.0005, train_loss=0.000499809

Batch 54350, train_perplexity=1.0004997, train_loss=0.0004995915

Batch 54360, train_perplexity=1.0004996, train_loss=0.0004995022

Batch 54370, train_perplexity=1.0004995, train_loss=0.0004994009

Batch 54380, train_perplexity=1.0004995, train_loss=0.00049938005

Batch 54390, train_perplexity=1.0004994, train_loss=0.0004992341

Batch 54400, train_perplexity=1.0004992, train_loss=0.00049908215

Batch 54410, train_perplexity=1.0004991, train_loss=0.00049906434

Batch 54420, train_perplexity=1.000499, train_loss=0.0004989451

Batch 54430, train_perplexity=1.0004989, train_loss=0.0004988052

Batch 54440, train_perplexity=1.0004988, train_loss=0.00049864734

Batch 54450, train_perplexity=1.0004988, train_loss=0.0004986354

Batch 54460, train_perplexity=1.0004987, train_loss=0.00049857283

Batch 54470, train_perplexity=1.0004984, train_loss=0.0004983435

Batch 54480, train_perplexity=1.0004984, train_loss=0.0004982958

Batch 54490, train_perplexity=1.0004983, train_loss=0.00049811415

Batch 54500, train_perplexity=1.0004982, train_loss=0.00049808435

Batch 54510, train_perplexity=1.000498, train_loss=0.0004979801

Batch 54520, train_perplexity=1.000498, train_loss=0.0004978937

Batch 54530, train_perplexity=1.0004978, train_loss=0.00049767626

Batch 54540, train_perplexity=1.0004978, train_loss=0.0004976941

Batch 54550, train_perplexity=1.0004977, train_loss=0.00049752137

Batch 54560, train_perplexity=1.0004976, train_loss=0.0004974797

Batch 54570, train_perplexity=1.0004975, train_loss=0.00049729506

Batch 54580, train_perplexity=1.0004973, train_loss=0.0004972414

Batch 54590, train_perplexity=1.0004973, train_loss=0.0004972325

Batch 54600, train_perplexity=1.0004972, train_loss=0.0004970865

Batch 54610, train_perplexity=1.0004971, train_loss=0.0004969704

Batch 54620, train_perplexity=1.000497, train_loss=0.00049687206

Batch 54630, train_perplexity=1.0004967, train_loss=0.00049666356

Batch 54640, train_perplexity=1.0004967, train_loss=0.00049659505

Batch 54650, train_perplexity=1.0004967, train_loss=0.000496598

Batch 54660, train_perplexity=1.0004965, train_loss=0.0004963299

Batch 54670, train_perplexity=1.0004965, train_loss=0.0004963925

Batch 54680, train_perplexity=1.0004964, train_loss=0.0004962197

Batch 54690, train_perplexity=1.0004963, train_loss=0.00049613934

Batch 54700, train_perplexity=1.0004961, train_loss=0.0004960589

Batch 54710, train_perplexity=1.0004959, train_loss=0.00049583847

Batch 54720, train_perplexity=1.0004959, train_loss=0.0004957968

Batch 54730, train_perplexity=1.0004958, train_loss=0.00049565977

Batch 54740, train_perplexity=1.0004957, train_loss=0.00049556745

Batch 54750, train_perplexity=1.0004957, train_loss=0.00049552275

Batch 54760, train_perplexity=1.0004956, train_loss=0.00049542147

Batch 54770, train_perplexity=1.0004956, train_loss=0.0004953738

Batch 54780, train_perplexity=1.0004953, train_loss=0.0004951385

Batch 54790, train_perplexity=1.0004951, train_loss=0.0004950015

Batch 54800, train_perplexity=1.0004952, train_loss=0.00049504614

Batch 54810, train_perplexity=1.000495, train_loss=0.00049488526

Batch 54820, train_perplexity=1.000495, train_loss=0.0004948644

Batch 54830, train_perplexity=1.0004948, train_loss=0.0004946798

Batch 54840, train_perplexity=1.0004946, train_loss=0.0004944832

Batch 54850, train_perplexity=1.0004946, train_loss=0.00049442064

Batch 54860, train_perplexity=1.0004945, train_loss=0.0004943909

Batch 54870, train_perplexity=1.0004942, train_loss=0.0004941764

Batch 54880, train_perplexity=1.0004942, train_loss=0.00049411686

Batch 54890, train_perplexity=1.0004941, train_loss=0.0004939679

Batch 54900, train_perplexity=1.0004941, train_loss=0.0004940067

Batch 54910, train_perplexity=1.000494, train_loss=0.00049385475

Batch 54920, train_perplexity=1.0004939, train_loss=0.00049375347

Batch 54930, train_perplexity=1.0004938, train_loss=0.00049361645

Batch 54940, train_perplexity=1.0004936, train_loss=0.0004935747

Batch 54950, train_perplexity=1.0004935, train_loss=0.0004934199

Batch 54960, train_perplexity=1.0004934, train_loss=0.0004933275

Batch 54970, train_perplexity=1.0004933, train_loss=0.000493116

Batch 54980, train_perplexity=1.0004933, train_loss=0.00049312494

Batch 54990, train_perplexity=1.000493, train_loss=0.00049297605

Batch 55000, train_perplexity=1.000493, train_loss=0.0004929641

Batch 55010, train_perplexity=1.0004929, train_loss=0.00049277645

Batch 55020, train_perplexity=1.0004928, train_loss=0.0004927258

Batch 55030, train_perplexity=1.0004928, train_loss=0.00049267523

Batch 55040, train_perplexity=1.0004927, train_loss=0.0004925203

Batch 55050, train_perplexity=1.0004925, train_loss=0.0004923237

Batch 55060, train_perplexity=1.0004925, train_loss=0.00049229094

Batch 55070, train_perplexity=1.0004923, train_loss=0.0004921778

Batch 55080, train_perplexity=1.0004922, train_loss=0.0004920944

Batch 55090, train_perplexity=1.0004921, train_loss=0.000492008

Batch 55100, train_perplexity=1.000492, train_loss=0.00049188884

Batch 55110, train_perplexity=1.0004919, train_loss=0.00049178454

Batch 55120, train_perplexity=1.0004919, train_loss=0.0004916863

Batch 55130, train_perplexity=1.0004917, train_loss=0.00049157604

Batch 55140, train_perplexity=1.0004916, train_loss=0.00049151655

Batch 55150, train_perplexity=1.0004915, train_loss=0.0004913825

Batch 55160, train_perplexity=1.0004914, train_loss=0.00049131096

Batch 55170, train_perplexity=1.0004914, train_loss=0.00049120677

Batch 55180, train_perplexity=1.0004913, train_loss=0.0004910965

Batch 55190, train_perplexity=1.0004911, train_loss=0.0004910131

Batch 55200, train_perplexity=1.000491, train_loss=0.00049093564

Batch 55210, train_perplexity=1.0004909, train_loss=0.00049084035

Batch 55220, train_perplexity=1.0004908, train_loss=0.00049063185

Batch 55230, train_perplexity=1.0004908, train_loss=0.0004906229

Batch 55240, train_perplexity=1.0004905, train_loss=0.000490474

Batch 55250, train_perplexity=1.0004905, train_loss=0.0004904412

Batch 55260, train_perplexity=1.0004904, train_loss=0.00049028336

Batch 55270, train_perplexity=1.0004903, train_loss=0.00049019395

Batch 55280, train_perplexity=1.0004902, train_loss=0.00049008377

Batch 55290, train_perplexity=1.0004901, train_loss=0.00048997655

Batch 55300, train_perplexity=1.00049, train_loss=0.0004898425

Batch 55310, train_perplexity=1.00049, train_loss=0.0004898008

Batch 55320, train_perplexity=1.0004898, train_loss=0.00048973237

Batch 55330, train_perplexity=1.0004897, train_loss=0.0004895953

Batch 55340, train_perplexity=1.0004896, train_loss=0.0004895238

Batch 55350, train_perplexity=1.0004895, train_loss=0.0004893927

Batch 55360, train_perplexity=1.0004894, train_loss=0.00048922596

Batch 55370, train_perplexity=1.0004892, train_loss=0.00048914255

Batch 55380, train_perplexity=1.0004892, train_loss=0.00048906216

Batch 55390, train_perplexity=1.0004891, train_loss=0.0004890055

Batch 55400, train_perplexity=1.000489, train_loss=0.0004888864

Batch 55410, train_perplexity=1.0004889, train_loss=0.0004887464

Batch 55420, train_perplexity=1.0004889, train_loss=0.00048877613

Batch 55430, train_perplexity=1.0004886, train_loss=0.0004885498

Batch 55440, train_perplexity=1.0004886, train_loss=0.0004885229

Batch 55450, train_perplexity=1.0004885, train_loss=0.00048836216

Batch 55460, train_perplexity=1.0004885, train_loss=0.00048837403

Batch 55470, train_perplexity=1.0004883, train_loss=0.0004881685

Batch 55480, train_perplexity=1.0004882, train_loss=0.00048806428

Batch 55490, train_perplexity=1.0004882, train_loss=0.00048803154

Batch 55500, train_perplexity=1.000488, train_loss=0.00048791533

Batch 55510, train_perplexity=1.0004879, train_loss=0.0004878617

Batch 55520, train_perplexity=1.0004878, train_loss=0.00048772476

Batch 55530, train_perplexity=1.0004878, train_loss=0.00048764725

Batch 55540, train_perplexity=1.0004876, train_loss=0.00048749236

Batch 55550, train_perplexity=1.0004874, train_loss=0.0004873732

Batch 55560, train_perplexity=1.0004873, train_loss=0.00048723625

Batch 55570, train_perplexity=1.0004872, train_loss=0.0004871409
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55580, train_perplexity=1.0004871, train_loss=0.00048695024

Batch 55590, train_perplexity=1.0004872, train_loss=0.00048707536

Batch 55600, train_perplexity=1.000487, train_loss=0.0004868937

Batch 55610, train_perplexity=1.0004869, train_loss=0.00048677155

Batch 55620, train_perplexity=1.0004867, train_loss=0.0004866524

Batch 55630, train_perplexity=1.0004866, train_loss=0.00048654818

Batch 55640, train_perplexity=1.0004866, train_loss=0.00048649157

Batch 55650, train_perplexity=1.0004865, train_loss=0.00048642902

Batch 55660, train_perplexity=1.0004864, train_loss=0.0004863009

Batch 55670, train_perplexity=1.0004863, train_loss=0.00048618182

Batch 55680, train_perplexity=1.0004861, train_loss=0.00048601197

Batch 55690, train_perplexity=1.0004861, train_loss=0.0004860001

Batch 55700, train_perplexity=1.000486, train_loss=0.00048589287

Batch 55710, train_perplexity=1.0004859, train_loss=0.00048576476

Batch 55720, train_perplexity=1.0004858, train_loss=0.00048567544

Batch 55730, train_perplexity=1.0004858, train_loss=0.00048560393

Batch 55740, train_perplexity=1.0004857, train_loss=0.00048553245

Batch 55750, train_perplexity=1.0004857, train_loss=0.00048553245

Batch 55760, train_perplexity=1.0004855, train_loss=0.0004853924

Batch 55770, train_perplexity=1.0004853, train_loss=0.0004852197

Batch 55780, train_perplexity=1.0004852, train_loss=0.00048503798

Batch 55790, train_perplexity=1.0004852, train_loss=0.0004850112

Batch 55800, train_perplexity=1.000485, train_loss=0.00048487412

Batch 55810, train_perplexity=1.000485, train_loss=0.00048486522

Batch 55820, train_perplexity=1.0004848, train_loss=0.0004847222

Batch 55830, train_perplexity=1.0004847, train_loss=0.0004846329

Batch 55840, train_perplexity=1.0004847, train_loss=0.00048461504

Batch 55850, train_perplexity=1.0004845, train_loss=0.00048435287

Batch 55860, train_perplexity=1.0004845, train_loss=0.00048432907

Batch 55870, train_perplexity=1.0004843, train_loss=0.00048425462

Batch 55880, train_perplexity=1.0004842, train_loss=0.0004840729

Batch 55890, train_perplexity=1.0004841, train_loss=0.00048404315

Batch 55900, train_perplexity=1.0004841, train_loss=0.0004839627

Batch 55910, train_perplexity=1.000484, train_loss=0.00048391504

Batch 55920, train_perplexity=1.0004839, train_loss=0.00048375418

Batch 55930, train_perplexity=1.0004838, train_loss=0.00048358142

Batch 55940, train_perplexity=1.0004838, train_loss=0.00048361422

Batch 55950, train_perplexity=1.0004835, train_loss=0.00048341163

Batch 55960, train_perplexity=1.0004835, train_loss=0.00048336998

Batch 55970, train_perplexity=1.0004834, train_loss=0.00048324186

Batch 55980, train_perplexity=1.0004833, train_loss=0.00048310484

Batch 55990, train_perplexity=1.0004833, train_loss=0.00048311974

Batch 56000, train_perplexity=1.0004833, train_loss=0.0004830989

Batch 56010, train_perplexity=1.000483, train_loss=0.0004828993

Batch 56020, train_perplexity=1.0004829, train_loss=0.00048281887

Batch 56030, train_perplexity=1.0004828, train_loss=0.00048271762

Batch 56040, train_perplexity=1.0004828, train_loss=0.00048262233

Batch 56050, train_perplexity=1.0004826, train_loss=0.00048249424

Batch 56060, train_perplexity=1.0004826, train_loss=0.00048241083

Batch 56070, train_perplexity=1.0004823, train_loss=0.00048224995

Batch 56080, train_perplexity=1.0004823, train_loss=0.0004821964

Batch 56090, train_perplexity=1.0004822, train_loss=0.0004820504

Batch 56100, train_perplexity=1.0004821, train_loss=0.00048197294

Batch 56110, train_perplexity=1.0004821, train_loss=0.0004819343

Batch 56120, train_perplexity=1.000482, train_loss=0.0004818091

Batch 56130, train_perplexity=1.0004818, train_loss=0.00048167212

Batch 56140, train_perplexity=1.0004817, train_loss=0.0004815827

Batch 56150, train_perplexity=1.0004817, train_loss=0.00048157977

Batch 56160, train_perplexity=1.0004815, train_loss=0.0004813504

Batch 56170, train_perplexity=1.0004815, train_loss=0.00048140404

Batch 56180, train_perplexity=1.0004814, train_loss=0.00048121935

Batch 56190, train_perplexity=1.0004812, train_loss=0.00048118658

Batch 56200, train_perplexity=1.0004811, train_loss=0.00048101088

Batch 56210, train_perplexity=1.000481, train_loss=0.0004809155

Batch 56220, train_perplexity=1.0004809, train_loss=0.00048082613

Batch 56230, train_perplexity=1.0004808, train_loss=0.00048065642

Batch 56240, train_perplexity=1.0004809, train_loss=0.0004807398

Batch 56250, train_perplexity=1.0004807, train_loss=0.00048054918

Batch 56260, train_perplexity=1.0004805, train_loss=0.00048044493

Batch 56270, train_perplexity=1.0004804, train_loss=0.0004803168

Batch 56280, train_perplexity=1.0004803, train_loss=0.0004802274

Batch 56290, train_perplexity=1.0004803, train_loss=0.00048014405

Batch 56300, train_perplexity=1.0004802, train_loss=0.00048011425

Batch 56310, train_perplexity=1.00048, train_loss=0.0004799862

Batch 56320, train_perplexity=1.0004799, train_loss=0.000479876

Batch 56330, train_perplexity=1.0004798, train_loss=0.00047970025

Batch 56340, train_perplexity=1.0004798, train_loss=0.00047965854

Batch 56350, train_perplexity=1.0004798, train_loss=0.00047970918

Batch 56360, train_perplexity=1.0004796, train_loss=0.00047948578

Batch 56370, train_perplexity=1.0004796, train_loss=0.00047948875

Batch 56380, train_perplexity=1.0004795, train_loss=0.00047932492

Batch 56390, train_perplexity=1.0004793, train_loss=0.00047922367

Batch 56400, train_perplexity=1.0004793, train_loss=0.00047917

Batch 56410, train_perplexity=1.0004792, train_loss=0.0004790479

Batch 56420, train_perplexity=1.0004791, train_loss=0.0004789824

Batch 56430, train_perplexity=1.0004789, train_loss=0.000478759

Batch 56440, train_perplexity=1.0004789, train_loss=0.0004787411

Batch 56450, train_perplexity=1.0004787, train_loss=0.00047865175

Batch 56460, train_perplexity=1.0004786, train_loss=0.00047854154

Batch 56470, train_perplexity=1.0004785, train_loss=0.00047837774

Batch 56480, train_perplexity=1.0004784, train_loss=0.00047833007

Batch 56490, train_perplexity=1.0004784, train_loss=0.0004782615

Batch 56500, train_perplexity=1.0004783, train_loss=0.00047813053

Batch 56510, train_perplexity=1.0004783, train_loss=0.00047812457

Batch 56520, train_perplexity=1.000478, train_loss=0.000477919

Batch 56530, train_perplexity=1.000478, train_loss=0.0004779011

Batch 56540, train_perplexity=1.0004779, train_loss=0.0004777373

Batch 56550, train_perplexity=1.0004778, train_loss=0.00047764793

Batch 56560, train_perplexity=1.0004778, train_loss=0.00047768067

Batch 56570, train_perplexity=1.0004777, train_loss=0.00047757645

Batch 56580, train_perplexity=1.0004774, train_loss=0.00047735305

Batch 56590, train_perplexity=1.0004774, train_loss=0.00047735305

Batch 56600, train_perplexity=1.0004773, train_loss=0.00047718623

Batch 56610, train_perplexity=1.0004772, train_loss=0.0004771118

Batch 56620, train_perplexity=1.0004772, train_loss=0.00047703137

Batch 56630, train_perplexity=1.0004771, train_loss=0.0004769629

Batch 56640, train_perplexity=1.000477, train_loss=0.00047679603

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 56650, train_perplexity=1.0004768, train_loss=0.00047674245

Batch 56660, train_perplexity=1.0004768, train_loss=0.00047666498

Batch 56670, train_perplexity=1.0004766, train_loss=0.0004765101

Batch 56680, train_perplexity=1.0004765, train_loss=0.00047640287

Batch 56690, train_perplexity=1.0004766, train_loss=0.0004764267

Batch 56700, train_perplexity=1.0004764, train_loss=0.00047627182

Batch 56710, train_perplexity=1.0004762, train_loss=0.00047616457

Batch 56720, train_perplexity=1.0004762, train_loss=0.00047607822

Batch 56730, train_perplexity=1.0004761, train_loss=0.0004759918

Batch 56740, train_perplexity=1.000476, train_loss=0.00047588756

Batch 56750, train_perplexity=1.0004759, train_loss=0.0004758131
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 56760, train_perplexity=1.0004758, train_loss=0.00047560758

Batch 56770, train_perplexity=1.0004758, train_loss=0.00047559262

Batch 56780, train_perplexity=1.0004756, train_loss=0.0004755063

Batch 56790, train_perplexity=1.0004754, train_loss=0.0004753454

Batch 56800, train_perplexity=1.0004754, train_loss=0.00047528587

Batch 56810, train_perplexity=1.0004753, train_loss=0.0004751965

Batch 56820, train_perplexity=1.0004753, train_loss=0.0004751369

Batch 56830, train_perplexity=1.000475, train_loss=0.00047496415

Batch 56840, train_perplexity=1.0004749, train_loss=0.00047485693

Batch 56850, train_perplexity=1.0004749, train_loss=0.0004747914

Batch 56860, train_perplexity=1.0004748, train_loss=0.00047465737

Batch 56870, train_perplexity=1.0004747, train_loss=0.00047456205

Batch 56880, train_perplexity=1.0004747, train_loss=0.00047456508

Batch 56890, train_perplexity=1.0004746, train_loss=0.00047446074

Batch 56900, train_perplexity=1.0004743, train_loss=0.0004742731

Batch 56910, train_perplexity=1.0004745, train_loss=0.00047434756

Batch 56920, train_perplexity=1.0004743, train_loss=0.00047416292

Batch 56930, train_perplexity=1.0004742, train_loss=0.00047405862

Batch 56940, train_perplexity=1.000474, train_loss=0.00047387398

Batch 56950, train_perplexity=1.000474, train_loss=0.00047390675

Batch 56960, train_perplexity=1.0004739, train_loss=0.0004737131

Batch 56970, train_perplexity=1.0004739, train_loss=0.00047369226

Batch 56980, train_perplexity=1.0004737, train_loss=0.00047358504

Batch 56990, train_perplexity=1.0004736, train_loss=0.0004735016

Batch 57000, train_perplexity=1.0004735, train_loss=0.00047343908

Batch 57010, train_perplexity=1.0004734, train_loss=0.000473311

Batch 57020, train_perplexity=1.0004733, train_loss=0.0004731948

Batch 57030, train_perplexity=1.0004731, train_loss=0.0004730727

Batch 57040, train_perplexity=1.0004731, train_loss=0.00047299225

Batch 57050, train_perplexity=1.0004731, train_loss=0.00047298335

Batch 57060, train_perplexity=1.000473, train_loss=0.0004729178

Batch 57070, train_perplexity=1.0004729, train_loss=0.0004728493

Batch 57080, train_perplexity=1.0004728, train_loss=0.00047271227

Batch 57090, train_perplexity=1.0004727, train_loss=0.00047258122

Batch 57100, train_perplexity=1.0004725, train_loss=0.00047244423

Batch 57110, train_perplexity=1.0004725, train_loss=0.00047239359

Batch 57120, train_perplexity=1.0004725, train_loss=0.00047239359

Batch 57130, train_perplexity=1.0004723, train_loss=0.00047214932

Batch 57140, train_perplexity=1.0004723, train_loss=0.00047216722

Batch 57150, train_perplexity=1.0004721, train_loss=0.00047200633

Batch 57160, train_perplexity=1.0004721, train_loss=0.0004719438

Batch 57170, train_perplexity=1.000472, train_loss=0.00047184847

Batch 57180, train_perplexity=1.000472, train_loss=0.00047184253

Batch 57190, train_perplexity=1.0004717, train_loss=0.00047161616

Batch 57200, train_perplexity=1.0004717, train_loss=0.0004715655

Batch 57210, train_perplexity=1.0004716, train_loss=0.0004714642

Batch 57220, train_perplexity=1.0004716, train_loss=0.0004715089

Batch 57230, train_perplexity=1.0004715, train_loss=0.00047132125

Batch 57240, train_perplexity=1.0004712, train_loss=0.00047116634

Batch 57250, train_perplexity=1.0004714, train_loss=0.0004711842

Batch 57260, train_perplexity=1.0004711, train_loss=0.00047104125

Batch 57270, train_perplexity=1.000471, train_loss=0.00047094296

Batch 57280, train_perplexity=1.0004709, train_loss=0.00047080888

Batch 57290, train_perplexity=1.0004709, train_loss=0.00047075824

Batch 57300, train_perplexity=1.0004708, train_loss=0.0004706898

Batch 57310, train_perplexity=1.0004706, train_loss=0.0004705021

Batch 57320, train_perplexity=1.0004705, train_loss=0.00047043953

Batch 57330, train_perplexity=1.0004704, train_loss=0.0004702906

Batch 57340, train_perplexity=1.0004704, train_loss=0.000470231

Batch 57350, train_perplexity=1.0004704, train_loss=0.0004702549

Batch 57360, train_perplexity=1.0004703, train_loss=0.00047013274

Batch 57370, train_perplexity=1.0004702, train_loss=0.0004700553

Batch 57380, train_perplexity=1.00047, train_loss=0.00046995102

Batch 57390, train_perplexity=1.0004699, train_loss=0.0004697604

Batch 57400, train_perplexity=1.0004698, train_loss=0.00046969188

Batch 57410, train_perplexity=1.0004698, train_loss=0.00046970084

Batch 57420, train_perplexity=1.0004697, train_loss=0.0004695757

Batch 57430, train_perplexity=1.0004696, train_loss=0.0004694417

Batch 57440, train_perplexity=1.0004694, train_loss=0.00046939103

Batch 57450, train_perplexity=1.0004693, train_loss=0.00046926597

Batch 57460, train_perplexity=1.0004692, train_loss=0.00046914085

Batch 57470, train_perplexity=1.0004692, train_loss=0.0004690723

Batch 57480, train_perplexity=1.0004692, train_loss=0.00046908722

Batch 57490, train_perplexity=1.000469, train_loss=0.00046889656

Batch 57500, train_perplexity=1.000469, train_loss=0.00046888166

Batch 57510, train_perplexity=1.0004689, train_loss=0.00046868212

Batch 57520, train_perplexity=1.0004687, train_loss=0.00046866128

Batch 57530, train_perplexity=1.0004687, train_loss=0.00046858977

Batch 57540, train_perplexity=1.0004685, train_loss=0.00046844085

Batch 57550, train_perplexity=1.0004685, train_loss=0.000468417

Batch 57560, train_perplexity=1.0004684, train_loss=0.0004683187

Batch 57570, train_perplexity=1.0004683, train_loss=0.0004681638

Batch 57580, train_perplexity=1.0004683, train_loss=0.0004681489

Batch 57590, train_perplexity=1.000468, train_loss=0.0004679523

Batch 57600, train_perplexity=1.000468, train_loss=0.00046785106

Batch 57610, train_perplexity=1.0004679, train_loss=0.00046779745

Batch 57620, train_perplexity=1.0004678, train_loss=0.00046771107

Batch 57630, train_perplexity=1.0004678, train_loss=0.0004676098

Batch 57640, train_perplexity=1.0004677, train_loss=0.00046749064

Batch 57650, train_perplexity=1.0004675, train_loss=0.00046737745

Batch 57660, train_perplexity=1.0004674, train_loss=0.00046734168

Batch 57670, train_perplexity=1.0004673, train_loss=0.00046723447

Batch 57680, train_perplexity=1.0004673, train_loss=0.0004671719

Batch 57690, train_perplexity=1.0004672, train_loss=0.00046711235

Batch 57700, train_perplexity=1.0004671, train_loss=0.00046690382

Batch 57710, train_perplexity=1.000467, train_loss=0.00046684424

Batch 57720, train_perplexity=1.000467, train_loss=0.0004668621

Batch 57730, train_perplexity=1.0004668, train_loss=0.0004666685

Batch 57740, train_perplexity=1.0004668, train_loss=0.00046668042

Batch 57750, train_perplexity=1.0004666, train_loss=0.00046647788

Batch 57760, train_perplexity=1.0004665, train_loss=0.00046636167

Batch 57770, train_perplexity=1.0004665, train_loss=0.00046640047

Batch 57780, train_perplexity=1.0004665, train_loss=0.000466323

Batch 57790, train_perplexity=1.0004663, train_loss=0.00046623062

Batch 57800, train_perplexity=1.0004661, train_loss=0.0004659953

Batch 57810, train_perplexity=1.0004661, train_loss=0.0004660132

Batch 57820, train_perplexity=1.0004661, train_loss=0.0004659417

Batch 57830, train_perplexity=1.000466, train_loss=0.0004658642

Batch 57840, train_perplexity=1.0004659, train_loss=0.00046575704

Batch 57850, train_perplexity=1.0004659, train_loss=0.00046570343

Batch 57860, train_perplexity=1.0004656, train_loss=0.0004655515

Batch 57870, train_perplexity=1.0004656, train_loss=0.00046547403

Batch 57880, train_perplexity=1.0004655, train_loss=0.00046539662

Batch 57890, train_perplexity=1.0004654, train_loss=0.00046534298

Batch 57900, train_perplexity=1.0004654, train_loss=0.0004652447

Batch 57910, train_perplexity=1.0004652, train_loss=0.00046506297

Batch 57920, train_perplexity=1.000465, train_loss=0.00046498555

Batch 57930, train_perplexity=1.000465, train_loss=0.00046496472

Batch 57940, train_perplexity=1.0004649, train_loss=0.00046481873

Batch 57950, train_perplexity=1.0004648, train_loss=0.0004647115

Batch 57960, train_perplexity=1.0004647, train_loss=0.0004646281

Batch 57970, train_perplexity=1.0004647, train_loss=0.00046453578

Batch 57980, train_perplexity=1.0004646, train_loss=0.00046447618

Batch 57990, train_perplexity=1.0004644, train_loss=0.00046433025
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 58000, train_perplexity=1.0004643, train_loss=0.00046421407

Batch 58010, train_perplexity=1.0004643, train_loss=0.0004642528

Batch 58020, train_perplexity=1.0004642, train_loss=0.00046408002

Batch 58030, train_perplexity=1.0004642, train_loss=0.0004640949

Batch 58040, train_perplexity=1.000464, train_loss=0.00046386552

Batch 58050, train_perplexity=1.000464, train_loss=0.00046388342

Batch 58060, train_perplexity=1.0004638, train_loss=0.00046374643

Batch 58070, train_perplexity=1.0004638, train_loss=0.00046372556

Batch 58080, train_perplexity=1.0004637, train_loss=0.0004635587

Batch 58090, train_perplexity=1.0004636, train_loss=0.00046351706

Batch 58100, train_perplexity=1.0004635, train_loss=0.00046333534

Batch 58110, train_perplexity=1.0004634, train_loss=0.00046326386

Batch 58120, train_perplexity=1.0004634, train_loss=0.00046327576

Batch 58130, train_perplexity=1.0004631, train_loss=0.00046306723

Batch 58140, train_perplexity=1.0004631, train_loss=0.00046307323

Batch 58150, train_perplexity=1.000463, train_loss=0.0004629213

Batch 58160, train_perplexity=1.000463, train_loss=0.00046290937

Batch 58170, train_perplexity=1.0004629, train_loss=0.00046280213

Batch 58180, train_perplexity=1.0004629, train_loss=0.00046275748

Batch 58190, train_perplexity=1.0004627, train_loss=0.00046259066

Batch 58200, train_perplexity=1.0004625, train_loss=0.00046245963

Batch 58210, train_perplexity=1.0004625, train_loss=0.00046239406

Batch 58220, train_perplexity=1.0004624, train_loss=0.00046229578

Batch 58230, train_perplexity=1.0004623, train_loss=0.0004621736

Batch 58240, train_perplexity=1.0004623, train_loss=0.00046213192

Batch 58250, train_perplexity=1.0004622, train_loss=0.00046210812

Batch 58260, train_perplexity=1.000462, train_loss=0.00046189365

Batch 58270, train_perplexity=1.0004619, train_loss=0.00046186388

Batch 58280, train_perplexity=1.0004618, train_loss=0.0004617268

Batch 58290, train_perplexity=1.0004617, train_loss=0.0004616494

Batch 58300, train_perplexity=1.0004618, train_loss=0.00046165235

Batch 58310, train_perplexity=1.0004616, train_loss=0.00046147068

Batch 58320, train_perplexity=1.0004616, train_loss=0.00046146772

Batch 58330, train_perplexity=1.0004615, train_loss=0.00046132173

Batch 58340, train_perplexity=1.0004613, train_loss=0.0004612115

Batch 58350, train_perplexity=1.0004612, train_loss=0.00046117278

Batch 58360, train_perplexity=1.0004612, train_loss=0.0004610775

Batch 58370, train_perplexity=1.000461, train_loss=0.00046092854

Batch 58380, train_perplexity=1.000461, train_loss=0.000460857

Batch 58390, train_perplexity=1.0004609, train_loss=0.00046079155

Batch 58400, train_perplexity=1.0004609, train_loss=0.00046075578

Batch 58410, train_perplexity=1.0004607, train_loss=0.0004605979

Batch 58420, train_perplexity=1.0004606, train_loss=0.00046057702

Batch 58430, train_perplexity=1.0004606, train_loss=0.00046048767

Batch 58440, train_perplexity=1.0004604, train_loss=0.00046030898

Batch 58450, train_perplexity=1.0004604, train_loss=0.00046029704

Batch 58460, train_perplexity=1.0004603, train_loss=0.000460166

Batch 58470, train_perplexity=1.0004601, train_loss=0.0004600081

Batch 58480, train_perplexity=1.0004601, train_loss=0.0004600379

Batch 58490, train_perplexity=1.00046, train_loss=0.00045988004

Batch 58500, train_perplexity=1.0004599, train_loss=0.0004597698

Batch 58510, train_perplexity=1.0004598, train_loss=0.0004597162

Batch 58520, train_perplexity=1.0004598, train_loss=0.00045966258

Batch 58530, train_perplexity=1.0004598, train_loss=0.0004597013

Batch 58540, train_perplexity=1.0004596, train_loss=0.00045947195

Batch 58550, train_perplexity=1.0004594, train_loss=0.00045929023

Batch 58560, train_perplexity=1.0004593, train_loss=0.00045926045

Batch 58570, train_perplexity=1.0004594, train_loss=0.0004592813

Batch 58580, train_perplexity=1.0004592, train_loss=0.00045909366

Batch 58590, train_perplexity=1.0004591, train_loss=0.00045901915

Batch 58600, train_perplexity=1.0004591, train_loss=0.00045898044

Batch 58610, train_perplexity=1.000459, train_loss=0.0004588673

Batch 58620, train_perplexity=1.0004588, train_loss=0.00045876598

Batch 58630, train_perplexity=1.0004587, train_loss=0.00045861112

Batch 58640, train_perplexity=1.0004587, train_loss=0.000458623

Batch 58650, train_perplexity=1.0004586, train_loss=0.00045848597

Batch 58660, train_perplexity=1.0004585, train_loss=0.00045834002

Batch 58670, train_perplexity=1.0004585, train_loss=0.00045843236

Batch 58680, train_perplexity=1.0004585, train_loss=0.00045834598

Batch 58690, train_perplexity=1.0004581, train_loss=0.000458063

Batch 58700, train_perplexity=1.0004581, train_loss=0.00045804214

Batch 58710, train_perplexity=1.000458, train_loss=0.00045794685

Batch 58720, train_perplexity=1.0004581, train_loss=0.0004579707

Batch 58730, train_perplexity=1.0004578, train_loss=0.00045771745

Batch 58740, train_perplexity=1.0004578, train_loss=0.0004577115

Batch 58750, train_perplexity=1.0004578, train_loss=0.000457643

Batch 58760, train_perplexity=1.0004578, train_loss=0.00045762217

Batch 58770, train_perplexity=1.0004575, train_loss=0.0004573898

Batch 58780, train_perplexity=1.0004575, train_loss=0.00045737787

Batch 58790, train_perplexity=1.0004574, train_loss=0.00045725278

Batch 58800, train_perplexity=1.0004572, train_loss=0.00045712173

Batch 58810, train_perplexity=1.0004572, train_loss=0.00045709193

Batch 58820, train_perplexity=1.0004572, train_loss=0.00045706512

Batch 58830, train_perplexity=1.000457, train_loss=0.00045696084

Batch 58840, train_perplexity=1.0004569, train_loss=0.00045681192

Batch 58850, train_perplexity=1.0004569, train_loss=0.00045678514

Batch 58860, train_perplexity=1.0004568, train_loss=0.0004567196

Batch 58870, train_perplexity=1.0004567, train_loss=0.00045660936

Batch 58880, train_perplexity=1.0004566, train_loss=0.00045643956

Batch 58890, train_perplexity=1.0004566, train_loss=0.0004564128

Batch 58900, train_perplexity=1.0004562, train_loss=0.00045614765

Batch 58910, train_perplexity=1.0004563, train_loss=0.0004562013

Batch 58920, train_perplexity=1.0004562, train_loss=0.0004560732

Batch 58930, train_perplexity=1.0004561, train_loss=0.0004560434

Batch 58940, train_perplexity=1.0004561, train_loss=0.00045599876

Batch 58950, train_perplexity=1.0004559, train_loss=0.00045575446

Batch 58960, train_perplexity=1.0004559, train_loss=0.00045574852

Batch 58970, train_perplexity=1.0004559, train_loss=0.00045571872

Batch 58980, train_perplexity=1.0004557, train_loss=0.00045564727

Batch 58990, train_perplexity=1.0004556, train_loss=0.0004555251

Batch 59000, train_perplexity=1.0004556, train_loss=0.0004555132

Batch 59010, train_perplexity=1.0004555, train_loss=0.00045535533

Batch 59020, train_perplexity=1.0004554, train_loss=0.00045528682

Batch 59030, train_perplexity=1.0004553, train_loss=0.00045520044

Batch 59040, train_perplexity=1.0004551, train_loss=0.00045504258

Batch 59050, train_perplexity=1.0004551, train_loss=0.00045508723

Batch 59060, train_perplexity=1.000455, train_loss=0.0004548698

Batch 59070, train_perplexity=1.000455, train_loss=0.00045486682

Batch 59080, train_perplexity=1.0004549, train_loss=0.00045477448

Batch 59090, train_perplexity=1.0004547, train_loss=0.00045456595

Batch 59100, train_perplexity=1.0004547, train_loss=0.00045453617

Batch 59110, train_perplexity=1.0004547, train_loss=0.00045453617

Batch 59120, train_perplexity=1.0004545, train_loss=0.00045438722

Batch 59130, train_perplexity=1.0004544, train_loss=0.00045435748

Batch 59140, train_perplexity=1.0004543, train_loss=0.00045424723

Batch 59150, train_perplexity=1.0004543, train_loss=0.00045420852

Batch 59160, train_perplexity=1.0004542, train_loss=0.00045406847

Batch 59170, train_perplexity=1.0004541, train_loss=0.0004539285

Batch 59180, train_perplexity=1.0004541, train_loss=0.0004539434

Batch 59190, train_perplexity=1.0004538, train_loss=0.00045373786

Batch 59200, train_perplexity=1.0004538, train_loss=0.00045371702

Batch 59210, train_perplexity=1.0004537, train_loss=0.0004536515

Batch 59220, train_perplexity=1.0004536, train_loss=0.00045349955

Batch 59230, train_perplexity=1.0004536, train_loss=0.0004534847
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 59240, train_perplexity=1.0004535, train_loss=0.00045336253

Batch 59250, train_perplexity=1.0004534, train_loss=0.00045327318

Batch 59260, train_perplexity=1.0004534, train_loss=0.00045325235

Batch 59270, train_perplexity=1.0004532, train_loss=0.0004531511

Batch 59280, train_perplexity=1.0004532, train_loss=0.00045313023

Batch 59290, train_perplexity=1.0004531, train_loss=0.00045300217

Batch 59300, train_perplexity=1.000453, train_loss=0.00045286806

Batch 59310, train_perplexity=1.0004529, train_loss=0.0004527936

Batch 59320, train_perplexity=1.0004529, train_loss=0.00045273104

Batch 59330, train_perplexity=1.0004526, train_loss=0.0004525672

Batch 59340, train_perplexity=1.0004526, train_loss=0.0004525344

Batch 59350, train_perplexity=1.0004525, train_loss=0.0004524123

Batch 59360, train_perplexity=1.0004525, train_loss=0.00045236765

Batch 59370, train_perplexity=1.0004524, train_loss=0.00045227233

Batch 59380, train_perplexity=1.0004522, train_loss=0.0004521055

Batch 59390, train_perplexity=1.0004522, train_loss=0.00045208167

Batch 59400, train_perplexity=1.000452, train_loss=0.0004519238

Batch 59410, train_perplexity=1.000452, train_loss=0.00045195065

Batch 59420, train_perplexity=1.0004519, train_loss=0.0004518642

Batch 59430, train_perplexity=1.0004518, train_loss=0.00045172125

Batch 59440, train_perplexity=1.0004518, train_loss=0.00045165874

Batch 59450, train_perplexity=1.0004517, train_loss=0.00045154852

Batch 59460, train_perplexity=1.0004516, train_loss=0.00045147698

Batch 59470, train_perplexity=1.0004516, train_loss=0.0004514204

Batch 59480, train_perplexity=1.0004513, train_loss=0.00045126554

Batch 59490, train_perplexity=1.0004513, train_loss=0.00045123574

Batch 59500, train_perplexity=1.0004512, train_loss=0.0004511404

Batch 59510, train_perplexity=1.0004511, train_loss=0.00045101828

Batch 59520, train_perplexity=1.0004511, train_loss=0.00045099744

Batch 59530, train_perplexity=1.0004511, train_loss=0.00045099144

Batch 59540, train_perplexity=1.0004508, train_loss=0.00045080675

Batch 59550, train_perplexity=1.0004508, train_loss=0.0004507561

Batch 59560, train_perplexity=1.0004507, train_loss=0.0004506817

Batch 59570, train_perplexity=1.0004507, train_loss=0.00045058932

Batch 59580, train_perplexity=1.0004506, train_loss=0.00045054464

Batch 59590, train_perplexity=1.0004505, train_loss=0.000450354

Batch 59600, train_perplexity=1.0004505, train_loss=0.0004503391

Batch 59610, train_perplexity=1.0004504, train_loss=0.0004502528

Batch 59620, train_perplexity=1.0004503, train_loss=0.00045011868

Batch 59630, train_perplexity=1.00045, train_loss=0.00044996076

Batch 59640, train_perplexity=1.0004501, train_loss=0.00044998762

Batch 59650, train_perplexity=1.00045, train_loss=0.00044996975

Batch 59660, train_perplexity=1.0004499, train_loss=0.00044977907

Batch 59670, train_perplexity=1.0004499, train_loss=0.00044977912

Batch 59680, train_perplexity=1.0004498, train_loss=0.00044969574

Batch 59690, train_perplexity=1.0004497, train_loss=0.00044954082

Batch 59700, train_perplexity=1.0004494, train_loss=0.0004493234

Batch 59710, train_perplexity=1.0004495, train_loss=0.0004494544

Batch 59720, train_perplexity=1.0004495, train_loss=0.00044939783

Batch 59730, train_perplexity=1.0004493, train_loss=0.00044925784

Batch 59740, train_perplexity=1.0004493, train_loss=0.00044919527

Batch 59750, train_perplexity=1.0004492, train_loss=0.00044905528

Batch 59760, train_perplexity=1.000449, train_loss=0.00044888252

Batch 59770, train_perplexity=1.000449, train_loss=0.00044887356

Batch 59780, train_perplexity=1.0004488, train_loss=0.00044875738

Batch 59790, train_perplexity=1.0004488, train_loss=0.0004486889

Batch 59800, train_perplexity=1.0004487, train_loss=0.00044861741

Batch 59810, train_perplexity=1.0004486, train_loss=0.00044853994

Batch 59820, train_perplexity=1.0004486, train_loss=0.00044851314

Batch 59830, train_perplexity=1.0004485, train_loss=0.00044835825

Batch 59840, train_perplexity=1.0004485, train_loss=0.00044832248

Batch 59850, train_perplexity=1.0004483, train_loss=0.00044823316

Batch 59860, train_perplexity=1.0004481, train_loss=0.00044803953

Batch 59870, train_perplexity=1.0004481, train_loss=0.00044800673

Batch 59880, train_perplexity=1.0004481, train_loss=0.00044800976

Batch 59890, train_perplexity=1.000448, train_loss=0.00044789058

Batch 59900, train_perplexity=1.0004479, train_loss=0.00044778333

Batch 59910, train_perplexity=1.0004478, train_loss=0.00044769695

Batch 59920, train_perplexity=1.0004478, train_loss=0.00044764334

Batch 59930, train_perplexity=1.0004476, train_loss=0.00044755993

Batch 59940, train_perplexity=1.0004475, train_loss=0.00044744374

Batch 59950, train_perplexity=1.0004474, train_loss=0.00044731866

Batch 59960, train_perplexity=1.0004473, train_loss=0.00044722925

Batch 59970, train_perplexity=1.0004473, train_loss=0.0004472084

Batch 59980, train_perplexity=1.0004473, train_loss=0.00044713696

Batch 59990, train_perplexity=1.000447, train_loss=0.00044691056

Batch 60000, train_perplexity=1.0004469, train_loss=0.00044685992

Batch 60010, train_perplexity=1.0004469, train_loss=0.00044685102

Batch 60020, train_perplexity=1.0004469, train_loss=0.00044683611

Batch 60030, train_perplexity=1.0004468, train_loss=0.00044664845

Batch 60040, train_perplexity=1.0004467, train_loss=0.00044663652

Batch 60050, train_perplexity=1.0004466, train_loss=0.00044649653

Batch 60060, train_perplexity=1.0004466, train_loss=0.0004464161

Batch 60070, train_perplexity=1.0004464, train_loss=0.00044635055

Batch 60080, train_perplexity=1.0004464, train_loss=0.00044631184

Batch 60090, train_perplexity=1.0004463, train_loss=0.00044623733

Batch 60100, train_perplexity=1.0004462, train_loss=0.00044608247

Batch 60110, train_perplexity=1.0004461, train_loss=0.00044596032

Batch 60120, train_perplexity=1.0004461, train_loss=0.00044594548

Batch 60130, train_perplexity=1.000446, train_loss=0.000445865

Batch 60140, train_perplexity=1.0004458, train_loss=0.00044572202

Batch 60150, train_perplexity=1.0004458, train_loss=0.00044571608

Batch 60160, train_perplexity=1.0004457, train_loss=0.00044557013

Batch 60170, train_perplexity=1.0004456, train_loss=0.00044552545

Batch 60180, train_perplexity=1.0004456, train_loss=0.00044549565

Batch 60190, train_perplexity=1.0004454, train_loss=0.00044523354

Batch 60200, train_perplexity=1.0004454, train_loss=0.00044522755

Batch 60210, train_perplexity=1.0004454, train_loss=0.00044523648

Batch 60220, train_perplexity=1.0004451, train_loss=0.0004450637

Batch 60230, train_perplexity=1.0004451, train_loss=0.00044503395

Batch 60240, train_perplexity=1.000445, train_loss=0.00044494457

Batch 60250, train_perplexity=1.000445, train_loss=0.0004448731

Batch 60260, train_perplexity=1.0004449, train_loss=0.00044476287

Batch 60270, train_perplexity=1.0004448, train_loss=0.00044471523

Batch 60280, train_perplexity=1.0004447, train_loss=0.00044458412

Batch 60290, train_perplexity=1.0004447, train_loss=0.00044455734

Batch 60300, train_perplexity=1.0004445, train_loss=0.00044445606

Batch 60310, train_perplexity=1.0004445, train_loss=0.0004444531

Batch 60320, train_perplexity=1.0004444, train_loss=0.00044431305

Batch 60330, train_perplexity=1.0004443, train_loss=0.00044417306

Batch 60340, train_perplexity=1.000444, train_loss=0.0004440033

Batch 60350, train_perplexity=1.0004442, train_loss=0.00044405693

Batch 60360, train_perplexity=1.000444, train_loss=0.00044390798

Batch 60370, train_perplexity=1.0004439, train_loss=0.00044387224

Batch 60380, train_perplexity=1.0004438, train_loss=0.0004437471

Batch 60390, train_perplexity=1.0004438, train_loss=0.00044368752

Batch 60400, train_perplexity=1.0004437, train_loss=0.00044364878

Batch 60410, train_perplexity=1.0004436, train_loss=0.0004435148

Batch 60420, train_perplexity=1.0004436, train_loss=0.00044349988

Batch 60430, train_perplexity=1.0004435, train_loss=0.0004433926

Batch 60440, train_perplexity=1.0004435, train_loss=0.00044330623

Batch 60450, train_perplexity=1.0004433, train_loss=0.0004432973

Batch 60460, train_perplexity=1.0004431, train_loss=0.000443059

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 60470, train_perplexity=1.0004432, train_loss=0.00044309476

Batch 60480, train_perplexity=1.0004431, train_loss=0.00044296368

Batch 60490, train_perplexity=1.000443, train_loss=0.0004428922

Batch 60500, train_perplexity=1.0004429, train_loss=0.00044274624

Batch 60510, train_perplexity=1.0004429, train_loss=0.00044277898

Batch 60520, train_perplexity=1.0004427, train_loss=0.00044267476

Batch 60530, train_perplexity=1.0004426, train_loss=0.00044250197

Batch 60540, train_perplexity=1.0004425, train_loss=0.00044242453

Batch 60550, train_perplexity=1.0004425, train_loss=0.00044235005

Batch 60560, train_perplexity=1.0004424, train_loss=0.0004423024

Batch 60570, train_perplexity=1.0004424, train_loss=0.00044226967

Batch 60580, train_perplexity=1.0004421, train_loss=0.00044209388

Batch 60590, train_perplexity=1.0004421, train_loss=0.00044209987

Batch 60600, train_perplexity=1.0004421, train_loss=0.00044204324

Batch 60610, train_perplexity=1.000442, train_loss=0.00044188238

Batch 60620, train_perplexity=1.000442, train_loss=0.00044189134

Batch 60630, train_perplexity=1.0004418, train_loss=0.0004417305

Batch 60640, train_perplexity=1.0004418, train_loss=0.0004417126

Batch 60650, train_perplexity=1.0004417, train_loss=0.00044159644

Batch 60660, train_perplexity=1.0004416, train_loss=0.00044151006

Batch 60670, train_perplexity=1.0004414, train_loss=0.00044136407

Batch 60680, train_perplexity=1.0004414, train_loss=0.000441373

Batch 60690, train_perplexity=1.0004413, train_loss=0.00044123596

Batch 60700, train_perplexity=1.0004412, train_loss=0.00044114364

Batch 60710, train_perplexity=1.0004412, train_loss=0.00044104535

Batch 60720, train_perplexity=1.0004411, train_loss=0.00044102746

Batch 60730, train_perplexity=1.000441, train_loss=0.00044091133

Batch 60740, train_perplexity=1.000441, train_loss=0.00044085766

Batch 60750, train_perplexity=1.0004408, train_loss=0.00044071767

Batch 60760, train_perplexity=1.0004408, train_loss=0.0004407445

Batch 60770, train_perplexity=1.0004407, train_loss=0.00044061936

Batch 60780, train_perplexity=1.0004406, train_loss=0.00044055976

Batch 60790, train_perplexity=1.0004406, train_loss=0.00044049427

Batch 60800, train_perplexity=1.0004405, train_loss=0.000440393

Batch 60810, train_perplexity=1.0004405, train_loss=0.00044032448

Batch 60820, train_perplexity=1.0004402, train_loss=0.0004401964

Batch 60830, train_perplexity=1.0004401, train_loss=0.0004400683

Batch 60840, train_perplexity=1.0004401, train_loss=0.00044000574

Batch 60850, train_perplexity=1.0004401, train_loss=0.0004399879

Batch 60860, train_perplexity=1.00044, train_loss=0.0004398598

Batch 60870, train_perplexity=1.00044, train_loss=0.00043986872

Batch 60880, train_perplexity=1.0004398, train_loss=0.00043968402

Batch 60890, train_perplexity=1.0004398, train_loss=0.00043965125

Batch 60900, train_perplexity=1.0004396, train_loss=0.00043954403

Batch 60910, train_perplexity=1.0004395, train_loss=0.00043942488

Batch 60920, train_perplexity=1.0004395, train_loss=0.00043941895

Batch 60930, train_perplexity=1.0004394, train_loss=0.00043926702

Batch 60940, train_perplexity=1.0004394, train_loss=0.00043929083

Batch 60950, train_perplexity=1.0004393, train_loss=0.00043915978

Batch 60960, train_perplexity=1.0004392, train_loss=0.00043908533

Batch 60970, train_perplexity=1.000439, train_loss=0.00043900489

Batch 60980, train_perplexity=1.0004389, train_loss=0.0004388917

Batch 60990, train_perplexity=1.0004389, train_loss=0.00043882022

Batch 61000, train_perplexity=1.0004388, train_loss=0.00043874868

Batch 61010, train_perplexity=1.0004387, train_loss=0.00043859083

Batch 61020, train_perplexity=1.0004387, train_loss=0.00043857892

Batch 61030, train_perplexity=1.0004386, train_loss=0.00043851338

Batch 61040, train_perplexity=1.0004385, train_loss=0.00043837933

Batch 61050, train_perplexity=1.0004383, train_loss=0.00043829292

Batch 61060, train_perplexity=1.0004383, train_loss=0.0004382602

Batch 61070, train_perplexity=1.0004382, train_loss=0.00043814696

Batch 61080, train_perplexity=1.0004382, train_loss=0.00043815293

Batch 61090, train_perplexity=1.0004381, train_loss=0.00043796824

Batch 61100, train_perplexity=1.0004381, train_loss=0.00043794446

Batch 61110, train_perplexity=1.000438, train_loss=0.0004378938

Batch 61120, train_perplexity=1.000438, train_loss=0.00043784615

Batch 61130, train_perplexity=1.0004377, train_loss=0.0004376704

Batch 61140, train_perplexity=1.0004376, train_loss=0.00043754227

Batch 61150, train_perplexity=1.0004376, train_loss=0.0004374917

Batch 61160, train_perplexity=1.0004375, train_loss=0.00043744402

Batch 61170, train_perplexity=1.0004375, train_loss=0.0004373993

Batch 61180, train_perplexity=1.0004374, train_loss=0.00043725036

Batch 61190, train_perplexity=1.0004373, train_loss=0.0004372057

Batch 61200, train_perplexity=1.0004373, train_loss=0.00043713715

Batch 61210, train_perplexity=1.000437, train_loss=0.00043694355

Batch 61220, train_perplexity=1.000437, train_loss=0.00043698525

Batch 61230, train_perplexity=1.000437, train_loss=0.00043695245

Batch 61240, train_perplexity=1.0004369, train_loss=0.0004367559

Batch 61250, train_perplexity=1.0004368, train_loss=0.00043669034

Batch 61260, train_perplexity=1.0004368, train_loss=0.00043670525

Batch 61270, train_perplexity=1.0004367, train_loss=0.00043659797

Batch 61280, train_perplexity=1.0004365, train_loss=0.000436455

Batch 61290, train_perplexity=1.0004365, train_loss=0.00043647888

Batch 61300, train_perplexity=1.0004364, train_loss=0.00043629715

Batch 61310, train_perplexity=1.0004364, train_loss=0.00043628522

Batch 61320, train_perplexity=1.0004363, train_loss=0.0004362078

Batch 61330, train_perplexity=1.0004362, train_loss=0.00043607372

Batch 61340, train_perplexity=1.0004361, train_loss=0.00043595757

Batch 61350, train_perplexity=1.0004361, train_loss=0.0004359814

Batch 61360, train_perplexity=1.000436, train_loss=0.00043589502

Batch 61370, train_perplexity=1.0004358, train_loss=0.00043579075

Batch 61380, train_perplexity=1.0004358, train_loss=0.0004357669

Batch 61390, train_perplexity=1.0004357, train_loss=0.00043560905

Batch 61400, train_perplexity=1.0004356, train_loss=0.00043548393

Batch 61410, train_perplexity=1.0004355, train_loss=0.00043543032

Batch 61420, train_perplexity=1.0004355, train_loss=0.00043541245

Batch 61430, train_perplexity=1.0004355, train_loss=0.000435332

Batch 61440, train_perplexity=1.0004352, train_loss=0.00043518608

Batch 61450, train_perplexity=1.0004352, train_loss=0.00043511455

Batch 61460, train_perplexity=1.0004351, train_loss=0.00043503116

Batch 61470, train_perplexity=1.000435, train_loss=0.0004349418

Batch 61480, train_perplexity=1.000435, train_loss=0.00043488515

Batch 61490, train_perplexity=1.0004349, train_loss=0.00043480477

Batch 61500, train_perplexity=1.000435, train_loss=0.0004348584

Batch 61510, train_perplexity=1.0004348, train_loss=0.0004346469

Batch 61520, train_perplexity=1.0004346, train_loss=0.00043454263

Batch 61530, train_perplexity=1.0004345, train_loss=0.00043445625

Batch 61540, train_perplexity=1.0004345, train_loss=0.00043444135

Batch 61550, train_perplexity=1.0004344, train_loss=0.00043434306

Batch 61560, train_perplexity=1.0004344, train_loss=0.00043430133

Batch 61570, train_perplexity=1.0004343, train_loss=0.0004341375

Batch 61580, train_perplexity=1.0004342, train_loss=0.00043405706

Batch 61590, train_perplexity=1.0004342, train_loss=0.00043407799

Batch 61600, train_perplexity=1.000434, train_loss=0.0004339647

Batch 61610, train_perplexity=1.0004339, train_loss=0.0004338575

Batch 61620, train_perplexity=1.0004339, train_loss=0.00043380685

Batch 61630, train_perplexity=1.0004337, train_loss=0.0004336073

Batch 61640, train_perplexity=1.0004338, train_loss=0.0004336728

Batch 61650, train_perplexity=1.0004337, train_loss=0.00043355365

Batch 61660, train_perplexity=1.0004336, train_loss=0.00043347327

Batch 61670, train_perplexity=1.0004334, train_loss=0.0004334077

Batch 61680, train_perplexity=1.0004334, train_loss=0.00043331838

Batch 61690, train_perplexity=1.0004334, train_loss=0.00043332131

Batch 61700, train_perplexity=1.0004333, train_loss=0.0004332141
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61710, train_perplexity=1.0004332, train_loss=0.000433086

Batch 61720, train_perplexity=1.0004331, train_loss=0.00043304428

Batch 61730, train_perplexity=1.000433, train_loss=0.00043286855

Batch 61740, train_perplexity=1.000433, train_loss=0.00043290725

Batch 61750, train_perplexity=1.0004328, train_loss=0.0004327643

Batch 61760, train_perplexity=1.0004328, train_loss=0.0004326958

Batch 61770, train_perplexity=1.0004327, train_loss=0.0004325885

Batch 61780, train_perplexity=1.0004326, train_loss=0.00043251703

Batch 61790, train_perplexity=1.0004325, train_loss=0.0004324515

Batch 61800, train_perplexity=1.0004325, train_loss=0.00043244555

Batch 61810, train_perplexity=1.0004324, train_loss=0.0004322668

Batch 61820, train_perplexity=1.0004323, train_loss=0.00043219235

Batch 61830, train_perplexity=1.0004324, train_loss=0.00043222512

Batch 61840, train_perplexity=1.0004323, train_loss=0.000432106

Batch 61850, train_perplexity=1.000432, train_loss=0.00043195108

Batch 61860, train_perplexity=1.0004321, train_loss=0.0004319898

Batch 61870, train_perplexity=1.0004319, train_loss=0.00043177535

Batch 61880, train_perplexity=1.0004319, train_loss=0.0004317932

Batch 61890, train_perplexity=1.0004318, train_loss=0.0004317068

Batch 61900, train_perplexity=1.0004317, train_loss=0.00043153402

Batch 61910, train_perplexity=1.0004317, train_loss=0.0004315102

Batch 61920, train_perplexity=1.0004315, train_loss=0.00043143868

Batch 61930, train_perplexity=1.0004315, train_loss=0.00043146254

Batch 61940, train_perplexity=1.0004314, train_loss=0.00043130765

Batch 61950, train_perplexity=1.0004312, train_loss=0.00043112892

Batch 61960, train_perplexity=1.0004313, train_loss=0.00043122424

Batch 61970, train_perplexity=1.0004312, train_loss=0.0004310336

Batch 61980, train_perplexity=1.0004312, train_loss=0.00043106638

Batch 61990, train_perplexity=1.000431, train_loss=0.00043090552

Batch 62000, train_perplexity=1.000431, train_loss=0.00043089062

Batch 62010, train_perplexity=1.0004308, train_loss=0.00043078337

Batch 62020, train_perplexity=1.0004307, train_loss=0.00043065823

Batch 62030, train_perplexity=1.0004308, train_loss=0.00043072083

Batch 62040, train_perplexity=1.0004307, train_loss=0.00043055994

Batch 62050, train_perplexity=1.0004306, train_loss=0.00043045566

Batch 62060, train_perplexity=1.0004305, train_loss=0.00043031567

Batch 62070, train_perplexity=1.0004303, train_loss=0.0004302293

Batch 62080, train_perplexity=1.0004303, train_loss=0.00043020252

Batch 62090, train_perplexity=1.0004302, train_loss=0.00043015188

Batch 62100, train_perplexity=1.0004302, train_loss=0.0004300744

Batch 62110, train_perplexity=1.0004301, train_loss=0.00042996122

Batch 62120, train_perplexity=1.0004299, train_loss=0.00042977653

Batch 62130, train_perplexity=1.0004299, train_loss=0.00042979143

Batch 62140, train_perplexity=1.0004299, train_loss=0.0004297229

Batch 62150, train_perplexity=1.0004297, train_loss=0.0004296514

Batch 62160, train_perplexity=1.0004297, train_loss=0.00042968715

Batch 62170, train_perplexity=1.0004296, train_loss=0.00042951142

Batch 62180, train_perplexity=1.0004296, train_loss=0.0004295144

Batch 62190, train_perplexity=1.0004295, train_loss=0.00042935947

Batch 62200, train_perplexity=1.0004294, train_loss=0.00042932673

Batch 62210, train_perplexity=1.0004293, train_loss=0.00042915693

Batch 62220, train_perplexity=1.0004293, train_loss=0.00042912713

Batch 62230, train_perplexity=1.0004292, train_loss=0.0004291182

Batch 62240, train_perplexity=1.0004292, train_loss=0.00042906162

Batch 62250, train_perplexity=1.000429, train_loss=0.00042896334

Batch 62260, train_perplexity=1.0004289, train_loss=0.0004287816

Batch 62270, train_perplexity=1.0004288, train_loss=0.00042873988

Batch 62280, train_perplexity=1.0004288, train_loss=0.0004286833

Batch 62290, train_perplexity=1.0004288, train_loss=0.0004286535

Batch 62300, train_perplexity=1.0004286, train_loss=0.00042842416

Batch 62310, train_perplexity=1.0004284, train_loss=0.0004283735

Batch 62320, train_perplexity=1.0004283, train_loss=0.00042827817

Batch 62330, train_perplexity=1.0004284, train_loss=0.00042830198

Batch 62340, train_perplexity=1.0004283, train_loss=0.0004282246

Batch 62350, train_perplexity=1.0004282, train_loss=0.00042809948

Batch 62360, train_perplexity=1.0004281, train_loss=0.00042803987

Batch 62370, train_perplexity=1.0004281, train_loss=0.00042802497

Batch 62380, train_perplexity=1.0004278, train_loss=0.0004277777

Batch 62390, train_perplexity=1.000428, train_loss=0.00042783134

Batch 62400, train_perplexity=1.000428, train_loss=0.00042784325

Batch 62410, train_perplexity=1.0004278, train_loss=0.00042769732

Batch 62420, train_perplexity=1.0004277, train_loss=0.00042764365

Batch 62430, train_perplexity=1.0004276, train_loss=0.0004275603

Batch 62440, train_perplexity=1.0004275, train_loss=0.00042742625

Batch 62450, train_perplexity=1.0004274, train_loss=0.00042729522

Batch 62460, train_perplexity=1.0004274, train_loss=0.00042728323

Batch 62470, train_perplexity=1.0004272, train_loss=0.00042721175

Batch 62480, train_perplexity=1.0004271, train_loss=0.00042707173

Batch 62490, train_perplexity=1.0004271, train_loss=0.00042707473

Batch 62500, train_perplexity=1.0004271, train_loss=0.0004270092

Batch 62510, train_perplexity=1.000427, train_loss=0.00042688113

Batch 62520, train_perplexity=1.000427, train_loss=0.00042685727

Batch 62530, train_perplexity=1.0004269, train_loss=0.00042674702

Batch 62540, train_perplexity=1.0004268, train_loss=0.00042665174

Batch 62550, train_perplexity=1.0004268, train_loss=0.0004266696

Batch 62560, train_perplexity=1.0004267, train_loss=0.00042654446

Batch 62570, train_perplexity=1.0004265, train_loss=0.00042644024

Batch 62580, train_perplexity=1.0004264, train_loss=0.00042635982

Batch 62590, train_perplexity=1.0004264, train_loss=0.00042627344

Batch 62600, train_perplexity=1.0004264, train_loss=0.00042626748

Batch 62610, train_perplexity=1.0004263, train_loss=0.0004262019

Batch 62620, train_perplexity=1.0004262, train_loss=0.0004260679

Batch 62630, train_perplexity=1.000426, train_loss=0.0004259785

Batch 62640, train_perplexity=1.000426, train_loss=0.00042591

Batch 62650, train_perplexity=1.0004259, train_loss=0.00042580278

Batch 62660, train_perplexity=1.0004259, train_loss=0.00042579978

Batch 62670, train_perplexity=1.0004257, train_loss=0.00042561808

Batch 62680, train_perplexity=1.0004258, train_loss=0.00042568066

Batch 62690, train_perplexity=1.0004257, train_loss=0.00042554957

Batch 62700, train_perplexity=1.0004255, train_loss=0.00042534404

Batch 62710, train_perplexity=1.0004255, train_loss=0.0004253172

Batch 62720, train_perplexity=1.0004253, train_loss=0.0004252398

Batch 62730, train_perplexity=1.0004252, train_loss=0.00042515638

Batch 62740, train_perplexity=1.0004252, train_loss=0.00042514148

Batch 62750, train_perplexity=1.0004252, train_loss=0.0004251236

Batch 62760, train_perplexity=1.0004251, train_loss=0.00042497166

Batch 62770, train_perplexity=1.0004251, train_loss=0.00042495976

Batch 62780, train_perplexity=1.000425, train_loss=0.00042485254

Batch 62790, train_perplexity=1.0004249, train_loss=0.0004247661

Batch 62800, train_perplexity=1.0004247, train_loss=0.00042468577

Batch 62810, train_perplexity=1.0004247, train_loss=0.00042463507

Batch 62820, train_perplexity=1.0004246, train_loss=0.00042453082

Batch 62830, train_perplexity=1.0004246, train_loss=0.00042454572

Batch 62840, train_perplexity=1.0004246, train_loss=0.00042449505

Batch 62850, train_perplexity=1.0004244, train_loss=0.0004243491

Batch 62860, train_perplexity=1.0004243, train_loss=0.00042422995

Batch 62870, train_perplexity=1.0004243, train_loss=0.00042420908

Batch 62880, train_perplexity=1.0004241, train_loss=0.0004240959

Batch 62890, train_perplexity=1.0004241, train_loss=0.00042402738

Batch 62900, train_perplexity=1.000424, train_loss=0.00042390527

Batch 62910, train_perplexity=1.0004239, train_loss=0.00042383076

Batch 62920, train_perplexity=1.0004239, train_loss=0.00042383972

Batch 62930, train_perplexity=1.0004238, train_loss=0.00042364013

Batch 62940, train_perplexity=1.0004237, train_loss=0.00042358352
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 62950, train_perplexity=1.0004237, train_loss=0.00042354479

Batch 62960, train_perplexity=1.0004236, train_loss=0.00042346437

Batch 62970, train_perplexity=1.0004234, train_loss=0.0004233899

Batch 62980, train_perplexity=1.0004234, train_loss=0.00042328268

Batch 62990, train_perplexity=1.0004233, train_loss=0.00042323204

Batch 63000, train_perplexity=1.0004233, train_loss=0.00042316652

Batch 63010, train_perplexity=1.0004232, train_loss=0.00042307418

Batch 63020, train_perplexity=1.0004231, train_loss=0.00042299076

Batch 63030, train_perplexity=1.0004231, train_loss=0.00042294606

Batch 63040, train_perplexity=1.000423, train_loss=0.00042287458

Batch 63050, train_perplexity=1.000423, train_loss=0.000422806

Batch 63060, train_perplexity=1.0004228, train_loss=0.00042278518

Batch 63070, train_perplexity=1.0004227, train_loss=0.0004226571

Batch 63080, train_perplexity=1.0004227, train_loss=0.0004226571

Batch 63090, train_perplexity=1.0004226, train_loss=0.00042256477

Batch 63100, train_perplexity=1.0004227, train_loss=0.00042257668

Batch 63110, train_perplexity=1.0004225, train_loss=0.00042242478

Batch 63120, train_perplexity=1.0004224, train_loss=0.0004222609

Batch 63130, train_perplexity=1.0004224, train_loss=0.00042222516

Batch 63140, train_perplexity=1.0004222, train_loss=0.00042216858

Batch 63150, train_perplexity=1.0004221, train_loss=0.00042197498

Batch 63160, train_perplexity=1.000422, train_loss=0.0004219273

Batch 63170, train_perplexity=1.000422, train_loss=0.00042187964

Batch 63180, train_perplexity=1.0004219, train_loss=0.00042183496

Batch 63190, train_perplexity=1.0004219, train_loss=0.00042175746

Batch 63200, train_perplexity=1.0004218, train_loss=0.00042172475

Batch 63210, train_perplexity=1.0004218, train_loss=0.000421683

Batch 63220, train_perplexity=1.0004216, train_loss=0.0004215818

Batch 63230, train_perplexity=1.0004216, train_loss=0.00042149832

Batch 63240, train_perplexity=1.0004215, train_loss=0.0004214149

Batch 63250, train_perplexity=1.0004214, train_loss=0.00042127195

Batch 63260, train_perplexity=1.0004213, train_loss=0.00042123022

Batch 63270, train_perplexity=1.0004213, train_loss=0.00042114977

Batch 63280, train_perplexity=1.000421, train_loss=0.00042101578

Batch 63290, train_perplexity=1.000421, train_loss=0.00042097704

Batch 63300, train_perplexity=1.000421, train_loss=0.00042092043

Batch 63310, train_perplexity=1.0004209, train_loss=0.00042089063

Batch 63320, train_perplexity=1.0004209, train_loss=0.00042080128

Batch 63330, train_perplexity=1.0004208, train_loss=0.0004207447

Batch 63340, train_perplexity=1.0004207, train_loss=0.00042055408

Batch 63350, train_perplexity=1.0004207, train_loss=0.00042055108

Batch 63360, train_perplexity=1.0004206, train_loss=0.00042048554

Batch 63370, train_perplexity=1.0004205, train_loss=0.0004204081

Batch 63380, train_perplexity=1.0004205, train_loss=0.0004203783

Batch 63390, train_perplexity=1.0004203, train_loss=0.00042024424

Batch 63400, train_perplexity=1.0004203, train_loss=0.00042021147

Batch 63410, train_perplexity=1.0004202, train_loss=0.00042010722

Batch 63420, train_perplexity=1.0004201, train_loss=0.00042005954

Batch 63430, train_perplexity=1.0004201, train_loss=0.0004199553

Batch 63440, train_perplexity=1.00042, train_loss=0.00041993443

Batch 63450, train_perplexity=1.00042, train_loss=0.00041985995

Batch 63460, train_perplexity=1.0004199, train_loss=0.00041979738

Batch 63470, train_perplexity=1.0004199, train_loss=0.000419717

Batch 63480, train_perplexity=1.0004196, train_loss=0.000419577

Batch 63490, train_perplexity=1.0004197, train_loss=0.00041960977

Batch 63500, train_perplexity=1.0004195, train_loss=0.00041938934

Batch 63510, train_perplexity=1.0004195, train_loss=0.00041937147

Batch 63520, train_perplexity=1.0004194, train_loss=0.00041932974

Batch 63530, train_perplexity=1.0004194, train_loss=0.00041926422

Batch 63540, train_perplexity=1.0004194, train_loss=0.0004192404

Batch 63550, train_perplexity=1.0004193, train_loss=0.000419148

Batch 63560, train_perplexity=1.000419, train_loss=0.00041896038

Batch 63570, train_perplexity=1.0004191, train_loss=0.00041903782

Batch 63580, train_perplexity=1.000419, train_loss=0.0004188978

Batch 63590, train_perplexity=1.0004189, train_loss=0.0004187876

Batch 63600, train_perplexity=1.0004188, train_loss=0.00041863864

Batch 63610, train_perplexity=1.0004188, train_loss=0.00041863864

Batch 63620, train_perplexity=1.0004187, train_loss=0.0004185731

Batch 63630, train_perplexity=1.0004187, train_loss=0.00041851652

Batch 63640, train_perplexity=1.0004185, train_loss=0.000418448

Batch 63650, train_perplexity=1.0004184, train_loss=0.0004183348

Batch 63660, train_perplexity=1.0004183, train_loss=0.00041823057

Batch 63670, train_perplexity=1.0004182, train_loss=0.00041814713

Batch 63680, train_perplexity=1.0004182, train_loss=0.0004180935

Batch 63690, train_perplexity=1.0004181, train_loss=0.00041798328

Batch 63700, train_perplexity=1.0004181, train_loss=0.00041795347

Batch 63710, train_perplexity=1.0004181, train_loss=0.00041799224

Batch 63720, train_perplexity=1.0004178, train_loss=0.00041776284

Batch 63730, train_perplexity=1.0004178, train_loss=0.00041774797

Batch 63740, train_perplexity=1.0004178, train_loss=0.00041768837

Batch 63750, train_perplexity=1.0004177, train_loss=0.00041761686

Batch 63760, train_perplexity=1.0004177, train_loss=0.0004175722

Batch 63770, train_perplexity=1.0004175, train_loss=0.00041741433

Batch 63780, train_perplexity=1.0004175, train_loss=0.0004173309

Batch 63790, train_perplexity=1.0004175, train_loss=0.00041733985

Batch 63800, train_perplexity=1.0004174, train_loss=0.0004172058

Batch 63810, train_perplexity=1.0004172, train_loss=0.000417176

Batch 63820, train_perplexity=1.0004172, train_loss=0.00041710451

Batch 63830, train_perplexity=1.0004171, train_loss=0.00041704794

Batch 63840, train_perplexity=1.0004171, train_loss=0.00041700026

Batch 63850, train_perplexity=1.0004169, train_loss=0.00041682448

Batch 63860, train_perplexity=1.0004169, train_loss=0.00041678874

Batch 63870, train_perplexity=1.0004169, train_loss=0.0004167292

Batch 63880, train_perplexity=1.0004166, train_loss=0.00041659217

Batch 63890, train_perplexity=1.0004168, train_loss=0.00041661895

Batch 63900, train_perplexity=1.0004166, train_loss=0.00041649686

Batch 63910, train_perplexity=1.0004165, train_loss=0.00041643128

Batch 63920, train_perplexity=1.0004165, train_loss=0.00041638067

Batch 63930, train_perplexity=1.0004163, train_loss=0.00041624362

Batch 63940, train_perplexity=1.0004163, train_loss=0.00041618702

Batch 63950, train_perplexity=1.0004163, train_loss=0.00041622878

Batch 63960, train_perplexity=1.000416, train_loss=0.00041600232

Batch 63970, train_perplexity=1.000416, train_loss=0.00041595765

Batch 63980, train_perplexity=1.000416, train_loss=0.00041593978

Batch 63990, train_perplexity=1.0004159, train_loss=0.0004158802

Batch 64000, train_perplexity=1.0004159, train_loss=0.00041579382

Batch 64010, train_perplexity=1.0004158, train_loss=0.0004156925

Batch 64020, train_perplexity=1.0004157, train_loss=0.0004156181

Batch 64030, train_perplexity=1.0004157, train_loss=0.00041554956

Batch 64040, train_perplexity=1.0004156, train_loss=0.0004154602

Batch 64050, train_perplexity=1.0004156, train_loss=0.00041544228

Batch 64060, train_perplexity=1.0004154, train_loss=0.00041532316

Batch 64070, train_perplexity=1.0004153, train_loss=0.0004152606

Batch 64080, train_perplexity=1.0004153, train_loss=0.00041521888

Batch 64090, train_perplexity=1.0004152, train_loss=0.0004151236

Batch 64100, train_perplexity=1.0004152, train_loss=0.00041506102

Batch 64110, train_perplexity=1.000415, train_loss=0.00041490016

Batch 64120, train_perplexity=1.000415, train_loss=0.00041492993

Batch 64130, train_perplexity=1.000415, train_loss=0.00041482865

Batch 64140, train_perplexity=1.0004148, train_loss=0.0004147095

Batch 64150, train_perplexity=1.0004147, train_loss=0.0004146559

Batch 64160, train_perplexity=1.0004146, train_loss=0.0004145695

Batch 64170, train_perplexity=1.0004145, train_loss=0.00041445036

Batch 64180, train_perplexity=1.0004146, train_loss=0.00041447417
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 64190, train_perplexity=1.0004144, train_loss=0.00041430438

Batch 64200, train_perplexity=1.0004144, train_loss=0.00041430735

Batch 64210, train_perplexity=1.0004143, train_loss=0.00041421503

Batch 64220, train_perplexity=1.0004143, train_loss=0.00041419416

Batch 64230, train_perplexity=1.0004141, train_loss=0.00041405714

Batch 64240, train_perplexity=1.0004141, train_loss=0.0004140899

Batch 64250, train_perplexity=1.0004141, train_loss=0.00041400947

Batch 64260, train_perplexity=1.000414, train_loss=0.00041396182

Batch 64270, train_perplexity=1.0004139, train_loss=0.00041383968

Batch 64280, train_perplexity=1.0004138, train_loss=0.0004137414

Batch 64290, train_perplexity=1.0004137, train_loss=0.00041360734

Batch 64300, train_perplexity=1.0004137, train_loss=0.00041362818

Batch 64310, train_perplexity=1.0004135, train_loss=0.0004135001

Batch 64320, train_perplexity=1.0004135, train_loss=0.00041344052

Batch 64330, train_perplexity=1.0004134, train_loss=0.00041337794

Batch 64340, train_perplexity=1.0004134, train_loss=0.0004133511

Batch 64350, train_perplexity=1.0004133, train_loss=0.00041321412

Batch 64360, train_perplexity=1.0004132, train_loss=0.00041314264

Batch 64370, train_perplexity=1.0004132, train_loss=0.00041314564

Batch 64380, train_perplexity=1.0004131, train_loss=0.00041300262

Batch 64390, train_perplexity=1.0004131, train_loss=0.00041296388

Batch 64400, train_perplexity=1.000413, train_loss=0.0004128477

Batch 64410, train_perplexity=1.0004128, train_loss=0.00041278516

Batch 64420, train_perplexity=1.0004128, train_loss=0.00041274348

Batch 64430, train_perplexity=1.0004127, train_loss=0.00041266004

Batch 64440, train_perplexity=1.0004127, train_loss=0.00041260046

Batch 64450, train_perplexity=1.0004126, train_loss=0.00041253492

Batch 64460, train_perplexity=1.0004126, train_loss=0.00041250518

Batch 64470, train_perplexity=1.0004123, train_loss=0.00041231746

Batch 64480, train_perplexity=1.0004123, train_loss=0.00041230852

Batch 64490, train_perplexity=1.0004123, train_loss=0.0004122102

Batch 64500, train_perplexity=1.0004122, train_loss=0.00041213873

Batch 64510, train_perplexity=1.0004122, train_loss=0.00041208806

Batch 64520, train_perplexity=1.0004121, train_loss=0.00041205235

Batch 64530, train_perplexity=1.000412, train_loss=0.00041193917

Batch 64540, train_perplexity=1.000412, train_loss=0.00041193917

Batch 64550, train_perplexity=1.0004119, train_loss=0.0004117604

Batch 64560, train_perplexity=1.0004117, train_loss=0.00041170086

Batch 64570, train_perplexity=1.0004117, train_loss=0.0004116919

Batch 64580, train_perplexity=1.0004116, train_loss=0.00041158765

Batch 64590, train_perplexity=1.0004116, train_loss=0.00041148637

Batch 64600, train_perplexity=1.0004115, train_loss=0.00041142676

Batch 64610, train_perplexity=1.0004115, train_loss=0.00041147743

Batch 64620, train_perplexity=1.0004114, train_loss=0.00041134038

Batch 64630, train_perplexity=1.0004113, train_loss=0.00041120336

Batch 64640, train_perplexity=1.0004113, train_loss=0.00041114376

Batch 64650, train_perplexity=1.0004112, train_loss=0.0004110991

Batch 64660, train_perplexity=1.000411, train_loss=0.00041100677

Batch 64670, train_perplexity=1.000411, train_loss=0.00041091442

Batch 64680, train_perplexity=1.0004109, train_loss=0.00041088165

Batch 64690, train_perplexity=1.0004108, train_loss=0.0004107327

Batch 64700, train_perplexity=1.0004109, train_loss=0.00041081314

Batch 64710, train_perplexity=1.0004108, train_loss=0.00041066122

Batch 64720, train_perplexity=1.0004107, train_loss=0.00041056884

Batch 64730, train_perplexity=1.0004107, train_loss=0.00041055994

Batch 64740, train_perplexity=1.0004106, train_loss=0.00041048546

Batch 64750, train_perplexity=1.0004106, train_loss=0.00041041692

Batch 64760, train_perplexity=1.0004104, train_loss=0.0004103097

Batch 64770, train_perplexity=1.0004103, train_loss=0.00041021436

Batch 64780, train_perplexity=1.0004102, train_loss=0.00041011904

Batch 64790, train_perplexity=1.0004102, train_loss=0.00041006843

Batch 64800, train_perplexity=1.0004101, train_loss=0.00040999393

Batch 64810, train_perplexity=1.00041, train_loss=0.00040988077

Batch 64820, train_perplexity=1.0004098, train_loss=0.00040980626

Batch 64830, train_perplexity=1.0004098, train_loss=0.00040975565

Batch 64840, train_perplexity=1.0004097, train_loss=0.00040969305

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 64850, train_perplexity=1.0004097, train_loss=0.00040960967

Batch 64860, train_perplexity=1.0004097, train_loss=0.00040960673

Batch 64870, train_perplexity=1.0004096, train_loss=0.00040948158

Batch 64880, train_perplexity=1.0004096, train_loss=0.0004094786

Batch 64890, train_perplexity=1.0004095, train_loss=0.00040941307

Batch 64900, train_perplexity=1.0004094, train_loss=0.00040928496

Batch 64910, train_perplexity=1.0004094, train_loss=0.00040924322

Batch 64920, train_perplexity=1.0004092, train_loss=0.00040912707

Batch 64930, train_perplexity=1.0004091, train_loss=0.00040907346

Batch 64940, train_perplexity=1.0004091, train_loss=0.00040904665

Batch 64950, train_perplexity=1.000409, train_loss=0.0004089424

Batch 64960, train_perplexity=1.000409, train_loss=0.00040887686

Batch 64970, train_perplexity=1.0004089, train_loss=0.00040883815

Batch 64980, train_perplexity=1.0004088, train_loss=0.00040870107

Batch 64990, train_perplexity=1.0004088, train_loss=0.0004086624

Batch 65000, train_perplexity=1.0004086, train_loss=0.00040857896

Batch 65010, train_perplexity=1.0004086, train_loss=0.00040858792

Batch 65020, train_perplexity=1.0004085, train_loss=0.00040841813

Batch 65030, train_perplexity=1.0004085, train_loss=0.00040838833

Batch 65040, train_perplexity=1.0004084, train_loss=0.00040827214

Batch 65050, train_perplexity=1.0004083, train_loss=0.00040824237

Batch 65060, train_perplexity=1.0004083, train_loss=0.00040817983

Batch 65070, train_perplexity=1.0004083, train_loss=0.00040817683

Batch 65080, train_perplexity=1.0004082, train_loss=0.00040803978

Batch 65090, train_perplexity=1.0004082, train_loss=0.00040804874

Batch 65100, train_perplexity=1.0004079, train_loss=0.00040789382

Batch 65110, train_perplexity=1.0004079, train_loss=0.0004078849

Batch 65120, train_perplexity=1.0004078, train_loss=0.0004077419

Batch 65130, train_perplexity=1.0004078, train_loss=0.00040767636

Batch 65140, train_perplexity=1.0004078, train_loss=0.0004076853

Batch 65150, train_perplexity=1.0004076, train_loss=0.00040753934

Batch 65160, train_perplexity=1.0004075, train_loss=0.00040740526

Batch 65170, train_perplexity=1.0004076, train_loss=0.00040743506

Batch 65180, train_perplexity=1.0004075, train_loss=0.0004073457

Batch 65190, train_perplexity=1.0004073, train_loss=0.00040727717

Batch 65200, train_perplexity=1.0004073, train_loss=0.0004072027

Batch 65210, train_perplexity=1.0004072, train_loss=0.00040719376

Batch 65220, train_perplexity=1.0004072, train_loss=0.0004070925

Batch 65230, train_perplexity=1.0004071, train_loss=0.00040698226

Batch 65240, train_perplexity=1.0004071, train_loss=0.00040696736

Batch 65250, train_perplexity=1.0004069, train_loss=0.00040682737

Batch 65260, train_perplexity=1.0004069, train_loss=0.00040680353

Batch 65270, train_perplexity=1.0004067, train_loss=0.0004067142

Batch 65280, train_perplexity=1.0004067, train_loss=0.0004066963

Batch 65290, train_perplexity=1.0004067, train_loss=0.00040662783

Batch 65300, train_perplexity=1.0004066, train_loss=0.0004065652

Batch 65310, train_perplexity=1.0004065, train_loss=0.0004064133

Batch 65320, train_perplexity=1.0004064, train_loss=0.00040634477

Batch 65330, train_perplexity=1.0004064, train_loss=0.0004063031

Batch 65340, train_perplexity=1.0004063, train_loss=0.00040620478

Batch 65350, train_perplexity=1.0004061, train_loss=0.0004060975
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65360, train_perplexity=1.0004061, train_loss=0.0004060469

Batch 65370, train_perplexity=1.0004061, train_loss=0.000406035

Batch 65380, train_perplexity=1.0004061, train_loss=0.00040602902

Batch 65390, train_perplexity=1.0004059, train_loss=0.0004058056

Batch 65400, train_perplexity=1.0004059, train_loss=0.00040583542

Batch 65410, train_perplexity=1.0004058, train_loss=0.00040575495

Batch 65420, train_perplexity=1.0004058, train_loss=0.00040568644

Batch 65430, train_perplexity=1.0004057, train_loss=0.0004055733

Batch 65440, train_perplexity=1.0004056, train_loss=0.00040550472

Batch 65450, train_perplexity=1.0004056, train_loss=0.00040547794

Batch 65460, train_perplexity=1.0004056, train_loss=0.00040547794

Batch 65470, train_perplexity=1.0004054, train_loss=0.000405332

Batch 65480, train_perplexity=1.0004053, train_loss=0.00040526345

Batch 65490, train_perplexity=1.0004052, train_loss=0.00040516217

Batch 65500, train_perplexity=1.0004052, train_loss=0.0004051532

Batch 65510, train_perplexity=1.0004052, train_loss=0.00040506985

Batch 65520, train_perplexity=1.0004051, train_loss=0.00040493277

Batch 65530, train_perplexity=1.0004051, train_loss=0.0004049417

Batch 65540, train_perplexity=1.000405, train_loss=0.0004048196

Batch 65550, train_perplexity=1.0004048, train_loss=0.00040479278

Batch 65560, train_perplexity=1.0004048, train_loss=0.0004047064

Batch 65570, train_perplexity=1.0004047, train_loss=0.00040467957

Batch 65580, train_perplexity=1.0004046, train_loss=0.0004045306

Batch 65590, train_perplexity=1.0004046, train_loss=0.00040447107

Batch 65600, train_perplexity=1.0004045, train_loss=0.0004044413

Batch 65610, train_perplexity=1.0004044, train_loss=0.0004043281

Batch 65620, train_perplexity=1.0004044, train_loss=0.00040421786

Batch 65630, train_perplexity=1.0004044, train_loss=0.00040425954

Batch 65640, train_perplexity=1.0004042, train_loss=0.000404197

Batch 65650, train_perplexity=1.0004041, train_loss=0.00040400936

Batch 65660, train_perplexity=1.0004041, train_loss=0.00040400936

Batch 65670, train_perplexity=1.000404, train_loss=0.00040397656

Batch 65680, train_perplexity=1.000404, train_loss=0.00040391402

Batch 65690, train_perplexity=1.0004039, train_loss=0.000403774

Batch 65700, train_perplexity=1.0004039, train_loss=0.00040374423

Batch 65710, train_perplexity=1.0004038, train_loss=0.00040372336

Batch 65720, train_perplexity=1.0004038, train_loss=0.00040362208

Batch 65730, train_perplexity=1.0004036, train_loss=0.00040356844

Batch 65740, train_perplexity=1.0004035, train_loss=0.00040342248

Batch 65750, train_perplexity=1.0004035, train_loss=0.00040346722

Batch 65760, train_perplexity=1.0004034, train_loss=0.00040331524

Batch 65770, train_perplexity=1.0004034, train_loss=0.00040335394

Batch 65780, train_perplexity=1.0004032, train_loss=0.00040311564

Batch 65790, train_perplexity=1.0004032, train_loss=0.00040304416

Batch 65800, train_perplexity=1.0004032, train_loss=0.0004030412

Batch 65810, train_perplexity=1.000403, train_loss=0.0004030084

Batch 65820, train_perplexity=1.0004029, train_loss=0.00040287437

Batch 65830, train_perplexity=1.0004029, train_loss=0.000402788

Batch 65840, train_perplexity=1.0004029, train_loss=0.00040281774

Batch 65850, train_perplexity=1.0004027, train_loss=0.000402642

Batch 65860, train_perplexity=1.0004027, train_loss=0.00040260924

Batch 65870, train_perplexity=1.0004026, train_loss=0.00040251692

Batch 65880, train_perplexity=1.0004026, train_loss=0.00040250795

Batch 65890, train_perplexity=1.0004026, train_loss=0.0004024424

Batch 65900, train_perplexity=1.0004025, train_loss=0.00040241558

Batch 65910, train_perplexity=1.0004025, train_loss=0.00040233217

Batch 65920, train_perplexity=1.0004022, train_loss=0.00040218327

Batch 65930, train_perplexity=1.0004022, train_loss=0.00040218327

Batch 65940, train_perplexity=1.0004021, train_loss=0.00040207006

Batch 65950, train_perplexity=1.0004021, train_loss=0.00040202838

Batch 65960, train_perplexity=1.000402, train_loss=0.00040191517

Batch 65970, train_perplexity=1.000402, train_loss=0.00040191214

Batch 65980, train_perplexity=1.000402, train_loss=0.00040183472

Batch 65990, train_perplexity=1.0004017, train_loss=0.00040170364

Batch 66000, train_perplexity=1.0004017, train_loss=0.00040160533

Batch 66010, train_perplexity=1.0004017, train_loss=0.0004016143

Batch 66020, train_perplexity=1.0004015, train_loss=0.00040145937

Batch 66030, train_perplexity=1.0004016, train_loss=0.0004014832

Batch 66040, train_perplexity=1.0004015, train_loss=0.00040140576

Batch 66050, train_perplexity=1.0004015, train_loss=0.00040137002

Batch 66060, train_perplexity=1.0004014, train_loss=0.00040128658

Batch 66070, train_perplexity=1.0004011, train_loss=0.0004011049

Batch 66080, train_perplexity=1.0004011, train_loss=0.00040111382

Batch 66090, train_perplexity=1.0004013, train_loss=0.00040115256

Batch 66100, train_perplexity=1.000401, train_loss=0.00040096784

Batch 66110, train_perplexity=1.000401, train_loss=0.00040097084

Batch 66120, train_perplexity=1.0004009, train_loss=0.00040086062

Batch 66130, train_perplexity=1.0004009, train_loss=0.00040078018

Batch 66140, train_perplexity=1.0004008, train_loss=0.00040070573

Batch 66150, train_perplexity=1.0004008, train_loss=0.0004006819

Batch 66160, train_perplexity=1.0004007, train_loss=0.0004006104

Batch 66170, train_perplexity=1.0004007, train_loss=0.000400524

Batch 66180, train_perplexity=1.0004005, train_loss=0.00040044653

Batch 66190, train_perplexity=1.0004005, train_loss=0.00040043163

Batch 66200, train_perplexity=1.0004004, train_loss=0.00040032144

Batch 66210, train_perplexity=1.0004003, train_loss=0.0004001993

Batch 66220, train_perplexity=1.0004002, train_loss=0.00040013378

Batch 66230, train_perplexity=1.0004002, train_loss=0.00040010695

Batch 66240, train_perplexity=1.0004002, train_loss=0.0004000563

Batch 66250, train_perplexity=1.0004001, train_loss=0.0003999431

Batch 66260, train_perplexity=1.0004, train_loss=0.00039989248

Batch 66270, train_perplexity=1.0004, train_loss=0.00039985374

Batch 66280, train_perplexity=1.0004, train_loss=0.0003998925

Batch 66290, train_perplexity=1.0003998, train_loss=0.00039971375

Batch 66300, train_perplexity=1.0003997, train_loss=0.00039962138

Batch 66310, train_perplexity=1.0003996, train_loss=0.00039953494

Batch 66320, train_perplexity=1.0003996, train_loss=0.00039951416

Batch 66330, train_perplexity=1.0003995, train_loss=0.00039941585

Batch 66340, train_perplexity=1.0003995, train_loss=0.00039937117

Batch 66350, train_perplexity=1.0003994, train_loss=0.00039928773

Batch 66360, train_perplexity=1.0003992, train_loss=0.00039919838

Batch 66370, train_perplexity=1.0003992, train_loss=0.00039919838

Batch 66380, train_perplexity=1.0003992, train_loss=0.00039917452

Batch 66390, train_perplexity=1.0003991, train_loss=0.0003990733

Batch 66400, train_perplexity=1.0003991, train_loss=0.00039897498

Batch 66410, train_perplexity=1.000399, train_loss=0.00039891238

Batch 66420, train_perplexity=1.0003989, train_loss=0.00039882597

Batch 66430, train_perplexity=1.0003989, train_loss=0.0003987694

Batch 66440, train_perplexity=1.0003988, train_loss=0.00039866514

Batch 66450, train_perplexity=1.0003986, train_loss=0.00039859963

Batch 66460, train_perplexity=1.0003986, train_loss=0.0003985758

Batch 66470, train_perplexity=1.0003985, train_loss=0.00039843874

Batch 66480, train_perplexity=1.0003985, train_loss=0.00039842684

Batch 66490, train_perplexity=1.0003985, train_loss=0.0003983881

Batch 66500, train_perplexity=1.0003984, train_loss=0.00039828385

Batch 66510, train_perplexity=1.0003983, train_loss=0.00039819744

Batch 66520, train_perplexity=1.0003983, train_loss=0.0003982064

Batch 66530, train_perplexity=1.0003982, train_loss=0.0003980843

Batch 66540, train_perplexity=1.0003982, train_loss=0.0003980515

Batch 66550, train_perplexity=1.000398, train_loss=0.00039793534

Batch 66560, train_perplexity=1.000398, train_loss=0.0003979085

Batch 66570, train_perplexity=1.0003979, train_loss=0.0003978251

Batch 66580, train_perplexity=1.0003979, train_loss=0.00039780722

Batch 66590, train_perplexity=1.0003978, train_loss=0.00039771188
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66600, train_perplexity=1.0003977, train_loss=0.0003976404

Batch 66610, train_perplexity=1.0003977, train_loss=0.0003976136

Batch 66620, train_perplexity=1.0003976, train_loss=0.00039743487

Batch 66630, train_perplexity=1.0003976, train_loss=0.00039747954

Batch 66640, train_perplexity=1.0003974, train_loss=0.00039733655

Batch 66650, train_perplexity=1.0003973, train_loss=0.00039724715

Batch 66660, train_perplexity=1.0003973, train_loss=0.00039719354

Batch 66670, train_perplexity=1.0003972, train_loss=0.0003970893

Batch 66680, train_perplexity=1.0003972, train_loss=0.00039706842

Batch 66690, train_perplexity=1.0003971, train_loss=0.00039700884

Batch 66700, train_perplexity=1.0003971, train_loss=0.00039695227

Batch 66710, train_perplexity=1.000397, train_loss=0.0003968897

Batch 66720, train_perplexity=1.000397, train_loss=0.00039686292

Batch 66730, train_perplexity=1.0003968, train_loss=0.00039679732

Batch 66740, train_perplexity=1.0003968, train_loss=0.00039671993

Batch 66750, train_perplexity=1.0003968, train_loss=0.000396711

Batch 66760, train_perplexity=1.0003966, train_loss=0.00039655308

Batch 66770, train_perplexity=1.0003966, train_loss=0.00039649644

Batch 66780, train_perplexity=1.0003966, train_loss=0.00039648757

Batch 66790, train_perplexity=1.0003965, train_loss=0.0003964518

Batch 66800, train_perplexity=1.0003965, train_loss=0.00039637735

Batch 66810, train_perplexity=1.0003964, train_loss=0.00039629987

Batch 66820, train_perplexity=1.0003964, train_loss=0.00039627007

Batch 66830, train_perplexity=1.0003961, train_loss=0.00039610622

Batch 66840, train_perplexity=1.0003961, train_loss=0.00039603177

Batch 66850, train_perplexity=1.000396, train_loss=0.00039598113

Batch 66860, train_perplexity=1.000396, train_loss=0.00039591562

Batch 66870, train_perplexity=1.0003959, train_loss=0.00039585604

Batch 66880, train_perplexity=1.0003959, train_loss=0.00039578747

Batch 66890, train_perplexity=1.0003958, train_loss=0.00039566238

Batch 66900, train_perplexity=1.0003957, train_loss=0.00039563258

Batch 66910, train_perplexity=1.0003957, train_loss=0.0003955611

Batch 66920, train_perplexity=1.0003957, train_loss=0.00039551943

Batch 66930, train_perplexity=1.0003955, train_loss=0.00039546576

Batch 66940, train_perplexity=1.0003954, train_loss=0.00039532874

Batch 66950, train_perplexity=1.0003954, train_loss=0.0003953675

Batch 66960, train_perplexity=1.0003953, train_loss=0.0003952483

Batch 66970, train_perplexity=1.0003952, train_loss=0.00039512618

Batch 66980, train_perplexity=1.0003952, train_loss=0.00039511127

Batch 66990, train_perplexity=1.0003952, train_loss=0.00039510534

Batch 67000, train_perplexity=1.000395, train_loss=0.00039498915

Batch 67010, train_perplexity=1.000395, train_loss=0.00039498915

Batch 67020, train_perplexity=1.000395, train_loss=0.00039500103

Batch 67030, train_perplexity=1.0003949, train_loss=0.0003948402

Batch 67040, train_perplexity=1.0003948, train_loss=0.00039468828

Batch 67050, train_perplexity=1.0003947, train_loss=0.00039465545

Batch 67060, train_perplexity=1.0003946, train_loss=0.00039455126

Batch 67070, train_perplexity=1.0003947, train_loss=0.00039457803

Batch 67080, train_perplexity=1.0003945, train_loss=0.0003943457

Batch 67090, train_perplexity=1.0003945, train_loss=0.0003943874

Batch 67100, train_perplexity=1.0003945, train_loss=0.00039433676

Batch 67110, train_perplexity=1.0003943, train_loss=0.00039423248

Batch 67120, train_perplexity=1.0003942, train_loss=0.0003941729

Batch 67130, train_perplexity=1.0003942, train_loss=0.00039413123

Batch 67140, train_perplexity=1.0003941, train_loss=0.00039407762

Batch 67150, train_perplexity=1.000394, train_loss=0.0003939614

Batch 67160, train_perplexity=1.000394, train_loss=0.00039396738

Batch 67170, train_perplexity=1.000394, train_loss=0.000393875

Batch 67180, train_perplexity=1.0003939, train_loss=0.00039382436

Batch 67190, train_perplexity=1.0003937, train_loss=0.00039368437

Batch 67200, train_perplexity=1.0003937, train_loss=0.00039369927

Batch 67210, train_perplexity=1.0003936, train_loss=0.00039355626

Batch 67220, train_perplexity=1.0003936, train_loss=0.000393598

Batch 67230, train_perplexity=1.0003935, train_loss=0.00039347587

Batch 67240, train_perplexity=1.0003935, train_loss=0.00039341033

Batch 67250, train_perplexity=1.0003934, train_loss=0.00039335073

Batch 67260, train_perplexity=1.0003934, train_loss=0.00039333885

Batch 67270, train_perplexity=1.0003933, train_loss=0.00039321074

Batch 67280, train_perplexity=1.0003932, train_loss=0.00039312133

Batch 67290, train_perplexity=1.000393, train_loss=0.00039300814

Batch 67300, train_perplexity=1.0003932, train_loss=0.00039306475

Batch 67310, train_perplexity=1.000393, train_loss=0.00039299025

Batch 67320, train_perplexity=1.0003929, train_loss=0.00039288308

Batch 67330, train_perplexity=1.0003929, train_loss=0.0003928026

Batch 67340, train_perplexity=1.0003928, train_loss=0.0003926596

Batch 67350, train_perplexity=1.0003928, train_loss=0.00039272517

Batch 67360, train_perplexity=1.0003927, train_loss=0.0003925464

Batch 67370, train_perplexity=1.0003926, train_loss=0.0003925375

Batch 67380, train_perplexity=1.0003926, train_loss=0.00039249577

Batch 67390, train_perplexity=1.0003926, train_loss=0.00039243026

Batch 67400, train_perplexity=1.0003924, train_loss=0.00039235578

Batch 67410, train_perplexity=1.0003923, train_loss=0.0003922664

Batch 67420, train_perplexity=1.0003923, train_loss=0.00039224554

Batch 67430, train_perplexity=1.0003922, train_loss=0.0003921502

Batch 67440, train_perplexity=1.0003921, train_loss=0.00039201317

Batch 67450, train_perplexity=1.0003921, train_loss=0.00039201914

Batch 67460, train_perplexity=1.0003921, train_loss=0.00039201614

Batch 67470, train_perplexity=1.0003921, train_loss=0.0003919953

Batch 67480, train_perplexity=1.000392, train_loss=0.00039182545

Batch 67490, train_perplexity=1.000392, train_loss=0.00039183744

Batch 67500, train_perplexity=1.0003918, train_loss=0.0003917123

Batch 67510, train_perplexity=1.0003916, train_loss=0.0003915812

Batch 67520, train_perplexity=1.0003917, train_loss=0.00039161404

Batch 67530, train_perplexity=1.0003916, train_loss=0.00039147103

Batch 67540, train_perplexity=1.0003915, train_loss=0.00039144122

Batch 67550, train_perplexity=1.0003915, train_loss=0.00039136974

Batch 67560, train_perplexity=1.0003914, train_loss=0.0003912863

Batch 67570, train_perplexity=1.0003914, train_loss=0.0003912565

Batch 67580, train_perplexity=1.0003912, train_loss=0.00039119995

Batch 67590, train_perplexity=1.0003912, train_loss=0.00039114332

Batch 67600, train_perplexity=1.0003911, train_loss=0.00039101523

Batch 67610, train_perplexity=1.0003911, train_loss=0.0003910063

Batch 67620, train_perplexity=1.000391, train_loss=0.0003909348

Batch 67630, train_perplexity=1.000391, train_loss=0.00039090798

Batch 67640, train_perplexity=1.0003909, train_loss=0.00039076502

Batch 67650, train_perplexity=1.0003909, train_loss=0.0003908067

Batch 67660, train_perplexity=1.0003908, train_loss=0.00039067862

Batch 67670, train_perplexity=1.0003906, train_loss=0.00039056543

Batch 67680, train_perplexity=1.0003905, train_loss=0.00039047305

Batch 67690, train_perplexity=1.0003905, train_loss=0.00039046115

Batch 67700, train_perplexity=1.0003904, train_loss=0.0003903688

Batch 67710, train_perplexity=1.0003904, train_loss=0.00039035094

Batch 67720, train_perplexity=1.0003903, train_loss=0.0003902437

Batch 67730, train_perplexity=1.0003903, train_loss=0.0003901811

Batch 67740, train_perplexity=1.0003903, train_loss=0.00039020198

Batch 67750, train_perplexity=1.00039, train_loss=0.00039002622

Batch 67760, train_perplexity=1.00039, train_loss=0.00038999942

Batch 67770, train_perplexity=1.00039, train_loss=0.00038994878

Batch 67780, train_perplexity=1.0003899, train_loss=0.0003898951

Batch 67790, train_perplexity=1.0003899, train_loss=0.00038983853

Batch 67800, train_perplexity=1.0003899, train_loss=0.0003898058

Batch 67810, train_perplexity=1.0003897, train_loss=0.0003896628

Batch 67820, train_perplexity=1.0003897, train_loss=0.00038963003

Batch 67830, train_perplexity=1.0003897, train_loss=0.0003895675
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 67840, train_perplexity=1.0003896, train_loss=0.0003894751

Batch 67850, train_perplexity=1.0003895, train_loss=0.00038941554

Batch 67860, train_perplexity=1.0003893, train_loss=0.00038927252

Batch 67870, train_perplexity=1.0003893, train_loss=0.00038927555

Batch 67880, train_perplexity=1.0003893, train_loss=0.00038923975

Batch 67890, train_perplexity=1.0003893, train_loss=0.0003892249

Batch 67900, train_perplexity=1.0003892, train_loss=0.00038910276

Batch 67910, train_perplexity=1.0003891, train_loss=0.00038902528

Batch 67920, train_perplexity=1.0003891, train_loss=0.00038897467

Batch 67930, train_perplexity=1.000389, train_loss=0.0003888823

Batch 67940, train_perplexity=1.0003889, train_loss=0.00038876612

Batch 67950, train_perplexity=1.0003889, train_loss=0.0003888287

Batch 67960, train_perplexity=1.0003887, train_loss=0.00038867973

Batch 67970, train_perplexity=1.0003889, train_loss=0.0003887542

Batch 67980, train_perplexity=1.0003886, train_loss=0.00038860526

Batch 67990, train_perplexity=1.0003886, train_loss=0.00038859932

Batch 68000, train_perplexity=1.0003885, train_loss=0.00038840566

Batch 68010, train_perplexity=1.0003884, train_loss=0.00038833715

Batch 68020, train_perplexity=1.0003884, train_loss=0.000388361

Batch 68030, train_perplexity=1.0003884, train_loss=0.00038832228

Batch 68040, train_perplexity=1.0003884, train_loss=0.00038833715

Batch 68050, train_perplexity=1.0003883, train_loss=0.00038814353

Batch 68060, train_perplexity=1.0003881, train_loss=0.00038808392

Batch 68070, train_perplexity=1.000388, train_loss=0.00038800947

Batch 68080, train_perplexity=1.000388, train_loss=0.00038799754

Batch 68090, train_perplexity=1.0003879, train_loss=0.00038786052

Batch 68100, train_perplexity=1.0003879, train_loss=0.00038782478

Batch 68110, train_perplexity=1.0003878, train_loss=0.0003877175

Batch 68120, train_perplexity=1.0003878, train_loss=0.00038765796

Batch 68130, train_perplexity=1.0003878, train_loss=0.00038765796

Batch 68140, train_perplexity=1.0003877, train_loss=0.00038754178

Batch 68150, train_perplexity=1.0003875, train_loss=0.0003875328

Batch 68160, train_perplexity=1.0003875, train_loss=0.00038750304

Batch 68170, train_perplexity=1.0003874, train_loss=0.0003873809

Batch 68180, train_perplexity=1.0003873, train_loss=0.00038728557

Batch 68190, train_perplexity=1.0003873, train_loss=0.00038726174

Batch 68200, train_perplexity=1.0003873, train_loss=0.00038722303

Batch 68210, train_perplexity=1.0003872, train_loss=0.00038716343

Batch 68220, train_perplexity=1.0003872, train_loss=0.00038710385

Batch 68230, train_perplexity=1.0003871, train_loss=0.00038696977

Batch 68240, train_perplexity=1.0003871, train_loss=0.0003869579

Batch 68250, train_perplexity=1.000387, train_loss=0.00038690132

Batch 68260, train_perplexity=1.0003868, train_loss=0.0003867613

Batch 68270, train_perplexity=1.0003868, train_loss=0.00038671662

Batch 68280, train_perplexity=1.0003868, train_loss=0.00038674043

Batch 68290, train_perplexity=1.0003867, train_loss=0.00038660935

Batch 68300, train_perplexity=1.0003867, train_loss=0.00038662722

Batch 68310, train_perplexity=1.0003866, train_loss=0.0003864902

Batch 68320, train_perplexity=1.0003865, train_loss=0.00038637698

Batch 68330, train_perplexity=1.0003865, train_loss=0.00038643955

Batch 68340, train_perplexity=1.0003864, train_loss=0.00038626677

Batch 68350, train_perplexity=1.0003864, train_loss=0.00038628763

Batch 68360, train_perplexity=1.0003862, train_loss=0.00038615655

Batch 68370, train_perplexity=1.0003862, train_loss=0.00038611784

Batch 68380, train_perplexity=1.0003861, train_loss=0.00038609694

Batch 68390, train_perplexity=1.0003861, train_loss=0.00038600463

Batch 68400, train_perplexity=1.000386, train_loss=0.00038590035

Batch 68410, train_perplexity=1.000386, train_loss=0.00038594208

Batch 68420, train_perplexity=1.0003859, train_loss=0.00038576033

Batch 68430, train_perplexity=1.0003858, train_loss=0.00038573652

Batch 68440, train_perplexity=1.0003858, train_loss=0.0003857127

Batch 68450, train_perplexity=1.0003856, train_loss=0.00038562034

Batch 68460, train_perplexity=1.0003856, train_loss=0.00038557267

Batch 68470, train_perplexity=1.0003855, train_loss=0.00038547436

Batch 68480, train_perplexity=1.0003855, train_loss=0.00038542668

Batch 68490, train_perplexity=1.0003854, train_loss=0.00038535224

Batch 68500, train_perplexity=1.0003854, train_loss=0.00038537307

Batch 68510, train_perplexity=1.0003854, train_loss=0.0003852867

Batch 68520, train_perplexity=1.0003853, train_loss=0.00038520328

Batch 68530, train_perplexity=1.0003852, train_loss=0.00038506027

Batch 68540, train_perplexity=1.0003852, train_loss=0.0003850752

Batch 68550, train_perplexity=1.000385, train_loss=0.00038500072

Batch 68560, train_perplexity=1.000385, train_loss=0.0003849262

Batch 68570, train_perplexity=1.0003849, train_loss=0.00038490538

Batch 68580, train_perplexity=1.0003848, train_loss=0.00038478326

Batch 68590, train_perplexity=1.0003848, train_loss=0.0003847743

Batch 68600, train_perplexity=1.0003847, train_loss=0.00038461643

Batch 68610, train_perplexity=1.0003847, train_loss=0.00038458366

Batch 68620, train_perplexity=1.0003847, train_loss=0.0003846194

Batch 68630, train_perplexity=1.0003846, train_loss=0.00038448238

Batch 68640, train_perplexity=1.0003844, train_loss=0.0003844168

Batch 68650, train_perplexity=1.0003843, train_loss=0.0003842947

Batch 68660, train_perplexity=1.0003844, train_loss=0.00038433936

Batch 68670, train_perplexity=1.0003843, train_loss=0.00038422615

Batch 68680, train_perplexity=1.0003842, train_loss=0.00038417554

Batch 68690, train_perplexity=1.0003842, train_loss=0.0003841457

Batch 68700, train_perplexity=1.0003841, train_loss=0.0003840683

Batch 68710, train_perplexity=1.0003841, train_loss=0.00038404448

Batch 68720, train_perplexity=1.000384, train_loss=0.00038393127

Batch 68730, train_perplexity=1.000384, train_loss=0.0003838955

Batch 68740, train_perplexity=1.0003839, train_loss=0.00038383296

Batch 68750, train_perplexity=1.0003839, train_loss=0.00038380612

Batch 68760, train_perplexity=1.0003837, train_loss=0.00038367804

Batch 68770, train_perplexity=1.0003837, train_loss=0.00038364826

Batch 68780, train_perplexity=1.0003836, train_loss=0.00038354995

Batch 68790, train_perplexity=1.0003835, train_loss=0.00038346357

Batch 68800, train_perplexity=1.0003835, train_loss=0.00038340694

Batch 68810, train_perplexity=1.0003835, train_loss=0.00038336823

Batch 68820, train_perplexity=1.0003834, train_loss=0.0003833176

Batch 68830, train_perplexity=1.0003833, train_loss=0.0003831746

Batch 68840, train_perplexity=1.0003833, train_loss=0.00038322527

Batch 68850, train_perplexity=1.0003833, train_loss=0.00038318947

Batch 68860, train_perplexity=1.0003831, train_loss=0.00038302862

Batch 68870, train_perplexity=1.0003831, train_loss=0.00038301968

Batch 68880, train_perplexity=1.000383, train_loss=0.00038292437

Batch 68890, train_perplexity=1.0003829, train_loss=0.00038277838

Batch 68900, train_perplexity=1.0003828, train_loss=0.0003827188

Batch 68910, train_perplexity=1.0003828, train_loss=0.0003827635

Batch 68920, train_perplexity=1.0003828, train_loss=0.00038270094

Batch 68930, train_perplexity=1.0003827, train_loss=0.0003826265

Batch 68940, train_perplexity=1.0003827, train_loss=0.0003826205

Batch 68950, train_perplexity=1.0003827, train_loss=0.0003825639

Batch 68960, train_perplexity=1.0003825, train_loss=0.0003824567

Batch 68970, train_perplexity=1.0003824, train_loss=0.00038237026

Batch 68980, train_perplexity=1.0003824, train_loss=0.00038229878

Batch 68990, train_perplexity=1.0003823, train_loss=0.00038225114

Batch 69000, train_perplexity=1.0003823, train_loss=0.0003821826

Batch 69010, train_perplexity=1.0003822, train_loss=0.00038212002

Batch 69020, train_perplexity=1.0003822, train_loss=0.00038211708

Batch 69030, train_perplexity=1.0003821, train_loss=0.00038199194

Batch 69040, train_perplexity=1.000382, train_loss=0.00038193236

Batch 69050, train_perplexity=1.000382, train_loss=0.0003819056

Batch 69060, train_perplexity=1.000382, train_loss=0.00038182212
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69070, train_perplexity=1.000382, train_loss=0.0003818192

Batch 69080, train_perplexity=1.0003817, train_loss=0.00038161065

Batch 69090, train_perplexity=1.0003817, train_loss=0.0003816881

Batch 69100, train_perplexity=1.0003816, train_loss=0.00038149743

Batch 69110, train_perplexity=1.0003816, train_loss=0.00038148253

Batch 69120, train_perplexity=1.0003815, train_loss=0.00038140808

Batch 69130, train_perplexity=1.0003815, train_loss=0.00038141102

Batch 69140, train_perplexity=1.0003814, train_loss=0.00038132464

Batch 69150, train_perplexity=1.0003812, train_loss=0.0003811638

Batch 69160, train_perplexity=1.0003812, train_loss=0.00038111018

Batch 69170, train_perplexity=1.0003812, train_loss=0.0003811787

Batch 69180, train_perplexity=1.0003811, train_loss=0.00038108637

Batch 69190, train_perplexity=1.0003811, train_loss=0.00038102973

Batch 69200, train_perplexity=1.000381, train_loss=0.00038088375

Batch 69210, train_perplexity=1.000381, train_loss=0.00038091652

Batch 69220, train_perplexity=1.0003809, train_loss=0.00038077353

Batch 69230, train_perplexity=1.0003809, train_loss=0.00038079143

Batch 69240, train_perplexity=1.0003809, train_loss=0.0003807497

Batch 69250, train_perplexity=1.0003808, train_loss=0.00038065735

Batch 69260, train_perplexity=1.0003806, train_loss=0.00038055607

Batch 69270, train_perplexity=1.0003806, train_loss=0.0003805561

Batch 69280, train_perplexity=1.0003805, train_loss=0.0003804667

Batch 69290, train_perplexity=1.0003804, train_loss=0.0003802671

Batch 69300, train_perplexity=1.0003804, train_loss=0.0003803654

Batch 69310, train_perplexity=1.0003804, train_loss=0.0003803118

Batch 69320, train_perplexity=1.0003803, train_loss=0.0003801837

Batch 69330, train_perplexity=1.0003803, train_loss=0.0003801688

Batch 69340, train_perplexity=1.0003802, train_loss=0.00038007647

Batch 69350, train_perplexity=1.0003802, train_loss=0.00038007347

Batch 69360, train_perplexity=1.00038, train_loss=0.00037998112

Batch 69370, train_perplexity=1.0003799, train_loss=0.00037990365

Batch 69380, train_perplexity=1.0003799, train_loss=0.00037986197

Batch 69390, train_perplexity=1.0003798, train_loss=0.0003797607

Batch 69400, train_perplexity=1.0003799, train_loss=0.0003797905

Batch 69410, train_perplexity=1.0003797, train_loss=0.00037965344

Batch 69420, train_perplexity=1.0003796, train_loss=0.0003795075

Batch 69430, train_perplexity=1.0003796, train_loss=0.0003795462

Batch 69440, train_perplexity=1.0003794, train_loss=0.00037939724

Batch 69450, train_perplexity=1.0003794, train_loss=0.00037938534

Batch 69460, train_perplexity=1.0003793, train_loss=0.00037926913

Batch 69470, train_perplexity=1.0003793, train_loss=0.000379287

Batch 69480, train_perplexity=1.0003793, train_loss=0.00037920062

Batch 69490, train_perplexity=1.0003792, train_loss=0.00037916488

Batch 69500, train_perplexity=1.0003792, train_loss=0.00037911127

Batch 69510, train_perplexity=1.0003791, train_loss=0.00037904573

Batch 69520, train_perplexity=1.000379, train_loss=0.00037895335

Batch 69530, train_perplexity=1.000379, train_loss=0.00037887297

Batch 69540, train_perplexity=1.000379, train_loss=0.00037886697

Batch 69550, train_perplexity=1.000379, train_loss=0.00037887593

Batch 69560, train_perplexity=1.0003788, train_loss=0.0003787508

Batch 69570, train_perplexity=1.0003787, train_loss=0.00037866738

Batch 69580, train_perplexity=1.0003787, train_loss=0.00037862273

Batch 69590, train_perplexity=1.0003786, train_loss=0.0003785899

Batch 69600, train_perplexity=1.0003785, train_loss=0.0003784678

Batch 69610, train_perplexity=1.0003785, train_loss=0.00037844098

Batch 69620, train_perplexity=1.0003784, train_loss=0.00037829205

Batch 69630, train_perplexity=1.0003784, train_loss=0.00037832183

Batch 69640, train_perplexity=1.0003783, train_loss=0.00037819968

Batch 69650, train_perplexity=1.0003783, train_loss=0.000378158

Batch 69660, train_perplexity=1.0003783, train_loss=0.0003781282

Batch 69670, train_perplexity=1.0003781, train_loss=0.00037801795

Batch 69680, train_perplexity=1.000378, train_loss=0.00037795544

Batch 69690, train_perplexity=1.000378, train_loss=0.0003779286

Batch 69700, train_perplexity=1.000378, train_loss=0.00037792264

Batch 69710, train_perplexity=1.0003778, train_loss=0.00037773198

Batch 69720, train_perplexity=1.0003779, train_loss=0.0003778005

Batch 69730, train_perplexity=1.0003777, train_loss=0.00037760683

Batch 69740, train_perplexity=1.0003777, train_loss=0.00037759196

Batch 69750, train_perplexity=1.0003777, train_loss=0.00037756813

Batch 69760, train_perplexity=1.0003775, train_loss=0.00037750258

Batch 69770, train_perplexity=1.0003777, train_loss=0.0003775473

Batch 69780, train_perplexity=1.0003774, train_loss=0.0003773745

Batch 69790, train_perplexity=1.0003774, train_loss=0.0003773775

Batch 69800, train_perplexity=1.0003773, train_loss=0.00037726728

Batch 69810, train_perplexity=1.0003774, train_loss=0.00037732086

Batch 69820, train_perplexity=1.0003772, train_loss=0.00037707065

Batch 69830, train_perplexity=1.0003772, train_loss=0.0003770796

Batch 69840, train_perplexity=1.0003772, train_loss=0.00037706172

Batch 69850, train_perplexity=1.0003769, train_loss=0.00037692467

Batch 69860, train_perplexity=1.000377, train_loss=0.0003769485

Batch 69870, train_perplexity=1.0003769, train_loss=0.00037683532

Batch 69880, train_perplexity=1.0003768, train_loss=0.00037676084

Batch 69890, train_perplexity=1.0003768, train_loss=0.00037671617

Batch 69900, train_perplexity=1.0003767, train_loss=0.0003766715

Batch 69910, train_perplexity=1.0003767, train_loss=0.0003766119

Batch 69920, train_perplexity=1.0003766, train_loss=0.00037654635

Batch 69930, train_perplexity=1.0003765, train_loss=0.0003764242

Batch 69940, train_perplexity=1.0003765, train_loss=0.00037641826

Batch 69950, train_perplexity=1.0003765, train_loss=0.0003763557

Batch 69960, train_perplexity=1.0003763, train_loss=0.00037628715

Batch 69970, train_perplexity=1.0003763, train_loss=0.00037624844

Batch 69980, train_perplexity=1.0003762, train_loss=0.00037614716

Batch 69990, train_perplexity=1.0003762, train_loss=0.0003761889

Batch 70000, train_perplexity=1.0003762, train_loss=0.00037609652

Batch 70010, train_perplexity=1.0003761, train_loss=0.0003759863

Batch 70020, train_perplexity=1.000376, train_loss=0.00037592073

Batch 70030, train_perplexity=1.000376, train_loss=0.0003759386

Batch 70040, train_perplexity=1.0003757, train_loss=0.00037572708

Batch 70050, train_perplexity=1.0003759, train_loss=0.00037575988

Batch 70060, train_perplexity=1.0003759, train_loss=0.0003757688

Batch 70070, train_perplexity=1.0003757, train_loss=0.00037563476

Batch 70080, train_perplexity=1.0003756, train_loss=0.00037557818

Batch 70090, train_perplexity=1.0003756, train_loss=0.0003755454

Batch 70100, train_perplexity=1.0003755, train_loss=0.0003754024

Batch 70110, train_perplexity=1.0003756, train_loss=0.0003755305

Batch 70120, train_perplexity=1.0003754, train_loss=0.00037532492

Batch 70130, train_perplexity=1.0003753, train_loss=0.00037525347

Batch 70140, train_perplexity=1.0003753, train_loss=0.0003752415

Batch 70150, train_perplexity=1.0003752, train_loss=0.00037507172

Batch 70160, train_perplexity=1.0003752, train_loss=0.00037510748

Batch 70170, train_perplexity=1.0003752, train_loss=0.00037509558

Batch 70180, train_perplexity=1.000375, train_loss=0.00037497643

Batch 70190, train_perplexity=1.000375, train_loss=0.00037492876

Batch 70200, train_perplexity=1.0003749, train_loss=0.00037490192

Batch 70210, train_perplexity=1.0003749, train_loss=0.00037486915

Batch 70220, train_perplexity=1.0003748, train_loss=0.0003747649

Batch 70230, train_perplexity=1.0003748, train_loss=0.0003746964

Batch 70240, train_perplexity=1.0003747, train_loss=0.0003746368

Batch 70250, train_perplexity=1.0003747, train_loss=0.00037463385

Batch 70260, train_perplexity=1.0003746, train_loss=0.0003744432

Batch 70270, train_perplexity=1.0003746, train_loss=0.0003744908

Batch 70280, train_perplexity=1.0003746, train_loss=0.0003744551

Batch 70290, train_perplexity=1.0003743, train_loss=0.0003742972

Batch 70300, train_perplexity=1.0003744, train_loss=0.00037434488
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70310, train_perplexity=1.0003743, train_loss=0.00037425844

Batch 70320, train_perplexity=1.0003743, train_loss=0.00037421082

Batch 70330, train_perplexity=1.0003741, train_loss=0.00037402907

Batch 70340, train_perplexity=1.0003741, train_loss=0.0003740529

Batch 70350, train_perplexity=1.0003741, train_loss=0.00037402014

Batch 70360, train_perplexity=1.0003741, train_loss=0.0003739993

Batch 70370, train_perplexity=1.000374, train_loss=0.00037383544

Batch 70380, train_perplexity=1.0003738, train_loss=0.00037380564

Batch 70390, train_perplexity=1.0003738, train_loss=0.0003737729

Batch 70400, train_perplexity=1.0003737, train_loss=0.00037363288

Batch 70410, train_perplexity=1.0003737, train_loss=0.00037360907

Batch 70420, train_perplexity=1.0003736, train_loss=0.00037352263

Batch 70430, train_perplexity=1.0003735, train_loss=0.0003734631

Batch 70440, train_perplexity=1.0003736, train_loss=0.0003735316

Batch 70450, train_perplexity=1.0003736, train_loss=0.00037348986

Batch 70460, train_perplexity=1.0003732, train_loss=0.0003732158

Batch 70470, train_perplexity=1.0003734, train_loss=0.0003732575

Batch 70480, train_perplexity=1.0003734, train_loss=0.0003732486

Batch 70490, train_perplexity=1.0003732, train_loss=0.00037312048

Batch 70500, train_perplexity=1.0003732, train_loss=0.00037312642

Batch 70510, train_perplexity=1.0003731, train_loss=0.00037308177

Batch 70520, train_perplexity=1.000373, train_loss=0.00037295365

Batch 70530, train_perplexity=1.000373, train_loss=0.00037289114

Batch 70540, train_perplexity=1.0003729, train_loss=0.0003728196

Batch 70550, train_perplexity=1.0003728, train_loss=0.0003727511

Batch 70560, train_perplexity=1.0003729, train_loss=0.00037276302

Batch 70570, train_perplexity=1.0003728, train_loss=0.00037270342

Batch 70580, train_perplexity=1.0003726, train_loss=0.00037256937

Batch 70590, train_perplexity=1.0003725, train_loss=0.0003725098

Batch 70600, train_perplexity=1.0003725, train_loss=0.00037251573

Batch 70610, train_perplexity=1.0003725, train_loss=0.0003725098

Batch 70620, train_perplexity=1.0003724, train_loss=0.0003723966

Batch 70630, train_perplexity=1.0003723, train_loss=0.00037227743

Batch 70640, train_perplexity=1.0003723, train_loss=0.00037225956

Batch 70650, train_perplexity=1.0003722, train_loss=0.00037213444

Batch 70660, train_perplexity=1.0003722, train_loss=0.00037214337

Batch 70670, train_perplexity=1.0003722, train_loss=0.00037208077

Batch 70680, train_perplexity=1.0003722, train_loss=0.00037204503

Batch 70690, train_perplexity=1.0003719, train_loss=0.00037189614

Batch 70700, train_perplexity=1.0003719, train_loss=0.0003718276

Batch 70710, train_perplexity=1.0003719, train_loss=0.00037182757

Batch 70720, train_perplexity=1.0003718, train_loss=0.00037176802

Batch 70730, train_perplexity=1.0003718, train_loss=0.00037168758

Batch 70740, train_perplexity=1.0003718, train_loss=0.00037168758

Batch 70750, train_perplexity=1.0003716, train_loss=0.00037156546

Batch 70760, train_perplexity=1.0003716, train_loss=0.00037152076

Batch 70770, train_perplexity=1.0003716, train_loss=0.00037152076

Batch 70780, train_perplexity=1.0003716, train_loss=0.0003715029

Batch 70790, train_perplexity=1.0003713, train_loss=0.00037130032

Batch 70800, train_perplexity=1.0003713, train_loss=0.00037128536

Batch 70810, train_perplexity=1.0003713, train_loss=0.00037123775

Batch 70820, train_perplexity=1.0003713, train_loss=0.00037125265

Batch 70830, train_perplexity=1.0003712, train_loss=0.0003711573

Batch 70840, train_perplexity=1.0003711, train_loss=0.00037103816

Batch 70850, train_perplexity=1.0003711, train_loss=0.00037097262

Batch 70860, train_perplexity=1.0003711, train_loss=0.00037097855

Batch 70870, train_perplexity=1.000371, train_loss=0.00037089817

Batch 70880, train_perplexity=1.000371, train_loss=0.00037091007

Batch 70890, train_perplexity=1.0003709, train_loss=0.00037076406

Batch 70900, train_perplexity=1.0003709, train_loss=0.00037074322

Batch 70910, train_perplexity=1.0003707, train_loss=0.00037065387

Batch 70920, train_perplexity=1.0003706, train_loss=0.00037060917

Batch 70930, train_perplexity=1.0003707, train_loss=0.000370636

Batch 70940, train_perplexity=1.0003705, train_loss=0.0003704602

Batch 70950, train_perplexity=1.0003705, train_loss=0.00037045724

Batch 70960, train_perplexity=1.0003705, train_loss=0.0003704185

Batch 70970, train_perplexity=1.0003704, train_loss=0.00037027552

Batch 70980, train_perplexity=1.0003703, train_loss=0.00037022785

Batch 70990, train_perplexity=1.0003703, train_loss=0.00037021594

Batch 71000, train_perplexity=1.0003703, train_loss=0.00037015934

Batch 71010, train_perplexity=1.0003701, train_loss=0.00037007296

Batch 71020, train_perplexity=1.0003701, train_loss=0.0003700402

Batch 71030, train_perplexity=1.00037, train_loss=0.00036992994

Batch 71040, train_perplexity=1.0003699, train_loss=0.0003698793

Batch 71050, train_perplexity=1.0003699, train_loss=0.00036978401

Batch 71060, train_perplexity=1.0003699, train_loss=0.00036985253

Batch 71070, train_perplexity=1.0003698, train_loss=0.0003697542

Batch 71080, train_perplexity=1.0003698, train_loss=0.00036974228

Batch 71090, train_perplexity=1.0003697, train_loss=0.0003696261

Batch 71100, train_perplexity=1.0003695, train_loss=0.00036948608

Batch 71110, train_perplexity=1.0003695, train_loss=0.00036951288

Batch 71120, train_perplexity=1.0003694, train_loss=0.00036942054

Batch 71130, train_perplexity=1.0003694, train_loss=0.00036940566

Batch 71140, train_perplexity=1.0003694, train_loss=0.00036936096

Batch 71150, train_perplexity=1.0003693, train_loss=0.00036928052

Batch 71160, train_perplexity=1.0003693, train_loss=0.0003691852

Batch 71170, train_perplexity=1.0003692, train_loss=0.0003691703

Batch 71180, train_perplexity=1.0003692, train_loss=0.00036906605

Batch 71190, train_perplexity=1.0003692, train_loss=0.000369072

Batch 71200, train_perplexity=1.0003691, train_loss=0.00036894687

Batch 71210, train_perplexity=1.000369, train_loss=0.00036888727

Batch 71220, train_perplexity=1.000369, train_loss=0.00036885156

Batch 71230, train_perplexity=1.000369, train_loss=0.0003688307

Batch 71240, train_perplexity=1.0003687, train_loss=0.00036870257

Batch 71250, train_perplexity=1.0003687, train_loss=0.00036867283

Batch 71260, train_perplexity=1.0003687, train_loss=0.0003686371

Batch 71270, train_perplexity=1.0003686, train_loss=0.00036855665

Batch 71280, train_perplexity=1.0003686, train_loss=0.00036853878

Batch 71290, train_perplexity=1.0003686, train_loss=0.00036852685

Batch 71300, train_perplexity=1.0003684, train_loss=0.00036833616

Batch 71310, train_perplexity=1.0003684, train_loss=0.00036831235

Batch 71320, train_perplexity=1.0003684, train_loss=0.0003682915

Batch 71330, train_perplexity=1.0003684, train_loss=0.00036824384

Batch 71340, train_perplexity=1.0003682, train_loss=0.0003681634

Batch 71350, train_perplexity=1.0003681, train_loss=0.00036806508

Batch 71360, train_perplexity=1.0003681, train_loss=0.00036799954

Batch 71370, train_perplexity=1.000368, train_loss=0.0003679489

Batch 71380, train_perplexity=1.0003679, train_loss=0.00036786252

Batch 71390, train_perplexity=1.0003679, train_loss=0.0003678238

Batch 71400, train_perplexity=1.0003679, train_loss=0.00036778214

Batch 71410, train_perplexity=1.0003678, train_loss=0.00036768676

Batch 71420, train_perplexity=1.0003678, train_loss=0.0003676659

Batch 71430, train_perplexity=1.0003678, train_loss=0.0003676838

Batch 71440, train_perplexity=1.0003676, train_loss=0.00036761822

Batch 71450, train_perplexity=1.0003676, train_loss=0.00036753184

Batch 71460, train_perplexity=1.0003675, train_loss=0.0003674067

Batch 71470, train_perplexity=1.0003674, train_loss=0.00036737393

Batch 71480, train_perplexity=1.0003674, train_loss=0.00036731735

Batch 71490, train_perplexity=1.0003674, train_loss=0.00036730245

Batch 71500, train_perplexity=1.0003673, train_loss=0.00036726374

Batch 71510, train_perplexity=1.0003673, train_loss=0.0003672399

Batch 71520, train_perplexity=1.000367, train_loss=0.00036701944

Batch 71530, train_perplexity=1.0003672, train_loss=0.0003670403

Batch 71540, train_perplexity=1.000367, train_loss=0.0003669837
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 71550, train_perplexity=1.0003669, train_loss=0.0003669152

Batch 71560, train_perplexity=1.000367, train_loss=0.00036696283

Batch 71570, train_perplexity=1.0003669, train_loss=0.0003668407

Batch 71580, train_perplexity=1.0003668, train_loss=0.00036678708

Batch 71590, train_perplexity=1.0003668, train_loss=0.00036678708

Batch 71600, train_perplexity=1.0003669, train_loss=0.00036680495

Batch 71610, train_perplexity=1.0003667, train_loss=0.00036662922

Batch 71620, train_perplexity=1.0003666, train_loss=0.0003664981

Batch 71630, train_perplexity=1.0003664, train_loss=0.00036639685

Batch 71640, train_perplexity=1.0003666, train_loss=0.0003664564

Batch 71650, train_perplexity=1.0003664, train_loss=0.00036633728

Batch 71660, train_perplexity=1.0003664, train_loss=0.00036637002

Batch 71670, train_perplexity=1.0003662, train_loss=0.00036615552

Batch 71680, train_perplexity=1.0003663, train_loss=0.00036622703

Batch 71690, train_perplexity=1.0003663, train_loss=0.00036623894

Batch 71700, train_perplexity=1.0003662, train_loss=0.00036615256

Batch 71710, train_perplexity=1.0003661, train_loss=0.00036600954

Batch 71720, train_perplexity=1.0003661, train_loss=0.0003660423

Batch 71730, train_perplexity=1.0003661, train_loss=0.00036599167

Batch 71740, train_perplexity=1.0003659, train_loss=0.0003658457

Batch 71750, train_perplexity=1.000366, train_loss=0.00036585762

Batch 71760, train_perplexity=1.0003659, train_loss=0.0003657742

Batch 71770, train_perplexity=1.0003659, train_loss=0.0003657355

Batch 71780, train_perplexity=1.0003657, train_loss=0.00036565505

Batch 71790, train_perplexity=1.0003657, train_loss=0.00036563125

Batch 71800, train_perplexity=1.0003655, train_loss=0.00036542566

Batch 71810, train_perplexity=1.0003656, train_loss=0.00036560145

Batch 71820, train_perplexity=1.0003655, train_loss=0.00036537801

Batch 71830, train_perplexity=1.0003654, train_loss=0.00036533928

Batch 71840, train_perplexity=1.0003654, train_loss=0.00036532735

Batch 71850, train_perplexity=1.0003653, train_loss=0.00036515456

Batch 71860, train_perplexity=1.0003653, train_loss=0.00036524696

Batch 71870, train_perplexity=1.0003653, train_loss=0.000365238

Batch 71880, train_perplexity=1.0003651, train_loss=0.00036504434

Batch 71890, train_perplexity=1.0003651, train_loss=0.00036502944

Batch 71900, train_perplexity=1.000365, train_loss=0.00036496093

Batch 71910, train_perplexity=1.0003649, train_loss=0.00036488345

Batch 71920, train_perplexity=1.0003649, train_loss=0.00036485068

Batch 71930, train_perplexity=1.0003649, train_loss=0.00036480007

Batch 71940, train_perplexity=1.0003648, train_loss=0.00036472856

Batch 71950, train_perplexity=1.0003648, train_loss=0.00036469876

Batch 71960, train_perplexity=1.0003647, train_loss=0.00036463025

Batch 71970, train_perplexity=1.0003647, train_loss=0.00036461238

Batch 71980, train_perplexity=1.0003645, train_loss=0.00036443066

Batch 71990, train_perplexity=1.0003644, train_loss=0.00036439195

Batch 72000, train_perplexity=1.0003644, train_loss=0.00036437408

Batch 72010, train_perplexity=1.0003644, train_loss=0.00036431447

Batch 72020, train_perplexity=1.0003643, train_loss=0.00036422213

Batch 72030, train_perplexity=1.0003643, train_loss=0.00036427277

Batch 72040, train_perplexity=1.0003643, train_loss=0.00036425196

Batch 72050, train_perplexity=1.0003642, train_loss=0.00036407018

Batch 72060, train_perplexity=1.0003641, train_loss=0.0003640225

Batch 72070, train_perplexity=1.0003641, train_loss=0.00036396892

Batch 72080, train_perplexity=1.000364, train_loss=0.00036392722

Batch 72090, train_perplexity=1.000364, train_loss=0.00036389445

Batch 72100, train_perplexity=1.0003638, train_loss=0.0003637991

Batch 72110, train_perplexity=1.0003638, train_loss=0.00036379613

Batch 72120, train_perplexity=1.0003638, train_loss=0.00036376336

Batch 72130, train_perplexity=1.0003637, train_loss=0.0003636591

Batch 72140, train_perplexity=1.0003636, train_loss=0.0003635727

Batch 72150, train_perplexity=1.0003637, train_loss=0.00036360844

Batch 72160, train_perplexity=1.0003635, train_loss=0.0003634625

Batch 72170, train_perplexity=1.0003635, train_loss=0.00036342672

Batch 72180, train_perplexity=1.0003635, train_loss=0.00036338205

Batch 72190, train_perplexity=1.0003633, train_loss=0.00036323903

Batch 72200, train_perplexity=1.0003633, train_loss=0.00036328373

Batch 72210, train_perplexity=1.0003632, train_loss=0.00036318542

Batch 72220, train_perplexity=1.0003632, train_loss=0.00036315562

Batch 72230, train_perplexity=1.0003631, train_loss=0.00036309604

Batch 72240, train_perplexity=1.0003631, train_loss=0.0003630186

Batch 72250, train_perplexity=1.0003631, train_loss=0.00036299476

Batch 72260, train_perplexity=1.0003631, train_loss=0.00036299476

Batch 72270, train_perplexity=1.0003629, train_loss=0.00036285777

Batch 72280, train_perplexity=1.0003629, train_loss=0.00036285474

Batch 72290, train_perplexity=1.0003628, train_loss=0.00036273856

Batch 72300, train_perplexity=1.0003629, train_loss=0.00036275346

Batch 72310, train_perplexity=1.0003626, train_loss=0.00036261347

Batch 72320, train_perplexity=1.0003628, train_loss=0.00036264322

Batch 72330, train_perplexity=1.0003628, train_loss=0.0003626492

Batch 72340, train_perplexity=1.0003626, train_loss=0.0003625509

Batch 72350, train_perplexity=1.0003625, train_loss=0.00036244665

Batch 72360, train_perplexity=1.0003625, train_loss=0.0003624109

Batch 72370, train_perplexity=1.0003624, train_loss=0.00036233343

Batch 72380, train_perplexity=1.0003624, train_loss=0.0003622798

Batch 72390, train_perplexity=1.0003623, train_loss=0.00036221129

Batch 72400, train_perplexity=1.0003622, train_loss=0.0003621308

Batch 72410, train_perplexity=1.0003622, train_loss=0.00036212488

Batch 72420, train_perplexity=1.000362, train_loss=0.00036198192

Batch 72430, train_perplexity=1.0003619, train_loss=0.00036189548

Batch 72440, train_perplexity=1.000362, train_loss=0.00036195805

Batch 72450, train_perplexity=1.0003619, train_loss=0.0003618955

Batch 72460, train_perplexity=1.0003618, train_loss=0.0003617942

Batch 72470, train_perplexity=1.0003618, train_loss=0.0003617406

Batch 72480, train_perplexity=1.0003617, train_loss=0.0003616661

Batch 72490, train_perplexity=1.0003617, train_loss=0.00036167208

Batch 72500, train_perplexity=1.0003616, train_loss=0.0003615261

Batch 72510, train_perplexity=1.0003617, train_loss=0.0003615857

Batch 72520, train_perplexity=1.0003614, train_loss=0.00036141885

Batch 72530, train_perplexity=1.0003616, train_loss=0.00036145461

Batch 72540, train_perplexity=1.0003614, train_loss=0.00036135927

Batch 72550, train_perplexity=1.0003614, train_loss=0.00036135927

Batch 72560, train_perplexity=1.0003613, train_loss=0.00036127586

Batch 72570, train_perplexity=1.0003612, train_loss=0.0003611984

Batch 72580, train_perplexity=1.0003612, train_loss=0.00036117755

Batch 72590, train_perplexity=1.0003612, train_loss=0.00036110607

Batch 72600, train_perplexity=1.0003612, train_loss=0.0003610852

Batch 72610, train_perplexity=1.0003611, train_loss=0.00036103753

Batch 72620, train_perplexity=1.000361, train_loss=0.00036088264

Batch 72630, train_perplexity=1.000361, train_loss=0.00036093325

Batch 72640, train_perplexity=1.000361, train_loss=0.0003608588

Batch 72650, train_perplexity=1.0003607, train_loss=0.00036068898

Batch 72660, train_perplexity=1.0003608, train_loss=0.00036075752

Batch 72670, train_perplexity=1.0003607, train_loss=0.0003606681

Batch 72680, train_perplexity=1.0003607, train_loss=0.00036061747

Batch 72690, train_perplexity=1.0003607, train_loss=0.00036063534

Batch 72700, train_perplexity=1.0003605, train_loss=0.00036046855

Batch 72710, train_perplexity=1.0003605, train_loss=0.00036044174

Batch 72720, train_perplexity=1.0003605, train_loss=0.0003604298

Batch 72730, train_perplexity=1.0003605, train_loss=0.00036037026

Batch 72740, train_perplexity=1.0003604, train_loss=0.0003602719

Batch 72750, train_perplexity=1.0003603, train_loss=0.00036021235

Batch 72760, train_perplexity=1.0003601, train_loss=0.00036009314

Batch 72770, train_perplexity=1.0003603, train_loss=0.0003601557
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72780, train_perplexity=1.0003601, train_loss=0.00036006043

Batch 72790, train_perplexity=1.0003601, train_loss=0.0003600187

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 72800, train_perplexity=1.00036, train_loss=0.00035997998

Batch 72810, train_perplexity=1.0003599, train_loss=0.00035986974

Batch 72820, train_perplexity=1.0003599, train_loss=0.00035980422

Batch 72830, train_perplexity=1.0003598, train_loss=0.00035969098

Batch 72840, train_perplexity=1.0003598, train_loss=0.00035973568

Batch 72850, train_perplexity=1.0003597, train_loss=0.00035960757

Batch 72860, train_perplexity=1.0003597, train_loss=0.0003596493

Batch 72870, train_perplexity=1.0003597, train_loss=0.00035958376

Batch 72880, train_perplexity=1.0003597, train_loss=0.00035958376

Batch 72890, train_perplexity=1.0003594, train_loss=0.00035933946

Batch 72900, train_perplexity=1.0003594, train_loss=0.00035936927

Batch 72910, train_perplexity=1.0003595, train_loss=0.00035942884

Batch 72920, train_perplexity=1.0003593, train_loss=0.00035925012

Batch 72930, train_perplexity=1.0003593, train_loss=0.00035923522

Batch 72940, train_perplexity=1.0003592, train_loss=0.00035913393

Batch 72950, train_perplexity=1.0003592, train_loss=0.00035905646

Batch 72960, train_perplexity=1.0003592, train_loss=0.0003590684

Batch 72970, train_perplexity=1.000359, train_loss=0.00035897602

Batch 72980, train_perplexity=1.0003589, train_loss=0.00035891944

Batch 72990, train_perplexity=1.0003588, train_loss=0.00035878236

Batch 73000, train_perplexity=1.0003589, train_loss=0.00035890454

Batch 73010, train_perplexity=1.0003588, train_loss=0.00035876752

Batch 73020, train_perplexity=1.0003588, train_loss=0.0003587854

Batch 73030, train_perplexity=1.0003588, train_loss=0.00035871984

Batch 73040, train_perplexity=1.0003587, train_loss=0.00035861853

Batch 73050, train_perplexity=1.0003587, train_loss=0.00035860363

Batch 73060, train_perplexity=1.0003586, train_loss=0.00035853215

Batch 73070, train_perplexity=1.0003586, train_loss=0.0003584964

Batch 73080, train_perplexity=1.0003585, train_loss=0.00035839807

Batch 73090, train_perplexity=1.0003585, train_loss=0.00035836233

Batch 73100, train_perplexity=1.0003583, train_loss=0.00035826105

Batch 73110, train_perplexity=1.0003582, train_loss=0.0003582104

Batch 73120, train_perplexity=1.0003582, train_loss=0.00035821635

Batch 73130, train_perplexity=1.0003582, train_loss=0.0003581419

Batch 73140, train_perplexity=1.0003581, train_loss=0.00035806146

Batch 73150, train_perplexity=1.0003581, train_loss=0.00035804062

Batch 73160, train_perplexity=1.000358, train_loss=0.0003579393

Batch 73170, train_perplexity=1.000358, train_loss=0.00035795418

Batch 73180, train_perplexity=1.000358, train_loss=0.00035790354

Batch 73190, train_perplexity=1.0003579, train_loss=0.00035784696

Batch 73200, train_perplexity=1.0003577, train_loss=0.00035771885

Batch 73210, train_perplexity=1.0003577, train_loss=0.00035766224

Batch 73220, train_perplexity=1.0003577, train_loss=0.00035770694

Batch 73230, train_perplexity=1.0003576, train_loss=0.00035760866

Batch 73240, train_perplexity=1.0003576, train_loss=0.00035758183

Batch 73250, train_perplexity=1.0003576, train_loss=0.0003575312

Batch 73260, train_perplexity=1.0003575, train_loss=0.00035739416

Batch 73270, train_perplexity=1.0003574, train_loss=0.0003573495

Batch 73280, train_perplexity=1.0003574, train_loss=0.00035732565

Batch 73290, train_perplexity=1.0003573, train_loss=0.00035718863

Batch 73300, train_perplexity=1.0003573, train_loss=0.00035724818

Batch 73310, train_perplexity=1.0003573, train_loss=0.00035725412

Batch 73320, train_perplexity=1.0003572, train_loss=0.00035714393

Batch 73330, train_perplexity=1.0003572, train_loss=0.00035705752

Batch 73340, train_perplexity=1.000357, train_loss=0.00035693834

Batch 73350, train_perplexity=1.000357, train_loss=0.0003570009

Batch 73360, train_perplexity=1.0003569, train_loss=0.00035683706

Batch 73370, train_perplexity=1.0003569, train_loss=0.000356843

Batch 73380, train_perplexity=1.0003569, train_loss=0.00035679535

Batch 73390, train_perplexity=1.0003568, train_loss=0.00035669704

Batch 73400, train_perplexity=1.0003568, train_loss=0.00035671494

Batch 73410, train_perplexity=1.0003567, train_loss=0.00035664043

Batch 73420, train_perplexity=1.0003567, train_loss=0.00035655405

Batch 73430, train_perplexity=1.0003566, train_loss=0.00035649445

Batch 73440, train_perplexity=1.0003564, train_loss=0.00035642

Batch 73450, train_perplexity=1.0003564, train_loss=0.00035634253

Batch 73460, train_perplexity=1.0003564, train_loss=0.00035632766

Batch 73470, train_perplexity=1.0003563, train_loss=0.0003562055

Batch 73480, train_perplexity=1.0003563, train_loss=0.00035626808

Batch 73490, train_perplexity=1.0003563, train_loss=0.00035623828

Batch 73500, train_perplexity=1.0003562, train_loss=0.00035611913

Batch 73510, train_perplexity=1.0003562, train_loss=0.0003560804

Batch 73520, train_perplexity=1.0003562, train_loss=0.00035613103

Batch 73530, train_perplexity=1.0003561, train_loss=0.00035604165

Batch 73540, train_perplexity=1.0003561, train_loss=0.00035595824

Batch 73550, train_perplexity=1.000356, train_loss=0.00035584206

Batch 73560, train_perplexity=1.0003558, train_loss=0.00035582716

Batch 73570, train_perplexity=1.0003557, train_loss=0.00035571097

Batch 73580, train_perplexity=1.0003557, train_loss=0.0003556246

Batch 73590, train_perplexity=1.0003556, train_loss=0.0003555799

Batch 73600, train_perplexity=1.0003557, train_loss=0.0003556395

Batch 73610, train_perplexity=1.0003556, train_loss=0.00035550538

Batch 73620, train_perplexity=1.0003556, train_loss=0.00035552628

Batch 73630, train_perplexity=1.0003555, train_loss=0.0003553922

Batch 73640, train_perplexity=1.0003555, train_loss=0.00035543094

Batch 73650, train_perplexity=1.0003554, train_loss=0.00035532075

Batch 73660, train_perplexity=1.0003554, train_loss=0.00035529688

Batch 73670, train_perplexity=1.0003552, train_loss=0.00035522238

Batch 73680, train_perplexity=1.0003551, train_loss=0.0003551092

Batch 73690, train_perplexity=1.0003551, train_loss=0.0003551122

Batch 73700, train_perplexity=1.0003551, train_loss=0.00035504068

Batch 73710, train_perplexity=1.0003551, train_loss=0.00035504665

Batch 73720, train_perplexity=1.000355, train_loss=0.0003549543

Batch 73730, train_perplexity=1.000355, train_loss=0.00035490663

Batch 73740, train_perplexity=1.0003549, train_loss=0.00035487383

Batch 73750, train_perplexity=1.0003548, train_loss=0.00035473087

Batch 73760, train_perplexity=1.0003548, train_loss=0.00035467427

Batch 73770, train_perplexity=1.0003548, train_loss=0.0003546832

Batch 73780, train_perplexity=1.0003546, train_loss=0.0003545998

Batch 73790, train_perplexity=1.0003546, train_loss=0.00035454618

Batch 73800, train_perplexity=1.0003545, train_loss=0.00035447767

Batch 73810, train_perplexity=1.0003545, train_loss=0.0003544449

Batch 73820, train_perplexity=1.0003545, train_loss=0.00035444187

Batch 73830, train_perplexity=1.0003544, train_loss=0.0003543853

Batch 73840, train_perplexity=1.0003543, train_loss=0.0003542512

Batch 73850, train_perplexity=1.0003543, train_loss=0.00035420357

Batch 73860, train_perplexity=1.0003542, train_loss=0.00035416483

Batch 73870, train_perplexity=1.0003543, train_loss=0.0003542006

Batch 73880, train_perplexity=1.0003542, train_loss=0.00035405462

Batch 73890, train_perplexity=1.000354, train_loss=0.0003540248

Batch 73900, train_perplexity=1.000354, train_loss=0.00035402778

Batch 73910, train_perplexity=1.0003539, train_loss=0.00035385205

Batch 73920, train_perplexity=1.000354, train_loss=0.00035393247

Batch 73930, train_perplexity=1.0003538, train_loss=0.00035379245

Batch 73940, train_perplexity=1.0003538, train_loss=0.00035373587
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 73950, train_perplexity=1.0003538, train_loss=0.00035378648

Batch 73960, train_perplexity=1.0003538, train_loss=0.00035370607

Batch 73970, train_perplexity=1.0003537, train_loss=0.00035358692

Batch 73980, train_perplexity=1.0003537, train_loss=0.00035359288

Batch 73990, train_perplexity=1.0003535, train_loss=0.0003534022

Batch 74000, train_perplexity=1.0003536, train_loss=0.00035349157

Batch 74010, train_perplexity=1.0003535, train_loss=0.00035339623

Batch 74020, train_perplexity=1.0003533, train_loss=0.00035330091

Batch 74030, train_perplexity=1.0003533, train_loss=0.00035329495

Batch 74040, train_perplexity=1.0003533, train_loss=0.00035325624

Batch 74050, train_perplexity=1.0003532, train_loss=0.00035309535

Batch 74060, train_perplexity=1.0003532, train_loss=0.00035314303

Batch 74070, train_perplexity=1.0003532, train_loss=0.00035309832

Batch 74080, train_perplexity=1.0003531, train_loss=0.0003529881

Batch 74090, train_perplexity=1.000353, train_loss=0.00035292257

Batch 74100, train_perplexity=1.0003529, train_loss=0.00035285106

Batch 74110, train_perplexity=1.0003529, train_loss=0.00035285408

Batch 74120, train_perplexity=1.0003529, train_loss=0.0003527408

Batch 74130, train_perplexity=1.0003529, train_loss=0.0003527885

Batch 74140, train_perplexity=1.0003527, train_loss=0.00035269314

Batch 74150, train_perplexity=1.0003527, train_loss=0.00035262766

Batch 74160, train_perplexity=1.0003526, train_loss=0.00035256805

Batch 74170, train_perplexity=1.0003526, train_loss=0.00035257102

Batch 74180, train_perplexity=1.0003526, train_loss=0.00035257402

Batch 74190, train_perplexity=1.0003525, train_loss=0.00035240123

Batch 74200, train_perplexity=1.0003524, train_loss=0.0003523774

Batch 74210, train_perplexity=1.0003524, train_loss=0.000352288

Batch 74220, train_perplexity=1.0003523, train_loss=0.00035223737

Batch 74230, train_perplexity=1.0003523, train_loss=0.00035218376

Batch 74240, train_perplexity=1.0003523, train_loss=0.00035214503

Batch 74250, train_perplexity=1.0003521, train_loss=0.00035208545

Batch 74260, train_perplexity=1.0003521, train_loss=0.00035206752

Batch 74270, train_perplexity=1.000352, train_loss=0.00035197224

Batch 74280, train_perplexity=1.000352, train_loss=0.00035191263

Batch 74290, train_perplexity=1.000352, train_loss=0.0003519186

Batch 74300, train_perplexity=1.0003519, train_loss=0.00035185905

Batch 74310, train_perplexity=1.0003519, train_loss=0.0003517965

Batch 74320, train_perplexity=1.0003518, train_loss=0.0003517637

Batch 74330, train_perplexity=1.0003518, train_loss=0.00035173094

Batch 74340, train_perplexity=1.0003517, train_loss=0.00035160282

Batch 74350, train_perplexity=1.0003517, train_loss=0.00035165943

Batch 74360, train_perplexity=1.0003517, train_loss=0.0003515492

Batch 74370, train_perplexity=1.0003515, train_loss=0.00035147474

Batch 74380, train_perplexity=1.0003515, train_loss=0.00035145087

Batch 74390, train_perplexity=1.0003514, train_loss=0.00035135556

Batch 74400, train_perplexity=1.0003514, train_loss=0.00035133175

Batch 74410, train_perplexity=1.0003513, train_loss=0.00035128108

Batch 74420, train_perplexity=1.0003513, train_loss=0.00035128405

Batch 74430, train_perplexity=1.0003512, train_loss=0.00035113812

Batch 74440, train_perplexity=1.0003511, train_loss=0.00035103978

Batch 74450, train_perplexity=1.0003512, train_loss=0.0003511202

Batch 74460, train_perplexity=1.0003511, train_loss=0.0003509504

Batch 74470, train_perplexity=1.0003511, train_loss=0.00035098914

Batch 74480, train_perplexity=1.000351, train_loss=0.00035088486

Batch 74490, train_perplexity=1.0003508, train_loss=0.00035080445

Batch 74500, train_perplexity=1.0003508, train_loss=0.00035078658

Batch 74510, train_perplexity=1.0003507, train_loss=0.00035067037

Batch 74520, train_perplexity=1.0003508, train_loss=0.000350724

Batch 74530, train_perplexity=1.0003507, train_loss=0.00035067933

Batch 74540, train_perplexity=1.0003506, train_loss=0.0003505542

Batch 74550, train_perplexity=1.0003505, train_loss=0.00035047077

Batch 74560, train_perplexity=1.0003506, train_loss=0.00035047968

Batch 74570, train_perplexity=1.0003505, train_loss=0.0003503963

Batch 74580, train_perplexity=1.0003505, train_loss=0.00035043503

Batch 74590, train_perplexity=1.0003504, train_loss=0.00035034266

Batch 74600, train_perplexity=1.0003504, train_loss=0.00035025924

Batch 74610, train_perplexity=1.0003502, train_loss=0.00035021457

Batch 74620, train_perplexity=1.0003502, train_loss=0.0003501163

Batch 74630, train_perplexity=1.0003502, train_loss=0.00035014903

Batch 74640, train_perplexity=1.0003501, train_loss=0.00035008352

Batch 74650, train_perplexity=1.0003501, train_loss=0.00035000307

Batch 74660, train_perplexity=1.00035, train_loss=0.00034997924

Batch 74670, train_perplexity=1.00035, train_loss=0.0003499107

Batch 74680, train_perplexity=1.00035, train_loss=0.00034988092

Batch 74690, train_perplexity=1.0003499, train_loss=0.0003497796

Batch 74700, train_perplexity=1.0003498, train_loss=0.0003497349

Batch 74710, train_perplexity=1.0003498, train_loss=0.00034971113

Batch 74720, train_perplexity=1.0003496, train_loss=0.00034963363

Batch 74730, train_perplexity=1.0003496, train_loss=0.00034960383

Batch 74740, train_perplexity=1.0003496, train_loss=0.00034953238

Batch 74750, train_perplexity=1.0003496, train_loss=0.00034953834

Batch 74760, train_perplexity=1.0003494, train_loss=0.0003493566

Batch 74770, train_perplexity=1.0003494, train_loss=0.00034936256

Batch 74780, train_perplexity=1.0003494, train_loss=0.00034932682

Batch 74790, train_perplexity=1.0003494, train_loss=0.00034928508

Batch 74800, train_perplexity=1.0003493, train_loss=0.0003491749

Batch 74810, train_perplexity=1.0003493, train_loss=0.00034920766

Batch 74820, train_perplexity=1.000349, train_loss=0.00034901395

Batch 74830, train_perplexity=1.0003492, train_loss=0.00034904378

Batch 74840, train_perplexity=1.0003492, train_loss=0.00034904975

Batch 74850, train_perplexity=1.000349, train_loss=0.00034896334

Batch 74860, train_perplexity=1.000349, train_loss=0.00034896337

Batch 74870, train_perplexity=1.0003489, train_loss=0.00034880848

Batch 74880, train_perplexity=1.0003489, train_loss=0.00034886802

Batch 74890, train_perplexity=1.0003487, train_loss=0.0003486565

Batch 74900, train_perplexity=1.0003488, train_loss=0.00034868927

Batch 74910, train_perplexity=1.0003487, train_loss=0.0003486267

Batch 74920, train_perplexity=1.0003486, train_loss=0.00034856418

Batch 74930, train_perplexity=1.0003486, train_loss=0.00034855225

Batch 74940, train_perplexity=1.0003486, train_loss=0.00034847774

Batch 74950, train_perplexity=1.0003486, train_loss=0.00034845393

Batch 74960, train_perplexity=1.0003484, train_loss=0.00034835562

Batch 74970, train_perplexity=1.0003483, train_loss=0.0003482722

Batch 74980, train_perplexity=1.0003483, train_loss=0.0003482156

Batch 74990, train_perplexity=1.0003482, train_loss=0.00034820073

Batch 75000, train_perplexity=1.0003482, train_loss=0.00034820667

Batch 75010, train_perplexity=1.0003482, train_loss=0.00034818283

Batch 75020, train_perplexity=1.0003481, train_loss=0.0003480279

Batch 75030, train_perplexity=1.0003481, train_loss=0.00034803688

Batch 75040, train_perplexity=1.000348, train_loss=0.00034796834

Batch 75050, train_perplexity=1.0003479, train_loss=0.00034781639

Batch 75060, train_perplexity=1.000348, train_loss=0.00034792366

Batch 75070, train_perplexity=1.0003479, train_loss=0.0003478462

Batch 75080, train_perplexity=1.0003479, train_loss=0.0003478164

Batch 75090, train_perplexity=1.0003477, train_loss=0.00034767343

Batch 75100, train_perplexity=1.0003476, train_loss=0.00034760192

Batch 75110, train_perplexity=1.0003476, train_loss=0.00034755428

Batch 75120, train_perplexity=1.0003476, train_loss=0.00034756315

Batch 75130, train_perplexity=1.0003476, train_loss=0.0003475751

Batch 75140, train_perplexity=1.0003475, train_loss=0.0003474887

Batch 75150, train_perplexity=1.0003475, train_loss=0.0003474321

Batch 75160, train_perplexity=1.0003474, train_loss=0.00034731295

Batch 75170, train_perplexity=1.0003474, train_loss=0.00034729805

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 75180, train_perplexity=1.0003473, train_loss=0.00034719973

Batch 75190, train_perplexity=1.0003473, train_loss=0.00034721463

Batch 75200, train_perplexity=1.0003473, train_loss=0.0003472414

Batch 75210, train_perplexity=1.0003471, train_loss=0.0003470627

Batch 75220, train_perplexity=1.0003471, train_loss=0.0003470448

Batch 75230, train_perplexity=1.000347, train_loss=0.0003469048

Batch 75240, train_perplexity=1.000347, train_loss=0.00034695544

Batch 75250, train_perplexity=1.0003469, train_loss=0.0003468601

Batch 75260, train_perplexity=1.0003469, train_loss=0.00034681841

Batch 75270, train_perplexity=1.0003469, train_loss=0.00034680055

Batch 75280, train_perplexity=1.0003468, train_loss=0.00034675584

Batch 75290, train_perplexity=1.0003468, train_loss=0.00034669926

Batch 75300, train_perplexity=1.0003467, train_loss=0.00034655922

Batch 75310, train_perplexity=1.0003467, train_loss=0.0003466218

Batch 75320, train_perplexity=1.0003467, train_loss=0.00034660986

Batch 75330, train_perplexity=1.0003465, train_loss=0.00034652944

Batch 75340, train_perplexity=1.0003465, train_loss=0.0003465354

Batch 75350, train_perplexity=1.0003464, train_loss=0.00034637155

Batch 75360, train_perplexity=1.0003464, train_loss=0.00034630898

Batch 75370, train_perplexity=1.0003464, train_loss=0.00034632982

Batch 75380, train_perplexity=1.0003462, train_loss=0.0003461779

Batch 75390, train_perplexity=1.0003463, train_loss=0.0003462196

Batch 75400, train_perplexity=1.0003462, train_loss=0.00034611236

Batch 75410, train_perplexity=1.0003461, train_loss=0.00034603488

Batch 75420, train_perplexity=1.0003461, train_loss=0.0003460021

Batch 75430, train_perplexity=1.000346, train_loss=0.0003459187

Batch 75440, train_perplexity=1.000346, train_loss=0.00034588

Batch 75450, train_perplexity=1.000346, train_loss=0.00034586212

Batch 75460, train_perplexity=1.000346, train_loss=0.0003458502

Batch 75470, train_perplexity=1.0003458, train_loss=0.0003457817

Batch 75480, train_perplexity=1.0003458, train_loss=0.0003457102

Batch 75490, train_perplexity=1.0003457, train_loss=0.00034567743

Batch 75500, train_perplexity=1.0003457, train_loss=0.0003456566

Batch 75510, train_perplexity=1.0003455, train_loss=0.0003454629

Batch 75520, train_perplexity=1.0003456, train_loss=0.0003455493

Batch 75530, train_perplexity=1.0003455, train_loss=0.00034541526

Batch 75540, train_perplexity=1.0003453, train_loss=0.0003453408

Batch 75550, train_perplexity=1.0003455, train_loss=0.00034540333

Batch 75560, train_perplexity=1.0003453, train_loss=0.00034532588

Batch 75570, train_perplexity=1.0003453, train_loss=0.00034523354

Batch 75580, train_perplexity=1.0003452, train_loss=0.00034520376

Batch 75590, train_perplexity=1.0003452, train_loss=0.00034513226

Batch 75600, train_perplexity=1.0003452, train_loss=0.0003451829

Batch 75610, train_perplexity=1.0003451, train_loss=0.00034507265

Batch 75620, train_perplexity=1.0003451, train_loss=0.000345022

Batch 75630, train_perplexity=1.000345, train_loss=0.00034492667

Batch 75640, train_perplexity=1.0003449, train_loss=0.00034486712

Batch 75650, train_perplexity=1.000345, train_loss=0.00034487306

Batch 75660, train_perplexity=1.0003448, train_loss=0.00034471814

Batch 75670, train_perplexity=1.0003448, train_loss=0.00034471814

Batch 75680, train_perplexity=1.0003448, train_loss=0.00034471217

Batch 75690, train_perplexity=1.0003446, train_loss=0.0003446109

Batch 75700, train_perplexity=1.0003446, train_loss=0.00034458408

Batch 75710, train_perplexity=1.0003446, train_loss=0.00034459005

Batch 75720, train_perplexity=1.0003446, train_loss=0.0003445394

Batch 75730, train_perplexity=1.0003445, train_loss=0.0003443994

Batch 75740, train_perplexity=1.0003445, train_loss=0.00034445003

Batch 75750, train_perplexity=1.0003444, train_loss=0.00034432492

Batch 75760, train_perplexity=1.0003443, train_loss=0.00034423854

Batch 75770, train_perplexity=1.0003443, train_loss=0.00034422358

Batch 75780, train_perplexity=1.0003443, train_loss=0.00034421767

Batch 75790, train_perplexity=1.0003442, train_loss=0.0003441134

Batch 75800, train_perplexity=1.0003442, train_loss=0.00034405978

Batch 75810, train_perplexity=1.0003442, train_loss=0.0003441223

Batch 75820, train_perplexity=1.000344, train_loss=0.00034398824

Batch 75830, train_perplexity=1.000344, train_loss=0.0003439436

Batch 75840, train_perplexity=1.000344, train_loss=0.00034396147

Batch 75850, train_perplexity=1.000344, train_loss=0.00034391973

Batch 75860, train_perplexity=1.0003438, train_loss=0.00034379464

Batch 75870, train_perplexity=1.0003439, train_loss=0.00034380954

Batch 75880, train_perplexity=1.0003437, train_loss=0.00034358312

Batch 75890, train_perplexity=1.0003437, train_loss=0.00034364866

Batch 75900, train_perplexity=1.0003437, train_loss=0.00034358905

Batch 75910, train_perplexity=1.0003437, train_loss=0.00034357415

Batch 75920, train_perplexity=1.0003436, train_loss=0.0003434699

Batch 75930, train_perplexity=1.0003434, train_loss=0.0003434401

Batch 75940, train_perplexity=1.0003434, train_loss=0.0003433567

Batch 75950, train_perplexity=1.0003432, train_loss=0.00034319877

Batch 75960, train_perplexity=1.0003433, train_loss=0.0003432703

Batch 75970, train_perplexity=1.0003433, train_loss=0.0003432703

Batch 75980, train_perplexity=1.0003433, train_loss=0.0003432167

Batch 75990, train_perplexity=1.0003432, train_loss=0.00034312432

Batch 76000, train_perplexity=1.0003432, train_loss=0.00034311836

Batch 76010, train_perplexity=1.0003431, train_loss=0.00034306175

Batch 76020, train_perplexity=1.000343, train_loss=0.00034294557

Batch 76030, train_perplexity=1.000343, train_loss=0.00034289496

Batch 76040, train_perplexity=1.0003428, train_loss=0.00034282045

Batch 76050, train_perplexity=1.0003428, train_loss=0.00034279068

Batch 76060, train_perplexity=1.0003428, train_loss=0.0003427996

Batch 76070, train_perplexity=1.0003428, train_loss=0.0003427549

Batch 76080, train_perplexity=1.0003427, train_loss=0.0003426149

Batch 76090, train_perplexity=1.0003427, train_loss=0.00034268043

Batch 76100, train_perplexity=1.0003426, train_loss=0.00034256428

Batch 76110, train_perplexity=1.0003426, train_loss=0.00034255232

Batch 76120, train_perplexity=1.0003425, train_loss=0.00034240933

Batch 76130, train_perplexity=1.0003425, train_loss=0.0003424838

Batch 76140, train_perplexity=1.0003424, train_loss=0.00034232892

Batch 76150, train_perplexity=1.0003424, train_loss=0.00034234382

Batch 76160, train_perplexity=1.0003424, train_loss=0.00034229018

Batch 76170, train_perplexity=1.0003424, train_loss=0.0003422544

Batch 76180, train_perplexity=1.0003422, train_loss=0.0003421412

Batch 76190, train_perplexity=1.0003422, train_loss=0.000342171

Batch 76200, train_perplexity=1.0003421, train_loss=0.00034209952

Batch 76210, train_perplexity=1.000342, train_loss=0.0003419595

Batch 76220, train_perplexity=1.000342, train_loss=0.00034197437

Batch 76230, train_perplexity=1.000342, train_loss=0.0003419327

Batch 76240, train_perplexity=1.0003419, train_loss=0.00034188203

Batch 76250, train_perplexity=1.0003419, train_loss=0.00034187306

Batch 76260, train_perplexity=1.0003418, train_loss=0.00034173307

Batch 76270, train_perplexity=1.0003418, train_loss=0.0003416973

Batch 76280, train_perplexity=1.0003417, train_loss=0.0003416288

Batch 76290, train_perplexity=1.0003417, train_loss=0.00034159902

Batch 76300, train_perplexity=1.0003417, train_loss=0.00034156325

Batch 76310, train_perplexity=1.0003415, train_loss=0.0003415335

Batch 76320, train_perplexity=1.0003415, train_loss=0.00034149177

Batch 76330, train_perplexity=1.0003414, train_loss=0.0003413696

Batch 76340, train_perplexity=1.0003414, train_loss=0.00034134876

Batch 76350, train_perplexity=1.0003414, train_loss=0.0003413458

Batch 76360, train_perplexity=1.0003414, train_loss=0.00034131005

Batch 76370, train_perplexity=1.0003413, train_loss=0.00034125644

Batch 76380, train_perplexity=1.0003413, train_loss=0.00034119387

Batch 76390, train_perplexity=1.0003412, train_loss=0.00034112233

Batch 76400, train_perplexity=1.0003412, train_loss=0.0003411283

Batch 76410, train_perplexity=1.000341, train_loss=0.00034100912
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76420, train_perplexity=1.000341, train_loss=0.00034101214

Batch 76430, train_perplexity=1.0003409, train_loss=0.0003409019

Batch 76440, train_perplexity=1.0003409, train_loss=0.00034086616

Batch 76450, train_perplexity=1.0003408, train_loss=0.00034079465

Batch 76460, train_perplexity=1.0003408, train_loss=0.000340741

Batch 76470, train_perplexity=1.0003408, train_loss=0.00034070824

Batch 76480, train_perplexity=1.0003408, train_loss=0.00034072017

Batch 76490, train_perplexity=1.0003407, train_loss=0.00034063676

Batch 76500, train_perplexity=1.0003407, train_loss=0.00034060702

Batch 76510, train_perplexity=1.0003406, train_loss=0.00034046994

Batch 76520, train_perplexity=1.0003406, train_loss=0.00034046994

Batch 76530, train_perplexity=1.0003405, train_loss=0.0003404431

Batch 76540, train_perplexity=1.0003403, train_loss=0.0003403359

Batch 76550, train_perplexity=1.0003403, train_loss=0.00034031802

Batch 76560, train_perplexity=1.0003403, train_loss=0.00034025247

Batch 76570, train_perplexity=1.0003402, train_loss=0.0003402167

Batch 76580, train_perplexity=1.0003402, train_loss=0.00034014817

Batch 76590, train_perplexity=1.0003401, train_loss=0.00034008862

Batch 76600, train_perplexity=1.0003401, train_loss=0.00034006475

Batch 76610, train_perplexity=1.00034, train_loss=0.00033992474

Batch 76620, train_perplexity=1.0003401, train_loss=0.0003400052

Batch 76630, train_perplexity=1.00034, train_loss=0.00033988006

Batch 76640, train_perplexity=1.0003399, train_loss=0.0003398056

Batch 76650, train_perplexity=1.0003399, train_loss=0.00033985326

Batch 76660, train_perplexity=1.0003399, train_loss=0.00033976985

Batch 76670, train_perplexity=1.0003397, train_loss=0.0003397222

Batch 76680, train_perplexity=1.0003397, train_loss=0.00033963876

Batch 76690, train_perplexity=1.0003396, train_loss=0.0003396209

Batch 76700, train_perplexity=1.0003396, train_loss=0.00033959706

Batch 76710, train_perplexity=1.0003396, train_loss=0.00033951958

Batch 76720, train_perplexity=1.0003395, train_loss=0.00033946

Batch 76730, train_perplexity=1.0003395, train_loss=0.00033942427

Batch 76740, train_perplexity=1.0003395, train_loss=0.00033942724

Batch 76750, train_perplexity=1.0003394, train_loss=0.00033928128

Batch 76760, train_perplexity=1.0003393, train_loss=0.00033925744

Batch 76770, train_perplexity=1.0003394, train_loss=0.00033931702

Batch 76780, train_perplexity=1.0003392, train_loss=0.00033913826

Batch 76790, train_perplexity=1.0003392, train_loss=0.00033909653

Batch 76800, train_perplexity=1.0003392, train_loss=0.0003391323

Batch 76810, train_perplexity=1.000339, train_loss=0.0003389625

Batch 76820, train_perplexity=1.000339, train_loss=0.0003389178

Batch 76830, train_perplexity=1.000339, train_loss=0.00033892377

Batch 76840, train_perplexity=1.0003389, train_loss=0.00033887016

Batch 76850, train_perplexity=1.0003389, train_loss=0.00033884333

Batch 76860, train_perplexity=1.0003388, train_loss=0.00033874798

Batch 76870, train_perplexity=1.0003388, train_loss=0.0003387331

Batch 76880, train_perplexity=1.0003387, train_loss=0.00033867353

Batch 76890, train_perplexity=1.0003387, train_loss=0.00033859012

Batch 76900, train_perplexity=1.0003387, train_loss=0.00033858413

Batch 76910, train_perplexity=1.0003386, train_loss=0.00033852158

Batch 76920, train_perplexity=1.0003386, train_loss=0.0003385484

Batch 76930, train_perplexity=1.0003386, train_loss=0.0003385037

Batch 76940, train_perplexity=1.0003384, train_loss=0.00033838753

Batch 76950, train_perplexity=1.0003383, train_loss=0.0003382624

Batch 76960, train_perplexity=1.0003384, train_loss=0.00033832196

Batch 76970, train_perplexity=1.0003383, train_loss=0.00033829222

Batch 76980, train_perplexity=1.0003382, train_loss=0.00033818794

Batch 76990, train_perplexity=1.0003382, train_loss=0.00033814023

Batch 77000, train_perplexity=1.0003382, train_loss=0.00033813133

Batch 77010, train_perplexity=1.0003381, train_loss=0.0003380628

Batch 77020, train_perplexity=1.0003381, train_loss=0.00033803005

Batch 77030, train_perplexity=1.000338, train_loss=0.00033795554

Batch 77040, train_perplexity=1.000338, train_loss=0.00033791683

Batch 77050, train_perplexity=1.0003378, train_loss=0.00033783045

Batch 77060, train_perplexity=1.0003378, train_loss=0.00033782743

Batch 77070, train_perplexity=1.0003377, train_loss=0.00033769337

Batch 77080, train_perplexity=1.0003377, train_loss=0.00033768744

Batch 77090, train_perplexity=1.0003377, train_loss=0.00033766654

Batch 77100, train_perplexity=1.0003377, train_loss=0.0003376189

Batch 77110, train_perplexity=1.0003376, train_loss=0.00033755635

Batch 77120, train_perplexity=1.0003376, train_loss=0.0003375474

Batch 77130, train_perplexity=1.0003375, train_loss=0.00033743418

Batch 77140, train_perplexity=1.0003374, train_loss=0.0003373359

Batch 77150, train_perplexity=1.0003375, train_loss=0.0003374014

Batch 77160, train_perplexity=1.0003374, train_loss=0.00033730312

Batch 77170, train_perplexity=1.0003374, train_loss=0.00033729116

Batch 77180, train_perplexity=1.0003372, train_loss=0.00033721374

Batch 77190, train_perplexity=1.0003372, train_loss=0.00033723458

Batch 77200, train_perplexity=1.0003371, train_loss=0.00033710353

Batch 77210, train_perplexity=1.0003371, train_loss=0.00033705286

Batch 77220, train_perplexity=1.0003371, train_loss=0.0003370499

Batch 77230, train_perplexity=1.0003369, train_loss=0.00033685626

Batch 77240, train_perplexity=1.000337, train_loss=0.00033688903

Batch 77250, train_perplexity=1.000337, train_loss=0.000336889

Batch 77260, train_perplexity=1.0003369, train_loss=0.00033680262

Batch 77270, train_perplexity=1.0003369, train_loss=0.0003367907

Batch 77280, train_perplexity=1.0003368, train_loss=0.00033674604

Batch 77290, train_perplexity=1.0003368, train_loss=0.00033672518

Batch 77300, train_perplexity=1.0003366, train_loss=0.00033659406

Batch 77310, train_perplexity=1.0003366, train_loss=0.00033654046

Batch 77320, train_perplexity=1.0003365, train_loss=0.00033648981

Batch 77330, train_perplexity=1.0003365, train_loss=0.00033645704

Batch 77340, train_perplexity=1.0003365, train_loss=0.00033648385

Batch 77350, train_perplexity=1.0003365, train_loss=0.00033643917

Batch 77360, train_perplexity=1.0003364, train_loss=0.00033631106

Batch 77370, train_perplexity=1.0003363, train_loss=0.0003362813

Batch 77380, train_perplexity=1.0003363, train_loss=0.00033617998

Batch 77390, train_perplexity=1.0003363, train_loss=0.00033621275

Batch 77400, train_perplexity=1.000336, train_loss=0.00033605186

Batch 77410, train_perplexity=1.000336, train_loss=0.00033602805

Batch 77420, train_perplexity=1.000336, train_loss=0.00033602212

Batch 77430, train_perplexity=1.000336, train_loss=0.00033598335

Batch 77440, train_perplexity=1.0003359, train_loss=0.00033591484

Batch 77450, train_perplexity=1.0003359, train_loss=0.00033588504

Batch 77460, train_perplexity=1.0003359, train_loss=0.0003358255

Batch 77470, train_perplexity=1.0003358, train_loss=0.00033573608

Batch 77480, train_perplexity=1.0003358, train_loss=0.00033577782

Batch 77490, train_perplexity=1.0003358, train_loss=0.00033572118

Batch 77500, train_perplexity=1.0003357, train_loss=0.0003355961

Batch 77510, train_perplexity=1.0003357, train_loss=0.0003356497

Batch 77520, train_perplexity=1.0003357, train_loss=0.0003355812

Batch 77530, train_perplexity=1.0003356, train_loss=0.00033548882

Batch 77540, train_perplexity=1.0003356, train_loss=0.00033547395

Batch 77550, train_perplexity=1.0003355, train_loss=0.00033535773

Batch 77560, train_perplexity=1.0003353, train_loss=0.00033532793

Batch 77570, train_perplexity=1.0003353, train_loss=0.0003353071

Batch 77580, train_perplexity=1.0003353, train_loss=0.00033528625

Batch 77590, train_perplexity=1.0003352, train_loss=0.00033518794

Batch 77600, train_perplexity=1.0003352, train_loss=0.00033512537

Batch 77610, train_perplexity=1.0003351, train_loss=0.0003350777

Batch 77620, train_perplexity=1.0003351, train_loss=0.00033501512

Batch 77630, train_perplexity=1.0003351, train_loss=0.00033506876

Batch 77640, train_perplexity=1.0003351, train_loss=0.00033498835

Batch 77650, train_perplexity=1.000335, train_loss=0.00033494364
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 77660, train_perplexity=1.000335, train_loss=0.00033487513

Batch 77670, train_perplexity=1.000335, train_loss=0.00033491087

Batch 77680, train_perplexity=1.0003347, train_loss=0.00033472318

Batch 77690, train_perplexity=1.0003347, train_loss=0.00033471425

Batch 77700, train_perplexity=1.0003347, train_loss=0.00033465168

Batch 77710, train_perplexity=1.0003346, train_loss=0.00033462187

Batch 77720, train_perplexity=1.0003347, train_loss=0.00033462787

Batch 77730, train_perplexity=1.0003346, train_loss=0.0003345772

Batch 77740, train_perplexity=1.0003345, train_loss=0.00033446995

Batch 77750, train_perplexity=1.0003346, train_loss=0.0003345236

Batch 77760, train_perplexity=1.0003345, train_loss=0.00033440144

Batch 77770, train_perplexity=1.0003344, train_loss=0.00033432996

Batch 77780, train_perplexity=1.0003343, train_loss=0.00033424952

Batch 77790, train_perplexity=1.0003344, train_loss=0.0003342674

Batch 77800, train_perplexity=1.0003343, train_loss=0.0003341512

Batch 77810, train_perplexity=1.0003341, train_loss=0.00033411547

Batch 77820, train_perplexity=1.0003341, train_loss=0.0003340827

Batch 77830, train_perplexity=1.000334, train_loss=0.00033400522

Batch 77840, train_perplexity=1.000334, train_loss=0.00033402012

Batch 77850, train_perplexity=1.000334, train_loss=0.00033398432

Batch 77860, train_perplexity=1.000334, train_loss=0.00033400225

Batch 77870, train_perplexity=1.0003339, train_loss=0.000333892

Batch 77880, train_perplexity=1.0003339, train_loss=0.0003338056

Batch 77890, train_perplexity=1.0003338, train_loss=0.00033376983

Batch 77900, train_perplexity=1.0003338, train_loss=0.00033367157

Batch 77910, train_perplexity=1.0003338, train_loss=0.00033371325

Batch 77920, train_perplexity=1.0003337, train_loss=0.00033361197

Batch 77930, train_perplexity=1.0003337, train_loss=0.00033356133

Batch 77940, train_perplexity=1.0003335, train_loss=0.00033354046

Batch 77950, train_perplexity=1.0003335, train_loss=0.00033352856

Batch 77960, train_perplexity=1.0003334, train_loss=0.0003333885

Batch 77970, train_perplexity=1.0003335, train_loss=0.00033344515

Batch 77980, train_perplexity=1.0003333, train_loss=0.0003332813

Batch 77990, train_perplexity=1.0003333, train_loss=0.00033325446

Batch 78000, train_perplexity=1.0003333, train_loss=0.00033322172

Batch 78010, train_perplexity=1.0003332, train_loss=0.0003331651

Batch 78020, train_perplexity=1.0003332, train_loss=0.00033313828

Batch 78030, train_perplexity=1.0003331, train_loss=0.0003330638

Batch 78040, train_perplexity=1.0003331, train_loss=0.00033303103

Batch 78050, train_perplexity=1.0003331, train_loss=0.00033303103

Batch 78060, train_perplexity=1.0003331, train_loss=0.00033296252

Batch 78070, train_perplexity=1.000333, train_loss=0.000332894

Batch 78080, train_perplexity=1.000333, train_loss=0.0003328374

Batch 78090, train_perplexity=1.0003328, train_loss=0.00033277483

Batch 78100, train_perplexity=1.0003328, train_loss=0.0003327391

Batch 78110, train_perplexity=1.0003327, train_loss=0.00033268248

Batch 78120, train_perplexity=1.0003327, train_loss=0.0003326229

Batch 78130, train_perplexity=1.0003327, train_loss=0.00033265268

Batch 78140, train_perplexity=1.0003326, train_loss=0.00033250672

Batch 78150, train_perplexity=1.0003325, train_loss=0.00033246502

Batch 78160, train_perplexity=1.0003325, train_loss=0.0003324382

Batch 78170, train_perplexity=1.0003325, train_loss=0.00033245605

Batch 78180, train_perplexity=1.0003325, train_loss=0.00033239648

Batch 78190, train_perplexity=1.0003324, train_loss=0.000332319

Batch 78200, train_perplexity=1.0003324, train_loss=0.00033229223

Batch 78210, train_perplexity=1.0003322, train_loss=0.00033223262

Batch 78220, train_perplexity=1.0003322, train_loss=0.00033220282

Batch 78230, train_perplexity=1.0003321, train_loss=0.00033208367

Batch 78240, train_perplexity=1.0003321, train_loss=0.000332036

Batch 78250, train_perplexity=1.0003321, train_loss=0.00033205686

Batch 78260, train_perplexity=1.000332, train_loss=0.0003319973

Batch 78270, train_perplexity=1.000332, train_loss=0.00033193472

Batch 78280, train_perplexity=1.000332, train_loss=0.00033190195

Batch 78290, train_perplexity=1.0003319, train_loss=0.00033180957

Batch 78300, train_perplexity=1.0003319, train_loss=0.00033181557

Batch 78310, train_perplexity=1.0003319, train_loss=0.0003317798

Batch 78320, train_perplexity=1.0003318, train_loss=0.00033169042

Batch 78330, train_perplexity=1.0003318, train_loss=0.0003316934

Batch 78340, train_perplexity=1.0003316, train_loss=0.000331601

Batch 78350, train_perplexity=1.0003316, train_loss=0.0003315832

Batch 78360, train_perplexity=1.0003316, train_loss=0.00033162488

Batch 78370, train_perplexity=1.0003315, train_loss=0.00033151466

Batch 78380, train_perplexity=1.0003315, train_loss=0.00033144315

Batch 78390, train_perplexity=1.0003314, train_loss=0.00033137464

Batch 78400, train_perplexity=1.0003314, train_loss=0.00033132994

Batch 78410, train_perplexity=1.0003314, train_loss=0.00033129717

Batch 78420, train_perplexity=1.0003313, train_loss=0.0003311959

Batch 78430, train_perplexity=1.0003313, train_loss=0.00033119292

Batch 78440, train_perplexity=1.0003312, train_loss=0.00033112738

Batch 78450, train_perplexity=1.0003312, train_loss=0.00033107374

Batch 78460, train_perplexity=1.0003312, train_loss=0.00033110057

Batch 78470, train_perplexity=1.000331, train_loss=0.00033102604

Batch 78480, train_perplexity=1.0003309, train_loss=0.0003309099

Batch 78490, train_perplexity=1.0003309, train_loss=0.00033090095

Batch 78500, train_perplexity=1.0003309, train_loss=0.00033085624

Batch 78510, train_perplexity=1.0003309, train_loss=0.0003308354

Batch 78520, train_perplexity=1.0003308, train_loss=0.00033078773

Batch 78530, train_perplexity=1.0003308, train_loss=0.00033072813

Batch 78540, train_perplexity=1.0003308, train_loss=0.00033071626

Batch 78550, train_perplexity=1.0003306, train_loss=0.00033050473

Batch 78560, train_perplexity=1.0003307, train_loss=0.00033060304

Batch 78570, train_perplexity=1.0003306, train_loss=0.0003305077

Batch 78580, train_perplexity=1.0003306, train_loss=0.00033051366

Batch 78590, train_perplexity=1.0003304, train_loss=0.00033042725

Batch 78600, train_perplexity=1.0003304, train_loss=0.00033042132

Batch 78610, train_perplexity=1.0003304, train_loss=0.00033035278

Batch 78620, train_perplexity=1.0003303, train_loss=0.0003302634

Batch 78630, train_perplexity=1.0003303, train_loss=0.00033029023

Batch 78640, train_perplexity=1.0003303, train_loss=0.00033021875

Batch 78650, train_perplexity=1.0003302, train_loss=0.00033013232

Batch 78660, train_perplexity=1.0003301, train_loss=0.00033009064

Batch 78670, train_perplexity=1.0003301, train_loss=0.0003300787

Batch 78680, train_perplexity=1.00033, train_loss=0.00032995362

Batch 78690, train_perplexity=1.0003301, train_loss=0.00032999233

Batch 78700, train_perplexity=1.00033, train_loss=0.00032992975

Batch 78710, train_perplexity=1.00033, train_loss=0.0003298761

Batch 78720, train_perplexity=1.0003299, train_loss=0.00032983144

Batch 78730, train_perplexity=1.0003299, train_loss=0.00032978973

Batch 78740, train_perplexity=1.0003297, train_loss=0.00032973016

Batch 78750, train_perplexity=1.0003297, train_loss=0.00032969142

Batch 78760, train_perplexity=1.0003297, train_loss=0.00032964075

Batch 78770, train_perplexity=1.0003296, train_loss=0.00032960201

Batch 78780, train_perplexity=1.0003295, train_loss=0.0003294918

Batch 78790, train_perplexity=1.0003296, train_loss=0.00032952457

Batch 78800, train_perplexity=1.0003295, train_loss=0.00032945012

Batch 78810, train_perplexity=1.0003294, train_loss=0.00032934584

Batch 78820, train_perplexity=1.0003294, train_loss=0.0003293458

Batch 78830, train_perplexity=1.0003294, train_loss=0.00032926537

Batch 78840, train_perplexity=1.0003293, train_loss=0.00032924156

Batch 78850, train_perplexity=1.0003293, train_loss=0.00032924156

Batch 78860, train_perplexity=1.0003291, train_loss=0.00032911648

Batch 78870, train_perplexity=1.0003291, train_loss=0.00032911942

Batch 78880, train_perplexity=1.0003291, train_loss=0.0003290896

Batch 78890, train_perplexity=1.000329, train_loss=0.00032897643
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78900, train_perplexity=1.000329, train_loss=0.0003290092

Batch 78910, train_perplexity=1.000329, train_loss=0.00032891385

Batch 78920, train_perplexity=1.0003289, train_loss=0.00032888702

Batch 78930, train_perplexity=1.0003289, train_loss=0.00032882445

Batch 78940, train_perplexity=1.0003289, train_loss=0.00032880658

Batch 78950, train_perplexity=1.0003288, train_loss=0.00032877678

Batch 78960, train_perplexity=1.0003288, train_loss=0.0003286964

Batch 78970, train_perplexity=1.0003288, train_loss=0.00032867852

Batch 78980, train_perplexity=1.0003287, train_loss=0.00032864272

Batch 78990, train_perplexity=1.0003287, train_loss=0.00032861595

Batch 79000, train_perplexity=1.0003285, train_loss=0.0003284521

Batch 79010, train_perplexity=1.0003285, train_loss=0.00032851764

Batch 79020, train_perplexity=1.0003284, train_loss=0.0003284223

Batch 79030, train_perplexity=1.0003284, train_loss=0.00032832695

Batch 79040, train_perplexity=1.0003284, train_loss=0.0003283329

Batch 79050, train_perplexity=1.0003283, train_loss=0.00032825247

Batch 79060, train_perplexity=1.0003283, train_loss=0.0003282167

Batch 79070, train_perplexity=1.0003283, train_loss=0.0003282078

Batch 79080, train_perplexity=1.0003282, train_loss=0.00032812436

Batch 79090, train_perplexity=1.0003282, train_loss=0.0003280767

Batch 79100, train_perplexity=1.0003282, train_loss=0.0003280916

Batch 79110, train_perplexity=1.000328, train_loss=0.0003279337

Batch 79120, train_perplexity=1.000328, train_loss=0.0003279337

Batch 79130, train_perplexity=1.000328, train_loss=0.00032786815

Batch 79140, train_perplexity=1.000328, train_loss=0.00032784138

Batch 79150, train_perplexity=1.0003278, train_loss=0.00032780855

Batch 79160, train_perplexity=1.0003278, train_loss=0.00032771623

Batch 79170, train_perplexity=1.0003277, train_loss=0.00032765072

Batch 79180, train_perplexity=1.0003277, train_loss=0.0003276775

Batch 79190, train_perplexity=1.0003277, train_loss=0.00032763882

Batch 79200, train_perplexity=1.0003276, train_loss=0.00032757325

Batch 79210, train_perplexity=1.0003275, train_loss=0.0003273915

Batch 79220, train_perplexity=1.0003275, train_loss=0.00032744513

Batch 79230, train_perplexity=1.0003275, train_loss=0.00032743323

Batch 79240, train_perplexity=1.0003275, train_loss=0.00032737962

Batch 79250, train_perplexity=1.0003275, train_loss=0.00032736472

Batch 79260, train_perplexity=1.0003273, train_loss=0.00032731402

Batch 79270, train_perplexity=1.0003273, train_loss=0.0003272515

Batch 79280, train_perplexity=1.0003272, train_loss=0.0003272217

Batch 79290, train_perplexity=1.0003272, train_loss=0.000327177

Batch 79300, train_perplexity=1.0003272, train_loss=0.00032718593

Batch 79310, train_perplexity=1.0003272, train_loss=0.0003271383

Batch 79320, train_perplexity=1.0003271, train_loss=0.0003270161

Batch 79330, train_perplexity=1.0003271, train_loss=0.00032699824

Batch 79340, train_perplexity=1.000327, train_loss=0.00032698037

Batch 79350, train_perplexity=1.000327, train_loss=0.00032692973

Batch 79360, train_perplexity=1.0003269, train_loss=0.00032684038

Batch 79370, train_perplexity=1.0003268, train_loss=0.00032672714

Batch 79380, train_perplexity=1.0003269, train_loss=0.00032676288

Batch 79390, train_perplexity=1.0003268, train_loss=0.00032672117

Batch 79400, train_perplexity=1.0003268, train_loss=0.00032672717

Batch 79410, train_perplexity=1.0003266, train_loss=0.00032655732

Batch 79420, train_perplexity=1.0003266, train_loss=0.0003265603

Batch 79430, train_perplexity=1.0003266, train_loss=0.00032654838

Batch 79440, train_perplexity=1.0003265, train_loss=0.00032645307

Batch 79450, train_perplexity=1.0003264, train_loss=0.00032638456

Batch 79460, train_perplexity=1.0003264, train_loss=0.00032631902

Batch 79470, train_perplexity=1.0003264, train_loss=0.00032632495

Batch 79480, train_perplexity=1.0003264, train_loss=0.00032631302

Batch 79490, train_perplexity=1.0003263, train_loss=0.00032619984

Batch 79500, train_perplexity=1.0003262, train_loss=0.00032611046

Batch 79510, train_perplexity=1.0003262, train_loss=0.00032612533

Batch 79520, train_perplexity=1.0003262, train_loss=0.0003260926

Batch 79530, train_perplexity=1.0003262, train_loss=0.00032605385

Batch 79540, train_perplexity=1.000326, train_loss=0.0003259466

Batch 79550, train_perplexity=1.000326, train_loss=0.00032597937

Batch 79560, train_perplexity=1.0003259, train_loss=0.00032590487

Batch 79570, train_perplexity=1.0003259, train_loss=0.0003259168

Batch 79580, train_perplexity=1.0003259, train_loss=0.0003258721

Batch 79590, train_perplexity=1.0003258, train_loss=0.00032575594

Batch 79600, train_perplexity=1.0003258, train_loss=0.00032575298

Batch 79610, train_perplexity=1.0003258, train_loss=0.00032569934

Batch 79620, train_perplexity=1.0003257, train_loss=0.00032565166

Batch 79630, train_perplexity=1.0003256, train_loss=0.00032556232

Batch 79640, train_perplexity=1.0003257, train_loss=0.00032558315

Batch 79650, train_perplexity=1.0003256, train_loss=0.00032555335

Batch 79660, train_perplexity=1.0003254, train_loss=0.0003254312

Batch 79670, train_perplexity=1.0003254, train_loss=0.00032544014

Batch 79680, train_perplexity=1.0003256, train_loss=0.00032545207

Batch 79690, train_perplexity=1.0003254, train_loss=0.00032536266

Batch 79700, train_perplexity=1.0003253, train_loss=0.00032532396

Batch 79710, train_perplexity=1.0003253, train_loss=0.0003252197

Batch 79720, train_perplexity=1.0003253, train_loss=0.00032522864

Batch 79730, train_perplexity=1.0003252, train_loss=0.00032518094

Batch 79740, train_perplexity=1.0003252, train_loss=0.00032512136

Batch 79750, train_perplexity=1.0003251, train_loss=0.00032507075

Batch 79760, train_perplexity=1.0003251, train_loss=0.0003249873

Batch 79770, train_perplexity=1.000325, train_loss=0.00032492177

Batch 79780, train_perplexity=1.000325, train_loss=0.00032490093

Batch 79790, train_perplexity=1.000325, train_loss=0.0003248562

Batch 79800, train_perplexity=1.0003248, train_loss=0.0003247758

Batch 79810, train_perplexity=1.0003248, train_loss=0.00032475495

Batch 79820, train_perplexity=1.0003247, train_loss=0.00032468938

Batch 79830, train_perplexity=1.0003247, train_loss=0.0003247162

Batch 79840, train_perplexity=1.0003247, train_loss=0.00032463874

Batch 79850, train_perplexity=1.0003246, train_loss=0.0003245762

Batch 79860, train_perplexity=1.0003246, train_loss=0.00032454642

Batch 79870, train_perplexity=1.0003246, train_loss=0.00032449575

Batch 79880, train_perplexity=1.0003245, train_loss=0.00032446

Batch 79890, train_perplexity=1.0003245, train_loss=0.0003244153

Batch 79900, train_perplexity=1.0003244, train_loss=0.00032435573

Batch 79910, train_perplexity=1.0003244, train_loss=0.0003243587

Batch 79920, train_perplexity=1.0003244, train_loss=0.0003242604

Batch 79930, train_perplexity=1.0003244, train_loss=0.0003242634

Batch 79940, train_perplexity=1.0003241, train_loss=0.00032413227

Batch 79950, train_perplexity=1.0003242, train_loss=0.00032415317

Batch 79960, train_perplexity=1.0003241, train_loss=0.00032408163

Batch 79970, train_perplexity=1.000324, train_loss=0.00032398335

Batch 79980, train_perplexity=1.0003241, train_loss=0.000324031

Batch 79990, train_perplexity=1.000324, train_loss=0.00032396545

Batch 80000, train_perplexity=1.000324, train_loss=0.00032390887

Batch 80010, train_perplexity=1.0003239, train_loss=0.0003238612

Batch 80020, train_perplexity=1.0003239, train_loss=0.0003238463

Batch 80030, train_perplexity=1.0003238, train_loss=0.00032370628

Batch 80040, train_perplexity=1.0003239, train_loss=0.00032381056

Batch 80050, train_perplexity=1.0003238, train_loss=0.00032372115

Batch 80060, train_perplexity=1.0003237, train_loss=0.00032363774

Batch 80070, train_perplexity=1.0003237, train_loss=0.00032361987

Batch 80080, train_perplexity=1.0003235, train_loss=0.00032351562

Batch 80090, train_perplexity=1.0003235, train_loss=0.00032352156

Batch 80100, train_perplexity=1.0003235, train_loss=0.00032346195

Batch 80110, train_perplexity=1.0003235, train_loss=0.00032351265

Batch 80120, train_perplexity=1.0003234, train_loss=0.0003234024

Batch 80130, train_perplexity=1.0003234, train_loss=0.00032335473

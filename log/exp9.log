nohup: ignoring input
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-30 12:23:49.125810: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-30 12:23:49.319633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0c:00.0
totalMemory: 22.38GiB freeMemory: 21.80GiB
2020-10-30 12:23:49.319689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-30 12:23:49.860736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-30 12:23:49.860793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-30 12:23:49.860804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-30 12:23:49.862203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21153 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:0c:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=150256.81, train_loss=11.920101

Batch 10, train_perplexity=7279.646, train_loss=8.892838

Batch 20, train_perplexity=6663.8896, train_loss=8.804459

Batch 30, train_perplexity=6090.2544, train_loss=8.714445

Batch 40, train_perplexity=4491.019, train_loss=8.409835

Batch 50, train_perplexity=3416.4338, train_loss=8.136353

Batch 60, train_perplexity=3789.5322, train_loss=8.239998

Batch 70, train_perplexity=4192.566, train_loss=8.341068

Batch 80, train_perplexity=4028.869, train_loss=8.301241

Batch 90, train_perplexity=3846.4565, train_loss=8.254908

Batch 100, train_perplexity=3926.4417, train_loss=8.275489

Batch 110, train_perplexity=3714.9463, train_loss=8.220119

Batch 120, train_perplexity=3449.3127, train_loss=8.14593

Batch 130, train_perplexity=3956.3616, train_loss=8.28308

Batch 140, train_perplexity=4176.4995, train_loss=8.337229

Batch 150, train_perplexity=3714.1492, train_loss=8.219905

Batch 160, train_perplexity=4009.2598, train_loss=8.296362

Batch 170, train_perplexity=4266.4023, train_loss=8.358526

Batch 180, train_perplexity=3631.0566, train_loss=8.197279

Batch 190, train_perplexity=4394.3833, train_loss=8.3880825

Batch 200, train_perplexity=4123.066, train_loss=8.324352

Batch 210, train_perplexity=4058.3384, train_loss=8.308529

Batch 220, train_perplexity=4004.8652, train_loss=8.295265

Batch 230, train_perplexity=4117.753, train_loss=8.323063

Batch 240, train_perplexity=3689.6448, train_loss=8.213285

Batch 250, train_perplexity=4775.716, train_loss=8.471299

Batch 260, train_perplexity=4271.911, train_loss=8.359817

Batch 270, train_perplexity=4199.3687, train_loss=8.3426895

Batch 280, train_perplexity=3889.356, train_loss=8.265999

Batch 290, train_perplexity=4343.1045, train_loss=8.376345

Batch 300, train_perplexity=3770.3042, train_loss=8.234911

Batch 310, train_perplexity=3956.8032, train_loss=8.283192

Batch 320, train_perplexity=4470.0684, train_loss=8.405159

Batch 330, train_perplexity=4004.6284, train_loss=8.295206

Batch 340, train_perplexity=4219.0503, train_loss=8.347365

Batch 350, train_perplexity=4279.267, train_loss=8.361537

Batch 360, train_perplexity=5150.8545, train_loss=8.546918

Batch 370, train_perplexity=4117.1323, train_loss=8.322912

Batch 380, train_perplexity=4757.147, train_loss=8.467403

Batch 390, train_perplexity=4620.949, train_loss=8.438355

Batch 400, train_perplexity=4692.5146, train_loss=8.453724

Batch 410, train_perplexity=4727.7764, train_loss=8.46121

Batch 420, train_perplexity=4431.1133, train_loss=8.396406

Batch 430, train_perplexity=4380.6924, train_loss=8.384962

Batch 440, train_perplexity=3910.0188, train_loss=8.271297

Batch 450, train_perplexity=4364.7124, train_loss=8.381308

Batch 460, train_perplexity=5466.948, train_loss=8.606476

Batch 470, train_perplexity=4371.732, train_loss=8.382915

Batch 480, train_perplexity=4459.2793, train_loss=8.402742

Batch 490, train_perplexity=5737.8096, train_loss=8.654833

Batch 500, train_perplexity=5085.464, train_loss=8.534142

Batch 510, train_perplexity=4735.4385, train_loss=8.46283

Batch 520, train_perplexity=5183.6196, train_loss=8.553259

Batch 530, train_perplexity=5738.609, train_loss=8.654972

Batch 540, train_perplexity=4590.5146, train_loss=8.431747

Batch 550, train_perplexity=5248.696, train_loss=8.565735

Batch 560, train_perplexity=4436.0264, train_loss=8.397514

Batch 570, train_perplexity=4413.762, train_loss=8.392483

Batch 580, train_perplexity=4359.076, train_loss=8.380015

Batch 590, train_perplexity=5175.361, train_loss=8.551664

Batch 600, train_perplexity=4913.5503, train_loss=8.499752

Batch 610, train_perplexity=4928.5586, train_loss=8.502802
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 620, train_perplexity=4816.9746, train_loss=8.479901

Batch 630, train_perplexity=4922.0205, train_loss=8.501474

Batch 640, train_perplexity=4640.2075, train_loss=8.442514

Batch 650, train_perplexity=4643.3774, train_loss=8.443197

Batch 660, train_perplexity=4439.7935, train_loss=8.398363

Batch 670, train_perplexity=4533.9453, train_loss=8.419348

Batch 680, train_perplexity=5021.914, train_loss=8.521566

Batch 690, train_perplexity=4671.7515, train_loss=8.449289

Batch 700, train_perplexity=4651.08, train_loss=8.444855

Batch 710, train_perplexity=4158.9634, train_loss=8.333021

Batch 720, train_perplexity=4197.391, train_loss=8.342218

Batch 730, train_perplexity=4281.4673, train_loss=8.362051

Batch 740, train_perplexity=5261.184, train_loss=8.568111

Batch 750, train_perplexity=5791.676, train_loss=8.664177

Batch 760, train_perplexity=4690.4297, train_loss=8.4532795

Batch 770, train_perplexity=4088.6318, train_loss=8.315966

Batch 780, train_perplexity=5474.346, train_loss=8.607828

Batch 790, train_perplexity=4740.725, train_loss=8.463945

Batch 800, train_perplexity=5691.437, train_loss=8.646718

Batch 810, train_perplexity=5374.0054, train_loss=8.589329

Batch 820, train_perplexity=5724.408, train_loss=8.652494

Batch 830, train_perplexity=5236.4863, train_loss=8.563406

Batch 840, train_perplexity=5424.125, train_loss=8.598612

Batch 850, train_perplexity=5997.023, train_loss=8.6990185

Batch 860, train_perplexity=4802.3105, train_loss=8.476852

Batch 870, train_perplexity=4735.4204, train_loss=8.462826

Batch 880, train_perplexity=4887.822, train_loss=8.494502

Batch 890, train_perplexity=4671.867, train_loss=8.449314

Batch 900, train_perplexity=4397.603, train_loss=8.388815

Batch 910, train_perplexity=5043.483, train_loss=8.525852

Batch 920, train_perplexity=4526.1343, train_loss=8.4176235

Batch 930, train_perplexity=4347.012, train_loss=8.377244

Batch 940, train_perplexity=4666.746, train_loss=8.448217

Batch 950, train_perplexity=5859.9463, train_loss=8.675896

Batch 960, train_perplexity=4908.9507, train_loss=8.498816

Batch 970, train_perplexity=5159.857, train_loss=8.548664

Batch 980, train_perplexity=4998.601, train_loss=8.516913

Batch 990, train_perplexity=4872.096, train_loss=8.49128

Batch 1000, train_perplexity=5259.5137, train_loss=8.567794

Batch 1010, train_perplexity=5634.9824, train_loss=8.636749

Batch 1020, train_perplexity=4966.0723, train_loss=8.510385

Batch 1030, train_perplexity=5171.769, train_loss=8.55097

Batch 1040, train_perplexity=5082.051, train_loss=8.53347

Batch 1050, train_perplexity=5379.143, train_loss=8.590284

Batch 1060, train_perplexity=4243.8965, train_loss=8.353237

Batch 1070, train_perplexity=4658.8486, train_loss=8.446524

Batch 1080, train_perplexity=5009.563, train_loss=8.519104

Batch 1090, train_perplexity=5314.0107, train_loss=8.578102

Batch 1100, train_perplexity=4865.9204, train_loss=8.490011

Batch 1110, train_perplexity=4839.8413, train_loss=8.484637

Batch 1120, train_perplexity=4788.5903, train_loss=8.473991

Batch 1130, train_perplexity=4824.6294, train_loss=8.481489

Batch 1140, train_perplexity=4748.4263, train_loss=8.465569

Batch 1150, train_perplexity=4438.498, train_loss=8.398071

Batch 1160, train_perplexity=5988.222, train_loss=8.69755

Batch 1170, train_perplexity=4680.461, train_loss=8.451152

Batch 1180, train_perplexity=4650.539, train_loss=8.444738

Batch 1190, train_perplexity=5262.0273, train_loss=8.568272

Batch 1200, train_perplexity=4684.8594, train_loss=8.452091

Batch 1210, train_perplexity=4462.325, train_loss=8.403425

Batch 1220, train_perplexity=4798.1216, train_loss=8.47598

Batch 1230, train_perplexity=5322.0493, train_loss=8.579614

Batch 1240, train_perplexity=5728.7007, train_loss=8.653244

Batch 1250, train_perplexity=5344.566, train_loss=8.583836

Batch 1260, train_perplexity=5662.424, train_loss=8.641607

Batch 1270, train_perplexity=4759.9336, train_loss=8.467989

Batch 1280, train_perplexity=5200.033, train_loss=8.55642

Batch 1290, train_perplexity=5289.125, train_loss=8.573408

Batch 1300, train_perplexity=5653.6445, train_loss=8.640056

Batch 1310, train_perplexity=4970.1235, train_loss=8.5112

Batch 1320, train_perplexity=4646.243, train_loss=8.443814

Batch 1330, train_perplexity=5074.7183, train_loss=8.532026

Batch 1340, train_perplexity=5738.8057, train_loss=8.655006

Batch 1350, train_perplexity=4995.389, train_loss=8.516271

Batch 1360, train_perplexity=4919.0454, train_loss=8.50087

Batch 1370, train_perplexity=4564.819, train_loss=8.426134

Batch 1380, train_perplexity=5100.0005, train_loss=8.536996

Batch 1390, train_perplexity=5344.3926, train_loss=8.583803

Batch 1400, train_perplexity=4811.474, train_loss=8.478759

Batch 1410, train_perplexity=5411.2236, train_loss=8.5962305

Batch 1420, train_perplexity=5843.84, train_loss=8.673143

Batch 1430, train_perplexity=5399.8203, train_loss=8.594121

Batch 1440, train_perplexity=5435.232, train_loss=8.600657

Batch 1450, train_perplexity=5252.6865, train_loss=8.566495

Batch 1460, train_perplexity=5916.3125, train_loss=8.685469

Batch 1470, train_perplexity=5585.295, train_loss=8.6278925

Batch 1480, train_perplexity=5720.937, train_loss=8.651888

Batch 1490, train_perplexity=5121.768, train_loss=8.541255

Batch 1500, train_perplexity=5626.047, train_loss=8.635162

Batch 1510, train_perplexity=4774.336, train_loss=8.47101

Batch 1520, train_perplexity=4537.419, train_loss=8.420114

Batch 1530, train_perplexity=4331.465, train_loss=8.373661

Batch 1540, train_perplexity=5789.9746, train_loss=8.663883

Batch 1550, train_perplexity=5365.832, train_loss=8.587807

Batch 1560, train_perplexity=5510.383, train_loss=8.614389

Batch 1570, train_perplexity=4543.572, train_loss=8.421469

Batch 1580, train_perplexity=4657.463, train_loss=8.446226

Batch 1590, train_perplexity=4665.647, train_loss=8.447982

Batch 1600, train_perplexity=5275.1763, train_loss=8.570767

Batch 1610, train_perplexity=4531.412, train_loss=8.418789

Batch 1620, train_perplexity=5435.2476, train_loss=8.60066

Batch 1630, train_perplexity=4650.3613, train_loss=8.4447

Batch 1640, train_perplexity=5589.723, train_loss=8.628685

Batch 1650, train_perplexity=4800.14, train_loss=8.4764

Batch 1660, train_perplexity=5895.5293, train_loss=8.68195

Batch 1670, train_perplexity=5501.7925, train_loss=8.612829

Batch 1680, train_perplexity=5170.9307, train_loss=8.550808

Batch 1690, train_perplexity=6039.5005, train_loss=8.706077

Batch 1700, train_perplexity=4592.2837, train_loss=8.432133

Batch 1710, train_perplexity=5299.9556, train_loss=8.575454

Batch 1720, train_perplexity=6023.5273, train_loss=8.703428

Batch 1730, train_perplexity=5362.7114, train_loss=8.587225

Batch 1740, train_perplexity=5733.631, train_loss=8.654104

Batch 1750, train_perplexity=4962.612, train_loss=8.509687

Batch 1760, train_perplexity=4578.439, train_loss=8.429113

Batch 1770, train_perplexity=5279.52, train_loss=8.57159

Batch 1780, train_perplexity=4704.263, train_loss=8.456224

Batch 1790, train_perplexity=4376.458, train_loss=8.383995

Batch 1800, train_perplexity=5021.6743, train_loss=8.521519

Batch 1810, train_perplexity=4272.8237, train_loss=8.36003

Batch 1820, train_perplexity=6567.9126, train_loss=8.789951

Batch 1830, train_perplexity=5434.2524, train_loss=8.600477

Batch 1840, train_perplexity=6060.248, train_loss=8.709506

Batch 1850, train_perplexity=4300.2495, train_loss=8.366428

Batch 1860, train_perplexity=5933.8066, train_loss=8.688421

Batch 1870, train_perplexity=5136.281, train_loss=8.544085

Batch 1880, train_perplexity=6074.2393, train_loss=8.711812

Batch 1890, train_perplexity=4791.231, train_loss=8.474543

Batch 1900, train_perplexity=5770.2627, train_loss=8.660473

Batch 1910, train_perplexity=4849.6553, train_loss=8.486663

Batch 1920, train_perplexity=5373.4517, train_loss=8.589226

Batch 1930, train_perplexity=4521.751, train_loss=8.416655

Batch 1940, train_perplexity=6608.349, train_loss=8.796089

Batch 1950, train_perplexity=5622.196, train_loss=8.634478

Batch 1960, train_perplexity=5050.135, train_loss=8.52717

Batch 1970, train_perplexity=5307.578, train_loss=8.576891

Batch 1980, train_perplexity=5479.4126, train_loss=8.608753
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 1990, train_perplexity=5474.482, train_loss=8.607853

Batch 2000, train_perplexity=6955.6006, train_loss=8.847302

Batch 2010, train_perplexity=5728.744, train_loss=8.653252

Batch 2020, train_perplexity=5635.6807, train_loss=8.636873

Batch 2030, train_perplexity=4850.7188, train_loss=8.486882

Batch 2040, train_perplexity=4913.9814, train_loss=8.49984

Batch 2050, train_perplexity=5881.72, train_loss=8.679605

Batch 2060, train_perplexity=5054.4805, train_loss=8.52803

Batch 2070, train_perplexity=6207.6855, train_loss=8.733543

Batch 2080, train_perplexity=4761.3, train_loss=8.468276

Batch 2090, train_perplexity=5969.611, train_loss=8.694437

Batch 2100, train_perplexity=5912.5337, train_loss=8.68483

Batch 2110, train_perplexity=6790.361, train_loss=8.823259

Batch 2120, train_perplexity=4933.6377, train_loss=8.503832

Batch 2130, train_perplexity=6570.18, train_loss=8.790297

Batch 2140, train_perplexity=5377.6094, train_loss=8.589999

Batch 2150, train_perplexity=5132.266, train_loss=8.543303

Batch 2160, train_perplexity=4093.8171, train_loss=8.317233

Batch 2170, train_perplexity=5253.383, train_loss=8.5666275

Batch 2180, train_perplexity=5256.2495, train_loss=8.567173

Batch 2190, train_perplexity=5198.2236, train_loss=8.556072

Batch 2200, train_perplexity=5218.698, train_loss=8.560003

Batch 2210, train_perplexity=5377.8916, train_loss=8.590052

Batch 2220, train_perplexity=4492.668, train_loss=8.410202

Batch 2230, train_perplexity=4905.8105, train_loss=8.498176

Batch 2240, train_perplexity=5123.1064, train_loss=8.541516

Batch 2250, train_perplexity=5621.7134, train_loss=8.634392

Batch 2260, train_perplexity=5488.3296, train_loss=8.610379

Batch 2270, train_perplexity=4553.5835, train_loss=8.42367

Batch 2280, train_perplexity=6148.1323, train_loss=8.723904

Batch 2290, train_perplexity=5345.509, train_loss=8.584012

Batch 2300, train_perplexity=5100.083, train_loss=8.537012

Batch 2310, train_perplexity=6082.778, train_loss=8.713217

Batch 2320, train_perplexity=4696.634, train_loss=8.454601

Batch 2330, train_perplexity=5263.3022, train_loss=8.568514

Batch 2340, train_perplexity=5448.4814, train_loss=8.603092

Batch 2350, train_perplexity=5904.4985, train_loss=8.68347

Batch 2360, train_perplexity=4793.841, train_loss=8.475087

Batch 2370, train_perplexity=5576.4756, train_loss=8.626312

Batch 2380, train_perplexity=5120.2007, train_loss=8.540949

Batch 2390, train_perplexity=5453.061, train_loss=8.603932

Batch 2400, train_perplexity=5044.4355, train_loss=8.526041

Batch 2410, train_perplexity=5632.339, train_loss=8.63628

Batch 2420, train_perplexity=5660.847, train_loss=8.641329

Batch 2430, train_perplexity=4797.7144, train_loss=8.475895

Batch 2440, train_perplexity=4996.676, train_loss=8.516528

Batch 2450, train_perplexity=4579.8013, train_loss=8.429411

Batch 2460, train_perplexity=5401.0513, train_loss=8.594349

Batch 2470, train_perplexity=5251.0938, train_loss=8.566192

Batch 2480, train_perplexity=5214.375, train_loss=8.559175

Batch 2490, train_perplexity=5688.8486, train_loss=8.646263

Batch 2500, train_perplexity=5161.5894, train_loss=8.549

Batch 2510, train_perplexity=5385.426, train_loss=8.591452

Batch 2520, train_perplexity=5747.646, train_loss=8.656546

Batch 2530, train_perplexity=5693.9614, train_loss=8.6471615

Batch 2540, train_perplexity=4725.4146, train_loss=8.460711

Batch 2550, train_perplexity=4763.9023, train_loss=8.4688225

Batch 2560, train_perplexity=5482.6953, train_loss=8.609352

Batch 2570, train_perplexity=5325.2275, train_loss=8.580211

Batch 2580, train_perplexity=6104.1455, train_loss=8.716723

Batch 2590, train_perplexity=4764.125, train_loss=8.468869

Batch 2600, train_perplexity=5423.4214, train_loss=8.598482

Batch 2610, train_perplexity=5139.1226, train_loss=8.544638

Batch 2620, train_perplexity=5242.9023, train_loss=8.5646305

Batch 2630, train_perplexity=4973.1533, train_loss=8.511809

Batch 2640, train_perplexity=4833.361, train_loss=8.483297

Batch 2650, train_perplexity=5414.491, train_loss=8.596834

Batch 2660, train_perplexity=4934.828, train_loss=8.504073

Batch 2670, train_perplexity=5578.646, train_loss=8.626701

Batch 2680, train_perplexity=5906.2217, train_loss=8.683762

Batch 2690, train_perplexity=6018.91, train_loss=8.7026615

Batch 2700, train_perplexity=6021.9995, train_loss=8.703175

Batch 2710, train_perplexity=4883.8564, train_loss=8.4936905

Batch 2720, train_perplexity=5345.086, train_loss=8.583933

Batch 2730, train_perplexity=5217.5737, train_loss=8.559788

Batch 2740, train_perplexity=4992.946, train_loss=8.515781

Batch 2750, train_perplexity=5225.556, train_loss=8.5613165

Batch 2760, train_perplexity=5317.4478, train_loss=8.578749

Batch 2770, train_perplexity=4930.392, train_loss=8.503174

Batch 2780, train_perplexity=5946.905, train_loss=8.690626

Batch 2790, train_perplexity=4929.175, train_loss=8.502927

Batch 2800, train_perplexity=5714.9497, train_loss=8.650841

Batch 2810, train_perplexity=4538.678, train_loss=8.420391

Batch 2820, train_perplexity=5780.1377, train_loss=8.662183

Batch 2830, train_perplexity=5227.7593, train_loss=8.561738

Batch 2840, train_perplexity=5047.578, train_loss=8.526664

Batch 2850, train_perplexity=5435.8486, train_loss=8.600771

Batch 2860, train_perplexity=5900.0967, train_loss=8.682724

Batch 2870, train_perplexity=5634.939, train_loss=8.636742

Batch 2880, train_perplexity=6333.3335, train_loss=8.753582

Batch 2890, train_perplexity=5413.123, train_loss=8.596581

Batch 2900, train_perplexity=5581.0938, train_loss=8.62714

Batch 2910, train_perplexity=5382.6323, train_loss=8.590933

Batch 2920, train_perplexity=5264.0146, train_loss=8.568649

Batch 2930, train_perplexity=5962.9595, train_loss=8.693322

Batch 2940, train_perplexity=5974.372, train_loss=8.695234

Batch 2950, train_perplexity=5624.674, train_loss=8.634918

Batch 2960, train_perplexity=5782.7734, train_loss=8.662639

Batch 2970, train_perplexity=4557.885, train_loss=8.424614

Batch 2980, train_perplexity=5175.8496, train_loss=8.551759

Batch 2990, train_perplexity=5338.473, train_loss=8.582695

Batch 3000, train_perplexity=5008.5693, train_loss=8.518906

Batch 3010, train_perplexity=4672.567, train_loss=8.449464

Batch 3020, train_perplexity=4687.872, train_loss=8.452734

Batch 3030, train_perplexity=5497.6646, train_loss=8.612079

Batch 3040, train_perplexity=5979.126, train_loss=8.69603

Batch 3050, train_perplexity=6339.243, train_loss=8.754515

Batch 3060, train_perplexity=5129.8096, train_loss=8.542824

Batch 3070, train_perplexity=4905.5063, train_loss=8.498114

Batch 3080, train_perplexity=5501.8867, train_loss=8.612846

Batch 3090, train_perplexity=5035.7163, train_loss=8.524311

Batch 3100, train_perplexity=4588.878, train_loss=8.431391

Batch 3110, train_perplexity=4998.3965, train_loss=8.516872

Batch 3120, train_perplexity=4846.0996, train_loss=8.4859295

Batch 3130, train_perplexity=5378.471, train_loss=8.590159

Batch 3140, train_perplexity=5140.5146, train_loss=8.544909

Batch 3150, train_perplexity=5051.9844, train_loss=8.527536

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Batch 3160, train_perplexity=4779.001, train_loss=8.471987

Batch 3170, train_perplexity=4810.965, train_loss=8.478653

Batch 3180, train_perplexity=4927.788, train_loss=8.5026455

Batch 3190, train_perplexity=6136.423, train_loss=8.721997

Batch 3200, train_perplexity=5597.031, train_loss=8.629992

Batch 3210, train_perplexity=5170.0034, train_loss=8.550629

Batch 3220, train_perplexity=5179.716, train_loss=8.5525055

Batch 3230, train_perplexity=5317.4478, train_loss=8.578749

Batch 3240, train_perplexity=5334.4834, train_loss=8.581947

Batch 3250, train_perplexity=6113.1465, train_loss=8.718197

Batch 3260, train_perplexity=6430.5356, train_loss=8.768813

Batch 3270, train_perplexity=5111.3696, train_loss=8.539223
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3280, train_perplexity=5442.81, train_loss=8.602051

Batch 3290, train_perplexity=5485.3154, train_loss=8.60983

Batch 3300, train_perplexity=5851.0225, train_loss=8.674372

Batch 3310, train_perplexity=4688.7573, train_loss=8.452923

Batch 3320, train_perplexity=4660.84, train_loss=8.446951

Batch 3330, train_perplexity=4681.309, train_loss=8.451333

Batch 3340, train_perplexity=6157.6973, train_loss=8.725458

Batch 3350, train_perplexity=5486.2676, train_loss=8.610003

Batch 3360, train_perplexity=5554.645, train_loss=8.62239

Batch 3370, train_perplexity=5117.8325, train_loss=8.540486

Batch 3380, train_perplexity=5448.268, train_loss=8.603053

Batch 3390, train_perplexity=5373.4106, train_loss=8.589218

Batch 3400, train_perplexity=5814.233, train_loss=8.668064

Batch 3410, train_perplexity=5830.602, train_loss=8.670876

Batch 3420, train_perplexity=4498.2627, train_loss=8.411447

Batch 3430, train_perplexity=5572.584, train_loss=8.625614

Batch 3440, train_perplexity=5710.0684, train_loss=8.649986

Batch 3450, train_perplexity=5398.1006, train_loss=8.593802

Batch 3460, train_perplexity=6569.8296, train_loss=8.790243

Batch 3470, train_perplexity=4859.484, train_loss=8.4886875

Batch 3480, train_perplexity=5252.2407, train_loss=8.56641

Batch 3490, train_perplexity=6424.8164, train_loss=8.767923

Batch 3500, train_perplexity=5290.5425, train_loss=8.573676

Batch 3510, train_perplexity=6514.5884, train_loss=8.781799

Batch 3520, train_perplexity=5335.796, train_loss=8.582193

Batch 3530, train_perplexity=6241.4326, train_loss=8.738965

Batch 3540, train_perplexity=4842.755, train_loss=8.485239

Batch 3550, train_perplexity=5078.485, train_loss=8.532768

Batch 3560, train_perplexity=5552.7646, train_loss=8.622051

Batch 3570, train_perplexity=6251.375, train_loss=8.740557

Batch 3580, train_perplexity=5345.774, train_loss=8.584062

Batch 3590, train_perplexity=4923.4478, train_loss=8.501764

Batch 3600, train_perplexity=6643.1025, train_loss=8.801334

Batch 3610, train_perplexity=4257.5903, train_loss=8.356459

Batch 3620, train_perplexity=5010.3135, train_loss=8.519254

Batch 3630, train_perplexity=5565.2974, train_loss=8.624306

Batch 3640, train_perplexity=5546.477, train_loss=8.620918

Batch 3650, train_perplexity=5074.4087, train_loss=8.531965

Batch 3660, train_perplexity=4484.74, train_loss=8.408436

Batch 3670, train_perplexity=5126.1367, train_loss=8.542108

Batch 3680, train_perplexity=5960.1904, train_loss=8.692858

Batch 3690, train_perplexity=6094.8154, train_loss=8.715194

Batch 3700, train_perplexity=5110.863, train_loss=8.539124

Batch 3710, train_perplexity=5786.701, train_loss=8.663318

Batch 3720, train_perplexity=4885.995, train_loss=8.494128

Batch 3730, train_perplexity=4641.6504, train_loss=8.442825

Batch 3740, train_perplexity=5866.287, train_loss=8.676977

Batch 3750, train_perplexity=5823.711, train_loss=8.669693

Batch 3760, train_perplexity=5267.902, train_loss=8.569387

Batch 3770, train_perplexity=5288.5503, train_loss=8.573299

Batch 3780, train_perplexity=6012.6914, train_loss=8.701628

Batch 3790, train_perplexity=5477.8296, train_loss=8.608464

Batch 3800, train_perplexity=5321.303, train_loss=8.5794735

Batch 3810, train_perplexity=5828.773, train_loss=8.670562

Batch 3820, train_perplexity=5634.0366, train_loss=8.636581

Batch 3830, train_perplexity=4616.8525, train_loss=8.437469

Batch 3840, train_perplexity=5501.7715, train_loss=8.612825

Batch 3850, train_perplexity=6778.5723, train_loss=8.821522

Batch 3860, train_perplexity=5912.5786, train_loss=8.684837

Batch 3870, train_perplexity=4850.0576, train_loss=8.486746

Batch 3880, train_perplexity=5089.5444, train_loss=8.534944

Batch 3890, train_perplexity=4964.9595, train_loss=8.51016

Batch 3900, train_perplexity=6109.463, train_loss=8.717594

Batch 3910, train_perplexity=4486.999, train_loss=8.408939

Batch 3920, train_perplexity=5798.534, train_loss=8.66536

Batch 3930, train_perplexity=5382.176, train_loss=8.590848

Batch 3940, train_perplexity=5542.1626, train_loss=8.62014

Batch 3950, train_perplexity=6224.0522, train_loss=8.7361765

Batch 3960, train_perplexity=5313.3467, train_loss=8.577977

Batch 3970, train_perplexity=5174.093, train_loss=8.551419

Batch 3980, train_perplexity=5174.7886, train_loss=8.551554

Batch 3990, train_perplexity=6552.4595, train_loss=8.787596

Batch 4000, train_perplexity=5052.1865, train_loss=8.527576

Batch 4010, train_perplexity=5961.066, train_loss=8.693005

Batch 4020, train_perplexity=5175.43, train_loss=8.551678

Batch 4030, train_perplexity=5334.9004, train_loss=8.582026

Batch 4040, train_perplexity=6239.082, train_loss=8.738588

Batch 4050, train_perplexity=5617.865, train_loss=8.633707

Batch 4060, train_perplexity=5118.7847, train_loss=8.540672

Batch 4070, train_perplexity=5001.4673, train_loss=8.517487

Batch 4080, train_perplexity=5363.8467, train_loss=8.587437

Batch 4090, train_perplexity=6103.8486, train_loss=8.716675

Batch 4100, train_perplexity=4761.686, train_loss=8.468357

Batch 4110, train_perplexity=4995.1133, train_loss=8.516215

Batch 4120, train_perplexity=6073.098, train_loss=8.711624

Batch 4130, train_perplexity=4909.822, train_loss=8.498993

Batch 4140, train_perplexity=5473.7876, train_loss=8.607726

Batch 4150, train_perplexity=4572.3823, train_loss=8.42779

Batch 4160, train_perplexity=5421.032, train_loss=8.598042

Batch 4170, train_perplexity=5467.918, train_loss=8.606653

Batch 4180, train_perplexity=6219.845, train_loss=8.7355

Batch 4190, train_perplexity=4939.4897, train_loss=8.505017

Batch 4200, train_perplexity=5745.1196, train_loss=8.656106

Batch 4210, train_perplexity=4391.338, train_loss=8.387389

Batch 4220, train_perplexity=6529.7593, train_loss=8.784125

Batch 4230, train_perplexity=5177.3057, train_loss=8.55204

Batch 4240, train_perplexity=4372.6826, train_loss=8.383132

Batch 4250, train_perplexity=5420.464, train_loss=8.597937

Batch 4260, train_perplexity=5230.4023, train_loss=8.562243

Batch 4270, train_perplexity=4920.969, train_loss=8.501261

Batch 4280, train_perplexity=5186.5474, train_loss=8.553823

Batch 4290, train_perplexity=4651.271, train_loss=8.444896

Batch 4300, train_perplexity=5915.8047, train_loss=8.685383

Batch 4310, train_perplexity=5446.8706, train_loss=8.602797

Batch 4320, train_perplexity=4738.0176, train_loss=8.463374

Batch 4330, train_perplexity=5264.4517, train_loss=8.568732

Batch 4340, train_perplexity=6254.583, train_loss=8.74107

Batch 4350, train_perplexity=5158.115, train_loss=8.5483265

Batch 4360, train_perplexity=4621.0464, train_loss=8.438376

Batch 4370, train_perplexity=4038.509, train_loss=8.303631

Batch 4380, train_perplexity=4604.4224, train_loss=8.4347725

Batch 4390, train_perplexity=5335.4756, train_loss=8.582133

Batch 4400, train_perplexity=4265.2876, train_loss=8.358265

Batch 4410, train_perplexity=5068.3774, train_loss=8.530776

Batch 4420, train_perplexity=5536.056, train_loss=8.619038

Batch 4430, train_perplexity=5954.021, train_loss=8.691822

Batch 4440, train_perplexity=5468.533, train_loss=8.606766

Batch 4450, train_perplexity=5614.598, train_loss=8.633125

Batch 4460, train_perplexity=5462.9927, train_loss=8.605752

Batch 4470, train_perplexity=5108.1636, train_loss=8.538595

Batch 4480, train_perplexity=5689.5103, train_loss=8.646379

Batch 4490, train_perplexity=4482.012, train_loss=8.407827

Batch 4500, train_perplexity=5507.651, train_loss=8.6138935

Batch 4510, train_perplexity=4995.318, train_loss=8.516256

Batch 4520, train_perplexity=6162.6143, train_loss=8.726256

Batch 4530, train_perplexity=4951.625, train_loss=8.507471

Batch 4540, train_perplexity=5223.588, train_loss=8.56094

Batch 4550, train_perplexity=5487.7173, train_loss=8.610268

Batch 4560, train_perplexity=6381.789, train_loss=8.761204

Batch 4570, train_perplexity=4833.965, train_loss=8.483422

Batch 4580, train_perplexity=5238.854, train_loss=8.563858

Batch 4590, train_perplexity=4470.081, train_loss=8.405162

Batch 4600, train_perplexity=5353.993, train_loss=8.585598

Batch 4610, train_perplexity=5289.266, train_loss=8.573435

Batch 4620, train_perplexity=4825.697, train_loss=8.48171

Batch 4630, train_perplexity=4553.484, train_loss=8.423648
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4640, train_perplexity=5449.9727, train_loss=8.603366

Batch 4650, train_perplexity=4958.704, train_loss=8.5089

Batch 4660, train_perplexity=4844.1777, train_loss=8.485533

Batch 4670, train_perplexity=5088.4814, train_loss=8.534735

Batch 4680, train_perplexity=5249.997, train_loss=8.565983

Batch 4690, train_perplexity=4986.6455, train_loss=8.514519

Batch 4700, train_perplexity=5316.1396, train_loss=8.578503

Batch 4710, train_perplexity=5744.347, train_loss=8.655972

Batch 4720, train_perplexity=5522.6514, train_loss=8.616613

Batch 4730, train_perplexity=4260.145, train_loss=8.357059

Batch 4740, train_perplexity=4640.681, train_loss=8.442616

Batch 4750, train_perplexity=4946.122, train_loss=8.506359

Batch 4760, train_perplexity=5855.064, train_loss=8.675062

Batch 4770, train_perplexity=6430.542, train_loss=8.768814

Batch 4780, train_perplexity=5871.2065, train_loss=8.677815

Batch 4790, train_perplexity=5077.313, train_loss=8.532537

Batch 4800, train_perplexity=5390.297, train_loss=8.592356

Batch 4810, train_perplexity=5197.936, train_loss=8.556017

Batch 4820, train_perplexity=5988.6104, train_loss=8.697615

Batch 4830, train_perplexity=4721.171, train_loss=8.459812

Batch 4840, train_perplexity=5146.617, train_loss=8.546095

Batch 4850, train_perplexity=5750.3325, train_loss=8.657013

Batch 4860, train_perplexity=5818.659, train_loss=8.668825

Batch 4870, train_perplexity=5240.393, train_loss=8.564152

Batch 4880, train_perplexity=5370.644, train_loss=8.588703

Batch 4890, train_perplexity=6920.8423, train_loss=8.842293

Batch 4900, train_perplexity=4529.8394, train_loss=8.418442

Batch 4910, train_perplexity=5728.493, train_loss=8.653208

Batch 4920, train_perplexity=5278.845, train_loss=8.571463

Batch 4930, train_perplexity=6251.983, train_loss=8.740654

Batch 4940, train_perplexity=5628.403, train_loss=8.635581

Batch 4950, train_perplexity=5062.2134, train_loss=8.529559

Batch 4960, train_perplexity=5443.6978, train_loss=8.602214

Batch 4970, train_perplexity=4782.3794, train_loss=8.472693

Batch 4980, train_perplexity=6286.6787, train_loss=8.746188

Batch 4990, train_perplexity=4999.1064, train_loss=8.5170145

Batch 5000, train_perplexity=5693.0654, train_loss=8.647004

Batch 5010, train_perplexity=5840.4077, train_loss=8.672556

Batch 5020, train_perplexity=5533.4062, train_loss=8.618559

Batch 5030, train_perplexity=5970.6245, train_loss=8.694607

Batch 5040, train_perplexity=5408.443, train_loss=8.595716

Batch 5050, train_perplexity=5282.37, train_loss=8.57213

Batch 5060, train_perplexity=5729.1816, train_loss=8.653328

Batch 5070, train_perplexity=5795.632, train_loss=8.66486

Batch 5080, train_perplexity=5766.302, train_loss=8.659786

Batch 5090, train_perplexity=5605.3906, train_loss=8.631484

Batch 5100, train_perplexity=5732.9307, train_loss=8.653982

Batch 5110, train_perplexity=5000.4463, train_loss=8.5172825

Batch 5120, train_perplexity=5630.459, train_loss=8.635946

Batch 5130, train_perplexity=4726.5503, train_loss=8.460951

Batch 5140, train_perplexity=6358.2915, train_loss=8.757515

Batch 5150, train_perplexity=5562.093, train_loss=8.62373

Batch 5160, train_perplexity=5809.344, train_loss=8.667223

Batch 5170, train_perplexity=4571.406, train_loss=8.427576

Batch 5180, train_perplexity=4714.3413, train_loss=8.4583645

Batch 5190, train_perplexity=6115.584, train_loss=8.7185955

Batch 5200, train_perplexity=5924.8105, train_loss=8.686904

Batch 5210, train_perplexity=4791.0664, train_loss=8.474508

Batch 5220, train_perplexity=5484.144, train_loss=8.609616

Batch 5230, train_perplexity=5239.5034, train_loss=8.563982

Batch 5240, train_perplexity=4429.415, train_loss=8.396023

Batch 5250, train_perplexity=5197.48, train_loss=8.555929

Batch 5260, train_perplexity=5058.1743, train_loss=8.528761

Batch 5270, train_perplexity=5813.39, train_loss=8.667919

Batch 5280, train_perplexity=5469.0967, train_loss=8.606869

Batch 5290, train_perplexity=5325.771, train_loss=8.580313

Batch 5300, train_perplexity=4973.566, train_loss=8.511892

Batch 5310, train_perplexity=6749.1255, train_loss=8.817168

Batch 5320, train_perplexity=4857.825, train_loss=8.488346

Batch 5330, train_perplexity=6159.142, train_loss=8.725693

Batch 5340, train_perplexity=4672.7583, train_loss=8.449505

Batch 5350, train_perplexity=5736.4473, train_loss=8.654595

Batch 5360, train_perplexity=5164.9375, train_loss=8.549648

Batch 5370, train_perplexity=4874.0205, train_loss=8.491674

Batch 5380, train_perplexity=5803.696, train_loss=8.66625

Batch 5390, train_perplexity=5022.1436, train_loss=8.521612

Batch 5400, train_perplexity=5320.0396, train_loss=8.579236

Batch 5410, train_perplexity=5733.565, train_loss=8.654093

Batch 5420, train_perplexity=5445.022, train_loss=8.602457

Batch 5430, train_perplexity=6113.088, train_loss=8.718187

Batch 5440, train_perplexity=5811.982, train_loss=8.667677

Batch 5450, train_perplexity=5647.1675, train_loss=8.638909

Batch 5460, train_perplexity=5916.166, train_loss=8.685444

Batch 5470, train_perplexity=5603.5894, train_loss=8.631163

Batch 5480, train_perplexity=4928.1733, train_loss=8.502724

Batch 5490, train_perplexity=4989.11, train_loss=8.515013

Batch 5500, train_perplexity=5847.3633, train_loss=8.673746

Batch 5510, train_perplexity=5495.311, train_loss=8.61165

Batch 5520, train_perplexity=6838.9727, train_loss=8.830393

Batch 5530, train_perplexity=6238.356, train_loss=8.738472

Batch 5540, train_perplexity=5276.0366, train_loss=8.5709305

Batch 5550, train_perplexity=5850.42, train_loss=8.674269

Batch 5560, train_perplexity=6563.329, train_loss=8.789253

Batch 5570, train_perplexity=6313.2705, train_loss=8.750409

Batch 5580, train_perplexity=5025.9385, train_loss=8.5223675

Batch 5590, train_perplexity=5367.019, train_loss=8.588028

Batch 5600, train_perplexity=5150.1475, train_loss=8.546781

Batch 5610, train_perplexity=5707.8906, train_loss=8.649605

Batch 5620, train_perplexity=5953.9756, train_loss=8.691814

Batch 5630, train_perplexity=5246.3335, train_loss=8.565285

Batch 5640, train_perplexity=4908.4077, train_loss=8.498705

Batch 5650, train_perplexity=5382.2627, train_loss=8.590864

Batch 5660, train_perplexity=4438.849, train_loss=8.39815

Batch 5670, train_perplexity=4934.8237, train_loss=8.504072

Batch 5680, train_perplexity=5690.2266, train_loss=8.646505

Batch 5690, train_perplexity=6031.3965, train_loss=8.704734

Batch 5700, train_perplexity=5554.926, train_loss=8.62244

Batch 5710, train_perplexity=5676.254, train_loss=8.644047

Batch 5720, train_perplexity=4868.4595, train_loss=8.490533

Batch 5730, train_perplexity=5357.773, train_loss=8.586304

Batch 5740, train_perplexity=5606.107, train_loss=8.631612

Batch 5750, train_perplexity=6312.2954, train_loss=8.750255

Batch 5760, train_perplexity=6225.9346, train_loss=8.736479

Batch 5770, train_perplexity=5029.9614, train_loss=8.523168

Batch 5780, train_perplexity=5961.646, train_loss=8.693102

Batch 5790, train_perplexity=4937.281, train_loss=8.50457

Batch 5800, train_perplexity=6054.9854, train_loss=8.708637

Batch 5810, train_perplexity=5639.552, train_loss=8.63756

Batch 5820, train_perplexity=5865.0283, train_loss=8.676763

Batch 5830, train_perplexity=5426.0596, train_loss=8.5989685

Batch 5840, train_perplexity=5302.3877, train_loss=8.575912

Batch 5850, train_perplexity=5489.9053, train_loss=8.610666

Batch 5860, train_perplexity=4303.294, train_loss=8.367136

Batch 5870, train_perplexity=5570.6924, train_loss=8.625275

Batch 5880, train_perplexity=4455.564, train_loss=8.401909

Batch 5890, train_perplexity=5173.0615, train_loss=8.55122

Batch 5900, train_perplexity=5254.395, train_loss=8.56682

Batch 5910, train_perplexity=4865.3823, train_loss=8.489901

Batch 5920, train_perplexity=4854.6294, train_loss=8.487688

Batch 5930, train_perplexity=5478.3677, train_loss=8.608562

Batch 5940, train_perplexity=5462.1484, train_loss=8.6055975

Batch 5950, train_perplexity=4864.533, train_loss=8.489726

Batch 5960, train_perplexity=5764.7236, train_loss=8.6595125

Batch 5970, train_perplexity=5118.228, train_loss=8.540564

Batch 5980, train_perplexity=5629.471, train_loss=8.635771

Batch 5990, train_perplexity=4853.87, train_loss=8.487532
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 6000, train_perplexity=5149.396, train_loss=8.546635

Batch 6010, train_perplexity=6028.6187, train_loss=8.704273

Batch 6020, train_perplexity=4468.538, train_loss=8.404817

Batch 6030, train_perplexity=5800.3813, train_loss=8.665679

Batch 6040, train_perplexity=5158.287, train_loss=8.54836

Batch 6050, train_perplexity=4739.0117, train_loss=8.463584

Batch 6060, train_perplexity=5222.4424, train_loss=8.56072

Batch 6070, train_perplexity=6112.5923, train_loss=8.718106

Batch 6080, train_perplexity=4821.1567, train_loss=8.480769

Batch 6090, train_perplexity=5642.274, train_loss=8.638042

Batch 6100, train_perplexity=5259.549, train_loss=8.5678005

Batch 6110, train_perplexity=5587.4844, train_loss=8.628284

Batch 6120, train_perplexity=4798.0806, train_loss=8.475971

Batch 6130, train_perplexity=5513.116, train_loss=8.614885

Batch 6140, train_perplexity=4970.0, train_loss=8.511175

Batch 6150, train_perplexity=5387.804, train_loss=8.591893

Batch 6160, train_perplexity=5323.5874, train_loss=8.579903

Batch 6170, train_perplexity=4777.739, train_loss=8.471723

Batch 6180, train_perplexity=5228.7314, train_loss=8.561924

Batch 6190, train_perplexity=5480.1025, train_loss=8.608879

Batch 6200, train_perplexity=4570.133, train_loss=8.427298

Batch 6210, train_perplexity=6476.831, train_loss=8.775987

Batch 6220, train_perplexity=4738.61, train_loss=8.463499

Batch 6230, train_perplexity=5796.14, train_loss=8.6649475

Batch 6240, train_perplexity=4388.411, train_loss=8.386723

Batch 6250, train_perplexity=4062.687, train_loss=8.3096

Batch 6260, train_perplexity=6451.9683, train_loss=8.7721405

Batch 6270, train_perplexity=5338.9927, train_loss=8.582792

Batch 6280, train_perplexity=4705.313, train_loss=8.456448

Batch 6290, train_perplexity=5420.1226, train_loss=8.597874

Batch 6300, train_perplexity=5856.5605, train_loss=8.675318

Batch 6310, train_perplexity=4852.315, train_loss=8.487211

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Batch 6320, train_perplexity=5636.8955, train_loss=8.637089

Batch 6330, train_perplexity=5046.88, train_loss=8.5265255

Batch 6340, train_perplexity=5360.3643, train_loss=8.586787

Batch 6350, train_perplexity=4914.497, train_loss=8.499945

Batch 6360, train_perplexity=4277.451, train_loss=8.361113

Batch 6370, train_perplexity=5553.9937, train_loss=8.6222725

Batch 6380, train_perplexity=4333.068, train_loss=8.374031

Batch 6390, train_perplexity=5788.3955, train_loss=8.66361

Batch 6400, train_perplexity=5743.6074, train_loss=8.655843

Batch 6410, train_perplexity=4356.9355, train_loss=8.379524

Batch 6420, train_perplexity=5027.842, train_loss=8.522746

Batch 6430, train_perplexity=5897.143, train_loss=8.682223

Batch 6440, train_perplexity=5761.558, train_loss=8.658963

Batch 6450, train_perplexity=6253.7007, train_loss=8.740929

Batch 6460, train_perplexity=5724.6265, train_loss=8.652533

Batch 6470, train_perplexity=5274.7993, train_loss=8.570696

Batch 6480, train_perplexity=6014.2227, train_loss=8.701882

Batch 6490, train_perplexity=5403.504, train_loss=8.594803

Batch 6500, train_perplexity=5374.605, train_loss=8.58944

Batch 6510, train_perplexity=5632.0327, train_loss=8.636226

Batch 6520, train_perplexity=5972.0024, train_loss=8.694838

Batch 6530, train_perplexity=5114.081, train_loss=8.539753

Batch 6540, train_perplexity=5818.698, train_loss=8.668832

Batch 6550, train_perplexity=4927.257, train_loss=8.502538

Batch 6560, train_perplexity=4974.576, train_loss=8.512095

Batch 6570, train_perplexity=4521.199, train_loss=8.4165325

Batch 6580, train_perplexity=5967.505, train_loss=8.694084

Batch 6590, train_perplexity=4921.57, train_loss=8.501383

Batch 6600, train_perplexity=5514.457, train_loss=8.6151285

Batch 6610, train_perplexity=4723.495, train_loss=8.460304

Batch 6620, train_perplexity=5457.02, train_loss=8.604658

Batch 6630, train_perplexity=6613.7715, train_loss=8.796909

Batch 6640, train_perplexity=4802.141, train_loss=8.476817

Batch 6650, train_perplexity=4679.4565, train_loss=8.450937

Batch 6660, train_perplexity=5189.9365, train_loss=8.554477

Batch 6670, train_perplexity=6371.5664, train_loss=8.759601

Batch 6680, train_perplexity=6680.892, train_loss=8.807007

Batch 6690, train_perplexity=4488.8477, train_loss=8.409351

Batch 6700, train_perplexity=5339.441, train_loss=8.582876

Batch 6710, train_perplexity=5239.6934, train_loss=8.564018

Batch 6720, train_perplexity=5013.4346, train_loss=8.5198765

Batch 6730, train_perplexity=5070.698, train_loss=8.531234

Batch 6740, train_perplexity=5417.332, train_loss=8.597359

Batch 6750, train_perplexity=5483.2705, train_loss=8.609457

Batch 6760, train_perplexity=4889.6636, train_loss=8.494879

Batch 6770, train_perplexity=6027.4004, train_loss=8.704071

Batch 6780, train_perplexity=4752.794, train_loss=8.466488

Batch 6790, train_perplexity=5723.0327, train_loss=8.652254

Batch 6800, train_perplexity=5194.1255, train_loss=8.555284

Batch 6810, train_perplexity=5139.0347, train_loss=8.5446205

Batch 6820, train_perplexity=7777.4844, train_loss=8.958988

Batch 6830, train_perplexity=5109.625, train_loss=8.538881

Batch 6840, train_perplexity=5485.7915, train_loss=8.609917

Batch 6850, train_perplexity=6614.5786, train_loss=8.797031

Batch 6860, train_perplexity=7106.478, train_loss=8.868762

Batch 6870, train_perplexity=5248.8457, train_loss=8.565763

Batch 6880, train_perplexity=5590.725, train_loss=8.628864

Batch 6890, train_perplexity=5680.846, train_loss=8.6448555

Batch 6900, train_perplexity=6802.2607, train_loss=8.82501

Batch 6910, train_perplexity=5790.527, train_loss=8.663979

Batch 6920, train_perplexity=4736.888, train_loss=8.463136

Batch 6930, train_perplexity=5615.964, train_loss=8.6333685

Batch 6940, train_perplexity=4725.793, train_loss=8.460791

Batch 6950, train_perplexity=4795.4272, train_loss=8.475418

Batch 6960, train_perplexity=5528.954, train_loss=8.617754

Batch 6970, train_perplexity=5299.4756, train_loss=8.575363

Batch 6980, train_perplexity=6025.883, train_loss=8.703819

Batch 6990, train_perplexity=4670.7715, train_loss=8.4490795

Batch 7000, train_perplexity=5369.4, train_loss=8.588471

Batch 7010, train_perplexity=5099.023, train_loss=8.536804

Batch 7020, train_perplexity=5205.1787, train_loss=8.557409

Batch 7030, train_perplexity=5881.7705, train_loss=8.679613

Batch 7040, train_perplexity=6254.7324, train_loss=8.741094

Batch 7050, train_perplexity=5664.746, train_loss=8.642017

Batch 7060, train_perplexity=4772.515, train_loss=8.470629

Batch 7070, train_perplexity=6601.106, train_loss=8.794992

Batch 7080, train_perplexity=5108.2705, train_loss=8.538616

Batch 7090, train_perplexity=5084.809, train_loss=8.534013

Batch 7100, train_perplexity=5635.208, train_loss=8.636789

Batch 7110, train_perplexity=6055.355, train_loss=8.708698

Batch 7120, train_perplexity=4835.3896, train_loss=8.483717

Batch 7130, train_perplexity=5458.9355, train_loss=8.605009

Batch 7140, train_perplexity=5492.021, train_loss=8.611052

Batch 7150, train_perplexity=5817.4443, train_loss=8.668616

Batch 7160, train_perplexity=5751.879, train_loss=8.657282

Batch 7170, train_perplexity=5164.7305, train_loss=8.549608

Batch 7180, train_perplexity=5580.657, train_loss=8.627062

Batch 7190, train_perplexity=4537.605, train_loss=8.420155

Batch 7200, train_perplexity=5438.244, train_loss=8.601212

Batch 7210, train_perplexity=5343.445, train_loss=8.583626

Batch 7220, train_perplexity=5774.639, train_loss=8.661231

Batch 7230, train_perplexity=5954.924, train_loss=8.691974

Batch 7240, train_perplexity=5472.942, train_loss=8.607572

Batch 7250, train_perplexity=5536.2144, train_loss=8.619066

Batch 7260, train_perplexity=5164.78, train_loss=8.549618

Batch 7270, train_perplexity=6675.4404, train_loss=8.8061905

Batch 7280, train_perplexity=5374.0, train_loss=8.589328

Batch 7290, train_perplexity=5575.4653, train_loss=8.626131
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7300, train_perplexity=5192.6147, train_loss=8.554993

Batch 7310, train_perplexity=4977.6274, train_loss=8.512709

Batch 7320, train_perplexity=5778.027, train_loss=8.661818

Batch 7330, train_perplexity=5055.6475, train_loss=8.528261

Batch 7340, train_perplexity=4549.703, train_loss=8.422817

Batch 7350, train_perplexity=5703.864, train_loss=8.648899

Batch 7360, train_perplexity=5968.5864, train_loss=8.694265

Batch 7370, train_perplexity=4354.7505, train_loss=8.379023

Batch 7380, train_perplexity=5940.9526, train_loss=8.689625

Batch 7390, train_perplexity=5858.7725, train_loss=8.675695

Batch 7400, train_perplexity=5482.9204, train_loss=8.609393

Batch 7410, train_perplexity=5153.4688, train_loss=8.547425

Batch 7420, train_perplexity=5172.701, train_loss=8.55115

Batch 7430, train_perplexity=5027.554, train_loss=8.522689

Batch 7440, train_perplexity=4544.6724, train_loss=8.421711

Batch 7450, train_perplexity=5303.1157, train_loss=8.57605

Batch 7460, train_perplexity=5526.914, train_loss=8.617385

Batch 7470, train_perplexity=6289.335, train_loss=8.746611

Batch 7480, train_perplexity=5884.778, train_loss=8.680124

Batch 7490, train_perplexity=4351.5664, train_loss=8.378291

Batch 7500, train_perplexity=5856.0576, train_loss=8.675232

Batch 7510, train_perplexity=5044.546, train_loss=8.526063

Batch 7520, train_perplexity=4771.564, train_loss=8.470429

Batch 7530, train_perplexity=4996.89, train_loss=8.516571

Batch 7540, train_perplexity=6036.57, train_loss=8.705591

Batch 7550, train_perplexity=4243.767, train_loss=8.353207

Batch 7560, train_perplexity=6214.74, train_loss=8.734679

Batch 7570, train_perplexity=5375.133, train_loss=8.589539

Batch 7580, train_perplexity=5598.7817, train_loss=8.630304

Batch 7590, train_perplexity=4501.8164, train_loss=8.412236

Batch 7600, train_perplexity=6118.7104, train_loss=8.719107

Batch 7610, train_perplexity=5528.8594, train_loss=8.617737

Batch 7620, train_perplexity=6725.8657, train_loss=8.813716

Batch 7630, train_perplexity=6539.724, train_loss=8.78565

Batch 7640, train_perplexity=5991.1924, train_loss=8.698046

Batch 7650, train_perplexity=6159.3887, train_loss=8.725733

Batch 7660, train_perplexity=5906.526, train_loss=8.683813

Batch 7670, train_perplexity=5365.576, train_loss=8.587759

Batch 7680, train_perplexity=5672.466, train_loss=8.643379

Batch 7690, train_perplexity=5940.188, train_loss=8.689496

Batch 7700, train_perplexity=5415.813, train_loss=8.597078

Batch 7710, train_perplexity=5149.062, train_loss=8.54657

Batch 7720, train_perplexity=5728.045, train_loss=8.65313

Batch 7730, train_perplexity=4800.204, train_loss=8.476414

Batch 7740, train_perplexity=5961.1, train_loss=8.69301

Batch 7750, train_perplexity=4681.6616, train_loss=8.451408

Batch 7760, train_perplexity=5220.864, train_loss=8.560418

Batch 7770, train_perplexity=5638.573, train_loss=8.637386

Batch 7780, train_perplexity=5265.5815, train_loss=8.568947

Batch 7790, train_perplexity=4371.8276, train_loss=8.3829365

Batch 7800, train_perplexity=5319.233, train_loss=8.579084

Batch 7810, train_perplexity=5657.0586, train_loss=8.640659

Batch 7820, train_perplexity=5227.166, train_loss=8.561625

Batch 7830, train_perplexity=5099.6743, train_loss=8.536932

Batch 7840, train_perplexity=5449.4116, train_loss=8.603263

Batch 7850, train_perplexity=4835.085, train_loss=8.483654

Batch 7860, train_perplexity=4755.6274, train_loss=8.467084

Batch 7870, train_perplexity=6095.35, train_loss=8.7152815

Batch 7880, train_perplexity=5299.5264, train_loss=8.575373

Batch 7890, train_perplexity=5443.267, train_loss=8.602135

Batch 7900, train_perplexity=5959.895, train_loss=8.692808

Batch 7910, train_perplexity=6070.0176, train_loss=8.711117

Batch 7920, train_perplexity=5429.989, train_loss=8.599692

Batch 7930, train_perplexity=5576.667, train_loss=8.626347

Batch 7940, train_perplexity=5416.536, train_loss=8.597212

Batch 7950, train_perplexity=5609.439, train_loss=8.632206

Batch 7960, train_perplexity=5077.2163, train_loss=8.532518

Batch 7970, train_perplexity=6069.566, train_loss=8.711042

Batch 7980, train_perplexity=4689.061, train_loss=8.452988

Batch 7990, train_perplexity=5590.3784, train_loss=8.628802

Batch 8000, train_perplexity=5073.6826, train_loss=8.531822

Batch 8010, train_perplexity=5715.004, train_loss=8.65085

Batch 8020, train_perplexity=5502.3223, train_loss=8.612926

Batch 8030, train_perplexity=5422.0044, train_loss=8.598221

Batch 8040, train_perplexity=6137.324, train_loss=8.722144

Batch 8050, train_perplexity=5230.936, train_loss=8.5623455

Batch 8060, train_perplexity=4706.633, train_loss=8.456728

Batch 8070, train_perplexity=5739.802, train_loss=8.65518

Batch 8080, train_perplexity=5354.412, train_loss=8.585676

Batch 8090, train_perplexity=5501.6245, train_loss=8.612799

Batch 8100, train_perplexity=5013.9507, train_loss=8.5199795

Batch 8110, train_perplexity=5051.1655, train_loss=8.527374

Batch 8120, train_perplexity=5627.3994, train_loss=8.635403

Batch 8130, train_perplexity=4671.9385, train_loss=8.449329

Batch 8140, train_perplexity=6110.3135, train_loss=8.717733

Batch 8150, train_perplexity=6576.4995, train_loss=8.791258

Batch 8160, train_perplexity=6025.9287, train_loss=8.703827

Batch 8170, train_perplexity=5027.5684, train_loss=8.522692

Batch 8180, train_perplexity=5480.74, train_loss=8.608995

Batch 8190, train_perplexity=6218.944, train_loss=8.735355

Batch 8200, train_perplexity=4581.75, train_loss=8.429836

Batch 8210, train_perplexity=5867.2603, train_loss=8.677143

Batch 8220, train_perplexity=5236.4663, train_loss=8.563402

Batch 8230, train_perplexity=5348.2217, train_loss=8.584519

Batch 8240, train_perplexity=5181.4155, train_loss=8.552834

Batch 8250, train_perplexity=5551.0703, train_loss=8.621746

Batch 8260, train_perplexity=5012.961, train_loss=8.519782

Batch 8270, train_perplexity=4643.0674, train_loss=8.4431305

Batch 8280, train_perplexity=4976.0566, train_loss=8.512393

Batch 8290, train_perplexity=5685.5723, train_loss=8.645687

Batch 8300, train_perplexity=5410.424, train_loss=8.596083

Batch 8310, train_perplexity=5851.7812, train_loss=8.674501

Batch 8320, train_perplexity=5724.217, train_loss=8.652461

Batch 8330, train_perplexity=6086.7007, train_loss=8.713861

Batch 8340, train_perplexity=5334.2695, train_loss=8.581907

Batch 8350, train_perplexity=5584.496, train_loss=8.627749

Batch 8360, train_perplexity=5544.203, train_loss=8.620508

Batch 8370, train_perplexity=5416.9907, train_loss=8.597296

Batch 8380, train_perplexity=5158.253, train_loss=8.548353

Batch 8390, train_perplexity=5816.174, train_loss=8.668398

Batch 8400, train_perplexity=5537.1333, train_loss=8.619232

Batch 8410, train_perplexity=5046.8413, train_loss=8.526518

Batch 8420, train_perplexity=4724.486, train_loss=8.460514

Batch 8430, train_perplexity=5377.6763, train_loss=8.590012

Batch 8440, train_perplexity=6448.222, train_loss=8.77156

Batch 8450, train_perplexity=4089.0918, train_loss=8.316078

Batch 8460, train_perplexity=4679.354, train_loss=8.450915

Batch 8470, train_perplexity=4670.1743, train_loss=8.448952

Batch 8480, train_perplexity=5443.89, train_loss=8.602249

Batch 8490, train_perplexity=5520.672, train_loss=8.616255

Batch 8500, train_perplexity=5813.8667, train_loss=8.668001

Batch 8510, train_perplexity=5264.6924, train_loss=8.568778

Batch 8520, train_perplexity=5349.0737, train_loss=8.584679

Batch 8530, train_perplexity=4723.2876, train_loss=8.46026

Batch 8540, train_perplexity=5057.1807, train_loss=8.528564

Batch 8550, train_perplexity=5211.2583, train_loss=8.558577

Batch 8560, train_perplexity=4745.6694, train_loss=8.464988

Batch 8570, train_perplexity=5289.781, train_loss=8.573532

Batch 8580, train_perplexity=5631.501, train_loss=8.636131

Batch 8590, train_perplexity=5552.394, train_loss=8.6219845

Batch 8600, train_perplexity=5764.5645, train_loss=8.659485

Batch 8610, train_perplexity=4769.6396, train_loss=8.470026

Batch 8620, train_perplexity=5737.8535, train_loss=8.65484

Batch 8630, train_perplexity=5598.878, train_loss=8.6303215

Batch 8640, train_perplexity=5160.049, train_loss=8.548701

Batch 8650, train_perplexity=5462.1123, train_loss=8.605591
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 8660, train_perplexity=4622.0205, train_loss=8.438587

Batch 8670, train_perplexity=5155.607, train_loss=8.54784

Batch 8680, train_perplexity=5328.2603, train_loss=8.58078

Batch 8690, train_perplexity=5232.463, train_loss=8.562637

Batch 8700, train_perplexity=5698.291, train_loss=8.647922

Batch 8710, train_perplexity=5619.5156, train_loss=8.634001

Batch 8720, train_perplexity=5096.0234, train_loss=8.536216

Batch 8730, train_perplexity=5599.2627, train_loss=8.63039

Batch 8740, train_perplexity=5481.0537, train_loss=8.609053

Batch 8750, train_perplexity=5454.0596, train_loss=8.6041155

Batch 8760, train_perplexity=6017.2803, train_loss=8.702391

Batch 8770, train_perplexity=5693.8525, train_loss=8.647142

Batch 8780, train_perplexity=4826.797, train_loss=8.481938

Batch 8790, train_perplexity=5328.143, train_loss=8.580758

Batch 8800, train_perplexity=4957.697, train_loss=8.508697

Batch 8810, train_perplexity=6550.2354, train_loss=8.787256

Batch 8820, train_perplexity=6428.2734, train_loss=8.768461

Batch 8830, train_perplexity=6220.48, train_loss=8.735602

Batch 8840, train_perplexity=5407.03, train_loss=8.595455

Batch 8850, train_perplexity=4573.1675, train_loss=8.427961

Batch 8860, train_perplexity=5042.9683, train_loss=8.52575

Batch 8870, train_perplexity=5639.616, train_loss=8.637571

Batch 8880, train_perplexity=4720.617, train_loss=8.459695

Batch 8890, train_perplexity=5987.868, train_loss=8.697491

Batch 8900, train_perplexity=5675.8535, train_loss=8.643976

Batch 8910, train_perplexity=5807.4556, train_loss=8.666898

Batch 8920, train_perplexity=5445.905, train_loss=8.602619

Batch 8930, train_perplexity=6050.725, train_loss=8.707933

Batch 8940, train_perplexity=6680.5103, train_loss=8.80695

Batch 8950, train_perplexity=4540.9077, train_loss=8.420882

Batch 8960, train_perplexity=5095.9795, train_loss=8.536207

Batch 8970, train_perplexity=6474.9106, train_loss=8.77569

Batch 8980, train_perplexity=4726.9106, train_loss=8.461027

Batch 8990, train_perplexity=5691.958, train_loss=8.64681

Batch 9000, train_perplexity=5334.463, train_loss=8.5819435

Batch 9010, train_perplexity=5574.976, train_loss=8.626043

Batch 9020, train_perplexity=4799.751, train_loss=8.476319

Batch 9030, train_perplexity=5528.274, train_loss=8.617631

Batch 9040, train_perplexity=5578.0073, train_loss=8.626587

Batch 9050, train_perplexity=5424.7764, train_loss=8.598732

Batch 9060, train_perplexity=5046.331, train_loss=8.526417

Batch 9070, train_perplexity=6047.783, train_loss=8.707447

Batch 9080, train_perplexity=4915.73, train_loss=8.5001955

Batch 9090, train_perplexity=4857.334, train_loss=8.488245

Batch 9100, train_perplexity=6853.807, train_loss=8.83256

Batch 9110, train_perplexity=5913.4585, train_loss=8.684986

Batch 9120, train_perplexity=5318.898, train_loss=8.579021

Batch 9130, train_perplexity=5108.9575, train_loss=8.538751

Batch 9140, train_perplexity=5660.356, train_loss=8.641242

Batch 9150, train_perplexity=5097.0635, train_loss=8.53642

Batch 9160, train_perplexity=4149.9775, train_loss=8.330858

Batch 9170, train_perplexity=5429.466, train_loss=8.599596

Batch 9180, train_perplexity=6376.977, train_loss=8.760449

Batch 9190, train_perplexity=5916.1772, train_loss=8.685446

Batch 9200, train_perplexity=5314.2993, train_loss=8.578156

Batch 9210, train_perplexity=5463.769, train_loss=8.605894

Batch 9220, train_perplexity=6041.0503, train_loss=8.706333

Batch 9230, train_perplexity=5785.758, train_loss=8.663155

Batch 9240, train_perplexity=5981.4014, train_loss=8.69641

Batch 9250, train_perplexity=5489.036, train_loss=8.610508

Batch 9260, train_perplexity=5527.51, train_loss=8.617493

Batch 9270, train_perplexity=6396.065, train_loss=8.763438

Batch 9280, train_perplexity=5516.682, train_loss=8.615532

Batch 9290, train_perplexity=7100.577, train_loss=8.867931

Batch 9300, train_perplexity=4934.555, train_loss=8.504018

Batch 9310, train_perplexity=4809.6025, train_loss=8.47837

Batch 9320, train_perplexity=5634.606, train_loss=8.6366825

Batch 9330, train_perplexity=5937.543, train_loss=8.689051

Batch 9340, train_perplexity=4849.9556, train_loss=8.486725

Batch 9350, train_perplexity=5103.2847, train_loss=8.53764

Batch 9360, train_perplexity=5982.4453, train_loss=8.696585

Batch 9370, train_perplexity=5156.8755, train_loss=8.548086

Batch 9380, train_perplexity=5209.1064, train_loss=8.558164

Batch 9390, train_perplexity=5551.928, train_loss=8.621901

Batch 9400, train_perplexity=5218.4795, train_loss=8.559961

Batch 9410, train_perplexity=4512.089, train_loss=8.4145155

Batch 9420, train_perplexity=6321.1567, train_loss=8.7516575

Batch 9430, train_perplexity=4735.565, train_loss=8.462856

Batch 9440, train_perplexity=4704.963, train_loss=8.456373

Batch 9450, train_perplexity=5398.523, train_loss=8.593881

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Batch 9460, train_perplexity=5982.223, train_loss=8.6965475

Batch 9470, train_perplexity=6152.097, train_loss=8.724548

Batch 9480, train_perplexity=5230.3774, train_loss=8.562239

Batch 9490, train_perplexity=6520.767, train_loss=8.782747

Batch 9500, train_perplexity=5680.857, train_loss=8.644857

Batch 9510, train_perplexity=4769.062, train_loss=8.469905

Batch 9520, train_perplexity=5876.517, train_loss=8.6787195

Batch 9530, train_perplexity=6050.725, train_loss=8.707933

Batch 9540, train_perplexity=5421.022, train_loss=8.59804

Batch 9550, train_perplexity=5392.2866, train_loss=8.592725

Batch 9560, train_perplexity=5157.013, train_loss=8.548113

Batch 9570, train_perplexity=4768.8345, train_loss=8.469857

Batch 9580, train_perplexity=5185.4, train_loss=8.553602

Batch 9590, train_perplexity=6909.328, train_loss=8.840628

Batch 9600, train_perplexity=5677.369, train_loss=8.644243

Batch 9610, train_perplexity=6731.8145, train_loss=8.8146

Batch 9620, train_perplexity=5570.1187, train_loss=8.625172

Batch 9630, train_perplexity=5276.4697, train_loss=8.5710125

Batch 9640, train_perplexity=6532.8423, train_loss=8.784597

Batch 9650, train_perplexity=5456.9263, train_loss=8.604641

Batch 9660, train_perplexity=4741.8467, train_loss=8.464182

Batch 9670, train_perplexity=5359.991, train_loss=8.586718

Batch 9680, train_perplexity=5692.5444, train_loss=8.646913

Batch 9690, train_perplexity=5993.8325, train_loss=8.698486

Batch 9700, train_perplexity=5825.9385, train_loss=8.670075

Batch 9710, train_perplexity=5218.196, train_loss=8.559907

Batch 9720, train_perplexity=6954.7715, train_loss=8.847183

Batch 9730, train_perplexity=4327.6494, train_loss=8.37278

Batch 9740, train_perplexity=5284.809, train_loss=8.572592

Batch 9750, train_perplexity=5755.336, train_loss=8.657883

Batch 9760, train_perplexity=5448.9595, train_loss=8.60318

Batch 9770, train_perplexity=4914.4126, train_loss=8.4999275

Batch 9780, train_perplexity=5496.789, train_loss=8.611919

Batch 9790, train_perplexity=5176.4863, train_loss=8.551882

Batch 9800, train_perplexity=4836.5054, train_loss=8.483948

Batch 9810, train_perplexity=5599.551, train_loss=8.630442

Batch 9820, train_perplexity=5225.7256, train_loss=8.561349

Batch 9830, train_perplexity=5595.1523, train_loss=8.629656

Batch 9840, train_perplexity=5800.6416, train_loss=8.665724

Batch 9850, train_perplexity=4134.161, train_loss=8.32704

Batch 9860, train_perplexity=6717.4746, train_loss=8.812468

Batch 9870, train_perplexity=5866.3706, train_loss=8.676991

Batch 9880, train_perplexity=6761.5015, train_loss=8.819

Batch 9890, train_perplexity=5262.7046, train_loss=8.5684

Batch 9900, train_perplexity=5399.239, train_loss=8.594013

Batch 9910, train_perplexity=5347.773, train_loss=8.584435

Batch 9920, train_perplexity=5404.6323, train_loss=8.595012

Batch 9930, train_perplexity=5316.0635, train_loss=8.578488

Batch 9940, train_perplexity=5815.608, train_loss=8.668301

Batch 9950, train_perplexity=5381.914, train_loss=8.590799
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9960, train_perplexity=5166.228, train_loss=8.549898

Batch 9970, train_perplexity=4881.715, train_loss=8.493252

Batch 9980, train_perplexity=5035.284, train_loss=8.524225

Batch 9990, train_perplexity=5906.999, train_loss=8.683893

Batch 10000, train_perplexity=6049.583, train_loss=8.707745

Batch 10010, train_perplexity=5134.6646, train_loss=8.54377

Batch 10020, train_perplexity=5941.893, train_loss=8.689783

Batch 10030, train_perplexity=4535.247, train_loss=8.419635

Batch 10040, train_perplexity=5298.6167, train_loss=8.575201

Batch 10050, train_perplexity=5700.166, train_loss=8.648251

Batch 10060, train_perplexity=6857.776, train_loss=8.833138

Batch 10070, train_perplexity=5386.6533, train_loss=8.59168

Batch 10080, train_perplexity=5671.33, train_loss=8.643179

Batch 10090, train_perplexity=5515.8354, train_loss=8.615378

Batch 10100, train_perplexity=4843.503, train_loss=8.485394

Batch 10110, train_perplexity=5080.0737, train_loss=8.533081

Batch 10120, train_perplexity=5662.7207, train_loss=8.64166

Batch 10130, train_perplexity=6722.1655, train_loss=8.813166

Batch 10140, train_perplexity=5463.3833, train_loss=8.6058235

Batch 10150, train_perplexity=5043.6323, train_loss=8.525882

Batch 10160, train_perplexity=5455.7554, train_loss=8.604426

Batch 10170, train_perplexity=6166.1123, train_loss=8.726824

Batch 10180, train_perplexity=5516.1245, train_loss=8.615431

Batch 10190, train_perplexity=5012.383, train_loss=8.519667

Batch 10200, train_perplexity=4519.5654, train_loss=8.416171

Batch 10210, train_perplexity=5024.4097, train_loss=8.522063

Batch 10220, train_perplexity=4822.389, train_loss=8.481025

Batch 10230, train_perplexity=6190.198, train_loss=8.730722

Batch 10240, train_perplexity=5201.407, train_loss=8.5566845

Batch 10250, train_perplexity=4560.981, train_loss=8.425293

Batch 10260, train_perplexity=5647.049, train_loss=8.638888

Batch 10270, train_perplexity=5170.3145, train_loss=8.550689

Batch 10280, train_perplexity=4943.5, train_loss=8.505829

Batch 10290, train_perplexity=5283.9727, train_loss=8.572433

Batch 10300, train_perplexity=5055.223, train_loss=8.528177

Batch 10310, train_perplexity=5245.508, train_loss=8.565127

Batch 10320, train_perplexity=5416.278, train_loss=8.597164

Batch 10330, train_perplexity=5643.038, train_loss=8.638178

Batch 10340, train_perplexity=5773.1797, train_loss=8.660978

Batch 10350, train_perplexity=5443.423, train_loss=8.602163

Batch 10360, train_perplexity=4851.2925, train_loss=8.487

Batch 10370, train_perplexity=6225.483, train_loss=8.736406

Batch 10380, train_perplexity=4449.339, train_loss=8.400511

Batch 10390, train_perplexity=6432.4434, train_loss=8.76911

Batch 10400, train_perplexity=5225.2373, train_loss=8.561255

Batch 10410, train_perplexity=5725.7075, train_loss=8.652721

Batch 10420, train_perplexity=5861.701, train_loss=8.676195

Batch 10430, train_perplexity=5233.6504, train_loss=8.562864

Batch 10440, train_perplexity=5972.538, train_loss=8.694927

Batch 10450, train_perplexity=4607.374, train_loss=8.435413

Batch 10460, train_perplexity=5199.8203, train_loss=8.556379

Batch 10470, train_perplexity=5387.5215, train_loss=8.591841

Batch 10480, train_perplexity=5964.762, train_loss=8.6936245

Batch 10490, train_perplexity=6173.85, train_loss=8.728078

Batch 10500, train_perplexity=5635.8745, train_loss=8.636908

Batch 10510, train_perplexity=4881.179, train_loss=8.493142

Batch 10520, train_perplexity=4347.8037, train_loss=8.377426

Batch 10530, train_perplexity=4848.6978, train_loss=8.486465

Batch 10540, train_perplexity=7088.0605, train_loss=8.866167

Batch 10550, train_perplexity=6818.394, train_loss=8.827379

Batch 10560, train_perplexity=5731.5586, train_loss=8.653743

Batch 10570, train_perplexity=5686.256, train_loss=8.645807

Batch 10580, train_perplexity=5849.7896, train_loss=8.674161

Batch 10590, train_perplexity=5524.09, train_loss=8.616874

Batch 10600, train_perplexity=5385.205, train_loss=8.591411

Batch 10610, train_perplexity=5713.7563, train_loss=8.650632

Batch 10620, train_perplexity=6142.3306, train_loss=8.7229595

Batch 10630, train_perplexity=4868.0186, train_loss=8.490442

Batch 10640, train_perplexity=4629.211, train_loss=8.440142

Batch 10650, train_perplexity=4707.086, train_loss=8.456824

Batch 10660, train_perplexity=5258.3955, train_loss=8.567581

Batch 10670, train_perplexity=4242.29, train_loss=8.352859

Batch 10680, train_perplexity=4876.787, train_loss=8.492242

Batch 10690, train_perplexity=5509.779, train_loss=8.61428

Batch 10700, train_perplexity=6196.743, train_loss=8.731779

Batch 10710, train_perplexity=5734.254, train_loss=8.654213

Batch 10720, train_perplexity=5878.2266, train_loss=8.67901

Batch 10730, train_perplexity=4377.0093, train_loss=8.384121

Batch 10740, train_perplexity=5470.4946, train_loss=8.607124

Batch 10750, train_perplexity=5111.926, train_loss=8.539331

Batch 10760, train_perplexity=5788.473, train_loss=8.663624

Batch 10770, train_perplexity=4315.1543, train_loss=8.369888

Batch 10780, train_perplexity=5393.526, train_loss=8.592955

Batch 10790, train_perplexity=6985.961, train_loss=8.851658

Batch 10800, train_perplexity=6580.383, train_loss=8.791848

Batch 10810, train_perplexity=5035.899, train_loss=8.524347

Batch 10820, train_perplexity=5575.5347, train_loss=8.626143

Batch 10830, train_perplexity=4546.9185, train_loss=8.422205

Batch 10840, train_perplexity=6003.5693, train_loss=8.7001095

Batch 10850, train_perplexity=5421.4614, train_loss=8.598121

Batch 10860, train_perplexity=5719.104, train_loss=8.651567

Batch 10870, train_perplexity=5187.101, train_loss=8.55393

Batch 10880, train_perplexity=5127.051, train_loss=8.542286

Batch 10890, train_perplexity=4957.9854, train_loss=8.508755

Batch 10900, train_perplexity=5668.713, train_loss=8.642717

Batch 10910, train_perplexity=5220.5747, train_loss=8.560363

Batch 10920, train_perplexity=4833.4116, train_loss=8.483308

Batch 10930, train_perplexity=5973.2554, train_loss=8.695047

Batch 10940, train_perplexity=6425.3496, train_loss=8.768006

Batch 10950, train_perplexity=5116.0127, train_loss=8.540131

Batch 10960, train_perplexity=5229.23, train_loss=8.562019

Batch 10970, train_perplexity=5139.495, train_loss=8.54471

Batch 10980, train_perplexity=5527.536, train_loss=8.617497

Batch 10990, train_perplexity=4831.1533, train_loss=8.482841

Batch 11000, train_perplexity=5876.7915, train_loss=8.678766

Batch 11010, train_perplexity=5586.3867, train_loss=8.628088

Batch 11020, train_perplexity=4949.576, train_loss=8.507057

Batch 11030, train_perplexity=5890.0444, train_loss=8.681019

Batch 11040, train_perplexity=4217.172, train_loss=8.34692

Batch 11050, train_perplexity=5720.686, train_loss=8.651844

Batch 11060, train_perplexity=6224.996, train_loss=8.736328

Batch 11070, train_perplexity=4760.687, train_loss=8.468147

Batch 11080, train_perplexity=6842.398, train_loss=8.8308935

Batch 11090, train_perplexity=4244.3457, train_loss=8.353343

Batch 11100, train_perplexity=5079.749, train_loss=8.533017

Batch 11110, train_perplexity=6015.054, train_loss=8.702021

Batch 11120, train_perplexity=5042.011, train_loss=8.52556

Batch 11130, train_perplexity=5318.107, train_loss=8.578873

Batch 11140, train_perplexity=5416.3555, train_loss=8.597178

Batch 11150, train_perplexity=6530.1143, train_loss=8.78418

Batch 11160, train_perplexity=5147.3335, train_loss=8.546234

Batch 11170, train_perplexity=6152.7427, train_loss=8.724653

Batch 11180, train_perplexity=5021.167, train_loss=8.521418

Batch 11190, train_perplexity=4991.4844, train_loss=8.515489

Batch 11200, train_perplexity=6693.6343, train_loss=8.808912

Batch 11210, train_perplexity=5816.7505, train_loss=8.668497

Batch 11220, train_perplexity=4852.7227, train_loss=8.487295

Batch 11230, train_perplexity=6561.8833, train_loss=8.789033

Batch 11240, train_perplexity=6121.6934, train_loss=8.719594

Batch 11250, train_perplexity=4894.7583, train_loss=8.49592

Batch 11260, train_perplexity=5095.2314, train_loss=8.53606

Batch 11270, train_perplexity=6157.897, train_loss=8.725491

Batch 11280, train_perplexity=4499.224, train_loss=8.41166

Batch 11290, train_perplexity=4561.246, train_loss=8.425351
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11300, train_perplexity=4960.1323, train_loss=8.509188

Batch 11310, train_perplexity=5727.8374, train_loss=8.653093

Batch 11320, train_perplexity=4305.067, train_loss=8.367548

Batch 11330, train_perplexity=5474.947, train_loss=8.607938

Batch 11340, train_perplexity=4962.361, train_loss=8.509637

Batch 11350, train_perplexity=5723.5894, train_loss=8.652351

Batch 11360, train_perplexity=5403.8696, train_loss=8.594871

Batch 11370, train_perplexity=4954.095, train_loss=8.50797

Batch 11380, train_perplexity=5914.406, train_loss=8.685146

Batch 11390, train_perplexity=4596.863, train_loss=8.433129

Batch 11400, train_perplexity=5896.0186, train_loss=8.682033

Batch 11410, train_perplexity=5260.8228, train_loss=8.568043

Batch 11420, train_perplexity=6218.8076, train_loss=8.735333

Batch 11430, train_perplexity=5249.4463, train_loss=8.565878

Batch 11440, train_perplexity=4356.358, train_loss=8.379392

Batch 11450, train_perplexity=5475.3594, train_loss=8.608013

Batch 11460, train_perplexity=5334.5547, train_loss=8.581961

Batch 11470, train_perplexity=4995.904, train_loss=8.516374

Batch 11480, train_perplexity=5199.726, train_loss=8.556361

Batch 11490, train_perplexity=7108.4434, train_loss=8.869039

Batch 11500, train_perplexity=5193.4272, train_loss=8.555149

Batch 11510, train_perplexity=5357.533, train_loss=8.586259

Batch 11520, train_perplexity=4970.588, train_loss=8.511293

Batch 11530, train_perplexity=7129.4424, train_loss=8.871988

Batch 11540, train_perplexity=5154.766, train_loss=8.547677

Batch 11550, train_perplexity=5176.0024, train_loss=8.551788

Batch 11560, train_perplexity=5534.599, train_loss=8.618774

Batch 11570, train_perplexity=4893.8154, train_loss=8.495728

Batch 11580, train_perplexity=6319.8545, train_loss=8.7514515

Batch 11590, train_perplexity=5048.993, train_loss=8.526944

Batch 11600, train_perplexity=6048.4062, train_loss=8.70755

Batch 11610, train_perplexity=5168.8203, train_loss=8.5504

Batch 11620, train_perplexity=5137.2217, train_loss=8.544268

Batch 11630, train_perplexity=4979.0044, train_loss=8.512985

Batch 11640, train_perplexity=4903.6357, train_loss=8.497732

Batch 11650, train_perplexity=5422.625, train_loss=8.598335

Batch 11660, train_perplexity=4998.549, train_loss=8.516903

Batch 11670, train_perplexity=5093.798, train_loss=8.535779

Batch 11680, train_perplexity=4503.8647, train_loss=8.412691

Batch 11690, train_perplexity=5180.571, train_loss=8.5526705

Batch 11700, train_perplexity=6038.1587, train_loss=8.705854

Batch 11710, train_perplexity=4467.5283, train_loss=8.404591

Batch 11720, train_perplexity=5860.449, train_loss=8.6759815

Batch 11730, train_perplexity=6367.3994, train_loss=8.758946

Batch 11740, train_perplexity=4743.154, train_loss=8.4644575

Batch 11750, train_perplexity=5403.8853, train_loss=8.594873

Batch 11760, train_perplexity=5539.457, train_loss=8.619652

Batch 11770, train_perplexity=6706.235, train_loss=8.810793

Batch 11780, train_perplexity=5421.094, train_loss=8.598053

Batch 11790, train_perplexity=5707.172, train_loss=8.649479

Batch 11800, train_perplexity=5854.0254, train_loss=8.674885

Batch 11810, train_perplexity=5537.1753, train_loss=8.61924

Batch 11820, train_perplexity=5780.992, train_loss=8.662331

Batch 11830, train_perplexity=4910.7305, train_loss=8.499178

Batch 11840, train_perplexity=5840.5195, train_loss=8.672575

Batch 11850, train_perplexity=5999.597, train_loss=8.699448

Batch 11860, train_perplexity=5702.417, train_loss=8.648645

Batch 11870, train_perplexity=6506.3057, train_loss=8.780527

Batch 11880, train_perplexity=5138.1523, train_loss=8.544449

Batch 11890, train_perplexity=4547.8374, train_loss=8.422407

Batch 11900, train_perplexity=4824.298, train_loss=8.4814205

Batch 11910, train_perplexity=4737.3174, train_loss=8.463226

Batch 11920, train_perplexity=6606.0054, train_loss=8.795734

Batch 11930, train_perplexity=4902.9014, train_loss=8.497582

Batch 11940, train_perplexity=5349.798, train_loss=8.584814

Batch 11950, train_perplexity=4869.4023, train_loss=8.490726

Batch 11960, train_perplexity=5617.244, train_loss=8.633596

Batch 11970, train_perplexity=5404.318, train_loss=8.594954

Batch 11980, train_perplexity=5850.4644, train_loss=8.674276

Batch 11990, train_perplexity=5540.54, train_loss=8.619847

Batch 12000, train_perplexity=6293.763, train_loss=8.747314

Batch 12010, train_perplexity=4709.47, train_loss=8.457331

Batch 12020, train_perplexity=4855.1807, train_loss=8.487802

Batch 12030, train_perplexity=5954.4185, train_loss=8.691889

Batch 12040, train_perplexity=6644.319, train_loss=8.8015175

Batch 12050, train_perplexity=4934.461, train_loss=8.503999

Batch 12060, train_perplexity=8529.05, train_loss=9.051233

Batch 12070, train_perplexity=5216.519, train_loss=8.559586

Batch 12080, train_perplexity=5844.0405, train_loss=8.673178

Batch 12090, train_perplexity=5627.378, train_loss=8.635399

Batch 12100, train_perplexity=5602.061, train_loss=8.63089

Batch 12110, train_perplexity=5988.1704, train_loss=8.697541

Batch 12120, train_perplexity=4983.508, train_loss=8.513889

Batch 12130, train_perplexity=6718.1025, train_loss=8.812561

Batch 12140, train_perplexity=5884.98, train_loss=8.680159

Batch 12150, train_perplexity=5711.0215, train_loss=8.650153

Batch 12160, train_perplexity=5259.0625, train_loss=8.567708

Batch 12170, train_perplexity=6579.153, train_loss=8.791661

Batch 12180, train_perplexity=5356.7515, train_loss=8.586113

Batch 12190, train_perplexity=5210.7812, train_loss=8.558485

Batch 12200, train_perplexity=5519.187, train_loss=8.615986

Batch 12210, train_perplexity=4727.4834, train_loss=8.461148

Batch 12220, train_perplexity=5364.9517, train_loss=8.587643

Batch 12230, train_perplexity=5204.851, train_loss=8.557346

Batch 12240, train_perplexity=6069.5894, train_loss=8.711046

Batch 12250, train_perplexity=4643.74, train_loss=8.443275

Batch 12260, train_perplexity=5096.4463, train_loss=8.536299

Batch 12270, train_perplexity=5086.9, train_loss=8.534424

Batch 12280, train_perplexity=4621.1387, train_loss=8.438396

Batch 12290, train_perplexity=5414.491, train_loss=8.596834

Batch 12300, train_perplexity=6675.7144, train_loss=8.8062315

Batch 12310, train_perplexity=5242.8726, train_loss=8.564625

Batch 12320, train_perplexity=5257.5327, train_loss=8.567417

Batch 12330, train_perplexity=5016.792, train_loss=8.520546

Batch 12340, train_perplexity=5333.0645, train_loss=8.581681

Batch 12350, train_perplexity=6135.387, train_loss=8.721828

Batch 12360, train_perplexity=4903.037, train_loss=8.49761

Batch 12370, train_perplexity=5363.7036, train_loss=8.58741

Batch 12380, train_perplexity=6287.0684, train_loss=8.74625

Batch 12390, train_perplexity=6418.215, train_loss=8.766895

Batch 12400, train_perplexity=5756.0, train_loss=8.657998

Batch 12410, train_perplexity=5761.514, train_loss=8.658956

Batch 12420, train_perplexity=5699.682, train_loss=8.648166

Batch 12430, train_perplexity=4855.3516, train_loss=8.487837

Batch 12440, train_perplexity=5895.164, train_loss=8.681888

Batch 12450, train_perplexity=6060.439, train_loss=8.7095375

Batch 12460, train_perplexity=5215.8125, train_loss=8.55945

Batch 12470, train_perplexity=5136.007, train_loss=8.544031

Batch 12480, train_perplexity=5386.0625, train_loss=8.59157

Batch 12490, train_perplexity=6140.48, train_loss=8.722658

Batch 12500, train_perplexity=5584.3735, train_loss=8.6277275

Batch 12510, train_perplexity=5979.2456, train_loss=8.69605

Batch 12520, train_perplexity=5850.4756, train_loss=8.674278

Batch 12530, train_perplexity=4829.9604, train_loss=8.482594

Batch 12540, train_perplexity=5196.8853, train_loss=8.555815

Batch 12550, train_perplexity=5999.3857, train_loss=8.699412

Batch 12560, train_perplexity=5156.8657, train_loss=8.548084

Batch 12570, train_perplexity=6109.731, train_loss=8.717638

Batch 12580, train_perplexity=6247.9956, train_loss=8.740016

Batch 12590, train_perplexity=4809.3228, train_loss=8.478312

Batch 12600, train_perplexity=5440.869, train_loss=8.601694

Batch 12610, train_perplexity=5409.196, train_loss=8.595856

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305432 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100
Loaded 305432 sentences.
Finished loading
Batch 12620, train_perplexity=4992.156, train_loss=8.515623

Batch 12630, train_perplexity=5781.5327, train_loss=8.662424

Batch 12640, train_perplexity=5253.2324, train_loss=8.566599

Batch 12650, train_perplexity=5329.978, train_loss=8.581102

Batch 12660, train_perplexity=5710.395, train_loss=8.6500435

Batch 12670, train_perplexity=4785.121, train_loss=8.473267

Batch 12680, train_perplexity=5043.8677, train_loss=8.5259285

Batch 12690, train_perplexity=4998.9253, train_loss=8.516978

Batch 12700, train_perplexity=5172.8936, train_loss=8.5511875

Batch 12710, train_perplexity=7150.1416, train_loss=8.874887

Batch 12720, train_perplexity=6274.0767, train_loss=8.744182

Batch 12730, train_perplexity=6180.0527, train_loss=8.729082

Batch 12740, train_perplexity=4858.5757, train_loss=8.488501

Batch 12750, train_perplexity=4881.1187, train_loss=8.49313

Batch 12760, train_perplexity=5649.02, train_loss=8.639237

Batch 12770, train_perplexity=5212.292, train_loss=8.558775

Batch 12780, train_perplexity=5353.044, train_loss=8.585421

Batch 12790, train_perplexity=5632.865, train_loss=8.6363735

Batch 12800, train_perplexity=5801.1895, train_loss=8.665818

Batch 12810, train_perplexity=5426.531, train_loss=8.599055

Batch 12820, train_perplexity=6443.47, train_loss=8.770823

Batch 12830, train_perplexity=4869.941, train_loss=8.490837

Batch 12840, train_perplexity=4890.5215, train_loss=8.495054

Batch 12850, train_perplexity=5566.7837, train_loss=8.624573

Batch 12860, train_perplexity=5202.4985, train_loss=8.556894

Batch 12870, train_perplexity=5104.6475, train_loss=8.537907

Batch 12880, train_perplexity=6066.875, train_loss=8.710599

Batch 12890, train_perplexity=4672.5537, train_loss=8.449461

Batch 12900, train_perplexity=5222.3677, train_loss=8.560706

Batch 12910, train_perplexity=4843.7617, train_loss=8.485447

Batch 12920, train_perplexity=5512.6694, train_loss=8.614804

Batch 12930, train_perplexity=5287.7837, train_loss=8.573154

Batch 12940, train_perplexity=4876.3125, train_loss=8.492145

Batch 12950, train_perplexity=4919.1953, train_loss=8.5009

Batch 12960, train_perplexity=5273.5015, train_loss=8.57045

Batch 12970, train_perplexity=5911.6655, train_loss=8.684683

Batch 12980, train_perplexity=5306.0146, train_loss=8.576596

Batch 12990, train_perplexity=5771.6167, train_loss=8.660707

Batch 13000, train_perplexity=5757.477, train_loss=8.658255

Batch 13010, train_perplexity=4963.2646, train_loss=8.509819

Batch 13020, train_perplexity=4811.5156, train_loss=8.478767

Batch 13030, train_perplexity=5431.175, train_loss=8.599911

Batch 13040, train_perplexity=6461.845, train_loss=8.77367

Batch 13050, train_perplexity=5529.471, train_loss=8.617847

Batch 13060, train_perplexity=5454.2314, train_loss=8.604147

Batch 13070, train_perplexity=5088.054, train_loss=8.534651

Batch 13080, train_perplexity=6128.4224, train_loss=8.720693

Batch 13090, train_perplexity=5541.444, train_loss=8.62001

Batch 13100, train_perplexity=5414.8477, train_loss=8.5969

Batch 13110, train_perplexity=5597.8477, train_loss=8.630137

Batch 13120, train_perplexity=5879.7964, train_loss=8.679277

Batch 13130, train_perplexity=5152.8643, train_loss=8.547308

Batch 13140, train_perplexity=6893.361, train_loss=8.838314

Batch 13150, train_perplexity=4810.5796, train_loss=8.478573

Batch 13160, train_perplexity=5516.356, train_loss=8.615473

Batch 13170, train_perplexity=5223.11, train_loss=8.560848

Batch 13180, train_perplexity=5584.427, train_loss=8.627737

Batch 13190, train_perplexity=4667.721, train_loss=8.448426

Batch 13200, train_perplexity=5078.611, train_loss=8.532793

Batch 13210, train_perplexity=5447.016, train_loss=8.602823

Batch 13220, train_perplexity=4883.5957, train_loss=8.493637

Batch 13230, train_perplexity=4678.805, train_loss=8.450798

Batch 13240, train_perplexity=6446.1255, train_loss=8.7712345

Batch 13250, train_perplexity=5067.59, train_loss=8.530621

Batch 13260, train_perplexity=5679.373, train_loss=8.644596

Batch 13270, train_perplexity=4670.8027, train_loss=8.449086

Batch 13280, train_perplexity=4823.608, train_loss=8.481277

Batch 13290, train_perplexity=6459.8364, train_loss=8.773359

Batch 13300, train_perplexity=5345.8506, train_loss=8.584076

Batch 13310, train_perplexity=5542.559, train_loss=8.620212

Batch 13320, train_perplexity=5354.7183, train_loss=8.585733

Batch 13330, train_perplexity=5165.529, train_loss=8.549763

Batch 13340, train_perplexity=4771.8145, train_loss=8.470482

Batch 13350, train_perplexity=4999.2544, train_loss=8.517044

Batch 13360, train_perplexity=4480.055, train_loss=8.407391

Batch 13370, train_perplexity=5394.9717, train_loss=8.593223

Batch 13380, train_perplexity=5388.323, train_loss=8.5919895

Batch 13390, train_perplexity=5210.006, train_loss=8.558336

Batch 13400, train_perplexity=5161.363, train_loss=8.548956

Batch 13410, train_perplexity=5149.9653, train_loss=8.546745

Batch 13420, train_perplexity=5201.68, train_loss=8.556737

Batch 13430, train_perplexity=6555.985, train_loss=8.788134

Batch 13440, train_perplexity=5943.5596, train_loss=8.690063

Batch 13450, train_perplexity=5549.197, train_loss=8.621408

Batch 13460, train_perplexity=5433.3193, train_loss=8.600306

Batch 13470, train_perplexity=5816.418, train_loss=8.66844

Batch 13480, train_perplexity=4332.5474, train_loss=8.373911

Batch 13490, train_perplexity=5186.5967, train_loss=8.553833

Batch 13500, train_perplexity=4516.183, train_loss=8.415422

Batch 13510, train_perplexity=6358.51, train_loss=8.757549

Batch 13520, train_perplexity=6296.999, train_loss=8.7478285

Batch 13530, train_perplexity=5652.9653, train_loss=8.6399355

Batch 13540, train_perplexity=5074.1567, train_loss=8.531916

Batch 13550, train_perplexity=5407.9116, train_loss=8.595618

Batch 13560, train_perplexity=5258.054, train_loss=8.567516

Batch 13570, train_perplexity=6411.406, train_loss=8.765834

Batch 13580, train_perplexity=5917.035, train_loss=8.685591

Batch 13590, train_perplexity=5909.546, train_loss=8.684324

Batch 13600, train_perplexity=5375.005, train_loss=8.589515

Batch 13610, train_perplexity=4845.675, train_loss=8.485842

Batch 13620, train_perplexity=5970.374, train_loss=8.694565

Batch 13630, train_perplexity=6802.183, train_loss=8.824999

Batch 13640, train_perplexity=5266.8267, train_loss=8.569183

Batch 13650, train_perplexity=6044.508, train_loss=8.706905

Batch 13660, train_perplexity=5239.9834, train_loss=8.564074

Batch 13670, train_perplexity=4957.1343, train_loss=8.508583

Batch 13680, train_perplexity=5920.337, train_loss=8.686149

Batch 13690, train_perplexity=5577.8267, train_loss=8.6265545

Batch 13700, train_perplexity=6682.262, train_loss=8.807212

Batch 13710, train_perplexity=6038.8843, train_loss=8.705975

Batch 13720, train_perplexity=4713.0376, train_loss=8.458088

Batch 13730, train_perplexity=5010.003, train_loss=8.519192

Batch 13740, train_perplexity=5079.3857, train_loss=8.532946

Batch 13750, train_perplexity=6635.669, train_loss=8.800215

Batch 13760, train_perplexity=5466.7188, train_loss=8.606434

Batch 13770, train_perplexity=4936.424, train_loss=8.504396

Batch 13780, train_perplexity=5093.4434, train_loss=8.535709

Batch 13790, train_perplexity=5138.231, train_loss=8.544464

Batch 13800, train_perplexity=4800.543, train_loss=8.476484

Batch 13810, train_perplexity=4421.506, train_loss=8.394236

Batch 13820, train_perplexity=6499.354, train_loss=8.779458

Batch 13830, train_perplexity=4879.308, train_loss=8.492759

Batch 13840, train_perplexity=5389.094, train_loss=8.592133

Batch 13850, train_perplexity=5571.3833, train_loss=8.625399

Batch 13860, train_perplexity=4924.3115, train_loss=8.50194

Batch 13870, train_perplexity=5006.655, train_loss=8.518523

Batch 13880, train_perplexity=5727.5205, train_loss=8.653038

Batch 13890, train_perplexity=5019.893, train_loss=8.521164

Batch 13900, train_perplexity=5500.156, train_loss=8.612532

Batch 13910, train_perplexity=7808.535, train_loss=8.962973
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13920, train_perplexity=5293.616, train_loss=8.574257

Batch 13930, train_perplexity=5540.831, train_loss=8.6199

Batch 13940, train_perplexity=5646.688, train_loss=8.638824

Batch 13950, train_perplexity=4564.4355, train_loss=8.42605

Batch 13960, train_perplexity=5680.391, train_loss=8.644775

Batch 13970, train_perplexity=4806.365, train_loss=8.477696

Batch 13980, train_perplexity=5392.919, train_loss=8.592842

Batch 13990, train_perplexity=5684.911, train_loss=8.645571

Batch 14000, train_perplexity=5168.446, train_loss=8.550327

Batch 14010, train_perplexity=5789.064, train_loss=8.663726

Batch 14020, train_perplexity=4880.3647, train_loss=8.492975

Batch 14030, train_perplexity=4941.115, train_loss=8.505346

Batch 14040, train_perplexity=4730.09, train_loss=8.4616995

Batch 14050, train_perplexity=4525.0815, train_loss=8.417391

Batch 14060, train_perplexity=6280.4043, train_loss=8.74519

Batch 14070, train_perplexity=4589.149, train_loss=8.43145

Batch 14080, train_perplexity=5954.6343, train_loss=8.691925

Batch 14090, train_perplexity=4728.917, train_loss=8.461452

Batch 14100, train_perplexity=7040.3833, train_loss=8.859418

Batch 14110, train_perplexity=5569.954, train_loss=8.625142

Batch 14120, train_perplexity=5022.6035, train_loss=8.521704

Batch 14130, train_perplexity=5137.055, train_loss=8.544235

Batch 14140, train_perplexity=5200.306, train_loss=8.556473

Batch 14150, train_perplexity=5616.7188, train_loss=8.633503

Batch 14160, train_perplexity=5595.1577, train_loss=8.629657

Batch 14170, train_perplexity=4889.9897, train_loss=8.494946

Batch 14180, train_perplexity=5666.3022, train_loss=8.642292

Batch 14190, train_perplexity=4432.3643, train_loss=8.396688

Batch 14200, train_perplexity=5069.8135, train_loss=8.531059

Batch 14210, train_perplexity=5965.525, train_loss=8.693752

Batch 14220, train_perplexity=5415.369, train_loss=8.596996

Batch 14230, train_perplexity=5974.959, train_loss=8.695333

Batch 14240, train_perplexity=4856.1436, train_loss=8.488

Batch 14250, train_perplexity=4992.218, train_loss=8.5156355

Batch 14260, train_perplexity=4893.8154, train_loss=8.495728

Batch 14270, train_perplexity=5316.9404, train_loss=8.578653

Batch 14280, train_perplexity=4768.4614, train_loss=8.469779

Batch 14290, train_perplexity=5338.55, train_loss=8.582709

Batch 14300, train_perplexity=5586.5625, train_loss=8.628119

Batch 14310, train_perplexity=5227.9336, train_loss=8.561771

Batch 14320, train_perplexity=5836.544, train_loss=8.671894

Batch 14330, train_perplexity=4801.367, train_loss=8.476656

Batch 14340, train_perplexity=4997.1475, train_loss=8.516623

Batch 14350, train_perplexity=5705.327, train_loss=8.649156

Batch 14360, train_perplexity=5914.361, train_loss=8.685139

Batch 14370, train_perplexity=5861.7065, train_loss=8.676196

Batch 14380, train_perplexity=6677.873, train_loss=8.806555

Batch 14390, train_perplexity=5274.8345, train_loss=8.570703

Batch 14400, train_perplexity=6562.8784, train_loss=8.789185

Batch 14410, train_perplexity=5059.3613, train_loss=8.5289955

Batch 14420, train_perplexity=4370.0605, train_loss=8.382532

Batch 14430, train_perplexity=6323.026, train_loss=8.751953

Batch 14440, train_perplexity=5614.2393, train_loss=8.633061

Batch 14450, train_perplexity=5259.659, train_loss=8.5678215

Batch 14460, train_perplexity=4877.145, train_loss=8.492315

Batch 14470, train_perplexity=4764.6978, train_loss=8.468989

Batch 14480, train_perplexity=5124.079, train_loss=8.541706

Batch 14490, train_perplexity=6739.0537, train_loss=8.815675

Batch 14500, train_perplexity=4767.5205, train_loss=8.469582

Batch 14510, train_perplexity=5634.7617, train_loss=8.63671

Batch 14520, train_perplexity=5101.08, train_loss=8.537208

Batch 14530, train_perplexity=4838.914, train_loss=8.484446

Batch 14540, train_perplexity=5068.2275, train_loss=8.530746

Batch 14550, train_perplexity=5407.927, train_loss=8.595621

Batch 14560, train_perplexity=5909.005, train_loss=8.684233

Batch 14570, train_perplexity=5313.96, train_loss=8.578093

Batch 14580, train_perplexity=4151.7944, train_loss=8.331296

Batch 14590, train_perplexity=5408.6543, train_loss=8.595756

Batch 14600, train_perplexity=4010.6443, train_loss=8.296707

Batch 14610, train_perplexity=5314.2236, train_loss=8.578142

Batch 14620, train_perplexity=5457.707, train_loss=8.604784

Batch 14630, train_perplexity=4295.8145, train_loss=8.3653965

Batch 14640, train_perplexity=5620.432, train_loss=8.634164

Batch 14650, train_perplexity=4817.209, train_loss=8.47995

Batch 14660, train_perplexity=4625.6143, train_loss=8.439364

Batch 14670, train_perplexity=5230.058, train_loss=8.562178

Batch 14680, train_perplexity=4981.0703, train_loss=8.5134

Batch 14690, train_perplexity=6076.192, train_loss=8.712133

Batch 14700, train_perplexity=4715.2363, train_loss=8.458554

Batch 14710, train_perplexity=5446.886, train_loss=8.602799

Batch 14720, train_perplexity=5473.8765, train_loss=8.607742

Batch 14730, train_perplexity=6086.6255, train_loss=8.713849

Batch 14740, train_perplexity=6082.5576, train_loss=8.713181

Batch 14750, train_perplexity=5616.8955, train_loss=8.633534

Batch 14760, train_perplexity=6192.1704, train_loss=8.731041

Batch 14770, train_perplexity=6122.3296, train_loss=8.719698

Batch 14780, train_perplexity=5008.9375, train_loss=8.518979

Batch 14790, train_perplexity=5387.4907, train_loss=8.591835

Batch 14800, train_perplexity=4516.2046, train_loss=8.415427

Batch 14810, train_perplexity=4842.247, train_loss=8.485134

Batch 14820, train_perplexity=5429.2744, train_loss=8.599561

Batch 14830, train_perplexity=6339.08, train_loss=8.754489

Batch 14840, train_perplexity=6387.379, train_loss=8.762079

Batch 14850, train_perplexity=4522.4106, train_loss=8.4168005

Batch 14860, train_perplexity=5527.1777, train_loss=8.617433

Batch 14870, train_perplexity=4949.3306, train_loss=8.507008

Batch 14880, train_perplexity=4547.8247, train_loss=8.422404

Batch 14890, train_perplexity=5562.549, train_loss=8.623812

Batch 14900, train_perplexity=4722.8823, train_loss=8.460175

Batch 14910, train_perplexity=6786.58, train_loss=8.822702

Batch 14920, train_perplexity=6639.632, train_loss=8.800812

Batch 14930, train_perplexity=5092.375, train_loss=8.5355

Batch 14940, train_perplexity=5547.107, train_loss=8.621032

Batch 14950, train_perplexity=4863.0024, train_loss=8.489411

Batch 14960, train_perplexity=5566.0566, train_loss=8.624442

Batch 14970, train_perplexity=4971.0903, train_loss=8.5113945

Batch 14980, train_perplexity=5626.589, train_loss=8.635259

Batch 14990, train_perplexity=4461.6357, train_loss=8.403271

Batch 15000, train_perplexity=5450.3315, train_loss=8.603432

Batch 15010, train_perplexity=6256.4326, train_loss=8.741365

Batch 15020, train_perplexity=5208.2275, train_loss=8.557995

Batch 15030, train_perplexity=4919.627, train_loss=8.500988

Batch 15040, train_perplexity=4459.8193, train_loss=8.4028635

Batch 15050, train_perplexity=4487.26, train_loss=8.408998

Batch 15060, train_perplexity=5461.3047, train_loss=8.605443

Batch 15070, train_perplexity=5528.5327, train_loss=8.617678

Batch 15080, train_perplexity=5408.128, train_loss=8.595658

Batch 15090, train_perplexity=6013.643, train_loss=8.701786

Batch 15100, train_perplexity=6205.3535, train_loss=8.733168

Batch 15110, train_perplexity=6378.54, train_loss=8.7606945

Batch 15120, train_perplexity=4645.858, train_loss=8.443731

Batch 15130, train_perplexity=5115.3833, train_loss=8.540008

Batch 15140, train_perplexity=5217.8423, train_loss=8.559839

Batch 15150, train_perplexity=4605.5903, train_loss=8.435026

Batch 15160, train_perplexity=6093.322, train_loss=8.714949

Batch 15170, train_perplexity=6267.5103, train_loss=8.7431345

Batch 15180, train_perplexity=4567.9194, train_loss=8.426813

Batch 15190, train_perplexity=5647.808, train_loss=8.639023

Batch 15200, train_perplexity=6105.1294, train_loss=8.716885

Batch 15210, train_perplexity=5144.0557, train_loss=8.545597

Batch 15220, train_perplexity=4735.2397, train_loss=8.462788

Batch 15230, train_perplexity=5622.861, train_loss=8.634596

Batch 15240, train_perplexity=5765.708, train_loss=8.659683

Batch 15250, train_perplexity=4554.704, train_loss=8.423916
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15260, train_perplexity=4561.7466, train_loss=8.425461

Batch 15270, train_perplexity=5782.029, train_loss=8.66251

Batch 15280, train_perplexity=5396.2476, train_loss=8.593459

Batch 15290, train_perplexity=4862.5435, train_loss=8.489317

Batch 15300, train_perplexity=5906.312, train_loss=8.683777

Batch 15310, train_perplexity=7556.9106, train_loss=8.930218

Batch 15320, train_perplexity=5524.569, train_loss=8.616961

Batch 15330, train_perplexity=4717.809, train_loss=8.4591

Batch 15340, train_perplexity=5119.888, train_loss=8.540888

Batch 15350, train_perplexity=4664.49, train_loss=8.447734

Batch 15360, train_perplexity=4520.863, train_loss=8.416458

Batch 15370, train_perplexity=5280.834, train_loss=8.571839

Batch 15380, train_perplexity=4318.4805, train_loss=8.370659

Batch 15390, train_perplexity=5317.407, train_loss=8.578741

Batch 15400, train_perplexity=5133.8765, train_loss=8.543616

Batch 15410, train_perplexity=6266.4883, train_loss=8.742971

Batch 15420, train_perplexity=6335.2183, train_loss=8.75388

Batch 15430, train_perplexity=5245.8633, train_loss=8.565195

Batch 15440, train_perplexity=8706.715, train_loss=9.07185

Batch 15450, train_perplexity=4791.0205, train_loss=8.474499

Batch 15460, train_perplexity=4961.845, train_loss=8.509533

Batch 15470, train_perplexity=5630.3354, train_loss=8.635924

Batch 15480, train_perplexity=5287.088, train_loss=8.573023

Batch 15490, train_perplexity=5503.9336, train_loss=8.613218

Batch 15500, train_perplexity=5545.372, train_loss=8.620719

Batch 15510, train_perplexity=5838.5034, train_loss=8.67223

Batch 15520, train_perplexity=4639.5176, train_loss=8.442366

Batch 15530, train_perplexity=5667.086, train_loss=8.64243

Batch 15540, train_perplexity=5069.3877, train_loss=8.530975

Batch 15550, train_perplexity=5486.3564, train_loss=8.61002

Batch 15560, train_perplexity=5296.697, train_loss=8.574839

Batch 15570, train_perplexity=5390.7593, train_loss=8.592442

Batch 15580, train_perplexity=5141.7554, train_loss=8.54515

Batch 15590, train_perplexity=6445.191, train_loss=8.77109

Batch 15600, train_perplexity=4896.201, train_loss=8.496215

Batch 15610, train_perplexity=6002.241, train_loss=8.699888

Batch 15620, train_perplexity=4558.4023, train_loss=8.424727

Batch 15630, train_perplexity=5204.5728, train_loss=8.557293

Batch 15640, train_perplexity=5407.3647, train_loss=8.595517

Batch 15650, train_perplexity=5663.914, train_loss=8.6418705

Batch 15660, train_perplexity=4929.6304, train_loss=8.503019

Batch 15670, train_perplexity=4858.84, train_loss=8.488555

Batch 15680, train_perplexity=4938.6274, train_loss=8.504843

Batch 15690, train_perplexity=4171.3647, train_loss=8.335999

Batch 15700, train_perplexity=4527.952, train_loss=8.418025

Batch 15710, train_perplexity=5626.8843, train_loss=8.635311

Batch 15720, train_perplexity=5956.1562, train_loss=8.692181

Batch 15730, train_perplexity=5518.677, train_loss=8.615893

Batch 15740, train_perplexity=6903.216, train_loss=8.839743

Batch 15750, train_perplexity=5310.606, train_loss=8.577461

Batch 15760, train_perplexity=5517.845, train_loss=8.615743

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Batch 15770, train_perplexity=4070.455, train_loss=8.31151

Batch 15780, train_perplexity=5755.4897, train_loss=8.657909

Batch 15790, train_perplexity=4749.817, train_loss=8.465861

Batch 15800, train_perplexity=4712.1387, train_loss=8.457897

Batch 15810, train_perplexity=6797.7666, train_loss=8.824349

Batch 15820, train_perplexity=7161.1763, train_loss=8.87643

Batch 15830, train_perplexity=5454.262, train_loss=8.604153

Batch 15840, train_perplexity=5286.2207, train_loss=8.572859

Batch 15850, train_perplexity=6114.7207, train_loss=8.718454

Batch 15860, train_perplexity=5784.1465, train_loss=8.662876

Batch 15870, train_perplexity=8353.306, train_loss=9.030413

Batch 15880, train_perplexity=6604.2036, train_loss=8.795462

Batch 15890, train_perplexity=4778.3174, train_loss=8.471844

Batch 15900, train_perplexity=4673.6855, train_loss=8.449703

Batch 15910, train_perplexity=5065.9854, train_loss=8.530304

Batch 15920, train_perplexity=5515.325, train_loss=8.615286

Batch 15930, train_perplexity=5556.9395, train_loss=8.622803

Batch 15940, train_perplexity=5192.9663, train_loss=8.55506

Batch 15950, train_perplexity=4571.258, train_loss=8.427544

Batch 15960, train_perplexity=5945.368, train_loss=8.690368

Batch 15970, train_perplexity=5332.164, train_loss=8.581512

Batch 15980, train_perplexity=5436.2324, train_loss=8.6008415

Batch 15990, train_perplexity=6209.604, train_loss=8.733852

Batch 16000, train_perplexity=4807.7407, train_loss=8.4779825

Batch 16010, train_perplexity=4702.747, train_loss=8.455902

Batch 16020, train_perplexity=6171.195, train_loss=8.727648

Batch 16030, train_perplexity=4275.5835, train_loss=8.360676

Batch 16040, train_perplexity=4356.329, train_loss=8.379385

Batch 16050, train_perplexity=4834.596, train_loss=8.483553

Batch 16060, train_perplexity=5114.578, train_loss=8.53985

Batch 16070, train_perplexity=4717.764, train_loss=8.45909

Batch 16080, train_perplexity=5539.0186, train_loss=8.619573

Batch 16090, train_perplexity=6519.287, train_loss=8.78252

Batch 16100, train_perplexity=4316.673, train_loss=8.37024

Batch 16110, train_perplexity=4669.707, train_loss=8.448852

Batch 16120, train_perplexity=5554.1523, train_loss=8.622301

Batch 16130, train_perplexity=4389.311, train_loss=8.386928

Batch 16140, train_perplexity=5128.4595, train_loss=8.542561

Batch 16150, train_perplexity=5052.0615, train_loss=8.527552

Batch 16160, train_perplexity=5209.618, train_loss=8.558262

Batch 16170, train_perplexity=4191.9863, train_loss=8.34093

Batch 16180, train_perplexity=4993.5225, train_loss=8.515897

Batch 16190, train_perplexity=5538.966, train_loss=8.619563

Batch 16200, train_perplexity=5439.852, train_loss=8.601507

Batch 16210, train_perplexity=5503.2725, train_loss=8.613098

Batch 16220, train_perplexity=5663.8496, train_loss=8.641859

Batch 16230, train_perplexity=4794.819, train_loss=8.475291

Batch 16240, train_perplexity=6678.8984, train_loss=8.806708

Batch 16250, train_perplexity=5397.38, train_loss=8.593669

Batch 16260, train_perplexity=5434.066, train_loss=8.600443

Batch 16270, train_perplexity=5361.3457, train_loss=8.58697

Batch 16280, train_perplexity=3814.0635, train_loss=8.24645

Batch 16290, train_perplexity=5892.382, train_loss=8.681416

Batch 16300, train_perplexity=4927.854, train_loss=8.502659

Batch 16310, train_perplexity=5045.5996, train_loss=8.526272

Batch 16320, train_perplexity=4675.825, train_loss=8.450161

Batch 16330, train_perplexity=5611.761, train_loss=8.63262

Batch 16340, train_perplexity=5083.544, train_loss=8.533764

Batch 16350, train_perplexity=5248.8257, train_loss=8.56576

Batch 16360, train_perplexity=5525.9233, train_loss=8.617206

Batch 16370, train_perplexity=5013.1284, train_loss=8.519815

Batch 16380, train_perplexity=5812.3643, train_loss=8.667743

Batch 16390, train_perplexity=5522.146, train_loss=8.616522

Batch 16400, train_perplexity=5107.871, train_loss=8.538538

Batch 16410, train_perplexity=4831.628, train_loss=8.482939

Batch 16420, train_perplexity=5554.8306, train_loss=8.622423

Batch 16430, train_perplexity=5443.0127, train_loss=8.602088

Batch 16440, train_perplexity=4267.021, train_loss=8.358671

Batch 16450, train_perplexity=5799.9004, train_loss=8.665596

Batch 16460, train_perplexity=4914.914, train_loss=8.50003

Batch 16470, train_perplexity=4961.8164, train_loss=8.509527

Batch 16480, train_perplexity=6459.6763, train_loss=8.7733345

Batch 16490, train_perplexity=4968.4175, train_loss=8.510857

Batch 16500, train_perplexity=4668.7764, train_loss=8.448652

Batch 16510, train_perplexity=5700.253, train_loss=8.648266

Batch 16520, train_perplexity=6484.359, train_loss=8.777148

Batch 16530, train_perplexity=4982.163, train_loss=8.513619
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 16540, train_perplexity=5400.7217, train_loss=8.594288

Batch 16550, train_perplexity=4714.962, train_loss=8.458496

Batch 16560, train_perplexity=6499.3354, train_loss=8.779455

Batch 16570, train_perplexity=5194.7544, train_loss=8.555405

Batch 16580, train_perplexity=5050.992, train_loss=8.52734

Batch 16590, train_perplexity=4849.4194, train_loss=8.486614

Batch 16600, train_perplexity=5197.3906, train_loss=8.555912

Batch 16610, train_perplexity=5817.6104, train_loss=8.668645

Batch 16620, train_perplexity=5512.3174, train_loss=8.61474

Batch 16630, train_perplexity=6060.277, train_loss=8.709511

Batch 16640, train_perplexity=5958.23, train_loss=8.692529

Batch 16650, train_perplexity=5012.5645, train_loss=8.519703

Batch 16660, train_perplexity=6902.5645, train_loss=8.839648

Batch 16670, train_perplexity=5597.0947, train_loss=8.630003

Batch 16680, train_perplexity=5512.312, train_loss=8.614739

Batch 16690, train_perplexity=6233.8184, train_loss=8.737744

Batch 16700, train_perplexity=5363.3916, train_loss=8.587352

Batch 16710, train_perplexity=6291.1826, train_loss=8.746904

Batch 16720, train_perplexity=5722.836, train_loss=8.65222

Batch 16730, train_perplexity=5504.3535, train_loss=8.613295

Batch 16740, train_perplexity=5676.1567, train_loss=8.64403

Batch 16750, train_perplexity=5401.005, train_loss=8.59434

Batch 16760, train_perplexity=5144.222, train_loss=8.5456295

Batch 16770, train_perplexity=5236.701, train_loss=8.563447

Batch 16780, train_perplexity=6571.9976, train_loss=8.790573

Batch 16790, train_perplexity=4833.656, train_loss=8.483358

Batch 16800, train_perplexity=5456.0625, train_loss=8.604483

Batch 16810, train_perplexity=5346.381, train_loss=8.584175

Batch 16820, train_perplexity=5040.252, train_loss=8.525211

Batch 16830, train_perplexity=5799.53, train_loss=8.665532

Batch 16840, train_perplexity=6867.613, train_loss=8.834572

Batch 16850, train_perplexity=4934.0, train_loss=8.503905

Batch 16860, train_perplexity=5770.8076, train_loss=8.660567

Batch 16870, train_perplexity=7041.458, train_loss=8.8595705

Batch 16880, train_perplexity=5583.766, train_loss=8.627619

Batch 16890, train_perplexity=5120.762, train_loss=8.541059

Batch 16900, train_perplexity=5558.8843, train_loss=8.623153

Batch 16910, train_perplexity=6281.698, train_loss=8.745396

Batch 16920, train_perplexity=6223.6665, train_loss=8.7361145

Batch 16930, train_perplexity=4714.5347, train_loss=8.4584055

Batch 16940, train_perplexity=5873.8555, train_loss=8.678267

Batch 16950, train_perplexity=4570.0635, train_loss=8.427282

Batch 16960, train_perplexity=6189.8677, train_loss=8.730669

Batch 16970, train_perplexity=5880.834, train_loss=8.679454

Batch 16980, train_perplexity=5965.081, train_loss=8.693678

Batch 16990, train_perplexity=5468.0117, train_loss=8.60667

Batch 17000, train_perplexity=5723.4365, train_loss=8.652325

Batch 17010, train_perplexity=4708.132, train_loss=8.4570465

Batch 17020, train_perplexity=5453.534, train_loss=8.604019

Batch 17030, train_perplexity=5523.231, train_loss=8.616718

Batch 17040, train_perplexity=4704.4966, train_loss=8.456274

Batch 17050, train_perplexity=4551.2393, train_loss=8.423155

Batch 17060, train_perplexity=5159.862, train_loss=8.548665

Batch 17070, train_perplexity=6244.8203, train_loss=8.739508

Batch 17080, train_perplexity=5276.872, train_loss=8.571089

Batch 17090, train_perplexity=5878.2715, train_loss=8.679018

Batch 17100, train_perplexity=5583.186, train_loss=8.627515

Batch 17110, train_perplexity=5332.352, train_loss=8.581548

Batch 17120, train_perplexity=5100.628, train_loss=8.537119

Batch 17130, train_perplexity=4738.7, train_loss=8.463518

Batch 17140, train_perplexity=5279.248, train_loss=8.571539

Batch 17150, train_perplexity=5173.718, train_loss=8.551347

Batch 17160, train_perplexity=4756.1445, train_loss=8.467193

Batch 17170, train_perplexity=5905.884, train_loss=8.683704

Batch 17180, train_perplexity=5532.24, train_loss=8.618348

Batch 17190, train_perplexity=5874.808, train_loss=8.678429

Batch 17200, train_perplexity=5418.107, train_loss=8.597502

Batch 17210, train_perplexity=5345.7026, train_loss=8.584048

Batch 17220, train_perplexity=4865.5356, train_loss=8.489932

Batch 17230, train_perplexity=4842.1455, train_loss=8.485113

Batch 17240, train_perplexity=5048.43, train_loss=8.526833

Batch 17250, train_perplexity=4331.8076, train_loss=8.37374

Batch 17260, train_perplexity=4518.492, train_loss=8.415934

Batch 17270, train_perplexity=5401.9785, train_loss=8.594521

Batch 17280, train_perplexity=5250.563, train_loss=8.566091

Batch 17290, train_perplexity=6197.056, train_loss=8.73183

Batch 17300, train_perplexity=4883.9453, train_loss=8.493709

Batch 17310, train_perplexity=5411.812, train_loss=8.596339

Batch 17320, train_perplexity=4580.9155, train_loss=8.429654

Batch 17330, train_perplexity=4574.912, train_loss=8.428343

Batch 17340, train_perplexity=4738.759, train_loss=8.463531

Batch 17350, train_perplexity=5119.517, train_loss=8.540815

Batch 17360, train_perplexity=4710.0, train_loss=8.457443

Batch 17370, train_perplexity=4888.9175, train_loss=8.494726

Batch 17380, train_perplexity=5487.864, train_loss=8.610294

Batch 17390, train_perplexity=5038.6226, train_loss=8.524888

Batch 17400, train_perplexity=5577.1245, train_loss=8.626429

Batch 17410, train_perplexity=5421.7563, train_loss=8.598175

Batch 17420, train_perplexity=4985.6943, train_loss=8.514328

Batch 17430, train_perplexity=3997.9849, train_loss=8.293546

Batch 17440, train_perplexity=5192.971, train_loss=8.555061

Batch 17450, train_perplexity=5904.921, train_loss=8.683541

Batch 17460, train_perplexity=4894.45, train_loss=8.495857

Batch 17470, train_perplexity=5730.8755, train_loss=8.653624

Batch 17480, train_perplexity=4847.417, train_loss=8.486201

Batch 17490, train_perplexity=5694.1245, train_loss=8.64719

Batch 17500, train_perplexity=5213.9575, train_loss=8.559094

Batch 17510, train_perplexity=4476.28, train_loss=8.406548

Batch 17520, train_perplexity=4655.0825, train_loss=8.445715

Batch 17530, train_perplexity=4268.34, train_loss=8.35898

Batch 17540, train_perplexity=6352.521, train_loss=8.756607

Batch 17550, train_perplexity=4737.1367, train_loss=8.463188

Batch 17560, train_perplexity=4807.1035, train_loss=8.47785

Batch 17570, train_perplexity=5312.2373, train_loss=8.577768

Batch 17580, train_perplexity=6411.638, train_loss=8.76587

Batch 17590, train_perplexity=6212.287, train_loss=8.734284

Batch 17600, train_perplexity=4936.0283, train_loss=8.504316

Batch 17610, train_perplexity=6076.273, train_loss=8.712147

Batch 17620, train_perplexity=4986.203, train_loss=8.51443

Batch 17630, train_perplexity=5684.5425, train_loss=8.645506

Batch 17640, train_perplexity=5234.1943, train_loss=8.562968

Batch 17650, train_perplexity=5464.0293, train_loss=8.605942

Batch 17660, train_perplexity=5789.0913, train_loss=8.663731

Batch 17670, train_perplexity=5238.4243, train_loss=8.563776

Batch 17680, train_perplexity=6323.629, train_loss=8.7520485

Batch 17690, train_perplexity=5494.3257, train_loss=8.611471

Batch 17700, train_perplexity=5272.4653, train_loss=8.570253

Batch 17710, train_perplexity=3962.301, train_loss=8.28458

Batch 17720, train_perplexity=5602.8945, train_loss=8.631039

Batch 17730, train_perplexity=4958.231, train_loss=8.508804

Batch 17740, train_perplexity=5412.8027, train_loss=8.596522

Batch 17750, train_perplexity=4200.514, train_loss=8.342962

Batch 17760, train_perplexity=5560.3003, train_loss=8.623407

Batch 17770, train_perplexity=5204.881, train_loss=8.557352

Batch 17780, train_perplexity=5267.023, train_loss=8.569221

Batch 17790, train_perplexity=5451.548, train_loss=8.603655

Batch 17800, train_perplexity=6640.031, train_loss=8.800872

Batch 17810, train_perplexity=5672.282, train_loss=8.643347

Batch 17820, train_perplexity=5012.33, train_loss=8.519656

Batch 17830, train_perplexity=5507.8716, train_loss=8.613934

Batch 17840, train_perplexity=7620.8594, train_loss=8.938644

Batch 17850, train_perplexity=4624.4497, train_loss=8.439113

Batch 17860, train_perplexity=5096.378, train_loss=8.536285

Batch 17870, train_perplexity=5328.0566, train_loss=8.580742
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17880, train_perplexity=5743.4595, train_loss=8.655817

Batch 17890, train_perplexity=5872.455, train_loss=8.678028

Batch 17900, train_perplexity=4732.689, train_loss=8.462249

Batch 17910, train_perplexity=5454.1846, train_loss=8.604138

Batch 17920, train_perplexity=5953.6914, train_loss=8.691767

Batch 17930, train_perplexity=5145.8022, train_loss=8.545937

Batch 17940, train_perplexity=5973.404, train_loss=8.695072

Batch 17950, train_perplexity=5329.531, train_loss=8.581018

Batch 17960, train_perplexity=5673.981, train_loss=8.643646

Batch 17970, train_perplexity=5268.6504, train_loss=8.56953

Batch 17980, train_perplexity=5575.0825, train_loss=8.626062

Batch 17990, train_perplexity=4657.463, train_loss=8.446226

Batch 18000, train_perplexity=5497.6016, train_loss=8.612067

Batch 18010, train_perplexity=4792.09, train_loss=8.474722

Batch 18020, train_perplexity=5356.455, train_loss=8.586058

Batch 18030, train_perplexity=4847.4863, train_loss=8.486216

Batch 18040, train_perplexity=5414.3105, train_loss=8.596801

Batch 18050, train_perplexity=4792.3276, train_loss=8.4747715

Batch 18060, train_perplexity=4957.7725, train_loss=8.508712

Batch 18070, train_perplexity=4600.2217, train_loss=8.43386

Batch 18080, train_perplexity=5040.646, train_loss=8.52529

Batch 18090, train_perplexity=4713.3975, train_loss=8.458164

Batch 18100, train_perplexity=4953.5806, train_loss=8.507866

Batch 18110, train_perplexity=4959.3896, train_loss=8.509038

Batch 18120, train_perplexity=4900.298, train_loss=8.497051

Batch 18130, train_perplexity=5458.8833, train_loss=8.605

Batch 18140, train_perplexity=5486.7544, train_loss=8.610092

Batch 18150, train_perplexity=5487.822, train_loss=8.610287

Batch 18160, train_perplexity=4668.754, train_loss=8.4486475

Batch 18170, train_perplexity=5124.485, train_loss=8.541785

Batch 18180, train_perplexity=5113.3247, train_loss=8.539605

Batch 18190, train_perplexity=5637.788, train_loss=8.637247

Batch 18200, train_perplexity=5563.9707, train_loss=8.624067

Batch 18210, train_perplexity=6183.873, train_loss=8.7297

Batch 18220, train_perplexity=5927.4272, train_loss=8.6873455

Batch 18230, train_perplexity=4798.8584, train_loss=8.476133

Batch 18240, train_perplexity=5786.464, train_loss=8.663277

Batch 18250, train_perplexity=5080.5825, train_loss=8.533181

Batch 18260, train_perplexity=6129.7026, train_loss=8.7209015

Batch 18270, train_perplexity=4656.5747, train_loss=8.446035

Batch 18280, train_perplexity=4389.4785, train_loss=8.386966

Batch 18290, train_perplexity=6059.133, train_loss=8.709322

Batch 18300, train_perplexity=4133.538, train_loss=8.326889

Batch 18310, train_perplexity=6781.999, train_loss=8.822027

Batch 18320, train_perplexity=5952.5674, train_loss=8.691578

Batch 18330, train_perplexity=4722.6978, train_loss=8.460135

Batch 18340, train_perplexity=6260.0137, train_loss=8.741938

Batch 18350, train_perplexity=4789.737, train_loss=8.474231

Batch 18360, train_perplexity=7061.747, train_loss=8.862448

Batch 18370, train_perplexity=5026.5522, train_loss=8.52249

Batch 18380, train_perplexity=5052.6445, train_loss=8.527667

Batch 18390, train_perplexity=6607.555, train_loss=8.795969

Batch 18400, train_perplexity=5848.7354, train_loss=8.673981

Batch 18410, train_perplexity=4617.469, train_loss=8.437602

Batch 18420, train_perplexity=5939.2983, train_loss=8.689346

Batch 18430, train_perplexity=4294.934, train_loss=8.365191

Batch 18440, train_perplexity=4376.4707, train_loss=8.383998

Batch 18450, train_perplexity=5985.379, train_loss=8.697075

Batch 18460, train_perplexity=5143.722, train_loss=8.545532

Batch 18470, train_perplexity=4537.518, train_loss=8.4201355

Batch 18480, train_perplexity=5175.094, train_loss=8.551613

Batch 18490, train_perplexity=6268.1562, train_loss=8.7432375

Batch 18500, train_perplexity=5187.2, train_loss=8.553949

Batch 18510, train_perplexity=4697.149, train_loss=8.454711

Batch 18520, train_perplexity=4839.7075, train_loss=8.48461

Batch 18530, train_perplexity=5076.18, train_loss=8.532314

Batch 18540, train_perplexity=6727.3154, train_loss=8.813931

Batch 18550, train_perplexity=5960.839, train_loss=8.692966

Batch 18560, train_perplexity=5011.6753, train_loss=8.519526

Batch 18570, train_perplexity=4377.548, train_loss=8.384244

Batch 18580, train_perplexity=5222.6865, train_loss=8.560767

Batch 18590, train_perplexity=4630.9507, train_loss=8.440517

Batch 18600, train_perplexity=5629.181, train_loss=8.635719

Batch 18610, train_perplexity=6414.9595, train_loss=8.766388

Batch 18620, train_perplexity=6463.848, train_loss=8.77398

Batch 18630, train_perplexity=5938.766, train_loss=8.689257

Batch 18640, train_perplexity=5824.1, train_loss=8.66976

Batch 18650, train_perplexity=5331.549, train_loss=8.581397

Batch 18660, train_perplexity=5088.7046, train_loss=8.534779

Batch 18670, train_perplexity=4769.494, train_loss=8.4699955

Batch 18680, train_perplexity=5331.7876, train_loss=8.581442

Batch 18690, train_perplexity=5398.734, train_loss=8.59392

Batch 18700, train_perplexity=5181.603, train_loss=8.55287

Batch 18710, train_perplexity=4859.521, train_loss=8.488695

Batch 18720, train_perplexity=5963.511, train_loss=8.693415

Batch 18730, train_perplexity=5121.861, train_loss=8.541273

Batch 18740, train_perplexity=5918.446, train_loss=8.685829

Batch 18750, train_perplexity=6116.9893, train_loss=8.718825

Batch 18760, train_perplexity=5837.7905, train_loss=8.672108

Batch 18770, train_perplexity=6704.4443, train_loss=8.810526

Batch 18780, train_perplexity=6593.518, train_loss=8.793842

Batch 18790, train_perplexity=5541.116, train_loss=8.619951

Batch 18800, train_perplexity=6184.628, train_loss=8.729822

Batch 18810, train_perplexity=5368.125, train_loss=8.588234

Batch 18820, train_perplexity=4951.9556, train_loss=8.507538

Batch 18830, train_perplexity=5678.9126, train_loss=8.644515

Batch 18840, train_perplexity=6154.6323, train_loss=8.72496

Batch 18850, train_perplexity=5172.07, train_loss=8.551028

Batch 18860, train_perplexity=4928.869, train_loss=8.502865

Batch 18870, train_perplexity=5739.1836, train_loss=8.655072

Batch 18880, train_perplexity=4471.245, train_loss=8.405422

Batch 18890, train_perplexity=5682.515, train_loss=8.645149

Batch 18900, train_perplexity=5268.525, train_loss=8.569506

Batch 18910, train_perplexity=6092.02, train_loss=8.714735

Batch 18920, train_perplexity=4927.9434, train_loss=8.502677

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100
Loaded 306074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100
Loaded 306074 sentences.
Finished loading
Batch 18930, train_perplexity=5348.6704, train_loss=8.584603

Batch 18940, train_perplexity=5361.0034, train_loss=8.586906

Batch 18950, train_perplexity=4905.0015, train_loss=8.498011

Batch 18960, train_perplexity=4507.448, train_loss=8.4134865

Batch 18970, train_perplexity=5396.8394, train_loss=8.593569

Batch 18980, train_perplexity=5591.8184, train_loss=8.62906

Batch 18990, train_perplexity=5852.6074, train_loss=8.674643

Batch 19000, train_perplexity=4989.3237, train_loss=8.515056

Batch 19010, train_perplexity=5766.5327, train_loss=8.659826

Batch 19020, train_perplexity=5363.4683, train_loss=8.587366

Batch 19030, train_perplexity=5527.4097, train_loss=8.617475

Batch 19040, train_perplexity=5036.643, train_loss=8.524495

Batch 19050, train_perplexity=5336.844, train_loss=8.58239

Batch 19060, train_perplexity=5502.968, train_loss=8.613043

Batch 19070, train_perplexity=5469.7954, train_loss=8.606997

Batch 19080, train_perplexity=5250.1074, train_loss=8.566004

Batch 19090, train_perplexity=6245.1006, train_loss=8.7395525

Batch 19100, train_perplexity=5497.911, train_loss=8.6121235

Batch 19110, train_perplexity=4943.66, train_loss=8.505861

Batch 19120, train_perplexity=4816.5796, train_loss=8.479819

Batch 19130, train_perplexity=5073.238, train_loss=8.531734

Batch 19140, train_perplexity=5937.962, train_loss=8.689121
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19150, train_perplexity=4785.8374, train_loss=8.473416

Batch 19160, train_perplexity=5252.361, train_loss=8.566433

Batch 19170, train_perplexity=5118.2964, train_loss=8.540577

Batch 19180, train_perplexity=5133.5093, train_loss=8.543545

Batch 19190, train_perplexity=4767.2793, train_loss=8.469531

Batch 19200, train_perplexity=6441.9155, train_loss=8.770581

Batch 19210, train_perplexity=5804.15, train_loss=8.666328

Batch 19220, train_perplexity=5678.3765, train_loss=8.644421

Batch 19230, train_perplexity=5721.1333, train_loss=8.651922

Batch 19240, train_perplexity=5274.266, train_loss=8.570595

Batch 19250, train_perplexity=5938.166, train_loss=8.689156

Batch 19260, train_perplexity=5847.8203, train_loss=8.673824

Batch 19270, train_perplexity=6145.1196, train_loss=8.723413

Batch 19280, train_perplexity=6183.012, train_loss=8.729561

Batch 19290, train_perplexity=5813.8057, train_loss=8.667991

Batch 19300, train_perplexity=4530.881, train_loss=8.418672

Batch 19310, train_perplexity=4905.6704, train_loss=8.498147

Batch 19320, train_perplexity=5790.9634, train_loss=8.664054

Batch 19330, train_perplexity=5146.8037, train_loss=8.546131

Batch 19340, train_perplexity=4557.9194, train_loss=8.424622

Batch 19350, train_perplexity=4955.0874, train_loss=8.50817

Batch 19360, train_perplexity=6433.1855, train_loss=8.769225

Batch 19370, train_perplexity=4755.5503, train_loss=8.467068

Batch 19380, train_perplexity=6577.4653, train_loss=8.791405

Batch 19390, train_perplexity=5375.0356, train_loss=8.58952

Batch 19400, train_perplexity=5405.6274, train_loss=8.595196

Batch 19410, train_perplexity=4388.194, train_loss=8.386673

Batch 19420, train_perplexity=6315.7397, train_loss=8.7508

Batch 19430, train_perplexity=4844.358, train_loss=8.48557

Batch 19440, train_perplexity=5565.7754, train_loss=8.624392

Batch 19450, train_perplexity=4759.289, train_loss=8.467854

Batch 19460, train_perplexity=5420.355, train_loss=8.597917

Batch 19470, train_perplexity=5476.0483, train_loss=8.608139

Batch 19480, train_perplexity=4803.35, train_loss=8.477069

Batch 19490, train_perplexity=4758.0864, train_loss=8.467601

Batch 19500, train_perplexity=4686.5757, train_loss=8.452457

Batch 19510, train_perplexity=4505.1104, train_loss=8.412968

Batch 19520, train_perplexity=4863.592, train_loss=8.489532

Batch 19530, train_perplexity=4978.767, train_loss=8.512938

Batch 19540, train_perplexity=5525.528, train_loss=8.617134

Batch 19550, train_perplexity=5556.0596, train_loss=8.622644

Batch 19560, train_perplexity=5924.601, train_loss=8.686869

Batch 19570, train_perplexity=5118.2476, train_loss=8.540567

Batch 19580, train_perplexity=5091.7583, train_loss=8.535378

Batch 19590, train_perplexity=5048.5405, train_loss=8.5268545

Batch 19600, train_perplexity=5682.5854, train_loss=8.645162

Batch 19610, train_perplexity=6216.7144, train_loss=8.734997

Batch 19620, train_perplexity=7648.148, train_loss=8.942219

Batch 19630, train_perplexity=4884.9795, train_loss=8.49392

Batch 19640, train_perplexity=5482.633, train_loss=8.609341

Batch 19650, train_perplexity=6491.5303, train_loss=8.778254

Batch 19660, train_perplexity=4572.6396, train_loss=8.427846

Batch 19670, train_perplexity=4615.88, train_loss=8.437258

Batch 19680, train_perplexity=4914.0425, train_loss=8.499852

Batch 19690, train_perplexity=6382.3125, train_loss=8.761286

Batch 19700, train_perplexity=4826.304, train_loss=8.481836

Batch 19710, train_perplexity=6246.1787, train_loss=8.739725

Batch 19720, train_perplexity=5390.8213, train_loss=8.592453

Batch 19730, train_perplexity=5458.097, train_loss=8.604856

Batch 19740, train_perplexity=5457.8945, train_loss=8.604818

Batch 19750, train_perplexity=5365.402, train_loss=8.587727

Batch 19760, train_perplexity=5215.733, train_loss=8.559435

Batch 19770, train_perplexity=5779.5977, train_loss=8.662089

Batch 19780, train_perplexity=4735.4385, train_loss=8.46283

Batch 19790, train_perplexity=5008.7417, train_loss=8.51894

Batch 19800, train_perplexity=4329.0117, train_loss=8.373095

Batch 19810, train_perplexity=5858.225, train_loss=8.675602

Batch 19820, train_perplexity=6900.8267, train_loss=8.839396

Batch 19830, train_perplexity=5694.108, train_loss=8.647187

Batch 19840, train_perplexity=5886.473, train_loss=8.680412

Batch 19850, train_perplexity=5133.343, train_loss=8.543512

Batch 19860, train_perplexity=5152.6133, train_loss=8.547259

Batch 19870, train_perplexity=6402.9795, train_loss=8.764519

Batch 19880, train_perplexity=5125.8726, train_loss=8.542056

Batch 19890, train_perplexity=5906.4585, train_loss=8.683802

Batch 19900, train_perplexity=4270.8765, train_loss=8.359574

Batch 19910, train_perplexity=4976.2036, train_loss=8.512423

Batch 19920, train_perplexity=5934.3613, train_loss=8.688515

Batch 19930, train_perplexity=4107.5317, train_loss=8.320578

Batch 19940, train_perplexity=5116.3345, train_loss=8.540194

Batch 19950, train_perplexity=5681.2583, train_loss=8.644928

Batch 19960, train_perplexity=4924.274, train_loss=8.501932

Batch 19970, train_perplexity=5218.101, train_loss=8.559889

Batch 19980, train_perplexity=5525.823, train_loss=8.6171875

Batch 19990, train_perplexity=4817.255, train_loss=8.4799595

Batch 20000, train_perplexity=6026.4346, train_loss=8.703911

Batch 20010, train_perplexity=5173.8013, train_loss=8.551363

Batch 20020, train_perplexity=5454.803, train_loss=8.604252

Batch 20030, train_perplexity=5385.046, train_loss=8.591381

Batch 20040, train_perplexity=5371.505, train_loss=8.588863

Batch 20050, train_perplexity=4261.6406, train_loss=8.3574095

Batch 20060, train_perplexity=5808.5576, train_loss=8.667088

Batch 20070, train_perplexity=5524.822, train_loss=8.617006

Batch 20080, train_perplexity=4004.449, train_loss=8.295161

Batch 20090, train_perplexity=4534.8057, train_loss=8.419538

Batch 20100, train_perplexity=4651.018, train_loss=8.444841

Batch 20110, train_perplexity=5288.858, train_loss=8.573358

Batch 20120, train_perplexity=5456.5205, train_loss=8.604567

Batch 20130, train_perplexity=6224.664, train_loss=8.736275

Batch 20140, train_perplexity=5433.594, train_loss=8.600356

Batch 20150, train_perplexity=5774.171, train_loss=8.66115

Batch 20160, train_perplexity=5063.744, train_loss=8.529861

Batch 20170, train_perplexity=5061.7114, train_loss=8.52946

Batch 20180, train_perplexity=7905.7803, train_loss=8.975349

Batch 20190, train_perplexity=5525.4067, train_loss=8.617112

Batch 20200, train_perplexity=5400.238, train_loss=8.594198

Batch 20210, train_perplexity=5658.202, train_loss=8.6408615

Batch 20220, train_perplexity=5014.3145, train_loss=8.520052

Batch 20230, train_perplexity=6286.8286, train_loss=8.746212

Batch 20240, train_perplexity=6040.52, train_loss=8.706245

Batch 20250, train_perplexity=4324.774, train_loss=8.372115

Batch 20260, train_perplexity=5489.9263, train_loss=8.61067

Batch 20270, train_perplexity=6279.7817, train_loss=8.7450905

Batch 20280, train_perplexity=6233.3965, train_loss=8.737677

Batch 20290, train_perplexity=5290.931, train_loss=8.57375

Batch 20300, train_perplexity=6290.835, train_loss=8.746849

Batch 20310, train_perplexity=5037.633, train_loss=8.524692

Batch 20320, train_perplexity=5524.8745, train_loss=8.617016

Batch 20330, train_perplexity=4728.791, train_loss=8.461425

Batch 20340, train_perplexity=4674.7153, train_loss=8.4499235

Batch 20350, train_perplexity=5442.348, train_loss=8.601966

Batch 20360, train_perplexity=4617.531, train_loss=8.437615

Batch 20370, train_perplexity=5249.832, train_loss=8.565951

Batch 20380, train_perplexity=5187.6206, train_loss=8.55403

Batch 20390, train_perplexity=5622.0996, train_loss=8.63446

Batch 20400, train_perplexity=6128.4927, train_loss=8.720704

Batch 20410, train_perplexity=5520.0293, train_loss=8.616138

Batch 20420, train_perplexity=5433.6616, train_loss=8.6003685

Batch 20430, train_perplexity=4692.5996, train_loss=8.453742

Batch 20440, train_perplexity=5184.6187, train_loss=8.553452

Batch 20450, train_perplexity=6129.0186, train_loss=8.72079

Batch 20460, train_perplexity=5368.8877, train_loss=8.588376

Batch 20470, train_perplexity=6093.345, train_loss=8.714952

Batch 20480, train_perplexity=6104.2446, train_loss=8.71674
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 20490, train_perplexity=4990.147, train_loss=8.515221

Batch 20500, train_perplexity=5301.659, train_loss=8.575775

Batch 20510, train_perplexity=5814.233, train_loss=8.668064

Batch 20520, train_perplexity=4802.4478, train_loss=8.476881

Batch 20530, train_perplexity=5063.3965, train_loss=8.529793

Batch 20540, train_perplexity=5217.5586, train_loss=8.559785

Batch 20550, train_perplexity=6751.604, train_loss=8.817535

Batch 20560, train_perplexity=5051.734, train_loss=8.527487

Batch 20570, train_perplexity=5845.9136, train_loss=8.673498

Batch 20580, train_perplexity=5860.047, train_loss=8.675913

Batch 20590, train_perplexity=5941.735, train_loss=8.689756

Batch 20600, train_perplexity=6175.864, train_loss=8.728404

Batch 20610, train_perplexity=6034.049, train_loss=8.7051735

Batch 20620, train_perplexity=5720.566, train_loss=8.651823

Batch 20630, train_perplexity=4505.403, train_loss=8.413033

Batch 20640, train_perplexity=5090.36, train_loss=8.535104

Batch 20650, train_perplexity=4997.1953, train_loss=8.516632

Batch 20660, train_perplexity=4861.1987, train_loss=8.48904

Batch 20670, train_perplexity=4684.9536, train_loss=8.452111

Batch 20680, train_perplexity=5378.9175, train_loss=8.590242

Batch 20690, train_perplexity=6344.1484, train_loss=8.755288

Batch 20700, train_perplexity=4686.3877, train_loss=8.452417

Batch 20710, train_perplexity=4884.574, train_loss=8.493837

Batch 20720, train_perplexity=5333.3643, train_loss=8.5817375

Batch 20730, train_perplexity=5659.082, train_loss=8.641017

Batch 20740, train_perplexity=5214.9966, train_loss=8.559294

Batch 20750, train_perplexity=5048.6226, train_loss=8.526871

Batch 20760, train_perplexity=5582.8984, train_loss=8.627463

Batch 20770, train_perplexity=4925.2837, train_loss=8.502137

Batch 20780, train_perplexity=5416.2163, train_loss=8.597153

Batch 20790, train_perplexity=5637.799, train_loss=8.637249

Batch 20800, train_perplexity=4936.8003, train_loss=8.504473

Batch 20810, train_perplexity=5323.0186, train_loss=8.579796

Batch 20820, train_perplexity=5193.3774, train_loss=8.55514

Batch 20830, train_perplexity=4535.485, train_loss=8.419687

Batch 20840, train_perplexity=5471.982, train_loss=8.607396

Batch 20850, train_perplexity=6219.9165, train_loss=8.735512

Batch 20860, train_perplexity=5472.7646, train_loss=8.607539

Batch 20870, train_perplexity=6473.923, train_loss=8.7755375

Batch 20880, train_perplexity=5818.4927, train_loss=8.668797

Batch 20890, train_perplexity=4606.232, train_loss=8.435165

Batch 20900, train_perplexity=6816.2354, train_loss=8.827063

Batch 20910, train_perplexity=6269.274, train_loss=8.743416

Batch 20920, train_perplexity=5030.9834, train_loss=8.523371

Batch 20930, train_perplexity=5895.417, train_loss=8.681931

Batch 20940, train_perplexity=5231.425, train_loss=8.562439

Batch 20950, train_perplexity=4713.0195, train_loss=8.458084

Batch 20960, train_perplexity=5354.3867, train_loss=8.585671

Batch 20970, train_perplexity=6275.4233, train_loss=8.744396

Batch 20980, train_perplexity=4993.17, train_loss=8.515826

Batch 20990, train_perplexity=4789.2114, train_loss=8.474121

Batch 21000, train_perplexity=4988.5483, train_loss=8.5149

Batch 21010, train_perplexity=5097.0005, train_loss=8.536407

Batch 21020, train_perplexity=6964.422, train_loss=8.84857

Batch 21030, train_perplexity=5563.159, train_loss=8.623921

Batch 21040, train_perplexity=4787.7686, train_loss=8.47382

Batch 21050, train_perplexity=5953.2715, train_loss=8.691696

Batch 21060, train_perplexity=5840.185, train_loss=8.672518

Batch 21070, train_perplexity=5417.3525, train_loss=8.5973625

Batch 21080, train_perplexity=6619.2295, train_loss=8.797734

Batch 21090, train_perplexity=5321.9272, train_loss=8.579591

Batch 21100, train_perplexity=6333.1284, train_loss=8.75355

Batch 21110, train_perplexity=4950.1753, train_loss=8.507178

Batch 21120, train_perplexity=5162.8594, train_loss=8.549246

Batch 21130, train_perplexity=5311.7, train_loss=8.577667

Batch 21140, train_perplexity=5417.673, train_loss=8.597422

Batch 21150, train_perplexity=6379.8296, train_loss=8.760897

Batch 21160, train_perplexity=5298.192, train_loss=8.575121

Batch 21170, train_perplexity=5765.235, train_loss=8.659601

Batch 21180, train_perplexity=4100.663, train_loss=8.318904

Batch 21190, train_perplexity=5502.884, train_loss=8.613028

Batch 21200, train_perplexity=5168.1895, train_loss=8.550278

Batch 21210, train_perplexity=5511.907, train_loss=8.614666

Batch 21220, train_perplexity=5760.7944, train_loss=8.658831

Batch 21230, train_perplexity=5727.947, train_loss=8.653112

Batch 21240, train_perplexity=5734.5713, train_loss=8.654268

Batch 21250, train_perplexity=4854.245, train_loss=8.487609

Batch 21260, train_perplexity=4994.737, train_loss=8.51614

Batch 21270, train_perplexity=5436.979, train_loss=8.600979

Batch 21280, train_perplexity=6211.0845, train_loss=8.734091

Batch 21290, train_perplexity=5840.4917, train_loss=8.67257

Batch 21300, train_perplexity=4458.7983, train_loss=8.402635

Batch 21310, train_perplexity=4643.355, train_loss=8.4431925

Batch 21320, train_perplexity=5943.3667, train_loss=8.690031

Batch 21330, train_perplexity=5803.602, train_loss=8.666234

Batch 21340, train_perplexity=5644.5454, train_loss=8.638445

Batch 21350, train_perplexity=6695.0137, train_loss=8.809118

Batch 21360, train_perplexity=5857.7, train_loss=8.675512

Batch 21370, train_perplexity=5831.636, train_loss=8.671053

Batch 21380, train_perplexity=4481.089, train_loss=8.407621

Batch 21390, train_perplexity=4619.6626, train_loss=8.438077

Batch 21400, train_perplexity=4880.23, train_loss=8.492948

Batch 21410, train_perplexity=4632.24, train_loss=8.440796

Batch 21420, train_perplexity=5187.576, train_loss=8.554022

Batch 21430, train_perplexity=4648.827, train_loss=8.44437

Batch 21440, train_perplexity=4107.485, train_loss=8.320566

Batch 21450, train_perplexity=5590.8423, train_loss=8.628885

Batch 21460, train_perplexity=5241.8525, train_loss=8.56443

Batch 21470, train_perplexity=5609.257, train_loss=8.632174

Batch 21480, train_perplexity=4697.207, train_loss=8.454723

Batch 21490, train_perplexity=4932.4473, train_loss=8.503591

Batch 21500, train_perplexity=5325.9536, train_loss=8.580347

Batch 21510, train_perplexity=5399.697, train_loss=8.594098

Batch 21520, train_perplexity=5708.1465, train_loss=8.64965

Batch 21530, train_perplexity=5380.9336, train_loss=8.590617

Batch 21540, train_perplexity=4655.0737, train_loss=8.445713

Batch 21550, train_perplexity=5188.14, train_loss=8.554131

Batch 21560, train_perplexity=5246.864, train_loss=8.565386

Batch 21570, train_perplexity=4824.3716, train_loss=8.481436

Batch 21580, train_perplexity=4996.5566, train_loss=8.516504

Batch 21590, train_perplexity=6171.4067, train_loss=8.727682

Batch 21600, train_perplexity=4370.773, train_loss=8.382695

Batch 21610, train_perplexity=5411.409, train_loss=8.596265

Batch 21620, train_perplexity=6938.5933, train_loss=8.844854

Batch 21630, train_perplexity=6750.986, train_loss=8.817444

Batch 21640, train_perplexity=4976.9204, train_loss=8.512567

Batch 21650, train_perplexity=5440.3506, train_loss=8.601599

Batch 21660, train_perplexity=4890.3022, train_loss=8.495009

Batch 21670, train_perplexity=5156.959, train_loss=8.548102

Batch 21680, train_perplexity=4487.487, train_loss=8.409048

Batch 21690, train_perplexity=5646.7524, train_loss=8.638836

Batch 21700, train_perplexity=5572.4883, train_loss=8.625597

Batch 21710, train_perplexity=5452.2134, train_loss=8.603777

Batch 21720, train_perplexity=5167.2383, train_loss=8.550094

Batch 21730, train_perplexity=4991.8604, train_loss=8.515564

Batch 21740, train_perplexity=5288.2275, train_loss=8.573238

Batch 21750, train_perplexity=4892.938, train_loss=8.495548

Batch 21760, train_perplexity=5375.9634, train_loss=8.589693

Batch 21770, train_perplexity=4855.0737, train_loss=8.48778

Batch 21780, train_perplexity=5192.3525, train_loss=8.554942

Batch 21790, train_perplexity=4819.0835, train_loss=8.480339

Batch 21800, train_perplexity=6160.505, train_loss=8.725914

Batch 21810, train_perplexity=6100.8926, train_loss=8.71619

Batch 21820, train_perplexity=5362.875, train_loss=8.5872555
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21830, train_perplexity=6732.5205, train_loss=8.814705

Batch 21840, train_perplexity=4926.51, train_loss=8.502386

Batch 21850, train_perplexity=4732.125, train_loss=8.46213

Batch 21860, train_perplexity=4588.8516, train_loss=8.431385

Batch 21870, train_perplexity=5326.5376, train_loss=8.580457

Batch 21880, train_perplexity=5817.96, train_loss=8.668705

Batch 21890, train_perplexity=6031.7646, train_loss=8.704795

Batch 21900, train_perplexity=4773.5576, train_loss=8.470847

Batch 21910, train_perplexity=5804.266, train_loss=8.666348

Batch 21920, train_perplexity=6900.07, train_loss=8.839287

Batch 21930, train_perplexity=6356.2056, train_loss=8.757187

Batch 21940, train_perplexity=5201.6553, train_loss=8.556732

Batch 21950, train_perplexity=5076.083, train_loss=8.532295

Batch 21960, train_perplexity=4136.472, train_loss=8.327599

Batch 21970, train_perplexity=5264.557, train_loss=8.568752

Batch 21980, train_perplexity=5789.025, train_loss=8.663719

Batch 21990, train_perplexity=6167.7, train_loss=8.727081

Batch 22000, train_perplexity=5189.4365, train_loss=8.55438

Batch 22010, train_perplexity=5189.526, train_loss=8.554398

Batch 22020, train_perplexity=4842.478, train_loss=8.485182

Batch 22030, train_perplexity=6776.633, train_loss=8.821236

Batch 22040, train_perplexity=4514.155, train_loss=8.414973

Batch 22050, train_perplexity=5358.1665, train_loss=8.586377

Batch 22060, train_perplexity=5365.709, train_loss=8.587784

Batch 22070, train_perplexity=4784.642, train_loss=8.473166

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Batch 22080, train_perplexity=5097.3213, train_loss=8.53647

Batch 22090, train_perplexity=5331.381, train_loss=8.581366

Batch 22100, train_perplexity=4597.481, train_loss=8.433264

Batch 22110, train_perplexity=5343.9136, train_loss=8.583714

Batch 22120, train_perplexity=5221.765, train_loss=8.560591

Batch 22130, train_perplexity=5210.2246, train_loss=8.558378

Batch 22140, train_perplexity=5879.118, train_loss=8.679162

Batch 22150, train_perplexity=5560.491, train_loss=8.623442

Batch 22160, train_perplexity=5375.984, train_loss=8.589697

Batch 22170, train_perplexity=4568.5815, train_loss=8.426958

Batch 22180, train_perplexity=4946.7637, train_loss=8.506489

Batch 22190, train_perplexity=6566.397, train_loss=8.789721

Batch 22200, train_perplexity=4530.712, train_loss=8.418634

Batch 22210, train_perplexity=5875.8276, train_loss=8.678602

Batch 22220, train_perplexity=4817.158, train_loss=8.479939

Batch 22230, train_perplexity=4783.8984, train_loss=8.473011

Batch 22240, train_perplexity=4795.2397, train_loss=8.475379

Batch 22250, train_perplexity=5236.4165, train_loss=8.563393

Batch 22260, train_perplexity=4584.7485, train_loss=8.4304905

Batch 22270, train_perplexity=4622.8228, train_loss=8.438761

Batch 22280, train_perplexity=5801.5933, train_loss=8.665888

Batch 22290, train_perplexity=5126.914, train_loss=8.542259

Batch 22300, train_perplexity=5596.081, train_loss=8.629822

Batch 22310, train_perplexity=4975.748, train_loss=8.512331

Batch 22320, train_perplexity=5290.033, train_loss=8.57358

Batch 22330, train_perplexity=5908.7905, train_loss=8.684196

Batch 22340, train_perplexity=6105.432, train_loss=8.716934

Batch 22350, train_perplexity=6032.6562, train_loss=8.704943

Batch 22360, train_perplexity=5987.982, train_loss=8.69751

Batch 22370, train_perplexity=5352.7173, train_loss=8.58536

Batch 22380, train_perplexity=4932.2217, train_loss=8.503545

Batch 22390, train_perplexity=5106.809, train_loss=8.53833

Batch 22400, train_perplexity=4452.7134, train_loss=8.401269

Batch 22410, train_perplexity=4503.169, train_loss=8.412537

Batch 22420, train_perplexity=5733.1934, train_loss=8.654028

Batch 22430, train_perplexity=5422.7285, train_loss=8.598354

Batch 22440, train_perplexity=5846.2256, train_loss=8.673552

Batch 22450, train_perplexity=5272.581, train_loss=8.570275

Batch 22460, train_perplexity=6263.1665, train_loss=8.742441

Batch 22470, train_perplexity=4823.953, train_loss=8.481349

Batch 22480, train_perplexity=5774.0938, train_loss=8.661137

Batch 22490, train_perplexity=5575.731, train_loss=8.626179

Batch 22500, train_perplexity=5195.037, train_loss=8.555459

Batch 22510, train_perplexity=4920.7344, train_loss=8.501213

Batch 22520, train_perplexity=5168.5737, train_loss=8.550352

Batch 22530, train_perplexity=4788.9697, train_loss=8.474071

Batch 22540, train_perplexity=5525.9707, train_loss=8.617214

Batch 22550, train_perplexity=4601.1475, train_loss=8.434061

Batch 22560, train_perplexity=5026.1304, train_loss=8.522406

Batch 22570, train_perplexity=5994.4844, train_loss=8.698595

Batch 22580, train_perplexity=5361.7295, train_loss=8.587042

Batch 22590, train_perplexity=5495.1904, train_loss=8.611629

Batch 22600, train_perplexity=5418.851, train_loss=8.597639

Batch 22610, train_perplexity=4948.401, train_loss=8.50682

Batch 22620, train_perplexity=5042.0884, train_loss=8.525576

Batch 22630, train_perplexity=7516.3086, train_loss=8.92483

Batch 22640, train_perplexity=5200.326, train_loss=8.556477

Batch 22650, train_perplexity=6048.337, train_loss=8.707539

Batch 22660, train_perplexity=5816.928, train_loss=8.668528

Batch 22670, train_perplexity=5699.057, train_loss=8.648056

Batch 22680, train_perplexity=4620.108, train_loss=8.438173

Batch 22690, train_perplexity=5724.4517, train_loss=8.652502

Batch 22700, train_perplexity=4788.782, train_loss=8.474031

Batch 22710, train_perplexity=4949.288, train_loss=8.506999

Batch 22720, train_perplexity=5941.6553, train_loss=8.689743

Batch 22730, train_perplexity=5328.6055, train_loss=8.580845

Batch 22740, train_perplexity=5166.6816, train_loss=8.549986

Batch 22750, train_perplexity=5158.7593, train_loss=8.548451

Batch 22760, train_perplexity=6100.0195, train_loss=8.716047

Batch 22770, train_perplexity=5976.857, train_loss=8.69565

Batch 22780, train_perplexity=5297.2627, train_loss=8.574945

Batch 22790, train_perplexity=5384.26, train_loss=8.591235

Batch 22800, train_perplexity=5935.341, train_loss=8.68868

Batch 22810, train_perplexity=5324.7803, train_loss=8.580127

Batch 22820, train_perplexity=4668.9277, train_loss=8.448685

Batch 22830, train_perplexity=5970.67, train_loss=8.694614

Batch 22840, train_perplexity=5669.7725, train_loss=8.642904

Batch 22850, train_perplexity=5050.7417, train_loss=8.52729

Batch 22860, train_perplexity=5051.3193, train_loss=8.527405

Batch 22870, train_perplexity=6798.402, train_loss=8.824443

Batch 22880, train_perplexity=6815.4033, train_loss=8.826941

Batch 22890, train_perplexity=5599.8394, train_loss=8.630493

Batch 22900, train_perplexity=4751.942, train_loss=8.466309

Batch 22910, train_perplexity=5588.6997, train_loss=8.628502

Batch 22920, train_perplexity=4783.967, train_loss=8.473025

Batch 22930, train_perplexity=5733.423, train_loss=8.654068

Batch 22940, train_perplexity=5139.7695, train_loss=8.544764

Batch 22950, train_perplexity=4893.932, train_loss=8.495751

Batch 22960, train_perplexity=4551.699, train_loss=8.423256

Batch 22970, train_perplexity=5099.0376, train_loss=8.536807

Batch 22980, train_perplexity=5479.6323, train_loss=8.608793

Batch 22990, train_perplexity=4827.722, train_loss=8.48213

Batch 23000, train_perplexity=4818.8354, train_loss=8.480288

Batch 23010, train_perplexity=5690.531, train_loss=8.646559

Batch 23020, train_perplexity=5144.0166, train_loss=8.545589

Batch 23030, train_perplexity=5828.0947, train_loss=8.670445

Batch 23040, train_perplexity=5350.135, train_loss=8.584877

Batch 23050, train_perplexity=5928.9985, train_loss=8.687611

Batch 23060, train_perplexity=6114.907, train_loss=8.718485

Batch 23070, train_perplexity=5228.522, train_loss=8.561884

Batch 23080, train_perplexity=4688.471, train_loss=8.452862

Batch 23090, train_perplexity=4687.639, train_loss=8.452684

Batch 23100, train_perplexity=4754.0947, train_loss=8.466762
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 23110, train_perplexity=5030.0, train_loss=8.523175

Batch 23120, train_perplexity=5853.1543, train_loss=8.674736

Batch 23130, train_perplexity=5249.5566, train_loss=8.565899

Batch 23140, train_perplexity=5073.189, train_loss=8.531725

Batch 23150, train_perplexity=5231.3203, train_loss=8.562419

Batch 23160, train_perplexity=4701.6978, train_loss=8.455679

Batch 23170, train_perplexity=4862.2607, train_loss=8.489259

Batch 23180, train_perplexity=6591.739, train_loss=8.793572

Batch 23190, train_perplexity=5803.8843, train_loss=8.666283

Batch 23200, train_perplexity=5403.4727, train_loss=8.594797

Batch 23210, train_perplexity=5507.157, train_loss=8.613804

Batch 23220, train_perplexity=4982.068, train_loss=8.5136

Batch 23230, train_perplexity=5534.2773, train_loss=8.618716

Batch 23240, train_perplexity=5631.8716, train_loss=8.636197

Batch 23250, train_perplexity=5716.9663, train_loss=8.651194

Batch 23260, train_perplexity=5785.3823, train_loss=8.66309

Batch 23270, train_perplexity=5224.8833, train_loss=8.561188

Batch 23280, train_perplexity=5512.2646, train_loss=8.614731

Batch 23290, train_perplexity=5145.356, train_loss=8.54585

Batch 23300, train_perplexity=5057.21, train_loss=8.52857

Batch 23310, train_perplexity=5713.478, train_loss=8.650583

Batch 23320, train_perplexity=5141.476, train_loss=8.545095

Batch 23330, train_perplexity=5201.804, train_loss=8.556761

Batch 23340, train_perplexity=4758.3994, train_loss=8.467667

Batch 23350, train_perplexity=5736.1357, train_loss=8.654541

Batch 23360, train_perplexity=5093.2637, train_loss=8.535674

Batch 23370, train_perplexity=5914.056, train_loss=8.685087

Batch 23380, train_perplexity=4892.1543, train_loss=8.495388

Batch 23390, train_perplexity=5517.3506, train_loss=8.615653

Batch 23400, train_perplexity=4695.5586, train_loss=8.454372

Batch 23410, train_perplexity=5488.345, train_loss=8.610382

Batch 23420, train_perplexity=5036.475, train_loss=8.524462

Batch 23430, train_perplexity=4557.6284, train_loss=8.424558

Batch 23440, train_perplexity=4928.0938, train_loss=8.5027075

Batch 23450, train_perplexity=4564.88, train_loss=8.426147

Batch 23460, train_perplexity=4003.567, train_loss=8.294941

Batch 23470, train_perplexity=5254.535, train_loss=8.566847

Batch 23480, train_perplexity=4994.8794, train_loss=8.516169

Batch 23490, train_perplexity=6018.388, train_loss=8.702575

Batch 23500, train_perplexity=4366.1196, train_loss=8.38163

Batch 23510, train_perplexity=4550.371, train_loss=8.422964

Batch 23520, train_perplexity=5333.151, train_loss=8.581697

Batch 23530, train_perplexity=4556.7983, train_loss=8.424376

Batch 23540, train_perplexity=4786.4673, train_loss=8.473548

Batch 23550, train_perplexity=6541.3706, train_loss=8.785902

Batch 23560, train_perplexity=5736.218, train_loss=8.654555

Batch 23570, train_perplexity=4694.5874, train_loss=8.454165

Batch 23580, train_perplexity=5789.671, train_loss=8.663831

Batch 23590, train_perplexity=5183.1553, train_loss=8.553169

Batch 23600, train_perplexity=5279.2783, train_loss=8.571545

Batch 23610, train_perplexity=5214.967, train_loss=8.559288

Batch 23620, train_perplexity=3779.6465, train_loss=8.237386

Batch 23630, train_perplexity=5341.223, train_loss=8.58321

Batch 23640, train_perplexity=5646.225, train_loss=8.638742

Batch 23650, train_perplexity=5625.725, train_loss=8.635105

Batch 23660, train_perplexity=5503.351, train_loss=8.613112

Batch 23670, train_perplexity=4933.9766, train_loss=8.503901

Batch 23680, train_perplexity=5919.2246, train_loss=8.685961

Batch 23690, train_perplexity=5374.569, train_loss=8.589434

Batch 23700, train_perplexity=6266.4287, train_loss=8.742962

Batch 23710, train_perplexity=5050.0044, train_loss=8.527144

Batch 23720, train_perplexity=5457.181, train_loss=8.604688

Batch 23730, train_perplexity=5484.0757, train_loss=8.609604

Batch 23740, train_perplexity=5493.283, train_loss=8.611281

Batch 23750, train_perplexity=4604.3433, train_loss=8.434755

Batch 23760, train_perplexity=6287.2964, train_loss=8.746286

Batch 23770, train_perplexity=6519.1504, train_loss=8.782499

Batch 23780, train_perplexity=5322.9375, train_loss=8.579781

Batch 23790, train_perplexity=5735.058, train_loss=8.654353

Batch 23800, train_perplexity=5481.697, train_loss=8.60917

Batch 23810, train_perplexity=6625.368, train_loss=8.798661

Batch 23820, train_perplexity=5755.6763, train_loss=8.657942

Batch 23830, train_perplexity=5906.143, train_loss=8.683748

Batch 23840, train_perplexity=5096.9907, train_loss=8.536406

Batch 23850, train_perplexity=5132.99, train_loss=8.543444

Batch 23860, train_perplexity=6409.688, train_loss=8.765566

Batch 23870, train_perplexity=4931.7466, train_loss=8.5034485

Batch 23880, train_perplexity=5755.9834, train_loss=8.657995

Batch 23890, train_perplexity=4941.9775, train_loss=8.505521

Batch 23900, train_perplexity=5385.6978, train_loss=8.591502

Batch 23910, train_perplexity=4667.543, train_loss=8.448388

Batch 23920, train_perplexity=4818.9272, train_loss=8.480307

Batch 23930, train_perplexity=5855.1084, train_loss=8.67507

Batch 23940, train_perplexity=4810.1665, train_loss=8.478487

Batch 23950, train_perplexity=5771.0938, train_loss=8.660617

Batch 23960, train_perplexity=6180.772, train_loss=8.729198

Batch 23970, train_perplexity=5070.7705, train_loss=8.531248

Batch 23980, train_perplexity=5597.7246, train_loss=8.6301155

Batch 23990, train_perplexity=6589.7964, train_loss=8.793278

Batch 24000, train_perplexity=4257.038, train_loss=8.356329

Batch 24010, train_perplexity=5211.2285, train_loss=8.558571

Batch 24020, train_perplexity=4962.995, train_loss=8.509765

Batch 24030, train_perplexity=5920.5234, train_loss=8.68618

Batch 24040, train_perplexity=5475.38, train_loss=8.608017

Batch 24050, train_perplexity=5301.604, train_loss=8.575765

Batch 24060, train_perplexity=5463.8784, train_loss=8.605914

Batch 24070, train_perplexity=5905.715, train_loss=8.683676

Batch 24080, train_perplexity=5451.834, train_loss=8.603707

Batch 24090, train_perplexity=5997.7207, train_loss=8.699135

Batch 24100, train_perplexity=5277.481, train_loss=8.571204

Batch 24110, train_perplexity=4478.9697, train_loss=8.407148

Batch 24120, train_perplexity=5516.577, train_loss=8.615513

Batch 24130, train_perplexity=5986.338, train_loss=8.697235

Batch 24140, train_perplexity=5227.45, train_loss=8.561679

Batch 24150, train_perplexity=5779.4707, train_loss=8.662067

Batch 24160, train_perplexity=6164.1133, train_loss=8.7265

Batch 24170, train_perplexity=5319.005, train_loss=8.5790415

Batch 24180, train_perplexity=4789.906, train_loss=8.474266

Batch 24190, train_perplexity=4945.924, train_loss=8.506319

Batch 24200, train_perplexity=5136.1733, train_loss=8.544064

Batch 24210, train_perplexity=4647.7544, train_loss=8.4441395

Batch 24220, train_perplexity=5340.8516, train_loss=8.58314

Batch 24230, train_perplexity=5172.637, train_loss=8.551138

Batch 24240, train_perplexity=4930.1523, train_loss=8.503125

Batch 24250, train_perplexity=5645.5034, train_loss=8.638615

Batch 24260, train_perplexity=4782.411, train_loss=8.4727

Batch 24270, train_perplexity=4540.687, train_loss=8.420834

Batch 24280, train_perplexity=6314.6616, train_loss=8.750629

Batch 24290, train_perplexity=4277.247, train_loss=8.361065

Batch 24300, train_perplexity=5337.0635, train_loss=8.582431

Batch 24310, train_perplexity=5918.1465, train_loss=8.685779

Batch 24320, train_perplexity=3753.965, train_loss=8.230568

Batch 24330, train_perplexity=5915.596, train_loss=8.685348

Batch 24340, train_perplexity=5811.5054, train_loss=8.667595

Batch 24350, train_perplexity=5444.513, train_loss=8.602364

Batch 24360, train_perplexity=5096.621, train_loss=8.536333

Batch 24370, train_perplexity=5578.768, train_loss=8.626723

Batch 24380, train_perplexity=5176.6147, train_loss=8.551907

Batch 24390, train_perplexity=5464.644, train_loss=8.606054

Batch 24400, train_perplexity=5975.1074, train_loss=8.695357

Batch 24410, train_perplexity=4774.0903, train_loss=8.470959

Batch 24420, train_perplexity=5472.9893, train_loss=8.60758

Batch 24430, train_perplexity=5724.3643, train_loss=8.652487

Batch 24440, train_perplexity=4995.175, train_loss=8.516228
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24450, train_perplexity=5736.261, train_loss=8.654563

Batch 24460, train_perplexity=5516.9453, train_loss=8.61558

Batch 24470, train_perplexity=5310.92, train_loss=8.57752

Batch 24480, train_perplexity=6130.9595, train_loss=8.721107

Batch 24490, train_perplexity=6087.467, train_loss=8.713987

Batch 24500, train_perplexity=4881.547, train_loss=8.493217

Batch 24510, train_perplexity=6108.6475, train_loss=8.717461

Batch 24520, train_perplexity=5189.714, train_loss=8.554434

Batch 24530, train_perplexity=5117.755, train_loss=8.540471

Batch 24540, train_perplexity=5339.95, train_loss=8.582972

Batch 24550, train_perplexity=6601.181, train_loss=8.795004

Batch 24560, train_perplexity=5816.39, train_loss=8.668435

Batch 24570, train_perplexity=6952.2446, train_loss=8.84682

Batch 24580, train_perplexity=4815.6655, train_loss=8.4796295

Batch 24590, train_perplexity=5287.0776, train_loss=8.573021

Batch 24600, train_perplexity=4733.4067, train_loss=8.4624

Batch 24610, train_perplexity=5280.577, train_loss=8.571791

Batch 24620, train_perplexity=6109.4805, train_loss=8.717597

Batch 24630, train_perplexity=4859.1177, train_loss=8.488612

Batch 24640, train_perplexity=5091.899, train_loss=8.535406

Batch 24650, train_perplexity=5152.982, train_loss=8.547331

Batch 24660, train_perplexity=8094.202, train_loss=8.998903

Batch 24670, train_perplexity=5741.899, train_loss=8.655545

Batch 24680, train_perplexity=5415.7925, train_loss=8.5970745

Batch 24690, train_perplexity=5287.355, train_loss=8.573073

Batch 24700, train_perplexity=5613.11, train_loss=8.63286

Batch 24710, train_perplexity=4762.7485, train_loss=8.46858

Batch 24720, train_perplexity=5168.495, train_loss=8.550337

Batch 24730, train_perplexity=6167.03, train_loss=8.726973

Batch 24740, train_perplexity=5402.2363, train_loss=8.594568

Batch 24750, train_perplexity=4729.039, train_loss=8.461477

Batch 24760, train_perplexity=5939.9385, train_loss=8.689454

Batch 24770, train_perplexity=6011.734, train_loss=8.701468

Batch 24780, train_perplexity=7319.633, train_loss=8.898315

Batch 24790, train_perplexity=7018.335, train_loss=8.856281

Batch 24800, train_perplexity=5814.643, train_loss=8.668135

Batch 24810, train_perplexity=5882.41, train_loss=8.679722

Batch 24820, train_perplexity=4478.1753, train_loss=8.406971

Batch 24830, train_perplexity=5273.864, train_loss=8.5705185

Batch 24840, train_perplexity=5680.142, train_loss=8.6447315

Batch 24850, train_perplexity=5132.687, train_loss=8.543385

Batch 24860, train_perplexity=4523.243, train_loss=8.416985

Batch 24870, train_perplexity=4207.9, train_loss=8.344719

Batch 24880, train_perplexity=5270.9976, train_loss=8.569975

Batch 24890, train_perplexity=5834.173, train_loss=8.671488

Batch 24900, train_perplexity=5229.185, train_loss=8.562011

Batch 24910, train_perplexity=5799.237, train_loss=8.665482

Batch 24920, train_perplexity=5200.2266, train_loss=8.5564575

Batch 24930, train_perplexity=4769.039, train_loss=8.4699

Batch 24940, train_perplexity=6270.2427, train_loss=8.74357

Batch 24950, train_perplexity=6949.9844, train_loss=8.846495

Batch 24960, train_perplexity=5048.969, train_loss=8.526939

Batch 24970, train_perplexity=5407.4316, train_loss=8.59553

Batch 24980, train_perplexity=4517.8413, train_loss=8.41579

Batch 24990, train_perplexity=4740.535, train_loss=8.463905

Batch 25000, train_perplexity=4251.383, train_loss=8.355

Batch 25010, train_perplexity=4915.8003, train_loss=8.50021

Batch 25020, train_perplexity=5331.035, train_loss=8.581301

Batch 25030, train_perplexity=6168.806, train_loss=8.727261

Batch 25040, train_perplexity=5157.81, train_loss=8.548267

Batch 25050, train_perplexity=6522.558, train_loss=8.783022

Batch 25060, train_perplexity=5109.976, train_loss=8.53895

Batch 25070, train_perplexity=5109.6055, train_loss=8.5388775

Batch 25080, train_perplexity=5644.895, train_loss=8.638507

Batch 25090, train_perplexity=5650.4155, train_loss=8.639484

Batch 25100, train_perplexity=6032.0117, train_loss=8.704836

Batch 25110, train_perplexity=5796.66, train_loss=8.665037

Batch 25120, train_perplexity=5407.731, train_loss=8.595585

Batch 25130, train_perplexity=5947.608, train_loss=8.690744

Batch 25140, train_perplexity=4984.449, train_loss=8.514078

Batch 25150, train_perplexity=4367.015, train_loss=8.381835

Batch 25160, train_perplexity=4675.812, train_loss=8.450158

Batch 25170, train_perplexity=5615.6694, train_loss=8.633316

Batch 25180, train_perplexity=4441.7837, train_loss=8.398811

Batch 25190, train_perplexity=5696.525, train_loss=8.647612

Batch 25200, train_perplexity=4866.0044, train_loss=8.490028

Batch 25210, train_perplexity=4783.8936, train_loss=8.47301

Batch 25220, train_perplexity=5548.3555, train_loss=8.621257

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Batch 25230, train_perplexity=5120.7427, train_loss=8.541055

Batch 25240, train_perplexity=5220.6147, train_loss=8.56037

Batch 25250, train_perplexity=5618.894, train_loss=8.63389

Batch 25260, train_perplexity=5275.6846, train_loss=8.570864

Batch 25270, train_perplexity=5249.907, train_loss=8.565966

Batch 25280, train_perplexity=4524.887, train_loss=8.417348

Batch 25290, train_perplexity=5063.657, train_loss=8.529844

Batch 25300, train_perplexity=5436.5435, train_loss=8.600899

Batch 25310, train_perplexity=5851.0615, train_loss=8.674378

Batch 25320, train_perplexity=4984.3774, train_loss=8.514064

Batch 25330, train_perplexity=4431.891, train_loss=8.396582

Batch 25340, train_perplexity=5676.173, train_loss=8.6440325

Batch 25350, train_perplexity=5251.254, train_loss=8.566222

Batch 25360, train_perplexity=5558.667, train_loss=8.623114

Batch 25370, train_perplexity=6535.229, train_loss=8.784963

Batch 25380, train_perplexity=6281.0215, train_loss=8.745288

Batch 25390, train_perplexity=6407.6895, train_loss=8.765254

Batch 25400, train_perplexity=5104.764, train_loss=8.53793

Batch 25410, train_perplexity=5473.8867, train_loss=8.607744

Batch 25420, train_perplexity=5452.484, train_loss=8.6038265

Batch 25430, train_perplexity=5880.189, train_loss=8.679344

Batch 25440, train_perplexity=5398.6206, train_loss=8.593899

Batch 25450, train_perplexity=4697.758, train_loss=8.454841

Batch 25460, train_perplexity=5622.185, train_loss=8.634476

Batch 25470, train_perplexity=5345.667, train_loss=8.584042

Batch 25480, train_perplexity=4728.4707, train_loss=8.461357

Batch 25490, train_perplexity=4819.341, train_loss=8.480392

Batch 25500, train_perplexity=5157.2246, train_loss=8.548154

Batch 25510, train_perplexity=5333.659, train_loss=8.581793

Batch 25520, train_perplexity=5825.411, train_loss=8.669985

Batch 25530, train_perplexity=6151.4053, train_loss=8.724436

Batch 25540, train_perplexity=5519.156, train_loss=8.61598

Batch 25550, train_perplexity=4145.077, train_loss=8.329677

Batch 25560, train_perplexity=5979.4453, train_loss=8.696083

Batch 25570, train_perplexity=5440.5527, train_loss=8.601636

Batch 25580, train_perplexity=5839.4834, train_loss=8.672398

Batch 25590, train_perplexity=4892.397, train_loss=8.495438

Batch 25600, train_perplexity=4919.693, train_loss=8.501001

Batch 25610, train_perplexity=4731.9985, train_loss=8.462103

Batch 25620, train_perplexity=5762.443, train_loss=8.659117

Batch 25630, train_perplexity=5073.32, train_loss=8.531751

Batch 25640, train_perplexity=6041.3267, train_loss=8.706379

Batch 25650, train_perplexity=4785.6274, train_loss=8.473372

Batch 25660, train_perplexity=6375.11, train_loss=8.760157

Batch 25670, train_perplexity=5655.99, train_loss=8.6404705

Batch 25680, train_perplexity=5338.4224, train_loss=8.582685

Batch 25690, train_perplexity=5739.605, train_loss=8.655146

Batch 25700, train_perplexity=6386.2275, train_loss=8.761899

Batch 25710, train_perplexity=4529.049, train_loss=8.418267

Batch 25720, train_perplexity=5667.5884, train_loss=8.642519
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 25730, train_perplexity=5226.7773, train_loss=8.56155

Batch 25740, train_perplexity=5135.0073, train_loss=8.543837

Batch 25750, train_perplexity=4854.9814, train_loss=8.487761

Batch 25760, train_perplexity=5086.017, train_loss=8.53425

Batch 25770, train_perplexity=4840.5156, train_loss=8.4847765

Batch 25780, train_perplexity=5957.372, train_loss=8.692385

Batch 25790, train_perplexity=5534.3774, train_loss=8.618734

Batch 25800, train_perplexity=5966.2188, train_loss=8.693869

Batch 25810, train_perplexity=4681.9834, train_loss=8.451477

Batch 25820, train_perplexity=5527.204, train_loss=8.617437

Batch 25830, train_perplexity=5666.9346, train_loss=8.642404

Batch 25840, train_perplexity=5292.647, train_loss=8.574074

Batch 25850, train_perplexity=4983.4604, train_loss=8.51388

Batch 25860, train_perplexity=5737.8535, train_loss=8.65484

Batch 25870, train_perplexity=5125.77, train_loss=8.542036

Batch 25880, train_perplexity=5059.501, train_loss=8.529023

Batch 25890, train_perplexity=6183.991, train_loss=8.729719

Batch 25900, train_perplexity=6380.049, train_loss=8.760931

Batch 25910, train_perplexity=5653.8604, train_loss=8.640094

Batch 25920, train_perplexity=5525.0854, train_loss=8.617054

Batch 25930, train_perplexity=4808.103, train_loss=8.478058

Batch 25940, train_perplexity=5236.616, train_loss=8.563431

Batch 25950, train_perplexity=6251.208, train_loss=8.74053

Batch 25960, train_perplexity=6000.4785, train_loss=8.6995945

Batch 25970, train_perplexity=4982.619, train_loss=8.513711

Batch 25980, train_perplexity=5157.2886, train_loss=8.548166

Batch 25990, train_perplexity=4644.458, train_loss=8.44343

Batch 26000, train_perplexity=5412.5293, train_loss=8.596472

Batch 26010, train_perplexity=6001.406, train_loss=8.699749

Batch 26020, train_perplexity=6591.745, train_loss=8.793573

Batch 26030, train_perplexity=4270.001, train_loss=8.359369

Batch 26040, train_perplexity=5076.1123, train_loss=8.532301

Batch 26050, train_perplexity=6500.5938, train_loss=8.779649

Batch 26060, train_perplexity=4937.436, train_loss=8.5046015

Batch 26070, train_perplexity=4803.767, train_loss=8.477156

Batch 26080, train_perplexity=5584.5596, train_loss=8.627761

Batch 26090, train_perplexity=5175.292, train_loss=8.551651

Batch 26100, train_perplexity=5889.5503, train_loss=8.680935

Batch 26110, train_perplexity=4702.025, train_loss=8.455749

Batch 26120, train_perplexity=4905.806, train_loss=8.498175

Batch 26130, train_perplexity=5034.607, train_loss=8.524091

Batch 26140, train_perplexity=6174.8096, train_loss=8.728233

Batch 26150, train_perplexity=6050.852, train_loss=8.707954

Batch 26160, train_perplexity=5039.252, train_loss=8.525013

Batch 26170, train_perplexity=6325.173, train_loss=8.752293

Batch 26180, train_perplexity=5227.7344, train_loss=8.561733

Batch 26190, train_perplexity=4958.458, train_loss=8.50885

Batch 26200, train_perplexity=5436.647, train_loss=8.600918

Batch 26210, train_perplexity=5056.342, train_loss=8.5283985

Batch 26220, train_perplexity=6432.063, train_loss=8.769051

Batch 26230, train_perplexity=4923.992, train_loss=8.501875

Batch 26240, train_perplexity=4546.7017, train_loss=8.422157

Batch 26250, train_perplexity=6282.345, train_loss=8.745499

Batch 26260, train_perplexity=4731.3125, train_loss=8.461958

Batch 26270, train_perplexity=4693.4277, train_loss=8.453918

Batch 26280, train_perplexity=5586.0244, train_loss=8.628023

Batch 26290, train_perplexity=5884.329, train_loss=8.680048

Batch 26300, train_perplexity=5383.536, train_loss=8.591101

Batch 26310, train_perplexity=6708.896, train_loss=8.81119

Batch 26320, train_perplexity=5793.1284, train_loss=8.664428

Batch 26330, train_perplexity=5096.5825, train_loss=8.536325

Batch 26340, train_perplexity=4845.448, train_loss=8.485795

Batch 26350, train_perplexity=4638.8096, train_loss=8.442213

Batch 26360, train_perplexity=4661.053, train_loss=8.446997

Batch 26370, train_perplexity=4995.394, train_loss=8.516272

Batch 26380, train_perplexity=4637.8145, train_loss=8.4419985

Batch 26390, train_perplexity=5302.489, train_loss=8.575932

Batch 26400, train_perplexity=5314.6084, train_loss=8.578215

Batch 26410, train_perplexity=6025.4863, train_loss=8.703753

Batch 26420, train_perplexity=5839.333, train_loss=8.672372

Batch 26430, train_perplexity=4792.026, train_loss=8.474709

Batch 26440, train_perplexity=5743.854, train_loss=8.655886

Batch 26450, train_perplexity=5865.8506, train_loss=8.676903

Batch 26460, train_perplexity=5841.288, train_loss=8.672707

Batch 26470, train_perplexity=4828.749, train_loss=8.482343

Batch 26480, train_perplexity=6035.499, train_loss=8.705414

Batch 26490, train_perplexity=5385.092, train_loss=8.59139

Batch 26500, train_perplexity=5305.291, train_loss=8.57646

Batch 26510, train_perplexity=5697.3184, train_loss=8.647751

Batch 26520, train_perplexity=5127.0854, train_loss=8.542293

Batch 26530, train_perplexity=4891.3237, train_loss=8.495218

Batch 26540, train_perplexity=5161.9434, train_loss=8.549068

Batch 26550, train_perplexity=5003.9287, train_loss=8.517979

Batch 26560, train_perplexity=6300.7837, train_loss=8.748429

Batch 26570, train_perplexity=4388.487, train_loss=8.38674

Batch 26580, train_perplexity=4998.6777, train_loss=8.516929

Batch 26590, train_perplexity=4800.3916, train_loss=8.476453

Batch 26600, train_perplexity=5973.187, train_loss=8.695036

Batch 26610, train_perplexity=5390.7183, train_loss=8.592434

Batch 26620, train_perplexity=5947.205, train_loss=8.690677

Batch 26630, train_perplexity=4754.0947, train_loss=8.466762

Batch 26640, train_perplexity=5408.2466, train_loss=8.59568

Batch 26650, train_perplexity=5528.754, train_loss=8.617718

Batch 26660, train_perplexity=5351.242, train_loss=8.585084

Batch 26670, train_perplexity=5235.013, train_loss=8.563125

Batch 26680, train_perplexity=5222.6016, train_loss=8.560751

Batch 26690, train_perplexity=4960.3545, train_loss=8.5092325

Batch 26700, train_perplexity=5705.3545, train_loss=8.64916

Batch 26710, train_perplexity=5118.223, train_loss=8.540563

Batch 26720, train_perplexity=6862.048, train_loss=8.833761

Batch 26730, train_perplexity=5338.881, train_loss=8.582771

Batch 26740, train_perplexity=5155.13, train_loss=8.547748

Batch 26750, train_perplexity=4585.4917, train_loss=8.430653

Batch 26760, train_perplexity=4293.8364, train_loss=8.364936

Batch 26770, train_perplexity=5021.722, train_loss=8.521528

Batch 26780, train_perplexity=6263.7163, train_loss=8.742529

Batch 26790, train_perplexity=6256.898, train_loss=8.74144

Batch 26800, train_perplexity=6902.1826, train_loss=8.839593

Batch 26810, train_perplexity=6280.9497, train_loss=8.745276

Batch 26820, train_perplexity=5468.0796, train_loss=8.606683

Batch 26830, train_perplexity=5979.2344, train_loss=8.696048

Batch 26840, train_perplexity=5487.623, train_loss=8.61025

Batch 26850, train_perplexity=5158.853, train_loss=8.54847

Batch 26860, train_perplexity=6069.381, train_loss=8.711012

Batch 26870, train_perplexity=5093.8125, train_loss=8.535782

Batch 26880, train_perplexity=6398.511, train_loss=8.763821

Batch 26890, train_perplexity=5723.4473, train_loss=8.652327

Batch 26900, train_perplexity=5666.767, train_loss=8.642374

Batch 26910, train_perplexity=5615.498, train_loss=8.6332855

Batch 26920, train_perplexity=5176.3926, train_loss=8.551864

Batch 26930, train_perplexity=6354.3267, train_loss=8.756891

Batch 26940, train_perplexity=5677.38, train_loss=8.644245

Batch 26950, train_perplexity=5767.545, train_loss=8.660002

Batch 26960, train_perplexity=5565.977, train_loss=8.624428

Batch 26970, train_perplexity=5306.657, train_loss=8.576717

Batch 26980, train_perplexity=6072.878, train_loss=8.711588

Batch 26990, train_perplexity=5025.7227, train_loss=8.522325

Batch 27000, train_perplexity=5106.4634, train_loss=8.538262

Batch 27010, train_perplexity=5090.3257, train_loss=8.535097

Batch 27020, train_perplexity=4769.6123, train_loss=8.47002

Batch 27030, train_perplexity=6455.618, train_loss=8.772706

Batch 27040, train_perplexity=4697.6104, train_loss=8.454809

Batch 27050, train_perplexity=5217.8027, train_loss=8.559832

Batch 27060, train_perplexity=5024.357, train_loss=8.522053
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 27070, train_perplexity=6056.747, train_loss=8.708928

Batch 27080, train_perplexity=6006.6733, train_loss=8.700626

Batch 27090, train_perplexity=5660.89, train_loss=8.641336

Batch 27100, train_perplexity=4839.634, train_loss=8.484594

Batch 27110, train_perplexity=5408.128, train_loss=8.595658

Batch 27120, train_perplexity=5093.8223, train_loss=8.535784

Batch 27130, train_perplexity=6440.153, train_loss=8.770308

Batch 27140, train_perplexity=5088.0933, train_loss=8.534658

Batch 27150, train_perplexity=5688.8267, train_loss=8.646259

Batch 27160, train_perplexity=5405.555, train_loss=8.595182

Batch 27170, train_perplexity=5727.3022, train_loss=8.653

Batch 27180, train_perplexity=4316.113, train_loss=8.3701105

Batch 27190, train_perplexity=5314.583, train_loss=8.57821

Batch 27200, train_perplexity=6105.584, train_loss=8.716959

Batch 27210, train_perplexity=5482.392, train_loss=8.609297

Batch 27220, train_perplexity=5775.9663, train_loss=8.661461

Batch 27230, train_perplexity=4550.445, train_loss=8.42298

Batch 27240, train_perplexity=5743.635, train_loss=8.655848

Batch 27250, train_perplexity=4986.8027, train_loss=8.51455

Batch 27260, train_perplexity=6005.8145, train_loss=8.700483

Batch 27270, train_perplexity=5193.021, train_loss=8.555071

Batch 27280, train_perplexity=5372.765, train_loss=8.589098

Batch 27290, train_perplexity=4200.106, train_loss=8.342865

Batch 27300, train_perplexity=4674.2026, train_loss=8.449814

Batch 27310, train_perplexity=4950.6855, train_loss=8.507281

Batch 27320, train_perplexity=6998.6445, train_loss=8.853472

Batch 27330, train_perplexity=5295.8633, train_loss=8.574681

Batch 27340, train_perplexity=5131.1304, train_loss=8.543081

Batch 27350, train_perplexity=5089.9717, train_loss=8.5350275

Batch 27360, train_perplexity=5686.8413, train_loss=8.64591

Batch 27370, train_perplexity=4844.436, train_loss=8.485586

Batch 27380, train_perplexity=4869.0215, train_loss=8.490648

Batch 27390, train_perplexity=5232.1035, train_loss=8.562569

Batch 27400, train_perplexity=4354.3022, train_loss=8.37892

Batch 27410, train_perplexity=5447.0786, train_loss=8.602835

Batch 27420, train_perplexity=6436.2046, train_loss=8.769694

Batch 27430, train_perplexity=5671.395, train_loss=8.64319

Batch 27440, train_perplexity=5481.629, train_loss=8.609158

Batch 27450, train_perplexity=5074.8926, train_loss=8.532061

Batch 27460, train_perplexity=5397.9307, train_loss=8.593771

Batch 27470, train_perplexity=5799.9336, train_loss=8.665602

Batch 27480, train_perplexity=6168.8765, train_loss=8.727272

Batch 27490, train_perplexity=5618.0312, train_loss=8.633737

Batch 27500, train_perplexity=5261.591, train_loss=8.568189

Batch 27510, train_perplexity=4729.8555, train_loss=8.46165

Batch 27520, train_perplexity=6950.1963, train_loss=8.846525

Batch 27530, train_perplexity=4282.145, train_loss=8.362209

Batch 27540, train_perplexity=5620.4536, train_loss=8.634168

Batch 27550, train_perplexity=5458.212, train_loss=8.6048765

Batch 27560, train_perplexity=5096.9272, train_loss=8.536393

Batch 27570, train_perplexity=4909.78, train_loss=8.498984

Batch 27580, train_perplexity=4816.2715, train_loss=8.479755

Batch 27590, train_perplexity=4569.2266, train_loss=8.427099

Batch 27600, train_perplexity=5012.9756, train_loss=8.519785

Batch 27610, train_perplexity=5789.682, train_loss=8.663833

Batch 27620, train_perplexity=4812.158, train_loss=8.478901

Batch 27630, train_perplexity=6182.688, train_loss=8.729508

Batch 27640, train_perplexity=5560.5547, train_loss=8.623453

Batch 27650, train_perplexity=4823.3687, train_loss=8.481228

Batch 27660, train_perplexity=5558.6987, train_loss=8.623119

Batch 27670, train_perplexity=5267.952, train_loss=8.569397

Batch 27680, train_perplexity=6469.954, train_loss=8.774924

Batch 27690, train_perplexity=5952.4087, train_loss=8.691551

Batch 27700, train_perplexity=5051.922, train_loss=8.527524

Batch 27710, train_perplexity=5056.6504, train_loss=8.52846

Batch 27720, train_perplexity=5486.723, train_loss=8.610086

Batch 27730, train_perplexity=4314.8374, train_loss=8.369815

Batch 27740, train_perplexity=4832.282, train_loss=8.483074

Batch 27750, train_perplexity=6154.926, train_loss=8.725008

Batch 27760, train_perplexity=4599.976, train_loss=8.433806

Batch 27770, train_perplexity=5058.0684, train_loss=8.52874

Batch 27780, train_perplexity=6147.775, train_loss=8.7238455

Batch 27790, train_perplexity=6920.4463, train_loss=8.842236

Batch 27800, train_perplexity=6523.7773, train_loss=8.783209

Batch 27810, train_perplexity=4992.8315, train_loss=8.5157585

Batch 27820, train_perplexity=4647.0186, train_loss=8.443981

Batch 27830, train_perplexity=5198.888, train_loss=8.5562

Batch 27840, train_perplexity=5899.618, train_loss=8.682643

Batch 27850, train_perplexity=5822.7446, train_loss=8.669527

Batch 27860, train_perplexity=5422.0405, train_loss=8.5982275

Batch 27870, train_perplexity=4828.671, train_loss=8.4823265

Batch 27880, train_perplexity=5721.1445, train_loss=8.651924

Batch 27890, train_perplexity=6757.0146, train_loss=8.8183365

Batch 27900, train_perplexity=6074.245, train_loss=8.711813

Batch 27910, train_perplexity=4762.967, train_loss=8.468626

Batch 27920, train_perplexity=4783.401, train_loss=8.472907

Batch 27930, train_perplexity=5038.0894, train_loss=8.524782

Batch 27940, train_perplexity=5596.102, train_loss=8.629826

Batch 27950, train_perplexity=5615.214, train_loss=8.633235

Batch 27960, train_perplexity=5498.509, train_loss=8.612232

Batch 27970, train_perplexity=5720.626, train_loss=8.651834

Batch 27980, train_perplexity=7035.021, train_loss=8.858656

Batch 27990, train_perplexity=4095.4768, train_loss=8.317638

Batch 28000, train_perplexity=5942.381, train_loss=8.689865

Batch 28010, train_perplexity=5087.817, train_loss=8.534604

Batch 28020, train_perplexity=4691.4854, train_loss=8.453505

Batch 28030, train_perplexity=6026.653, train_loss=8.703947

Batch 28040, train_perplexity=5525.433, train_loss=8.617117

Batch 28050, train_perplexity=5195.81, train_loss=8.555608

Batch 28060, train_perplexity=5519.9243, train_loss=8.616119

Batch 28070, train_perplexity=5897.16, train_loss=8.682226

Batch 28080, train_perplexity=5384.388, train_loss=8.591259

Batch 28090, train_perplexity=5582.3926, train_loss=8.627373

Batch 28100, train_perplexity=5514.105, train_loss=8.615065

Batch 28110, train_perplexity=4985.97, train_loss=8.514383

Batch 28120, train_perplexity=6626.992, train_loss=8.798906

Batch 28130, train_perplexity=5662.2725, train_loss=8.641581

Batch 28140, train_perplexity=4759.026, train_loss=8.467798

Batch 28150, train_perplexity=5439.7485, train_loss=8.601488

Batch 28160, train_perplexity=5189.526, train_loss=8.554398

Batch 28170, train_perplexity=5397.9517, train_loss=8.593775

Batch 28180, train_perplexity=6339.4307, train_loss=8.754544

Batch 28190, train_perplexity=4807.3003, train_loss=8.477891

Batch 28200, train_perplexity=4721.6753, train_loss=8.459919

Batch 28210, train_perplexity=4965.831, train_loss=8.510336

Batch 28220, train_perplexity=5534.8945, train_loss=8.618828

Batch 28230, train_perplexity=5831.8867, train_loss=8.671096

Batch 28240, train_perplexity=6260.6045, train_loss=8.742032

Batch 28250, train_perplexity=5093.191, train_loss=8.53566

Batch 28260, train_perplexity=5605.075, train_loss=8.631428

Batch 28270, train_perplexity=5433.5317, train_loss=8.600345

Batch 28280, train_perplexity=6197.9604, train_loss=8.731976

Batch 28290, train_perplexity=6978.3037, train_loss=8.850561

Batch 28300, train_perplexity=4542.3633, train_loss=8.421203

Batch 28310, train_perplexity=7264.659, train_loss=8.890777

Batch 28320, train_perplexity=4613.1206, train_loss=8.43666

Batch 28330, train_perplexity=5992.8496, train_loss=8.698322

Batch 28340, train_perplexity=4914.3003, train_loss=8.499905

Batch 28350, train_perplexity=5331.6455, train_loss=8.581415

Batch 28360, train_perplexity=4234.578, train_loss=8.351039

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305594 sentences.
Finished loading
Batch 28370, train_perplexity=5975.5005, train_loss=8.695423

Batch 28380, train_perplexity=5378.979, train_loss=8.590254

Batch 28390, train_perplexity=5585.2896, train_loss=8.627892

Batch 28400, train_perplexity=5316.636, train_loss=8.578596

Batch 28410, train_perplexity=4584.556, train_loss=8.430449

Batch 28420, train_perplexity=6623.2773, train_loss=8.798346

Batch 28430, train_perplexity=5162.111, train_loss=8.549101

Batch 28440, train_perplexity=5662.661, train_loss=8.641649

Batch 28450, train_perplexity=5954.5093, train_loss=8.691904

Batch 28460, train_perplexity=5135.5366, train_loss=8.54394

Batch 28470, train_perplexity=4564.8623, train_loss=8.426144

Batch 28480, train_perplexity=5933.507, train_loss=8.688371

Batch 28490, train_perplexity=5996.371, train_loss=8.69891

Batch 28500, train_perplexity=5045.7637, train_loss=8.526304

Batch 28510, train_perplexity=5305.807, train_loss=8.576557

Batch 28520, train_perplexity=5148.3203, train_loss=8.546426

Batch 28530, train_perplexity=6447.7114, train_loss=8.771481

Batch 28540, train_perplexity=4444.6396, train_loss=8.399454

Batch 28550, train_perplexity=5137.9272, train_loss=8.544405

Batch 28560, train_perplexity=5637.3364, train_loss=8.637167

Batch 28570, train_perplexity=5439.2036, train_loss=8.601388

Batch 28580, train_perplexity=5200.559, train_loss=8.556521

Batch 28590, train_perplexity=5637.842, train_loss=8.637257

Batch 28600, train_perplexity=5625.0117, train_loss=8.634978

Batch 28610, train_perplexity=5396.114, train_loss=8.593434

Batch 28620, train_perplexity=4749.495, train_loss=8.465794

Batch 28630, train_perplexity=6575.891, train_loss=8.791165

Batch 28640, train_perplexity=5395.2134, train_loss=8.593267

Batch 28650, train_perplexity=5236.701, train_loss=8.563447

Batch 28660, train_perplexity=4724.2607, train_loss=8.460466

Batch 28670, train_perplexity=4857.7974, train_loss=8.48834

Batch 28680, train_perplexity=5165.568, train_loss=8.54977

Batch 28690, train_perplexity=5693.212, train_loss=8.64703

Batch 28700, train_perplexity=5533.6807, train_loss=8.618608

Batch 28710, train_perplexity=5332.24, train_loss=8.581527

Batch 28720, train_perplexity=5061.475, train_loss=8.529413

Batch 28730, train_perplexity=4251.9746, train_loss=8.355139

Batch 28740, train_perplexity=5310.449, train_loss=8.577432

Batch 28750, train_perplexity=4778.0713, train_loss=8.471792

Batch 28760, train_perplexity=4872.575, train_loss=8.491378

Batch 28770, train_perplexity=5453.857, train_loss=8.604078

Batch 28780, train_perplexity=4530.816, train_loss=8.418657

Batch 28790, train_perplexity=3970.6836, train_loss=8.286694

Batch 28800, train_perplexity=5735.0415, train_loss=8.65435

Batch 28810, train_perplexity=4612.5356, train_loss=8.436533

Batch 28820, train_perplexity=5632.758, train_loss=8.636354

Batch 28830, train_perplexity=4699.698, train_loss=8.455254

Batch 28840, train_perplexity=4021.5408, train_loss=8.29942

Batch 28850, train_perplexity=4711.1235, train_loss=8.457682

Batch 28860, train_perplexity=4485.125, train_loss=8.408522

Batch 28870, train_perplexity=6345.0015, train_loss=8.755423

Batch 28880, train_perplexity=5482.1514, train_loss=8.609253

Batch 28890, train_perplexity=5641.951, train_loss=8.637985

Batch 28900, train_perplexity=4726.1533, train_loss=8.460867

Batch 28910, train_perplexity=4771.764, train_loss=8.470471

Batch 28920, train_perplexity=5549.2866, train_loss=8.621425

Batch 28930, train_perplexity=5630.518, train_loss=8.635957

Batch 28940, train_perplexity=6230.478, train_loss=8.737208

Batch 28950, train_perplexity=4909.0728, train_loss=8.49884

Batch 28960, train_perplexity=4994.0317, train_loss=8.515999

Batch 28970, train_perplexity=5310.2617, train_loss=8.577396

Batch 28980, train_perplexity=4096.4297, train_loss=8.317871

Batch 28990, train_perplexity=4827.423, train_loss=8.482068

Batch 29000, train_perplexity=5317.0625, train_loss=8.578676

Batch 29010, train_perplexity=7088.169, train_loss=8.866182

Batch 29020, train_perplexity=4965.509, train_loss=8.510271

Batch 29030, train_perplexity=5521.293, train_loss=8.616367

Batch 29040, train_perplexity=5609.7275, train_loss=8.632257

Batch 29050, train_perplexity=4893.7783, train_loss=8.49572

Batch 29060, train_perplexity=3272.637, train_loss=8.093351

Batch 29070, train_perplexity=5827.9893, train_loss=8.670427

Batch 29080, train_perplexity=6021.988, train_loss=8.703173

Batch 29090, train_perplexity=4794.5674, train_loss=8.475239

Batch 29100, train_perplexity=4734.111, train_loss=8.462549

Batch 29110, train_perplexity=4425.6484, train_loss=8.395172

Batch 29120, train_perplexity=5679.9795, train_loss=8.644703

Batch 29130, train_perplexity=4958.6475, train_loss=8.508888

Batch 29140, train_perplexity=4727.9023, train_loss=8.461237

Batch 29150, train_perplexity=5602.7397, train_loss=8.631011

Batch 29160, train_perplexity=5067.754, train_loss=8.530653

Batch 29170, train_perplexity=5197.0537, train_loss=8.555847

Batch 29180, train_perplexity=4989.357, train_loss=8.515062

Batch 29190, train_perplexity=4915.9595, train_loss=8.500242

Batch 29200, train_perplexity=4853.5464, train_loss=8.487465

Batch 29210, train_perplexity=4861.8203, train_loss=8.489168

Batch 29220, train_perplexity=5571.4844, train_loss=8.625417

Batch 29230, train_perplexity=4892.2334, train_loss=8.495404

Batch 29240, train_perplexity=5410.687, train_loss=8.596131

Batch 29250, train_perplexity=5571.277, train_loss=8.62538

Batch 29260, train_perplexity=5072.628, train_loss=8.531614

Batch 29270, train_perplexity=6221.7676, train_loss=8.735809

Batch 29280, train_perplexity=5728.5693, train_loss=8.653221

Batch 29290, train_perplexity=5838.6816, train_loss=8.67226

Batch 29300, train_perplexity=5839.0044, train_loss=8.672316

Batch 29310, train_perplexity=5592.709, train_loss=8.629219

Batch 29320, train_perplexity=5803.934, train_loss=8.666291

Batch 29330, train_perplexity=5618.626, train_loss=8.633842

Batch 29340, train_perplexity=4332.002, train_loss=8.373785

Batch 29350, train_perplexity=5925.686, train_loss=8.687052

Batch 29360, train_perplexity=7061.545, train_loss=8.862419

Batch 29370, train_perplexity=6378.7104, train_loss=8.760721

Batch 29380, train_perplexity=4812.3877, train_loss=8.478949

Batch 29390, train_perplexity=5489.382, train_loss=8.610571

Batch 29400, train_perplexity=5583.4307, train_loss=8.627559

Batch 29410, train_perplexity=5231.6196, train_loss=8.562476

Batch 29420, train_perplexity=5389.485, train_loss=8.592205

Batch 29430, train_perplexity=6001.509, train_loss=8.699766

Batch 29440, train_perplexity=5464.415, train_loss=8.606012

Batch 29450, train_perplexity=5734.7354, train_loss=8.654297

Batch 29460, train_perplexity=5433.2676, train_loss=8.600296

Batch 29470, train_perplexity=4407.1084, train_loss=8.390974

Batch 29480, train_perplexity=5815.6855, train_loss=8.668314

Batch 29490, train_perplexity=5523.3467, train_loss=8.616739

Batch 29500, train_perplexity=5001.9966, train_loss=8.517592

Batch 29510, train_perplexity=5315.8047, train_loss=8.57844

Batch 29520, train_perplexity=6368.3286, train_loss=8.759092

Batch 29530, train_perplexity=6263.794, train_loss=8.742541

Batch 29540, train_perplexity=6725.205, train_loss=8.813618

Batch 29550, train_perplexity=5923.7764, train_loss=8.686729

Batch 29560, train_perplexity=5812.0596, train_loss=8.66769

Batch 29570, train_perplexity=5664.487, train_loss=8.641972

Batch 29580, train_perplexity=5232.837, train_loss=8.562709

Batch 29590, train_perplexity=6166.63, train_loss=8.726908

Batch 29600, train_perplexity=5095.1196, train_loss=8.536038

Batch 29610, train_perplexity=5276.1074, train_loss=8.570944

Batch 29620, train_perplexity=5912.5503, train_loss=8.684833

Batch 29630, train_perplexity=5404.5137, train_loss=8.59499

Batch 29640, train_perplexity=7023.8257, train_loss=8.857063

Batch 29650, train_perplexity=4847.126, train_loss=8.486141

Batch 29660, train_perplexity=4652.2603, train_loss=8.445108

Batch 29670, train_perplexity=5381.339, train_loss=8.5906925

Batch 29680, train_perplexity=5365.6274, train_loss=8.587769

Batch 29690, train_perplexity=4732.2017, train_loss=8.462146
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 29700, train_perplexity=4985.4946, train_loss=8.514288

Batch 29710, train_perplexity=5363.274, train_loss=8.58733

Batch 29720, train_perplexity=6402.9307, train_loss=8.764511

Batch 29730, train_perplexity=5844.2188, train_loss=8.673208

Batch 29740, train_perplexity=4988.301, train_loss=8.514851

Batch 29750, train_perplexity=5091.7437, train_loss=8.535376

Batch 29760, train_perplexity=5096.563, train_loss=8.536322

Batch 29770, train_perplexity=4716.734, train_loss=8.458872

Batch 29780, train_perplexity=6427.985, train_loss=8.768416

Batch 29790, train_perplexity=5105.5044, train_loss=8.5380745

Batch 29800, train_perplexity=5446.289, train_loss=8.60269

Batch 29810, train_perplexity=5364.9414, train_loss=8.587641

Batch 29820, train_perplexity=6089.064, train_loss=8.71425

Batch 29830, train_perplexity=5274.3364, train_loss=8.570608

Batch 29840, train_perplexity=5087.87, train_loss=8.534615

Batch 29850, train_perplexity=5189.001, train_loss=8.5542965

Batch 29860, train_perplexity=4872.7236, train_loss=8.491408

Batch 29870, train_perplexity=5419.859, train_loss=8.597825

Batch 29880, train_perplexity=4550.1196, train_loss=8.422909

Batch 29890, train_perplexity=6437.7026, train_loss=8.769927

Batch 29900, train_perplexity=5743.1475, train_loss=8.655763

Batch 29910, train_perplexity=5057.8945, train_loss=8.528706

Batch 29920, train_perplexity=5171.7246, train_loss=8.5509615

Batch 29930, train_perplexity=5914.688, train_loss=8.685194

Batch 29940, train_perplexity=5339.502, train_loss=8.582888

Batch 29950, train_perplexity=5138.927, train_loss=8.5446

Batch 29960, train_perplexity=5042.122, train_loss=8.525582

Batch 29970, train_perplexity=8906.289, train_loss=9.094513

Batch 29980, train_perplexity=5585.806, train_loss=8.627984

Batch 29990, train_perplexity=5173.8706, train_loss=8.551376

Batch 30000, train_perplexity=5580.604, train_loss=8.627052

Batch 30010, train_perplexity=5297.4243, train_loss=8.574976

Batch 30020, train_perplexity=5262.243, train_loss=8.568313

Batch 30030, train_perplexity=5525.776, train_loss=8.617179

Batch 30040, train_perplexity=5963.5454, train_loss=8.69342

Batch 30050, train_perplexity=4823.2676, train_loss=8.481207

Batch 30060, train_perplexity=4971.1567, train_loss=8.511408

Batch 30070, train_perplexity=5519.5713, train_loss=8.6160555

Batch 30080, train_perplexity=5581.7007, train_loss=8.627249

Batch 30090, train_perplexity=5428.4043, train_loss=8.5994005

Batch 30100, train_perplexity=6041.7417, train_loss=8.706448

Batch 30110, train_perplexity=4931.187, train_loss=8.503335

Batch 30120, train_perplexity=4628.893, train_loss=8.440073

Batch 30130, train_perplexity=6651.851, train_loss=8.80265

Batch 30140, train_perplexity=6412.672, train_loss=8.766031

Batch 30150, train_perplexity=5455.594, train_loss=8.604397

Batch 30160, train_perplexity=5059.747, train_loss=8.529072

Batch 30170, train_perplexity=5936.79, train_loss=8.688924

Batch 30180, train_perplexity=6768.3657, train_loss=8.820015

Batch 30190, train_perplexity=4098.798, train_loss=8.318449

Batch 30200, train_perplexity=5685.366, train_loss=8.645651

Batch 30210, train_perplexity=4438.079, train_loss=8.397977

Batch 30220, train_perplexity=5672.1523, train_loss=8.643324

Batch 30230, train_perplexity=5658.44, train_loss=8.640903

Batch 30240, train_perplexity=5808.746, train_loss=8.66712

Batch 30250, train_perplexity=4899.8584, train_loss=8.496962

Batch 30260, train_perplexity=5109.635, train_loss=8.538883

Batch 30270, train_perplexity=4967.2754, train_loss=8.510627

Batch 30280, train_perplexity=5404.287, train_loss=8.594948

Batch 30290, train_perplexity=4906.4985, train_loss=8.498316

Batch 30300, train_perplexity=4675.036, train_loss=8.449992

Batch 30310, train_perplexity=5710.341, train_loss=8.650034

Batch 30320, train_perplexity=4436.915, train_loss=8.397715

Batch 30330, train_perplexity=5476.226, train_loss=8.608171

Batch 30340, train_perplexity=6213.537, train_loss=8.734486

Batch 30350, train_perplexity=5529.8506, train_loss=8.617916

Batch 30360, train_perplexity=5700.476, train_loss=8.648305

Batch 30370, train_perplexity=5349.5835, train_loss=8.584774

Batch 30380, train_perplexity=5638.88, train_loss=8.637441

Batch 30390, train_perplexity=5268.505, train_loss=8.569502

Batch 30400, train_perplexity=4323.376, train_loss=8.371792

Batch 30410, train_perplexity=5439.4683, train_loss=8.601437

Batch 30420, train_perplexity=4319.189, train_loss=8.370823

Batch 30430, train_perplexity=4888.563, train_loss=8.494654

Batch 30440, train_perplexity=5194.19, train_loss=8.555296

Batch 30450, train_perplexity=6226.944, train_loss=8.736641

Batch 30460, train_perplexity=5152.2695, train_loss=8.547193

Batch 30470, train_perplexity=4822.5547, train_loss=8.481059

Batch 30480, train_perplexity=4811.8643, train_loss=8.47884

Batch 30490, train_perplexity=6039.5527, train_loss=8.706085

Batch 30500, train_perplexity=5185.1426, train_loss=8.553553

Batch 30510, train_perplexity=4899.12, train_loss=8.496811

Batch 30520, train_perplexity=5543.981, train_loss=8.620468

Batch 30530, train_perplexity=6373.317, train_loss=8.759875

Batch 30540, train_perplexity=5922.003, train_loss=8.68643

Batch 30550, train_perplexity=4652.8325, train_loss=8.445231

Batch 30560, train_perplexity=4898.527, train_loss=8.49669

Batch 30570, train_perplexity=4879.881, train_loss=8.492876

Batch 30580, train_perplexity=5541.433, train_loss=8.620008

Batch 30590, train_perplexity=6571.1577, train_loss=8.790445

Batch 30600, train_perplexity=5308.6816, train_loss=8.577099

Batch 30610, train_perplexity=6118.1504, train_loss=8.719015

Batch 30620, train_perplexity=4132.0327, train_loss=8.326525

Batch 30630, train_perplexity=5592.864, train_loss=8.629247

Batch 30640, train_perplexity=4936.424, train_loss=8.504396

Batch 30650, train_perplexity=4963.213, train_loss=8.509809

Batch 30660, train_perplexity=5774.986, train_loss=8.661291

Batch 30670, train_perplexity=5313.96, train_loss=8.578093

Batch 30680, train_perplexity=5370.89, train_loss=8.588749

Batch 30690, train_perplexity=5950.7285, train_loss=8.691269

Batch 30700, train_perplexity=5614.855, train_loss=8.633171

Batch 30710, train_perplexity=5676.6763, train_loss=8.644121

Batch 30720, train_perplexity=4869.913, train_loss=8.490831

Batch 30730, train_perplexity=5861.701, train_loss=8.676195

Batch 30740, train_perplexity=5406.849, train_loss=8.595422

Batch 30750, train_perplexity=5047.2363, train_loss=8.526596

Batch 30760, train_perplexity=6051.6313, train_loss=8.708083

Batch 30770, train_perplexity=6124.829, train_loss=8.720106

Batch 30780, train_perplexity=5714.737, train_loss=8.650804

Batch 30790, train_perplexity=5794.261, train_loss=8.664623

Batch 30800, train_perplexity=5364.532, train_loss=8.587564

Batch 30810, train_perplexity=5337.969, train_loss=8.582601

Batch 30820, train_perplexity=5862.204, train_loss=8.676281

Batch 30830, train_perplexity=5653.7417, train_loss=8.640073

Batch 30840, train_perplexity=4766.543, train_loss=8.469377

Batch 30850, train_perplexity=4550.1455, train_loss=8.4229145

Batch 30860, train_perplexity=5927.687, train_loss=8.687389

Batch 30870, train_perplexity=6028.964, train_loss=8.70433

Batch 30880, train_perplexity=5267.369, train_loss=8.569286

Batch 30890, train_perplexity=5441.0454, train_loss=8.601727

Batch 30900, train_perplexity=4445.81, train_loss=8.399717

Batch 30910, train_perplexity=5491.885, train_loss=8.611027

Batch 30920, train_perplexity=4701.393, train_loss=8.455614

Batch 30930, train_perplexity=6084.9014, train_loss=8.713566

Batch 30940, train_perplexity=6181.0786, train_loss=8.729248

Batch 30950, train_perplexity=4629.599, train_loss=8.440226

Batch 30960, train_perplexity=5399.5166, train_loss=8.594065

Batch 30970, train_perplexity=4852.5464, train_loss=8.487259

Batch 30980, train_perplexity=5311.5684, train_loss=8.577642

Batch 30990, train_perplexity=5694.222, train_loss=8.647207

Batch 31000, train_perplexity=4383.856, train_loss=8.385684

Batch 31010, train_perplexity=4695.205, train_loss=8.454297

Batch 31020, train_perplexity=5598.157, train_loss=8.630193

Batch 31030, train_perplexity=5660.037, train_loss=8.641186
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 31040, train_perplexity=5106.1323, train_loss=8.5381975

Batch 31050, train_perplexity=6367.363, train_loss=8.758941

Batch 31060, train_perplexity=4569.9326, train_loss=8.427254

Batch 31070, train_perplexity=4043.92, train_loss=8.30497

Batch 31080, train_perplexity=5922.6016, train_loss=8.686531

Batch 31090, train_perplexity=5163.573, train_loss=8.549384

Batch 31100, train_perplexity=4989.6665, train_loss=8.515124

Batch 31110, train_perplexity=5394.534, train_loss=8.593142

Batch 31120, train_perplexity=5412.6323, train_loss=8.596491

Batch 31130, train_perplexity=4894.9775, train_loss=8.495965

Batch 31140, train_perplexity=4809.818, train_loss=8.478415

Batch 31150, train_perplexity=4111.812, train_loss=8.321619

Batch 31160, train_perplexity=5867.339, train_loss=8.677156

Batch 31170, train_perplexity=4643.594, train_loss=8.443244

Batch 31180, train_perplexity=4858.4272, train_loss=8.48847

Batch 31190, train_perplexity=5206.112, train_loss=8.557589

Batch 31200, train_perplexity=4435.2314, train_loss=8.397335

Batch 31210, train_perplexity=5710.7383, train_loss=8.650104

Batch 31220, train_perplexity=4284.5713, train_loss=8.362776

Batch 31230, train_perplexity=4284.931, train_loss=8.36286

Batch 31240, train_perplexity=4861.4263, train_loss=8.489087

Batch 31250, train_perplexity=4938.345, train_loss=8.504786

Batch 31260, train_perplexity=5431.615, train_loss=8.599992

Batch 31270, train_perplexity=5354.6265, train_loss=8.585716

Batch 31280, train_perplexity=5407.066, train_loss=8.595462

Batch 31290, train_perplexity=5914.6484, train_loss=8.685187

Batch 31300, train_perplexity=5567.3306, train_loss=8.624671

Batch 31310, train_perplexity=5088.6074, train_loss=8.5347595

Batch 31320, train_perplexity=5970.3057, train_loss=8.694553

Batch 31330, train_perplexity=5442.665, train_loss=8.602024

Batch 31340, train_perplexity=5586.5254, train_loss=8.628113

Batch 31350, train_perplexity=4527.0757, train_loss=8.417831

Batch 31360, train_perplexity=6371.4937, train_loss=8.759589

Batch 31370, train_perplexity=4583.0435, train_loss=8.430119

Batch 31380, train_perplexity=5525.602, train_loss=8.617147

Batch 31390, train_perplexity=5241.4624, train_loss=8.564356

Batch 31400, train_perplexity=5558.9214, train_loss=8.623159

Batch 31410, train_perplexity=6205.874, train_loss=8.733252

Batch 31420, train_perplexity=5494.829, train_loss=8.611563

Batch 31430, train_perplexity=5442.2183, train_loss=8.601942

Batch 31440, train_perplexity=6301.3247, train_loss=8.748515

Batch 31450, train_perplexity=5039.1367, train_loss=8.52499

Batch 31460, train_perplexity=5124.9146, train_loss=8.541869

Batch 31470, train_perplexity=5491.1094, train_loss=8.610886

Batch 31480, train_perplexity=5878.664, train_loss=8.679085

Batch 31490, train_perplexity=5659.6323, train_loss=8.641114

Batch 31500, train_perplexity=4978.2114, train_loss=8.512826

Batch 31510, train_perplexity=6300.6694, train_loss=8.748411

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Batch 31520, train_perplexity=4813.627, train_loss=8.479206

Batch 31530, train_perplexity=5855.1196, train_loss=8.675072

Batch 31540, train_perplexity=5175.4004, train_loss=8.551672

Batch 31550, train_perplexity=6160.4463, train_loss=8.725904

Batch 31560, train_perplexity=5389.7983, train_loss=8.592263

Batch 31570, train_perplexity=5668.3613, train_loss=8.642655

Batch 31580, train_perplexity=5934.0674, train_loss=8.688465

Batch 31590, train_perplexity=4920.27, train_loss=8.501119

Batch 31600, train_perplexity=6182.6997, train_loss=8.72951

Batch 31610, train_perplexity=5150.717, train_loss=8.546891

Batch 31620, train_perplexity=5780.9316, train_loss=8.66232

Batch 31630, train_perplexity=6560.4316, train_loss=8.788812

Batch 31640, train_perplexity=5069.2285, train_loss=8.530944

Batch 31650, train_perplexity=5574.009, train_loss=8.62587

Batch 31660, train_perplexity=6610.076, train_loss=8.7963505

Batch 31670, train_perplexity=5803.8784, train_loss=8.666282

Batch 31680, train_perplexity=5086.924, train_loss=8.534429

Batch 31690, train_perplexity=5891.9155, train_loss=8.681336

Batch 31700, train_perplexity=5294.101, train_loss=8.574348

Batch 31710, train_perplexity=6034.7046, train_loss=8.705282

Batch 31720, train_perplexity=5831.147, train_loss=8.670969

Batch 31730, train_perplexity=5097.9775, train_loss=8.536599

Batch 31740, train_perplexity=5224.664, train_loss=8.561146

Batch 31750, train_perplexity=5346.9824, train_loss=8.584288

Batch 31760, train_perplexity=4833.6055, train_loss=8.483348

Batch 31770, train_perplexity=5726.5156, train_loss=8.652863

Batch 31780, train_perplexity=6028.021, train_loss=8.704174

Batch 31790, train_perplexity=5002.8125, train_loss=8.5177555

Batch 31800, train_perplexity=5483.1714, train_loss=8.609439

Batch 31810, train_perplexity=5576.9595, train_loss=8.626399

Batch 31820, train_perplexity=4935.812, train_loss=8.504272

Batch 31830, train_perplexity=5967.0894, train_loss=8.694015

Batch 31840, train_perplexity=5028.556, train_loss=8.522888

Batch 31850, train_perplexity=5120.2593, train_loss=8.54096

Batch 31860, train_perplexity=5458.5503, train_loss=8.6049385

Batch 31870, train_perplexity=6456.8555, train_loss=8.772898

Batch 31880, train_perplexity=4906.466, train_loss=8.498309

Batch 31890, train_perplexity=5326.9897, train_loss=8.580542

Batch 31900, train_perplexity=4897.6953, train_loss=8.49652

Batch 31910, train_perplexity=4732.3955, train_loss=8.462187

Batch 31920, train_perplexity=5547.916, train_loss=8.621178

Batch 31930, train_perplexity=5003.6714, train_loss=8.517927

Batch 31940, train_perplexity=4643.5547, train_loss=8.443235

Batch 31950, train_perplexity=5333.385, train_loss=8.581741

Batch 31960, train_perplexity=5026.408, train_loss=8.522461

Batch 31970, train_perplexity=5248.8057, train_loss=8.565756

Batch 31980, train_perplexity=5113.2715, train_loss=8.539595

Batch 31990, train_perplexity=5712.138, train_loss=8.650349

Batch 32000, train_perplexity=5887.5845, train_loss=8.680601

Batch 32010, train_perplexity=5236.027, train_loss=8.563318

Batch 32020, train_perplexity=5105.9717, train_loss=8.538166

Batch 32030, train_perplexity=5575.981, train_loss=8.626224

Batch 32040, train_perplexity=7204.7915, train_loss=8.882502

Batch 32050, train_perplexity=4283.6235, train_loss=8.362555

Batch 32060, train_perplexity=4625.354, train_loss=8.439308

Batch 32070, train_perplexity=4173.139, train_loss=8.336424

Batch 32080, train_perplexity=4985.2476, train_loss=8.514238

Batch 32090, train_perplexity=5683.068, train_loss=8.6452465

Batch 32100, train_perplexity=5771.6606, train_loss=8.660715

Batch 32110, train_perplexity=4454.2593, train_loss=8.401616

Batch 32120, train_perplexity=5319.025, train_loss=8.579045

Batch 32130, train_perplexity=6035.7407, train_loss=8.705454

Batch 32140, train_perplexity=4743.8955, train_loss=8.464614

Batch 32150, train_perplexity=5099.8984, train_loss=8.536976

Batch 32160, train_perplexity=4789.636, train_loss=8.47421

Batch 32170, train_perplexity=4486.498, train_loss=8.408828

Batch 32180, train_perplexity=5180.19, train_loss=8.552597

Batch 32190, train_perplexity=5835.0356, train_loss=8.671636

Batch 32200, train_perplexity=4804.8394, train_loss=8.477379

Batch 32210, train_perplexity=5289.8315, train_loss=8.573542

Batch 32220, train_perplexity=4624.476, train_loss=8.439118

Batch 32230, train_perplexity=5945.0903, train_loss=8.690321

Batch 32240, train_perplexity=6620.9785, train_loss=8.797998

Batch 32250, train_perplexity=5671.849, train_loss=8.6432705

Batch 32260, train_perplexity=5351.88, train_loss=8.585203

Batch 32270, train_perplexity=4875.75, train_loss=8.492029

Batch 32280, train_perplexity=5143.5405, train_loss=8.545497

Batch 32290, train_perplexity=5365.719, train_loss=8.587786

Batch 32300, train_perplexity=6067.228, train_loss=8.710657
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 32310, train_perplexity=5500.717, train_loss=8.612634

Batch 32320, train_perplexity=5163.3667, train_loss=8.549344

Batch 32330, train_perplexity=5901.087, train_loss=8.682892

Batch 32340, train_perplexity=4740.8247, train_loss=8.463966

Batch 32350, train_perplexity=7159.558, train_loss=8.876204

Batch 32360, train_perplexity=5791.4106, train_loss=8.664131

Batch 32370, train_perplexity=5192.506, train_loss=8.554972

Batch 32380, train_perplexity=6043.77, train_loss=8.706783

Batch 32390, train_perplexity=4241.445, train_loss=8.352659

Batch 32400, train_perplexity=5766.5933, train_loss=8.659837

Batch 32410, train_perplexity=4866.9697, train_loss=8.490227

Batch 32420, train_perplexity=4882.073, train_loss=8.493325

Batch 32430, train_perplexity=5660.21, train_loss=8.641216

Batch 32440, train_perplexity=6396.8276, train_loss=8.763557

Batch 32450, train_perplexity=4905.731, train_loss=8.498159

Batch 32460, train_perplexity=5266.5054, train_loss=8.569122

Batch 32470, train_perplexity=5719.715, train_loss=8.651674

Batch 32480, train_perplexity=5916.5044, train_loss=8.685501

Batch 32490, train_perplexity=5672.769, train_loss=8.643433

Batch 32500, train_perplexity=4940.8604, train_loss=8.505295

Batch 32510, train_perplexity=6224.4263, train_loss=8.736237

Batch 32520, train_perplexity=6141.159, train_loss=8.722769

Batch 32530, train_perplexity=5195.582, train_loss=8.555564

Batch 32540, train_perplexity=5986.989, train_loss=8.697344

Batch 32550, train_perplexity=5795.875, train_loss=8.664902

Batch 32560, train_perplexity=5816.784, train_loss=8.668503

Batch 32570, train_perplexity=5146.3325, train_loss=8.54604

Batch 32580, train_perplexity=5411.7036, train_loss=8.596319

Batch 32590, train_perplexity=4708.931, train_loss=8.457216

Batch 32600, train_perplexity=4598.919, train_loss=8.433577

Batch 32610, train_perplexity=6092.334, train_loss=8.714787

Batch 32620, train_perplexity=4913.0444, train_loss=8.499649

Batch 32630, train_perplexity=5648.282, train_loss=8.639107

Batch 32640, train_perplexity=6428.9106, train_loss=8.76856

Batch 32650, train_perplexity=6042.6753, train_loss=8.706602

Batch 32660, train_perplexity=5080.8877, train_loss=8.533241

Batch 32670, train_perplexity=5943.5254, train_loss=8.690058

Batch 32680, train_perplexity=5306.495, train_loss=8.576687

Batch 32690, train_perplexity=5450.0977, train_loss=8.603389

Batch 32700, train_perplexity=5945.5493, train_loss=8.690398

Batch 32710, train_perplexity=5751.1113, train_loss=8.657148

Batch 32720, train_perplexity=5865.0005, train_loss=8.676758

Batch 32730, train_perplexity=5223.4585, train_loss=8.560915

Batch 32740, train_perplexity=5631.297, train_loss=8.636095

Batch 32750, train_perplexity=5333.293, train_loss=8.581724

Batch 32760, train_perplexity=4997.991, train_loss=8.516791

Batch 32770, train_perplexity=5762.8384, train_loss=8.659185

Batch 32780, train_perplexity=6466.5923, train_loss=8.774405

Batch 32790, train_perplexity=5466.5156, train_loss=8.606397

Batch 32800, train_perplexity=5580.157, train_loss=8.626972

Batch 32810, train_perplexity=6380.0366, train_loss=8.760929

Batch 32820, train_perplexity=5256.7705, train_loss=8.567272

Batch 32830, train_perplexity=6549.1235, train_loss=8.7870865

Batch 32840, train_perplexity=4899.2886, train_loss=8.496845

Batch 32850, train_perplexity=4871.488, train_loss=8.491155

Batch 32860, train_perplexity=5237.755, train_loss=8.563648

Batch 32870, train_perplexity=4671.1367, train_loss=8.449158

Batch 32880, train_perplexity=5339.5527, train_loss=8.582897

Batch 32890, train_perplexity=5243.2725, train_loss=8.564701

Batch 32900, train_perplexity=4554.93, train_loss=8.423965

Batch 32910, train_perplexity=6037.5083, train_loss=8.705747

Batch 32920, train_perplexity=5559.017, train_loss=8.623177

Batch 32930, train_perplexity=5288.843, train_loss=8.573355

Batch 32940, train_perplexity=6605.287, train_loss=8.795626

Batch 32950, train_perplexity=5206.2856, train_loss=8.557622

Batch 32960, train_perplexity=4877.289, train_loss=8.492345

Batch 32970, train_perplexity=6035.5337, train_loss=8.70542

Batch 32980, train_perplexity=4887.244, train_loss=8.494384

Batch 32990, train_perplexity=5237.2256, train_loss=8.563547

Batch 33000, train_perplexity=5371.6074, train_loss=8.588882

Batch 33010, train_perplexity=4645.5654, train_loss=8.443668

Batch 33020, train_perplexity=5732.034, train_loss=8.653826

Batch 33030, train_perplexity=5242.8022, train_loss=8.564611

Batch 33040, train_perplexity=4893.6333, train_loss=8.49569

Batch 33050, train_perplexity=5849.5327, train_loss=8.674117

Batch 33060, train_perplexity=5484.735, train_loss=8.609724

Batch 33070, train_perplexity=5892.747, train_loss=8.681478

Batch 33080, train_perplexity=5042.228, train_loss=8.525603

Batch 33090, train_perplexity=5501.9497, train_loss=8.612858

Batch 33100, train_perplexity=4940.319, train_loss=8.505185

Batch 33110, train_perplexity=4952.7773, train_loss=8.507704

Batch 33120, train_perplexity=5544.991, train_loss=8.62065

Batch 33130, train_perplexity=5760.981, train_loss=8.658863

Batch 33140, train_perplexity=5578.029, train_loss=8.626591

Batch 33150, train_perplexity=5919.4507, train_loss=8.685999

Batch 33160, train_perplexity=5706.5244, train_loss=8.649365

Batch 33170, train_perplexity=5905.422, train_loss=8.683626

Batch 33180, train_perplexity=4911.456, train_loss=8.499326

Batch 33190, train_perplexity=5013.855, train_loss=8.51996

Batch 33200, train_perplexity=4372.024, train_loss=8.382981

Batch 33210, train_perplexity=6602.34, train_loss=8.795179

Batch 33220, train_perplexity=5255.337, train_loss=8.566999

Batch 33230, train_perplexity=4700.4424, train_loss=8.455412

Batch 33240, train_perplexity=5400.289, train_loss=8.594208

Batch 33250, train_perplexity=4766.902, train_loss=8.469452

Batch 33260, train_perplexity=4703.734, train_loss=8.456112

Batch 33270, train_perplexity=5339.8022, train_loss=8.582944

Batch 33280, train_perplexity=4671.7515, train_loss=8.449289

Batch 33290, train_perplexity=5507.0156, train_loss=8.613778

Batch 33300, train_perplexity=4071.018, train_loss=8.311648

Batch 33310, train_perplexity=5499.72, train_loss=8.6124525

Batch 33320, train_perplexity=5435.5586, train_loss=8.600718

Batch 33330, train_perplexity=5229.4644, train_loss=8.562064

Batch 33340, train_perplexity=4540.921, train_loss=8.420885

Batch 33350, train_perplexity=4655.691, train_loss=8.445846

Batch 33360, train_perplexity=5012.88, train_loss=8.519766

Batch 33370, train_perplexity=5130.2104, train_loss=8.542902

Batch 33380, train_perplexity=6379.0874, train_loss=8.76078

Batch 33390, train_perplexity=5379.471, train_loss=8.590345

Batch 33400, train_perplexity=6233.0396, train_loss=8.737619

Batch 33410, train_perplexity=4980.016, train_loss=8.513188

Batch 33420, train_perplexity=5575.678, train_loss=8.626169

Batch 33430, train_perplexity=5056.4526, train_loss=8.52842

Batch 33440, train_perplexity=5715.767, train_loss=8.650984

Batch 33450, train_perplexity=5451.527, train_loss=8.603651

Batch 33460, train_perplexity=5835.9316, train_loss=8.671789

Batch 33470, train_perplexity=6720.64, train_loss=8.812939

Batch 33480, train_perplexity=4172.9404, train_loss=8.336376

Batch 33490, train_perplexity=4743.0723, train_loss=8.46444

Batch 33500, train_perplexity=4954.563, train_loss=8.508064

Batch 33510, train_perplexity=5441.533, train_loss=8.601816

Batch 33520, train_perplexity=5137.5156, train_loss=8.544325

Batch 33530, train_perplexity=5227.4897, train_loss=8.5616865

Batch 33540, train_perplexity=5275.403, train_loss=8.57081

Batch 33550, train_perplexity=4956.9214, train_loss=8.50854

Batch 33560, train_perplexity=6030.9937, train_loss=8.704667

Batch 33570, train_perplexity=5509.6367, train_loss=8.614254

Batch 33580, train_perplexity=4989.9946, train_loss=8.51519

Batch 33590, train_perplexity=4874.8013, train_loss=8.491835

Batch 33600, train_perplexity=5834.852, train_loss=8.671604

Batch 33610, train_perplexity=5288.883, train_loss=8.573362

Batch 33620, train_perplexity=5086.327, train_loss=8.534311

Batch 33630, train_perplexity=5250.578, train_loss=8.566093

Batch 33640, train_perplexity=5660.4585, train_loss=8.64126
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33650, train_perplexity=4529.4595, train_loss=8.418358

Batch 33660, train_perplexity=4769.867, train_loss=8.470074

Batch 33670, train_perplexity=5183.546, train_loss=8.553245

Batch 33680, train_perplexity=4448.3887, train_loss=8.400297

Batch 33690, train_perplexity=4896.0093, train_loss=8.496176

Batch 33700, train_perplexity=5593.093, train_loss=8.629288

Batch 33710, train_perplexity=4510.8022, train_loss=8.41423

Batch 33720, train_perplexity=5391.4434, train_loss=8.592568

Batch 33730, train_perplexity=5066.0435, train_loss=8.530315

Batch 33740, train_perplexity=6282.4414, train_loss=8.745514

Batch 33750, train_perplexity=5401.7314, train_loss=8.594475

Batch 33760, train_perplexity=5885.4404, train_loss=8.680237

Batch 33770, train_perplexity=5985.3047, train_loss=8.6970625

Batch 33780, train_perplexity=5369.0107, train_loss=8.588399

Batch 33790, train_perplexity=4859.6323, train_loss=8.488718

Batch 33800, train_perplexity=7833.2974, train_loss=8.966139

Batch 33810, train_perplexity=5464.337, train_loss=8.605998

Batch 33820, train_perplexity=5691.768, train_loss=8.646776

Batch 33830, train_perplexity=5184.51, train_loss=8.553431

Batch 33840, train_perplexity=6874.2573, train_loss=8.835539

Batch 33850, train_perplexity=5550.0327, train_loss=8.621559

Batch 33860, train_perplexity=4828.832, train_loss=8.48236

Batch 33870, train_perplexity=5457.832, train_loss=8.604807

Batch 33880, train_perplexity=6252.126, train_loss=8.740677

Batch 33890, train_perplexity=4553.245, train_loss=8.423595

Batch 33900, train_perplexity=4366.9106, train_loss=8.381811

Batch 33910, train_perplexity=5225.83, train_loss=8.561369

Batch 33920, train_perplexity=4942.378, train_loss=8.505602

Batch 33930, train_perplexity=5635.2456, train_loss=8.636796

Batch 33940, train_perplexity=5480.6934, train_loss=8.608987

Batch 33950, train_perplexity=5358.821, train_loss=8.586499

Batch 33960, train_perplexity=5177.78, train_loss=8.552132

Batch 33970, train_perplexity=5030.158, train_loss=8.523207

Batch 33980, train_perplexity=5616.2207, train_loss=8.633414

Batch 33990, train_perplexity=5107.696, train_loss=8.538504

Batch 34000, train_perplexity=4890.587, train_loss=8.495068

Batch 34010, train_perplexity=5508.6597, train_loss=8.614077

Batch 34020, train_perplexity=4120.9624, train_loss=8.323842

Batch 34030, train_perplexity=4492.677, train_loss=8.410204

Batch 34040, train_perplexity=5598.9795, train_loss=8.63034

Batch 34050, train_perplexity=5228.741, train_loss=8.561926

Batch 34060, train_perplexity=5579.0557, train_loss=8.626775

Batch 34070, train_perplexity=5051.6377, train_loss=8.527468

Batch 34080, train_perplexity=4962.8765, train_loss=8.509741

Batch 34090, train_perplexity=5613.7095, train_loss=8.632967

Batch 34100, train_perplexity=5165.5386, train_loss=8.549765

Batch 34110, train_perplexity=5330.3945, train_loss=8.581181

Batch 34120, train_perplexity=4291.2695, train_loss=8.364338

Batch 34130, train_perplexity=4155.8154, train_loss=8.332264

Batch 34140, train_perplexity=4977.1196, train_loss=8.512607

Batch 34150, train_perplexity=6113.5254, train_loss=8.718259

Batch 34160, train_perplexity=5608.0586, train_loss=8.63196

Batch 34170, train_perplexity=5030.091, train_loss=8.523193

Batch 34180, train_perplexity=5501.0996, train_loss=8.612703

Batch 34190, train_perplexity=6532.861, train_loss=8.7846

Batch 34200, train_perplexity=5743.4595, train_loss=8.655817

Batch 34210, train_perplexity=6007.6816, train_loss=8.700794

Batch 34220, train_perplexity=5536.1035, train_loss=8.619046

Batch 34230, train_perplexity=6264.8154, train_loss=8.742704

Batch 34240, train_perplexity=5022.968, train_loss=8.521776

Batch 34250, train_perplexity=5485.7183, train_loss=8.609903

Batch 34260, train_perplexity=6671.52, train_loss=8.805603

Batch 34270, train_perplexity=5621.3115, train_loss=8.63432

Batch 34280, train_perplexity=4973.1626, train_loss=8.511811

Batch 34290, train_perplexity=5744.5664, train_loss=8.65601

Batch 34300, train_perplexity=6866.0605, train_loss=8.834346

Batch 34310, train_perplexity=6629.091, train_loss=8.799223

Batch 34320, train_perplexity=6299.7324, train_loss=8.748262

Batch 34330, train_perplexity=4304.057, train_loss=8.367313

Batch 34340, train_perplexity=5570.087, train_loss=8.625166

Batch 34350, train_perplexity=4785.8647, train_loss=8.473422

Batch 34360, train_perplexity=5778.2256, train_loss=8.661852

Batch 34370, train_perplexity=5621.7993, train_loss=8.634407

Batch 34380, train_perplexity=5342.344, train_loss=8.58342

Batch 34390, train_perplexity=5022.019, train_loss=8.521587

Batch 34400, train_perplexity=4863.963, train_loss=8.489609

Batch 34410, train_perplexity=5449.7856, train_loss=8.603332

Batch 34420, train_perplexity=5444.1235, train_loss=8.602292

Batch 34430, train_perplexity=5202.1465, train_loss=8.556827

Batch 34440, train_perplexity=5297.52, train_loss=8.574994

Batch 34450, train_perplexity=5135.125, train_loss=8.5438595

Batch 34460, train_perplexity=5675.756, train_loss=8.643959

Batch 34470, train_perplexity=6068.478, train_loss=8.710863

Batch 34480, train_perplexity=5145.714, train_loss=8.545919

Batch 34490, train_perplexity=5511.8125, train_loss=8.614649

Batch 34500, train_perplexity=4524.1494, train_loss=8.417185

Batch 34510, train_perplexity=4607.932, train_loss=8.4355345

Batch 34520, train_perplexity=6281.7583, train_loss=8.745405

Batch 34530, train_perplexity=4935.398, train_loss=8.504189

Batch 34540, train_perplexity=4720.604, train_loss=8.459692

Batch 34550, train_perplexity=6255.645, train_loss=8.74124

Batch 34560, train_perplexity=7239.871, train_loss=8.887359

Batch 34570, train_perplexity=5889.028, train_loss=8.680846

Batch 34580, train_perplexity=5338.3765, train_loss=8.582677

Batch 34590, train_perplexity=5251.229, train_loss=8.566217

Batch 34600, train_perplexity=5373.831, train_loss=8.589296

Batch 34610, train_perplexity=5578.912, train_loss=8.626749

Batch 34620, train_perplexity=6440.0176, train_loss=8.770287

Batch 34630, train_perplexity=5278.4624, train_loss=8.57139

Batch 34640, train_perplexity=5411.7036, train_loss=8.596319

Batch 34650, train_perplexity=5554.311, train_loss=8.62233

Batch 34660, train_perplexity=5192.8726, train_loss=8.555042

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Batch 34670, train_perplexity=5144.556, train_loss=8.545694

Batch 34680, train_perplexity=5868.0327, train_loss=8.677275

Batch 34690, train_perplexity=5503.944, train_loss=8.61322

Batch 34700, train_perplexity=4623.475, train_loss=8.438902

Batch 34710, train_perplexity=5490.209, train_loss=8.610722

Batch 34720, train_perplexity=4457.595, train_loss=8.402365

Batch 34730, train_perplexity=5264.4263, train_loss=8.5687275

Batch 34740, train_perplexity=4865.438, train_loss=8.489912

Batch 34750, train_perplexity=5680.608, train_loss=8.644814

Batch 34760, train_perplexity=4694.941, train_loss=8.454241

Batch 34770, train_perplexity=5040.5786, train_loss=8.525276

Batch 34780, train_perplexity=6618.239, train_loss=8.797585

Batch 34790, train_perplexity=5483.297, train_loss=8.609462

Batch 34800, train_perplexity=5367.2803, train_loss=8.588077

Batch 34810, train_perplexity=4496.7144, train_loss=8.411102

Batch 34820, train_perplexity=5952.3345, train_loss=8.691539

Batch 34830, train_perplexity=5234.449, train_loss=8.563017

Batch 34840, train_perplexity=4839.4536, train_loss=8.484557

Batch 34850, train_perplexity=5690.411, train_loss=8.646538

Batch 34860, train_perplexity=5100.1123, train_loss=8.537018

Batch 34870, train_perplexity=5329.968, train_loss=8.5811

Batch 34880, train_perplexity=5226.119, train_loss=8.561424

Batch 34890, train_perplexity=5279.802, train_loss=8.571644

Batch 34900, train_perplexity=5395.805, train_loss=8.593377

Batch 34910, train_perplexity=4938.7124, train_loss=8.50486
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34920, train_perplexity=4771.582, train_loss=8.470433

Batch 34930, train_perplexity=6079.02, train_loss=8.712599

Batch 34940, train_perplexity=4593.431, train_loss=8.432383

Batch 34950, train_perplexity=5554.5444, train_loss=8.622372

Batch 34960, train_perplexity=5504.4478, train_loss=8.613312

Batch 34970, train_perplexity=5113.8564, train_loss=8.539709

Batch 34980, train_perplexity=4631.0522, train_loss=8.440539

Batch 34990, train_perplexity=5330.9033, train_loss=8.581276

Batch 35000, train_perplexity=4704.667, train_loss=8.45631

Batch 35010, train_perplexity=5782.5254, train_loss=8.662596

Batch 35020, train_perplexity=5312.622, train_loss=8.577841

Batch 35030, train_perplexity=4907.6826, train_loss=8.498557

Batch 35040, train_perplexity=5276.0166, train_loss=8.570927

Batch 35050, train_perplexity=6727.1807, train_loss=8.813911

Batch 35060, train_perplexity=5262.3486, train_loss=8.568333

Batch 35070, train_perplexity=5630.314, train_loss=8.635921

Batch 35080, train_perplexity=4505.36, train_loss=8.413023

Batch 35090, train_perplexity=5433.454, train_loss=8.60033

Batch 35100, train_perplexity=5561.96, train_loss=8.623706

Batch 35110, train_perplexity=5468.6533, train_loss=8.606788

Batch 35120, train_perplexity=6034.532, train_loss=8.705254

Batch 35130, train_perplexity=5307.796, train_loss=8.576932

Batch 35140, train_perplexity=6412.4395, train_loss=8.765995

Batch 35150, train_perplexity=6107.389, train_loss=8.717255

Batch 35160, train_perplexity=5427.2915, train_loss=8.5991955

Batch 35170, train_perplexity=6243.0815, train_loss=8.739229

Batch 35180, train_perplexity=4715.151, train_loss=8.458536

Batch 35190, train_perplexity=4973.3193, train_loss=8.511843

Batch 35200, train_perplexity=5397.1484, train_loss=8.593626

Batch 35210, train_perplexity=4850.423, train_loss=8.486821

Batch 35220, train_perplexity=4829.9053, train_loss=8.482582

Batch 35230, train_perplexity=5286.876, train_loss=8.572983

Batch 35240, train_perplexity=4443.483, train_loss=8.399194

Batch 35250, train_perplexity=5571.787, train_loss=8.625471

Batch 35260, train_perplexity=5404.72, train_loss=8.595028

Batch 35270, train_perplexity=4182.949, train_loss=8.338772

Batch 35280, train_perplexity=5578.0713, train_loss=8.626598

Batch 35290, train_perplexity=5506.8525, train_loss=8.613749

Batch 35300, train_perplexity=4419.9546, train_loss=8.393885

Batch 35310, train_perplexity=6056.1055, train_loss=8.708822

Batch 35320, train_perplexity=4957.692, train_loss=8.508696

Batch 35330, train_perplexity=4654.1416, train_loss=8.445513

Batch 35340, train_perplexity=5439.6655, train_loss=8.601473

Batch 35350, train_perplexity=5237.49, train_loss=8.563598

Batch 35360, train_perplexity=5063.9517, train_loss=8.529902

Batch 35370, train_perplexity=5620.845, train_loss=8.634237

Batch 35380, train_perplexity=5768.392, train_loss=8.660149

Batch 35390, train_perplexity=4652.34, train_loss=8.445126

Batch 35400, train_perplexity=5378.8457, train_loss=8.590229

Batch 35410, train_perplexity=5308.6416, train_loss=8.577091

Batch 35420, train_perplexity=8055.3135, train_loss=8.994087

Batch 35430, train_perplexity=5728.9683, train_loss=8.653291

Batch 35440, train_perplexity=4464.9473, train_loss=8.404013

Batch 35450, train_perplexity=5349.8184, train_loss=8.584818

Batch 35460, train_perplexity=4971.029, train_loss=8.511382

Batch 35470, train_perplexity=6368.286, train_loss=8.759086

Batch 35480, train_perplexity=5741.302, train_loss=8.655441

Batch 35490, train_perplexity=5579.109, train_loss=8.626784

Batch 35500, train_perplexity=5323.689, train_loss=8.579922

Batch 35510, train_perplexity=6847.3525, train_loss=8.831617

Batch 35520, train_perplexity=5683.968, train_loss=8.645405

Batch 35530, train_perplexity=6057.839, train_loss=8.709108

Batch 35540, train_perplexity=6400.9404, train_loss=8.7642

Batch 35550, train_perplexity=5236.796, train_loss=8.563465

Batch 35560, train_perplexity=6974.671, train_loss=8.85004

Batch 35570, train_perplexity=6606.6606, train_loss=8.795834

Batch 35580, train_perplexity=6512.9795, train_loss=8.781552

Batch 35590, train_perplexity=5127.662, train_loss=8.542405

Batch 35600, train_perplexity=4805.4624, train_loss=8.477509

Batch 35610, train_perplexity=4687.2817, train_loss=8.452608

Batch 35620, train_perplexity=5852.4346, train_loss=8.674613

Batch 35630, train_perplexity=5573.1953, train_loss=8.625724

Batch 35640, train_perplexity=5169.6533, train_loss=8.550561

Batch 35650, train_perplexity=4869.0493, train_loss=8.490654

Batch 35660, train_perplexity=5234.2744, train_loss=8.5629835

Batch 35670, train_perplexity=6630.836, train_loss=8.799486

Batch 35680, train_perplexity=4901.499, train_loss=8.497296

Batch 35690, train_perplexity=5824.461, train_loss=8.669822

Batch 35700, train_perplexity=5801.416, train_loss=8.665857

Batch 35710, train_perplexity=5251.0386, train_loss=8.566181

Batch 35720, train_perplexity=5111.9453, train_loss=8.539335

Batch 35730, train_perplexity=6113.6304, train_loss=8.718276

Batch 35740, train_perplexity=5920.3823, train_loss=8.686156

Batch 35750, train_perplexity=5399.6606, train_loss=8.594091

Batch 35760, train_perplexity=5237.8296, train_loss=8.563663

Batch 35770, train_perplexity=5037.9595, train_loss=8.524756

Batch 35780, train_perplexity=5483.0093, train_loss=8.609409

Batch 35790, train_perplexity=5451.137, train_loss=8.6035795

Batch 35800, train_perplexity=5478.1587, train_loss=8.608524

Batch 35810, train_perplexity=4486.7764, train_loss=8.40889

Batch 35820, train_perplexity=5538.9185, train_loss=8.6195545

Batch 35830, train_perplexity=4929.997, train_loss=8.503094

Batch 35840, train_perplexity=5218.0615, train_loss=8.559881

Batch 35850, train_perplexity=5814.998, train_loss=8.668196

Batch 35860, train_perplexity=5085.6724, train_loss=8.534183

Batch 35870, train_perplexity=6138.5127, train_loss=8.722338

Batch 35880, train_perplexity=6070.0815, train_loss=8.711127

Batch 35890, train_perplexity=5430.522, train_loss=8.599791

Batch 35900, train_perplexity=6247.7334, train_loss=8.739974

Batch 35910, train_perplexity=6327.393, train_loss=8.752644

Batch 35920, train_perplexity=6001.6, train_loss=8.699781

Batch 35930, train_perplexity=5460.867, train_loss=8.605363

Batch 35940, train_perplexity=5813.096, train_loss=8.667869

Batch 35950, train_perplexity=6255.7344, train_loss=8.741254

Batch 35960, train_perplexity=4952.107, train_loss=8.507568

Batch 35970, train_perplexity=5587.607, train_loss=8.628306

Batch 35980, train_perplexity=4849.8447, train_loss=8.486702

Batch 35990, train_perplexity=4954.095, train_loss=8.50797

Batch 36000, train_perplexity=4163.273, train_loss=8.334057

Batch 36010, train_perplexity=6439.219, train_loss=8.770163

Batch 36020, train_perplexity=5513.495, train_loss=8.614954

Batch 36030, train_perplexity=5577.534, train_loss=8.626502

Batch 36040, train_perplexity=6215.7183, train_loss=8.734837

Batch 36050, train_perplexity=7546.4463, train_loss=8.928832

Batch 36060, train_perplexity=5311.325, train_loss=8.577597

Batch 36070, train_perplexity=4618.4775, train_loss=8.43782

Batch 36080, train_perplexity=4852.574, train_loss=8.487265

Batch 36090, train_perplexity=5395.1465, train_loss=8.593255

Batch 36100, train_perplexity=3940.926, train_loss=8.279171

Batch 36110, train_perplexity=5287.8745, train_loss=8.573172

Batch 36120, train_perplexity=5478.6235, train_loss=8.608609

Batch 36130, train_perplexity=6101.8003, train_loss=8.716339

Batch 36140, train_perplexity=5350.553, train_loss=8.584955

Batch 36150, train_perplexity=5220.4805, train_loss=8.560345

Batch 36160, train_perplexity=5299.2026, train_loss=8.575312

Batch 36170, train_perplexity=5151.3213, train_loss=8.5470085

Batch 36180, train_perplexity=3958.0674, train_loss=8.283511

Batch 36190, train_perplexity=5347.11, train_loss=8.5843115

Batch 36200, train_perplexity=7019.1514, train_loss=8.856398

Batch 36210, train_perplexity=5726.2754, train_loss=8.652821

Batch 36220, train_perplexity=4992.9985, train_loss=8.515792

Batch 36230, train_perplexity=5452.151, train_loss=8.6037655

Batch 36240, train_perplexity=5343.567, train_loss=8.583649

Batch 36250, train_perplexity=4988.401, train_loss=8.514871
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 36260, train_perplexity=5059.2114, train_loss=8.528966

Batch 36270, train_perplexity=5644.965, train_loss=8.638519

Batch 36280, train_perplexity=5479.9146, train_loss=8.608845

Batch 36290, train_perplexity=5192.318, train_loss=8.554935

Batch 36300, train_perplexity=4768.53, train_loss=8.469793

Batch 36310, train_perplexity=5079.3857, train_loss=8.532946

Batch 36320, train_perplexity=5547.255, train_loss=8.621058

Batch 36330, train_perplexity=4974.543, train_loss=8.512089

Batch 36340, train_perplexity=5118.111, train_loss=8.540541

Batch 36350, train_perplexity=5272.5156, train_loss=8.570263

Batch 36360, train_perplexity=5459.159, train_loss=8.60505

Batch 36370, train_perplexity=5288.298, train_loss=8.573252

Batch 36380, train_perplexity=5690.5957, train_loss=8.64657

Batch 36390, train_perplexity=5519.124, train_loss=8.615974

Batch 36400, train_perplexity=4936.744, train_loss=8.504461

Batch 36410, train_perplexity=4512.618, train_loss=8.414633

Batch 36420, train_perplexity=6606.3076, train_loss=8.79578

Batch 36430, train_perplexity=5157.9575, train_loss=8.548296

Batch 36440, train_perplexity=5172.568, train_loss=8.551125

Batch 36450, train_perplexity=4660.9907, train_loss=8.446983

Batch 36460, train_perplexity=4384.86, train_loss=8.385913

Batch 36470, train_perplexity=5520.503, train_loss=8.616224

Batch 36480, train_perplexity=4711.631, train_loss=8.457789

Batch 36490, train_perplexity=6458.666, train_loss=8.773178

Batch 36500, train_perplexity=5087.715, train_loss=8.534584

Batch 36510, train_perplexity=4981.1323, train_loss=8.513412

Batch 36520, train_perplexity=5199.225, train_loss=8.556265

Batch 36530, train_perplexity=5235.657, train_loss=8.563248

Batch 36540, train_perplexity=4682.1084, train_loss=8.451504

Batch 36550, train_perplexity=5797.8433, train_loss=8.665241

Batch 36560, train_perplexity=4599.5024, train_loss=8.433703

Batch 36570, train_perplexity=6083.103, train_loss=8.71327

Batch 36580, train_perplexity=5709.6, train_loss=8.649904

Batch 36590, train_perplexity=4899.9893, train_loss=8.496988

Batch 36600, train_perplexity=4741.1865, train_loss=8.464043

Batch 36610, train_perplexity=4527.5547, train_loss=8.417937

Batch 36620, train_perplexity=5169.0864, train_loss=8.550451

Batch 36630, train_perplexity=5103.2603, train_loss=8.537635

Batch 36640, train_perplexity=4806.929, train_loss=8.477814

Batch 36650, train_perplexity=4767.6294, train_loss=8.4696045

Batch 36660, train_perplexity=4901.167, train_loss=8.497229

Batch 36670, train_perplexity=5099.1494, train_loss=8.536829

Batch 36680, train_perplexity=5564.443, train_loss=8.624152

Batch 36690, train_perplexity=5381.077, train_loss=8.590644

Batch 36700, train_perplexity=4195.582, train_loss=8.341787

Batch 36710, train_perplexity=4470.1367, train_loss=8.405174

Batch 36720, train_perplexity=4505.222, train_loss=8.4129925

Batch 36730, train_perplexity=4865.99, train_loss=8.4900255

Batch 36740, train_perplexity=5045.0176, train_loss=8.526156

Batch 36750, train_perplexity=5479.951, train_loss=8.608851

Batch 36760, train_perplexity=5863.166, train_loss=8.676445

Batch 36770, train_perplexity=4599.533, train_loss=8.43371

Batch 36780, train_perplexity=4924.7344, train_loss=8.502026

Batch 36790, train_perplexity=5638.2236, train_loss=8.637324

Batch 36800, train_perplexity=6215.3687, train_loss=8.73478

Batch 36810, train_perplexity=6607.8955, train_loss=8.7960205

Batch 36820, train_perplexity=5561.5146, train_loss=8.623626

Batch 36830, train_perplexity=6472.256, train_loss=8.77528

Batch 36840, train_perplexity=5005.8, train_loss=8.5183525

Batch 36850, train_perplexity=6014.274, train_loss=8.701891

Batch 36860, train_perplexity=5675.507, train_loss=8.643915

Batch 36870, train_perplexity=6145.6704, train_loss=8.723503

Batch 36880, train_perplexity=5015.2085, train_loss=8.52023

Batch 36890, train_perplexity=5608.3584, train_loss=8.632013

Batch 36900, train_perplexity=5147.8984, train_loss=8.546344

Batch 36910, train_perplexity=4768.821, train_loss=8.469854

Batch 36920, train_perplexity=5643.619, train_loss=8.638281

Batch 36930, train_perplexity=5709.7197, train_loss=8.649925

Batch 36940, train_perplexity=5269.6455, train_loss=8.569718

Batch 36950, train_perplexity=5033.652, train_loss=8.523901

Batch 36960, train_perplexity=5148.2173, train_loss=8.546406

Batch 36970, train_perplexity=6096.2046, train_loss=8.715422

Batch 36980, train_perplexity=5447.224, train_loss=8.602861

Batch 36990, train_perplexity=4632.311, train_loss=8.440811

Batch 37000, train_perplexity=4369.0728, train_loss=8.382306

Batch 37010, train_perplexity=5664.903, train_loss=8.642045

Batch 37020, train_perplexity=5627.421, train_loss=8.6354065

Batch 37030, train_perplexity=6074.6387, train_loss=8.711878

Batch 37040, train_perplexity=6487.05, train_loss=8.777563

Batch 37050, train_perplexity=5127.149, train_loss=8.542305

Batch 37060, train_perplexity=6036.4087, train_loss=8.7055645

Batch 37070, train_perplexity=6493.4126, train_loss=8.778543

Batch 37080, train_perplexity=5460.383, train_loss=8.605274

Batch 37090, train_perplexity=6001.858, train_loss=8.699824

Batch 37100, train_perplexity=4586.467, train_loss=8.430865

Batch 37110, train_perplexity=5801.8364, train_loss=8.66593

Batch 37120, train_perplexity=5059.6074, train_loss=8.529044

Batch 37130, train_perplexity=5545.335, train_loss=8.620712

Batch 37140, train_perplexity=5939.14, train_loss=8.68932

Batch 37150, train_perplexity=4979.726, train_loss=8.51313

Batch 37160, train_perplexity=5084.814, train_loss=8.534014

Batch 37170, train_perplexity=4252.9805, train_loss=8.355375

Batch 37180, train_perplexity=5434.066, train_loss=8.600443

Batch 37190, train_perplexity=6675.224, train_loss=8.806158

Batch 37200, train_perplexity=5858.052, train_loss=8.675572

Batch 37210, train_perplexity=5121.597, train_loss=8.541222

Batch 37220, train_perplexity=6114.254, train_loss=8.718378

Batch 37230, train_perplexity=5517.1665, train_loss=8.61562

Batch 37240, train_perplexity=4840.0493, train_loss=8.48468

Batch 37250, train_perplexity=5386.3916, train_loss=8.591631

Batch 37260, train_perplexity=5779.8184, train_loss=8.6621275

Batch 37270, train_perplexity=5169.629, train_loss=8.550556

Batch 37280, train_perplexity=5691.3013, train_loss=8.646694

Batch 37290, train_perplexity=4261.7017, train_loss=8.357424

Batch 37300, train_perplexity=6778.74, train_loss=8.821547

Batch 37310, train_perplexity=5401.9785, train_loss=8.594521

Batch 37320, train_perplexity=5389.4536, train_loss=8.592199

Batch 37330, train_perplexity=5356.5674, train_loss=8.586079

Batch 37340, train_perplexity=5238.599, train_loss=8.563809

Batch 37350, train_perplexity=4957.607, train_loss=8.508678

Batch 37360, train_perplexity=4961.698, train_loss=8.509503

Batch 37370, train_perplexity=5449.796, train_loss=8.603333

Batch 37380, train_perplexity=5023.0156, train_loss=8.521786

Batch 37390, train_perplexity=5004.5303, train_loss=8.518099

Batch 37400, train_perplexity=5662.9204, train_loss=8.641695

Batch 37410, train_perplexity=4780.2314, train_loss=8.472244

Batch 37420, train_perplexity=4939.6875, train_loss=8.505057

Batch 37430, train_perplexity=4228.948, train_loss=8.349709

Batch 37440, train_perplexity=5169.905, train_loss=8.55061

Batch 37450, train_perplexity=4900.8022, train_loss=8.497154

Batch 37460, train_perplexity=5157.515, train_loss=8.54821

Batch 37470, train_perplexity=5105.884, train_loss=8.538149

Batch 37480, train_perplexity=5530.2725, train_loss=8.617992

Batch 37490, train_perplexity=6147.37, train_loss=8.72378

Batch 37500, train_perplexity=5621.2363, train_loss=8.634307

Batch 37510, train_perplexity=5962.7036, train_loss=8.693279

Batch 37520, train_perplexity=5101.6836, train_loss=8.537326

Batch 37530, train_perplexity=5396.191, train_loss=8.593449

Batch 37540, train_perplexity=6922.8955, train_loss=8.842589

Batch 37550, train_perplexity=6307.752, train_loss=8.749535

Batch 37560, train_perplexity=6699.114, train_loss=8.809731

Batch 37570, train_perplexity=5557.2676, train_loss=8.622862

Batch 37580, train_perplexity=5636.6807, train_loss=8.637051

Batch 37590, train_perplexity=5465.061, train_loss=8.606131
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37600, train_perplexity=4667.1646, train_loss=8.448307

Batch 37610, train_perplexity=5167.6274, train_loss=8.550169

Batch 37620, train_perplexity=5292.6978, train_loss=8.574083

Batch 37630, train_perplexity=5373.821, train_loss=8.589294

Batch 37640, train_perplexity=5550.1123, train_loss=8.621573

Batch 37650, train_perplexity=5144.6934, train_loss=8.545721

Batch 37660, train_perplexity=5478.7544, train_loss=8.608633

Batch 37670, train_perplexity=4638.8496, train_loss=8.442222

Batch 37680, train_perplexity=5352.3037, train_loss=8.585282

Batch 37690, train_perplexity=4806.6357, train_loss=8.477753

Batch 37700, train_perplexity=6073.272, train_loss=8.711653

Batch 37710, train_perplexity=5595.766, train_loss=8.6297655

Batch 37720, train_perplexity=4499.623, train_loss=8.411749

Batch 37730, train_perplexity=4669.676, train_loss=8.448845

Batch 37740, train_perplexity=5346.9414, train_loss=8.58428

Batch 37750, train_perplexity=5673.2397, train_loss=8.643516

Batch 37760, train_perplexity=5432.993, train_loss=8.600245

Batch 37770, train_perplexity=4738.8403, train_loss=8.463548

Batch 37780, train_perplexity=5275.861, train_loss=8.570897

Batch 37790, train_perplexity=5151.8765, train_loss=8.547116

Batch 37800, train_perplexity=5292.6167, train_loss=8.574068

Batch 37810, train_perplexity=5514.4834, train_loss=8.615133

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Batch 37820, train_perplexity=5332.764, train_loss=8.581625

Batch 37830, train_perplexity=5915.98, train_loss=8.685412

Batch 37840, train_perplexity=4479.7646, train_loss=8.407326

Batch 37850, train_perplexity=4539.427, train_loss=8.420556

Batch 37860, train_perplexity=6046.9756, train_loss=8.707314

Batch 37870, train_perplexity=5749.0493, train_loss=8.65679

Batch 37880, train_perplexity=4615.8447, train_loss=8.43725

Batch 37890, train_perplexity=5337.5977, train_loss=8.582531

Batch 37900, train_perplexity=4609.831, train_loss=8.435946

Batch 37910, train_perplexity=6002.665, train_loss=8.699959

Batch 37920, train_perplexity=5859.8345, train_loss=8.675877

Batch 37930, train_perplexity=5090.651, train_loss=8.535161

Batch 37940, train_perplexity=4852.139, train_loss=8.487175

Batch 37950, train_perplexity=4764.4204, train_loss=8.468931

Batch 37960, train_perplexity=6138.4307, train_loss=8.722324

Batch 37970, train_perplexity=5830.6577, train_loss=8.670885

Batch 37980, train_perplexity=6008.037, train_loss=8.700853

Batch 37990, train_perplexity=5826.7666, train_loss=8.6702175

Batch 38000, train_perplexity=5318.3657, train_loss=8.578921

Batch 38010, train_perplexity=4980.785, train_loss=8.513343

Batch 38020, train_perplexity=4767.493, train_loss=8.469576

Batch 38030, train_perplexity=5568.9346, train_loss=8.624959

Batch 38040, train_perplexity=5213.485, train_loss=8.559004

Batch 38050, train_perplexity=5305.0884, train_loss=8.576422

Batch 38060, train_perplexity=6051.6313, train_loss=8.708083

Batch 38070, train_perplexity=5528.4375, train_loss=8.6176605

Batch 38080, train_perplexity=5509.28, train_loss=8.614189

Batch 38090, train_perplexity=5705.311, train_loss=8.649153

Batch 38100, train_perplexity=5494.504, train_loss=8.611504

Batch 38110, train_perplexity=5190.1147, train_loss=8.554511

Batch 38120, train_perplexity=5689.0327, train_loss=8.646296

Batch 38130, train_perplexity=5731.5586, train_loss=8.653743

Batch 38140, train_perplexity=5341.325, train_loss=8.583229

Batch 38150, train_perplexity=4877.9824, train_loss=8.492487

Batch 38160, train_perplexity=6311.892, train_loss=8.750191

Batch 38170, train_perplexity=5273.5317, train_loss=8.570456

Batch 38180, train_perplexity=5182.374, train_loss=8.553019

Batch 38190, train_perplexity=6095.3677, train_loss=8.715284

Batch 38200, train_perplexity=5611.531, train_loss=8.632579

Batch 38210, train_perplexity=4907.739, train_loss=8.498569

Batch 38220, train_perplexity=5049.6626, train_loss=8.527077

Batch 38230, train_perplexity=5245.6133, train_loss=8.565147

Batch 38240, train_perplexity=6658.4517, train_loss=8.803642

Batch 38250, train_perplexity=6385.089, train_loss=8.761721

Batch 38260, train_perplexity=4981.5454, train_loss=8.513495

Batch 38270, train_perplexity=5769.696, train_loss=8.660375

Batch 38280, train_perplexity=6060.861, train_loss=8.709607

Batch 38290, train_perplexity=6252.8535, train_loss=8.740793

Batch 38300, train_perplexity=4940.931, train_loss=8.505309

Batch 38310, train_perplexity=4320.371, train_loss=8.371097

Batch 38320, train_perplexity=5395.0693, train_loss=8.593241

Batch 38330, train_perplexity=5041.372, train_loss=8.525434

Batch 38340, train_perplexity=5296.0903, train_loss=8.574724

Batch 38350, train_perplexity=4661.8887, train_loss=8.447176

Batch 38360, train_perplexity=6255.2036, train_loss=8.741169

Batch 38370, train_perplexity=4794.563, train_loss=8.475238

Batch 38380, train_perplexity=5210.6123, train_loss=8.558453

Batch 38390, train_perplexity=5824.217, train_loss=8.66978

Batch 38400, train_perplexity=5266.988, train_loss=8.569214

Batch 38410, train_perplexity=4231.522, train_loss=8.350317

Batch 38420, train_perplexity=5253.854, train_loss=8.566717

Batch 38430, train_perplexity=5240.423, train_loss=8.5641575

Batch 38440, train_perplexity=5973.9277, train_loss=8.69516

Batch 38450, train_perplexity=4767.9976, train_loss=8.469682

Batch 38460, train_perplexity=4201.015, train_loss=8.343081

Batch 38470, train_perplexity=5474.179, train_loss=8.607798

Batch 38480, train_perplexity=5569.3433, train_loss=8.625032

Batch 38490, train_perplexity=5324.938, train_loss=8.580156

Batch 38500, train_perplexity=6447.312, train_loss=8.771419

Batch 38510, train_perplexity=6760.3022, train_loss=8.818823

Batch 38520, train_perplexity=4829.758, train_loss=8.482552

Batch 38530, train_perplexity=4731.3574, train_loss=8.461967

Batch 38540, train_perplexity=5744.9937, train_loss=8.656084

Batch 38550, train_perplexity=5843.673, train_loss=8.673115

Batch 38560, train_perplexity=5582.084, train_loss=8.627317

Batch 38570, train_perplexity=5619.2104, train_loss=8.633946

Batch 38580, train_perplexity=6209.9355, train_loss=8.733906

Batch 38590, train_perplexity=4979.517, train_loss=8.513088

Batch 38600, train_perplexity=4892.4155, train_loss=8.495441

Batch 38610, train_perplexity=5059.3564, train_loss=8.528995

Batch 38620, train_perplexity=4729.1743, train_loss=8.461506

Batch 38630, train_perplexity=5203.5903, train_loss=8.557104

Batch 38640, train_perplexity=6127.2827, train_loss=8.720507

Batch 38650, train_perplexity=5420.5566, train_loss=8.597954

Batch 38660, train_perplexity=5095.1343, train_loss=8.536041

Batch 38670, train_perplexity=5531.027, train_loss=8.618129

Batch 38680, train_perplexity=5044.1274, train_loss=8.52598

Batch 38690, train_perplexity=5750.387, train_loss=8.657022

Batch 38700, train_perplexity=4827.5425, train_loss=8.482093

Batch 38710, train_perplexity=5709.4805, train_loss=8.649883

Batch 38720, train_perplexity=5505.687, train_loss=8.613537

Batch 38730, train_perplexity=5238.1646, train_loss=8.563726

Batch 38740, train_perplexity=4910.337, train_loss=8.499098

Batch 38750, train_perplexity=5660.7173, train_loss=8.641306

Batch 38760, train_perplexity=5004.8164, train_loss=8.518156

Batch 38770, train_perplexity=5125.252, train_loss=8.541935

Batch 38780, train_perplexity=5768.337, train_loss=8.660139

Batch 38790, train_perplexity=5368.2886, train_loss=8.588264

Batch 38800, train_perplexity=5273.7427, train_loss=8.570496

Batch 38810, train_perplexity=5900.0405, train_loss=8.682714

Batch 38820, train_perplexity=5824.922, train_loss=8.669901

Batch 38830, train_perplexity=5456.8325, train_loss=8.604624

Batch 38840, train_perplexity=5774.821, train_loss=8.6612625

Batch 38850, train_perplexity=4864.668, train_loss=8.489754

Batch 38860, train_perplexity=5704.816, train_loss=8.649066
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38870, train_perplexity=4929.292, train_loss=8.502951

Batch 38880, train_perplexity=5484.7505, train_loss=8.609727

Batch 38890, train_perplexity=5987.434, train_loss=8.697418

Batch 38900, train_perplexity=5457.103, train_loss=8.604673

Batch 38910, train_perplexity=5999.597, train_loss=8.699448

Batch 38920, train_perplexity=5925.9575, train_loss=8.687098

Batch 38930, train_perplexity=5977.5635, train_loss=8.695768

Batch 38940, train_perplexity=5648.1963, train_loss=8.6390915

Batch 38950, train_perplexity=4921.3916, train_loss=8.501347

Batch 38960, train_perplexity=4702.2715, train_loss=8.455801

Batch 38970, train_perplexity=5519.1133, train_loss=8.6159725

Batch 38980, train_perplexity=5618.7764, train_loss=8.633869

Batch 38990, train_perplexity=4929.555, train_loss=8.503004

Batch 39000, train_perplexity=5973.1074, train_loss=8.695023

Batch 39010, train_perplexity=6163.9604, train_loss=8.726475

Batch 39020, train_perplexity=5682.938, train_loss=8.645224

Batch 39030, train_perplexity=4516.3467, train_loss=8.415459

Batch 39040, train_perplexity=4837.53, train_loss=8.484159

Batch 39050, train_perplexity=4548.792, train_loss=8.422617

Batch 39060, train_perplexity=4847.6343, train_loss=8.486246

Batch 39070, train_perplexity=5050.746, train_loss=8.527291

Batch 39080, train_perplexity=5250.9385, train_loss=8.566162

Batch 39090, train_perplexity=6321.1987, train_loss=8.751664

Batch 39100, train_perplexity=4499.5713, train_loss=8.411737

Batch 39110, train_perplexity=5188.447, train_loss=8.55419

Batch 39120, train_perplexity=5597.218, train_loss=8.630025

Batch 39130, train_perplexity=5158.8726, train_loss=8.548473

Batch 39140, train_perplexity=5224.1855, train_loss=8.561054

Batch 39150, train_perplexity=4744.8545, train_loss=8.464816

Batch 39160, train_perplexity=5481.655, train_loss=8.609162

Batch 39170, train_perplexity=4994.1987, train_loss=8.516032

Batch 39180, train_perplexity=5408.1333, train_loss=8.595659

Batch 39190, train_perplexity=5812.658, train_loss=8.667793

Batch 39200, train_perplexity=4756.612, train_loss=8.467291

Batch 39210, train_perplexity=5469.1333, train_loss=8.606875

Batch 39220, train_perplexity=4850.6494, train_loss=8.486868

Batch 39230, train_perplexity=5272.576, train_loss=8.570274

Batch 39240, train_perplexity=6174.9453, train_loss=8.728255

Batch 39250, train_perplexity=5222.95, train_loss=8.560818

Batch 39260, train_perplexity=5456.047, train_loss=8.60448

Batch 39270, train_perplexity=4421.1475, train_loss=8.394155

Batch 39280, train_perplexity=5704.5273, train_loss=8.649015

Batch 39290, train_perplexity=5639.7886, train_loss=8.637602

Batch 39300, train_perplexity=5522.4937, train_loss=8.616585

Batch 39310, train_perplexity=6104.5825, train_loss=8.716795

Batch 39320, train_perplexity=4913.7236, train_loss=8.499787

Batch 39330, train_perplexity=6074.326, train_loss=8.711826

Batch 39340, train_perplexity=6008.8506, train_loss=8.700989

Batch 39350, train_perplexity=5025.823, train_loss=8.522345

Batch 39360, train_perplexity=6442.954, train_loss=8.770742

Batch 39370, train_perplexity=5994.6675, train_loss=8.698626

Batch 39380, train_perplexity=5054.693, train_loss=8.528072

Batch 39390, train_perplexity=4271.019, train_loss=8.359608

Batch 39400, train_perplexity=5894.2476, train_loss=8.681732

Batch 39410, train_perplexity=6226.4805, train_loss=8.736567

Batch 39420, train_perplexity=5759.8823, train_loss=8.658672

Batch 39430, train_perplexity=5919.733, train_loss=8.686047

Batch 39440, train_perplexity=6084.072, train_loss=8.713429

Batch 39450, train_perplexity=5380.559, train_loss=8.590548

Batch 39460, train_perplexity=5719.7534, train_loss=8.651681

Batch 39470, train_perplexity=6076.6206, train_loss=8.712204

Batch 39480, train_perplexity=6054.7773, train_loss=8.708603

Batch 39490, train_perplexity=5315.3843, train_loss=8.578361

Batch 39500, train_perplexity=4905.628, train_loss=8.498138

Batch 39510, train_perplexity=5714.0884, train_loss=8.65069

Batch 39520, train_perplexity=5344.581, train_loss=8.583838

Batch 39530, train_perplexity=5318.0864, train_loss=8.578869

Batch 39540, train_perplexity=5136.1636, train_loss=8.544062

Batch 39550, train_perplexity=7790.46, train_loss=8.960655

Batch 39560, train_perplexity=5060.481, train_loss=8.529217

Batch 39570, train_perplexity=5566.7837, train_loss=8.624573

Batch 39580, train_perplexity=5371.4844, train_loss=8.58886

Batch 39590, train_perplexity=5298.7983, train_loss=8.575235

Batch 39600, train_perplexity=4571.275, train_loss=8.427547

Batch 39610, train_perplexity=5323.9526, train_loss=8.579971

Batch 39620, train_perplexity=4823.07, train_loss=8.481166

Batch 39630, train_perplexity=4930.35, train_loss=8.503165

Batch 39640, train_perplexity=5040.1606, train_loss=8.525193

Batch 39650, train_perplexity=4156.5684, train_loss=8.332445

Batch 39660, train_perplexity=4603.43, train_loss=8.434557

Batch 39670, train_perplexity=5292.728, train_loss=8.574089

Batch 39680, train_perplexity=5030.0767, train_loss=8.5231905

Batch 39690, train_perplexity=6011.6133, train_loss=8.701448

Batch 39700, train_perplexity=5156.8755, train_loss=8.548086

Batch 39710, train_perplexity=5085.8374, train_loss=8.534215

Batch 39720, train_perplexity=4920.847, train_loss=8.501236

Batch 39730, train_perplexity=6589.5264, train_loss=8.793237

Batch 39740, train_perplexity=4057.8584, train_loss=8.308411

Batch 39750, train_perplexity=5423.049, train_loss=8.598413

Batch 39760, train_perplexity=4910.852, train_loss=8.499203

Batch 39770, train_perplexity=6002.459, train_loss=8.699924

Batch 39780, train_perplexity=5010.4326, train_loss=8.519278

Batch 39790, train_perplexity=5007.3755, train_loss=8.518667

Batch 39800, train_perplexity=5273.7026, train_loss=8.570488

Batch 39810, train_perplexity=5251.3394, train_loss=8.566238

Batch 39820, train_perplexity=5360.7783, train_loss=8.586864

Batch 39830, train_perplexity=6057.4575, train_loss=8.709045

Batch 39840, train_perplexity=4786.2803, train_loss=8.473509

Batch 39850, train_perplexity=5609.9683, train_loss=8.6323

Batch 39860, train_perplexity=5522.673, train_loss=8.616617

Batch 39870, train_perplexity=5559.038, train_loss=8.62318

Batch 39880, train_perplexity=5650.001, train_loss=8.639411

Batch 39890, train_perplexity=5773.053, train_loss=8.660956

Batch 39900, train_perplexity=5307.0215, train_loss=8.576786

Batch 39910, train_perplexity=5531.9556, train_loss=8.618297

Batch 39920, train_perplexity=5310.2666, train_loss=8.577397

Batch 39930, train_perplexity=5979.628, train_loss=8.696114

Batch 39940, train_perplexity=5001.095, train_loss=8.517412

Batch 39950, train_perplexity=5517.4663, train_loss=8.615674

Batch 39960, train_perplexity=6031.701, train_loss=8.704784

Batch 39970, train_perplexity=5444.0874, train_loss=8.602285

Batch 39980, train_perplexity=4794.8374, train_loss=8.475295

Batch 39990, train_perplexity=5759.5967, train_loss=8.658623

Batch 40000, train_perplexity=6220.2134, train_loss=8.735559

Batch 40010, train_perplexity=5537.244, train_loss=8.619252

Batch 40020, train_perplexity=5810.4966, train_loss=8.667421

Batch 40030, train_perplexity=4521.13, train_loss=8.416517

Batch 40040, train_perplexity=4912.5757, train_loss=8.499554

Batch 40050, train_perplexity=5032.5283, train_loss=8.523678

Batch 40060, train_perplexity=5300.8657, train_loss=8.575625

Batch 40070, train_perplexity=4776.3535, train_loss=8.471433

Batch 40080, train_perplexity=5838.6035, train_loss=8.672247

Batch 40090, train_perplexity=6201.26, train_loss=8.732508

Batch 40100, train_perplexity=4961.0215, train_loss=8.509367

Batch 40110, train_perplexity=5878.776, train_loss=8.679104

Batch 40120, train_perplexity=4958.113, train_loss=8.5087805

Batch 40130, train_perplexity=5974.56, train_loss=8.695266

Batch 40140, train_perplexity=5374.7227, train_loss=8.589462

Batch 40150, train_perplexity=5873.0264, train_loss=8.678125

Batch 40160, train_perplexity=5797.08, train_loss=8.66511

Batch 40170, train_perplexity=4147.1924, train_loss=8.330187

Batch 40180, train_perplexity=6439.207, train_loss=8.770161

Batch 40190, train_perplexity=6030.091, train_loss=8.704517

Batch 40200, train_perplexity=5055.884, train_loss=8.528308
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40210, train_perplexity=5446.393, train_loss=8.602709

Batch 40220, train_perplexity=4370.077, train_loss=8.382536

Batch 40230, train_perplexity=5202.4736, train_loss=8.55689

Batch 40240, train_perplexity=4586.4844, train_loss=8.430869

Batch 40250, train_perplexity=6988.96, train_loss=8.852087

Batch 40260, train_perplexity=5356.5317, train_loss=8.586072

Batch 40270, train_perplexity=5407.401, train_loss=8.595524

Batch 40280, train_perplexity=4634.2817, train_loss=8.4412365

Batch 40290, train_perplexity=6305.791, train_loss=8.749224

Batch 40300, train_perplexity=3723.125, train_loss=8.222319

Batch 40310, train_perplexity=5151.508, train_loss=8.547045

Batch 40320, train_perplexity=5775.9497, train_loss=8.661458

Batch 40330, train_perplexity=4440.149, train_loss=8.398443

Batch 40340, train_perplexity=4831.8813, train_loss=8.482991

Batch 40350, train_perplexity=5416.278, train_loss=8.597164

Batch 40360, train_perplexity=5511.245, train_loss=8.614546

Batch 40370, train_perplexity=5230.1377, train_loss=8.562193

Batch 40380, train_perplexity=4056.222, train_loss=8.308007

Batch 40390, train_perplexity=5941.1455, train_loss=8.689657

Batch 40400, train_perplexity=5347.125, train_loss=8.584314

Batch 40410, train_perplexity=5949.168, train_loss=8.691007

Batch 40420, train_perplexity=4706.013, train_loss=8.456596

Batch 40430, train_perplexity=5773.169, train_loss=8.660976

Batch 40440, train_perplexity=5224.8735, train_loss=8.561186

Batch 40450, train_perplexity=5688.577, train_loss=8.646215

Batch 40460, train_perplexity=4946.4897, train_loss=8.5064335

Batch 40470, train_perplexity=5003.5947, train_loss=8.517912

Batch 40480, train_perplexity=6060.641, train_loss=8.709571

Batch 40490, train_perplexity=5112.374, train_loss=8.539419

Batch 40500, train_perplexity=4802.9976, train_loss=8.476995

Batch 40510, train_perplexity=5087.5303, train_loss=8.534548

Batch 40520, train_perplexity=7033.0757, train_loss=8.858379

Batch 40530, train_perplexity=5798.606, train_loss=8.665373

Batch 40540, train_perplexity=4014.6814, train_loss=8.297713

Batch 40550, train_perplexity=5168.8794, train_loss=8.550411

Batch 40560, train_perplexity=5344.7495, train_loss=8.58387

Batch 40570, train_perplexity=6015.4097, train_loss=8.70208

Batch 40580, train_perplexity=5874.842, train_loss=8.678434

Batch 40590, train_perplexity=6067.876, train_loss=8.710764

Batch 40600, train_perplexity=5834.0396, train_loss=8.671465

Batch 40610, train_perplexity=4661.4663, train_loss=8.447085

Batch 40620, train_perplexity=6041.701, train_loss=8.706441

Batch 40630, train_perplexity=6158.7544, train_loss=8.72563

Batch 40640, train_perplexity=7184.126, train_loss=8.879629

Batch 40650, train_perplexity=4002.7727, train_loss=8.294743

Batch 40660, train_perplexity=4750.03, train_loss=8.465906

Batch 40670, train_perplexity=5693.668, train_loss=8.64711

Batch 40680, train_perplexity=5331.376, train_loss=8.581365

Batch 40690, train_perplexity=4748.39, train_loss=8.465561

Batch 40700, train_perplexity=6751.939, train_loss=8.817585

Batch 40710, train_perplexity=4681.4565, train_loss=8.4513645

Batch 40720, train_perplexity=5062.4355, train_loss=8.529603

Batch 40730, train_perplexity=5071.622, train_loss=8.531416

Batch 40740, train_perplexity=6061.306, train_loss=8.709681

Batch 40750, train_perplexity=4273.2476, train_loss=8.360129

Batch 40760, train_perplexity=5022.891, train_loss=8.521761

Batch 40770, train_perplexity=5453.477, train_loss=8.604009

Batch 40780, train_perplexity=5238.8438, train_loss=8.563856

Batch 40790, train_perplexity=5177.9526, train_loss=8.552165

Batch 40800, train_perplexity=6598.3867, train_loss=8.79458

Batch 40810, train_perplexity=5305.6045, train_loss=8.576519

Batch 40820, train_perplexity=5201.68, train_loss=8.556737

Batch 40830, train_perplexity=5529.6396, train_loss=8.617878

Batch 40840, train_perplexity=5723.655, train_loss=8.652363

Batch 40850, train_perplexity=6340.8457, train_loss=8.754767

Batch 40860, train_perplexity=5844.2803, train_loss=8.673219

Batch 40870, train_perplexity=5556.9287, train_loss=8.622801

Batch 40880, train_perplexity=5151.4, train_loss=8.547024

Batch 40890, train_perplexity=4726.113, train_loss=8.460858

Batch 40900, train_perplexity=5115.671, train_loss=8.540064

Batch 40910, train_perplexity=5468.3247, train_loss=8.606728

Batch 40920, train_perplexity=6003.7124, train_loss=8.700133

Batch 40930, train_perplexity=5493.8335, train_loss=8.611382

Batch 40940, train_perplexity=5340.388, train_loss=8.583054

Batch 40950, train_perplexity=5618.9907, train_loss=8.633907

Batch 40960, train_perplexity=5507.084, train_loss=8.6137905

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Batch 40970, train_perplexity=5356.731, train_loss=8.586109

Batch 40980, train_perplexity=5439.531, train_loss=8.601448

Batch 40990, train_perplexity=5666.6587, train_loss=8.642355

Batch 41000, train_perplexity=5279.4595, train_loss=8.571579

Batch 41010, train_perplexity=5581.594, train_loss=8.62723

Batch 41020, train_perplexity=5524.9697, train_loss=8.617033

Batch 41030, train_perplexity=5249.552, train_loss=8.565898

Batch 41040, train_perplexity=6380.8765, train_loss=8.761061

Batch 41050, train_perplexity=5972.982, train_loss=8.695002

Batch 41060, train_perplexity=5433.454, train_loss=8.60033

Batch 41070, train_perplexity=5545.7476, train_loss=8.620787

Batch 41080, train_perplexity=4974.377, train_loss=8.512055

Batch 41090, train_perplexity=4795.2124, train_loss=8.475373

Batch 41100, train_perplexity=5770.246, train_loss=8.66047

Batch 41110, train_perplexity=6622.488, train_loss=8.798226

Batch 41120, train_perplexity=6110.372, train_loss=8.717743

Batch 41130, train_perplexity=5025.833, train_loss=8.5223465

Batch 41140, train_perplexity=5667.967, train_loss=8.642586

Batch 41150, train_perplexity=6314.0474, train_loss=8.750532

Batch 41160, train_perplexity=6475.022, train_loss=8.775707

Batch 41170, train_perplexity=4730.5366, train_loss=8.461794

Batch 41180, train_perplexity=5069.6924, train_loss=8.531035

Batch 41190, train_perplexity=4914.8765, train_loss=8.500022

Batch 41200, train_perplexity=6588.5273, train_loss=8.793085

Batch 41210, train_perplexity=6270.5835, train_loss=8.743625

Batch 41220, train_perplexity=5370.6953, train_loss=8.588713

Batch 41230, train_perplexity=6615.7837, train_loss=8.797214

Batch 41240, train_perplexity=4867.2017, train_loss=8.490274

Batch 41250, train_perplexity=5605.2783, train_loss=8.631464

Batch 41260, train_perplexity=4843.7617, train_loss=8.485447

Batch 41270, train_perplexity=6042.2026, train_loss=8.706524

Batch 41280, train_perplexity=6055.8745, train_loss=8.708784

Batch 41290, train_perplexity=4967.6924, train_loss=8.510711

Batch 41300, train_perplexity=6098.8794, train_loss=8.71586

Batch 41310, train_perplexity=5121.5825, train_loss=8.541219

Batch 41320, train_perplexity=6774.2227, train_loss=8.82088

Batch 41330, train_perplexity=5477.276, train_loss=8.608363

Batch 41340, train_perplexity=5406.4883, train_loss=8.595355

Batch 41350, train_perplexity=5170.0083, train_loss=8.55063

Batch 41360, train_perplexity=5154.2695, train_loss=8.547581

Batch 41370, train_perplexity=5661.0737, train_loss=8.641369

Batch 41380, train_perplexity=5958.5254, train_loss=8.692578

Batch 41390, train_perplexity=5081.358, train_loss=8.533334

Batch 41400, train_perplexity=4985.894, train_loss=8.514368

Batch 41410, train_perplexity=5746.2046, train_loss=8.656295

Batch 41420, train_perplexity=5873.195, train_loss=8.678154

Batch 41430, train_perplexity=5551.3247, train_loss=8.621792

Batch 41440, train_perplexity=5206.1216, train_loss=8.5575905

Batch 41450, train_perplexity=4990.9893, train_loss=8.515389

Batch 41460, train_perplexity=5396.994, train_loss=8.593597

Batch 41470, train_perplexity=5385.8677, train_loss=8.591534
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 41480, train_perplexity=5788.898, train_loss=8.663697

Batch 41490, train_perplexity=4663.521, train_loss=8.447526

Batch 41500, train_perplexity=4419.0903, train_loss=8.393689

Batch 41510, train_perplexity=5627.7534, train_loss=8.635466

Batch 41520, train_perplexity=5134.7676, train_loss=8.54379

Batch 41530, train_perplexity=5927.02, train_loss=8.687277

Batch 41540, train_perplexity=4646.939, train_loss=8.443964

Batch 41550, train_perplexity=5601.2173, train_loss=8.630739

Batch 41560, train_perplexity=5186.745, train_loss=8.553862

Batch 41570, train_perplexity=5826.8887, train_loss=8.6702385

Batch 41580, train_perplexity=5642.188, train_loss=8.638027

Batch 41590, train_perplexity=5110.5024, train_loss=8.539053

Batch 41600, train_perplexity=5240.6377, train_loss=8.5641985

Batch 41610, train_perplexity=4541.67, train_loss=8.42105

Batch 41620, train_perplexity=6264.2896, train_loss=8.74262

Batch 41630, train_perplexity=5550.181, train_loss=8.621586

Batch 41640, train_perplexity=6287.2964, train_loss=8.746286

Batch 41650, train_perplexity=5378.3223, train_loss=8.590132

Batch 41660, train_perplexity=5897.9194, train_loss=8.682355

Batch 41670, train_perplexity=5620.009, train_loss=8.6340885

Batch 41680, train_perplexity=5825.6167, train_loss=8.67002

Batch 41690, train_perplexity=5215.8027, train_loss=8.559448

Batch 41700, train_perplexity=5028.163, train_loss=8.52281

Batch 41710, train_perplexity=6860.556, train_loss=8.833544

Batch 41720, train_perplexity=5418.293, train_loss=8.597536

Batch 41730, train_perplexity=5293.9546, train_loss=8.574321

Batch 41740, train_perplexity=4674.6797, train_loss=8.449916

Batch 41750, train_perplexity=4735.7905, train_loss=8.462904

Batch 41760, train_perplexity=4965.651, train_loss=8.5103

Batch 41770, train_perplexity=5923.675, train_loss=8.686712

Batch 41780, train_perplexity=5356.588, train_loss=8.586082

Batch 41790, train_perplexity=4653.9287, train_loss=8.445467

Batch 41800, train_perplexity=6002.745, train_loss=8.699972

Batch 41810, train_perplexity=5173.1206, train_loss=8.551231

Batch 41820, train_perplexity=5414.393, train_loss=8.596816

Batch 41830, train_perplexity=5566.3164, train_loss=8.624489

Batch 41840, train_perplexity=4761.4546, train_loss=8.468308

Batch 41850, train_perplexity=5633.6924, train_loss=8.63652

Batch 41860, train_perplexity=5288.207, train_loss=8.573235

Batch 41870, train_perplexity=5591.4556, train_loss=8.628995

Batch 41880, train_perplexity=4821.3086, train_loss=8.480801

Batch 41890, train_perplexity=5019.788, train_loss=8.521143

Batch 41900, train_perplexity=5517.324, train_loss=8.615648

Batch 41910, train_perplexity=4848.0044, train_loss=8.486322

Batch 41920, train_perplexity=5401.5356, train_loss=8.594439

Batch 41930, train_perplexity=6231.2803, train_loss=8.737337

Batch 41940, train_perplexity=5280.557, train_loss=8.571787

Batch 41950, train_perplexity=5226.927, train_loss=8.561579

Batch 41960, train_perplexity=5442.976, train_loss=8.602081

Batch 41970, train_perplexity=6389.261, train_loss=8.762374

Batch 41980, train_perplexity=4913.813, train_loss=8.499805

Batch 41990, train_perplexity=5760.283, train_loss=8.658742

Batch 42000, train_perplexity=5274.0747, train_loss=8.570559

Batch 42010, train_perplexity=5481.2524, train_loss=8.609089

Batch 42020, train_perplexity=5055.5894, train_loss=8.52825

Batch 42030, train_perplexity=5723.7476, train_loss=8.652379

Batch 42040, train_perplexity=5310.1045, train_loss=8.577367

Batch 42050, train_perplexity=4701.877, train_loss=8.455717

Batch 42060, train_perplexity=5373.3594, train_loss=8.589209

Batch 42070, train_perplexity=5526.7456, train_loss=8.617354

Batch 42080, train_perplexity=5194.5464, train_loss=8.555365

Batch 42090, train_perplexity=4716.3965, train_loss=8.4588

Batch 42100, train_perplexity=5091.083, train_loss=8.535246

Batch 42110, train_perplexity=5291.6274, train_loss=8.573881

Batch 42120, train_perplexity=5924.884, train_loss=8.686916

Batch 42130, train_perplexity=6235.424, train_loss=8.738002

Batch 42140, train_perplexity=5382.2524, train_loss=8.590862

Batch 42150, train_perplexity=5119.136, train_loss=8.540741

Batch 42160, train_perplexity=5570.236, train_loss=8.625193

Batch 42170, train_perplexity=11083.231, train_loss=9.313189

Batch 42180, train_perplexity=5341.8145, train_loss=8.583321

Batch 42190, train_perplexity=6227.4307, train_loss=8.736719

Batch 42200, train_perplexity=5703.037, train_loss=8.648754

Batch 42210, train_perplexity=5212.7144, train_loss=8.558856

Batch 42220, train_perplexity=5804.371, train_loss=8.666367

Batch 42230, train_perplexity=6183.5664, train_loss=8.7296505

Batch 42240, train_perplexity=4595.0, train_loss=8.432724

Batch 42250, train_perplexity=6431.376, train_loss=8.768944

Batch 42260, train_perplexity=5469.999, train_loss=8.607034

Batch 42270, train_perplexity=6501.474, train_loss=8.779784

Batch 42280, train_perplexity=4665.358, train_loss=8.44792

Batch 42290, train_perplexity=6010.765, train_loss=8.701307

Batch 42300, train_perplexity=4235.293, train_loss=8.351208

Batch 42310, train_perplexity=5155.322, train_loss=8.547785

Batch 42320, train_perplexity=5845.6123, train_loss=8.673447

Batch 42330, train_perplexity=5250.2773, train_loss=8.566036

Batch 42340, train_perplexity=5969.1953, train_loss=8.694367

Batch 42350, train_perplexity=6682.307, train_loss=8.807219

Batch 42360, train_perplexity=5145.4097, train_loss=8.54586

Batch 42370, train_perplexity=4902.7656, train_loss=8.497555

Batch 42380, train_perplexity=4952.206, train_loss=8.507588

Batch 42390, train_perplexity=6595.1655, train_loss=8.794092

Batch 42400, train_perplexity=4910.927, train_loss=8.499218

Batch 42410, train_perplexity=4639.7876, train_loss=8.442424

Batch 42420, train_perplexity=4985.5566, train_loss=8.5143

Batch 42430, train_perplexity=4647.1914, train_loss=8.444018

Batch 42440, train_perplexity=5621.4023, train_loss=8.634336

Batch 42450, train_perplexity=6226.813, train_loss=8.73662

Batch 42460, train_perplexity=5632.049, train_loss=8.636229

Batch 42470, train_perplexity=6176.341, train_loss=8.728481

Batch 42480, train_perplexity=5411.3423, train_loss=8.596252

Batch 42490, train_perplexity=6642.2285, train_loss=8.801203

Batch 42500, train_perplexity=6031.322, train_loss=8.704721

Batch 42510, train_perplexity=6667.6465, train_loss=8.805022

Batch 42520, train_perplexity=5968.0854, train_loss=8.694181

Batch 42530, train_perplexity=5904.6616, train_loss=8.683497

Batch 42540, train_perplexity=5300.3145, train_loss=8.575521

Batch 42550, train_perplexity=4981.7783, train_loss=8.513542

Batch 42560, train_perplexity=4810.3594, train_loss=8.478527

Batch 42570, train_perplexity=4748.8794, train_loss=8.465664

Batch 42580, train_perplexity=6118.1387, train_loss=8.719013

Batch 42590, train_perplexity=4657.445, train_loss=8.446222

Batch 42600, train_perplexity=4633.8223, train_loss=8.441137

Batch 42610, train_perplexity=4941.0303, train_loss=8.505329

Batch 42620, train_perplexity=6081.0205, train_loss=8.712928

Batch 42630, train_perplexity=4659.6265, train_loss=8.446691

Batch 42640, train_perplexity=4681.3403, train_loss=8.45134

Batch 42650, train_perplexity=5506.4956, train_loss=8.613684

Batch 42660, train_perplexity=4373.8213, train_loss=8.383392

Batch 42670, train_perplexity=5836.6772, train_loss=8.671917

Batch 42680, train_perplexity=5846.326, train_loss=8.673569

Batch 42690, train_perplexity=5617.394, train_loss=8.633623

Batch 42700, train_perplexity=5963.909, train_loss=8.693481

Batch 42710, train_perplexity=5676.492, train_loss=8.644089

Batch 42720, train_perplexity=5192.971, train_loss=8.555061

Batch 42730, train_perplexity=5001.9014, train_loss=8.517573

Batch 42740, train_perplexity=5737.2515, train_loss=8.654736

Batch 42750, train_perplexity=4490.9033, train_loss=8.409809

Batch 42760, train_perplexity=4262.1895, train_loss=8.357538

Batch 42770, train_perplexity=5968.4385, train_loss=8.694241

Batch 42780, train_perplexity=4898.055, train_loss=8.496593

Batch 42790, train_perplexity=5003.3467, train_loss=8.517862

Batch 42800, train_perplexity=5325.7505, train_loss=8.580309

Batch 42810, train_perplexity=4936.4097, train_loss=8.504394
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42820, train_perplexity=5066.691, train_loss=8.530443

Batch 42830, train_perplexity=5230.936, train_loss=8.5623455

Batch 42840, train_perplexity=4653.6357, train_loss=8.445404

Batch 42850, train_perplexity=4361.592, train_loss=8.380592

Batch 42860, train_perplexity=4734.192, train_loss=8.462566

Batch 42870, train_perplexity=6468.06, train_loss=8.7746315

Batch 42880, train_perplexity=5168.8647, train_loss=8.550408

Batch 42890, train_perplexity=4330.787, train_loss=8.373505

Batch 42900, train_perplexity=5441.4194, train_loss=8.601795

Batch 42910, train_perplexity=5127.8335, train_loss=8.5424385

Batch 42920, train_perplexity=5844.3193, train_loss=8.673225

Batch 42930, train_perplexity=5647.3667, train_loss=8.638945

Batch 42940, train_perplexity=4803.854, train_loss=8.477174

Batch 42950, train_perplexity=6658.5024, train_loss=8.80365

Batch 42960, train_perplexity=5811.816, train_loss=8.667648

Batch 42970, train_perplexity=5397.21, train_loss=8.593637

Batch 42980, train_perplexity=4922.2646, train_loss=8.501524

Batch 42990, train_perplexity=4989.9375, train_loss=8.515179

Batch 43000, train_perplexity=4943.6084, train_loss=8.505851

Batch 43010, train_perplexity=5429.186, train_loss=8.599545

Batch 43020, train_perplexity=5155.646, train_loss=8.547848

Batch 43030, train_perplexity=5634.896, train_loss=8.636734

Batch 43040, train_perplexity=5177.0786, train_loss=8.551996

Batch 43050, train_perplexity=5751.2373, train_loss=8.65717

Batch 43060, train_perplexity=4649.98, train_loss=8.444618

Batch 43070, train_perplexity=5668.6587, train_loss=8.642708

Batch 43080, train_perplexity=4701.4824, train_loss=8.455633

Batch 43090, train_perplexity=5793.6094, train_loss=8.664511

Batch 43100, train_perplexity=5872.8696, train_loss=8.678099

Batch 43110, train_perplexity=5155.2236, train_loss=8.547766

Batch 43120, train_perplexity=5937.973, train_loss=8.689123

Batch 43130, train_perplexity=5326.4414, train_loss=8.580439

Batch 43140, train_perplexity=4495.111, train_loss=8.410746

Batch 43150, train_perplexity=6308.606, train_loss=8.74967

Batch 43160, train_perplexity=4616.7207, train_loss=8.43744

Batch 43170, train_perplexity=5299.723, train_loss=8.57541

Batch 43180, train_perplexity=5508.8696, train_loss=8.614115

Batch 43190, train_perplexity=4931.112, train_loss=8.50332

Batch 43200, train_perplexity=6349.953, train_loss=8.756203

Batch 43210, train_perplexity=4974.9224, train_loss=8.512165

Batch 43220, train_perplexity=5687.2754, train_loss=8.645987

Batch 43230, train_perplexity=6197.9604, train_loss=8.731976

Batch 43240, train_perplexity=4841.9697, train_loss=8.485077

Batch 43250, train_perplexity=5665.054, train_loss=8.642072

Batch 43260, train_perplexity=5472.175, train_loss=8.607431

Batch 43270, train_perplexity=5474.033, train_loss=8.607771

Batch 43280, train_perplexity=4870.201, train_loss=8.4908905

Batch 43290, train_perplexity=4856.4727, train_loss=8.488068

Batch 43300, train_perplexity=6009.16, train_loss=8.70104

Batch 43310, train_perplexity=5612.0073, train_loss=8.632664

Batch 43320, train_perplexity=5575.0186, train_loss=8.626051

Batch 43330, train_perplexity=5237.5103, train_loss=8.5636015

Batch 43340, train_perplexity=4372.0073, train_loss=8.3829775

Batch 43350, train_perplexity=5174.9463, train_loss=8.551584

Batch 43360, train_perplexity=4789.9834, train_loss=8.474282

Batch 43370, train_perplexity=5148.3403, train_loss=8.54643

Batch 43380, train_perplexity=5437.995, train_loss=8.601166

Batch 43390, train_perplexity=4811.433, train_loss=8.47875

Batch 43400, train_perplexity=5098.488, train_loss=8.536699

Batch 43410, train_perplexity=4368.9854, train_loss=8.382286

Batch 43420, train_perplexity=5973.4775, train_loss=8.695085

Batch 43430, train_perplexity=5796.732, train_loss=8.66505

Batch 43440, train_perplexity=4921.64, train_loss=8.501397

Batch 43450, train_perplexity=5321.115, train_loss=8.579438

Batch 43460, train_perplexity=5337.516, train_loss=8.582516

Batch 43470, train_perplexity=5351.5635, train_loss=8.585144

Batch 43480, train_perplexity=5761.827, train_loss=8.65901

Batch 43490, train_perplexity=5036.754, train_loss=8.524517

Batch 43500, train_perplexity=5065.5796, train_loss=8.530224

Batch 43510, train_perplexity=5032.6484, train_loss=8.523702

Batch 43520, train_perplexity=6065.4, train_loss=8.710356

Batch 43530, train_perplexity=5251.314, train_loss=8.566234

Batch 43540, train_perplexity=5131.3604, train_loss=8.543126

Batch 43550, train_perplexity=5669.0425, train_loss=8.642776

Batch 43560, train_perplexity=5417.084, train_loss=8.597313

Batch 43570, train_perplexity=4785.4404, train_loss=8.473333

Batch 43580, train_perplexity=5383.351, train_loss=8.591066

Batch 43590, train_perplexity=5176.585, train_loss=8.551901

Batch 43600, train_perplexity=5757.9766, train_loss=8.658341

Batch 43610, train_perplexity=6254.9175, train_loss=8.741123

Batch 43620, train_perplexity=5118.9556, train_loss=8.540706

Batch 43630, train_perplexity=6369.203, train_loss=8.75923

Batch 43640, train_perplexity=5463.769, train_loss=8.605894

Batch 43650, train_perplexity=6909.3413, train_loss=8.84063

Batch 43660, train_perplexity=4747.3486, train_loss=8.465342

Batch 43670, train_perplexity=5692.224, train_loss=8.646856

Batch 43680, train_perplexity=5487.2305, train_loss=8.610179

Batch 43690, train_perplexity=6293.199, train_loss=8.747225

Batch 43700, train_perplexity=6232.3145, train_loss=8.737503

Batch 43710, train_perplexity=5564.0874, train_loss=8.624088

Batch 43720, train_perplexity=5679.9688, train_loss=8.644701

Batch 43730, train_perplexity=6347.592, train_loss=8.755831

Batch 43740, train_perplexity=5687.6606, train_loss=8.646054

Batch 43750, train_perplexity=5543.167, train_loss=8.620321

Batch 43760, train_perplexity=5675.9673, train_loss=8.643996

Batch 43770, train_perplexity=6099.083, train_loss=8.715894

Batch 43780, train_perplexity=5167.5884, train_loss=8.550161

Batch 43790, train_perplexity=5465.572, train_loss=8.606224

Batch 43800, train_perplexity=5286.9062, train_loss=8.5729885

Batch 43810, train_perplexity=6140.825, train_loss=8.722714

Batch 43820, train_perplexity=5093.555, train_loss=8.535731

Batch 43830, train_perplexity=6776.53, train_loss=8.82122

Batch 43840, train_perplexity=5391.675, train_loss=8.592611

Batch 43850, train_perplexity=4761.132, train_loss=8.468241

Batch 43860, train_perplexity=6217.343, train_loss=8.735098

Batch 43870, train_perplexity=5947.534, train_loss=8.690732

Batch 43880, train_perplexity=5117.4033, train_loss=8.540402

Batch 43890, train_perplexity=5434.185, train_loss=8.600465

Batch 43900, train_perplexity=5438.8047, train_loss=8.601315

Batch 43910, train_perplexity=5180.378, train_loss=8.552633

Batch 43920, train_perplexity=6084.698, train_loss=8.713532

Batch 43930, train_perplexity=5644.3516, train_loss=8.638411

Batch 43940, train_perplexity=7384.566, train_loss=8.907147

Batch 43950, train_perplexity=5179.1724, train_loss=8.552401

Batch 43960, train_perplexity=4781.2617, train_loss=8.47246

Batch 43970, train_perplexity=6179.275, train_loss=8.728956

Batch 43980, train_perplexity=4619.676, train_loss=8.43808

Batch 43990, train_perplexity=4575.8984, train_loss=8.428558

Batch 44000, train_perplexity=4897.0366, train_loss=8.496386

Batch 44010, train_perplexity=5308.125, train_loss=8.576994

Batch 44020, train_perplexity=5351.788, train_loss=8.585186

Batch 44030, train_perplexity=5428.922, train_loss=8.599496

Batch 44040, train_perplexity=5145.302, train_loss=8.545839

Batch 44050, train_perplexity=5811.3613, train_loss=8.66757

Batch 44060, train_perplexity=6085.337, train_loss=8.713637

Batch 44070, train_perplexity=5974.2754, train_loss=8.695218

Batch 44080, train_perplexity=6137.681, train_loss=8.722202

Batch 44090, train_perplexity=6419.99, train_loss=8.767172

Batch 44100, train_perplexity=5259.629, train_loss=8.567816

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100
Loaded 306206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306206 sentences.
Finished loading
Batch 44110, train_perplexity=4965.433, train_loss=8.510256

Batch 44120, train_perplexity=5315.217, train_loss=8.578329

Batch 44130, train_perplexity=5369.6406, train_loss=8.588516

Batch 44140, train_perplexity=5496.071, train_loss=8.611789

Batch 44150, train_perplexity=4728.092, train_loss=8.461277

Batch 44160, train_perplexity=6025.584, train_loss=8.70377

Batch 44170, train_perplexity=4839.412, train_loss=8.484549

Batch 44180, train_perplexity=6491.691, train_loss=8.778278

Batch 44190, train_perplexity=6186.9585, train_loss=8.730199

Batch 44200, train_perplexity=5252.942, train_loss=8.566544

Batch 44210, train_perplexity=6099.1123, train_loss=8.7158985

Batch 44220, train_perplexity=4873.1094, train_loss=8.4914875

Batch 44230, train_perplexity=6261.7153, train_loss=8.742209

Batch 44240, train_perplexity=5310.748, train_loss=8.577488

Batch 44250, train_perplexity=5586.243, train_loss=8.628062

Batch 44260, train_perplexity=5940.885, train_loss=8.689613

Batch 44270, train_perplexity=5168.9336, train_loss=8.550422

Batch 44280, train_perplexity=5542.665, train_loss=8.620231

Batch 44290, train_perplexity=5164.9375, train_loss=8.549648

Batch 44300, train_perplexity=4727.7495, train_loss=8.461205

Batch 44310, train_perplexity=5838.821, train_loss=8.672284

Batch 44320, train_perplexity=5090.3066, train_loss=8.535093

Batch 44330, train_perplexity=4973.556, train_loss=8.51189

Batch 44340, train_perplexity=5515.872, train_loss=8.615385

Batch 44350, train_perplexity=4783.597, train_loss=8.472948

Batch 44360, train_perplexity=4992.603, train_loss=8.515713

Batch 44370, train_perplexity=4710.9077, train_loss=8.457636

Batch 44380, train_perplexity=5818.482, train_loss=8.668795

Batch 44390, train_perplexity=6548.2803, train_loss=8.786958

Batch 44400, train_perplexity=6257.7095, train_loss=8.7415695

Batch 44410, train_perplexity=6003.329, train_loss=8.700069

Batch 44420, train_perplexity=5114.617, train_loss=8.539858

Batch 44430, train_perplexity=4959.338, train_loss=8.5090275

Batch 44440, train_perplexity=5596.209, train_loss=8.629845

Batch 44450, train_perplexity=6087.2173, train_loss=8.713946

Batch 44460, train_perplexity=5827.478, train_loss=8.67034

Batch 44470, train_perplexity=5546.2393, train_loss=8.620875

Batch 44480, train_perplexity=5749.433, train_loss=8.656857

Batch 44490, train_perplexity=5863.6523, train_loss=8.676528

Batch 44500, train_perplexity=5516.3667, train_loss=8.615475

Batch 44510, train_perplexity=5157.52, train_loss=8.548211

Batch 44520, train_perplexity=5295.8887, train_loss=8.574686

Batch 44530, train_perplexity=5213.0474, train_loss=8.55892

Batch 44540, train_perplexity=5613.549, train_loss=8.632938

Batch 44550, train_perplexity=4171.8105, train_loss=8.336105

Batch 44560, train_perplexity=4976.8584, train_loss=8.512554

Batch 44570, train_perplexity=5252.6914, train_loss=8.566496

Batch 44580, train_perplexity=5437.28, train_loss=8.601034

Batch 44590, train_perplexity=4661.8, train_loss=8.447157

Batch 44600, train_perplexity=5061.3594, train_loss=8.52939

Batch 44610, train_perplexity=6111.473, train_loss=8.717923

Batch 44620, train_perplexity=5341.8345, train_loss=8.583324

Batch 44630, train_perplexity=5606.1123, train_loss=8.631613

Batch 44640, train_perplexity=5411.6777, train_loss=8.596314

Batch 44650, train_perplexity=5243.9126, train_loss=8.564823

Batch 44660, train_perplexity=5097.5596, train_loss=8.536517

Batch 44670, train_perplexity=5732.679, train_loss=8.653938

Batch 44680, train_perplexity=5141.672, train_loss=8.545134

Batch 44690, train_perplexity=5563.472, train_loss=8.623978

Batch 44700, train_perplexity=4870.6377, train_loss=8.49098

Batch 44710, train_perplexity=5159.0645, train_loss=8.548511

Batch 44720, train_perplexity=5746.3413, train_loss=8.656319

Batch 44730, train_perplexity=4778.9097, train_loss=8.471968

Batch 44740, train_perplexity=7113.075, train_loss=8.86969

Batch 44750, train_perplexity=6521.4634, train_loss=8.782854

Batch 44760, train_perplexity=5402.8647, train_loss=8.594685

Batch 44770, train_perplexity=4925.448, train_loss=8.502171

Batch 44780, train_perplexity=5212.913, train_loss=8.558894

Batch 44790, train_perplexity=5532.0293, train_loss=8.61831

Batch 44800, train_perplexity=5711.5386, train_loss=8.650244

Batch 44810, train_perplexity=5551.2983, train_loss=8.621787

Batch 44820, train_perplexity=4742.719, train_loss=8.464366

Batch 44830, train_perplexity=5741.4277, train_loss=8.655463

Batch 44840, train_perplexity=4992.127, train_loss=8.515617

Batch 44850, train_perplexity=5590.949, train_loss=8.628904

Batch 44860, train_perplexity=5698.182, train_loss=8.6479025

Batch 44870, train_perplexity=5497.9688, train_loss=8.612134

Batch 44880, train_perplexity=5329.449, train_loss=8.581003

Batch 44890, train_perplexity=5204.9004, train_loss=8.557356

Batch 44900, train_perplexity=4584.0664, train_loss=8.430342

Batch 44910, train_perplexity=6213.502, train_loss=8.73448

Batch 44920, train_perplexity=5428.834, train_loss=8.59948

Batch 44930, train_perplexity=5374.9536, train_loss=8.589505

Batch 44940, train_perplexity=4288.2505, train_loss=8.363634

Batch 44950, train_perplexity=4874.583, train_loss=8.49179

Batch 44960, train_perplexity=4703.357, train_loss=8.456032

Batch 44970, train_perplexity=6877.411, train_loss=8.835998

Batch 44980, train_perplexity=6743.509, train_loss=8.816336

Batch 44990, train_perplexity=5389.1455, train_loss=8.592142

Batch 45000, train_perplexity=4823.5986, train_loss=8.481276

Batch 45010, train_perplexity=5819.6973, train_loss=8.6690035

Batch 45020, train_perplexity=5622.55, train_loss=8.634541

Batch 45030, train_perplexity=4607.8926, train_loss=8.435526

Batch 45040, train_perplexity=6538.028, train_loss=8.785391

Batch 45050, train_perplexity=5119.9956, train_loss=8.540909

Batch 45060, train_perplexity=4583.4326, train_loss=8.430203

Batch 45070, train_perplexity=5962.2144, train_loss=8.693197

Batch 45080, train_perplexity=5434.667, train_loss=8.6005535

Batch 45090, train_perplexity=6037.4795, train_loss=8.705742

Batch 45100, train_perplexity=5479.6216, train_loss=8.608791

Batch 45110, train_perplexity=4900.148, train_loss=8.497021

Batch 45120, train_perplexity=4515.304, train_loss=8.415228

Batch 45130, train_perplexity=5170.5903, train_loss=8.550742

Batch 45140, train_perplexity=5421.694, train_loss=8.598164

Batch 45150, train_perplexity=5258.917, train_loss=8.56768

Batch 45160, train_perplexity=5655.052, train_loss=8.640305

Batch 45170, train_perplexity=4861.537, train_loss=8.48911

Batch 45180, train_perplexity=5769.0464, train_loss=8.660262

Batch 45190, train_perplexity=5067.222, train_loss=8.530548

Batch 45200, train_perplexity=6140.058, train_loss=8.7225895

Batch 45210, train_perplexity=4990.2373, train_loss=8.515239

Batch 45220, train_perplexity=5210.761, train_loss=8.558481

Batch 45230, train_perplexity=4849.743, train_loss=8.486681

Batch 45240, train_perplexity=5287.763, train_loss=8.573151

Batch 45250, train_perplexity=5085.3667, train_loss=8.534122

Batch 45260, train_perplexity=6173.7734, train_loss=8.7280655

Batch 45270, train_perplexity=6489.9453, train_loss=8.778009

Batch 45280, train_perplexity=5542.596, train_loss=8.620218

Batch 45290, train_perplexity=5887.7812, train_loss=8.6806345

Batch 45300, train_perplexity=5959.611, train_loss=8.69276

Batch 45310, train_perplexity=5039.0215, train_loss=8.524967

Batch 45320, train_perplexity=7405.1655, train_loss=8.909933

Batch 45330, train_perplexity=4468.483, train_loss=8.404804

Batch 45340, train_perplexity=4977.6514, train_loss=8.512713

Batch 45350, train_perplexity=4525.491, train_loss=8.417481

Batch 45360, train_perplexity=5724.2007, train_loss=8.652458

Batch 45370, train_perplexity=5571.92, train_loss=8.625495

Batch 45380, train_perplexity=5035.068, train_loss=8.524182

Batch 45390, train_perplexity=5205.397, train_loss=8.557451

Batch 45400, train_perplexity=5027.233, train_loss=8.522625

Batch 45410, train_perplexity=5168.1206, train_loss=8.550264

Batch 45420, train_perplexity=6484.7607, train_loss=8.77721

Batch 45430, train_perplexity=4994.027, train_loss=8.515998
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 45440, train_perplexity=5815.2144, train_loss=8.668233

Batch 45450, train_perplexity=4785.605, train_loss=8.473368

Batch 45460, train_perplexity=5857.9067, train_loss=8.675548

Batch 45470, train_perplexity=5140.162, train_loss=8.54484

Batch 45480, train_perplexity=6393.26, train_loss=8.763

Batch 45490, train_perplexity=7311.3306, train_loss=8.897181

Batch 45500, train_perplexity=5044.3394, train_loss=8.526022

Batch 45510, train_perplexity=5423.2246, train_loss=8.598446

Batch 45520, train_perplexity=5700.6445, train_loss=8.6483345

Batch 45530, train_perplexity=5922.342, train_loss=8.686487

Batch 45540, train_perplexity=6076.574, train_loss=8.712196

Batch 45550, train_perplexity=5981.3613, train_loss=8.6964035

Batch 45560, train_perplexity=4430.5347, train_loss=8.3962755

Batch 45570, train_perplexity=4604.8965, train_loss=8.4348755

Batch 45580, train_perplexity=6129.656, train_loss=8.720894

Batch 45590, train_perplexity=5749.625, train_loss=8.65689

Batch 45600, train_perplexity=5259.619, train_loss=8.567814

Batch 45610, train_perplexity=5279.8066, train_loss=8.571645

Batch 45620, train_perplexity=5300.557, train_loss=8.575567

Batch 45630, train_perplexity=5129.178, train_loss=8.542701

Batch 45640, train_perplexity=5340.3267, train_loss=8.583042

Batch 45650, train_perplexity=5117.3594, train_loss=8.540394

Batch 45660, train_perplexity=5835.0913, train_loss=8.671645

Batch 45670, train_perplexity=5415.658, train_loss=8.59705

Batch 45680, train_perplexity=4836.801, train_loss=8.484009

Batch 45690, train_perplexity=7158.548, train_loss=8.876062

Batch 45700, train_perplexity=4446.09, train_loss=8.39978

Batch 45710, train_perplexity=5090.583, train_loss=8.535148

Batch 45720, train_perplexity=5101.421, train_loss=8.537274

Batch 45730, train_perplexity=6429.9775, train_loss=8.768726

Batch 45740, train_perplexity=4700.631, train_loss=8.455452

Batch 45750, train_perplexity=4877.5405, train_loss=8.492396

Batch 45760, train_perplexity=5824.189, train_loss=8.669775

Batch 45770, train_perplexity=5394.6987, train_loss=8.593172

Batch 45780, train_perplexity=5822.3286, train_loss=8.669456

Batch 45790, train_perplexity=6009.143, train_loss=8.701037

Batch 45800, train_perplexity=5564.119, train_loss=8.624094

Batch 45810, train_perplexity=5257.934, train_loss=8.567493

Batch 45820, train_perplexity=5763.652, train_loss=8.659327

Batch 45830, train_perplexity=6186.138, train_loss=8.730066

Batch 45840, train_perplexity=5356.2456, train_loss=8.586019

Batch 45850, train_perplexity=4989.576, train_loss=8.515106

Batch 45860, train_perplexity=4999.3213, train_loss=8.517057

Batch 45870, train_perplexity=5961.35, train_loss=8.693052

Batch 45880, train_perplexity=6461.9316, train_loss=8.773684

Batch 45890, train_perplexity=5177.7554, train_loss=8.552127

Batch 45900, train_perplexity=6211.606, train_loss=8.734175

Batch 45910, train_perplexity=5853.2827, train_loss=8.674758

Batch 45920, train_perplexity=4998.606, train_loss=8.516914

Batch 45930, train_perplexity=5180.8623, train_loss=8.552727

Batch 45940, train_perplexity=5926.3135, train_loss=8.687158

Batch 45950, train_perplexity=5044.719, train_loss=8.526097

Batch 45960, train_perplexity=4586.8125, train_loss=8.430941

Batch 45970, train_perplexity=4847.593, train_loss=8.486238

Batch 45980, train_perplexity=5945.7817, train_loss=8.690437

Batch 45990, train_perplexity=4885.3613, train_loss=8.493999

Batch 46000, train_perplexity=5865.515, train_loss=8.676846

Batch 46010, train_perplexity=5732.3076, train_loss=8.653873

Batch 46020, train_perplexity=4209.8184, train_loss=8.345175

Batch 46030, train_perplexity=5527.378, train_loss=8.617469

Batch 46040, train_perplexity=5871.2627, train_loss=8.677825

Batch 46050, train_perplexity=5057.5664, train_loss=8.528641

Batch 46060, train_perplexity=5058.001, train_loss=8.528727

Batch 46070, train_perplexity=5235.6675, train_loss=8.56325

Batch 46080, train_perplexity=6040.624, train_loss=8.706263

Batch 46090, train_perplexity=5149.9165, train_loss=8.546736

Batch 46100, train_perplexity=5548.117, train_loss=8.621214

Batch 46110, train_perplexity=4427.73, train_loss=8.395642

Batch 46120, train_perplexity=6122.8667, train_loss=8.719786

Batch 46130, train_perplexity=5282.622, train_loss=8.572178

Batch 46140, train_perplexity=4692.9175, train_loss=8.45381

Batch 46150, train_perplexity=5098.464, train_loss=8.536695

Batch 46160, train_perplexity=4313.352, train_loss=8.369471

Batch 46170, train_perplexity=5737.591, train_loss=8.654795

Batch 46180, train_perplexity=4841.6187, train_loss=8.485004

Batch 46190, train_perplexity=5285.797, train_loss=8.572779

Batch 46200, train_perplexity=5322.8813, train_loss=8.57977

Batch 46210, train_perplexity=4780.9062, train_loss=8.472385

Batch 46220, train_perplexity=5060.447, train_loss=8.52921

Batch 46230, train_perplexity=4963.0376, train_loss=8.509773

Batch 46240, train_perplexity=5832.326, train_loss=8.671171

Batch 46250, train_perplexity=5443.589, train_loss=8.602194

Batch 46260, train_perplexity=5806.6636, train_loss=8.666761

Batch 46270, train_perplexity=5346.7173, train_loss=8.584238

Batch 46280, train_perplexity=5204.012, train_loss=8.557185

Batch 46290, train_perplexity=5273.793, train_loss=8.570505

Batch 46300, train_perplexity=5297.3735, train_loss=8.574966

Batch 46310, train_perplexity=4816.9976, train_loss=8.479906

Batch 46320, train_perplexity=4515.1064, train_loss=8.415184

Batch 46330, train_perplexity=4919.3457, train_loss=8.500931

Batch 46340, train_perplexity=5593.4556, train_loss=8.629353

Batch 46350, train_perplexity=4769.103, train_loss=8.4699135

Batch 46360, train_perplexity=5842.179, train_loss=8.672859

Batch 46370, train_perplexity=5977.313, train_loss=8.695726

Batch 46380, train_perplexity=7139.335, train_loss=8.873375

Batch 46390, train_perplexity=5045.7637, train_loss=8.526304

Batch 46400, train_perplexity=6407.512, train_loss=8.765226

Batch 46410, train_perplexity=4659.8887, train_loss=8.446747

Batch 46420, train_perplexity=5504.653, train_loss=8.613349

Batch 46430, train_perplexity=6087.7397, train_loss=8.714032

Batch 46440, train_perplexity=5336.5596, train_loss=8.582336

Batch 46450, train_perplexity=5541.56, train_loss=8.620031

Batch 46460, train_perplexity=5216.638, train_loss=8.559608

Batch 46470, train_perplexity=4373.8545, train_loss=8.3834

Batch 46480, train_perplexity=7439.695, train_loss=8.914585

Batch 46490, train_perplexity=6102.469, train_loss=8.716449

Batch 46500, train_perplexity=5580.9023, train_loss=8.627106

Batch 46510, train_perplexity=4339.8545, train_loss=8.375596

Batch 46520, train_perplexity=5754.738, train_loss=8.657779

Batch 46530, train_perplexity=6340.9727, train_loss=8.754787

Batch 46540, train_perplexity=5367.4185, train_loss=8.588102

Batch 46550, train_perplexity=5039.074, train_loss=8.524978

Batch 46560, train_perplexity=5153.277, train_loss=8.547388

Batch 46570, train_perplexity=5294.0405, train_loss=8.574337

Batch 46580, train_perplexity=5882.8364, train_loss=8.679794

Batch 46590, train_perplexity=6625.0083, train_loss=8.798607

Batch 46600, train_perplexity=5515.425, train_loss=8.615304

Batch 46610, train_perplexity=5730.7554, train_loss=8.653603

Batch 46620, train_perplexity=5607.8447, train_loss=8.631922

Batch 46630, train_perplexity=6095.013, train_loss=8.715226

Batch 46640, train_perplexity=5451.2256, train_loss=8.603596

Batch 46650, train_perplexity=5464.811, train_loss=8.606085

Batch 46660, train_perplexity=5841.795, train_loss=8.672793

Batch 46670, train_perplexity=5242.682, train_loss=8.564589

Batch 46680, train_perplexity=6792.1743, train_loss=8.823526

Batch 46690, train_perplexity=5649.0366, train_loss=8.63924

Batch 46700, train_perplexity=5463.456, train_loss=8.605837

Batch 46710, train_perplexity=5002.0205, train_loss=8.517597

Batch 46720, train_perplexity=5129.966, train_loss=8.542854

Batch 46730, train_perplexity=7328.3564, train_loss=8.899507

Batch 46740, train_perplexity=4707.288, train_loss=8.456867

Batch 46750, train_perplexity=5614.197, train_loss=8.633054

Batch 46760, train_perplexity=5484.013, train_loss=8.609592

Batch 46770, train_perplexity=4325.298, train_loss=8.372236
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46780, train_perplexity=4990.642, train_loss=8.51532

Batch 46790, train_perplexity=5517.2505, train_loss=8.615635

Batch 46800, train_perplexity=5100.18, train_loss=8.537031

Batch 46810, train_perplexity=5750.9136, train_loss=8.657114

Batch 46820, train_perplexity=5133.3086, train_loss=8.543506

Batch 46830, train_perplexity=4669.5244, train_loss=8.4488125

Batch 46840, train_perplexity=5460.81, train_loss=8.605352

Batch 46850, train_perplexity=6211.8545, train_loss=8.734215

Batch 46860, train_perplexity=4266.1665, train_loss=8.358471

Batch 46870, train_perplexity=5121.568, train_loss=8.541216

Batch 46880, train_perplexity=4996.9854, train_loss=8.51659

Batch 46890, train_perplexity=5021.545, train_loss=8.521493

Batch 46900, train_perplexity=5830.6465, train_loss=8.670883

Batch 46910, train_perplexity=5060.278, train_loss=8.529177

Batch 46920, train_perplexity=6200.9165, train_loss=8.732452

Batch 46930, train_perplexity=5195.037, train_loss=8.555459

Batch 46940, train_perplexity=4689.2534, train_loss=8.453029

Batch 46950, train_perplexity=5477.9077, train_loss=8.608479

Batch 46960, train_perplexity=5869.947, train_loss=8.677601

Batch 46970, train_perplexity=5736.212, train_loss=8.654554

Batch 46980, train_perplexity=5041.002, train_loss=8.52536

Batch 46990, train_perplexity=5137.3833, train_loss=8.544299

Batch 47000, train_perplexity=5633.1714, train_loss=8.636428

Batch 47010, train_perplexity=4811.8457, train_loss=8.478836

Batch 47020, train_perplexity=6506.8955, train_loss=8.780618

Batch 47030, train_perplexity=5481.7593, train_loss=8.609181

Batch 47040, train_perplexity=6432.3022, train_loss=8.769088

Batch 47050, train_perplexity=6692.856, train_loss=8.808796

Batch 47060, train_perplexity=6560.2, train_loss=8.788776

Batch 47070, train_perplexity=5692.137, train_loss=8.646841

Batch 47080, train_perplexity=5677.889, train_loss=8.644335

Batch 47090, train_perplexity=5022.1055, train_loss=8.521605

Batch 47100, train_perplexity=4861.579, train_loss=8.489119

Batch 47110, train_perplexity=6347.9976, train_loss=8.755895

Batch 47120, train_perplexity=5602.5635, train_loss=8.63098

Batch 47130, train_perplexity=5756.2085, train_loss=8.658034

Batch 47140, train_perplexity=5466.7554, train_loss=8.606441

Batch 47150, train_perplexity=4985.609, train_loss=8.514311

Batch 47160, train_perplexity=4526.2466, train_loss=8.417648

Batch 47170, train_perplexity=5694.059, train_loss=8.647179

Batch 47180, train_perplexity=5543.696, train_loss=8.620417

Batch 47190, train_perplexity=5890.1343, train_loss=8.681034

Batch 47200, train_perplexity=4967.6875, train_loss=8.51071

Batch 47210, train_perplexity=6755.8936, train_loss=8.818171

Batch 47220, train_perplexity=4881.7427, train_loss=8.4932575

Batch 47230, train_perplexity=5708.065, train_loss=8.649635

Batch 47240, train_perplexity=4627.322, train_loss=8.4397335

Batch 47250, train_perplexity=5189.204, train_loss=8.554336

Batch 47260, train_perplexity=4888.19, train_loss=8.494577

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Batch 47270, train_perplexity=5742.047, train_loss=8.655571

Batch 47280, train_perplexity=5857.722, train_loss=8.675516

Batch 47290, train_perplexity=5371.669, train_loss=8.588894

Batch 47300, train_perplexity=5870.725, train_loss=8.677733

Batch 47310, train_perplexity=5523.4155, train_loss=8.616752

Batch 47320, train_perplexity=4969.3745, train_loss=8.511049

Batch 47330, train_perplexity=5651.332, train_loss=8.639647

Batch 47340, train_perplexity=5343.3276, train_loss=8.583604

Batch 47350, train_perplexity=5140.201, train_loss=8.5448475

Batch 47360, train_perplexity=5714.2305, train_loss=8.650715

Batch 47370, train_perplexity=5431.791, train_loss=8.600024

Batch 47380, train_perplexity=5484.4893, train_loss=8.609679

Batch 47390, train_perplexity=6595.216, train_loss=8.7941

Batch 47400, train_perplexity=5334.117, train_loss=8.581879

Batch 47410, train_perplexity=5475.067, train_loss=8.60796

Batch 47420, train_perplexity=5689.9336, train_loss=8.646454

Batch 47430, train_perplexity=5811.3057, train_loss=8.667561

Batch 47440, train_perplexity=5698.802, train_loss=8.648011

Batch 47450, train_perplexity=5923.7593, train_loss=8.686727

Batch 47460, train_perplexity=5586.2163, train_loss=8.6280575

Batch 47470, train_perplexity=5491.712, train_loss=8.610995

Batch 47480, train_perplexity=6399.494, train_loss=8.763974

Batch 47490, train_perplexity=4989.9897, train_loss=8.515189

Batch 47500, train_perplexity=6025.492, train_loss=8.703754

Batch 47510, train_perplexity=5956.134, train_loss=8.692177

Batch 47520, train_perplexity=5173.7275, train_loss=8.551349

Batch 47530, train_perplexity=4889.719, train_loss=8.49489

Batch 47540, train_perplexity=6068.304, train_loss=8.7108345

Batch 47550, train_perplexity=4985.894, train_loss=8.514368

Batch 47560, train_perplexity=5200.5146, train_loss=8.556513

Batch 47570, train_perplexity=5132.442, train_loss=8.543337

Batch 47580, train_perplexity=4734.0386, train_loss=8.462534

Batch 47590, train_perplexity=4972.418, train_loss=8.511662

Batch 47600, train_perplexity=5090.0103, train_loss=8.535035

Batch 47610, train_perplexity=5527.436, train_loss=8.617479

Batch 47620, train_perplexity=5357.9263, train_loss=8.586332

Batch 47630, train_perplexity=5385.169, train_loss=8.591404

Batch 47640, train_perplexity=5040.7134, train_loss=8.525303

Batch 47650, train_perplexity=4922.218, train_loss=8.501514

Batch 47660, train_perplexity=5587.122, train_loss=8.62822

Batch 47670, train_perplexity=4823.1523, train_loss=8.481183

Batch 47680, train_perplexity=4474.79, train_loss=8.406215

Batch 47690, train_perplexity=5558.54, train_loss=8.623091

Batch 47700, train_perplexity=5168.909, train_loss=8.550417

Batch 47710, train_perplexity=5711.114, train_loss=8.650169

Batch 47720, train_perplexity=5075.667, train_loss=8.532213

Batch 47730, train_perplexity=5872.287, train_loss=8.6779995

Batch 47740, train_perplexity=5237.8047, train_loss=8.563658

Batch 47750, train_perplexity=5023.3555, train_loss=8.521853

Batch 47760, train_perplexity=6153.318, train_loss=8.724747

Batch 47770, train_perplexity=6832.05, train_loss=8.82938

Batch 47780, train_perplexity=5564.008, train_loss=8.624074

Batch 47790, train_perplexity=5985.698, train_loss=8.697128

Batch 47800, train_perplexity=5980.392, train_loss=8.696241

Batch 47810, train_perplexity=4185.543, train_loss=8.339392

Batch 47820, train_perplexity=4460.6187, train_loss=8.403043

Batch 47830, train_perplexity=5242.8325, train_loss=8.564617

Batch 47840, train_perplexity=5109.177, train_loss=8.538794

Batch 47850, train_perplexity=5218.8276, train_loss=8.560028

Batch 47860, train_perplexity=6156.4053, train_loss=8.725248

Batch 47870, train_perplexity=5621.424, train_loss=8.63434

Batch 47880, train_perplexity=4622.36, train_loss=8.438661

Batch 47890, train_perplexity=4509.439, train_loss=8.413928

Batch 47900, train_perplexity=5476.571, train_loss=8.608234

Batch 47910, train_perplexity=4579.247, train_loss=8.42929

Batch 47920, train_perplexity=6235.162, train_loss=8.73796

Batch 47930, train_perplexity=5495.023, train_loss=8.611598

Batch 47940, train_perplexity=5981.544, train_loss=8.696434

Batch 47950, train_perplexity=4880.3413, train_loss=8.49297

Batch 47960, train_perplexity=5625.3174, train_loss=8.635033

Batch 47970, train_perplexity=6434.1978, train_loss=8.769382

Batch 47980, train_perplexity=5197.148, train_loss=8.555865

Batch 47990, train_perplexity=5798.584, train_loss=8.665369

Batch 48000, train_perplexity=4797.998, train_loss=8.475954

Batch 48010, train_perplexity=6245.678, train_loss=8.739645

Batch 48020, train_perplexity=5660.145, train_loss=8.641205

Batch 48030, train_perplexity=4836.201, train_loss=8.483885

Batch 48040, train_perplexity=5028.2686, train_loss=8.522831

Batch 48050, train_perplexity=5114.9395, train_loss=8.539921
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48060, train_perplexity=6460.5635, train_loss=8.773472

Batch 48070, train_perplexity=4961.2773, train_loss=8.5094185

Batch 48080, train_perplexity=5995.2617, train_loss=8.698725

Batch 48090, train_perplexity=5753.898, train_loss=8.657633

Batch 48100, train_perplexity=5027.1895, train_loss=8.522616

Batch 48110, train_perplexity=5376.64, train_loss=8.589819

Batch 48120, train_perplexity=5278.558, train_loss=8.571408

Batch 48130, train_perplexity=6130.9595, train_loss=8.721107

Batch 48140, train_perplexity=6689.11, train_loss=8.808236

Batch 48150, train_perplexity=5159.0693, train_loss=8.5485115

Batch 48160, train_perplexity=6364.358, train_loss=8.758469

Batch 48170, train_perplexity=5303.3486, train_loss=8.576094

Batch 48180, train_perplexity=5026.0103, train_loss=8.522382

Batch 48190, train_perplexity=4849.493, train_loss=8.4866295

Batch 48200, train_perplexity=5402.4575, train_loss=8.594609

Batch 48210, train_perplexity=5583.5747, train_loss=8.627584

Batch 48220, train_perplexity=5956.8945, train_loss=8.692305

Batch 48230, train_perplexity=4709.3623, train_loss=8.457308

Batch 48240, train_perplexity=5020.094, train_loss=8.521204

Batch 48250, train_perplexity=5408.5615, train_loss=8.595738

Batch 48260, train_perplexity=6860.242, train_loss=8.833498

Batch 48270, train_perplexity=5203.9326, train_loss=8.55717

Batch 48280, train_perplexity=5906.988, train_loss=8.683891

Batch 48290, train_perplexity=5554.846, train_loss=8.622426

Batch 48300, train_perplexity=5982.069, train_loss=8.696522

Batch 48310, train_perplexity=6027.6875, train_loss=8.704119

Batch 48320, train_perplexity=5335.9893, train_loss=8.58223

Batch 48330, train_perplexity=4642.08, train_loss=8.442918

Batch 48340, train_perplexity=5651.041, train_loss=8.639595

Batch 48350, train_perplexity=4786.3896, train_loss=8.473532

Batch 48360, train_perplexity=5288.0103, train_loss=8.573197

Batch 48370, train_perplexity=5760.7173, train_loss=8.658817

Batch 48380, train_perplexity=5080.907, train_loss=8.533245

Batch 48390, train_perplexity=4359.745, train_loss=8.380169

Batch 48400, train_perplexity=5041.05, train_loss=8.52537

Batch 48410, train_perplexity=4356.6406, train_loss=8.3794565

Batch 48420, train_perplexity=5162.249, train_loss=8.549128

Batch 48430, train_perplexity=5837.2007, train_loss=8.672007

Batch 48440, train_perplexity=4745.9365, train_loss=8.465044

Batch 48450, train_perplexity=5150.57, train_loss=8.546863

Batch 48460, train_perplexity=6860.726, train_loss=8.833569

Batch 48470, train_perplexity=5128.3223, train_loss=8.542534

Batch 48480, train_perplexity=5201.0054, train_loss=8.556607

Batch 48490, train_perplexity=5545.0176, train_loss=8.620655

Batch 48500, train_perplexity=5556.0063, train_loss=8.622635

Batch 48510, train_perplexity=5264.617, train_loss=8.568764

Batch 48520, train_perplexity=6665.237, train_loss=8.804661

Batch 48530, train_perplexity=4393.8804, train_loss=8.387968

Batch 48540, train_perplexity=6538.957, train_loss=8.785533

Batch 48550, train_perplexity=5288.1973, train_loss=8.573233

Batch 48560, train_perplexity=5130.5874, train_loss=8.542975

Batch 48570, train_perplexity=5362.3433, train_loss=8.587156

Batch 48580, train_perplexity=6764.1455, train_loss=8.819391

Batch 48590, train_perplexity=5070.0454, train_loss=8.531105

Batch 48600, train_perplexity=6229.997, train_loss=8.737131

Batch 48610, train_perplexity=5244.8228, train_loss=8.564997

Batch 48620, train_perplexity=6055.459, train_loss=8.708715

Batch 48630, train_perplexity=5007.543, train_loss=8.518701

Batch 48640, train_perplexity=4763.5845, train_loss=8.468756

Batch 48650, train_perplexity=5164.617, train_loss=8.549586

Batch 48660, train_perplexity=5751.572, train_loss=8.657228

Batch 48670, train_perplexity=5570.2305, train_loss=8.625192

Batch 48680, train_perplexity=5475.119, train_loss=8.607969

Batch 48690, train_perplexity=5464.144, train_loss=8.605963

Batch 48700, train_perplexity=4690.1167, train_loss=8.453213

Batch 48710, train_perplexity=5168.83, train_loss=8.550402

Batch 48720, train_perplexity=4991.2607, train_loss=8.515444

Batch 48730, train_perplexity=5031.5493, train_loss=8.523483

Batch 48740, train_perplexity=5390.307, train_loss=8.592358

Batch 48750, train_perplexity=6251.6133, train_loss=8.740595

Batch 48760, train_perplexity=5524.548, train_loss=8.616957

Batch 48770, train_perplexity=4756.539, train_loss=8.467276

Batch 48780, train_perplexity=5514.8726, train_loss=8.615204

Batch 48790, train_perplexity=6410.049, train_loss=8.765622

Batch 48800, train_perplexity=5907.348, train_loss=8.683952

Batch 48810, train_perplexity=4943.2075, train_loss=8.50577

Batch 48820, train_perplexity=4845.582, train_loss=8.485823

Batch 48830, train_perplexity=4531.905, train_loss=8.418898

Batch 48840, train_perplexity=4080.8796, train_loss=8.314068

Batch 48850, train_perplexity=5244.3525, train_loss=8.564907

Batch 48860, train_perplexity=6857.6646, train_loss=8.833122

Batch 48870, train_perplexity=5801.881, train_loss=8.665937

Batch 48880, train_perplexity=5529.8086, train_loss=8.6179085

Batch 48890, train_perplexity=4563.5044, train_loss=8.425846

Batch 48900, train_perplexity=6373.4443, train_loss=8.759895

Batch 48910, train_perplexity=4360.5396, train_loss=8.380351

Batch 48920, train_perplexity=6050.01, train_loss=8.707815

Batch 48930, train_perplexity=6028.0093, train_loss=8.704172

Batch 48940, train_perplexity=6120.695, train_loss=8.719431

Batch 48950, train_perplexity=5376.394, train_loss=8.589773

Batch 48960, train_perplexity=5190.9956, train_loss=8.554681

Batch 48970, train_perplexity=6330.012, train_loss=8.7530575

Batch 48980, train_perplexity=4724.153, train_loss=8.4604435

Batch 48990, train_perplexity=4595.2803, train_loss=8.432785

Batch 49000, train_perplexity=4451.168, train_loss=8.400922

Batch 49010, train_perplexity=5314.3096, train_loss=8.578158

Batch 49020, train_perplexity=5667.232, train_loss=8.642456

Batch 49030, train_perplexity=5910.8027, train_loss=8.684537

Batch 49040, train_perplexity=6984.096, train_loss=8.851391

Batch 49050, train_perplexity=5785.3164, train_loss=8.663078

Batch 49060, train_perplexity=4361.75, train_loss=8.380629

Batch 49070, train_perplexity=5042.2617, train_loss=8.52561

Batch 49080, train_perplexity=5711.871, train_loss=8.650302

Batch 49090, train_perplexity=6158.619, train_loss=8.725608

Batch 49100, train_perplexity=5868.346, train_loss=8.677328

Batch 49110, train_perplexity=5229.2, train_loss=8.562014

Batch 49120, train_perplexity=5627.646, train_loss=8.635447

Batch 49130, train_perplexity=5492.7646, train_loss=8.611187

Batch 49140, train_perplexity=5064.3335, train_loss=8.529978

Batch 49150, train_perplexity=6123.807, train_loss=8.719939

Batch 49160, train_perplexity=5692.0176, train_loss=8.64682

Batch 49170, train_perplexity=5292.6724, train_loss=8.574079

Batch 49180, train_perplexity=5935.1196, train_loss=8.6886425

Batch 49190, train_perplexity=6459.9966, train_loss=8.773384

Batch 49200, train_perplexity=4900.1343, train_loss=8.497018

Batch 49210, train_perplexity=5901.211, train_loss=8.682913

Batch 49220, train_perplexity=6538.0156, train_loss=8.785389

Batch 49230, train_perplexity=5370.6953, train_loss=8.588713

Batch 49240, train_perplexity=5935.6406, train_loss=8.68873

Batch 49250, train_perplexity=5125.423, train_loss=8.541968

Batch 49260, train_perplexity=4907.5654, train_loss=8.498533

Batch 49270, train_perplexity=4761.2046, train_loss=8.468256

Batch 49280, train_perplexity=4939.796, train_loss=8.505079

Batch 49290, train_perplexity=6221.554, train_loss=8.735775

Batch 49300, train_perplexity=6509.347, train_loss=8.780994

Batch 49310, train_perplexity=5220.0522, train_loss=8.560263

Batch 49320, train_perplexity=5419.0317, train_loss=8.597672

Batch 49330, train_perplexity=5309.492, train_loss=8.577251

Batch 49340, train_perplexity=5134.0283, train_loss=8.543646

Batch 49350, train_perplexity=5463.6646, train_loss=8.605875

Batch 49360, train_perplexity=4671.0254, train_loss=8.449134

Batch 49370, train_perplexity=5293.7275, train_loss=8.574278

Batch 49380, train_perplexity=5427.2344, train_loss=8.599185

Batch 49390, train_perplexity=5243.2925, train_loss=8.564705
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 49400, train_perplexity=4759.0845, train_loss=8.467811

Batch 49410, train_perplexity=5305.6855, train_loss=8.576534

Batch 49420, train_perplexity=4674.127, train_loss=8.449798

Batch 49430, train_perplexity=5716.9663, train_loss=8.651194

Batch 49440, train_perplexity=5628.8916, train_loss=8.635668

Batch 49450, train_perplexity=5135.6245, train_loss=8.543957

Batch 49460, train_perplexity=6729.478, train_loss=8.814253

Batch 49470, train_perplexity=5268.6606, train_loss=8.569531

Batch 49480, train_perplexity=5387.1826, train_loss=8.591778

Batch 49490, train_perplexity=4446.87, train_loss=8.399956

Batch 49500, train_perplexity=5196.172, train_loss=8.555677

Batch 49510, train_perplexity=4431.908, train_loss=8.396585

Batch 49520, train_perplexity=4713.0557, train_loss=8.458092

Batch 49530, train_perplexity=5648.9395, train_loss=8.639223

Batch 49540, train_perplexity=5848.54, train_loss=8.673947

Batch 49550, train_perplexity=5500.2764, train_loss=8.612554

Batch 49560, train_perplexity=4592.6343, train_loss=8.432209

Batch 49570, train_perplexity=9193.563, train_loss=9.126259

Batch 49580, train_perplexity=5573.2646, train_loss=8.625736

Batch 49590, train_perplexity=5141.829, train_loss=8.545164

Batch 49600, train_perplexity=4922.2554, train_loss=8.501522

Batch 49610, train_perplexity=4672.4775, train_loss=8.449445

Batch 49620, train_perplexity=4856.4077, train_loss=8.488054

Batch 49630, train_perplexity=5788.236, train_loss=8.663583

Batch 49640, train_perplexity=4976.8965, train_loss=8.512562

Batch 49650, train_perplexity=4449.4067, train_loss=8.400526

Batch 49660, train_perplexity=6354.3633, train_loss=8.756897

Batch 49670, train_perplexity=5364.5576, train_loss=8.587569

Batch 49680, train_perplexity=5501.2783, train_loss=8.612736

Batch 49690, train_perplexity=4886.95, train_loss=8.494324

Batch 49700, train_perplexity=5925.6807, train_loss=8.687051

Batch 49710, train_perplexity=7277.0503, train_loss=8.892481

Batch 49720, train_perplexity=5273.1597, train_loss=8.570385

Batch 49730, train_perplexity=6297.1855, train_loss=8.747858

Batch 49740, train_perplexity=4133.562, train_loss=8.326895

Batch 49750, train_perplexity=5840.597, train_loss=8.672588

Batch 49760, train_perplexity=5208.585, train_loss=8.5580635

Batch 49770, train_perplexity=5913.8022, train_loss=8.685044

Batch 49780, train_perplexity=5868.3066, train_loss=8.677321

Batch 49790, train_perplexity=4889.104, train_loss=8.494764

Batch 49800, train_perplexity=4755.641, train_loss=8.467087

Batch 49810, train_perplexity=5206.6084, train_loss=8.557684

Batch 49820, train_perplexity=4533.2407, train_loss=8.419192

Batch 49830, train_perplexity=5746.8564, train_loss=8.656408

Batch 49840, train_perplexity=4748.064, train_loss=8.465492

Batch 49850, train_perplexity=5661.884, train_loss=8.641512

Batch 49860, train_perplexity=5915.094, train_loss=8.685263

Batch 49870, train_perplexity=5279.117, train_loss=8.571514

Batch 49880, train_perplexity=6801.2876, train_loss=8.824867

Batch 49890, train_perplexity=6542.338, train_loss=8.78605

Batch 49900, train_perplexity=5231.6694, train_loss=8.562486

Batch 49910, train_perplexity=5177.207, train_loss=8.552021

Batch 49920, train_perplexity=6793.7354, train_loss=8.823756

Batch 49930, train_perplexity=5596.929, train_loss=8.629973

Batch 49940, train_perplexity=4899.3306, train_loss=8.496854

Batch 49950, train_perplexity=4494.099, train_loss=8.410521

Batch 49960, train_perplexity=5966.708, train_loss=8.693951

Batch 49970, train_perplexity=5474.1113, train_loss=8.607785

Batch 49980, train_perplexity=4933.346, train_loss=8.503773

Batch 49990, train_perplexity=5313.975, train_loss=8.578095

Batch 50000, train_perplexity=4160.3164, train_loss=8.333346

Batch 50010, train_perplexity=4837.696, train_loss=8.484194

Batch 50020, train_perplexity=4879.983, train_loss=8.492897

Batch 50030, train_perplexity=5505.792, train_loss=8.613556

Batch 50040, train_perplexity=5767.0005, train_loss=8.659907

Batch 50050, train_perplexity=5850.275, train_loss=8.674244

Batch 50060, train_perplexity=4490.5093, train_loss=8.409721

Batch 50070, train_perplexity=5773.7197, train_loss=8.661072

Batch 50080, train_perplexity=5323.11, train_loss=8.579813

Batch 50090, train_perplexity=4778.8823, train_loss=8.471962

Batch 50100, train_perplexity=6302.196, train_loss=8.748653

Batch 50110, train_perplexity=5658.7476, train_loss=8.640958

Batch 50120, train_perplexity=5405.6064, train_loss=8.595192

Batch 50130, train_perplexity=4004.1663, train_loss=8.295091

Batch 50140, train_perplexity=4888.0503, train_loss=8.494549

Batch 50150, train_perplexity=5118.15, train_loss=8.540548

Batch 50160, train_perplexity=5747.2295, train_loss=8.656473

Batch 50170, train_perplexity=6038.4697, train_loss=8.705906

Batch 50180, train_perplexity=5216.9966, train_loss=8.559677

Batch 50190, train_perplexity=5315.308, train_loss=8.578346

Batch 50200, train_perplexity=4520.328, train_loss=8.41634

Batch 50210, train_perplexity=4660.1196, train_loss=8.446796

Batch 50220, train_perplexity=5753.514, train_loss=8.657566

Batch 50230, train_perplexity=5489.2456, train_loss=8.610546

Batch 50240, train_perplexity=6482.72, train_loss=8.7768955

Batch 50250, train_perplexity=4506.249, train_loss=8.41322

Batch 50260, train_perplexity=5459.248, train_loss=8.605066

Batch 50270, train_perplexity=6433.848, train_loss=8.769328

Batch 50280, train_perplexity=6525.7314, train_loss=8.783508

Batch 50290, train_perplexity=5623.687, train_loss=8.634743

Batch 50300, train_perplexity=5507.5776, train_loss=8.61388

Batch 50310, train_perplexity=4472.7036, train_loss=8.405748

Batch 50320, train_perplexity=4403.3022, train_loss=8.39011

Batch 50330, train_perplexity=5216.6035, train_loss=8.559602

Batch 50340, train_perplexity=5842.5806, train_loss=8.672928

Batch 50350, train_perplexity=6315.6914, train_loss=8.7507925

Batch 50360, train_perplexity=5632.876, train_loss=8.636375

Batch 50370, train_perplexity=5755.226, train_loss=8.657864

Batch 50380, train_perplexity=5179.6865, train_loss=8.5525

Batch 50390, train_perplexity=4801.5137, train_loss=8.4766865

Batch 50400, train_perplexity=5453.992, train_loss=8.604103

Batch 50410, train_perplexity=6527.5176, train_loss=8.783782

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Batch 50420, train_perplexity=4809.2354, train_loss=8.478293

Batch 50430, train_perplexity=5296.626, train_loss=8.574825

Batch 50440, train_perplexity=5525.201, train_loss=8.617075

Batch 50450, train_perplexity=4809.7124, train_loss=8.478393

Batch 50460, train_perplexity=4981.1177, train_loss=8.51341

Batch 50470, train_perplexity=4789.8057, train_loss=8.474245

Batch 50480, train_perplexity=5745.6016, train_loss=8.65619

Batch 50490, train_perplexity=5040.7085, train_loss=8.525302

Batch 50500, train_perplexity=5325.6387, train_loss=8.580288

Batch 50510, train_perplexity=5385.6157, train_loss=8.591487

Batch 50520, train_perplexity=7190.089, train_loss=8.880459

Batch 50530, train_perplexity=5079.308, train_loss=8.53293

Batch 50540, train_perplexity=5051.8545, train_loss=8.527511

Batch 50550, train_perplexity=5253.8086, train_loss=8.566709

Batch 50560, train_perplexity=4938.326, train_loss=8.504782

Batch 50570, train_perplexity=5868.626, train_loss=8.677376

Batch 50580, train_perplexity=4855.75, train_loss=8.487919

Batch 50590, train_perplexity=5908.847, train_loss=8.684206

Batch 50600, train_perplexity=4687.97, train_loss=8.452755

Batch 50610, train_perplexity=5495.9346, train_loss=8.611764

Batch 50620, train_perplexity=5036.9365, train_loss=8.524553

Batch 50630, train_perplexity=5314.8315, train_loss=8.578257

Batch 50640, train_perplexity=6007.029, train_loss=8.7006855

Batch 50650, train_perplexity=5569.46, train_loss=8.625053

Batch 50660, train_perplexity=4811.2036, train_loss=8.478703
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 50670, train_perplexity=6407.7075, train_loss=8.765257

Batch 50680, train_perplexity=6368.0737, train_loss=8.759052

Batch 50690, train_perplexity=5039.6704, train_loss=8.525096

Batch 50700, train_perplexity=4990.7563, train_loss=8.515343

Batch 50710, train_perplexity=5428.746, train_loss=8.599463

Batch 50720, train_perplexity=5811.2173, train_loss=8.667545

Batch 50730, train_perplexity=5001.5625, train_loss=8.517506

Batch 50740, train_perplexity=4958.7417, train_loss=8.508907

Batch 50750, train_perplexity=4384.2783, train_loss=8.38578

Batch 50760, train_perplexity=6016.236, train_loss=8.702217

Batch 50770, train_perplexity=4925.918, train_loss=8.502266

Batch 50780, train_perplexity=4825.964, train_loss=8.481766

Batch 50790, train_perplexity=5725.2163, train_loss=8.652636

Batch 50800, train_perplexity=5630.448, train_loss=8.635944

Batch 50810, train_perplexity=5986.5205, train_loss=8.697266

Batch 50820, train_perplexity=6520.605, train_loss=8.782722

Batch 50830, train_perplexity=5731.575, train_loss=8.653746

Batch 50840, train_perplexity=6268.981, train_loss=8.743369

Batch 50850, train_perplexity=6453.9434, train_loss=8.772447

Batch 50860, train_perplexity=5275.423, train_loss=8.570814

Batch 50870, train_perplexity=6154.5913, train_loss=8.724954

Batch 50880, train_perplexity=6058.7803, train_loss=8.709264

Batch 50890, train_perplexity=5236.796, train_loss=8.563465

Batch 50900, train_perplexity=5155.2334, train_loss=8.547768

Batch 50910, train_perplexity=5258.897, train_loss=8.567677

Batch 50920, train_perplexity=4853.87, train_loss=8.487532

Batch 50930, train_perplexity=5544.1875, train_loss=8.620505

Batch 50940, train_perplexity=4681.358, train_loss=8.451344

Batch 50950, train_perplexity=4631.326, train_loss=8.4405985

Batch 50960, train_perplexity=5532.0767, train_loss=8.618319

Batch 50970, train_perplexity=5627.0825, train_loss=8.635346

Batch 50980, train_perplexity=4931.319, train_loss=8.503362

Batch 50990, train_perplexity=6517.5713, train_loss=8.782257

Batch 51000, train_perplexity=4586.41, train_loss=8.430853

Batch 51010, train_perplexity=5490.251, train_loss=8.610729

Batch 51020, train_perplexity=5706.4756, train_loss=8.649357

Batch 51030, train_perplexity=6343.8276, train_loss=8.755238

Batch 51040, train_perplexity=5545.0386, train_loss=8.620659

Batch 51050, train_perplexity=5135.938, train_loss=8.544018

Batch 51060, train_perplexity=6030.045, train_loss=8.70451

Batch 51070, train_perplexity=4155.5063, train_loss=8.33219

Batch 51080, train_perplexity=4843.3647, train_loss=8.485365

Batch 51090, train_perplexity=5034.252, train_loss=8.52402

Batch 51100, train_perplexity=4589.6523, train_loss=8.43156

Batch 51110, train_perplexity=6670.324, train_loss=8.805424

Batch 51120, train_perplexity=4931.06, train_loss=8.503309

Batch 51130, train_perplexity=6602.321, train_loss=8.7951765

Batch 51140, train_perplexity=5856.153, train_loss=8.675248

Batch 51150, train_perplexity=6192.017, train_loss=8.731016

Batch 51160, train_perplexity=5589.014, train_loss=8.628558

Batch 51170, train_perplexity=5394.133, train_loss=8.593067

Batch 51180, train_perplexity=6009.8477, train_loss=8.701155

Batch 51190, train_perplexity=4952.7017, train_loss=8.5076885

Batch 51200, train_perplexity=5318.827, train_loss=8.579008

Batch 51210, train_perplexity=7050.307, train_loss=8.8608265

Batch 51220, train_perplexity=5384.681, train_loss=8.591313

Batch 51230, train_perplexity=5307.1025, train_loss=8.576801

Batch 51240, train_perplexity=5548.509, train_loss=8.6212845

Batch 51250, train_perplexity=7155.0806, train_loss=8.875578

Batch 51260, train_perplexity=5573.9023, train_loss=8.625851

Batch 51270, train_perplexity=4893.503, train_loss=8.495664

Batch 51280, train_perplexity=5487.309, train_loss=8.610193

Batch 51290, train_perplexity=5294.404, train_loss=8.574406

Batch 51300, train_perplexity=5067.9907, train_loss=8.5307

Batch 51310, train_perplexity=4391.0444, train_loss=8.387322

Batch 51320, train_perplexity=5038.387, train_loss=8.524841

Batch 51330, train_perplexity=4803.6753, train_loss=8.477137

Batch 51340, train_perplexity=4651.1245, train_loss=8.444864

Batch 51350, train_perplexity=5733.871, train_loss=8.654146

Batch 51360, train_perplexity=5880.3906, train_loss=8.6793785

Batch 51370, train_perplexity=6567.7246, train_loss=8.789923

Batch 51380, train_perplexity=6464.397, train_loss=8.774065

Batch 51390, train_perplexity=5165.3613, train_loss=8.54973

Batch 51400, train_perplexity=7182.441, train_loss=8.879395

Batch 51410, train_perplexity=5757.071, train_loss=8.658184

Batch 51420, train_perplexity=4922.9126, train_loss=8.501656

Batch 51430, train_perplexity=4183.021, train_loss=8.338789

Batch 51440, train_perplexity=5682.7427, train_loss=8.645189

Batch 51450, train_perplexity=4598.3403, train_loss=8.433451

Batch 51460, train_perplexity=5023.9546, train_loss=8.521973

Batch 51470, train_perplexity=6582.661, train_loss=8.792194

Batch 51480, train_perplexity=6545.552, train_loss=8.786541

Batch 51490, train_perplexity=6242.5044, train_loss=8.739137

Batch 51500, train_perplexity=5897.52, train_loss=8.682287

Batch 51510, train_perplexity=5235.033, train_loss=8.563128

Batch 51520, train_perplexity=6610.505, train_loss=8.796415

Batch 51530, train_perplexity=5988.9985, train_loss=8.6976795

Batch 51540, train_perplexity=6576.869, train_loss=8.791314

Batch 51550, train_perplexity=4785.961, train_loss=8.473442

Batch 51560, train_perplexity=4811.6533, train_loss=8.478796

Batch 51570, train_perplexity=4176.691, train_loss=8.337275

Batch 51580, train_perplexity=4961.263, train_loss=8.509416

Batch 51590, train_perplexity=4655.0205, train_loss=8.445702

Batch 51600, train_perplexity=5683.534, train_loss=8.6453285

Batch 51610, train_perplexity=5269.4946, train_loss=8.56969

Batch 51620, train_perplexity=5980.9624, train_loss=8.696337

Batch 51630, train_perplexity=5150.899, train_loss=8.5469265

Batch 51640, train_perplexity=7008.0615, train_loss=8.854816

Batch 51650, train_perplexity=4783.661, train_loss=8.472961

Batch 51660, train_perplexity=5644.5454, train_loss=8.638445

Batch 51670, train_perplexity=4895.697, train_loss=8.496112

Batch 51680, train_perplexity=6160.9746, train_loss=8.72599

Batch 51690, train_perplexity=6532.032, train_loss=8.784473

Batch 51700, train_perplexity=5564.4165, train_loss=8.624147

Batch 51710, train_perplexity=6375.888, train_loss=8.760279

Batch 51720, train_perplexity=5681.973, train_loss=8.645054

Batch 51730, train_perplexity=5970.2656, train_loss=8.694547

Batch 51740, train_perplexity=5568.6953, train_loss=8.624916

Batch 51750, train_perplexity=5738.297, train_loss=8.654918

Batch 51760, train_perplexity=5197.51, train_loss=8.555935

Batch 51770, train_perplexity=5618.0796, train_loss=8.633745

Batch 51780, train_perplexity=6344.36, train_loss=8.7553215

Batch 51790, train_perplexity=7329.587, train_loss=8.899674

Batch 51800, train_perplexity=5268.786, train_loss=8.569555

Batch 51810, train_perplexity=5170.4326, train_loss=8.550712

Batch 51820, train_perplexity=4716.5356, train_loss=8.45883

Batch 51830, train_perplexity=5369.19, train_loss=8.588432

Batch 51840, train_perplexity=5136.154, train_loss=8.54406

Batch 51850, train_perplexity=5230.7466, train_loss=8.562309

Batch 51860, train_perplexity=5266.9224, train_loss=8.569201

Batch 51870, train_perplexity=5643.0273, train_loss=8.638176

Batch 51880, train_perplexity=4738.203, train_loss=8.463413

Batch 51890, train_perplexity=5729.8975, train_loss=8.653453

Batch 51900, train_perplexity=5233.79, train_loss=8.562891

Batch 51910, train_perplexity=6004.1133, train_loss=8.7002

Batch 51920, train_perplexity=4952.9614, train_loss=8.507741

Batch 51930, train_perplexity=6649.758, train_loss=8.802336

Batch 51940, train_perplexity=6356.157, train_loss=8.757179

Batch 51950, train_perplexity=5297.2524, train_loss=8.574944

Batch 51960, train_perplexity=6346.3086, train_loss=8.755629

Batch 51970, train_perplexity=5676.3296, train_loss=8.64406

Batch 51980, train_perplexity=6065.759, train_loss=8.710415

Batch 51990, train_perplexity=5262.1177, train_loss=8.568289

Batch 52000, train_perplexity=5432.3403, train_loss=8.600125
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52010, train_perplexity=5220.6943, train_loss=8.560386

Batch 52020, train_perplexity=5005.0073, train_loss=8.518194

Batch 52030, train_perplexity=5770.758, train_loss=8.660559

Batch 52040, train_perplexity=7313.939, train_loss=8.897537

Batch 52050, train_perplexity=6173.279, train_loss=8.727985

Batch 52060, train_perplexity=5583.0635, train_loss=8.627493

Batch 52070, train_perplexity=5382.2217, train_loss=8.590857

Batch 52080, train_perplexity=5348.584, train_loss=8.584587

Batch 52090, train_perplexity=5214.798, train_loss=8.559256

Batch 52100, train_perplexity=4909.438, train_loss=8.498915

Batch 52110, train_perplexity=5877.935, train_loss=8.678961

Batch 52120, train_perplexity=4918.2993, train_loss=8.500718

Batch 52130, train_perplexity=4646.8506, train_loss=8.443945

Batch 52140, train_perplexity=4742.3984, train_loss=8.464298

Batch 52150, train_perplexity=4505.0503, train_loss=8.412954

Batch 52160, train_perplexity=5006.449, train_loss=8.518482

Batch 52170, train_perplexity=5114.3833, train_loss=8.539812

Batch 52180, train_perplexity=5131.184, train_loss=8.543092

Batch 52190, train_perplexity=4895.0522, train_loss=8.49598

Batch 52200, train_perplexity=5882.023, train_loss=8.679656

Batch 52210, train_perplexity=5623.5527, train_loss=8.634719

Batch 52220, train_perplexity=5824.7666, train_loss=8.669874

Batch 52230, train_perplexity=6688.938, train_loss=8.80821

Batch 52240, train_perplexity=5367.7974, train_loss=8.588173

Batch 52250, train_perplexity=4876.6475, train_loss=8.492213

Batch 52260, train_perplexity=5179.6963, train_loss=8.552502

Batch 52270, train_perplexity=5134.498, train_loss=8.543737

Batch 52280, train_perplexity=5521.6196, train_loss=8.616426

Batch 52290, train_perplexity=4982.363, train_loss=8.5136595

Batch 52300, train_perplexity=5153.788, train_loss=8.547487

Batch 52310, train_perplexity=4073.9426, train_loss=8.3123665

Batch 52320, train_perplexity=4601.876, train_loss=8.434219

Batch 52330, train_perplexity=4935.652, train_loss=8.50424

Batch 52340, train_perplexity=5902.494, train_loss=8.68313

Batch 52350, train_perplexity=4839.523, train_loss=8.484571

Batch 52360, train_perplexity=5827.5225, train_loss=8.670347

Batch 52370, train_perplexity=5370.47, train_loss=8.588671

Batch 52380, train_perplexity=4757.374, train_loss=8.467451

Batch 52390, train_perplexity=5154.348, train_loss=8.547596

Batch 52400, train_perplexity=4878.559, train_loss=8.492605

Batch 52410, train_perplexity=4831.849, train_loss=8.482985

Batch 52420, train_perplexity=5682.1846, train_loss=8.645091

Batch 52430, train_perplexity=5836.0874, train_loss=8.671816

Batch 52440, train_perplexity=5186.478, train_loss=8.55381

Batch 52450, train_perplexity=5103.849, train_loss=8.53775

Batch 52460, train_perplexity=5030.024, train_loss=8.52318

Batch 52470, train_perplexity=5264.6323, train_loss=8.568767

Batch 52480, train_perplexity=5458.384, train_loss=8.604908

Batch 52490, train_perplexity=4576.1167, train_loss=8.428606

Batch 52500, train_perplexity=5165.5386, train_loss=8.549765

Batch 52510, train_perplexity=5104.8325, train_loss=8.537943

Batch 52520, train_perplexity=6431.621, train_loss=8.768982

Batch 52530, train_perplexity=6060.826, train_loss=8.709601

Batch 52540, train_perplexity=5182.226, train_loss=8.55299

Batch 52550, train_perplexity=5260.0005, train_loss=8.567886

Batch 52560, train_perplexity=5911.564, train_loss=8.684666

Batch 52570, train_perplexity=5740.7324, train_loss=8.655342

Batch 52580, train_perplexity=5215.2505, train_loss=8.559342

Batch 52590, train_perplexity=5124.861, train_loss=8.541859

Batch 52600, train_perplexity=5425.2217, train_loss=8.598814

Batch 52610, train_perplexity=4859.5024, train_loss=8.488691

Batch 52620, train_perplexity=5646.887, train_loss=8.63886

Batch 52630, train_perplexity=5030.945, train_loss=8.523363

Batch 52640, train_perplexity=5204.0566, train_loss=8.557194

Batch 52650, train_perplexity=6210.528, train_loss=8.734001

Batch 52660, train_perplexity=4445.3857, train_loss=8.399622

Batch 52670, train_perplexity=5874.4604, train_loss=8.6783695

Batch 52680, train_perplexity=6554.397, train_loss=8.787891

Batch 52690, train_perplexity=5010.2607, train_loss=8.519243

Batch 52700, train_perplexity=6367.2354, train_loss=8.758921

Batch 52710, train_perplexity=6525.7188, train_loss=8.783506

Batch 52720, train_perplexity=6398.011, train_loss=8.763742

Batch 52730, train_perplexity=5142.3, train_loss=8.545256

Batch 52740, train_perplexity=5412.4985, train_loss=8.596466

Batch 52750, train_perplexity=5930.763, train_loss=8.687908

Batch 52760, train_perplexity=6184.416, train_loss=8.729788

Batch 52770, train_perplexity=5024.6636, train_loss=8.522114

Batch 52780, train_perplexity=6093.9263, train_loss=8.715048

Batch 52790, train_perplexity=4791.5825, train_loss=8.474616

Batch 52800, train_perplexity=4476.2456, train_loss=8.40654

Batch 52810, train_perplexity=5317.3613, train_loss=8.5787325

Batch 52820, train_perplexity=4275.893, train_loss=8.360748

Batch 52830, train_perplexity=5011.6084, train_loss=8.519512

Batch 52840, train_perplexity=5598.798, train_loss=8.630307

Batch 52850, train_perplexity=4880.216, train_loss=8.492945

Batch 52860, train_perplexity=5476.6963, train_loss=8.608257

Batch 52870, train_perplexity=4218.5557, train_loss=8.347248

Batch 52880, train_perplexity=5987.8394, train_loss=8.697486

Batch 52890, train_perplexity=6844.924, train_loss=8.831263

Batch 52900, train_perplexity=5587.8896, train_loss=8.628357

Batch 52910, train_perplexity=6180.206, train_loss=8.729107

Batch 52920, train_perplexity=4395.8335, train_loss=8.388412

Batch 52930, train_perplexity=5681.583, train_loss=8.644985

Batch 52940, train_perplexity=6079.0376, train_loss=8.712602

Batch 52950, train_perplexity=7002.5566, train_loss=8.854031

Batch 52960, train_perplexity=5243.9873, train_loss=8.564837

Batch 52970, train_perplexity=5682.331, train_loss=8.645117

Batch 52980, train_perplexity=5256.4, train_loss=8.567202

Batch 52990, train_perplexity=5603.4185, train_loss=8.631132

Batch 53000, train_perplexity=5481.1006, train_loss=8.609061

Batch 53010, train_perplexity=5083.5728, train_loss=8.53377

Batch 53020, train_perplexity=6460.2803, train_loss=8.773428

Batch 53030, train_perplexity=4716.7876, train_loss=8.458883

Batch 53040, train_perplexity=6444.9883, train_loss=8.771058

Batch 53050, train_perplexity=5283.373, train_loss=8.57232

Batch 53060, train_perplexity=5426.3755, train_loss=8.599027

Batch 53070, train_perplexity=5446.2007, train_loss=8.602674

Batch 53080, train_perplexity=5496.7524, train_loss=8.611913

Batch 53090, train_perplexity=5261.0034, train_loss=8.568077

Batch 53100, train_perplexity=5625.344, train_loss=8.635037

Batch 53110, train_perplexity=6280.141, train_loss=8.745148

Batch 53120, train_perplexity=6510.626, train_loss=8.781191

Batch 53130, train_perplexity=5564.1885, train_loss=8.624106

Batch 53140, train_perplexity=5131.0425, train_loss=8.543064

Batch 53150, train_perplexity=5829.985, train_loss=8.67077

Batch 53160, train_perplexity=5790.1406, train_loss=8.663912

Batch 53170, train_perplexity=4601.2617, train_loss=8.434086

Batch 53180, train_perplexity=5272.727, train_loss=8.570303

Batch 53190, train_perplexity=5043.8677, train_loss=8.5259285

Batch 53200, train_perplexity=5903.3047, train_loss=8.683268

Batch 53210, train_perplexity=5719.737, train_loss=8.651678

Batch 53220, train_perplexity=6747.755, train_loss=8.816965

Batch 53230, train_perplexity=5869.275, train_loss=8.677486

Batch 53240, train_perplexity=5329.8154, train_loss=8.581072

Batch 53250, train_perplexity=4981.455, train_loss=8.513477

Batch 53260, train_perplexity=6903.4004, train_loss=8.839769

Batch 53270, train_perplexity=4943.5234, train_loss=8.505834

Batch 53280, train_perplexity=6946.936, train_loss=8.846056

Batch 53290, train_perplexity=5025.162, train_loss=8.522213

Batch 53300, train_perplexity=4630.253, train_loss=8.440367

Batch 53310, train_perplexity=5027.664, train_loss=8.522711

Batch 53320, train_perplexity=5568.6743, train_loss=8.624912

Batch 53330, train_perplexity=5540.213, train_loss=8.619788

Batch 53340, train_perplexity=4796.845, train_loss=8.475714
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53350, train_perplexity=5444.378, train_loss=8.602339

Batch 53360, train_perplexity=5334.2695, train_loss=8.581907

Batch 53370, train_perplexity=4649.5986, train_loss=8.444536

Batch 53380, train_perplexity=5705.5884, train_loss=8.649201

Batch 53390, train_perplexity=5797.9536, train_loss=8.66526

Batch 53400, train_perplexity=7177.6816, train_loss=8.878732

Batch 53410, train_perplexity=6270.2305, train_loss=8.743568

Batch 53420, train_perplexity=4711.4917, train_loss=8.45776

Batch 53430, train_perplexity=4952.7866, train_loss=8.507706

Batch 53440, train_perplexity=5516.577, train_loss=8.615513

Batch 53450, train_perplexity=5207.552, train_loss=8.557865

Batch 53460, train_perplexity=4293.464, train_loss=8.364849

Batch 53470, train_perplexity=4927.694, train_loss=8.502626

Batch 53480, train_perplexity=6109.1543, train_loss=8.717544

Batch 53490, train_perplexity=5743.876, train_loss=8.6558895

Batch 53500, train_perplexity=5763.014, train_loss=8.659216

Batch 53510, train_perplexity=4739.0073, train_loss=8.463583

Batch 53520, train_perplexity=5285.575, train_loss=8.572737

Batch 53530, train_perplexity=5070.4805, train_loss=8.531191

Batch 53540, train_perplexity=4967.1763, train_loss=8.510607

Batch 53550, train_perplexity=5348.135, train_loss=8.584503

Batch 53560, train_perplexity=6320.506, train_loss=8.7515545

Batch 53570, train_perplexity=5467.751, train_loss=8.606623

Batch 53580, train_perplexity=4444.288, train_loss=8.399375

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Batch 53590, train_perplexity=5085.92, train_loss=8.534231

Batch 53600, train_perplexity=5594.192, train_loss=8.629484

Batch 53610, train_perplexity=6026.6987, train_loss=8.703955

Batch 53620, train_perplexity=5619.205, train_loss=8.633945

Batch 53630, train_perplexity=5473.7617, train_loss=8.607721

Batch 53640, train_perplexity=5191.9316, train_loss=8.554861

Batch 53650, train_perplexity=5325.8315, train_loss=8.580324

Batch 53660, train_perplexity=4949.373, train_loss=8.507016

Batch 53670, train_perplexity=6793.975, train_loss=8.8237915

Batch 53680, train_perplexity=5437.0776, train_loss=8.600997

Batch 53690, train_perplexity=6268.0425, train_loss=8.743219

Batch 53700, train_perplexity=5422.3145, train_loss=8.598278

Batch 53710, train_perplexity=6086.84, train_loss=8.713884

Batch 53720, train_perplexity=5764.784, train_loss=8.659523

Batch 53730, train_perplexity=5776.418, train_loss=8.661539

Batch 53740, train_perplexity=5836.7773, train_loss=8.671934

Batch 53750, train_perplexity=6163.4785, train_loss=8.726397

Batch 53760, train_perplexity=5725.249, train_loss=8.652641

Batch 53770, train_perplexity=4951.4927, train_loss=8.507444

Batch 53780, train_perplexity=5483.2183, train_loss=8.6094475

Batch 53790, train_perplexity=5203.1733, train_loss=8.557024

Batch 53800, train_perplexity=5178.95, train_loss=8.552358

Batch 53810, train_perplexity=4676.017, train_loss=8.450202

Batch 53820, train_perplexity=5478.253, train_loss=8.6085415

Batch 53830, train_perplexity=5745.021, train_loss=8.656089

Batch 53840, train_perplexity=6057.8965, train_loss=8.709118

Batch 53850, train_perplexity=4720.5586, train_loss=8.459682

Batch 53860, train_perplexity=6008.232, train_loss=8.700886

Batch 53870, train_perplexity=6469.6025, train_loss=8.77487

Batch 53880, train_perplexity=5031.6553, train_loss=8.523504

Batch 53890, train_perplexity=5927.32, train_loss=8.687327

Batch 53900, train_perplexity=6170.3125, train_loss=8.727505

Batch 53910, train_perplexity=4672.5312, train_loss=8.449456

Batch 53920, train_perplexity=6081.444, train_loss=8.712997

Batch 53930, train_perplexity=4580.0024, train_loss=8.429455

Batch 53940, train_perplexity=4617.192, train_loss=8.437542

Batch 53950, train_perplexity=5567.851, train_loss=8.624764

Batch 53960, train_perplexity=5711.3916, train_loss=8.650218

Batch 53970, train_perplexity=5246.7437, train_loss=8.565363

Batch 53980, train_perplexity=6557.373, train_loss=8.788345

Batch 53990, train_perplexity=5941.8594, train_loss=8.689777

Batch 54000, train_perplexity=6138.553, train_loss=8.722344

Batch 54010, train_perplexity=5640.181, train_loss=8.637671

Batch 54020, train_perplexity=4512.842, train_loss=8.414682

Batch 54030, train_perplexity=4359.4253, train_loss=8.3800955

Batch 54040, train_perplexity=6206.5605, train_loss=8.733362

Batch 54050, train_perplexity=4423.159, train_loss=8.394609

Batch 54060, train_perplexity=4586.261, train_loss=8.43082

Batch 54070, train_perplexity=5225.1626, train_loss=8.561241

Batch 54080, train_perplexity=5221.7847, train_loss=8.560595

Batch 54090, train_perplexity=4724.9365, train_loss=8.460609

Batch 54100, train_perplexity=5355.883, train_loss=8.585951

Batch 54110, train_perplexity=4244.1235, train_loss=8.353291

Batch 54120, train_perplexity=5005.9575, train_loss=8.518384

Batch 54130, train_perplexity=4818.0176, train_loss=8.480118

Batch 54140, train_perplexity=5573.477, train_loss=8.625774

Batch 54150, train_perplexity=5295.4136, train_loss=8.574596

Batch 54160, train_perplexity=6153.0713, train_loss=8.724707

Batch 54170, train_perplexity=6210.504, train_loss=8.733997

Batch 54180, train_perplexity=5412.5396, train_loss=8.596474

Batch 54190, train_perplexity=5438.2026, train_loss=8.601204

Batch 54200, train_perplexity=4029.18, train_loss=8.301318

Batch 54210, train_perplexity=5419.7866, train_loss=8.597812

Batch 54220, train_perplexity=5543.764, train_loss=8.620429

Batch 54230, train_perplexity=5844.085, train_loss=8.673185

Batch 54240, train_perplexity=5706.2793, train_loss=8.6493225

Batch 54250, train_perplexity=4865.772, train_loss=8.489981

Batch 54260, train_perplexity=5544.88, train_loss=8.62063

Batch 54270, train_perplexity=4751.0806, train_loss=8.466127

Batch 54280, train_perplexity=5510.698, train_loss=8.614447

Batch 54290, train_perplexity=4635.767, train_loss=8.441557

Batch 54300, train_perplexity=5407.1377, train_loss=8.595475

Batch 54310, train_perplexity=5618.364, train_loss=8.633796

Batch 54320, train_perplexity=5115.32, train_loss=8.539995

Batch 54330, train_perplexity=5014.2476, train_loss=8.520039

Batch 54340, train_perplexity=4291.335, train_loss=8.364353

Batch 54350, train_perplexity=4996.5757, train_loss=8.516508

Batch 54360, train_perplexity=4758.1, train_loss=8.467604

Batch 54370, train_perplexity=4750.1294, train_loss=8.465927

Batch 54380, train_perplexity=4824.8135, train_loss=8.481527

Batch 54390, train_perplexity=5686.0713, train_loss=8.645775

Batch 54400, train_perplexity=5759.344, train_loss=8.658579

Batch 54410, train_perplexity=5460.4404, train_loss=8.605285

Batch 54420, train_perplexity=7351.491, train_loss=8.902658

Batch 54430, train_perplexity=4904.15, train_loss=8.497837

Batch 54440, train_perplexity=5421.844, train_loss=8.598191

Batch 54450, train_perplexity=5961.9985, train_loss=8.693161

Batch 54460, train_perplexity=4848.5312, train_loss=8.486431

Batch 54470, train_perplexity=6080.238, train_loss=8.712799

Batch 54480, train_perplexity=5941.8594, train_loss=8.689777

Batch 54490, train_perplexity=5665.2, train_loss=8.642097

Batch 54500, train_perplexity=6172.16, train_loss=8.727804

Batch 54510, train_perplexity=5222.502, train_loss=8.560732

Batch 54520, train_perplexity=4906.9946, train_loss=8.498417

Batch 54530, train_perplexity=7267.0913, train_loss=8.891111

Batch 54540, train_perplexity=4938.006, train_loss=8.504717

Batch 54550, train_perplexity=6387.555, train_loss=8.762107

Batch 54560, train_perplexity=4974.932, train_loss=8.512167

Batch 54570, train_perplexity=4985.314, train_loss=8.514252

Batch 54580, train_perplexity=5750.1567, train_loss=8.656982

Batch 54590, train_perplexity=5027.621, train_loss=8.522702

Batch 54600, train_perplexity=5779.1733, train_loss=8.662016

Batch 54610, train_perplexity=5018.5244, train_loss=8.520891
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 54620, train_perplexity=4916.372, train_loss=8.500326

Batch 54630, train_perplexity=4985.091, train_loss=8.514207

Batch 54640, train_perplexity=5545.7793, train_loss=8.620792

Batch 54650, train_perplexity=5077.5405, train_loss=8.532582

Batch 54660, train_perplexity=5541.798, train_loss=8.620074

Batch 54670, train_perplexity=6426.5874, train_loss=8.768199

Batch 54680, train_perplexity=4961.3813, train_loss=8.509439

Batch 54690, train_perplexity=6266.9966, train_loss=8.7430525

Batch 54700, train_perplexity=4968.8247, train_loss=8.510939

Batch 54710, train_perplexity=6168.3823, train_loss=8.727192

Batch 54720, train_perplexity=5118.0083, train_loss=8.540521

Batch 54730, train_perplexity=6078.6895, train_loss=8.712544

Batch 54740, train_perplexity=5605.428, train_loss=8.631491

Batch 54750, train_perplexity=5374.0464, train_loss=8.589336

Batch 54760, train_perplexity=5417.735, train_loss=8.597433

Batch 54770, train_perplexity=4535.7183, train_loss=8.419739

Batch 54780, train_perplexity=6014.9395, train_loss=8.702002

Batch 54790, train_perplexity=5277.471, train_loss=8.571202

Batch 54800, train_perplexity=5214.181, train_loss=8.559137

Batch 54810, train_perplexity=6465.0137, train_loss=8.77416

Batch 54820, train_perplexity=5684.282, train_loss=8.64546

Batch 54830, train_perplexity=4945.754, train_loss=8.506285

Batch 54840, train_perplexity=6840.173, train_loss=8.830568

Batch 54850, train_perplexity=5889.292, train_loss=8.680891

Batch 54860, train_perplexity=5346.605, train_loss=8.584217

Batch 54870, train_perplexity=4629.4624, train_loss=8.440196

Batch 54880, train_perplexity=5569.0244, train_loss=8.624975

Batch 54890, train_perplexity=4588.8076, train_loss=8.4313755

Batch 54900, train_perplexity=5136.261, train_loss=8.544081

Batch 54910, train_perplexity=4894.7725, train_loss=8.495923

Batch 54920, train_perplexity=6337.1035, train_loss=8.754177

Batch 54930, train_perplexity=4938.3027, train_loss=8.504777

Batch 54940, train_perplexity=5347.2886, train_loss=8.584345

Batch 54950, train_perplexity=5773.8735, train_loss=8.6610985

Batch 54960, train_perplexity=5665.8535, train_loss=8.642213

Batch 54970, train_perplexity=5199.205, train_loss=8.556261

Batch 54980, train_perplexity=5062.6772, train_loss=8.529651

Batch 54990, train_perplexity=4396.903, train_loss=8.388656

Batch 55000, train_perplexity=4736.784, train_loss=8.463114

Batch 55010, train_perplexity=5219.674, train_loss=8.56019

Batch 55020, train_perplexity=5411.5176, train_loss=8.596285

Batch 55030, train_perplexity=4878.894, train_loss=8.492674

Batch 55040, train_perplexity=6302.1177, train_loss=8.748641

Batch 55050, train_perplexity=5934.995, train_loss=8.6886215

Batch 55060, train_perplexity=6124.9106, train_loss=8.720119

Batch 55070, train_perplexity=5248.736, train_loss=8.5657425

Batch 55080, train_perplexity=5297.9697, train_loss=8.575079

Batch 55090, train_perplexity=6190.8833, train_loss=8.730833

Batch 55100, train_perplexity=5285.303, train_loss=8.572685

Batch 55110, train_perplexity=5700.503, train_loss=8.64831

Batch 55120, train_perplexity=6633.201, train_loss=8.799843

Batch 55130, train_perplexity=5192.58, train_loss=8.554986

Batch 55140, train_perplexity=5636.068, train_loss=8.636942

Batch 55150, train_perplexity=5791.974, train_loss=8.664228

Batch 55160, train_perplexity=5420.1636, train_loss=8.597881

Batch 55170, train_perplexity=5834.1284, train_loss=8.67148

Batch 55180, train_perplexity=4834.7393, train_loss=8.4835825

Batch 55190, train_perplexity=5441.414, train_loss=8.601794

Batch 55200, train_perplexity=6042.0757, train_loss=8.706503

Batch 55210, train_perplexity=5398.708, train_loss=8.593915

Batch 55220, train_perplexity=5190.283, train_loss=8.5545435

Batch 55230, train_perplexity=5876.859, train_loss=8.678778

Batch 55240, train_perplexity=5402.102, train_loss=8.594543

Batch 55250, train_perplexity=5849.767, train_loss=8.674157

Batch 55260, train_perplexity=5853.4, train_loss=8.674778

Batch 55270, train_perplexity=6080.2495, train_loss=8.712801

Batch 55280, train_perplexity=5630.561, train_loss=8.635964

Batch 55290, train_perplexity=4504.1265, train_loss=8.412749

Batch 55300, train_perplexity=4966.177, train_loss=8.510406

Batch 55310, train_perplexity=5956.491, train_loss=8.692237

Batch 55320, train_perplexity=5599.727, train_loss=8.630473

Batch 55330, train_perplexity=4764.457, train_loss=8.468939

Batch 55340, train_perplexity=5976.863, train_loss=8.695651

Batch 55350, train_perplexity=4826.01, train_loss=8.481775

Batch 55360, train_perplexity=6274.2744, train_loss=8.744213

Batch 55370, train_perplexity=6358.764, train_loss=8.757589

Batch 55380, train_perplexity=4991.9272, train_loss=8.515577

Batch 55390, train_perplexity=5606.487, train_loss=8.63168

Batch 55400, train_perplexity=5227.166, train_loss=8.561625

Batch 55410, train_perplexity=5196.915, train_loss=8.55582

Batch 55420, train_perplexity=5284.6226, train_loss=8.5725565

Batch 55430, train_perplexity=5483.9453, train_loss=8.60958

Batch 55440, train_perplexity=5407.4316, train_loss=8.59553

Batch 55450, train_perplexity=6469.226, train_loss=8.774812

Batch 55460, train_perplexity=4851.6396, train_loss=8.487072

Batch 55470, train_perplexity=4926.106, train_loss=8.502304

Batch 55480, train_perplexity=5292.385, train_loss=8.574024

Batch 55490, train_perplexity=5033.7334, train_loss=8.523917

Batch 55500, train_perplexity=5490.251, train_loss=8.610729

Batch 55510, train_perplexity=5609.246, train_loss=8.632172

Batch 55520, train_perplexity=6257.7393, train_loss=8.741574

Batch 55530, train_perplexity=5627.077, train_loss=8.635345

Batch 55540, train_perplexity=5789.478, train_loss=8.663797

Batch 55550, train_perplexity=5804.061, train_loss=8.666313

Batch 55560, train_perplexity=5333.598, train_loss=8.581781

Batch 55570, train_perplexity=4888.1436, train_loss=8.494568

Batch 55580, train_perplexity=5714.9443, train_loss=8.65084

Batch 55590, train_perplexity=5864.128, train_loss=8.676609

Batch 55600, train_perplexity=4998.897, train_loss=8.516973

Batch 55610, train_perplexity=4970.768, train_loss=8.51133

Batch 55620, train_perplexity=6719.1406, train_loss=8.812716

Batch 55630, train_perplexity=5198.496, train_loss=8.556125

Batch 55640, train_perplexity=5260.6626, train_loss=8.568012

Batch 55650, train_perplexity=5200.7974, train_loss=8.556567

Batch 55660, train_perplexity=5603.0767, train_loss=8.631071

Batch 55670, train_perplexity=4719.204, train_loss=8.459395

Batch 55680, train_perplexity=6108.4956, train_loss=8.717436

Batch 55690, train_perplexity=5803.9614, train_loss=8.666296

Batch 55700, train_perplexity=4824.2334, train_loss=8.481407

Batch 55710, train_perplexity=5196.841, train_loss=8.555806

Batch 55720, train_perplexity=5696.759, train_loss=8.647653

Batch 55730, train_perplexity=4848.0737, train_loss=8.486337

Batch 55740, train_perplexity=4597.621, train_loss=8.433294

Batch 55750, train_perplexity=5343.8525, train_loss=8.583702

Batch 55760, train_perplexity=5406.4727, train_loss=8.595352

Batch 55770, train_perplexity=5958.6333, train_loss=8.692596

Batch 55780, train_perplexity=4847.274, train_loss=8.486172

Batch 55790, train_perplexity=4695.375, train_loss=8.454333

Batch 55800, train_perplexity=5748.7095, train_loss=8.656731

Batch 55810, train_perplexity=4691.9824, train_loss=8.45361

Batch 55820, train_perplexity=5204.856, train_loss=8.557347

Batch 55830, train_perplexity=5907.4272, train_loss=8.683966

Batch 55840, train_perplexity=6756.3706, train_loss=8.818241

Batch 55850, train_perplexity=5655.3594, train_loss=8.640359

Batch 55860, train_perplexity=4890.2793, train_loss=8.495005

Batch 55870, train_perplexity=5327.8335, train_loss=8.5807

Batch 55880, train_perplexity=5195.032, train_loss=8.555458

Batch 55890, train_perplexity=6426.011, train_loss=8.768109

Batch 55900, train_perplexity=6655.9375, train_loss=8.803265

Batch 55910, train_perplexity=5030.384, train_loss=8.523252

Batch 55920, train_perplexity=6273.4126, train_loss=8.744076

Batch 55930, train_perplexity=6039.9673, train_loss=8.706154

Batch 55940, train_perplexity=4966.101, train_loss=8.51039

Batch 55950, train_perplexity=4953.24, train_loss=8.507797
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55960, train_perplexity=6317.354, train_loss=8.751056

Batch 55970, train_perplexity=5554.661, train_loss=8.622393

Batch 55980, train_perplexity=5509.269, train_loss=8.614187

Batch 55990, train_perplexity=5724.703, train_loss=8.652546

Batch 56000, train_perplexity=5700.2964, train_loss=8.648273

Batch 56010, train_perplexity=5303.3433, train_loss=8.576093

Batch 56020, train_perplexity=5137.736, train_loss=8.544368

Batch 56030, train_perplexity=6367.9033, train_loss=8.759026

Batch 56040, train_perplexity=5906.0527, train_loss=8.683733

Batch 56050, train_perplexity=6118.209, train_loss=8.719025

Batch 56060, train_perplexity=4734.2373, train_loss=8.462576

Batch 56070, train_perplexity=5757.6143, train_loss=8.658278

Batch 56080, train_perplexity=5232.922, train_loss=8.562725

Batch 56090, train_perplexity=5764.9053, train_loss=8.659544

Batch 56100, train_perplexity=5627.3135, train_loss=8.635387

Batch 56110, train_perplexity=5471.319, train_loss=8.607275

Batch 56120, train_perplexity=5508.46, train_loss=8.61404

Batch 56130, train_perplexity=5847.0454, train_loss=8.673692

Batch 56140, train_perplexity=5626.1543, train_loss=8.635181

Batch 56150, train_perplexity=6519.337, train_loss=8.782528

Batch 56160, train_perplexity=6528.0283, train_loss=8.78386

Batch 56170, train_perplexity=5161.0674, train_loss=8.548899

Batch 56180, train_perplexity=4319.432, train_loss=8.370879

Batch 56190, train_perplexity=5653.596, train_loss=8.640047

Batch 56200, train_perplexity=5479.3027, train_loss=8.608733

Batch 56210, train_perplexity=5103.343, train_loss=8.537651

Batch 56220, train_perplexity=5326.6343, train_loss=8.580475

Batch 56230, train_perplexity=5579.4863, train_loss=8.626852

Batch 56240, train_perplexity=4779.648, train_loss=8.472122

Batch 56250, train_perplexity=6960.531, train_loss=8.848011

Batch 56260, train_perplexity=5020.152, train_loss=8.521215

Batch 56270, train_perplexity=5344.6475, train_loss=8.583851

Batch 56280, train_perplexity=4885.529, train_loss=8.494033

Batch 56290, train_perplexity=5500.093, train_loss=8.61252

Batch 56300, train_perplexity=5741.795, train_loss=8.655527

Batch 56310, train_perplexity=5261.8267, train_loss=8.5682335

Batch 56320, train_perplexity=5477.725, train_loss=8.608445

Batch 56330, train_perplexity=6844.9497, train_loss=8.831266

Batch 56340, train_perplexity=4985.5664, train_loss=8.514302

Batch 56350, train_perplexity=5346.0034, train_loss=8.584105

Batch 56360, train_perplexity=4974.9365, train_loss=8.512168

Batch 56370, train_perplexity=6550.2227, train_loss=8.787254

Batch 56380, train_perplexity=5584.043, train_loss=8.627668

Batch 56390, train_perplexity=4452.0723, train_loss=8.401125

Batch 56400, train_perplexity=6476.2876, train_loss=8.775903

Batch 56410, train_perplexity=4994.6416, train_loss=8.516121

Batch 56420, train_perplexity=5497.497, train_loss=8.612048

Batch 56430, train_perplexity=4737.5522, train_loss=8.463276

Batch 56440, train_perplexity=5788.5835, train_loss=8.663643

Batch 56450, train_perplexity=5406.9834, train_loss=8.595447

Batch 56460, train_perplexity=5777.983, train_loss=8.66181

Batch 56470, train_perplexity=6400.391, train_loss=8.764114

Batch 56480, train_perplexity=6061.7046, train_loss=8.709746

Batch 56490, train_perplexity=4857.6816, train_loss=8.488317

Batch 56500, train_perplexity=5327.249, train_loss=8.58059

Batch 56510, train_perplexity=4440.2505, train_loss=8.398466

Batch 56520, train_perplexity=4197.7593, train_loss=8.342306

Batch 56530, train_perplexity=5429.7456, train_loss=8.5996475

Batch 56540, train_perplexity=4488.27, train_loss=8.409223

Batch 56550, train_perplexity=5309.75, train_loss=8.5773

Batch 56560, train_perplexity=6079.5884, train_loss=8.712692

Batch 56570, train_perplexity=6482.028, train_loss=8.776789

Batch 56580, train_perplexity=4991.2275, train_loss=8.515437

Batch 56590, train_perplexity=4839.32, train_loss=8.4845295

Batch 56600, train_perplexity=5732.012, train_loss=8.653822

Batch 56610, train_perplexity=5084.9497, train_loss=8.53404

Batch 56620, train_perplexity=6717.244, train_loss=8.812433

Batch 56630, train_perplexity=5938.313, train_loss=8.68918

Batch 56640, train_perplexity=5065.551, train_loss=8.530218

Batch 56650, train_perplexity=4302.6, train_loss=8.366975

Batch 56660, train_perplexity=5382.1963, train_loss=8.590852

Batch 56670, train_perplexity=4887.1133, train_loss=8.494357

Batch 56680, train_perplexity=5633.1606, train_loss=8.636426

Batch 56690, train_perplexity=5235.108, train_loss=8.563143

Batch 56700, train_perplexity=6613.065, train_loss=8.7968025

Batch 56710, train_perplexity=6274.7173, train_loss=8.744284

Batch 56720, train_perplexity=5747.536, train_loss=8.656527

Batch 56730, train_perplexity=6740.9883, train_loss=8.815962

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Batch 56740, train_perplexity=4219.368, train_loss=8.347441

Batch 56750, train_perplexity=4919.0923, train_loss=8.500879

Batch 56760, train_perplexity=5203.65, train_loss=8.557116

Batch 56770, train_perplexity=5931.951, train_loss=8.688108

Batch 56780, train_perplexity=6542.8555, train_loss=8.786129

Batch 56790, train_perplexity=4781.0156, train_loss=8.472408

Batch 56800, train_perplexity=4865.846, train_loss=8.489996

Batch 56810, train_perplexity=6610.013, train_loss=8.796341

Batch 56820, train_perplexity=6025.1245, train_loss=8.703693

Batch 56830, train_perplexity=5213.644, train_loss=8.559034

Batch 56840, train_perplexity=5733.554, train_loss=8.654091

Batch 56850, train_perplexity=5773.6973, train_loss=8.661068

Batch 56860, train_perplexity=4452.7603, train_loss=8.401279

Batch 56870, train_perplexity=7038.0405, train_loss=8.859085

Batch 56880, train_perplexity=5053.3916, train_loss=8.527815

Batch 56890, train_perplexity=4404.9653, train_loss=8.390488

Batch 56900, train_perplexity=4978.0356, train_loss=8.512791

Batch 56910, train_perplexity=4405.7886, train_loss=8.390675

Batch 56920, train_perplexity=5084.509, train_loss=8.533954

Batch 56930, train_perplexity=5765.609, train_loss=8.659666

Batch 56940, train_perplexity=5209.4297, train_loss=8.558226

Batch 56950, train_perplexity=6848.463, train_loss=8.8317795

Batch 56960, train_perplexity=4483.8765, train_loss=8.408243

Batch 56970, train_perplexity=4908.389, train_loss=8.498701

Batch 56980, train_perplexity=4628.1865, train_loss=8.43992

Batch 56990, train_perplexity=5826.494, train_loss=8.670171

Batch 57000, train_perplexity=5096.383, train_loss=8.536286

Batch 57010, train_perplexity=5163.017, train_loss=8.549276

Batch 57020, train_perplexity=6480.718, train_loss=8.776587

Batch 57030, train_perplexity=4919.1577, train_loss=8.500893

Batch 57040, train_perplexity=5196.9053, train_loss=8.555819

Batch 57050, train_perplexity=5285.953, train_loss=8.572808

Batch 57060, train_perplexity=4800.4194, train_loss=8.476459

Batch 57070, train_perplexity=5170.8813, train_loss=8.550798

Batch 57080, train_perplexity=5781.588, train_loss=8.662434

Batch 57090, train_perplexity=5148.3105, train_loss=8.546424

Batch 57100, train_perplexity=5731.936, train_loss=8.653809

Batch 57110, train_perplexity=5634.407, train_loss=8.636647

Batch 57120, train_perplexity=5554.1577, train_loss=8.622302

Batch 57130, train_perplexity=5861.584, train_loss=8.676175

Batch 57140, train_perplexity=5514.589, train_loss=8.615152

Batch 57150, train_perplexity=6687.1772, train_loss=8.807947

Batch 57160, train_perplexity=6713.7856, train_loss=8.811918

Batch 57170, train_perplexity=5663.4985, train_loss=8.641797

Batch 57180, train_perplexity=4522.303, train_loss=8.416777

Batch 57190, train_perplexity=4837.922, train_loss=8.484241

Batch 57200, train_perplexity=5136.2856, train_loss=8.5440855

Batch 57210, train_perplexity=5262.559, train_loss=8.568373

Batch 57220, train_perplexity=5863.0317, train_loss=8.676422
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 57230, train_perplexity=5839.4946, train_loss=8.6723995

Batch 57240, train_perplexity=5119.8687, train_loss=8.540884

Batch 57250, train_perplexity=5886.4956, train_loss=8.680416

Batch 57260, train_perplexity=5256.219, train_loss=8.567167

Batch 57270, train_perplexity=5479.9404, train_loss=8.60885

Batch 57280, train_perplexity=5872.6455, train_loss=8.678061

Batch 57290, train_perplexity=6611.892, train_loss=8.796625

Batch 57300, train_perplexity=5235.7573, train_loss=8.563267

Batch 57310, train_perplexity=5235.1333, train_loss=8.563148

Batch 57320, train_perplexity=5718.4717, train_loss=8.651457

Batch 57330, train_perplexity=7135.952, train_loss=8.872901

Batch 57340, train_perplexity=5172.2227, train_loss=8.551058

Batch 57350, train_perplexity=5439.0947, train_loss=8.601368

Batch 57360, train_perplexity=4989.9756, train_loss=8.515186

Batch 57370, train_perplexity=5944.954, train_loss=8.690298

Batch 57380, train_perplexity=5414.925, train_loss=8.596914

Batch 57390, train_perplexity=6157.022, train_loss=8.725348

Batch 57400, train_perplexity=5683.3716, train_loss=8.6453

Batch 57410, train_perplexity=5982.183, train_loss=8.696541

Batch 57420, train_perplexity=5232.6323, train_loss=8.56267

Batch 57430, train_perplexity=5284.3755, train_loss=8.57251

Batch 57440, train_perplexity=4785.8374, train_loss=8.473416

Batch 57450, train_perplexity=5778.38, train_loss=8.661879

Batch 57460, train_perplexity=5391.4946, train_loss=8.592578

Batch 57470, train_perplexity=4920.434, train_loss=8.501152

Batch 57480, train_perplexity=5542.998, train_loss=8.620291

Batch 57490, train_perplexity=4808.1396, train_loss=8.4780655

Batch 57500, train_perplexity=6036.5005, train_loss=8.70558

Batch 57510, train_perplexity=6173.367, train_loss=8.728

Batch 57520, train_perplexity=4889.9155, train_loss=8.49493

Batch 57530, train_perplexity=5258.4956, train_loss=8.5676

Batch 57540, train_perplexity=5531.3486, train_loss=8.618187

Batch 57550, train_perplexity=5458.816, train_loss=8.604987

Batch 57560, train_perplexity=5890.37, train_loss=8.681074

Batch 57570, train_perplexity=5389.8545, train_loss=8.592274

Batch 57580, train_perplexity=5148.8657, train_loss=8.546532

Batch 57590, train_perplexity=5248.916, train_loss=8.565777

Batch 57600, train_perplexity=4858.4785, train_loss=8.488481

Batch 57610, train_perplexity=4528.3447, train_loss=8.418112

Batch 57620, train_perplexity=5899.8267, train_loss=8.682678

Batch 57630, train_perplexity=5167.066, train_loss=8.55006

Batch 57640, train_perplexity=5096.524, train_loss=8.536314

Batch 57650, train_perplexity=5823.589, train_loss=8.669672

Batch 57660, train_perplexity=5700.753, train_loss=8.648354

Batch 57670, train_perplexity=5791.7974, train_loss=8.664198

Batch 57680, train_perplexity=5789.4062, train_loss=8.663785

Batch 57690, train_perplexity=6387.257, train_loss=8.76206

Batch 57700, train_perplexity=5673.4233, train_loss=8.643548

Batch 57710, train_perplexity=4979.052, train_loss=8.512995

Batch 57720, train_perplexity=5755.819, train_loss=8.657967

Batch 57730, train_perplexity=5065.2026, train_loss=8.530149

Batch 57740, train_perplexity=5639.3315, train_loss=8.637521

Batch 57750, train_perplexity=4556.7114, train_loss=8.424356

Batch 57760, train_perplexity=5337.272, train_loss=8.58247

Batch 57770, train_perplexity=4613.257, train_loss=8.436689

Batch 57780, train_perplexity=5947.5513, train_loss=8.690735

Batch 57790, train_perplexity=6091.4976, train_loss=8.714649

Batch 57800, train_perplexity=5293.7676, train_loss=8.5742855

Batch 57810, train_perplexity=5583.7183, train_loss=8.62761

Batch 57820, train_perplexity=5889.8086, train_loss=8.680979

Batch 57830, train_perplexity=4402.928, train_loss=8.390025

Batch 57840, train_perplexity=5182.6606, train_loss=8.553074

Batch 57850, train_perplexity=6294.2075, train_loss=8.747385

Batch 57860, train_perplexity=4549.2256, train_loss=8.422712

Batch 57870, train_perplexity=5091.2046, train_loss=8.53527

Batch 57880, train_perplexity=4589.587, train_loss=8.431545

Batch 57890, train_perplexity=5113.7344, train_loss=8.539685

Batch 57900, train_perplexity=4998.3774, train_loss=8.516869

Batch 57910, train_perplexity=5045.032, train_loss=8.526159

Batch 57920, train_perplexity=5317.8535, train_loss=8.578825

Batch 57930, train_perplexity=5763.0034, train_loss=8.659214

Batch 57940, train_perplexity=5412.1577, train_loss=8.596403

Batch 57950, train_perplexity=4622.126, train_loss=8.43861

Batch 57960, train_perplexity=5264.386, train_loss=8.56872

Batch 57970, train_perplexity=4956.1606, train_loss=8.508387

Batch 57980, train_perplexity=5478.033, train_loss=8.608501

Batch 57990, train_perplexity=6222.2896, train_loss=8.735893

Batch 58000, train_perplexity=5312.359, train_loss=8.577791

Batch 58010, train_perplexity=4866.5845, train_loss=8.490148

Batch 58020, train_perplexity=5983.3354, train_loss=8.696733

Batch 58030, train_perplexity=5449.0425, train_loss=8.603195

Batch 58040, train_perplexity=4830.32, train_loss=8.482668

Batch 58050, train_perplexity=5004.96, train_loss=8.518185

Batch 58060, train_perplexity=6682.326, train_loss=8.807221

Batch 58070, train_perplexity=5677.0986, train_loss=8.644196

Batch 58080, train_perplexity=4565.5723, train_loss=8.426299

Batch 58090, train_perplexity=5148.738, train_loss=8.546507

Batch 58100, train_perplexity=5046.615, train_loss=8.526473

Batch 58110, train_perplexity=6267.128, train_loss=8.743073

Batch 58120, train_perplexity=5948.1865, train_loss=8.690842

Batch 58130, train_perplexity=6879.451, train_loss=8.836294

Batch 58140, train_perplexity=4797.353, train_loss=8.47582

Batch 58150, train_perplexity=7253.1606, train_loss=8.889193

Batch 58160, train_perplexity=6100.9624, train_loss=8.716202

Batch 58170, train_perplexity=5325.1514, train_loss=8.580196

Batch 58180, train_perplexity=4988.986, train_loss=8.514988

Batch 58190, train_perplexity=5352.4517, train_loss=8.58531

Batch 58200, train_perplexity=5569.1733, train_loss=8.625002

Batch 58210, train_perplexity=5192.575, train_loss=8.554985

Batch 58220, train_perplexity=6691.988, train_loss=8.808666

Batch 58230, train_perplexity=4468.0566, train_loss=8.404709

Batch 58240, train_perplexity=5731.706, train_loss=8.653769

Batch 58250, train_perplexity=6371.208, train_loss=8.759544

Batch 58260, train_perplexity=4690.4565, train_loss=8.453285

Batch 58270, train_perplexity=5346.0034, train_loss=8.584105

Batch 58280, train_perplexity=4657.2188, train_loss=8.446174

Batch 58290, train_perplexity=4960.8325, train_loss=8.509329

Batch 58300, train_perplexity=5383.0327, train_loss=8.591007

Batch 58310, train_perplexity=5635.7725, train_loss=8.636889

Batch 58320, train_perplexity=4866.2827, train_loss=8.490086

Batch 58330, train_perplexity=6261.6494, train_loss=8.742199

Batch 58340, train_perplexity=5095.28, train_loss=8.53607

Batch 58350, train_perplexity=4895.365, train_loss=8.496044

Batch 58360, train_perplexity=5074.399, train_loss=8.531963

Batch 58370, train_perplexity=5141.564, train_loss=8.545113

Batch 58380, train_perplexity=5022.6226, train_loss=8.521708

Batch 58390, train_perplexity=5470.2964, train_loss=8.607088

Batch 58400, train_perplexity=5129.1294, train_loss=8.542691

Batch 58410, train_perplexity=5602.879, train_loss=8.631036

Batch 58420, train_perplexity=4789.166, train_loss=8.474112

Batch 58430, train_perplexity=4663.801, train_loss=8.447586

Batch 58440, train_perplexity=6176.323, train_loss=8.728478

Batch 58450, train_perplexity=5482.089, train_loss=8.6092415

Batch 58460, train_perplexity=6411.595, train_loss=8.765863

Batch 58470, train_perplexity=5465.6294, train_loss=8.606235

Batch 58480, train_perplexity=5730.668, train_loss=8.653587

Batch 58490, train_perplexity=5784.29, train_loss=8.662901

Batch 58500, train_perplexity=5012.5835, train_loss=8.519707

Batch 58510, train_perplexity=4393.4785, train_loss=8.3878765

Batch 58520, train_perplexity=4443.792, train_loss=8.399263

Batch 58530, train_perplexity=5911.902, train_loss=8.684723

Batch 58540, train_perplexity=5564.3105, train_loss=8.624128

Batch 58550, train_perplexity=5238.1743, train_loss=8.563728

Batch 58560, train_perplexity=5591.365, train_loss=8.628979
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 58570, train_perplexity=4974.002, train_loss=8.51198

Batch 58580, train_perplexity=5736.464, train_loss=8.654598

Batch 58590, train_perplexity=5314.8975, train_loss=8.578269

Batch 58600, train_perplexity=4926.153, train_loss=8.502314

Batch 58610, train_perplexity=5287.0977, train_loss=8.573025

Batch 58620, train_perplexity=5077.9907, train_loss=8.532671

Batch 58630, train_perplexity=5126.963, train_loss=8.542269

Batch 58640, train_perplexity=7214.363, train_loss=8.883829

Batch 58650, train_perplexity=4968.6636, train_loss=8.510906

Batch 58660, train_perplexity=5767.677, train_loss=8.660025

Batch 58670, train_perplexity=4180.847, train_loss=8.338269

Batch 58680, train_perplexity=4543.3813, train_loss=8.421427

Batch 58690, train_perplexity=5476.4976, train_loss=8.608221

Batch 58700, train_perplexity=5343.399, train_loss=8.583617

Batch 58710, train_perplexity=6698.807, train_loss=8.809685

Batch 58720, train_perplexity=4627.4673, train_loss=8.439765

Batch 58730, train_perplexity=5274.3564, train_loss=8.570612

Batch 58740, train_perplexity=5045.0415, train_loss=8.526161

Batch 58750, train_perplexity=5860.192, train_loss=8.675938

Batch 58760, train_perplexity=5438.172, train_loss=8.601198

Batch 58770, train_perplexity=5473.636, train_loss=8.607698

Batch 58780, train_perplexity=5335.1753, train_loss=8.582077

Batch 58790, train_perplexity=5404.8643, train_loss=8.595055

Batch 58800, train_perplexity=4765.066, train_loss=8.469067

Batch 58810, train_perplexity=5039.0264, train_loss=8.524968

Batch 58820, train_perplexity=5553.04, train_loss=8.622101

Batch 58830, train_perplexity=4976.2417, train_loss=8.51243

Batch 58840, train_perplexity=3946.7747, train_loss=8.280654

Batch 58850, train_perplexity=5440.9004, train_loss=8.6017

Batch 58860, train_perplexity=4446.989, train_loss=8.399982

Batch 58870, train_perplexity=6113.5547, train_loss=8.718264

Batch 58880, train_perplexity=6151.411, train_loss=8.724437

Batch 58890, train_perplexity=5311.047, train_loss=8.577544

Batch 58900, train_perplexity=5178.4565, train_loss=8.552262

Batch 58910, train_perplexity=5642.995, train_loss=8.63817

Batch 58920, train_perplexity=6255.7407, train_loss=8.741255

Batch 58930, train_perplexity=5846.1475, train_loss=8.673538

Batch 58940, train_perplexity=4780.961, train_loss=8.472397

Batch 58950, train_perplexity=5474.9204, train_loss=8.607933

Batch 58960, train_perplexity=5769.6685, train_loss=8.66037

Batch 58970, train_perplexity=5679.6436, train_loss=8.644644

Batch 58980, train_perplexity=5628.1187, train_loss=8.63553

Batch 58990, train_perplexity=4686.307, train_loss=8.4524

Batch 59000, train_perplexity=5808.4746, train_loss=8.667073

Batch 59010, train_perplexity=5007.648, train_loss=8.518722

Batch 59020, train_perplexity=4963.937, train_loss=8.509954

Batch 59030, train_perplexity=6168.7, train_loss=8.727243

Batch 59040, train_perplexity=5502.8105, train_loss=8.613014

Batch 59050, train_perplexity=5778.843, train_loss=8.661959

Batch 59060, train_perplexity=5639.2183, train_loss=8.637501

Batch 59070, train_perplexity=5509.238, train_loss=8.6141815

Batch 59080, train_perplexity=4546.58, train_loss=8.422131

Batch 59090, train_perplexity=5303.2676, train_loss=8.576078

Batch 59100, train_perplexity=6272.826, train_loss=8.743982

Batch 59110, train_perplexity=5534.293, train_loss=8.618719

Batch 59120, train_perplexity=5990.627, train_loss=8.697951

Batch 59130, train_perplexity=5252.0205, train_loss=8.566368

Batch 59140, train_perplexity=5789.3896, train_loss=8.663782

Batch 59150, train_perplexity=4701.33, train_loss=8.455601

Batch 59160, train_perplexity=4849.8354, train_loss=8.4867

Batch 59170, train_perplexity=4846.035, train_loss=8.485916

Batch 59180, train_perplexity=4986.85, train_loss=8.51456

Batch 59190, train_perplexity=5242.9023, train_loss=8.5646305

Batch 59200, train_perplexity=5076.8145, train_loss=8.532439

Batch 59210, train_perplexity=6426.1157, train_loss=8.768126

Batch 59220, train_perplexity=5373.954, train_loss=8.589319

Batch 59230, train_perplexity=5946.6606, train_loss=8.690585

Batch 59240, train_perplexity=5021.0806, train_loss=8.5214

Batch 59250, train_perplexity=5051.9844, train_loss=8.527536

Batch 59260, train_perplexity=5706.59, train_loss=8.649377

Batch 59270, train_perplexity=5413.9126, train_loss=8.596727

Batch 59280, train_perplexity=5428.078, train_loss=8.59934

Batch 59290, train_perplexity=5756.8125, train_loss=8.658139

Batch 59300, train_perplexity=5797.2236, train_loss=8.665134

Batch 59310, train_perplexity=5131.4634, train_loss=8.543146

Batch 59320, train_perplexity=4183.811, train_loss=8.338978

Batch 59330, train_perplexity=5822.3896, train_loss=8.669466

Batch 59340, train_perplexity=5250.488, train_loss=8.566076

Batch 59350, train_perplexity=5676.871, train_loss=8.6441555

Batch 59360, train_perplexity=6235.602, train_loss=8.73803

Batch 59370, train_perplexity=5269.937, train_loss=8.569774

Batch 59380, train_perplexity=7260.1504, train_loss=8.890156

Batch 59390, train_perplexity=4305.0713, train_loss=8.367549

Batch 59400, train_perplexity=5416.0713, train_loss=8.597126

Batch 59410, train_perplexity=4619.4556, train_loss=8.438032

Batch 59420, train_perplexity=5508.8276, train_loss=8.614107

Batch 59430, train_perplexity=4779.9307, train_loss=8.472181

Batch 59440, train_perplexity=6593.581, train_loss=8.793852

Batch 59450, train_perplexity=5092.21, train_loss=8.535467

Batch 59460, train_perplexity=6282.513, train_loss=8.745525

Batch 59470, train_perplexity=4750.0386, train_loss=8.465908

Batch 59480, train_perplexity=6888.7344, train_loss=8.837643

Batch 59490, train_perplexity=5605.653, train_loss=8.631531

Batch 59500, train_perplexity=6693.992, train_loss=8.808966

Batch 59510, train_perplexity=4641.589, train_loss=8.442812

Batch 59520, train_perplexity=5566.9004, train_loss=8.624594

Batch 59530, train_perplexity=5678.918, train_loss=8.644516

Batch 59540, train_perplexity=6006.1924, train_loss=8.700546

Batch 59550, train_perplexity=5065.5312, train_loss=8.530214

Batch 59560, train_perplexity=4914.347, train_loss=8.499914

Batch 59570, train_perplexity=5687.612, train_loss=8.646046

Batch 59580, train_perplexity=6897.583, train_loss=8.838926

Batch 59590, train_perplexity=5500.9634, train_loss=8.612679

Batch 59600, train_perplexity=5500.344, train_loss=8.612566

Batch 59610, train_perplexity=4226.134, train_loss=8.349043

Batch 59620, train_perplexity=5003.2705, train_loss=8.517847

Batch 59630, train_perplexity=6381.2534, train_loss=8.76112

Batch 59640, train_perplexity=5665.7944, train_loss=8.642202

Batch 59650, train_perplexity=5171.1772, train_loss=8.550856

Batch 59660, train_perplexity=5229.1006, train_loss=8.561995

Batch 59670, train_perplexity=4659.3555, train_loss=8.446632

Batch 59680, train_perplexity=4983.6313, train_loss=8.513914

Batch 59690, train_perplexity=5608.1123, train_loss=8.631969

Batch 59700, train_perplexity=5629.181, train_loss=8.635719

Batch 59710, train_perplexity=4944.8296, train_loss=8.506098

Batch 59720, train_perplexity=4799.6914, train_loss=8.476307

Batch 59730, train_perplexity=5255.0063, train_loss=8.5669365

Batch 59740, train_perplexity=5426.65, train_loss=8.599077

Batch 59750, train_perplexity=4902.808, train_loss=8.497563

Batch 59760, train_perplexity=5070.6113, train_loss=8.531217

Batch 59770, train_perplexity=5530.6157, train_loss=8.618054

Batch 59780, train_perplexity=5582.3125, train_loss=8.627358

Batch 59790, train_perplexity=5792.416, train_loss=8.664305

Batch 59800, train_perplexity=5402.7515, train_loss=8.594664

Batch 59810, train_perplexity=4829.3525, train_loss=8.482468

Batch 59820, train_perplexity=4667.441, train_loss=8.448366

Batch 59830, train_perplexity=5530.7686, train_loss=8.618082

Batch 59840, train_perplexity=5380.138, train_loss=8.590469

Batch 59850, train_perplexity=6080.76, train_loss=8.712885

Batch 59860, train_perplexity=4930.4487, train_loss=8.503185

Batch 59870, train_perplexity=4711.0693, train_loss=8.45767

Batch 59880, train_perplexity=4922.0767, train_loss=8.501486

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306116 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100
Loaded 306116 sentences.
Finished loading
Batch 59890, train_perplexity=5921.9463, train_loss=8.68642

Batch 59900, train_perplexity=5042.3096, train_loss=8.5256195

Batch 59910, train_perplexity=6141.2646, train_loss=8.722786

Batch 59920, train_perplexity=5282.9043, train_loss=8.572231

Batch 59930, train_perplexity=6580.9536, train_loss=8.791935

Batch 59940, train_perplexity=6529.778, train_loss=8.784128

Batch 59950, train_perplexity=5304.6436, train_loss=8.576338

Batch 59960, train_perplexity=6246.691, train_loss=8.739807

Batch 59970, train_perplexity=6341.408, train_loss=8.754856

Batch 59980, train_perplexity=5631.1465, train_loss=8.636068

Batch 59990, train_perplexity=4833.5176, train_loss=8.48333

Batch 60000, train_perplexity=5324.6533, train_loss=8.580103

Batch 60010, train_perplexity=5169.2837, train_loss=8.550489

Batch 60020, train_perplexity=6100.334, train_loss=8.716099

Batch 60030, train_perplexity=4796.6025, train_loss=8.475663

Batch 60040, train_perplexity=6239.2783, train_loss=8.73862

Batch 60050, train_perplexity=5503.01, train_loss=8.61305

Batch 60060, train_perplexity=6007.258, train_loss=8.700724

Batch 60070, train_perplexity=5071.9023, train_loss=8.531471

Batch 60080, train_perplexity=5052.524, train_loss=8.527643

Batch 60090, train_perplexity=5137.7114, train_loss=8.544363

Batch 60100, train_perplexity=5503.776, train_loss=8.61319

Batch 60110, train_perplexity=4991.5986, train_loss=8.5155115

Batch 60120, train_perplexity=6098.868, train_loss=8.715858

Batch 60130, train_perplexity=4668.1616, train_loss=8.448521

Batch 60140, train_perplexity=4434.779, train_loss=8.397233

Batch 60150, train_perplexity=5061.832, train_loss=8.529484

Batch 60160, train_perplexity=5286.4272, train_loss=8.572898

Batch 60170, train_perplexity=5359.337, train_loss=8.586596

Batch 60180, train_perplexity=5338.58, train_loss=8.582715

Batch 60190, train_perplexity=4811.13, train_loss=8.478687

Batch 60200, train_perplexity=5372.002, train_loss=8.588956

Batch 60210, train_perplexity=5506.8945, train_loss=8.613756

Batch 60220, train_perplexity=5953.777, train_loss=8.691781

Batch 60230, train_perplexity=6344.481, train_loss=8.755341

Batch 60240, train_perplexity=5501.6245, train_loss=8.612799

Batch 60250, train_perplexity=5339.848, train_loss=8.5829525

Batch 60260, train_perplexity=5216.27, train_loss=8.559538

Batch 60270, train_perplexity=5765.8013, train_loss=8.659699

Batch 60280, train_perplexity=4951.6016, train_loss=8.507466

Batch 60290, train_perplexity=5385.205, train_loss=8.591411

Batch 60300, train_perplexity=5329.9473, train_loss=8.581097

Batch 60310, train_perplexity=6561.089, train_loss=8.788912

Batch 60320, train_perplexity=5636.439, train_loss=8.637008

Batch 60330, train_perplexity=5746.4565, train_loss=8.656339

Batch 60340, train_perplexity=5338.565, train_loss=8.582712

Batch 60350, train_perplexity=5738.5757, train_loss=8.654966

Batch 60360, train_perplexity=5082.71, train_loss=8.5336

Batch 60370, train_perplexity=5014.3237, train_loss=8.520054

Batch 60380, train_perplexity=6969.458, train_loss=8.849293

Batch 60390, train_perplexity=6343.041, train_loss=8.755114

Batch 60400, train_perplexity=5597.362, train_loss=8.630051

Batch 60410, train_perplexity=5521.093, train_loss=8.616331

Batch 60420, train_perplexity=4976.161, train_loss=8.512414

Batch 60430, train_perplexity=4533.059, train_loss=8.419152

Batch 60440, train_perplexity=5643.7915, train_loss=8.638311

Batch 60450, train_perplexity=5565.0005, train_loss=8.624252

Batch 60460, train_perplexity=4603.8164, train_loss=8.434641

Batch 60470, train_perplexity=5786.37, train_loss=8.66326

Batch 60480, train_perplexity=6088.1753, train_loss=8.714104

Batch 60490, train_perplexity=5465.702, train_loss=8.606248

Batch 60500, train_perplexity=5549.6836, train_loss=8.621496

Batch 60510, train_perplexity=6062.9595, train_loss=8.709953

Batch 60520, train_perplexity=5754.1064, train_loss=8.657669

Batch 60530, train_perplexity=5800.636, train_loss=8.665723

Batch 60540, train_perplexity=4360.003, train_loss=8.380228

Batch 60550, train_perplexity=6070.863, train_loss=8.711256

Batch 60560, train_perplexity=5013.2383, train_loss=8.519837

Batch 60570, train_perplexity=4781.745, train_loss=8.472561

Batch 60580, train_perplexity=4677.7925, train_loss=8.450582

Batch 60590, train_perplexity=5476.8213, train_loss=8.60828

Batch 60600, train_perplexity=6635.9097, train_loss=8.800251

Batch 60610, train_perplexity=5315.3184, train_loss=8.578348

Batch 60620, train_perplexity=5701.5635, train_loss=8.648496

Batch 60630, train_perplexity=6032.064, train_loss=8.704844

Batch 60640, train_perplexity=5642.769, train_loss=8.63813

Batch 60650, train_perplexity=5065.0, train_loss=8.530109

Batch 60660, train_perplexity=5459.1387, train_loss=8.605046

Batch 60670, train_perplexity=5517.745, train_loss=8.615725

Batch 60680, train_perplexity=4769.567, train_loss=8.470011

Batch 60690, train_perplexity=5616.28, train_loss=8.633425

Batch 60700, train_perplexity=5985.607, train_loss=8.697113

Batch 60710, train_perplexity=5322.425, train_loss=8.579684

Batch 60720, train_perplexity=6218.469, train_loss=8.735279

Batch 60730, train_perplexity=5866.231, train_loss=8.676968

Batch 60740, train_perplexity=4282.5493, train_loss=8.362304

Batch 60750, train_perplexity=4281.957, train_loss=8.362165

Batch 60760, train_perplexity=4654.8696, train_loss=8.445669

Batch 60770, train_perplexity=6480.718, train_loss=8.776587

Batch 60780, train_perplexity=4793.5024, train_loss=8.475017

Batch 60790, train_perplexity=5542.4165, train_loss=8.620186

Batch 60800, train_perplexity=4514.0903, train_loss=8.414959

Batch 60810, train_perplexity=6037.8594, train_loss=8.705805

Batch 60820, train_perplexity=5455.022, train_loss=8.604292

Batch 60830, train_perplexity=5372.888, train_loss=8.589121

Batch 60840, train_perplexity=5325.5933, train_loss=8.580279

Batch 60850, train_perplexity=6277.4346, train_loss=8.744717

Batch 60860, train_perplexity=5882.5615, train_loss=8.679748

Batch 60870, train_perplexity=4095.7812, train_loss=8.317713

Batch 60880, train_perplexity=4825.863, train_loss=8.481745

Batch 60890, train_perplexity=5164.3613, train_loss=8.549537

Batch 60900, train_perplexity=5053.43, train_loss=8.5278225

Batch 60910, train_perplexity=5303.7124, train_loss=8.576162

Batch 60920, train_perplexity=4583.4453, train_loss=8.430206

Batch 60930, train_perplexity=6342.5933, train_loss=8.755043

Batch 60940, train_perplexity=5695.6016, train_loss=8.6474495

Batch 60950, train_perplexity=5293.48, train_loss=8.574231

Batch 60960, train_perplexity=4597.2, train_loss=8.433203

Batch 60970, train_perplexity=5281.2925, train_loss=8.571926

Batch 60980, train_perplexity=5103.411, train_loss=8.537664

Batch 60990, train_perplexity=5169.2197, train_loss=8.550477

Batch 61000, train_perplexity=4255.378, train_loss=8.355939

Batch 61010, train_perplexity=4817.1904, train_loss=8.479946

Batch 61020, train_perplexity=6515.7563, train_loss=8.781979

Batch 61030, train_perplexity=6012.095, train_loss=8.701529

Batch 61040, train_perplexity=4958.9497, train_loss=8.508949

Batch 61050, train_perplexity=5416.4795, train_loss=8.597201

Batch 61060, train_perplexity=5119.1167, train_loss=8.540737

Batch 61070, train_perplexity=4989.043, train_loss=8.514999

Batch 61080, train_perplexity=5569.1626, train_loss=8.625

Batch 61090, train_perplexity=5264.9688, train_loss=8.5688305

Batch 61100, train_perplexity=5267.6807, train_loss=8.569345

Batch 61110, train_perplexity=5921.156, train_loss=8.686287

Batch 61120, train_perplexity=4599.735, train_loss=8.433754

Batch 61130, train_perplexity=5298.288, train_loss=8.575139

Batch 61140, train_perplexity=5235.0884, train_loss=8.563139

Batch 61150, train_perplexity=5480.16, train_loss=8.60889

Batch 61160, train_perplexity=4672.335, train_loss=8.449414

Batch 61170, train_perplexity=4932.819, train_loss=8.503666

Batch 61180, train_perplexity=5047.0967, train_loss=8.526568
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61190, train_perplexity=5690.286, train_loss=8.646516

Batch 61200, train_perplexity=6242.4565, train_loss=8.739129

Batch 61210, train_perplexity=6029.5044, train_loss=8.70442

Batch 61220, train_perplexity=6265.2573, train_loss=8.742775

Batch 61230, train_perplexity=6060.034, train_loss=8.709471

Batch 61240, train_perplexity=4349.782, train_loss=8.377881

Batch 61250, train_perplexity=5646.117, train_loss=8.638723

Batch 61260, train_perplexity=6048.6597, train_loss=8.707592

Batch 61270, train_perplexity=5500.051, train_loss=8.612513

Batch 61280, train_perplexity=6002.012, train_loss=8.69985

Batch 61290, train_perplexity=4375.323, train_loss=8.383736

Batch 61300, train_perplexity=5636.842, train_loss=8.637079

Batch 61310, train_perplexity=5596.545, train_loss=8.629905

Batch 61320, train_perplexity=5030.3213, train_loss=8.523239

Batch 61330, train_perplexity=6059.5723, train_loss=8.709394

Batch 61340, train_perplexity=4792.9766, train_loss=8.474907

Batch 61350, train_perplexity=5091.826, train_loss=8.535392

Batch 61360, train_perplexity=5247.224, train_loss=8.5654545

Batch 61370, train_perplexity=4946.433, train_loss=8.506422

Batch 61380, train_perplexity=5207.055, train_loss=8.55777

Batch 61390, train_perplexity=4902.504, train_loss=8.497501

Batch 61400, train_perplexity=5992.781, train_loss=8.698311

Batch 61410, train_perplexity=5913.03, train_loss=8.684914

Batch 61420, train_perplexity=5984.2256, train_loss=8.696882

Batch 61430, train_perplexity=6138.688, train_loss=8.722366

Batch 61440, train_perplexity=5422.847, train_loss=8.598376

Batch 61450, train_perplexity=5577.029, train_loss=8.626411

Batch 61460, train_perplexity=6077.3853, train_loss=8.71233

Batch 61470, train_perplexity=4670.308, train_loss=8.44898

Batch 61480, train_perplexity=5826.489, train_loss=8.67017

Batch 61490, train_perplexity=5854.757, train_loss=8.67501

Batch 61500, train_perplexity=6774.5522, train_loss=8.820929

Batch 61510, train_perplexity=4707.3374, train_loss=8.456878

Batch 61520, train_perplexity=4785.249, train_loss=8.473293

Batch 61530, train_perplexity=5091.5786, train_loss=8.535343

Batch 61540, train_perplexity=5305.4834, train_loss=8.576496

Batch 61550, train_perplexity=5057.128, train_loss=8.528554

Batch 61560, train_perplexity=5053.8926, train_loss=8.527914

Batch 61570, train_perplexity=6332.3433, train_loss=8.753426

Batch 61580, train_perplexity=4953.089, train_loss=8.507767

Batch 61590, train_perplexity=5304.593, train_loss=8.576328

Batch 61600, train_perplexity=6157.4507, train_loss=8.725418

Batch 61610, train_perplexity=6531.702, train_loss=8.784423

Batch 61620, train_perplexity=4487.26, train_loss=8.408998

Batch 61630, train_perplexity=4975.9663, train_loss=8.512375

Batch 61640, train_perplexity=4659.702, train_loss=8.446707

Batch 61650, train_perplexity=6313.385, train_loss=8.750427

Batch 61660, train_perplexity=5516.8716, train_loss=8.615566

Batch 61670, train_perplexity=5264.3813, train_loss=8.568719

Batch 61680, train_perplexity=4742.823, train_loss=8.464388

Batch 61690, train_perplexity=6462.56, train_loss=8.773781

Batch 61700, train_perplexity=5891.3477, train_loss=8.68124

Batch 61710, train_perplexity=5829.9126, train_loss=8.670757

Batch 61720, train_perplexity=5259.1875, train_loss=8.567732

Batch 61730, train_perplexity=5339.777, train_loss=8.582939

Batch 61740, train_perplexity=6625.45, train_loss=8.798674

Batch 61750, train_perplexity=4918.0415, train_loss=8.500666

Batch 61760, train_perplexity=5313.0327, train_loss=8.577918

Batch 61770, train_perplexity=5866.9585, train_loss=8.677092

Batch 61780, train_perplexity=5846.354, train_loss=8.6735735

Batch 61790, train_perplexity=5666.508, train_loss=8.642328

Batch 61800, train_perplexity=4680.1616, train_loss=8.451088

Batch 61810, train_perplexity=5902.404, train_loss=8.683115

Batch 61820, train_perplexity=4966.333, train_loss=8.510437

Batch 61830, train_perplexity=5373.554, train_loss=8.589245

Batch 61840, train_perplexity=5491.408, train_loss=8.61094

Batch 61850, train_perplexity=5534.5566, train_loss=8.618767

Batch 61860, train_perplexity=5235.727, train_loss=8.563261

Batch 61870, train_perplexity=5333.166, train_loss=8.5817

Batch 61880, train_perplexity=5096.553, train_loss=8.53632

Batch 61890, train_perplexity=4863.4062, train_loss=8.489494

Batch 61900, train_perplexity=5459.8413, train_loss=8.605175

Batch 61910, train_perplexity=5775.2505, train_loss=8.661337

Batch 61920, train_perplexity=5785.427, train_loss=8.663097

Batch 61930, train_perplexity=5422.997, train_loss=8.598404

Batch 61940, train_perplexity=4814.963, train_loss=8.479484

Batch 61950, train_perplexity=5069.3975, train_loss=8.530977

Batch 61960, train_perplexity=4484.655, train_loss=8.408417

Batch 61970, train_perplexity=4530.513, train_loss=8.418591

Batch 61980, train_perplexity=4454.2593, train_loss=8.401616

Batch 61990, train_perplexity=6262.0615, train_loss=8.742265

Batch 62000, train_perplexity=7297.9834, train_loss=8.895353

Batch 62010, train_perplexity=6434.345, train_loss=8.769405

Batch 62020, train_perplexity=5901.492, train_loss=8.6829605

Batch 62030, train_perplexity=5589.009, train_loss=8.628557

Batch 62040, train_perplexity=5180.793, train_loss=8.552713

Batch 62050, train_perplexity=5766.0103, train_loss=8.659736

Batch 62060, train_perplexity=5487.8325, train_loss=8.610289

Batch 62070, train_perplexity=5826.8667, train_loss=8.670235

Batch 62080, train_perplexity=5367.521, train_loss=8.588121

Batch 62090, train_perplexity=4255.7637, train_loss=8.3560295

Batch 62100, train_perplexity=4582.3833, train_loss=8.429975

Batch 62110, train_perplexity=5107.6226, train_loss=8.538489

Batch 62120, train_perplexity=5301.8516, train_loss=8.575811

Batch 62130, train_perplexity=5499.5996, train_loss=8.612431

Batch 62140, train_perplexity=5949.151, train_loss=8.691004

Batch 62150, train_perplexity=4657.0454, train_loss=8.446136

Batch 62160, train_perplexity=6357.9155, train_loss=8.757456

Batch 62170, train_perplexity=4605.358, train_loss=8.434976

Batch 62180, train_perplexity=4870.345, train_loss=8.49092

Batch 62190, train_perplexity=5230.387, train_loss=8.562241

Batch 62200, train_perplexity=4553.3276, train_loss=8.423614

Batch 62210, train_perplexity=4839.043, train_loss=8.484472

Batch 62220, train_perplexity=4692.828, train_loss=8.453791

Batch 62230, train_perplexity=5250.638, train_loss=8.566105

Batch 62240, train_perplexity=5202.429, train_loss=8.556881

Batch 62250, train_perplexity=4867.0854, train_loss=8.490251

Batch 62260, train_perplexity=5519.787, train_loss=8.616095

Batch 62270, train_perplexity=5284.3306, train_loss=8.572501

Batch 62280, train_perplexity=6342.86, train_loss=8.755085

Batch 62290, train_perplexity=5116.696, train_loss=8.540264

Batch 62300, train_perplexity=5356.7515, train_loss=8.586113

Batch 62310, train_perplexity=5371.674, train_loss=8.588895

Batch 62320, train_perplexity=5647.3345, train_loss=8.638939

Batch 62330, train_perplexity=5650.863, train_loss=8.639564

Batch 62340, train_perplexity=6020.3228, train_loss=8.702896

Batch 62350, train_perplexity=4369.652, train_loss=8.382439

Batch 62360, train_perplexity=6239.534, train_loss=8.738661

Batch 62370, train_perplexity=6131.538, train_loss=8.721201

Batch 62380, train_perplexity=4827.446, train_loss=8.482073

Batch 62390, train_perplexity=4909.7983, train_loss=8.498988

Batch 62400, train_perplexity=5824.5, train_loss=8.669828

Batch 62410, train_perplexity=4786.536, train_loss=8.473562

Batch 62420, train_perplexity=5185.3105, train_loss=8.553585

Batch 62430, train_perplexity=6028.8486, train_loss=8.704311

Batch 62440, train_perplexity=5367.5923, train_loss=8.588135

Batch 62450, train_perplexity=6302.6465, train_loss=8.748725

Batch 62460, train_perplexity=4884.779, train_loss=8.493879

Batch 62470, train_perplexity=4813.237, train_loss=8.479125

Batch 62480, train_perplexity=4927.929, train_loss=8.502674

Batch 62490, train_perplexity=5066.763, train_loss=8.5304575

Batch 62500, train_perplexity=5615.4497, train_loss=8.633277

Batch 62510, train_perplexity=4517.6733, train_loss=8.415752

Batch 62520, train_perplexity=6881.8135, train_loss=8.8366375
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 62530, train_perplexity=5696.862, train_loss=8.647671

Batch 62540, train_perplexity=5289.9775, train_loss=8.573569

Batch 62550, train_perplexity=5689.0547, train_loss=8.646299

Batch 62560, train_perplexity=5115.4224, train_loss=8.540015

Batch 62570, train_perplexity=6015.708, train_loss=8.702129

Batch 62580, train_perplexity=5334.3257, train_loss=8.581918

Batch 62590, train_perplexity=5517.777, train_loss=8.61573

Batch 62600, train_perplexity=5159.5024, train_loss=8.548595

Batch 62610, train_perplexity=5416.1543, train_loss=8.597141

Batch 62620, train_perplexity=4691.9556, train_loss=8.453605

Batch 62630, train_perplexity=5336.422, train_loss=8.582311

Batch 62640, train_perplexity=5570.3154, train_loss=8.625207

Batch 62650, train_perplexity=5460.581, train_loss=8.60531

Batch 62660, train_perplexity=5229.729, train_loss=8.562115

Batch 62670, train_perplexity=5274.9, train_loss=8.570715

Batch 62680, train_perplexity=6859.699, train_loss=8.833419

Batch 62690, train_perplexity=5116.637, train_loss=8.540253

Batch 62700, train_perplexity=5607.8877, train_loss=8.631929

Batch 62710, train_perplexity=4809.442, train_loss=8.478336

Batch 62720, train_perplexity=5783.584, train_loss=8.662779

Batch 62730, train_perplexity=4539.5264, train_loss=8.420578

Batch 62740, train_perplexity=5259.7593, train_loss=8.567841

Batch 62750, train_perplexity=4579.675, train_loss=8.429383

Batch 62760, train_perplexity=6048.971, train_loss=8.7076435

Batch 62770, train_perplexity=5246.4487, train_loss=8.565307

Batch 62780, train_perplexity=4312.969, train_loss=8.369382

Batch 62790, train_perplexity=5019.261, train_loss=8.521038

Batch 62800, train_perplexity=5999.609, train_loss=8.69945

Batch 62810, train_perplexity=4736.8477, train_loss=8.463127

Batch 62820, train_perplexity=5814.1997, train_loss=8.668058

Batch 62830, train_perplexity=4953.2637, train_loss=8.507802

Batch 62840, train_perplexity=4999.1494, train_loss=8.517023

Batch 62850, train_perplexity=5449.9624, train_loss=8.603364

Batch 62860, train_perplexity=5083.9365, train_loss=8.533841

Batch 62870, train_perplexity=4786.0796, train_loss=8.473467

Batch 62880, train_perplexity=5998.8994, train_loss=8.699331

Batch 62890, train_perplexity=4934.014, train_loss=8.503908

Batch 62900, train_perplexity=4932.3955, train_loss=8.50358

Batch 62910, train_perplexity=5512.622, train_loss=8.614796

Batch 62920, train_perplexity=4388.6123, train_loss=8.386768

Batch 62930, train_perplexity=4773.1436, train_loss=8.47076

Batch 62940, train_perplexity=4348.9526, train_loss=8.37769

Batch 62950, train_perplexity=6016.213, train_loss=8.702213

Batch 62960, train_perplexity=5187.017, train_loss=8.553914

Batch 62970, train_perplexity=4860.6055, train_loss=8.488918

Batch 62980, train_perplexity=5572.159, train_loss=8.625538

Batch 62990, train_perplexity=4677.2393, train_loss=8.450463

Batch 63000, train_perplexity=5051.7437, train_loss=8.527489

Batch 63010, train_perplexity=6615.443, train_loss=8.797162

Batch 63020, train_perplexity=5184.149, train_loss=8.553361

Batch 63030, train_perplexity=5550.3823, train_loss=8.621622

Batch 63040, train_perplexity=6245.309, train_loss=8.739586

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Batch 63050, train_perplexity=6479.432, train_loss=8.776388

Batch 63060, train_perplexity=5396.057, train_loss=8.593424

Batch 63070, train_perplexity=6260.193, train_loss=8.741966

Batch 63080, train_perplexity=5768.018, train_loss=8.660084

Batch 63090, train_perplexity=6407.347, train_loss=8.765201

Batch 63100, train_perplexity=5511.865, train_loss=8.614658

Batch 63110, train_perplexity=5164.829, train_loss=8.549627

Batch 63120, train_perplexity=5261.375, train_loss=8.568148

Batch 63130, train_perplexity=5475.5366, train_loss=8.608046

Batch 63140, train_perplexity=5866.2646, train_loss=8.676973

Batch 63150, train_perplexity=5770.78, train_loss=8.6605625

Batch 63160, train_perplexity=6608.847, train_loss=8.7961645

Batch 63170, train_perplexity=6271.3667, train_loss=8.74375

Batch 63180, train_perplexity=5251.274, train_loss=8.566226

Batch 63190, train_perplexity=5258.1343, train_loss=8.567532

Batch 63200, train_perplexity=6255.263, train_loss=8.7411785

Batch 63210, train_perplexity=4276.0522, train_loss=8.3607855

Batch 63220, train_perplexity=5123.219, train_loss=8.541538

Batch 63230, train_perplexity=4927.281, train_loss=8.5025425

Batch 63240, train_perplexity=5329.378, train_loss=8.58099

Batch 63250, train_perplexity=5112.262, train_loss=8.539397

Batch 63260, train_perplexity=5457.239, train_loss=8.604698

Batch 63270, train_perplexity=4634.463, train_loss=8.441276

Batch 63280, train_perplexity=6382.574, train_loss=8.761327

Batch 63290, train_perplexity=5764.4873, train_loss=8.6594715

Batch 63300, train_perplexity=6020.713, train_loss=8.702961

Batch 63310, train_perplexity=5130.4355, train_loss=8.542946

Batch 63320, train_perplexity=5303.5254, train_loss=8.576127

Batch 63330, train_perplexity=5039.877, train_loss=8.525137

Batch 63340, train_perplexity=5552.3833, train_loss=8.621983

Batch 63350, train_perplexity=5489.727, train_loss=8.610634

Batch 63360, train_perplexity=5900.603, train_loss=8.68281

Batch 63370, train_perplexity=4685.8647, train_loss=8.452306

Batch 63380, train_perplexity=4411.073, train_loss=8.391873

Batch 63390, train_perplexity=6138.5127, train_loss=8.722338

Batch 63400, train_perplexity=4911.7373, train_loss=8.499383

Batch 63410, train_perplexity=5000.294, train_loss=8.517252

Batch 63420, train_perplexity=4769.212, train_loss=8.469936

Batch 63430, train_perplexity=5539.357, train_loss=8.619634

Batch 63440, train_perplexity=5191.768, train_loss=8.55483

Batch 63450, train_perplexity=4966.2573, train_loss=8.510422

Batch 63460, train_perplexity=4740.8877, train_loss=8.46398

Batch 63470, train_perplexity=5235.957, train_loss=8.563305

Batch 63480, train_perplexity=6035.0503, train_loss=8.705339

Batch 63490, train_perplexity=5316.9053, train_loss=8.578647

Batch 63500, train_perplexity=5456.895, train_loss=8.604635

Batch 63510, train_perplexity=4726.1807, train_loss=8.460873

Batch 63520, train_perplexity=5597.8955, train_loss=8.630146

Batch 63530, train_perplexity=5089.3354, train_loss=8.534903

Batch 63540, train_perplexity=5308.905, train_loss=8.577141

Batch 63550, train_perplexity=4948.764, train_loss=8.506893

Batch 63560, train_perplexity=5384.4707, train_loss=8.591274

Batch 63570, train_perplexity=6374.0767, train_loss=8.7599945

Batch 63580, train_perplexity=4407.8735, train_loss=8.391148

Batch 63590, train_perplexity=4987.15, train_loss=8.51462

Batch 63600, train_perplexity=5692.7886, train_loss=8.6469555

Batch 63610, train_perplexity=6477.4365, train_loss=8.77608

Batch 63620, train_perplexity=6004.669, train_loss=8.700293

Batch 63630, train_perplexity=5187.5957, train_loss=8.554026

Batch 63640, train_perplexity=5033.2246, train_loss=8.523816

Batch 63650, train_perplexity=5661.4463, train_loss=8.641435

Batch 63660, train_perplexity=5118.306, train_loss=8.540579

Batch 63670, train_perplexity=5317.6655, train_loss=8.57879

Batch 63680, train_perplexity=5743.0215, train_loss=8.655741

Batch 63690, train_perplexity=5590.32, train_loss=8.628792

Batch 63700, train_perplexity=6209.1064, train_loss=8.733772

Batch 63710, train_perplexity=5296.01, train_loss=8.574709

Batch 63720, train_perplexity=4837.6265, train_loss=8.4841795

Batch 63730, train_perplexity=5924.2793, train_loss=8.686814

Batch 63740, train_perplexity=5300.451, train_loss=8.575547

Batch 63750, train_perplexity=4932.17, train_loss=8.503534

Batch 63760, train_perplexity=5067.1064, train_loss=8.530525

Batch 63770, train_perplexity=4521.376, train_loss=8.416572

Batch 63780, train_perplexity=5123.4634, train_loss=8.541586

Batch 63790, train_perplexity=5339.8022, train_loss=8.582944
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 63800, train_perplexity=5644.1577, train_loss=8.638376

Batch 63810, train_perplexity=5331.259, train_loss=8.581343

Batch 63820, train_perplexity=4759.044, train_loss=8.467802

Batch 63830, train_perplexity=5991.912, train_loss=8.698166

Batch 63840, train_perplexity=5348.482, train_loss=8.584568

Batch 63850, train_perplexity=5095.095, train_loss=8.536034

Batch 63860, train_perplexity=5137.29, train_loss=8.544281

Batch 63870, train_perplexity=4795.555, train_loss=8.475445

Batch 63880, train_perplexity=5056.2886, train_loss=8.528388

Batch 63890, train_perplexity=6968.342, train_loss=8.849133

Batch 63900, train_perplexity=6398.2124, train_loss=8.763774

Batch 63910, train_perplexity=6380.3164, train_loss=8.760973

Batch 63920, train_perplexity=5191.06, train_loss=8.554693

Batch 63930, train_perplexity=4545.5654, train_loss=8.421907

Batch 63940, train_perplexity=6273.275, train_loss=8.744054

Batch 63950, train_perplexity=6085.621, train_loss=8.713684

Batch 63960, train_perplexity=5333.339, train_loss=8.581733

Batch 63970, train_perplexity=5299.314, train_loss=8.575333

Batch 63980, train_perplexity=5858.27, train_loss=8.67561

Batch 63990, train_perplexity=5375.364, train_loss=8.5895815

Batch 64000, train_perplexity=5981.5728, train_loss=8.696439

Batch 64010, train_perplexity=5688.4795, train_loss=8.646198

Batch 64020, train_perplexity=5563.7373, train_loss=8.624025

Batch 64030, train_perplexity=5810.4414, train_loss=8.667412

Batch 64040, train_perplexity=6503.7993, train_loss=8.780142

Batch 64050, train_perplexity=6067.402, train_loss=8.710686

Batch 64060, train_perplexity=5033.191, train_loss=8.523809

Batch 64070, train_perplexity=5889.7915, train_loss=8.680976

Batch 64080, train_perplexity=6483.4873, train_loss=8.777014

Batch 64090, train_perplexity=5058.4155, train_loss=8.528809

Batch 64100, train_perplexity=5477.6626, train_loss=8.608434

Batch 64110, train_perplexity=5839.7563, train_loss=8.672444

Batch 64120, train_perplexity=6077.913, train_loss=8.712417

Batch 64130, train_perplexity=5766.456, train_loss=8.659813

Batch 64140, train_perplexity=5170.714, train_loss=8.550766

Batch 64150, train_perplexity=4750.691, train_loss=8.466045

Batch 64160, train_perplexity=6272.874, train_loss=8.74399

Batch 64170, train_perplexity=6211.2266, train_loss=8.734114

Batch 64180, train_perplexity=5755.8574, train_loss=8.657973

Batch 64190, train_perplexity=4499.7773, train_loss=8.411783

Batch 64200, train_perplexity=5793.9683, train_loss=8.664573

Batch 64210, train_perplexity=6023.2515, train_loss=8.7033825

Batch 64220, train_perplexity=4618.46, train_loss=8.437817

Batch 64230, train_perplexity=6394.3086, train_loss=8.763164

Batch 64240, train_perplexity=6144.9087, train_loss=8.723379

Batch 64250, train_perplexity=5984.2715, train_loss=8.69689

Batch 64260, train_perplexity=4802.3286, train_loss=8.476856

Batch 64270, train_perplexity=5722.159, train_loss=8.6521015

Batch 64280, train_perplexity=6814.5713, train_loss=8.826818

Batch 64290, train_perplexity=5523.2993, train_loss=8.616731

Batch 64300, train_perplexity=5320.0854, train_loss=8.579245

Batch 64310, train_perplexity=4795.6377, train_loss=8.475462

Batch 64320, train_perplexity=4937.1772, train_loss=8.504549

Batch 64330, train_perplexity=4263.043, train_loss=8.3577385

Batch 64340, train_perplexity=5806.874, train_loss=8.666798

Batch 64350, train_perplexity=5717.1846, train_loss=8.651232

Batch 64360, train_perplexity=5479.6323, train_loss=8.608793

Batch 64370, train_perplexity=6116.7446, train_loss=8.718785

Batch 64380, train_perplexity=4722.6797, train_loss=8.460132

Batch 64390, train_perplexity=5052.8423, train_loss=8.527706

Batch 64400, train_perplexity=5694.0264, train_loss=8.647173

Batch 64410, train_perplexity=5908.7793, train_loss=8.684195

Batch 64420, train_perplexity=5253.278, train_loss=8.566607

Batch 64430, train_perplexity=5331.5947, train_loss=8.581406

Batch 64440, train_perplexity=6351.6367, train_loss=8.756468

Batch 64450, train_perplexity=5397.195, train_loss=8.593635

Batch 64460, train_perplexity=5698.927, train_loss=8.648033

Batch 64470, train_perplexity=5636.111, train_loss=8.63695

Batch 64480, train_perplexity=4691.405, train_loss=8.453487

Batch 64490, train_perplexity=5390.2144, train_loss=8.59234

Batch 64500, train_perplexity=5481.085, train_loss=8.609058

Batch 64510, train_perplexity=4404.709, train_loss=8.3904295

Batch 64520, train_perplexity=4738.2393, train_loss=8.463421

Batch 64530, train_perplexity=5815.0093, train_loss=8.668198

Batch 64540, train_perplexity=5360.257, train_loss=8.586767

Batch 64550, train_perplexity=6551.9595, train_loss=8.787519

Batch 64560, train_perplexity=5862.2603, train_loss=8.6762905

Batch 64570, train_perplexity=5573.626, train_loss=8.625801

Batch 64580, train_perplexity=6478.394, train_loss=8.776228

Batch 64590, train_perplexity=5186.7646, train_loss=8.553865

Batch 64600, train_perplexity=5376.066, train_loss=8.589712

Batch 64610, train_perplexity=4923.8374, train_loss=8.501843

Batch 64620, train_perplexity=5840.5083, train_loss=8.672573

Batch 64630, train_perplexity=5046.5913, train_loss=8.526468

Batch 64640, train_perplexity=6651.1914, train_loss=8.802551

Batch 64650, train_perplexity=5827.067, train_loss=8.670269

Batch 64660, train_perplexity=5686.256, train_loss=8.645807

Batch 64670, train_perplexity=5832.6265, train_loss=8.671223

Batch 64680, train_perplexity=6359.31, train_loss=8.757675

Batch 64690, train_perplexity=6129.1294, train_loss=8.720808

Batch 64700, train_perplexity=5258.21, train_loss=8.567546

Batch 64710, train_perplexity=5492.8276, train_loss=8.611198

Batch 64720, train_perplexity=4981.878, train_loss=8.513562

Batch 64730, train_perplexity=5663.482, train_loss=8.641794

Batch 64740, train_perplexity=5562.3794, train_loss=8.623781

Batch 64750, train_perplexity=5478.43, train_loss=8.608574

Batch 64760, train_perplexity=5901.1885, train_loss=8.682909

Batch 64770, train_perplexity=5751.0947, train_loss=8.6571455

Batch 64780, train_perplexity=6222.029, train_loss=8.735851

Batch 64790, train_perplexity=5038.8867, train_loss=8.5249405

Batch 64800, train_perplexity=6611.634, train_loss=8.796586

Batch 64810, train_perplexity=4294.635, train_loss=8.365122

Batch 64820, train_perplexity=5996.497, train_loss=8.698931

Batch 64830, train_perplexity=5472.394, train_loss=8.607471

Batch 64840, train_perplexity=5618.6475, train_loss=8.633846

Batch 64850, train_perplexity=6553.7905, train_loss=8.787799

Batch 64860, train_perplexity=4705.982, train_loss=8.45659

Batch 64870, train_perplexity=5593.4663, train_loss=8.6293545

Batch 64880, train_perplexity=5339.2065, train_loss=8.582832

Batch 64890, train_perplexity=5062.402, train_loss=8.529596

Batch 64900, train_perplexity=4565.9595, train_loss=8.426384

Batch 64910, train_perplexity=5314.659, train_loss=8.578224

Batch 64920, train_perplexity=6134.416, train_loss=8.72167

Batch 64930, train_perplexity=6040.002, train_loss=8.70616

Batch 64940, train_perplexity=4230.606, train_loss=8.3501005

Batch 64950, train_perplexity=5905.788, train_loss=8.683688

Batch 64960, train_perplexity=6139.5547, train_loss=8.722507

Batch 64970, train_perplexity=5623.306, train_loss=8.634675

Batch 64980, train_perplexity=5921.641, train_loss=8.686369

Batch 64990, train_perplexity=5139.564, train_loss=8.5447235

Batch 65000, train_perplexity=6816.1445, train_loss=8.827049

Batch 65010, train_perplexity=6009.5386, train_loss=8.701103

Batch 65020, train_perplexity=5597.0894, train_loss=8.630002

Batch 65030, train_perplexity=4264.5312, train_loss=8.358088

Batch 65040, train_perplexity=5811.982, train_loss=8.667677

Batch 65050, train_perplexity=5631.3506, train_loss=8.636105

Batch 65060, train_perplexity=5253.2476, train_loss=8.566602

Batch 65070, train_perplexity=4554.131, train_loss=8.42379

Batch 65080, train_perplexity=5799.4355, train_loss=8.665516

Batch 65090, train_perplexity=5718.1333, train_loss=8.651398

Batch 65100, train_perplexity=4313.78, train_loss=8.36957

Batch 65110, train_perplexity=5447.707, train_loss=8.60295

Batch 65120, train_perplexity=5586.76, train_loss=8.628155

Batch 65130, train_perplexity=6818.914, train_loss=8.8274555
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65140, train_perplexity=5567.9995, train_loss=8.624791

Batch 65150, train_perplexity=5776.319, train_loss=8.661522

Batch 65160, train_perplexity=5609.3, train_loss=8.632181

Batch 65170, train_perplexity=6186.044, train_loss=8.730051

Batch 65180, train_perplexity=5795.1235, train_loss=8.664772

Batch 65190, train_perplexity=5453.1494, train_loss=8.603949

Batch 65200, train_perplexity=5652.949, train_loss=8.639933

Batch 65210, train_perplexity=5027.276, train_loss=8.522634

Batch 65220, train_perplexity=4935.3506, train_loss=8.504179

Batch 65230, train_perplexity=5013.9746, train_loss=8.519984

Batch 65240, train_perplexity=4776.167, train_loss=8.471394

Batch 65250, train_perplexity=4989.9185, train_loss=8.515175

Batch 65260, train_perplexity=4406.104, train_loss=8.390746

Batch 65270, train_perplexity=5619.1514, train_loss=8.633936

Batch 65280, train_perplexity=5077.042, train_loss=8.532484

Batch 65290, train_perplexity=6913.316, train_loss=8.841205

Batch 65300, train_perplexity=5140.819, train_loss=8.544968

Batch 65310, train_perplexity=5761.08, train_loss=8.65888

Batch 65320, train_perplexity=4943.99, train_loss=8.505928

Batch 65330, train_perplexity=5976.777, train_loss=8.695637

Batch 65340, train_perplexity=6113.9277, train_loss=8.718325

Batch 65350, train_perplexity=4856.213, train_loss=8.488014

Batch 65360, train_perplexity=5663.1475, train_loss=8.641735

Batch 65370, train_perplexity=5809.1836, train_loss=8.667195

Batch 65380, train_perplexity=5683.507, train_loss=8.645324

Batch 65390, train_perplexity=6088.3496, train_loss=8.714132

Batch 65400, train_perplexity=5447.603, train_loss=8.602931

Batch 65410, train_perplexity=5382.889, train_loss=8.590981

Batch 65420, train_perplexity=6592.2793, train_loss=8.793654

Batch 65430, train_perplexity=4610.7324, train_loss=8.436142

Batch 65440, train_perplexity=4673.097, train_loss=8.449577

Batch 65450, train_perplexity=5564.268, train_loss=8.624121

Batch 65460, train_perplexity=4764.884, train_loss=8.469028

Batch 65470, train_perplexity=4967.678, train_loss=8.510708

Batch 65480, train_perplexity=4602.4116, train_loss=8.434336

Batch 65490, train_perplexity=4848.5312, train_loss=8.486431

Batch 65500, train_perplexity=5832.1816, train_loss=8.671146

Batch 65510, train_perplexity=6690.035, train_loss=8.808374

Batch 65520, train_perplexity=5447.1045, train_loss=8.602839

Batch 65530, train_perplexity=5956.9287, train_loss=8.69231

Batch 65540, train_perplexity=5930.571, train_loss=8.687876

Batch 65550, train_perplexity=5359.8687, train_loss=8.586695

Batch 65560, train_perplexity=5367.2344, train_loss=8.588068

Batch 65570, train_perplexity=5257.0215, train_loss=8.56732

Batch 65580, train_perplexity=5813.1904, train_loss=8.667885

Batch 65590, train_perplexity=5777.6743, train_loss=8.6617565

Batch 65600, train_perplexity=5825.211, train_loss=8.6699505

Batch 65610, train_perplexity=6255.1084, train_loss=8.741154

Batch 65620, train_perplexity=5067.4062, train_loss=8.530584

Batch 65630, train_perplexity=5061.427, train_loss=8.529404

Batch 65640, train_perplexity=4608.6836, train_loss=8.435698

Batch 65650, train_perplexity=5544.7373, train_loss=8.6206045

Batch 65660, train_perplexity=5254.455, train_loss=8.566832

Batch 65670, train_perplexity=6036.7886, train_loss=8.705627

Batch 65680, train_perplexity=4353.5547, train_loss=8.378748

Batch 65690, train_perplexity=5907.1963, train_loss=8.683927

Batch 65700, train_perplexity=5210.473, train_loss=8.558426

Batch 65710, train_perplexity=4868.799, train_loss=8.4906025

Batch 65720, train_perplexity=5229.1406, train_loss=8.562002

Batch 65730, train_perplexity=4565.707, train_loss=8.426329

Batch 65740, train_perplexity=5156.8164, train_loss=8.548075

Batch 65750, train_perplexity=5795.82, train_loss=8.664892

Batch 65760, train_perplexity=4813.108, train_loss=8.479098

Batch 65770, train_perplexity=3993.6636, train_loss=8.292464

Batch 65780, train_perplexity=5015.1704, train_loss=8.520223

Batch 65790, train_perplexity=5454.6577, train_loss=8.604225

Batch 65800, train_perplexity=5457.478, train_loss=8.604742

Batch 65810, train_perplexity=4698.085, train_loss=8.45491

Batch 65820, train_perplexity=5558.768, train_loss=8.623132

Batch 65830, train_perplexity=4922.4854, train_loss=8.501569

Batch 65840, train_perplexity=4839.846, train_loss=8.484638

Batch 65850, train_perplexity=4412.525, train_loss=8.392202

Batch 65860, train_perplexity=5985.5156, train_loss=8.697098

Batch 65870, train_perplexity=6172.8784, train_loss=8.727921

Batch 65880, train_perplexity=5722.4595, train_loss=8.652154

Batch 65890, train_perplexity=5192.0703, train_loss=8.554888

Batch 65900, train_perplexity=5041.5786, train_loss=8.525475

Batch 65910, train_perplexity=6189.2656, train_loss=8.730572

Batch 65920, train_perplexity=5583.106, train_loss=8.627501

Batch 65930, train_perplexity=6029.9243, train_loss=8.70449

Batch 65940, train_perplexity=5476.571, train_loss=8.608234

Batch 65950, train_perplexity=5623.5635, train_loss=8.634721

Batch 65960, train_perplexity=4787.65, train_loss=8.473795

Batch 65970, train_perplexity=5041.29, train_loss=8.525417

Batch 65980, train_perplexity=5567.973, train_loss=8.624786

Batch 65990, train_perplexity=6114.569, train_loss=8.71843

Batch 66000, train_perplexity=4564.823, train_loss=8.426135

Batch 66010, train_perplexity=5571.1387, train_loss=8.625355

Batch 66020, train_perplexity=5239.1035, train_loss=8.563906

Batch 66030, train_perplexity=6045.3496, train_loss=8.707045

Batch 66040, train_perplexity=5937.622, train_loss=8.689064

Batch 66050, train_perplexity=6234.9126, train_loss=8.73792

Batch 66060, train_perplexity=4842.441, train_loss=8.485174

Batch 66070, train_perplexity=4783.962, train_loss=8.473024

Batch 66080, train_perplexity=5645.1914, train_loss=8.638559

Batch 66090, train_perplexity=5603.173, train_loss=8.631088

Batch 66100, train_perplexity=4420.0767, train_loss=8.393912

Batch 66110, train_perplexity=6327.785, train_loss=8.752706

Batch 66120, train_perplexity=4907.3784, train_loss=8.498495

Batch 66130, train_perplexity=5879.0225, train_loss=8.679146

Batch 66140, train_perplexity=4844.094, train_loss=8.485516

Batch 66150, train_perplexity=5013.1763, train_loss=8.519825

Batch 66160, train_perplexity=6930.6177, train_loss=8.843704

Batch 66170, train_perplexity=5314.8926, train_loss=8.578268

Batch 66180, train_perplexity=5029.429, train_loss=8.523062

Batch 66190, train_perplexity=5613.8164, train_loss=8.632986

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Batch 66200, train_perplexity=5386.4272, train_loss=8.591638

Batch 66210, train_perplexity=4698.8647, train_loss=8.455076

Batch 66220, train_perplexity=6107.063, train_loss=8.717201

Batch 66230, train_perplexity=5952.528, train_loss=8.691571

Batch 66240, train_perplexity=5732.4663, train_loss=8.653901

Batch 66250, train_perplexity=4717.872, train_loss=8.459113

Batch 66260, train_perplexity=4732.6304, train_loss=8.462236

Batch 66270, train_perplexity=4877.3545, train_loss=8.492358

Batch 66280, train_perplexity=6474.6636, train_loss=8.775652

Batch 66290, train_perplexity=5826.2056, train_loss=8.670121

Batch 66300, train_perplexity=4615.255, train_loss=8.437122

Batch 66310, train_perplexity=5942.5166, train_loss=8.689888

Batch 66320, train_perplexity=4377.7104, train_loss=8.384281

Batch 66330, train_perplexity=4909.475, train_loss=8.498922

Batch 66340, train_perplexity=5495.112, train_loss=8.611614

Batch 66350, train_perplexity=6703.3574, train_loss=8.810364

Batch 66360, train_perplexity=5480.2383, train_loss=8.608904

Batch 66370, train_perplexity=5179.6865, train_loss=8.5525

Batch 66380, train_perplexity=5002.655, train_loss=8.517724

Batch 66390, train_perplexity=5799.104, train_loss=8.665459

Batch 66400, train_perplexity=6084.043, train_loss=8.713425
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66410, train_perplexity=5010.4805, train_loss=8.519287

Batch 66420, train_perplexity=5775.8784, train_loss=8.661446

Batch 66430, train_perplexity=5077.9814, train_loss=8.532669

Batch 66440, train_perplexity=5045.1523, train_loss=8.526183

Batch 66450, train_perplexity=5661.441, train_loss=8.641434

Batch 66460, train_perplexity=6334.2275, train_loss=8.753723

Batch 66470, train_perplexity=6196.755, train_loss=8.731781

Batch 66480, train_perplexity=5025.5405, train_loss=8.522288

Batch 66490, train_perplexity=5287.0977, train_loss=8.573025

Batch 66500, train_perplexity=5901.689, train_loss=8.682994

Batch 66510, train_perplexity=5371.52, train_loss=8.588866

Batch 66520, train_perplexity=4902.733, train_loss=8.497548

Batch 66530, train_perplexity=6184.0264, train_loss=8.729725

Batch 66540, train_perplexity=5341.824, train_loss=8.583323

Batch 66550, train_perplexity=5596.908, train_loss=8.62997

Batch 66560, train_perplexity=5502.737, train_loss=8.613001

Batch 66570, train_perplexity=6429.9653, train_loss=8.768724

Batch 66580, train_perplexity=5210.632, train_loss=8.558456

Batch 66590, train_perplexity=5213.028, train_loss=8.558916

Batch 66600, train_perplexity=6283.975, train_loss=8.745758

Batch 66610, train_perplexity=5919.778, train_loss=8.686054

Batch 66620, train_perplexity=6145.565, train_loss=8.723486

Batch 66630, train_perplexity=5084.145, train_loss=8.533882

Batch 66640, train_perplexity=5189.783, train_loss=8.554447

Batch 66650, train_perplexity=5113.7637, train_loss=8.539691

Batch 66660, train_perplexity=5343.9697, train_loss=8.583724

Batch 66670, train_perplexity=5556.6953, train_loss=8.622759

Batch 66680, train_perplexity=5802.927, train_loss=8.666118

Batch 66690, train_perplexity=5583.133, train_loss=8.627505

Batch 66700, train_perplexity=4934.3247, train_loss=8.503971

Batch 66710, train_perplexity=4957.4224, train_loss=8.508641

Batch 66720, train_perplexity=5164.514, train_loss=8.549566

Batch 66730, train_perplexity=5409.0825, train_loss=8.595835

Batch 66740, train_perplexity=5228.891, train_loss=8.5619545

Batch 66750, train_perplexity=5005.4946, train_loss=8.518291

Batch 66760, train_perplexity=4780.9155, train_loss=8.472387

Batch 66770, train_perplexity=5082.5547, train_loss=8.533569

Batch 66780, train_perplexity=4891.1235, train_loss=8.495177

Batch 66790, train_perplexity=5988.6333, train_loss=8.6976185

Batch 66800, train_perplexity=5613.4634, train_loss=8.632923

Batch 66810, train_perplexity=5342.4663, train_loss=8.583443

Batch 66820, train_perplexity=5414.135, train_loss=8.596768

Batch 66830, train_perplexity=7023.7183, train_loss=8.857048

Batch 66840, train_perplexity=4442.3345, train_loss=8.398935

Batch 66850, train_perplexity=5208.4756, train_loss=8.558043

Batch 66860, train_perplexity=6353.5146, train_loss=8.756763

Batch 66870, train_perplexity=4750.188, train_loss=8.4659395

Batch 66880, train_perplexity=5675.6587, train_loss=8.643942

Batch 66890, train_perplexity=5537.5874, train_loss=8.619314

Batch 66900, train_perplexity=5815.442, train_loss=8.668272

Batch 66910, train_perplexity=6537.105, train_loss=8.78525

Batch 66920, train_perplexity=6249.885, train_loss=8.740318

Batch 66930, train_perplexity=6267.558, train_loss=8.743142

Batch 66940, train_perplexity=5403.514, train_loss=8.594805

Batch 66950, train_perplexity=5923.319, train_loss=8.686652

Batch 66960, train_perplexity=5489.005, train_loss=8.610502

Batch 66970, train_perplexity=5841.5557, train_loss=8.672752

Batch 66980, train_perplexity=5663.6333, train_loss=8.641821

Batch 66990, train_perplexity=6620.227, train_loss=8.797885

Batch 67000, train_perplexity=4840.8203, train_loss=8.484839

Batch 67010, train_perplexity=5680.088, train_loss=8.644722

Batch 67020, train_perplexity=5461.982, train_loss=8.605567

Batch 67030, train_perplexity=6554.303, train_loss=8.787877

Batch 67040, train_perplexity=5714.726, train_loss=8.650802

Batch 67050, train_perplexity=5178.738, train_loss=8.552317

Batch 67060, train_perplexity=4452.3696, train_loss=8.401192

Batch 67070, train_perplexity=5644.05, train_loss=8.638357

Batch 67080, train_perplexity=6317.4565, train_loss=8.751072

Batch 67090, train_perplexity=5177.6514, train_loss=8.552107

Batch 67100, train_perplexity=3977.6042, train_loss=8.288435

Batch 67110, train_perplexity=4919.8755, train_loss=8.501039

Batch 67120, train_perplexity=5139.0347, train_loss=8.5446205

Batch 67130, train_perplexity=5374.3027, train_loss=8.589384

Batch 67140, train_perplexity=6356.6665, train_loss=8.757259

Batch 67150, train_perplexity=5785.46, train_loss=8.663103

Batch 67160, train_perplexity=5115.349, train_loss=8.540001

Batch 67170, train_perplexity=4734.418, train_loss=8.462614

Batch 67180, train_perplexity=4890.093, train_loss=8.4949665

Batch 67190, train_perplexity=5087.123, train_loss=8.534468

Batch 67200, train_perplexity=5503.797, train_loss=8.6131935

Batch 67210, train_perplexity=5351.4106, train_loss=8.585115

Batch 67220, train_perplexity=5471.7363, train_loss=8.607351

Batch 67230, train_perplexity=5330.4355, train_loss=8.581188

Batch 67240, train_perplexity=5577.582, train_loss=8.626511

Batch 67250, train_perplexity=5193.536, train_loss=8.55517

Batch 67260, train_perplexity=5830.9355, train_loss=8.670933

Batch 67270, train_perplexity=4517.277, train_loss=8.415665

Batch 67280, train_perplexity=5168.988, train_loss=8.550432

Batch 67290, train_perplexity=5585.0283, train_loss=8.627845

Batch 67300, train_perplexity=7276.8696, train_loss=8.892456

Batch 67310, train_perplexity=5773.4385, train_loss=8.661023

Batch 67320, train_perplexity=5601.121, train_loss=8.630722

Batch 67330, train_perplexity=5757.971, train_loss=8.65834

Batch 67340, train_perplexity=5901.098, train_loss=8.682894

Batch 67350, train_perplexity=5562.0713, train_loss=8.623726

Batch 67360, train_perplexity=5925.7993, train_loss=8.687071

Batch 67370, train_perplexity=5723.6167, train_loss=8.652356

Batch 67380, train_perplexity=6549.9727, train_loss=8.787216

Batch 67390, train_perplexity=6261.214, train_loss=8.742129

Batch 67400, train_perplexity=7167.4146, train_loss=8.8773

Batch 67410, train_perplexity=4381.954, train_loss=8.38525

Batch 67420, train_perplexity=4970.2656, train_loss=8.511229

Batch 67430, train_perplexity=5170.398, train_loss=8.550705

Batch 67440, train_perplexity=5051.4834, train_loss=8.527437

Batch 67450, train_perplexity=6788.509, train_loss=8.822987

Batch 67460, train_perplexity=6422.9907, train_loss=8.767639

Batch 67470, train_perplexity=4276.9087, train_loss=8.360986

Batch 67480, train_perplexity=6701.657, train_loss=8.81011

Batch 67490, train_perplexity=5973.7456, train_loss=8.695129

Batch 67500, train_perplexity=5254.5605, train_loss=8.566852

Batch 67510, train_perplexity=4667.307, train_loss=8.448338

Batch 67520, train_perplexity=5537.001, train_loss=8.619208

Batch 67530, train_perplexity=5375.9478, train_loss=8.58969

Batch 67540, train_perplexity=5617.378, train_loss=8.63362

Batch 67550, train_perplexity=5136.741, train_loss=8.544174

Batch 67560, train_perplexity=5370.8696, train_loss=8.588745

Batch 67570, train_perplexity=7011.8457, train_loss=8.855356

Batch 67580, train_perplexity=4724.7563, train_loss=8.460571

Batch 67590, train_perplexity=5248.941, train_loss=8.565782

Batch 67600, train_perplexity=5247.064, train_loss=8.565424

Batch 67610, train_perplexity=5648.74, train_loss=8.639188

Batch 67620, train_perplexity=4583.3755, train_loss=8.430191

Batch 67630, train_perplexity=5702.5366, train_loss=8.648666

Batch 67640, train_perplexity=5852.8306, train_loss=8.674681

Batch 67650, train_perplexity=4883.0884, train_loss=8.493533

Batch 67660, train_perplexity=5295.3633, train_loss=8.574587

Batch 67670, train_perplexity=5263.352, train_loss=8.568523

Batch 67680, train_perplexity=5385.667, train_loss=8.591496

Batch 67690, train_perplexity=6944.194, train_loss=8.845661

Batch 67700, train_perplexity=5201.7646, train_loss=8.556753

Batch 67710, train_perplexity=4460.168, train_loss=8.402942

Batch 67720, train_perplexity=5518.135, train_loss=8.615795

Batch 67730, train_perplexity=5632.5483, train_loss=8.636317

Batch 67740, train_perplexity=5572.8394, train_loss=8.62566
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 67750, train_perplexity=4526.5317, train_loss=8.417711

Batch 67760, train_perplexity=5883.274, train_loss=8.679869

Batch 67770, train_perplexity=5932.715, train_loss=8.688237

Batch 67780, train_perplexity=5487.288, train_loss=8.610189

Batch 67790, train_perplexity=5838.9487, train_loss=8.672306

Batch 67800, train_perplexity=5228.1333, train_loss=8.56181

Batch 67810, train_perplexity=5441.0454, train_loss=8.601727

Batch 67820, train_perplexity=4977.1104, train_loss=8.512605

Batch 67830, train_perplexity=5506.2646, train_loss=8.613642

Batch 67840, train_perplexity=5259.8896, train_loss=8.567865

Batch 67850, train_perplexity=5216.2305, train_loss=8.55953

Batch 67860, train_perplexity=7562.202, train_loss=8.930918

Batch 67870, train_perplexity=5989.0786, train_loss=8.697693

Batch 67880, train_perplexity=5794.709, train_loss=8.6647005

Batch 67890, train_perplexity=5563.0054, train_loss=8.623894

Batch 67900, train_perplexity=6153.036, train_loss=8.724701

Batch 67910, train_perplexity=5232.827, train_loss=8.562707

Batch 67920, train_perplexity=5579.3643, train_loss=8.62683

Batch 67930, train_perplexity=5748.249, train_loss=8.656651

Batch 67940, train_perplexity=5527.668, train_loss=8.617521

Batch 67950, train_perplexity=5336.3765, train_loss=8.582302

Batch 67960, train_perplexity=5935.086, train_loss=8.688637

Batch 67970, train_perplexity=5722.2466, train_loss=8.652117

Batch 67980, train_perplexity=6424.3447, train_loss=8.76785

Batch 67990, train_perplexity=5217.434, train_loss=8.559761

Batch 68000, train_perplexity=4749.2417, train_loss=8.46574

Batch 68010, train_perplexity=5715.1567, train_loss=8.650877

Batch 68020, train_perplexity=6722.1016, train_loss=8.813156

Batch 68030, train_perplexity=3953.5063, train_loss=8.282358

Batch 68040, train_perplexity=5635.9067, train_loss=8.636913

Batch 68050, train_perplexity=5744.0566, train_loss=8.655921

Batch 68060, train_perplexity=7553.877, train_loss=8.929816

Batch 68070, train_perplexity=6156.464, train_loss=8.725258

Batch 68080, train_perplexity=5680.3047, train_loss=8.64476

Batch 68090, train_perplexity=4562.647, train_loss=8.425658

Batch 68100, train_perplexity=4724.6934, train_loss=8.460558

Batch 68110, train_perplexity=5982.183, train_loss=8.696541

Batch 68120, train_perplexity=5115.837, train_loss=8.540096

Batch 68130, train_perplexity=5160.669, train_loss=8.548821

Batch 68140, train_perplexity=5739.8457, train_loss=8.655188

Batch 68150, train_perplexity=5005.924, train_loss=8.518377

Batch 68160, train_perplexity=5366.41, train_loss=8.587914

Batch 68170, train_perplexity=5377.958, train_loss=8.590064

Batch 68180, train_perplexity=5251.865, train_loss=8.566339

Batch 68190, train_perplexity=4750.229, train_loss=8.465948

Batch 68200, train_perplexity=5332.632, train_loss=8.5816

Batch 68210, train_perplexity=5510.3306, train_loss=8.61438

Batch 68220, train_perplexity=5125.1055, train_loss=8.541906

Batch 68230, train_perplexity=6096.8735, train_loss=8.715531

Batch 68240, train_perplexity=5008.9136, train_loss=8.518974

Batch 68250, train_perplexity=4784.7334, train_loss=8.473186

Batch 68260, train_perplexity=4847.4355, train_loss=8.486205

Batch 68270, train_perplexity=7093.6465, train_loss=8.866955

Batch 68280, train_perplexity=4467.6777, train_loss=8.404624

Batch 68290, train_perplexity=5600.5654, train_loss=8.630623

Batch 68300, train_perplexity=5763.08, train_loss=8.659227

Batch 68310, train_perplexity=5718.3296, train_loss=8.651432

Batch 68320, train_perplexity=4746.525, train_loss=8.465168

Batch 68330, train_perplexity=6232.219, train_loss=8.737488

Batch 68340, train_perplexity=5194.378, train_loss=8.555332

Batch 68350, train_perplexity=6261.494, train_loss=8.742174

Batch 68360, train_perplexity=4823.461, train_loss=8.481247

Batch 68370, train_perplexity=6084.426, train_loss=8.713488

Batch 68380, train_perplexity=5032.6055, train_loss=8.523693

Batch 68390, train_perplexity=5153.896, train_loss=8.547508

Batch 68400, train_perplexity=5843.7173, train_loss=8.673122

Batch 68410, train_perplexity=5937.141, train_loss=8.688983

Batch 68420, train_perplexity=5336.386, train_loss=8.582304

Batch 68430, train_perplexity=5070.171, train_loss=8.53113

Batch 68440, train_perplexity=5203.0693, train_loss=8.557004

Batch 68450, train_perplexity=5960.6, train_loss=8.692926

Batch 68460, train_perplexity=5438.5034, train_loss=8.601259

Batch 68470, train_perplexity=5083.602, train_loss=8.533775

Batch 68480, train_perplexity=6733.8306, train_loss=8.814899

Batch 68490, train_perplexity=6061.3755, train_loss=8.709692

Batch 68500, train_perplexity=6283.61, train_loss=8.7457

Batch 68510, train_perplexity=4769.221, train_loss=8.469938

Batch 68520, train_perplexity=6124.0933, train_loss=8.719986

Batch 68530, train_perplexity=5208.3066, train_loss=8.55801

Batch 68540, train_perplexity=6376.8125, train_loss=8.760424

Batch 68550, train_perplexity=5695.808, train_loss=8.647486

Batch 68560, train_perplexity=5790.1074, train_loss=8.663906

Batch 68570, train_perplexity=4563.5913, train_loss=8.425865

Batch 68580, train_perplexity=5914.1636, train_loss=8.685105

Batch 68590, train_perplexity=4695.3213, train_loss=8.454322

Batch 68600, train_perplexity=5183.2095, train_loss=8.55318

Batch 68610, train_perplexity=5757.834, train_loss=8.658317

Batch 68620, train_perplexity=5265.3906, train_loss=8.568911

Batch 68630, train_perplexity=4420.7134, train_loss=8.394056

Batch 68640, train_perplexity=5155.2627, train_loss=8.547773

Batch 68650, train_perplexity=6205.371, train_loss=8.7331705

Batch 68660, train_perplexity=5140.103, train_loss=8.544828

Batch 68670, train_perplexity=5675.9834, train_loss=8.643999

Batch 68680, train_perplexity=5645.299, train_loss=8.638578

Batch 68690, train_perplexity=4680.5503, train_loss=8.451171

Batch 68700, train_perplexity=5963.636, train_loss=8.693436

Batch 68710, train_perplexity=4874.499, train_loss=8.491773

Batch 68720, train_perplexity=4622.924, train_loss=8.438783

Batch 68730, train_perplexity=5999.168, train_loss=8.699376

Batch 68740, train_perplexity=5232.6924, train_loss=8.562681

Batch 68750, train_perplexity=6879.333, train_loss=8.836277

Batch 68760, train_perplexity=6210.1133, train_loss=8.733934

Batch 68770, train_perplexity=4890.1206, train_loss=8.494972

Batch 68780, train_perplexity=4625.2656, train_loss=8.439289

Batch 68790, train_perplexity=5593.285, train_loss=8.629322

Batch 68800, train_perplexity=6025.814, train_loss=8.703808

Batch 68810, train_perplexity=6249.6343, train_loss=8.740278

Batch 68820, train_perplexity=5696.5356, train_loss=8.647614

Batch 68830, train_perplexity=5944.2056, train_loss=8.690172

Batch 68840, train_perplexity=6430.113, train_loss=8.768747

Batch 68850, train_perplexity=5261.7563, train_loss=8.56822

Batch 68860, train_perplexity=5922.6694, train_loss=8.6865425

Batch 68870, train_perplexity=6143.1685, train_loss=8.723096

Batch 68880, train_perplexity=5967.7554, train_loss=8.694126

Batch 68890, train_perplexity=6288.6875, train_loss=8.746508

Batch 68900, train_perplexity=6100.689, train_loss=8.716157

Batch 68910, train_perplexity=5655.176, train_loss=8.6403265

Batch 68920, train_perplexity=5195.567, train_loss=8.555561

Batch 68930, train_perplexity=5187.8037, train_loss=8.554066

Batch 68940, train_perplexity=4743.0312, train_loss=8.464432

Batch 68950, train_perplexity=5093.57, train_loss=8.535734

Batch 68960, train_perplexity=5050.3945, train_loss=8.527222

Batch 68970, train_perplexity=4394.593, train_loss=8.38813

Batch 68980, train_perplexity=5033.4883, train_loss=8.523869

Batch 68990, train_perplexity=6438.857, train_loss=8.770106

Batch 69000, train_perplexity=5144.5366, train_loss=8.545691

Batch 69010, train_perplexity=5678.9287, train_loss=8.644518

Batch 69020, train_perplexity=4614.832, train_loss=8.437031

Batch 69030, train_perplexity=5370.4497, train_loss=8.588667

Batch 69040, train_perplexity=6835.941, train_loss=8.829949

Batch 69050, train_perplexity=5122.8477, train_loss=8.541466

Batch 69060, train_perplexity=5858.89, train_loss=8.675715

Batch 69070, train_perplexity=5086.7446, train_loss=8.534393

Batch 69080, train_perplexity=5680.9546, train_loss=8.644875
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69090, train_perplexity=5941.304, train_loss=8.689684

Batch 69100, train_perplexity=4896.7847, train_loss=8.496334

Batch 69110, train_perplexity=5076.645, train_loss=8.532406

Batch 69120, train_perplexity=4231.9414, train_loss=8.350416

Batch 69130, train_perplexity=4620.5747, train_loss=8.438274

Batch 69140, train_perplexity=4361.467, train_loss=8.380564

Batch 69150, train_perplexity=5606.433, train_loss=8.63167

Batch 69160, train_perplexity=5793.775, train_loss=8.664539

Batch 69170, train_perplexity=5315.115, train_loss=8.57831

Batch 69180, train_perplexity=5635.67, train_loss=8.636871

Batch 69190, train_perplexity=5061.673, train_loss=8.529452

Batch 69200, train_perplexity=6048.037, train_loss=8.707489

Batch 69210, train_perplexity=5601.1533, train_loss=8.630728

Batch 69220, train_perplexity=5251.469, train_loss=8.566263

Batch 69230, train_perplexity=6147.857, train_loss=8.723859

Batch 69240, train_perplexity=5796.063, train_loss=8.664934

Batch 69250, train_perplexity=4903.1167, train_loss=8.497626

Batch 69260, train_perplexity=4701.8726, train_loss=8.455716

Batch 69270, train_perplexity=5636.9814, train_loss=8.637104

Batch 69280, train_perplexity=5391.0986, train_loss=8.5925045

Batch 69290, train_perplexity=4920.0493, train_loss=8.501074

Batch 69300, train_perplexity=4885.7017, train_loss=8.494068

Batch 69310, train_perplexity=6313.8423, train_loss=8.7505

Batch 69320, train_perplexity=5632.339, train_loss=8.63628

Batch 69330, train_perplexity=6474.8984, train_loss=8.775688

Batch 69340, train_perplexity=5617.9834, train_loss=8.633728

Batch 69350, train_perplexity=4682.4834, train_loss=8.451584

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Batch 69360, train_perplexity=5320.177, train_loss=8.579262

Batch 69370, train_perplexity=5250.458, train_loss=8.566071

Batch 69380, train_perplexity=6582.0835, train_loss=8.792107

Batch 69390, train_perplexity=5183.491, train_loss=8.553234

Batch 69400, train_perplexity=5480.0293, train_loss=8.608866

Batch 69410, train_perplexity=5946.4966, train_loss=8.6905575

Batch 69420, train_perplexity=5640.24, train_loss=8.637682

Batch 69430, train_perplexity=5240.183, train_loss=8.564112

Batch 69440, train_perplexity=5489.036, train_loss=8.610508

Batch 69450, train_perplexity=5052.182, train_loss=8.5275755

Batch 69460, train_perplexity=6136.183, train_loss=8.721958

Batch 69470, train_perplexity=5799.5244, train_loss=8.665531

Batch 69480, train_perplexity=5349.323, train_loss=8.584725

Batch 69490, train_perplexity=5822.4614, train_loss=8.669478

Batch 69500, train_perplexity=5364.6245, train_loss=8.587582

Batch 69510, train_perplexity=5136.7217, train_loss=8.54417

Batch 69520, train_perplexity=5389.844, train_loss=8.592272

Batch 69530, train_perplexity=5574.157, train_loss=8.625896

Batch 69540, train_perplexity=5121.0796, train_loss=8.541121

Batch 69550, train_perplexity=6178.9746, train_loss=8.728908

Batch 69560, train_perplexity=6510.6196, train_loss=8.78119

Batch 69570, train_perplexity=4889.682, train_loss=8.494883

Batch 69580, train_perplexity=4563.278, train_loss=8.4257965

Batch 69590, train_perplexity=5602.0186, train_loss=8.630882

Batch 69600, train_perplexity=5497.3027, train_loss=8.612013

Batch 69610, train_perplexity=6644.699, train_loss=8.801575

Batch 69620, train_perplexity=4887.0435, train_loss=8.494343

Batch 69630, train_perplexity=5543.2305, train_loss=8.620333

Batch 69640, train_perplexity=6202.549, train_loss=8.732716

Batch 69650, train_perplexity=4513.234, train_loss=8.414769

Batch 69660, train_perplexity=5581.2534, train_loss=8.627169

Batch 69670, train_perplexity=5167.401, train_loss=8.550125

Batch 69680, train_perplexity=5753.421, train_loss=8.65755

Batch 69690, train_perplexity=4598.9365, train_loss=8.43358

Batch 69700, train_perplexity=4687.255, train_loss=8.452602

Batch 69710, train_perplexity=5702.874, train_loss=8.6487255

Batch 69720, train_perplexity=4193.27, train_loss=8.341236

Batch 69730, train_perplexity=4779.657, train_loss=8.472124

Batch 69740, train_perplexity=5310.966, train_loss=8.577529

Batch 69750, train_perplexity=4946.3013, train_loss=8.506395

Batch 69760, train_perplexity=5214.1714, train_loss=8.559135

Batch 69770, train_perplexity=5287.854, train_loss=8.573168

Batch 69780, train_perplexity=5841.0317, train_loss=8.672663

Batch 69790, train_perplexity=5176.378, train_loss=8.551861

Batch 69800, train_perplexity=5512.9062, train_loss=8.614847

Batch 69810, train_perplexity=7189.7324, train_loss=8.880409

Batch 69820, train_perplexity=5749.389, train_loss=8.656849

Batch 69830, train_perplexity=5655.6235, train_loss=8.640406

Batch 69840, train_perplexity=4882.888, train_loss=8.493492

Batch 69850, train_perplexity=5529.724, train_loss=8.617893

Batch 69860, train_perplexity=5820.9126, train_loss=8.669212

Batch 69870, train_perplexity=5110.356, train_loss=8.539024

Batch 69880, train_perplexity=4725.2793, train_loss=8.460682

Batch 69890, train_perplexity=4714.8584, train_loss=8.458474

Batch 69900, train_perplexity=6322.471, train_loss=8.751865

Batch 69910, train_perplexity=4409.6353, train_loss=8.391547

Batch 69920, train_perplexity=5120.7866, train_loss=8.541063

Batch 69930, train_perplexity=4606.179, train_loss=8.435154

Batch 69940, train_perplexity=6922.8623, train_loss=8.842585

Batch 69950, train_perplexity=5927.687, train_loss=8.687389

Batch 69960, train_perplexity=5349.747, train_loss=8.584805

Batch 69970, train_perplexity=5559.0435, train_loss=8.623181

Batch 69980, train_perplexity=5639.9443, train_loss=8.6376295

Batch 69990, train_perplexity=6352.049, train_loss=8.756533

Batch 70000, train_perplexity=5507.9507, train_loss=8.613948

Batch 70010, train_perplexity=4807.0757, train_loss=8.477844

Batch 70020, train_perplexity=4864.6587, train_loss=8.489752

Batch 70030, train_perplexity=5589.0513, train_loss=8.628565

Batch 70040, train_perplexity=4836.4688, train_loss=8.48394

Batch 70050, train_perplexity=6351.976, train_loss=8.756521

Batch 70060, train_perplexity=6577.9795, train_loss=8.791483

Batch 70070, train_perplexity=5200.9507, train_loss=8.556597

Batch 70080, train_perplexity=5742.989, train_loss=8.655735

Batch 70090, train_perplexity=4706.2646, train_loss=8.45665

Batch 70100, train_perplexity=6152.1206, train_loss=8.724552

Batch 70110, train_perplexity=5305.291, train_loss=8.57646

Batch 70120, train_perplexity=4832.914, train_loss=8.483205

Batch 70130, train_perplexity=4954.9224, train_loss=8.508137

Batch 70140, train_perplexity=4774.5137, train_loss=8.471047

Batch 70150, train_perplexity=6267.7197, train_loss=8.743168

Batch 70160, train_perplexity=4966.186, train_loss=8.510407

Batch 70170, train_perplexity=4723.1885, train_loss=8.460239

Batch 70180, train_perplexity=5743.306, train_loss=8.65579

Batch 70190, train_perplexity=5102.5933, train_loss=8.537504

Batch 70200, train_perplexity=4394.0146, train_loss=8.387999

Batch 70210, train_perplexity=4960.4443, train_loss=8.509251

Batch 70220, train_perplexity=5831.0024, train_loss=8.670944

Batch 70230, train_perplexity=4784.1445, train_loss=8.4730625

Batch 70240, train_perplexity=5200.4995, train_loss=8.55651

Batch 70250, train_perplexity=5545.5674, train_loss=8.620754

Batch 70260, train_perplexity=5256.099, train_loss=8.567144

Batch 70270, train_perplexity=5580.327, train_loss=8.627003

Batch 70280, train_perplexity=4541.0596, train_loss=8.420916

Batch 70290, train_perplexity=6603.6875, train_loss=8.795383

Batch 70300, train_perplexity=5886.0576, train_loss=8.680342

Batch 70310, train_perplexity=5601.3135, train_loss=8.630756

Batch 70320, train_perplexity=4749.5225, train_loss=8.465799

Batch 70330, train_perplexity=5080.7134, train_loss=8.533207

Batch 70340, train_perplexity=5433.4854, train_loss=8.600336

Batch 70350, train_perplexity=5673.223, train_loss=8.643513
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70360, train_perplexity=5086.143, train_loss=8.534275

Batch 70370, train_perplexity=5746.5386, train_loss=8.656353

Batch 70380, train_perplexity=5059.072, train_loss=8.528938

Batch 70390, train_perplexity=5378.122, train_loss=8.590095

Batch 70400, train_perplexity=3956.9993, train_loss=8.283241

Batch 70410, train_perplexity=5935.086, train_loss=8.688637

Batch 70420, train_perplexity=5643.3877, train_loss=8.63824

Batch 70430, train_perplexity=5591.2583, train_loss=8.62896

Batch 70440, train_perplexity=6360.6445, train_loss=8.757885

Batch 70450, train_perplexity=6137.8687, train_loss=8.722233

Batch 70460, train_perplexity=4930.284, train_loss=8.503152

Batch 70470, train_perplexity=6035.079, train_loss=8.705344

Batch 70480, train_perplexity=5832.5933, train_loss=8.671217

Batch 70490, train_perplexity=5230.1377, train_loss=8.562193

Batch 70500, train_perplexity=6573.8027, train_loss=8.790848

Batch 70510, train_perplexity=4580.8237, train_loss=8.429634

Batch 70520, train_perplexity=5501.079, train_loss=8.6126995

Batch 70530, train_perplexity=4795.2534, train_loss=8.475382

Batch 70540, train_perplexity=5504.5947, train_loss=8.613338

Batch 70550, train_perplexity=5601.292, train_loss=8.630753

Batch 70560, train_perplexity=5531.6284, train_loss=8.6182375

Batch 70570, train_perplexity=5677.023, train_loss=8.644182

Batch 70580, train_perplexity=6142.8345, train_loss=8.723042

Batch 70590, train_perplexity=5474.08, train_loss=8.6077795

Batch 70600, train_perplexity=6041.373, train_loss=8.706387

Batch 70610, train_perplexity=5625.6177, train_loss=8.635086

Batch 70620, train_perplexity=5414.1865, train_loss=8.596778

Batch 70630, train_perplexity=5966.3438, train_loss=8.69389

Batch 70640, train_perplexity=5806.907, train_loss=8.666803

Batch 70650, train_perplexity=5470.667, train_loss=8.607156

Batch 70660, train_perplexity=6330.344, train_loss=8.75311

Batch 70670, train_perplexity=6459.5347, train_loss=8.773313

Batch 70680, train_perplexity=4981.1797, train_loss=8.513422

Batch 70690, train_perplexity=4372.845, train_loss=8.383169

Batch 70700, train_perplexity=5597.933, train_loss=8.630153

Batch 70710, train_perplexity=5786.707, train_loss=8.663319

Batch 70720, train_perplexity=5377.7993, train_loss=8.5900345

Batch 70730, train_perplexity=4627.6396, train_loss=8.439802

Batch 70740, train_perplexity=5121.47, train_loss=8.541197

Batch 70750, train_perplexity=4967.555, train_loss=8.510683

Batch 70760, train_perplexity=5462.513, train_loss=8.605664

Batch 70770, train_perplexity=5233.7803, train_loss=8.562889

Batch 70780, train_perplexity=4410.0093, train_loss=8.391632

Batch 70790, train_perplexity=5043.6704, train_loss=8.525889

Batch 70800, train_perplexity=5296.6914, train_loss=8.574838

Batch 70810, train_perplexity=5221.775, train_loss=8.560593

Batch 70820, train_perplexity=5671.8926, train_loss=8.643278

Batch 70830, train_perplexity=5520.0977, train_loss=8.616151

Batch 70840, train_perplexity=5389.2275, train_loss=8.592157

Batch 70850, train_perplexity=5020.353, train_loss=8.5212555

Batch 70860, train_perplexity=5637.5894, train_loss=8.637212

Batch 70870, train_perplexity=4808.7725, train_loss=8.478197

Batch 70880, train_perplexity=5448.0967, train_loss=8.603022

Batch 70890, train_perplexity=5662.143, train_loss=8.641558

Batch 70900, train_perplexity=5839.673, train_loss=8.67243

Batch 70910, train_perplexity=5640.079, train_loss=8.637653

Batch 70920, train_perplexity=4833.4575, train_loss=8.483317

Batch 70930, train_perplexity=5719.846, train_loss=8.651697

Batch 70940, train_perplexity=4845.083, train_loss=8.48572

Batch 70950, train_perplexity=5080.8877, train_loss=8.533241

Batch 70960, train_perplexity=4823.792, train_loss=8.481316

Batch 70970, train_perplexity=5128.283, train_loss=8.542526

Batch 70980, train_perplexity=4587.101, train_loss=8.431004

Batch 70990, train_perplexity=5874.27, train_loss=8.678337

Batch 71000, train_perplexity=5887.091, train_loss=8.680517

Batch 71010, train_perplexity=4968.8438, train_loss=8.510942

Batch 71020, train_perplexity=5815.6245, train_loss=8.6683035

Batch 71030, train_perplexity=4981.5264, train_loss=8.513492

Batch 71040, train_perplexity=5087.0503, train_loss=8.534453

Batch 71050, train_perplexity=4766.8203, train_loss=8.469435

Batch 71060, train_perplexity=5273.255, train_loss=8.570403

Batch 71070, train_perplexity=5719.4424, train_loss=8.651627

Batch 71080, train_perplexity=5245.583, train_loss=8.565142

Batch 71090, train_perplexity=5079.9043, train_loss=8.533048

Batch 71100, train_perplexity=5587.0264, train_loss=8.628202

Batch 71110, train_perplexity=5241.4375, train_loss=8.564351

Batch 71120, train_perplexity=6040.9004, train_loss=8.706308

Batch 71130, train_perplexity=6586.5044, train_loss=8.792778

Batch 71140, train_perplexity=5185.266, train_loss=8.553576

Batch 71150, train_perplexity=5291.9907, train_loss=8.57395

Batch 71160, train_perplexity=5855.2983, train_loss=8.675102

Batch 71170, train_perplexity=5699.0137, train_loss=8.648048

Batch 71180, train_perplexity=5340.953, train_loss=8.583159

Batch 71190, train_perplexity=4876.508, train_loss=8.492185

Batch 71200, train_perplexity=5221.586, train_loss=8.560556

Batch 71210, train_perplexity=4821.465, train_loss=8.480833

Batch 71220, train_perplexity=4437.11, train_loss=8.3977585

Batch 71230, train_perplexity=6640.018, train_loss=8.80087

Batch 71240, train_perplexity=5874.0234, train_loss=8.678295

Batch 71250, train_perplexity=6724.2305, train_loss=8.813473

Batch 71260, train_perplexity=5393.3564, train_loss=8.592923

Batch 71270, train_perplexity=4874.8477, train_loss=8.491844

Batch 71280, train_perplexity=6367.9883, train_loss=8.759039

Batch 71290, train_perplexity=5538.411, train_loss=8.619463

Batch 71300, train_perplexity=6422.5376, train_loss=8.767569

Batch 71310, train_perplexity=5243.5273, train_loss=8.56475

Batch 71320, train_perplexity=5395.975, train_loss=8.593409

Batch 71330, train_perplexity=5361.852, train_loss=8.587065

Batch 71340, train_perplexity=5545.2397, train_loss=8.620695

Batch 71350, train_perplexity=5763.7783, train_loss=8.6593485

Batch 71360, train_perplexity=4611.5415, train_loss=8.436317

Batch 71370, train_perplexity=5205.819, train_loss=8.557532

Batch 71380, train_perplexity=4751.1807, train_loss=8.466148

Batch 71390, train_perplexity=5167.0854, train_loss=8.550064

Batch 71400, train_perplexity=6040.785, train_loss=8.706289

Batch 71410, train_perplexity=4973.196, train_loss=8.511818

Batch 71420, train_perplexity=5628.274, train_loss=8.635558

Batch 71430, train_perplexity=4590.1646, train_loss=8.431671

Batch 71440, train_perplexity=5505.2827, train_loss=8.613463

Batch 71450, train_perplexity=5403.9727, train_loss=8.59489

Batch 71460, train_perplexity=5452.588, train_loss=8.603846

Batch 71470, train_perplexity=6315.408, train_loss=8.750748

Batch 71480, train_perplexity=5600.9233, train_loss=8.630687

Batch 71490, train_perplexity=6339.8, train_loss=8.754602

Batch 71500, train_perplexity=5387.66, train_loss=8.5918665

Batch 71510, train_perplexity=5903.5977, train_loss=8.683317

Batch 71520, train_perplexity=5022.393, train_loss=8.521662

Batch 71530, train_perplexity=4757.256, train_loss=8.467426

Batch 71540, train_perplexity=5972.2188, train_loss=8.694874

Batch 71550, train_perplexity=5612.853, train_loss=8.632814

Batch 71560, train_perplexity=5458.613, train_loss=8.60495

Batch 71570, train_perplexity=5849.6167, train_loss=8.674131

Batch 71580, train_perplexity=5163.701, train_loss=8.549409

Batch 71590, train_perplexity=5188.0015, train_loss=8.554104

Batch 71600, train_perplexity=5599.343, train_loss=8.630404

Batch 71610, train_perplexity=5124.5728, train_loss=8.541802

Batch 71620, train_perplexity=5739.0356, train_loss=8.655046

Batch 71630, train_perplexity=4387.738, train_loss=8.386569

Batch 71640, train_perplexity=7532.548, train_loss=8.926989

Batch 71650, train_perplexity=5670.562, train_loss=8.6430435

Batch 71660, train_perplexity=6714.5923, train_loss=8.812038

Batch 71670, train_perplexity=5734.511, train_loss=8.654258

Batch 71680, train_perplexity=5019.5293, train_loss=8.521091

Batch 71690, train_perplexity=5114.276, train_loss=8.539791
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 71700, train_perplexity=5584.5757, train_loss=8.627764

Batch 71710, train_perplexity=5428.3525, train_loss=8.599391

Batch 71720, train_perplexity=5394.7144, train_loss=8.593175

Batch 71730, train_perplexity=6036.6963, train_loss=8.705612

Batch 71740, train_perplexity=5361.1465, train_loss=8.586933

Batch 71750, train_perplexity=5447.1616, train_loss=8.60285

Batch 71760, train_perplexity=5387.4033, train_loss=8.591819

Batch 71770, train_perplexity=4731.2495, train_loss=8.461945

Batch 71780, train_perplexity=5236.3013, train_loss=8.563371

Batch 71790, train_perplexity=5777.1675, train_loss=8.661669

Batch 71800, train_perplexity=5176.6836, train_loss=8.55192

Batch 71810, train_perplexity=4861.737, train_loss=8.489151

Batch 71820, train_perplexity=6042.0127, train_loss=8.706492

Batch 71830, train_perplexity=5572.0845, train_loss=8.6255245

Batch 71840, train_perplexity=5482.5176, train_loss=8.60932

Batch 71850, train_perplexity=5235.957, train_loss=8.563305

Batch 71860, train_perplexity=5567.0493, train_loss=8.62462

Batch 71870, train_perplexity=5011.097, train_loss=8.51941

Batch 71880, train_perplexity=4749.5044, train_loss=8.4657955

Batch 71890, train_perplexity=6326.2705, train_loss=8.752466

Batch 71900, train_perplexity=5436.3877, train_loss=8.60087

Batch 71910, train_perplexity=5381.8315, train_loss=8.590784

Batch 71920, train_perplexity=5409.701, train_loss=8.595949

Batch 71930, train_perplexity=5799.751, train_loss=8.66557

Batch 71940, train_perplexity=5279.9023, train_loss=8.571663

Batch 71950, train_perplexity=5767.4126, train_loss=8.659979

Batch 71960, train_perplexity=5126.748, train_loss=8.542227

Batch 71970, train_perplexity=4995.47, train_loss=8.516287

Batch 71980, train_perplexity=4959.3755, train_loss=8.509035

Batch 71990, train_perplexity=5407.055, train_loss=8.59546

Batch 72000, train_perplexity=5016.706, train_loss=8.520529

Batch 72010, train_perplexity=6545.04, train_loss=8.786463

Batch 72020, train_perplexity=6602.138, train_loss=8.795149

Batch 72030, train_perplexity=5588.998, train_loss=8.628555

Batch 72040, train_perplexity=4786.5083, train_loss=8.4735565

Batch 72050, train_perplexity=5408.0713, train_loss=8.595648

Batch 72060, train_perplexity=5445.7383, train_loss=8.602589

Batch 72070, train_perplexity=5841.5444, train_loss=8.67275

Batch 72080, train_perplexity=5132.6772, train_loss=8.543383

Batch 72090, train_perplexity=4551.5083, train_loss=8.423214

Batch 72100, train_perplexity=4444.5425, train_loss=8.399432

Batch 72110, train_perplexity=4814.3154, train_loss=8.479349

Batch 72120, train_perplexity=5966.2188, train_loss=8.693869

Batch 72130, train_perplexity=4574.895, train_loss=8.428339

Batch 72140, train_perplexity=7538.1816, train_loss=8.927736

Batch 72150, train_perplexity=6071.8354, train_loss=8.711416

Batch 72160, train_perplexity=4764.107, train_loss=8.468865

Batch 72170, train_perplexity=5080.5483, train_loss=8.5331745

Batch 72180, train_perplexity=6075.2705, train_loss=8.711982

Batch 72190, train_perplexity=5152.058, train_loss=8.547152

Batch 72200, train_perplexity=5080.282, train_loss=8.533122

Batch 72210, train_perplexity=5431.537, train_loss=8.5999775

Batch 72220, train_perplexity=4869.737, train_loss=8.490795

Batch 72230, train_perplexity=6579.2593, train_loss=8.791677

Batch 72240, train_perplexity=4843.9097, train_loss=8.485477

Batch 72250, train_perplexity=6169.877, train_loss=8.727434

Batch 72260, train_perplexity=5331.5386, train_loss=8.581395

Batch 72270, train_perplexity=5939.8423, train_loss=8.689438

Batch 72280, train_perplexity=5475.0146, train_loss=8.60795

Batch 72290, train_perplexity=4796.5615, train_loss=8.475655

Batch 72300, train_perplexity=4889.5845, train_loss=8.494863

Batch 72310, train_perplexity=5199.3887, train_loss=8.556296

Batch 72320, train_perplexity=6267.965, train_loss=8.743207

Batch 72330, train_perplexity=5668.5884, train_loss=8.642695

Batch 72340, train_perplexity=5622.346, train_loss=8.634504

Batch 72350, train_perplexity=5906.25, train_loss=8.683766

Batch 72360, train_perplexity=5250.788, train_loss=8.5661335

Batch 72370, train_perplexity=5346.768, train_loss=8.584248

Batch 72380, train_perplexity=5082.7485, train_loss=8.5336075

Batch 72390, train_perplexity=4233.4272, train_loss=8.350767

Batch 72400, train_perplexity=4965.949, train_loss=8.51036

Batch 72410, train_perplexity=7345.5693, train_loss=8.901853

Batch 72420, train_perplexity=5129.183, train_loss=8.542702

Batch 72430, train_perplexity=5350.8286, train_loss=8.585007

Batch 72440, train_perplexity=4540.2886, train_loss=8.420746

Batch 72450, train_perplexity=6055.5977, train_loss=8.708738

Batch 72460, train_perplexity=5022.915, train_loss=8.521766

Batch 72470, train_perplexity=5442.9346, train_loss=8.602074

Batch 72480, train_perplexity=4961.2393, train_loss=8.509411

Batch 72490, train_perplexity=4866.496, train_loss=8.490129

Batch 72500, train_perplexity=5098.488, train_loss=8.536699

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Batch 72510, train_perplexity=5166.7207, train_loss=8.5499935

Batch 72520, train_perplexity=6127.785, train_loss=8.720589

Batch 72530, train_perplexity=5873.6313, train_loss=8.678228

Batch 72540, train_perplexity=5415.9785, train_loss=8.597109

Batch 72550, train_perplexity=6181.261, train_loss=8.729278

Batch 72560, train_perplexity=5234.953, train_loss=8.563113

Batch 72570, train_perplexity=5565.669, train_loss=8.6243725

Batch 72580, train_perplexity=4823.033, train_loss=8.481158

Batch 72590, train_perplexity=6045.736, train_loss=8.7071085

Batch 72600, train_perplexity=5679.048, train_loss=8.644539

Batch 72610, train_perplexity=5121.2847, train_loss=8.541161

Batch 72620, train_perplexity=5347.4106, train_loss=8.584368

Batch 72630, train_perplexity=5176.5454, train_loss=8.551893

Batch 72640, train_perplexity=4614.542, train_loss=8.436968

Batch 72650, train_perplexity=5647.512, train_loss=8.63897

Batch 72660, train_perplexity=5231.824, train_loss=8.562515

Batch 72670, train_perplexity=5346.5947, train_loss=8.584215

Batch 72680, train_perplexity=5079.8604, train_loss=8.533039

Batch 72690, train_perplexity=5349.2114, train_loss=8.584704

Batch 72700, train_perplexity=5693.234, train_loss=8.647034

Batch 72710, train_perplexity=5285.938, train_loss=8.572805

Batch 72720, train_perplexity=5779.989, train_loss=8.662157

Batch 72730, train_perplexity=5638.858, train_loss=8.637437

Batch 72740, train_perplexity=5250.3027, train_loss=8.566041

Batch 72750, train_perplexity=4929.367, train_loss=8.502966

Batch 72760, train_perplexity=4854.838, train_loss=8.487731

Batch 72770, train_perplexity=5240.073, train_loss=8.564091

Batch 72780, train_perplexity=4630.871, train_loss=8.4405

Batch 72790, train_perplexity=5339.461, train_loss=8.58288

Batch 72800, train_perplexity=5070.2676, train_loss=8.531149

Batch 72810, train_perplexity=5948.7026, train_loss=8.690928

Batch 72820, train_perplexity=5932.2227, train_loss=8.688154

Batch 72830, train_perplexity=6102.452, train_loss=8.716446

Batch 72840, train_perplexity=5263.799, train_loss=8.568608

Batch 72850, train_perplexity=6036.581, train_loss=8.705593

Batch 72860, train_perplexity=6008.5356, train_loss=8.700936

Batch 72870, train_perplexity=5757.323, train_loss=8.658228

Batch 72880, train_perplexity=6025.2505, train_loss=8.703714

Batch 72890, train_perplexity=4940.672, train_loss=8.505257

Batch 72900, train_perplexity=5797.7324, train_loss=8.665222

Batch 72910, train_perplexity=5599.4336, train_loss=8.630421

Batch 72920, train_perplexity=5608.9683, train_loss=8.632122

Batch 72930, train_perplexity=7351.281, train_loss=8.90263

Batch 72940, train_perplexity=4629.1313, train_loss=8.4401245

Batch 72950, train_perplexity=5713.6636, train_loss=8.650616

Batch 72960, train_perplexity=5093.5747, train_loss=8.535735
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72970, train_perplexity=5041.92, train_loss=8.525542

Batch 72980, train_perplexity=5682.2334, train_loss=8.6451

Batch 72990, train_perplexity=5291.133, train_loss=8.573788

Batch 73000, train_perplexity=6246.4644, train_loss=8.739771

Batch 73010, train_perplexity=5801.383, train_loss=8.665852

Batch 73020, train_perplexity=4790.5547, train_loss=8.474401

Batch 73030, train_perplexity=4974.837, train_loss=8.512148

Batch 73040, train_perplexity=4102.059, train_loss=8.319244

Batch 73050, train_perplexity=6410.5684, train_loss=8.765703

Batch 73060, train_perplexity=5098.7114, train_loss=8.536743

Batch 73070, train_perplexity=5512.2383, train_loss=8.614726

Batch 73080, train_perplexity=5613.126, train_loss=8.632863

Batch 73090, train_perplexity=5829.696, train_loss=8.67072

Batch 73100, train_perplexity=5362.8853, train_loss=8.587257

Batch 73110, train_perplexity=5545.9644, train_loss=8.620826

Batch 73120, train_perplexity=5344.413, train_loss=8.583807

Batch 73130, train_perplexity=5617.9136, train_loss=8.633716

Batch 73140, train_perplexity=4735.348, train_loss=8.4628105

Batch 73150, train_perplexity=5458.186, train_loss=8.604872

Batch 73160, train_perplexity=5857.443, train_loss=8.675468

Batch 73170, train_perplexity=5231.9736, train_loss=8.562544

Batch 73180, train_perplexity=4721.8193, train_loss=8.4599495

Batch 73190, train_perplexity=6070.203, train_loss=8.711147

Batch 73200, train_perplexity=4002.6658, train_loss=8.294716

Batch 73210, train_perplexity=5674.3057, train_loss=8.643703

Batch 73220, train_perplexity=6275.7466, train_loss=8.744448

Batch 73230, train_perplexity=5553.713, train_loss=8.622222

Batch 73240, train_perplexity=6236.9404, train_loss=8.738245

Batch 73250, train_perplexity=5351.4717, train_loss=8.585127

Batch 73260, train_perplexity=5040.247, train_loss=8.52521

Batch 73270, train_perplexity=5080.6064, train_loss=8.533186

Batch 73280, train_perplexity=6151.065, train_loss=8.7243805

Batch 73290, train_perplexity=5466.974, train_loss=8.606481

Batch 73300, train_perplexity=6154.756, train_loss=8.72498

Batch 73310, train_perplexity=5138.28, train_loss=8.544474

Batch 73320, train_perplexity=6128.849, train_loss=8.720762

Batch 73330, train_perplexity=5937.9507, train_loss=8.689119

Batch 73340, train_perplexity=5295.6157, train_loss=8.574635

Batch 73350, train_perplexity=6051.1177, train_loss=8.707998

Batch 73360, train_perplexity=6492.397, train_loss=8.778387

Batch 73370, train_perplexity=6283.0283, train_loss=8.745607

Batch 73380, train_perplexity=5559.102, train_loss=8.623192

Batch 73390, train_perplexity=5225.476, train_loss=8.561301

Batch 73400, train_perplexity=5059.887, train_loss=8.529099

Batch 73410, train_perplexity=5894.4556, train_loss=8.681767

Batch 73420, train_perplexity=4812.1533, train_loss=8.4789

Batch 73430, train_perplexity=4656.3037, train_loss=8.445977

Batch 73440, train_perplexity=4651.027, train_loss=8.444843

Batch 73450, train_perplexity=5063.8213, train_loss=8.529877

Batch 73460, train_perplexity=5020.3145, train_loss=8.521248

Batch 73470, train_perplexity=5905.0615, train_loss=8.683565

Batch 73480, train_perplexity=4902.71, train_loss=8.497543

Batch 73490, train_perplexity=4625.9404, train_loss=8.439435

Batch 73500, train_perplexity=6812.3945, train_loss=8.826499

Batch 73510, train_perplexity=5645.805, train_loss=8.638668

Batch 73520, train_perplexity=5012.698, train_loss=8.51973

Batch 73530, train_perplexity=6241.802, train_loss=8.739024

Batch 73540, train_perplexity=6258.8438, train_loss=8.741751

Batch 73550, train_perplexity=6577.0264, train_loss=8.791338

Batch 73560, train_perplexity=4325.8135, train_loss=8.372355

Batch 73570, train_perplexity=6743.419, train_loss=8.816322

Batch 73580, train_perplexity=6039.069, train_loss=8.706005

Batch 73590, train_perplexity=5936.5293, train_loss=8.68888

Batch 73600, train_perplexity=5542.099, train_loss=8.620129

Batch 73610, train_perplexity=4894.8237, train_loss=8.495934

Batch 73620, train_perplexity=6239.8677, train_loss=8.738714

Batch 73630, train_perplexity=5359.7354, train_loss=8.58667

Batch 73640, train_perplexity=4589.219, train_loss=8.431465

Batch 73650, train_perplexity=4685.731, train_loss=8.452277

Batch 73660, train_perplexity=5089.122, train_loss=8.534861

Batch 73670, train_perplexity=5591.722, train_loss=8.629043

Batch 73680, train_perplexity=4975.8716, train_loss=8.512356

Batch 73690, train_perplexity=4447.235, train_loss=8.400038

Batch 73700, train_perplexity=3616.352, train_loss=8.193221

Batch 73710, train_perplexity=6227.888, train_loss=8.736793

Batch 73720, train_perplexity=5926.1494, train_loss=8.68713

Batch 73730, train_perplexity=7755.7524, train_loss=8.95619

Batch 73740, train_perplexity=5244.5825, train_loss=8.564951

Batch 73750, train_perplexity=5773.356, train_loss=8.661009

Batch 73760, train_perplexity=5447.1147, train_loss=8.602841

Batch 73770, train_perplexity=5801.776, train_loss=8.665919

Batch 73780, train_perplexity=5146.691, train_loss=8.546109

Batch 73790, train_perplexity=5222.268, train_loss=8.560687

Batch 73800, train_perplexity=5464.4463, train_loss=8.606018

Batch 73810, train_perplexity=4900.756, train_loss=8.497145

Batch 73820, train_perplexity=5151.336, train_loss=8.547011

Batch 73830, train_perplexity=5023.9785, train_loss=8.521977

Batch 73840, train_perplexity=5484.2744, train_loss=8.60964

Batch 73850, train_perplexity=5196.474, train_loss=8.555736

Batch 73860, train_perplexity=4973.9404, train_loss=8.511968

Batch 73870, train_perplexity=5544.605, train_loss=8.620581

Batch 73880, train_perplexity=5384.2344, train_loss=8.59123

Batch 73890, train_perplexity=5431.061, train_loss=8.59989

Batch 73900, train_perplexity=5696.525, train_loss=8.647612

Batch 73910, train_perplexity=4389.6587, train_loss=8.387007

Batch 73920, train_perplexity=5519.277, train_loss=8.616002

Batch 73930, train_perplexity=5072.1733, train_loss=8.531525

Batch 73940, train_perplexity=5127.6475, train_loss=8.542402

Batch 73950, train_perplexity=6050.258, train_loss=8.707856

Batch 73960, train_perplexity=6200.834, train_loss=8.732439

Batch 73970, train_perplexity=5871.593, train_loss=8.677881

Batch 73980, train_perplexity=6820.41, train_loss=8.827675

Batch 73990, train_perplexity=6133.5503, train_loss=8.721529

Batch 74000, train_perplexity=4710.3057, train_loss=8.457508

Batch 74010, train_perplexity=4913.705, train_loss=8.4997835

Batch 74020, train_perplexity=5711.9526, train_loss=8.650316

Batch 74030, train_perplexity=5328.9004, train_loss=8.5809

Batch 74040, train_perplexity=5661.7324, train_loss=8.641485

Batch 74050, train_perplexity=5237.2954, train_loss=8.5635605

Batch 74060, train_perplexity=5173.2886, train_loss=8.551264

Batch 74070, train_perplexity=5419.9883, train_loss=8.597849

Batch 74080, train_perplexity=5105.397, train_loss=8.5380535

Batch 74090, train_perplexity=4695.335, train_loss=8.454325

Batch 74100, train_perplexity=5348.63, train_loss=8.584596

Batch 74110, train_perplexity=5778.7983, train_loss=8.661951

Batch 74120, train_perplexity=5183.3726, train_loss=8.553211

Batch 74130, train_perplexity=5795.7314, train_loss=8.664877

Batch 74140, train_perplexity=5227.7344, train_loss=8.561733

Batch 74150, train_perplexity=5797.7935, train_loss=8.665233

Batch 74160, train_perplexity=5380.3335, train_loss=8.590506

Batch 74170, train_perplexity=4535.7964, train_loss=8.419756

Batch 74180, train_perplexity=3964.622, train_loss=8.285166

Batch 74190, train_perplexity=6168.4473, train_loss=8.727202

Batch 74200, train_perplexity=4926.214, train_loss=8.502326

Batch 74210, train_perplexity=6233.5747, train_loss=8.737705

Batch 74220, train_perplexity=4650.7476, train_loss=8.444783

Batch 74230, train_perplexity=4143.5396, train_loss=8.329306

Batch 74240, train_perplexity=5965.4224, train_loss=8.693735

Batch 74250, train_perplexity=4608.5034, train_loss=8.435658

Batch 74260, train_perplexity=8530.783, train_loss=9.051436

Batch 74270, train_perplexity=5880.974, train_loss=8.679478

Batch 74280, train_perplexity=4471.3047, train_loss=8.405436

Batch 74290, train_perplexity=5077.9814, train_loss=8.532669

Batch 74300, train_perplexity=4837.4697, train_loss=8.484147
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74310, train_perplexity=5864.2676, train_loss=8.676633

Batch 74320, train_perplexity=5613.7896, train_loss=8.632981

Batch 74330, train_perplexity=4629.012, train_loss=8.440099

Batch 74340, train_perplexity=5038.435, train_loss=8.524851

Batch 74350, train_perplexity=5026.825, train_loss=8.522544

Batch 74360, train_perplexity=4736.8203, train_loss=8.463121

Batch 74370, train_perplexity=4818.6973, train_loss=8.480259

Batch 74380, train_perplexity=6004.898, train_loss=8.700331

Batch 74390, train_perplexity=5492.6963, train_loss=8.611175

Batch 74400, train_perplexity=5855.734, train_loss=8.675177

Batch 74410, train_perplexity=5562.496, train_loss=8.623802

Batch 74420, train_perplexity=4920.115, train_loss=8.501087

Batch 74430, train_perplexity=6537.392, train_loss=8.785294

Batch 74440, train_perplexity=5580.785, train_loss=8.627085

Batch 74450, train_perplexity=5033.7812, train_loss=8.523927

Batch 74460, train_perplexity=6445.185, train_loss=8.771089

Batch 74470, train_perplexity=6054.0386, train_loss=8.708481

Batch 74480, train_perplexity=5816.795, train_loss=8.668505

Batch 74490, train_perplexity=4235.822, train_loss=8.351333

Batch 74500, train_perplexity=5003.7573, train_loss=8.517944

Batch 74510, train_perplexity=6020.5405, train_loss=8.702932

Batch 74520, train_perplexity=5067.319, train_loss=8.530567

Batch 74530, train_perplexity=4913.85, train_loss=8.499813

Batch 74540, train_perplexity=4662.849, train_loss=8.447382

Batch 74550, train_perplexity=5254.0444, train_loss=8.566753

Batch 74560, train_perplexity=5152.9624, train_loss=8.547327

Batch 74570, train_perplexity=6803.922, train_loss=8.825254

Batch 74580, train_perplexity=6257.0054, train_loss=8.741457

Batch 74590, train_perplexity=5272.4004, train_loss=8.570241

Batch 74600, train_perplexity=5274.6836, train_loss=8.570674

Batch 74610, train_perplexity=4533.9365, train_loss=8.419346

Batch 74620, train_perplexity=4815.629, train_loss=8.479622

Batch 74630, train_perplexity=5425.594, train_loss=8.598883

Batch 74640, train_perplexity=5382.6733, train_loss=8.59094

Batch 74650, train_perplexity=5913.7007, train_loss=8.685027

Batch 74660, train_perplexity=5790.273, train_loss=8.663935

Batch 74670, train_perplexity=5183.269, train_loss=8.553191

Batch 74680, train_perplexity=5357.109, train_loss=8.58618

Batch 74690, train_perplexity=5082.6714, train_loss=8.533592

Batch 74700, train_perplexity=5112.8716, train_loss=8.539516

Batch 74710, train_perplexity=6440.687, train_loss=8.7703905

Batch 74720, train_perplexity=5016.022, train_loss=8.520392

Batch 74730, train_perplexity=6429.751, train_loss=8.768691

Batch 74740, train_perplexity=5239.6133, train_loss=8.564003

Batch 74750, train_perplexity=4522.0225, train_loss=8.416715

Batch 74760, train_perplexity=6035.188, train_loss=8.705362

Batch 74770, train_perplexity=4925.3545, train_loss=8.5021515

Batch 74780, train_perplexity=5354.1416, train_loss=8.585626

Batch 74790, train_perplexity=5987.8335, train_loss=8.697485

Batch 74800, train_perplexity=5461.2944, train_loss=8.605441

Batch 74810, train_perplexity=5889.4097, train_loss=8.680911

Batch 74820, train_perplexity=5406.823, train_loss=8.595417

Batch 74830, train_perplexity=4098.274, train_loss=8.318321

Batch 74840, train_perplexity=4979.1514, train_loss=8.513015

Batch 74850, train_perplexity=4058.226, train_loss=8.308501

Batch 74860, train_perplexity=5208.58, train_loss=8.558063

Batch 74870, train_perplexity=5291.3955, train_loss=8.573837

Batch 74880, train_perplexity=4697.2964, train_loss=8.454742

Batch 74890, train_perplexity=5954.4297, train_loss=8.691891

Batch 74900, train_perplexity=6879.7793, train_loss=8.836342

Batch 74910, train_perplexity=5571.091, train_loss=8.625346

Batch 74920, train_perplexity=5078.863, train_loss=8.532843

Batch 74930, train_perplexity=5407.7153, train_loss=8.595582

Batch 74940, train_perplexity=5324.811, train_loss=8.5801325

Batch 74950, train_perplexity=5509.5425, train_loss=8.614237

Batch 74960, train_perplexity=5983.1987, train_loss=8.696711

Batch 74970, train_perplexity=4543.2817, train_loss=8.421405

Batch 74980, train_perplexity=6296.6987, train_loss=8.747781

Batch 74990, train_perplexity=5667.621, train_loss=8.642525

Batch 75000, train_perplexity=5693.4077, train_loss=8.647064

Batch 75010, train_perplexity=5079.168, train_loss=8.532903

Batch 75020, train_perplexity=6628.446, train_loss=8.799126

Batch 75030, train_perplexity=6088.9478, train_loss=8.714231

Batch 75040, train_perplexity=5759.668, train_loss=8.658635

Batch 75050, train_perplexity=6141.0303, train_loss=8.722748

Batch 75060, train_perplexity=5473.2188, train_loss=8.607622

Batch 75070, train_perplexity=6121.524, train_loss=8.719566

Batch 75080, train_perplexity=5299.7485, train_loss=8.575415

Batch 75090, train_perplexity=6253.313, train_loss=8.740867

Batch 75100, train_perplexity=6468.862, train_loss=8.7747555

Batch 75110, train_perplexity=5561.064, train_loss=8.623545

Batch 75120, train_perplexity=6023.372, train_loss=8.7034025

Batch 75130, train_perplexity=5530.784, train_loss=8.618085

Batch 75140, train_perplexity=4645.521, train_loss=8.443659

Batch 75150, train_perplexity=5012.4736, train_loss=8.519685

Batch 75160, train_perplexity=4866.9883, train_loss=8.490231

Batch 75170, train_perplexity=4378.8047, train_loss=8.384531

Batch 75180, train_perplexity=6269.824, train_loss=8.743504

Batch 75190, train_perplexity=5790.6206, train_loss=8.663995

Batch 75200, train_perplexity=5619.4194, train_loss=8.633984

Batch 75210, train_perplexity=5497.8794, train_loss=8.612118

Batch 75220, train_perplexity=6366.3003, train_loss=8.758774

Batch 75230, train_perplexity=5538.105, train_loss=8.619408

Batch 75240, train_perplexity=5373.6978, train_loss=8.589272

Batch 75250, train_perplexity=4483.5254, train_loss=8.408165

Batch 75260, train_perplexity=5564.884, train_loss=8.624231

Batch 75270, train_perplexity=4954.969, train_loss=8.508146

Batch 75280, train_perplexity=5565.9873, train_loss=8.62443

Batch 75290, train_perplexity=5538.0522, train_loss=8.619398

Batch 75300, train_perplexity=5148.9736, train_loss=8.546553

Batch 75310, train_perplexity=4619.4995, train_loss=8.438042

Batch 75320, train_perplexity=6486.728, train_loss=8.7775135

Batch 75330, train_perplexity=5175.6177, train_loss=8.551714

Batch 75340, train_perplexity=4586.222, train_loss=8.430812

Batch 75350, train_perplexity=6200.9756, train_loss=8.732462

Batch 75360, train_perplexity=6261.47, train_loss=8.74217

Batch 75370, train_perplexity=5282.587, train_loss=8.572171

Batch 75380, train_perplexity=4888.237, train_loss=8.494587

Batch 75390, train_perplexity=4769.3667, train_loss=8.469969

Batch 75400, train_perplexity=4413.636, train_loss=8.392454

Batch 75410, train_perplexity=4921.795, train_loss=8.501429

Batch 75420, train_perplexity=5135.5415, train_loss=8.543941

Batch 75430, train_perplexity=6107.7095, train_loss=8.717307

Batch 75440, train_perplexity=5972.5264, train_loss=8.694925

Batch 75450, train_perplexity=5414.362, train_loss=8.59681

Batch 75460, train_perplexity=4179.9385, train_loss=8.338052

Batch 75470, train_perplexity=6839.5664, train_loss=8.83048

Batch 75480, train_perplexity=4931.5537, train_loss=8.503409

Batch 75490, train_perplexity=6386.124, train_loss=8.761883

Batch 75500, train_perplexity=4797.312, train_loss=8.475811

Batch 75510, train_perplexity=5264.045, train_loss=8.568655

Batch 75520, train_perplexity=6407.683, train_loss=8.765253

Batch 75530, train_perplexity=5358.039, train_loss=8.586353

Batch 75540, train_perplexity=5003.6235, train_loss=8.517918

Batch 75550, train_perplexity=4772.8477, train_loss=8.470698

Batch 75560, train_perplexity=6339.6304, train_loss=8.754576

Batch 75570, train_perplexity=5143.467, train_loss=8.545483

Batch 75580, train_perplexity=5862.305, train_loss=8.676298

Batch 75590, train_perplexity=6009.286, train_loss=8.701061

Batch 75600, train_perplexity=5129.8096, train_loss=8.542824

Batch 75610, train_perplexity=4798.5015, train_loss=8.476059

Batch 75620, train_perplexity=5468.3354, train_loss=8.6067295

Batch 75630, train_perplexity=5557.957, train_loss=8.622986

Batch 75640, train_perplexity=5199.83, train_loss=8.556381
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Batch 75650, train_perplexity=6175.428, train_loss=8.728333

Batch 75660, train_perplexity=5464.7695, train_loss=8.606077

Batch 75670, train_perplexity=5298.303, train_loss=8.575142

Batch 75680, train_perplexity=5284.0835, train_loss=8.572454

Batch 75690, train_perplexity=5200.5044, train_loss=8.556511

Batch 75700, train_perplexity=6291.5186, train_loss=8.746958

Batch 75710, train_perplexity=4774.227, train_loss=8.470987

Batch 75720, train_perplexity=6388.4385, train_loss=8.762245

Batch 75730, train_perplexity=5424.3887, train_loss=8.59866

Batch 75740, train_perplexity=5365.0645, train_loss=8.587664

Batch 75750, train_perplexity=4613.1997, train_loss=8.436677

Batch 75760, train_perplexity=5725.6255, train_loss=8.652707

Batch 75770, train_perplexity=5575.1304, train_loss=8.626071

Batch 75780, train_perplexity=4840.437, train_loss=8.48476

Batch 75790, train_perplexity=4940.531, train_loss=8.505228

Batch 75800, train_perplexity=5108.29, train_loss=8.53862

Batch 75810, train_perplexity=5493.4614, train_loss=8.611314

Batch 75820, train_perplexity=5265.8525, train_loss=8.568998

Batch 75830, train_perplexity=5847.7646, train_loss=8.673815

Batch 75840, train_perplexity=5816.03, train_loss=8.668373

Batch 75850, train_perplexity=5751.649, train_loss=8.657242

Batch 75860, train_perplexity=5419.7295, train_loss=8.597801

Batch 75870, train_perplexity=6033.088, train_loss=8.705014

Batch 75880, train_perplexity=5666.3457, train_loss=8.6423

Batch 75890, train_perplexity=5938.4375, train_loss=8.689201

Batch 75900, train_perplexity=5078.214, train_loss=8.532715

Batch 75910, train_perplexity=5986.68, train_loss=8.697292

Batch 75920, train_perplexity=5814.9424, train_loss=8.668186

Batch 75930, train_perplexity=4445.6147, train_loss=8.399673

Batch 75940, train_perplexity=5922.2285, train_loss=8.686468

Batch 75950, train_perplexity=5578.22, train_loss=8.626625

Batch 75960, train_perplexity=4894.6836, train_loss=8.495905

Batch 75970, train_perplexity=5393.9634, train_loss=8.593036

Batch 75980, train_perplexity=4751.325, train_loss=8.466179

Batch 75990, train_perplexity=4609.519, train_loss=8.435879

Batch 76000, train_perplexity=4743.8594, train_loss=8.464606

Batch 76010, train_perplexity=5104.657, train_loss=8.537909

Batch 76020, train_perplexity=6105.613, train_loss=8.716964

Batch 76030, train_perplexity=5910.8647, train_loss=8.684547

Batch 76040, train_perplexity=4832.559, train_loss=8.483131

Batch 76050, train_perplexity=5070.5, train_loss=8.531195

Batch 76060, train_perplexity=5072.7246, train_loss=8.531633

Batch 76070, train_perplexity=5421.482, train_loss=8.5981245

Batch 76080, train_perplexity=5281.917, train_loss=8.572044

Batch 76090, train_perplexity=6025.337, train_loss=8.703729

Batch 76100, train_perplexity=5886.765, train_loss=8.680462

Batch 76110, train_perplexity=4967.555, train_loss=8.510683

Batch 76120, train_perplexity=4417.2573, train_loss=8.393274

Batch 76130, train_perplexity=4660.8755, train_loss=8.446959

Batch 76140, train_perplexity=5180.1113, train_loss=8.552582

Batch 76150, train_perplexity=5046.466, train_loss=8.5264435

Batch 76160, train_perplexity=6155.384, train_loss=8.725082

Batch 76170, train_perplexity=5800.686, train_loss=8.665731

Batch 76180, train_perplexity=5258.335, train_loss=8.56757

Batch 76190, train_perplexity=6119.2007, train_loss=8.719187

Batch 76200, train_perplexity=5543.754, train_loss=8.620427

Batch 76210, train_perplexity=5590.3735, train_loss=8.628801

Batch 76220, train_perplexity=4598.529, train_loss=8.433492

Batch 76230, train_perplexity=5271.7515, train_loss=8.570118

Batch 76240, train_perplexity=5941.6724, train_loss=8.689746

Batch 76250, train_perplexity=5617.3296, train_loss=8.633612

Batch 76260, train_perplexity=4026.1802, train_loss=8.300573

Batch 76270, train_perplexity=6318.047, train_loss=8.751165

Batch 76280, train_perplexity=5242.682, train_loss=8.564589

Batch 76290, train_perplexity=4706.453, train_loss=8.45669

Batch 76300, train_perplexity=6035.326, train_loss=8.705385

Batch 76310, train_perplexity=4819.828, train_loss=8.480494

Batch 76320, train_perplexity=5611.0654, train_loss=8.632496

Batch 76330, train_perplexity=5431.936, train_loss=8.600051

Batch 76340, train_perplexity=4646.89, train_loss=8.4439535

Batch 76350, train_perplexity=5876.4775, train_loss=8.678713

Batch 76360, train_perplexity=5937.628, train_loss=8.689065

Batch 76370, train_perplexity=6383.201, train_loss=8.761425

Batch 76380, train_perplexity=5362.9873, train_loss=8.587276

Batch 76390, train_perplexity=5846.1587, train_loss=8.67354

Batch 76400, train_perplexity=6289.179, train_loss=8.746586

Batch 76410, train_perplexity=5444.471, train_loss=8.602356

Batch 76420, train_perplexity=5628.263, train_loss=8.635556

Batch 76430, train_perplexity=5132.08, train_loss=8.543266

Batch 76440, train_perplexity=6623.947, train_loss=8.798447

Batch 76450, train_perplexity=5739.2983, train_loss=8.655092

Batch 76460, train_perplexity=5041.781, train_loss=8.525515

Batch 76470, train_perplexity=4793.749, train_loss=8.475068

Batch 76480, train_perplexity=5981.9546, train_loss=8.696503

Batch 76490, train_perplexity=4617.0684, train_loss=8.437515

Batch 76500, train_perplexity=5166.854, train_loss=8.550019

Batch 76510, train_perplexity=5618.626, train_loss=8.633842

Batch 76520, train_perplexity=5264.7075, train_loss=8.568781

Batch 76530, train_perplexity=5295.919, train_loss=8.574692

Batch 76540, train_perplexity=6092.276, train_loss=8.714777

Batch 76550, train_perplexity=6174.786, train_loss=8.7282295

Batch 76560, train_perplexity=5222.482, train_loss=8.560728

Batch 76570, train_perplexity=4408.9204, train_loss=8.391385

Batch 76580, train_perplexity=5132.1533, train_loss=8.543281

Batch 76590, train_perplexity=5962.9424, train_loss=8.693319

Batch 76600, train_perplexity=6064.376, train_loss=8.710187

Batch 76610, train_perplexity=5320.765, train_loss=8.579372

Batch 76620, train_perplexity=5098.775, train_loss=8.536756

Batch 76630, train_perplexity=5689.011, train_loss=8.646292

Batch 76640, train_perplexity=5098.4297, train_loss=8.536688

Batch 76650, train_perplexity=4799.591, train_loss=8.476286

Batch 76660, train_perplexity=4956.2646, train_loss=8.508408

Batch 76670, train_perplexity=5825.922, train_loss=8.670073

Batch 76680, train_perplexity=5436.6313, train_loss=8.600915

Batch 76690, train_perplexity=5643.081, train_loss=8.6381855

Batch 76700, train_perplexity=6148.865, train_loss=8.724023

Batch 76710, train_perplexity=5402.1123, train_loss=8.594545

Batch 76720, train_perplexity=5335.7656, train_loss=8.582188

Batch 76730, train_perplexity=5505.3247, train_loss=8.613471

Batch 76740, train_perplexity=5917.379, train_loss=8.685649

Batch 76750, train_perplexity=4553.6704, train_loss=8.423689

Batch 76760, train_perplexity=4692.698, train_loss=8.453763

Batch 76770, train_perplexity=5553.591, train_loss=8.6222

Batch 76780, train_perplexity=5710.635, train_loss=8.650085

Batch 76790, train_perplexity=5233.6953, train_loss=8.562873

Batch 76800, train_perplexity=5657.1016, train_loss=8.640667

Batch 76810, train_perplexity=4535.6406, train_loss=8.419722

Batch 76820, train_perplexity=5529.371, train_loss=8.617829

Batch 76830, train_perplexity=5757.3945, train_loss=8.65824

Batch 76840, train_perplexity=4772.7744, train_loss=8.470683

Batch 76850, train_perplexity=6535.821, train_loss=8.785053

Batch 76860, train_perplexity=5246.894, train_loss=8.565392

Batch 76870, train_perplexity=5170.6646, train_loss=8.550756

Batch 76880, train_perplexity=5325.212, train_loss=8.580208

Batch 76890, train_perplexity=6908.017, train_loss=8.840438

Batch 76900, train_perplexity=5968.814, train_loss=8.6943035

Batch 76910, train_perplexity=5883.6387, train_loss=8.679931

Batch 76920, train_perplexity=5679.9473, train_loss=8.644697
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76930, train_perplexity=6175.0864, train_loss=8.728278

Batch 76940, train_perplexity=4599.1514, train_loss=8.433627

Batch 76950, train_perplexity=4522.191, train_loss=8.416752

Batch 76960, train_perplexity=4571.624, train_loss=8.427624

Batch 76970, train_perplexity=5422.18, train_loss=8.598253

Batch 76980, train_perplexity=5405.5757, train_loss=8.595186

Batch 76990, train_perplexity=5895.2256, train_loss=8.681898

Batch 77000, train_perplexity=5853.813, train_loss=8.674849

Batch 77010, train_perplexity=5917.114, train_loss=8.685604

Batch 77020, train_perplexity=5115.1636, train_loss=8.539965

Batch 77030, train_perplexity=5843.684, train_loss=8.673117

Batch 77040, train_perplexity=4631.1226, train_loss=8.440555

Batch 77050, train_perplexity=5126.826, train_loss=8.542242

Batch 77060, train_perplexity=5055.3916, train_loss=8.528211

Batch 77070, train_perplexity=6226.6113, train_loss=8.736588

Batch 77080, train_perplexity=4418.475, train_loss=8.39355

Batch 77090, train_perplexity=5675.6587, train_loss=8.643942

Batch 77100, train_perplexity=4959.7773, train_loss=8.509116

Batch 77110, train_perplexity=4574.162, train_loss=8.428179

Batch 77120, train_perplexity=4604.361, train_loss=8.434759

Batch 77130, train_perplexity=5137.648, train_loss=8.544351

Batch 77140, train_perplexity=4676.771, train_loss=8.450363

Batch 77150, train_perplexity=4481.3027, train_loss=8.407669

Batch 77160, train_perplexity=4996.652, train_loss=8.516523

Batch 77170, train_perplexity=5122.379, train_loss=8.541374

Batch 77180, train_perplexity=4901.022, train_loss=8.497199

Batch 77190, train_perplexity=5177.454, train_loss=8.552069

Batch 77200, train_perplexity=5150.069, train_loss=8.546765

Batch 77210, train_perplexity=5438.431, train_loss=8.601246

Batch 77220, train_perplexity=4032.0168, train_loss=8.302022

Batch 77230, train_perplexity=5204.682, train_loss=8.557314

Batch 77240, train_perplexity=5251.244, train_loss=8.56622

Batch 77250, train_perplexity=6201.0347, train_loss=8.732471

Batch 77260, train_perplexity=5975.409, train_loss=8.695408

Batch 77270, train_perplexity=5492.398, train_loss=8.61112

Batch 77280, train_perplexity=5694.211, train_loss=8.647205

Batch 77290, train_perplexity=5186.7104, train_loss=8.553855

Batch 77300, train_perplexity=5452.1196, train_loss=8.60376

Batch 77310, train_perplexity=5193.7935, train_loss=8.55522

Batch 77320, train_perplexity=5893.197, train_loss=8.681554

Batch 77330, train_perplexity=5048.6753, train_loss=8.526881

Batch 77340, train_perplexity=4999.1064, train_loss=8.5170145

Batch 77350, train_perplexity=5548.673, train_loss=8.621314

Batch 77360, train_perplexity=5930.486, train_loss=8.687861

Batch 77370, train_perplexity=5131.6787, train_loss=8.543188

Batch 77380, train_perplexity=5391.037, train_loss=8.592493

Batch 77390, train_perplexity=6880.094, train_loss=8.836388

Batch 77400, train_perplexity=4289.993, train_loss=8.36404

Batch 77410, train_perplexity=5270.6255, train_loss=8.569904

Batch 77420, train_perplexity=5873.4185, train_loss=8.678192

Batch 77430, train_perplexity=5491.7485, train_loss=8.611002

Batch 77440, train_perplexity=5184.554, train_loss=8.553439

Batch 77450, train_perplexity=6951.423, train_loss=8.846702

Batch 77460, train_perplexity=4698.0986, train_loss=8.454913

Batch 77470, train_perplexity=4715.668, train_loss=8.458646

Batch 77480, train_perplexity=6043.759, train_loss=8.706781

Batch 77490, train_perplexity=5595.0347, train_loss=8.629635

Batch 77500, train_perplexity=5790.538, train_loss=8.6639805

Batch 77510, train_perplexity=4875.778, train_loss=8.492035

Batch 77520, train_perplexity=5522.5674, train_loss=8.616598

Batch 77530, train_perplexity=4112.953, train_loss=8.321897

Batch 77540, train_perplexity=5956.6733, train_loss=8.692267

Batch 77550, train_perplexity=5903.7495, train_loss=8.683343

Batch 77560, train_perplexity=6496.5405, train_loss=8.779025

Batch 77570, train_perplexity=4952.0405, train_loss=8.507555

Batch 77580, train_perplexity=5083.248, train_loss=8.533706

Batch 77590, train_perplexity=5380.8154, train_loss=8.590595

Batch 77600, train_perplexity=4934.405, train_loss=8.503987

Batch 77610, train_perplexity=5724.419, train_loss=8.652496

Batch 77620, train_perplexity=5963.739, train_loss=8.693453

Batch 77630, train_perplexity=5274.115, train_loss=8.570566

Batch 77640, train_perplexity=5142.653, train_loss=8.545324

Batch 77650, train_perplexity=5639.7563, train_loss=8.637596

Batch 77660, train_perplexity=4888.1343, train_loss=8.494566

Batch 77670, train_perplexity=4564.484, train_loss=8.426061

Batch 77680, train_perplexity=5342.451, train_loss=8.58344

Batch 77690, train_perplexity=4746.407, train_loss=8.465143

Batch 77700, train_perplexity=5180.1406, train_loss=8.5525875

Batch 77710, train_perplexity=4951.082, train_loss=8.507361

Batch 77720, train_perplexity=5284.6934, train_loss=8.57257

Batch 77730, train_perplexity=5809.832, train_loss=8.667307

Batch 77740, train_perplexity=6201.1294, train_loss=8.732487

Batch 77750, train_perplexity=5445.936, train_loss=8.602625

Batch 77760, train_perplexity=5824.989, train_loss=8.669912

Batch 77770, train_perplexity=5377.163, train_loss=8.589916

Batch 77780, train_perplexity=5090.918, train_loss=8.535213

Batch 77790, train_perplexity=5585.588, train_loss=8.627945

Batch 77800, train_perplexity=5265.883, train_loss=8.569004

Batch 77810, train_perplexity=6016.1846, train_loss=8.7022085

Batch 77820, train_perplexity=7504.9272, train_loss=8.923315

Batch 77830, train_perplexity=5251.1587, train_loss=8.566204

Batch 77840, train_perplexity=4315.862, train_loss=8.370052

Batch 77850, train_perplexity=5557.406, train_loss=8.622887

Batch 77860, train_perplexity=4933.887, train_loss=8.503882

Batch 77870, train_perplexity=5648.627, train_loss=8.639168

Batch 77880, train_perplexity=5018.3374, train_loss=8.520854

Batch 77890, train_perplexity=6128.7437, train_loss=8.720745

Batch 77900, train_perplexity=4932.0854, train_loss=8.503517

Batch 77910, train_perplexity=5796.1514, train_loss=8.664949

Batch 77920, train_perplexity=5717.5664, train_loss=8.6512985

Batch 77930, train_perplexity=5426.51, train_loss=8.599051

Batch 77940, train_perplexity=5075.8413, train_loss=8.532248

Batch 77950, train_perplexity=5917.87, train_loss=8.685732

Batch 77960, train_perplexity=4997.777, train_loss=8.516748

Batch 77970, train_perplexity=4801.413, train_loss=8.4766655

Batch 77980, train_perplexity=5638.5625, train_loss=8.637384

Batch 77990, train_perplexity=5799.425, train_loss=8.665514

Batch 78000, train_perplexity=4842.39, train_loss=8.485164

Batch 78010, train_perplexity=4346.85, train_loss=8.377207

Batch 78020, train_perplexity=5477.0825, train_loss=8.608328

Batch 78030, train_perplexity=5559.982, train_loss=8.62335

Batch 78040, train_perplexity=5249.211, train_loss=8.565833

Batch 78050, train_perplexity=5402.2207, train_loss=8.594565

Batch 78060, train_perplexity=4866.4917, train_loss=8.4901285

Batch 78070, train_perplexity=5385.4873, train_loss=8.591463

Batch 78080, train_perplexity=6424.688, train_loss=8.767903

Batch 78090, train_perplexity=5329.9272, train_loss=8.581093

Batch 78100, train_perplexity=5620.781, train_loss=8.634226

Batch 78110, train_perplexity=5870.238, train_loss=8.67765

Batch 78120, train_perplexity=5642.9307, train_loss=8.638159

Batch 78130, train_perplexity=5394.061, train_loss=8.593054

Batch 78140, train_perplexity=5608.7646, train_loss=8.632086

Batch 78150, train_perplexity=5218.151, train_loss=8.559898

Batch 78160, train_perplexity=5049.509, train_loss=8.527046

Batch 78170, train_perplexity=6453.1865, train_loss=8.772329

Batch 78180, train_perplexity=5617.5654, train_loss=8.633654

Batch 78190, train_perplexity=5421.7563, train_loss=8.598175

Batch 78200, train_perplexity=5679.7573, train_loss=8.644664

Batch 78210, train_perplexity=6070.0117, train_loss=8.711116

Batch 78220, train_perplexity=4758.0684, train_loss=8.467597

Batch 78230, train_perplexity=4637.496, train_loss=8.44193

Batch 78240, train_perplexity=5423.623, train_loss=8.598519

Batch 78250, train_perplexity=5335.5977, train_loss=8.582156

Batch 78260, train_perplexity=5455.573, train_loss=8.604393
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78270, train_perplexity=4864.065, train_loss=8.48963

Batch 78280, train_perplexity=5690.5522, train_loss=8.646563

Batch 78290, train_perplexity=5146.5874, train_loss=8.546089

Batch 78300, train_perplexity=5415.8647, train_loss=8.597088

Batch 78310, train_perplexity=4165.4136, train_loss=8.334571

Batch 78320, train_perplexity=6229.8306, train_loss=8.737104

Batch 78330, train_perplexity=6787.661, train_loss=8.822862

Batch 78340, train_perplexity=6078.8, train_loss=8.712563

Batch 78350, train_perplexity=5832.677, train_loss=8.671231

Batch 78360, train_perplexity=4606.32, train_loss=8.4351845

Batch 78370, train_perplexity=5010.748, train_loss=8.5193405

Batch 78380, train_perplexity=5101.3574, train_loss=8.537262

Batch 78390, train_perplexity=5523.9053, train_loss=8.61684

Batch 78400, train_perplexity=5172.8096, train_loss=8.551171

Batch 78410, train_perplexity=4684.761, train_loss=8.45207

Batch 78420, train_perplexity=4489.302, train_loss=8.409452

Batch 78430, train_perplexity=5644.825, train_loss=8.6384945

Batch 78440, train_perplexity=5158.115, train_loss=8.5483265

Batch 78450, train_perplexity=5245.623, train_loss=8.565149

Batch 78460, train_perplexity=5944.7783, train_loss=8.6902685

Batch 78470, train_perplexity=4906.7886, train_loss=8.498375

Batch 78480, train_perplexity=5347.7676, train_loss=8.5844345

Batch 78490, train_perplexity=5776.5503, train_loss=8.661562

Batch 78500, train_perplexity=6050.933, train_loss=8.707968

Batch 78510, train_perplexity=5474.8945, train_loss=8.607928

Batch 78520, train_perplexity=4955.976, train_loss=8.508349

Batch 78530, train_perplexity=4676.7485, train_loss=8.450358

Batch 78540, train_perplexity=5651.9355, train_loss=8.639753

Batch 78550, train_perplexity=6161.3154, train_loss=8.726046

Batch 78560, train_perplexity=5197.8667, train_loss=8.556004

Batch 78570, train_perplexity=5727.0728, train_loss=8.65296

Batch 78580, train_perplexity=6865.5303, train_loss=8.834269

Batch 78590, train_perplexity=5601.1104, train_loss=8.63072

Batch 78600, train_perplexity=5954.5547, train_loss=8.691912

Batch 78610, train_perplexity=5115.2515, train_loss=8.539982

Batch 78620, train_perplexity=5059.0044, train_loss=8.528925

Batch 78630, train_perplexity=5288.3687, train_loss=8.573265

Batch 78640, train_perplexity=5176.5557, train_loss=8.551895

Batch 78650, train_perplexity=5326.868, train_loss=8.580519

Batch 78660, train_perplexity=6003.6553, train_loss=8.700124

Batch 78670, train_perplexity=5478.3154, train_loss=8.608553

Batch 78680, train_perplexity=5666.648, train_loss=8.642353

Batch 78690, train_perplexity=6439.514, train_loss=8.770208

Batch 78700, train_perplexity=5720.6533, train_loss=8.651838

Batch 78710, train_perplexity=5364.3228, train_loss=8.587525

Batch 78720, train_perplexity=5876.231, train_loss=8.678671

Batch 78730, train_perplexity=5396.3096, train_loss=8.593471

Batch 78740, train_perplexity=5249.837, train_loss=8.565952

Batch 78750, train_perplexity=4213.2324, train_loss=8.345985

Batch 78760, train_perplexity=5098.809, train_loss=8.536762

Batch 78770, train_perplexity=5661.1113, train_loss=8.641376

Batch 78780, train_perplexity=5282.154, train_loss=8.572089

Batch 78790, train_perplexity=4528.777, train_loss=8.418207

Batch 78800, train_perplexity=5656.217, train_loss=8.640511

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Batch 78810, train_perplexity=6104.5825, train_loss=8.716795

Batch 78820, train_perplexity=5726.8105, train_loss=8.652914

Batch 78830, train_perplexity=5067.725, train_loss=8.530647

Batch 78840, train_perplexity=5360.0625, train_loss=8.586731

Batch 78850, train_perplexity=4599.8755, train_loss=8.4337845

Batch 78860, train_perplexity=6236.7856, train_loss=8.73822

Batch 78870, train_perplexity=5034.074, train_loss=8.523985

Batch 78880, train_perplexity=4620.742, train_loss=8.438311

Batch 78890, train_perplexity=6012.611, train_loss=8.701614

Batch 78900, train_perplexity=4786.9375, train_loss=8.473646

Batch 78910, train_perplexity=5418.5356, train_loss=8.597581

Batch 78920, train_perplexity=4821.782, train_loss=8.480899

Batch 78930, train_perplexity=5745.7827, train_loss=8.656221

Batch 78940, train_perplexity=5365.934, train_loss=8.587826

Batch 78950, train_perplexity=5394.6733, train_loss=8.593167

Batch 78960, train_perplexity=5565.791, train_loss=8.624394

Batch 78970, train_perplexity=5159.0645, train_loss=8.548511

Batch 78980, train_perplexity=4956.8364, train_loss=8.508523

Batch 78990, train_perplexity=4624.3306, train_loss=8.439087

Batch 79000, train_perplexity=5760.9756, train_loss=8.658862

Batch 79010, train_perplexity=5884.267, train_loss=8.6800375

Batch 79020, train_perplexity=5560.035, train_loss=8.62336

Batch 79030, train_perplexity=4638.96, train_loss=8.4422455

Batch 79040, train_perplexity=5547.768, train_loss=8.621151

Batch 79050, train_perplexity=4597.5684, train_loss=8.433283

Batch 79060, train_perplexity=5546.5723, train_loss=8.620935

Batch 79070, train_perplexity=4993.9937, train_loss=8.515991

Batch 79080, train_perplexity=4979.294, train_loss=8.513043

Batch 79090, train_perplexity=5571.2715, train_loss=8.625379

Batch 79100, train_perplexity=4898.812, train_loss=8.496748

Batch 79110, train_perplexity=4951.3794, train_loss=8.5074215

Batch 79120, train_perplexity=5687.156, train_loss=8.645966

Batch 79130, train_perplexity=5521.282, train_loss=8.616365

Batch 79140, train_perplexity=6069.925, train_loss=8.711102

Batch 79150, train_perplexity=5032.356, train_loss=8.5236435

Batch 79160, train_perplexity=4690.7695, train_loss=8.453352

Batch 79170, train_perplexity=5670.443, train_loss=8.643023

Batch 79180, train_perplexity=5159.394, train_loss=8.548574

Batch 79190, train_perplexity=4846.511, train_loss=8.486014

Batch 79200, train_perplexity=4437.6133, train_loss=8.397872

Batch 79210, train_perplexity=5624.491, train_loss=8.634886

Batch 79220, train_perplexity=5324.6025, train_loss=8.580093

Batch 79230, train_perplexity=5754.9077, train_loss=8.657808

Batch 79240, train_perplexity=5008.727, train_loss=8.518937

Batch 79250, train_perplexity=4629.9126, train_loss=8.440293

Batch 79260, train_perplexity=4583.896, train_loss=8.430305

Batch 79270, train_perplexity=5527.3726, train_loss=8.617468

Batch 79280, train_perplexity=5232.8867, train_loss=8.562718

Batch 79290, train_perplexity=4792.94, train_loss=8.474899

Batch 79300, train_perplexity=5942.834, train_loss=8.689941

Batch 79310, train_perplexity=6294.3994, train_loss=8.747416

Batch 79320, train_perplexity=6444.478, train_loss=8.770979

Batch 79330, train_perplexity=5064.198, train_loss=8.529951

Batch 79340, train_perplexity=5288.1367, train_loss=8.573221

Batch 79350, train_perplexity=4598.836, train_loss=8.433558

Batch 79360, train_perplexity=4897.7607, train_loss=8.496533

Batch 79370, train_perplexity=5138.907, train_loss=8.544596

Batch 79380, train_perplexity=5249.1562, train_loss=8.565823

Batch 79390, train_perplexity=5418.8975, train_loss=8.597648

Batch 79400, train_perplexity=4879.462, train_loss=8.49279

Batch 79410, train_perplexity=6173.579, train_loss=8.728034

Batch 79420, train_perplexity=6735.4106, train_loss=8.815134

Batch 79430, train_perplexity=4662.7827, train_loss=8.447368

Batch 79440, train_perplexity=4777.3057, train_loss=8.471632

Batch 79450, train_perplexity=5123.918, train_loss=8.541675

Batch 79460, train_perplexity=5787.038, train_loss=8.663376

Batch 79470, train_perplexity=5161.323, train_loss=8.548948

Batch 79480, train_perplexity=5171.562, train_loss=8.55093

Batch 79490, train_perplexity=5543.273, train_loss=8.62034

Batch 79500, train_perplexity=5864.318, train_loss=8.676641

Batch 79510, train_perplexity=5692.9624, train_loss=8.646986

Batch 79520, train_perplexity=5382.15, train_loss=8.590843

Batch 79530, train_perplexity=5180.5605, train_loss=8.552669
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 79540, train_perplexity=5290.8403, train_loss=8.573732

Batch 79550, train_perplexity=4237.054, train_loss=8.351624

Batch 79560, train_perplexity=5581.413, train_loss=8.627197

Batch 79570, train_perplexity=5905.1514, train_loss=8.68358

Batch 79580, train_perplexity=5128.0093, train_loss=8.542473

Batch 79590, train_perplexity=5211.2285, train_loss=8.558571

Batch 79600, train_perplexity=4408.096, train_loss=8.391198

Batch 79610, train_perplexity=5971.9795, train_loss=8.694834

Batch 79620, train_perplexity=6003.312, train_loss=8.700067

Batch 79630, train_perplexity=5159.222, train_loss=8.548541

Batch 79640, train_perplexity=4765.8022, train_loss=8.469221

Batch 79650, train_perplexity=5231.6343, train_loss=8.562479

Batch 79660, train_perplexity=4844.3994, train_loss=8.485579

Batch 79670, train_perplexity=5397.3696, train_loss=8.593667

Batch 79680, train_perplexity=5215.0615, train_loss=8.559306

Batch 79690, train_perplexity=6357.479, train_loss=8.757387

Batch 79700, train_perplexity=5828.145, train_loss=8.670454

Batch 79710, train_perplexity=5161.0723, train_loss=8.5489

Batch 79720, train_perplexity=5978.3276, train_loss=8.695896

Batch 79730, train_perplexity=5732.898, train_loss=8.653976

Batch 79740, train_perplexity=5171.3105, train_loss=8.550881

Batch 79750, train_perplexity=4946.622, train_loss=8.50646

Batch 79760, train_perplexity=5998.236, train_loss=8.699221

Batch 79770, train_perplexity=5753.333, train_loss=8.657535

Batch 79780, train_perplexity=5675.826, train_loss=8.643971

Batch 79790, train_perplexity=6741.233, train_loss=8.815998

Batch 79800, train_perplexity=5621.74, train_loss=8.634397

Batch 79810, train_perplexity=5713.9688, train_loss=8.650669

Batch 79820, train_perplexity=5328.006, train_loss=8.580732

Batch 79830, train_perplexity=6011.258, train_loss=8.701389

Batch 79840, train_perplexity=5063.3726, train_loss=8.529788

Batch 79850, train_perplexity=5936.7275, train_loss=8.688913

Batch 79860, train_perplexity=4226.505, train_loss=8.349131

Batch 79870, train_perplexity=6262.5874, train_loss=8.742349

Batch 79880, train_perplexity=4884.6763, train_loss=8.493858

Batch 79890, train_perplexity=5158.056, train_loss=8.548315

Batch 79900, train_perplexity=5791.5874, train_loss=8.664162

Batch 79910, train_perplexity=5934.5596, train_loss=8.688548

Batch 79920, train_perplexity=5893.1123, train_loss=8.68154

Batch 79930, train_perplexity=5056.8623, train_loss=8.5285015

Batch 79940, train_perplexity=5699.6987, train_loss=8.648169

Batch 79950, train_perplexity=5875.514, train_loss=8.678549

Batch 79960, train_perplexity=5577.002, train_loss=8.626407

Batch 79970, train_perplexity=6753.7935, train_loss=8.81786

Batch 79980, train_perplexity=5124.3574, train_loss=8.54176

Batch 79990, train_perplexity=6143.315, train_loss=8.72312

Batch 80000, train_perplexity=5288.0254, train_loss=8.5732

Batch 80010, train_perplexity=5379.3276, train_loss=8.590319

Batch 80020, train_perplexity=5062.305, train_loss=8.529577

Batch 80030, train_perplexity=5222.243, train_loss=8.560682

Batch 80040, train_perplexity=5676.8604, train_loss=8.644154

Batch 80050, train_perplexity=6003.787, train_loss=8.700146

Batch 80060, train_perplexity=5266.7266, train_loss=8.569164

Batch 80070, train_perplexity=5248.926, train_loss=8.565779

Batch 80080, train_perplexity=4321.4263, train_loss=8.371341

Batch 80090, train_perplexity=5968.6885, train_loss=8.694283

Batch 80100, train_perplexity=4272.094, train_loss=8.359859

Batch 80110, train_perplexity=5058.3237, train_loss=8.52879

Batch 80120, train_perplexity=5317.8584, train_loss=8.578826

Batch 80130, train_perplexity=5509.4897, train_loss=8.614227

Batch 80140, train_perplexity=5517.0874, train_loss=8.615605

Batch 80150, train_perplexity=5991.8438, train_loss=8.698154

Batch 80160, train_perplexity=4431.6626, train_loss=8.39653

Batch 80170, train_perplexity=5056.602, train_loss=8.52845

Batch 80180, train_perplexity=5196.6177, train_loss=8.555763

Batch 80190, train_perplexity=5355.3877, train_loss=8.585858

Batch 80200, train_perplexity=4606.6885, train_loss=8.435265

Batch 80210, train_perplexity=5321.374, train_loss=8.579487

Batch 80220, train_perplexity=5562.1245, train_loss=8.623735

Batch 80230, train_perplexity=5112.8228, train_loss=8.539507

Batch 80240, train_perplexity=6886.403, train_loss=8.837304

Batch 80250, train_perplexity=5184.5293, train_loss=8.553434

Batch 80260, train_perplexity=5853.3105, train_loss=8.674763

Batch 80270, train_perplexity=5835.364, train_loss=8.671692

Batch 80280, train_perplexity=4126.3228, train_loss=8.325142

Batch 80290, train_perplexity=5932.0757, train_loss=8.688129

Batch 80300, train_perplexity=4545.609, train_loss=8.421917

Batch 80310, train_perplexity=4890.5776, train_loss=8.495066

Batch 80320, train_perplexity=5259.4585, train_loss=8.567783

Batch 80330, train_perplexity=5734.8174, train_loss=8.654311

Batch 80340, train_perplexity=6115.759, train_loss=8.718624

Batch 80350, train_perplexity=6033.652, train_loss=8.705108

Batch 80360, train_perplexity=5298.5254, train_loss=8.575184

Batch 80370, train_perplexity=5304.871, train_loss=8.576381

Batch 80380, train_perplexity=5665.027, train_loss=8.642067

Batch 80390, train_perplexity=5376.4966, train_loss=8.589792

Batch 80400, train_perplexity=5593.9785, train_loss=8.629446

Batch 80410, train_perplexity=4958.1836, train_loss=8.508795

Batch 80420, train_perplexity=5726.45, train_loss=8.652851

Batch 80430, train_perplexity=5626.7446, train_loss=8.635286

Batch 80440, train_perplexity=4734.978, train_loss=8.462732

Batch 80450, train_perplexity=7173.227, train_loss=8.878111

Batch 80460, train_perplexity=5581.03, train_loss=8.627129

Batch 80470, train_perplexity=4582.318, train_loss=8.42996

Batch 80480, train_perplexity=5604.605, train_loss=8.631344

Batch 80490, train_perplexity=5342.018, train_loss=8.583359

Batch 80500, train_perplexity=4722.549, train_loss=8.460104

Batch 80510, train_perplexity=5352.0737, train_loss=8.585239

Batch 80520, train_perplexity=5114.081, train_loss=8.539753

Batch 80530, train_perplexity=5137.6035, train_loss=8.544342

Batch 80540, train_perplexity=6737.8843, train_loss=8.815501

Batch 80550, train_perplexity=4793.5938, train_loss=8.475036

Batch 80560, train_perplexity=4889.0713, train_loss=8.494758

Batch 80570, train_perplexity=4534.4985, train_loss=8.41947

Batch 80580, train_perplexity=5489.455, train_loss=8.610584

Batch 80590, train_perplexity=5070.7515, train_loss=8.531244

Batch 80600, train_perplexity=5180.3926, train_loss=8.552636

Batch 80610, train_perplexity=5809.976, train_loss=8.667332

Batch 80620, train_perplexity=5904.673, train_loss=8.683499

Batch 80630, train_perplexity=5389.1147, train_loss=8.592136

Batch 80640, train_perplexity=5052.582, train_loss=8.527655

Batch 80650, train_perplexity=4452.896, train_loss=8.40131

Batch 80660, train_perplexity=5495.248, train_loss=8.611639

Batch 80670, train_perplexity=4789.143, train_loss=8.474107

Batch 80680, train_perplexity=6516.465, train_loss=8.782087

Batch 80690, train_perplexity=5541.7607, train_loss=8.620068

Batch 80700, train_perplexity=5190.397, train_loss=8.554565

Batch 80710, train_perplexity=5273.9287, train_loss=8.570531

Batch 80720, train_perplexity=5041.492, train_loss=8.525457

Batch 80730, train_perplexity=5316.317, train_loss=8.578536

Batch 80740, train_perplexity=5756.4062, train_loss=8.658069

Batch 80750, train_perplexity=6251.846, train_loss=8.740632

Batch 80760, train_perplexity=5810.8623, train_loss=8.667484

Batch 80770, train_perplexity=4783.0815, train_loss=8.47284

Batch 80780, train_perplexity=5222.4375, train_loss=8.5607195

Batch 80790, train_perplexity=5524.685, train_loss=8.6169815

Batch 80800, train_perplexity=5305.0176, train_loss=8.576408

Batch 80810, train_perplexity=4981.256, train_loss=8.513437

Batch 80820, train_perplexity=6405.985, train_loss=8.764988

Batch 80830, train_perplexity=4467.4604, train_loss=8.404575

Batch 80840, train_perplexity=5046.524, train_loss=8.526455

Batch 80850, train_perplexity=5363.3047, train_loss=8.587336

Batch 80860, train_perplexity=4728.7773, train_loss=8.461422

Batch 80870, train_perplexity=4686.7812, train_loss=8.452501
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 80880, train_perplexity=5022.637, train_loss=8.52171

Batch 80890, train_perplexity=5046.1963, train_loss=8.52639

Batch 80900, train_perplexity=5317.407, train_loss=8.578741

Batch 80910, train_perplexity=5691.388, train_loss=8.646709

Batch 80920, train_perplexity=4694.82, train_loss=8.454215

Batch 80930, train_perplexity=5226.702, train_loss=8.561536

Batch 80940, train_perplexity=5672.899, train_loss=8.6434555

Batch 80950, train_perplexity=6362.082, train_loss=8.758111

Batch 80960, train_perplexity=4874.2715, train_loss=8.491726

Batch 80970, train_perplexity=5077.526, train_loss=8.532579

Batch 80980, train_perplexity=4985.652, train_loss=8.514319

Batch 80990, train_perplexity=4705.7485, train_loss=8.45654

Batch 81000, train_perplexity=4942.3027, train_loss=8.505587

Batch 81010, train_perplexity=4528.3105, train_loss=8.418104

Batch 81020, train_perplexity=6138.0034, train_loss=8.722255

Batch 81030, train_perplexity=4212.5854, train_loss=8.345832

Batch 81040, train_perplexity=6283.6157, train_loss=8.745701

Batch 81050, train_perplexity=5477.6206, train_loss=8.608426

Batch 81060, train_perplexity=4761.5225, train_loss=8.468323

Batch 81070, train_perplexity=5062.788, train_loss=8.529673

Batch 81080, train_perplexity=6654.5225, train_loss=8.803052

Batch 81090, train_perplexity=4498.8506, train_loss=8.411577

Batch 81100, train_perplexity=5600.571, train_loss=8.630624

Batch 81110, train_perplexity=7026.016, train_loss=8.857375

Batch 81120, train_perplexity=5163.8345, train_loss=8.549435

Batch 81130, train_perplexity=5674.3813, train_loss=8.643717

Batch 81140, train_perplexity=5530.8213, train_loss=8.618092

Batch 81150, train_perplexity=5363.7954, train_loss=8.587427

Batch 81160, train_perplexity=5764.4434, train_loss=8.659464

Batch 81170, train_perplexity=7442.519, train_loss=8.914965

Batch 81180, train_perplexity=5005.361, train_loss=8.518265

Batch 81190, train_perplexity=5585.279, train_loss=8.62789

Batch 81200, train_perplexity=5178.2046, train_loss=8.552214

Batch 81210, train_perplexity=5397.066, train_loss=8.593611

Batch 81220, train_perplexity=5627.104, train_loss=8.63535

Batch 81230, train_perplexity=5127.4863, train_loss=8.542371

Batch 81240, train_perplexity=4818.1, train_loss=8.480135

Batch 81250, train_perplexity=5654.5825, train_loss=8.640222

Batch 81260, train_perplexity=5384.065, train_loss=8.591199

Batch 81270, train_perplexity=4894.1235, train_loss=8.4957905

Batch 81280, train_perplexity=6974.724, train_loss=8.850048

Batch 81290, train_perplexity=5206.5093, train_loss=8.557665

Batch 81300, train_perplexity=5482.544, train_loss=8.609324

Batch 81310, train_perplexity=4997.872, train_loss=8.5167675

Batch 81320, train_perplexity=5313.423, train_loss=8.5779915

Batch 81330, train_perplexity=5403.5244, train_loss=8.594807

Batch 81340, train_perplexity=5338.5854, train_loss=8.582716

Batch 81350, train_perplexity=5163.1055, train_loss=8.5492935

Batch 81360, train_perplexity=4935.713, train_loss=8.504252

Batch 81370, train_perplexity=6722.9927, train_loss=8.813289

Batch 81380, train_perplexity=5321.6484, train_loss=8.579538

Batch 81390, train_perplexity=4653.4136, train_loss=8.445356

Batch 81400, train_perplexity=5250.8784, train_loss=8.566151

Batch 81410, train_perplexity=3989.8071, train_loss=8.291498

Batch 81420, train_perplexity=4522.5273, train_loss=8.416826

Batch 81430, train_perplexity=6274.621, train_loss=8.744268

Batch 81440, train_perplexity=4516.312, train_loss=8.415451

Batch 81450, train_perplexity=5716.814, train_loss=8.651167

Batch 81460, train_perplexity=5703.619, train_loss=8.648856

Batch 81470, train_perplexity=4457.0684, train_loss=8.402246

Batch 81480, train_perplexity=5847.1123, train_loss=8.673703

Batch 81490, train_perplexity=4827.354, train_loss=8.482054

Batch 81500, train_perplexity=5464.0293, train_loss=8.605942

Batch 81510, train_perplexity=6295.414, train_loss=8.747577

Batch 81520, train_perplexity=4562.6904, train_loss=8.425668

Batch 81530, train_perplexity=5399.4907, train_loss=8.59406

Batch 81540, train_perplexity=5763.465, train_loss=8.659294

Batch 81550, train_perplexity=5088.8115, train_loss=8.5348

Batch 81560, train_perplexity=5060.809, train_loss=8.529282

Batch 81570, train_perplexity=5591.994, train_loss=8.629091

Batch 81580, train_perplexity=4980.149, train_loss=8.513215

Batch 81590, train_perplexity=6290.7446, train_loss=8.746835

Batch 81600, train_perplexity=7262.456, train_loss=8.890473

Batch 81610, train_perplexity=5093.5938, train_loss=8.535739

Batch 81620, train_perplexity=5787.8657, train_loss=8.663519

Batch 81630, train_perplexity=5664.6924, train_loss=8.642008

Batch 81640, train_perplexity=6379.367, train_loss=8.760824

Batch 81650, train_perplexity=4918.06, train_loss=8.5006695

Batch 81660, train_perplexity=5867.227, train_loss=8.677137

Batch 81670, train_perplexity=5205.814, train_loss=8.557531

Batch 81680, train_perplexity=5562.963, train_loss=8.623886

Batch 81690, train_perplexity=5372.76, train_loss=8.589097

Batch 81700, train_perplexity=5075.0376, train_loss=8.532089

Batch 81710, train_perplexity=4880.5464, train_loss=8.493012

Batch 81720, train_perplexity=5382.7354, train_loss=8.590952

Batch 81730, train_perplexity=5859.4766, train_loss=8.675816

Batch 81740, train_perplexity=5281.071, train_loss=8.571884

Batch 81750, train_perplexity=5444.1807, train_loss=8.602303

Batch 81760, train_perplexity=4563.056, train_loss=8.425748

Batch 81770, train_perplexity=4642.6953, train_loss=8.44305

Batch 81780, train_perplexity=4457.136, train_loss=8.402262

Batch 81790, train_perplexity=4926.059, train_loss=8.502295

Batch 81800, train_perplexity=6021.655, train_loss=8.703117

Batch 81810, train_perplexity=5639.159, train_loss=8.63749

Batch 81820, train_perplexity=5301.3257, train_loss=8.575712

Batch 81830, train_perplexity=5222.243, train_loss=8.560682

Batch 81840, train_perplexity=4865.2153, train_loss=8.489866

Batch 81850, train_perplexity=6819.122, train_loss=8.827486

Batch 81860, train_perplexity=5520.3135, train_loss=8.61619

Batch 81870, train_perplexity=7254.738, train_loss=8.88941

Batch 81880, train_perplexity=6182.175, train_loss=8.729425

Batch 81890, train_perplexity=4866.3706, train_loss=8.490104

Batch 81900, train_perplexity=5189.9116, train_loss=8.554472

Batch 81910, train_perplexity=4732.6753, train_loss=8.462246

Batch 81920, train_perplexity=5644.7173, train_loss=8.638475

Batch 81930, train_perplexity=4554.4956, train_loss=8.42387

Batch 81940, train_perplexity=5685.518, train_loss=8.645678

Batch 81950, train_perplexity=5811.9873, train_loss=8.667678

Batch 81960, train_perplexity=5335.8877, train_loss=8.582211

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Batch 81970, train_perplexity=4336.189, train_loss=8.374751

Batch 81980, train_perplexity=5546.7363, train_loss=8.620965

Batch 81990, train_perplexity=5686.614, train_loss=8.64587

Batch 82000, train_perplexity=5080.442, train_loss=8.533154

Batch 82010, train_perplexity=4987.445, train_loss=8.514679

Batch 82020, train_perplexity=5472.2583, train_loss=8.607447

Batch 82030, train_perplexity=6053.9116, train_loss=8.70846

Batch 82040, train_perplexity=4871.1953, train_loss=8.491095

Batch 82050, train_perplexity=5458.7583, train_loss=8.604977

Batch 82060, train_perplexity=4716.014, train_loss=8.458719

Batch 82070, train_perplexity=5116.696, train_loss=8.540264

Batch 82080, train_perplexity=4842.5977, train_loss=8.485207

Batch 82090, train_perplexity=6915.5513, train_loss=8.841528

Batch 82100, train_perplexity=4828.97, train_loss=8.4823885

Batch 82110, train_perplexity=5531.3066, train_loss=8.618179

Batch 82120, train_perplexity=5534.958, train_loss=8.618839

Batch 82130, train_perplexity=5871.8003, train_loss=8.677917

Batch 82140, train_perplexity=5675.3447, train_loss=8.643887
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82150, train_perplexity=5700.6987, train_loss=8.648344

Batch 82160, train_perplexity=5235.882, train_loss=8.563291

Batch 82170, train_perplexity=5236.431, train_loss=8.5633955

Batch 82180, train_perplexity=5295.7017, train_loss=8.574651

Batch 82190, train_perplexity=5299.7485, train_loss=8.575415

Batch 82200, train_perplexity=5367.326, train_loss=8.588085

Batch 82210, train_perplexity=4652.2954, train_loss=8.445116

Batch 82220, train_perplexity=4822.8535, train_loss=8.481121

Batch 82230, train_perplexity=4650.9116, train_loss=8.4448185

Batch 82240, train_perplexity=4926.341, train_loss=8.502352

Batch 82250, train_perplexity=4807.4517, train_loss=8.477922

Batch 82260, train_perplexity=5640.1597, train_loss=8.637668

Batch 82270, train_perplexity=5408.4326, train_loss=8.595715

Batch 82280, train_perplexity=5944.183, train_loss=8.690168

Batch 82290, train_perplexity=5245.228, train_loss=8.565074

Batch 82300, train_perplexity=6339.243, train_loss=8.754515

Batch 82310, train_perplexity=5028.949, train_loss=8.522966

Batch 82320, train_perplexity=4770.0127, train_loss=8.470104

Batch 82330, train_perplexity=4500.6187, train_loss=8.41197

Batch 82340, train_perplexity=5228.0933, train_loss=8.561802

Batch 82350, train_perplexity=6728.201, train_loss=8.814063

Batch 82360, train_perplexity=6156.6577, train_loss=8.725289

Batch 82370, train_perplexity=6145.0845, train_loss=8.723408

Batch 82380, train_perplexity=5526.392, train_loss=8.6172905

Batch 82390, train_perplexity=4803.987, train_loss=8.477201

Batch 82400, train_perplexity=5099.0327, train_loss=8.536806

Batch 82410, train_perplexity=5066.744, train_loss=8.530454

Batch 82420, train_perplexity=4788.4077, train_loss=8.473953

Batch 82430, train_perplexity=4686.4995, train_loss=8.452441

Batch 82440, train_perplexity=4421.767, train_loss=8.394295

Batch 82450, train_perplexity=6218.2915, train_loss=8.73525

Batch 82460, train_perplexity=5320.735, train_loss=8.579367

Batch 82470, train_perplexity=4991.765, train_loss=8.515545

Batch 82480, train_perplexity=5319.984, train_loss=8.579226

Batch 82490, train_perplexity=6493.196, train_loss=8.77851

Batch 82500, train_perplexity=5176.7036, train_loss=8.551924

Batch 82510, train_perplexity=4949.175, train_loss=8.506976

Batch 82520, train_perplexity=6063.3813, train_loss=8.710023

Batch 82530, train_perplexity=5273.6924, train_loss=8.570486

Batch 82540, train_perplexity=4830.688, train_loss=8.482744

Batch 82550, train_perplexity=6357.6484, train_loss=8.757414

Batch 82560, train_perplexity=5825.3, train_loss=8.669966

Batch 82570, train_perplexity=5820.28, train_loss=8.669104

Batch 82580, train_perplexity=5020.6978, train_loss=8.521324

Batch 82590, train_perplexity=5050.963, train_loss=8.527334

Batch 82600, train_perplexity=5877.251, train_loss=8.678844

Batch 82610, train_perplexity=6153.0303, train_loss=8.7247

Batch 82620, train_perplexity=5268.485, train_loss=8.569498

Batch 82630, train_perplexity=4919.2705, train_loss=8.500916

Batch 82640, train_perplexity=4855.2964, train_loss=8.487825

Batch 82650, train_perplexity=4283.297, train_loss=8.362478

Batch 82660, train_perplexity=6014.8135, train_loss=8.701981

Batch 82670, train_perplexity=5661.3438, train_loss=8.641417

Batch 82680, train_perplexity=6069.9136, train_loss=8.7111

Batch 82690, train_perplexity=4357.8496, train_loss=8.379734

Batch 82700, train_perplexity=5904.904, train_loss=8.683538

Batch 82710, train_perplexity=5632.242, train_loss=8.636263

Batch 82720, train_perplexity=6323.249, train_loss=8.751988

Batch 82730, train_perplexity=5188.041, train_loss=8.5541115

Batch 82740, train_perplexity=5857.1523, train_loss=8.675419

Batch 82750, train_perplexity=5059.1636, train_loss=8.528956

Batch 82760, train_perplexity=6914.3774, train_loss=8.841358

Batch 82770, train_perplexity=5097.2095, train_loss=8.5364485

Batch 82780, train_perplexity=5593.4556, train_loss=8.629353

Batch 82790, train_perplexity=6158.8247, train_loss=8.725641

Batch 82800, train_perplexity=6058.5205, train_loss=8.709221

Batch 82810, train_perplexity=6060.6064, train_loss=8.709565

Batch 82820, train_perplexity=5162.1406, train_loss=8.549107

Batch 82830, train_perplexity=4991.4033, train_loss=8.515472

Batch 82840, train_perplexity=5020.415, train_loss=8.521268

Batch 82850, train_perplexity=5258.526, train_loss=8.567606

Batch 82860, train_perplexity=4872.849, train_loss=8.491434

Batch 82870, train_perplexity=5441.5645, train_loss=8.601822

Batch 82880, train_perplexity=5156.492, train_loss=8.548012

Batch 82890, train_perplexity=5970.2485, train_loss=8.694544

Batch 82900, train_perplexity=5056.1343, train_loss=8.5283575

Batch 82910, train_perplexity=5394.169, train_loss=8.593074

Batch 82920, train_perplexity=5147.1816, train_loss=8.546205

Batch 82930, train_perplexity=4578.8623, train_loss=8.429206

Batch 82940, train_perplexity=4889.8125, train_loss=8.494909

Batch 82950, train_perplexity=5677.629, train_loss=8.644289

Batch 82960, train_perplexity=5868.8496, train_loss=8.677414

Batch 82970, train_perplexity=5577.454, train_loss=8.626488

Batch 82980, train_perplexity=5795.925, train_loss=8.66491

Batch 82990, train_perplexity=5207.2935, train_loss=8.557816

Batch 83000, train_perplexity=5441.8447, train_loss=8.601873

Batch 83010, train_perplexity=5705.507, train_loss=8.649187

Batch 83020, train_perplexity=5895.923, train_loss=8.682016

Batch 83030, train_perplexity=4924.772, train_loss=8.502033

Batch 83040, train_perplexity=5634.993, train_loss=8.636751

Batch 83050, train_perplexity=5691.0083, train_loss=8.646643

Batch 83060, train_perplexity=6098.8447, train_loss=8.715855

Batch 83070, train_perplexity=5310.272, train_loss=8.577398

Batch 83080, train_perplexity=6267.6123, train_loss=8.743151

Batch 83090, train_perplexity=5096.276, train_loss=8.536265

Batch 83100, train_perplexity=5803.2144, train_loss=8.666167

Batch 83110, train_perplexity=7307.831, train_loss=8.896702

Batch 83120, train_perplexity=5885.036, train_loss=8.680168

Batch 83130, train_perplexity=4982.1533, train_loss=8.5136175

Batch 83140, train_perplexity=5805.9546, train_loss=8.666639

Batch 83150, train_perplexity=5271.158, train_loss=8.570005

Batch 83160, train_perplexity=5069.939, train_loss=8.531084

Batch 83170, train_perplexity=4738.5327, train_loss=8.463483

Batch 83180, train_perplexity=5072.3184, train_loss=8.531553

Batch 83190, train_perplexity=5720.3804, train_loss=8.651791

Batch 83200, train_perplexity=7202.3184, train_loss=8.882158

Batch 83210, train_perplexity=5251.9, train_loss=8.566345

Batch 83220, train_perplexity=5521.0137, train_loss=8.616317

Batch 83230, train_perplexity=6872.114, train_loss=8.835227

Batch 83240, train_perplexity=4284.9106, train_loss=8.362855

Batch 83250, train_perplexity=6783.1245, train_loss=8.822193

Batch 83260, train_perplexity=5561.52, train_loss=8.623627

Batch 83270, train_perplexity=4808.598, train_loss=8.478161

Batch 83280, train_perplexity=5471.606, train_loss=8.607327

Batch 83290, train_perplexity=5662.3047, train_loss=8.641586

Batch 83300, train_perplexity=6395.522, train_loss=8.763353

Batch 83310, train_perplexity=4599.3403, train_loss=8.433668

Batch 83320, train_perplexity=6011.5674, train_loss=8.701441

Batch 83330, train_perplexity=6144.6626, train_loss=8.723339

Batch 83340, train_perplexity=6780.809, train_loss=8.821852

Batch 83350, train_perplexity=6226.736, train_loss=8.736608

Batch 83360, train_perplexity=5209.042, train_loss=8.558151

Batch 83370, train_perplexity=5927.382, train_loss=8.687338

Batch 83380, train_perplexity=5405.2817, train_loss=8.595132

Batch 83390, train_perplexity=5445.936, train_loss=8.602625

Batch 83400, train_perplexity=5269.57, train_loss=8.569704

Batch 83410, train_perplexity=6980.1406, train_loss=8.850824

Batch 83420, train_perplexity=6076.997, train_loss=8.712266

Batch 83430, train_perplexity=5801.975, train_loss=8.665954

Batch 83440, train_perplexity=4692.2285, train_loss=8.453663

Batch 83450, train_perplexity=5030.902, train_loss=8.523355

Batch 83460, train_perplexity=4955.1772, train_loss=8.508188

Batch 83470, train_perplexity=6143.7954, train_loss=8.723198

Batch 83480, train_perplexity=4571.877, train_loss=8.427679
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 83490, train_perplexity=5373.713, train_loss=8.589274

Batch 83500, train_perplexity=4831.204, train_loss=8.482851

Batch 83510, train_perplexity=5570.7246, train_loss=8.62528

Batch 83520, train_perplexity=5525.122, train_loss=8.617061

Batch 83530, train_perplexity=6274.083, train_loss=8.744183

Batch 83540, train_perplexity=6460.1445, train_loss=8.773407

Batch 83550, train_perplexity=7247.968, train_loss=8.888476

Batch 83560, train_perplexity=5791.9907, train_loss=8.664231

Batch 83570, train_perplexity=5872.926, train_loss=8.678108

Batch 83580, train_perplexity=5349.7773, train_loss=8.58481

Batch 83590, train_perplexity=5134.4883, train_loss=8.5437355

Batch 83600, train_perplexity=5161.8745, train_loss=8.549055

Batch 83610, train_perplexity=5027.233, train_loss=8.522625

Batch 83620, train_perplexity=5354.688, train_loss=8.585728

Batch 83630, train_perplexity=5178.6143, train_loss=8.552293

Batch 83640, train_perplexity=5065.8066, train_loss=8.530269

Batch 83650, train_perplexity=5565.547, train_loss=8.624351

Batch 83660, train_perplexity=4679.3984, train_loss=8.450925

Batch 83670, train_perplexity=6938.9243, train_loss=8.844902

Batch 83680, train_perplexity=5539.605, train_loss=8.6196785

Batch 83690, train_perplexity=5560.915, train_loss=8.623518

Batch 83700, train_perplexity=6004.886, train_loss=8.700329

Batch 83710, train_perplexity=5306.0093, train_loss=8.576595

Batch 83720, train_perplexity=5674.9766, train_loss=8.643822

Batch 83730, train_perplexity=5089.792, train_loss=8.534992

Batch 83740, train_perplexity=6058.4277, train_loss=8.709206

Batch 83750, train_perplexity=5065.7, train_loss=8.530248

Batch 83760, train_perplexity=5270.4946, train_loss=8.56988

Batch 83770, train_perplexity=4786.933, train_loss=8.473645

Batch 83780, train_perplexity=4236.553, train_loss=8.351505

Batch 83790, train_perplexity=5843.216, train_loss=8.673037

Batch 83800, train_perplexity=5268.2236, train_loss=8.569448

Batch 83810, train_perplexity=4911.9385, train_loss=8.499424

Batch 83820, train_perplexity=5148.9785, train_loss=8.546554

Batch 83830, train_perplexity=5969.52, train_loss=8.694422

Batch 83840, train_perplexity=4626.7656, train_loss=8.439613

Batch 83850, train_perplexity=5971.0684, train_loss=8.694681

Batch 83860, train_perplexity=4987.735, train_loss=8.514737

Batch 83870, train_perplexity=5810.364, train_loss=8.667398

Batch 83880, train_perplexity=5428.7925, train_loss=8.599472

Batch 83890, train_perplexity=5615.048, train_loss=8.633205

Batch 83900, train_perplexity=4909.7515, train_loss=8.498979

Batch 83910, train_perplexity=5517.303, train_loss=8.615644

Batch 83920, train_perplexity=5309.2993, train_loss=8.577215

Batch 83930, train_perplexity=5470.01, train_loss=8.607036

Batch 83940, train_perplexity=4800.5156, train_loss=8.476479

Batch 83950, train_perplexity=6178.1084, train_loss=8.728767

Batch 83960, train_perplexity=5554.788, train_loss=8.622416

Batch 83970, train_perplexity=4535.2817, train_loss=8.419642

Batch 83980, train_perplexity=5619.044, train_loss=8.633917

Batch 83990, train_perplexity=5230.4272, train_loss=8.562248

Batch 84000, train_perplexity=5092.5107, train_loss=8.535526

Batch 84010, train_perplexity=5012.354, train_loss=8.519661

Batch 84020, train_perplexity=5114.876, train_loss=8.539908

Batch 84030, train_perplexity=7336.2095, train_loss=8.900578

Batch 84040, train_perplexity=5937.09, train_loss=8.688974

Batch 84050, train_perplexity=5160.6885, train_loss=8.548825

Batch 84060, train_perplexity=6224.005, train_loss=8.736169

Batch 84070, train_perplexity=5303.7983, train_loss=8.576179

Batch 84080, train_perplexity=5665.049, train_loss=8.642071

Batch 84090, train_perplexity=5899.9614, train_loss=8.682701

Batch 84100, train_perplexity=5300.658, train_loss=8.575586

Batch 84110, train_perplexity=6058.0986, train_loss=8.709151

Batch 84120, train_perplexity=5294.707, train_loss=8.574463

Batch 84130, train_perplexity=5232.747, train_loss=8.562692

Batch 84140, train_perplexity=4852.7964, train_loss=8.48731

Batch 84150, train_perplexity=5728.1436, train_loss=8.653147

Batch 84160, train_perplexity=5243.5625, train_loss=8.564756

Batch 84170, train_perplexity=6063.2197, train_loss=8.709996

Batch 84180, train_perplexity=5197.0835, train_loss=8.555853

Batch 84190, train_perplexity=5355.168, train_loss=8.585817

Batch 84200, train_perplexity=5205.546, train_loss=8.55748

Batch 84210, train_perplexity=5721.963, train_loss=8.652067

Batch 84220, train_perplexity=6984.6353, train_loss=8.851468

Batch 84230, train_perplexity=5501.9395, train_loss=8.612856

Batch 84240, train_perplexity=5445.7695, train_loss=8.602594

Batch 84250, train_perplexity=4902.004, train_loss=8.497399

Batch 84260, train_perplexity=5517.9136, train_loss=8.615755

Batch 84270, train_perplexity=5405.906, train_loss=8.595247

Batch 84280, train_perplexity=5382.848, train_loss=8.590973

Batch 84290, train_perplexity=5458.8315, train_loss=8.60499

Batch 84300, train_perplexity=6142.9634, train_loss=8.7230625

Batch 84310, train_perplexity=5430.5327, train_loss=8.5997925

Batch 84320, train_perplexity=6712.8125, train_loss=8.811773

Batch 84330, train_perplexity=5304.274, train_loss=8.576268

Batch 84340, train_perplexity=5462.175, train_loss=8.605602

Batch 84350, train_perplexity=5011.484, train_loss=8.519487

Batch 84360, train_perplexity=7296.313, train_loss=8.895124

Batch 84370, train_perplexity=5642.796, train_loss=8.638135

Batch 84380, train_perplexity=6144.358, train_loss=8.7232895

Batch 84390, train_perplexity=5403.782, train_loss=8.594854

Batch 84400, train_perplexity=5349.497, train_loss=8.584758

Batch 84410, train_perplexity=6034.0947, train_loss=8.705181

Batch 84420, train_perplexity=5332.2456, train_loss=8.581528

Batch 84430, train_perplexity=5529.471, train_loss=8.617847

Batch 84440, train_perplexity=5424.7354, train_loss=8.598724

Batch 84450, train_perplexity=6308.9854, train_loss=8.74973

Batch 84460, train_perplexity=6146.872, train_loss=8.723699

Batch 84470, train_perplexity=5368.4116, train_loss=8.588287

Batch 84480, train_perplexity=5707.5312, train_loss=8.649542

Batch 84490, train_perplexity=5195.6167, train_loss=8.555571

Batch 84500, train_perplexity=7481.117, train_loss=8.920137

Batch 84510, train_perplexity=6330.1816, train_loss=8.753084

Batch 84520, train_perplexity=4892.145, train_loss=8.495386

Batch 84530, train_perplexity=5399.3057, train_loss=8.594026

Batch 84540, train_perplexity=5457.124, train_loss=8.604677

Batch 84550, train_perplexity=4962.6875, train_loss=8.509703

Batch 84560, train_perplexity=6242.766, train_loss=8.739179

Batch 84570, train_perplexity=4623.405, train_loss=8.438887

Batch 84580, train_perplexity=5135.9087, train_loss=8.544012

Batch 84590, train_perplexity=4947.037, train_loss=8.506544

Batch 84600, train_perplexity=6603.7563, train_loss=8.795394

Batch 84610, train_perplexity=5889.449, train_loss=8.680918

Batch 84620, train_perplexity=5196.563, train_loss=8.555753

Batch 84630, train_perplexity=5848.579, train_loss=8.673954

Batch 84640, train_perplexity=5591.088, train_loss=8.628929

Batch 84650, train_perplexity=5093.5063, train_loss=8.535722

Batch 84660, train_perplexity=6101.183, train_loss=8.716238

Batch 84670, train_perplexity=4553.601, train_loss=8.423674

Batch 84680, train_perplexity=5330.278, train_loss=8.581159

Batch 84690, train_perplexity=5555.519, train_loss=8.622547

Batch 84700, train_perplexity=5220.495, train_loss=8.560348

Batch 84710, train_perplexity=5562.1245, train_loss=8.623735

Batch 84720, train_perplexity=4636.943, train_loss=8.441811

Batch 84730, train_perplexity=5040.867, train_loss=8.525333

Batch 84740, train_perplexity=5892.989, train_loss=8.681519

Batch 84750, train_perplexity=6026.285, train_loss=8.703886

Batch 84760, train_perplexity=5115.8076, train_loss=8.540091

Batch 84770, train_perplexity=5512.2334, train_loss=8.614725

Batch 84780, train_perplexity=6654.497, train_loss=8.803048

Batch 84790, train_perplexity=6407.592, train_loss=8.765239

Batch 84800, train_perplexity=4996.2183, train_loss=8.516437

Batch 84810, train_perplexity=5916.183, train_loss=8.685447

Batch 84820, train_perplexity=4762.6216, train_loss=8.468554
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 84830, train_perplexity=5164.4746, train_loss=8.549559

Batch 84840, train_perplexity=5298.1416, train_loss=8.575111

Batch 84850, train_perplexity=6064.5093, train_loss=8.710209

Batch 84860, train_perplexity=5355.699, train_loss=8.5859165

Batch 84870, train_perplexity=6522.9937, train_loss=8.783089

Batch 84880, train_perplexity=4522.247, train_loss=8.416764

Batch 84890, train_perplexity=4793.0728, train_loss=8.474927

Batch 84900, train_perplexity=6010.7305, train_loss=8.701302

Batch 84910, train_perplexity=5278.5986, train_loss=8.571416

Batch 84920, train_perplexity=5535.5703, train_loss=8.61895

Batch 84930, train_perplexity=5222.268, train_loss=8.560687

Batch 84940, train_perplexity=6301.3125, train_loss=8.748513

Batch 84950, train_perplexity=6056.013, train_loss=8.708807

Batch 84960, train_perplexity=4404.9736, train_loss=8.39049

Batch 84970, train_perplexity=5223.3735, train_loss=8.560899

Batch 84980, train_perplexity=6083.196, train_loss=8.713285

Batch 84990, train_perplexity=5210.4185, train_loss=8.558415

Batch 85000, train_perplexity=4870.1177, train_loss=8.490873

Batch 85010, train_perplexity=5615.7334, train_loss=8.6333275

Batch 85020, train_perplexity=5249.797, train_loss=8.565945

Batch 85030, train_perplexity=6671.431, train_loss=8.80559

Batch 85040, train_perplexity=4075.87, train_loss=8.3128395

Batch 85050, train_perplexity=4371.5693, train_loss=8.382877

Batch 85060, train_perplexity=5844.5923, train_loss=8.673272

Batch 85070, train_perplexity=4590.252, train_loss=8.43169

Batch 85080, train_perplexity=5269.2485, train_loss=8.569643

Batch 85090, train_perplexity=5352.2476, train_loss=8.585272

Batch 85100, train_perplexity=4523.558, train_loss=8.417054

Batch 85110, train_perplexity=5561.1167, train_loss=8.623554

Batch 85120, train_perplexity=5266.1235, train_loss=8.56905

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Batch 85130, train_perplexity=4884.434, train_loss=8.493809

Batch 85140, train_perplexity=6125.127, train_loss=8.720155

Batch 85150, train_perplexity=6113.9277, train_loss=8.718325

Batch 85160, train_perplexity=5008.2544, train_loss=8.518843

Batch 85170, train_perplexity=5364.921, train_loss=8.587637

Batch 85180, train_perplexity=5853.752, train_loss=8.674838

Batch 85190, train_perplexity=6293.199, train_loss=8.747225

Batch 85200, train_perplexity=5047.958, train_loss=8.526739

Batch 85210, train_perplexity=5486.5083, train_loss=8.610047

Batch 85220, train_perplexity=5618.326, train_loss=8.633789

Batch 85230, train_perplexity=5114.705, train_loss=8.539875

Batch 85240, train_perplexity=5202.573, train_loss=8.556909

Batch 85250, train_perplexity=5014.759, train_loss=8.520141

Batch 85260, train_perplexity=4586.5938, train_loss=8.430893

Batch 85270, train_perplexity=5522.0093, train_loss=8.616497

Batch 85280, train_perplexity=5220.59, train_loss=8.560366

Batch 85290, train_perplexity=4431.0586, train_loss=8.396394

Batch 85300, train_perplexity=5676.5464, train_loss=8.644098

Batch 85310, train_perplexity=5413.3345, train_loss=8.596621

Batch 85320, train_perplexity=6611.6777, train_loss=8.796593

Batch 85330, train_perplexity=6361.7183, train_loss=8.758054

Batch 85340, train_perplexity=5312.531, train_loss=8.577824

Batch 85350, train_perplexity=5496.4434, train_loss=8.611856

Batch 85360, train_perplexity=5359.3726, train_loss=8.586602

Batch 85370, train_perplexity=5051.6377, train_loss=8.527468

Batch 85380, train_perplexity=6190.1157, train_loss=8.730709

Batch 85390, train_perplexity=5973.8706, train_loss=8.69515

Batch 85400, train_perplexity=5850.755, train_loss=8.674326

Batch 85410, train_perplexity=4486.8877, train_loss=8.408915

Batch 85420, train_perplexity=5394.8223, train_loss=8.593195

Batch 85430, train_perplexity=5219.3354, train_loss=8.560125

Batch 85440, train_perplexity=5110.4927, train_loss=8.539051

Batch 85450, train_perplexity=6167.5117, train_loss=8.727051

Batch 85460, train_perplexity=5304.785, train_loss=8.5763645

Batch 85470, train_perplexity=6449.9194, train_loss=8.771823

Batch 85480, train_perplexity=5655.5806, train_loss=8.640398

Batch 85490, train_perplexity=4508.7554, train_loss=8.413776

Batch 85500, train_perplexity=5006.5063, train_loss=8.518494

Batch 85510, train_perplexity=5015.5674, train_loss=8.520302

Batch 85520, train_perplexity=5127.6475, train_loss=8.542402

Batch 85530, train_perplexity=6038.942, train_loss=8.705984

Batch 85540, train_perplexity=5330.7964, train_loss=8.581256

Batch 85550, train_perplexity=5052.514, train_loss=8.527641

Batch 85560, train_perplexity=5290.76, train_loss=8.573717

Batch 85570, train_perplexity=5003.2417, train_loss=8.517841

Batch 85580, train_perplexity=5212.451, train_loss=8.558805

Batch 85590, train_perplexity=4767.6157, train_loss=8.469602

Batch 85600, train_perplexity=5977.535, train_loss=8.695764

Batch 85610, train_perplexity=5178.985, train_loss=8.552364

Batch 85620, train_perplexity=5768.3037, train_loss=8.660133

Batch 85630, train_perplexity=5021.4927, train_loss=8.521482

Batch 85640, train_perplexity=6456.5723, train_loss=8.772854

Batch 85650, train_perplexity=5180.2397, train_loss=8.552607

Batch 85660, train_perplexity=4643.6255, train_loss=8.443251

Batch 85670, train_perplexity=6437.8257, train_loss=8.769946

Batch 85680, train_perplexity=4377.427, train_loss=8.384216

Batch 85690, train_perplexity=6233.5034, train_loss=8.737694

Batch 85700, train_perplexity=5016.567, train_loss=8.520501

Batch 85710, train_perplexity=6134.03, train_loss=8.721607

Batch 85720, train_perplexity=6728.708, train_loss=8.814138

Batch 85730, train_perplexity=5003.361, train_loss=8.517865

Batch 85740, train_perplexity=5299.1826, train_loss=8.575308

Batch 85750, train_perplexity=4563.361, train_loss=8.425815

Batch 85760, train_perplexity=5441.035, train_loss=8.601725

Batch 85770, train_perplexity=5609.8613, train_loss=8.632281

Batch 85780, train_perplexity=5598.3867, train_loss=8.630234

Batch 85790, train_perplexity=4699.358, train_loss=8.455181

Batch 85800, train_perplexity=6305.4424, train_loss=8.749168

Batch 85810, train_perplexity=4848.6514, train_loss=8.486456

Batch 85820, train_perplexity=4721.306, train_loss=8.459841

Batch 85830, train_perplexity=5805.3066, train_loss=8.666528

Batch 85840, train_perplexity=5614.0093, train_loss=8.63302

Batch 85850, train_perplexity=4377.669, train_loss=8.384272

Batch 85860, train_perplexity=6291.1587, train_loss=8.746901

Batch 85870, train_perplexity=6615.8975, train_loss=8.797231

Batch 85880, train_perplexity=5079.1387, train_loss=8.532897

Batch 85890, train_perplexity=5383.115, train_loss=8.5910225

Batch 85900, train_perplexity=5032.111, train_loss=8.523595

Batch 85910, train_perplexity=4276.0034, train_loss=8.360774

Batch 85920, train_perplexity=5141.9023, train_loss=8.545178

Batch 85930, train_perplexity=5518.0874, train_loss=8.615787

Batch 85940, train_perplexity=4909.925, train_loss=8.499014

Batch 85950, train_perplexity=4547.5166, train_loss=8.422337

Batch 85960, train_perplexity=5183.852, train_loss=8.553304

Batch 85970, train_perplexity=6686.4375, train_loss=8.807837

Batch 85980, train_perplexity=4884.4204, train_loss=8.493806

Batch 85990, train_perplexity=5898.42, train_loss=8.68244

Batch 86000, train_perplexity=6150.144, train_loss=8.724231

Batch 86010, train_perplexity=5627.764, train_loss=8.635468

Batch 86020, train_perplexity=5399.2695, train_loss=8.594019

Batch 86030, train_perplexity=4652.278, train_loss=8.445112

Batch 86040, train_perplexity=5048.565, train_loss=8.526859

Batch 86050, train_perplexity=5853.785, train_loss=8.674844

Batch 86060, train_perplexity=5916.042, train_loss=8.685423

Batch 86070, train_perplexity=4588.6675, train_loss=8.431345

Batch 86080, train_perplexity=5807.2617, train_loss=8.666864

Batch 86090, train_perplexity=5756.84, train_loss=8.658144
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86100, train_perplexity=4914.019, train_loss=8.499847

Batch 86110, train_perplexity=4254.9033, train_loss=8.355827

Batch 86120, train_perplexity=5463.842, train_loss=8.605907

Batch 86130, train_perplexity=5249.106, train_loss=8.565813

Batch 86140, train_perplexity=5478.352, train_loss=8.60856

Batch 86150, train_perplexity=6253.486, train_loss=8.740894

Batch 86160, train_perplexity=5298.091, train_loss=8.575102

Batch 86170, train_perplexity=5825.5166, train_loss=8.670003

Batch 86180, train_perplexity=5207.1147, train_loss=8.557781

Batch 86190, train_perplexity=4269.182, train_loss=8.359178

Batch 86200, train_perplexity=4957.3564, train_loss=8.508628

Batch 86210, train_perplexity=6463.8794, train_loss=8.773985

Batch 86220, train_perplexity=5342.93, train_loss=8.583529

Batch 86230, train_perplexity=5742.7803, train_loss=8.655699

Batch 86240, train_perplexity=5853.601, train_loss=8.674812

Batch 86250, train_perplexity=5973.2954, train_loss=8.695054

Batch 86260, train_perplexity=5056.4673, train_loss=8.528423

Batch 86270, train_perplexity=5937.8203, train_loss=8.689097

Batch 86280, train_perplexity=5316.043, train_loss=8.578485

Batch 86290, train_perplexity=5669.0913, train_loss=8.642784

Batch 86300, train_perplexity=6895.9253, train_loss=8.838686

Batch 86310, train_perplexity=5280.0786, train_loss=8.571696

Batch 86320, train_perplexity=4546.1855, train_loss=8.422044

Batch 86330, train_perplexity=5557.6387, train_loss=8.622929

Batch 86340, train_perplexity=5537.8516, train_loss=8.619362

Batch 86350, train_perplexity=4086.823, train_loss=8.315523

Batch 86360, train_perplexity=6009.4927, train_loss=8.701096

Batch 86370, train_perplexity=5321.0293, train_loss=8.579422

Batch 86380, train_perplexity=7158.2607, train_loss=8.876022

Batch 86390, train_perplexity=5401.072, train_loss=8.594353

Batch 86400, train_perplexity=4239.8433, train_loss=8.352282

Batch 86410, train_perplexity=5186.7993, train_loss=8.553872

Batch 86420, train_perplexity=5447.515, train_loss=8.602915

Batch 86430, train_perplexity=4440.7207, train_loss=8.398572

Batch 86440, train_perplexity=5304.5977, train_loss=8.576329

Batch 86450, train_perplexity=5734.265, train_loss=8.654215

Batch 86460, train_perplexity=4742.5654, train_loss=8.464334

Batch 86470, train_perplexity=4853.4536, train_loss=8.487446

Batch 86480, train_perplexity=6520.0703, train_loss=8.78264

Batch 86490, train_perplexity=5267.5903, train_loss=8.569328

Batch 86500, train_perplexity=5336.7324, train_loss=8.582369

Batch 86510, train_perplexity=4994.918, train_loss=8.516176

Batch 86520, train_perplexity=6465.9385, train_loss=8.774303

Batch 86530, train_perplexity=5995.445, train_loss=8.698755

Batch 86540, train_perplexity=5171.473, train_loss=8.550913

Batch 86550, train_perplexity=4514.2627, train_loss=8.414997

Batch 86560, train_perplexity=5306.4346, train_loss=8.576675

Batch 86570, train_perplexity=4580.8716, train_loss=8.429645

Batch 86580, train_perplexity=6780.525, train_loss=8.82181

Batch 86590, train_perplexity=5340.6733, train_loss=8.583107

Batch 86600, train_perplexity=6006.1123, train_loss=8.700533

Batch 86610, train_perplexity=4632.8325, train_loss=8.440924

Batch 86620, train_perplexity=5500.2812, train_loss=8.612555

Batch 86630, train_perplexity=5670.173, train_loss=8.642975

Batch 86640, train_perplexity=5972.5776, train_loss=8.694934

Batch 86650, train_perplexity=5459.7007, train_loss=8.605149

Batch 86660, train_perplexity=4917.507, train_loss=8.500557

Batch 86670, train_perplexity=5987.046, train_loss=8.697353

Batch 86680, train_perplexity=5439.126, train_loss=8.601374

Batch 86690, train_perplexity=4872.9097, train_loss=8.4914465

Batch 86700, train_perplexity=5465.197, train_loss=8.606155

Batch 86710, train_perplexity=6458.7896, train_loss=8.773197

Batch 86720, train_perplexity=6200.053, train_loss=8.732313

Batch 86730, train_perplexity=5549.5723, train_loss=8.621476

Batch 86740, train_perplexity=5052.3843, train_loss=8.527616

Batch 86750, train_perplexity=6209.2725, train_loss=8.733799

Batch 86760, train_perplexity=5490.659, train_loss=8.610804

Batch 86770, train_perplexity=5945.317, train_loss=8.690359

Batch 86780, train_perplexity=6195.7266, train_loss=8.731615

Batch 86790, train_perplexity=5785.658, train_loss=8.663137

Batch 86800, train_perplexity=5455.0947, train_loss=8.604305

Batch 86810, train_perplexity=5311.234, train_loss=8.5775795

Batch 86820, train_perplexity=6083.863, train_loss=8.713395

Batch 86830, train_perplexity=5200.7974, train_loss=8.556567

Batch 86840, train_perplexity=4959.4604, train_loss=8.509052

Batch 86850, train_perplexity=5090.53, train_loss=8.535137

Batch 86860, train_perplexity=4822.3843, train_loss=8.481024

Batch 86870, train_perplexity=5937.3955, train_loss=8.689026

Batch 86880, train_perplexity=5632.339, train_loss=8.63628

Batch 86890, train_perplexity=4787.906, train_loss=8.473848

Batch 86900, train_perplexity=5021.0137, train_loss=8.521387

Batch 86910, train_perplexity=5895.1133, train_loss=8.681879

Batch 86920, train_perplexity=5892.646, train_loss=8.68146

Batch 86930, train_perplexity=5606.1606, train_loss=8.631621

Batch 86940, train_perplexity=5568.3447, train_loss=8.624853

Batch 86950, train_perplexity=5842.029, train_loss=8.672833

Batch 86960, train_perplexity=5532.8735, train_loss=8.618463

Batch 86970, train_perplexity=5731.9688, train_loss=8.653814

Batch 86980, train_perplexity=5119.317, train_loss=8.540776

Batch 86990, train_perplexity=5103.966, train_loss=8.537773

Batch 87000, train_perplexity=5428.9375, train_loss=8.599499

Batch 87010, train_perplexity=5577.593, train_loss=8.626513

Batch 87020, train_perplexity=6529.8027, train_loss=8.784132

Batch 87030, train_perplexity=5574.455, train_loss=8.62595

Batch 87040, train_perplexity=5722.7812, train_loss=8.65221

Batch 87050, train_perplexity=3686.581, train_loss=8.212455

Batch 87060, train_perplexity=5398.7856, train_loss=8.593929

Batch 87070, train_perplexity=4984.2207, train_loss=8.514032

Batch 87080, train_perplexity=5706.2197, train_loss=8.649312

Batch 87090, train_perplexity=5710.025, train_loss=8.649979

Batch 87100, train_perplexity=6112.482, train_loss=8.718088

Batch 87110, train_perplexity=5948.056, train_loss=8.69082

Batch 87120, train_perplexity=5012.7363, train_loss=8.519737

Batch 87130, train_perplexity=5347.421, train_loss=8.58437

Batch 87140, train_perplexity=5229.534, train_loss=8.5620775

Batch 87150, train_perplexity=4677.0967, train_loss=8.450433

Batch 87160, train_perplexity=4627.5156, train_loss=8.439775

Batch 87170, train_perplexity=4877.21, train_loss=8.492329

Batch 87180, train_perplexity=5334.4326, train_loss=8.581938

Batch 87190, train_perplexity=5396.4897, train_loss=8.593504

Batch 87200, train_perplexity=6539.2065, train_loss=8.785571

Batch 87210, train_perplexity=5196.92, train_loss=8.555821

Batch 87220, train_perplexity=5126.5425, train_loss=8.542187

Batch 87230, train_perplexity=6634.0806, train_loss=8.799975

Batch 87240, train_perplexity=5360.523, train_loss=8.586817

Batch 87250, train_perplexity=5542.6914, train_loss=8.620235

Batch 87260, train_perplexity=4754.5347, train_loss=8.466854

Batch 87270, train_perplexity=5714.7207, train_loss=8.650801

Batch 87280, train_perplexity=6314.7456, train_loss=8.750643

Batch 87290, train_perplexity=5012.5166, train_loss=8.519693

Batch 87300, train_perplexity=5823.2, train_loss=8.669605

Batch 87310, train_perplexity=6064.0464, train_loss=8.710133

Batch 87320, train_perplexity=5202.5483, train_loss=8.556904

Batch 87330, train_perplexity=4922.5464, train_loss=8.501581

Batch 87340, train_perplexity=5457.0615, train_loss=8.604666

Batch 87350, train_perplexity=6448.105, train_loss=8.771542

Batch 87360, train_perplexity=6073.9204, train_loss=8.71176

Batch 87370, train_perplexity=5782.1445, train_loss=8.66253

Batch 87380, train_perplexity=4917.0474, train_loss=8.5004635

Batch 87390, train_perplexity=6294.6875, train_loss=8.747461

Batch 87400, train_perplexity=5442.5815, train_loss=8.602009

Batch 87410, train_perplexity=5109.313, train_loss=8.53882

Batch 87420, train_perplexity=4553.297, train_loss=8.423607

Batch 87430, train_perplexity=5190.2188, train_loss=8.554531
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 87440, train_perplexity=4592.0864, train_loss=8.43209

Batch 87450, train_perplexity=5496.81, train_loss=8.611923

Batch 87460, train_perplexity=6304.1313, train_loss=8.7489605

Batch 87470, train_perplexity=4906.999, train_loss=8.498418

Batch 87480, train_perplexity=6468.171, train_loss=8.774649

Batch 87490, train_perplexity=4975.245, train_loss=8.51223

Batch 87500, train_perplexity=4821.0737, train_loss=8.480752

Batch 87510, train_perplexity=5539.9116, train_loss=8.619734

Batch 87520, train_perplexity=4923.57, train_loss=8.501789

Batch 87530, train_perplexity=4726.8657, train_loss=8.461018

Batch 87540, train_perplexity=4976.0425, train_loss=8.51239

Batch 87550, train_perplexity=4967.7207, train_loss=8.510716

Batch 87560, train_perplexity=5949.9907, train_loss=8.691145

Batch 87570, train_perplexity=4767.425, train_loss=8.469562

Batch 87580, train_perplexity=5746.6704, train_loss=8.656376

Batch 87590, train_perplexity=6633.992, train_loss=8.799962

Batch 87600, train_perplexity=4741.367, train_loss=8.464081

Batch 87610, train_perplexity=5442.961, train_loss=8.602078

Batch 87620, train_perplexity=5416.7017, train_loss=8.597242

Batch 87630, train_perplexity=5429.7144, train_loss=8.599642

Batch 87640, train_perplexity=5318.092, train_loss=8.57887

Batch 87650, train_perplexity=4739.3643, train_loss=8.463658

Batch 87660, train_perplexity=5798.883, train_loss=8.665421

Batch 87670, train_perplexity=5414.0005, train_loss=8.596744

Batch 87680, train_perplexity=5567.4634, train_loss=8.624695

Batch 87690, train_perplexity=5970.989, train_loss=8.694668

Batch 87700, train_perplexity=5937.956, train_loss=8.68912

Batch 87710, train_perplexity=5444.9595, train_loss=8.602446

Batch 87720, train_perplexity=5190.2188, train_loss=8.554531

Batch 87730, train_perplexity=4896.7754, train_loss=8.496332

Batch 87740, train_perplexity=6034.5205, train_loss=8.705252

Batch 87750, train_perplexity=5560.7666, train_loss=8.623491

Batch 87760, train_perplexity=5076.199, train_loss=8.532318

Batch 87770, train_perplexity=6729.497, train_loss=8.814256

Batch 87780, train_perplexity=5602.4995, train_loss=8.630968

Batch 87790, train_perplexity=5964.91, train_loss=8.693649

Batch 87800, train_perplexity=8849.529, train_loss=9.0881195

Batch 87810, train_perplexity=5962.465, train_loss=8.693239

Batch 87820, train_perplexity=5276.062, train_loss=8.570935

Batch 87830, train_perplexity=5739.5283, train_loss=8.655132

Batch 87840, train_perplexity=4887.435, train_loss=8.494423

Batch 87850, train_perplexity=4598.1387, train_loss=8.433407

Batch 87860, train_perplexity=5548.3184, train_loss=8.62125

Batch 87870, train_perplexity=5286.4727, train_loss=8.5729065

Batch 87880, train_perplexity=5095.678, train_loss=8.536148

Batch 87890, train_perplexity=4781.827, train_loss=8.472578

Batch 87900, train_perplexity=5720.7134, train_loss=8.651849

Batch 87910, train_perplexity=5386.792, train_loss=8.591705

Batch 87920, train_perplexity=5219.594, train_loss=8.560175

Batch 87930, train_perplexity=5604.9043, train_loss=8.631397

Batch 87940, train_perplexity=6023.2974, train_loss=8.70339

Batch 87950, train_perplexity=6295.48, train_loss=8.747587

Batch 87960, train_perplexity=5176.274, train_loss=8.551841

Batch 87970, train_perplexity=4989.471, train_loss=8.515085

Batch 87980, train_perplexity=6406.455, train_loss=8.765061

Batch 87990, train_perplexity=6498.226, train_loss=8.7792845

Batch 88000, train_perplexity=5764.2124, train_loss=8.659424

Batch 88010, train_perplexity=5058.888, train_loss=8.528902

Batch 88020, train_perplexity=4948.802, train_loss=8.506901

Batch 88030, train_perplexity=6293.667, train_loss=8.747299

Batch 88040, train_perplexity=5975.341, train_loss=8.695396

Batch 88050, train_perplexity=4233.633, train_loss=8.350816

Batch 88060, train_perplexity=5575.8696, train_loss=8.626204

Batch 88070, train_perplexity=4205.862, train_loss=8.344234

Batch 88080, train_perplexity=5268.1177, train_loss=8.569428

Batch 88090, train_perplexity=5303.3384, train_loss=8.576092

Batch 88100, train_perplexity=5115.056, train_loss=8.539944

Batch 88110, train_perplexity=5383.69, train_loss=8.591129

Batch 88120, train_perplexity=5566.6987, train_loss=8.6245575

Batch 88130, train_perplexity=6156.452, train_loss=8.725256

Batch 88140, train_perplexity=4719.357, train_loss=8.459428

Batch 88150, train_perplexity=5240.963, train_loss=8.5642605

Batch 88160, train_perplexity=5178.0415, train_loss=8.552182

Batch 88170, train_perplexity=5276.651, train_loss=8.571047

Batch 88180, train_perplexity=5139.412, train_loss=8.544694

Batch 88190, train_perplexity=5582.8716, train_loss=8.627459

Batch 88200, train_perplexity=5272.244, train_loss=8.570211

Batch 88210, train_perplexity=5998.3613, train_loss=8.699242

Batch 88220, train_perplexity=6506.2812, train_loss=8.780523

Batch 88230, train_perplexity=5582.76, train_loss=8.627439

Batch 88240, train_perplexity=5297.5454, train_loss=8.574999

Batch 88250, train_perplexity=5290.366, train_loss=8.573643

Batch 88260, train_perplexity=5825.933, train_loss=8.670074

Batch 88270, train_perplexity=5888.595, train_loss=8.680773

Batch 88280, train_perplexity=4917.9854, train_loss=8.500654

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Batch 88290, train_perplexity=4845.85, train_loss=8.485878

Batch 88300, train_perplexity=5326.5327, train_loss=8.580456

Batch 88310, train_perplexity=5666.394, train_loss=8.642308

Batch 88320, train_perplexity=5568.9077, train_loss=8.624954

Batch 88330, train_perplexity=5693.2773, train_loss=8.647041

Batch 88340, train_perplexity=4448.8467, train_loss=8.4004

Batch 88350, train_perplexity=5130.563, train_loss=8.542971

Batch 88360, train_perplexity=5193.516, train_loss=8.555166

Batch 88370, train_perplexity=5169.0815, train_loss=8.55045

Batch 88380, train_perplexity=4792.8896, train_loss=8.474889

Batch 88390, train_perplexity=4198.648, train_loss=8.342518

Batch 88400, train_perplexity=5340.653, train_loss=8.583103

Batch 88410, train_perplexity=4754.87, train_loss=8.466925

Batch 88420, train_perplexity=4414.3433, train_loss=8.392614

Batch 88430, train_perplexity=4847.274, train_loss=8.486172

Batch 88440, train_perplexity=5566.545, train_loss=8.62453

Batch 88450, train_perplexity=5911.9697, train_loss=8.684734

Batch 88460, train_perplexity=5415.6895, train_loss=8.597055

Batch 88470, train_perplexity=4468.994, train_loss=8.404919

Batch 88480, train_perplexity=4950.605, train_loss=8.507265

Batch 88490, train_perplexity=5092.326, train_loss=8.53549

Batch 88500, train_perplexity=6435.855, train_loss=8.76964

Batch 88510, train_perplexity=5595.9634, train_loss=8.629801

Batch 88520, train_perplexity=5320.684, train_loss=8.579357

Batch 88530, train_perplexity=5117.7886, train_loss=8.540478

Batch 88540, train_perplexity=5552.1875, train_loss=8.621947

Batch 88550, train_perplexity=4803.135, train_loss=8.477024

Batch 88560, train_perplexity=5491.5493, train_loss=8.610966

Batch 88570, train_perplexity=5700.3184, train_loss=8.648277

Batch 88580, train_perplexity=5221.2373, train_loss=8.56049

Batch 88590, train_perplexity=5225.541, train_loss=8.561314

Batch 88600, train_perplexity=5071.656, train_loss=8.531423

Batch 88610, train_perplexity=5218.7383, train_loss=8.560011

Batch 88620, train_perplexity=5618.8086, train_loss=8.633875

Batch 88630, train_perplexity=6570.2744, train_loss=8.790311

Batch 88640, train_perplexity=4923.41, train_loss=8.501757

Batch 88650, train_perplexity=6773.144, train_loss=8.820721

Batch 88660, train_perplexity=5473.4014, train_loss=8.607656

Batch 88670, train_perplexity=4927.4873, train_loss=8.502584

Batch 88680, train_perplexity=5985.0874, train_loss=8.697026

Batch 88690, train_perplexity=5829.123, train_loss=8.670622

Batch 88700, train_perplexity=4903.93, train_loss=8.497792

Batch 88710, train_perplexity=5445.271, train_loss=8.602503
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88720, train_perplexity=5401.2676, train_loss=8.594389

Batch 88730, train_perplexity=5430.408, train_loss=8.59977

Batch 88740, train_perplexity=5414.2847, train_loss=8.596796

Batch 88750, train_perplexity=6074.1753, train_loss=8.711802

Batch 88760, train_perplexity=4783.036, train_loss=8.472831

Batch 88770, train_perplexity=6897.622, train_loss=8.838932

Batch 88780, train_perplexity=4493.5894, train_loss=8.410407

Batch 88790, train_perplexity=5508.46, train_loss=8.61404

Batch 88800, train_perplexity=5229.9834, train_loss=8.562163

Batch 88810, train_perplexity=5210.2144, train_loss=8.558376

Batch 88820, train_perplexity=6009.16, train_loss=8.70104

Batch 88830, train_perplexity=5075.154, train_loss=8.532112

Batch 88840, train_perplexity=4928.592, train_loss=8.502809

Batch 88850, train_perplexity=5973.985, train_loss=8.695169

Batch 88860, train_perplexity=4908.2954, train_loss=8.498682

Batch 88870, train_perplexity=5165.5186, train_loss=8.549761

Batch 88880, train_perplexity=4677.2256, train_loss=8.45046

Batch 88890, train_perplexity=5043.5166, train_loss=8.525859

Batch 88900, train_perplexity=4974.1494, train_loss=8.51201

Batch 88910, train_perplexity=5893.028, train_loss=8.681525

Batch 88920, train_perplexity=5891.82, train_loss=8.68132

Batch 88930, train_perplexity=5461.315, train_loss=8.605445

Batch 88940, train_perplexity=5499.5103, train_loss=8.612414

Batch 88950, train_perplexity=5569.046, train_loss=8.624979

Batch 88960, train_perplexity=6598.374, train_loss=8.794579

Batch 88970, train_perplexity=4947.3296, train_loss=8.506603

Batch 88980, train_perplexity=6522.595, train_loss=8.783028

Batch 88990, train_perplexity=5447.5356, train_loss=8.602919

Batch 89000, train_perplexity=5159.074, train_loss=8.548512

Batch 89010, train_perplexity=6654.573, train_loss=8.80306

Batch 89020, train_perplexity=4428.8027, train_loss=8.3958845

Batch 89030, train_perplexity=5375.02, train_loss=8.589518

Batch 89040, train_perplexity=5721.4497, train_loss=8.651978

Batch 89050, train_perplexity=5875.6484, train_loss=8.678572

Batch 89060, train_perplexity=5430.9106, train_loss=8.599862

Batch 89070, train_perplexity=6346.042, train_loss=8.755587

Batch 89080, train_perplexity=4684.636, train_loss=8.452044

Batch 89090, train_perplexity=4669.876, train_loss=8.448888

Batch 89100, train_perplexity=4736.513, train_loss=8.463057

Batch 89110, train_perplexity=5349.5376, train_loss=8.584765

Batch 89120, train_perplexity=5499.935, train_loss=8.612492

Batch 89130, train_perplexity=5155.9707, train_loss=8.547911

Batch 89140, train_perplexity=3995.1948, train_loss=8.292848

Batch 89150, train_perplexity=5231.834, train_loss=8.562517

Batch 89160, train_perplexity=6424.1914, train_loss=8.767826

Batch 89170, train_perplexity=5661.835, train_loss=8.641503

Batch 89180, train_perplexity=5051.4595, train_loss=8.527432

Batch 89190, train_perplexity=4824.542, train_loss=8.481471

Batch 89200, train_perplexity=4855.2173, train_loss=8.487809

Batch 89210, train_perplexity=5097.224, train_loss=8.536451

Batch 89220, train_perplexity=4616.6284, train_loss=8.43742

Batch 89230, train_perplexity=4592.2183, train_loss=8.432118

Batch 89240, train_perplexity=6088.2217, train_loss=8.714111

Batch 89250, train_perplexity=5400.696, train_loss=8.594283

Batch 89260, train_perplexity=6273.915, train_loss=8.744156

Batch 89270, train_perplexity=5900.4116, train_loss=8.682777

Batch 89280, train_perplexity=4933.073, train_loss=8.503717

Batch 89290, train_perplexity=4781.5903, train_loss=8.472528

Batch 89300, train_perplexity=4561.3984, train_loss=8.4253845

Batch 89310, train_perplexity=5967.357, train_loss=8.694059

Batch 89320, train_perplexity=5460.867, train_loss=8.605363

Batch 89330, train_perplexity=4448.4565, train_loss=8.400312

Batch 89340, train_perplexity=5584.1816, train_loss=8.627693

Batch 89350, train_perplexity=5156.133, train_loss=8.547942

Batch 89360, train_perplexity=5970.0093, train_loss=8.694504

Batch 89370, train_perplexity=5040.511, train_loss=8.525263

Batch 89380, train_perplexity=4985.7896, train_loss=8.514347

Batch 89390, train_perplexity=4775.429, train_loss=8.471239

Batch 89400, train_perplexity=5914.6543, train_loss=8.685188

Batch 89410, train_perplexity=5036.168, train_loss=8.524401

Batch 89420, train_perplexity=5059.1973, train_loss=8.528963

Batch 89430, train_perplexity=4815.849, train_loss=8.479668

Batch 89440, train_perplexity=5546.382, train_loss=8.620901

Batch 89450, train_perplexity=5776.2417, train_loss=8.661509

Batch 89460, train_perplexity=4652.837, train_loss=8.445232

Batch 89470, train_perplexity=7190.2812, train_loss=8.880486

Batch 89480, train_perplexity=4719.8657, train_loss=8.459536

Batch 89490, train_perplexity=4498.5933, train_loss=8.41152

Batch 89500, train_perplexity=6537.3423, train_loss=8.785286

Batch 89510, train_perplexity=5118.511, train_loss=8.540619

Batch 89520, train_perplexity=4808.731, train_loss=8.4781885

Batch 89530, train_perplexity=5914.327, train_loss=8.685133

Batch 89540, train_perplexity=4877.68, train_loss=8.492425

Batch 89550, train_perplexity=5510.8457, train_loss=8.614473

Batch 89560, train_perplexity=5825.7554, train_loss=8.670044

Batch 89570, train_perplexity=5617.935, train_loss=8.633719

Batch 89580, train_perplexity=4951.918, train_loss=8.50753

Batch 89590, train_perplexity=5334.646, train_loss=8.581978

Batch 89600, train_perplexity=5025.79, train_loss=8.522338

Batch 89610, train_perplexity=5305.028, train_loss=8.57641

Batch 89620, train_perplexity=4571.031, train_loss=8.427494

Batch 89630, train_perplexity=5319.01, train_loss=8.579042

Batch 89640, train_perplexity=4840.3726, train_loss=8.484747

Batch 89650, train_perplexity=5199.5176, train_loss=8.556321

Batch 89660, train_perplexity=5144.5312, train_loss=8.54569

Batch 89670, train_perplexity=5788.992, train_loss=8.663713

Batch 89680, train_perplexity=5479.3447, train_loss=8.608741

Batch 89690, train_perplexity=6061.017, train_loss=8.709633

Batch 89700, train_perplexity=5581.019, train_loss=8.627127

Batch 89710, train_perplexity=5440.859, train_loss=8.601692

Batch 89720, train_perplexity=6378.704, train_loss=8.76072

Batch 89730, train_perplexity=5733.893, train_loss=8.65415

Batch 89740, train_perplexity=5703.8857, train_loss=8.648903

Batch 89750, train_perplexity=6267.606, train_loss=8.74315

Batch 89760, train_perplexity=5005.38, train_loss=8.518269

Batch 89770, train_perplexity=5008.751, train_loss=8.518942

Batch 89780, train_perplexity=5309.2236, train_loss=8.577201

Batch 89790, train_perplexity=5328.8496, train_loss=8.580891

Batch 89800, train_perplexity=4517.4194, train_loss=8.415696

Batch 89810, train_perplexity=4665.2466, train_loss=8.447896

Batch 89820, train_perplexity=5325.5474, train_loss=8.580271

Batch 89830, train_perplexity=4786.8965, train_loss=8.473638

Batch 89840, train_perplexity=6222.521, train_loss=8.73593

Batch 89850, train_perplexity=4623.8765, train_loss=8.438989

Batch 89860, train_perplexity=5039.1416, train_loss=8.524991

Batch 89870, train_perplexity=4803.5103, train_loss=8.477102

Batch 89880, train_perplexity=5236.3115, train_loss=8.563373

Batch 89890, train_perplexity=5250.558, train_loss=8.56609

Batch 89900, train_perplexity=6143.8423, train_loss=8.723206

Batch 89910, train_perplexity=6199.0127, train_loss=8.732145

Batch 89920, train_perplexity=6423.8486, train_loss=8.767773

Batch 89930, train_perplexity=5305.7363, train_loss=8.576544

Batch 89940, train_perplexity=5406.746, train_loss=8.595403

Batch 89950, train_perplexity=5070.2676, train_loss=8.531149

Batch 89960, train_perplexity=4504.191, train_loss=8.412764

Batch 89970, train_perplexity=5979.3823, train_loss=8.696073

Batch 89980, train_perplexity=5693.1904, train_loss=8.647026

Batch 89990, train_perplexity=5283.519, train_loss=8.572348

Batch 90000, train_perplexity=5472.994, train_loss=8.607581

Batch 90010, train_perplexity=5127.5156, train_loss=8.5423765

Batch 90020, train_perplexity=5474.273, train_loss=8.607815

Batch 90030, train_perplexity=5801.471, train_loss=8.665867

Batch 90040, train_perplexity=4659.9688, train_loss=8.446764

Batch 90050, train_perplexity=4995.8184, train_loss=8.516356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 90060, train_perplexity=6955.461, train_loss=8.847282

Batch 90070, train_perplexity=5966.731, train_loss=8.693954

Batch 90080, train_perplexity=4750.854, train_loss=8.46608

Batch 90090, train_perplexity=5290.139, train_loss=8.5736

Batch 90100, train_perplexity=4604.462, train_loss=8.434781

Batch 90110, train_perplexity=5700.492, train_loss=8.648308

Batch 90120, train_perplexity=6237.642, train_loss=8.738358

Batch 90130, train_perplexity=5212.769, train_loss=8.5588665

Batch 90140, train_perplexity=5674.587, train_loss=8.643753

Batch 90150, train_perplexity=5521.893, train_loss=8.616476

Batch 90160, train_perplexity=5219.9927, train_loss=8.560251

Batch 90170, train_perplexity=5366.538, train_loss=8.587938

Batch 90180, train_perplexity=5489.4604, train_loss=8.610585

Batch 90190, train_perplexity=5501.8447, train_loss=8.612839

Batch 90200, train_perplexity=4690.367, train_loss=8.453266

Batch 90210, train_perplexity=4561.594, train_loss=8.425427

Batch 90220, train_perplexity=4439.332, train_loss=8.398259

Batch 90230, train_perplexity=5237.9897, train_loss=8.563693

Batch 90240, train_perplexity=4433.7725, train_loss=8.397006

Batch 90250, train_perplexity=4490.573, train_loss=8.409736

Batch 90260, train_perplexity=4258.1426, train_loss=8.356588

Batch 90270, train_perplexity=6731.9556, train_loss=8.814621

Batch 90280, train_perplexity=5084.324, train_loss=8.533917

Batch 90290, train_perplexity=4954.8374, train_loss=8.50812

Batch 90300, train_perplexity=5549.176, train_loss=8.621405

Batch 90310, train_perplexity=6541.7886, train_loss=8.785966

Batch 90320, train_perplexity=5756.582, train_loss=8.658099

Batch 90330, train_perplexity=5071.385, train_loss=8.531369

Batch 90340, train_perplexity=5172.6025, train_loss=8.551131

Batch 90350, train_perplexity=4819.6167, train_loss=8.48045

Batch 90360, train_perplexity=5935.844, train_loss=8.688765

Batch 90370, train_perplexity=5222.99, train_loss=8.560825

Batch 90380, train_perplexity=4534.6025, train_loss=8.419493

Batch 90390, train_perplexity=5197.0933, train_loss=8.555855

Batch 90400, train_perplexity=5930.3784, train_loss=8.687843

Batch 90410, train_perplexity=5772.095, train_loss=8.66079

Batch 90420, train_perplexity=5315.4043, train_loss=8.578364

Batch 90430, train_perplexity=6394.601, train_loss=8.763209

Batch 90440, train_perplexity=5115.925, train_loss=8.540113

Batch 90450, train_perplexity=5462.05, train_loss=8.605579

Batch 90460, train_perplexity=5478.221, train_loss=8.608536

Batch 90470, train_perplexity=4916.635, train_loss=8.50038

Batch 90480, train_perplexity=4516.2217, train_loss=8.415431

Batch 90490, train_perplexity=5054.678, train_loss=8.5280695

Batch 90500, train_perplexity=4751.2075, train_loss=8.466154

Batch 90510, train_perplexity=5572.584, train_loss=8.625614

Batch 90520, train_perplexity=5706.4375, train_loss=8.64935

Batch 90530, train_perplexity=5962.908, train_loss=8.693314

Batch 90540, train_perplexity=5008.3594, train_loss=8.518864

Batch 90550, train_perplexity=4963.8706, train_loss=8.509941

Batch 90560, train_perplexity=6316.7935, train_loss=8.750967

Batch 90570, train_perplexity=4538.492, train_loss=8.42035

Batch 90580, train_perplexity=4629.14, train_loss=8.440126

Batch 90590, train_perplexity=6820.111, train_loss=8.827631

Batch 90600, train_perplexity=5357.354, train_loss=8.5862255

Batch 90610, train_perplexity=7687.0205, train_loss=8.9472885

Batch 90620, train_perplexity=7019.727, train_loss=8.85648

Batch 90630, train_perplexity=5777.3105, train_loss=8.661694

Batch 90640, train_perplexity=5203.5703, train_loss=8.5571

Batch 90650, train_perplexity=5507.6772, train_loss=8.613898

Batch 90660, train_perplexity=4628.8843, train_loss=8.440071

Batch 90670, train_perplexity=6081.2466, train_loss=8.712965

Batch 90680, train_perplexity=5335.196, train_loss=8.582081

Batch 90690, train_perplexity=5560.343, train_loss=8.623415

Batch 90700, train_perplexity=5258.952, train_loss=8.567687

Batch 90710, train_perplexity=5211.8843, train_loss=8.558697

Batch 90720, train_perplexity=5361.7397, train_loss=8.587044

Batch 90730, train_perplexity=5616.033, train_loss=8.633381

Batch 90740, train_perplexity=5092.229, train_loss=8.535471

Batch 90750, train_perplexity=4606.197, train_loss=8.435158

Batch 90760, train_perplexity=6538.1777, train_loss=8.785414

Batch 90770, train_perplexity=5506.5586, train_loss=8.613695

Batch 90780, train_perplexity=5631.619, train_loss=8.636152

Batch 90790, train_perplexity=5525.823, train_loss=8.6171875

Batch 90800, train_perplexity=5927.308, train_loss=8.6873255

Batch 90810, train_perplexity=5658.8823, train_loss=8.640982

Batch 90820, train_perplexity=4649.12, train_loss=8.444433

Batch 90830, train_perplexity=6335.859, train_loss=8.753981

Batch 90840, train_perplexity=6444.0723, train_loss=8.770916

Batch 90850, train_perplexity=6142.6353, train_loss=8.723009

Batch 90860, train_perplexity=5940.545, train_loss=8.689556

Batch 90870, train_perplexity=4948.2827, train_loss=8.506796

Batch 90880, train_perplexity=5857.7896, train_loss=8.675528

Batch 90890, train_perplexity=5240.9077, train_loss=8.56425

Batch 90900, train_perplexity=4850.1035, train_loss=8.486755

Batch 90910, train_perplexity=5652.987, train_loss=8.639939

Batch 90920, train_perplexity=5529.566, train_loss=8.617865

Batch 90930, train_perplexity=4550.662, train_loss=8.423028

Batch 90940, train_perplexity=5817.128, train_loss=8.668562

Batch 90950, train_perplexity=5637.67, train_loss=8.637226

Batch 90960, train_perplexity=6277.71, train_loss=8.7447605

Batch 90970, train_perplexity=5244.963, train_loss=8.565023

Batch 90980, train_perplexity=5897.087, train_loss=8.682214

Batch 90990, train_perplexity=4654.8296, train_loss=8.445661

Batch 91000, train_perplexity=5928.354, train_loss=8.687502

Batch 91010, train_perplexity=6180.1353, train_loss=8.729095

Batch 91020, train_perplexity=5221.481, train_loss=8.560536

Batch 91030, train_perplexity=5920.546, train_loss=8.686184

Batch 91040, train_perplexity=6128.171, train_loss=8.720652

Batch 91050, train_perplexity=4953.5615, train_loss=8.507862

Batch 91060, train_perplexity=5330.11, train_loss=8.581127

Batch 91070, train_perplexity=5533.3325, train_loss=8.618546

Batch 91080, train_perplexity=4758.1724, train_loss=8.467619

Batch 91090, train_perplexity=5674.6787, train_loss=8.643769

Batch 91100, train_perplexity=5909.0103, train_loss=8.684234

Batch 91110, train_perplexity=5664.1196, train_loss=8.641907

Batch 91120, train_perplexity=4953.8877, train_loss=8.507928

Batch 91130, train_perplexity=4681.903, train_loss=8.45146

Batch 91140, train_perplexity=4729.03, train_loss=8.461475

Batch 91150, train_perplexity=5554.555, train_loss=8.622374

Batch 91160, train_perplexity=6757.5693, train_loss=8.8184185

Batch 91170, train_perplexity=6189.2246, train_loss=8.730565

Batch 91180, train_perplexity=5492.7124, train_loss=8.611177

Batch 91190, train_perplexity=5349.4917, train_loss=8.584757

Batch 91200, train_perplexity=5307.2744, train_loss=8.576834

Batch 91210, train_perplexity=5748.0625, train_loss=8.656618

Batch 91220, train_perplexity=4476.604, train_loss=8.40662

Batch 91230, train_perplexity=4998.735, train_loss=8.51694

Batch 91240, train_perplexity=5732.734, train_loss=8.653948

Batch 91250, train_perplexity=4738.42, train_loss=8.463459

Batch 91260, train_perplexity=6134.568, train_loss=8.721695

Batch 91270, train_perplexity=5293.9897, train_loss=8.574327

Batch 91280, train_perplexity=4807.03, train_loss=8.477835

Batch 91290, train_perplexity=5010.8057, train_loss=8.519352

Batch 91300, train_perplexity=5300.911, train_loss=8.575634

Batch 91310, train_perplexity=5406.7666, train_loss=8.595407

Batch 91320, train_perplexity=5907.23, train_loss=8.683932

Batch 91330, train_perplexity=6947.2275, train_loss=8.846098

Batch 91340, train_perplexity=4129.7793, train_loss=8.325979

Batch 91350, train_perplexity=4832.7847, train_loss=8.483178

Batch 91360, train_perplexity=5141.0835, train_loss=8.545019

Batch 91370, train_perplexity=5170.605, train_loss=8.550745

Batch 91380, train_perplexity=5793.6865, train_loss=8.664524

Batch 91390, train_perplexity=4831.96, train_loss=8.483007
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 91400, train_perplexity=5065.2803, train_loss=8.530165

Batch 91410, train_perplexity=4802.6675, train_loss=8.476927

Batch 91420, train_perplexity=5225.5957, train_loss=8.561324

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100
Loaded 306536 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100
Loaded 306536 sentences.
Finished loading
Batch 91430, train_perplexity=6774.7656, train_loss=8.82096

Batch 91440, train_perplexity=6252.3887, train_loss=8.740719

Batch 91450, train_perplexity=4626.673, train_loss=8.439593

Batch 91460, train_perplexity=5635.353, train_loss=8.636815

Batch 91470, train_perplexity=5523.5103, train_loss=8.616769

Batch 91480, train_perplexity=4696.1143, train_loss=8.454491

Batch 91490, train_perplexity=5068.0103, train_loss=8.530704

Batch 91500, train_perplexity=5547.535, train_loss=8.621109

Batch 91510, train_perplexity=4648.4595, train_loss=8.444291

Batch 91520, train_perplexity=4555.443, train_loss=8.424078

Batch 91530, train_perplexity=6305.671, train_loss=8.749205

Batch 91540, train_perplexity=6308.7085, train_loss=8.749686

Batch 91550, train_perplexity=5548.826, train_loss=8.621342

Batch 91560, train_perplexity=5654.464, train_loss=8.640201

Batch 91570, train_perplexity=4813.1035, train_loss=8.479097

Batch 91580, train_perplexity=5170.393, train_loss=8.550704

Batch 91590, train_perplexity=5359.8687, train_loss=8.586695

Batch 91600, train_perplexity=5577.348, train_loss=8.626469

Batch 91610, train_perplexity=5558.593, train_loss=8.6231

Batch 91620, train_perplexity=4116.579, train_loss=8.322778

Batch 91630, train_perplexity=5574.391, train_loss=8.625938

Batch 91640, train_perplexity=5136.3203, train_loss=8.544092

Batch 91650, train_perplexity=5929.1455, train_loss=8.687635

Batch 91660, train_perplexity=6612.7495, train_loss=8.796755

Batch 91670, train_perplexity=4583.358, train_loss=8.430187

Batch 91680, train_perplexity=5834.685, train_loss=8.671576

Batch 91690, train_perplexity=6494.837, train_loss=8.778763

Batch 91700, train_perplexity=5606.904, train_loss=8.631754

Batch 91710, train_perplexity=5904.4927, train_loss=8.683469

Batch 91720, train_perplexity=6071.5576, train_loss=8.71137

Batch 91730, train_perplexity=5952.1587, train_loss=8.691509

Batch 91740, train_perplexity=5625.9507, train_loss=8.635145

Batch 91750, train_perplexity=4489.9653, train_loss=8.4096

Batch 91760, train_perplexity=5754.787, train_loss=8.657787

Batch 91770, train_perplexity=4343.908, train_loss=8.37653

Batch 91780, train_perplexity=6256.5938, train_loss=8.741391

Batch 91790, train_perplexity=4834.0156, train_loss=8.483433

Batch 91800, train_perplexity=4623.0737, train_loss=8.438815

Batch 91810, train_perplexity=5748.7754, train_loss=8.656742

Batch 91820, train_perplexity=4836.2563, train_loss=8.483896

Batch 91830, train_perplexity=5421.508, train_loss=8.598129

Batch 91840, train_perplexity=4321.01, train_loss=8.371244

Batch 91850, train_perplexity=4636.5537, train_loss=8.441727

Batch 91860, train_perplexity=7392.535, train_loss=8.908226

Batch 91870, train_perplexity=5946.4453, train_loss=8.690549

Batch 91880, train_perplexity=5421.973, train_loss=8.598215

Batch 91890, train_perplexity=4637.7124, train_loss=8.441977

Batch 91900, train_perplexity=5467.7827, train_loss=8.606628

Batch 91910, train_perplexity=5102.0776, train_loss=8.537403

Batch 91920, train_perplexity=4989.9756, train_loss=8.515186

Batch 91930, train_perplexity=7772.494, train_loss=8.958346

Batch 91940, train_perplexity=5682.1035, train_loss=8.645077

Batch 91950, train_perplexity=6792.2197, train_loss=8.823533

Batch 91960, train_perplexity=7204.2695, train_loss=8.882429

Batch 91970, train_perplexity=4859.683, train_loss=8.488729

Batch 91980, train_perplexity=5131.8794, train_loss=8.543227

Batch 91990, train_perplexity=4493.3877, train_loss=8.410362

Batch 92000, train_perplexity=6921.793, train_loss=8.84243

Batch 92010, train_perplexity=4546.671, train_loss=8.422151

Batch 92020, train_perplexity=5902.01, train_loss=8.683048

Batch 92030, train_perplexity=6844.643, train_loss=8.831222

Batch 92040, train_perplexity=5267.1934, train_loss=8.569253

Batch 92050, train_perplexity=5184.065, train_loss=8.553345

Batch 92060, train_perplexity=6311.061, train_loss=8.750059

Batch 92070, train_perplexity=4742.5474, train_loss=8.46433

Batch 92080, train_perplexity=5934.7124, train_loss=8.688574

Batch 92090, train_perplexity=4705.5283, train_loss=8.456493

Batch 92100, train_perplexity=4225.3765, train_loss=8.348864

Batch 92110, train_perplexity=6364.795, train_loss=8.758537

Batch 92120, train_perplexity=4908.7217, train_loss=8.498769

Batch 92130, train_perplexity=5779.5703, train_loss=8.662085

Batch 92140, train_perplexity=4412.092, train_loss=8.392104

Batch 92150, train_perplexity=4764.9297, train_loss=8.469038

Batch 92160, train_perplexity=5508.3394, train_loss=8.614018

Batch 92170, train_perplexity=5815.763, train_loss=8.668327

Batch 92180, train_perplexity=4535.9907, train_loss=8.419799

Batch 92190, train_perplexity=4530.185, train_loss=8.418518

Batch 92200, train_perplexity=5362.2305, train_loss=8.587135

Batch 92210, train_perplexity=5724.5063, train_loss=8.652512

Batch 92220, train_perplexity=5085.7646, train_loss=8.534201

Batch 92230, train_perplexity=5725.5054, train_loss=8.652686

Batch 92240, train_perplexity=4514.874, train_loss=8.4151325

Batch 92250, train_perplexity=4958.023, train_loss=8.508762

Batch 92260, train_perplexity=4799.49, train_loss=8.476265

Batch 92270, train_perplexity=5818.5703, train_loss=8.66881

Batch 92280, train_perplexity=4920.669, train_loss=8.5012

Batch 92290, train_perplexity=5323.8716, train_loss=8.579956

Batch 92300, train_perplexity=4264.3604, train_loss=8.3580475

Batch 92310, train_perplexity=5522.083, train_loss=8.61651

Batch 92320, train_perplexity=6405.4897, train_loss=8.764911

Batch 92330, train_perplexity=5643.178, train_loss=8.638203

Batch 92340, train_perplexity=5557.8403, train_loss=8.622965

Batch 92350, train_perplexity=5904.341, train_loss=8.683443

Batch 92360, train_perplexity=5004.2153, train_loss=8.518036

Batch 92370, train_perplexity=4696.414, train_loss=8.454555

Batch 92380, train_perplexity=5534.0396, train_loss=8.618673

Batch 92390, train_perplexity=5831.7534, train_loss=8.671073

Batch 92400, train_perplexity=5555.9326, train_loss=8.622622

Batch 92410, train_perplexity=5352.0537, train_loss=8.585236

Batch 92420, train_perplexity=6058.578, train_loss=8.70923

Batch 92430, train_perplexity=5336.763, train_loss=8.582375

Batch 92440, train_perplexity=4733.3164, train_loss=8.462381

Batch 92450, train_perplexity=5136.2515, train_loss=8.544079

Batch 92460, train_perplexity=5650.1465, train_loss=8.639437

Batch 92470, train_perplexity=5502.9204, train_loss=8.613034

Batch 92480, train_perplexity=4692.1387, train_loss=8.453644

Batch 92490, train_perplexity=5402.0044, train_loss=8.594525

Batch 92500, train_perplexity=4767.3477, train_loss=8.469545

Batch 92510, train_perplexity=5641.6284, train_loss=8.637928

Batch 92520, train_perplexity=6218.2324, train_loss=8.735241

Batch 92530, train_perplexity=4973.556, train_loss=8.51189

Batch 92540, train_perplexity=5352.5024, train_loss=8.5853195

Batch 92550, train_perplexity=4961.4478, train_loss=8.509453

Batch 92560, train_perplexity=5962.3853, train_loss=8.693226

Batch 92570, train_perplexity=4310.1406, train_loss=8.368726

Batch 92580, train_perplexity=5629.428, train_loss=8.635763

Batch 92590, train_perplexity=5512.9062, train_loss=8.614847

Batch 92600, train_perplexity=5030.9976, train_loss=8.523374

Batch 92610, train_perplexity=4337.6157, train_loss=8.37508

Batch 92620, train_perplexity=5396.15, train_loss=8.593441

Batch 92630, train_perplexity=5947.869, train_loss=8.690788

Batch 92640, train_perplexity=5838.637, train_loss=8.672253

Batch 92650, train_perplexity=4917.91, train_loss=8.500639

Batch 92660, train_perplexity=6269.1426, train_loss=8.743395
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 92670, train_perplexity=4708.8013, train_loss=8.457189

Batch 92680, train_perplexity=5889.202, train_loss=8.680876

Batch 92690, train_perplexity=5482.324, train_loss=8.609284

Batch 92700, train_perplexity=5634.5845, train_loss=8.636679

Batch 92710, train_perplexity=5325.7505, train_loss=8.580309

Batch 92720, train_perplexity=5340.4185, train_loss=8.583059

Batch 92730, train_perplexity=6124.3384, train_loss=8.720026

Batch 92740, train_perplexity=5083.19, train_loss=8.533694

Batch 92750, train_perplexity=5685.193, train_loss=8.64562

Batch 92760, train_perplexity=4917.61, train_loss=8.500578

Batch 92770, train_perplexity=5053.84, train_loss=8.527904

Batch 92780, train_perplexity=4407.3267, train_loss=8.391024

Batch 92790, train_perplexity=4308.4062, train_loss=8.368323

Batch 92800, train_perplexity=5519.445, train_loss=8.616033

Batch 92810, train_perplexity=5720.435, train_loss=8.6518

Batch 92820, train_perplexity=6799.5366, train_loss=8.82461

Batch 92830, train_perplexity=6323.3877, train_loss=8.75201

Batch 92840, train_perplexity=5573.4453, train_loss=8.625769

Batch 92850, train_perplexity=5024.5776, train_loss=8.522097

Batch 92860, train_perplexity=4964.2397, train_loss=8.5100155

Batch 92870, train_perplexity=5303.94, train_loss=8.576205

Batch 92880, train_perplexity=6205.744, train_loss=8.733231

Batch 92890, train_perplexity=5314.5073, train_loss=8.578196

Batch 92900, train_perplexity=5642.9897, train_loss=8.638169

Batch 92910, train_perplexity=4956.784, train_loss=8.5085125

Batch 92920, train_perplexity=5153.5522, train_loss=8.5474415

Batch 92930, train_perplexity=6139.812, train_loss=8.722549

Batch 92940, train_perplexity=5327.8027, train_loss=8.580694

Batch 92950, train_perplexity=4653.205, train_loss=8.445312

Batch 92960, train_perplexity=5416.2886, train_loss=8.597166

Batch 92970, train_perplexity=4814.302, train_loss=8.479346

Batch 92980, train_perplexity=4759.1934, train_loss=8.4678335

Batch 92990, train_perplexity=5520.572, train_loss=8.616237

Batch 93000, train_perplexity=5235.887, train_loss=8.563292

Batch 93010, train_perplexity=5108.631, train_loss=8.538687

Batch 93020, train_perplexity=6371.688, train_loss=8.75962

Batch 93030, train_perplexity=6648.6675, train_loss=8.802172

Batch 93040, train_perplexity=4974.6, train_loss=8.5121

Batch 93050, train_perplexity=4477.881, train_loss=8.406905

Batch 93060, train_perplexity=5216.36, train_loss=8.559555

Batch 93070, train_perplexity=5686.82, train_loss=8.645906

Batch 93080, train_perplexity=5305.5845, train_loss=8.576515

Batch 93090, train_perplexity=4641.4824, train_loss=8.442789

Batch 93100, train_perplexity=4738.8267, train_loss=8.463545

Batch 93110, train_perplexity=4781.0933, train_loss=8.4724245

Batch 93120, train_perplexity=5150.776, train_loss=8.546903

Batch 93130, train_perplexity=5933.8013, train_loss=8.68842

Batch 93140, train_perplexity=5142.285, train_loss=8.545253

Batch 93150, train_perplexity=5405.9883, train_loss=8.595263

Batch 93160, train_perplexity=5698.8994, train_loss=8.648028

Batch 93170, train_perplexity=6455.5996, train_loss=8.772703

Batch 93180, train_perplexity=4986.893, train_loss=8.514568

Batch 93190, train_perplexity=5196.786, train_loss=8.555796

Batch 93200, train_perplexity=5853.3105, train_loss=8.674763

Batch 93210, train_perplexity=5087.9185, train_loss=8.534624

Batch 93220, train_perplexity=5915.692, train_loss=8.685364

Batch 93230, train_perplexity=4987.093, train_loss=8.514608

Batch 93240, train_perplexity=5066.338, train_loss=8.530374

Batch 93250, train_perplexity=5912.7085, train_loss=8.684859

Batch 93260, train_perplexity=5762.47, train_loss=8.6591215

Batch 93270, train_perplexity=5275.9966, train_loss=8.570923

Batch 93280, train_perplexity=5326.685, train_loss=8.580484

Batch 93290, train_perplexity=5477.537, train_loss=8.608411

Batch 93300, train_perplexity=5303.6416, train_loss=8.576149

Batch 93310, train_perplexity=4826.2812, train_loss=8.481832

Batch 93320, train_perplexity=5873.486, train_loss=8.678204

Batch 93330, train_perplexity=5291.3906, train_loss=8.573836

Batch 93340, train_perplexity=6169.159, train_loss=8.727318

Batch 93350, train_perplexity=4685.628, train_loss=8.452255

Batch 93360, train_perplexity=6095.4956, train_loss=8.715305

Batch 93370, train_perplexity=5139.103, train_loss=8.544634

Batch 93380, train_perplexity=5247.9946, train_loss=8.565601

Batch 93390, train_perplexity=3951.4146, train_loss=8.281829

Batch 93400, train_perplexity=5311.4316, train_loss=8.577617

Batch 93410, train_perplexity=4950.2417, train_loss=8.507192

Batch 93420, train_perplexity=6577.666, train_loss=8.791435

Batch 93430, train_perplexity=5663.0176, train_loss=8.641712

Batch 93440, train_perplexity=5336.4985, train_loss=8.582325

Batch 93450, train_perplexity=4518.212, train_loss=8.415872

Batch 93460, train_perplexity=5506.0493, train_loss=8.613603

Batch 93470, train_perplexity=5096.3296, train_loss=8.536276

Batch 93480, train_perplexity=5512.9795, train_loss=8.614861

Batch 93490, train_perplexity=6093.38, train_loss=8.714958

Batch 93500, train_perplexity=5650.5664, train_loss=8.639511

Batch 93510, train_perplexity=5086.0073, train_loss=8.534248

Batch 93520, train_perplexity=4735.9893, train_loss=8.462946

Batch 93530, train_perplexity=5533.095, train_loss=8.618503

Batch 93540, train_perplexity=5262.0625, train_loss=8.568278

Batch 93550, train_perplexity=6469.7876, train_loss=8.774899

Batch 93560, train_perplexity=6405.557, train_loss=8.764921

Batch 93570, train_perplexity=5074.3696, train_loss=8.531958

Batch 93580, train_perplexity=5854.9746, train_loss=8.675047

Batch 93590, train_perplexity=5550.5146, train_loss=8.621646

Batch 93600, train_perplexity=5440.0806, train_loss=8.601549

Batch 93610, train_perplexity=5542.755, train_loss=8.620247

Batch 93620, train_perplexity=5604.5195, train_loss=8.631329

Batch 93630, train_perplexity=5161.1953, train_loss=8.5489235

Batch 93640, train_perplexity=5438.566, train_loss=8.601271

Batch 93650, train_perplexity=5699.8726, train_loss=8.648199

Batch 93660, train_perplexity=4912.651, train_loss=8.499569

Batch 93670, train_perplexity=5308.874, train_loss=8.577135

Batch 93680, train_perplexity=6640.6323, train_loss=8.800962

Batch 93690, train_perplexity=5484.3687, train_loss=8.609657

Batch 93700, train_perplexity=5113.6763, train_loss=8.539674

Batch 93710, train_perplexity=4518.087, train_loss=8.415844

Batch 93720, train_perplexity=5716.5356, train_loss=8.651118

Batch 93730, train_perplexity=5066.4346, train_loss=8.530393

Batch 93740, train_perplexity=5519.34, train_loss=8.616014

Batch 93750, train_perplexity=5525.4756, train_loss=8.617125

Batch 93760, train_perplexity=5524.8955, train_loss=8.61702

Batch 93770, train_perplexity=5500.344, train_loss=8.612566

Batch 93780, train_perplexity=6875.0503, train_loss=8.835654

Batch 93790, train_perplexity=4286.533, train_loss=8.363234

Batch 93800, train_perplexity=4868.557, train_loss=8.490553

Batch 93810, train_perplexity=4449.4575, train_loss=8.4005375

Batch 93820, train_perplexity=5088.9473, train_loss=8.534826

Batch 93830, train_perplexity=5367.521, train_loss=8.588121

Batch 93840, train_perplexity=4929.1416, train_loss=8.50292

Batch 93850, train_perplexity=6998.1704, train_loss=8.853404

Batch 93860, train_perplexity=5395.965, train_loss=8.593407

Batch 93870, train_perplexity=4980.2676, train_loss=8.513239

Batch 93880, train_perplexity=5224.7935, train_loss=8.561171

Batch 93890, train_perplexity=4362.4697, train_loss=8.380794

Batch 93900, train_perplexity=5311.3354, train_loss=8.577599

Batch 93910, train_perplexity=5192.001, train_loss=8.554874

Batch 93920, train_perplexity=5288.152, train_loss=8.573224

Batch 93930, train_perplexity=6053.7095, train_loss=8.708426

Batch 93940, train_perplexity=4935.925, train_loss=8.504295

Batch 93950, train_perplexity=4933.929, train_loss=8.503891

Batch 93960, train_perplexity=5323.648, train_loss=8.579914

Batch 93970, train_perplexity=6304.9854, train_loss=8.749096

Batch 93980, train_perplexity=4496.4917, train_loss=8.411053

Batch 93990, train_perplexity=6689.5503, train_loss=8.808302

Batch 94000, train_perplexity=5082.87, train_loss=8.533631
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 94010, train_perplexity=5415.782, train_loss=8.597073

Batch 94020, train_perplexity=5830.313, train_loss=8.670826

Batch 94030, train_perplexity=5366.5176, train_loss=8.5879345

Batch 94040, train_perplexity=5846.17, train_loss=8.673542

Batch 94050, train_perplexity=4342.7397, train_loss=8.376261

Batch 94060, train_perplexity=4892.7466, train_loss=8.495509

Batch 94070, train_perplexity=5085.449, train_loss=8.534139

Batch 94080, train_perplexity=5225.4316, train_loss=8.561293

Batch 94090, train_perplexity=6224.2837, train_loss=8.736214

Batch 94100, train_perplexity=4927.5156, train_loss=8.50259

Batch 94110, train_perplexity=5102.2817, train_loss=8.537443

Batch 94120, train_perplexity=5480.3745, train_loss=8.608929

Batch 94130, train_perplexity=4769.9126, train_loss=8.470083

Batch 94140, train_perplexity=4706.498, train_loss=8.456699

Batch 94150, train_perplexity=4977.7417, train_loss=8.512732

Batch 94160, train_perplexity=4580.662, train_loss=8.429599

Batch 94170, train_perplexity=5149.337, train_loss=8.546623

Batch 94180, train_perplexity=5405.6943, train_loss=8.595208

Batch 94190, train_perplexity=5442.779, train_loss=8.602045

Batch 94200, train_perplexity=6189.077, train_loss=8.730541

Batch 94210, train_perplexity=6440.5396, train_loss=8.770368

Batch 94220, train_perplexity=4557.763, train_loss=8.424587

Batch 94230, train_perplexity=4838.8955, train_loss=8.484442

Batch 94240, train_perplexity=5161.6187, train_loss=8.5490055

Batch 94250, train_perplexity=5149.327, train_loss=8.546621

Batch 94260, train_perplexity=5083.4272, train_loss=8.533741

Batch 94270, train_perplexity=5700.775, train_loss=8.648357

Batch 94280, train_perplexity=5055.199, train_loss=8.5281725

Batch 94290, train_perplexity=5912.1953, train_loss=8.6847725

Batch 94300, train_perplexity=5943.1855, train_loss=8.690001

Batch 94310, train_perplexity=6101.8467, train_loss=8.716347

Batch 94320, train_perplexity=5882.1855, train_loss=8.679684

Batch 94330, train_perplexity=4723.9727, train_loss=8.460405

Batch 94340, train_perplexity=5789.9194, train_loss=8.663874

Batch 94350, train_perplexity=6304.5522, train_loss=8.749027

Batch 94360, train_perplexity=5767.2095, train_loss=8.659944

Batch 94370, train_perplexity=5745.492, train_loss=8.656171

Batch 94380, train_perplexity=6114.7905, train_loss=8.718466

Batch 94390, train_perplexity=6452.916, train_loss=8.772287

Batch 94400, train_perplexity=5706.3774, train_loss=8.64934

Batch 94410, train_perplexity=4528.9453, train_loss=8.418244

Batch 94420, train_perplexity=5862.976, train_loss=8.676413

Batch 94430, train_perplexity=5116.227, train_loss=8.540173

Batch 94440, train_perplexity=5933.3994, train_loss=8.688353

Batch 94450, train_perplexity=7477.8145, train_loss=8.919696

Batch 94460, train_perplexity=5177.6514, train_loss=8.552107

Batch 94470, train_perplexity=5458.4043, train_loss=8.604912

Batch 94480, train_perplexity=6348.8145, train_loss=8.756023

Batch 94490, train_perplexity=5943.962, train_loss=8.690131

Batch 94500, train_perplexity=4423.0073, train_loss=8.394575

Batch 94510, train_perplexity=5005.6665, train_loss=8.518326

Batch 94520, train_perplexity=5885.6084, train_loss=8.680265

Batch 94530, train_perplexity=4740.9194, train_loss=8.463986

Batch 94540, train_perplexity=5134.1455, train_loss=8.543669

Batch 94550, train_perplexity=4802.823, train_loss=8.476959

Batch 94560, train_perplexity=5986.1836, train_loss=8.697209

Batch 94570, train_perplexity=5553.7183, train_loss=8.622223

Batch 94580, train_perplexity=4714.247, train_loss=8.458344

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Batch 94590, train_perplexity=5145.91, train_loss=8.545958

Batch 94600, train_perplexity=6068.3447, train_loss=8.710841

Batch 94610, train_perplexity=4445.297, train_loss=8.399602

Batch 94620, train_perplexity=4213.53, train_loss=8.346056

Batch 94630, train_perplexity=4649.5103, train_loss=8.444517

Batch 94640, train_perplexity=4411.7256, train_loss=8.392021

Batch 94650, train_perplexity=5391.685, train_loss=8.592613

Batch 94660, train_perplexity=4797.5635, train_loss=8.475863

Batch 94670, train_perplexity=4799.261, train_loss=8.476217

Batch 94680, train_perplexity=5126.474, train_loss=8.542173

Batch 94690, train_perplexity=5133.01, train_loss=8.5434475

Batch 94700, train_perplexity=5539.9854, train_loss=8.619747

Batch 94710, train_perplexity=5782.5474, train_loss=8.6626

Batch 94720, train_perplexity=5878.742, train_loss=8.679098

Batch 94730, train_perplexity=7522.2534, train_loss=8.925621

Batch 94740, train_perplexity=4830.504, train_loss=8.482706

Batch 94750, train_perplexity=4556.911, train_loss=8.4244

Batch 94760, train_perplexity=5795.864, train_loss=8.6649

Batch 94770, train_perplexity=5191.3174, train_loss=8.554743

Batch 94780, train_perplexity=5276.7563, train_loss=8.571067

Batch 94790, train_perplexity=5087.6855, train_loss=8.534578

Batch 94800, train_perplexity=4794.5083, train_loss=8.475226

Batch 94810, train_perplexity=5154.3926, train_loss=8.547605

Batch 94820, train_perplexity=5040.141, train_loss=8.525189

Batch 94830, train_perplexity=5501.593, train_loss=8.612793

Batch 94840, train_perplexity=5486.1895, train_loss=8.609989

Batch 94850, train_perplexity=5535.792, train_loss=8.61899

Batch 94860, train_perplexity=5849.5327, train_loss=8.674117

Batch 94870, train_perplexity=5300.891, train_loss=8.57563

Batch 94880, train_perplexity=5215.7627, train_loss=8.559441

Batch 94890, train_perplexity=5675.15, train_loss=8.643852

Batch 94900, train_perplexity=4775.8936, train_loss=8.471336

Batch 94910, train_perplexity=5218.2803, train_loss=8.559923

Batch 94920, train_perplexity=4883.5493, train_loss=8.493628

Batch 94930, train_perplexity=5251.9053, train_loss=8.566346

Batch 94940, train_perplexity=5066.222, train_loss=8.530351

Batch 94950, train_perplexity=5618.112, train_loss=8.633751

Batch 94960, train_perplexity=5322.836, train_loss=8.5797615

Batch 94970, train_perplexity=4379.9697, train_loss=8.384797

Batch 94980, train_perplexity=4614.797, train_loss=8.437023

Batch 94990, train_perplexity=5382.7456, train_loss=8.590954

Batch 95000, train_perplexity=6060.358, train_loss=8.709524

Batch 95010, train_perplexity=5054.8613, train_loss=8.528106

Batch 95020, train_perplexity=4855.037, train_loss=8.487772

Batch 95030, train_perplexity=6233.664, train_loss=8.73772

Batch 95040, train_perplexity=5465.7285, train_loss=8.606253

Batch 95050, train_perplexity=5654.103, train_loss=8.640137

Batch 95060, train_perplexity=6052.849, train_loss=8.708284

Batch 95070, train_perplexity=5273.582, train_loss=8.570465

Batch 95080, train_perplexity=4707.297, train_loss=8.456869

Batch 95090, train_perplexity=4630.571, train_loss=8.440435

Batch 95100, train_perplexity=5186.379, train_loss=8.553791

Batch 95110, train_perplexity=5988.873, train_loss=8.697659

Batch 95120, train_perplexity=5223.02, train_loss=8.560831

Batch 95130, train_perplexity=5387.0386, train_loss=8.591751

Batch 95140, train_perplexity=4754.285, train_loss=8.466802

Batch 95150, train_perplexity=4651.985, train_loss=8.445049

Batch 95160, train_perplexity=5012.4307, train_loss=8.519676

Batch 95170, train_perplexity=5193.645, train_loss=8.555191

Batch 95180, train_perplexity=5481.3047, train_loss=8.609098

Batch 95190, train_perplexity=5916.617, train_loss=8.68552

Batch 95200, train_perplexity=6489.364, train_loss=8.77792

Batch 95210, train_perplexity=4563.361, train_loss=8.425815

Batch 95220, train_perplexity=5355.5767, train_loss=8.585894

Batch 95230, train_perplexity=4833.5454, train_loss=8.4833355

Batch 95240, train_perplexity=4684.386, train_loss=8.45199

Batch 95250, train_perplexity=5534.5464, train_loss=8.618765

Batch 95260, train_perplexity=5250.763, train_loss=8.566129

Batch 95270, train_perplexity=5424.7144, train_loss=8.598721
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 95280, train_perplexity=5937.6504, train_loss=8.689069

Batch 95290, train_perplexity=5907.185, train_loss=8.683925

Batch 95300, train_perplexity=5059.593, train_loss=8.529041

Batch 95310, train_perplexity=5261.064, train_loss=8.568089

Batch 95320, train_perplexity=5712.018, train_loss=8.650328

Batch 95330, train_perplexity=5106.7656, train_loss=8.5383215

Batch 95340, train_perplexity=6262.169, train_loss=8.742282

Batch 95350, train_perplexity=5857.89, train_loss=8.675545

Batch 95360, train_perplexity=5992.781, train_loss=8.698311

Batch 95370, train_perplexity=4617.914, train_loss=8.437698

Batch 95380, train_perplexity=4687.4785, train_loss=8.45265

Batch 95390, train_perplexity=5848.1772, train_loss=8.673885

Batch 95400, train_perplexity=5706.323, train_loss=8.64933

Batch 95410, train_perplexity=5425.6045, train_loss=8.598885

Batch 95420, train_perplexity=6019.243, train_loss=8.702717

Batch 95430, train_perplexity=5242.2725, train_loss=8.56451

Batch 95440, train_perplexity=4268.572, train_loss=8.359035

Batch 95450, train_perplexity=5482.6587, train_loss=8.609345

Batch 95460, train_perplexity=5730.952, train_loss=8.653637

Batch 95470, train_perplexity=5524.759, train_loss=8.616995

Batch 95480, train_perplexity=5062.1797, train_loss=8.529552

Batch 95490, train_perplexity=4986.265, train_loss=8.514442

Batch 95500, train_perplexity=5126.792, train_loss=8.542235

Batch 95510, train_perplexity=5363.632, train_loss=8.587397

Batch 95520, train_perplexity=5297.3584, train_loss=8.574964

Batch 95530, train_perplexity=4958.16, train_loss=8.50879

Batch 95540, train_perplexity=5595.3604, train_loss=8.629693

Batch 95550, train_perplexity=6272.4194, train_loss=8.743917

Batch 95560, train_perplexity=4906.592, train_loss=8.498335

Batch 95570, train_perplexity=5438.711, train_loss=8.601297

Batch 95580, train_perplexity=5320.1665, train_loss=8.57926

Batch 95590, train_perplexity=5443.786, train_loss=8.60223

Batch 95600, train_perplexity=5689.4346, train_loss=8.646366

Batch 95610, train_perplexity=5297.1616, train_loss=8.574926

Batch 95620, train_perplexity=5570.14, train_loss=8.625175

Batch 95630, train_perplexity=5987.5366, train_loss=8.697435

Batch 95640, train_perplexity=6133.907, train_loss=8.721587

Batch 95650, train_perplexity=5111.9546, train_loss=8.539337

Batch 95660, train_perplexity=5427.654, train_loss=8.599262

Batch 95670, train_perplexity=6980.553, train_loss=8.8508835

Batch 95680, train_perplexity=5139.1226, train_loss=8.544638

Batch 95690, train_perplexity=5195.448, train_loss=8.555538

Batch 95700, train_perplexity=6131.9946, train_loss=8.721275

Batch 95710, train_perplexity=5185.588, train_loss=8.553638

Batch 95720, train_perplexity=5032.418, train_loss=8.523656

Batch 95730, train_perplexity=5916.753, train_loss=8.685543

Batch 95740, train_perplexity=5733.713, train_loss=8.654119

Batch 95750, train_perplexity=6014.205, train_loss=8.7018795

Batch 95760, train_perplexity=5288.984, train_loss=8.573381

Batch 95770, train_perplexity=5141.7554, train_loss=8.54515

Batch 95780, train_perplexity=4612.3945, train_loss=8.436502

Batch 95790, train_perplexity=6422.881, train_loss=8.767622

Batch 95800, train_perplexity=5251.289, train_loss=8.566229

Batch 95810, train_perplexity=5051.031, train_loss=8.527348

Batch 95820, train_perplexity=5375.461, train_loss=8.5896

Batch 95830, train_perplexity=5096.1157, train_loss=8.536234

Batch 95840, train_perplexity=5066.546, train_loss=8.530415

Batch 95850, train_perplexity=6060.647, train_loss=8.709572

Batch 95860, train_perplexity=4773.412, train_loss=8.470817

Batch 95870, train_perplexity=5986.4062, train_loss=8.697247

Batch 95880, train_perplexity=4925.655, train_loss=8.502213

Batch 95890, train_perplexity=6107.232, train_loss=8.717229

Batch 95900, train_perplexity=4933.6094, train_loss=8.503826

Batch 95910, train_perplexity=5328.423, train_loss=8.580811

Batch 95920, train_perplexity=5419.8125, train_loss=8.597816

Batch 95930, train_perplexity=6651.2295, train_loss=8.802557

Batch 95940, train_perplexity=4999.3306, train_loss=8.517059

Batch 95950, train_perplexity=5129.0366, train_loss=8.542673

Batch 95960, train_perplexity=5537.973, train_loss=8.619384

Batch 95970, train_perplexity=5047.2456, train_loss=8.526598

Batch 95980, train_perplexity=5469.3574, train_loss=8.606916

Batch 95990, train_perplexity=4765.3477, train_loss=8.469126

Batch 96000, train_perplexity=6227.7397, train_loss=8.736769

Batch 96010, train_perplexity=6060.358, train_loss=8.709524

Batch 96020, train_perplexity=4991.5034, train_loss=8.515492

Batch 96030, train_perplexity=7048.506, train_loss=8.860571

Batch 96040, train_perplexity=6525.5073, train_loss=8.783474

Batch 96050, train_perplexity=4624.3745, train_loss=8.439096

Batch 96060, train_perplexity=6184.4097, train_loss=8.729787

Batch 96070, train_perplexity=6986.361, train_loss=8.851715

Batch 96080, train_perplexity=4935.9014, train_loss=8.504291

Batch 96090, train_perplexity=5170.4473, train_loss=8.5507145

Batch 96100, train_perplexity=5990.5923, train_loss=8.697946

Batch 96110, train_perplexity=4808.4424, train_loss=8.478128

Batch 96120, train_perplexity=6139.0864, train_loss=8.722431

Batch 96130, train_perplexity=5211.3027, train_loss=8.558585

Batch 96140, train_perplexity=6983.703, train_loss=8.851335

Batch 96150, train_perplexity=6975.416, train_loss=8.850147

Batch 96160, train_perplexity=5365.6836, train_loss=8.587779

Batch 96170, train_perplexity=5263.1714, train_loss=8.568489

Batch 96180, train_perplexity=4888.773, train_loss=8.494697

Batch 96190, train_perplexity=4686.5933, train_loss=8.452461

Batch 96200, train_perplexity=5776.7046, train_loss=8.661589

Batch 96210, train_perplexity=6688.555, train_loss=8.808153

Batch 96220, train_perplexity=4474.8457, train_loss=8.406227

Batch 96230, train_perplexity=5361.453, train_loss=8.58699

Batch 96240, train_perplexity=5984.728, train_loss=8.696966

Batch 96250, train_perplexity=5257.613, train_loss=8.567432

Batch 96260, train_perplexity=5802.567, train_loss=8.666056

Batch 96270, train_perplexity=5954.5605, train_loss=8.691913

Batch 96280, train_perplexity=4199.3647, train_loss=8.342689

Batch 96290, train_perplexity=5962.2544, train_loss=8.693204

Batch 96300, train_perplexity=5450.633, train_loss=8.603487

Batch 96310, train_perplexity=5352.181, train_loss=8.585259

Batch 96320, train_perplexity=5075.5024, train_loss=8.532181

Batch 96330, train_perplexity=4697.9863, train_loss=8.454889

Batch 96340, train_perplexity=5274.4067, train_loss=8.5706215

Batch 96350, train_perplexity=6088.6978, train_loss=8.71419

Batch 96360, train_perplexity=4975.221, train_loss=8.512225

Batch 96370, train_perplexity=5762.058, train_loss=8.65905

Batch 96380, train_perplexity=5773.455, train_loss=8.661026

Batch 96390, train_perplexity=4924.5557, train_loss=8.501989

Batch 96400, train_perplexity=5672.704, train_loss=8.643421

Batch 96410, train_perplexity=5845.4062, train_loss=8.673411

Batch 96420, train_perplexity=5804.6035, train_loss=8.666407

Batch 96430, train_perplexity=6108.624, train_loss=8.717457

Batch 96440, train_perplexity=5836.833, train_loss=8.671944

Batch 96450, train_perplexity=5662.7314, train_loss=8.641662

Batch 96460, train_perplexity=5219.6836, train_loss=8.560192

Batch 96470, train_perplexity=6684.971, train_loss=8.807617

Batch 96480, train_perplexity=5458.2017, train_loss=8.604875

Batch 96490, train_perplexity=5871.862, train_loss=8.677927

Batch 96500, train_perplexity=4560.7026, train_loss=8.425232

Batch 96510, train_perplexity=6567.43, train_loss=8.789878

Batch 96520, train_perplexity=5640.757, train_loss=8.6377735

Batch 96530, train_perplexity=5327.6606, train_loss=8.5806675

Batch 96540, train_perplexity=5130.4844, train_loss=8.542955

Batch 96550, train_perplexity=5114.5103, train_loss=8.539837

Batch 96560, train_perplexity=5940.5337, train_loss=8.689554

Batch 96570, train_perplexity=4850.1914, train_loss=8.4867735

Batch 96580, train_perplexity=4997.7, train_loss=8.516733

Batch 96590, train_perplexity=5255.0015, train_loss=8.566936

Batch 96600, train_perplexity=4878.08, train_loss=8.492507

Batch 96610, train_perplexity=5094.6675, train_loss=8.53595
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 96620, train_perplexity=4744.9043, train_loss=8.464827

Batch 96630, train_perplexity=5013.8027, train_loss=8.51995

Batch 96640, train_perplexity=5555.8955, train_loss=8.622615

Batch 96650, train_perplexity=5035.001, train_loss=8.524169

Batch 96660, train_perplexity=4738.0854, train_loss=8.463388

Batch 96670, train_perplexity=5716.427, train_loss=8.651099

Batch 96680, train_perplexity=4920.3965, train_loss=8.501144

Batch 96690, train_perplexity=5446.954, train_loss=8.602812

Batch 96700, train_perplexity=4933.261, train_loss=8.503756

Batch 96710, train_perplexity=5281.6553, train_loss=8.571995

Batch 96720, train_perplexity=6148.8887, train_loss=8.724027

Batch 96730, train_perplexity=5626.938, train_loss=8.635321

Batch 96740, train_perplexity=6047.864, train_loss=8.70746

Batch 96750, train_perplexity=6135.832, train_loss=8.721901

Batch 96760, train_perplexity=6084.194, train_loss=8.7134495

Batch 96770, train_perplexity=4808.433, train_loss=8.478127

Batch 96780, train_perplexity=5050.313, train_loss=8.527205

Batch 96790, train_perplexity=4376.454, train_loss=8.383994

Batch 96800, train_perplexity=5775.256, train_loss=8.661338

Batch 96810, train_perplexity=6093.304, train_loss=8.714946

Batch 96820, train_perplexity=4395.708, train_loss=8.388384

Batch 96830, train_perplexity=5280.9497, train_loss=8.571861

Batch 96840, train_perplexity=5106.0254, train_loss=8.538177

Batch 96850, train_perplexity=4808.919, train_loss=8.478228

Batch 96860, train_perplexity=5586.046, train_loss=8.628027

Batch 96870, train_perplexity=5948.669, train_loss=8.690923

Batch 96880, train_perplexity=5656.163, train_loss=8.640501

Batch 96890, train_perplexity=5387.0693, train_loss=8.591757

Batch 96900, train_perplexity=5454.33, train_loss=8.604165

Batch 96910, train_perplexity=5304.0513, train_loss=8.576226

Batch 96920, train_perplexity=4661.191, train_loss=8.447026

Batch 96930, train_perplexity=4770.2856, train_loss=8.470161

Batch 96940, train_perplexity=5922.0703, train_loss=8.686441

Batch 96950, train_perplexity=4772.242, train_loss=8.4705715

Batch 96960, train_perplexity=4359.5127, train_loss=8.3801155

Batch 96970, train_perplexity=5131.6494, train_loss=8.543182

Batch 96980, train_perplexity=5699.2637, train_loss=8.648092

Batch 96990, train_perplexity=4782.904, train_loss=8.472803

Batch 97000, train_perplexity=5315.6274, train_loss=8.578406

Batch 97010, train_perplexity=5770.813, train_loss=8.660568

Batch 97020, train_perplexity=5246.6436, train_loss=8.565344

Batch 97030, train_perplexity=5257.944, train_loss=8.567495

Batch 97040, train_perplexity=5873.654, train_loss=8.678232

Batch 97050, train_perplexity=6531.8516, train_loss=8.784446

Batch 97060, train_perplexity=6076.458, train_loss=8.712177

Batch 97070, train_perplexity=4785.3037, train_loss=8.473305

Batch 97080, train_perplexity=5039.8384, train_loss=8.525129

Batch 97090, train_perplexity=6017.1885, train_loss=8.702375

Batch 97100, train_perplexity=5177.4834, train_loss=8.552074

Batch 97110, train_perplexity=5098.3613, train_loss=8.5366745

Batch 97120, train_perplexity=4761.4546, train_loss=8.468308

Batch 97130, train_perplexity=6537.3545, train_loss=8.785288

Batch 97140, train_perplexity=5456.1147, train_loss=8.604492

Batch 97150, train_perplexity=4567.423, train_loss=8.426704

Batch 97160, train_perplexity=5102.0483, train_loss=8.537397

Batch 97170, train_perplexity=6119.586, train_loss=8.71925

Batch 97180, train_perplexity=4601.8193, train_loss=8.434207

Batch 97190, train_perplexity=5738.379, train_loss=8.654932

Batch 97200, train_perplexity=5836.31, train_loss=8.671854

Batch 97210, train_perplexity=4601.7886, train_loss=8.4342

Batch 97220, train_perplexity=5304.1577, train_loss=8.576246

Batch 97230, train_perplexity=5896.429, train_loss=8.682102

Batch 97240, train_perplexity=5161.6187, train_loss=8.5490055

Batch 97250, train_perplexity=4912.632, train_loss=8.499565

Batch 97260, train_perplexity=4287.273, train_loss=8.363406

Batch 97270, train_perplexity=6974.019, train_loss=8.849947

Batch 97280, train_perplexity=5531.971, train_loss=8.6182995

Batch 97290, train_perplexity=6684.9395, train_loss=8.807612

Batch 97300, train_perplexity=5223.4233, train_loss=8.560908

Batch 97310, train_perplexity=6991.813, train_loss=8.852495

Batch 97320, train_perplexity=5935.097, train_loss=8.688639

Batch 97330, train_perplexity=6135.083, train_loss=8.721779

Batch 97340, train_perplexity=6254.774, train_loss=8.7411

Batch 97350, train_perplexity=5774.6006, train_loss=8.661224

Batch 97360, train_perplexity=5666.7725, train_loss=8.642375

Batch 97370, train_perplexity=5910.583, train_loss=8.6845

Batch 97380, train_perplexity=4699.183, train_loss=8.455144

Batch 97390, train_perplexity=4803.3, train_loss=8.477058

Batch 97400, train_perplexity=5059.7715, train_loss=8.529077

Batch 97410, train_perplexity=4425.2773, train_loss=8.395088

Batch 97420, train_perplexity=4595.2363, train_loss=8.4327755

Batch 97430, train_perplexity=5982.3657, train_loss=8.696571

Batch 97440, train_perplexity=4956.071, train_loss=8.5083685

Batch 97450, train_perplexity=6013.655, train_loss=8.701788

Batch 97460, train_perplexity=5706.2686, train_loss=8.649321

Batch 97470, train_perplexity=5069.1606, train_loss=8.5309305

Batch 97480, train_perplexity=4708.864, train_loss=8.457202

Batch 97490, train_perplexity=5545.8213, train_loss=8.6208

Batch 97500, train_perplexity=5376.343, train_loss=8.589764

Batch 97510, train_perplexity=6926.171, train_loss=8.843062

Batch 97520, train_perplexity=5262.504, train_loss=8.568362

Batch 97530, train_perplexity=4134.737, train_loss=8.327179

Batch 97540, train_perplexity=4980.9707, train_loss=8.51338

Batch 97550, train_perplexity=6220.2134, train_loss=8.735559

Batch 97560, train_perplexity=5136.7905, train_loss=8.544184

Batch 97570, train_perplexity=6459.2207, train_loss=8.773264

Batch 97580, train_perplexity=6142.2837, train_loss=8.722952

Batch 97590, train_perplexity=5565.913, train_loss=8.624416

Batch 97600, train_perplexity=4059.1475, train_loss=8.308728

Batch 97610, train_perplexity=5915.963, train_loss=8.68541

Batch 97620, train_perplexity=5271.998, train_loss=8.570165

Batch 97630, train_perplexity=5153.2915, train_loss=8.547391

Batch 97640, train_perplexity=4787.5405, train_loss=8.473772

Batch 97650, train_perplexity=5486.749, train_loss=8.610091

Batch 97660, train_perplexity=5766.522, train_loss=8.659824

Batch 97670, train_perplexity=4978.544, train_loss=8.512893

Batch 97680, train_perplexity=4643.298, train_loss=8.44318

Batch 97690, train_perplexity=5545.811, train_loss=8.620798

Batch 97700, train_perplexity=4907.8184, train_loss=8.498585

Batch 97710, train_perplexity=5699.7744, train_loss=8.648182

Batch 97720, train_perplexity=6172.7783, train_loss=8.727904

Batch 97730, train_perplexity=5384.604, train_loss=8.591299

Batch 97740, train_perplexity=4948.2217, train_loss=8.5067835

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Batch 97750, train_perplexity=5548.8794, train_loss=8.621351

Batch 97760, train_perplexity=5733.04, train_loss=8.654001

Batch 97770, train_perplexity=5134.0527, train_loss=8.543651

Batch 97780, train_perplexity=4903.355, train_loss=8.497675

Batch 97790, train_perplexity=4272.502, train_loss=8.359955

Batch 97800, train_perplexity=6132.591, train_loss=8.721373

Batch 97810, train_perplexity=6424.253, train_loss=8.767836

Batch 97820, train_perplexity=4888.5537, train_loss=8.494652

Batch 97830, train_perplexity=5102.1553, train_loss=8.537418

Batch 97840, train_perplexity=4479.9907, train_loss=8.407376

Batch 97850, train_perplexity=5940.2104, train_loss=8.6895

Batch 97860, train_perplexity=6428.69, train_loss=8.768526

Batch 97870, train_perplexity=4858.325, train_loss=8.488449

Batch 97880, train_perplexity=6588.15, train_loss=8.793028
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 97890, train_perplexity=5072.7876, train_loss=8.531646

Batch 97900, train_perplexity=6147.4053, train_loss=8.723785

Batch 97910, train_perplexity=4874.216, train_loss=8.4917145

Batch 97920, train_perplexity=4750.12, train_loss=8.465925

Batch 97930, train_perplexity=4338.5425, train_loss=8.375294

Batch 97940, train_perplexity=7163.615, train_loss=8.87677

Batch 97950, train_perplexity=5217.643, train_loss=8.559801

Batch 97960, train_perplexity=6213.087, train_loss=8.734413

Batch 97970, train_perplexity=5739.003, train_loss=8.655041

Batch 97980, train_perplexity=4992.3936, train_loss=8.515671

Batch 97990, train_perplexity=5524.432, train_loss=8.616936

Batch 98000, train_perplexity=5655.057, train_loss=8.6403055

Batch 98010, train_perplexity=4641.7305, train_loss=8.4428425

Batch 98020, train_perplexity=5078.025, train_loss=8.532678

Batch 98030, train_perplexity=5533.248, train_loss=8.61853

Batch 98040, train_perplexity=4666.408, train_loss=8.448145

Batch 98050, train_perplexity=6829.1704, train_loss=8.8289585

Batch 98060, train_perplexity=6014.8994, train_loss=8.701995

Batch 98070, train_perplexity=5441.907, train_loss=8.601885

Batch 98080, train_perplexity=4390.312, train_loss=8.387156

Batch 98090, train_perplexity=5076.645, train_loss=8.532406

Batch 98100, train_perplexity=5281.9976, train_loss=8.57206

Batch 98110, train_perplexity=5279.268, train_loss=8.571543

Batch 98120, train_perplexity=5677.618, train_loss=8.644287

Batch 98130, train_perplexity=6093.508, train_loss=8.714979

Batch 98140, train_perplexity=5150.034, train_loss=8.546759

Batch 98150, train_perplexity=5176.2544, train_loss=8.551837

Batch 98160, train_perplexity=5457.2124, train_loss=8.604693

Batch 98170, train_perplexity=5251.309, train_loss=8.566233

Batch 98180, train_perplexity=5428.772, train_loss=8.599468

Batch 98190, train_perplexity=6224.076, train_loss=8.73618

Batch 98200, train_perplexity=5537.149, train_loss=8.619235

Batch 98210, train_perplexity=5257.0967, train_loss=8.567334

Batch 98220, train_perplexity=5753.2725, train_loss=8.657524

Batch 98230, train_perplexity=5199.7456, train_loss=8.556365

Batch 98240, train_perplexity=5368.1914, train_loss=8.588246

Batch 98250, train_perplexity=5089.5103, train_loss=8.534937

Batch 98260, train_perplexity=5311.7407, train_loss=8.577675

Batch 98270, train_perplexity=4737.9863, train_loss=8.463367

Batch 98280, train_perplexity=4945.2964, train_loss=8.506192

Batch 98290, train_perplexity=5108.105, train_loss=8.538584

Batch 98300, train_perplexity=6489.6484, train_loss=8.777964

Batch 98310, train_perplexity=5966.128, train_loss=8.693853

Batch 98320, train_perplexity=6688.855, train_loss=8.808198

Batch 98330, train_perplexity=6646.0996, train_loss=8.801785

Batch 98340, train_perplexity=6042.9287, train_loss=8.706644

Batch 98350, train_perplexity=4795.4775, train_loss=8.475429

Batch 98360, train_perplexity=5531.623, train_loss=8.618237

Batch 98370, train_perplexity=5925.234, train_loss=8.6869755

Batch 98380, train_perplexity=6352.2305, train_loss=8.756561

Batch 98390, train_perplexity=6072.7275, train_loss=8.711563

Batch 98400, train_perplexity=5694.8955, train_loss=8.6473255

Batch 98410, train_perplexity=4898.401, train_loss=8.496664

Batch 98420, train_perplexity=6079.4956, train_loss=8.712677

Batch 98430, train_perplexity=5771.022, train_loss=8.6606045

Batch 98440, train_perplexity=6931.1597, train_loss=8.843782

Batch 98450, train_perplexity=5639.1753, train_loss=8.637493

Batch 98460, train_perplexity=5648.8857, train_loss=8.639214

Batch 98470, train_perplexity=4611.3394, train_loss=8.436274

Batch 98480, train_perplexity=6249.5273, train_loss=8.740261

Batch 98490, train_perplexity=5706.149, train_loss=8.6493

Batch 98500, train_perplexity=7156.336, train_loss=8.875753

Batch 98510, train_perplexity=5661.4517, train_loss=8.641436

Batch 98520, train_perplexity=5070.819, train_loss=8.531258

Batch 98530, train_perplexity=6286.361, train_loss=8.746138

Batch 98540, train_perplexity=5017.6484, train_loss=8.520717

Batch 98550, train_perplexity=5852.161, train_loss=8.674566

Batch 98560, train_perplexity=6054.8237, train_loss=8.708611

Batch 98570, train_perplexity=5320.0293, train_loss=8.579234

Batch 98580, train_perplexity=4974.8228, train_loss=8.512145

Batch 98590, train_perplexity=6499.9673, train_loss=8.779552

Batch 98600, train_perplexity=6917.055, train_loss=8.841745

Batch 98610, train_perplexity=6567.643, train_loss=8.78991

Batch 98620, train_perplexity=5150.899, train_loss=8.5469265

Batch 98630, train_perplexity=4973.6133, train_loss=8.511902

Batch 98640, train_perplexity=5047.4, train_loss=8.5266285

Batch 98650, train_perplexity=5224.0513, train_loss=8.5610285

Batch 98660, train_perplexity=5241.4424, train_loss=8.564352

Batch 98670, train_perplexity=5987.691, train_loss=8.697461

Batch 98680, train_perplexity=5482.141, train_loss=8.609251

Batch 98690, train_perplexity=4670.2637, train_loss=8.448971

Batch 98700, train_perplexity=6243.022, train_loss=8.73922

Batch 98710, train_perplexity=6595.159, train_loss=8.794091

Batch 98720, train_perplexity=5289.0396, train_loss=8.573392

Batch 98730, train_perplexity=5801.4824, train_loss=8.665869

Batch 98740, train_perplexity=4743.172, train_loss=8.464461

Batch 98750, train_perplexity=4894.88, train_loss=8.495945

Batch 98760, train_perplexity=5186.295, train_loss=8.553775

Batch 98770, train_perplexity=6031.103, train_loss=8.704685

Batch 98780, train_perplexity=5752.219, train_loss=8.657341

Batch 98790, train_perplexity=5555.699, train_loss=8.62258

Batch 98800, train_perplexity=5345.1978, train_loss=8.583954

Batch 98810, train_perplexity=6657.4106, train_loss=8.803486

Batch 98820, train_perplexity=5202.3994, train_loss=8.556875

Batch 98830, train_perplexity=5430.2114, train_loss=8.599733

Batch 98840, train_perplexity=5258.4756, train_loss=8.567596

Batch 98850, train_perplexity=6005.133, train_loss=8.70037

Batch 98860, train_perplexity=5460.2163, train_loss=8.605244

Batch 98870, train_perplexity=5550.202, train_loss=8.62159

Batch 98880, train_perplexity=4973.3193, train_loss=8.511843

Batch 98890, train_perplexity=5375.9634, train_loss=8.589693

Batch 98900, train_perplexity=4817.549, train_loss=8.4800205

Batch 98910, train_perplexity=5902.719, train_loss=8.683168

Batch 98920, train_perplexity=6112.062, train_loss=8.7180195

Batch 98930, train_perplexity=5155.179, train_loss=8.547757

Batch 98940, train_perplexity=4793.996, train_loss=8.47512

Batch 98950, train_perplexity=5208.6245, train_loss=8.558071

Batch 98960, train_perplexity=6293.847, train_loss=8.747328

Batch 98970, train_perplexity=5896.575, train_loss=8.682127

Batch 98980, train_perplexity=6112.855, train_loss=8.718149

Batch 98990, train_perplexity=5420.107, train_loss=8.597871

Batch 99000, train_perplexity=4541.813, train_loss=8.421082

Batch 99010, train_perplexity=5070.761, train_loss=8.531246

Batch 99020, train_perplexity=6592.1724, train_loss=8.793638

Batch 99030, train_perplexity=5150.5747, train_loss=8.546864

Batch 99040, train_perplexity=6568.564, train_loss=8.7900505

Batch 99050, train_perplexity=5533.554, train_loss=8.618586

Batch 99060, train_perplexity=5264.3916, train_loss=8.568721

Batch 99070, train_perplexity=5856.527, train_loss=8.675312

Batch 99080, train_perplexity=5510.4253, train_loss=8.614397

Batch 99090, train_perplexity=4710.9077, train_loss=8.457636

Batch 99100, train_perplexity=5166.44, train_loss=8.549939

Batch 99110, train_perplexity=6320.7827, train_loss=8.751598

Batch 99120, train_perplexity=5191.4365, train_loss=8.554766

Batch 99130, train_perplexity=4115.1543, train_loss=8.322432

Batch 99140, train_perplexity=5018.419, train_loss=8.52087

Batch 99150, train_perplexity=4397.5947, train_loss=8.388813

Batch 99160, train_perplexity=5883.274, train_loss=8.679869

Batch 99170, train_perplexity=5200.7275, train_loss=8.556554

Batch 99180, train_perplexity=6240.7007, train_loss=8.738848

Batch 99190, train_perplexity=6064.2314, train_loss=8.710163

Batch 99200, train_perplexity=5494.31, train_loss=8.611468

Batch 99210, train_perplexity=4739.7666, train_loss=8.463743

Batch 99220, train_perplexity=5729.362, train_loss=8.653359
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 99230, train_perplexity=4386.131, train_loss=8.386203

Batch 99240, train_perplexity=5475.4795, train_loss=8.608035

Batch 99250, train_perplexity=5827.122, train_loss=8.670279

Batch 99260, train_perplexity=4840.2153, train_loss=8.4847145

Batch 99270, train_perplexity=5107.8613, train_loss=8.538536

Batch 99280, train_perplexity=5284.0933, train_loss=8.572456

Batch 99290, train_perplexity=4636.5317, train_loss=8.441722

Batch 99300, train_perplexity=5397.9517, train_loss=8.593775

Batch 99310, train_perplexity=5856.806, train_loss=8.67536

Batch 99320, train_perplexity=4805.9346, train_loss=8.477607

Batch 99330, train_perplexity=5417.559, train_loss=8.597401

Batch 99340, train_perplexity=5412.7256, train_loss=8.596508

Batch 99350, train_perplexity=5636.342, train_loss=8.636991

Batch 99360, train_perplexity=6204.7734, train_loss=8.733074

Batch 99370, train_perplexity=6092.2876, train_loss=8.714779

Batch 99380, train_perplexity=5400.2993, train_loss=8.59421

Batch 99390, train_perplexity=6242.9746, train_loss=8.739212

Batch 99400, train_perplexity=5027.3335, train_loss=8.522645

Batch 99410, train_perplexity=5619.816, train_loss=8.634054

Batch 99420, train_perplexity=5018.4478, train_loss=8.520876

Batch 99430, train_perplexity=4909.5503, train_loss=8.498938

Batch 99440, train_perplexity=4463.6826, train_loss=8.403729

Batch 99450, train_perplexity=6038.539, train_loss=8.705917

Batch 99460, train_perplexity=5629.643, train_loss=8.635801

Batch 99470, train_perplexity=5764.2236, train_loss=8.659426

Batch 99480, train_perplexity=5130.054, train_loss=8.542871

Batch 99490, train_perplexity=4657.9517, train_loss=8.446331

Batch 99500, train_perplexity=6501.1826, train_loss=8.779739

Batch 99510, train_perplexity=5268.2534, train_loss=8.569454

Batch 99520, train_perplexity=6504.4756, train_loss=8.780246

Batch 99530, train_perplexity=5213.714, train_loss=8.559048

Batch 99540, train_perplexity=6188.3213, train_loss=8.730419

Batch 99550, train_perplexity=5520.819, train_loss=8.6162815

Batch 99560, train_perplexity=4717.8047, train_loss=8.459099

Batch 99570, train_perplexity=5255.0366, train_loss=8.566942

Batch 99580, train_perplexity=5148.2026, train_loss=8.546403

Batch 99590, train_perplexity=5408.773, train_loss=8.5957775

Batch 99600, train_perplexity=4733.154, train_loss=8.462347

Batch 99610, train_perplexity=4615.519, train_loss=8.43718

Batch 99620, train_perplexity=4702.9624, train_loss=8.455948

Batch 99630, train_perplexity=4640.721, train_loss=8.442625

Batch 99640, train_perplexity=5875.049, train_loss=8.67847

Batch 99650, train_perplexity=4713.7837, train_loss=8.458246

Batch 99660, train_perplexity=4938.5713, train_loss=8.504831

Batch 99670, train_perplexity=5434.3457, train_loss=8.600494

Batch 99680, train_perplexity=4621.5225, train_loss=8.438479

Batch 99690, train_perplexity=6020.8223, train_loss=8.702979

Batch 99700, train_perplexity=4643.6343, train_loss=8.443253

Batch 99710, train_perplexity=5503.991, train_loss=8.613229

Batch 99720, train_perplexity=5747.169, train_loss=8.656463

Batch 99730, train_perplexity=5346.0083, train_loss=8.5841055

Batch 99740, train_perplexity=5912.968, train_loss=8.684903

Batch 99750, train_perplexity=5503.881, train_loss=8.613209

Batch 99760, train_perplexity=4697.758, train_loss=8.454841

Batch 99770, train_perplexity=5568.2915, train_loss=8.624844

Batch 99780, train_perplexity=5177.9873, train_loss=8.552172

Batch 99790, train_perplexity=5491.3296, train_loss=8.610926

Batch 99800, train_perplexity=5457.707, train_loss=8.604784

Batch 99810, train_perplexity=6562.9785, train_loss=8.7892

Batch 99820, train_perplexity=4465.688, train_loss=8.404179

Batch 99830, train_perplexity=5336.478, train_loss=8.582321

Batch 99840, train_perplexity=5241.1274, train_loss=8.564292

Batch 99850, train_perplexity=5299.875, train_loss=8.5754385

Batch 99860, train_perplexity=4094.1958, train_loss=8.317326

Batch 99870, train_perplexity=5322.8765, train_loss=8.579769

Batch 99880, train_perplexity=5664.908, train_loss=8.642046

Batch 99890, train_perplexity=5034.698, train_loss=8.524109

Batch 99900, train_perplexity=5903.012, train_loss=8.683218

Batch 99910, train_perplexity=5630.0405, train_loss=8.635872

Batch 99920, train_perplexity=4883.3726, train_loss=8.493591

Batch 99930, train_perplexity=5039.7134, train_loss=8.5251045

Batch 99940, train_perplexity=5614.5767, train_loss=8.6331215

Batch 99950, train_perplexity=4761.0547, train_loss=8.468225

Batch 99960, train_perplexity=5936.586, train_loss=8.6888895

Batch 99970, train_perplexity=6280.069, train_loss=8.745136

Batch 99980, train_perplexity=5447.8525, train_loss=8.602977

Batch 99990, train_perplexity=5903.541, train_loss=8.683308

Batch 100000, train_perplexity=5127.941, train_loss=8.5424595

Batch 100010, train_perplexity=5082.448, train_loss=8.533548

Batch 100020, train_perplexity=6138.735, train_loss=8.722374

Batch 100030, train_perplexity=5118.1597, train_loss=8.54055

Batch 100040, train_perplexity=4848.9385, train_loss=8.486515

Batch 100050, train_perplexity=7257.312, train_loss=8.889765

Batch 100060, train_perplexity=6112.68, train_loss=8.718121

Batch 100070, train_perplexity=5851.3853, train_loss=8.674434

Batch 100080, train_perplexity=4854.3057, train_loss=8.487621

Batch 100090, train_perplexity=6849.684, train_loss=8.831958

Batch 100100, train_perplexity=3987.563, train_loss=8.2909355

Batch 100110, train_perplexity=5390.333, train_loss=8.592362

Batch 100120, train_perplexity=5289.115, train_loss=8.573406

Batch 100130, train_perplexity=5233.6206, train_loss=8.562859

Batch 100140, train_perplexity=5236.556, train_loss=8.563419

Batch 100150, train_perplexity=4586.113, train_loss=8.430788

Batch 100160, train_perplexity=5883.386, train_loss=8.679888

Batch 100170, train_perplexity=5364.22, train_loss=8.587506

Batch 100180, train_perplexity=5379.2456, train_loss=8.590303

Batch 100190, train_perplexity=5012.5356, train_loss=8.519697

Batch 100200, train_perplexity=5244.6274, train_loss=8.56496

Batch 100210, train_perplexity=5145.4688, train_loss=8.545872

Batch 100220, train_perplexity=6314.427, train_loss=8.750592

Batch 100230, train_perplexity=6034.6357, train_loss=8.705271

Batch 100240, train_perplexity=5840.5303, train_loss=8.672577

Batch 100250, train_perplexity=5530.795, train_loss=8.618087

Batch 100260, train_perplexity=5502.779, train_loss=8.6130085

Batch 100270, train_perplexity=5105.7183, train_loss=8.538116

Batch 100280, train_perplexity=6517.223, train_loss=8.782204

Batch 100290, train_perplexity=4633.959, train_loss=8.441167

Batch 100300, train_perplexity=5299.0815, train_loss=8.575289

Batch 100310, train_perplexity=5333.9136, train_loss=8.5818405

Batch 100320, train_perplexity=5893.843, train_loss=8.6816635

Batch 100330, train_perplexity=4494.588, train_loss=8.410629

Batch 100340, train_perplexity=5059.5396, train_loss=8.529031

Batch 100350, train_perplexity=6158.402, train_loss=8.725573

Batch 100360, train_perplexity=6084.89, train_loss=8.713564

Batch 100370, train_perplexity=5692.2676, train_loss=8.646864

Batch 100380, train_perplexity=5882.5615, train_loss=8.679748

Batch 100390, train_perplexity=4848.4297, train_loss=8.48641

Batch 100400, train_perplexity=8278.331, train_loss=9.021397

Batch 100410, train_perplexity=5312.455, train_loss=8.577809

Batch 100420, train_perplexity=5541.2534, train_loss=8.619976

Batch 100430, train_perplexity=5239.4233, train_loss=8.563967

Batch 100440, train_perplexity=4856.792, train_loss=8.488133

Batch 100450, train_perplexity=5535.8237, train_loss=8.618996

Batch 100460, train_perplexity=5306.6924, train_loss=8.576724

Batch 100470, train_perplexity=5206.9062, train_loss=8.557741

Batch 100480, train_perplexity=5139.848, train_loss=8.544779

Batch 100490, train_perplexity=5372.519, train_loss=8.589052

Batch 100500, train_perplexity=5230.2026, train_loss=8.562205

Batch 100510, train_perplexity=6509.906, train_loss=8.78108

Batch 100520, train_perplexity=4756.875, train_loss=8.467346

Batch 100530, train_perplexity=5093.997, train_loss=8.535818

Batch 100540, train_perplexity=5335.796, train_loss=8.582193

Batch 100550, train_perplexity=4614.119, train_loss=8.436876
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 100560, train_perplexity=4763.7207, train_loss=8.468784

Batch 100570, train_perplexity=4976.2134, train_loss=8.512424

Batch 100580, train_perplexity=5765.6694, train_loss=8.659677

Batch 100590, train_perplexity=5769.2114, train_loss=8.660291

Batch 100600, train_perplexity=5126.88, train_loss=8.542253

Batch 100610, train_perplexity=5996.428, train_loss=8.698919

Batch 100620, train_perplexity=5654.049, train_loss=8.640127

Batch 100630, train_perplexity=5870.9546, train_loss=8.6777725

Batch 100640, train_perplexity=5946.281, train_loss=8.690521

Batch 100650, train_perplexity=5207.1895, train_loss=8.557796

Batch 100660, train_perplexity=5360.385, train_loss=8.586791

Batch 100670, train_perplexity=4976.887, train_loss=8.51256

Batch 100680, train_perplexity=4797.6597, train_loss=8.4758835

Batch 100690, train_perplexity=5305.1797, train_loss=8.576439

Batch 100700, train_perplexity=6481.4966, train_loss=8.776707

Batch 100710, train_perplexity=5830.697, train_loss=8.670892

Batch 100720, train_perplexity=5042.348, train_loss=8.525627

Batch 100730, train_perplexity=6247.656, train_loss=8.739962

Batch 100740, train_perplexity=5093.0693, train_loss=8.535636

Batch 100750, train_perplexity=5370.598, train_loss=8.588695

Batch 100760, train_perplexity=5387.568, train_loss=8.591849

Batch 100770, train_perplexity=5716.3125, train_loss=8.651079

Batch 100780, train_perplexity=5419.6885, train_loss=8.597794

Batch 100790, train_perplexity=6473.5645, train_loss=8.775482

Batch 100800, train_perplexity=4218.845, train_loss=8.347317

Batch 100810, train_perplexity=6456.289, train_loss=8.77281

Batch 100820, train_perplexity=6110.698, train_loss=8.717796

Batch 100830, train_perplexity=4777.602, train_loss=8.471694

Batch 100840, train_perplexity=4768.284, train_loss=8.469742

Batch 100850, train_perplexity=5768.975, train_loss=8.66025

Batch 100860, train_perplexity=5902.3813, train_loss=8.683111

Batch 100870, train_perplexity=5074.278, train_loss=8.5319395

Batch 100880, train_perplexity=5504.789, train_loss=8.613374

Batch 100890, train_perplexity=4739.5225, train_loss=8.463692

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Batch 100900, train_perplexity=4941.9727, train_loss=8.50552

Batch 100910, train_perplexity=5632.6826, train_loss=8.636341

Batch 100920, train_perplexity=5650.836, train_loss=8.639559

Batch 100930, train_perplexity=4677.11, train_loss=8.450436

Batch 100940, train_perplexity=5818.665, train_loss=8.668826

Batch 100950, train_perplexity=5564.4272, train_loss=8.624149

Batch 100960, train_perplexity=4079.5645, train_loss=8.3137455

Batch 100970, train_perplexity=5599.7, train_loss=8.630468

Batch 100980, train_perplexity=5056.0767, train_loss=8.528346

Batch 100990, train_perplexity=5429.818, train_loss=8.599661

Batch 101000, train_perplexity=5275.2017, train_loss=8.570772

Batch 101010, train_perplexity=5225.232, train_loss=8.5612545

Batch 101020, train_perplexity=5929.106, train_loss=8.687629

Batch 101030, train_perplexity=4746.9414, train_loss=8.465256

Batch 101040, train_perplexity=6148.461, train_loss=8.723957

Batch 101050, train_perplexity=4721.9995, train_loss=8.459988

Batch 101060, train_perplexity=4558.076, train_loss=8.424656

Batch 101070, train_perplexity=5311.786, train_loss=8.577683

Batch 101080, train_perplexity=4209.3364, train_loss=8.34506

Batch 101090, train_perplexity=5476.947, train_loss=8.608303

Batch 101100, train_perplexity=5222.432, train_loss=8.560719

Batch 101110, train_perplexity=6264.26, train_loss=8.742616

Batch 101120, train_perplexity=4904.3887, train_loss=8.497886

Batch 101130, train_perplexity=5526.0444, train_loss=8.617228

Batch 101140, train_perplexity=5806.032, train_loss=8.666653

Batch 101150, train_perplexity=5617.78, train_loss=8.633692

Batch 101160, train_perplexity=4878.8804, train_loss=8.492671

Batch 101170, train_perplexity=5575.396, train_loss=8.626119

Batch 101180, train_perplexity=5682.3257, train_loss=8.645116

Batch 101190, train_perplexity=4932.8804, train_loss=8.503678

Batch 101200, train_perplexity=6136.8384, train_loss=8.722065

Batch 101210, train_perplexity=4473.489, train_loss=8.405924

Batch 101220, train_perplexity=4600.051, train_loss=8.433823

Batch 101230, train_perplexity=5009.6016, train_loss=8.519112

Batch 101240, train_perplexity=4749.9434, train_loss=8.465888

Batch 101250, train_perplexity=5999.506, train_loss=8.699432

Batch 101260, train_perplexity=5906.875, train_loss=8.683872

Batch 101270, train_perplexity=5437.8086, train_loss=8.601131

Batch 101280, train_perplexity=5569.221, train_loss=8.6250105

Batch 101290, train_perplexity=5621.2524, train_loss=8.63431

Batch 101300, train_perplexity=4913.0396, train_loss=8.499648

Batch 101310, train_perplexity=4679.6353, train_loss=8.450975

Batch 101320, train_perplexity=5041.0884, train_loss=8.525377

Batch 101330, train_perplexity=4891.837, train_loss=8.495323

Batch 101340, train_perplexity=5525.454, train_loss=8.617121

Batch 101350, train_perplexity=4918.905, train_loss=8.500841

Batch 101360, train_perplexity=5347.686, train_loss=8.584419

Batch 101370, train_perplexity=5432.7393, train_loss=8.600199

Batch 101380, train_perplexity=5760.3877, train_loss=8.65876

Batch 101390, train_perplexity=5679.286, train_loss=8.644581

Batch 101400, train_perplexity=5641.1763, train_loss=8.637848

Batch 101410, train_perplexity=5908.509, train_loss=8.684149

Batch 101420, train_perplexity=6078.3477, train_loss=8.712488

Batch 101430, train_perplexity=4512.7944, train_loss=8.414672

Batch 101440, train_perplexity=5890.432, train_loss=8.681085

Batch 101450, train_perplexity=5250.8286, train_loss=8.566141

Batch 101460, train_perplexity=6425.9688, train_loss=8.768103

Batch 101470, train_perplexity=4528.0083, train_loss=8.418037

Batch 101480, train_perplexity=4199.257, train_loss=8.342663

Batch 101490, train_perplexity=6293.061, train_loss=8.747203

Batch 101500, train_perplexity=5275.031, train_loss=8.57074

Batch 101510, train_perplexity=5123.0234, train_loss=8.5415

Batch 101520, train_perplexity=5134.6353, train_loss=8.543764

Batch 101530, train_perplexity=6365.0864, train_loss=8.758583

Batch 101540, train_perplexity=5595.5205, train_loss=8.629722

Batch 101550, train_perplexity=5534.8154, train_loss=8.6188135

Batch 101560, train_perplexity=5073.992, train_loss=8.531883

Batch 101570, train_perplexity=4994.5273, train_loss=8.516098

Batch 101580, train_perplexity=5481.9736, train_loss=8.6092205

Batch 101590, train_perplexity=6178.5796, train_loss=8.728844

Batch 101600, train_perplexity=4949.5854, train_loss=8.507059

Batch 101610, train_perplexity=5980.7856, train_loss=8.696307

Batch 101620, train_perplexity=4858.242, train_loss=8.488432

Batch 101630, train_perplexity=5730.5474, train_loss=8.653566

Batch 101640, train_perplexity=5970.7783, train_loss=8.694633

Batch 101650, train_perplexity=5875.1665, train_loss=8.67849

Batch 101660, train_perplexity=4675.0186, train_loss=8.449988

Batch 101670, train_perplexity=5095.2314, train_loss=8.53606

Batch 101680, train_perplexity=5505.603, train_loss=8.613522

Batch 101690, train_perplexity=5149.9653, train_loss=8.546745

Batch 101700, train_perplexity=5368.801, train_loss=8.58836

Batch 101710, train_perplexity=4809.6714, train_loss=8.478384

Batch 101720, train_perplexity=6845.485, train_loss=8.831345

Batch 101730, train_perplexity=6175.434, train_loss=8.728334

Batch 101740, train_perplexity=5998.7505, train_loss=8.6993065

Batch 101750, train_perplexity=5368.6216, train_loss=8.588326

Batch 101760, train_perplexity=5724.91, train_loss=8.652582

Batch 101770, train_perplexity=5296.237, train_loss=8.574752

Batch 101780, train_perplexity=6332.174, train_loss=8.753399

Batch 101790, train_perplexity=4580.981, train_loss=8.429668

Batch 101800, train_perplexity=4269.508, train_loss=8.359254
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 101810, train_perplexity=4868.548, train_loss=8.490551

Batch 101820, train_perplexity=5798.8716, train_loss=8.665419

Batch 101830, train_perplexity=5606.1978, train_loss=8.631628

Batch 101840, train_perplexity=5599.38, train_loss=8.630411

Batch 101850, train_perplexity=5955.9233, train_loss=8.692142

Batch 101860, train_perplexity=6174.798, train_loss=8.728231

Batch 101870, train_perplexity=5822.634, train_loss=8.669508

Batch 101880, train_perplexity=5665.0596, train_loss=8.642073

Batch 101890, train_perplexity=5601.837, train_loss=8.63085

Batch 101900, train_perplexity=5171.108, train_loss=8.550842

Batch 101910, train_perplexity=4986.6836, train_loss=8.514526

Batch 101920, train_perplexity=6272.563, train_loss=8.74394

Batch 101930, train_perplexity=5421.1562, train_loss=8.598064

Batch 101940, train_perplexity=5827.4893, train_loss=8.6703415

Batch 101950, train_perplexity=5010.8247, train_loss=8.519356

Batch 101960, train_perplexity=5136.957, train_loss=8.544216

Batch 101970, train_perplexity=6453.525, train_loss=8.772382

Batch 101980, train_perplexity=5588.7954, train_loss=8.628519

Batch 101990, train_perplexity=5983.2783, train_loss=8.696724

Batch 102000, train_perplexity=4882.7856, train_loss=8.493471

Batch 102010, train_perplexity=5440.895, train_loss=8.601699

Batch 102020, train_perplexity=5912.9116, train_loss=8.684894

Batch 102030, train_perplexity=4837.8247, train_loss=8.4842205

Batch 102040, train_perplexity=5067.88, train_loss=8.530678

Batch 102050, train_perplexity=5158.8433, train_loss=8.548468

Batch 102060, train_perplexity=5084.5522, train_loss=8.533962

Batch 102070, train_perplexity=4816.088, train_loss=8.479717

Batch 102080, train_perplexity=5725.664, train_loss=8.652714

Batch 102090, train_perplexity=6601.055, train_loss=8.794985

Batch 102100, train_perplexity=5357.7476, train_loss=8.586299

Batch 102110, train_perplexity=5090.3257, train_loss=8.535097

Batch 102120, train_perplexity=5360.451, train_loss=8.586803

Batch 102130, train_perplexity=5721.5264, train_loss=8.651991

Batch 102140, train_perplexity=5434.159, train_loss=8.60046

Batch 102150, train_perplexity=4943.844, train_loss=8.505898

Batch 102160, train_perplexity=4907.6167, train_loss=8.498544

Batch 102170, train_perplexity=5156.0347, train_loss=8.547923

Batch 102180, train_perplexity=5334.0103, train_loss=8.581859

Batch 102190, train_perplexity=5641.7627, train_loss=8.637952

Batch 102200, train_perplexity=5585.8594, train_loss=8.627994

Batch 102210, train_perplexity=4533.1323, train_loss=8.419168

Batch 102220, train_perplexity=5119.4585, train_loss=8.540804

Batch 102230, train_perplexity=4655.3667, train_loss=8.445776

Batch 102240, train_perplexity=5552.733, train_loss=8.6220455

Batch 102250, train_perplexity=5371.894, train_loss=8.588936

Batch 102260, train_perplexity=5199.929, train_loss=8.5564

Batch 102270, train_perplexity=4880.877, train_loss=8.49308

Batch 102280, train_perplexity=5353.9014, train_loss=8.585581

Batch 102290, train_perplexity=5901.757, train_loss=8.683005

Batch 102300, train_perplexity=5476.9365, train_loss=8.608301

Batch 102310, train_perplexity=5154.24, train_loss=8.547575

Batch 102320, train_perplexity=5159.389, train_loss=8.5485735

Batch 102330, train_perplexity=4776.9688, train_loss=8.471561

Batch 102340, train_perplexity=5622.0674, train_loss=8.634455

Batch 102350, train_perplexity=4269.6343, train_loss=8.359283

Batch 102360, train_perplexity=5092.778, train_loss=8.535579

Batch 102370, train_perplexity=6214.58, train_loss=8.734653

Batch 102380, train_perplexity=5944.3306, train_loss=8.690193

Batch 102390, train_perplexity=5748.441, train_loss=8.656684

Batch 102400, train_perplexity=4822.2466, train_loss=8.480995

Batch 102410, train_perplexity=4741.991, train_loss=8.464212

Batch 102420, train_perplexity=5396.896, train_loss=8.593579

Batch 102430, train_perplexity=6470.565, train_loss=8.775019

Batch 102440, train_perplexity=4946.117, train_loss=8.506358

Batch 102450, train_perplexity=5623.386, train_loss=8.634689

Batch 102460, train_perplexity=5421.5596, train_loss=8.598139

Batch 102470, train_perplexity=4871.139, train_loss=8.491083

Batch 102480, train_perplexity=5353.82, train_loss=8.585566

Batch 102490, train_perplexity=4397.3433, train_loss=8.388756

Batch 102500, train_perplexity=5811.406, train_loss=8.667578

Batch 102510, train_perplexity=5854.997, train_loss=8.675051

Batch 102520, train_perplexity=5006.43, train_loss=8.518478

Batch 102530, train_perplexity=5657.172, train_loss=8.640679

Batch 102540, train_perplexity=4458.9263, train_loss=8.402663

Batch 102550, train_perplexity=4797.916, train_loss=8.475937

Batch 102560, train_perplexity=6116.4297, train_loss=8.718734

Batch 102570, train_perplexity=5673.029, train_loss=8.643478

Batch 102580, train_perplexity=7100.2656, train_loss=8.8678875

Batch 102590, train_perplexity=6609.925, train_loss=8.796328

Batch 102600, train_perplexity=5219.783, train_loss=8.560211

Batch 102610, train_perplexity=4611.937, train_loss=8.436403

Batch 102620, train_perplexity=4319.411, train_loss=8.370874

Batch 102630, train_perplexity=4939.504, train_loss=8.50502

Batch 102640, train_perplexity=6131.86, train_loss=8.721253

Batch 102650, train_perplexity=5939.5024, train_loss=8.689381

Batch 102660, train_perplexity=4997.076, train_loss=8.516608

Batch 102670, train_perplexity=5605.4814, train_loss=8.6315

Batch 102680, train_perplexity=5213.664, train_loss=8.559038

Batch 102690, train_perplexity=5127.6865, train_loss=8.54241

Batch 102700, train_perplexity=4692.394, train_loss=8.453698

Batch 102710, train_perplexity=6341.771, train_loss=8.754913

Batch 102720, train_perplexity=6590.1484, train_loss=8.793331

Batch 102730, train_perplexity=6348.657, train_loss=8.755999

Batch 102740, train_perplexity=6374.6357, train_loss=8.760082

Batch 102750, train_perplexity=5765.8564, train_loss=8.659709

Batch 102760, train_perplexity=5459.9453, train_loss=8.605194

Batch 102770, train_perplexity=6693.4556, train_loss=8.808886

Batch 102780, train_perplexity=5886.6304, train_loss=8.680439

Batch 102790, train_perplexity=5618.5728, train_loss=8.633833

Batch 102800, train_perplexity=4670.2812, train_loss=8.448975

Batch 102810, train_perplexity=5459.8467, train_loss=8.605176

Batch 102820, train_perplexity=5233.0366, train_loss=8.562747

Batch 102830, train_perplexity=6309.4004, train_loss=8.749796

Batch 102840, train_perplexity=4537.8037, train_loss=8.420198

Batch 102850, train_perplexity=6120.625, train_loss=8.7194195

Batch 102860, train_perplexity=5215.0864, train_loss=8.559311

Batch 102870, train_perplexity=5201.5757, train_loss=8.556717

Batch 102880, train_perplexity=5096.0767, train_loss=8.536226

Batch 102890, train_perplexity=6498.9756, train_loss=8.7794

Batch 102900, train_perplexity=5719.1367, train_loss=8.651573

Batch 102910, train_perplexity=5873.8726, train_loss=8.678269

Batch 102920, train_perplexity=5404.8022, train_loss=8.595043

Batch 102930, train_perplexity=5537.994, train_loss=8.619388

Batch 102940, train_perplexity=6076.8525, train_loss=8.712242

Batch 102950, train_perplexity=5581.477, train_loss=8.627209

Batch 102960, train_perplexity=6750.0654, train_loss=8.817307

Batch 102970, train_perplexity=6241.772, train_loss=8.739019

Batch 102980, train_perplexity=5162.623, train_loss=8.5492

Batch 102990, train_perplexity=4474.133, train_loss=8.406068

Batch 103000, train_perplexity=4366.511, train_loss=8.38172

Batch 103010, train_perplexity=5050.9727, train_loss=8.527336

Batch 103020, train_perplexity=5975.1587, train_loss=8.695366

Batch 103030, train_perplexity=5531.903, train_loss=8.618287

Batch 103040, train_perplexity=4994.3413, train_loss=8.516061

Batch 103050, train_perplexity=4705.7617, train_loss=8.456543

Batch 103060, train_perplexity=6401.4165, train_loss=8.764275

Batch 103070, train_perplexity=5652.1406, train_loss=8.63979

Batch 103080, train_perplexity=5785.758, train_loss=8.663155

Batch 103090, train_perplexity=5367.4287, train_loss=8.588104

Batch 103100, train_perplexity=5488.586, train_loss=8.610426

Batch 103110, train_perplexity=5392.652, train_loss=8.5927925

Batch 103120, train_perplexity=4466.2334, train_loss=8.404301
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 103130, train_perplexity=5302.1045, train_loss=8.575859

Batch 103140, train_perplexity=6064.515, train_loss=8.71021

Batch 103150, train_perplexity=5473.527, train_loss=8.607678

Batch 103160, train_perplexity=5784.5713, train_loss=8.66295

Batch 103170, train_perplexity=5465.791, train_loss=8.606264

Batch 103180, train_perplexity=6528.2085, train_loss=8.783888

Batch 103190, train_perplexity=6283.8794, train_loss=8.745743

Batch 103200, train_perplexity=4913.6675, train_loss=8.499776

Batch 103210, train_perplexity=4936.8525, train_loss=8.504483

Batch 103220, train_perplexity=5481.6235, train_loss=8.609157

Batch 103230, train_perplexity=4495.501, train_loss=8.410832

Batch 103240, train_perplexity=6010.948, train_loss=8.701338

Batch 103250, train_perplexity=5986.749, train_loss=8.697304

Batch 103260, train_perplexity=5616.869, train_loss=8.63353

Batch 103270, train_perplexity=5132.912, train_loss=8.543428

Batch 103280, train_perplexity=5411.425, train_loss=8.596268

Batch 103290, train_perplexity=5277.179, train_loss=8.571147

Batch 103300, train_perplexity=6802.131, train_loss=8.824991

Batch 103310, train_perplexity=5192.734, train_loss=8.555016

Batch 103320, train_perplexity=4921.7856, train_loss=8.501427

Batch 103330, train_perplexity=6889.9565, train_loss=8.83782

Batch 103340, train_perplexity=6441.025, train_loss=8.770443

Batch 103350, train_perplexity=5219.3105, train_loss=8.560121

Batch 103360, train_perplexity=4725.856, train_loss=8.460804

Batch 103370, train_perplexity=6211.15, train_loss=8.734101

Batch 103380, train_perplexity=4800.383, train_loss=8.476451

Batch 103390, train_perplexity=6318.3843, train_loss=8.751219

Batch 103400, train_perplexity=4937.9873, train_loss=8.504713

Batch 103410, train_perplexity=5365.438, train_loss=8.587733

Batch 103420, train_perplexity=5299.299, train_loss=8.57533

Batch 103430, train_perplexity=6540.46, train_loss=8.785763

Batch 103440, train_perplexity=4789.9883, train_loss=8.474283

Batch 103450, train_perplexity=6162.4204, train_loss=8.726225

Batch 103460, train_perplexity=4865.206, train_loss=8.489864

Batch 103470, train_perplexity=5544.1396, train_loss=8.620497

Batch 103480, train_perplexity=5665.1675, train_loss=8.642092

Batch 103490, train_perplexity=5124.475, train_loss=8.541783

Batch 103500, train_perplexity=4801.88, train_loss=8.476763

Batch 103510, train_perplexity=5670.3457, train_loss=8.643005

Batch 103520, train_perplexity=5119.7026, train_loss=8.540852

Batch 103530, train_perplexity=4649.151, train_loss=8.44444

Batch 103540, train_perplexity=5435.3096, train_loss=8.600672

Batch 103550, train_perplexity=6281.1294, train_loss=8.745305

Batch 103560, train_perplexity=5026.782, train_loss=8.522535

Batch 103570, train_perplexity=4940.4414, train_loss=8.50521

Batch 103580, train_perplexity=5397.246, train_loss=8.593644

Batch 103590, train_perplexity=6493.598, train_loss=8.778572

Batch 103600, train_perplexity=7004.6675, train_loss=8.854332

Batch 103610, train_perplexity=4870.749, train_loss=8.491003

Batch 103620, train_perplexity=5477.793, train_loss=8.608458

Batch 103630, train_perplexity=4730.87, train_loss=8.461864

Batch 103640, train_perplexity=4897.336, train_loss=8.496447

Batch 103650, train_perplexity=4852.3706, train_loss=8.487223

Batch 103660, train_perplexity=5836.711, train_loss=8.671923

Batch 103670, train_perplexity=4968.389, train_loss=8.510851

Batch 103680, train_perplexity=6452.1895, train_loss=8.772175

Batch 103690, train_perplexity=5436.3413, train_loss=8.600862

Batch 103700, train_perplexity=5299.976, train_loss=8.575458

Batch 103710, train_perplexity=5676.2646, train_loss=8.644049

Batch 103720, train_perplexity=5253.904, train_loss=8.566727

Batch 103730, train_perplexity=6229.284, train_loss=8.737017

Batch 103740, train_perplexity=5409.887, train_loss=8.5959835

Batch 103750, train_perplexity=4969.9243, train_loss=8.51116

Batch 103760, train_perplexity=5861.075, train_loss=8.676088

Batch 103770, train_perplexity=4879.3223, train_loss=8.492762

Batch 103780, train_perplexity=4947.2354, train_loss=8.506584

Batch 103790, train_perplexity=4686.6826, train_loss=8.45248

Batch 103800, train_perplexity=5406.772, train_loss=8.5954075

Batch 103810, train_perplexity=6663.801, train_loss=8.804445

Batch 103820, train_perplexity=5093.2783, train_loss=8.535677

Batch 103830, train_perplexity=4535.662, train_loss=8.419726

Batch 103840, train_perplexity=5399.46, train_loss=8.594054

Batch 103850, train_perplexity=5809.4995, train_loss=8.66725

Batch 103860, train_perplexity=4701.1597, train_loss=8.4555645

Batch 103870, train_perplexity=4549.855, train_loss=8.422851

Batch 103880, train_perplexity=5001.0093, train_loss=8.517395

Batch 103890, train_perplexity=4874.7085, train_loss=8.491816

Batch 103900, train_perplexity=6417.7007, train_loss=8.766815

Batch 103910, train_perplexity=5494.2104, train_loss=8.61145

Batch 103920, train_perplexity=5353.626, train_loss=8.585529

Batch 103930, train_perplexity=5251.144, train_loss=8.566201

Batch 103940, train_perplexity=6148.977, train_loss=8.724041

Batch 103950, train_perplexity=5511.981, train_loss=8.614679

Batch 103960, train_perplexity=6185.1177, train_loss=8.729901

Batch 103970, train_perplexity=5377.4194, train_loss=8.589964

Batch 103980, train_perplexity=6067.095, train_loss=8.710635

Batch 103990, train_perplexity=5949.2812, train_loss=8.691026

Batch 104000, train_perplexity=6409.902, train_loss=8.765599

Batch 104010, train_perplexity=4844.3486, train_loss=8.485568

Batch 104020, train_perplexity=4754.48, train_loss=8.466843

Batch 104030, train_perplexity=5093.463, train_loss=8.535713

Batch 104040, train_perplexity=6825.694, train_loss=8.828449

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Batch 104050, train_perplexity=5873.0044, train_loss=8.678122

Batch 104060, train_perplexity=5252.952, train_loss=8.5665455

Batch 104070, train_perplexity=5489.403, train_loss=8.610575

Batch 104080, train_perplexity=5381.2363, train_loss=8.590673

Batch 104090, train_perplexity=4827.612, train_loss=8.482107

Batch 104100, train_perplexity=4912.276, train_loss=8.499493

Batch 104110, train_perplexity=5680.662, train_loss=8.644823

Batch 104120, train_perplexity=5127.931, train_loss=8.542458

Batch 104130, train_perplexity=5643.1455, train_loss=8.638197

Batch 104140, train_perplexity=5357.482, train_loss=8.586249

Batch 104150, train_perplexity=4901.0596, train_loss=8.497207

Batch 104160, train_perplexity=5447.3696, train_loss=8.602888

Batch 104170, train_perplexity=5559.1973, train_loss=8.623209

Batch 104180, train_perplexity=5640.9126, train_loss=8.637801

Batch 104190, train_perplexity=5658.9414, train_loss=8.640992

Batch 104200, train_perplexity=7044.5205, train_loss=8.860005

Batch 104210, train_perplexity=5112.296, train_loss=8.539404

Batch 104220, train_perplexity=5374.718, train_loss=8.589461

Batch 104230, train_perplexity=5054.423, train_loss=8.528019

Batch 104240, train_perplexity=5079.3228, train_loss=8.532933

Batch 104250, train_perplexity=5446.1953, train_loss=8.602673

Batch 104260, train_perplexity=5040.0596, train_loss=8.525173

Batch 104270, train_perplexity=4812.383, train_loss=8.478948

Batch 104280, train_perplexity=4534.8706, train_loss=8.419552

Batch 104290, train_perplexity=5314.3955, train_loss=8.578175

Batch 104300, train_perplexity=4998.2817, train_loss=8.5168495

Batch 104310, train_perplexity=5714.4917, train_loss=8.650761

Batch 104320, train_perplexity=4770.1216, train_loss=8.470127

Batch 104330, train_perplexity=5417.2236, train_loss=8.597339

Batch 104340, train_perplexity=4582.3267, train_loss=8.429962

Batch 104350, train_perplexity=4869.3467, train_loss=8.490715

Batch 104360, train_perplexity=5960.202, train_loss=8.69286

Batch 104370, train_perplexity=6774.1973, train_loss=8.820876
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 104380, train_perplexity=5379.9946, train_loss=8.590443

Batch 104390, train_perplexity=5076.761, train_loss=8.532429

Batch 104400, train_perplexity=5266.5356, train_loss=8.569128

Batch 104410, train_perplexity=5307.892, train_loss=8.57695

Batch 104420, train_perplexity=5523.8525, train_loss=8.616831

Batch 104430, train_perplexity=6445.314, train_loss=8.771109

Batch 104440, train_perplexity=4941.822, train_loss=8.505489

Batch 104450, train_perplexity=4935.087, train_loss=8.504126

Batch 104460, train_perplexity=4902.7563, train_loss=8.497553

Batch 104470, train_perplexity=4986.5503, train_loss=8.5145

Batch 104480, train_perplexity=4352.417, train_loss=8.378487

Batch 104490, train_perplexity=5182.4336, train_loss=8.55303

Batch 104500, train_perplexity=6327.0493, train_loss=8.752589

Batch 104510, train_perplexity=5959.537, train_loss=8.692748

Batch 104520, train_perplexity=5770.648, train_loss=8.66054

Batch 104530, train_perplexity=5169.1655, train_loss=8.550467

Batch 104540, train_perplexity=6253.987, train_loss=8.740974

Batch 104550, train_perplexity=5491.434, train_loss=8.610945

Batch 104560, train_perplexity=4417.2153, train_loss=8.393265

Batch 104570, train_perplexity=5326.365, train_loss=8.580424

Batch 104580, train_perplexity=4459.245, train_loss=8.402735

Batch 104590, train_perplexity=5756.8457, train_loss=8.658145

Batch 104600, train_perplexity=4602.4336, train_loss=8.4343405

Batch 104610, train_perplexity=5655.3594, train_loss=8.640359

Batch 104620, train_perplexity=5622.046, train_loss=8.634451

Batch 104630, train_perplexity=3890.2686, train_loss=8.266233

Batch 104640, train_perplexity=4306.997, train_loss=8.367996

Batch 104650, train_perplexity=5808.5522, train_loss=8.667087

Batch 104660, train_perplexity=5163.007, train_loss=8.549274

Batch 104670, train_perplexity=6297.606, train_loss=8.747925

Batch 104680, train_perplexity=5274.3765, train_loss=8.570616

Batch 104690, train_perplexity=5518.45, train_loss=8.615852

Batch 104700, train_perplexity=5575.784, train_loss=8.626188

Batch 104710, train_perplexity=4930.529, train_loss=8.5032015

Batch 104720, train_perplexity=5377.7686, train_loss=8.590029

Batch 104730, train_perplexity=4872.7007, train_loss=8.491404

Batch 104740, train_perplexity=4309.9883, train_loss=8.3686905

Batch 104750, train_perplexity=6211.9077, train_loss=8.734223

Batch 104760, train_perplexity=5457.5977, train_loss=8.604764

Batch 104770, train_perplexity=4394.27, train_loss=8.388057

Batch 104780, train_perplexity=4665.6694, train_loss=8.447987

Batch 104790, train_perplexity=5186.3296, train_loss=8.5537815

Batch 104800, train_perplexity=5022.92, train_loss=8.521767

Batch 104810, train_perplexity=5573.3496, train_loss=8.6257515

Batch 104820, train_perplexity=4689.7676, train_loss=8.453138

Batch 104830, train_perplexity=5952.193, train_loss=8.691515

Batch 104840, train_perplexity=5090.6123, train_loss=8.535153

Batch 104850, train_perplexity=4496.423, train_loss=8.411037

Batch 104860, train_perplexity=5848.289, train_loss=8.673904

Batch 104870, train_perplexity=6472.972, train_loss=8.775391

Batch 104880, train_perplexity=5014.4194, train_loss=8.520073

Batch 104890, train_perplexity=5501.7085, train_loss=8.612814

Batch 104900, train_perplexity=4184.713, train_loss=8.339193

Batch 104910, train_perplexity=4648.1, train_loss=8.444214

Batch 104920, train_perplexity=6655.525, train_loss=8.803203

Batch 104930, train_perplexity=4398.387, train_loss=8.388993

Batch 104940, train_perplexity=5731.9575, train_loss=8.653812

Batch 104950, train_perplexity=5322.8154, train_loss=8.579758

Batch 104960, train_perplexity=5203.6895, train_loss=8.557123

Batch 104970, train_perplexity=5650.8145, train_loss=8.639555

Batch 104980, train_perplexity=5572.239, train_loss=8.625552

Batch 104990, train_perplexity=5425.6665, train_loss=8.598896

Batch 105000, train_perplexity=5643.49, train_loss=8.638258

Batch 105010, train_perplexity=5497.481, train_loss=8.612045

Batch 105020, train_perplexity=5636.498, train_loss=8.637018

Batch 105030, train_perplexity=5077.584, train_loss=8.532591

Batch 105040, train_perplexity=6353.224, train_loss=8.756718

Batch 105050, train_perplexity=6326.102, train_loss=8.7524395

Batch 105060, train_perplexity=5173.2095, train_loss=8.551249

Batch 105070, train_perplexity=6008.9023, train_loss=8.700997

Batch 105080, train_perplexity=5193.5957, train_loss=8.5551815

Batch 105090, train_perplexity=4925.6455, train_loss=8.502211

Batch 105100, train_perplexity=4921.34, train_loss=8.501336

Batch 105110, train_perplexity=6509.595, train_loss=8.781033

Batch 105120, train_perplexity=5278.01, train_loss=8.571304

Batch 105130, train_perplexity=5018.084, train_loss=8.520803

Batch 105140, train_perplexity=6591.0283, train_loss=8.793465

Batch 105150, train_perplexity=4576.2827, train_loss=8.428642

Batch 105160, train_perplexity=6756.119, train_loss=8.818204

Batch 105170, train_perplexity=5618.2725, train_loss=8.63378

Batch 105180, train_perplexity=5182.0186, train_loss=8.55295

Batch 105190, train_perplexity=5386.16, train_loss=8.591588

Batch 105200, train_perplexity=4815.4497, train_loss=8.479585

Batch 105210, train_perplexity=5642.543, train_loss=8.63809

Batch 105220, train_perplexity=6613.355, train_loss=8.796846

Batch 105230, train_perplexity=5869.656, train_loss=8.677551

Batch 105240, train_perplexity=4735.61, train_loss=8.462866

Batch 105250, train_perplexity=6313.9326, train_loss=8.750514

Batch 105260, train_perplexity=5005.16, train_loss=8.518225

Batch 105270, train_perplexity=6387.19, train_loss=8.76205

Batch 105280, train_perplexity=4856.797, train_loss=8.488134

Batch 105290, train_perplexity=5324.826, train_loss=8.580135

Batch 105300, train_perplexity=4590.0728, train_loss=8.431651

Batch 105310, train_perplexity=6319.9814, train_loss=8.7514715

Batch 105320, train_perplexity=4686.1646, train_loss=8.45237

Batch 105330, train_perplexity=6864.9805, train_loss=8.834188

Batch 105340, train_perplexity=5333.954, train_loss=8.581848

Batch 105350, train_perplexity=5695.808, train_loss=8.647486

Batch 105360, train_perplexity=5906.819, train_loss=8.683863

Batch 105370, train_perplexity=5148.3354, train_loss=8.546429

Batch 105380, train_perplexity=5790.2617, train_loss=8.663933

Batch 105390, train_perplexity=5875.5923, train_loss=8.678562

Batch 105400, train_perplexity=5636.96, train_loss=8.6371

Batch 105410, train_perplexity=5478.603, train_loss=8.608605

Batch 105420, train_perplexity=5403.6377, train_loss=8.594828

Batch 105430, train_perplexity=5456.13, train_loss=8.604495

Batch 105440, train_perplexity=5022.8433, train_loss=8.521751

Batch 105450, train_perplexity=6044.1333, train_loss=8.706843

Batch 105460, train_perplexity=5483.0405, train_loss=8.609415

Batch 105470, train_perplexity=4668.3843, train_loss=8.448568

Batch 105480, train_perplexity=5934.7915, train_loss=8.688587

Batch 105490, train_perplexity=5867.0366, train_loss=8.677105

Batch 105500, train_perplexity=4763.939, train_loss=8.46883

Batch 105510, train_perplexity=5861.9473, train_loss=8.676237

Batch 105520, train_perplexity=5907.923, train_loss=8.68405

Batch 105530, train_perplexity=5914.801, train_loss=8.685213

Batch 105540, train_perplexity=6122.02, train_loss=8.719647

Batch 105550, train_perplexity=5138.623, train_loss=8.54454

Batch 105560, train_perplexity=7041.0615, train_loss=8.859514

Batch 105570, train_perplexity=6053.9116, train_loss=8.70846

Batch 105580, train_perplexity=5428.2285, train_loss=8.599368

Batch 105590, train_perplexity=6369.95, train_loss=8.759347

Batch 105600, train_perplexity=5455.8335, train_loss=8.604441

Batch 105610, train_perplexity=5469.5347, train_loss=8.606949

Batch 105620, train_perplexity=4624.922, train_loss=8.439215

Batch 105630, train_perplexity=5140.2847, train_loss=8.544864

Batch 105640, train_perplexity=5850.431, train_loss=8.674271

Batch 105650, train_perplexity=5876.377, train_loss=8.678696

Batch 105660, train_perplexity=5198.9873, train_loss=8.556219

Batch 105670, train_perplexity=4769.8896, train_loss=8.470078

Batch 105680, train_perplexity=5866.8574, train_loss=8.677074

Batch 105690, train_perplexity=4929.5317, train_loss=8.502999
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 105700, train_perplexity=5540.6987, train_loss=8.619876

Batch 105710, train_perplexity=4830.5684, train_loss=8.482719

Batch 105720, train_perplexity=5906.1655, train_loss=8.683752

Batch 105730, train_perplexity=5295.1714, train_loss=8.574551

Batch 105740, train_perplexity=5791.858, train_loss=8.664208

Batch 105750, train_perplexity=5676.8604, train_loss=8.644154

Batch 105760, train_perplexity=5763.9375, train_loss=8.659376

Batch 105770, train_perplexity=5267.741, train_loss=8.569357

Batch 105780, train_perplexity=5567.7817, train_loss=8.624752

Batch 105790, train_perplexity=5116.1006, train_loss=8.540148

Batch 105800, train_perplexity=4665.2065, train_loss=8.447887

Batch 105810, train_perplexity=5258.581, train_loss=8.567616

Batch 105820, train_perplexity=4950.345, train_loss=8.507213

Batch 105830, train_perplexity=5653.9844, train_loss=8.640116

Batch 105840, train_perplexity=5081.0234, train_loss=8.533268

Batch 105850, train_perplexity=4141.0034, train_loss=8.328693

Batch 105860, train_perplexity=6408.1724, train_loss=8.765329

Batch 105870, train_perplexity=5650.8794, train_loss=8.639566

Batch 105880, train_perplexity=5347.9155, train_loss=8.584462

Batch 105890, train_perplexity=5784.467, train_loss=8.662931

Batch 105900, train_perplexity=4392.6235, train_loss=8.387682

Batch 105910, train_perplexity=5974.3267, train_loss=8.695227

Batch 105920, train_perplexity=5457.7744, train_loss=8.604796

Batch 105930, train_perplexity=8079.348, train_loss=8.9970665

Batch 105940, train_perplexity=5793.178, train_loss=8.664436

Batch 105950, train_perplexity=4636.103, train_loss=8.441629

Batch 105960, train_perplexity=5627.5176, train_loss=8.635424

Batch 105970, train_perplexity=5219.7983, train_loss=8.560214

Batch 105980, train_perplexity=5019.946, train_loss=8.521174

Batch 105990, train_perplexity=4882.2173, train_loss=8.493355

Batch 106000, train_perplexity=5577.715, train_loss=8.626534

Batch 106010, train_perplexity=5395.2754, train_loss=8.593279

Batch 106020, train_perplexity=5244.4927, train_loss=8.564934

Batch 106030, train_perplexity=5872.6455, train_loss=8.678061

Batch 106040, train_perplexity=5183.6196, train_loss=8.553259

Batch 106050, train_perplexity=5257.523, train_loss=8.567415

Batch 106060, train_perplexity=4696.477, train_loss=8.454568

Batch 106070, train_perplexity=6653.272, train_loss=8.802864

Batch 106080, train_perplexity=6255.3945, train_loss=8.7411995

Batch 106090, train_perplexity=5274.492, train_loss=8.570638

Batch 106100, train_perplexity=5927.9585, train_loss=8.687435

Batch 106110, train_perplexity=6434.6274, train_loss=8.769449

Batch 106120, train_perplexity=4936.9795, train_loss=8.504509

Batch 106130, train_perplexity=4672.901, train_loss=8.449535

Batch 106140, train_perplexity=5534.9263, train_loss=8.618834

Batch 106150, train_perplexity=6454.1343, train_loss=8.772476

Batch 106160, train_perplexity=5692.68, train_loss=8.646936

Batch 106170, train_perplexity=4593.878, train_loss=8.43248

Batch 106180, train_perplexity=5662.013, train_loss=8.641535

Batch 106190, train_perplexity=4788.2344, train_loss=8.473917

Batch 106200, train_perplexity=4260.881, train_loss=8.357231

Batch 106210, train_perplexity=5154.722, train_loss=8.547668

Batch 106220, train_perplexity=5667.794, train_loss=8.642555

Batch 106230, train_perplexity=5695.5146, train_loss=8.647434

Batch 106240, train_perplexity=5384.2036, train_loss=8.591225

Batch 106250, train_perplexity=5779.796, train_loss=8.662124

Batch 106260, train_perplexity=4751.62, train_loss=8.466241

Batch 106270, train_perplexity=5115.398, train_loss=8.54001

Batch 106280, train_perplexity=7785.6772, train_loss=8.960041

Batch 106290, train_perplexity=6048.879, train_loss=8.707628

Batch 106300, train_perplexity=6039.0513, train_loss=8.706002

Batch 106310, train_perplexity=5236.746, train_loss=8.563456

Batch 106320, train_perplexity=5375.2866, train_loss=8.589567

Batch 106330, train_perplexity=5606.219, train_loss=8.631632

Batch 106340, train_perplexity=4869.8804, train_loss=8.490825

Batch 106350, train_perplexity=5054.0854, train_loss=8.527952

Batch 106360, train_perplexity=5950.2007, train_loss=8.69118

Batch 106370, train_perplexity=5617.019, train_loss=8.633556

Batch 106380, train_perplexity=4153.751, train_loss=8.331767

Batch 106390, train_perplexity=5223.3687, train_loss=8.560898

Batch 106400, train_perplexity=5145.145, train_loss=8.545809

Batch 106410, train_perplexity=5765.125, train_loss=8.659582

Batch 106420, train_perplexity=5936.931, train_loss=8.688948

Batch 106430, train_perplexity=5485.7705, train_loss=8.609913

Batch 106440, train_perplexity=5010.232, train_loss=8.5192375

Batch 106450, train_perplexity=5505.603, train_loss=8.613522

Batch 106460, train_perplexity=5875.3457, train_loss=8.67852

Batch 106470, train_perplexity=6051.2676, train_loss=8.708023

Batch 106480, train_perplexity=3858.5146, train_loss=8.258038

Batch 106490, train_perplexity=5114.92, train_loss=8.539917

Batch 106500, train_perplexity=5857.3423, train_loss=8.675451

Batch 106510, train_perplexity=5570.2305, train_loss=8.625192

Batch 106520, train_perplexity=6734.6143, train_loss=8.815016

Batch 106530, train_perplexity=6015.129, train_loss=8.702033

Batch 106540, train_perplexity=4927.4966, train_loss=8.502586

Batch 106550, train_perplexity=5948.028, train_loss=8.690815

Batch 106560, train_perplexity=5616.4297, train_loss=8.633451

Batch 106570, train_perplexity=5885.732, train_loss=8.680286

Batch 106580, train_perplexity=5890.6064, train_loss=8.681114

Batch 106590, train_perplexity=4756.979, train_loss=8.467368

Batch 106600, train_perplexity=6600.023, train_loss=8.794828

Batch 106610, train_perplexity=4587.101, train_loss=8.431004

Batch 106620, train_perplexity=4696.2305, train_loss=8.454515

Batch 106630, train_perplexity=5722.9507, train_loss=8.65224

Batch 106640, train_perplexity=5432.0244, train_loss=8.600067

Batch 106650, train_perplexity=5821.1123, train_loss=8.669247

Batch 106660, train_perplexity=4898.4663, train_loss=8.496677

Batch 106670, train_perplexity=5449.947, train_loss=8.603361

Batch 106680, train_perplexity=4962.356, train_loss=8.509636

Batch 106690, train_perplexity=4890.382, train_loss=8.495026

Batch 106700, train_perplexity=5615.562, train_loss=8.633297

Batch 106710, train_perplexity=5280.869, train_loss=8.571846

Batch 106720, train_perplexity=4701.3706, train_loss=8.455609

Batch 106730, train_perplexity=5130.8955, train_loss=8.5430355

Batch 106740, train_perplexity=6061.884, train_loss=8.709776

Batch 106750, train_perplexity=5024.343, train_loss=8.52205

Batch 106760, train_perplexity=5197.3315, train_loss=8.555901

Batch 106770, train_perplexity=5150.899, train_loss=8.5469265

Batch 106780, train_perplexity=5870.35, train_loss=8.67767

Batch 106790, train_perplexity=4906.129, train_loss=8.49824

Batch 106800, train_perplexity=6072.9126, train_loss=8.711594

Batch 106810, train_perplexity=6051.6484, train_loss=8.708086

Batch 106820, train_perplexity=5345.402, train_loss=8.583992

Batch 106830, train_perplexity=5955.066, train_loss=8.691998

Batch 106840, train_perplexity=5411.363, train_loss=8.596256

Batch 106850, train_perplexity=5236.631, train_loss=8.563434

Batch 106860, train_perplexity=4801.248, train_loss=8.476631

Batch 106870, train_perplexity=5341.4116, train_loss=8.583245

Batch 106880, train_perplexity=5336.173, train_loss=8.582264

Batch 106890, train_perplexity=7030.433, train_loss=8.858004

Batch 106900, train_perplexity=6199.675, train_loss=8.732252

Batch 106910, train_perplexity=5520.366, train_loss=8.6161995

Batch 106920, train_perplexity=5038.714, train_loss=8.524906

Batch 106930, train_perplexity=5447.3174, train_loss=8.602879

Batch 106940, train_perplexity=5539.7373, train_loss=8.619702

Batch 106950, train_perplexity=4997.2, train_loss=8.516633

Batch 106960, train_perplexity=4087.2087, train_loss=8.315618

Batch 106970, train_perplexity=5626.8413, train_loss=8.6353035

Batch 106980, train_perplexity=5951.557, train_loss=8.691408

Batch 106990, train_perplexity=5674.7764, train_loss=8.643786

Batch 107000, train_perplexity=5969.2236, train_loss=8.694372

Batch 107010, train_perplexity=5224.171, train_loss=8.561051
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 107020, train_perplexity=5809.167, train_loss=8.667192

Batch 107030, train_perplexity=5544.309, train_loss=8.620527

Batch 107040, train_perplexity=5884.5703, train_loss=8.680089

Batch 107050, train_perplexity=4847.431, train_loss=8.486204

Batch 107060, train_perplexity=5031.751, train_loss=8.523523

Batch 107070, train_perplexity=5630.835, train_loss=8.636013

Batch 107080, train_perplexity=6019.301, train_loss=8.702726

Batch 107090, train_perplexity=4995.537, train_loss=8.5163

Batch 107100, train_perplexity=5184.203, train_loss=8.553371

Batch 107110, train_perplexity=5534.5674, train_loss=8.618769

Batch 107120, train_perplexity=4133.2505, train_loss=8.326819

Batch 107130, train_perplexity=5041.978, train_loss=8.525554

Batch 107140, train_perplexity=4443.7754, train_loss=8.39926

Batch 107150, train_perplexity=5657.08, train_loss=8.640663

Batch 107160, train_perplexity=4830.7617, train_loss=8.482759

Batch 107170, train_perplexity=5350.7876, train_loss=8.584999

Batch 107180, train_perplexity=4769.8486, train_loss=8.47007

Batch 107190, train_perplexity=5317.255, train_loss=8.578712

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Batch 107200, train_perplexity=5452.598, train_loss=8.6038475

Batch 107210, train_perplexity=6175.6045, train_loss=8.728362

Batch 107220, train_perplexity=4686.79, train_loss=8.452503

Batch 107230, train_perplexity=5631.6943, train_loss=8.636166

Batch 107240, train_perplexity=4682.662, train_loss=8.451622

Batch 107250, train_perplexity=5055.9946, train_loss=8.52833

Batch 107260, train_perplexity=5492.173, train_loss=8.611079

Batch 107270, train_perplexity=4923.2974, train_loss=8.501734

Batch 107280, train_perplexity=6260.324, train_loss=8.741987

Batch 107290, train_perplexity=5129.0073, train_loss=8.542667

Batch 107300, train_perplexity=5436.2944, train_loss=8.600853

Batch 107310, train_perplexity=5959.8267, train_loss=8.692797

Batch 107320, train_perplexity=5778.2803, train_loss=8.661861

Batch 107330, train_perplexity=4976.5737, train_loss=8.512497

Batch 107340, train_perplexity=5561.122, train_loss=8.623555

Batch 107350, train_perplexity=6745.754, train_loss=8.8166685

Batch 107360, train_perplexity=5215.2305, train_loss=8.559339

Batch 107370, train_perplexity=6149.8447, train_loss=8.724182

Batch 107380, train_perplexity=7028.516, train_loss=8.857731

Batch 107390, train_perplexity=5386.453, train_loss=8.591642

Batch 107400, train_perplexity=5092.5107, train_loss=8.535526

Batch 107410, train_perplexity=6044.6924, train_loss=8.706936

Batch 107420, train_perplexity=4753.2334, train_loss=8.46658

Batch 107430, train_perplexity=5270.8716, train_loss=8.569951

Batch 107440, train_perplexity=5477.5215, train_loss=8.608408

Batch 107450, train_perplexity=5550.896, train_loss=8.621715

Batch 107460, train_perplexity=7443.002, train_loss=8.91503

Batch 107470, train_perplexity=5548.5566, train_loss=8.621293

Batch 107480, train_perplexity=4822.536, train_loss=8.481055

Batch 107490, train_perplexity=5105.397, train_loss=8.5380535

Batch 107500, train_perplexity=6154.404, train_loss=8.724923

Batch 107510, train_perplexity=6033.669, train_loss=8.705111

Batch 107520, train_perplexity=4917.066, train_loss=8.500467

Batch 107530, train_perplexity=6196.944, train_loss=8.731812

Batch 107540, train_perplexity=5137.5254, train_loss=8.544327

Batch 107550, train_perplexity=5235.957, train_loss=8.563305

Batch 107560, train_perplexity=5630.7705, train_loss=8.636002

Batch 107570, train_perplexity=4874.5225, train_loss=8.491777

Batch 107580, train_perplexity=4379.235, train_loss=8.384629

Batch 107590, train_perplexity=6156.1294, train_loss=8.7252035

Batch 107600, train_perplexity=5200.589, train_loss=8.556527

Batch 107610, train_perplexity=6162.761, train_loss=8.72628

Batch 107620, train_perplexity=5011.914, train_loss=8.519573

Batch 107630, train_perplexity=6152.8896, train_loss=8.724677

Batch 107640, train_perplexity=4947.306, train_loss=8.506598

Batch 107650, train_perplexity=5228.7314, train_loss=8.561924

Batch 107660, train_perplexity=4142.71, train_loss=8.329105

Batch 107670, train_perplexity=5324.796, train_loss=8.58013

Batch 107680, train_perplexity=5893.708, train_loss=8.681641

Batch 107690, train_perplexity=5919.326, train_loss=8.685978

Batch 107700, train_perplexity=5324.7246, train_loss=8.580116

Batch 107710, train_perplexity=5074.7134, train_loss=8.532025

Batch 107720, train_perplexity=5825.7886, train_loss=8.67005

Batch 107730, train_perplexity=5398.003, train_loss=8.593784

Batch 107740, train_perplexity=5140.8286, train_loss=8.54497

Batch 107750, train_perplexity=5259.549, train_loss=8.5678005

Batch 107760, train_perplexity=5098.77, train_loss=8.536755

Batch 107770, train_perplexity=5333.2017, train_loss=8.581707

Batch 107780, train_perplexity=4378.2656, train_loss=8.384408

Batch 107790, train_perplexity=3732.1582, train_loss=8.224742

Batch 107800, train_perplexity=5499.1274, train_loss=8.612345

Batch 107810, train_perplexity=5062.817, train_loss=8.529678

Batch 107820, train_perplexity=5190.06, train_loss=8.554501

Batch 107830, train_perplexity=5321.577, train_loss=8.579525

Batch 107840, train_perplexity=5440.1533, train_loss=8.6015625

Batch 107850, train_perplexity=6340.3315, train_loss=8.754686

Batch 107860, train_perplexity=4864.872, train_loss=8.489796

Batch 107870, train_perplexity=4853.6665, train_loss=8.48749

Batch 107880, train_perplexity=5637.3096, train_loss=8.637162

Batch 107890, train_perplexity=5297.571, train_loss=8.575004

Batch 107900, train_perplexity=6145.319, train_loss=8.723446

Batch 107910, train_perplexity=4352.865, train_loss=8.37859

Batch 107920, train_perplexity=7136.6665, train_loss=8.873001

Batch 107930, train_perplexity=4572.5005, train_loss=8.427815

Batch 107940, train_perplexity=5488.68, train_loss=8.610443

Batch 107950, train_perplexity=5138.5444, train_loss=8.544525

Batch 107960, train_perplexity=5220.4604, train_loss=8.560341

Batch 107970, train_perplexity=5303.7275, train_loss=8.576165

Batch 107980, train_perplexity=5783.86, train_loss=8.662827

Batch 107990, train_perplexity=4855.9443, train_loss=8.487959

Batch 108000, train_perplexity=5321.8105, train_loss=8.579569

Batch 108010, train_perplexity=4422.1045, train_loss=8.394371

Batch 108020, train_perplexity=4343.0464, train_loss=8.376331

Batch 108030, train_perplexity=6277.159, train_loss=8.744673

Batch 108040, train_perplexity=4007.0657, train_loss=8.2958145

Batch 108050, train_perplexity=4852.5835, train_loss=8.487267

Batch 108060, train_perplexity=6148.109, train_loss=8.7239

Batch 108070, train_perplexity=5207.1943, train_loss=8.5577965

Batch 108080, train_perplexity=5477.882, train_loss=8.608474

Batch 108090, train_perplexity=6513.433, train_loss=8.781622

Batch 108100, train_perplexity=5165.824, train_loss=8.54982

Batch 108110, train_perplexity=6642.2915, train_loss=8.801212

Batch 108120, train_perplexity=5403.854, train_loss=8.594868

Batch 108130, train_perplexity=5009.6587, train_loss=8.519123

Batch 108140, train_perplexity=5646.6074, train_loss=8.63881

Batch 108150, train_perplexity=4312.6035, train_loss=8.369297

Batch 108160, train_perplexity=5244.0425, train_loss=8.564848

Batch 108170, train_perplexity=5577.1777, train_loss=8.626438

Batch 108180, train_perplexity=5986.4805, train_loss=8.697259

Batch 108190, train_perplexity=5567.697, train_loss=8.624737

Batch 108200, train_perplexity=4218.7163, train_loss=8.347286

Batch 108210, train_perplexity=5087.0693, train_loss=8.534457

Batch 108220, train_perplexity=6321.705, train_loss=8.751744

Batch 108230, train_perplexity=4909.171, train_loss=8.49886

Batch 108240, train_perplexity=4762.517, train_loss=8.468532

Batch 108250, train_perplexity=6059.803, train_loss=8.709433

Batch 108260, train_perplexity=4995.8135, train_loss=8.5163555
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 108270, train_perplexity=6076.134, train_loss=8.712124

Batch 108280, train_perplexity=5310.8696, train_loss=8.577511

Batch 108290, train_perplexity=4473.28, train_loss=8.405877

Batch 108300, train_perplexity=6543.3423, train_loss=8.786203

Batch 108310, train_perplexity=4983.042, train_loss=8.513796

Batch 108320, train_perplexity=5494.677, train_loss=8.611535

Batch 108330, train_perplexity=5646.112, train_loss=8.638722

Batch 108340, train_perplexity=5902.984, train_loss=8.683213

Batch 108350, train_perplexity=5847.14, train_loss=8.673708

Batch 108360, train_perplexity=5445.6865, train_loss=8.602579

Batch 108370, train_perplexity=4661.782, train_loss=8.447153

Batch 108380, train_perplexity=5866.796, train_loss=8.677064

Batch 108390, train_perplexity=4731.872, train_loss=8.462076

Batch 108400, train_perplexity=5281.3325, train_loss=8.571934

Batch 108410, train_perplexity=6337.333, train_loss=8.754213

Batch 108420, train_perplexity=5148.3843, train_loss=8.546438

Batch 108430, train_perplexity=6873.274, train_loss=8.835396

Batch 108440, train_perplexity=5405.792, train_loss=8.595226

Batch 108450, train_perplexity=5500.995, train_loss=8.612684

Batch 108460, train_perplexity=5714.786, train_loss=8.650812

Batch 108470, train_perplexity=5461.362, train_loss=8.6054535

Batch 108480, train_perplexity=5255.7783, train_loss=8.567083

Batch 108490, train_perplexity=5143.065, train_loss=8.545404

Batch 108500, train_perplexity=5794.941, train_loss=8.664741

Batch 108510, train_perplexity=4836.8286, train_loss=8.4840145

Batch 108520, train_perplexity=6231.9697, train_loss=8.737448

Batch 108530, train_perplexity=6745.747, train_loss=8.816668

Batch 108540, train_perplexity=4830.992, train_loss=8.482807

Batch 108550, train_perplexity=5897.2334, train_loss=8.682239

Batch 108560, train_perplexity=5398.43, train_loss=8.5938635

Batch 108570, train_perplexity=5119.976, train_loss=8.540905

Batch 108580, train_perplexity=6130.8833, train_loss=8.721094

Batch 108590, train_perplexity=6065.6084, train_loss=8.71039

Batch 108600, train_perplexity=6154.985, train_loss=8.725018

Batch 108610, train_perplexity=4980.125, train_loss=8.51321

Batch 108620, train_perplexity=4775.538, train_loss=8.471262

Batch 108630, train_perplexity=4885.5615, train_loss=8.49404

Batch 108640, train_perplexity=4987.9014, train_loss=8.5147705

Batch 108650, train_perplexity=5225.0625, train_loss=8.561222

Batch 108660, train_perplexity=5357.032, train_loss=8.586165

Batch 108670, train_perplexity=6177.8374, train_loss=8.728724

Batch 108680, train_perplexity=5723.8076, train_loss=8.65239

Batch 108690, train_perplexity=5507.609, train_loss=8.613886

Batch 108700, train_perplexity=5738.4116, train_loss=8.654938

Batch 108710, train_perplexity=5427.545, train_loss=8.599242

Batch 108720, train_perplexity=5465.885, train_loss=8.606281

Batch 108730, train_perplexity=5071.2446, train_loss=8.531342

Batch 108740, train_perplexity=5028.1055, train_loss=8.522799

Batch 108750, train_perplexity=5369.989, train_loss=8.588581

Batch 108760, train_perplexity=4904.0005, train_loss=8.497807

Batch 108770, train_perplexity=6957.2393, train_loss=8.847538

Batch 108780, train_perplexity=4694.6274, train_loss=8.454174

Batch 108790, train_perplexity=5100.764, train_loss=8.537146

Batch 108800, train_perplexity=5620.8877, train_loss=8.634245

Batch 108810, train_perplexity=5056.496, train_loss=8.528429

Batch 108820, train_perplexity=5245.088, train_loss=8.565047

Batch 108830, train_perplexity=4690.935, train_loss=8.453387

Batch 108840, train_perplexity=5596.9136, train_loss=8.629971

Batch 108850, train_perplexity=4581.8765, train_loss=8.429864

Batch 108860, train_perplexity=6368.9844, train_loss=8.759195

Batch 108870, train_perplexity=5254.841, train_loss=8.566905

Batch 108880, train_perplexity=4495.1367, train_loss=8.410751

Batch 108890, train_perplexity=4819.502, train_loss=8.480426

Batch 108900, train_perplexity=5508.25, train_loss=8.614002

Batch 108910, train_perplexity=6049.7676, train_loss=8.707775

Batch 108920, train_perplexity=5407.5557, train_loss=8.595552

Batch 108930, train_perplexity=5622.6357, train_loss=8.634556

Batch 108940, train_perplexity=5642.1396, train_loss=8.638019

Batch 108950, train_perplexity=6464.835, train_loss=8.774133

Batch 108960, train_perplexity=5640.843, train_loss=8.637789

Batch 108970, train_perplexity=4826.433, train_loss=8.481863

Batch 108980, train_perplexity=6291.189, train_loss=8.746905

Batch 108990, train_perplexity=5432.708, train_loss=8.600193

Batch 109000, train_perplexity=5335.796, train_loss=8.582193

Batch 109010, train_perplexity=5906.695, train_loss=8.683842

Batch 109020, train_perplexity=5237.415, train_loss=8.563583

Batch 109030, train_perplexity=5004.053, train_loss=8.518003

Batch 109040, train_perplexity=5573.3813, train_loss=8.625757

Batch 109050, train_perplexity=5721.0464, train_loss=8.651907

Batch 109060, train_perplexity=5357.109, train_loss=8.58618

Batch 109070, train_perplexity=5214.092, train_loss=8.55912

Batch 109080, train_perplexity=5091.088, train_loss=8.535247

Batch 109090, train_perplexity=7124.7935, train_loss=8.871336

Batch 109100, train_perplexity=5899.444, train_loss=8.682613

Batch 109110, train_perplexity=4615.47, train_loss=8.437169

Batch 109120, train_perplexity=5816.1685, train_loss=8.668397

Batch 109130, train_perplexity=4879.155, train_loss=8.492727

Batch 109140, train_perplexity=5779.8896, train_loss=8.66214

Batch 109150, train_perplexity=5586.829, train_loss=8.628167

Batch 109160, train_perplexity=4654.834, train_loss=8.445662

Batch 109170, train_perplexity=5213.097, train_loss=8.558929

Batch 109180, train_perplexity=5315.8403, train_loss=8.578446

Batch 109190, train_perplexity=4610.9214, train_loss=8.436183

Batch 109200, train_perplexity=4568.1895, train_loss=8.426872

Batch 109210, train_perplexity=4830.596, train_loss=8.482725

Batch 109220, train_perplexity=5005.4277, train_loss=8.518278

Batch 109230, train_perplexity=5013.683, train_loss=8.519926

Batch 109240, train_perplexity=5595.5474, train_loss=8.629726

Batch 109250, train_perplexity=5516.398, train_loss=8.61548

Batch 109260, train_perplexity=4965.8735, train_loss=8.5103445

Batch 109270, train_perplexity=5584.416, train_loss=8.627735

Batch 109280, train_perplexity=5285.3184, train_loss=8.572688

Batch 109290, train_perplexity=5687.953, train_loss=8.646106

Batch 109300, train_perplexity=6495.1094, train_loss=8.778805

Batch 109310, train_perplexity=6131.7954, train_loss=8.721243

Batch 109320, train_perplexity=4216.3433, train_loss=8.346724

Batch 109330, train_perplexity=5483.093, train_loss=8.609425

Batch 109340, train_perplexity=5866.1973, train_loss=8.676962

Batch 109350, train_perplexity=5146.2686, train_loss=8.546027

Batch 109360, train_perplexity=7549.7646, train_loss=8.929272

Batch 109370, train_perplexity=5599.487, train_loss=8.63043

Batch 109380, train_perplexity=5351.778, train_loss=8.585184

Batch 109390, train_perplexity=5370.4854, train_loss=8.588674

Batch 109400, train_perplexity=5863.166, train_loss=8.676445

Batch 109410, train_perplexity=5780.772, train_loss=8.6622925

Batch 109420, train_perplexity=6127.3647, train_loss=8.72052

Batch 109430, train_perplexity=5303.207, train_loss=8.576067

Batch 109440, train_perplexity=5261.3247, train_loss=8.568138

Batch 109450, train_perplexity=5324.608, train_loss=8.580094

Batch 109460, train_perplexity=6686.3486, train_loss=8.807823

Batch 109470, train_perplexity=4442.182, train_loss=8.398901

Batch 109480, train_perplexity=5517.2715, train_loss=8.615639

Batch 109490, train_perplexity=5478.9526, train_loss=8.608669

Batch 109500, train_perplexity=4582.7944, train_loss=8.430064

Batch 109510, train_perplexity=6723.153, train_loss=8.813313

Batch 109520, train_perplexity=6186.091, train_loss=8.730059

Batch 109530, train_perplexity=4439.429, train_loss=8.398281

Batch 109540, train_perplexity=5076.737, train_loss=8.532424

Batch 109550, train_perplexity=5483.9087, train_loss=8.609573

Batch 109560, train_perplexity=4052.5525, train_loss=8.307102

Batch 109570, train_perplexity=4702.8325, train_loss=8.45592

Batch 109580, train_perplexity=5974.338, train_loss=8.695229
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 109590, train_perplexity=5164.8833, train_loss=8.549638

Batch 109600, train_perplexity=5607.1284, train_loss=8.631794

Batch 109610, train_perplexity=5184.371, train_loss=8.553404

Batch 109620, train_perplexity=6591.204, train_loss=8.793491

Batch 109630, train_perplexity=5631.9253, train_loss=8.636207

Batch 109640, train_perplexity=5177.434, train_loss=8.552065

Batch 109650, train_perplexity=5738.899, train_loss=8.655023

Batch 109660, train_perplexity=4873.6997, train_loss=8.491609

Batch 109670, train_perplexity=4653.982, train_loss=8.445478

Batch 109680, train_perplexity=4546.762, train_loss=8.422171

Batch 109690, train_perplexity=5365.2896, train_loss=8.587706

Batch 109700, train_perplexity=5085.343, train_loss=8.534118

Batch 109710, train_perplexity=6041.8105, train_loss=8.706459

Batch 109720, train_perplexity=5012.23, train_loss=8.519636

Batch 109730, train_perplexity=6981.093, train_loss=8.850961

Batch 109740, train_perplexity=5063.203, train_loss=8.529755

Batch 109750, train_perplexity=5290.386, train_loss=8.573647

Batch 109760, train_perplexity=5899.354, train_loss=8.682598

Batch 109770, train_perplexity=5376.24, train_loss=8.589745

Batch 109780, train_perplexity=4467.805, train_loss=8.404653

Batch 109790, train_perplexity=5328.3975, train_loss=8.580806

Batch 109800, train_perplexity=5444.388, train_loss=8.602341

Batch 109810, train_perplexity=4549.807, train_loss=8.42284

Batch 109820, train_perplexity=6149.7627, train_loss=8.724169

Batch 109830, train_perplexity=5962.2656, train_loss=8.693206

Batch 109840, train_perplexity=5847.352, train_loss=8.673744

Batch 109850, train_perplexity=4917.7227, train_loss=8.500601

Batch 109860, train_perplexity=5946.326, train_loss=8.690529

Batch 109870, train_perplexity=5797.0137, train_loss=8.665098

Batch 109880, train_perplexity=6209.983, train_loss=8.733913

Batch 109890, train_perplexity=5157.466, train_loss=8.548201

Batch 109900, train_perplexity=4774.737, train_loss=8.471094

Batch 109910, train_perplexity=5639.9175, train_loss=8.637625

Batch 109920, train_perplexity=5190.179, train_loss=8.554523

Batch 109930, train_perplexity=5750.947, train_loss=8.65712

Batch 109940, train_perplexity=4944.641, train_loss=8.50606

Batch 109950, train_perplexity=5890.786, train_loss=8.681145

Batch 109960, train_perplexity=4860.133, train_loss=8.488821

Batch 109970, train_perplexity=6735.7705, train_loss=8.815187

Batch 109980, train_perplexity=4716.311, train_loss=8.458782

Batch 109990, train_perplexity=5552.3887, train_loss=8.621984

Batch 110000, train_perplexity=5889.427, train_loss=8.680914

Batch 110010, train_perplexity=4803.437, train_loss=8.477087

Batch 110020, train_perplexity=5281.0005, train_loss=8.571871

Batch 110030, train_perplexity=4574.4062, train_loss=8.428232

Batch 110040, train_perplexity=5562.2095, train_loss=8.623751

Batch 110050, train_perplexity=4630.7207, train_loss=8.440468

Batch 110060, train_perplexity=4891.4404, train_loss=8.495242

Batch 110070, train_perplexity=6168.206, train_loss=8.727163

Batch 110080, train_perplexity=5507.5356, train_loss=8.613873

Batch 110090, train_perplexity=4400.985, train_loss=8.389584

Batch 110100, train_perplexity=4619.61, train_loss=8.438066

Batch 110110, train_perplexity=5344.403, train_loss=8.583805

Batch 110120, train_perplexity=5650.8574, train_loss=8.639563

Batch 110130, train_perplexity=6368.687, train_loss=8.759149

Batch 110140, train_perplexity=6178.3735, train_loss=8.72881

Batch 110150, train_perplexity=4744.57, train_loss=8.464756

Batch 110160, train_perplexity=5414.119, train_loss=8.5967655

Batch 110170, train_perplexity=5290.427, train_loss=8.573654

Batch 110180, train_perplexity=5517.808, train_loss=8.615736

Batch 110190, train_perplexity=4696.723, train_loss=8.45462

Batch 110200, train_perplexity=5616.9062, train_loss=8.633536

Batch 110210, train_perplexity=5568.4297, train_loss=8.624868

Batch 110220, train_perplexity=5061.0117, train_loss=8.529322

Batch 110230, train_perplexity=4544.4297, train_loss=8.421658

Batch 110240, train_perplexity=5028.2876, train_loss=8.522835

Batch 110250, train_perplexity=4780.5645, train_loss=8.472314

Batch 110260, train_perplexity=5035.2075, train_loss=8.52421

Batch 110270, train_perplexity=6526.9575, train_loss=8.783696

Batch 110280, train_perplexity=5578.066, train_loss=8.626597

Batch 110290, train_perplexity=5521.672, train_loss=8.616436

Batch 110300, train_perplexity=5515.236, train_loss=8.61527

Batch 110310, train_perplexity=5074.805, train_loss=8.532043

Batch 110320, train_perplexity=5810.5854, train_loss=8.667437

Batch 110330, train_perplexity=6313.391, train_loss=8.750428

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Batch 110340, train_perplexity=5289.781, train_loss=8.573532

Batch 110350, train_perplexity=5196.925, train_loss=8.555822

Batch 110360, train_perplexity=6358.6553, train_loss=8.757572

Batch 110370, train_perplexity=4724.2427, train_loss=8.460463

Batch 110380, train_perplexity=5259.644, train_loss=8.567819

Batch 110390, train_perplexity=4852.63, train_loss=8.487276

Batch 110400, train_perplexity=5888.5728, train_loss=8.680769

Batch 110410, train_perplexity=4735.375, train_loss=8.462816

Batch 110420, train_perplexity=6123.007, train_loss=8.719809

Batch 110430, train_perplexity=6107.564, train_loss=8.717283

Batch 110440, train_perplexity=5688.5825, train_loss=8.646216

Batch 110450, train_perplexity=4995.28, train_loss=8.516249

Batch 110460, train_perplexity=5329.7188, train_loss=8.581054

Batch 110470, train_perplexity=5099.086, train_loss=8.536817

Batch 110480, train_perplexity=6101.195, train_loss=8.71624

Batch 110490, train_perplexity=5328.1787, train_loss=8.580765

Batch 110500, train_perplexity=5235.5376, train_loss=8.563225

Batch 110510, train_perplexity=5851.7983, train_loss=8.674504

Batch 110520, train_perplexity=5977.045, train_loss=8.695682

Batch 110530, train_perplexity=5601.7676, train_loss=8.630837

Batch 110540, train_perplexity=6128.095, train_loss=8.720639

Batch 110550, train_perplexity=5540.4663, train_loss=8.619834

Batch 110560, train_perplexity=6592.933, train_loss=8.793754

Batch 110570, train_perplexity=4868.446, train_loss=8.49053

Batch 110580, train_perplexity=4789.0747, train_loss=8.4740925

Batch 110590, train_perplexity=6478.079, train_loss=8.776179

Batch 110600, train_perplexity=5159.424, train_loss=8.54858

Batch 110610, train_perplexity=4485.6855, train_loss=8.408647

Batch 110620, train_perplexity=4556.52, train_loss=8.4243145

Batch 110630, train_perplexity=5271.857, train_loss=8.570138

Batch 110640, train_perplexity=4610.064, train_loss=8.435997

Batch 110650, train_perplexity=5833.9614, train_loss=8.671452

Batch 110660, train_perplexity=4788.5083, train_loss=8.473974

Batch 110670, train_perplexity=5091.282, train_loss=8.535285

Batch 110680, train_perplexity=5192.912, train_loss=8.55505

Batch 110690, train_perplexity=4844.709, train_loss=8.485642

Batch 110700, train_perplexity=6181.5737, train_loss=8.729328

Batch 110710, train_perplexity=4341.249, train_loss=8.375917

Batch 110720, train_perplexity=4864.0186, train_loss=8.48962

Batch 110730, train_perplexity=6157.2217, train_loss=8.725381

Batch 110740, train_perplexity=6387.7563, train_loss=8.762138

Batch 110750, train_perplexity=10307.816, train_loss=9.240658

Batch 110760, train_perplexity=5493.283, train_loss=8.611281

Batch 110770, train_perplexity=5164.041, train_loss=8.549475

Batch 110780, train_perplexity=5195.6416, train_loss=8.555575

Batch 110790, train_perplexity=4367.5356, train_loss=8.381954

Batch 110800, train_perplexity=6395.876, train_loss=8.763409

Batch 110810, train_perplexity=5312.08, train_loss=8.577739

Batch 110820, train_perplexity=4223.4146, train_loss=8.348399

Batch 110830, train_perplexity=5753.47, train_loss=8.657558

Batch 110840, train_perplexity=6520.966, train_loss=8.782778
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 110850, train_perplexity=5211.576, train_loss=8.558638

Batch 110860, train_perplexity=5357.2417, train_loss=8.586205

Batch 110870, train_perplexity=5476.4873, train_loss=8.608219

Batch 110880, train_perplexity=5784.5107, train_loss=8.662939

Batch 110890, train_perplexity=5688.572, train_loss=8.6462145

Batch 110900, train_perplexity=6513.0107, train_loss=8.781557

Batch 110910, train_perplexity=5271.867, train_loss=8.57014

Batch 110920, train_perplexity=4746.8687, train_loss=8.4652405

Batch 110930, train_perplexity=5413.531, train_loss=8.596657

Batch 110940, train_perplexity=3833.054, train_loss=8.251417

Batch 110950, train_perplexity=5008.2593, train_loss=8.518844

Batch 110960, train_perplexity=6060.5083, train_loss=8.709549

Batch 110970, train_perplexity=5358.5806, train_loss=8.586454

Batch 110980, train_perplexity=4470.38, train_loss=8.405229

Batch 110990, train_perplexity=5081.232, train_loss=8.533309

Batch 111000, train_perplexity=5953.5044, train_loss=8.691735

Batch 111010, train_perplexity=5177.429, train_loss=8.552064

Batch 111020, train_perplexity=6006.685, train_loss=8.700628

Batch 111030, train_perplexity=6094.4375, train_loss=8.715132

Batch 111040, train_perplexity=4819.3135, train_loss=8.480387

Batch 111050, train_perplexity=4088.3198, train_loss=8.315889

Batch 111060, train_perplexity=5214.6787, train_loss=8.559233

Batch 111070, train_perplexity=5787.8823, train_loss=8.663522

Batch 111080, train_perplexity=5282.6323, train_loss=8.57218

Batch 111090, train_perplexity=5289.4175, train_loss=8.573463

Batch 111100, train_perplexity=5823.878, train_loss=8.669722

Batch 111110, train_perplexity=4921.6494, train_loss=8.501399

Batch 111120, train_perplexity=6197.854, train_loss=8.731958

Batch 111130, train_perplexity=5850.47, train_loss=8.674277

Batch 111140, train_perplexity=4883.093, train_loss=8.493534

Batch 111150, train_perplexity=4732.206, train_loss=8.462147

Batch 111160, train_perplexity=4484.5605, train_loss=8.408396

Batch 111170, train_perplexity=6807.6084, train_loss=8.825796

Batch 111180, train_perplexity=7042.4854, train_loss=8.859716

Batch 111190, train_perplexity=5321.9834, train_loss=8.579601

Batch 111200, train_perplexity=6818.251, train_loss=8.827358

Batch 111210, train_perplexity=5797.401, train_loss=8.665165

Batch 111220, train_perplexity=4937.601, train_loss=8.504635

Batch 111230, train_perplexity=5101.4014, train_loss=8.537271

Batch 111240, train_perplexity=5206.5093, train_loss=8.557665

Batch 111250, train_perplexity=4862.1445, train_loss=8.489235

Batch 111260, train_perplexity=6319.3486, train_loss=8.751371

Batch 111270, train_perplexity=5561.9443, train_loss=8.623703

Batch 111280, train_perplexity=5485.9067, train_loss=8.609938

Batch 111290, train_perplexity=5631.2, train_loss=8.636078

Batch 111300, train_perplexity=4775.256, train_loss=8.471203

Batch 111310, train_perplexity=5240.5728, train_loss=8.564186

Batch 111320, train_perplexity=5354.877, train_loss=8.585763

Batch 111330, train_perplexity=5034.5063, train_loss=8.524071

Batch 111340, train_perplexity=6042.174, train_loss=8.706519

Batch 111350, train_perplexity=4990.4897, train_loss=8.515289

Batch 111360, train_perplexity=4999.7217, train_loss=8.517138

Batch 111370, train_perplexity=4552.4546, train_loss=8.423422

Batch 111380, train_perplexity=5023.897, train_loss=8.521961

Batch 111390, train_perplexity=4685.8784, train_loss=8.452309

Batch 111400, train_perplexity=5157.8345, train_loss=8.548272

Batch 111410, train_perplexity=5532.3403, train_loss=8.618366

Batch 111420, train_perplexity=4698.399, train_loss=8.454977

Batch 111430, train_perplexity=5058.792, train_loss=8.528883

Batch 111440, train_perplexity=4647.1475, train_loss=8.444009

Batch 111450, train_perplexity=6073.99, train_loss=8.711771

Batch 111460, train_perplexity=4474.2056, train_loss=8.406084

Batch 111470, train_perplexity=6396.6016, train_loss=8.763522

Batch 111480, train_perplexity=5194.403, train_loss=8.555337

Batch 111490, train_perplexity=5188.625, train_loss=8.554224

Batch 111500, train_perplexity=4922.0347, train_loss=8.501477

Batch 111510, train_perplexity=4165.9697, train_loss=8.334704

Batch 111520, train_perplexity=5637.2935, train_loss=8.637159

Batch 111530, train_perplexity=5825.022, train_loss=8.669918

Batch 111540, train_perplexity=5233.96, train_loss=8.562923

Batch 111550, train_perplexity=5409.8975, train_loss=8.595985

Batch 111560, train_perplexity=5362.568, train_loss=8.587198

Batch 111570, train_perplexity=5447.6187, train_loss=8.602934

Batch 111580, train_perplexity=5739.8076, train_loss=8.655181

Batch 111590, train_perplexity=5655.791, train_loss=8.640435

Batch 111600, train_perplexity=5275.8203, train_loss=8.570889

Batch 111610, train_perplexity=5042.6943, train_loss=8.525696

Batch 111620, train_perplexity=6407.2617, train_loss=8.765187

Batch 111630, train_perplexity=5220.4204, train_loss=8.560333

Batch 111640, train_perplexity=5215.494, train_loss=8.559389

Batch 111650, train_perplexity=5713.102, train_loss=8.650517

Batch 111660, train_perplexity=6005.528, train_loss=8.700436

Batch 111670, train_perplexity=4932.631, train_loss=8.503628

Batch 111680, train_perplexity=4956.992, train_loss=8.508554

Batch 111690, train_perplexity=5536.605, train_loss=8.619137

Batch 111700, train_perplexity=4660.653, train_loss=8.446911

Batch 111710, train_perplexity=4874.2993, train_loss=8.491732

Batch 111720, train_perplexity=5297.4946, train_loss=8.574989

Batch 111730, train_perplexity=6161.139, train_loss=8.726017

Batch 111740, train_perplexity=5697.862, train_loss=8.647846

Batch 111750, train_perplexity=5774.854, train_loss=8.661268

Batch 111760, train_perplexity=5634.5415, train_loss=8.636671

Batch 111770, train_perplexity=6647.0635, train_loss=8.80193

Batch 111780, train_perplexity=5173.5747, train_loss=8.551319

Batch 111790, train_perplexity=5721.112, train_loss=8.651918

Batch 111800, train_perplexity=4448.367, train_loss=8.400292

Batch 111810, train_perplexity=6731.352, train_loss=8.814531

Batch 111820, train_perplexity=5643.1025, train_loss=8.638189

Batch 111830, train_perplexity=4969.839, train_loss=8.511143

Batch 111840, train_perplexity=4736.9287, train_loss=8.463144

Batch 111850, train_perplexity=6351.0796, train_loss=8.75638

Batch 111860, train_perplexity=5604.4497, train_loss=8.631316

Batch 111870, train_perplexity=5542.258, train_loss=8.620157

Batch 111880, train_perplexity=5083.738, train_loss=8.533802

Batch 111890, train_perplexity=5999.0195, train_loss=8.699351

Batch 111900, train_perplexity=4916.6113, train_loss=8.500375

Batch 111910, train_perplexity=5972.988, train_loss=8.695003

Batch 111920, train_perplexity=6030.488, train_loss=8.704583

Batch 111930, train_perplexity=4694.22, train_loss=8.454087

Batch 111940, train_perplexity=5880.604, train_loss=8.679415

Batch 111950, train_perplexity=5212.988, train_loss=8.558908

Batch 111960, train_perplexity=4890.018, train_loss=8.494951

Batch 111970, train_perplexity=5363.473, train_loss=8.587367

Batch 111980, train_perplexity=5558.773, train_loss=8.623133

Batch 111990, train_perplexity=5205.3125, train_loss=8.557435

Batch 112000, train_perplexity=5065.9663, train_loss=8.5303

Batch 112010, train_perplexity=5199.7803, train_loss=8.556372

Batch 112020, train_perplexity=5040.281, train_loss=8.525217

Batch 112030, train_perplexity=6385.137, train_loss=8.761728

Batch 112040, train_perplexity=6384.991, train_loss=8.761705

Batch 112050, train_perplexity=5722.76, train_loss=8.652206

Batch 112060, train_perplexity=6194.5806, train_loss=8.73143

Batch 112070, train_perplexity=6611.829, train_loss=8.796616

Batch 112080, train_perplexity=4791.368, train_loss=8.474571

Batch 112090, train_perplexity=5434.7085, train_loss=8.600561

Batch 112100, train_perplexity=5994.5645, train_loss=8.698608

Batch 112110, train_perplexity=5399.4546, train_loss=8.594053

Batch 112120, train_perplexity=5389.9985, train_loss=8.5923

Batch 112130, train_perplexity=5556.833, train_loss=8.622784

Batch 112140, train_perplexity=5608.032, train_loss=8.631955

Batch 112150, train_perplexity=6385.442, train_loss=8.761776

Batch 112160, train_perplexity=5106.249, train_loss=8.53822
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 112170, train_perplexity=5151.331, train_loss=8.54701

Batch 112180, train_perplexity=5383.0947, train_loss=8.591019

Batch 112190, train_perplexity=4469.949, train_loss=8.405132

Batch 112200, train_perplexity=5679.373, train_loss=8.644596

Batch 112210, train_perplexity=5305.7563, train_loss=8.576548

Batch 112220, train_perplexity=5854.986, train_loss=8.675049

Batch 112230, train_perplexity=6096.6816, train_loss=8.7155

Batch 112240, train_perplexity=5642.156, train_loss=8.638021

Batch 112250, train_perplexity=4543.4634, train_loss=8.421445

Batch 112260, train_perplexity=5719.797, train_loss=8.651689

Batch 112270, train_perplexity=5655.7856, train_loss=8.640434

Batch 112280, train_perplexity=5024.4624, train_loss=8.522074

Batch 112290, train_perplexity=5506.7583, train_loss=8.613731

Batch 112300, train_perplexity=6116.319, train_loss=8.718716

Batch 112310, train_perplexity=4624.9834, train_loss=8.439228

Batch 112320, train_perplexity=6141.3115, train_loss=8.722794

Batch 112330, train_perplexity=5105.1147, train_loss=8.537998

Batch 112340, train_perplexity=5399.393, train_loss=8.594042

Batch 112350, train_perplexity=5932.9976, train_loss=8.688285

Batch 112360, train_perplexity=5910.47, train_loss=8.684481

Batch 112370, train_perplexity=5002.3257, train_loss=8.517658

Batch 112380, train_perplexity=5913.481, train_loss=8.68499

Batch 112390, train_perplexity=5924.3525, train_loss=8.686827

Batch 112400, train_perplexity=5329.541, train_loss=8.58102

Batch 112410, train_perplexity=6031.471, train_loss=8.704746

Batch 112420, train_perplexity=5985.5444, train_loss=8.697103

Batch 112430, train_perplexity=4630.288, train_loss=8.440374

Batch 112440, train_perplexity=5479.888, train_loss=8.60884

Batch 112450, train_perplexity=5723.404, train_loss=8.652319

Batch 112460, train_perplexity=6252.436, train_loss=8.740726

Batch 112470, train_perplexity=5765.829, train_loss=8.659704

Batch 112480, train_perplexity=5374.241, train_loss=8.589373

Batch 112490, train_perplexity=5179.454, train_loss=8.552455

Batch 112500, train_perplexity=5783.562, train_loss=8.662775

Batch 112510, train_perplexity=7087.939, train_loss=8.86615

Batch 112520, train_perplexity=5863.3228, train_loss=8.676472

Batch 112530, train_perplexity=5292.8135, train_loss=8.574105

Batch 112540, train_perplexity=5133.039, train_loss=8.543453

Batch 112550, train_perplexity=6199.042, train_loss=8.73215

Batch 112560, train_perplexity=4398.9707, train_loss=8.389126

Batch 112570, train_perplexity=4628.6367, train_loss=8.440018

Batch 112580, train_perplexity=5693.8525, train_loss=8.647142

Batch 112590, train_perplexity=5317.9395, train_loss=8.578841

Batch 112600, train_perplexity=6553.672, train_loss=8.787781

Batch 112610, train_perplexity=5145.4785, train_loss=8.545874

Batch 112620, train_perplexity=5529.8613, train_loss=8.617918

Batch 112630, train_perplexity=5385.7183, train_loss=8.591506

Batch 112640, train_perplexity=5649.36, train_loss=8.6392975

Batch 112650, train_perplexity=6033.473, train_loss=8.705078

Batch 112660, train_perplexity=5247.269, train_loss=8.565463

Batch 112670, train_perplexity=5420.1636, train_loss=8.597881

Batch 112680, train_perplexity=5200.5737, train_loss=8.556524

Batch 112690, train_perplexity=6614.415, train_loss=8.797007

Batch 112700, train_perplexity=5055.6763, train_loss=8.528267

Batch 112710, train_perplexity=5344.0356, train_loss=8.583736

Batch 112720, train_perplexity=4448.045, train_loss=8.40022

Batch 112730, train_perplexity=6177.6133, train_loss=8.728687

Batch 112740, train_perplexity=5341.7886, train_loss=8.583316

Batch 112750, train_perplexity=5887.478, train_loss=8.680583

Batch 112760, train_perplexity=5271.455, train_loss=8.570062

Batch 112770, train_perplexity=5020.951, train_loss=8.521375

Batch 112780, train_perplexity=7187.5454, train_loss=8.880105

Batch 112790, train_perplexity=5575.178, train_loss=8.62608

Batch 112800, train_perplexity=5376.6606, train_loss=8.589823

Batch 112810, train_perplexity=4927.027, train_loss=8.502491

Batch 112820, train_perplexity=5456.713, train_loss=8.604602

Batch 112830, train_perplexity=7815.896, train_loss=8.963915

Batch 112840, train_perplexity=4658.1025, train_loss=8.446363

Batch 112850, train_perplexity=5402.6484, train_loss=8.594645

Batch 112860, train_perplexity=5391.819, train_loss=8.592638

Batch 112870, train_perplexity=5541.2114, train_loss=8.619968

Batch 112880, train_perplexity=5414.873, train_loss=8.596905

Batch 112890, train_perplexity=5276.0117, train_loss=8.570926

Batch 112900, train_perplexity=4573.272, train_loss=8.427984

Batch 112910, train_perplexity=5565.9556, train_loss=8.624424

Batch 112920, train_perplexity=5617.833, train_loss=8.633701

Batch 112930, train_perplexity=5773.9067, train_loss=8.661104

Batch 112940, train_perplexity=4936.278, train_loss=8.504367

Batch 112950, train_perplexity=4595.495, train_loss=8.432832

Batch 112960, train_perplexity=5943.9336, train_loss=8.690126

Batch 112970, train_perplexity=4506.301, train_loss=8.413232

Batch 112980, train_perplexity=4996.871, train_loss=8.516567

Batch 112990, train_perplexity=5724.55, train_loss=8.652519

Batch 113000, train_perplexity=5476.592, train_loss=8.608238

Batch 113010, train_perplexity=5068.595, train_loss=8.530819

Batch 113020, train_perplexity=5333.802, train_loss=8.58182

Batch 113030, train_perplexity=5623.0645, train_loss=8.634632

Batch 113040, train_perplexity=7550.629, train_loss=8.929386

Batch 113050, train_perplexity=5353.585, train_loss=8.585522

Batch 113060, train_perplexity=5457.8267, train_loss=8.604806

Batch 113070, train_perplexity=5378.163, train_loss=8.590102

Batch 113080, train_perplexity=5816.1626, train_loss=8.668396

Batch 113090, train_perplexity=5040.6943, train_loss=8.525299

Batch 113100, train_perplexity=5006.0293, train_loss=8.518398

Batch 113110, train_perplexity=4558.6196, train_loss=8.424775

Batch 113120, train_perplexity=5112.852, train_loss=8.539513

Batch 113130, train_perplexity=5179.706, train_loss=8.552504

Batch 113140, train_perplexity=6006.2954, train_loss=8.700563

Batch 113150, train_perplexity=4950.803, train_loss=8.507305

Batch 113160, train_perplexity=6013.7695, train_loss=8.701807

Batch 113170, train_perplexity=5600.416, train_loss=8.630596

Batch 113180, train_perplexity=5546.3716, train_loss=8.620899

Batch 113190, train_perplexity=5531.623, train_loss=8.618237

Batch 113200, train_perplexity=5005.151, train_loss=8.518223

Batch 113210, train_perplexity=4969.3936, train_loss=8.511053

Batch 113220, train_perplexity=6115.3213, train_loss=8.718553

Batch 113230, train_perplexity=5661.398, train_loss=8.641426

Batch 113240, train_perplexity=6230.9834, train_loss=8.737289

Batch 113250, train_perplexity=5010.3564, train_loss=8.519262

Batch 113260, train_perplexity=5922.7485, train_loss=8.686556

Batch 113270, train_perplexity=5366.7324, train_loss=8.587975

Batch 113280, train_perplexity=4434.398, train_loss=8.397147

Batch 113290, train_perplexity=4707.1445, train_loss=8.456837

Batch 113300, train_perplexity=5651.6016, train_loss=8.639694

Batch 113310, train_perplexity=4973.4, train_loss=8.511859

Batch 113320, train_perplexity=5483.3438, train_loss=8.60947

Batch 113330, train_perplexity=4765.7793, train_loss=8.469216

Batch 113340, train_perplexity=4906.7275, train_loss=8.498363

Batch 113350, train_perplexity=5532.003, train_loss=8.618305

Batch 113360, train_perplexity=5070.0645, train_loss=8.531109

Batch 113370, train_perplexity=5028.796, train_loss=8.522936

Batch 113380, train_perplexity=5345.2437, train_loss=8.583962

Batch 113390, train_perplexity=5734.5054, train_loss=8.654257

Batch 113400, train_perplexity=4755.1787, train_loss=8.4669895

Batch 113410, train_perplexity=5698.6006, train_loss=8.647976

Batch 113420, train_perplexity=4509.237, train_loss=8.413883

Batch 113430, train_perplexity=5214.35, train_loss=8.55917

Batch 113440, train_perplexity=4641.239, train_loss=8.442737

Batch 113450, train_perplexity=5158.5776, train_loss=8.548416

Batch 113460, train_perplexity=5627.0557, train_loss=8.635342

Batch 113470, train_perplexity=5570.7773, train_loss=8.62529

Batch 113480, train_perplexity=5599.1665, train_loss=8.630373
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Batch 113490, train_perplexity=4975.378, train_loss=8.512257

Batch 113500, train_perplexity=4111.004, train_loss=8.321423

Batch 113510, train_perplexity=6165.689, train_loss=8.726755

Batch 113520, train_perplexity=4520.2676, train_loss=8.4163265

Batch 113530, train_perplexity=5468.262, train_loss=8.606716

Batch 113540, train_perplexity=6831.431, train_loss=8.829289

Batch 113550, train_perplexity=6323.2188, train_loss=8.751984

Batch 113560, train_perplexity=5420.6084, train_loss=8.597963

Batch 113570, train_perplexity=4999.8267, train_loss=8.5171585

Batch 113580, train_perplexity=5804.338, train_loss=8.666361

Batch 113590, train_perplexity=4825.844, train_loss=8.481741

Batch 113600, train_perplexity=4910.5337, train_loss=8.499138

Batch 113610, train_perplexity=4371.7446, train_loss=8.382917

Batch 113620, train_perplexity=5512.612, train_loss=8.614794

Batch 113630, train_perplexity=5711.1846, train_loss=8.650182

Batch 113640, train_perplexity=4631.688, train_loss=8.440677

Batch 113650, train_perplexity=5414.7285, train_loss=8.596878

Batch 113660, train_perplexity=5412.168, train_loss=8.596405

Batch 113670, train_perplexity=5226.7773, train_loss=8.56155

Batch 113680, train_perplexity=4832.4116, train_loss=8.483101

Batch 113690, train_perplexity=6771.723, train_loss=8.820511

Batch 113700, train_perplexity=5220.6494, train_loss=8.560377

Batch 113710, train_perplexity=4528.695, train_loss=8.418189

Batch 113720, train_perplexity=5271.4097, train_loss=8.570053

Batch 113730, train_perplexity=5783.595, train_loss=8.662781

Batch 113740, train_perplexity=5027.995, train_loss=8.522777

Batch 113750, train_perplexity=5377.7993, train_loss=8.5900345

Batch 113760, train_perplexity=4256.283, train_loss=8.356152

Batch 113770, train_perplexity=5376.343, train_loss=8.589764

Batch 113780, train_perplexity=4936.3013, train_loss=8.504372

Batch 113790, train_perplexity=5504.7944, train_loss=8.613375

Batch 113800, train_perplexity=5117.1494, train_loss=8.540353

Batch 113810, train_perplexity=5704.4404, train_loss=8.649

Batch 113820, train_perplexity=5234.369, train_loss=8.563002

Batch 113830, train_perplexity=3941.7754, train_loss=8.2793865

Batch 113840, train_perplexity=5836.2207, train_loss=8.671839

Batch 113850, train_perplexity=4993.203, train_loss=8.515833

Batch 113860, train_perplexity=5471.726, train_loss=8.607349

Batch 113870, train_perplexity=5756.335, train_loss=8.658056

Batch 113880, train_perplexity=5648.907, train_loss=8.639217

Batch 113890, train_perplexity=5782.36, train_loss=8.662567

Batch 113900, train_perplexity=4970.3843, train_loss=8.511252

Batch 113910, train_perplexity=5627.576, train_loss=8.635434

Batch 113920, train_perplexity=5090.588, train_loss=8.535149

Batch 113930, train_perplexity=6121.524, train_loss=8.719566

Batch 113940, train_perplexity=5051.358, train_loss=8.527412

Batch 113950, train_perplexity=5275.3374, train_loss=8.570798

Batch 113960, train_perplexity=5233.92, train_loss=8.562916

Batch 113970, train_perplexity=5500.785, train_loss=8.612646

Batch 113980, train_perplexity=6364.825, train_loss=8.758542

Batch 113990, train_perplexity=4692.291, train_loss=8.453676

Batch 114000, train_perplexity=4779.6753, train_loss=8.472128

Batch 114010, train_perplexity=5265.285, train_loss=8.568891

Batch 114020, train_perplexity=4369.523, train_loss=8.382409

Batch 114030, train_perplexity=6982.391, train_loss=8.851147

Batch 114040, train_perplexity=4890.6055, train_loss=8.495071

Batch 114050, train_perplexity=5857.253, train_loss=8.675436

Batch 114060, train_perplexity=6663.6226, train_loss=8.804419

Batch 114070, train_perplexity=5601.757, train_loss=8.630836

Batch 114080, train_perplexity=4947.962, train_loss=8.506731

Batch 114090, train_perplexity=4514.538, train_loss=8.415058

Batch 114100, train_perplexity=5414.0366, train_loss=8.59675

Batch 114110, train_perplexity=5065.9756, train_loss=8.530302

Batch 114120, train_perplexity=5645.74, train_loss=8.638657

Batch 114130, train_perplexity=4969.8105, train_loss=8.511137

Batch 114140, train_perplexity=6519.461, train_loss=8.782547

Batch 114150, train_perplexity=5605.866, train_loss=8.631569

Batch 114160, train_perplexity=4997.2905, train_loss=8.516651

Batch 114170, train_perplexity=5630.631, train_loss=8.635977

Batch 114180, train_perplexity=5613.2705, train_loss=8.632889

Batch 114190, train_perplexity=6395.949, train_loss=8.76342

Batch 114200, train_perplexity=5210.8057, train_loss=8.55849

Batch 114210, train_perplexity=6280.5244, train_loss=8.745209

Batch 114220, train_perplexity=5842.5474, train_loss=8.672922

Batch 114230, train_perplexity=5056.66, train_loss=8.528461

Batch 114240, train_perplexity=5360.7373, train_loss=8.586857

Batch 114250, train_perplexity=5694.222, train_loss=8.647207

Batch 114260, train_perplexity=5581.163, train_loss=8.627152

Batch 114270, train_perplexity=4352.052, train_loss=8.378403

Batch 114280, train_perplexity=5241.8174, train_loss=8.564424

Batch 114290, train_perplexity=4452.2466, train_loss=8.401164

Batch 114300, train_perplexity=6539.774, train_loss=8.785658

Batch 114310, train_perplexity=4712.1973, train_loss=8.45791

Batch 114320, train_perplexity=4294.0, train_loss=8.364974

Batch 114330, train_perplexity=5450.768, train_loss=8.603512

Batch 114340, train_perplexity=5493.7915, train_loss=8.611374

Batch 114350, train_perplexity=4758.8076, train_loss=8.467752

Batch 114360, train_perplexity=5730.728, train_loss=8.653598

Batch 114370, train_perplexity=6153.5527, train_loss=8.724785

Batch 114380, train_perplexity=5909.095, train_loss=8.684248

Batch 114390, train_perplexity=5057.0894, train_loss=8.528546

Batch 114400, train_perplexity=6162.585, train_loss=8.726252

Batch 114410, train_perplexity=6200.2427, train_loss=8.732344

Batch 114420, train_perplexity=5121.983, train_loss=8.541297

Batch 114430, train_perplexity=5286.2407, train_loss=8.572863

Batch 114440, train_perplexity=4657.605, train_loss=8.446257

Batch 114450, train_perplexity=5659.94, train_loss=8.641169

Batch 114460, train_perplexity=5618.5405, train_loss=8.633827

Batch 114470, train_perplexity=5288.9233, train_loss=8.57337

Batch 114480, train_perplexity=5538.4272, train_loss=8.619466

Batch 114490, train_perplexity=5377.1377, train_loss=8.589911

Batch 114500, train_perplexity=6928.6616, train_loss=8.843422

Batch 114510, train_perplexity=4921.3115, train_loss=8.50133

Batch 114520, train_perplexity=5368.56, train_loss=8.588315

Batch 114530, train_perplexity=5321.405, train_loss=8.579493

Batch 114540, train_perplexity=6286.6484, train_loss=8.746183

Batch 114550, train_perplexity=5796.516, train_loss=8.665012

Batch 114560, train_perplexity=5213.8926, train_loss=8.559082

Batch 114570, train_perplexity=6499.304, train_loss=8.77945

Batch 114580, train_perplexity=6488.12, train_loss=8.777728

Batch 114590, train_perplexity=4387.541, train_loss=8.386524

Batch 114600, train_perplexity=5189.234, train_loss=8.554341

Batch 114610, train_perplexity=4752.0415, train_loss=8.46633

Batch 114620, train_perplexity=6055.0547, train_loss=8.708649

Batch 114630, train_perplexity=5757.1147, train_loss=8.658192

Batch 114640, train_perplexity=4916.1846, train_loss=8.500288

Batch 114650, train_perplexity=6016.4253, train_loss=8.702249

Batch 114660, train_perplexity=5552.3623, train_loss=8.621979

Batch 114670, train_perplexity=6300.0205, train_loss=8.748308

Batch 114680, train_perplexity=5507.761, train_loss=8.613914

Batch 114690, train_perplexity=4724.09, train_loss=8.46043

Batch 114700, train_perplexity=4387.495, train_loss=8.386514

Batch 114710, train_perplexity=5869.7007, train_loss=8.677559

Batch 114720, train_perplexity=5493.87, train_loss=8.611388

Batch 114730, train_perplexity=6802.4683, train_loss=8.825041

Batch 114740, train_perplexity=4929.7715, train_loss=8.503048
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 114750, train_perplexity=4612.399, train_loss=8.436503

Batch 114760, train_perplexity=5034.2183, train_loss=8.5240135

Batch 114770, train_perplexity=5193.7935, train_loss=8.55522

Batch 114780, train_perplexity=5316.8394, train_loss=8.578634

Batch 114790, train_perplexity=6141.1006, train_loss=8.722759

Batch 114800, train_perplexity=5806.6743, train_loss=8.666763

Batch 114810, train_perplexity=5430.7085, train_loss=8.599825

Batch 114820, train_perplexity=5252.2104, train_loss=8.566404

Batch 114830, train_perplexity=6200.4907, train_loss=8.732384

Batch 114840, train_perplexity=5416.4897, train_loss=8.597203

Batch 114850, train_perplexity=4919.9556, train_loss=8.501055

Batch 114860, train_perplexity=5321.0596, train_loss=8.579428

Batch 114870, train_perplexity=5002.9604, train_loss=8.517785

Batch 114880, train_perplexity=5467.1987, train_loss=8.606522

Batch 114890, train_perplexity=5089.923, train_loss=8.535018

Batch 114900, train_perplexity=5890.7974, train_loss=8.681147

Batch 114910, train_perplexity=4733.9663, train_loss=8.462519

Batch 114920, train_perplexity=5430.833, train_loss=8.599848

Batch 114930, train_perplexity=4620.266, train_loss=8.438208

Batch 114940, train_perplexity=5478.8643, train_loss=8.608653

Batch 114950, train_perplexity=5454.803, train_loss=8.604252

Batch 114960, train_perplexity=6105.246, train_loss=8.716904

Batch 114970, train_perplexity=5567.4473, train_loss=8.624692

Batch 114980, train_perplexity=4032.4167, train_loss=8.302121

Batch 114990, train_perplexity=4866.0786, train_loss=8.490044

Batch 115000, train_perplexity=6380.7183, train_loss=8.761036

Batch 115010, train_perplexity=6152.602, train_loss=8.72463

Batch 115020, train_perplexity=5297.687, train_loss=8.575026

Batch 115030, train_perplexity=4883.7915, train_loss=8.493677

Batch 115040, train_perplexity=5244.1777, train_loss=8.564874

Batch 115050, train_perplexity=5088.826, train_loss=8.534802

Batch 115060, train_perplexity=5893.7812, train_loss=8.681653

Batch 115070, train_perplexity=5136.4575, train_loss=8.544119

Batch 115080, train_perplexity=4422.94, train_loss=8.39456

Batch 115090, train_perplexity=4802.0767, train_loss=8.476804

Batch 115100, train_perplexity=5468.153, train_loss=8.606696

Batch 115110, train_perplexity=4932.123, train_loss=8.503525

Batch 115120, train_perplexity=5076.199, train_loss=8.532318

Batch 115130, train_perplexity=5586.0884, train_loss=8.628035

Batch 115140, train_perplexity=4817.374, train_loss=8.479984

Batch 115150, train_perplexity=5293.707, train_loss=8.574274

Batch 115160, train_perplexity=5420.846, train_loss=8.598007

Batch 115170, train_perplexity=4489.7896, train_loss=8.409561

Batch 115180, train_perplexity=4869.3696, train_loss=8.49072

Batch 115190, train_perplexity=5637.0356, train_loss=8.637114

Batch 115200, train_perplexity=5111.0674, train_loss=8.539164

Batch 115210, train_perplexity=4752.173, train_loss=8.466357

Batch 115220, train_perplexity=4536.8477, train_loss=8.419988

Batch 115230, train_perplexity=4828.114, train_loss=8.482211

Batch 115240, train_perplexity=5650.61, train_loss=8.639519

Batch 115250, train_perplexity=5316.8594, train_loss=8.578638

Batch 115260, train_perplexity=4851.8804, train_loss=8.487122

Batch 115270, train_perplexity=4755.786, train_loss=8.467117

Batch 115280, train_perplexity=5854.455, train_loss=8.674958

Batch 115290, train_perplexity=4573.0103, train_loss=8.427927

Batch 115300, train_perplexity=6145.36, train_loss=8.723453

Batch 115310, train_perplexity=5373.739, train_loss=8.589279

Batch 115320, train_perplexity=5787.259, train_loss=8.663414

Batch 115330, train_perplexity=4856.769, train_loss=8.488129

Batch 115340, train_perplexity=5505.687, train_loss=8.613537

Batch 115350, train_perplexity=5876.371, train_loss=8.678695

Batch 115360, train_perplexity=5417.239, train_loss=8.597342

Batch 115370, train_perplexity=4392.23, train_loss=8.387592

Batch 115380, train_perplexity=6343.9546, train_loss=8.755258

Batch 115390, train_perplexity=5553.7393, train_loss=8.622227

Batch 115400, train_perplexity=6210.149, train_loss=8.73394

Batch 115410, train_perplexity=6302.7607, train_loss=8.748743

Batch 115420, train_perplexity=4987.782, train_loss=8.514747

Batch 115430, train_perplexity=5325.319, train_loss=8.580228

Batch 115440, train_perplexity=5639.084, train_loss=8.637477

Batch 115450, train_perplexity=5542.1416, train_loss=8.620136

Batch 115460, train_perplexity=5862.0923, train_loss=8.676262

Batch 115470, train_perplexity=4971.844, train_loss=8.511546

Batch 115480, train_perplexity=4467.524, train_loss=8.40459

Batch 115490, train_perplexity=5094.483, train_loss=8.535913

Batch 115500, train_perplexity=5132.4517, train_loss=8.543339

Batch 115510, train_perplexity=6619.8735, train_loss=8.797832

Batch 115520, train_perplexity=5859.801, train_loss=8.675871

Batch 115530, train_perplexity=5563.3286, train_loss=8.623952

Batch 115540, train_perplexity=5492.005, train_loss=8.611049

Batch 115550, train_perplexity=5229.33, train_loss=8.562038

Batch 115560, train_perplexity=4810.7495, train_loss=8.478608

Batch 115570, train_perplexity=5158.238, train_loss=8.54835

Batch 115580, train_perplexity=5349.635, train_loss=8.584784

Batch 115590, train_perplexity=4916.5503, train_loss=8.500362

Batch 115600, train_perplexity=6209.136, train_loss=8.733777

Batch 115610, train_perplexity=5688.507, train_loss=8.646203

Batch 115620, train_perplexity=5508.5547, train_loss=8.614058

Batch 115630, train_perplexity=4988.3296, train_loss=8.514856

Batch 115640, train_perplexity=5161.52, train_loss=8.548986

Batch 115650, train_perplexity=5326.9087, train_loss=8.580526

Batch 115660, train_perplexity=5222.552, train_loss=8.560741

Batch 115670, train_perplexity=5527.5254, train_loss=8.617496

Batch 115680, train_perplexity=4725.676, train_loss=8.460766

Batch 115690, train_perplexity=5996.966, train_loss=8.699009

Batch 115700, train_perplexity=5457.478, train_loss=8.604742

Batch 115710, train_perplexity=5517.282, train_loss=8.615641

Batch 115720, train_perplexity=4832.923, train_loss=8.483207

Batch 115730, train_perplexity=4439.103, train_loss=8.398208

Batch 115740, train_perplexity=5911.361, train_loss=8.684631

Batch 115750, train_perplexity=5437.871, train_loss=8.601143

Batch 115760, train_perplexity=4715.1055, train_loss=8.458527

Batch 115770, train_perplexity=5294.919, train_loss=8.574503

Batch 115780, train_perplexity=4920.6313, train_loss=8.501192

Batch 115790, train_perplexity=5722.9014, train_loss=8.652231

Batch 115800, train_perplexity=5089.0347, train_loss=8.534843

Batch 115810, train_perplexity=4424.923, train_loss=8.395008

Batch 115820, train_perplexity=5807.2007, train_loss=8.666854

Batch 115830, train_perplexity=5110.7607, train_loss=8.5391035

Batch 115840, train_perplexity=5187.0815, train_loss=8.553926

Batch 115850, train_perplexity=4773.5986, train_loss=8.470856

Batch 115860, train_perplexity=6406.773, train_loss=8.765111

Batch 115870, train_perplexity=5463.55, train_loss=8.605854

Batch 115880, train_perplexity=4707.2114, train_loss=8.456851

Batch 115890, train_perplexity=5253.6934, train_loss=8.566687

Batch 115900, train_perplexity=6167.965, train_loss=8.727124

Batch 115910, train_perplexity=4838.739, train_loss=8.484409

Batch 115920, train_perplexity=6190.612, train_loss=8.730789

Batch 115930, train_perplexity=5122.9014, train_loss=8.541476

Batch 115940, train_perplexity=4095.305, train_loss=8.317596

Batch 115950, train_perplexity=5079.744, train_loss=8.533016

Batch 115960, train_perplexity=4846.442, train_loss=8.486

Batch 115970, train_perplexity=5740.1523, train_loss=8.655241

Batch 115980, train_perplexity=5086.8413, train_loss=8.534412

Batch 115990, train_perplexity=5127.7456, train_loss=8.542421

Batch 116000, train_perplexity=5010.108, train_loss=8.519213

Batch 116010, train_perplexity=4718.74, train_loss=8.459297

Batch 116020, train_perplexity=4740.9556, train_loss=8.463994

Batch 116030, train_perplexity=5680.2827, train_loss=8.644756

Batch 116040, train_perplexity=5736.037, train_loss=8.654524

Batch 116050, train_perplexity=5930.8535, train_loss=8.687923

Batch 116060, train_perplexity=5538.3057, train_loss=8.619444
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 116070, train_perplexity=5883.4927, train_loss=8.679906

Batch 116080, train_perplexity=4657.445, train_loss=8.446222

Batch 116090, train_perplexity=5426.774, train_loss=8.5991

Batch 116100, train_perplexity=7499.733, train_loss=8.922623

Batch 116110, train_perplexity=5523.505, train_loss=8.616768

Batch 116120, train_perplexity=5629.466, train_loss=8.63577

Batch 116130, train_perplexity=5285.308, train_loss=8.572686

Batch 116140, train_perplexity=4600.5947, train_loss=8.433941

Batch 116150, train_perplexity=5379.7793, train_loss=8.590403

Batch 116160, train_perplexity=5468.8516, train_loss=8.606824

Batch 116170, train_perplexity=5803.4414, train_loss=8.666206

Batch 116180, train_perplexity=5263.573, train_loss=8.568565

Batch 116190, train_perplexity=5749.411, train_loss=8.656853

Batch 116200, train_perplexity=4382.857, train_loss=8.385456

Batch 116210, train_perplexity=4009.0227, train_loss=8.296303

Batch 116220, train_perplexity=5738.948, train_loss=8.655031

Batch 116230, train_perplexity=5559.669, train_loss=8.623294

Batch 116240, train_perplexity=5246.7886, train_loss=8.5653715

Batch 116250, train_perplexity=4837.908, train_loss=8.484238

Batch 116260, train_perplexity=5217.6733, train_loss=8.559807

Batch 116270, train_perplexity=5427.7573, train_loss=8.599281

Batch 116280, train_perplexity=5749.5864, train_loss=8.656883

Batch 116290, train_perplexity=5654.9277, train_loss=8.640283

Batch 116300, train_perplexity=4635.1836, train_loss=8.441431

Batch 116310, train_perplexity=4884.5645, train_loss=8.493835

Batch 116320, train_perplexity=5372.463, train_loss=8.589042

Batch 116330, train_perplexity=5135.497, train_loss=8.543932

Batch 116340, train_perplexity=5235.867, train_loss=8.563288

Batch 116350, train_perplexity=5251.78, train_loss=8.566322

Batch 116360, train_perplexity=5059.4097, train_loss=8.529005

Batch 116370, train_perplexity=4853.139, train_loss=8.487381

Batch 116380, train_perplexity=5059.405, train_loss=8.529004

Batch 116390, train_perplexity=5852.775, train_loss=8.674671

Batch 116400, train_perplexity=5387.5166, train_loss=8.59184

Batch 116410, train_perplexity=5536.4575, train_loss=8.61911

Batch 116420, train_perplexity=5419.8125, train_loss=8.597816

Batch 116430, train_perplexity=4632.408, train_loss=8.440832

Batch 116440, train_perplexity=4463.4443, train_loss=8.403676

Batch 116450, train_perplexity=5071.506, train_loss=8.531393

Batch 116460, train_perplexity=6106.7427, train_loss=8.717149

Batch 116470, train_perplexity=5275.4683, train_loss=8.570823

Batch 116480, train_perplexity=4775.388, train_loss=8.4712305

Batch 116490, train_perplexity=6134.6265, train_loss=8.7217045

Batch 116500, train_perplexity=5867.921, train_loss=8.677256

Batch 116510, train_perplexity=5286.755, train_loss=8.57296

Batch 116520, train_perplexity=6590.84, train_loss=8.793436

Batch 116530, train_perplexity=5475.9546, train_loss=8.608122

Batch 116540, train_perplexity=5120.6597, train_loss=8.5410385

Batch 116550, train_perplexity=6022.6025, train_loss=8.703275

Batch 116560, train_perplexity=4826.7373, train_loss=8.481926

Batch 116570, train_perplexity=5192.6943, train_loss=8.555008

Batch 116580, train_perplexity=5822.5117, train_loss=8.669487

Batch 116590, train_perplexity=5866.0967, train_loss=8.676945

Batch 116600, train_perplexity=6695.422, train_loss=8.809179

Batch 116610, train_perplexity=5139.319, train_loss=8.544676

Batch 116620, train_perplexity=5270.143, train_loss=8.569813

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Batch 116630, train_perplexity=5179.1724, train_loss=8.552401

Batch 116640, train_perplexity=4819.943, train_loss=8.480517

Batch 116650, train_perplexity=5508.6543, train_loss=8.614076

Batch 116660, train_perplexity=4636.2886, train_loss=8.441669

Batch 116670, train_perplexity=5907.1626, train_loss=8.683921

Batch 116680, train_perplexity=5212.272, train_loss=8.558771

Batch 116690, train_perplexity=5992.7124, train_loss=8.698299

Batch 116700, train_perplexity=5211.596, train_loss=8.558641

Batch 116710, train_perplexity=5080.117, train_loss=8.53309

Batch 116720, train_perplexity=4684.386, train_loss=8.45199

Batch 116730, train_perplexity=5865.8843, train_loss=8.6769085

Batch 116740, train_perplexity=5949.781, train_loss=8.69111

Batch 116750, train_perplexity=5524.4214, train_loss=8.616934

Batch 116760, train_perplexity=6150.3726, train_loss=8.724268

Batch 116770, train_perplexity=4525.8926, train_loss=8.41757

Batch 116780, train_perplexity=5739.4624, train_loss=8.655121

Batch 116790, train_perplexity=5349.0635, train_loss=8.584677

Batch 116800, train_perplexity=4533.0376, train_loss=8.4191475

Batch 116810, train_perplexity=5722.039, train_loss=8.652081

Batch 116820, train_perplexity=5111.877, train_loss=8.539322

Batch 116830, train_perplexity=5539.3623, train_loss=8.619635

Batch 116840, train_perplexity=5746.6265, train_loss=8.656368

Batch 116850, train_perplexity=6151.053, train_loss=8.724379

Batch 116860, train_perplexity=6144.2876, train_loss=8.723278

Batch 116870, train_perplexity=5498.6504, train_loss=8.612258

Batch 116880, train_perplexity=4812.5894, train_loss=8.478991

Batch 116890, train_perplexity=5577.981, train_loss=8.626582

Batch 116900, train_perplexity=5728.6353, train_loss=8.653233

Batch 116910, train_perplexity=4766.043, train_loss=8.469272

Batch 116920, train_perplexity=4511.1553, train_loss=8.414309

Batch 116930, train_perplexity=6478.586, train_loss=8.7762575

Batch 116940, train_perplexity=5721.7554, train_loss=8.652031

Batch 116950, train_perplexity=5417.859, train_loss=8.597456

Batch 116960, train_perplexity=4597.4897, train_loss=8.433266

Batch 116970, train_perplexity=4940.9546, train_loss=8.505314

Batch 116980, train_perplexity=5516.556, train_loss=8.615509

Batch 116990, train_perplexity=4922.8325, train_loss=8.501639

Batch 117000, train_perplexity=5728.111, train_loss=8.653141

Batch 117010, train_perplexity=6030.476, train_loss=8.704581

Batch 117020, train_perplexity=4844.612, train_loss=8.485622

Batch 117030, train_perplexity=5522.146, train_loss=8.616522

Batch 117040, train_perplexity=5189.5107, train_loss=8.554395

Batch 117050, train_perplexity=4914.8813, train_loss=8.500023

Batch 117060, train_perplexity=5583.5107, train_loss=8.627573

Batch 117070, train_perplexity=6332.2583, train_loss=8.753412

Batch 117080, train_perplexity=4478.052, train_loss=8.406943

Batch 117090, train_perplexity=5372.9956, train_loss=8.589141

Batch 117100, train_perplexity=5095.946, train_loss=8.536201

Batch 117110, train_perplexity=6037.053, train_loss=8.705671

Batch 117120, train_perplexity=5519.0083, train_loss=8.615953

Batch 117130, train_perplexity=5747.361, train_loss=8.656496

Batch 117140, train_perplexity=5802.788, train_loss=8.666094

Batch 117150, train_perplexity=5501.724, train_loss=8.612817

Batch 117160, train_perplexity=6790.089, train_loss=8.823219

Batch 117170, train_perplexity=4945.292, train_loss=8.506191

Batch 117180, train_perplexity=5538.57, train_loss=8.619492

Batch 117190, train_perplexity=5297.995, train_loss=8.575084

Batch 117200, train_perplexity=4978.881, train_loss=8.51296

Batch 117210, train_perplexity=5956.4287, train_loss=8.692226

Batch 117220, train_perplexity=5255.9185, train_loss=8.56711

Batch 117230, train_perplexity=4929.8564, train_loss=8.503065

Batch 117240, train_perplexity=5880.503, train_loss=8.679398

Batch 117250, train_perplexity=4835.7954, train_loss=8.483801

Batch 117260, train_perplexity=4755.1514, train_loss=8.466984

Batch 117270, train_perplexity=7710.971, train_loss=8.950399

Batch 117280, train_perplexity=5343.027, train_loss=8.583548

Batch 117290, train_perplexity=4419.8447, train_loss=8.39386

Batch 117300, train_perplexity=6736.708, train_loss=8.815327

Batch 117310, train_perplexity=6420.553, train_loss=8.76726
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 117320, train_perplexity=4807.1216, train_loss=8.477854

Batch 117330, train_perplexity=5428.865, train_loss=8.599485

Batch 117340, train_perplexity=5457.457, train_loss=8.604738

Batch 117350, train_perplexity=5693.8203, train_loss=8.647137

Batch 117360, train_perplexity=4741.218, train_loss=8.464049

Batch 117370, train_perplexity=5318.036, train_loss=8.578859

Batch 117380, train_perplexity=6381.3022, train_loss=8.761127

Batch 117390, train_perplexity=5348.053, train_loss=8.584488

Batch 117400, train_perplexity=5746.681, train_loss=8.656378

Batch 117410, train_perplexity=4985.4043, train_loss=8.51427

Batch 117420, train_perplexity=5625.8755, train_loss=8.635132

Batch 117430, train_perplexity=6511.7188, train_loss=8.781359

Batch 117440, train_perplexity=6066.6147, train_loss=8.710556

Batch 117450, train_perplexity=5315.8506, train_loss=8.578448

Batch 117460, train_perplexity=5761.992, train_loss=8.659039

Batch 117470, train_perplexity=5957.014, train_loss=8.692325

Batch 117480, train_perplexity=5857.5713, train_loss=8.67549

Batch 117490, train_perplexity=4829.9604, train_loss=8.482594

Batch 117500, train_perplexity=5686.755, train_loss=8.645895

Batch 117510, train_perplexity=5110.7656, train_loss=8.539104

Batch 117520, train_perplexity=4988.8003, train_loss=8.514951

Batch 117530, train_perplexity=6636.84, train_loss=8.800391

Batch 117540, train_perplexity=5500.355, train_loss=8.612568

Batch 117550, train_perplexity=5462.711, train_loss=8.6057005

Batch 117560, train_perplexity=5702.308, train_loss=8.648626

Batch 117570, train_perplexity=6442.6714, train_loss=8.770699

Batch 117580, train_perplexity=4685.6323, train_loss=8.452256

Batch 117590, train_perplexity=5279.162, train_loss=8.571523

Batch 117600, train_perplexity=5190.9907, train_loss=8.55468

Batch 117610, train_perplexity=4905.254, train_loss=8.498062

Batch 117620, train_perplexity=4562.669, train_loss=8.425663

Batch 117630, train_perplexity=4943.4717, train_loss=8.505823

Batch 117640, train_perplexity=5662.2075, train_loss=8.641569

Batch 117650, train_perplexity=4855.213, train_loss=8.487808

Batch 117660, train_perplexity=5587.1006, train_loss=8.628216

Batch 117670, train_perplexity=6603.0703, train_loss=8.79529

Batch 117680, train_perplexity=5389.073, train_loss=8.592129

Batch 117690, train_perplexity=4797.001, train_loss=8.475746

Batch 117700, train_perplexity=5548.6567, train_loss=8.621311

Batch 117710, train_perplexity=5107.345, train_loss=8.538435

Batch 117720, train_perplexity=5218.126, train_loss=8.559894

Batch 117730, train_perplexity=4926.2563, train_loss=8.502335

Batch 117740, train_perplexity=5897.576, train_loss=8.682297

Batch 117750, train_perplexity=4848.374, train_loss=8.486399

Batch 117760, train_perplexity=5698.7744, train_loss=8.648006

Batch 117770, train_perplexity=5714.5356, train_loss=8.650768

Batch 117780, train_perplexity=5212.436, train_loss=8.558803

Batch 117790, train_perplexity=5775.8564, train_loss=8.661442

Batch 117800, train_perplexity=5247.554, train_loss=8.565517

Batch 117810, train_perplexity=5304.856, train_loss=8.576378

Batch 117820, train_perplexity=6041.269, train_loss=8.706369

Batch 117830, train_perplexity=4634.114, train_loss=8.4412

Batch 117840, train_perplexity=7086.209, train_loss=8.865906

Batch 117850, train_perplexity=6973.793, train_loss=8.849915

Batch 117860, train_perplexity=5482.5596, train_loss=8.609327

Batch 117870, train_perplexity=6105.9507, train_loss=8.717019

Batch 117880, train_perplexity=6223.3936, train_loss=8.736071

Batch 117890, train_perplexity=5415.276, train_loss=8.596979

Batch 117900, train_perplexity=5479.899, train_loss=8.608842

Batch 117910, train_perplexity=5358.3657, train_loss=8.586414

Batch 117920, train_perplexity=6449.452, train_loss=8.77175

Batch 117930, train_perplexity=5125.487, train_loss=8.541981

Batch 117940, train_perplexity=5003.3467, train_loss=8.517862

Batch 117950, train_perplexity=5472.551, train_loss=8.6075

Batch 117960, train_perplexity=5793.8135, train_loss=8.664546

Batch 117970, train_perplexity=6165.1714, train_loss=8.726671

Batch 117980, train_perplexity=5505.9077, train_loss=8.613577

Batch 117990, train_perplexity=4796.0586, train_loss=8.47555

Batch 118000, train_perplexity=4486.1943, train_loss=8.40876

Batch 118010, train_perplexity=5366.9272, train_loss=8.588011

Batch 118020, train_perplexity=5526.2344, train_loss=8.617262

Batch 118030, train_perplexity=5702.6455, train_loss=8.648685

Batch 118040, train_perplexity=4746.8823, train_loss=8.465243

Batch 118050, train_perplexity=4507.6504, train_loss=8.413531

Batch 118060, train_perplexity=5654.653, train_loss=8.640234

Batch 118070, train_perplexity=5581.498, train_loss=8.627213

Batch 118080, train_perplexity=5028.767, train_loss=8.52293

Batch 118090, train_perplexity=6109.941, train_loss=8.717672

Batch 118100, train_perplexity=5069.3687, train_loss=8.530972

Batch 118110, train_perplexity=5091.7144, train_loss=8.53537

Batch 118120, train_perplexity=5142.746, train_loss=8.545342

Batch 118130, train_perplexity=5062.9717, train_loss=8.529709

Batch 118140, train_perplexity=5561.5303, train_loss=8.623629

Batch 118150, train_perplexity=5067.3384, train_loss=8.530571

Batch 118160, train_perplexity=6061.0576, train_loss=8.70964

Batch 118170, train_perplexity=5973.233, train_loss=8.695044

Batch 118180, train_perplexity=5655.5806, train_loss=8.640398

Batch 118190, train_perplexity=4433.523, train_loss=8.39695

Batch 118200, train_perplexity=6300.5854, train_loss=8.748398

Batch 118210, train_perplexity=7100.0967, train_loss=8.867864

Batch 118220, train_perplexity=5826.9775, train_loss=8.670254

Batch 118230, train_perplexity=6081.734, train_loss=8.713045

Batch 118240, train_perplexity=5868.738, train_loss=8.677395

Batch 118250, train_perplexity=5965.365, train_loss=8.693726

Batch 118260, train_perplexity=5316.682, train_loss=8.578605

Batch 118270, train_perplexity=5411.6724, train_loss=8.596313

Batch 118280, train_perplexity=5986.9146, train_loss=8.697331

Batch 118290, train_perplexity=5220.804, train_loss=8.560407

Batch 118300, train_perplexity=5518.5186, train_loss=8.615865

Batch 118310, train_perplexity=6005.6367, train_loss=8.700454

Batch 118320, train_perplexity=6469.707, train_loss=8.774886

Batch 118330, train_perplexity=6033.496, train_loss=8.705082

Batch 118340, train_perplexity=5151.464, train_loss=8.547036

Batch 118350, train_perplexity=5488.1934, train_loss=8.610354

Batch 118360, train_perplexity=4905.2163, train_loss=8.4980545

Batch 118370, train_perplexity=5301.5635, train_loss=8.575757

Batch 118380, train_perplexity=4916.1562, train_loss=8.500282

Batch 118390, train_perplexity=5465.7803, train_loss=8.606262

Batch 118400, train_perplexity=4851.8525, train_loss=8.487116

Batch 118410, train_perplexity=5928.9985, train_loss=8.687611

Batch 118420, train_perplexity=5826.844, train_loss=8.670231

Batch 118430, train_perplexity=5545.4937, train_loss=8.620741

Batch 118440, train_perplexity=4476.3438, train_loss=8.406562

Batch 118450, train_perplexity=5690.2373, train_loss=8.646507

Batch 118460, train_perplexity=4784.4185, train_loss=8.47312

Batch 118470, train_perplexity=5635.2295, train_loss=8.636793

Batch 118480, train_perplexity=6243.534, train_loss=8.739302

Batch 118490, train_perplexity=6301.4385, train_loss=8.748533

Batch 118500, train_perplexity=4982.2153, train_loss=8.51363

Batch 118510, train_perplexity=5941.695, train_loss=8.68975

Batch 118520, train_perplexity=5015.5244, train_loss=8.520293

Batch 118530, train_perplexity=7499.2607, train_loss=8.92256

Batch 118540, train_perplexity=5324.542, train_loss=8.580082

Batch 118550, train_perplexity=5559.028, train_loss=8.6231785

Batch 118560, train_perplexity=5824.1777, train_loss=8.669773

Batch 118570, train_perplexity=5028.043, train_loss=8.522786

Batch 118580, train_perplexity=4834.0063, train_loss=8.483431

Batch 118590, train_perplexity=7007.8613, train_loss=8.854788

Batch 118600, train_perplexity=5020.669, train_loss=8.521318

Batch 118610, train_perplexity=3805.8374, train_loss=8.244291

Batch 118620, train_perplexity=4653.2627, train_loss=8.445324

Batch 118630, train_perplexity=4504.5176, train_loss=8.412836
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 118640, train_perplexity=5139.814, train_loss=8.544772

Batch 118650, train_perplexity=5434.055, train_loss=8.600441

Batch 118660, train_perplexity=5969.144, train_loss=8.694359

Batch 118670, train_perplexity=4381.641, train_loss=8.385179

Batch 118680, train_perplexity=5028.93, train_loss=8.522963

Batch 118690, train_perplexity=5144.5312, train_loss=8.54569

Batch 118700, train_perplexity=4663.365, train_loss=8.447493

Batch 118710, train_perplexity=5502.5376, train_loss=8.612965

Batch 118720, train_perplexity=4982.101, train_loss=8.513607

Batch 118730, train_perplexity=5456.8535, train_loss=8.604628

Batch 118740, train_perplexity=5619.8267, train_loss=8.634056

Batch 118750, train_perplexity=4783.529, train_loss=8.472934

Batch 118760, train_perplexity=5598.221, train_loss=8.630204

Batch 118770, train_perplexity=4729.8506, train_loss=8.461649

Batch 118780, train_perplexity=6222.7173, train_loss=8.735962

Batch 118790, train_perplexity=5777.1895, train_loss=8.661673

Batch 118800, train_perplexity=5194.923, train_loss=8.555437

Batch 118810, train_perplexity=4568.6167, train_loss=8.426966

Batch 118820, train_perplexity=4907.2285, train_loss=8.498465

Batch 118830, train_perplexity=6285.258, train_loss=8.745962

Batch 118840, train_perplexity=6099.723, train_loss=8.715999

Batch 118850, train_perplexity=5543.1196, train_loss=8.620313

Batch 118860, train_perplexity=6205.14, train_loss=8.733133

Batch 118870, train_perplexity=5007.5234, train_loss=8.518697

Batch 118880, train_perplexity=6618.447, train_loss=8.797616

Batch 118890, train_perplexity=4673.311, train_loss=8.449623

Batch 118900, train_perplexity=4542.688, train_loss=8.421274

Batch 118910, train_perplexity=5411.7656, train_loss=8.596331

Batch 118920, train_perplexity=7429.733, train_loss=8.913245

Batch 118930, train_perplexity=6604.84, train_loss=8.795558

Batch 118940, train_perplexity=4566.456, train_loss=8.426493

Batch 118950, train_perplexity=5445.8994, train_loss=8.602618

Batch 118960, train_perplexity=4666.871, train_loss=8.448244

Batch 118970, train_perplexity=5701.5635, train_loss=8.648496

Batch 118980, train_perplexity=5286.2256, train_loss=8.57286

Batch 118990, train_perplexity=5775.691, train_loss=8.661413

Batch 119000, train_perplexity=5598.8354, train_loss=8.630314

Batch 119010, train_perplexity=5962.055, train_loss=8.693171

Batch 119020, train_perplexity=6199.4146, train_loss=8.73221

Batch 119030, train_perplexity=5207.85, train_loss=8.557922

Batch 119040, train_perplexity=6298.591, train_loss=8.748081

Batch 119050, train_perplexity=5026.5234, train_loss=8.522484

Batch 119060, train_perplexity=6195.2715, train_loss=8.731542

Batch 119070, train_perplexity=5184.8955, train_loss=8.553505

Batch 119080, train_perplexity=6612.031, train_loss=8.796646

Batch 119090, train_perplexity=5305.1646, train_loss=8.576436

Batch 119100, train_perplexity=5742.1123, train_loss=8.655582

Batch 119110, train_perplexity=4772.7607, train_loss=8.47068

Batch 119120, train_perplexity=6038.6255, train_loss=8.705932

Batch 119130, train_perplexity=6027.2046, train_loss=8.704039

Batch 119140, train_perplexity=5383.0947, train_loss=8.591019

Batch 119150, train_perplexity=5290.2754, train_loss=8.573626

Batch 119160, train_perplexity=5117.086, train_loss=8.54034

Batch 119170, train_perplexity=6025.8657, train_loss=8.703816

Batch 119180, train_perplexity=5554.2373, train_loss=8.622316

Batch 119190, train_perplexity=4841.9976, train_loss=8.485083

Batch 119200, train_perplexity=4848.439, train_loss=8.486412

Batch 119210, train_perplexity=5413.892, train_loss=8.596724

Batch 119220, train_perplexity=5616.183, train_loss=8.633408

Batch 119230, train_perplexity=5389.5874, train_loss=8.592224

Batch 119240, train_perplexity=5805.401, train_loss=8.666544

Batch 119250, train_perplexity=5554.1523, train_loss=8.622301

Batch 119260, train_perplexity=5039.6655, train_loss=8.525095

Batch 119270, train_perplexity=4748.2495, train_loss=8.465531

Batch 119280, train_perplexity=5732.4116, train_loss=8.653892

Batch 119290, train_perplexity=5030.561, train_loss=8.523287

Batch 119300, train_perplexity=4682.4385, train_loss=8.451574

Batch 119310, train_perplexity=6045.3843, train_loss=8.70705

Batch 119320, train_perplexity=6305.827, train_loss=8.749229

Batch 119330, train_perplexity=4844.464, train_loss=8.485592

Batch 119340, train_perplexity=4857.881, train_loss=8.488358

Batch 119350, train_perplexity=4495.265, train_loss=8.41078

Batch 119360, train_perplexity=5731.2417, train_loss=8.6536875

Batch 119370, train_perplexity=6568.6265, train_loss=8.79006

Batch 119380, train_perplexity=5281.3125, train_loss=8.57193

Batch 119390, train_perplexity=5019.7017, train_loss=8.521126

Batch 119400, train_perplexity=6006.983, train_loss=8.700678

Batch 119410, train_perplexity=4553.549, train_loss=8.423662

Batch 119420, train_perplexity=5117.803, train_loss=8.540481

Batch 119430, train_perplexity=4783.839, train_loss=8.472999

Batch 119440, train_perplexity=4706.5337, train_loss=8.456707

Batch 119450, train_perplexity=5631.5225, train_loss=8.636135

Batch 119460, train_perplexity=5854.991, train_loss=8.67505

Batch 119470, train_perplexity=5723.518, train_loss=8.652339

Batch 119480, train_perplexity=5173.545, train_loss=8.551313

Batch 119490, train_perplexity=5900.502, train_loss=8.682793

Batch 119500, train_perplexity=5668.053, train_loss=8.642601

Batch 119510, train_perplexity=5039.214, train_loss=8.525005

Batch 119520, train_perplexity=5118.101, train_loss=8.540539

Batch 119530, train_perplexity=5171.508, train_loss=8.55092

Batch 119540, train_perplexity=4669.2305, train_loss=8.44875

Batch 119550, train_perplexity=6007.074, train_loss=8.700693

Batch 119560, train_perplexity=4844.4316, train_loss=8.485585

Batch 119570, train_perplexity=5009.315, train_loss=8.519054

Batch 119580, train_perplexity=5912.861, train_loss=8.684885

Batch 119590, train_perplexity=5953.4077, train_loss=8.691719

Batch 119600, train_perplexity=5998.6304, train_loss=8.699286

Batch 119610, train_perplexity=5679.3677, train_loss=8.644595

Batch 119620, train_perplexity=5309.1577, train_loss=8.5771885

Batch 119630, train_perplexity=5642.575, train_loss=8.638096

Batch 119640, train_perplexity=5357.947, train_loss=8.586336

Batch 119650, train_perplexity=6371.5117, train_loss=8.759592

Batch 119660, train_perplexity=5322.5566, train_loss=8.579709

Batch 119670, train_perplexity=5064.0293, train_loss=8.529918

Batch 119680, train_perplexity=4822.403, train_loss=8.481028

Batch 119690, train_perplexity=5063.2515, train_loss=8.529764

Batch 119700, train_perplexity=5277.2295, train_loss=8.5711565

Batch 119710, train_perplexity=5109.586, train_loss=8.538874

Batch 119720, train_perplexity=4433.8486, train_loss=8.397023

Batch 119730, train_perplexity=5564.7827, train_loss=8.624213

Batch 119740, train_perplexity=5047.9917, train_loss=8.526746

Batch 119750, train_perplexity=5810.508, train_loss=8.667423

Batch 119760, train_perplexity=4382.276, train_loss=8.385324

Batch 119770, train_perplexity=5464.644, train_loss=8.606054

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100
Loaded 304503 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100
Loaded 304503 sentences.
Finished loading
Batch 119780, train_perplexity=5661.873, train_loss=8.64151

Batch 119790, train_perplexity=4535.1733, train_loss=8.419619

Batch 119800, train_perplexity=4890.2417, train_loss=8.494997

Batch 119810, train_perplexity=5587.5166, train_loss=8.62829

Batch 119820, train_perplexity=4723.1797, train_loss=8.4602375

Batch 119830, train_perplexity=5136.0557, train_loss=8.544041

Batch 119840, train_perplexity=6181.739, train_loss=8.729355

Batch 119850, train_perplexity=6269.698, train_loss=8.743484

Batch 119860, train_perplexity=5374.036, train_loss=8.5893345

Batch 119870, train_perplexity=5669.6646, train_loss=8.642885

Batch 119880, train_perplexity=5224.943, train_loss=8.561199
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 119890, train_perplexity=3999.598, train_loss=8.293949

Batch 119900, train_perplexity=4253.0493, train_loss=8.3553915

Batch 119910, train_perplexity=5465.52, train_loss=8.606215

Batch 119920, train_perplexity=4568.5293, train_loss=8.426947

Batch 119930, train_perplexity=6554.122, train_loss=8.787849

Batch 119940, train_perplexity=5766.9014, train_loss=8.65989

Batch 119950, train_perplexity=5680.478, train_loss=8.644791

Batch 119960, train_perplexity=4416.213, train_loss=8.393038

Batch 119970, train_perplexity=4911.9995, train_loss=8.499436

Batch 119980, train_perplexity=5317.5796, train_loss=8.5787735

Batch 119990, train_perplexity=5187.116, train_loss=8.553933

Batch 120000, train_perplexity=5949.412, train_loss=8.691048

Batch 120010, train_perplexity=7069.6445, train_loss=8.863565

Batch 120020, train_perplexity=5402.942, train_loss=8.594699

Batch 120030, train_perplexity=6175.552, train_loss=8.7283535

Batch 120040, train_perplexity=5174.6157, train_loss=8.55152

Batch 120050, train_perplexity=5470.098, train_loss=8.607052

Batch 120060, train_perplexity=5802.6113, train_loss=8.666063

Batch 120070, train_perplexity=5449.999, train_loss=8.603371

Batch 120080, train_perplexity=5297.4546, train_loss=8.574982

Batch 120090, train_perplexity=4427.8774, train_loss=8.395676

Batch 120100, train_perplexity=5411.6934, train_loss=8.596317

Batch 120110, train_perplexity=5289.1655, train_loss=8.573416

Batch 120120, train_perplexity=5946.6777, train_loss=8.690588

Batch 120130, train_perplexity=5233.4907, train_loss=8.562834

Batch 120140, train_perplexity=5035.068, train_loss=8.524182

Batch 120150, train_perplexity=5239.748, train_loss=8.564029

Batch 120160, train_perplexity=5592.181, train_loss=8.629125

Batch 120170, train_perplexity=4749.459, train_loss=8.465786

Batch 120180, train_perplexity=5802.025, train_loss=8.665962

Batch 120190, train_perplexity=6085.476, train_loss=8.71366

Batch 120200, train_perplexity=5677.651, train_loss=8.644293

Batch 120210, train_perplexity=5014.9453, train_loss=8.520178

Batch 120220, train_perplexity=5277.023, train_loss=8.571117

Batch 120230, train_perplexity=4640.4424, train_loss=8.442565

Batch 120240, train_perplexity=5257.217, train_loss=8.567357

Batch 120250, train_perplexity=5571.5107, train_loss=8.625422

Batch 120260, train_perplexity=5864.866, train_loss=8.676735

Batch 120270, train_perplexity=4712.642, train_loss=8.458004

Batch 120280, train_perplexity=4873.049, train_loss=8.491475

Batch 120290, train_perplexity=4889.8267, train_loss=8.494912

Batch 120300, train_perplexity=5922.3584, train_loss=8.68649

Batch 120310, train_perplexity=5047.4478, train_loss=8.526638

Batch 120320, train_perplexity=5500.722, train_loss=8.612635

Batch 120330, train_perplexity=5298.5054, train_loss=8.57518

Batch 120340, train_perplexity=4971.2515, train_loss=8.511427

Batch 120350, train_perplexity=5155.543, train_loss=8.547828

Batch 120360, train_perplexity=6429.696, train_loss=8.7686825

Batch 120370, train_perplexity=4978.7764, train_loss=8.512939

Batch 120380, train_perplexity=5251.78, train_loss=8.566322

Batch 120390, train_perplexity=5759.086, train_loss=8.658534

Batch 120400, train_perplexity=6014.114, train_loss=8.701864

Batch 120410, train_perplexity=5405.6685, train_loss=8.595203

Batch 120420, train_perplexity=6057.804, train_loss=8.709103

Batch 120430, train_perplexity=6135.8496, train_loss=8.721904

Batch 120440, train_perplexity=4945.113, train_loss=8.506155

Batch 120450, train_perplexity=4645.0913, train_loss=8.443566

Batch 120460, train_perplexity=4966.1484, train_loss=8.5104

Batch 120470, train_perplexity=4889.6357, train_loss=8.494873

Batch 120480, train_perplexity=5872.4946, train_loss=8.678035

Batch 120490, train_perplexity=4648.1665, train_loss=8.444228

Batch 120500, train_perplexity=5451.9795, train_loss=8.603734

Batch 120510, train_perplexity=6023.2974, train_loss=8.70339

Batch 120520, train_perplexity=5494.64, train_loss=8.611528

Batch 120530, train_perplexity=6180.206, train_loss=8.729107

Batch 120540, train_perplexity=6140.2456, train_loss=8.72262

Batch 120550, train_perplexity=5017.9404, train_loss=8.520775

Batch 120560, train_perplexity=5492.398, train_loss=8.61112

Batch 120570, train_perplexity=6091.3, train_loss=8.714617

Batch 120580, train_perplexity=5594.469, train_loss=8.629534

Batch 120590, train_perplexity=5842.1016, train_loss=8.672846

Batch 120600, train_perplexity=5719.257, train_loss=8.651594

Batch 120610, train_perplexity=6084.983, train_loss=8.713579

Batch 120620, train_perplexity=6082.4995, train_loss=8.713171

Batch 120630, train_perplexity=5828.845, train_loss=8.670574

Batch 120640, train_perplexity=5187.0566, train_loss=8.553922

Batch 120650, train_perplexity=5963.625, train_loss=8.693434

Batch 120660, train_perplexity=5069.4507, train_loss=8.530988

Batch 120670, train_perplexity=4857.047, train_loss=8.488186

Batch 120680, train_perplexity=5416.402, train_loss=8.597187

Batch 120690, train_perplexity=3962.0972, train_loss=8.284529

Batch 120700, train_perplexity=5707.5474, train_loss=8.649545

Batch 120710, train_perplexity=6802.968, train_loss=8.825114

Batch 120720, train_perplexity=4653.915, train_loss=8.445464

Batch 120730, train_perplexity=5211.4766, train_loss=8.558619

Batch 120740, train_perplexity=4769.644, train_loss=8.470027

Batch 120750, train_perplexity=4283.3457, train_loss=8.36249

Batch 120760, train_perplexity=4653.711, train_loss=8.44542

Batch 120770, train_perplexity=5739.353, train_loss=8.655102

Batch 120780, train_perplexity=5100.5693, train_loss=8.537107

Batch 120790, train_perplexity=5373.216, train_loss=8.589182

Batch 120800, train_perplexity=5398.4043, train_loss=8.593859

Batch 120810, train_perplexity=4803.1484, train_loss=8.477027

Batch 120820, train_perplexity=4763.3984, train_loss=8.468717

Batch 120830, train_perplexity=5543.6904, train_loss=8.620416

Batch 120840, train_perplexity=5902.6855, train_loss=8.683163

Batch 120850, train_perplexity=4227.8555, train_loss=8.34945

Batch 120860, train_perplexity=6088.001, train_loss=8.714075

Batch 120870, train_perplexity=5642.828, train_loss=8.638141

Batch 120880, train_perplexity=4943.4624, train_loss=8.505821

Batch 120890, train_perplexity=6157.098, train_loss=8.725361

Batch 120900, train_perplexity=4885.734, train_loss=8.494075

Batch 120910, train_perplexity=5211.6953, train_loss=8.5586605

Batch 120920, train_perplexity=6039.6104, train_loss=8.706095

Batch 120930, train_perplexity=4873.9136, train_loss=8.4916525

Batch 120940, train_perplexity=4910.693, train_loss=8.49917

Batch 120950, train_perplexity=5697.0356, train_loss=8.647701

Batch 120960, train_perplexity=4915.613, train_loss=8.500172

Batch 120970, train_perplexity=5349.395, train_loss=8.584739

Batch 120980, train_perplexity=4817.7695, train_loss=8.480066

Batch 120990, train_perplexity=3926.7937, train_loss=8.2755785

Batch 121000, train_perplexity=5274.9097, train_loss=8.570717

Batch 121010, train_perplexity=4359.8076, train_loss=8.380183

Batch 121020, train_perplexity=5433.3145, train_loss=8.600305

Batch 121030, train_perplexity=5190.852, train_loss=8.554653

Batch 121040, train_perplexity=4936.4614, train_loss=8.504404

Batch 121050, train_perplexity=5685.9844, train_loss=8.64576

Batch 121060, train_perplexity=4970.2563, train_loss=8.511227

Batch 121070, train_perplexity=4861.0874, train_loss=8.4890175

Batch 121080, train_perplexity=6319.674, train_loss=8.751423

Batch 121090, train_perplexity=6554.478, train_loss=8.787904

Batch 121100, train_perplexity=5711.931, train_loss=8.650312

Batch 121110, train_perplexity=6046.993, train_loss=8.707316

Batch 121120, train_perplexity=5536.3516, train_loss=8.619091

Batch 121130, train_perplexity=5389.793, train_loss=8.592262

Batch 121140, train_perplexity=4503.8477, train_loss=8.412687

Batch 121150, train_perplexity=4873.4814, train_loss=8.491564

Batch 121160, train_perplexity=4690.837, train_loss=8.453366

Batch 121170, train_perplexity=6758.452, train_loss=8.818549

Batch 121180, train_perplexity=4902.9717, train_loss=8.497597

Batch 121190, train_perplexity=4885.7666, train_loss=8.4940815

Batch 121200, train_perplexity=5631.0605, train_loss=8.636053
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 121210, train_perplexity=5740.3276, train_loss=8.655272

Batch 121220, train_perplexity=6684.7036, train_loss=8.807577

Batch 121230, train_perplexity=4461.3164, train_loss=8.403199

Batch 121240, train_perplexity=5442.696, train_loss=8.60203

Batch 121250, train_perplexity=4746.3257, train_loss=8.465126

Batch 121260, train_perplexity=4541.8647, train_loss=8.421093

Batch 121270, train_perplexity=5030.192, train_loss=8.523213

Batch 121280, train_perplexity=5418.655, train_loss=8.597603

Batch 121290, train_perplexity=4748.499, train_loss=8.465584

Batch 121300, train_perplexity=5423.9697, train_loss=8.598583

Batch 121310, train_perplexity=6046.2085, train_loss=8.707187

Batch 121320, train_perplexity=4744.2803, train_loss=8.464695

Batch 121330, train_perplexity=6260.527, train_loss=8.74202

Batch 121340, train_perplexity=6015.7427, train_loss=8.702135

Batch 121350, train_perplexity=6021.092, train_loss=8.703024

Batch 121360, train_perplexity=5915.0996, train_loss=8.685264

Batch 121370, train_perplexity=6402.9736, train_loss=8.764518

Batch 121380, train_perplexity=4699.138, train_loss=8.455134

Batch 121390, train_perplexity=5495.4736, train_loss=8.61168

Batch 121400, train_perplexity=5366.876, train_loss=8.588001

Batch 121410, train_perplexity=6270.5894, train_loss=8.743626

Batch 121420, train_perplexity=5029.491, train_loss=8.523074

Batch 121430, train_perplexity=4830.43, train_loss=8.482691

Batch 121440, train_perplexity=5069.301, train_loss=8.530958

Batch 121450, train_perplexity=4797.733, train_loss=8.475899

Batch 121460, train_perplexity=6018.853, train_loss=8.702652

Batch 121470, train_perplexity=5453.836, train_loss=8.6040745

Batch 121480, train_perplexity=4623.4136, train_loss=8.438889

Batch 121490, train_perplexity=5606.294, train_loss=8.631645

Batch 121500, train_perplexity=4907.781, train_loss=8.498577

Batch 121510, train_perplexity=6404.494, train_loss=8.764755

Batch 121520, train_perplexity=4984.354, train_loss=8.514059

Batch 121530, train_perplexity=6197.8657, train_loss=8.73196

Batch 121540, train_perplexity=6201.2124, train_loss=8.7325

Batch 121550, train_perplexity=4726.428, train_loss=8.460925

Batch 121560, train_perplexity=5891.64, train_loss=8.68129

Batch 121570, train_perplexity=4978.8477, train_loss=8.512954

Batch 121580, train_perplexity=4950.3784, train_loss=8.507219

Batch 121590, train_perplexity=5209.7974, train_loss=8.558296

Batch 121600, train_perplexity=4301.267, train_loss=8.366665

Batch 121610, train_perplexity=5012.0195, train_loss=8.519594

Batch 121620, train_perplexity=6076.435, train_loss=8.712173

Batch 121630, train_perplexity=5090.627, train_loss=8.535156

Batch 121640, train_perplexity=5605.0483, train_loss=8.631423

Batch 121650, train_perplexity=5514.3994, train_loss=8.615118

Batch 121660, train_perplexity=4689.1953, train_loss=8.453016

Batch 121670, train_perplexity=4881.0723, train_loss=8.49312

Batch 121680, train_perplexity=4805.224, train_loss=8.477459

Batch 121690, train_perplexity=6148.373, train_loss=8.723943

Batch 121700, train_perplexity=4790.678, train_loss=8.474427

Batch 121710, train_perplexity=5081.0864, train_loss=8.53328

Batch 121720, train_perplexity=4577.6533, train_loss=8.428942

Batch 121730, train_perplexity=5082.7437, train_loss=8.533607

Batch 121740, train_perplexity=4298.3926, train_loss=8.365996

Batch 121750, train_perplexity=7092.916, train_loss=8.866852

Batch 121760, train_perplexity=6775.082, train_loss=8.821007

Batch 121770, train_perplexity=4966.21, train_loss=8.510412

Batch 121780, train_perplexity=4985.0386, train_loss=8.514196

Batch 121790, train_perplexity=5732.0234, train_loss=8.653824

Batch 121800, train_perplexity=5471.011, train_loss=8.607219

Batch 121810, train_perplexity=4698.914, train_loss=8.455087

Batch 121820, train_perplexity=5847.5527, train_loss=8.673779

Batch 121830, train_perplexity=4816.5933, train_loss=8.479822

Batch 121840, train_perplexity=4839.0894, train_loss=8.484482

Batch 121850, train_perplexity=4900.354, train_loss=8.497063

Batch 121860, train_perplexity=6159.236, train_loss=8.725708

Batch 121870, train_perplexity=4740.9106, train_loss=8.4639845

Batch 121880, train_perplexity=4287.715, train_loss=8.363509

Batch 121890, train_perplexity=5432.356, train_loss=8.600128

Batch 121900, train_perplexity=5120.103, train_loss=8.54093

Batch 121910, train_perplexity=4973.049, train_loss=8.511788

Batch 121920, train_perplexity=6163.2197, train_loss=8.726355

Batch 121930, train_perplexity=5505.4507, train_loss=8.613494

Batch 121940, train_perplexity=4986.3696, train_loss=8.514463

Batch 121950, train_perplexity=4941.36, train_loss=8.505396

Batch 121960, train_perplexity=4860.008, train_loss=8.488795

Batch 121970, train_perplexity=4292.9927, train_loss=8.364739

Batch 121980, train_perplexity=5661.0845, train_loss=8.641371

Batch 121990, train_perplexity=5622.3354, train_loss=8.634502

Batch 122000, train_perplexity=5259.88, train_loss=8.567863

Batch 122010, train_perplexity=5386.8896, train_loss=8.591723

Batch 122020, train_perplexity=5196.925, train_loss=8.555822

Batch 122030, train_perplexity=4630.7075, train_loss=8.440465

Batch 122040, train_perplexity=5871.817, train_loss=8.677919

Batch 122050, train_perplexity=5537.6245, train_loss=8.619321

Batch 122060, train_perplexity=5214.1416, train_loss=8.55913

Batch 122070, train_perplexity=5274.7236, train_loss=8.570682

Batch 122080, train_perplexity=5078.2573, train_loss=8.532723

Batch 122090, train_perplexity=5443.7964, train_loss=8.602232

Batch 122100, train_perplexity=4724.252, train_loss=8.4604645

Batch 122110, train_perplexity=4722.4907, train_loss=8.460092

Batch 122120, train_perplexity=4742.0815, train_loss=8.4642315

Batch 122130, train_perplexity=5510.1836, train_loss=8.614353

Batch 122140, train_perplexity=5307.9326, train_loss=8.576958

Batch 122150, train_perplexity=4835.5923, train_loss=8.483759

Batch 122160, train_perplexity=6274.2026, train_loss=8.744202

Batch 122170, train_perplexity=5951.097, train_loss=8.691331

Batch 122180, train_perplexity=6438.433, train_loss=8.7700405

Batch 122190, train_perplexity=4877.182, train_loss=8.492323

Batch 122200, train_perplexity=4525.7114, train_loss=8.41753

Batch 122210, train_perplexity=4800.589, train_loss=8.476494

Batch 122220, train_perplexity=4654.186, train_loss=8.445522

Batch 122230, train_perplexity=5678.463, train_loss=8.644436

Batch 122240, train_perplexity=6514.4453, train_loss=8.781777

Batch 122250, train_perplexity=4595.1445, train_loss=8.432755

Batch 122260, train_perplexity=5868.055, train_loss=8.6772785

Batch 122270, train_perplexity=5376.5684, train_loss=8.589806

Batch 122280, train_perplexity=5181.1387, train_loss=8.55278

Batch 122290, train_perplexity=4450.8667, train_loss=8.400854

Batch 122300, train_perplexity=5202.8706, train_loss=8.556966

Batch 122310, train_perplexity=5638.8687, train_loss=8.637439

Batch 122320, train_perplexity=6500.178, train_loss=8.779585

Batch 122330, train_perplexity=4700.6978, train_loss=8.455466

Batch 122340, train_perplexity=5142.1426, train_loss=8.545225

Batch 122350, train_perplexity=5954.3843, train_loss=8.691883

Batch 122360, train_perplexity=5723.5566, train_loss=8.652346

Batch 122370, train_perplexity=5059.313, train_loss=8.528986

Batch 122380, train_perplexity=4108.6055, train_loss=8.320839

Batch 122390, train_perplexity=5724.7793, train_loss=8.652559

Batch 122400, train_perplexity=4814.747, train_loss=8.479439

Batch 122410, train_perplexity=5475.5054, train_loss=8.60804

Batch 122420, train_perplexity=5082.012, train_loss=8.533463

Batch 122430, train_perplexity=4754.616, train_loss=8.466871

Batch 122440, train_perplexity=4908.67, train_loss=8.498758

Batch 122450, train_perplexity=5277.486, train_loss=8.571205

Batch 122460, train_perplexity=5581.349, train_loss=8.627186

Batch 122470, train_perplexity=4867.5356, train_loss=8.490343

Batch 122480, train_perplexity=4918.562, train_loss=8.5007715

Batch 122490, train_perplexity=6099.5894, train_loss=8.715977

Batch 122500, train_perplexity=4481.7046, train_loss=8.407759

Batch 122510, train_perplexity=5200.6533, train_loss=8.55654

Batch 122520, train_perplexity=4504.3457, train_loss=8.412798
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 122530, train_perplexity=5021.2437, train_loss=8.521433

Batch 122540, train_perplexity=5897.925, train_loss=8.682356

Batch 122550, train_perplexity=4791.0527, train_loss=8.474505

Batch 122560, train_perplexity=4548.1714, train_loss=8.422481

Batch 122570, train_perplexity=4580.92, train_loss=8.429655

Batch 122580, train_perplexity=5083.2725, train_loss=8.5337105

Batch 122590, train_perplexity=5367.9253, train_loss=8.588197

Batch 122600, train_perplexity=4892.7607, train_loss=8.495512

Batch 122610, train_perplexity=4764.6206, train_loss=8.468973

Batch 122620, train_perplexity=5582.989, train_loss=8.62748

Batch 122630, train_perplexity=5562.5596, train_loss=8.623814

Batch 122640, train_perplexity=5946.899, train_loss=8.690625

Batch 122650, train_perplexity=4695.478, train_loss=8.454355

Batch 122660, train_perplexity=5157.2886, train_loss=8.548166

Batch 122670, train_perplexity=5632.994, train_loss=8.636396

Batch 122680, train_perplexity=5141.3237, train_loss=8.545066

Batch 122690, train_perplexity=4913.5693, train_loss=8.499756

Batch 122700, train_perplexity=5813.3125, train_loss=8.667906

Batch 122710, train_perplexity=5200.435, train_loss=8.556498

Batch 122720, train_perplexity=6074.465, train_loss=8.711849

Batch 122730, train_perplexity=5350.6855, train_loss=8.58498

Batch 122740, train_perplexity=5393.9683, train_loss=8.593037

Batch 122750, train_perplexity=5097.8994, train_loss=8.536584

Batch 122760, train_perplexity=4549.911, train_loss=8.422863

Batch 122770, train_perplexity=6726.052, train_loss=8.813744

Batch 122780, train_perplexity=5211.3774, train_loss=8.558599

Batch 122790, train_perplexity=5308.51, train_loss=8.577066

Batch 122800, train_perplexity=6480.3467, train_loss=8.776529

Batch 122810, train_perplexity=5032.356, train_loss=8.5236435

Batch 122820, train_perplexity=5865.4253, train_loss=8.67683

Batch 122830, train_perplexity=5820.441, train_loss=8.669131

Batch 122840, train_perplexity=4678.43, train_loss=8.450718

Batch 122850, train_perplexity=6326.9346, train_loss=8.752571

Batch 122860, train_perplexity=6027.6763, train_loss=8.704117

Batch 122870, train_perplexity=5731.526, train_loss=8.653737

Batch 122880, train_perplexity=5035.4907, train_loss=8.524266

Batch 122890, train_perplexity=4871.6924, train_loss=8.491197

Batch 122900, train_perplexity=4646.93, train_loss=8.443962

Batch 122910, train_perplexity=4842.792, train_loss=8.485247

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Batch 122920, train_perplexity=5120.7524, train_loss=8.541057

Batch 122930, train_perplexity=5384.604, train_loss=8.591299

Batch 122940, train_perplexity=6320.506, train_loss=8.7515545

Batch 122950, train_perplexity=5219.0566, train_loss=8.560072

Batch 122960, train_perplexity=5525.6333, train_loss=8.617153

Batch 122970, train_perplexity=4488.514, train_loss=8.409277

Batch 122980, train_perplexity=5278.946, train_loss=8.571482

Batch 122990, train_perplexity=5162.7217, train_loss=8.549219

Batch 123000, train_perplexity=5360.7275, train_loss=8.586855

Batch 123010, train_perplexity=4671.867, train_loss=8.449314

Batch 123020, train_perplexity=5308.0034, train_loss=8.576971

Batch 123030, train_perplexity=5070.2676, train_loss=8.531149

Batch 123040, train_perplexity=6221.0796, train_loss=8.735699

Batch 123050, train_perplexity=4355.0996, train_loss=8.379103

Batch 123060, train_perplexity=5045.2534, train_loss=8.526203

Batch 123070, train_perplexity=5004.4775, train_loss=8.518088

Batch 123080, train_perplexity=5822.6562, train_loss=8.669512

Batch 123090, train_perplexity=5412.9424, train_loss=8.596548

Batch 123100, train_perplexity=5724.943, train_loss=8.652588

Batch 123110, train_perplexity=5939.525, train_loss=8.689384

Batch 123120, train_perplexity=6155.636, train_loss=8.725123

Batch 123130, train_perplexity=5345.0244, train_loss=8.583921

Batch 123140, train_perplexity=4951.417, train_loss=8.507429

Batch 123150, train_perplexity=5120.9087, train_loss=8.541087

Batch 123160, train_perplexity=4996.7617, train_loss=8.516545

Batch 123170, train_perplexity=5807.627, train_loss=8.666927

Batch 123180, train_perplexity=5071.801, train_loss=8.531451

Batch 123190, train_perplexity=7442.8315, train_loss=8.915007

Batch 123200, train_perplexity=5870.1934, train_loss=8.677643

Batch 123210, train_perplexity=5628.1562, train_loss=8.635537

Batch 123220, train_perplexity=4855.4536, train_loss=8.487858

Batch 123230, train_perplexity=5529.176, train_loss=8.617794

Batch 123240, train_perplexity=5307.6543, train_loss=8.576905

Batch 123250, train_perplexity=5262.9355, train_loss=8.568444

Batch 123260, train_perplexity=4447.2686, train_loss=8.400045

Batch 123270, train_perplexity=5368.5347, train_loss=8.58831

Batch 123280, train_perplexity=5975.597, train_loss=8.695439

Batch 123290, train_perplexity=4822.7295, train_loss=8.481095

Batch 123300, train_perplexity=5310.748, train_loss=8.577488

Batch 123310, train_perplexity=5600.72, train_loss=8.6306505

Batch 123320, train_perplexity=4447.854, train_loss=8.400177

Batch 123330, train_perplexity=5367.6436, train_loss=8.588144

Batch 123340, train_perplexity=4695.245, train_loss=8.454306

Batch 123350, train_perplexity=5842.8813, train_loss=8.672979

Batch 123360, train_perplexity=5322.5415, train_loss=8.579706

Batch 123370, train_perplexity=4792.7344, train_loss=8.474856

Batch 123380, train_perplexity=6010.6157, train_loss=8.7012825

Batch 123390, train_perplexity=5256.3896, train_loss=8.5672

Batch 123400, train_perplexity=5465.051, train_loss=8.606129

Batch 123410, train_perplexity=6078.649, train_loss=8.712538

Batch 123420, train_perplexity=6476.973, train_loss=8.776009

Batch 123430, train_perplexity=5308.803, train_loss=8.577122

Batch 123440, train_perplexity=4504.3545, train_loss=8.4128

Batch 123450, train_perplexity=6512.278, train_loss=8.781445

Batch 123460, train_perplexity=4761.559, train_loss=8.46833

Batch 123470, train_perplexity=4442.966, train_loss=8.399077

Batch 123480, train_perplexity=5702.2646, train_loss=8.648619

Batch 123490, train_perplexity=5914.428, train_loss=8.68515

Batch 123500, train_perplexity=4358.706, train_loss=8.3799305

Batch 123510, train_perplexity=4572.932, train_loss=8.42791

Batch 123520, train_perplexity=5984.614, train_loss=8.696947

Batch 123530, train_perplexity=5370.4907, train_loss=8.588675

Batch 123540, train_perplexity=5320.892, train_loss=8.579396

Batch 123550, train_perplexity=5874.612, train_loss=8.678395

Batch 123560, train_perplexity=6004.1533, train_loss=8.700207

Batch 123570, train_perplexity=5618.0312, train_loss=8.633737

Batch 123580, train_perplexity=5892.8877, train_loss=8.681501

Batch 123590, train_perplexity=4666.7773, train_loss=8.448224

Batch 123600, train_perplexity=5507.0103, train_loss=8.613777

Batch 123610, train_perplexity=5124.939, train_loss=8.541874

Batch 123620, train_perplexity=7836.899, train_loss=8.9665985

Batch 123630, train_perplexity=6028.981, train_loss=8.704333

Batch 123640, train_perplexity=5189.006, train_loss=8.554297

Batch 123650, train_perplexity=5852.362, train_loss=8.674601

Batch 123660, train_perplexity=5121.9487, train_loss=8.54129

Batch 123670, train_perplexity=4836.8794, train_loss=8.484025

Batch 123680, train_perplexity=4670.032, train_loss=8.448921

Batch 123690, train_perplexity=5578.348, train_loss=8.626648

Batch 123700, train_perplexity=5746.9883, train_loss=8.656431

Batch 123710, train_perplexity=4942.6562, train_loss=8.505658

Batch 123720, train_perplexity=5557.4375, train_loss=8.622892

Batch 123730, train_perplexity=5384.984, train_loss=8.59137

Batch 123740, train_perplexity=5619.8696, train_loss=8.634064

Batch 123750, train_perplexity=5021.8804, train_loss=8.52156

Batch 123760, train_perplexity=5741.5757, train_loss=8.655489

Batch 123770, train_perplexity=5947.3926, train_loss=8.690708
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 123780, train_perplexity=5709.061, train_loss=8.64981

Batch 123790, train_perplexity=5673.2397, train_loss=8.643516

Batch 123800, train_perplexity=5809.832, train_loss=8.667307

Batch 123810, train_perplexity=5302.9844, train_loss=8.576025

Batch 123820, train_perplexity=5281.2925, train_loss=8.571926

Batch 123830, train_perplexity=6284.6226, train_loss=8.745861

Batch 123840, train_perplexity=4558.759, train_loss=8.424806

Batch 123850, train_perplexity=5076.19, train_loss=8.532316

Batch 123860, train_perplexity=4735.443, train_loss=8.462831

Batch 123870, train_perplexity=4879.3223, train_loss=8.492762

Batch 123880, train_perplexity=4443.004, train_loss=8.399086

Batch 123890, train_perplexity=5363.53, train_loss=8.587378

Batch 123900, train_perplexity=6043.217, train_loss=8.706692

Batch 123910, train_perplexity=4625.213, train_loss=8.439278

Batch 123920, train_perplexity=5886.035, train_loss=8.680338

Batch 123930, train_perplexity=4408.088, train_loss=8.391196

Batch 123940, train_perplexity=5224.7935, train_loss=8.561171

Batch 123950, train_perplexity=5639.111, train_loss=8.637482

Batch 123960, train_perplexity=5096.2617, train_loss=8.5362625

Batch 123970, train_perplexity=5572.4033, train_loss=8.625582

Batch 123980, train_perplexity=5331.5947, train_loss=8.581406

Batch 123990, train_perplexity=4550.341, train_loss=8.422957

Batch 124000, train_perplexity=4976.821, train_loss=8.512547

Batch 124010, train_perplexity=5389.356, train_loss=8.592181

Batch 124020, train_perplexity=5373.9746, train_loss=8.589323

Batch 124030, train_perplexity=4946.669, train_loss=8.50647

Batch 124040, train_perplexity=5511.2764, train_loss=8.614552

Batch 124050, train_perplexity=5781.654, train_loss=8.662445

Batch 124060, train_perplexity=4477.612, train_loss=8.406845

Batch 124070, train_perplexity=5113.247, train_loss=8.53959

Batch 124080, train_perplexity=6188.327, train_loss=8.73042

Batch 124090, train_perplexity=5426.5205, train_loss=8.599053

Batch 124100, train_perplexity=5710.8525, train_loss=8.650124

Batch 124110, train_perplexity=5756.3623, train_loss=8.658061

Batch 124120, train_perplexity=5986.8516, train_loss=8.697321

Batch 124130, train_perplexity=5418.9854, train_loss=8.597664

Batch 124140, train_perplexity=5368.898, train_loss=8.588378

Batch 124150, train_perplexity=5348.023, train_loss=8.584482

Batch 124160, train_perplexity=4968.768, train_loss=8.510927

Batch 124170, train_perplexity=5059.5107, train_loss=8.529025

Batch 124180, train_perplexity=5639.6436, train_loss=8.637576

Batch 124190, train_perplexity=5381.4673, train_loss=8.590716

Batch 124200, train_perplexity=5127.266, train_loss=8.542328

Batch 124210, train_perplexity=6020.4717, train_loss=8.702921

Batch 124220, train_perplexity=5673.31, train_loss=8.643528

Batch 124230, train_perplexity=5437.622, train_loss=8.601097

Batch 124240, train_perplexity=4448.872, train_loss=8.400406

Batch 124250, train_perplexity=5938.596, train_loss=8.689228

Batch 124260, train_perplexity=4815.8584, train_loss=8.47967

Batch 124270, train_perplexity=5111.4526, train_loss=8.539239

Batch 124280, train_perplexity=5616.917, train_loss=8.633538

Batch 124290, train_perplexity=5487.0894, train_loss=8.610153

Batch 124300, train_perplexity=4903.9443, train_loss=8.497795

Batch 124310, train_perplexity=5181.0894, train_loss=8.552771

Batch 124320, train_perplexity=4687.1836, train_loss=8.452587

Batch 124330, train_perplexity=5506.0073, train_loss=8.613595

Batch 124340, train_perplexity=5466.5, train_loss=8.606394

Batch 124350, train_perplexity=4323.9946, train_loss=8.371935

Batch 124360, train_perplexity=5328.7173, train_loss=8.580866

Batch 124370, train_perplexity=6461.0566, train_loss=8.773548

Batch 124380, train_perplexity=5144.257, train_loss=8.545636

Batch 124390, train_perplexity=5490.502, train_loss=8.610775

Batch 124400, train_perplexity=5862.808, train_loss=8.676384

Batch 124410, train_perplexity=5232.328, train_loss=8.562612

Batch 124420, train_perplexity=6720.6914, train_loss=8.812946

Batch 124430, train_perplexity=5067.7104, train_loss=8.530644

Batch 124440, train_perplexity=4670.5264, train_loss=8.449027

Batch 124450, train_perplexity=5260.989, train_loss=8.568074

Batch 124460, train_perplexity=5893.607, train_loss=8.681623

Batch 124470, train_perplexity=5891.269, train_loss=8.681227

Batch 124480, train_perplexity=5065.406, train_loss=8.5301895

Batch 124490, train_perplexity=5863.1772, train_loss=8.676447

Batch 124500, train_perplexity=5566.9326, train_loss=8.624599

Batch 124510, train_perplexity=4901.6997, train_loss=8.497337

Batch 124520, train_perplexity=5690.2427, train_loss=8.646508

Batch 124530, train_perplexity=4993.789, train_loss=8.51595

Batch 124540, train_perplexity=5581.2534, train_loss=8.627169

Batch 124550, train_perplexity=5744.4893, train_loss=8.655996

Batch 124560, train_perplexity=5679.2373, train_loss=8.644572

Batch 124570, train_perplexity=5493.0737, train_loss=8.611243

Batch 124580, train_perplexity=6241.8315, train_loss=8.739029

Batch 124590, train_perplexity=5359.1323, train_loss=8.586557

Batch 124600, train_perplexity=6119.1016, train_loss=8.719171

Batch 124610, train_perplexity=6040.313, train_loss=8.706211

Batch 124620, train_perplexity=5204.811, train_loss=8.557339

Batch 124630, train_perplexity=4752.3223, train_loss=8.466389

Batch 124640, train_perplexity=5482.7163, train_loss=8.609356

Batch 124650, train_perplexity=6675.186, train_loss=8.806152

Batch 124660, train_perplexity=5822.634, train_loss=8.669508

Batch 124670, train_perplexity=5338.4175, train_loss=8.5826845

Batch 124680, train_perplexity=5149.2095, train_loss=8.546598

Batch 124690, train_perplexity=6673.302, train_loss=8.80587

Batch 124700, train_perplexity=5191.5254, train_loss=8.554783

Batch 124710, train_perplexity=4964.33, train_loss=8.510034

Batch 124720, train_perplexity=4746.615, train_loss=8.465187

Batch 124730, train_perplexity=7048.4854, train_loss=8.860568

Batch 124740, train_perplexity=5037.83, train_loss=8.524731

Batch 124750, train_perplexity=5272.8477, train_loss=8.570326

Batch 124760, train_perplexity=5454.502, train_loss=8.604197

Batch 124770, train_perplexity=5585.641, train_loss=8.6279545

Batch 124780, train_perplexity=6385.6, train_loss=8.761801

Batch 124790, train_perplexity=5402.2363, train_loss=8.594568

Batch 124800, train_perplexity=5320.4355, train_loss=8.57931

Batch 124810, train_perplexity=5903.6426, train_loss=8.683325

Batch 124820, train_perplexity=6780.525, train_loss=8.82181

Batch 124830, train_perplexity=5857.3984, train_loss=8.675461

Batch 124840, train_perplexity=5451.371, train_loss=8.603622

Batch 124850, train_perplexity=4983.6694, train_loss=8.513922

Batch 124860, train_perplexity=5345.3813, train_loss=8.583988

Batch 124870, train_perplexity=6031.299, train_loss=8.704718

Batch 124880, train_perplexity=5923.46, train_loss=8.686676

Batch 124890, train_perplexity=6549.2734, train_loss=8.787109

Batch 124900, train_perplexity=4540.9033, train_loss=8.420881

Batch 124910, train_perplexity=5218.2603, train_loss=8.559919

Batch 124920, train_perplexity=5267.063, train_loss=8.569228

Batch 124930, train_perplexity=5771.143, train_loss=8.660625

Batch 124940, train_perplexity=5119.224, train_loss=8.540758

Batch 124950, train_perplexity=5464.488, train_loss=8.606026

Batch 124960, train_perplexity=6431.2534, train_loss=8.768925

Batch 124970, train_perplexity=5103.635, train_loss=8.537708

Batch 124980, train_perplexity=5666.5293, train_loss=8.642332

Batch 124990, train_perplexity=6425.466, train_loss=8.768024

Batch 125000, train_perplexity=5688.675, train_loss=8.646233

Batch 125010, train_perplexity=5559.8228, train_loss=8.623322

Batch 125020, train_perplexity=5079.8315, train_loss=8.533033

Batch 125030, train_perplexity=5074.4087, train_loss=8.531965

Batch 125040, train_perplexity=6504.5747, train_loss=8.780261

Batch 125050, train_perplexity=5628.247, train_loss=8.635553

Batch 125060, train_perplexity=5752.3013, train_loss=8.657355

Batch 125070, train_perplexity=4948.6885, train_loss=8.506878

Batch 125080, train_perplexity=5056.1777, train_loss=8.528366

Batch 125090, train_perplexity=5570.2305, train_loss=8.625192
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 125100, train_perplexity=5177.5923, train_loss=8.552095

Batch 125110, train_perplexity=5414.1606, train_loss=8.596773

Batch 125120, train_perplexity=4993.908, train_loss=8.515974

Batch 125130, train_perplexity=5668.41, train_loss=8.642664

Batch 125140, train_perplexity=4958.2876, train_loss=8.508816

Batch 125150, train_perplexity=5295.717, train_loss=8.574654

Batch 125160, train_perplexity=5982.5254, train_loss=8.696598

Batch 125170, train_perplexity=6264.278, train_loss=8.742619

Batch 125180, train_perplexity=6367.5693, train_loss=8.758973

Batch 125190, train_perplexity=5675.7344, train_loss=8.643955

Batch 125200, train_perplexity=5276.0923, train_loss=8.570941

Batch 125210, train_perplexity=5451.9224, train_loss=8.603724

Batch 125220, train_perplexity=5618.6475, train_loss=8.633846

Batch 125230, train_perplexity=5951.3643, train_loss=8.691376

Batch 125240, train_perplexity=5032.327, train_loss=8.523638

Batch 125250, train_perplexity=4817.8887, train_loss=8.480091

Batch 125260, train_perplexity=5572.2334, train_loss=8.625551

Batch 125270, train_perplexity=6925.266, train_loss=8.842932

Batch 125280, train_perplexity=6176.3057, train_loss=8.728476

Batch 125290, train_perplexity=5309.249, train_loss=8.577206

Batch 125300, train_perplexity=5172.9824, train_loss=8.551205

Batch 125310, train_perplexity=4909.129, train_loss=8.498852

Batch 125320, train_perplexity=4765.6567, train_loss=8.469191

Batch 125330, train_perplexity=5713.6416, train_loss=8.650612

Batch 125340, train_perplexity=6346.7505, train_loss=8.755698

Batch 125350, train_perplexity=4796.891, train_loss=8.475723

Batch 125360, train_perplexity=5121.5923, train_loss=8.541221

Batch 125370, train_perplexity=6058.457, train_loss=8.70921

Batch 125380, train_perplexity=4972.2144, train_loss=8.5116205

Batch 125390, train_perplexity=5434.8174, train_loss=8.600581

Batch 125400, train_perplexity=5161.909, train_loss=8.549062

Batch 125410, train_perplexity=5628.4727, train_loss=8.635593

Batch 125420, train_perplexity=4636.8105, train_loss=8.441782

Batch 125430, train_perplexity=5936.7104, train_loss=8.6889105

Batch 125440, train_perplexity=4941.294, train_loss=8.505383

Batch 125450, train_perplexity=5941.638, train_loss=8.68974

Batch 125460, train_perplexity=4032.4744, train_loss=8.302135

Batch 125470, train_perplexity=5751.051, train_loss=8.657138

Batch 125480, train_perplexity=6395.2173, train_loss=8.763306

Batch 125490, train_perplexity=6160.07, train_loss=8.725843

Batch 125500, train_perplexity=6023.8716, train_loss=8.7034855

Batch 125510, train_perplexity=4879.099, train_loss=8.492716

Batch 125520, train_perplexity=4994.975, train_loss=8.516188

Batch 125530, train_perplexity=4892.831, train_loss=8.495526

Batch 125540, train_perplexity=5727.537, train_loss=8.653041

Batch 125550, train_perplexity=5086.4243, train_loss=8.53433

Batch 125560, train_perplexity=5450.477, train_loss=8.603458

Batch 125570, train_perplexity=5590.832, train_loss=8.628883

Batch 125580, train_perplexity=4608.288, train_loss=8.435612

Batch 125590, train_perplexity=5925.2456, train_loss=8.686977

Batch 125600, train_perplexity=4947.688, train_loss=8.506676

Batch 125610, train_perplexity=4923.3677, train_loss=8.501748

Batch 125620, train_perplexity=6112.004, train_loss=8.71801

Batch 125630, train_perplexity=4509.693, train_loss=8.413984

Batch 125640, train_perplexity=5512.2856, train_loss=8.614735

Batch 125650, train_perplexity=4518.9404, train_loss=8.416033

Batch 125660, train_perplexity=4924.3447, train_loss=8.501946

Batch 125670, train_perplexity=6046.4277, train_loss=8.707223

Batch 125680, train_perplexity=4785.349, train_loss=8.473314

Batch 125690, train_perplexity=4933.939, train_loss=8.503893

Batch 125700, train_perplexity=5893.573, train_loss=8.681618

Batch 125710, train_perplexity=5656.794, train_loss=8.640613

Batch 125720, train_perplexity=6076.1626, train_loss=8.712129

Batch 125730, train_perplexity=5752.51, train_loss=8.657392

Batch 125740, train_perplexity=5696.884, train_loss=8.647675

Batch 125750, train_perplexity=4996.614, train_loss=8.516516

Batch 125760, train_perplexity=6653.025, train_loss=8.802827

Batch 125770, train_perplexity=5493.058, train_loss=8.61124

Batch 125780, train_perplexity=5840.2354, train_loss=8.672526

Batch 125790, train_perplexity=6016.58, train_loss=8.702274

Batch 125800, train_perplexity=5239.2334, train_loss=8.5639305

Batch 125810, train_perplexity=5924.929, train_loss=8.686924

Batch 125820, train_perplexity=4975.079, train_loss=8.512197

Batch 125830, train_perplexity=5580.008, train_loss=8.6269455

Batch 125840, train_perplexity=6323.5806, train_loss=8.752041

Batch 125850, train_perplexity=4529.606, train_loss=8.41839

Batch 125860, train_perplexity=4794.787, train_loss=8.475285

Batch 125870, train_perplexity=5337.221, train_loss=8.58246

Batch 125880, train_perplexity=6447.7856, train_loss=8.771492

Batch 125890, train_perplexity=5369.651, train_loss=8.588518

Batch 125900, train_perplexity=6163.3315, train_loss=8.726373

Batch 125910, train_perplexity=5827.1333, train_loss=8.67028

Batch 125920, train_perplexity=6625.716, train_loss=8.798714

Batch 125930, train_perplexity=4989.5234, train_loss=8.515096

Batch 125940, train_perplexity=5118.6675, train_loss=8.540649

Batch 125950, train_perplexity=6398.499, train_loss=8.763819

Batch 125960, train_perplexity=5793.2666, train_loss=8.664452

Batch 125970, train_perplexity=5319.9434, train_loss=8.579218

Batch 125980, train_perplexity=5477.6675, train_loss=8.608435

Batch 125990, train_perplexity=5318.0054, train_loss=8.578854

Batch 126000, train_perplexity=5907.6636, train_loss=8.684006

Batch 126010, train_perplexity=5486.0796, train_loss=8.609969

Batch 126020, train_perplexity=6078.9854, train_loss=8.712593

Batch 126030, train_perplexity=5473.5947, train_loss=8.607691

Batch 126040, train_perplexity=5158.7544, train_loss=8.54845

Batch 126050, train_perplexity=5828.356, train_loss=8.67049

Batch 126060, train_perplexity=4906.8867, train_loss=8.498395

Batch 126070, train_perplexity=5831.0137, train_loss=8.670946

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Batch 126080, train_perplexity=4325.644, train_loss=8.372316

Batch 126090, train_perplexity=5553.898, train_loss=8.622255

Batch 126100, train_perplexity=5770.3506, train_loss=8.660488

Batch 126110, train_perplexity=5399.5786, train_loss=8.594076

Batch 126120, train_perplexity=5591.1626, train_loss=8.6289425

Batch 126130, train_perplexity=4456.707, train_loss=8.402165

Batch 126140, train_perplexity=5170.566, train_loss=8.550737

Batch 126150, train_perplexity=5393.7886, train_loss=8.593003

Batch 126160, train_perplexity=4589.832, train_loss=8.431599

Batch 126170, train_perplexity=4947.7163, train_loss=8.506681

Batch 126180, train_perplexity=5787.1704, train_loss=8.663399

Batch 126190, train_perplexity=5207.353, train_loss=8.557827

Batch 126200, train_perplexity=4800.0713, train_loss=8.476386

Batch 126210, train_perplexity=5471.9976, train_loss=8.607399

Batch 126220, train_perplexity=4504.4575, train_loss=8.412823

Batch 126230, train_perplexity=5601.228, train_loss=8.630741

Batch 126240, train_perplexity=6096.0884, train_loss=8.715403

Batch 126250, train_perplexity=5831.926, train_loss=8.671103

Batch 126260, train_perplexity=4787.741, train_loss=8.473814

Batch 126270, train_perplexity=5184.9746, train_loss=8.55352

Batch 126280, train_perplexity=5638.799, train_loss=8.637426

Batch 126290, train_perplexity=5385.5386, train_loss=8.591473

Batch 126300, train_perplexity=4255.8896, train_loss=8.356059

Batch 126310, train_perplexity=5412.9424, train_loss=8.596548

Batch 126320, train_perplexity=5283.7407, train_loss=8.57239

Batch 126330, train_perplexity=5188.4766, train_loss=8.554195

Batch 126340, train_perplexity=4717.301, train_loss=8.458992
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 126350, train_perplexity=5006.344, train_loss=8.518461

Batch 126360, train_perplexity=6340.144, train_loss=8.754657

Batch 126370, train_perplexity=5501.0054, train_loss=8.612686

Batch 126380, train_perplexity=4538.28, train_loss=8.420303

Batch 126390, train_perplexity=5384.481, train_loss=8.591276

Batch 126400, train_perplexity=5552.9395, train_loss=8.622083

Batch 126410, train_perplexity=5534.863, train_loss=8.618822

Batch 126420, train_perplexity=4884.411, train_loss=8.493804

Batch 126430, train_perplexity=4955.8203, train_loss=8.508318

Batch 126440, train_perplexity=5371.8994, train_loss=8.588937

Batch 126450, train_perplexity=4830.9736, train_loss=8.482803

Batch 126460, train_perplexity=5728.952, train_loss=8.653288

Batch 126470, train_perplexity=5056.3174, train_loss=8.528394

Batch 126480, train_perplexity=5895.9116, train_loss=8.682014

Batch 126490, train_perplexity=5065.923, train_loss=8.530292

Batch 126500, train_perplexity=5595.6113, train_loss=8.629738

Batch 126510, train_perplexity=5551.5576, train_loss=8.621834

Batch 126520, train_perplexity=4824.022, train_loss=8.481363

Batch 126530, train_perplexity=5255.5273, train_loss=8.567036

Batch 126540, train_perplexity=4168.6523, train_loss=8.335348

Batch 126550, train_perplexity=5507.9663, train_loss=8.613951

Batch 126560, train_perplexity=5860.594, train_loss=8.676006

Batch 126570, train_perplexity=6136.7563, train_loss=8.722052

Batch 126580, train_perplexity=4862.3486, train_loss=8.489277

Batch 126590, train_perplexity=4601.165, train_loss=8.434065

Batch 126600, train_perplexity=4704.712, train_loss=8.45632

Batch 126610, train_perplexity=5396.3403, train_loss=8.593476

Batch 126620, train_perplexity=5004.645, train_loss=8.518122

Batch 126630, train_perplexity=5833.0493, train_loss=8.671295

Batch 126640, train_perplexity=5158.381, train_loss=8.548378

Batch 126650, train_perplexity=6249.3364, train_loss=8.740231

Batch 126660, train_perplexity=4357.4717, train_loss=8.379647

Batch 126670, train_perplexity=5643.5977, train_loss=8.638277

Batch 126680, train_perplexity=4905.6377, train_loss=8.49814

Batch 126690, train_perplexity=5824.5, train_loss=8.669828

Batch 126700, train_perplexity=5225.8203, train_loss=8.561367

Batch 126710, train_perplexity=4763.312, train_loss=8.4686985

Batch 126720, train_perplexity=5680.2124, train_loss=8.644744

Batch 126730, train_perplexity=5131.85, train_loss=8.543221

Batch 126740, train_perplexity=5230.966, train_loss=8.562351

Batch 126750, train_perplexity=5206.718, train_loss=8.557705

Batch 126760, train_perplexity=4679.6978, train_loss=8.450989

Batch 126770, train_perplexity=5250.808, train_loss=8.566137

Batch 126780, train_perplexity=5680.695, train_loss=8.644829

Batch 126790, train_perplexity=5338.5244, train_loss=8.582705

Batch 126800, train_perplexity=4987.14, train_loss=8.514618

Batch 126810, train_perplexity=4738.217, train_loss=8.463416

Batch 126820, train_perplexity=4304.7715, train_loss=8.367479

Batch 126830, train_perplexity=4791.249, train_loss=8.474546

Batch 126840, train_perplexity=5115.071, train_loss=8.539947

Batch 126850, train_perplexity=4531.166, train_loss=8.418735

Batch 126860, train_perplexity=5328.433, train_loss=8.580812

Batch 126870, train_perplexity=4677.636, train_loss=8.450548

Batch 126880, train_perplexity=4631.7544, train_loss=8.440691

Batch 126890, train_perplexity=5265.953, train_loss=8.569017

Batch 126900, train_perplexity=6083.6597, train_loss=8.713362

Batch 126910, train_perplexity=5965.5757, train_loss=8.693761

Batch 126920, train_perplexity=6759.1353, train_loss=8.81865

Batch 126930, train_perplexity=5562.538, train_loss=8.62381

Batch 126940, train_perplexity=5271.5654, train_loss=8.570083

Batch 126950, train_perplexity=5616.087, train_loss=8.63339

Batch 126960, train_perplexity=6198.2856, train_loss=8.732028

Batch 126970, train_perplexity=5598.985, train_loss=8.630341

Batch 126980, train_perplexity=5374.5845, train_loss=8.589437

Batch 126990, train_perplexity=6429.1313, train_loss=8.768595

Batch 127000, train_perplexity=5570.554, train_loss=8.62525

Batch 127010, train_perplexity=5016.9497, train_loss=8.520577

Batch 127020, train_perplexity=5883.145, train_loss=8.679847

Batch 127030, train_perplexity=4353.0483, train_loss=8.378632

Batch 127040, train_perplexity=4947.877, train_loss=8.506714

Batch 127050, train_perplexity=4989.005, train_loss=8.514992

Batch 127060, train_perplexity=5289.6094, train_loss=8.5735

Batch 127070, train_perplexity=5856.767, train_loss=8.675353

Batch 127080, train_perplexity=4951.37, train_loss=8.50742

Batch 127090, train_perplexity=5609.599, train_loss=8.632235

Batch 127100, train_perplexity=4868.251, train_loss=8.49049

Batch 127110, train_perplexity=5550.5146, train_loss=8.621646

Batch 127120, train_perplexity=5444.8867, train_loss=8.602432

Batch 127130, train_perplexity=6221.352, train_loss=8.735743

Batch 127140, train_perplexity=5407.411, train_loss=8.595526

Batch 127150, train_perplexity=5493.6655, train_loss=8.611351

Batch 127160, train_perplexity=4917.019, train_loss=8.500458

Batch 127170, train_perplexity=5330.954, train_loss=8.581285

Batch 127180, train_perplexity=5477.7305, train_loss=8.608446

Batch 127190, train_perplexity=4940.5825, train_loss=8.505239

Batch 127200, train_perplexity=4869.7183, train_loss=8.490791

Batch 127210, train_perplexity=6042.825, train_loss=8.706627

Batch 127220, train_perplexity=5377.9272, train_loss=8.590058

Batch 127230, train_perplexity=6037.3643, train_loss=8.705723

Batch 127240, train_perplexity=5529.6294, train_loss=8.617876

Batch 127250, train_perplexity=6337.1763, train_loss=8.754189

Batch 127260, train_perplexity=5757.5757, train_loss=8.658272

Batch 127270, train_perplexity=5656.4326, train_loss=8.640549

Batch 127280, train_perplexity=5370.3525, train_loss=8.588649

Batch 127290, train_perplexity=5635.2456, train_loss=8.636796

Batch 127300, train_perplexity=4925.777, train_loss=8.502237

Batch 127310, train_perplexity=5503.7866, train_loss=8.613192

Batch 127320, train_perplexity=5616.119, train_loss=8.633396

Batch 127330, train_perplexity=4668.8296, train_loss=8.448664

Batch 127340, train_perplexity=5251.71, train_loss=8.566309

Batch 127350, train_perplexity=4780.7876, train_loss=8.472361

Batch 127360, train_perplexity=5941.3154, train_loss=8.689686

Batch 127370, train_perplexity=5242.1826, train_loss=8.564493

Batch 127380, train_perplexity=5454.07, train_loss=8.604117

Batch 127390, train_perplexity=5146.4307, train_loss=8.546059

Batch 127400, train_perplexity=5624.8667, train_loss=8.634953

Batch 127410, train_perplexity=5067.1113, train_loss=8.530526

Batch 127420, train_perplexity=5609.1284, train_loss=8.632151

Batch 127430, train_perplexity=4889.3184, train_loss=8.494808

Batch 127440, train_perplexity=5483.4746, train_loss=8.609494

Batch 127450, train_perplexity=5437.2227, train_loss=8.601024

Batch 127460, train_perplexity=4943.764, train_loss=8.505882

Batch 127470, train_perplexity=5015.027, train_loss=8.520194

Batch 127480, train_perplexity=4735.0815, train_loss=8.462754

Batch 127490, train_perplexity=4339.345, train_loss=8.375479

Batch 127500, train_perplexity=4886.1626, train_loss=8.494163

Batch 127510, train_perplexity=5247.6245, train_loss=8.565531

Batch 127520, train_perplexity=5546.8, train_loss=8.620976

Batch 127530, train_perplexity=5262.017, train_loss=8.56827

Batch 127540, train_perplexity=5029.9185, train_loss=8.523159

Batch 127550, train_perplexity=4557.963, train_loss=8.424631

Batch 127560, train_perplexity=5648.1206, train_loss=8.639078

Batch 127570, train_perplexity=4859.3447, train_loss=8.488659

Batch 127580, train_perplexity=5604.6157, train_loss=8.631346

Batch 127590, train_perplexity=5206.301, train_loss=8.557625

Batch 127600, train_perplexity=5699.177, train_loss=8.648077

Batch 127610, train_perplexity=4177.284, train_loss=8.337417

Batch 127620, train_perplexity=5250.137, train_loss=8.5660095

Batch 127630, train_perplexity=4970.6636, train_loss=8.511309

Batch 127640, train_perplexity=4904.5474, train_loss=8.497918

Batch 127650, train_perplexity=5057.75, train_loss=8.528677

Batch 127660, train_perplexity=5998.47, train_loss=8.69926
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 127670, train_perplexity=5415.8647, train_loss=8.597088

Batch 127680, train_perplexity=5748.3145, train_loss=8.656662

Batch 127690, train_perplexity=5824.433, train_loss=8.669817

Batch 127700, train_perplexity=5006.1724, train_loss=8.518427

Batch 127710, train_perplexity=4893.3677, train_loss=8.495636

Batch 127720, train_perplexity=6686.9224, train_loss=8.807909

Batch 127730, train_perplexity=5157.8394, train_loss=8.548273

Batch 127740, train_perplexity=6198.995, train_loss=8.732142

Batch 127750, train_perplexity=5027.5107, train_loss=8.52268

Batch 127760, train_perplexity=5055.623, train_loss=8.528256

Batch 127770, train_perplexity=6270.637, train_loss=8.743633

Batch 127780, train_perplexity=5363.274, train_loss=8.58733

Batch 127790, train_perplexity=5347.5586, train_loss=8.584395

Batch 127800, train_perplexity=4804.665, train_loss=8.477343

Batch 127810, train_perplexity=5326.9287, train_loss=8.58053

Batch 127820, train_perplexity=4929.2075, train_loss=8.5029335

Batch 127830, train_perplexity=5861.0693, train_loss=8.676087

Batch 127840, train_perplexity=5300.658, train_loss=8.575586

Batch 127850, train_perplexity=5321.567, train_loss=8.579523

Batch 127860, train_perplexity=5676.2593, train_loss=8.644048

Batch 127870, train_perplexity=6108.8687, train_loss=8.717497

Batch 127880, train_perplexity=4504.5005, train_loss=8.412832

Batch 127890, train_perplexity=5762.833, train_loss=8.659184

Batch 127900, train_perplexity=4896.3315, train_loss=8.496242

Batch 127910, train_perplexity=6233.5864, train_loss=8.737707

Batch 127920, train_perplexity=5127.77, train_loss=8.542426

Batch 127930, train_perplexity=5820.496, train_loss=8.669141

Batch 127940, train_perplexity=5624.1694, train_loss=8.634829

Batch 127950, train_perplexity=5026.7104, train_loss=8.522521

Batch 127960, train_perplexity=5813.2847, train_loss=8.667901

Batch 127970, train_perplexity=6261.7095, train_loss=8.7422085

Batch 127980, train_perplexity=6031.3677, train_loss=8.704729

Batch 127990, train_perplexity=5467.355, train_loss=8.60655

Batch 128000, train_perplexity=6375.4565, train_loss=8.760211

Batch 128010, train_perplexity=5933.275, train_loss=8.688332

Batch 128020, train_perplexity=5286.2812, train_loss=8.57287

Batch 128030, train_perplexity=4930.3735, train_loss=8.50317

Batch 128040, train_perplexity=6194.202, train_loss=8.731369

Batch 128050, train_perplexity=5178.787, train_loss=8.552326

Batch 128060, train_perplexity=5146.5093, train_loss=8.546074

Batch 128070, train_perplexity=5221.227, train_loss=8.560488

Batch 128080, train_perplexity=4730.988, train_loss=8.461889

Batch 128090, train_perplexity=5785.2393, train_loss=8.663065

Batch 128100, train_perplexity=5168.2734, train_loss=8.550294

Batch 128110, train_perplexity=5455.7607, train_loss=8.604427

Batch 128120, train_perplexity=5311.786, train_loss=8.577683

Batch 128130, train_perplexity=4357.0396, train_loss=8.379548

Batch 128140, train_perplexity=4582.392, train_loss=8.429976

Batch 128150, train_perplexity=4120.762, train_loss=8.323793

Batch 128160, train_perplexity=5509.8784, train_loss=8.614298

Batch 128170, train_perplexity=5563.3022, train_loss=8.623947

Batch 128180, train_perplexity=5771.9575, train_loss=8.660767

Batch 128190, train_perplexity=6427.709, train_loss=8.7683735

Batch 128200, train_perplexity=5264.482, train_loss=8.568738

Batch 128210, train_perplexity=5446.2476, train_loss=8.602682

Batch 128220, train_perplexity=5693.223, train_loss=8.647032

Batch 128230, train_perplexity=5178.955, train_loss=8.552359

Batch 128240, train_perplexity=5399.537, train_loss=8.594069

Batch 128250, train_perplexity=5625.4087, train_loss=8.635049

Batch 128260, train_perplexity=5578.093, train_loss=8.626602

Batch 128270, train_perplexity=4841.4897, train_loss=8.484978

Batch 128280, train_perplexity=5376.6914, train_loss=8.5898285

Batch 128290, train_perplexity=6465.772, train_loss=8.774278

Batch 128300, train_perplexity=4592.9976, train_loss=8.432288

Batch 128310, train_perplexity=4834.8037, train_loss=8.483596

Batch 128320, train_perplexity=5945.64, train_loss=8.690413

Batch 128330, train_perplexity=5065.029, train_loss=8.530115

Batch 128340, train_perplexity=4989.319, train_loss=8.515055

Batch 128350, train_perplexity=5191.9116, train_loss=8.554857

Batch 128360, train_perplexity=5348.803, train_loss=8.584628

Batch 128370, train_perplexity=7360.527, train_loss=8.903887

Batch 128380, train_perplexity=5190.1445, train_loss=8.554517

Batch 128390, train_perplexity=4730.0225, train_loss=8.461685

Batch 128400, train_perplexity=5871.6543, train_loss=8.677892

Batch 128410, train_perplexity=7085.9116, train_loss=8.865864

Batch 128420, train_perplexity=6244.7373, train_loss=8.739494

Batch 128430, train_perplexity=5808.491, train_loss=8.667076

Batch 128440, train_perplexity=6020.897, train_loss=8.7029915

Batch 128450, train_perplexity=6031.287, train_loss=8.704716

Batch 128460, train_perplexity=4975.8525, train_loss=8.512352

Batch 128470, train_perplexity=4762.022, train_loss=8.468428

Batch 128480, train_perplexity=4710.746, train_loss=8.457602

Batch 128490, train_perplexity=6083.7993, train_loss=8.713385

Batch 128500, train_perplexity=5536.5103, train_loss=8.61912

Batch 128510, train_perplexity=5194.68, train_loss=8.55539

Batch 128520, train_perplexity=6271.8813, train_loss=8.743832

Batch 128530, train_perplexity=4810.763, train_loss=8.478611

Batch 128540, train_perplexity=6263.298, train_loss=8.742462

Batch 128550, train_perplexity=5638.6753, train_loss=8.637404

Batch 128560, train_perplexity=4885.2075, train_loss=8.493967

Batch 128570, train_perplexity=5416.846, train_loss=8.597269

Batch 128580, train_perplexity=5247.88, train_loss=8.565579

Batch 128590, train_perplexity=6459.0356, train_loss=8.773235

Batch 128600, train_perplexity=4791.3037, train_loss=8.474558

Batch 128610, train_perplexity=5939.814, train_loss=8.689433

Batch 128620, train_perplexity=5025.2, train_loss=8.522221

Batch 128630, train_perplexity=6692.728, train_loss=8.808777

Batch 128640, train_perplexity=4466.6807, train_loss=8.404401

Batch 128650, train_perplexity=5376.343, train_loss=8.589764

Batch 128660, train_perplexity=5080.321, train_loss=8.53313

Batch 128670, train_perplexity=6909.2026, train_loss=8.84061

Batch 128680, train_perplexity=6405.6426, train_loss=8.764935

Batch 128690, train_perplexity=5801.3716, train_loss=8.66585

Batch 128700, train_perplexity=5375.4663, train_loss=8.589601

Batch 128710, train_perplexity=5115.9053, train_loss=8.54011

Batch 128720, train_perplexity=4413.413, train_loss=8.392404

Batch 128730, train_perplexity=5154.732, train_loss=8.54767

Batch 128740, train_perplexity=4664.5303, train_loss=8.447742

Batch 128750, train_perplexity=5726.603, train_loss=8.652878

Batch 128760, train_perplexity=5288.878, train_loss=8.573361

Batch 128770, train_perplexity=4830.665, train_loss=8.482739

Batch 128780, train_perplexity=5724.5664, train_loss=8.652522

Batch 128790, train_perplexity=4537.7085, train_loss=8.420177

Batch 128800, train_perplexity=5261.631, train_loss=8.568196

Batch 128810, train_perplexity=5732.2256, train_loss=8.653859

Batch 128820, train_perplexity=5187.4673, train_loss=8.554001

Batch 128830, train_perplexity=5466.281, train_loss=8.606354

Batch 128840, train_perplexity=5800.2983, train_loss=8.665665

Batch 128850, train_perplexity=6298.1763, train_loss=8.748015

Batch 128860, train_perplexity=5175.331, train_loss=8.551659

Batch 128870, train_perplexity=5240.733, train_loss=8.564217

Batch 128880, train_perplexity=6205.4893, train_loss=8.73319

Batch 128890, train_perplexity=5233.79, train_loss=8.562891

Batch 128900, train_perplexity=7234.467, train_loss=8.886612

Batch 128910, train_perplexity=4963.426, train_loss=8.509851

Batch 128920, train_perplexity=5726.2754, train_loss=8.652821

Batch 128930, train_perplexity=6767.4624, train_loss=8.819881

Batch 128940, train_perplexity=4834.6934, train_loss=8.483573

Batch 128950, train_perplexity=5854.2876, train_loss=8.67493

Batch 128960, train_perplexity=5909.839, train_loss=8.684374

Batch 128970, train_perplexity=5979.097, train_loss=8.696025

Batch 128980, train_perplexity=5478.43, train_loss=8.608574
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 128990, train_perplexity=5632.441, train_loss=8.636298

Batch 129000, train_perplexity=5822.584, train_loss=8.669499

Batch 129010, train_perplexity=5575.3804, train_loss=8.626116

Batch 129020, train_perplexity=5355.4844, train_loss=8.585876

Batch 129030, train_perplexity=5084.94, train_loss=8.534039

Batch 129040, train_perplexity=6465.328, train_loss=8.774209

Batch 129050, train_perplexity=6113.391, train_loss=8.718237

Batch 129060, train_perplexity=5479.9614, train_loss=8.608853

Batch 129070, train_perplexity=4670.9497, train_loss=8.449118

Batch 129080, train_perplexity=5607.417, train_loss=8.631845

Batch 129090, train_perplexity=5882.0845, train_loss=8.6796665

Batch 129100, train_perplexity=5416.6553, train_loss=8.597234

Batch 129110, train_perplexity=4997.929, train_loss=8.516779

Batch 129120, train_perplexity=5179.8047, train_loss=8.552523

Batch 129130, train_perplexity=5047.852, train_loss=8.526718

Batch 129140, train_perplexity=5926.805, train_loss=8.687241

Batch 129150, train_perplexity=4790.4814, train_loss=8.474386

Batch 129160, train_perplexity=4365.025, train_loss=8.381379

Batch 129170, train_perplexity=5880.9517, train_loss=8.679474

Batch 129180, train_perplexity=4974.529, train_loss=8.512086

Batch 129190, train_perplexity=5330.9897, train_loss=8.581292

Batch 129200, train_perplexity=6103.7266, train_loss=8.716655

Batch 129210, train_perplexity=5615.8833, train_loss=8.633354

Batch 129220, train_perplexity=5571.7764, train_loss=8.625469

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Batch 129230, train_perplexity=5373.9795, train_loss=8.589324

Batch 129240, train_perplexity=4928.7656, train_loss=8.502844

Batch 129250, train_perplexity=6700.724, train_loss=8.809971

Batch 129260, train_perplexity=4871.251, train_loss=8.491106

Batch 129270, train_perplexity=5569.1943, train_loss=8.625006

Batch 129280, train_perplexity=4856.315, train_loss=8.488035

Batch 129290, train_perplexity=5112.457, train_loss=8.539435

Batch 129300, train_perplexity=4052.313, train_loss=8.307043

Batch 129310, train_perplexity=5218.3003, train_loss=8.559927

Batch 129320, train_perplexity=5260.547, train_loss=8.56799

Batch 129330, train_perplexity=5782.5415, train_loss=8.662599

Batch 129340, train_perplexity=6152.379, train_loss=8.724594

Batch 129350, train_perplexity=5842.2017, train_loss=8.672863

Batch 129360, train_perplexity=4521.294, train_loss=8.4165535

Batch 129370, train_perplexity=5583.042, train_loss=8.627489

Batch 129380, train_perplexity=4846.562, train_loss=8.486025

Batch 129390, train_perplexity=4948.2637, train_loss=8.506792

Batch 129400, train_perplexity=5862.9087, train_loss=8.676401

Batch 129410, train_perplexity=5804.515, train_loss=8.666391

Batch 129420, train_perplexity=5404.416, train_loss=8.594972

Batch 129430, train_perplexity=5771.1763, train_loss=8.660631

Batch 129440, train_perplexity=5212.0635, train_loss=8.558731

Batch 129450, train_perplexity=5632.994, train_loss=8.636396

Batch 129460, train_perplexity=4535.364, train_loss=8.419661

Batch 129470, train_perplexity=5682.6562, train_loss=8.645174

Batch 129480, train_perplexity=5038.829, train_loss=8.524929

Batch 129490, train_perplexity=5361.269, train_loss=8.586956

Batch 129500, train_perplexity=5026.466, train_loss=8.522472

Batch 129510, train_perplexity=5551.4517, train_loss=8.621815

Batch 129520, train_perplexity=5004.3296, train_loss=8.518059

Batch 129530, train_perplexity=7178.6606, train_loss=8.878868

Batch 129540, train_perplexity=5018.0073, train_loss=8.520788

Batch 129550, train_perplexity=6231.7915, train_loss=8.737419

Batch 129560, train_perplexity=5911.7104, train_loss=8.68469

Batch 129570, train_perplexity=5520.5454, train_loss=8.616232

Batch 129580, train_perplexity=6221.542, train_loss=8.735773

Batch 129590, train_perplexity=5925.9463, train_loss=8.687096

Batch 129600, train_perplexity=4528.768, train_loss=8.418205

Batch 129610, train_perplexity=4750.809, train_loss=8.46607

Batch 129620, train_perplexity=4992.508, train_loss=8.515694

Batch 129630, train_perplexity=5726.6904, train_loss=8.652893

Batch 129640, train_perplexity=5371.638, train_loss=8.588888

Batch 129650, train_perplexity=4814.747, train_loss=8.479439

Batch 129660, train_perplexity=4992.3936, train_loss=8.515671

Batch 129670, train_perplexity=4742.0996, train_loss=8.464235

Batch 129680, train_perplexity=4623.519, train_loss=8.438911

Batch 129690, train_perplexity=5493.4297, train_loss=8.611308

Batch 129700, train_perplexity=5173.4263, train_loss=8.5512905

Batch 129710, train_perplexity=5255.177, train_loss=8.566969

Batch 129720, train_perplexity=3936.9292, train_loss=8.278156

Batch 129730, train_perplexity=5714.726, train_loss=8.650802

Batch 129740, train_perplexity=6410.929, train_loss=8.765759

Batch 129750, train_perplexity=6090.208, train_loss=8.7144375

Batch 129760, train_perplexity=4676.6016, train_loss=8.450327

Batch 129770, train_perplexity=4698.65, train_loss=8.45503

Batch 129780, train_perplexity=5809.3774, train_loss=8.667229

Batch 129790, train_perplexity=5958.611, train_loss=8.692593

Batch 129800, train_perplexity=4668.9453, train_loss=8.4486885

Batch 129810, train_perplexity=5045.306, train_loss=8.526214

Batch 129820, train_perplexity=6578.927, train_loss=8.791627

Batch 129830, train_perplexity=5532.9526, train_loss=8.618477

Batch 129840, train_perplexity=5119.849, train_loss=8.54088

Batch 129850, train_perplexity=5369.2666, train_loss=8.588447

Batch 129860, train_perplexity=5364.225, train_loss=8.587507

Batch 129870, train_perplexity=4707.0186, train_loss=8.45681

Batch 129880, train_perplexity=5672.9365, train_loss=8.643462

Batch 129890, train_perplexity=6869.2373, train_loss=8.834808

Batch 129900, train_perplexity=4531.7363, train_loss=8.41886

Batch 129910, train_perplexity=5140.427, train_loss=8.544891

Batch 129920, train_perplexity=5838.2305, train_loss=8.672183

Batch 129930, train_perplexity=6290.8228, train_loss=8.746847

Batch 129940, train_perplexity=5627.802, train_loss=8.635474

Batch 129950, train_perplexity=5113.2715, train_loss=8.539595

Batch 129960, train_perplexity=5657.2637, train_loss=8.640696

Batch 129970, train_perplexity=4847.7637, train_loss=8.486273

Batch 129980, train_perplexity=5433.1177, train_loss=8.600268

Batch 129990, train_perplexity=4849.0957, train_loss=8.486547

Batch 130000, train_perplexity=5494.525, train_loss=8.611507

Batch 130010, train_perplexity=4168.732, train_loss=8.335367

Batch 130020, train_perplexity=5171.0537, train_loss=8.550832

Batch 130030, train_perplexity=5198.972, train_loss=8.556216

Batch 130040, train_perplexity=5375.6304, train_loss=8.589631

Batch 130050, train_perplexity=4542.125, train_loss=8.42115

Batch 130060, train_perplexity=5083.3257, train_loss=8.533721

Batch 130070, train_perplexity=5405.957, train_loss=8.595257

Batch 130080, train_perplexity=5242.012, train_loss=8.564461

Batch 130090, train_perplexity=4882.32, train_loss=8.493376

Batch 130100, train_perplexity=5571.8613, train_loss=8.625484

Batch 130110, train_perplexity=4801.825, train_loss=8.476751

Batch 130120, train_perplexity=5421.632, train_loss=8.598152

Batch 130130, train_perplexity=5797.0913, train_loss=8.665112

Batch 130140, train_perplexity=4407.209, train_loss=8.390997

Batch 130150, train_perplexity=4498.855, train_loss=8.411578

Batch 130160, train_perplexity=5601.2544, train_loss=8.630746

Batch 130170, train_perplexity=6125.939, train_loss=8.720287

Batch 130180, train_perplexity=6348.7905, train_loss=8.75602

Batch 130190, train_perplexity=5974.6455, train_loss=8.69528

Batch 130200, train_perplexity=5512.438, train_loss=8.614762

Batch 130210, train_perplexity=5751.0454, train_loss=8.657137

Batch 130220, train_perplexity=4950.1045, train_loss=8.507164

Batch 130230, train_perplexity=6615.7773, train_loss=8.797213
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 130240, train_perplexity=5753.086, train_loss=8.657492

Batch 130250, train_perplexity=6328.461, train_loss=8.752812

Batch 130260, train_perplexity=5689.955, train_loss=8.646458

Batch 130270, train_perplexity=4548.6055, train_loss=8.422576

Batch 130280, train_perplexity=5266.033, train_loss=8.569033

Batch 130290, train_perplexity=5040.175, train_loss=8.525196

Batch 130300, train_perplexity=6042.2256, train_loss=8.706528

Batch 130310, train_perplexity=5235.807, train_loss=8.563276

Batch 130320, train_perplexity=6086.9736, train_loss=8.713906

Batch 130330, train_perplexity=4895.272, train_loss=8.496025

Batch 130340, train_perplexity=6707.501, train_loss=8.810982

Batch 130350, train_perplexity=5039.728, train_loss=8.525107

Batch 130360, train_perplexity=5905.839, train_loss=8.683697

Batch 130370, train_perplexity=5400.8247, train_loss=8.594307

Batch 130380, train_perplexity=5224.6343, train_loss=8.56114

Batch 130390, train_perplexity=5300.542, train_loss=8.575564

Batch 130400, train_perplexity=5560.629, train_loss=8.6234665

Batch 130410, train_perplexity=4731.236, train_loss=8.461942

Batch 130420, train_perplexity=5224.6045, train_loss=8.561134

Batch 130430, train_perplexity=4984.4253, train_loss=8.514073

Batch 130440, train_perplexity=6771.413, train_loss=8.820465

Batch 130450, train_perplexity=5075.5654, train_loss=8.532193

Batch 130460, train_perplexity=6922.202, train_loss=8.842489

Batch 130470, train_perplexity=5416.1436, train_loss=8.597139

Batch 130480, train_perplexity=5287.1733, train_loss=8.573039

Batch 130490, train_perplexity=5478.4824, train_loss=8.608583

Batch 130500, train_perplexity=5908.227, train_loss=8.684101

Batch 130510, train_perplexity=5890.46, train_loss=8.681089

Batch 130520, train_perplexity=4929.1465, train_loss=8.502921

Batch 130530, train_perplexity=5964.0117, train_loss=8.693499

Batch 130540, train_perplexity=5840.107, train_loss=8.672504

Batch 130550, train_perplexity=5055.763, train_loss=8.528284

Batch 130560, train_perplexity=5921.6978, train_loss=8.6863785

Batch 130570, train_perplexity=4731.4526, train_loss=8.4619875

Batch 130580, train_perplexity=6498.616, train_loss=8.779345

Batch 130590, train_perplexity=4773.6807, train_loss=8.470873

Batch 130600, train_perplexity=4464.9556, train_loss=8.404015

Batch 130610, train_perplexity=5045.513, train_loss=8.526255

Batch 130620, train_perplexity=6212.743, train_loss=8.734358

Batch 130630, train_perplexity=5949.48, train_loss=8.691059

Batch 130640, train_perplexity=5018.955, train_loss=8.520977

Batch 130650, train_perplexity=4788.3486, train_loss=8.473941

Batch 130660, train_perplexity=5622.3784, train_loss=8.63451

Batch 130670, train_perplexity=5798.938, train_loss=8.66543

Batch 130680, train_perplexity=4912.285, train_loss=8.499495

Batch 130690, train_perplexity=4752.3086, train_loss=8.466386

Batch 130700, train_perplexity=5177.8887, train_loss=8.552153

Batch 130710, train_perplexity=5603.6963, train_loss=8.631182

Batch 130720, train_perplexity=4576.6143, train_loss=8.428715

Batch 130730, train_perplexity=5406.906, train_loss=8.595432

Batch 130740, train_perplexity=5362.645, train_loss=8.587213

Batch 130750, train_perplexity=4961.6606, train_loss=8.509496

Batch 130760, train_perplexity=5711.441, train_loss=8.650227

Batch 130770, train_perplexity=5046.851, train_loss=8.52652

Batch 130780, train_perplexity=4602.776, train_loss=8.434415

Batch 130790, train_perplexity=5395.45, train_loss=8.593311

Batch 130800, train_perplexity=5645.945, train_loss=8.638693

Batch 130810, train_perplexity=4870.6655, train_loss=8.490986

Batch 130820, train_perplexity=4253.4956, train_loss=8.355496

Batch 130830, train_perplexity=5088.967, train_loss=8.53483

Batch 130840, train_perplexity=5287.814, train_loss=8.57316

Batch 130850, train_perplexity=5422.237, train_loss=8.598264

Batch 130860, train_perplexity=4844.658, train_loss=8.485632

Batch 130870, train_perplexity=5053.3335, train_loss=8.527803

Batch 130880, train_perplexity=5806.021, train_loss=8.666651

Batch 130890, train_perplexity=6978.65, train_loss=8.850611

Batch 130900, train_perplexity=5646.0205, train_loss=8.638706

Batch 130910, train_perplexity=6369.75, train_loss=8.7593155

Batch 130920, train_perplexity=5868.6816, train_loss=8.677385

Batch 130930, train_perplexity=5294.9746, train_loss=8.574513

Batch 130940, train_perplexity=5848.4565, train_loss=8.673933

Batch 130950, train_perplexity=6799.1475, train_loss=8.824553

Batch 130960, train_perplexity=5541.6235, train_loss=8.620043

Batch 130970, train_perplexity=5274.3564, train_loss=8.570612

Batch 130980, train_perplexity=5522.5884, train_loss=8.616602

Batch 130990, train_perplexity=5574.3594, train_loss=8.625933

Batch 131000, train_perplexity=5253.8887, train_loss=8.566724

Batch 131010, train_perplexity=5776.721, train_loss=8.661592

Batch 131020, train_perplexity=5137.5693, train_loss=8.544335

Batch 131030, train_perplexity=5543.151, train_loss=8.620318

Batch 131040, train_perplexity=5406.7925, train_loss=8.595411

Batch 131050, train_perplexity=4294.0166, train_loss=8.364978

Batch 131060, train_perplexity=5501.4565, train_loss=8.612768

Batch 131070, train_perplexity=4851.0566, train_loss=8.486952

Batch 131080, train_perplexity=7280.951, train_loss=8.893017

Batch 131090, train_perplexity=5097.224, train_loss=8.536451

Batch 131100, train_perplexity=5592.4634, train_loss=8.629175

Batch 131110, train_perplexity=5837.2954, train_loss=8.672023

Batch 131120, train_perplexity=5640.4126, train_loss=8.6377125

Batch 131130, train_perplexity=5157.564, train_loss=8.54822

Batch 131140, train_perplexity=6182.175, train_loss=8.729425

Batch 131150, train_perplexity=4349.8735, train_loss=8.377902

Batch 131160, train_perplexity=5598.472, train_loss=8.630249

Batch 131170, train_perplexity=5378.948, train_loss=8.590248

Batch 131180, train_perplexity=5227.724, train_loss=8.561731

Batch 131190, train_perplexity=5048.059, train_loss=8.526759

Batch 131200, train_perplexity=4575.759, train_loss=8.428528

Batch 131210, train_perplexity=6063.7803, train_loss=8.710089

Batch 131220, train_perplexity=5744.676, train_loss=8.656029

Batch 131230, train_perplexity=4598.529, train_loss=8.433492

Batch 131240, train_perplexity=4735.2397, train_loss=8.462788

Batch 131250, train_perplexity=4633.9194, train_loss=8.441158

Batch 131260, train_perplexity=5827.211, train_loss=8.670294

Batch 131270, train_perplexity=5992.0493, train_loss=8.698189

Batch 131280, train_perplexity=5385.1895, train_loss=8.591408

Batch 131290, train_perplexity=6606.7236, train_loss=8.795843

Batch 131300, train_perplexity=5606.599, train_loss=8.6317

Batch 131310, train_perplexity=5955.2075, train_loss=8.692021

Batch 131320, train_perplexity=4780.163, train_loss=8.47223

Batch 131330, train_perplexity=5325.9434, train_loss=8.580345

Batch 131340, train_perplexity=5019.63, train_loss=8.5211115

Batch 131350, train_perplexity=5382.997, train_loss=8.591001

Batch 131360, train_perplexity=4919.9272, train_loss=8.501049

Batch 131370, train_perplexity=6107.8315, train_loss=8.717327

Batch 131380, train_perplexity=4991.7607, train_loss=8.515544

Batch 131390, train_perplexity=5306.111, train_loss=8.576614

Batch 131400, train_perplexity=5336.9463, train_loss=8.582409

Batch 131410, train_perplexity=6107.593, train_loss=8.717288

Batch 131420, train_perplexity=5327.6455, train_loss=8.580665

Batch 131430, train_perplexity=4961.5894, train_loss=8.509481

Batch 131440, train_perplexity=5307.3457, train_loss=8.576847

Batch 131450, train_perplexity=4663.8364, train_loss=8.447594

Batch 131460, train_perplexity=5750.2666, train_loss=8.6570015

Batch 131470, train_perplexity=5396.2427, train_loss=8.593458

Batch 131480, train_perplexity=5130.6753, train_loss=8.542993

Batch 131490, train_perplexity=4650.951, train_loss=8.444827

Batch 131500, train_perplexity=5291.446, train_loss=8.573847

Batch 131510, train_perplexity=5436.108, train_loss=8.600819

Batch 131520, train_perplexity=4461.41, train_loss=8.40322

Batch 131530, train_perplexity=5585.87, train_loss=8.6279955

Batch 131540, train_perplexity=5575.758, train_loss=8.6261835

Batch 131550, train_perplexity=4614.9204, train_loss=8.43705
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 131560, train_perplexity=7011.745, train_loss=8.855342

Batch 131570, train_perplexity=5024.3477, train_loss=8.522051

Batch 131580, train_perplexity=5216.3247, train_loss=8.559548

Batch 131590, train_perplexity=4536.6875, train_loss=8.419952

Batch 131600, train_perplexity=5313.4126, train_loss=8.57799

Batch 131610, train_perplexity=6760.979, train_loss=8.818923

Batch 131620, train_perplexity=6772.86, train_loss=8.820679

Batch 131630, train_perplexity=6212.2754, train_loss=8.7342825

Batch 131640, train_perplexity=5217.524, train_loss=8.559778

Batch 131650, train_perplexity=5522.146, train_loss=8.616522

Batch 131660, train_perplexity=6204.3945, train_loss=8.733013

Batch 131670, train_perplexity=4920.946, train_loss=8.501256

Batch 131680, train_perplexity=4909.3257, train_loss=8.498892

Batch 131690, train_perplexity=6646.6895, train_loss=8.801874

Batch 131700, train_perplexity=5364.9517, train_loss=8.587643

Batch 131710, train_perplexity=5777.575, train_loss=8.661739

Batch 131720, train_perplexity=5951.523, train_loss=8.691402

Batch 131730, train_perplexity=5526.0127, train_loss=8.617222

Batch 131740, train_perplexity=5510.2886, train_loss=8.614372

Batch 131750, train_perplexity=5669.654, train_loss=8.642883

Batch 131760, train_perplexity=5388.529, train_loss=8.592028

Batch 131770, train_perplexity=4450.9644, train_loss=8.400876

Batch 131780, train_perplexity=4724.7925, train_loss=8.460579

Batch 131790, train_perplexity=5168.0317, train_loss=8.550247

Batch 131800, train_perplexity=5484.6514, train_loss=8.609709

Batch 131810, train_perplexity=4853.912, train_loss=8.48754

Batch 131820, train_perplexity=5368.4014, train_loss=8.588285

Batch 131830, train_perplexity=5137.932, train_loss=8.544406

Batch 131840, train_perplexity=6273.1074, train_loss=8.744027

Batch 131850, train_perplexity=5587.3726, train_loss=8.628264

Batch 131860, train_perplexity=5165.0063, train_loss=8.549662

Batch 131870, train_perplexity=5068.6436, train_loss=8.530828

Batch 131880, train_perplexity=4596.6436, train_loss=8.433082

Batch 131890, train_perplexity=6132.755, train_loss=8.721399

Batch 131900, train_perplexity=4741.991, train_loss=8.464212

Batch 131910, train_perplexity=5697.525, train_loss=8.647787

Batch 131920, train_perplexity=5059.149, train_loss=8.528954

Batch 131930, train_perplexity=5505.0254, train_loss=8.613417

Batch 131940, train_perplexity=5506.5903, train_loss=8.613701

Batch 131950, train_perplexity=7282.771, train_loss=8.893267

Batch 131960, train_perplexity=5140.1865, train_loss=8.544845

Batch 131970, train_perplexity=4470.7847, train_loss=8.405319

Batch 131980, train_perplexity=5369.2974, train_loss=8.588452

Batch 131990, train_perplexity=6345.2554, train_loss=8.755463

Batch 132000, train_perplexity=5776.71, train_loss=8.66159

Batch 132010, train_perplexity=4829.302, train_loss=8.482457

Batch 132020, train_perplexity=5277.224, train_loss=8.571156

Batch 132030, train_perplexity=5007.3706, train_loss=8.518666

Batch 132040, train_perplexity=4939.2114, train_loss=8.504961

Batch 132050, train_perplexity=5301.558, train_loss=8.575756

Batch 132060, train_perplexity=5241.0425, train_loss=8.564276

Batch 132070, train_perplexity=4954.374, train_loss=8.508026

Batch 132080, train_perplexity=6424.265, train_loss=8.767838

Batch 132090, train_perplexity=5587.6494, train_loss=8.628314

Batch 132100, train_perplexity=5346.59, train_loss=8.584214

Batch 132110, train_perplexity=5567.3677, train_loss=8.624678

Batch 132120, train_perplexity=5025.3535, train_loss=8.522251

Batch 132130, train_perplexity=5388.8164, train_loss=8.592081

Batch 132140, train_perplexity=5238.0444, train_loss=8.563704

Batch 132150, train_perplexity=5249.612, train_loss=8.565909

Batch 132160, train_perplexity=5512.1387, train_loss=8.614708

Batch 132170, train_perplexity=5925.618, train_loss=8.68704

Batch 132180, train_perplexity=4586.8213, train_loss=8.430943

Batch 132190, train_perplexity=4774.0312, train_loss=8.470946

Batch 132200, train_perplexity=6019.662, train_loss=8.702786

Batch 132210, train_perplexity=4283.1865, train_loss=8.3624525

Batch 132220, train_perplexity=6044.8423, train_loss=8.706961

Batch 132230, train_perplexity=4593.738, train_loss=8.432449

Batch 132240, train_perplexity=5282.451, train_loss=8.572145

Batch 132250, train_perplexity=5110.4536, train_loss=8.539043

Batch 132260, train_perplexity=5210.1597, train_loss=8.558366

Batch 132270, train_perplexity=5852.8755, train_loss=8.674688

Batch 132280, train_perplexity=4358.3237, train_loss=8.379843

Batch 132290, train_perplexity=5871.7383, train_loss=8.677906

Batch 132300, train_perplexity=5095.795, train_loss=8.536171

Batch 132310, train_perplexity=5767.424, train_loss=8.659981

Batch 132320, train_perplexity=5009.334, train_loss=8.519058

Batch 132330, train_perplexity=5947.9824, train_loss=8.690807

Batch 132340, train_perplexity=5004.7305, train_loss=8.518139

Batch 132350, train_perplexity=5279.399, train_loss=8.571568

Batch 132360, train_perplexity=4948.2827, train_loss=8.506796

Batch 132370, train_perplexity=5808.79, train_loss=8.667128

Batch 132380, train_perplexity=4785.9473, train_loss=8.473439

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Batch 132390, train_perplexity=5214.967, train_loss=8.559288

Batch 132400, train_perplexity=5637.885, train_loss=8.637264

Batch 132410, train_perplexity=5770.8735, train_loss=8.660579

Batch 132420, train_perplexity=5674.0024, train_loss=8.64365

Batch 132430, train_perplexity=5285.4844, train_loss=8.57272

Batch 132440, train_perplexity=5019.85, train_loss=8.521155

Batch 132450, train_perplexity=5738.2476, train_loss=8.654909

Batch 132460, train_perplexity=5001.2524, train_loss=8.517444

Batch 132470, train_perplexity=4850.0854, train_loss=8.486752

Batch 132480, train_perplexity=4254.4893, train_loss=8.35573

Batch 132490, train_perplexity=5608.0483, train_loss=8.631958

Batch 132500, train_perplexity=5101.469, train_loss=8.537284

Batch 132510, train_perplexity=4872.919, train_loss=8.491448

Batch 132520, train_perplexity=6171.8247, train_loss=8.72775

Batch 132530, train_perplexity=5020.5537, train_loss=8.521296

Batch 132540, train_perplexity=5184.47, train_loss=8.553423

Batch 132550, train_perplexity=5001.4575, train_loss=8.517485

Batch 132560, train_perplexity=5371.679, train_loss=8.588896

Batch 132570, train_perplexity=5847.3467, train_loss=8.673743

Batch 132580, train_perplexity=5077.865, train_loss=8.532646

Batch 132590, train_perplexity=4728.101, train_loss=8.461279

Batch 132600, train_perplexity=5453.893, train_loss=8.604085

Batch 132610, train_perplexity=5538.8394, train_loss=8.61954

Batch 132620, train_perplexity=5078.344, train_loss=8.532741

Batch 132630, train_perplexity=6174.786, train_loss=8.7282295

Batch 132640, train_perplexity=5214.4893, train_loss=8.559196

Batch 132650, train_perplexity=5160.211, train_loss=8.548733

Batch 132660, train_perplexity=5263.3374, train_loss=8.568521

Batch 132670, train_perplexity=5140.054, train_loss=8.544819

Batch 132680, train_perplexity=4636.6514, train_loss=8.441748

Batch 132690, train_perplexity=4488.0986, train_loss=8.409184

Batch 132700, train_perplexity=4891.93, train_loss=8.495342

Batch 132710, train_perplexity=5713.2275, train_loss=8.650539

Batch 132720, train_perplexity=5851.419, train_loss=8.674439

Batch 132730, train_perplexity=4581.7847, train_loss=8.429844

Batch 132740, train_perplexity=5511.534, train_loss=8.614598

Batch 132750, train_perplexity=5844.9434, train_loss=8.673332

Batch 132760, train_perplexity=5147.0586, train_loss=8.546181

Batch 132770, train_perplexity=6174.409, train_loss=8.7281685

Batch 132780, train_perplexity=4978.2495, train_loss=8.512834

Batch 132790, train_perplexity=5906.0977, train_loss=8.683741

Batch 132800, train_perplexity=5154.501, train_loss=8.547626
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 132810, train_perplexity=5539.864, train_loss=8.619725

Batch 132820, train_perplexity=5083.9365, train_loss=8.533841

Batch 132830, train_perplexity=7116.101, train_loss=8.870115

Batch 132840, train_perplexity=6137.342, train_loss=8.722147

Batch 132850, train_perplexity=5898.2793, train_loss=8.682416

Batch 132860, train_perplexity=4968.1807, train_loss=8.510809

Batch 132870, train_perplexity=5176.279, train_loss=8.551842

Batch 132880, train_perplexity=6781.223, train_loss=8.821913

Batch 132890, train_perplexity=6114.1606, train_loss=8.718363

Batch 132900, train_perplexity=4614.819, train_loss=8.437028

Batch 132910, train_perplexity=4787.4673, train_loss=8.473757

Batch 132920, train_perplexity=4995.032, train_loss=8.516199

Batch 132930, train_perplexity=5957.986, train_loss=8.692488

Batch 132940, train_perplexity=5706.258, train_loss=8.649319

Batch 132950, train_perplexity=4621.562, train_loss=8.438488

Batch 132960, train_perplexity=7384.08, train_loss=8.907082

Batch 132970, train_perplexity=4632.4746, train_loss=8.440846

Batch 132980, train_perplexity=5415.359, train_loss=8.596994

Batch 132990, train_perplexity=4881.8867, train_loss=8.493287

Batch 133000, train_perplexity=4306.447, train_loss=8.367868

Batch 133010, train_perplexity=5250.9634, train_loss=8.566167

Batch 133020, train_perplexity=4895.9956, train_loss=8.496173

Batch 133030, train_perplexity=5905.0107, train_loss=8.683557

Batch 133040, train_perplexity=5184.935, train_loss=8.553513

Batch 133050, train_perplexity=5343.3633, train_loss=8.583611

Batch 133060, train_perplexity=4601.5864, train_loss=8.434156

Batch 133070, train_perplexity=4498.8677, train_loss=8.411581

Batch 133080, train_perplexity=5033.805, train_loss=8.5239315

Batch 133090, train_perplexity=5595.0347, train_loss=8.629635

Batch 133100, train_perplexity=5309.016, train_loss=8.577162

Batch 133110, train_perplexity=6751.089, train_loss=8.817459

Batch 133120, train_perplexity=6111.584, train_loss=8.717941

Batch 133130, train_perplexity=5089.593, train_loss=8.534953

Batch 133140, train_perplexity=5500.638, train_loss=8.612619

Batch 133150, train_perplexity=4945.363, train_loss=8.506206

Batch 133160, train_perplexity=5927.161, train_loss=8.687301

Batch 133170, train_perplexity=5975.2725, train_loss=8.695385

Batch 133180, train_perplexity=5835.375, train_loss=8.671694

Batch 133190, train_perplexity=4401.8325, train_loss=8.389776

Batch 133200, train_perplexity=5507.3677, train_loss=8.613842

Batch 133210, train_perplexity=4790.427, train_loss=8.474375

Batch 133220, train_perplexity=4697.4087, train_loss=8.454766

Batch 133230, train_perplexity=5751.698, train_loss=8.65725

Batch 133240, train_perplexity=6287.9316, train_loss=8.7463875

Batch 133250, train_perplexity=5205.253, train_loss=8.557424

Batch 133260, train_perplexity=4304.944, train_loss=8.367519

Batch 133270, train_perplexity=5523.252, train_loss=8.616722

Batch 133280, train_perplexity=5198.1343, train_loss=8.556055

Batch 133290, train_perplexity=5926.6357, train_loss=8.687212

Batch 133300, train_perplexity=5026.3364, train_loss=8.522447

Batch 133310, train_perplexity=5221.4263, train_loss=8.560526

Batch 133320, train_perplexity=5658.742, train_loss=8.640957

Batch 133330, train_perplexity=6963.917, train_loss=8.848497

Batch 133340, train_perplexity=5552.558, train_loss=8.622014

Batch 133350, train_perplexity=7078.4214, train_loss=8.864806

Batch 133360, train_perplexity=5959.935, train_loss=8.692815

Batch 133370, train_perplexity=5851.1006, train_loss=8.674385

Batch 133380, train_perplexity=5912.68, train_loss=8.6848545

Batch 133390, train_perplexity=7101.0107, train_loss=8.867992

Batch 133400, train_perplexity=4200.394, train_loss=8.342934

Batch 133410, train_perplexity=5635.0146, train_loss=8.636755

Batch 133420, train_perplexity=5557.8296, train_loss=8.622963

Batch 133430, train_perplexity=6151.3345, train_loss=8.724424

Batch 133440, train_perplexity=4834.979, train_loss=8.483632

Batch 133450, train_perplexity=4957.2856, train_loss=8.508614

Batch 133460, train_perplexity=4992.8174, train_loss=8.515756

Batch 133470, train_perplexity=6290.685, train_loss=8.746825

Batch 133480, train_perplexity=4984.9624, train_loss=8.514181

Batch 133490, train_perplexity=5796.5273, train_loss=8.665014

Batch 133500, train_perplexity=6737.7876, train_loss=8.815487

Batch 133510, train_perplexity=4958.3447, train_loss=8.508827

Batch 133520, train_perplexity=5864.5527, train_loss=8.6766815

Batch 133530, train_perplexity=5502.165, train_loss=8.612897

Batch 133540, train_perplexity=5033.7188, train_loss=8.523914

Batch 133550, train_perplexity=4953.2637, train_loss=8.507802

Batch 133560, train_perplexity=6537.1675, train_loss=8.785259

Batch 133570, train_perplexity=5127.657, train_loss=8.542404

Batch 133580, train_perplexity=5219.9526, train_loss=8.560244

Batch 133590, train_perplexity=5702.939, train_loss=8.648737

Batch 133600, train_perplexity=5064.71, train_loss=8.530052

Batch 133610, train_perplexity=5099.519, train_loss=8.536901

Batch 133620, train_perplexity=4945.5654, train_loss=8.506247

Batch 133630, train_perplexity=5007.1177, train_loss=8.518616

Batch 133640, train_perplexity=5511.2134, train_loss=8.61454

Batch 133650, train_perplexity=4530.885, train_loss=8.418673

Batch 133660, train_perplexity=6001.9893, train_loss=8.699846

Batch 133670, train_perplexity=6415.51, train_loss=8.766474

Batch 133680, train_perplexity=4687.742, train_loss=8.452706

Batch 133690, train_perplexity=5814.499, train_loss=8.66811

Batch 133700, train_perplexity=5328.5854, train_loss=8.580841

Batch 133710, train_perplexity=4921.87, train_loss=8.501444

Batch 133720, train_perplexity=5030.4077, train_loss=8.523256

Batch 133730, train_perplexity=5065.541, train_loss=8.530216

Batch 133740, train_perplexity=4317.793, train_loss=8.3705

Batch 133750, train_perplexity=6202.632, train_loss=8.732729

Batch 133760, train_perplexity=5363.4375, train_loss=8.58736

Batch 133770, train_perplexity=5657.598, train_loss=8.640755

Batch 133780, train_perplexity=5907.1567, train_loss=8.68392

Batch 133790, train_perplexity=5611.9004, train_loss=8.632645

Batch 133800, train_perplexity=4860.828, train_loss=8.488964

Batch 133810, train_perplexity=5882.8364, train_loss=8.679794

Batch 133820, train_perplexity=4638.0312, train_loss=8.442045

Batch 133830, train_perplexity=5428.1973, train_loss=8.599362

Batch 133840, train_perplexity=6091.3345, train_loss=8.7146225

Batch 133850, train_perplexity=6008.427, train_loss=8.700918

Batch 133860, train_perplexity=5793.9297, train_loss=8.664566

Batch 133870, train_perplexity=5698.6167, train_loss=8.647979

Batch 133880, train_perplexity=5845.501, train_loss=8.673428

Batch 133890, train_perplexity=5742.594, train_loss=8.655666

Batch 133900, train_perplexity=4894.735, train_loss=8.495915

Batch 133910, train_perplexity=5349.849, train_loss=8.584824

Batch 133920, train_perplexity=6882.339, train_loss=8.836714

Batch 133930, train_perplexity=6373.2314, train_loss=8.759862

Batch 133940, train_perplexity=5213.3955, train_loss=8.558987

Batch 133950, train_perplexity=5214.047, train_loss=8.559112

Batch 133960, train_perplexity=5546.9375, train_loss=8.621001

Batch 133970, train_perplexity=5517.766, train_loss=8.615728

Batch 133980, train_perplexity=4815.647, train_loss=8.479626

Batch 133990, train_perplexity=4780.2227, train_loss=8.472242

Batch 134000, train_perplexity=6740.905, train_loss=8.815949

Batch 134010, train_perplexity=6496.0264, train_loss=8.778946

Batch 134020, train_perplexity=5589.611, train_loss=8.628665

Batch 134030, train_perplexity=5259.649, train_loss=8.56782

Batch 134040, train_perplexity=7034.7124, train_loss=8.858612

Batch 134050, train_perplexity=4376.4497, train_loss=8.383993

Batch 134060, train_perplexity=7499.747, train_loss=8.922625

Batch 134070, train_perplexity=6454.639, train_loss=8.772554

Batch 134080, train_perplexity=4099.869, train_loss=8.31871

Batch 134090, train_perplexity=5414.491, train_loss=8.596834

Batch 134100, train_perplexity=6359.5527, train_loss=8.757713

Batch 134110, train_perplexity=5215.9814, train_loss=8.559483

Batch 134120, train_perplexity=5248.5654, train_loss=8.56571
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 134130, train_perplexity=5422.501, train_loss=8.598312

Batch 134140, train_perplexity=5338.7383, train_loss=8.582745

Batch 134150, train_perplexity=5000.0366, train_loss=8.5172

Batch 134160, train_perplexity=6286.9604, train_loss=8.746233

Batch 134170, train_perplexity=4580.3213, train_loss=8.429524

Batch 134180, train_perplexity=5213.803, train_loss=8.559065

Batch 134190, train_perplexity=5874.012, train_loss=8.678293

Batch 134200, train_perplexity=5539.2036, train_loss=8.619606

Batch 134210, train_perplexity=4643.4395, train_loss=8.443211

Batch 134220, train_perplexity=5186.27, train_loss=8.55377

Batch 134230, train_perplexity=4975.928, train_loss=8.512367

Batch 134240, train_perplexity=4645.326, train_loss=8.443617

Batch 134250, train_perplexity=4507.5083, train_loss=8.4135

Batch 134260, train_perplexity=5043.3867, train_loss=8.525833

Batch 134270, train_perplexity=4887.0483, train_loss=8.494344

Batch 134280, train_perplexity=5976.1104, train_loss=8.695525

Batch 134290, train_perplexity=4906.4795, train_loss=8.498312

Batch 134300, train_perplexity=6490.428, train_loss=8.778084

Batch 134310, train_perplexity=4785.6826, train_loss=8.473384

Batch 134320, train_perplexity=6220.8423, train_loss=8.735661

Batch 134330, train_perplexity=4382.2134, train_loss=8.385309

Batch 134340, train_perplexity=4762.2627, train_loss=8.468478

Batch 134350, train_perplexity=5517.829, train_loss=8.61574

Batch 134360, train_perplexity=5690.3516, train_loss=8.646527

Batch 134370, train_perplexity=4415.674, train_loss=8.392916

Batch 134380, train_perplexity=5113.052, train_loss=8.539552

Batch 134390, train_perplexity=4687.474, train_loss=8.452649

Batch 134400, train_perplexity=6092.45, train_loss=8.714806

Batch 134410, train_perplexity=4490.5947, train_loss=8.40974

Batch 134420, train_perplexity=5147.717, train_loss=8.5463085

Batch 134430, train_perplexity=6018.497, train_loss=8.702593

Batch 134440, train_perplexity=4511.8994, train_loss=8.414474

Batch 134450, train_perplexity=4738.7544, train_loss=8.46353

Batch 134460, train_perplexity=5669.0806, train_loss=8.642782

Batch 134470, train_perplexity=4552.6025, train_loss=8.423454

Batch 134480, train_perplexity=4794.3574, train_loss=8.475195

Batch 134490, train_perplexity=5006.764, train_loss=8.518545

Batch 134500, train_perplexity=4993.9653, train_loss=8.5159855

Batch 134510, train_perplexity=4891.394, train_loss=8.495233

Batch 134520, train_perplexity=6594.235, train_loss=8.793951

Batch 134530, train_perplexity=5692.3164, train_loss=8.6468725

Batch 134540, train_perplexity=5445.0947, train_loss=8.60247

Batch 134550, train_perplexity=5007.3804, train_loss=8.518668

Batch 134560, train_perplexity=5819.2974, train_loss=8.668935

Batch 134570, train_perplexity=5277.5767, train_loss=8.571222

Batch 134580, train_perplexity=5354.693, train_loss=8.585729

Batch 134590, train_perplexity=4419.4487, train_loss=8.39377

Batch 134600, train_perplexity=5299.3696, train_loss=8.575343

Batch 134610, train_perplexity=6100.1245, train_loss=8.716064

Batch 134620, train_perplexity=4841.7295, train_loss=8.485027

Batch 134630, train_perplexity=5019.271, train_loss=8.52104

Batch 134640, train_perplexity=5279.8975, train_loss=8.571662

Batch 134650, train_perplexity=4838.3325, train_loss=8.484325

Batch 134660, train_perplexity=5878.602, train_loss=8.679074

Batch 134670, train_perplexity=6974.039, train_loss=8.84995

Batch 134680, train_perplexity=5532.203, train_loss=8.618341

Batch 134690, train_perplexity=5081.3867, train_loss=8.5333395

Batch 134700, train_perplexity=5474.628, train_loss=8.60788

Batch 134710, train_perplexity=6373.286, train_loss=8.759871

Batch 134720, train_perplexity=6254.631, train_loss=8.741077

Batch 134730, train_perplexity=5271.5503, train_loss=8.57008

Batch 134740, train_perplexity=5094.833, train_loss=8.535982

Batch 134750, train_perplexity=5074.341, train_loss=8.531952

Batch 134760, train_perplexity=5608.2515, train_loss=8.631994

Batch 134770, train_perplexity=5226.388, train_loss=8.561476

Batch 134780, train_perplexity=6233.4556, train_loss=8.737686

Batch 134790, train_perplexity=5795.0073, train_loss=8.664752

Batch 134800, train_perplexity=5785.879, train_loss=8.663176

Batch 134810, train_perplexity=5248.345, train_loss=8.565668

Batch 134820, train_perplexity=6752.57, train_loss=8.817678

Batch 134830, train_perplexity=5022.3833, train_loss=8.52166

Batch 134840, train_perplexity=5515.8193, train_loss=8.6153755

Batch 134850, train_perplexity=5948.958, train_loss=8.690971

Batch 134860, train_perplexity=4786.3623, train_loss=8.473526

Batch 134870, train_perplexity=4701.756, train_loss=8.455691

Batch 134880, train_perplexity=6433.4062, train_loss=8.769259

Batch 134890, train_perplexity=5714.2954, train_loss=8.650726

Batch 134900, train_perplexity=4836.035, train_loss=8.4838505

Batch 134910, train_perplexity=4621.1387, train_loss=8.438396

Batch 134920, train_perplexity=5554.126, train_loss=8.622296

Batch 134930, train_perplexity=5887.5957, train_loss=8.680603

Batch 134940, train_perplexity=5111.077, train_loss=8.5391655

Batch 134950, train_perplexity=5530.6997, train_loss=8.61807

Batch 134960, train_perplexity=4593.5146, train_loss=8.432401

Batch 134970, train_perplexity=5373.3647, train_loss=8.58921

Batch 134980, train_perplexity=5349.767, train_loss=8.584808

Batch 134990, train_perplexity=4690.752, train_loss=8.453348

Batch 135000, train_perplexity=5125.7993, train_loss=8.542042

Batch 135010, train_perplexity=4990.2134, train_loss=8.515234

Batch 135020, train_perplexity=6939.5264, train_loss=8.844989

Batch 135030, train_perplexity=5368.545, train_loss=8.588312

Batch 135040, train_perplexity=5991.6265, train_loss=8.698118

Batch 135050, train_perplexity=4378.358, train_loss=8.384429

Batch 135060, train_perplexity=4617.359, train_loss=8.437578

Batch 135070, train_perplexity=5172.3066, train_loss=8.551074

Batch 135080, train_perplexity=5118.804, train_loss=8.540676

Batch 135090, train_perplexity=4668.331, train_loss=8.448557

Batch 135100, train_perplexity=4776.7725, train_loss=8.47152

Batch 135110, train_perplexity=4862.757, train_loss=8.489361

Batch 135120, train_perplexity=5868.665, train_loss=8.677382

Batch 135130, train_perplexity=6282.6807, train_loss=8.745552

Batch 135140, train_perplexity=4947.462, train_loss=8.50663

Batch 135150, train_perplexity=4859.5303, train_loss=8.488697

Batch 135160, train_perplexity=4737.6787, train_loss=8.463303

Batch 135170, train_perplexity=5630.0835, train_loss=8.6358795

Batch 135180, train_perplexity=4758.985, train_loss=8.46779

Batch 135190, train_perplexity=5524.5005, train_loss=8.616948

Batch 135200, train_perplexity=4583.262, train_loss=8.430166

Batch 135210, train_perplexity=4978.9805, train_loss=8.51298

Batch 135220, train_perplexity=6470.997, train_loss=8.775085

Batch 135230, train_perplexity=4979.299, train_loss=8.513044

Batch 135240, train_perplexity=5464.5923, train_loss=8.606045

Batch 135250, train_perplexity=4634.865, train_loss=8.441362

Batch 135260, train_perplexity=5090.651, train_loss=8.535161

Batch 135270, train_perplexity=5799.209, train_loss=8.665477

Batch 135280, train_perplexity=6885.5947, train_loss=8.837187

Batch 135290, train_perplexity=4483.2007, train_loss=8.4080925

Batch 135300, train_perplexity=5618.353, train_loss=8.633794

Batch 135310, train_perplexity=6576.537, train_loss=8.791264

Batch 135320, train_perplexity=5297.7676, train_loss=8.575041

Batch 135330, train_perplexity=5111.428, train_loss=8.539234

Batch 135340, train_perplexity=5958.872, train_loss=8.6926365

Batch 135350, train_perplexity=4872.663, train_loss=8.491396

Batch 135360, train_perplexity=5396.788, train_loss=8.593559

Batch 135370, train_perplexity=5057.8076, train_loss=8.528688

Batch 135380, train_perplexity=4576.5137, train_loss=8.428693

Batch 135390, train_perplexity=5003.2036, train_loss=8.517834

Batch 135400, train_perplexity=5905.3433, train_loss=8.683613

Batch 135410, train_perplexity=6645.3076, train_loss=8.801666

Batch 135420, train_perplexity=4164.222, train_loss=8.334285

Batch 135430, train_perplexity=5568.9556, train_loss=8.624963

Batch 135440, train_perplexity=5778.082, train_loss=8.661827
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 135450, train_perplexity=4898.578, train_loss=8.4967

Batch 135460, train_perplexity=5333.293, train_loss=8.581724

Batch 135470, train_perplexity=5061.7983, train_loss=8.529477

Batch 135480, train_perplexity=5535.681, train_loss=8.61897

Batch 135490, train_perplexity=4802.9883, train_loss=8.476994

Batch 135500, train_perplexity=6120.9985, train_loss=8.7194805

Batch 135510, train_perplexity=4993.9507, train_loss=8.515983

Batch 135520, train_perplexity=5065.652, train_loss=8.530238

Batch 135530, train_perplexity=5818.748, train_loss=8.66884

Batch 135540, train_perplexity=5516.7085, train_loss=8.615537

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Batch 135550, train_perplexity=6086.4224, train_loss=8.713816

Batch 135560, train_perplexity=5212.719, train_loss=8.558857

Batch 135570, train_perplexity=5424.7354, train_loss=8.598724

Batch 135580, train_perplexity=5128.9434, train_loss=8.542655

Batch 135590, train_perplexity=5582.201, train_loss=8.627338

Batch 135600, train_perplexity=5011.9766, train_loss=8.519586

Batch 135610, train_perplexity=5009.0283, train_loss=8.518997

Batch 135620, train_perplexity=5275.4785, train_loss=8.570825

Batch 135630, train_perplexity=5418.8047, train_loss=8.5976305

Batch 135640, train_perplexity=5317.7314, train_loss=8.578802

Batch 135650, train_perplexity=6034.0547, train_loss=8.705174

Batch 135660, train_perplexity=4691.3154, train_loss=8.453468

Batch 135670, train_perplexity=5800.487, train_loss=8.665697

Batch 135680, train_perplexity=5681.6914, train_loss=8.645004

Batch 135690, train_perplexity=5454.668, train_loss=8.604227

Batch 135700, train_perplexity=4447.557, train_loss=8.40011

Batch 135710, train_perplexity=5338.7686, train_loss=8.58275

Batch 135720, train_perplexity=5398.1367, train_loss=8.593809

Batch 135730, train_perplexity=5347.4263, train_loss=8.584371

Batch 135740, train_perplexity=5645.783, train_loss=8.638664

Batch 135750, train_perplexity=5994.2783, train_loss=8.698561

Batch 135760, train_perplexity=4728.678, train_loss=8.461401

Batch 135770, train_perplexity=6217.408, train_loss=8.735108

Batch 135780, train_perplexity=5859.499, train_loss=8.675819

Batch 135790, train_perplexity=5271.7065, train_loss=8.570109

Batch 135800, train_perplexity=5907.703, train_loss=8.684012

Batch 135810, train_perplexity=5346.442, train_loss=8.584187

Batch 135820, train_perplexity=5295.217, train_loss=8.574559

Batch 135830, train_perplexity=6137.892, train_loss=8.722237

Batch 135840, train_perplexity=5795.853, train_loss=8.664898

Batch 135850, train_perplexity=6029.4355, train_loss=8.704409

Batch 135860, train_perplexity=4339.5356, train_loss=8.375523

Batch 135870, train_perplexity=5387.9634, train_loss=8.591923

Batch 135880, train_perplexity=5267.485, train_loss=8.569308

Batch 135890, train_perplexity=5383.1255, train_loss=8.591024

Batch 135900, train_perplexity=5305.3516, train_loss=8.576471

Batch 135910, train_perplexity=5546.0806, train_loss=8.620847

Batch 135920, train_perplexity=5908.83, train_loss=8.684203

Batch 135930, train_perplexity=5568.3022, train_loss=8.6248455

Batch 135940, train_perplexity=4775.661, train_loss=8.471288

Batch 135950, train_perplexity=4813.434, train_loss=8.479166

Batch 135960, train_perplexity=5700.503, train_loss=8.64831

Batch 135970, train_perplexity=5504.432, train_loss=8.613309

Batch 135980, train_perplexity=4583.791, train_loss=8.430282

Batch 135990, train_perplexity=5373.298, train_loss=8.589197

Batch 136000, train_perplexity=5910.5996, train_loss=8.684503

Batch 136010, train_perplexity=4390.5127, train_loss=8.387201

Batch 136020, train_perplexity=4289.731, train_loss=8.363979

Batch 136030, train_perplexity=5981.9775, train_loss=8.6965065

Batch 136040, train_perplexity=5630.1265, train_loss=8.635887

Batch 136050, train_perplexity=4740.3545, train_loss=8.463867

Batch 136060, train_perplexity=5409.0254, train_loss=8.595824

Batch 136070, train_perplexity=4642.8105, train_loss=8.443075

Batch 136080, train_perplexity=5589.755, train_loss=8.628691

Batch 136090, train_perplexity=5178.649, train_loss=8.5522995

Batch 136100, train_perplexity=5357.7427, train_loss=8.586298

Batch 136110, train_perplexity=5424.311, train_loss=8.598646

Batch 136120, train_perplexity=6285.12, train_loss=8.74594

Batch 136130, train_perplexity=4985.8657, train_loss=8.514362

Batch 136140, train_perplexity=5933.92, train_loss=8.68844

Batch 136150, train_perplexity=4886.2515, train_loss=8.494181

Batch 136160, train_perplexity=4152.1904, train_loss=8.331391

Batch 136170, train_perplexity=5722.814, train_loss=8.652216

Batch 136180, train_perplexity=5447.463, train_loss=8.602905

Batch 136190, train_perplexity=4798.6206, train_loss=8.476084

Batch 136200, train_perplexity=6255.329, train_loss=8.741189

Batch 136210, train_perplexity=5598.7925, train_loss=8.630306

Batch 136220, train_perplexity=5546.017, train_loss=8.620835

Batch 136230, train_perplexity=5571.277, train_loss=8.62538

Batch 136240, train_perplexity=6832.356, train_loss=8.829425

Batch 136250, train_perplexity=6058.1504, train_loss=8.70916

Batch 136260, train_perplexity=4257.8667, train_loss=8.3565235

Batch 136270, train_perplexity=4744.09, train_loss=8.464655

Batch 136280, train_perplexity=5322.09, train_loss=8.579621

Batch 136290, train_perplexity=5042.271, train_loss=8.525612

Batch 136300, train_perplexity=5968.495, train_loss=8.69425

Batch 136310, train_perplexity=4497.0273, train_loss=8.411172

Batch 136320, train_perplexity=5003.7188, train_loss=8.517937

Batch 136330, train_perplexity=5724.6265, train_loss=8.652533

Batch 136340, train_perplexity=5123.1455, train_loss=8.541524

Batch 136350, train_perplexity=5793.8467, train_loss=8.664552

Batch 136360, train_perplexity=5332.8, train_loss=8.581632

Batch 136370, train_perplexity=5787.22, train_loss=8.663407

Batch 136380, train_perplexity=4855.287, train_loss=8.4878235

Batch 136390, train_perplexity=5163.2334, train_loss=8.549318

Batch 136400, train_perplexity=4995.5703, train_loss=8.516307

Batch 136410, train_perplexity=4893.209, train_loss=8.495604

Batch 136420, train_perplexity=4759.2524, train_loss=8.467846

Batch 136430, train_perplexity=5736.3433, train_loss=8.654577

Batch 136440, train_perplexity=5278.1304, train_loss=8.571327

Batch 136450, train_perplexity=5322.2773, train_loss=8.579657

Batch 136460, train_perplexity=4756.761, train_loss=8.467322

Batch 136470, train_perplexity=4792.031, train_loss=8.4747095

Batch 136480, train_perplexity=5398.708, train_loss=8.593915

Batch 136490, train_perplexity=5825.572, train_loss=8.670012

Batch 136500, train_perplexity=5430.7812, train_loss=8.599838

Batch 136510, train_perplexity=5676.5356, train_loss=8.644096

Batch 136520, train_perplexity=4859.6323, train_loss=8.488718

Batch 136530, train_perplexity=6057.0874, train_loss=8.708984

Batch 136540, train_perplexity=5278.7847, train_loss=8.571451

Batch 136550, train_perplexity=5940.8223, train_loss=8.689603

Batch 136560, train_perplexity=5485.3784, train_loss=8.609841

Batch 136570, train_perplexity=5963.312, train_loss=8.693381

Batch 136580, train_perplexity=6448.056, train_loss=8.771534

Batch 136590, train_perplexity=6295.912, train_loss=8.747656

Batch 136600, train_perplexity=5038.6274, train_loss=8.524889

Batch 136610, train_perplexity=5493.257, train_loss=8.611277

Batch 136620, train_perplexity=4456.2227, train_loss=8.402057

Batch 136630, train_perplexity=5479.716, train_loss=8.6088085

Batch 136640, train_perplexity=5229.1655, train_loss=8.562007

Batch 136650, train_perplexity=5636.1914, train_loss=8.636964

Batch 136660, train_perplexity=5205.794, train_loss=8.557528

Batch 136670, train_perplexity=4841.4526, train_loss=8.48497

Batch 136680, train_perplexity=5855.7896, train_loss=8.675186

Batch 136690, train_perplexity=6561.2075, train_loss=8.78893
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 136700, train_perplexity=6177.79, train_loss=8.728716

Batch 136710, train_perplexity=5152.0776, train_loss=8.547155

Batch 136720, train_perplexity=4092.5835, train_loss=8.316932

Batch 136730, train_perplexity=5634.2085, train_loss=8.636612

Batch 136740, train_perplexity=4901.307, train_loss=8.497257

Batch 136750, train_perplexity=4876.0195, train_loss=8.4920845

Batch 136760, train_perplexity=4552.537, train_loss=8.42344

Batch 136770, train_perplexity=4958.472, train_loss=8.508853

Batch 136780, train_perplexity=5856.717, train_loss=8.675344

Batch 136790, train_perplexity=6328.0146, train_loss=8.752742

Batch 136800, train_perplexity=5511.823, train_loss=8.614651

Batch 136810, train_perplexity=6017.005, train_loss=8.702345

Batch 136820, train_perplexity=5540.9575, train_loss=8.619923

Batch 136830, train_perplexity=5882.971, train_loss=8.679817

Batch 136840, train_perplexity=5576.0234, train_loss=8.626231

Batch 136850, train_perplexity=5208.242, train_loss=8.557998

Batch 136860, train_perplexity=5183.412, train_loss=8.553219

Batch 136870, train_perplexity=5507.63, train_loss=8.61389

Batch 136880, train_perplexity=5681.7134, train_loss=8.645008

Batch 136890, train_perplexity=4655.216, train_loss=8.445744

Batch 136900, train_perplexity=5323.0796, train_loss=8.579807

Batch 136910, train_perplexity=5259.1274, train_loss=8.56772

Batch 136920, train_perplexity=6859.496, train_loss=8.833389

Batch 136930, train_perplexity=5670.1294, train_loss=8.642967

Batch 136940, train_perplexity=4929.809, train_loss=8.503056

Batch 136950, train_perplexity=6217.1885, train_loss=8.735073

Batch 136960, train_perplexity=5392.7646, train_loss=8.5928135

Batch 136970, train_perplexity=5296.328, train_loss=8.574769

Batch 136980, train_perplexity=5037.6665, train_loss=8.524698

Batch 136990, train_perplexity=5216.832, train_loss=8.559646

Batch 137000, train_perplexity=4699.326, train_loss=8.455174

Batch 137010, train_perplexity=5135.556, train_loss=8.543943

Batch 137020, train_perplexity=5547.8633, train_loss=8.621168

Batch 137030, train_perplexity=5513.7524, train_loss=8.615001

Batch 137040, train_perplexity=5421.591, train_loss=8.598145

Batch 137050, train_perplexity=5431.6616, train_loss=8.6

Batch 137060, train_perplexity=5113.3003, train_loss=8.5396

Batch 137070, train_perplexity=6022.183, train_loss=8.703205

Batch 137080, train_perplexity=4352.7573, train_loss=8.378565

Batch 137090, train_perplexity=5769.1733, train_loss=8.660284

Batch 137100, train_perplexity=5399.604, train_loss=8.594081

Batch 137110, train_perplexity=6166.8296, train_loss=8.72694

Batch 137120, train_perplexity=5088.3604, train_loss=8.534711

Batch 137130, train_perplexity=5212.8687, train_loss=8.558886

Batch 137140, train_perplexity=5420.0913, train_loss=8.597868

Batch 137150, train_perplexity=5660.1235, train_loss=8.641201

Batch 137160, train_perplexity=4874.8384, train_loss=8.491842

Batch 137170, train_perplexity=7536.2554, train_loss=8.927481

Batch 137180, train_perplexity=5133.2646, train_loss=8.543497

Batch 137190, train_perplexity=5049.7397, train_loss=8.527092

Batch 137200, train_perplexity=5392.23, train_loss=8.592714

Batch 137210, train_perplexity=5934.192, train_loss=8.688486

Batch 137220, train_perplexity=4517.1304, train_loss=8.415632

Batch 137230, train_perplexity=5122.496, train_loss=8.541397

Batch 137240, train_perplexity=6559.2744, train_loss=8.788635

Batch 137250, train_perplexity=5617.8867, train_loss=8.633711

Batch 137260, train_perplexity=4223.6963, train_loss=8.348466

Batch 137270, train_perplexity=5042.757, train_loss=8.525708

Batch 137280, train_perplexity=4709.3174, train_loss=8.457298

Batch 137290, train_perplexity=6305.454, train_loss=8.74917

Batch 137300, train_perplexity=5194.9976, train_loss=8.555451

Batch 137310, train_perplexity=5511.634, train_loss=8.614616

Batch 137320, train_perplexity=5437.6997, train_loss=8.601111

Batch 137330, train_perplexity=5614.6626, train_loss=8.633137

Batch 137340, train_perplexity=5472.217, train_loss=8.607439

Batch 137350, train_perplexity=5319.3955, train_loss=8.579115

Batch 137360, train_perplexity=4861.1895, train_loss=8.489038

Batch 137370, train_perplexity=5092.3604, train_loss=8.535497

Batch 137380, train_perplexity=4524.4946, train_loss=8.417261

Batch 137390, train_perplexity=5499.8726, train_loss=8.61248

Batch 137400, train_perplexity=4790.6367, train_loss=8.474419

Batch 137410, train_perplexity=6642.6846, train_loss=8.801271

Batch 137420, train_perplexity=5118.858, train_loss=8.540687

Batch 137430, train_perplexity=4532.994, train_loss=8.419138

Batch 137440, train_perplexity=4825.66, train_loss=8.481703

Batch 137450, train_perplexity=6952.6626, train_loss=8.84688

Batch 137460, train_perplexity=6318.7456, train_loss=8.751276

Batch 137470, train_perplexity=5061.0215, train_loss=8.529324

Batch 137480, train_perplexity=6010.364, train_loss=8.701241

Batch 137490, train_perplexity=5091.219, train_loss=8.535273

Batch 137500, train_perplexity=5378.435, train_loss=8.590153

Batch 137510, train_perplexity=4533.8936, train_loss=8.419336

Batch 137520, train_perplexity=5076.8047, train_loss=8.532437

Batch 137530, train_perplexity=4635.126, train_loss=8.441419

Batch 137540, train_perplexity=6371.2993, train_loss=8.759559

Batch 137550, train_perplexity=4621.963, train_loss=8.438575

Batch 137560, train_perplexity=5742.693, train_loss=8.6556835

Batch 137570, train_perplexity=5814.976, train_loss=8.668192

Batch 137580, train_perplexity=5035.14, train_loss=8.524197

Batch 137590, train_perplexity=5204.9653, train_loss=8.557368

Batch 137600, train_perplexity=7516.33, train_loss=8.924833

Batch 137610, train_perplexity=5026.202, train_loss=8.52242

Batch 137620, train_perplexity=5374.8867, train_loss=8.589493

Batch 137630, train_perplexity=4859.4795, train_loss=8.488687

Batch 137640, train_perplexity=5616.8853, train_loss=8.633533

Batch 137650, train_perplexity=6171.113, train_loss=8.727634

Batch 137660, train_perplexity=4813.9663, train_loss=8.479277

Batch 137670, train_perplexity=5135.091, train_loss=8.543853

Batch 137680, train_perplexity=4522.372, train_loss=8.416792

Batch 137690, train_perplexity=6891.6455, train_loss=8.838065

Batch 137700, train_perplexity=4353.517, train_loss=8.378739

Batch 137710, train_perplexity=5621.5957, train_loss=8.634371

Batch 137720, train_perplexity=7158.732, train_loss=8.876088

Batch 137730, train_perplexity=5194.2197, train_loss=8.555302

Batch 137740, train_perplexity=4850.492, train_loss=8.4868355

Batch 137750, train_perplexity=5477.4272, train_loss=8.608391

Batch 137760, train_perplexity=6169.9595, train_loss=8.7274475

Batch 137770, train_perplexity=4908.8525, train_loss=8.4987955

Batch 137780, train_perplexity=4825.1816, train_loss=8.481604

Batch 137790, train_perplexity=6729.3496, train_loss=8.814234

Batch 137800, train_perplexity=5335.6636, train_loss=8.582169

Batch 137810, train_perplexity=5827.689, train_loss=8.670376

Batch 137820, train_perplexity=5655.133, train_loss=8.640319

Batch 137830, train_perplexity=5915.3535, train_loss=8.685307

Batch 137840, train_perplexity=6354.3633, train_loss=8.756897

Batch 137850, train_perplexity=5991.3525, train_loss=8.698072

Batch 137860, train_perplexity=4711.7075, train_loss=8.457806

Batch 137870, train_perplexity=4654.883, train_loss=8.445672

Batch 137880, train_perplexity=7295.4155, train_loss=8.895001

Batch 137890, train_perplexity=5154.2007, train_loss=8.547567

Batch 137900, train_perplexity=5856.231, train_loss=8.6752615

Batch 137910, train_perplexity=4956.572, train_loss=8.50847

Batch 137920, train_perplexity=5036.197, train_loss=8.524406

Batch 137930, train_perplexity=6099.368, train_loss=8.71594

Batch 137940, train_perplexity=6010.8794, train_loss=8.701326

Batch 137950, train_perplexity=5140.6323, train_loss=8.544931

Batch 137960, train_perplexity=4695.801, train_loss=8.454424

Batch 137970, train_perplexity=4344.3267, train_loss=8.376626

Batch 137980, train_perplexity=5541.296, train_loss=8.619984

Batch 137990, train_perplexity=5088.2534, train_loss=8.53469

Batch 138000, train_perplexity=5273.27, train_loss=8.570406

Batch 138010, train_perplexity=4976.8774, train_loss=8.512558
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 138020, train_perplexity=5060.2104, train_loss=8.529163

Batch 138030, train_perplexity=4800.859, train_loss=8.47655

Batch 138040, train_perplexity=6582.134, train_loss=8.792114

Batch 138050, train_perplexity=4618.588, train_loss=8.437844

Batch 138060, train_perplexity=5661.5166, train_loss=8.641447

Batch 138070, train_perplexity=6174.7036, train_loss=8.728216

Batch 138080, train_perplexity=4522.721, train_loss=8.416869

Batch 138090, train_perplexity=6386.2334, train_loss=8.7619

Batch 138100, train_perplexity=5423.4834, train_loss=8.598494

Batch 138110, train_perplexity=4759.1255, train_loss=8.467819

Batch 138120, train_perplexity=6487.241, train_loss=8.777593

Batch 138130, train_perplexity=5935.1763, train_loss=8.688652

Batch 138140, train_perplexity=4524.0977, train_loss=8.417173

Batch 138150, train_perplexity=5065.9756, train_loss=8.530302

Batch 138160, train_perplexity=4912.857, train_loss=8.499611

Batch 138170, train_perplexity=4970.863, train_loss=8.511349

Batch 138180, train_perplexity=4823.838, train_loss=8.481325

Batch 138190, train_perplexity=4372.595, train_loss=8.383112

Batch 138200, train_perplexity=5172.134, train_loss=8.551041

Batch 138210, train_perplexity=5638.541, train_loss=8.637381

Batch 138220, train_perplexity=5566.582, train_loss=8.6245365

Batch 138230, train_perplexity=5861.8184, train_loss=8.676215

Batch 138240, train_perplexity=6534.8115, train_loss=8.784899

Batch 138250, train_perplexity=5415.8857, train_loss=8.597092

Batch 138260, train_perplexity=5743.2734, train_loss=8.655785

Batch 138270, train_perplexity=5328.1177, train_loss=8.580753

Batch 138280, train_perplexity=6386.112, train_loss=8.761881

Batch 138290, train_perplexity=5250.403, train_loss=8.56606

Batch 138300, train_perplexity=5733.8164, train_loss=8.654137

Batch 138310, train_perplexity=5893.0, train_loss=8.68152

Batch 138320, train_perplexity=5376.476, train_loss=8.589788

Batch 138330, train_perplexity=4995.494, train_loss=8.516292

Batch 138340, train_perplexity=6033.4443, train_loss=8.705073

Batch 138350, train_perplexity=4666.31, train_loss=8.448124

Batch 138360, train_perplexity=6170.9595, train_loss=8.72761

Batch 138370, train_perplexity=5667.2427, train_loss=8.642458

Batch 138380, train_perplexity=5208.004, train_loss=8.557952

Batch 138390, train_perplexity=6156.828, train_loss=8.725317

Batch 138400, train_perplexity=5281.7, train_loss=8.572003

Batch 138410, train_perplexity=4568.939, train_loss=8.427036

Batch 138420, train_perplexity=5642.8174, train_loss=8.638139

Batch 138430, train_perplexity=5196.449, train_loss=8.555731

Batch 138440, train_perplexity=6428.2915, train_loss=8.768464

Batch 138450, train_perplexity=5266.7866, train_loss=8.569176

Batch 138460, train_perplexity=4831.448, train_loss=8.482902

Batch 138470, train_perplexity=6027.6533, train_loss=8.704113

Batch 138480, train_perplexity=5719.584, train_loss=8.651651

Batch 138490, train_perplexity=5499.883, train_loss=8.612482

Batch 138500, train_perplexity=5690.0312, train_loss=8.646471

Batch 138510, train_perplexity=4592.148, train_loss=8.432103

Batch 138520, train_perplexity=5064.034, train_loss=8.529919

Batch 138530, train_perplexity=5200.762, train_loss=8.5565605

Batch 138540, train_perplexity=5012.0195, train_loss=8.519594

Batch 138550, train_perplexity=4128.299, train_loss=8.325621

Batch 138560, train_perplexity=5782.9775, train_loss=8.662674

Batch 138570, train_perplexity=4521.859, train_loss=8.416678

Batch 138580, train_perplexity=5311.0264, train_loss=8.57754

Batch 138590, train_perplexity=4852.8057, train_loss=8.487312

Batch 138600, train_perplexity=6652.105, train_loss=8.802689

Batch 138610, train_perplexity=6463.2627, train_loss=8.77389

Batch 138620, train_perplexity=4540.808, train_loss=8.42086

Batch 138630, train_perplexity=5320.43, train_loss=8.579309

Batch 138640, train_perplexity=5324.42, train_loss=8.580059

Batch 138650, train_perplexity=6693.59, train_loss=8.808906

Batch 138660, train_perplexity=6120.485, train_loss=8.719397

Batch 138670, train_perplexity=7677.3936, train_loss=8.946035

Batch 138680, train_perplexity=5071.09, train_loss=8.531311

Batch 138690, train_perplexity=6578.9585, train_loss=8.791632

Batch 138700, train_perplexity=5509.1797, train_loss=8.614171

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Batch 138710, train_perplexity=4090.0122, train_loss=8.316303

Batch 138720, train_perplexity=4844.6675, train_loss=8.485634

Batch 138730, train_perplexity=5703.755, train_loss=8.64888

Batch 138740, train_perplexity=5377.3374, train_loss=8.589949

Batch 138750, train_perplexity=5708.7344, train_loss=8.649753

Batch 138760, train_perplexity=6295.5283, train_loss=8.747595

Batch 138770, train_perplexity=6700.021, train_loss=8.809866

Batch 138780, train_perplexity=5687.373, train_loss=8.646004

Batch 138790, train_perplexity=5386.2783, train_loss=8.59161

Batch 138800, train_perplexity=5875.407, train_loss=8.678531

Batch 138810, train_perplexity=6599.482, train_loss=8.794746

Batch 138820, train_perplexity=5987.702, train_loss=8.697463

Batch 138830, train_perplexity=5099.9663, train_loss=8.536989

Batch 138840, train_perplexity=5224.8833, train_loss=8.561188

Batch 138850, train_perplexity=6228.8086, train_loss=8.73694

Batch 138860, train_perplexity=4916.5317, train_loss=8.500359

Batch 138870, train_perplexity=5368.35, train_loss=8.588276

Batch 138880, train_perplexity=4045.4707, train_loss=8.305353

Batch 138890, train_perplexity=5039.5117, train_loss=8.525064

Batch 138900, train_perplexity=4534.274, train_loss=8.41942

Batch 138910, train_perplexity=4525.055, train_loss=8.417385

Batch 138920, train_perplexity=6161.3037, train_loss=8.726044

Batch 138930, train_perplexity=5797.854, train_loss=8.665243

Batch 138940, train_perplexity=6201.5435, train_loss=8.7325535

Batch 138950, train_perplexity=5292.9097, train_loss=8.574123

Batch 138960, train_perplexity=4987.4824, train_loss=8.514687

Batch 138970, train_perplexity=4941.464, train_loss=8.505417

Batch 138980, train_perplexity=5003.5283, train_loss=8.517899

Batch 138990, train_perplexity=6053.357, train_loss=8.708368

Batch 139000, train_perplexity=4667.69, train_loss=8.44842

Batch 139010, train_perplexity=5216.9565, train_loss=8.5596695

Batch 139020, train_perplexity=5734.1665, train_loss=8.654198

Batch 139030, train_perplexity=7412.415, train_loss=8.910912

Batch 139040, train_perplexity=5513.8105, train_loss=8.615011

Batch 139050, train_perplexity=4747.3306, train_loss=8.465338

Batch 139060, train_perplexity=6733.773, train_loss=8.814891

Batch 139070, train_perplexity=6258.951, train_loss=8.741768

Batch 139080, train_perplexity=5017.146, train_loss=8.520617

Batch 139090, train_perplexity=6762.0044, train_loss=8.819075

Batch 139100, train_perplexity=5652.9653, train_loss=8.6399355

Batch 139110, train_perplexity=6957.9424, train_loss=8.847639

Batch 139120, train_perplexity=4765.0474, train_loss=8.469063

Batch 139130, train_perplexity=6033.9277, train_loss=8.705153

Batch 139140, train_perplexity=5664.4004, train_loss=8.641956

Batch 139150, train_perplexity=5082.6714, train_loss=8.533592

Batch 139160, train_perplexity=5016.2656, train_loss=8.520441

Batch 139170, train_perplexity=5275.6743, train_loss=8.570862

Batch 139180, train_perplexity=6488.559, train_loss=8.777796

Batch 139190, train_perplexity=6082.7085, train_loss=8.713205

Batch 139200, train_perplexity=5074.273, train_loss=8.531939

Batch 139210, train_perplexity=5841.2603, train_loss=8.672702

Batch 139220, train_perplexity=6041.6265, train_loss=8.706429

Batch 139230, train_perplexity=5421.8594, train_loss=8.598194

Batch 139240, train_perplexity=6049.352, train_loss=8.707706

Batch 139250, train_perplexity=5373.903, train_loss=8.58931

Batch 139260, train_perplexity=6054.57, train_loss=8.708569

Batch 139270, train_perplexity=6064.243, train_loss=8.710165
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 139280, train_perplexity=5147.7754, train_loss=8.54632

Batch 139290, train_perplexity=4918.905, train_loss=8.500841

Batch 139300, train_perplexity=5659.46, train_loss=8.641084

Batch 139310, train_perplexity=5571.3726, train_loss=8.625397

Batch 139320, train_perplexity=5576.359, train_loss=8.626291

Batch 139330, train_perplexity=5502.8, train_loss=8.613012

Batch 139340, train_perplexity=4557.6416, train_loss=8.424561

Batch 139350, train_perplexity=4338.625, train_loss=8.375313

Batch 139360, train_perplexity=5243.0625, train_loss=8.564661

Batch 139370, train_perplexity=4983.4697, train_loss=8.513882

Batch 139380, train_perplexity=4679.778, train_loss=8.451006

Batch 139390, train_perplexity=4607.9937, train_loss=8.435548

Batch 139400, train_perplexity=5333.6084, train_loss=8.581783

Batch 139410, train_perplexity=6153.805, train_loss=8.724826

Batch 139420, train_perplexity=5773.048, train_loss=8.660955

Batch 139430, train_perplexity=4458.82, train_loss=8.402639

Batch 139440, train_perplexity=4699.1245, train_loss=8.455132

Batch 139450, train_perplexity=6710.982, train_loss=8.811501

Batch 139460, train_perplexity=5277.7124, train_loss=8.571248

Batch 139470, train_perplexity=6013.4883, train_loss=8.70176

Batch 139480, train_perplexity=4589.976, train_loss=8.43163

Batch 139490, train_perplexity=5059.284, train_loss=8.52898

Batch 139500, train_perplexity=5298.369, train_loss=8.575154

Batch 139510, train_perplexity=5755.3525, train_loss=8.657886

Batch 139520, train_perplexity=5322.8613, train_loss=8.579766

Batch 139530, train_perplexity=4987.763, train_loss=8.514743

Batch 139540, train_perplexity=5424.57, train_loss=8.598694

Batch 139550, train_perplexity=6389.4014, train_loss=8.762396

Batch 139560, train_perplexity=5857.3423, train_loss=8.675451

Batch 139570, train_perplexity=4754.8247, train_loss=8.466915

Batch 139580, train_perplexity=5538.58, train_loss=8.6194935

Batch 139590, train_perplexity=5275.2974, train_loss=8.57079

Batch 139600, train_perplexity=5414.4033, train_loss=8.596818

Batch 139610, train_perplexity=5354.1567, train_loss=8.5856285

Batch 139620, train_perplexity=4721.27, train_loss=8.459833

Batch 139630, train_perplexity=5378.548, train_loss=8.590174

Batch 139640, train_perplexity=6092.729, train_loss=8.714851

Batch 139650, train_perplexity=6533.59, train_loss=8.784712

Batch 139660, train_perplexity=6675.7974, train_loss=8.806244

Batch 139670, train_perplexity=6147.652, train_loss=8.723825

Batch 139680, train_perplexity=4971.337, train_loss=8.511444

Batch 139690, train_perplexity=4830.1816, train_loss=8.482639

Batch 139700, train_perplexity=5852.825, train_loss=8.67468

Batch 139710, train_perplexity=4992.5605, train_loss=8.515704

Batch 139720, train_perplexity=5421.4043, train_loss=8.59811

Batch 139730, train_perplexity=5032.6343, train_loss=8.523699

Batch 139740, train_perplexity=4933.515, train_loss=8.503807

Batch 139750, train_perplexity=5223.2095, train_loss=8.560867

Batch 139760, train_perplexity=5140.162, train_loss=8.54484

Batch 139770, train_perplexity=7027.5107, train_loss=8.857588

Batch 139780, train_perplexity=5786.8726, train_loss=8.663347

Batch 139790, train_perplexity=5244.4175, train_loss=8.564919

Batch 139800, train_perplexity=5201.784, train_loss=8.556757

Batch 139810, train_perplexity=6191.8164, train_loss=8.730984

Batch 139820, train_perplexity=5028.4795, train_loss=8.522873

Batch 139830, train_perplexity=5461.029, train_loss=8.605392

Batch 139840, train_perplexity=5365.8833, train_loss=8.587816

Batch 139850, train_perplexity=5511.7495, train_loss=8.614637

Batch 139860, train_perplexity=6225.857, train_loss=8.736466

Batch 139870, train_perplexity=5838.3696, train_loss=8.672207

Batch 139880, train_perplexity=5053.671, train_loss=8.52787

Batch 139890, train_perplexity=4951.918, train_loss=8.50753

Batch 139900, train_perplexity=5354.0137, train_loss=8.585602

Batch 139910, train_perplexity=5092.754, train_loss=8.535574

Batch 139920, train_perplexity=4672.3574, train_loss=8.449419

Batch 139930, train_perplexity=5961.953, train_loss=8.693153

Batch 139940, train_perplexity=5315.5107, train_loss=8.578384

Batch 139950, train_perplexity=5389.4385, train_loss=8.592196

Batch 139960, train_perplexity=4762.231, train_loss=8.468472

Batch 139970, train_perplexity=5308.3833, train_loss=8.577043

Batch 139980, train_perplexity=5505.246, train_loss=8.613457

Batch 139990, train_perplexity=5294.6665, train_loss=8.574455

Batch 140000, train_perplexity=5867.54, train_loss=8.677191

Batch 140010, train_perplexity=6214.7817, train_loss=8.734686

Batch 140020, train_perplexity=6484.217, train_loss=8.777126

Batch 140030, train_perplexity=6399.915, train_loss=8.76404

Batch 140040, train_perplexity=4438.722, train_loss=8.398122

Batch 140050, train_perplexity=5356.026, train_loss=8.585978

Batch 140060, train_perplexity=5281.499, train_loss=8.571965

Batch 140070, train_perplexity=5811.134, train_loss=8.667531

Batch 140080, train_perplexity=4688.207, train_loss=8.4528055

Batch 140090, train_perplexity=5213.0225, train_loss=8.558915

Batch 140100, train_perplexity=5876.108, train_loss=8.67865

Batch 140110, train_perplexity=4511.099, train_loss=8.414296

Batch 140120, train_perplexity=5942.8906, train_loss=8.689951

Batch 140130, train_perplexity=5603.3438, train_loss=8.631119

Batch 140140, train_perplexity=5739.802, train_loss=8.65518

Batch 140150, train_perplexity=4656.2905, train_loss=8.445974

Batch 140160, train_perplexity=4274.678, train_loss=8.360464

Batch 140170, train_perplexity=7148.2734, train_loss=8.874626

Batch 140180, train_perplexity=4662.485, train_loss=8.447304

Batch 140190, train_perplexity=4942.0625, train_loss=8.505538

Batch 140200, train_perplexity=5274.3965, train_loss=8.57062

Batch 140210, train_perplexity=5661.6514, train_loss=8.641471

Batch 140220, train_perplexity=5763.63, train_loss=8.659323

Batch 140230, train_perplexity=5737.678, train_loss=8.65481

Batch 140240, train_perplexity=5441.45, train_loss=8.601801

Batch 140250, train_perplexity=4925.0537, train_loss=8.50209

Batch 140260, train_perplexity=5917.2944, train_loss=8.685635

Batch 140270, train_perplexity=4946.084, train_loss=8.506351

Batch 140280, train_perplexity=5644.427, train_loss=8.638424

Batch 140290, train_perplexity=6072.1597, train_loss=8.71147

Batch 140300, train_perplexity=5853.188, train_loss=8.674742

Batch 140310, train_perplexity=4733.542, train_loss=8.462429

Batch 140320, train_perplexity=5331.4927, train_loss=8.581387

Batch 140330, train_perplexity=4792.273, train_loss=8.47476

Batch 140340, train_perplexity=6149.1646, train_loss=8.7240715

Batch 140350, train_perplexity=5155.3315, train_loss=8.547787

Batch 140360, train_perplexity=7191.1997, train_loss=8.880613

Batch 140370, train_perplexity=5720.506, train_loss=8.651813

Batch 140380, train_perplexity=4471.1597, train_loss=8.405403

Batch 140390, train_perplexity=6411.883, train_loss=8.765908

Batch 140400, train_perplexity=5151.9106, train_loss=8.547123

Batch 140410, train_perplexity=5188.343, train_loss=8.55417

Batch 140420, train_perplexity=5973.193, train_loss=8.695037

Batch 140430, train_perplexity=5157.151, train_loss=8.54814

Batch 140440, train_perplexity=4948.495, train_loss=8.506839

Batch 140450, train_perplexity=5374.948, train_loss=8.589504

Batch 140460, train_perplexity=4771.8916, train_loss=8.470498

Batch 140470, train_perplexity=5799.01, train_loss=8.665442

Batch 140480, train_perplexity=5366.0674, train_loss=8.587851

Batch 140490, train_perplexity=4890.7173, train_loss=8.495094

Batch 140500, train_perplexity=5821.507, train_loss=8.669314

Batch 140510, train_perplexity=5278.468, train_loss=8.571391

Batch 140520, train_perplexity=5093.8125, train_loss=8.535782

Batch 140530, train_perplexity=5958.355, train_loss=8.69255

Batch 140540, train_perplexity=4368.41, train_loss=8.382154

Batch 140550, train_perplexity=5425.9253, train_loss=8.598944

Batch 140560, train_perplexity=5235.468, train_loss=8.563211

Batch 140570, train_perplexity=5070.321, train_loss=8.531159

Batch 140580, train_perplexity=4478.4917, train_loss=8.407042

Batch 140590, train_perplexity=5092.229, train_loss=8.535471
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 140600, train_perplexity=4456.116, train_loss=8.402033

Batch 140610, train_perplexity=6132.31, train_loss=8.721327

Batch 140620, train_perplexity=5198.1543, train_loss=8.556059

Batch 140630, train_perplexity=5755.2153, train_loss=8.657862

Batch 140640, train_perplexity=4913.869, train_loss=8.499817

Batch 140650, train_perplexity=5095.426, train_loss=8.5360985

Batch 140660, train_perplexity=6189.077, train_loss=8.730541

Batch 140670, train_perplexity=4828.938, train_loss=8.482382

Batch 140680, train_perplexity=6337.128, train_loss=8.754181

Batch 140690, train_perplexity=4810.497, train_loss=8.478556

Batch 140700, train_perplexity=6106.632, train_loss=8.717131

Batch 140710, train_perplexity=4771.1226, train_loss=8.470337

Batch 140720, train_perplexity=5864.6313, train_loss=8.676695

Batch 140730, train_perplexity=4821.2485, train_loss=8.480788

Batch 140740, train_perplexity=6019.209, train_loss=8.702711

Batch 140750, train_perplexity=4855.597, train_loss=8.487887

Batch 140760, train_perplexity=5968.8877, train_loss=8.694316

Batch 140770, train_perplexity=5217.265, train_loss=8.559729

Batch 140780, train_perplexity=5441.99, train_loss=8.6019

Batch 140790, train_perplexity=5142.3877, train_loss=8.545273

Batch 140800, train_perplexity=4885.9297, train_loss=8.494115

Batch 140810, train_perplexity=5797.296, train_loss=8.665147

Batch 140820, train_perplexity=6109.696, train_loss=8.717632

Batch 140830, train_perplexity=4449.1816, train_loss=8.4004755

Batch 140840, train_perplexity=5226.159, train_loss=8.561432

Batch 140850, train_perplexity=5860.326, train_loss=8.675961

Batch 140860, train_perplexity=5242.6626, train_loss=8.564585

Batch 140870, train_perplexity=5088.4136, train_loss=8.534721

Batch 140880, train_perplexity=4693.0024, train_loss=8.453828

Batch 140890, train_perplexity=5456.026, train_loss=8.604476

Batch 140900, train_perplexity=5714.503, train_loss=8.650763

Batch 140910, train_perplexity=5414.2227, train_loss=8.596785

Batch 140920, train_perplexity=4974.1826, train_loss=8.512016

Batch 140930, train_perplexity=5047.1978, train_loss=8.526588

Batch 140940, train_perplexity=5513.4424, train_loss=8.614944

Batch 140950, train_perplexity=4819.7085, train_loss=8.480469

Batch 140960, train_perplexity=5403.0195, train_loss=8.594713

Batch 140970, train_perplexity=5731.1157, train_loss=8.653666

Batch 140980, train_perplexity=3854.9836, train_loss=8.257122

Batch 140990, train_perplexity=5086.395, train_loss=8.534325

Batch 141000, train_perplexity=5626.9434, train_loss=8.635322

Batch 141010, train_perplexity=5224.425, train_loss=8.5611

Batch 141020, train_perplexity=5820.6353, train_loss=8.669165

Batch 141030, train_perplexity=5100.939, train_loss=8.53718

Batch 141040, train_perplexity=5275.0205, train_loss=8.570738

Batch 141050, train_perplexity=5605.615, train_loss=8.631524

Batch 141060, train_perplexity=4877.5264, train_loss=8.4923935

Batch 141070, train_perplexity=6364.862, train_loss=8.758548

Batch 141080, train_perplexity=5882.999, train_loss=8.679822

Batch 141090, train_perplexity=5445.697, train_loss=8.602581

Batch 141100, train_perplexity=6041.108, train_loss=8.706343

Batch 141110, train_perplexity=5148.183, train_loss=8.546399

Batch 141120, train_perplexity=6232.065, train_loss=8.737463

Batch 141130, train_perplexity=5386.263, train_loss=8.591607

Batch 141140, train_perplexity=4812.6167, train_loss=8.478996

Batch 141150, train_perplexity=5249.697, train_loss=8.565926

Batch 141160, train_perplexity=6145.0083, train_loss=8.723395

Batch 141170, train_perplexity=5359.6895, train_loss=8.586661

Batch 141180, train_perplexity=5617.0728, train_loss=8.633566

Batch 141190, train_perplexity=6253.5215, train_loss=8.7409

Batch 141200, train_perplexity=5653.111, train_loss=8.639961

Batch 141210, train_perplexity=5338.412, train_loss=8.582684

Batch 141220, train_perplexity=4809.8867, train_loss=8.478429

Batch 141230, train_perplexity=5238.0747, train_loss=8.563709

Batch 141240, train_perplexity=5629.235, train_loss=8.635729

Batch 141250, train_perplexity=5775.3716, train_loss=8.661358

Batch 141260, train_perplexity=4664.9175, train_loss=8.447825

Batch 141270, train_perplexity=6227.1455, train_loss=8.736673

Batch 141280, train_perplexity=4284.4326, train_loss=8.362743

Batch 141290, train_perplexity=4913.269, train_loss=8.499695

Batch 141300, train_perplexity=4333.8735, train_loss=8.374217

Batch 141310, train_perplexity=5834.585, train_loss=8.671558

Batch 141320, train_perplexity=5215.9717, train_loss=8.559481

Batch 141330, train_perplexity=5439.313, train_loss=8.601408

Batch 141340, train_perplexity=5351.2163, train_loss=8.585079

Batch 141350, train_perplexity=6406.999, train_loss=8.765146

Batch 141360, train_perplexity=5611.6113, train_loss=8.632593

Batch 141370, train_perplexity=5456.848, train_loss=8.604627

Batch 141380, train_perplexity=4982.453, train_loss=8.513678

Batch 141390, train_perplexity=4719.645, train_loss=8.459489

Batch 141400, train_perplexity=4684.913, train_loss=8.452103

Batch 141410, train_perplexity=5443.952, train_loss=8.602261

Batch 141420, train_perplexity=4838.5586, train_loss=8.484372

Batch 141430, train_perplexity=5377.7583, train_loss=8.590027

Batch 141440, train_perplexity=5882.769, train_loss=8.679783

Batch 141450, train_perplexity=4993.8984, train_loss=8.515972

Batch 141460, train_perplexity=5872.153, train_loss=8.677977

Batch 141470, train_perplexity=4833.3335, train_loss=8.483292

Batch 141480, train_perplexity=4789.4946, train_loss=8.47418

Batch 141490, train_perplexity=5328.926, train_loss=8.580905

Batch 141500, train_perplexity=5602.585, train_loss=8.630983

Batch 141510, train_perplexity=4762.6216, train_loss=8.468554

Batch 141520, train_perplexity=5195.6465, train_loss=8.555576

Batch 141530, train_perplexity=5021.8994, train_loss=8.521564

Batch 141540, train_perplexity=7101.1055, train_loss=8.868006

Batch 141550, train_perplexity=6057.55, train_loss=8.709061

Batch 141560, train_perplexity=6328.2983, train_loss=8.752787

Batch 141570, train_perplexity=5804.77, train_loss=8.666435

Batch 141580, train_perplexity=4896.2944, train_loss=8.496234

Batch 141590, train_perplexity=4965.1772, train_loss=8.510204

Batch 141600, train_perplexity=5597.997, train_loss=8.630164

Batch 141610, train_perplexity=5896.086, train_loss=8.682044

Batch 141620, train_perplexity=5451.756, train_loss=8.603693

Batch 141630, train_perplexity=5484.421, train_loss=8.609667

Batch 141640, train_perplexity=5647.0435, train_loss=8.638887

Batch 141650, train_perplexity=4862.33, train_loss=8.489273

Batch 141660, train_perplexity=6014.928, train_loss=8.702

Batch 141670, train_perplexity=5006.344, train_loss=8.518461

Batch 141680, train_perplexity=5858.605, train_loss=8.675667

Batch 141690, train_perplexity=5806.298, train_loss=8.666698

Batch 141700, train_perplexity=5715.974, train_loss=8.65102

Batch 141710, train_perplexity=5018.5723, train_loss=8.520901

Batch 141720, train_perplexity=6204.5015, train_loss=8.73303

Batch 141730, train_perplexity=4910.9927, train_loss=8.499231

Batch 141740, train_perplexity=5190.575, train_loss=8.5546

Batch 141750, train_perplexity=4217.526, train_loss=8.347004

Batch 141760, train_perplexity=5290.1943, train_loss=8.57361

Batch 141770, train_perplexity=5256.2095, train_loss=8.567165

Batch 141780, train_perplexity=5644.039, train_loss=8.638355

Batch 141790, train_perplexity=4604.69, train_loss=8.434831

Batch 141800, train_perplexity=4654.488, train_loss=8.445587

Batch 141810, train_perplexity=5102.389, train_loss=8.537464

Batch 141820, train_perplexity=5574.721, train_loss=8.625998

Batch 141830, train_perplexity=5363.4478, train_loss=8.587362

Batch 141840, train_perplexity=6428.7515, train_loss=8.768536

Batch 141850, train_perplexity=4535.1475, train_loss=8.419613

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305032 sentences.
Finished loading
Batch 141860, train_perplexity=4781.3394, train_loss=8.472476

Batch 141870, train_perplexity=6097.6816, train_loss=8.715664

Batch 141880, train_perplexity=5948.595, train_loss=8.69091

Batch 141890, train_perplexity=5826.511, train_loss=8.670174

Batch 141900, train_perplexity=5755.797, train_loss=8.657963

Batch 141910, train_perplexity=4414.9326, train_loss=8.392748

Batch 141920, train_perplexity=5212.6, train_loss=8.558834

Batch 141930, train_perplexity=5151.164, train_loss=8.546978

Batch 141940, train_perplexity=4492.595, train_loss=8.410186

Batch 141950, train_perplexity=5279.4795, train_loss=8.571583

Batch 141960, train_perplexity=5375.912, train_loss=8.589684

Batch 141970, train_perplexity=6190.1455, train_loss=8.730714

Batch 141980, train_perplexity=5565.865, train_loss=8.624408

Batch 141990, train_perplexity=5026.739, train_loss=8.522527

Batch 142000, train_perplexity=5065.425, train_loss=8.530193

Batch 142010, train_perplexity=5501.8765, train_loss=8.612844

Batch 142020, train_perplexity=5017.0405, train_loss=8.520596

Batch 142030, train_perplexity=5140.427, train_loss=8.544891

Batch 142040, train_perplexity=4862.6826, train_loss=8.489346

Batch 142050, train_perplexity=4473.975, train_loss=8.406033

Batch 142060, train_perplexity=5527.4043, train_loss=8.617474

Batch 142070, train_perplexity=4600.178, train_loss=8.43385

Batch 142080, train_perplexity=5056.332, train_loss=8.528397

Batch 142090, train_perplexity=5499.2114, train_loss=8.61236

Batch 142100, train_perplexity=4425.661, train_loss=8.395175

Batch 142110, train_perplexity=5022.498, train_loss=8.521683

Batch 142120, train_perplexity=4611.2334, train_loss=8.436251

Batch 142130, train_perplexity=6075.954, train_loss=8.712094

Batch 142140, train_perplexity=5716.1323, train_loss=8.651048

Batch 142150, train_perplexity=4944.613, train_loss=8.506054

Batch 142160, train_perplexity=6009.1255, train_loss=8.701035

Batch 142170, train_perplexity=5830.0405, train_loss=8.670779

Batch 142180, train_perplexity=5482.34, train_loss=8.609287

Batch 142190, train_perplexity=5357.768, train_loss=8.586303

Batch 142200, train_perplexity=5499.5996, train_loss=8.612431

Batch 142210, train_perplexity=4504.0493, train_loss=8.412732

Batch 142220, train_perplexity=4491.426, train_loss=8.409925

Batch 142230, train_perplexity=6140.392, train_loss=8.722644

Batch 142240, train_perplexity=5126.1123, train_loss=8.542103

Batch 142250, train_perplexity=5419.7505, train_loss=8.597805

Batch 142260, train_perplexity=5875.94, train_loss=8.678621

Batch 142270, train_perplexity=4927.0034, train_loss=8.502486

Batch 142280, train_perplexity=4579.5396, train_loss=8.429354

Batch 142290, train_perplexity=5388.3438, train_loss=8.591993

Batch 142300, train_perplexity=5447.38, train_loss=8.60289

Batch 142310, train_perplexity=5758.372, train_loss=8.65841

Batch 142320, train_perplexity=6025.1016, train_loss=8.70369

Batch 142330, train_perplexity=5276.1274, train_loss=8.570948

Batch 142340, train_perplexity=4422.2144, train_loss=8.394396

Batch 142350, train_perplexity=5492.3247, train_loss=8.611107

Batch 142360, train_perplexity=5331.808, train_loss=8.581446

Batch 142370, train_perplexity=5182.7744, train_loss=8.553096

Batch 142380, train_perplexity=5471.2617, train_loss=8.6072645

Batch 142390, train_perplexity=4894.198, train_loss=8.495806

Batch 142400, train_perplexity=4556.842, train_loss=8.424385

Batch 142410, train_perplexity=4501.3955, train_loss=8.412143

Batch 142420, train_perplexity=5072.0767, train_loss=8.531506

Batch 142430, train_perplexity=4721.851, train_loss=8.459956

Batch 142440, train_perplexity=5796.9307, train_loss=8.665084

Batch 142450, train_perplexity=5759.1406, train_loss=8.658544

Batch 142460, train_perplexity=5081.862, train_loss=8.533433

Batch 142470, train_perplexity=5550.7954, train_loss=8.621696

Batch 142480, train_perplexity=5770.923, train_loss=8.660587

Batch 142490, train_perplexity=5526.4873, train_loss=8.617308

Batch 142500, train_perplexity=5141.7896, train_loss=8.5451565

Batch 142510, train_perplexity=4725.153, train_loss=8.460655

Batch 142520, train_perplexity=5483.1294, train_loss=8.609431

Batch 142530, train_perplexity=5535.1743, train_loss=8.618878

Batch 142540, train_perplexity=5681.35, train_loss=8.644944

Batch 142550, train_perplexity=5300.613, train_loss=8.575578

Batch 142560, train_perplexity=5097.146, train_loss=8.536436

Batch 142570, train_perplexity=4637.0137, train_loss=8.441826

Batch 142580, train_perplexity=6096.1235, train_loss=8.715408

Batch 142590, train_perplexity=4322.63, train_loss=8.371619

Batch 142600, train_perplexity=5045.672, train_loss=8.526286

Batch 142610, train_perplexity=5139.1475, train_loss=8.544642

Batch 142620, train_perplexity=5637.374, train_loss=8.637174

Batch 142630, train_perplexity=5531.2695, train_loss=8.618173

Batch 142640, train_perplexity=4863.4897, train_loss=8.4895115

Batch 142650, train_perplexity=5193.006, train_loss=8.555068

Batch 142660, train_perplexity=6188.481, train_loss=8.730445

Batch 142670, train_perplexity=5436.8545, train_loss=8.600956

Batch 142680, train_perplexity=5143.6143, train_loss=8.545511

Batch 142690, train_perplexity=5841.622, train_loss=8.672764

Batch 142700, train_perplexity=6743.509, train_loss=8.816336

Batch 142710, train_perplexity=4568.6646, train_loss=8.426976

Batch 142720, train_perplexity=5659.395, train_loss=8.641072

Batch 142730, train_perplexity=5085.503, train_loss=8.534149

Batch 142740, train_perplexity=5648.853, train_loss=8.639208

Batch 142750, train_perplexity=5036.086, train_loss=8.5243845

Batch 142760, train_perplexity=6366.003, train_loss=8.758727

Batch 142770, train_perplexity=4962.261, train_loss=8.509617

Batch 142780, train_perplexity=5426.1113, train_loss=8.598978

Batch 142790, train_perplexity=4829.928, train_loss=8.482587

Batch 142800, train_perplexity=4382.907, train_loss=8.385468

Batch 142810, train_perplexity=5294.717, train_loss=8.574465

Batch 142820, train_perplexity=5582.8613, train_loss=8.627457

Batch 142830, train_perplexity=5682.51, train_loss=8.645148

Batch 142840, train_perplexity=5511.497, train_loss=8.614592

Batch 142850, train_perplexity=5358.739, train_loss=8.586484

Batch 142860, train_perplexity=5775.7075, train_loss=8.661416

Batch 142870, train_perplexity=5637.4814, train_loss=8.637193

Batch 142880, train_perplexity=5104.4624, train_loss=8.53787

Batch 142890, train_perplexity=6111.514, train_loss=8.71793

Batch 142900, train_perplexity=4891.184, train_loss=8.49519

Batch 142910, train_perplexity=6549.8853, train_loss=8.787203

Batch 142920, train_perplexity=4404.7344, train_loss=8.390435

Batch 142930, train_perplexity=4451.215, train_loss=8.400932

Batch 142940, train_perplexity=6452.2637, train_loss=8.772186

Batch 142950, train_perplexity=5393.742, train_loss=8.592995

Batch 142960, train_perplexity=5040.6025, train_loss=8.525281

Batch 142970, train_perplexity=6413.8584, train_loss=8.766216

Batch 142980, train_perplexity=5592.373, train_loss=8.629159

Batch 142990, train_perplexity=4891.9395, train_loss=8.495344

Batch 143000, train_perplexity=5039.584, train_loss=8.525079

Batch 143010, train_perplexity=5665.2163, train_loss=8.6421

Batch 143020, train_perplexity=5150.781, train_loss=8.546904

Batch 143030, train_perplexity=5433.3716, train_loss=8.600315

Batch 143040, train_perplexity=5858.963, train_loss=8.675728

Batch 143050, train_perplexity=5384.4756, train_loss=8.591275

Batch 143060, train_perplexity=5137.4077, train_loss=8.544304

Batch 143070, train_perplexity=5733.106, train_loss=8.654013

Batch 143080, train_perplexity=5635.5195, train_loss=8.636845

Batch 143090, train_perplexity=4264.352, train_loss=8.358046

Batch 143100, train_perplexity=5672.7744, train_loss=8.643434

Batch 143110, train_perplexity=5866.175, train_loss=8.676958

Batch 143120, train_perplexity=6303.7227, train_loss=8.748896

Batch 143130, train_perplexity=5593.306, train_loss=8.629326

Batch 143140, train_perplexity=4828.091, train_loss=8.482206

Batch 143150, train_perplexity=6126.319, train_loss=8.720349

Batch 143160, train_perplexity=4558.3193, train_loss=8.424709
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 143170, train_perplexity=5228.672, train_loss=8.561913

Batch 143180, train_perplexity=5316.008, train_loss=8.578478

Batch 143190, train_perplexity=4185.495, train_loss=8.33938

Batch 143200, train_perplexity=5626.0146, train_loss=8.635157

Batch 143210, train_perplexity=5078.514, train_loss=8.532774

Batch 143220, train_perplexity=5007.82, train_loss=8.518756

Batch 143230, train_perplexity=5431.729, train_loss=8.600013

Batch 143240, train_perplexity=6094.1235, train_loss=8.71508

Batch 143250, train_perplexity=5665.1836, train_loss=8.642095

Batch 143260, train_perplexity=5932.336, train_loss=8.688173

Batch 143270, train_perplexity=6800.5483, train_loss=8.824759

Batch 143280, train_perplexity=6248.6807, train_loss=8.740126

Batch 143290, train_perplexity=5148.1387, train_loss=8.546391

Batch 143300, train_perplexity=4262.8643, train_loss=8.357697

Batch 143310, train_perplexity=4376.9424, train_loss=8.384106

Batch 143320, train_perplexity=5643.4795, train_loss=8.638256

Batch 143330, train_perplexity=5532.8364, train_loss=8.618456

Batch 143340, train_perplexity=5844.8154, train_loss=8.67331

Batch 143350, train_perplexity=5861.3213, train_loss=8.67613

Batch 143360, train_perplexity=6056.152, train_loss=8.70883

Batch 143370, train_perplexity=6738.597, train_loss=8.815607

Batch 143380, train_perplexity=6269.7524, train_loss=8.743492

Batch 143390, train_perplexity=5412.328, train_loss=8.596435

Batch 143400, train_perplexity=4843.771, train_loss=8.485449

Batch 143410, train_perplexity=5414.3726, train_loss=8.596812

Batch 143420, train_perplexity=4262.872, train_loss=8.357698

Batch 143430, train_perplexity=4977.3047, train_loss=8.512644

Batch 143440, train_perplexity=5505.9336, train_loss=8.613582

Batch 143450, train_perplexity=5349.012, train_loss=8.584667

Batch 143460, train_perplexity=5182.9478, train_loss=8.553129

Batch 143470, train_perplexity=4984.9624, train_loss=8.514181

Batch 143480, train_perplexity=4700.142, train_loss=8.455348

Batch 143490, train_perplexity=4990.166, train_loss=8.515224

Batch 143500, train_perplexity=4940.8887, train_loss=8.5053005

Batch 143510, train_perplexity=5938.0923, train_loss=8.689143

Batch 143520, train_perplexity=5376.6196, train_loss=8.589815

Batch 143530, train_perplexity=4703.339, train_loss=8.456028

Batch 143540, train_perplexity=5335.9893, train_loss=8.58223

Batch 143550, train_perplexity=5104.5063, train_loss=8.537879

Batch 143560, train_perplexity=5911.7554, train_loss=8.684698

Batch 143570, train_perplexity=6592.336, train_loss=8.793663

Batch 143580, train_perplexity=5407.1226, train_loss=8.595472

Batch 143590, train_perplexity=6000.6846, train_loss=8.699629

Batch 143600, train_perplexity=5670.513, train_loss=8.643035

Batch 143610, train_perplexity=5761.5967, train_loss=8.65897

Batch 143620, train_perplexity=4150.7773, train_loss=8.331051

Batch 143630, train_perplexity=4791.2217, train_loss=8.474541

Batch 143640, train_perplexity=5884.8003, train_loss=8.680128

Batch 143650, train_perplexity=5225.048, train_loss=8.561219

Batch 143660, train_perplexity=5241.2427, train_loss=8.564314

Batch 143670, train_perplexity=4976.251, train_loss=8.512432

Batch 143680, train_perplexity=5047.6743, train_loss=8.526683

Batch 143690, train_perplexity=4853.8423, train_loss=8.487526

Batch 143700, train_perplexity=4494.545, train_loss=8.41062

Batch 143710, train_perplexity=5359.521, train_loss=8.58663

Batch 143720, train_perplexity=5095.6396, train_loss=8.53614

Batch 143730, train_perplexity=4812.4424, train_loss=8.47896

Batch 143740, train_perplexity=5953.4644, train_loss=8.691729

Batch 143750, train_perplexity=6208.372, train_loss=8.733654

Batch 143760, train_perplexity=6563.855, train_loss=8.789333

Batch 143770, train_perplexity=6073.6655, train_loss=8.711718

Batch 143780, train_perplexity=5640.461, train_loss=8.637721

Batch 143790, train_perplexity=4538.0547, train_loss=8.420254

Batch 143800, train_perplexity=6164.454, train_loss=8.726555

Batch 143810, train_perplexity=5077.381, train_loss=8.532551

Batch 143820, train_perplexity=5670.746, train_loss=8.643076

Batch 143830, train_perplexity=4729.8687, train_loss=8.461653

Batch 143840, train_perplexity=4367.123, train_loss=8.38186

Batch 143850, train_perplexity=5368.4937, train_loss=8.588303

Batch 143860, train_perplexity=5913.4697, train_loss=8.684988

Batch 143870, train_perplexity=5495.992, train_loss=8.611774

Batch 143880, train_perplexity=5105.9473, train_loss=8.538161

Batch 143890, train_perplexity=4987.378, train_loss=8.514666

Batch 143900, train_perplexity=6084.0776, train_loss=8.71343

Batch 143910, train_perplexity=5730.99, train_loss=8.653644

Batch 143920, train_perplexity=8504.765, train_loss=9.048382

Batch 143930, train_perplexity=4956.827, train_loss=8.508521

Batch 143940, train_perplexity=4943.8726, train_loss=8.505904

Batch 143950, train_perplexity=5920.529, train_loss=8.686181

Batch 143960, train_perplexity=4712.2197, train_loss=8.457914

Batch 143970, train_perplexity=4860.4897, train_loss=8.488894

Batch 143980, train_perplexity=5858.2476, train_loss=8.675606

Batch 143990, train_perplexity=4906.751, train_loss=8.498367

Batch 144000, train_perplexity=7140.0977, train_loss=8.873482

Batch 144010, train_perplexity=5451.777, train_loss=8.603697

Batch 144020, train_perplexity=5647.178, train_loss=8.638911

Batch 144030, train_perplexity=4830.177, train_loss=8.482638

Batch 144040, train_perplexity=4995.88, train_loss=8.516369

Batch 144050, train_perplexity=4780.491, train_loss=8.472299

Batch 144060, train_perplexity=5110.7266, train_loss=8.539097

Batch 144070, train_perplexity=5114.1196, train_loss=8.539761

Batch 144080, train_perplexity=4908.291, train_loss=8.498681

Batch 144090, train_perplexity=5134.5815, train_loss=8.543754

Batch 144100, train_perplexity=5277.275, train_loss=8.571165

Batch 144110, train_perplexity=5477.6885, train_loss=8.6084385

Batch 144120, train_perplexity=6324.4126, train_loss=8.752172

Batch 144130, train_perplexity=5663.104, train_loss=8.641727

Batch 144140, train_perplexity=5473.5947, train_loss=8.607691

Batch 144150, train_perplexity=5900.102, train_loss=8.682725

Batch 144160, train_perplexity=5509.311, train_loss=8.614195

Batch 144170, train_perplexity=5134.3906, train_loss=8.543716

Batch 144180, train_perplexity=5655.931, train_loss=8.64046

Batch 144190, train_perplexity=6215.754, train_loss=8.734842

Batch 144200, train_perplexity=5109.7905, train_loss=8.538914

Batch 144210, train_perplexity=5848.908, train_loss=8.67401

Batch 144220, train_perplexity=5003.1606, train_loss=8.517825

Batch 144230, train_perplexity=4855.88, train_loss=8.487946

Batch 144240, train_perplexity=4404.6543, train_loss=8.390417

Batch 144250, train_perplexity=5529.5503, train_loss=8.617862

Batch 144260, train_perplexity=4867.652, train_loss=8.490367

Batch 144270, train_perplexity=5175.894, train_loss=8.551767

Batch 144280, train_perplexity=5405.7715, train_loss=8.595222

Batch 144290, train_perplexity=4814.5815, train_loss=8.479404

Batch 144300, train_perplexity=5458.5347, train_loss=8.604936

Batch 144310, train_perplexity=5747.3936, train_loss=8.656502

Batch 144320, train_perplexity=5169.451, train_loss=8.550522

Batch 144330, train_perplexity=5823.9277, train_loss=8.66973

Batch 144340, train_perplexity=5134.42, train_loss=8.543722

Batch 144350, train_perplexity=5334.234, train_loss=8.581901

Batch 144360, train_perplexity=6811.7583, train_loss=8.826406

Batch 144370, train_perplexity=4802.095, train_loss=8.476808

Batch 144380, train_perplexity=5033.0947, train_loss=8.52379

Batch 144390, train_perplexity=4730.6626, train_loss=8.461821

Batch 144400, train_perplexity=4613.6704, train_loss=8.436779

Batch 144410, train_perplexity=4396.408, train_loss=8.388543

Batch 144420, train_perplexity=5115.3247, train_loss=8.539996

Batch 144430, train_perplexity=6081.3857, train_loss=8.712988

Batch 144440, train_perplexity=3981.5476, train_loss=8.289426

Batch 144450, train_perplexity=5178.891, train_loss=8.552346

Batch 144460, train_perplexity=6011.648, train_loss=8.701454

Batch 144470, train_perplexity=4185.7666, train_loss=8.339445

Batch 144480, train_perplexity=5257.7134, train_loss=8.5674515
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 144490, train_perplexity=5800.553, train_loss=8.665709

Batch 144500, train_perplexity=5762.8276, train_loss=8.6591835

Batch 144510, train_perplexity=4867.4985, train_loss=8.490335

Batch 144520, train_perplexity=5816.8174, train_loss=8.668509

Batch 144530, train_perplexity=5500.617, train_loss=8.612616

Batch 144540, train_perplexity=6160.1055, train_loss=8.725849

Batch 144550, train_perplexity=5269.148, train_loss=8.569624

Batch 144560, train_perplexity=4289.8906, train_loss=8.364017

Batch 144570, train_perplexity=4965.348, train_loss=8.510239

Batch 144580, train_perplexity=4948.745, train_loss=8.506889

Batch 144590, train_perplexity=5468.8984, train_loss=8.6068325

Batch 144600, train_perplexity=4638.4688, train_loss=8.44214

Batch 144610, train_perplexity=5515.43, train_loss=8.615305

Batch 144620, train_perplexity=5187.388, train_loss=8.553986

Batch 144630, train_perplexity=4845.1895, train_loss=8.485742

Batch 144640, train_perplexity=5386.16, train_loss=8.591588

Batch 144650, train_perplexity=4822.8906, train_loss=8.481129

Batch 144660, train_perplexity=5395.2085, train_loss=8.5932665

Batch 144670, train_perplexity=5164.11, train_loss=8.549488

Batch 144680, train_perplexity=4861.403, train_loss=8.489082

Batch 144690, train_perplexity=5872.388, train_loss=8.678017

Batch 144700, train_perplexity=5454.491, train_loss=8.604195

Batch 144710, train_perplexity=4356.1587, train_loss=8.379346

Batch 144720, train_perplexity=5428.896, train_loss=8.599491

Batch 144730, train_perplexity=4506.2964, train_loss=8.413231

Batch 144740, train_perplexity=5702.156, train_loss=8.6486

Batch 144750, train_perplexity=5149.553, train_loss=8.546665

Batch 144760, train_perplexity=5282.466, train_loss=8.572148

Batch 144770, train_perplexity=5449.7285, train_loss=8.603321

Batch 144780, train_perplexity=5376.23, train_loss=8.589743

Batch 144790, train_perplexity=4666.2256, train_loss=8.448106

Batch 144800, train_perplexity=4751.262, train_loss=8.466166

Batch 144810, train_perplexity=5224.405, train_loss=8.561096

Batch 144820, train_perplexity=5431.5786, train_loss=8.599985

Batch 144830, train_perplexity=5205.248, train_loss=8.557423

Batch 144840, train_perplexity=5147.069, train_loss=8.546183

Batch 144850, train_perplexity=4390.412, train_loss=8.387178

Batch 144860, train_perplexity=5022.1294, train_loss=8.521609

Batch 144870, train_perplexity=5700.6772, train_loss=8.64834

Batch 144880, train_perplexity=4618.905, train_loss=8.437913

Batch 144890, train_perplexity=5431.7393, train_loss=8.600015

Batch 144900, train_perplexity=7157.428, train_loss=8.875906

Batch 144910, train_perplexity=4979.878, train_loss=8.513161

Batch 144920, train_perplexity=5901.6836, train_loss=8.682993

Batch 144930, train_perplexity=5585.3374, train_loss=8.6279

Batch 144940, train_perplexity=5251.239, train_loss=8.566219

Batch 144950, train_perplexity=5047.708, train_loss=8.52669

Batch 144960, train_perplexity=5610.391, train_loss=8.632376

Batch 144970, train_perplexity=5143.4375, train_loss=8.545477

Batch 144980, train_perplexity=6333.454, train_loss=8.753601

Batch 144990, train_perplexity=5139.25, train_loss=8.544662

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Batch 145000, train_perplexity=5581.328, train_loss=8.627182

Batch 145010, train_perplexity=5149.1797, train_loss=8.546593

Batch 145020, train_perplexity=5322.755, train_loss=8.579746

Batch 145030, train_perplexity=4779.8076, train_loss=8.472156

Batch 145040, train_perplexity=5835.03, train_loss=8.671635

Batch 145050, train_perplexity=5354.1465, train_loss=8.585627

Batch 145060, train_perplexity=4947.0137, train_loss=8.506539

Batch 145070, train_perplexity=5462.133, train_loss=8.605595

Batch 145080, train_perplexity=5071.6753, train_loss=8.531426

Batch 145090, train_perplexity=5128.87, train_loss=8.542641

Batch 145100, train_perplexity=5601.89, train_loss=8.630859

Batch 145110, train_perplexity=5011.183, train_loss=8.519427

Batch 145120, train_perplexity=6177.8076, train_loss=8.728719

Batch 145130, train_perplexity=5938.1094, train_loss=8.689146

Batch 145140, train_perplexity=4311.485, train_loss=8.369038

Batch 145150, train_perplexity=3928.4604, train_loss=8.276003

Batch 145160, train_perplexity=5500.7744, train_loss=8.612644

Batch 145170, train_perplexity=5091.4766, train_loss=8.535323

Batch 145180, train_perplexity=6600.7847, train_loss=8.794944

Batch 145190, train_perplexity=4741.7197, train_loss=8.464155

Batch 145200, train_perplexity=5824.583, train_loss=8.669843

Batch 145210, train_perplexity=4309.232, train_loss=8.368515

Batch 145220, train_perplexity=5192.867, train_loss=8.555041

Batch 145230, train_perplexity=4843.808, train_loss=8.485456

Batch 145240, train_perplexity=4799.06, train_loss=8.476175

Batch 145250, train_perplexity=4630.4204, train_loss=8.440403

Batch 145260, train_perplexity=5065.386, train_loss=8.530186

Batch 145270, train_perplexity=5560.4272, train_loss=8.62343

Batch 145280, train_perplexity=5248.4453, train_loss=8.565687

Batch 145290, train_perplexity=5830.2905, train_loss=8.670822

Batch 145300, train_perplexity=5768.5127, train_loss=8.66017

Batch 145310, train_perplexity=6830.317, train_loss=8.829126

Batch 145320, train_perplexity=4941.5156, train_loss=8.505427

Batch 145330, train_perplexity=6606.169, train_loss=8.795759

Batch 145340, train_perplexity=4762.8125, train_loss=8.468594

Batch 145350, train_perplexity=4664.7886, train_loss=8.447798

Batch 145360, train_perplexity=5092.1177, train_loss=8.535449

Batch 145370, train_perplexity=4937.168, train_loss=8.504547

Batch 145380, train_perplexity=4795.4316, train_loss=8.475419

Batch 145390, train_perplexity=4493.328, train_loss=8.410349

Batch 145400, train_perplexity=5951.9883, train_loss=8.691481

Batch 145410, train_perplexity=6074.755, train_loss=8.711897

Batch 145420, train_perplexity=5739.9824, train_loss=8.655211

Batch 145430, train_perplexity=5426.515, train_loss=8.599052

Batch 145440, train_perplexity=5936.365, train_loss=8.688852

Batch 145450, train_perplexity=4720.329, train_loss=8.459634

Batch 145460, train_perplexity=5660.874, train_loss=8.641334

Batch 145470, train_perplexity=5501.8867, train_loss=8.612846

Batch 145480, train_perplexity=5708.582, train_loss=8.649726

Batch 145490, train_perplexity=5418.851, train_loss=8.597639

Batch 145500, train_perplexity=5005.7524, train_loss=8.518343

Batch 145510, train_perplexity=4885.6177, train_loss=8.494051

Batch 145520, train_perplexity=4851.7876, train_loss=8.4871025

Batch 145530, train_perplexity=5916.042, train_loss=8.685423

Batch 145540, train_perplexity=4823.769, train_loss=8.481311

Batch 145550, train_perplexity=5164.3467, train_loss=8.549534

Batch 145560, train_perplexity=5720.473, train_loss=8.651807

Batch 145570, train_perplexity=4835.583, train_loss=8.483757

Batch 145580, train_perplexity=5249.2964, train_loss=8.565849

Batch 145590, train_perplexity=4965.509, train_loss=8.510271

Batch 145600, train_perplexity=5022.5894, train_loss=8.521701

Batch 145610, train_perplexity=4801.4355, train_loss=8.47667

Batch 145620, train_perplexity=6402.363, train_loss=8.764422

Batch 145630, train_perplexity=6200.408, train_loss=8.73237

Batch 145640, train_perplexity=4454.285, train_loss=8.401622

Batch 145650, train_perplexity=5279.288, train_loss=8.571547

Batch 145660, train_perplexity=5241.3574, train_loss=8.564336

Batch 145670, train_perplexity=5775.2065, train_loss=8.661329

Batch 145680, train_perplexity=5311.1074, train_loss=8.577556

Batch 145690, train_perplexity=5528.2583, train_loss=8.617628

Batch 145700, train_perplexity=4995.918, train_loss=8.5163765

Batch 145710, train_perplexity=4868.2227, train_loss=8.490484

Batch 145720, train_perplexity=5401.134, train_loss=8.594364

Batch 145730, train_perplexity=5351.094, train_loss=8.585056

Batch 145740, train_perplexity=5657.6143, train_loss=8.640758
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 145750, train_perplexity=4956.6094, train_loss=8.508477

Batch 145760, train_perplexity=5349.3745, train_loss=8.584735

Batch 145770, train_perplexity=5283.882, train_loss=8.572416

Batch 145780, train_perplexity=5164.2676, train_loss=8.549519

Batch 145790, train_perplexity=6431.695, train_loss=8.768993

Batch 145800, train_perplexity=5411.146, train_loss=8.596216

Batch 145810, train_perplexity=5254.42, train_loss=8.566825

Batch 145820, train_perplexity=5426.8877, train_loss=8.599121

Batch 145830, train_perplexity=5426.122, train_loss=8.59898

Batch 145840, train_perplexity=5475.4165, train_loss=8.608024

Batch 145850, train_perplexity=3963.5938, train_loss=8.284906

Batch 145860, train_perplexity=4471.6206, train_loss=8.405506

Batch 145870, train_perplexity=4937.309, train_loss=8.504576

Batch 145880, train_perplexity=4591.329, train_loss=8.431925

Batch 145890, train_perplexity=5433.7393, train_loss=8.600383

Batch 145900, train_perplexity=4046.744, train_loss=8.305668

Batch 145910, train_perplexity=5793.88, train_loss=8.664557

Batch 145920, train_perplexity=4250.6772, train_loss=8.354834

Batch 145930, train_perplexity=5315.217, train_loss=8.578329

Batch 145940, train_perplexity=5761.761, train_loss=8.6589985

Batch 145950, train_perplexity=5493.6133, train_loss=8.611341

Batch 145960, train_perplexity=5733.0894, train_loss=8.65401

Batch 145970, train_perplexity=4240.07, train_loss=8.352335

Batch 145980, train_perplexity=5863.602, train_loss=8.676519

Batch 145990, train_perplexity=6406.1924, train_loss=8.76502

Batch 146000, train_perplexity=5367.828, train_loss=8.588179

Batch 146010, train_perplexity=4169.38, train_loss=8.335523

Batch 146020, train_perplexity=6065.8975, train_loss=8.710438

Batch 146030, train_perplexity=5512.8745, train_loss=8.614841

Batch 146040, train_perplexity=6271.893, train_loss=8.743834

Batch 146050, train_perplexity=6039.691, train_loss=8.706108

Batch 146060, train_perplexity=4564.057, train_loss=8.425967

Batch 146070, train_perplexity=5453.503, train_loss=8.604013

Batch 146080, train_perplexity=4576.636, train_loss=8.4287195

Batch 146090, train_perplexity=5047.607, train_loss=8.5266695

Batch 146100, train_perplexity=4534.8447, train_loss=8.419546

Batch 146110, train_perplexity=5278.966, train_loss=8.5714855

Batch 146120, train_perplexity=6015.364, train_loss=8.702072

Batch 146130, train_perplexity=6427.617, train_loss=8.768359

Batch 146140, train_perplexity=5685.5835, train_loss=8.645689

Batch 146150, train_perplexity=4405.957, train_loss=8.390713

Batch 146160, train_perplexity=4717.2017, train_loss=8.458971

Batch 146170, train_perplexity=4910.1494, train_loss=8.49906

Batch 146180, train_perplexity=5508.024, train_loss=8.613961

Batch 146190, train_perplexity=6111.5435, train_loss=8.717935

Batch 146200, train_perplexity=5662.5103, train_loss=8.641623

Batch 146210, train_perplexity=5665.6216, train_loss=8.642172

Batch 146220, train_perplexity=5452.577, train_loss=8.603844

Batch 146230, train_perplexity=5551.8223, train_loss=8.6218815

Batch 146240, train_perplexity=4755.387, train_loss=8.467033

Batch 146250, train_perplexity=4903.107, train_loss=8.497624

Batch 146260, train_perplexity=5005.294, train_loss=8.518251

Batch 146270, train_perplexity=4751.724, train_loss=8.466263

Batch 146280, train_perplexity=4938.081, train_loss=8.504732

Batch 146290, train_perplexity=6595.776, train_loss=8.794185

Batch 146300, train_perplexity=5770.4937, train_loss=8.660513

Batch 146310, train_perplexity=4912.037, train_loss=8.499444

Batch 146320, train_perplexity=5010.146, train_loss=8.51922

Batch 146330, train_perplexity=5035.606, train_loss=8.524289

Batch 146340, train_perplexity=6277.5303, train_loss=8.744732

Batch 146350, train_perplexity=5223.3984, train_loss=8.560904

Batch 146360, train_perplexity=4976.7637, train_loss=8.512535

Batch 146370, train_perplexity=4529.3213, train_loss=8.418327

Batch 146380, train_perplexity=5935.8784, train_loss=8.68877

Batch 146390, train_perplexity=4556.0293, train_loss=8.424207

Batch 146400, train_perplexity=5329.8916, train_loss=8.581086

Batch 146410, train_perplexity=5044.6426, train_loss=8.526082

Batch 146420, train_perplexity=6299.4077, train_loss=8.748211

Batch 146430, train_perplexity=5800.868, train_loss=8.665763

Batch 146440, train_perplexity=4249.437, train_loss=8.354542

Batch 146450, train_perplexity=4762.2036, train_loss=8.468466

Batch 146460, train_perplexity=5705.365, train_loss=8.649162

Batch 146470, train_perplexity=4544.9585, train_loss=8.421774

Batch 146480, train_perplexity=5482.6533, train_loss=8.6093445

Batch 146490, train_perplexity=4177.169, train_loss=8.337389

Batch 146500, train_perplexity=4559.6846, train_loss=8.425009

Batch 146510, train_perplexity=5461.9976, train_loss=8.60557

Batch 146520, train_perplexity=5902.0776, train_loss=8.68306

Batch 146530, train_perplexity=5671.1895, train_loss=8.643154

Batch 146540, train_perplexity=5411.724, train_loss=8.596323

Batch 146550, train_perplexity=4898.2466, train_loss=8.496633

Batch 146560, train_perplexity=5110.6875, train_loss=8.539089

Batch 146570, train_perplexity=5290.1543, train_loss=8.573603

Batch 146580, train_perplexity=6023.3833, train_loss=8.703404

Batch 146590, train_perplexity=6133.843, train_loss=8.721577

Batch 146600, train_perplexity=5700.884, train_loss=8.648376

Batch 146610, train_perplexity=4741.8735, train_loss=8.464188

Batch 146620, train_perplexity=5091.7583, train_loss=8.535378

Batch 146630, train_perplexity=6035.695, train_loss=8.705446

Batch 146640, train_perplexity=5260.166, train_loss=8.567918

Batch 146650, train_perplexity=5432.864, train_loss=8.600222

Batch 146660, train_perplexity=4181.7246, train_loss=8.338479

Batch 146670, train_perplexity=4682.8228, train_loss=8.451656

Batch 146680, train_perplexity=5139.5835, train_loss=8.544727

Batch 146690, train_perplexity=4629.908, train_loss=8.440292

Batch 146700, train_perplexity=5745.1577, train_loss=8.656113

Batch 146710, train_perplexity=5863.412, train_loss=8.676487

Batch 146720, train_perplexity=6865.1245, train_loss=8.834209

Batch 146730, train_perplexity=4938.1, train_loss=8.504736

Batch 146740, train_perplexity=5275.161, train_loss=8.570765

Batch 146750, train_perplexity=6599.96, train_loss=8.794819

Batch 146760, train_perplexity=5702.2754, train_loss=8.648621

Batch 146770, train_perplexity=5242.0474, train_loss=8.564467

Batch 146780, train_perplexity=5514.773, train_loss=8.615186

Batch 146790, train_perplexity=4698.363, train_loss=8.454969

Batch 146800, train_perplexity=4715.7534, train_loss=8.458664

Batch 146810, train_perplexity=5734.954, train_loss=8.654335

Batch 146820, train_perplexity=4460.5977, train_loss=8.403038

Batch 146830, train_perplexity=5396.7573, train_loss=8.593554

Batch 146840, train_perplexity=5417.8022, train_loss=8.5974455

Batch 146850, train_perplexity=5495.002, train_loss=8.611594

Batch 146860, train_perplexity=5119.9565, train_loss=8.540901

Batch 146870, train_perplexity=4853.912, train_loss=8.48754

Batch 146880, train_perplexity=5441.0093, train_loss=8.60172

Batch 146890, train_perplexity=5427.09, train_loss=8.599158

Batch 146900, train_perplexity=5425.982, train_loss=8.598954

Batch 146910, train_perplexity=5314.406, train_loss=8.5781765

Batch 146920, train_perplexity=4529.1353, train_loss=8.418286

Batch 146930, train_perplexity=5606.294, train_loss=8.631645

Batch 146940, train_perplexity=5974.019, train_loss=8.695175

Batch 146950, train_perplexity=5278.2715, train_loss=8.571354

Batch 146960, train_perplexity=5113.9297, train_loss=8.539723

Batch 146970, train_perplexity=6166.9004, train_loss=8.726952

Batch 146980, train_perplexity=4479.175, train_loss=8.407194

Batch 146990, train_perplexity=5577.7627, train_loss=8.626543

Batch 147000, train_perplexity=5035.4233, train_loss=8.524253

Batch 147010, train_perplexity=4636.6646, train_loss=8.441751

Batch 147020, train_perplexity=4865.4473, train_loss=8.489914

Batch 147030, train_perplexity=5182.7896, train_loss=8.553099

Batch 147040, train_perplexity=5554.094, train_loss=8.622291

Batch 147050, train_perplexity=6078.0635, train_loss=8.712441

Batch 147060, train_perplexity=5726.816, train_loss=8.652915
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 147070, train_perplexity=5363.018, train_loss=8.587282

Batch 147080, train_perplexity=5854.299, train_loss=8.674932

Batch 147090, train_perplexity=5461.461, train_loss=8.605472

Batch 147100, train_perplexity=5053.112, train_loss=8.52776

Batch 147110, train_perplexity=5254.2046, train_loss=8.566784

Batch 147120, train_perplexity=5059.752, train_loss=8.529073

Batch 147130, train_perplexity=5353.2275, train_loss=8.585455

Batch 147140, train_perplexity=5911.2256, train_loss=8.684608

Batch 147150, train_perplexity=5395.821, train_loss=8.59338

Batch 147160, train_perplexity=4627.511, train_loss=8.4397745

Batch 147170, train_perplexity=5515.4673, train_loss=8.615312

Batch 147180, train_perplexity=4994.108, train_loss=8.516014

Batch 147190, train_perplexity=4763.857, train_loss=8.468813

Batch 147200, train_perplexity=5215.161, train_loss=8.559325

Batch 147210, train_perplexity=5248.13, train_loss=8.565627

Batch 147220, train_perplexity=5389.351, train_loss=8.59218

Batch 147230, train_perplexity=6051.8506, train_loss=8.708119

Batch 147240, train_perplexity=6165.23, train_loss=8.726681

Batch 147250, train_perplexity=4668.4736, train_loss=8.448587

Batch 147260, train_perplexity=7139.335, train_loss=8.873375

Batch 147270, train_perplexity=5522.5464, train_loss=8.616594

Batch 147280, train_perplexity=5560.979, train_loss=8.623529

Batch 147290, train_perplexity=5378.804, train_loss=8.590221

Batch 147300, train_perplexity=5513.3896, train_loss=8.614935

Batch 147310, train_perplexity=6537.3545, train_loss=8.785288

Batch 147320, train_perplexity=4376.091, train_loss=8.383911

Batch 147330, train_perplexity=4435.2314, train_loss=8.397335

Batch 147340, train_perplexity=4568.072, train_loss=8.4268465

Batch 147350, train_perplexity=6133.9014, train_loss=8.721586

Batch 147360, train_perplexity=5062.3247, train_loss=8.529581

Batch 147370, train_perplexity=4702.8994, train_loss=8.455935

Batch 147380, train_perplexity=4385.6963, train_loss=8.386104

Batch 147390, train_perplexity=4817.172, train_loss=8.479942

Batch 147400, train_perplexity=5014.984, train_loss=8.520185

Batch 147410, train_perplexity=5806.774, train_loss=8.66678

Batch 147420, train_perplexity=5481.5244, train_loss=8.6091385

Batch 147430, train_perplexity=4558.0063, train_loss=8.424641

Batch 147440, train_perplexity=5950.2065, train_loss=8.691181

Batch 147450, train_perplexity=5299.759, train_loss=8.575417

Batch 147460, train_perplexity=5939.0835, train_loss=8.68931

Batch 147470, train_perplexity=5359.235, train_loss=8.586576

Batch 147480, train_perplexity=4875.3823, train_loss=8.491954

Batch 147490, train_perplexity=5282.4004, train_loss=8.572136

Batch 147500, train_perplexity=4396.2695, train_loss=8.388512

Batch 147510, train_perplexity=6289.059, train_loss=8.746567

Batch 147520, train_perplexity=4200.1777, train_loss=8.342882

Batch 147530, train_perplexity=5133.754, train_loss=8.543592

Batch 147540, train_perplexity=5581.296, train_loss=8.627176

Batch 147550, train_perplexity=5744.5005, train_loss=8.655998

Batch 147560, train_perplexity=5326.863, train_loss=8.580518

Batch 147570, train_perplexity=6129.9946, train_loss=8.720949

Batch 147580, train_perplexity=5192.7188, train_loss=8.555013

Batch 147590, train_perplexity=5986.041, train_loss=8.6971855

Batch 147600, train_perplexity=5060.2876, train_loss=8.529179

Batch 147610, train_perplexity=6420.468, train_loss=8.767246

Batch 147620, train_perplexity=4911.311, train_loss=8.499296

Batch 147630, train_perplexity=4865.234, train_loss=8.48987

Batch 147640, train_perplexity=5385.549, train_loss=8.591475

Batch 147650, train_perplexity=4856.9126, train_loss=8.488158

Batch 147660, train_perplexity=5283.6196, train_loss=8.572367

Batch 147670, train_perplexity=4499.988, train_loss=8.41183

Batch 147680, train_perplexity=5226.438, train_loss=8.561485

Batch 147690, train_perplexity=5240.483, train_loss=8.564169

Batch 147700, train_perplexity=5270.153, train_loss=8.569815

Batch 147710, train_perplexity=6755.9194, train_loss=8.818174

Batch 147720, train_perplexity=5804.936, train_loss=8.666464

Batch 147730, train_perplexity=5343.6587, train_loss=8.583666

Batch 147740, train_perplexity=4594.1675, train_loss=8.432543

Batch 147750, train_perplexity=5500.7744, train_loss=8.612644

Batch 147760, train_perplexity=4883.065, train_loss=8.493528

Batch 147770, train_perplexity=4733.235, train_loss=8.462364

Batch 147780, train_perplexity=5903.4907, train_loss=8.683299

Batch 147790, train_perplexity=4998.2056, train_loss=8.516834

Batch 147800, train_perplexity=5329.1445, train_loss=8.580946

Batch 147810, train_perplexity=5106.9893, train_loss=8.538365

Batch 147820, train_perplexity=6260.1807, train_loss=8.741964

Batch 147830, train_perplexity=4867.494, train_loss=8.4903345

Batch 147840, train_perplexity=5025.4927, train_loss=8.522279

Batch 147850, train_perplexity=4987.397, train_loss=8.514669

Batch 147860, train_perplexity=5164.8784, train_loss=8.549637

Batch 147870, train_perplexity=5536.2354, train_loss=8.61907

Batch 147880, train_perplexity=5486.5137, train_loss=8.610048

Batch 147890, train_perplexity=5411.167, train_loss=8.59622

Batch 147900, train_perplexity=6363.0713, train_loss=8.758266

Batch 147910, train_perplexity=4422.2397, train_loss=8.394402

Batch 147920, train_perplexity=4568.477, train_loss=8.426935

Batch 147930, train_perplexity=5336.3354, train_loss=8.582294

Batch 147940, train_perplexity=5408.7627, train_loss=8.595776

Batch 147950, train_perplexity=4813.7646, train_loss=8.479235

Batch 147960, train_perplexity=4329.301, train_loss=8.373161

Batch 147970, train_perplexity=5580.934, train_loss=8.627111

Batch 147980, train_perplexity=5751.797, train_loss=8.657268

Batch 147990, train_perplexity=4501.898, train_loss=8.412254

Batch 148000, train_perplexity=5220.61, train_loss=8.5603695

Batch 148010, train_perplexity=5039.185, train_loss=8.525

Batch 148020, train_perplexity=4566.1597, train_loss=8.426428

Batch 148030, train_perplexity=4442.1313, train_loss=8.39889

Batch 148040, train_perplexity=5814.266, train_loss=8.66807

Batch 148050, train_perplexity=5585.9927, train_loss=8.628017

Batch 148060, train_perplexity=4766.4385, train_loss=8.469355

Batch 148070, train_perplexity=5663.0933, train_loss=8.641726

Batch 148080, train_perplexity=6115.753, train_loss=8.718623

Batch 148090, train_perplexity=5587.9, train_loss=8.628359

Batch 148100, train_perplexity=5810.192, train_loss=8.667369

Batch 148110, train_perplexity=5660.7876, train_loss=8.641318

Batch 148120, train_perplexity=3976.656, train_loss=8.288197

Batch 148130, train_perplexity=5022.1294, train_loss=8.521609

Batch 148140, train_perplexity=5154.9136, train_loss=8.547706

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Batch 148150, train_perplexity=5439.1777, train_loss=8.601383

Batch 148160, train_perplexity=4389.533, train_loss=8.386978

Batch 148170, train_perplexity=4847.2183, train_loss=8.48616

Batch 148180, train_perplexity=6191.45, train_loss=8.730925

Batch 148190, train_perplexity=4682.9253, train_loss=8.451678

Batch 148200, train_perplexity=5630.674, train_loss=8.635984

Batch 148210, train_perplexity=6317.758, train_loss=8.75112

Batch 148220, train_perplexity=5378.107, train_loss=8.590092

Batch 148230, train_perplexity=4935.8164, train_loss=8.504273

Batch 148240, train_perplexity=7188.032, train_loss=8.880173

Batch 148250, train_perplexity=4440.4707, train_loss=8.398516

Batch 148260, train_perplexity=4755.1787, train_loss=8.4669895

Batch 148270, train_perplexity=5623.569, train_loss=8.634722

Batch 148280, train_perplexity=5023.7627, train_loss=8.5219345

Batch 148290, train_perplexity=5578.9756, train_loss=8.6267605

Batch 148300, train_perplexity=6082.465, train_loss=8.713165

Batch 148310, train_perplexity=6060.7046, train_loss=8.709581
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 148320, train_perplexity=5406.8853, train_loss=8.595428

Batch 148330, train_perplexity=5157.8394, train_loss=8.548273

Batch 148340, train_perplexity=5002.4404, train_loss=8.517681

Batch 148350, train_perplexity=5013.7837, train_loss=8.519946

Batch 148360, train_perplexity=4223.06, train_loss=8.348315

Batch 148370, train_perplexity=4613.037, train_loss=8.436642

Batch 148380, train_perplexity=5166.5386, train_loss=8.549958

Batch 148390, train_perplexity=5031.3193, train_loss=8.5234375

Batch 148400, train_perplexity=5121.3384, train_loss=8.541171

Batch 148410, train_perplexity=5569.136, train_loss=8.624995

Batch 148420, train_perplexity=6122.8086, train_loss=8.719776

Batch 148430, train_perplexity=4769.6807, train_loss=8.470035

Batch 148440, train_perplexity=4995.58, train_loss=8.516309

Batch 148450, train_perplexity=5161.274, train_loss=8.548939

Batch 148460, train_perplexity=5517.1978, train_loss=8.615625

Batch 148470, train_perplexity=6482.102, train_loss=8.7768

Batch 148480, train_perplexity=5042.9106, train_loss=8.525739

Batch 148490, train_perplexity=6855.7554, train_loss=8.832844

Batch 148500, train_perplexity=5644.373, train_loss=8.638414

Batch 148510, train_perplexity=5983.541, train_loss=8.696768

Batch 148520, train_perplexity=4846.109, train_loss=8.485931

Batch 148530, train_perplexity=4652.4995, train_loss=8.44516

Batch 148540, train_perplexity=5618.2295, train_loss=8.633772

Batch 148550, train_perplexity=5471.893, train_loss=8.60738

Batch 148560, train_perplexity=5481.5034, train_loss=8.609135

Batch 148570, train_perplexity=4931.2104, train_loss=8.50334

Batch 148580, train_perplexity=5675.4365, train_loss=8.643903

Batch 148590, train_perplexity=6519.411, train_loss=8.782539

Batch 148600, train_perplexity=5813.03, train_loss=8.667857

Batch 148610, train_perplexity=5358.095, train_loss=8.586364

Batch 148620, train_perplexity=5486.5083, train_loss=8.610047

Batch 148630, train_perplexity=5570.1294, train_loss=8.625174

Batch 148640, train_perplexity=5803.231, train_loss=8.66617

Batch 148650, train_perplexity=5293.182, train_loss=8.574175

Batch 148660, train_perplexity=5508.8384, train_loss=8.614109

Batch 148670, train_perplexity=4450.6885, train_loss=8.400814

Batch 148680, train_perplexity=4419.7607, train_loss=8.393841

Batch 148690, train_perplexity=6844.506, train_loss=8.831202

Batch 148700, train_perplexity=6228.5, train_loss=8.736891

Batch 148710, train_perplexity=5521.456, train_loss=8.616397

Batch 148720, train_perplexity=4548.358, train_loss=8.422522

Batch 148730, train_perplexity=4675.6294, train_loss=8.450119

Batch 148740, train_perplexity=5543.1777, train_loss=8.620323

Batch 148750, train_perplexity=5125.5405, train_loss=8.541991

Batch 148760, train_perplexity=5922.319, train_loss=8.686483

Batch 148770, train_perplexity=6151.8804, train_loss=8.724513

Batch 148780, train_perplexity=5959.3154, train_loss=8.692711

Batch 148790, train_perplexity=4911.8823, train_loss=8.499413

Batch 148800, train_perplexity=6042.7847, train_loss=8.70662

Batch 148810, train_perplexity=6111.9453, train_loss=8.718

Batch 148820, train_perplexity=5560.4434, train_loss=8.623433

Batch 148830, train_perplexity=4644.046, train_loss=8.443341

Batch 148840, train_perplexity=5423.985, train_loss=8.598586

Batch 148850, train_perplexity=5370.757, train_loss=8.588724

Batch 148860, train_perplexity=5435.6826, train_loss=8.60074

Batch 148870, train_perplexity=4791.9165, train_loss=8.474686

Batch 148880, train_perplexity=5646.042, train_loss=8.63871

Batch 148890, train_perplexity=4431.3584, train_loss=8.3964615

Batch 148900, train_perplexity=6174.9395, train_loss=8.728254

Batch 148910, train_perplexity=5574.9336, train_loss=8.626036

Batch 148920, train_perplexity=4172.6616, train_loss=8.336309

Batch 148930, train_perplexity=6437.8315, train_loss=8.769947

Batch 148940, train_perplexity=5743.3174, train_loss=8.655792

Batch 148950, train_perplexity=5634.638, train_loss=8.636688

Batch 148960, train_perplexity=4822.412, train_loss=8.4810295

Batch 148970, train_perplexity=4831.96, train_loss=8.483007

Batch 148980, train_perplexity=4855.2593, train_loss=8.487818

Batch 148990, train_perplexity=4362.461, train_loss=8.380792

Batch 149000, train_perplexity=5532.5884, train_loss=8.618411

Batch 149010, train_perplexity=6351.243, train_loss=8.756406

Batch 149020, train_perplexity=4639.7563, train_loss=8.442417

Batch 149030, train_perplexity=5194.1553, train_loss=8.555289

Batch 149040, train_perplexity=5926.127, train_loss=8.687126

Batch 149050, train_perplexity=5396.85, train_loss=8.593571

Batch 149060, train_perplexity=5005.547, train_loss=8.518302

Batch 149070, train_perplexity=5245.0977, train_loss=8.565049

Batch 149080, train_perplexity=4984.7676, train_loss=8.514142

Batch 149090, train_perplexity=5412.55, train_loss=8.596476

Batch 149100, train_perplexity=4940.3613, train_loss=8.505194

Batch 149110, train_perplexity=6166.571, train_loss=8.726898

Batch 149120, train_perplexity=7242.005, train_loss=8.887653

Batch 149130, train_perplexity=4692.631, train_loss=8.453749

Batch 149140, train_perplexity=4722.603, train_loss=8.460115

Batch 149150, train_perplexity=6324.8047, train_loss=8.752234

Batch 149160, train_perplexity=6302.9893, train_loss=8.748779

Batch 149170, train_perplexity=5478.441, train_loss=8.608576

Batch 149180, train_perplexity=5949.117, train_loss=8.690998

Batch 149190, train_perplexity=5057.6055, train_loss=8.528648

Batch 149200, train_perplexity=6131.819, train_loss=8.721247

Batch 149210, train_perplexity=5323.1, train_loss=8.579811

Batch 149220, train_perplexity=5024.5293, train_loss=8.522087

Batch 149230, train_perplexity=6116.0503, train_loss=8.718672

Batch 149240, train_perplexity=6277.566, train_loss=8.744738

Batch 149250, train_perplexity=5391.5, train_loss=8.592579

Batch 149260, train_perplexity=6323.0376, train_loss=8.751955

Batch 149270, train_perplexity=6241.4746, train_loss=8.738972

Batch 149280, train_perplexity=4976.3364, train_loss=8.512449

Batch 149290, train_perplexity=4355.523, train_loss=8.3792

Batch 149300, train_perplexity=6296.681, train_loss=8.747778

Batch 149310, train_perplexity=4749.3955, train_loss=8.465773

Batch 149320, train_perplexity=4820.15, train_loss=8.48056

Batch 149330, train_perplexity=5232.223, train_loss=8.562592

Batch 149340, train_perplexity=5231.794, train_loss=8.56251

Batch 149350, train_perplexity=5420.2773, train_loss=8.597902

Batch 149360, train_perplexity=4684.6987, train_loss=8.452057

Batch 149370, train_perplexity=5401.726, train_loss=8.594474

Batch 149380, train_perplexity=5712.541, train_loss=8.650419

Batch 149390, train_perplexity=5253.2725, train_loss=8.5666065

Batch 149400, train_perplexity=4597.3228, train_loss=8.433229

Batch 149410, train_perplexity=5052.9673, train_loss=8.527731

Batch 149420, train_perplexity=6197.8184, train_loss=8.731953

Batch 149430, train_perplexity=5209.971, train_loss=8.55833

Batch 149440, train_perplexity=4607.998, train_loss=8.435549

Batch 149450, train_perplexity=5229.33, train_loss=8.562038

Batch 149460, train_perplexity=5336.692, train_loss=8.582361

Batch 149470, train_perplexity=5316.231, train_loss=8.57852

Batch 149480, train_perplexity=6344.408, train_loss=8.755329

Batch 149490, train_perplexity=4828.8916, train_loss=8.482372

Batch 149500, train_perplexity=5415.1416, train_loss=8.596954

Batch 149510, train_perplexity=4751.5156, train_loss=8.466219

Batch 149520, train_perplexity=5276.097, train_loss=8.570942

Batch 149530, train_perplexity=5778.92, train_loss=8.661972

Batch 149540, train_perplexity=5647.302, train_loss=8.638933

Batch 149550, train_perplexity=6009.1714, train_loss=8.701042

Batch 149560, train_perplexity=6430.4375, train_loss=8.768798

Batch 149570, train_perplexity=4614.1323, train_loss=8.436879

Batch 149580, train_perplexity=6400.843, train_loss=8.764185

Batch 149590, train_perplexity=4770.5312, train_loss=8.470213

Batch 149600, train_perplexity=5610.7017, train_loss=8.632431

Batch 149610, train_perplexity=5169.067, train_loss=8.550447

Batch 149620, train_perplexity=4985.428, train_loss=8.514275

Batch 149630, train_perplexity=4959.0254, train_loss=8.508965
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 149640, train_perplexity=5841.015, train_loss=8.67266

Batch 149650, train_perplexity=5643.361, train_loss=8.638235

Batch 149660, train_perplexity=5449.2607, train_loss=8.603235

Batch 149670, train_perplexity=5441.523, train_loss=8.601814

Batch 149680, train_perplexity=5211.417, train_loss=8.558607

Batch 149690, train_perplexity=5170.891, train_loss=8.5508

Batch 149700, train_perplexity=5711.735, train_loss=8.650278

Batch 149710, train_perplexity=4845.67, train_loss=8.485841

Batch 149720, train_perplexity=5626.868, train_loss=8.635308

Batch 149730, train_perplexity=6364.0786, train_loss=8.758425

Batch 149740, train_perplexity=5502.8154, train_loss=8.613015

Batch 149750, train_perplexity=5387.2646, train_loss=8.591793

Batch 149760, train_perplexity=5076.9644, train_loss=8.532469

Batch 149770, train_perplexity=5381.9653, train_loss=8.590809

Batch 149780, train_perplexity=5219.4, train_loss=8.560138

Batch 149790, train_perplexity=5308.368, train_loss=8.57704

Batch 149800, train_perplexity=4711.249, train_loss=8.457708

Batch 149810, train_perplexity=4997.51, train_loss=8.516695

Batch 149820, train_perplexity=4671.355, train_loss=8.449204

Batch 149830, train_perplexity=4690.233, train_loss=8.453238

Batch 149840, train_perplexity=6176.4116, train_loss=8.728493

Batch 149850, train_perplexity=4528.276, train_loss=8.418097

Batch 149860, train_perplexity=5862.1763, train_loss=8.676276

Batch 149870, train_perplexity=5130.837, train_loss=8.543024

Batch 149880, train_perplexity=6031.678, train_loss=8.704781

Batch 149890, train_perplexity=5425.6665, train_loss=8.598896

Batch 149900, train_perplexity=5167.6523, train_loss=8.550174

Batch 149910, train_perplexity=6490.453, train_loss=8.778088

Batch 149920, train_perplexity=5196.0527, train_loss=8.555655

Batch 149930, train_perplexity=6095.2515, train_loss=8.715265

Batch 149940, train_perplexity=5288.4897, train_loss=8.573288

Batch 149950, train_perplexity=5621.9062, train_loss=8.634426

Batch 149960, train_perplexity=7540.792, train_loss=8.928082

Batch 149970, train_perplexity=4269.8706, train_loss=8.359339

Batch 149980, train_perplexity=4836.224, train_loss=8.48389

Batch 149990, train_perplexity=4952.3145, train_loss=8.50761

Batch 150000, train_perplexity=6057.0586, train_loss=8.70898

Batch 150010, train_perplexity=4958.3066, train_loss=8.50882

Batch 150020, train_perplexity=4884.453, train_loss=8.493813

Batch 150030, train_perplexity=4299.6836, train_loss=8.366297

Batch 150040, train_perplexity=4794.4214, train_loss=8.475208

Batch 150050, train_perplexity=7235.681, train_loss=8.88678

Batch 150060, train_perplexity=6509.2104, train_loss=8.780973

Batch 150070, train_perplexity=5113.5444, train_loss=8.539648

Batch 150080, train_perplexity=5647.9053, train_loss=8.63904

Batch 150090, train_perplexity=5296.126, train_loss=8.574731

Batch 150100, train_perplexity=4804.0283, train_loss=8.47721

Batch 150110, train_perplexity=5498.0264, train_loss=8.612144

Batch 150120, train_perplexity=4704.8735, train_loss=8.456354

Batch 150130, train_perplexity=5908.013, train_loss=8.684065

Batch 150140, train_perplexity=5389.937, train_loss=8.592289

Batch 150150, train_perplexity=6118.781, train_loss=8.719118

Batch 150160, train_perplexity=5259.99, train_loss=8.567884

Batch 150170, train_perplexity=4539.0156, train_loss=8.420465

Batch 150180, train_perplexity=5486.8486, train_loss=8.610109

Batch 150190, train_perplexity=6485.639, train_loss=8.777346

Batch 150200, train_perplexity=5118.6187, train_loss=8.54064

Batch 150210, train_perplexity=5981.4414, train_loss=8.696417

Batch 150220, train_perplexity=5733.5703, train_loss=8.654094

Batch 150230, train_perplexity=5812.2534, train_loss=8.667724

Batch 150240, train_perplexity=4808.1626, train_loss=8.47807

Batch 150250, train_perplexity=5262.057, train_loss=8.568277

Batch 150260, train_perplexity=5096.4463, train_loss=8.536299

Batch 150270, train_perplexity=4649.2974, train_loss=8.444471

Batch 150280, train_perplexity=5177.681, train_loss=8.552113

Batch 150290, train_perplexity=5583.0312, train_loss=8.627487

Batch 150300, train_perplexity=5930.435, train_loss=8.687853

Batch 150310, train_perplexity=5031.012, train_loss=8.523376

Batch 150320, train_perplexity=5175.2817, train_loss=8.551649

Batch 150330, train_perplexity=5749.2686, train_loss=8.656828

Batch 150340, train_perplexity=5498.666, train_loss=8.612261

Batch 150350, train_perplexity=5435.0195, train_loss=8.600618

Batch 150360, train_perplexity=4928.8125, train_loss=8.502853

Batch 150370, train_perplexity=5302.878, train_loss=8.576005

Batch 150380, train_perplexity=6935.246, train_loss=8.844372

Batch 150390, train_perplexity=5605.6206, train_loss=8.631525

Batch 150400, train_perplexity=6216.1807, train_loss=8.734911

Batch 150410, train_perplexity=4994.222, train_loss=8.516037

Batch 150420, train_perplexity=5878.35, train_loss=8.679031

Batch 150430, train_perplexity=5726.647, train_loss=8.652885

Batch 150440, train_perplexity=5557.7344, train_loss=8.622946

Batch 150450, train_perplexity=4856.445, train_loss=8.488062

Batch 150460, train_perplexity=5328.082, train_loss=8.580747

Batch 150470, train_perplexity=5478.3467, train_loss=8.608559

Batch 150480, train_perplexity=4843.4893, train_loss=8.485391

Batch 150490, train_perplexity=4971.7163, train_loss=8.51152

Batch 150500, train_perplexity=6269.9917, train_loss=8.74353

Batch 150510, train_perplexity=5147.137, train_loss=8.546196

Batch 150520, train_perplexity=5381.9346, train_loss=8.590803

Batch 150530, train_perplexity=5900.209, train_loss=8.682743

Batch 150540, train_perplexity=4256.462, train_loss=8.356194

Batch 150550, train_perplexity=5096.563, train_loss=8.536322

Batch 150560, train_perplexity=5627.4424, train_loss=8.63541

Batch 150570, train_perplexity=5392.878, train_loss=8.592834

Batch 150580, train_perplexity=5930.028, train_loss=8.687784

Batch 150590, train_perplexity=5178.471, train_loss=8.552265

Batch 150600, train_perplexity=5451.605, train_loss=8.603665

Batch 150610, train_perplexity=5384.0034, train_loss=8.5911875

Batch 150620, train_perplexity=5322.724, train_loss=8.579741

Batch 150630, train_perplexity=6214.071, train_loss=8.734571

Batch 150640, train_perplexity=5698.416, train_loss=8.6479435

Batch 150650, train_perplexity=4861.3564, train_loss=8.489073

Batch 150660, train_perplexity=5818.5093, train_loss=8.668799

Batch 150670, train_perplexity=5834.2676, train_loss=8.671504

Batch 150680, train_perplexity=6737.235, train_loss=8.815405

Batch 150690, train_perplexity=4836.9946, train_loss=8.484049

Batch 150700, train_perplexity=5123.805, train_loss=8.541653

Batch 150710, train_perplexity=6328.214, train_loss=8.752773

Batch 150720, train_perplexity=5162.5835, train_loss=8.549192

Batch 150730, train_perplexity=4497.418, train_loss=8.411259

Batch 150740, train_perplexity=5744.8345, train_loss=8.656056

Batch 150750, train_perplexity=5160.856, train_loss=8.548858

Batch 150760, train_perplexity=5086.177, train_loss=8.534282

Batch 150770, train_perplexity=5743.416, train_loss=8.655809

Batch 150780, train_perplexity=5230.033, train_loss=8.562173

Batch 150790, train_perplexity=5229.5396, train_loss=8.562078

Batch 150800, train_perplexity=5990.564, train_loss=8.697941

Batch 150810, train_perplexity=5182.9575, train_loss=8.553131

Batch 150820, train_perplexity=5175.5684, train_loss=8.551704

Batch 150830, train_perplexity=6557.679, train_loss=8.788392

Batch 150840, train_perplexity=4529.0923, train_loss=8.418277

Batch 150850, train_perplexity=4667.6323, train_loss=8.448407

Batch 150860, train_perplexity=6294.8315, train_loss=8.747484

Batch 150870, train_perplexity=4893.638, train_loss=8.495691

Batch 150880, train_perplexity=4807.846, train_loss=8.478004

Batch 150890, train_perplexity=6228.7017, train_loss=8.736923

Batch 150900, train_perplexity=5644.992, train_loss=8.638524

Batch 150910, train_perplexity=4789.9697, train_loss=8.474279

Batch 150920, train_perplexity=4759.135, train_loss=8.467821

Batch 150930, train_perplexity=4797.312, train_loss=8.475811

Batch 150940, train_perplexity=4831.504, train_loss=8.482913

Batch 150950, train_perplexity=5643.485, train_loss=8.638257
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 150960, train_perplexity=6019.8633, train_loss=8.70282

Batch 150970, train_perplexity=5682.9106, train_loss=8.645219

Batch 150980, train_perplexity=4192.546, train_loss=8.3410635

Batch 150990, train_perplexity=5755.3965, train_loss=8.657893

Batch 151000, train_perplexity=6039.207, train_loss=8.706028

Batch 151010, train_perplexity=4993.527, train_loss=8.515898

Batch 151020, train_perplexity=5821.9785, train_loss=8.669395

Batch 151030, train_perplexity=5631.8877, train_loss=8.6362

Batch 151040, train_perplexity=5757.6416, train_loss=8.658283

Batch 151050, train_perplexity=4415.438, train_loss=8.392862

Batch 151060, train_perplexity=6057.3994, train_loss=8.709036

Batch 151070, train_perplexity=4805.3843, train_loss=8.477492

Batch 151080, train_perplexity=4671.021, train_loss=8.449133

Batch 151090, train_perplexity=6266.3213, train_loss=8.742945

Batch 151100, train_perplexity=5348.099, train_loss=8.5844965

Batch 151110, train_perplexity=5425.2837, train_loss=8.598825

Batch 151120, train_perplexity=6243.939, train_loss=8.739367

Batch 151130, train_perplexity=6203.8564, train_loss=8.732926

Batch 151140, train_perplexity=4692.4746, train_loss=8.453715

Batch 151150, train_perplexity=4407.991, train_loss=8.391174

Batch 151160, train_perplexity=6045.027, train_loss=8.706991

Batch 151170, train_perplexity=4218.9863, train_loss=8.34735

Batch 151180, train_perplexity=5756.818, train_loss=8.65814

Batch 151190, train_perplexity=6347.7793, train_loss=8.75586

Batch 151200, train_perplexity=4454.68, train_loss=8.4017105

Batch 151210, train_perplexity=5414.9146, train_loss=8.596912

Batch 151220, train_perplexity=5612.082, train_loss=8.632677

Batch 151230, train_perplexity=4712.085, train_loss=8.457886

Batch 151240, train_perplexity=4879.099, train_loss=8.492716

Batch 151250, train_perplexity=4224.393, train_loss=8.348631

Batch 151260, train_perplexity=4676.521, train_loss=8.45031

Batch 151270, train_perplexity=5140.77, train_loss=8.544958

Batch 151280, train_perplexity=5695.982, train_loss=8.647516

Batch 151290, train_perplexity=5599.882, train_loss=8.630501

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Batch 151300, train_perplexity=4569.7363, train_loss=8.427211

Batch 151310, train_perplexity=4028.185, train_loss=8.301071

Batch 151320, train_perplexity=5842.2573, train_loss=8.672873

Batch 151330, train_perplexity=5374.5894, train_loss=8.5894375

Batch 151340, train_perplexity=5942.7773, train_loss=8.689932

Batch 151350, train_perplexity=5637.068, train_loss=8.637119

Batch 151360, train_perplexity=5288.8022, train_loss=8.573347

Batch 151370, train_perplexity=5579.9976, train_loss=8.626944

Batch 151380, train_perplexity=5144.291, train_loss=8.545643

Batch 151390, train_perplexity=5576.933, train_loss=8.626394

Batch 151400, train_perplexity=5678.701, train_loss=8.644478

Batch 151410, train_perplexity=6245.3267, train_loss=8.739589

Batch 151420, train_perplexity=5433.278, train_loss=8.600298

Batch 151430, train_perplexity=6290.445, train_loss=8.746787

Batch 151440, train_perplexity=5274.5576, train_loss=8.57065

Batch 151450, train_perplexity=5940.658, train_loss=8.689575

Batch 151460, train_perplexity=5873.3345, train_loss=8.678178

Batch 151470, train_perplexity=5841.6333, train_loss=8.672766

Batch 151480, train_perplexity=4458.1396, train_loss=8.402487

Batch 151490, train_perplexity=5117.2227, train_loss=8.540367

Batch 151500, train_perplexity=5367.48, train_loss=8.588114

Batch 151510, train_perplexity=4695.859, train_loss=8.454436

Batch 151520, train_perplexity=5131.0425, train_loss=8.543064

Batch 151530, train_perplexity=5164.179, train_loss=8.549501

Batch 151540, train_perplexity=5784.919, train_loss=8.66301

Batch 151550, train_perplexity=4970.73, train_loss=8.511322

Batch 151560, train_perplexity=6115.2456, train_loss=8.71854

Batch 151570, train_perplexity=5048.5264, train_loss=8.526852

Batch 151580, train_perplexity=4441.7925, train_loss=8.398813

Batch 151590, train_perplexity=4599.998, train_loss=8.433811

Batch 151600, train_perplexity=5707.433, train_loss=8.649525

Batch 151610, train_perplexity=5197.956, train_loss=8.556021

Batch 151620, train_perplexity=5307.826, train_loss=8.576938

Batch 151630, train_perplexity=4868.7334, train_loss=8.490589

Batch 151640, train_perplexity=5938.415, train_loss=8.689198

Batch 151650, train_perplexity=4883.5215, train_loss=8.493622

Batch 151660, train_perplexity=5123.967, train_loss=8.541684

Batch 151670, train_perplexity=5361.7295, train_loss=8.587042

Batch 151680, train_perplexity=5409.485, train_loss=8.595909

Batch 151690, train_perplexity=4625.8213, train_loss=8.439409

Batch 151700, train_perplexity=5110.3804, train_loss=8.539029

Batch 151710, train_perplexity=5243.8022, train_loss=8.564802

Batch 151720, train_perplexity=6143.057, train_loss=8.723078

Batch 151730, train_perplexity=5309.35, train_loss=8.577225

Batch 151740, train_perplexity=5640.531, train_loss=8.637733

Batch 151750, train_perplexity=6051.331, train_loss=8.708034

Batch 151760, train_perplexity=6478.32, train_loss=8.7762165

Batch 151770, train_perplexity=4982.0015, train_loss=8.513587

Batch 151780, train_perplexity=4867.554, train_loss=8.490347

Batch 151790, train_perplexity=4652.571, train_loss=8.445175

Batch 151800, train_perplexity=5563.801, train_loss=8.624037

Batch 151810, train_perplexity=4861.593, train_loss=8.489121

Batch 151820, train_perplexity=4624.388, train_loss=8.439099

Batch 151830, train_perplexity=4203.616, train_loss=8.3437

Batch 151840, train_perplexity=6697.466, train_loss=8.8094845

Batch 151850, train_perplexity=5909.8613, train_loss=8.684378

Batch 151860, train_perplexity=5827.311, train_loss=8.670311

Batch 151870, train_perplexity=5121.7827, train_loss=8.541258

Batch 151880, train_perplexity=5532.6675, train_loss=8.618425

Batch 151890, train_perplexity=5954.7764, train_loss=8.691949

Batch 151900, train_perplexity=4582.414, train_loss=8.429981

Batch 151910, train_perplexity=4624.0923, train_loss=8.439035

Batch 151920, train_perplexity=6340.8276, train_loss=8.754765

Batch 151930, train_perplexity=5755.775, train_loss=8.657959

Batch 151940, train_perplexity=4860.893, train_loss=8.488977

Batch 151950, train_perplexity=4803.3774, train_loss=8.477075

Batch 151960, train_perplexity=6393.461, train_loss=8.763031

Batch 151970, train_perplexity=6489.4873, train_loss=8.777939

Batch 151980, train_perplexity=4790.285, train_loss=8.474345

Batch 151990, train_perplexity=6298.242, train_loss=8.748026

Batch 152000, train_perplexity=5119.8784, train_loss=8.540886

Batch 152010, train_perplexity=5412.555, train_loss=8.596477

Batch 152020, train_perplexity=5308.5854, train_loss=8.577081

Batch 152030, train_perplexity=4697.4043, train_loss=8.454765

Batch 152040, train_perplexity=5396.8706, train_loss=8.593575

Batch 152050, train_perplexity=5738.8823, train_loss=8.65502

Batch 152060, train_perplexity=5091.661, train_loss=8.535359

Batch 152070, train_perplexity=7298.38, train_loss=8.895408

Batch 152080, train_perplexity=5293.162, train_loss=8.574171

Batch 152090, train_perplexity=4966.7734, train_loss=8.510526

Batch 152100, train_perplexity=4999.3022, train_loss=8.517054

Batch 152110, train_perplexity=5255.0264, train_loss=8.56694

Batch 152120, train_perplexity=5946.088, train_loss=8.690489

Batch 152130, train_perplexity=5490.6436, train_loss=8.610801

Batch 152140, train_perplexity=5620.9307, train_loss=8.634253

Batch 152150, train_perplexity=6250.731, train_loss=8.740454

Batch 152160, train_perplexity=4944.5327, train_loss=8.506038

Batch 152170, train_perplexity=5315.617, train_loss=8.578404

Batch 152180, train_perplexity=5214.221, train_loss=8.559145

Batch 152190, train_perplexity=4614.4185, train_loss=8.436941

Batch 152200, train_perplexity=4836.1274, train_loss=8.48387

Batch 152210, train_perplexity=6059.104, train_loss=8.709317
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 152220, train_perplexity=5171.6016, train_loss=8.550938

Batch 152230, train_perplexity=6467.209, train_loss=8.7745

Batch 152240, train_perplexity=5309.198, train_loss=8.577196

Batch 152250, train_perplexity=4762.04, train_loss=8.468431

Batch 152260, train_perplexity=6246.762, train_loss=8.739819

Batch 152270, train_perplexity=5008.656, train_loss=8.518923

Batch 152280, train_perplexity=5659.584, train_loss=8.641106

Batch 152290, train_perplexity=4645.1445, train_loss=8.443578

Batch 152300, train_perplexity=5748.846, train_loss=8.6567545

Batch 152310, train_perplexity=6813.025, train_loss=8.8265915

Batch 152320, train_perplexity=4812.773, train_loss=8.479029

Batch 152330, train_perplexity=6039.6045, train_loss=8.706094

Batch 152340, train_perplexity=5138.03, train_loss=8.544425

Batch 152350, train_perplexity=6516.85, train_loss=8.782146

Batch 152360, train_perplexity=5425.4336, train_loss=8.598853

Batch 152370, train_perplexity=5665.481, train_loss=8.642147

Batch 152380, train_perplexity=4987.673, train_loss=8.514725

Batch 152390, train_perplexity=5369.3433, train_loss=8.588461

Batch 152400, train_perplexity=6269.836, train_loss=8.7435055

Batch 152410, train_perplexity=6179.599, train_loss=8.729009

Batch 152420, train_perplexity=4698.2417, train_loss=8.454944

Batch 152430, train_perplexity=5757.8613, train_loss=8.658321

Batch 152440, train_perplexity=5761.8164, train_loss=8.659008

Batch 152450, train_perplexity=4687.912, train_loss=8.452743

Batch 152460, train_perplexity=5148.831, train_loss=8.546525

Batch 152470, train_perplexity=4727.33, train_loss=8.461116

Batch 152480, train_perplexity=5690.948, train_loss=8.646632

Batch 152490, train_perplexity=4089.357, train_loss=8.316143

Batch 152500, train_perplexity=4977.0103, train_loss=8.512585

Batch 152510, train_perplexity=5765.961, train_loss=8.659727

Batch 152520, train_perplexity=4873.7183, train_loss=8.491612

Batch 152530, train_perplexity=6158.2783, train_loss=8.725553

Batch 152540, train_perplexity=5401.2314, train_loss=8.594382

Batch 152550, train_perplexity=6566.8354, train_loss=8.789787

Batch 152560, train_perplexity=5524.4688, train_loss=8.616942

Batch 152570, train_perplexity=6415.314, train_loss=8.766443

Batch 152580, train_perplexity=5592.773, train_loss=8.6292305

Batch 152590, train_perplexity=6963.6777, train_loss=8.848463

Batch 152600, train_perplexity=6671.0366, train_loss=8.805531

Batch 152610, train_perplexity=4953.41, train_loss=8.507832

Batch 152620, train_perplexity=4695.3706, train_loss=8.454332

Batch 152630, train_perplexity=5830.8247, train_loss=8.670914

Batch 152640, train_perplexity=6324.72, train_loss=8.752221

Batch 152650, train_perplexity=5441.44, train_loss=8.601799

Batch 152660, train_perplexity=5709.894, train_loss=8.649956

Batch 152670, train_perplexity=5508.124, train_loss=8.613979

Batch 152680, train_perplexity=5108.48, train_loss=8.538657

Batch 152690, train_perplexity=5413.5, train_loss=8.596651

Batch 152700, train_perplexity=5870.1094, train_loss=8.6776285

Batch 152710, train_perplexity=5349.4766, train_loss=8.584754

Batch 152720, train_perplexity=4688.784, train_loss=8.452929

Batch 152730, train_perplexity=5230.946, train_loss=8.562347

Batch 152740, train_perplexity=5024.4194, train_loss=8.522065

Batch 152750, train_perplexity=5998.1787, train_loss=8.699211

Batch 152760, train_perplexity=6039.046, train_loss=8.706001

Batch 152770, train_perplexity=5456.458, train_loss=8.604555

Batch 152780, train_perplexity=3927.0708, train_loss=8.275649

Batch 152790, train_perplexity=5723.3164, train_loss=8.652304

Batch 152800, train_perplexity=4428.1055, train_loss=8.395727

Batch 152810, train_perplexity=5775.4155, train_loss=8.6613655

Batch 152820, train_perplexity=4742.4434, train_loss=8.464308

Batch 152830, train_perplexity=4944.9004, train_loss=8.506112

Batch 152840, train_perplexity=5626.133, train_loss=8.635178

Batch 152850, train_perplexity=5128.029, train_loss=8.542477

Batch 152860, train_perplexity=5386.9976, train_loss=8.591743

Batch 152870, train_perplexity=5538.7124, train_loss=8.619517

Batch 152880, train_perplexity=3973.3618, train_loss=8.287368

Batch 152890, train_perplexity=6160.358, train_loss=8.72589

Batch 152900, train_perplexity=5098.5073, train_loss=8.536703

Batch 152910, train_perplexity=4929.2637, train_loss=8.502945

Batch 152920, train_perplexity=4825.43, train_loss=8.481655

Batch 152930, train_perplexity=4660.2666, train_loss=8.446828

Batch 152940, train_perplexity=4349.2554, train_loss=8.37776

Batch 152950, train_perplexity=6876.48, train_loss=8.835862

Batch 152960, train_perplexity=5780.419, train_loss=8.662231

Batch 152970, train_perplexity=5358.1104, train_loss=8.586367

Batch 152980, train_perplexity=4724.7837, train_loss=8.460577

Batch 152990, train_perplexity=5945.436, train_loss=8.690379

Batch 153000, train_perplexity=5444.835, train_loss=8.602423

Batch 153010, train_perplexity=5174.369, train_loss=8.551473

Batch 153020, train_perplexity=6017.263, train_loss=8.702388

Batch 153030, train_perplexity=4921.8984, train_loss=8.50145

Batch 153040, train_perplexity=5012.1294, train_loss=8.519616

Batch 153050, train_perplexity=5324.5366, train_loss=8.580081

Batch 153060, train_perplexity=4638.4644, train_loss=8.442139

Batch 153070, train_perplexity=5215.355, train_loss=8.559362

Batch 153080, train_perplexity=5509.984, train_loss=8.614317

Batch 153090, train_perplexity=5198.343, train_loss=8.556095

Batch 153100, train_perplexity=5185.558, train_loss=8.553633

Batch 153110, train_perplexity=5225.068, train_loss=8.561223

Batch 153120, train_perplexity=4651.6655, train_loss=8.444981

Batch 153130, train_perplexity=4745.71, train_loss=8.464996

Batch 153140, train_perplexity=5066.8066, train_loss=8.530466

Batch 153150, train_perplexity=4773.2754, train_loss=8.470788

Batch 153160, train_perplexity=6362.064, train_loss=8.758108

Batch 153170, train_perplexity=5061.3594, train_loss=8.52939

Batch 153180, train_perplexity=5736.9673, train_loss=8.654686

Batch 153190, train_perplexity=4642.7397, train_loss=8.44306

Batch 153200, train_perplexity=5462.1484, train_loss=8.6055975

Batch 153210, train_perplexity=5153.926, train_loss=8.547514

Batch 153220, train_perplexity=5441.305, train_loss=8.601774

Batch 153230, train_perplexity=5835.2305, train_loss=8.671669

Batch 153240, train_perplexity=4803.744, train_loss=8.477151

Batch 153250, train_perplexity=5107.8125, train_loss=8.538527

Batch 153260, train_perplexity=4038.5244, train_loss=8.303635

Batch 153270, train_perplexity=4904.646, train_loss=8.497938

Batch 153280, train_perplexity=4383.3086, train_loss=8.385559

Batch 153290, train_perplexity=5924.895, train_loss=8.686918

Batch 153300, train_perplexity=4935.6895, train_loss=8.504248

Batch 153310, train_perplexity=4938.326, train_loss=8.504782

Batch 153320, train_perplexity=5872.64, train_loss=8.67806

Batch 153330, train_perplexity=5068.5273, train_loss=8.530806

Batch 153340, train_perplexity=5392.4307, train_loss=8.5927515

Batch 153350, train_perplexity=6343.0957, train_loss=8.755122

Batch 153360, train_perplexity=5841.0205, train_loss=8.672661

Batch 153370, train_perplexity=5895.507, train_loss=8.681946

Batch 153380, train_perplexity=5695.6123, train_loss=8.647451

Batch 153390, train_perplexity=4358.81, train_loss=8.379954

Batch 153400, train_perplexity=6963.6777, train_loss=8.848463

Batch 153410, train_perplexity=4651.04, train_loss=8.444846

Batch 153420, train_perplexity=6034.9463, train_loss=8.705322

Batch 153430, train_perplexity=4807.1265, train_loss=8.477855

Batch 153440, train_perplexity=4992.3936, train_loss=8.515671

Batch 153450, train_perplexity=4901.27, train_loss=8.49725

Batch 153460, train_perplexity=5429.2173, train_loss=8.59955

Batch 153470, train_perplexity=5896.463, train_loss=8.682108

Batch 153480, train_perplexity=5316.19, train_loss=8.578512

Batch 153490, train_perplexity=5307.143, train_loss=8.576809

Batch 153500, train_perplexity=5282.8843, train_loss=8.5722275

Batch 153510, train_perplexity=4942.6987, train_loss=8.505667

Batch 153520, train_perplexity=6063.208, train_loss=8.709994

Batch 153530, train_perplexity=5609.23, train_loss=8.632169
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 153540, train_perplexity=5514.4414, train_loss=8.615126

Batch 153550, train_perplexity=4878.3823, train_loss=8.492569

Batch 153560, train_perplexity=5101.4985, train_loss=8.53729

Batch 153570, train_perplexity=4971.849, train_loss=8.511547

Batch 153580, train_perplexity=6227.894, train_loss=8.7367935

Batch 153590, train_perplexity=5255.0967, train_loss=8.566954

Batch 153600, train_perplexity=5937.0444, train_loss=8.688967

Batch 153610, train_perplexity=5561.5835, train_loss=8.623638

Batch 153620, train_perplexity=5395.759, train_loss=8.593369

Batch 153630, train_perplexity=5548.022, train_loss=8.621197

Batch 153640, train_perplexity=5234.908, train_loss=8.563105

Batch 153650, train_perplexity=4773.371, train_loss=8.470808

Batch 153660, train_perplexity=5756.115, train_loss=8.658018

Batch 153670, train_perplexity=4870.294, train_loss=8.49091

Batch 153680, train_perplexity=5406.189, train_loss=8.5953

Batch 153690, train_perplexity=4816.9424, train_loss=8.479895

Batch 153700, train_perplexity=5706.41, train_loss=8.649345

Batch 153710, train_perplexity=4990.9277, train_loss=8.515377

Batch 153720, train_perplexity=5830.28, train_loss=8.67082

Batch 153730, train_perplexity=5292.3745, train_loss=8.574022

Batch 153740, train_perplexity=5454.772, train_loss=8.604246

Batch 153750, train_perplexity=4993.251, train_loss=8.515842

Batch 153760, train_perplexity=5076.645, train_loss=8.532406

Batch 153770, train_perplexity=5980.848, train_loss=8.696318

Batch 153780, train_perplexity=5473.939, train_loss=8.607754

Batch 153790, train_perplexity=5748.205, train_loss=8.656643

Batch 153800, train_perplexity=5607.364, train_loss=8.631836

Batch 153810, train_perplexity=5035.822, train_loss=8.524332

Batch 153820, train_perplexity=5176.743, train_loss=8.551931

Batch 153830, train_perplexity=5626.1006, train_loss=8.635172

Batch 153840, train_perplexity=4746.706, train_loss=8.465206

Batch 153850, train_perplexity=4745.5244, train_loss=8.464957

Batch 153860, train_perplexity=5552.3994, train_loss=8.621985

Batch 153870, train_perplexity=6000.0435, train_loss=8.699522

Batch 153880, train_perplexity=4355.735, train_loss=8.379249

Batch 153890, train_perplexity=5638.5034, train_loss=8.637374

Batch 153900, train_perplexity=4601.661, train_loss=8.434173

Batch 153910, train_perplexity=4952.2295, train_loss=8.507593

Batch 153920, train_perplexity=4592.45, train_loss=8.432169

Batch 153930, train_perplexity=5212.2075, train_loss=8.558759

Batch 153940, train_perplexity=5494.792, train_loss=8.611556

Batch 153950, train_perplexity=5414.682, train_loss=8.596869

Batch 153960, train_perplexity=5075.1636, train_loss=8.532114

Batch 153970, train_perplexity=4345.5156, train_loss=8.3769

Batch 153980, train_perplexity=5491.9946, train_loss=8.611047

Batch 153990, train_perplexity=4373.0786, train_loss=8.383223

Batch 154000, train_perplexity=5557.7236, train_loss=8.622944

Batch 154010, train_perplexity=4532.134, train_loss=8.418948

Batch 154020, train_perplexity=4811.1943, train_loss=8.478701

Batch 154030, train_perplexity=5079.9863, train_loss=8.533064

Batch 154040, train_perplexity=4946.603, train_loss=8.506456

Batch 154050, train_perplexity=6251.84, train_loss=8.740631

Batch 154060, train_perplexity=5332.3774, train_loss=8.5815525

Batch 154070, train_perplexity=4980.334, train_loss=8.513252

Batch 154080, train_perplexity=5079.594, train_loss=8.532987

Batch 154090, train_perplexity=5002.9746, train_loss=8.517788

Batch 154100, train_perplexity=5805.5117, train_loss=8.666563

Batch 154110, train_perplexity=5510.567, train_loss=8.614423

Batch 154120, train_perplexity=5977.3013, train_loss=8.6957245

Batch 154130, train_perplexity=6291.795, train_loss=8.747002

Batch 154140, train_perplexity=6179.711, train_loss=8.729027

Batch 154150, train_perplexity=5320.704, train_loss=8.579361

Batch 154160, train_perplexity=4929.038, train_loss=8.502899

Batch 154170, train_perplexity=5301.553, train_loss=8.575755

Batch 154180, train_perplexity=4317.3853, train_loss=8.370405

Batch 154190, train_perplexity=5811.8047, train_loss=8.667646

Batch 154200, train_perplexity=5563.2705, train_loss=8.623941

Batch 154210, train_perplexity=5239.3135, train_loss=8.563946

Batch 154220, train_perplexity=5825.089, train_loss=8.6699295

Batch 154230, train_perplexity=5970.4307, train_loss=8.694574

Batch 154240, train_perplexity=5053.3916, train_loss=8.527815

Batch 154250, train_perplexity=4440.949, train_loss=8.398623

Batch 154260, train_perplexity=6018.7495, train_loss=8.702635

Batch 154270, train_perplexity=5169.353, train_loss=8.550503

Batch 154280, train_perplexity=5889.1064, train_loss=8.68086

Batch 154290, train_perplexity=5410.8623, train_loss=8.596164

Batch 154300, train_perplexity=5110.346, train_loss=8.539022

Batch 154310, train_perplexity=4081.5957, train_loss=8.314243

Batch 154320, train_perplexity=4645.6187, train_loss=8.44368

Batch 154330, train_perplexity=5476.1006, train_loss=8.608149

Batch 154340, train_perplexity=5460.883, train_loss=8.605366

Batch 154350, train_perplexity=5985.333, train_loss=8.697067

Batch 154360, train_perplexity=5192.189, train_loss=8.554911

Batch 154370, train_perplexity=5741.795, train_loss=8.655527

Batch 154380, train_perplexity=5246.799, train_loss=8.565373

Batch 154390, train_perplexity=5594.7095, train_loss=8.629577

Batch 154400, train_perplexity=5539.721, train_loss=8.6196995

Batch 154410, train_perplexity=5715.4673, train_loss=8.650931

Batch 154420, train_perplexity=5185.9736, train_loss=8.553713

Batch 154430, train_perplexity=4683.716, train_loss=8.451847

Batch 154440, train_perplexity=4870.6655, train_loss=8.490986

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Batch 154450, train_perplexity=5443.942, train_loss=8.602259

Batch 154460, train_perplexity=5023.3364, train_loss=8.52185

Batch 154470, train_perplexity=5226.0444, train_loss=8.56141

Batch 154480, train_perplexity=5153.66, train_loss=8.547462

Batch 154490, train_perplexity=5732.898, train_loss=8.653976

Batch 154500, train_perplexity=4994.4985, train_loss=8.516092

Batch 154510, train_perplexity=5255.9785, train_loss=8.5671215

Batch 154520, train_perplexity=5546.0645, train_loss=8.620844

Batch 154530, train_perplexity=4986.037, train_loss=8.514397

Batch 154540, train_perplexity=5918.6265, train_loss=8.68586

Batch 154550, train_perplexity=4333.4604, train_loss=8.374122

Batch 154560, train_perplexity=5291.648, train_loss=8.573885

Batch 154570, train_perplexity=5254.5654, train_loss=8.566853

Batch 154580, train_perplexity=4518.1045, train_loss=8.415848

Batch 154590, train_perplexity=5404.725, train_loss=8.595029

Batch 154600, train_perplexity=4936.8525, train_loss=8.504483

Batch 154610, train_perplexity=5243.327, train_loss=8.564712

Batch 154620, train_perplexity=4444.784, train_loss=8.399487

Batch 154630, train_perplexity=5650.5664, train_loss=8.639511

Batch 154640, train_perplexity=5666.3887, train_loss=8.642307

Batch 154650, train_perplexity=5364.9824, train_loss=8.587648

Batch 154660, train_perplexity=4949.1323, train_loss=8.506968

Batch 154670, train_perplexity=6349.82, train_loss=8.756182

Batch 154680, train_perplexity=5339.2676, train_loss=8.582844

Batch 154690, train_perplexity=5150.236, train_loss=8.546798

Batch 154700, train_perplexity=6476.8496, train_loss=8.77599

Batch 154710, train_perplexity=5097.4185, train_loss=8.5364895

Batch 154720, train_perplexity=4985.913, train_loss=8.514372

Batch 154730, train_perplexity=5359.2603, train_loss=8.586581

Batch 154740, train_perplexity=5445.915, train_loss=8.602621

Batch 154750, train_perplexity=4917.788, train_loss=8.500614

Batch 154760, train_perplexity=5739.2656, train_loss=8.6550865

Batch 154770, train_perplexity=6199.7104, train_loss=8.732258

Batch 154780, train_perplexity=5865.7275, train_loss=8.676882
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 154790, train_perplexity=5603.8994, train_loss=8.631218

Batch 154800, train_perplexity=5931.1816, train_loss=8.687979

Batch 154810, train_perplexity=5441.7876, train_loss=8.601863

Batch 154820, train_perplexity=4341.001, train_loss=8.37586

Batch 154830, train_perplexity=5703.374, train_loss=8.648813

Batch 154840, train_perplexity=4583.9087, train_loss=8.430307

Batch 154850, train_perplexity=6730.3125, train_loss=8.814377

Batch 154860, train_perplexity=5682.1304, train_loss=8.6450815

Batch 154870, train_perplexity=5933.6597, train_loss=8.688396

Batch 154880, train_perplexity=5116.1978, train_loss=8.540167

Batch 154890, train_perplexity=5390.739, train_loss=8.592438

Batch 154900, train_perplexity=6685.953, train_loss=8.807764

Batch 154910, train_perplexity=5287.552, train_loss=8.573111

Batch 154920, train_perplexity=5456.5205, train_loss=8.604567

Batch 154930, train_perplexity=5051.031, train_loss=8.527348

Batch 154940, train_perplexity=5729.127, train_loss=8.653318

Batch 154950, train_perplexity=6222.711, train_loss=8.735961

Batch 154960, train_perplexity=5454.065, train_loss=8.604116

Batch 154970, train_perplexity=5302.4736, train_loss=8.575929

Batch 154980, train_perplexity=5085.6484, train_loss=8.534178

Batch 154990, train_perplexity=4586.6987, train_loss=8.430916

Batch 155000, train_perplexity=6237.898, train_loss=8.738399

Batch 155010, train_perplexity=5634.3374, train_loss=8.636635

Batch 155020, train_perplexity=5481.357, train_loss=8.609108

Batch 155030, train_perplexity=5102.4326, train_loss=8.537473

Batch 155040, train_perplexity=5335.379, train_loss=8.582115

Batch 155050, train_perplexity=6511.129, train_loss=8.781268

Batch 155060, train_perplexity=5866.449, train_loss=8.677005

Batch 155070, train_perplexity=5250.3677, train_loss=8.566053

Batch 155080, train_perplexity=5106.0254, train_loss=8.538177

Batch 155090, train_perplexity=4737.6157, train_loss=8.463289

Batch 155100, train_perplexity=5227.096, train_loss=8.561611

Batch 155110, train_perplexity=5195.2153, train_loss=8.555493

Batch 155120, train_perplexity=4622.1704, train_loss=8.43862

Batch 155130, train_perplexity=6039.2993, train_loss=8.706043

Batch 155140, train_perplexity=5650.4966, train_loss=8.639499

Batch 155150, train_perplexity=5478.8535, train_loss=8.608651

Batch 155160, train_perplexity=5685.4316, train_loss=8.645662

Batch 155170, train_perplexity=5853.9805, train_loss=8.674877

Batch 155180, train_perplexity=5957.0537, train_loss=8.692331

Batch 155190, train_perplexity=4527.6973, train_loss=8.417969

Batch 155200, train_perplexity=5593.1357, train_loss=8.629295

Batch 155210, train_perplexity=5241.8125, train_loss=8.564423

Batch 155220, train_perplexity=5200.4253, train_loss=8.556496

Batch 155230, train_perplexity=6038.562, train_loss=8.705921

Batch 155240, train_perplexity=5231.8438, train_loss=8.562519

Batch 155250, train_perplexity=4746.226, train_loss=8.465105

Batch 155260, train_perplexity=5264.5522, train_loss=8.568751

Batch 155270, train_perplexity=5345.203, train_loss=8.583955

Batch 155280, train_perplexity=6030.0566, train_loss=8.704512

Batch 155290, train_perplexity=5108.797, train_loss=8.538719

Batch 155300, train_perplexity=4841.3047, train_loss=8.48494

Batch 155310, train_perplexity=5543.6743, train_loss=8.620413

Batch 155320, train_perplexity=5747.887, train_loss=8.656588

Batch 155330, train_perplexity=6917.345, train_loss=8.841787

Batch 155340, train_perplexity=5553.4478, train_loss=8.622174

Batch 155350, train_perplexity=5564.3633, train_loss=8.624138

Batch 155360, train_perplexity=5509.08, train_loss=8.614153

Batch 155370, train_perplexity=7032.74, train_loss=8.858332

Batch 155380, train_perplexity=5823.933, train_loss=8.669731

Batch 155390, train_perplexity=5776.088, train_loss=8.661482

Batch 155400, train_perplexity=5107.92, train_loss=8.5385475

Batch 155410, train_perplexity=5581.136, train_loss=8.627148

Batch 155420, train_perplexity=4760.946, train_loss=8.468202

Batch 155430, train_perplexity=5501.4883, train_loss=8.612774

Batch 155440, train_perplexity=5169.2886, train_loss=8.55049

Batch 155450, train_perplexity=5600.213, train_loss=8.63056

Batch 155460, train_perplexity=5752.735, train_loss=8.657431

Batch 155470, train_perplexity=5510.751, train_loss=8.614456

Batch 155480, train_perplexity=5283.5596, train_loss=8.572355

Batch 155490, train_perplexity=5936.4897, train_loss=8.688873

Batch 155500, train_perplexity=5460.9453, train_loss=8.605377

Batch 155510, train_perplexity=5624.352, train_loss=8.634861

Batch 155520, train_perplexity=5575.4653, train_loss=8.626131

Batch 155530, train_perplexity=5128.4253, train_loss=8.542554

Batch 155540, train_perplexity=5715.0586, train_loss=8.65086

Batch 155550, train_perplexity=4915.8374, train_loss=8.500217

Batch 155560, train_perplexity=6408.783, train_loss=8.765425

Batch 155570, train_perplexity=5311.72, train_loss=8.577671

Batch 155580, train_perplexity=4149.8867, train_loss=8.330836

Batch 155590, train_perplexity=5436.9375, train_loss=8.600971

Batch 155600, train_perplexity=5485.624, train_loss=8.609886

Batch 155610, train_perplexity=5493.6763, train_loss=8.611353

Batch 155620, train_perplexity=5072.7925, train_loss=8.531647

Batch 155630, train_perplexity=5766.3623, train_loss=8.659797

Batch 155640, train_perplexity=5394.0557, train_loss=8.593053

Batch 155650, train_perplexity=5688.8105, train_loss=8.646256

Batch 155660, train_perplexity=4257.0913, train_loss=8.356341

Batch 155670, train_perplexity=5551.6, train_loss=8.621841

Batch 155680, train_perplexity=5767.0386, train_loss=8.659914

Batch 155690, train_perplexity=4654.9805, train_loss=8.445693

Batch 155700, train_perplexity=5923.765, train_loss=8.686728

Batch 155710, train_perplexity=5526.519, train_loss=8.617313

Batch 155720, train_perplexity=5496.4326, train_loss=8.611855

Batch 155730, train_perplexity=4587.5474, train_loss=8.431101

Batch 155740, train_perplexity=6944.737, train_loss=8.845739

Batch 155750, train_perplexity=4928.8315, train_loss=8.502857

Batch 155760, train_perplexity=6174.833, train_loss=8.728237

Batch 155770, train_perplexity=5061.137, train_loss=8.529346

Batch 155780, train_perplexity=6057.9253, train_loss=8.709123

Batch 155790, train_perplexity=5780.4575, train_loss=8.662238

Batch 155800, train_perplexity=5611.59, train_loss=8.632589

Batch 155810, train_perplexity=5241.6523, train_loss=8.564392

Batch 155820, train_perplexity=5788.418, train_loss=8.663614

Batch 155830, train_perplexity=5546.165, train_loss=8.620862

Batch 155840, train_perplexity=6490.057, train_loss=8.778027

Batch 155850, train_perplexity=4457.468, train_loss=8.402336

Batch 155860, train_perplexity=4630.253, train_loss=8.440367

Batch 155870, train_perplexity=6090.423, train_loss=8.714473

Batch 155880, train_perplexity=5411.6934, train_loss=8.596317

Batch 155890, train_perplexity=5083.8057, train_loss=8.533815

Batch 155900, train_perplexity=4878.3545, train_loss=8.492563

Batch 155910, train_perplexity=6394.9185, train_loss=8.763259

Batch 155920, train_perplexity=5211.392, train_loss=8.558602

Batch 155930, train_perplexity=5625.9775, train_loss=8.63515

Batch 155940, train_perplexity=6286.043, train_loss=8.746087

Batch 155950, train_perplexity=5347.5435, train_loss=8.584393

Batch 155960, train_perplexity=6589.2686, train_loss=8.793198

Batch 155970, train_perplexity=5239.913, train_loss=8.56406

Batch 155980, train_perplexity=6711.654, train_loss=8.811601

Batch 155990, train_perplexity=4886.3633, train_loss=8.494204

Batch 156000, train_perplexity=5051.536, train_loss=8.527448

Batch 156010, train_perplexity=5396.608, train_loss=8.593526

Batch 156020, train_perplexity=4567.397, train_loss=8.426699

Batch 156030, train_perplexity=4919.5615, train_loss=8.500975

Batch 156040, train_perplexity=6586.8623, train_loss=8.792832

Batch 156050, train_perplexity=4498.4214, train_loss=8.411482

Batch 156060, train_perplexity=6340.9907, train_loss=8.75479

Batch 156070, train_perplexity=4907.481, train_loss=8.498516

Batch 156080, train_perplexity=5847.564, train_loss=8.67378

Batch 156090, train_perplexity=5159.827, train_loss=8.548658

Batch 156100, train_perplexity=6046.6987, train_loss=8.707268
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 156110, train_perplexity=5136.178, train_loss=8.5440645

Batch 156120, train_perplexity=5757.51, train_loss=8.65826

Batch 156130, train_perplexity=4922.443, train_loss=8.50156

Batch 156140, train_perplexity=5155.794, train_loss=8.547876

Batch 156150, train_perplexity=5010.901, train_loss=8.519371

Batch 156160, train_perplexity=5360.267, train_loss=8.586769

Batch 156170, train_perplexity=5835.876, train_loss=8.67178

Batch 156180, train_perplexity=5359.163, train_loss=8.586563

Batch 156190, train_perplexity=5615.262, train_loss=8.633244

Batch 156200, train_perplexity=5411.301, train_loss=8.596245

Batch 156210, train_perplexity=5342.7617, train_loss=8.583498

Batch 156220, train_perplexity=5423.3955, train_loss=8.598477

Batch 156230, train_perplexity=6111.526, train_loss=8.717932

Batch 156240, train_perplexity=6106.6084, train_loss=8.717127

Batch 156250, train_perplexity=4801.468, train_loss=8.476677

Batch 156260, train_perplexity=5492.1777, train_loss=8.61108

Batch 156270, train_perplexity=5408.876, train_loss=8.595797

Batch 156280, train_perplexity=5889.528, train_loss=8.680931

Batch 156290, train_perplexity=5371.177, train_loss=8.588802

Batch 156300, train_perplexity=4607.897, train_loss=8.435527

Batch 156310, train_perplexity=6594.939, train_loss=8.794058

Batch 156320, train_perplexity=4993.4507, train_loss=8.5158825

Batch 156330, train_perplexity=6534.076, train_loss=8.784786

Batch 156340, train_perplexity=6479.9883, train_loss=8.776474

Batch 156350, train_perplexity=4825.085, train_loss=8.481584

Batch 156360, train_perplexity=5357.8447, train_loss=8.586317

Batch 156370, train_perplexity=6279.075, train_loss=8.744978

Batch 156380, train_perplexity=6492.558, train_loss=8.778412

Batch 156390, train_perplexity=5212.7593, train_loss=8.558865

Batch 156400, train_perplexity=6102.9116, train_loss=8.716521

Batch 156410, train_perplexity=4647.3423, train_loss=8.444051

Batch 156420, train_perplexity=6280.5483, train_loss=8.745213

Batch 156430, train_perplexity=4793.598, train_loss=8.475037

Batch 156440, train_perplexity=5257.919, train_loss=8.567491

Batch 156450, train_perplexity=3787.339, train_loss=8.239419

Batch 156460, train_perplexity=5346.707, train_loss=8.584236

Batch 156470, train_perplexity=5749.817, train_loss=8.656923

Batch 156480, train_perplexity=4717.7056, train_loss=8.459078

Batch 156490, train_perplexity=5564.1616, train_loss=8.624102

Batch 156500, train_perplexity=5300.3145, train_loss=8.575521

Batch 156510, train_perplexity=5335.4756, train_loss=8.582133

Batch 156520, train_perplexity=5294.1514, train_loss=8.574358

Batch 156530, train_perplexity=4647.2183, train_loss=8.444024

Batch 156540, train_perplexity=6317.2275, train_loss=8.751036

Batch 156550, train_perplexity=5447.037, train_loss=8.602827

Batch 156560, train_perplexity=5540.1177, train_loss=8.619771

Batch 156570, train_perplexity=5521.8247, train_loss=8.616464

Batch 156580, train_perplexity=5448.928, train_loss=8.603174

Batch 156590, train_perplexity=5198.5903, train_loss=8.556143

Batch 156600, train_perplexity=4951.9507, train_loss=8.507537

Batch 156610, train_perplexity=4561.2505, train_loss=8.425352

Batch 156620, train_perplexity=5847.196, train_loss=8.6737175

Batch 156630, train_perplexity=5816.3623, train_loss=8.66843

Batch 156640, train_perplexity=4899.433, train_loss=8.496875

Batch 156650, train_perplexity=5697.188, train_loss=8.647728

Batch 156660, train_perplexity=7631.514, train_loss=8.940042

Batch 156670, train_perplexity=4255.066, train_loss=8.3558655

Batch 156680, train_perplexity=4931.1494, train_loss=8.503327

Batch 156690, train_perplexity=5701.259, train_loss=8.648442

Batch 156700, train_perplexity=5923.8105, train_loss=8.686735

Batch 156710, train_perplexity=5125.0957, train_loss=8.541904

Batch 156720, train_perplexity=5548.0854, train_loss=8.621208

Batch 156730, train_perplexity=5058.1455, train_loss=8.528755

Batch 156740, train_perplexity=5406.215, train_loss=8.5953045

Batch 156750, train_perplexity=6068.6924, train_loss=8.710898

Batch 156760, train_perplexity=6152.614, train_loss=8.724632

Batch 156770, train_perplexity=5535.275, train_loss=8.6188965

Batch 156780, train_perplexity=7767.559, train_loss=8.957711

Batch 156790, train_perplexity=6036.9897, train_loss=8.705661

Batch 156800, train_perplexity=5312.0903, train_loss=8.577741

Batch 156810, train_perplexity=6037.9746, train_loss=8.705824

Batch 156820, train_perplexity=5437.1914, train_loss=8.601018

Batch 156830, train_perplexity=5529.587, train_loss=8.617868

Batch 156840, train_perplexity=6047.57, train_loss=8.707412

Batch 156850, train_perplexity=5262.8906, train_loss=8.568436

Batch 156860, train_perplexity=6235.4297, train_loss=8.738003

Batch 156870, train_perplexity=6041.442, train_loss=8.706398

Batch 156880, train_perplexity=5431.087, train_loss=8.599895

Batch 156890, train_perplexity=4828.307, train_loss=8.482251

Batch 156900, train_perplexity=5088.064, train_loss=8.534653

Batch 156910, train_perplexity=4371.1943, train_loss=8.3827915

Batch 156920, train_perplexity=4883.2188, train_loss=8.49356

Batch 156930, train_perplexity=4752.182, train_loss=8.466359

Batch 156940, train_perplexity=5140.716, train_loss=8.544948

Batch 156950, train_perplexity=4673.7744, train_loss=8.449722

Batch 156960, train_perplexity=4911.255, train_loss=8.499285

Batch 156970, train_perplexity=4149.8394, train_loss=8.330825

Batch 156980, train_perplexity=5189.5654, train_loss=8.554405

Batch 156990, train_perplexity=5040.0117, train_loss=8.525164

Batch 157000, train_perplexity=6394.912, train_loss=8.763258

Batch 157010, train_perplexity=5592.1597, train_loss=8.629121

Batch 157020, train_perplexity=5459.6436, train_loss=8.605139

Batch 157030, train_perplexity=5727.018, train_loss=8.65295

Batch 157040, train_perplexity=5346.401, train_loss=8.584179

Batch 157050, train_perplexity=4662.4624, train_loss=8.447299

Batch 157060, train_perplexity=5119.81, train_loss=8.540873

Batch 157070, train_perplexity=6219.656, train_loss=8.73547

Batch 157080, train_perplexity=5067.063, train_loss=8.530517

Batch 157090, train_perplexity=5381.58, train_loss=8.590737

Batch 157100, train_perplexity=4549.252, train_loss=8.422718

Batch 157110, train_perplexity=6440.239, train_loss=8.770321

Batch 157120, train_perplexity=4697.561, train_loss=8.454799

Batch 157130, train_perplexity=4761.114, train_loss=8.468237

Batch 157140, train_perplexity=4422.522, train_loss=8.394465

Batch 157150, train_perplexity=6039.691, train_loss=8.706108

Batch 157160, train_perplexity=5840.0015, train_loss=8.672486

Batch 157170, train_perplexity=6170.489, train_loss=8.727533

Batch 157180, train_perplexity=6176.706, train_loss=8.72854

Batch 157190, train_perplexity=5121.568, train_loss=8.541216

Batch 157200, train_perplexity=4707.0635, train_loss=8.45682

Batch 157210, train_perplexity=5185.9785, train_loss=8.553714

Batch 157220, train_perplexity=4758.259, train_loss=8.467637

Batch 157230, train_perplexity=4190.791, train_loss=8.340645

Batch 157240, train_perplexity=6147.3994, train_loss=8.723784

Batch 157250, train_perplexity=5310.555, train_loss=8.577452

Batch 157260, train_perplexity=4807.2866, train_loss=8.477888

Batch 157270, train_perplexity=5728.9956, train_loss=8.6532955

Batch 157280, train_perplexity=5122.2026, train_loss=8.54134

Batch 157290, train_perplexity=5175.529, train_loss=8.551697

Batch 157300, train_perplexity=5894.1353, train_loss=8.681713

Batch 157310, train_perplexity=5399.4033, train_loss=8.594044

Batch 157320, train_perplexity=5509.6895, train_loss=8.614264

Batch 157330, train_perplexity=5040.348, train_loss=8.52523

Batch 157340, train_perplexity=5346.3296, train_loss=8.584166

Batch 157350, train_perplexity=4723.504, train_loss=8.460306

Batch 157360, train_perplexity=5927.5176, train_loss=8.687361

Batch 157370, train_perplexity=5377.7017, train_loss=8.590016

Batch 157380, train_perplexity=5397.9, train_loss=8.593765

Batch 157390, train_perplexity=5495.4053, train_loss=8.611668

Batch 157400, train_perplexity=5511.981, train_loss=8.614679

Batch 157410, train_perplexity=6198.2915, train_loss=8.732029

Batch 157420, train_perplexity=6591.066, train_loss=8.79347
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 157430, train_perplexity=5845.5347, train_loss=8.673433

Batch 157440, train_perplexity=5408.185, train_loss=8.595669

Batch 157450, train_perplexity=5066.623, train_loss=8.53043

Batch 157460, train_perplexity=4529.3384, train_loss=8.418331

Batch 157470, train_perplexity=6234.6924, train_loss=8.7378845

Batch 157480, train_perplexity=5092.6226, train_loss=8.535548

Batch 157490, train_perplexity=4845.439, train_loss=8.485793

Batch 157500, train_perplexity=4708.599, train_loss=8.457146

Batch 157510, train_perplexity=5735.463, train_loss=8.654424

Batch 157520, train_perplexity=5372.2017, train_loss=8.588993

Batch 157530, train_perplexity=5713.947, train_loss=8.650665

Batch 157540, train_perplexity=5393.099, train_loss=8.5928755

Batch 157550, train_perplexity=4704.3755, train_loss=8.456248

Batch 157560, train_perplexity=5445.915, train_loss=8.602621

Batch 157570, train_perplexity=5467.23, train_loss=8.606527

Batch 157580, train_perplexity=5217.295, train_loss=8.559734

Batch 157590, train_perplexity=4626.607, train_loss=8.439579

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Batch 157600, train_perplexity=5108.831, train_loss=8.538726

Batch 157610, train_perplexity=5260.4565, train_loss=8.567973

Batch 157620, train_perplexity=5297.5254, train_loss=8.574995

Batch 157630, train_perplexity=6002.4473, train_loss=8.699923

Batch 157640, train_perplexity=5461.2056, train_loss=8.605425

Batch 157650, train_perplexity=5761.9043, train_loss=8.659023

Batch 157660, train_perplexity=5480.761, train_loss=8.608999

Batch 157670, train_perplexity=5756.9004, train_loss=8.6581545

Batch 157680, train_perplexity=6085.656, train_loss=8.71369

Batch 157690, train_perplexity=6192.2827, train_loss=8.731059

Batch 157700, train_perplexity=6617.633, train_loss=8.797493

Batch 157710, train_perplexity=5591.578, train_loss=8.629017

Batch 157720, train_perplexity=6066.1113, train_loss=8.710473

Batch 157730, train_perplexity=5585.076, train_loss=8.627853

Batch 157740, train_perplexity=6500.265, train_loss=8.779598

Batch 157750, train_perplexity=5785.3936, train_loss=8.663092

Batch 157760, train_perplexity=4824.7397, train_loss=8.481512

Batch 157770, train_perplexity=4660.2354, train_loss=8.446821

Batch 157780, train_perplexity=5103.2163, train_loss=8.537626

Batch 157790, train_perplexity=5316.2104, train_loss=8.578516

Batch 157800, train_perplexity=6212.1865, train_loss=8.734268

Batch 157810, train_perplexity=4898.667, train_loss=8.496718

Batch 157820, train_perplexity=6555.3223, train_loss=8.788033

Batch 157830, train_perplexity=4583.428, train_loss=8.4302025

Batch 157840, train_perplexity=4876.6055, train_loss=8.492205

Batch 157850, train_perplexity=5809.444, train_loss=8.66724

Batch 157860, train_perplexity=5119.4487, train_loss=8.540802

Batch 157870, train_perplexity=4957.4272, train_loss=8.508642

Batch 157880, train_perplexity=6388.189, train_loss=8.762206

Batch 157890, train_perplexity=4983.4033, train_loss=8.513868

Batch 157900, train_perplexity=4967.512, train_loss=8.510674

Batch 157910, train_perplexity=4934.527, train_loss=8.504012

Batch 157920, train_perplexity=5161.2544, train_loss=8.548935

Batch 157930, train_perplexity=5258.275, train_loss=8.567558

Batch 157940, train_perplexity=4983.9165, train_loss=8.513971

Batch 157950, train_perplexity=4999.7837, train_loss=8.51715

Batch 157960, train_perplexity=5804.515, train_loss=8.666391

Batch 157970, train_perplexity=5072.1733, train_loss=8.531525

Batch 157980, train_perplexity=5693.2554, train_loss=8.6470375

Batch 157990, train_perplexity=5622.7964, train_loss=8.634584

Batch 158000, train_perplexity=5662.121, train_loss=8.641554

Batch 158010, train_perplexity=7506.201, train_loss=8.923485

Batch 158020, train_perplexity=5430.5635, train_loss=8.599798

Batch 158030, train_perplexity=5670.6055, train_loss=8.643051

Batch 158040, train_perplexity=5347.951, train_loss=8.584469

Batch 158050, train_perplexity=4461.002, train_loss=8.403129

Batch 158060, train_perplexity=4864.496, train_loss=8.489718

Batch 158070, train_perplexity=5373.913, train_loss=8.589312

Batch 158080, train_perplexity=5428.482, train_loss=8.599415

Batch 158090, train_perplexity=5892.303, train_loss=8.681402

Batch 158100, train_perplexity=4873.532, train_loss=8.491574

Batch 158110, train_perplexity=5786.138, train_loss=8.66322

Batch 158120, train_perplexity=5556.5684, train_loss=8.622736

Batch 158130, train_perplexity=5008.4453, train_loss=8.518881

Batch 158140, train_perplexity=5535.1743, train_loss=8.618878

Batch 158150, train_perplexity=5675.285, train_loss=8.643876

Batch 158160, train_perplexity=5536.5786, train_loss=8.619132

Batch 158170, train_perplexity=5435.1646, train_loss=8.600645

Batch 158180, train_perplexity=5225.2373, train_loss=8.561255

Batch 158190, train_perplexity=5820.6963, train_loss=8.669175

Batch 158200, train_perplexity=5420.603, train_loss=8.597962

Batch 158210, train_perplexity=6053.184, train_loss=8.70834

Batch 158220, train_perplexity=5205.1787, train_loss=8.557409

Batch 158230, train_perplexity=5174.0776, train_loss=8.551416

Batch 158240, train_perplexity=4244.969, train_loss=8.35349

Batch 158250, train_perplexity=4808.974, train_loss=8.478239

Batch 158260, train_perplexity=4490.4062, train_loss=8.4096985

Batch 158270, train_perplexity=5240.718, train_loss=8.564214

Batch 158280, train_perplexity=5663.9194, train_loss=8.641871

Batch 158290, train_perplexity=5117.9253, train_loss=8.540504

Batch 158300, train_perplexity=4731.132, train_loss=8.46192

Batch 158310, train_perplexity=5931.125, train_loss=8.687969

Batch 158320, train_perplexity=5289.877, train_loss=8.57355

Batch 158330, train_perplexity=6717.59, train_loss=8.812485

Batch 158340, train_perplexity=5570.8945, train_loss=8.625311

Batch 158350, train_perplexity=5158.292, train_loss=8.548361

Batch 158360, train_perplexity=4866.0137, train_loss=8.49003

Batch 158370, train_perplexity=5402.8027, train_loss=8.594673

Batch 158380, train_perplexity=4580.614, train_loss=8.429588

Batch 158390, train_perplexity=6131.515, train_loss=8.721197

Batch 158400, train_perplexity=5532.003, train_loss=8.618305

Batch 158410, train_perplexity=4816.529, train_loss=8.479809

Batch 158420, train_perplexity=4894.5625, train_loss=8.49588

Batch 158430, train_perplexity=5416.867, train_loss=8.597273

Batch 158440, train_perplexity=5260.873, train_loss=8.568052

Batch 158450, train_perplexity=5281.116, train_loss=8.571893

Batch 158460, train_perplexity=5017.9453, train_loss=8.520776

Batch 158470, train_perplexity=5507.021, train_loss=8.613779

Batch 158480, train_perplexity=6654.6304, train_loss=8.803068

Batch 158490, train_perplexity=4471.19, train_loss=8.40541

Batch 158500, train_perplexity=5227.2856, train_loss=8.561647

Batch 158510, train_perplexity=4946.3906, train_loss=8.506413

Batch 158520, train_perplexity=4742.407, train_loss=8.4643

Batch 158530, train_perplexity=7248.728, train_loss=8.888581

Batch 158540, train_perplexity=5726.5483, train_loss=8.652868

Batch 158550, train_perplexity=4381.545, train_loss=8.385157

Batch 158560, train_perplexity=5005.17, train_loss=8.518227

Batch 158570, train_perplexity=5398.003, train_loss=8.593784

Batch 158580, train_perplexity=4687.416, train_loss=8.452637

Batch 158590, train_perplexity=5397.4263, train_loss=8.5936775

Batch 158600, train_perplexity=4792.922, train_loss=8.4748955

Batch 158610, train_perplexity=5790.698, train_loss=8.664008

Batch 158620, train_perplexity=5183.1255, train_loss=8.553164

Batch 158630, train_perplexity=4812.897, train_loss=8.479054

Batch 158640, train_perplexity=5047.1494, train_loss=8.526579

Batch 158650, train_perplexity=5076.9014, train_loss=8.532456

Batch 158660, train_perplexity=6230.4604, train_loss=8.7372055

Batch 158670, train_perplexity=4864.3755, train_loss=8.489694
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 158680, train_perplexity=5255.332, train_loss=8.5669985

Batch 158690, train_perplexity=6096.6465, train_loss=8.715494

Batch 158700, train_perplexity=4423.2056, train_loss=8.39462

Batch 158710, train_perplexity=5449.219, train_loss=8.603228

Batch 158720, train_perplexity=5072.841, train_loss=8.531656

Batch 158730, train_perplexity=5998.7505, train_loss=8.6993065

Batch 158740, train_perplexity=5309.5376, train_loss=8.57726

Batch 158750, train_perplexity=4584.4116, train_loss=8.430417

Batch 158760, train_perplexity=5699.492, train_loss=8.648132

Batch 158770, train_perplexity=5916.5664, train_loss=8.685512

Batch 158780, train_perplexity=6107.3774, train_loss=8.717253

Batch 158790, train_perplexity=5246.6436, train_loss=8.565344

Batch 158800, train_perplexity=4500.0605, train_loss=8.411846

Batch 158810, train_perplexity=5567.57, train_loss=8.624714

Batch 158820, train_perplexity=5359.4185, train_loss=8.586611

Batch 158830, train_perplexity=6723.435, train_loss=8.8133545

Batch 158840, train_perplexity=5624.5127, train_loss=8.63489

Batch 158850, train_perplexity=5281.383, train_loss=8.571943

Batch 158860, train_perplexity=4714.975, train_loss=8.458499

Batch 158870, train_perplexity=5134.64, train_loss=8.543765

Batch 158880, train_perplexity=4626.488, train_loss=8.439553

Batch 158890, train_perplexity=4542.2637, train_loss=8.421181

Batch 158900, train_perplexity=5353.2173, train_loss=8.585453

Batch 158910, train_perplexity=4749.0513, train_loss=8.4657

Batch 158920, train_perplexity=4840.4507, train_loss=8.484763

Batch 158930, train_perplexity=5050.4814, train_loss=8.527239

Batch 158940, train_perplexity=6289.815, train_loss=8.746687

Batch 158950, train_perplexity=6020.8276, train_loss=8.70298

Batch 158960, train_perplexity=4799.9937, train_loss=8.47637

Batch 158970, train_perplexity=4615.5103, train_loss=8.437178

Batch 158980, train_perplexity=5737.405, train_loss=8.654762

Batch 158990, train_perplexity=4442.1694, train_loss=8.398898

Batch 159000, train_perplexity=4743.009, train_loss=8.464427

Batch 159010, train_perplexity=5167.0513, train_loss=8.550057

Batch 159020, train_perplexity=4550.6924, train_loss=8.423035

Batch 159030, train_perplexity=5272.0684, train_loss=8.570178

Batch 159040, train_perplexity=4937.0923, train_loss=8.504532

Batch 159050, train_perplexity=6120.6074, train_loss=8.719417

Batch 159060, train_perplexity=5515.304, train_loss=8.615282

Batch 159070, train_perplexity=5241.8726, train_loss=8.564434

Batch 159080, train_perplexity=5804.77, train_loss=8.666435

Batch 159090, train_perplexity=5104.053, train_loss=8.53779

Batch 159100, train_perplexity=5164.553, train_loss=8.549574

Batch 159110, train_perplexity=5486.142, train_loss=8.609981

Batch 159120, train_perplexity=4531.52, train_loss=8.418813

Batch 159130, train_perplexity=4757.115, train_loss=8.467397

Batch 159140, train_perplexity=3730.1797, train_loss=8.224212

Batch 159150, train_perplexity=5428.979, train_loss=8.599506

Batch 159160, train_perplexity=5535.1587, train_loss=8.6188755

Batch 159170, train_perplexity=4457.8804, train_loss=8.402429

Batch 159180, train_perplexity=5387.82, train_loss=8.591896

Batch 159190, train_perplexity=6662.1104, train_loss=8.804192

Batch 159200, train_perplexity=5844.096, train_loss=8.673187

Batch 159210, train_perplexity=5099.6113, train_loss=8.53692

Batch 159220, train_perplexity=5160.6147, train_loss=8.548811

Batch 159230, train_perplexity=4490.629, train_loss=8.409748

Batch 159240, train_perplexity=5875.066, train_loss=8.6784725

Batch 159250, train_perplexity=4833.444, train_loss=8.4833145

Batch 159260, train_perplexity=5836.5493, train_loss=8.671895

Batch 159270, train_perplexity=4760.1284, train_loss=8.46803

Batch 159280, train_perplexity=5895.26, train_loss=8.681904

Batch 159290, train_perplexity=5374.4204, train_loss=8.589406

Batch 159300, train_perplexity=5006.745, train_loss=8.518541

Batch 159310, train_perplexity=4996.3564, train_loss=8.516464

Batch 159320, train_perplexity=5456.1978, train_loss=8.604507

Batch 159330, train_perplexity=5301.1284, train_loss=8.575675

Batch 159340, train_perplexity=5360.8706, train_loss=8.586882

Batch 159350, train_perplexity=5723.2397, train_loss=8.65229

Batch 159360, train_perplexity=5445.9463, train_loss=8.602627

Batch 159370, train_perplexity=6768.514, train_loss=8.820037

Batch 159380, train_perplexity=4778.1074, train_loss=8.4718

Batch 159390, train_perplexity=4754.6978, train_loss=8.466888

Batch 159400, train_perplexity=5831.2583, train_loss=8.670988

Batch 159410, train_perplexity=6050.2983, train_loss=8.707863

Batch 159420, train_perplexity=5461.846, train_loss=8.605542

Batch 159430, train_perplexity=4342.33, train_loss=8.376166

Batch 159440, train_perplexity=4940.8604, train_loss=8.505295

Batch 159450, train_perplexity=5247.6895, train_loss=8.565543

Batch 159460, train_perplexity=5314.259, train_loss=8.578149

Batch 159470, train_perplexity=5219.047, train_loss=8.56007

Batch 159480, train_perplexity=5225.2817, train_loss=8.561264

Batch 159490, train_perplexity=4776.121, train_loss=8.471384

Batch 159500, train_perplexity=5515.0938, train_loss=8.615244

Batch 159510, train_perplexity=5251.93, train_loss=8.566351

Batch 159520, train_perplexity=6540.2915, train_loss=8.785737

Batch 159530, train_perplexity=6561.7705, train_loss=8.789016

Batch 159540, train_perplexity=5876.007, train_loss=8.678633

Batch 159550, train_perplexity=4602.4907, train_loss=8.434353

Batch 159560, train_perplexity=5157.0034, train_loss=8.548111

Batch 159570, train_perplexity=6747.2397, train_loss=8.816889

Batch 159580, train_perplexity=5866.9023, train_loss=8.677082

Batch 159590, train_perplexity=5081.852, train_loss=8.533431

Batch 159600, train_perplexity=6000.032, train_loss=8.69952

Batch 159610, train_perplexity=5641.65, train_loss=8.637932

Batch 159620, train_perplexity=5313.058, train_loss=8.577923

Batch 159630, train_perplexity=5886.125, train_loss=8.680353

Batch 159640, train_perplexity=5529.935, train_loss=8.617931

Batch 159650, train_perplexity=5650.157, train_loss=8.639439

Batch 159660, train_perplexity=7245.8735, train_loss=8.888187

Batch 159670, train_perplexity=4535.022, train_loss=8.419585

Batch 159680, train_perplexity=4969.593, train_loss=8.511093

Batch 159690, train_perplexity=4812.2544, train_loss=8.478921

Batch 159700, train_perplexity=4907.9307, train_loss=8.498608

Batch 159710, train_perplexity=5726.2427, train_loss=8.652815

Batch 159720, train_perplexity=5893.382, train_loss=8.681585

Batch 159730, train_perplexity=4712.53, train_loss=8.45798

Batch 159740, train_perplexity=5180.763, train_loss=8.552708

Batch 159750, train_perplexity=5053.8687, train_loss=8.527909

Batch 159760, train_perplexity=5006.793, train_loss=8.518551

Batch 159770, train_perplexity=4872.106, train_loss=8.4912815

Batch 159780, train_perplexity=4956.7607, train_loss=8.508508

Batch 159790, train_perplexity=4697.162, train_loss=8.454714

Batch 159800, train_perplexity=5599.2783, train_loss=8.630393

Batch 159810, train_perplexity=6135.399, train_loss=8.72183

Batch 159820, train_perplexity=5711.381, train_loss=8.650216

Batch 159830, train_perplexity=5947.3584, train_loss=8.690702

Batch 159840, train_perplexity=5520.2715, train_loss=8.616182

Batch 159850, train_perplexity=5628.5586, train_loss=8.635609

Batch 159860, train_perplexity=5863.367, train_loss=8.676479

Batch 159870, train_perplexity=5252.1006, train_loss=8.566383

Batch 159880, train_perplexity=5369.1436, train_loss=8.588424

Batch 159890, train_perplexity=6312.1147, train_loss=8.750226

Batch 159900, train_perplexity=5363.3354, train_loss=8.587341

Batch 159910, train_perplexity=4793.818, train_loss=8.475082

Batch 159920, train_perplexity=5079.066, train_loss=8.532883

Batch 159930, train_perplexity=6788.91, train_loss=8.823046

Batch 159940, train_perplexity=4857.612, train_loss=8.488302

Batch 159950, train_perplexity=5291.0625, train_loss=8.573774

Batch 159960, train_perplexity=5133.514, train_loss=8.543546

Batch 159970, train_perplexity=5350.8896, train_loss=8.585018

Batch 159980, train_perplexity=5666.875, train_loss=8.642393

Batch 159990, train_perplexity=4939.4565, train_loss=8.505011
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 160000, train_perplexity=5867.904, train_loss=8.677253

Batch 160010, train_perplexity=4775.3833, train_loss=8.47123

Batch 160020, train_perplexity=5741.357, train_loss=8.655451

Batch 160030, train_perplexity=4993.4224, train_loss=8.515877

Batch 160040, train_perplexity=6112.884, train_loss=8.718154

Batch 160050, train_perplexity=5672.3037, train_loss=8.643351

Batch 160060, train_perplexity=5340.7036, train_loss=8.583113

Batch 160070, train_perplexity=6735.5264, train_loss=8.815151

Batch 160080, train_perplexity=6172.655, train_loss=8.727884

Batch 160090, train_perplexity=4696.62, train_loss=8.454598

Batch 160100, train_perplexity=6209.3076, train_loss=8.733805

Batch 160110, train_perplexity=4577.5264, train_loss=8.428914

Batch 160120, train_perplexity=6103.575, train_loss=8.71663

Batch 160130, train_perplexity=5732.3184, train_loss=8.653875

Batch 160140, train_perplexity=5037.8203, train_loss=8.524729

Batch 160150, train_perplexity=5439.8057, train_loss=8.601499

Batch 160160, train_perplexity=5191.159, train_loss=8.554712

Batch 160170, train_perplexity=5616.6494, train_loss=8.633491

Batch 160180, train_perplexity=4862.8726, train_loss=8.489385

Batch 160190, train_perplexity=4808.786, train_loss=8.4782

Batch 160200, train_perplexity=5273.5264, train_loss=8.570455

Batch 160210, train_perplexity=4984.107, train_loss=8.514009

Batch 160220, train_perplexity=5686.527, train_loss=8.645855

Batch 160230, train_perplexity=4968.768, train_loss=8.510927

Batch 160240, train_perplexity=5268.8916, train_loss=8.569575

Batch 160250, train_perplexity=4214.3574, train_loss=8.346252

Batch 160260, train_perplexity=6804.895, train_loss=8.8253975

Batch 160270, train_perplexity=5535.206, train_loss=8.618884

Batch 160280, train_perplexity=5413.149, train_loss=8.596586

Batch 160290, train_perplexity=5463.842, train_loss=8.605907

Batch 160300, train_perplexity=4925.2744, train_loss=8.502135

Batch 160310, train_perplexity=4938.2036, train_loss=8.504757

Batch 160320, train_perplexity=4535.6577, train_loss=8.419725

Batch 160330, train_perplexity=5234.9336, train_loss=8.563109

Batch 160340, train_perplexity=5143.516, train_loss=8.545492

Batch 160350, train_perplexity=5391.3457, train_loss=8.59255

Batch 160360, train_perplexity=5023.7246, train_loss=8.521927

Batch 160370, train_perplexity=6047.558, train_loss=8.70741

Batch 160380, train_perplexity=5727.876, train_loss=8.6531

Batch 160390, train_perplexity=5253.378, train_loss=8.566627

Batch 160400, train_perplexity=5456.2812, train_loss=8.604523

Batch 160410, train_perplexity=5583.4785, train_loss=8.627567

Batch 160420, train_perplexity=5246.914, train_loss=8.565395

Batch 160430, train_perplexity=5580.402, train_loss=8.627016

Batch 160440, train_perplexity=5114.949, train_loss=8.539923

Batch 160450, train_perplexity=4988.8335, train_loss=8.514957

Batch 160460, train_perplexity=4750.048, train_loss=8.46591

Batch 160470, train_perplexity=6555.722, train_loss=8.788094

Batch 160480, train_perplexity=5911.987, train_loss=8.684737

Batch 160490, train_perplexity=4497.392, train_loss=8.411253

Batch 160500, train_perplexity=5392.05, train_loss=8.592681

Batch 160510, train_perplexity=6024.4463, train_loss=8.703581

Batch 160520, train_perplexity=5500.628, train_loss=8.6126175

Batch 160530, train_perplexity=4396.337, train_loss=8.388527

Batch 160540, train_perplexity=5456.994, train_loss=8.604653

Batch 160550, train_perplexity=4863.763, train_loss=8.489568

Batch 160560, train_perplexity=5281.7256, train_loss=8.572008

Batch 160570, train_perplexity=5143.987, train_loss=8.545584

Batch 160580, train_perplexity=4581.8154, train_loss=8.429851

Batch 160590, train_perplexity=6103.686, train_loss=8.716648

Batch 160600, train_perplexity=4361.2925, train_loss=8.380524

Batch 160610, train_perplexity=4925.777, train_loss=8.502237

Batch 160620, train_perplexity=4928.0137, train_loss=8.502691

Batch 160630, train_perplexity=5942.199, train_loss=8.689835

Batch 160640, train_perplexity=5774.171, train_loss=8.66115

Batch 160650, train_perplexity=5377.1787, train_loss=8.589919

Batch 160660, train_perplexity=4963.9087, train_loss=8.509949

Batch 160670, train_perplexity=6053.5996, train_loss=8.708408

Batch 160680, train_perplexity=4620.0283, train_loss=8.438156

Batch 160690, train_perplexity=4166.347, train_loss=8.334795

Batch 160700, train_perplexity=5740.021, train_loss=8.655218

Batch 160710, train_perplexity=5502.1963, train_loss=8.612903

Batch 160720, train_perplexity=5298.071, train_loss=8.575098

Batch 160730, train_perplexity=5479.904, train_loss=8.608843

Batch 160740, train_perplexity=6204.3594, train_loss=8.733007

Batch 160750, train_perplexity=5491.927, train_loss=8.611034

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Batch 160760, train_perplexity=5054.2397, train_loss=8.527983

Batch 160770, train_perplexity=6232.344, train_loss=8.737508

Batch 160780, train_perplexity=4682.519, train_loss=8.4515915

Batch 160790, train_perplexity=4944.778, train_loss=8.506087

Batch 160800, train_perplexity=5053.7095, train_loss=8.527878

Batch 160810, train_perplexity=6205.5903, train_loss=8.733206

Batch 160820, train_perplexity=6905.007, train_loss=8.840002

Batch 160830, train_perplexity=5677.835, train_loss=8.644325

Batch 160840, train_perplexity=6410.1035, train_loss=8.765631

Batch 160850, train_perplexity=4784.446, train_loss=8.473125

Batch 160860, train_perplexity=5903.496, train_loss=8.6833

Batch 160870, train_perplexity=4998.3867, train_loss=8.5168705

Batch 160880, train_perplexity=5221.5957, train_loss=8.560558

Batch 160890, train_perplexity=4811.7725, train_loss=8.478821

Batch 160900, train_perplexity=5081.508, train_loss=8.533363

Batch 160910, train_perplexity=4894.5527, train_loss=8.495878

Batch 160920, train_perplexity=5606.4546, train_loss=8.631674

Batch 160930, train_perplexity=5695.183, train_loss=8.647376

Batch 160940, train_perplexity=5127.2026, train_loss=8.5423155

Batch 160950, train_perplexity=4671.0073, train_loss=8.44913

Batch 160960, train_perplexity=6747.922, train_loss=8.81699

Batch 160970, train_perplexity=5673.0396, train_loss=8.64348

Batch 160980, train_perplexity=5345.616, train_loss=8.584032

Batch 160990, train_perplexity=5531.0005, train_loss=8.618124

Batch 161000, train_perplexity=4867.378, train_loss=8.490311

Batch 161010, train_perplexity=6453.125, train_loss=8.77232

Batch 161020, train_perplexity=6712.435, train_loss=8.811717

Batch 161030, train_perplexity=5767.3413, train_loss=8.659966

Batch 161040, train_perplexity=4874.067, train_loss=8.491684

Batch 161050, train_perplexity=4811.5615, train_loss=8.478777

Batch 161060, train_perplexity=6297.3657, train_loss=8.747887

Batch 161070, train_perplexity=5117.252, train_loss=8.540373

Batch 161080, train_perplexity=4997.2334, train_loss=8.51664

Batch 161090, train_perplexity=5026.0967, train_loss=8.522399

Batch 161100, train_perplexity=5031.708, train_loss=8.523515

Batch 161110, train_perplexity=5389.202, train_loss=8.592153

Batch 161120, train_perplexity=5504.1436, train_loss=8.613256

Batch 161130, train_perplexity=5725.183, train_loss=8.65263

Batch 161140, train_perplexity=6395.2905, train_loss=8.763317

Batch 161150, train_perplexity=5090.908, train_loss=8.535212

Batch 161160, train_perplexity=5397.73, train_loss=8.593734

Batch 161170, train_perplexity=6350.0864, train_loss=8.756224

Batch 161180, train_perplexity=5847.3296, train_loss=8.67374

Batch 161190, train_perplexity=6386.843, train_loss=8.761995

Batch 161200, train_perplexity=5884.4077, train_loss=8.680061

Batch 161210, train_perplexity=4831.656, train_loss=8.4829445

Batch 161220, train_perplexity=7314.818, train_loss=8.897657

Batch 161230, train_perplexity=5842.2017, train_loss=8.672863

Batch 161240, train_perplexity=6193.0327, train_loss=8.73118

Batch 161250, train_perplexity=5837.1895, train_loss=8.672005
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 161260, train_perplexity=4671.8496, train_loss=8.44931

Batch 161270, train_perplexity=5372.186, train_loss=8.58899

Batch 161280, train_perplexity=5081.3237, train_loss=8.533327

Batch 161290, train_perplexity=4827.6304, train_loss=8.482111

Batch 161300, train_perplexity=4575.305, train_loss=8.428429

Batch 161310, train_perplexity=5727.734, train_loss=8.653075

Batch 161320, train_perplexity=5779.1733, train_loss=8.662016

Batch 161330, train_perplexity=4808.1304, train_loss=8.478064

Batch 161340, train_perplexity=5446.7773, train_loss=8.602779

Batch 161350, train_perplexity=6883.9404, train_loss=8.8369465

Batch 161360, train_perplexity=4872.491, train_loss=8.491361

Batch 161370, train_perplexity=5622.4854, train_loss=8.634529

Batch 161380, train_perplexity=4521.1733, train_loss=8.416527

Batch 161390, train_perplexity=5671.6006, train_loss=8.643227

Batch 161400, train_perplexity=4670.3525, train_loss=8.44899

Batch 161410, train_perplexity=4856.3936, train_loss=8.488051

Batch 161420, train_perplexity=6124.28, train_loss=8.7200165

Batch 161430, train_perplexity=5692.1914, train_loss=8.646851

Batch 161440, train_perplexity=4810.176, train_loss=8.478489

Batch 161450, train_perplexity=4772.256, train_loss=8.470574

Batch 161460, train_perplexity=5323.029, train_loss=8.579798

Batch 161470, train_perplexity=4597.4897, train_loss=8.433266

Batch 161480, train_perplexity=4863.471, train_loss=8.489508

Batch 161490, train_perplexity=5189.6743, train_loss=8.554426

Batch 161500, train_perplexity=6160.176, train_loss=8.725861

Batch 161510, train_perplexity=5973.734, train_loss=8.6951275

Batch 161520, train_perplexity=5990.1753, train_loss=8.697876

Batch 161530, train_perplexity=5132.692, train_loss=8.5433855

Batch 161540, train_perplexity=5468.007, train_loss=8.606669

Batch 161550, train_perplexity=4203.8286, train_loss=8.343751

Batch 161560, train_perplexity=4810.653, train_loss=8.478588

Batch 161570, train_perplexity=5503.2515, train_loss=8.613094

Batch 161580, train_perplexity=5104.735, train_loss=8.537924

Batch 161590, train_perplexity=5838.988, train_loss=8.672313

Batch 161600, train_perplexity=5659.1953, train_loss=8.641037

Batch 161610, train_perplexity=5445.728, train_loss=8.602587

Batch 161620, train_perplexity=6323.0137, train_loss=8.751951

Batch 161630, train_perplexity=6361.4814, train_loss=8.758017

Batch 161640, train_perplexity=6063.595, train_loss=8.710058

Batch 161650, train_perplexity=6263.7163, train_loss=8.742529

Batch 161660, train_perplexity=4801.193, train_loss=8.47662

Batch 161670, train_perplexity=5035.2075, train_loss=8.52421

Batch 161680, train_perplexity=4608.2266, train_loss=8.435598

Batch 161690, train_perplexity=5002.445, train_loss=8.517682

Batch 161700, train_perplexity=5428.834, train_loss=8.59948

Batch 161710, train_perplexity=5771.556, train_loss=8.660697

Batch 161720, train_perplexity=4948.481, train_loss=8.506836

Batch 161730, train_perplexity=6004.451, train_loss=8.700256

Batch 161740, train_perplexity=4183.2764, train_loss=8.33885

Batch 161750, train_perplexity=4316.1006, train_loss=8.370108

Batch 161760, train_perplexity=5860.7173, train_loss=8.676027

Batch 161770, train_perplexity=4286.9175, train_loss=8.363323

Batch 161780, train_perplexity=5896.3955, train_loss=8.6820965

Batch 161790, train_perplexity=5780.54, train_loss=8.662252

Batch 161800, train_perplexity=5896.5527, train_loss=8.682123

Batch 161810, train_perplexity=5758.927, train_loss=8.658506

Batch 161820, train_perplexity=4996.781, train_loss=8.516549

Batch 161830, train_perplexity=6937.8257, train_loss=8.844744

Batch 161840, train_perplexity=4722.7114, train_loss=8.460138

Batch 161850, train_perplexity=5003.4136, train_loss=8.517876

Batch 161860, train_perplexity=5143.511, train_loss=8.545491

Batch 161870, train_perplexity=7027.698, train_loss=8.8576145

Batch 161880, train_perplexity=4389.328, train_loss=8.386931

Batch 161890, train_perplexity=7144.6206, train_loss=8.874115

Batch 161900, train_perplexity=4184.226, train_loss=8.339077

Batch 161910, train_perplexity=5769.8223, train_loss=8.660397

Batch 161920, train_perplexity=4717.3726, train_loss=8.459007

Batch 161930, train_perplexity=5137.976, train_loss=8.5444145

Batch 161940, train_perplexity=5534.9473, train_loss=8.618837

Batch 161950, train_perplexity=5656.2603, train_loss=8.640518

Batch 161960, train_perplexity=5346.1514, train_loss=8.584132

Batch 161970, train_perplexity=5821.7456, train_loss=8.669355

Batch 161980, train_perplexity=4826.852, train_loss=8.48195

Batch 161990, train_perplexity=5101.8345, train_loss=8.537355

Batch 162000, train_perplexity=4775.4834, train_loss=8.471251

Batch 162010, train_perplexity=5566.508, train_loss=8.624523

Batch 162020, train_perplexity=4479.4653, train_loss=8.407259

Batch 162030, train_perplexity=4735.9893, train_loss=8.462946

Batch 162040, train_perplexity=5544.2456, train_loss=8.620516

Batch 162050, train_perplexity=5243.4824, train_loss=8.564741

Batch 162060, train_perplexity=4411.515, train_loss=8.3919735

Batch 162070, train_perplexity=5585.5503, train_loss=8.627938

Batch 162080, train_perplexity=7205.2866, train_loss=8.88257

Batch 162090, train_perplexity=5429.207, train_loss=8.599548

Batch 162100, train_perplexity=5681.0576, train_loss=8.644893

Batch 162110, train_perplexity=5497.1245, train_loss=8.61198

Batch 162120, train_perplexity=5733.346, train_loss=8.654055

Batch 162130, train_perplexity=5462.9194, train_loss=8.605739

Batch 162140, train_perplexity=4646.146, train_loss=8.443793

Batch 162150, train_perplexity=4730.257, train_loss=8.461735

Batch 162160, train_perplexity=5657.1123, train_loss=8.640669

Batch 162170, train_perplexity=4363.3726, train_loss=8.3810005

Batch 162180, train_perplexity=5738.893, train_loss=8.655022

Batch 162190, train_perplexity=4435.46, train_loss=8.397387

Batch 162200, train_perplexity=5788.0093, train_loss=8.663544

Batch 162210, train_perplexity=5327.945, train_loss=8.580721

Batch 162220, train_perplexity=5088.7437, train_loss=8.534786

Batch 162230, train_perplexity=5572.5415, train_loss=8.625607

Batch 162240, train_perplexity=5048.0547, train_loss=8.526758

Batch 162250, train_perplexity=4870.2334, train_loss=8.490897

Batch 162260, train_perplexity=5903.0513, train_loss=8.683225

Batch 162270, train_perplexity=5016.763, train_loss=8.52054

Batch 162280, train_perplexity=5790.4277, train_loss=8.663961

Batch 162290, train_perplexity=4962.867, train_loss=8.509739

Batch 162300, train_perplexity=5737.6455, train_loss=8.654804

Batch 162310, train_perplexity=5933.105, train_loss=8.688303

Batch 162320, train_perplexity=4672.901, train_loss=8.449535

Batch 162330, train_perplexity=5862.45, train_loss=8.676323

Batch 162340, train_perplexity=4878.4385, train_loss=8.49258

Batch 162350, train_perplexity=4743.882, train_loss=8.464611

Batch 162360, train_perplexity=4420.8564, train_loss=8.394089

Batch 162370, train_perplexity=5198.04, train_loss=8.556037

Batch 162380, train_perplexity=5205.933, train_loss=8.557554

Batch 162390, train_perplexity=5166.0757, train_loss=8.549869

Batch 162400, train_perplexity=5183.6694, train_loss=8.553268

Batch 162410, train_perplexity=5478.5557, train_loss=8.608597

Batch 162420, train_perplexity=5366.8604, train_loss=8.587998

Batch 162430, train_perplexity=4508.2305, train_loss=8.41366

Batch 162440, train_perplexity=5118.277, train_loss=8.540573

Batch 162450, train_perplexity=4758.8486, train_loss=8.467761

Batch 162460, train_perplexity=6604.777, train_loss=8.795548

Batch 162470, train_perplexity=6531.185, train_loss=8.784344

Batch 162480, train_perplexity=5525.8022, train_loss=8.617184

Batch 162490, train_perplexity=5100.589, train_loss=8.537111

Batch 162500, train_perplexity=6210.9067, train_loss=8.734062

Batch 162510, train_perplexity=5280.416, train_loss=8.57176

Batch 162520, train_perplexity=5344.189, train_loss=8.583765

Batch 162530, train_perplexity=5471.126, train_loss=8.60724

Batch 162540, train_perplexity=5401.803, train_loss=8.594488

Batch 162550, train_perplexity=6195.183, train_loss=8.731527

Batch 162560, train_perplexity=5781.99, train_loss=8.662503

Batch 162570, train_perplexity=5346.8804, train_loss=8.584269
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 162580, train_perplexity=5602.0825, train_loss=8.630894

Batch 162590, train_perplexity=5901.0586, train_loss=8.682887

Batch 162600, train_perplexity=4838.042, train_loss=8.484265

Batch 162610, train_perplexity=4451.5713, train_loss=8.401012

Batch 162620, train_perplexity=5201.6353, train_loss=8.556728

Batch 162630, train_perplexity=4850.423, train_loss=8.486821

Batch 162640, train_perplexity=6114.295, train_loss=8.718385

Batch 162650, train_perplexity=6624.648, train_loss=8.7985525

Batch 162660, train_perplexity=5157.554, train_loss=8.548218

Batch 162670, train_perplexity=6129.556, train_loss=8.720878

Batch 162680, train_perplexity=4626.695, train_loss=8.439598

Batch 162690, train_perplexity=5493.8335, train_loss=8.611382

Batch 162700, train_perplexity=5492.3037, train_loss=8.611103

Batch 162710, train_perplexity=4100.995, train_loss=8.318985

Batch 162720, train_perplexity=5009.7876, train_loss=8.519149

Batch 162730, train_perplexity=5632.2744, train_loss=8.636269

Batch 162740, train_perplexity=6217.752, train_loss=8.735164

Batch 162750, train_perplexity=5552.7437, train_loss=8.622047

Batch 162760, train_perplexity=5343.261, train_loss=8.583591

Batch 162770, train_perplexity=4813.452, train_loss=8.47917

Batch 162780, train_perplexity=5105.241, train_loss=8.538023

Batch 162790, train_perplexity=5634.638, train_loss=8.636688

Batch 162800, train_perplexity=5655.1436, train_loss=8.640321

Batch 162810, train_perplexity=5094.6387, train_loss=8.535944

Batch 162820, train_perplexity=5226.822, train_loss=8.561559

Batch 162830, train_perplexity=4491.8584, train_loss=8.410022

Batch 162840, train_perplexity=4413.013, train_loss=8.392313

Batch 162850, train_perplexity=7691.78, train_loss=8.947907

Batch 162860, train_perplexity=5650.7285, train_loss=8.63954

Batch 162870, train_perplexity=4984.088, train_loss=8.514006

Batch 162880, train_perplexity=5292.1123, train_loss=8.573973

Batch 162890, train_perplexity=5299.2783, train_loss=8.575326

Batch 162900, train_perplexity=4950.624, train_loss=8.507269

Batch 162910, train_perplexity=4791.7793, train_loss=8.474657

Batch 162920, train_perplexity=5687.0693, train_loss=8.64595

Batch 162930, train_perplexity=5584.3096, train_loss=8.627716

Batch 162940, train_perplexity=5600.042, train_loss=8.630529

Batch 162950, train_perplexity=5571.4736, train_loss=8.625415

Batch 162960, train_perplexity=5863.742, train_loss=8.676543

Batch 162970, train_perplexity=5175.8247, train_loss=8.551754

Batch 162980, train_perplexity=5802.7993, train_loss=8.666096

Batch 162990, train_perplexity=5279.52, train_loss=8.57159

Batch 163000, train_perplexity=5753.0586, train_loss=8.657487

Batch 163010, train_perplexity=4351.5205, train_loss=8.378281

Batch 163020, train_perplexity=5162.318, train_loss=8.549141

Batch 163030, train_perplexity=6762.6753, train_loss=8.819174

Batch 163040, train_perplexity=5355.1934, train_loss=8.585822

Batch 163050, train_perplexity=5903.592, train_loss=8.683316

Batch 163060, train_perplexity=5178.1455, train_loss=8.552202

Batch 163070, train_perplexity=5487.0786, train_loss=8.610151

Batch 163080, train_perplexity=5116.6665, train_loss=8.540258

Batch 163090, train_perplexity=5367.6025, train_loss=8.588137

Batch 163100, train_perplexity=5002.4214, train_loss=8.517677

Batch 163110, train_perplexity=4481.5166, train_loss=8.407717

Batch 163120, train_perplexity=6033.876, train_loss=8.705145

Batch 163130, train_perplexity=4427.0796, train_loss=8.395495

Batch 163140, train_perplexity=5014.7354, train_loss=8.520136

Batch 163150, train_perplexity=6132.7896, train_loss=8.721405

Batch 163160, train_perplexity=5823.35, train_loss=8.669631

Batch 163170, train_perplexity=4704.2856, train_loss=8.456229

Batch 163180, train_perplexity=4965.9116, train_loss=8.510352

Batch 163190, train_perplexity=5596.1714, train_loss=8.629838

Batch 163200, train_perplexity=5362.742, train_loss=8.587231

Batch 163210, train_perplexity=5256.5, train_loss=8.567221

Batch 163220, train_perplexity=5810.8403, train_loss=8.66748

Batch 163230, train_perplexity=5011.183, train_loss=8.519427

Batch 163240, train_perplexity=4843.5864, train_loss=8.485411

Batch 163250, train_perplexity=4392.012, train_loss=8.387543

Batch 163260, train_perplexity=4557.885, train_loss=8.424614

Batch 163270, train_perplexity=5431.2886, train_loss=8.599932

Batch 163280, train_perplexity=4978.976, train_loss=8.5129795

Batch 163290, train_perplexity=6527.81, train_loss=8.783827

Batch 163300, train_perplexity=5360.2466, train_loss=8.586765

Batch 163310, train_perplexity=4790.061, train_loss=8.4742985

Batch 163320, train_perplexity=4572.2734, train_loss=8.427766

Batch 163330, train_perplexity=5895.406, train_loss=8.681929

Batch 163340, train_perplexity=6421.876, train_loss=8.767466

Batch 163350, train_perplexity=5626.562, train_loss=8.635254

Batch 163360, train_perplexity=6108.3325, train_loss=8.717409

Batch 163370, train_perplexity=4758.3813, train_loss=8.467663

Batch 163380, train_perplexity=6534.7617, train_loss=8.784891

Batch 163390, train_perplexity=5381.4316, train_loss=8.59071

Batch 163400, train_perplexity=4991.28, train_loss=8.515448

Batch 163410, train_perplexity=6105.4673, train_loss=8.71694

Batch 163420, train_perplexity=4468.5513, train_loss=8.4048195

Batch 163430, train_perplexity=4578.0503, train_loss=8.4290285

Batch 163440, train_perplexity=4278.8423, train_loss=8.361438

Batch 163450, train_perplexity=5588.3, train_loss=8.62843

Batch 163460, train_perplexity=6615.323, train_loss=8.797144

Batch 163470, train_perplexity=4571.9946, train_loss=8.427705

Batch 163480, train_perplexity=5206.862, train_loss=8.557733

Batch 163490, train_perplexity=5215.1113, train_loss=8.559316

Batch 163500, train_perplexity=4287.4736, train_loss=8.363453

Batch 163510, train_perplexity=5036.5903, train_loss=8.524485

Batch 163520, train_perplexity=4989.2905, train_loss=8.515049

Batch 163530, train_perplexity=6046.8833, train_loss=8.707298

Batch 163540, train_perplexity=6661.151, train_loss=8.804048

Batch 163550, train_perplexity=5506.144, train_loss=8.61362

Batch 163560, train_perplexity=5551.1655, train_loss=8.621763

Batch 163570, train_perplexity=4990.594, train_loss=8.51531

Batch 163580, train_perplexity=5458.3105, train_loss=8.604895

Batch 163590, train_perplexity=6463.127, train_loss=8.773869

Batch 163600, train_perplexity=5205.4663, train_loss=8.557465

Batch 163610, train_perplexity=5121.0063, train_loss=8.541106

Batch 163620, train_perplexity=5796.765, train_loss=8.665055

Batch 163630, train_perplexity=5015.223, train_loss=8.520233

Batch 163640, train_perplexity=5088.559, train_loss=8.53475

Batch 163650, train_perplexity=5997.349, train_loss=8.699073

Batch 163660, train_perplexity=4921.5557, train_loss=8.50138

Batch 163670, train_perplexity=5151.4097, train_loss=8.547026

Batch 163680, train_perplexity=4666.515, train_loss=8.448168

Batch 163690, train_perplexity=5915.094, train_loss=8.685263

Batch 163700, train_perplexity=4595.6484, train_loss=8.432865

Batch 163710, train_perplexity=5733.248, train_loss=8.654037

Batch 163720, train_perplexity=4452.425, train_loss=8.401204

Batch 163730, train_perplexity=5341.4067, train_loss=8.583244

Batch 163740, train_perplexity=4802.631, train_loss=8.476919

Batch 163750, train_perplexity=5090.025, train_loss=8.535038

Batch 163760, train_perplexity=5490.3345, train_loss=8.610744

Batch 163770, train_perplexity=5074.583, train_loss=8.532

Batch 163780, train_perplexity=4430.5854, train_loss=8.396287

Batch 163790, train_perplexity=5647.383, train_loss=8.6389475

Batch 163800, train_perplexity=5341.58, train_loss=8.583277

Batch 163810, train_perplexity=5700.275, train_loss=8.64827

Batch 163820, train_perplexity=5661.9917, train_loss=8.641531

Batch 163830, train_perplexity=5438.343, train_loss=8.60123

Batch 163840, train_perplexity=5616.6763, train_loss=8.633495

Batch 163850, train_perplexity=5957.44, train_loss=8.692396

Batch 163860, train_perplexity=4904.7207, train_loss=8.497953

Batch 163870, train_perplexity=4614.674, train_loss=8.436996

Batch 163880, train_perplexity=5153.149, train_loss=8.547363

Batch 163890, train_perplexity=5144.816, train_loss=8.545745
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 163900, train_perplexity=4581.2866, train_loss=8.429735

Batch 163910, train_perplexity=5097.569, train_loss=8.536519

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Batch 163920, train_perplexity=5741.795, train_loss=8.655527

Batch 163930, train_perplexity=5067.237, train_loss=8.530551

Batch 163940, train_perplexity=5218.554, train_loss=8.559976

Batch 163950, train_perplexity=5544.489, train_loss=8.62056

Batch 163960, train_perplexity=5037.3687, train_loss=8.524639

Batch 163970, train_perplexity=5026.1396, train_loss=8.522408

Batch 163980, train_perplexity=4822.265, train_loss=8.480999

Batch 163990, train_perplexity=5569.9062, train_loss=8.6251335

Batch 164000, train_perplexity=5580.1465, train_loss=8.62697

Batch 164010, train_perplexity=6083.126, train_loss=8.713274

Batch 164020, train_perplexity=5896.609, train_loss=8.682133

Batch 164030, train_perplexity=5100.8076, train_loss=8.537154

Batch 164040, train_perplexity=5131.6934, train_loss=8.543191

Batch 164050, train_perplexity=4832.6973, train_loss=8.48316

Batch 164060, train_perplexity=6032.725, train_loss=8.704954

Batch 164070, train_perplexity=5274.0747, train_loss=8.570559

Batch 164080, train_perplexity=6118.8857, train_loss=8.719135

Batch 164090, train_perplexity=5110.809, train_loss=8.539113

Batch 164100, train_perplexity=4284.964, train_loss=8.362867

Batch 164110, train_perplexity=5335.4653, train_loss=8.582131

Batch 164120, train_perplexity=5639.7456, train_loss=8.637594

Batch 164130, train_perplexity=4482.4224, train_loss=8.407919

Batch 164140, train_perplexity=4977.02, train_loss=8.512587

Batch 164150, train_perplexity=4490.6807, train_loss=8.4097595

Batch 164160, train_perplexity=5416.9287, train_loss=8.597284

Batch 164170, train_perplexity=5167.1445, train_loss=8.550076

Batch 164180, train_perplexity=5124.861, train_loss=8.541859

Batch 164190, train_perplexity=5329.2056, train_loss=8.580957

Batch 164200, train_perplexity=5345.3306, train_loss=8.583979

Batch 164210, train_perplexity=5749.066, train_loss=8.656793

Batch 164220, train_perplexity=4552.2505, train_loss=8.423377

Batch 164230, train_perplexity=4662.52, train_loss=8.447311

Batch 164240, train_perplexity=6416.501, train_loss=8.766628

Batch 164250, train_perplexity=5195.805, train_loss=8.555607

Batch 164260, train_perplexity=5195.944, train_loss=8.555634

Batch 164270, train_perplexity=5508.502, train_loss=8.614048

Batch 164280, train_perplexity=4643.5454, train_loss=8.4432335

Batch 164290, train_perplexity=4742.195, train_loss=8.464255

Batch 164300, train_perplexity=5321.851, train_loss=8.5795765

Batch 164310, train_perplexity=4980.2773, train_loss=8.513241

Batch 164320, train_perplexity=5286.044, train_loss=8.572825

Batch 164330, train_perplexity=5070.239, train_loss=8.531143

Batch 164340, train_perplexity=5724.321, train_loss=8.652479

Batch 164350, train_perplexity=4822.2188, train_loss=8.480989

Batch 164360, train_perplexity=4824.427, train_loss=8.481447

Batch 164370, train_perplexity=5349.5835, train_loss=8.584774

Batch 164380, train_perplexity=6068.42, train_loss=8.710854

Batch 164390, train_perplexity=4615.88, train_loss=8.437258

Batch 164400, train_perplexity=4689.75, train_loss=8.453135

Batch 164410, train_perplexity=5186.0425, train_loss=8.553726

Batch 164420, train_perplexity=4896.8687, train_loss=8.496351

Batch 164430, train_perplexity=6327.3447, train_loss=8.752636

Batch 164440, train_perplexity=5062.624, train_loss=8.52964

Batch 164450, train_perplexity=5019.7065, train_loss=8.521127

Batch 164460, train_perplexity=5242.6973, train_loss=8.564591

Batch 164470, train_perplexity=5204.851, train_loss=8.557346

Batch 164480, train_perplexity=5630.0083, train_loss=8.635866

Batch 164490, train_perplexity=5061.7163, train_loss=8.529461

Batch 164500, train_perplexity=5459.8467, train_loss=8.605176

Batch 164510, train_perplexity=4765.6157, train_loss=8.469182

Batch 164520, train_perplexity=4825.881, train_loss=8.481749

Batch 164530, train_perplexity=4740.757, train_loss=8.463952

Batch 164540, train_perplexity=4807.6396, train_loss=8.477962

Batch 164550, train_perplexity=5289.7407, train_loss=8.573524

Batch 164560, train_perplexity=4971.9863, train_loss=8.511575

Batch 164570, train_perplexity=5793.333, train_loss=8.664463

Batch 164580, train_perplexity=6307.469, train_loss=8.74949

Batch 164590, train_perplexity=6364.4795, train_loss=8.758488

Batch 164600, train_perplexity=5210.7563, train_loss=8.55848

Batch 164610, train_perplexity=5314.3604, train_loss=8.578168

Batch 164620, train_perplexity=5406.5654, train_loss=8.595369

Batch 164630, train_perplexity=5425.8525, train_loss=8.59893

Batch 164640, train_perplexity=6222.759, train_loss=8.735969

Batch 164650, train_perplexity=5946.6665, train_loss=8.690586

Batch 164660, train_perplexity=5453.217, train_loss=8.603961

Batch 164670, train_perplexity=4704.802, train_loss=8.456339

Batch 164680, train_perplexity=5882.348, train_loss=8.679711

Batch 164690, train_perplexity=5256.224, train_loss=8.567168

Batch 164700, train_perplexity=6435.628, train_loss=8.769605

Batch 164710, train_perplexity=4913.1333, train_loss=8.499667

Batch 164720, train_perplexity=4908.74, train_loss=8.498773

Batch 164730, train_perplexity=6959.867, train_loss=8.847916

Batch 164740, train_perplexity=5479.4077, train_loss=8.608752

Batch 164750, train_perplexity=5803.9395, train_loss=8.666292

Batch 164760, train_perplexity=5625.8057, train_loss=8.635119

Batch 164770, train_perplexity=5382.258, train_loss=8.590863

Batch 164780, train_perplexity=4591.907, train_loss=8.432051

Batch 164790, train_perplexity=5668.7236, train_loss=8.642719

Batch 164800, train_perplexity=5238.4644, train_loss=8.563784

Batch 164810, train_perplexity=6219.2524, train_loss=8.735405

Batch 164820, train_perplexity=5493.8384, train_loss=8.6113825

Batch 164830, train_perplexity=5366.845, train_loss=8.587996

Batch 164840, train_perplexity=5950.025, train_loss=8.691151

Batch 164850, train_perplexity=4816.9424, train_loss=8.479895

Batch 164860, train_perplexity=5492.0522, train_loss=8.611057

Batch 164870, train_perplexity=5403.952, train_loss=8.594886

Batch 164880, train_perplexity=5324.796, train_loss=8.58013

Batch 164890, train_perplexity=4369.923, train_loss=8.382501

Batch 164900, train_perplexity=5176.8613, train_loss=8.551954

Batch 164910, train_perplexity=5271.9224, train_loss=8.57015

Batch 164920, train_perplexity=5195.3, train_loss=8.55551

Batch 164930, train_perplexity=4935.5015, train_loss=8.5042095

Batch 164940, train_perplexity=6197.984, train_loss=8.731979

Batch 164950, train_perplexity=5353.054, train_loss=8.5854225

Batch 164960, train_perplexity=5367.613, train_loss=8.588139

Batch 164970, train_perplexity=4964.5146, train_loss=8.510071

Batch 164980, train_perplexity=5432.055, train_loss=8.600073

Batch 164990, train_perplexity=5318.959, train_loss=8.579033

Batch 165000, train_perplexity=4517.441, train_loss=8.415701

Batch 165010, train_perplexity=5470.9434, train_loss=8.607206

Batch 165020, train_perplexity=5224.0166, train_loss=8.561022

Batch 165030, train_perplexity=6035.1597, train_loss=8.705358

Batch 165040, train_perplexity=4644.0327, train_loss=8.443338

Batch 165050, train_perplexity=5109.815, train_loss=8.5389185

Batch 165060, train_perplexity=6312.7104, train_loss=8.75032

Batch 165070, train_perplexity=5817.394, train_loss=8.668608

Batch 165080, train_perplexity=5445.2397, train_loss=8.602497

Batch 165090, train_perplexity=6953.14, train_loss=8.846949

Batch 165100, train_perplexity=5270.389, train_loss=8.5698595

Batch 165110, train_perplexity=4987.887, train_loss=8.514768

Batch 165120, train_perplexity=5217.902, train_loss=8.559851

Batch 165130, train_perplexity=4953.344, train_loss=8.507818

Batch 165140, train_perplexity=5235.5522, train_loss=8.563228
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 165150, train_perplexity=5270.3643, train_loss=8.569855

Batch 165160, train_perplexity=6088.28, train_loss=8.714121

Batch 165170, train_perplexity=6317.5586, train_loss=8.751088

Batch 165180, train_perplexity=6002.928, train_loss=8.700003

Batch 165190, train_perplexity=5532.3403, train_loss=8.618366

Batch 165200, train_perplexity=5547.44, train_loss=8.621092

Batch 165210, train_perplexity=4790.253, train_loss=8.474339

Batch 165220, train_perplexity=4688.4575, train_loss=8.452859

Batch 165230, train_perplexity=6148.4023, train_loss=8.723948

Batch 165240, train_perplexity=5271.6206, train_loss=8.570093

Batch 165250, train_perplexity=5423.892, train_loss=8.598569

Batch 165260, train_perplexity=4633.4067, train_loss=8.441048

Batch 165270, train_perplexity=5871.4307, train_loss=8.677854

Batch 165280, train_perplexity=4753.8955, train_loss=8.46672

Batch 165290, train_perplexity=6720.384, train_loss=8.812901

Batch 165300, train_perplexity=5918.0957, train_loss=8.68577

Batch 165310, train_perplexity=4980.6953, train_loss=8.513325

Batch 165320, train_perplexity=5632.006, train_loss=8.636221

Batch 165330, train_perplexity=4690.5015, train_loss=8.453295

Batch 165340, train_perplexity=5692.202, train_loss=8.6468525

Batch 165350, train_perplexity=4983.137, train_loss=8.513815

Batch 165360, train_perplexity=4814.55, train_loss=8.479398

Batch 165370, train_perplexity=6479.2964, train_loss=8.776367

Batch 165380, train_perplexity=6346.0664, train_loss=8.75559

Batch 165390, train_perplexity=6364.4062, train_loss=8.758476

Batch 165400, train_perplexity=5589.078, train_loss=8.62857

Batch 165410, train_perplexity=5293.8486, train_loss=8.574301

Batch 165420, train_perplexity=5361.781, train_loss=8.587051

Batch 165430, train_perplexity=5233.3213, train_loss=8.562801

Batch 165440, train_perplexity=5290.053, train_loss=8.573584

Batch 165450, train_perplexity=6176.9595, train_loss=8.728581

Batch 165460, train_perplexity=5275.0254, train_loss=8.570739

Batch 165470, train_perplexity=5055.2085, train_loss=8.528174

Batch 165480, train_perplexity=5013.6733, train_loss=8.519924

Batch 165490, train_perplexity=6687.2666, train_loss=8.8079605

Batch 165500, train_perplexity=5902.1055, train_loss=8.683064

Batch 165510, train_perplexity=5661.549, train_loss=8.641453

Batch 165520, train_perplexity=4216.138, train_loss=8.346675

Batch 165530, train_perplexity=4678.564, train_loss=8.450747

Batch 165540, train_perplexity=5867.2046, train_loss=8.677134

Batch 165550, train_perplexity=4753.7, train_loss=8.466679

Batch 165560, train_perplexity=5175.6865, train_loss=8.551727

Batch 165570, train_perplexity=5215.7925, train_loss=8.559446

Batch 165580, train_perplexity=5908.2725, train_loss=8.684109

Batch 165590, train_perplexity=5412.5293, train_loss=8.596472

Batch 165600, train_perplexity=5427.053, train_loss=8.599152

Batch 165610, train_perplexity=5705.3545, train_loss=8.64916

Batch 165620, train_perplexity=4647.7056, train_loss=8.444129

Batch 165630, train_perplexity=6309.5205, train_loss=8.749815

Batch 165640, train_perplexity=4660.36, train_loss=8.446848

Batch 165650, train_perplexity=5087.2246, train_loss=8.534488

Batch 165660, train_perplexity=5414.3413, train_loss=8.596807

Batch 165670, train_perplexity=5471.4653, train_loss=8.607302

Batch 165680, train_perplexity=4950.454, train_loss=8.507235

Batch 165690, train_perplexity=5680.4727, train_loss=8.64479

Batch 165700, train_perplexity=4561.1724, train_loss=8.425335

Batch 165710, train_perplexity=5523.199, train_loss=8.616713

Batch 165720, train_perplexity=4814.554, train_loss=8.479399

Batch 165730, train_perplexity=5423.8765, train_loss=8.598566

Batch 165740, train_perplexity=4903.729, train_loss=8.497751

Batch 165750, train_perplexity=4991.9893, train_loss=8.51559

Batch 165760, train_perplexity=5617.244, train_loss=8.633596

Batch 165770, train_perplexity=5215.151, train_loss=8.559323

Batch 165780, train_perplexity=5415.658, train_loss=8.59705

Batch 165790, train_perplexity=5644.992, train_loss=8.638524

Batch 165800, train_perplexity=5252.902, train_loss=8.566536

Batch 165810, train_perplexity=6500.8853, train_loss=8.779694

Batch 165820, train_perplexity=4960.4585, train_loss=8.5092535

Batch 165830, train_perplexity=4529.801, train_loss=8.418433

Batch 165840, train_perplexity=6000.9365, train_loss=8.699671

Batch 165850, train_perplexity=4678.02, train_loss=8.45063

Batch 165860, train_perplexity=5521.3247, train_loss=8.616373

Batch 165870, train_perplexity=5262.133, train_loss=8.568292

Batch 165880, train_perplexity=4583.7427, train_loss=8.430271

Batch 165890, train_perplexity=4919.5566, train_loss=8.500974

Batch 165900, train_perplexity=5129.34, train_loss=8.542732

Batch 165910, train_perplexity=6282.393, train_loss=8.745506

Batch 165920, train_perplexity=4814.251, train_loss=8.479336

Batch 165930, train_perplexity=6302.4663, train_loss=8.748696

Batch 165940, train_perplexity=5440.9624, train_loss=8.601711

Batch 165950, train_perplexity=4797.9385, train_loss=8.475942

Batch 165960, train_perplexity=5791.162, train_loss=8.664088

Batch 165970, train_perplexity=4645.8623, train_loss=8.443732

Batch 165980, train_perplexity=5323.6987, train_loss=8.579924

Batch 165990, train_perplexity=4558.9976, train_loss=8.424858

Batch 166000, train_perplexity=4842.182, train_loss=8.485121

Batch 166010, train_perplexity=5489.9785, train_loss=8.61068

Batch 166020, train_perplexity=4963.5254, train_loss=8.5098715

Batch 166030, train_perplexity=5534.916, train_loss=8.618832

Batch 166040, train_perplexity=5353.59, train_loss=8.585523

Batch 166050, train_perplexity=6143.2915, train_loss=8.723116

Batch 166060, train_perplexity=5442.8, train_loss=8.602049

Batch 166070, train_perplexity=5143.5947, train_loss=8.545507

Batch 166080, train_perplexity=6285.0, train_loss=8.745921

Batch 166090, train_perplexity=5538.1157, train_loss=8.61941

Batch 166100, train_perplexity=4919.552, train_loss=8.500973

Batch 166110, train_perplexity=4885.0444, train_loss=8.493934

Batch 166120, train_perplexity=5398.909, train_loss=8.593952

Batch 166130, train_perplexity=5278.1, train_loss=8.5713215

Batch 166140, train_perplexity=5317.3867, train_loss=8.578737

Batch 166150, train_perplexity=4797.6826, train_loss=8.475888

Batch 166160, train_perplexity=5691.942, train_loss=8.646807

Batch 166170, train_perplexity=6034.843, train_loss=8.705305

Batch 166180, train_perplexity=6216.6904, train_loss=8.734993

Batch 166190, train_perplexity=5540.2075, train_loss=8.619787

Batch 166200, train_perplexity=5252.1704, train_loss=8.566397

Batch 166210, train_perplexity=5732.559, train_loss=8.653917

Batch 166220, train_perplexity=5079.909, train_loss=8.533049

Batch 166230, train_perplexity=5375.7173, train_loss=8.589647

Batch 166240, train_perplexity=5656.4814, train_loss=8.640557

Batch 166250, train_perplexity=4249.307, train_loss=8.354511

Batch 166260, train_perplexity=6653.932, train_loss=8.802963

Batch 166270, train_perplexity=6851.703, train_loss=8.8322525

Batch 166280, train_perplexity=4972.271, train_loss=8.511632

Batch 166290, train_perplexity=6042.341, train_loss=8.706547

Batch 166300, train_perplexity=5255.0615, train_loss=8.566947

Batch 166310, train_perplexity=5761.783, train_loss=8.659002

Batch 166320, train_perplexity=5531.7075, train_loss=8.618252

Batch 166330, train_perplexity=4774.978, train_loss=8.471145

Batch 166340, train_perplexity=5981.972, train_loss=8.696506

Batch 166350, train_perplexity=5095.9746, train_loss=8.536206

Batch 166360, train_perplexity=5065.546, train_loss=8.530217

Batch 166370, train_perplexity=5799.0815, train_loss=8.665455

Batch 166380, train_perplexity=6296.969, train_loss=8.747824

Batch 166390, train_perplexity=4851.158, train_loss=8.486973

Batch 166400, train_perplexity=5137.2803, train_loss=8.544279

Batch 166410, train_perplexity=5881.0977, train_loss=8.679499

Batch 166420, train_perplexity=5201.705, train_loss=8.556742

Batch 166430, train_perplexity=5718.057, train_loss=8.651384

Batch 166440, train_perplexity=5343.8423, train_loss=8.5837

Batch 166450, train_perplexity=4569.1655, train_loss=8.427086

Batch 166460, train_perplexity=5292.2334, train_loss=8.573996
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 166470, train_perplexity=4764.684, train_loss=8.4689865

Batch 166480, train_perplexity=5127.775, train_loss=8.542427

Batch 166490, train_perplexity=5624.5503, train_loss=8.634896

Batch 166500, train_perplexity=4787.2847, train_loss=8.473719

Batch 166510, train_perplexity=5231.6245, train_loss=8.562477

Batch 166520, train_perplexity=5664.8057, train_loss=8.642028

Batch 166530, train_perplexity=6679.529, train_loss=8.806803

Batch 166540, train_perplexity=5430.325, train_loss=8.599754

Batch 166550, train_perplexity=5172.2324, train_loss=8.55106

Batch 166560, train_perplexity=5652.8145, train_loss=8.639909

Batch 166570, train_perplexity=6391.4614, train_loss=8.762718

Batch 166580, train_perplexity=5156.6787, train_loss=8.548048

Batch 166590, train_perplexity=4868.4736, train_loss=8.490536

Batch 166600, train_perplexity=5181.317, train_loss=8.5528145

Batch 166610, train_perplexity=4618.918, train_loss=8.437916

Batch 166620, train_perplexity=5452.832, train_loss=8.60389

Batch 166630, train_perplexity=5459.5913, train_loss=8.605129

Batch 166640, train_perplexity=4236.949, train_loss=8.351599

Batch 166650, train_perplexity=4948.8726, train_loss=8.506915

Batch 166660, train_perplexity=6394.839, train_loss=8.763247

Batch 166670, train_perplexity=5138.8433, train_loss=8.544583

Batch 166680, train_perplexity=5165.952, train_loss=8.549845

Batch 166690, train_perplexity=5589.659, train_loss=8.628674

Batch 166700, train_perplexity=5633.5205, train_loss=8.63649

Batch 166710, train_perplexity=5398.147, train_loss=8.593811

Batch 166720, train_perplexity=4743.873, train_loss=8.464609

Batch 166730, train_perplexity=5657.652, train_loss=8.640764

Batch 166740, train_perplexity=5112.7056, train_loss=8.539484

Batch 166750, train_perplexity=5250.9185, train_loss=8.566158

Batch 166760, train_perplexity=4773.225, train_loss=8.4707775

Batch 166770, train_perplexity=5851.006, train_loss=8.674369

Batch 166780, train_perplexity=5091.1123, train_loss=8.535252

Batch 166790, train_perplexity=5949.7866, train_loss=8.691111

Batch 166800, train_perplexity=5912.6294, train_loss=8.684846

Batch 166810, train_perplexity=4653.813, train_loss=8.445442

Batch 166820, train_perplexity=6000.7534, train_loss=8.69964

Batch 166830, train_perplexity=5673.115, train_loss=8.643494

Batch 166840, train_perplexity=4657.7026, train_loss=8.446278

Batch 166850, train_perplexity=4934.259, train_loss=8.503958

Batch 166860, train_perplexity=4975.6626, train_loss=8.512314

Batch 166870, train_perplexity=5794.493, train_loss=8.664663

Batch 166880, train_perplexity=5480.8135, train_loss=8.609009

Batch 166890, train_perplexity=5023.394, train_loss=8.521861

Batch 166900, train_perplexity=5257.4224, train_loss=8.567396

Batch 166910, train_perplexity=5896.0806, train_loss=8.682043

Batch 166920, train_perplexity=5707.64, train_loss=8.649561

Batch 166930, train_perplexity=5208.0186, train_loss=8.557955

Batch 166940, train_perplexity=4751.8735, train_loss=8.466294

Batch 166950, train_perplexity=5202.7417, train_loss=8.556941

Batch 166960, train_perplexity=5950.581, train_loss=8.691244

Batch 166970, train_perplexity=5390.508, train_loss=8.592395

Batch 166980, train_perplexity=4238.089, train_loss=8.351868

Batch 166990, train_perplexity=5164.2725, train_loss=8.54952

Batch 167000, train_perplexity=4641.872, train_loss=8.442873

Batch 167010, train_perplexity=4904.571, train_loss=8.497923

Batch 167020, train_perplexity=5847.09, train_loss=8.673699

Batch 167030, train_perplexity=5393.886, train_loss=8.593021

Batch 167040, train_perplexity=4991.5176, train_loss=8.515495

Batch 167050, train_perplexity=5327.376, train_loss=8.580614

Batch 167060, train_perplexity=5129.868, train_loss=8.542835

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Batch 167070, train_perplexity=6243.7725, train_loss=8.73934

Batch 167080, train_perplexity=4719.677, train_loss=8.459496

Batch 167090, train_perplexity=5465.9263, train_loss=8.606289

Batch 167100, train_perplexity=5491.424, train_loss=8.610943

Batch 167110, train_perplexity=5759.498, train_loss=8.658606

Batch 167120, train_perplexity=5524.2, train_loss=8.616894

Batch 167130, train_perplexity=5492.314, train_loss=8.611105

Batch 167140, train_perplexity=4847.274, train_loss=8.486172

Batch 167150, train_perplexity=6526.9263, train_loss=8.783691

Batch 167160, train_perplexity=4729.6343, train_loss=8.461603

Batch 167170, train_perplexity=5229.998, train_loss=8.562166

Batch 167180, train_perplexity=5316.8037, train_loss=8.578628

Batch 167190, train_perplexity=4736.766, train_loss=8.46311

Batch 167200, train_perplexity=6005.585, train_loss=8.700445

Batch 167210, train_perplexity=4778.6953, train_loss=8.471923

Batch 167220, train_perplexity=6409.2603, train_loss=8.765499

Batch 167230, train_perplexity=4902.0176, train_loss=8.497402

Batch 167240, train_perplexity=5260.577, train_loss=8.567996

Batch 167250, train_perplexity=5563.8115, train_loss=8.624039

Batch 167260, train_perplexity=5690.232, train_loss=8.646506

Batch 167270, train_perplexity=5411.688, train_loss=8.596316

Batch 167280, train_perplexity=5193.2637, train_loss=8.555118

Batch 167290, train_perplexity=5245.533, train_loss=8.565132

Batch 167300, train_perplexity=5060.5723, train_loss=8.529235

Batch 167310, train_perplexity=5961.532, train_loss=8.693083

Batch 167320, train_perplexity=5306.5913, train_loss=8.576705

Batch 167330, train_perplexity=4936.2827, train_loss=8.504368

Batch 167340, train_perplexity=5376.3223, train_loss=8.58976

Batch 167350, train_perplexity=6251.3335, train_loss=8.74055

Batch 167360, train_perplexity=6147.693, train_loss=8.723832

Batch 167370, train_perplexity=5603.3545, train_loss=8.631121

Batch 167380, train_perplexity=4837.751, train_loss=8.484205

Batch 167390, train_perplexity=4861.032, train_loss=8.489006

Batch 167400, train_perplexity=5047.2266, train_loss=8.526594

Batch 167410, train_perplexity=4683.9395, train_loss=8.451895

Batch 167420, train_perplexity=4330.242, train_loss=8.373379

Batch 167430, train_perplexity=4571.079, train_loss=8.427505

Batch 167440, train_perplexity=5373.4673, train_loss=8.589229

Batch 167450, train_perplexity=5466.3643, train_loss=8.606369

Batch 167460, train_perplexity=5871.8843, train_loss=8.677931

Batch 167470, train_perplexity=5277.164, train_loss=8.571144

Batch 167480, train_perplexity=4710.5166, train_loss=8.457553

Batch 167490, train_perplexity=4588.974, train_loss=8.431412

Batch 167500, train_perplexity=4719.231, train_loss=8.459401

Batch 167510, train_perplexity=4453.563, train_loss=8.40146

Batch 167520, train_perplexity=6882.562, train_loss=8.836746

Batch 167530, train_perplexity=5298.576, train_loss=8.575193

Batch 167540, train_perplexity=5096.2617, train_loss=8.5362625

Batch 167550, train_perplexity=5775.7354, train_loss=8.661421

Batch 167560, train_perplexity=5057.837, train_loss=8.528694

Batch 167570, train_perplexity=4700.227, train_loss=8.455366

Batch 167580, train_perplexity=6051.2676, train_loss=8.708023

Batch 167590, train_perplexity=5436.849, train_loss=8.600955

Batch 167600, train_perplexity=5246.2637, train_loss=8.565271

Batch 167610, train_perplexity=5235.947, train_loss=8.563303

Batch 167620, train_perplexity=4449.763, train_loss=8.400606

Batch 167630, train_perplexity=5505.246, train_loss=8.613457

Batch 167640, train_perplexity=5766.797, train_loss=8.659872

Batch 167650, train_perplexity=5349.8135, train_loss=8.584817

Batch 167660, train_perplexity=5455.344, train_loss=8.604351

Batch 167670, train_perplexity=5124.7046, train_loss=8.541828

Batch 167680, train_perplexity=5590.4907, train_loss=8.628822

Batch 167690, train_perplexity=5502.3853, train_loss=8.612937

Batch 167700, train_perplexity=5001.8105, train_loss=8.517555

Batch 167710, train_perplexity=5753.503, train_loss=8.657564
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 167720, train_perplexity=5721.6465, train_loss=8.652012

Batch 167730, train_perplexity=5633.7676, train_loss=8.636534

Batch 167740, train_perplexity=5407.7827, train_loss=8.595594

Batch 167750, train_perplexity=4928.211, train_loss=8.502731

Batch 167760, train_perplexity=5011.36, train_loss=8.519463

Batch 167770, train_perplexity=5284.436, train_loss=8.572521

Batch 167780, train_perplexity=6172.478, train_loss=8.727856

Batch 167790, train_perplexity=5466.031, train_loss=8.606308

Batch 167800, train_perplexity=3915.295, train_loss=8.272646

Batch 167810, train_perplexity=5714.857, train_loss=8.650825

Batch 167820, train_perplexity=5876.87, train_loss=8.67878

Batch 167830, train_perplexity=5527.7417, train_loss=8.617535

Batch 167840, train_perplexity=6155.842, train_loss=8.725157

Batch 167850, train_perplexity=4998.444, train_loss=8.516882

Batch 167860, train_perplexity=6052.145, train_loss=8.708168

Batch 167870, train_perplexity=4141.501, train_loss=8.328814

Batch 167880, train_perplexity=6023.6533, train_loss=8.703449

Batch 167890, train_perplexity=4535.6924, train_loss=8.419733

Batch 167900, train_perplexity=5774.986, train_loss=8.661291

Batch 167910, train_perplexity=5635.466, train_loss=8.636835

Batch 167920, train_perplexity=6673.289, train_loss=8.805868

Batch 167930, train_perplexity=6487.1797, train_loss=8.777583

Batch 167940, train_perplexity=5609.5244, train_loss=8.632221

Batch 167950, train_perplexity=4579.7314, train_loss=8.429396

Batch 167960, train_perplexity=4678.0244, train_loss=8.450631

Batch 167970, train_perplexity=5109.776, train_loss=8.538911

Batch 167980, train_perplexity=6270.35, train_loss=8.7435875

Batch 167990, train_perplexity=5373.5747, train_loss=8.589249

Batch 168000, train_perplexity=6040.497, train_loss=8.706242

Batch 168010, train_perplexity=5537.017, train_loss=8.619211

Batch 168020, train_perplexity=5155.7544, train_loss=8.547869

Batch 168030, train_perplexity=4998.7207, train_loss=8.516937

Batch 168040, train_perplexity=5326.5024, train_loss=8.58045

Batch 168050, train_perplexity=4360.361, train_loss=8.38031

Batch 168060, train_perplexity=4382.485, train_loss=8.385371

Batch 168070, train_perplexity=4837.142, train_loss=8.484079

Batch 168080, train_perplexity=6791.313, train_loss=8.8234

Batch 168090, train_perplexity=6394.04, train_loss=8.763122

Batch 168100, train_perplexity=5731.4604, train_loss=8.653726

Batch 168110, train_perplexity=5574.2, train_loss=8.625904

Batch 168120, train_perplexity=5172.672, train_loss=8.551145

Batch 168130, train_perplexity=5378.8765, train_loss=8.590235

Batch 168140, train_perplexity=4905.455, train_loss=8.498103

Batch 168150, train_perplexity=5626.852, train_loss=8.635305

Batch 168160, train_perplexity=5972.7144, train_loss=8.694957

Batch 168170, train_perplexity=5810.796, train_loss=8.667473

Batch 168180, train_perplexity=4859.197, train_loss=8.488628

Batch 168190, train_perplexity=5764.3223, train_loss=8.659443

Batch 168200, train_perplexity=5009.53, train_loss=8.519097

Batch 168210, train_perplexity=4868.135, train_loss=8.490466

Batch 168220, train_perplexity=5127.5303, train_loss=8.542379

Batch 168230, train_perplexity=5714.5137, train_loss=8.650764

Batch 168240, train_perplexity=4763.8843, train_loss=8.468819

Batch 168250, train_perplexity=5949.1567, train_loss=8.691005

Batch 168260, train_perplexity=5192.793, train_loss=8.555027

Batch 168270, train_perplexity=5402.772, train_loss=8.594667

Batch 168280, train_perplexity=5294.4243, train_loss=8.5744095

Batch 168290, train_perplexity=5193.9077, train_loss=8.555242

Batch 168300, train_perplexity=5583.1807, train_loss=8.627514

Batch 168310, train_perplexity=4966.7026, train_loss=8.510511

Batch 168320, train_perplexity=5798.645, train_loss=8.66538

Batch 168330, train_perplexity=5728.788, train_loss=8.653259

Batch 168340, train_perplexity=5136.1143, train_loss=8.544052

Batch 168350, train_perplexity=4509.4175, train_loss=8.413923

Batch 168360, train_perplexity=5891.932, train_loss=8.681339

Batch 168370, train_perplexity=5876.6963, train_loss=8.67875

Batch 168380, train_perplexity=4951.181, train_loss=8.507381

Batch 168390, train_perplexity=5145.449, train_loss=8.545868

Batch 168400, train_perplexity=5280.215, train_loss=8.571722

Batch 168410, train_perplexity=4656.135, train_loss=8.445941

Batch 168420, train_perplexity=6863.3833, train_loss=8.833956

Batch 168430, train_perplexity=5601.837, train_loss=8.63085

Batch 168440, train_perplexity=5388.0044, train_loss=8.59193

Batch 168450, train_perplexity=5396.747, train_loss=8.593552

Batch 168460, train_perplexity=4547.2607, train_loss=8.42228

Batch 168470, train_perplexity=4983.6553, train_loss=8.513919

Batch 168480, train_perplexity=5345.7026, train_loss=8.584048

Batch 168490, train_perplexity=5631.1787, train_loss=8.636074

Batch 168500, train_perplexity=6134.2754, train_loss=8.721647

Batch 168510, train_perplexity=5229.8687, train_loss=8.562141

Batch 168520, train_perplexity=4709.466, train_loss=8.45733

Batch 168530, train_perplexity=5118.355, train_loss=8.540588

Batch 168540, train_perplexity=5348.594, train_loss=8.584589

Batch 168550, train_perplexity=4475.4346, train_loss=8.406359

Batch 168560, train_perplexity=5479.815, train_loss=8.608827

Batch 168570, train_perplexity=6704.93, train_loss=8.810598

Batch 168580, train_perplexity=5593.552, train_loss=8.62937

Batch 168590, train_perplexity=5704.93, train_loss=8.649086

Batch 168600, train_perplexity=4968.659, train_loss=8.510905

Batch 168610, train_perplexity=6053.894, train_loss=8.708457

Batch 168620, train_perplexity=5614.4214, train_loss=8.633094

Batch 168630, train_perplexity=5538.5063, train_loss=8.61948

Batch 168640, train_perplexity=5723.404, train_loss=8.652319

Batch 168650, train_perplexity=5129.927, train_loss=8.542847

Batch 168660, train_perplexity=5652.2534, train_loss=8.63981

Batch 168670, train_perplexity=5123.346, train_loss=8.541563

Batch 168680, train_perplexity=5310.2363, train_loss=8.577392

Batch 168690, train_perplexity=4875.5454, train_loss=8.491987

Batch 168700, train_perplexity=5533.148, train_loss=8.618512

Batch 168710, train_perplexity=5448.429, train_loss=8.603083

Batch 168720, train_perplexity=6786.347, train_loss=8.822668

Batch 168730, train_perplexity=5028.892, train_loss=8.522955

Batch 168740, train_perplexity=4433.2437, train_loss=8.396887

Batch 168750, train_perplexity=6018.6924, train_loss=8.702625

Batch 168760, train_perplexity=4769.5396, train_loss=8.470005

Batch 168770, train_perplexity=4950.5674, train_loss=8.507257

Batch 168780, train_perplexity=4293.685, train_loss=8.364901

Batch 168790, train_perplexity=5823.0557, train_loss=8.66958

Batch 168800, train_perplexity=4657.1475, train_loss=8.446158

Batch 168810, train_perplexity=5554.481, train_loss=8.62236

Batch 168820, train_perplexity=6606.314, train_loss=8.795781

Batch 168830, train_perplexity=4617.17, train_loss=8.437537

Batch 168840, train_perplexity=5706.8564, train_loss=8.649424

Batch 168850, train_perplexity=5282.28, train_loss=8.572113

Batch 168860, train_perplexity=5462.581, train_loss=8.605677

Batch 168870, train_perplexity=5864.2173, train_loss=8.676624

Batch 168880, train_perplexity=5437.254, train_loss=8.601029

Batch 168890, train_perplexity=5725.8877, train_loss=8.652753

Batch 168900, train_perplexity=6313.24, train_loss=8.750404

Batch 168910, train_perplexity=4255.313, train_loss=8.355924

Batch 168920, train_perplexity=4687.38, train_loss=8.452629

Batch 168930, train_perplexity=5682.938, train_loss=8.645224

Batch 168940, train_perplexity=4627.066, train_loss=8.439678

Batch 168950, train_perplexity=6090.6606, train_loss=8.714512

Batch 168960, train_perplexity=5883.139, train_loss=8.679846

Batch 168970, train_perplexity=4864.4775, train_loss=8.489715

Batch 168980, train_perplexity=4324.481, train_loss=8.372047

Batch 168990, train_perplexity=4489.5884, train_loss=8.409516

Batch 169000, train_perplexity=5694.4717, train_loss=8.647251

Batch 169010, train_perplexity=5064.4395, train_loss=8.529999

Batch 169020, train_perplexity=6770.7417, train_loss=8.820366

Batch 169030, train_perplexity=5453.108, train_loss=8.603941
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 169040, train_perplexity=4892.6396, train_loss=8.495487

Batch 169050, train_perplexity=5570.9795, train_loss=8.625326

Batch 169060, train_perplexity=5679.568, train_loss=8.64463

Batch 169070, train_perplexity=4193.23, train_loss=8.341227

Batch 169080, train_perplexity=4526.6914, train_loss=8.417747

Batch 169090, train_perplexity=5320.075, train_loss=8.579243

Batch 169100, train_perplexity=5298.273, train_loss=8.575136

Batch 169110, train_perplexity=5665.962, train_loss=8.642232

Batch 169120, train_perplexity=4163.853, train_loss=8.334196

Batch 169130, train_perplexity=5373.3545, train_loss=8.589208

Batch 169140, train_perplexity=5952.0225, train_loss=8.691486

Batch 169150, train_perplexity=4991.4414, train_loss=8.51548

Batch 169160, train_perplexity=5054.495, train_loss=8.528033

Batch 169170, train_perplexity=4680.7153, train_loss=8.451206

Batch 169180, train_perplexity=5667.1235, train_loss=8.642437

Batch 169190, train_perplexity=6102.964, train_loss=8.71653

Batch 169200, train_perplexity=5783.6006, train_loss=8.662782

Batch 169210, train_perplexity=4707.683, train_loss=8.456951

Batch 169220, train_perplexity=4953.8164, train_loss=8.507914

Batch 169230, train_perplexity=5220.515, train_loss=8.560351

Batch 169240, train_perplexity=5017.763, train_loss=8.52074

Batch 169250, train_perplexity=6059.9883, train_loss=8.709463

Batch 169260, train_perplexity=5063.512, train_loss=8.529816

Batch 169270, train_perplexity=5307.325, train_loss=8.576843

Batch 169280, train_perplexity=5319.487, train_loss=8.579132

Batch 169290, train_perplexity=5079.2793, train_loss=8.532925

Batch 169300, train_perplexity=5274.9, train_loss=8.570715

Batch 169310, train_perplexity=4966.466, train_loss=8.510464

Batch 169320, train_perplexity=6048.844, train_loss=8.707623

Batch 169330, train_perplexity=4872.2266, train_loss=8.491306

Batch 169340, train_perplexity=4818.362, train_loss=8.480189

Batch 169350, train_perplexity=5936.739, train_loss=8.688915

Batch 169360, train_perplexity=7341.3887, train_loss=8.901283

Batch 169370, train_perplexity=5505.5244, train_loss=8.613507

Batch 169380, train_perplexity=5375.4253, train_loss=8.589593

Batch 169390, train_perplexity=5682.8076, train_loss=8.645201

Batch 169400, train_perplexity=5801.803, train_loss=8.665924

Batch 169410, train_perplexity=4458.0884, train_loss=8.402475

Batch 169420, train_perplexity=5384.3677, train_loss=8.591255

Batch 169430, train_perplexity=5026.854, train_loss=8.52255

Batch 169440, train_perplexity=6113.2046, train_loss=8.718206

Batch 169450, train_perplexity=5240.153, train_loss=8.564106

Batch 169460, train_perplexity=5608.9414, train_loss=8.632117

Batch 169470, train_perplexity=5196.1616, train_loss=8.5556755

Batch 169480, train_perplexity=5190.4116, train_loss=8.554568

Batch 169490, train_perplexity=5169.7373, train_loss=8.550577

Batch 169500, train_perplexity=5548.8687, train_loss=8.621349

Batch 169510, train_perplexity=5737.416, train_loss=8.654764

Batch 169520, train_perplexity=5338.8096, train_loss=8.582758

Batch 169530, train_perplexity=5770.323, train_loss=8.660483

Batch 169540, train_perplexity=5958.963, train_loss=8.692652

Batch 169550, train_perplexity=5066.594, train_loss=8.530424

Batch 169560, train_perplexity=5247.7646, train_loss=8.5655575

Batch 169570, train_perplexity=5045.9365, train_loss=8.526339

Batch 169580, train_perplexity=4949.722, train_loss=8.507087

Batch 169590, train_perplexity=4759.116, train_loss=8.467817

Batch 169600, train_perplexity=5564.7085, train_loss=8.6242

Batch 169610, train_perplexity=4910.59, train_loss=8.499149

Batch 169620, train_perplexity=5272.0986, train_loss=8.570184

Batch 169630, train_perplexity=5453.3364, train_loss=8.603983

Batch 169640, train_perplexity=4889.477, train_loss=8.494841

Batch 169650, train_perplexity=5338.881, train_loss=8.582771

Batch 169660, train_perplexity=5250.8384, train_loss=8.566143

Batch 169670, train_perplexity=7942.598, train_loss=8.979996

Batch 169680, train_perplexity=4985.68, train_loss=8.514325

Batch 169690, train_perplexity=4715.6094, train_loss=8.458633

Batch 169700, train_perplexity=5736.9126, train_loss=8.654676

Batch 169710, train_perplexity=5702.6562, train_loss=8.648687

Batch 169720, train_perplexity=5194.1006, train_loss=8.555279

Batch 169730, train_perplexity=5612.3765, train_loss=8.63273

Batch 169740, train_perplexity=4277.0474, train_loss=8.361018

Batch 169750, train_perplexity=5633.7085, train_loss=8.636523

Batch 169760, train_perplexity=4736.92, train_loss=8.463142

Batch 169770, train_perplexity=5013.0615, train_loss=8.519802

Batch 169780, train_perplexity=4732.806, train_loss=8.462274

Batch 169790, train_perplexity=5816.8174, train_loss=8.668509

Batch 169800, train_perplexity=5839.6616, train_loss=8.672428

Batch 169810, train_perplexity=5394.051, train_loss=8.593052

Batch 169820, train_perplexity=5494.043, train_loss=8.61142

Batch 169830, train_perplexity=5078.8335, train_loss=8.532837

Batch 169840, train_perplexity=4809.0566, train_loss=8.478256

Batch 169850, train_perplexity=5228.901, train_loss=8.561956

Batch 169860, train_perplexity=5374.5176, train_loss=8.589424

Batch 169870, train_perplexity=4996.4897, train_loss=8.516491

Batch 169880, train_perplexity=4966.1577, train_loss=8.510402

Batch 169890, train_perplexity=4937.2856, train_loss=8.504571

Batch 169900, train_perplexity=5070.8, train_loss=8.531254

Batch 169910, train_perplexity=4774.659, train_loss=8.471078

Batch 169920, train_perplexity=5787.899, train_loss=8.663525

Batch 169930, train_perplexity=5525.3486, train_loss=8.617102

Batch 169940, train_perplexity=4884.2666, train_loss=8.493774

Batch 169950, train_perplexity=5168.653, train_loss=8.550367

Batch 169960, train_perplexity=5250.648, train_loss=8.566107

Batch 169970, train_perplexity=5025.411, train_loss=8.522263

Batch 169980, train_perplexity=6052.5723, train_loss=8.708239

Batch 169990, train_perplexity=4813.8564, train_loss=8.479254

Batch 170000, train_perplexity=5341.9263, train_loss=8.583342

Batch 170010, train_perplexity=5827.278, train_loss=8.670305

Batch 170020, train_perplexity=4441.6484, train_loss=8.398781

Batch 170030, train_perplexity=5233.2314, train_loss=8.562784

Batch 170040, train_perplexity=4558.763, train_loss=8.424807

Batch 170050, train_perplexity=5925.466, train_loss=8.687015

Batch 170060, train_perplexity=4965.992, train_loss=8.510368

Batch 170070, train_perplexity=5529.033, train_loss=8.617768

Batch 170080, train_perplexity=5067.7056, train_loss=8.530643

Batch 170090, train_perplexity=5464.436, train_loss=8.606016

Batch 170100, train_perplexity=5114.2563, train_loss=8.539787

Batch 170110, train_perplexity=6208.858, train_loss=8.733732

Batch 170120, train_perplexity=5447.354, train_loss=8.602885

Batch 170130, train_perplexity=5411.167, train_loss=8.59622

Batch 170140, train_perplexity=7549.52, train_loss=8.929239

Batch 170150, train_perplexity=5222.009, train_loss=8.560637

Batch 170160, train_perplexity=4422.109, train_loss=8.394372

Batch 170170, train_perplexity=5846.2705, train_loss=8.673559

Batch 170180, train_perplexity=5470.9536, train_loss=8.607208

Batch 170190, train_perplexity=5517.2295, train_loss=8.615631

Batch 170200, train_perplexity=4249.7085, train_loss=8.354606

Batch 170210, train_perplexity=5889.9717, train_loss=8.681006

Batch 170220, train_perplexity=5130.2056, train_loss=8.542901

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Batch 170230, train_perplexity=6280.536, train_loss=8.745211

Batch 170240, train_perplexity=4972.565, train_loss=8.511691

Batch 170250, train_perplexity=6616.888, train_loss=8.79738

Batch 170260, train_perplexity=5046.514, train_loss=8.526453

Batch 170270, train_perplexity=5156.1675, train_loss=8.547949

Batch 170280, train_perplexity=5755.336, train_loss=8.657883
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 170290, train_perplexity=5881.0356, train_loss=8.679488

Batch 170300, train_perplexity=5688.127, train_loss=8.646136

Batch 170310, train_perplexity=5609.61, train_loss=8.6322365

Batch 170320, train_perplexity=6051.0254, train_loss=8.707983

Batch 170330, train_perplexity=5685.93, train_loss=8.64575

Batch 170340, train_perplexity=6844.715, train_loss=8.831232

Batch 170350, train_perplexity=4895.4307, train_loss=8.4960575

Batch 170360, train_perplexity=6129.784, train_loss=8.720915

Batch 170370, train_perplexity=4813.4707, train_loss=8.479174

Batch 170380, train_perplexity=4118.719, train_loss=8.3232975

Batch 170390, train_perplexity=5568.6846, train_loss=8.624914

Batch 170400, train_perplexity=6035.0156, train_loss=8.705334

Batch 170410, train_perplexity=5368.9287, train_loss=8.588384

Batch 170420, train_perplexity=5050.424, train_loss=8.527227

Batch 170430, train_perplexity=5844.208, train_loss=8.673206

Batch 170440, train_perplexity=5071.98, train_loss=8.5314865

Batch 170450, train_perplexity=5103.771, train_loss=8.537735

Batch 170460, train_perplexity=5385.9907, train_loss=8.591557

Batch 170470, train_perplexity=5264.286, train_loss=8.568701

Batch 170480, train_perplexity=5587.708, train_loss=8.6283245

Batch 170490, train_perplexity=5654.114, train_loss=8.640139

Batch 170500, train_perplexity=5392.3945, train_loss=8.592745

Batch 170510, train_perplexity=5762.393, train_loss=8.659108

Batch 170520, train_perplexity=6472.114, train_loss=8.775258

Batch 170530, train_perplexity=5546.731, train_loss=8.620964

Batch 170540, train_perplexity=5942.93, train_loss=8.689958

Batch 170550, train_perplexity=5478.253, train_loss=8.6085415

Batch 170560, train_perplexity=5898.746, train_loss=8.682495

Batch 170570, train_perplexity=5134.6597, train_loss=8.543769

Batch 170580, train_perplexity=5021.971, train_loss=8.521578

Batch 170590, train_perplexity=5590.1387, train_loss=8.628759

Batch 170600, train_perplexity=4912.5947, train_loss=8.4995575

Batch 170610, train_perplexity=5979.9585, train_loss=8.696169

Batch 170620, train_perplexity=4630.2925, train_loss=8.440375

Batch 170630, train_perplexity=4623.916, train_loss=8.438997

Batch 170640, train_perplexity=5326.548, train_loss=8.580459

Batch 170650, train_perplexity=5449.7803, train_loss=8.603331

Batch 170660, train_perplexity=5781.0913, train_loss=8.662348

Batch 170670, train_perplexity=6424.743, train_loss=8.767912

Batch 170680, train_perplexity=5264.8984, train_loss=8.568817

Batch 170690, train_perplexity=4613.292, train_loss=8.436697

Batch 170700, train_perplexity=5442.1045, train_loss=8.601921

Batch 170710, train_perplexity=5102.564, train_loss=8.537498

Batch 170720, train_perplexity=7101.485, train_loss=8.868059

Batch 170730, train_perplexity=5465.598, train_loss=8.606229

Batch 170740, train_perplexity=6107.354, train_loss=8.717249

Batch 170750, train_perplexity=6174.6626, train_loss=8.7282095

Batch 170760, train_perplexity=5809.5327, train_loss=8.667255

Batch 170770, train_perplexity=5316.231, train_loss=8.57852

Batch 170780, train_perplexity=4556.503, train_loss=8.424311

Batch 170790, train_perplexity=5292.526, train_loss=8.574051

Batch 170800, train_perplexity=6051.943, train_loss=8.708135

Batch 170810, train_perplexity=6350.6675, train_loss=8.756315

Batch 170820, train_perplexity=5905.743, train_loss=8.683681

Batch 170830, train_perplexity=5378.671, train_loss=8.590197

Batch 170840, train_perplexity=5004.6304, train_loss=8.518119

Batch 170850, train_perplexity=4536.5405, train_loss=8.41992

Batch 170860, train_perplexity=4875.2896, train_loss=8.491935

Batch 170870, train_perplexity=6053.3975, train_loss=8.708375

Batch 170880, train_perplexity=4971.375, train_loss=8.511452

Batch 170890, train_perplexity=5906.0415, train_loss=8.683731

Batch 170900, train_perplexity=4381.9336, train_loss=8.385245

Batch 170910, train_perplexity=5057.34, train_loss=8.528596

Batch 170920, train_perplexity=5639.821, train_loss=8.637608

Batch 170930, train_perplexity=5049.4507, train_loss=8.527035

Batch 170940, train_perplexity=5601.8154, train_loss=8.630846

Batch 170950, train_perplexity=6573.502, train_loss=8.790802

Batch 170960, train_perplexity=5001.7964, train_loss=8.517552

Batch 170970, train_perplexity=4468.585, train_loss=8.404827

Batch 170980, train_perplexity=6345.982, train_loss=8.755577

Batch 170990, train_perplexity=5447.894, train_loss=8.602984

Batch 171000, train_perplexity=4908.4033, train_loss=8.498704

Batch 171010, train_perplexity=5946.321, train_loss=8.690528

Batch 171020, train_perplexity=4762.7485, train_loss=8.46858

Batch 171030, train_perplexity=4963.123, train_loss=8.50979

Batch 171040, train_perplexity=7766.7812, train_loss=8.957611

Batch 171050, train_perplexity=5078.5093, train_loss=8.532773

Batch 171060, train_perplexity=5662.0293, train_loss=8.641538

Batch 171070, train_perplexity=4462.376, train_loss=8.403437

Batch 171080, train_perplexity=5804.3936, train_loss=8.66637

Batch 171090, train_perplexity=4857.561, train_loss=8.488292

Batch 171100, train_perplexity=6459.7812, train_loss=8.773351

Batch 171110, train_perplexity=4874.7456, train_loss=8.491823

Batch 171120, train_perplexity=4897.322, train_loss=8.496444

Batch 171130, train_perplexity=5150.57, train_loss=8.546863

Batch 171140, train_perplexity=5695.3516, train_loss=8.647406

Batch 171150, train_perplexity=5509.784, train_loss=8.614281

Batch 171160, train_perplexity=5638.659, train_loss=8.637402

Batch 171170, train_perplexity=6115.269, train_loss=8.718544

Batch 171180, train_perplexity=5707.4116, train_loss=8.649521

Batch 171190, train_perplexity=6222.545, train_loss=8.735934

Batch 171200, train_perplexity=5972.0366, train_loss=8.694843

Batch 171210, train_perplexity=5622.035, train_loss=8.634449

Batch 171220, train_perplexity=4606.75, train_loss=8.435278

Batch 171230, train_perplexity=4622.117, train_loss=8.438608

Batch 171240, train_perplexity=6384.0903, train_loss=8.761564

Batch 171250, train_perplexity=5282.713, train_loss=8.572195

Batch 171260, train_perplexity=4277.145, train_loss=8.361041

Batch 171270, train_perplexity=5224.6045, train_loss=8.561134

Batch 171280, train_perplexity=5447.681, train_loss=8.602945

Batch 171290, train_perplexity=4848.3047, train_loss=8.486384

Batch 171300, train_perplexity=5309.9375, train_loss=8.577335

Batch 171310, train_perplexity=4954.2466, train_loss=8.508

Batch 171320, train_perplexity=5453.108, train_loss=8.603941

Batch 171330, train_perplexity=4752.032, train_loss=8.466328

Batch 171340, train_perplexity=5898.6, train_loss=8.68247

Batch 171350, train_perplexity=5291.4663, train_loss=8.573851

Batch 171360, train_perplexity=5839.779, train_loss=8.672448

Batch 171370, train_perplexity=4831.218, train_loss=8.482854

Batch 171380, train_perplexity=5501.3516, train_loss=8.612749

Batch 171390, train_perplexity=5329.617, train_loss=8.581035

Batch 171400, train_perplexity=5666.5723, train_loss=8.64234

Batch 171410, train_perplexity=5346.8955, train_loss=8.584271

Batch 171420, train_perplexity=5442.7944, train_loss=8.602048

Batch 171430, train_perplexity=5612.7246, train_loss=8.6327915

Batch 171440, train_perplexity=5190.1343, train_loss=8.554515

Batch 171450, train_perplexity=4734.924, train_loss=8.462721

Batch 171460, train_perplexity=5198.9277, train_loss=8.556208

Batch 171470, train_perplexity=5912.218, train_loss=8.684776

Batch 171480, train_perplexity=4817.7188, train_loss=8.480056

Batch 171490, train_perplexity=5961.3843, train_loss=8.693058

Batch 171500, train_perplexity=4806.3696, train_loss=8.477697

Batch 171510, train_perplexity=4481.1914, train_loss=8.407644

Batch 171520, train_perplexity=4966.5034, train_loss=8.510471

Batch 171530, train_perplexity=6693.1494, train_loss=8.80884

Batch 171540, train_perplexity=5141.731, train_loss=8.545145

Batch 171550, train_perplexity=4416.377, train_loss=8.393075

Batch 171560, train_perplexity=5142.6577, train_loss=8.545325

Batch 171570, train_perplexity=5331.579, train_loss=8.581403

Batch 171580, train_perplexity=5029.2085, train_loss=8.523018

Batch 171590, train_perplexity=5421.26, train_loss=8.5980835

Batch 171600, train_perplexity=5516.672, train_loss=8.61553
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 171610, train_perplexity=5632.763, train_loss=8.636355

Batch 171620, train_perplexity=5419.4297, train_loss=8.597746

Batch 171630, train_perplexity=5228.4424, train_loss=8.561869

Batch 171640, train_perplexity=5356.9863, train_loss=8.586157

Batch 171650, train_perplexity=5095.4062, train_loss=8.536095

Batch 171660, train_perplexity=5077.1143, train_loss=8.532498

Batch 171670, train_perplexity=4651.621, train_loss=8.444971

Batch 171680, train_perplexity=4559.8545, train_loss=8.425046

Batch 171690, train_perplexity=4886.074, train_loss=8.494144

Batch 171700, train_perplexity=6380.1704, train_loss=8.76095

Batch 171710, train_perplexity=5209.345, train_loss=8.558209

Batch 171720, train_perplexity=6001.188, train_loss=8.699713

Batch 171730, train_perplexity=7241.6733, train_loss=8.887608

Batch 171740, train_perplexity=4613.9785, train_loss=8.436846

Batch 171750, train_perplexity=5197.5146, train_loss=8.555936

Batch 171760, train_perplexity=4699.277, train_loss=8.455164

Batch 171770, train_perplexity=4436.7544, train_loss=8.397678

Batch 171780, train_perplexity=5364.7163, train_loss=8.587599

Batch 171790, train_perplexity=5242.5923, train_loss=8.564571

Batch 171800, train_perplexity=6134.1353, train_loss=8.721624

Batch 171810, train_perplexity=5549.2495, train_loss=8.621418

Batch 171820, train_perplexity=6371.4756, train_loss=8.759586

Batch 171830, train_perplexity=6586.7427, train_loss=8.792814

Batch 171840, train_perplexity=4345.9385, train_loss=8.376997

Batch 171850, train_perplexity=4430.2686, train_loss=8.396215

Batch 171860, train_perplexity=4797.2476, train_loss=8.475798

Batch 171870, train_perplexity=4615.5454, train_loss=8.437185

Batch 171880, train_perplexity=5239.029, train_loss=8.563891

Batch 171890, train_perplexity=4884.164, train_loss=8.493753

Batch 171900, train_perplexity=4508.0757, train_loss=8.413626

Batch 171910, train_perplexity=4785.6914, train_loss=8.473386

Batch 171920, train_perplexity=5478.2734, train_loss=8.608545

Batch 171930, train_perplexity=4992.9077, train_loss=8.515774

Batch 171940, train_perplexity=5109.4155, train_loss=8.53884

Batch 171950, train_perplexity=6833.4966, train_loss=8.829592

Batch 171960, train_perplexity=4710.849, train_loss=8.4576235

Batch 171970, train_perplexity=5341.356, train_loss=8.583235

Batch 171980, train_perplexity=5442.9814, train_loss=8.602082

Batch 171990, train_perplexity=5240.178, train_loss=8.564111

Batch 172000, train_perplexity=5628.6553, train_loss=8.635626

Batch 172010, train_perplexity=5206.718, train_loss=8.557705

Batch 172020, train_perplexity=6094.6934, train_loss=8.715174

Batch 172030, train_perplexity=4891.5386, train_loss=8.495262

Batch 172040, train_perplexity=5151.336, train_loss=8.547011

Batch 172050, train_perplexity=5682.7427, train_loss=8.645189

Batch 172060, train_perplexity=4616.7866, train_loss=8.437454

Batch 172070, train_perplexity=4543.4766, train_loss=8.421448

Batch 172080, train_perplexity=4878.894, train_loss=8.492674

Batch 172090, train_perplexity=4918.3135, train_loss=8.500721

Batch 172100, train_perplexity=5998.156, train_loss=8.699207

Batch 172110, train_perplexity=5712.04, train_loss=8.6503315

Batch 172120, train_perplexity=5012.2773, train_loss=8.519646

Batch 172130, train_perplexity=5596.588, train_loss=8.629912

Batch 172140, train_perplexity=5324.562, train_loss=8.580086

Batch 172150, train_perplexity=5117.242, train_loss=8.540371

Batch 172160, train_perplexity=6198.9653, train_loss=8.732138

Batch 172170, train_perplexity=5357.681, train_loss=8.586287

Batch 172180, train_perplexity=5316.023, train_loss=8.578481

Batch 172190, train_perplexity=5912.4717, train_loss=8.684819

Batch 172200, train_perplexity=6030.091, train_loss=8.704517

Batch 172210, train_perplexity=5384.8506, train_loss=8.591345

Batch 172220, train_perplexity=5801.167, train_loss=8.665814

Batch 172230, train_perplexity=5951.319, train_loss=8.691368

Batch 172240, train_perplexity=4505.368, train_loss=8.413025

Batch 172250, train_perplexity=5283.0757, train_loss=8.572264

Batch 172260, train_perplexity=5547.419, train_loss=8.621088

Batch 172270, train_perplexity=6053.2476, train_loss=8.70835

Batch 172280, train_perplexity=5055.927, train_loss=8.5283165

Batch 172290, train_perplexity=4994.1177, train_loss=8.516016

Batch 172300, train_perplexity=4186.0537, train_loss=8.339514

Batch 172310, train_perplexity=5449.2295, train_loss=8.6032295

Batch 172320, train_perplexity=5101.805, train_loss=8.53735

Batch 172330, train_perplexity=5872.7354, train_loss=8.678076

Batch 172340, train_perplexity=5454.897, train_loss=8.604269

Batch 172350, train_perplexity=6379.209, train_loss=8.760799

Batch 172360, train_perplexity=4883.07, train_loss=8.493529

Batch 172370, train_perplexity=4896.803, train_loss=8.496338

Batch 172380, train_perplexity=5585.204, train_loss=8.627876

Batch 172390, train_perplexity=5242.1675, train_loss=8.56449

Batch 172400, train_perplexity=4527.1533, train_loss=8.417849

Batch 172410, train_perplexity=4829.8496, train_loss=8.482571

Batch 172420, train_perplexity=4888.526, train_loss=8.494646

Batch 172430, train_perplexity=6159.136, train_loss=8.725692

Batch 172440, train_perplexity=5333.9287, train_loss=8.581843

Batch 172450, train_perplexity=6054.7773, train_loss=8.708603

Batch 172460, train_perplexity=5782.6025, train_loss=8.662609

Batch 172470, train_perplexity=4768.4844, train_loss=8.469784

Batch 172480, train_perplexity=5242.032, train_loss=8.564465

Batch 172490, train_perplexity=5106.8384, train_loss=8.538336

Batch 172500, train_perplexity=5405.9316, train_loss=8.595252

Batch 172510, train_perplexity=5278.614, train_loss=8.571419

Batch 172520, train_perplexity=5437.1914, train_loss=8.601018

Batch 172530, train_perplexity=4985.6377, train_loss=8.514317

Batch 172540, train_perplexity=5507.8926, train_loss=8.613937

Batch 172550, train_perplexity=6398.652, train_loss=8.763843

Batch 172560, train_perplexity=5663.758, train_loss=8.641843

Batch 172570, train_perplexity=5030.3164, train_loss=8.523238

Batch 172580, train_perplexity=5617.0938, train_loss=8.63357

Batch 172590, train_perplexity=5708.282, train_loss=8.649673

Batch 172600, train_perplexity=6244.6714, train_loss=8.739484

Batch 172610, train_perplexity=5790.4883, train_loss=8.663972

Batch 172620, train_perplexity=4886.9224, train_loss=8.494318

Batch 172630, train_perplexity=5823.3115, train_loss=8.669624

Batch 172640, train_perplexity=6178.173, train_loss=8.728778

Batch 172650, train_perplexity=5089.7334, train_loss=8.534981

Batch 172660, train_perplexity=5586.776, train_loss=8.628158

Batch 172670, train_perplexity=5866.3877, train_loss=8.676994

Batch 172680, train_perplexity=4589.3154, train_loss=8.431486

Batch 172690, train_perplexity=5144.703, train_loss=8.545723

Batch 172700, train_perplexity=4573.6997, train_loss=8.428078

Batch 172710, train_perplexity=5672.6553, train_loss=8.643413

Batch 172720, train_perplexity=5195.914, train_loss=8.555628

Batch 172730, train_perplexity=5411.6313, train_loss=8.596306

Batch 172740, train_perplexity=5270.6655, train_loss=8.569912

Batch 172750, train_perplexity=5810.419, train_loss=8.667408

Batch 172760, train_perplexity=6099.054, train_loss=8.715889

Batch 172770, train_perplexity=5019.9507, train_loss=8.521175

Batch 172780, train_perplexity=5185.1724, train_loss=8.553558

Batch 172790, train_perplexity=4635.8555, train_loss=8.441576

Batch 172800, train_perplexity=5569.0776, train_loss=8.624985

Batch 172810, train_perplexity=4542.229, train_loss=8.421173

Batch 172820, train_perplexity=4960.8135, train_loss=8.509325

Batch 172830, train_perplexity=5252.5815, train_loss=8.566475

Batch 172840, train_perplexity=4681.5547, train_loss=8.4513855

Batch 172850, train_perplexity=4642.988, train_loss=8.443113

Batch 172860, train_perplexity=5140.4463, train_loss=8.544895

Batch 172870, train_perplexity=5957.2017, train_loss=8.692356

Batch 172880, train_perplexity=4783.159, train_loss=8.4728565

Batch 172890, train_perplexity=5407.8857, train_loss=8.5956135

Batch 172900, train_perplexity=5231.0557, train_loss=8.562368

Batch 172910, train_perplexity=4013.2268, train_loss=8.297351
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 172920, train_perplexity=5218.4893, train_loss=8.559963

Batch 172930, train_perplexity=5070.577, train_loss=8.53121

Batch 172940, train_perplexity=5254.866, train_loss=8.56691

Batch 172950, train_perplexity=6170.5005, train_loss=8.727535

Batch 172960, train_perplexity=5416.588, train_loss=8.597221

Batch 172970, train_perplexity=4782.817, train_loss=8.472785

Batch 172980, train_perplexity=5032.5957, train_loss=8.523691

Batch 172990, train_perplexity=6472.5767, train_loss=8.77533

Batch 173000, train_perplexity=5149.6265, train_loss=8.5466795

Batch 173010, train_perplexity=5248.676, train_loss=8.565731

Batch 173020, train_perplexity=4578.43, train_loss=8.4291115

Batch 173030, train_perplexity=5208.58, train_loss=8.558063

Batch 173040, train_perplexity=6370.0415, train_loss=8.759361

Batch 173050, train_perplexity=5995.1475, train_loss=8.698706

Batch 173060, train_perplexity=4734.824, train_loss=8.4627

Batch 173070, train_perplexity=7011.13, train_loss=8.855254

Batch 173080, train_perplexity=4628.9634, train_loss=8.440088

Batch 173090, train_perplexity=5523.268, train_loss=8.616725

Batch 173100, train_perplexity=5500.135, train_loss=8.612528

Batch 173110, train_perplexity=6198.5576, train_loss=8.732072

Batch 173120, train_perplexity=5663.6387, train_loss=8.641822

Batch 173130, train_perplexity=5085.454, train_loss=8.53414

Batch 173140, train_perplexity=4394.773, train_loss=8.388171

Batch 173150, train_perplexity=5313.6304, train_loss=8.578031

Batch 173160, train_perplexity=5061.7065, train_loss=8.529459

Batch 173170, train_perplexity=5207.984, train_loss=8.557948

Batch 173180, train_perplexity=6149.956, train_loss=8.7242

Batch 173190, train_perplexity=4864.0137, train_loss=8.489619

Batch 173200, train_perplexity=4975.1787, train_loss=8.512217

Batch 173210, train_perplexity=5880.5366, train_loss=8.679403

Batch 173220, train_perplexity=4639.221, train_loss=8.442302

Batch 173230, train_perplexity=5534.4673, train_loss=8.618751

Batch 173240, train_perplexity=4759.0166, train_loss=8.467796

Batch 173250, train_perplexity=5527.1304, train_loss=8.617424

Batch 173260, train_perplexity=5708.321, train_loss=8.64968

Batch 173270, train_perplexity=5374.6714, train_loss=8.589453

Batch 173280, train_perplexity=5083.8154, train_loss=8.533817

Batch 173290, train_perplexity=6491.914, train_loss=8.778313

Batch 173300, train_perplexity=7148.2734, train_loss=8.874626

Batch 173310, train_perplexity=6108.845, train_loss=8.717493

Batch 173320, train_perplexity=5011.221, train_loss=8.519435

Batch 173330, train_perplexity=5125.2812, train_loss=8.541941

Batch 173340, train_perplexity=4884.872, train_loss=8.493898

Batch 173350, train_perplexity=5917.4526, train_loss=8.685661

Batch 173360, train_perplexity=5447.5356, train_loss=8.602919

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Batch 173370, train_perplexity=5329.124, train_loss=8.580942

Batch 173380, train_perplexity=5934.7744, train_loss=8.688584

Batch 173390, train_perplexity=4462.3335, train_loss=8.403427

Batch 173400, train_perplexity=6324.66, train_loss=8.752212

Batch 173410, train_perplexity=4358.972, train_loss=8.379992

Batch 173420, train_perplexity=4796.3374, train_loss=8.475608

Batch 173430, train_perplexity=6036.057, train_loss=8.705506

Batch 173440, train_perplexity=5617.6353, train_loss=8.633666

Batch 173450, train_perplexity=5853.858, train_loss=8.674856

Batch 173460, train_perplexity=4998.525, train_loss=8.516898

Batch 173470, train_perplexity=6077.061, train_loss=8.712276

Batch 173480, train_perplexity=5888.247, train_loss=8.680714

Batch 173490, train_perplexity=5004.4824, train_loss=8.518089

Batch 173500, train_perplexity=4987.711, train_loss=8.514732

Batch 173510, train_perplexity=5529.656, train_loss=8.617881

Batch 173520, train_perplexity=5726.45, train_loss=8.652851

Batch 173530, train_perplexity=5575.2207, train_loss=8.626087

Batch 173540, train_perplexity=5090.1655, train_loss=8.535066

Batch 173550, train_perplexity=5888.9326, train_loss=8.68083

Batch 173560, train_perplexity=6048.2793, train_loss=8.707529

Batch 173570, train_perplexity=6548.7485, train_loss=8.787029

Batch 173580, train_perplexity=5290.366, train_loss=8.573643

Batch 173590, train_perplexity=5216.544, train_loss=8.55959

Batch 173600, train_perplexity=4840.7876, train_loss=8.484833

Batch 173610, train_perplexity=5088.1904, train_loss=8.5346775

Batch 173620, train_perplexity=5939.3325, train_loss=8.689352

Batch 173630, train_perplexity=4942.548, train_loss=8.505636

Batch 173640, train_perplexity=4959.1865, train_loss=8.508997

Batch 173650, train_perplexity=5699.1333, train_loss=8.648069

Batch 173660, train_perplexity=4856.4077, train_loss=8.488054

Batch 173670, train_perplexity=5015.486, train_loss=8.520286

Batch 173680, train_perplexity=5773.6313, train_loss=8.6610565

Batch 173690, train_perplexity=5518.7134, train_loss=8.6159

Batch 173700, train_perplexity=5237.885, train_loss=8.563673

Batch 173710, train_perplexity=6479.4507, train_loss=8.776391

Batch 173720, train_perplexity=6015.674, train_loss=8.702124

Batch 173730, train_perplexity=5073.2715, train_loss=8.531741

Batch 173740, train_perplexity=4602.6484, train_loss=8.434387

Batch 173750, train_perplexity=4794.869, train_loss=8.475302

Batch 173760, train_perplexity=5796.107, train_loss=8.664942

Batch 173770, train_perplexity=5158.164, train_loss=8.548336

Batch 173780, train_perplexity=5612.9062, train_loss=8.632824

Batch 173790, train_perplexity=5078.6543, train_loss=8.532802

Batch 173800, train_perplexity=5282.421, train_loss=8.57214

Batch 173810, train_perplexity=5651.569, train_loss=8.6396885

Batch 173820, train_perplexity=4785.8057, train_loss=8.47341

Batch 173830, train_perplexity=5000.7944, train_loss=8.517352

Batch 173840, train_perplexity=5145.135, train_loss=8.545807

Batch 173850, train_perplexity=7040.229, train_loss=8.859396

Batch 173860, train_perplexity=5185.0635, train_loss=8.553537

Batch 173870, train_perplexity=5440.506, train_loss=8.601627

Batch 173880, train_perplexity=4773.0293, train_loss=8.4707365

Batch 173890, train_perplexity=4465.135, train_loss=8.404055

Batch 173900, train_perplexity=4824.3623, train_loss=8.481434

Batch 173910, train_perplexity=5792.4766, train_loss=8.664315

Batch 173920, train_perplexity=5386.761, train_loss=8.5917

Batch 173930, train_perplexity=5250.418, train_loss=8.566063

Batch 173940, train_perplexity=5397.9976, train_loss=8.593783

Batch 173950, train_perplexity=5518.4663, train_loss=8.615855

Batch 173960, train_perplexity=5201.7646, train_loss=8.556753

Batch 173970, train_perplexity=5862.305, train_loss=8.676298

Batch 173980, train_perplexity=4897.0415, train_loss=8.496387

Batch 173990, train_perplexity=6748.5337, train_loss=8.8170805

Batch 174000, train_perplexity=6392.7656, train_loss=8.762922

Batch 174010, train_perplexity=6087.4204, train_loss=8.71398

Batch 174020, train_perplexity=5887.7026, train_loss=8.680621

Batch 174030, train_perplexity=6573.038, train_loss=8.790731

Batch 174040, train_perplexity=5162.869, train_loss=8.549248

Batch 174050, train_perplexity=6885.5947, train_loss=8.837187

Batch 174060, train_perplexity=4522.3936, train_loss=8.416797

Batch 174070, train_perplexity=4961.6084, train_loss=8.509485

Batch 174080, train_perplexity=4609.7915, train_loss=8.435938

Batch 174090, train_perplexity=4668.874, train_loss=8.448673

Batch 174100, train_perplexity=4602.9297, train_loss=8.434448

Batch 174110, train_perplexity=5766.5933, train_loss=8.659837

Batch 174120, train_perplexity=5617.324, train_loss=8.633611

Batch 174130, train_perplexity=5422.987, train_loss=8.598402

Batch 174140, train_perplexity=5689.684, train_loss=8.64641

Batch 174150, train_perplexity=5216.8174, train_loss=8.559643

Batch 174160, train_perplexity=5413.3345, train_loss=8.596621
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 174170, train_perplexity=5089.1465, train_loss=8.534865

Batch 174180, train_perplexity=4678.573, train_loss=8.450748

Batch 174190, train_perplexity=5087.768, train_loss=8.534595

Batch 174200, train_perplexity=5363.7393, train_loss=8.587417

Batch 174210, train_perplexity=5518.4766, train_loss=8.615857

Batch 174220, train_perplexity=4738.3657, train_loss=8.463448

Batch 174230, train_perplexity=6261.781, train_loss=8.74222

Batch 174240, train_perplexity=6256.027, train_loss=8.741301

Batch 174250, train_perplexity=5904.6167, train_loss=8.68349

Batch 174260, train_perplexity=5661.392, train_loss=8.641425

Batch 174270, train_perplexity=5220.3013, train_loss=8.56031

Batch 174280, train_perplexity=5104.4233, train_loss=8.537863

Batch 174290, train_perplexity=5067.3047, train_loss=8.530564

Batch 174300, train_perplexity=5020.401, train_loss=8.521265

Batch 174310, train_perplexity=5458.946, train_loss=8.605011

Batch 174320, train_perplexity=4978.653, train_loss=8.512915

Batch 174330, train_perplexity=5589.4673, train_loss=8.628639

Batch 174340, train_perplexity=5269.6406, train_loss=8.569717

Batch 174350, train_perplexity=4681.9834, train_loss=8.451477

Batch 174360, train_perplexity=4545.635, train_loss=8.421923

Batch 174370, train_perplexity=6489.1655, train_loss=8.777889

Batch 174380, train_perplexity=5120.7085, train_loss=8.541048

Batch 174390, train_perplexity=5056.8623, train_loss=8.5285015

Batch 174400, train_perplexity=5518.6714, train_loss=8.615892

Batch 174410, train_perplexity=5743.35, train_loss=8.655798

Batch 174420, train_perplexity=4787.878, train_loss=8.473843

Batch 174430, train_perplexity=4615.7476, train_loss=8.437229

Batch 174440, train_perplexity=5807.948, train_loss=8.666983

Batch 174450, train_perplexity=5666.648, train_loss=8.642353

Batch 174460, train_perplexity=5149.838, train_loss=8.5467205

Batch 174470, train_perplexity=5269.359, train_loss=8.569664

Batch 174480, train_perplexity=5174.1763, train_loss=8.551435

Batch 174490, train_perplexity=6080.226, train_loss=8.712797

Batch 174500, train_perplexity=4415.5225, train_loss=8.392881

Batch 174510, train_perplexity=4836.4316, train_loss=8.4839325

Batch 174520, train_perplexity=5669.5454, train_loss=8.642864

Batch 174530, train_perplexity=5439.6187, train_loss=8.601464

Batch 174540, train_perplexity=4579.234, train_loss=8.429287

Batch 174550, train_perplexity=5262.3433, train_loss=8.568332

Batch 174560, train_perplexity=5487.6963, train_loss=8.610264

Batch 174570, train_perplexity=5607.5776, train_loss=8.631874

Batch 174580, train_perplexity=6377.9805, train_loss=8.760607

Batch 174590, train_perplexity=4697.87, train_loss=8.4548645

Batch 174600, train_perplexity=4835.832, train_loss=8.4838085

Batch 174610, train_perplexity=5875.447, train_loss=8.678537

Batch 174620, train_perplexity=5563.5464, train_loss=8.623991

Batch 174630, train_perplexity=5951.4834, train_loss=8.691396

Batch 174640, train_perplexity=5751.846, train_loss=8.657276

Batch 174650, train_perplexity=5971.034, train_loss=8.694675

Batch 174660, train_perplexity=6233.9077, train_loss=8.737759

Batch 174670, train_perplexity=5011.1113, train_loss=8.519413

Batch 174680, train_perplexity=4566.6606, train_loss=8.4265375

Batch 174690, train_perplexity=5424.166, train_loss=8.598619

Batch 174700, train_perplexity=5135.855, train_loss=8.544002

Batch 174710, train_perplexity=5025.6987, train_loss=8.52232

Batch 174720, train_perplexity=5590.8745, train_loss=8.628891

Batch 174730, train_perplexity=5444.2793, train_loss=8.602321

Batch 174740, train_perplexity=5677.813, train_loss=8.644321

Batch 174750, train_perplexity=6550.2227, train_loss=8.787254

Batch 174760, train_perplexity=5065.831, train_loss=8.530273

Batch 174770, train_perplexity=4917.943, train_loss=8.500646

Batch 174780, train_perplexity=5187.131, train_loss=8.553936

Batch 174790, train_perplexity=4619.658, train_loss=8.438076

Batch 174800, train_perplexity=5411.347, train_loss=8.596253

Batch 174810, train_perplexity=5327.701, train_loss=8.580675

Batch 174820, train_perplexity=5069.4653, train_loss=8.530991

Batch 174830, train_perplexity=5398.5693, train_loss=8.593889

Batch 174840, train_perplexity=5466.2915, train_loss=8.606356

Batch 174850, train_perplexity=4875.6104, train_loss=8.492001

Batch 174860, train_perplexity=6208.4077, train_loss=8.73366

Batch 174870, train_perplexity=5660.4478, train_loss=8.641258

Batch 174880, train_perplexity=5203.764, train_loss=8.5571375

Batch 174890, train_perplexity=4898.8584, train_loss=8.4967575

Batch 174900, train_perplexity=4682.001, train_loss=8.451481

Batch 174910, train_perplexity=5015.462, train_loss=8.520281

Batch 174920, train_perplexity=5478.723, train_loss=8.608627

Batch 174930, train_perplexity=5608.5347, train_loss=8.632045

Batch 174940, train_perplexity=5220.032, train_loss=8.560259

Batch 174950, train_perplexity=4918.7637, train_loss=8.500813

Batch 174960, train_perplexity=5538.4854, train_loss=8.619476

Batch 174970, train_perplexity=6465.026, train_loss=8.774162

Batch 174980, train_perplexity=5195.8594, train_loss=8.555617

Batch 174990, train_perplexity=5398.6206, train_loss=8.593899

Batch 175000, train_perplexity=5413.216, train_loss=8.596599

Batch 175010, train_perplexity=5903.327, train_loss=8.683271

Batch 175020, train_perplexity=5320.5723, train_loss=8.579336

Batch 175030, train_perplexity=5515.504, train_loss=8.615318

Batch 175040, train_perplexity=5720.6533, train_loss=8.651838

Batch 175050, train_perplexity=5052.871, train_loss=8.527712

Batch 175060, train_perplexity=5518.366, train_loss=8.615837

Batch 175070, train_perplexity=5660.7393, train_loss=8.64131

Batch 175080, train_perplexity=4654.643, train_loss=8.445621

Batch 175090, train_perplexity=5924.409, train_loss=8.686836

Batch 175100, train_perplexity=5740.207, train_loss=8.655251

Batch 175110, train_perplexity=5358.345, train_loss=8.5864105

Batch 175120, train_perplexity=4566.4473, train_loss=8.426491

Batch 175130, train_perplexity=6049.837, train_loss=8.707787

Batch 175140, train_perplexity=4785.308, train_loss=8.473306

Batch 175150, train_perplexity=5059.062, train_loss=8.528936

Batch 175160, train_perplexity=5199.6714, train_loss=8.556351

Batch 175170, train_perplexity=5551.1655, train_loss=8.621763

Batch 175180, train_perplexity=5877.4307, train_loss=8.678875

Batch 175190, train_perplexity=5248.375, train_loss=8.565674

Batch 175200, train_perplexity=5606.583, train_loss=8.631697

Batch 175210, train_perplexity=5320.3286, train_loss=8.57929

Batch 175220, train_perplexity=5911.203, train_loss=8.684605

Batch 175230, train_perplexity=4618.8564, train_loss=8.437902

Batch 175240, train_perplexity=5499.385, train_loss=8.612391

Batch 175250, train_perplexity=6124.946, train_loss=8.720125

Batch 175260, train_perplexity=6950.004, train_loss=8.846498

Batch 175270, train_perplexity=4924.0156, train_loss=8.50188

Batch 175280, train_perplexity=5451.0176, train_loss=8.603558

Batch 175290, train_perplexity=5182.221, train_loss=8.552989

Batch 175300, train_perplexity=4607.4136, train_loss=8.435422

Batch 175310, train_perplexity=5541.375, train_loss=8.619998

Batch 175320, train_perplexity=5557.093, train_loss=8.62283

Batch 175330, train_perplexity=5362.036, train_loss=8.587099

Batch 175340, train_perplexity=5771.809, train_loss=8.660741

Batch 175350, train_perplexity=5098.381, train_loss=8.536678

Batch 175360, train_perplexity=7158.889, train_loss=8.87611

Batch 175370, train_perplexity=5151.528, train_loss=8.547049

Batch 175380, train_perplexity=6287.668, train_loss=8.7463455

Batch 175390, train_perplexity=4359.0386, train_loss=8.380007

Batch 175400, train_perplexity=4996.652, train_loss=8.516523

Batch 175410, train_perplexity=5639.073, train_loss=8.637475

Batch 175420, train_perplexity=5758.8057, train_loss=8.658485

Batch 175430, train_perplexity=5018.735, train_loss=8.520933

Batch 175440, train_perplexity=5011.5654, train_loss=8.519504

Batch 175450, train_perplexity=5143.668, train_loss=8.545522

Batch 175460, train_perplexity=5094.726, train_loss=8.535961

Batch 175470, train_perplexity=5800.105, train_loss=8.665631

Batch 175480, train_perplexity=4484.3037, train_loss=8.408339
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 175490, train_perplexity=5148.5317, train_loss=8.546467

Batch 175500, train_perplexity=4923.6587, train_loss=8.501807

Batch 175510, train_perplexity=6578.67, train_loss=8.791588

Batch 175520, train_perplexity=4955.5225, train_loss=8.508258

Batch 175530, train_perplexity=4916.2363, train_loss=8.5002985

Batch 175540, train_perplexity=4279.153, train_loss=8.36151

Batch 175550, train_perplexity=5585.641, train_loss=8.6279545

Batch 175560, train_perplexity=4701.55, train_loss=8.455647

Batch 175570, train_perplexity=5800.265, train_loss=8.665659

Batch 175580, train_perplexity=5063.0684, train_loss=8.529728

Batch 175590, train_perplexity=5823.933, train_loss=8.669731

Batch 175600, train_perplexity=4690.9575, train_loss=8.453392

Batch 175610, train_perplexity=5144.595, train_loss=8.545702

Batch 175620, train_perplexity=4933.3555, train_loss=8.503775

Batch 175630, train_perplexity=5854.8184, train_loss=8.67502

Batch 175640, train_perplexity=5183.8423, train_loss=8.553302

Batch 175650, train_perplexity=6026.343, train_loss=8.703896

Batch 175660, train_perplexity=5278.09, train_loss=8.57132

Batch 175670, train_perplexity=6707.962, train_loss=8.81105

Batch 175680, train_perplexity=4351.1055, train_loss=8.378185

Batch 175690, train_perplexity=5273.1494, train_loss=8.570383

Batch 175700, train_perplexity=6142.723, train_loss=8.723023

Batch 175710, train_perplexity=5777.2446, train_loss=8.661682

Batch 175720, train_perplexity=4686.714, train_loss=8.452487

Batch 175730, train_perplexity=4947.613, train_loss=8.50666

Batch 175740, train_perplexity=4992.275, train_loss=8.515647

Batch 175750, train_perplexity=6173.726, train_loss=8.728058

Batch 175760, train_perplexity=4882.5435, train_loss=8.493422

Batch 175770, train_perplexity=5121.0405, train_loss=8.541113

Batch 175780, train_perplexity=4713.748, train_loss=8.458239

Batch 175790, train_perplexity=5382.078, train_loss=8.59083

Batch 175800, train_perplexity=5426.7534, train_loss=8.599096

Batch 175810, train_perplexity=5715.8438, train_loss=8.650997

Batch 175820, train_perplexity=5647.9644, train_loss=8.6390505

Batch 175830, train_perplexity=4406.3687, train_loss=8.390806

Batch 175840, train_perplexity=5388.94, train_loss=8.592104

Batch 175850, train_perplexity=4782.676, train_loss=8.472755

Batch 175860, train_perplexity=5201.928, train_loss=8.556785

Batch 175870, train_perplexity=5977.3926, train_loss=8.69574

Batch 175880, train_perplexity=5690.118, train_loss=8.646486

Batch 175890, train_perplexity=5403.493, train_loss=8.594801

Batch 175900, train_perplexity=4998.902, train_loss=8.5169735

Batch 175910, train_perplexity=5228.4473, train_loss=8.56187

Batch 175920, train_perplexity=5301.71, train_loss=8.575785

Batch 175930, train_perplexity=4385.8594, train_loss=8.386141

Batch 175940, train_perplexity=5180.1704, train_loss=8.552593

Batch 175950, train_perplexity=5842.837, train_loss=8.672972

Batch 175960, train_perplexity=7046.805, train_loss=8.86033

Batch 175970, train_perplexity=5755.358, train_loss=8.6578865

Batch 175980, train_perplexity=5050.645, train_loss=8.527271

Batch 175990, train_perplexity=5358.775, train_loss=8.586491

Batch 176000, train_perplexity=5518.7188, train_loss=8.615901

Batch 176010, train_perplexity=4301.464, train_loss=8.366711

Batch 176020, train_perplexity=5530.1567, train_loss=8.617971

Batch 176030, train_perplexity=6425.8213, train_loss=8.76808

Batch 176040, train_perplexity=5710.0356, train_loss=8.649981

Batch 176050, train_perplexity=5871.3804, train_loss=8.677845

Batch 176060, train_perplexity=5476.2676, train_loss=8.608179

Batch 176070, train_perplexity=5920.546, train_loss=8.686184

Batch 176080, train_perplexity=5029.7935, train_loss=8.523134

Batch 176090, train_perplexity=4548.727, train_loss=8.422603

Batch 176100, train_perplexity=4590.528, train_loss=8.43175

Batch 176110, train_perplexity=5263.9946, train_loss=8.5686455

Batch 176120, train_perplexity=5096.9663, train_loss=8.536401

Batch 176130, train_perplexity=5154.26, train_loss=8.547579

Batch 176140, train_perplexity=4967.811, train_loss=8.510735

Batch 176150, train_perplexity=4660.32, train_loss=8.446839

Batch 176160, train_perplexity=6329.3364, train_loss=8.752951

Batch 176170, train_perplexity=5122.6523, train_loss=8.541428

Batch 176180, train_perplexity=5936.365, train_loss=8.688852

Batch 176190, train_perplexity=5116.1104, train_loss=8.54015

Batch 176200, train_perplexity=4779.402, train_loss=8.472071

Batch 176210, train_perplexity=4871.8364, train_loss=8.491226

Batch 176220, train_perplexity=4961.329, train_loss=8.509429

Batch 176230, train_perplexity=5453.139, train_loss=8.603947

Batch 176240, train_perplexity=4941.5107, train_loss=8.505426

Batch 176250, train_perplexity=4085.6772, train_loss=8.315243

Batch 176260, train_perplexity=5735.4736, train_loss=8.654426

Batch 176270, train_perplexity=4793.1733, train_loss=8.474948

Batch 176280, train_perplexity=5851.709, train_loss=8.674489

Batch 176290, train_perplexity=5077.7974, train_loss=8.532633

Batch 176300, train_perplexity=5753.8433, train_loss=8.657623

Batch 176310, train_perplexity=5838.3027, train_loss=8.672195

Batch 176320, train_perplexity=4945.7446, train_loss=8.506283

Batch 176330, train_perplexity=5806.9346, train_loss=8.666808

Batch 176340, train_perplexity=5450.0977, train_loss=8.603389

Batch 176350, train_perplexity=4788.3257, train_loss=8.473936

Batch 176360, train_perplexity=5151.4736, train_loss=8.547038

Batch 176370, train_perplexity=5690.943, train_loss=8.646631

Batch 176380, train_perplexity=5302.711, train_loss=8.5759735

Batch 176390, train_perplexity=4120.0786, train_loss=8.323627

Batch 176400, train_perplexity=5683.041, train_loss=8.645242

Batch 176410, train_perplexity=5322.755, train_loss=8.579746

Batch 176420, train_perplexity=6420.0024, train_loss=8.767174

Batch 176430, train_perplexity=6000.3525, train_loss=8.6995735

Batch 176440, train_perplexity=4961.6465, train_loss=8.509493

Batch 176450, train_perplexity=5949.832, train_loss=8.691118

Batch 176460, train_perplexity=4501.28, train_loss=8.412117

Batch 176470, train_perplexity=5323.176, train_loss=8.579825

Batch 176480, train_perplexity=4863.6567, train_loss=8.489546

Batch 176490, train_perplexity=6372.6543, train_loss=8.759771

Batch 176500, train_perplexity=4292.7925, train_loss=8.364693

Batch 176510, train_perplexity=5627.8447, train_loss=8.635482

Batch 176520, train_perplexity=5892.5503, train_loss=8.681444

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Batch 176530, train_perplexity=6073.3413, train_loss=8.711664

Batch 176540, train_perplexity=5316.54, train_loss=8.578578

Batch 176550, train_perplexity=4374.3345, train_loss=8.38351

Batch 176560, train_perplexity=4810.1484, train_loss=8.478483

Batch 176570, train_perplexity=5067.4253, train_loss=8.530588

Batch 176580, train_perplexity=4850.5986, train_loss=8.486857

Batch 176590, train_perplexity=5175.2275, train_loss=8.551639

Batch 176600, train_perplexity=6456.646, train_loss=8.772865

Batch 176610, train_perplexity=4020.5627, train_loss=8.299177

Batch 176620, train_perplexity=5451.527, train_loss=8.603651

Batch 176630, train_perplexity=4829.1543, train_loss=8.482427

Batch 176640, train_perplexity=4711.946, train_loss=8.457856

Batch 176650, train_perplexity=6545.514, train_loss=8.786535

Batch 176660, train_perplexity=5287.108, train_loss=8.573027

Batch 176670, train_perplexity=5589.659, train_loss=8.628674

Batch 176680, train_perplexity=4537.592, train_loss=8.420152

Batch 176690, train_perplexity=5667.502, train_loss=8.642504

Batch 176700, train_perplexity=5656.465, train_loss=8.640554

Batch 176710, train_perplexity=4850.89, train_loss=8.4869175

Batch 176720, train_perplexity=5768.502, train_loss=8.660168

Batch 176730, train_perplexity=5041.136, train_loss=8.525387
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 176740, train_perplexity=5179.4097, train_loss=8.552446

Batch 176750, train_perplexity=6959.482, train_loss=8.84786

Batch 176760, train_perplexity=4498.319, train_loss=8.411459

Batch 176770, train_perplexity=4559.0933, train_loss=8.424879

Batch 176780, train_perplexity=5644.0767, train_loss=8.638362

Batch 176790, train_perplexity=5418.9854, train_loss=8.597664

Batch 176800, train_perplexity=4518.5825, train_loss=8.415954

Batch 176810, train_perplexity=5052.784, train_loss=8.527695

Batch 176820, train_perplexity=4388.7544, train_loss=8.386801

Batch 176830, train_perplexity=6077.078, train_loss=8.712279

Batch 176840, train_perplexity=6282.903, train_loss=8.745587

Batch 176850, train_perplexity=5054.6445, train_loss=8.528063

Batch 176860, train_perplexity=4635.3604, train_loss=8.441469

Batch 176870, train_perplexity=6119.3174, train_loss=8.719206

Batch 176880, train_perplexity=5600.202, train_loss=8.630558

Batch 176890, train_perplexity=5373.7437, train_loss=8.58928

Batch 176900, train_perplexity=5527.9736, train_loss=8.617577

Batch 176910, train_perplexity=5974.8164, train_loss=8.695309

Batch 176920, train_perplexity=6302.4844, train_loss=8.748699

Batch 176930, train_perplexity=4939.786, train_loss=8.505077

Batch 176940, train_perplexity=5382.027, train_loss=8.59082

Batch 176950, train_perplexity=4470.273, train_loss=8.405205

Batch 176960, train_perplexity=6248.7104, train_loss=8.74013

Batch 176970, train_perplexity=5348.2114, train_loss=8.5845175

Batch 176980, train_perplexity=5869.5215, train_loss=8.677528

Batch 176990, train_perplexity=4435.629, train_loss=8.397425

Batch 177000, train_perplexity=5094.843, train_loss=8.535984

Batch 177010, train_perplexity=6004.6343, train_loss=8.700287

Batch 177020, train_perplexity=6085.6094, train_loss=8.713682

Batch 177030, train_perplexity=4908.0757, train_loss=8.498637

Batch 177040, train_perplexity=5420.65, train_loss=8.597971

Batch 177050, train_perplexity=4928.0513, train_loss=8.502699

Batch 177060, train_perplexity=5275.8857, train_loss=8.570902

Batch 177070, train_perplexity=5501.693, train_loss=8.612811

Batch 177080, train_perplexity=5516.698, train_loss=8.615535

Batch 177090, train_perplexity=5885.1597, train_loss=8.680189

Batch 177100, train_perplexity=5544.214, train_loss=8.62051

Batch 177110, train_perplexity=5250.648, train_loss=8.566107

Batch 177120, train_perplexity=5466.2393, train_loss=8.606346

Batch 177130, train_perplexity=5057.3545, train_loss=8.528599

Batch 177140, train_perplexity=5910.036, train_loss=8.684407

Batch 177150, train_perplexity=4560.0894, train_loss=8.425097

Batch 177160, train_perplexity=5573.9766, train_loss=8.625864

Batch 177170, train_perplexity=5813.3735, train_loss=8.667916

Batch 177180, train_perplexity=5441.5073, train_loss=8.601811

Batch 177190, train_perplexity=5128.9683, train_loss=8.54266

Batch 177200, train_perplexity=4954.384, train_loss=8.508028

Batch 177210, train_perplexity=5090.86, train_loss=8.535202

Batch 177220, train_perplexity=5882.365, train_loss=8.679714

Batch 177230, train_perplexity=5416.624, train_loss=8.597228

Batch 177240, train_perplexity=5009.7397, train_loss=8.519139

Batch 177250, train_perplexity=6505.89, train_loss=8.780463

Batch 177260, train_perplexity=4296.265, train_loss=8.365501

Batch 177270, train_perplexity=4284.04, train_loss=8.362652

Batch 177280, train_perplexity=5415.2036, train_loss=8.596966

Batch 177290, train_perplexity=4194.11, train_loss=8.341436

Batch 177300, train_perplexity=6308.209, train_loss=8.749607

Batch 177310, train_perplexity=5932.63, train_loss=8.688223

Batch 177320, train_perplexity=5452.3745, train_loss=8.6038065

Batch 177330, train_perplexity=5710.264, train_loss=8.650021

Batch 177340, train_perplexity=5715.424, train_loss=8.650924

Batch 177350, train_perplexity=4756.707, train_loss=8.467311

Batch 177360, train_perplexity=5644.33, train_loss=8.638407

Batch 177370, train_perplexity=4918.4683, train_loss=8.500752

Batch 177380, train_perplexity=5403.4316, train_loss=8.5947895

Batch 177390, train_perplexity=4798.5337, train_loss=8.476066

Batch 177400, train_perplexity=6073.1445, train_loss=8.711632

Batch 177410, train_perplexity=4308.7554, train_loss=8.368404

Batch 177420, train_perplexity=4576.0034, train_loss=8.428581

Batch 177430, train_perplexity=4462.2866, train_loss=8.403417

Batch 177440, train_perplexity=4872.4634, train_loss=8.491355

Batch 177450, train_perplexity=4762.767, train_loss=8.468584

Batch 177460, train_perplexity=6674.74, train_loss=8.806086

Batch 177470, train_perplexity=5458.998, train_loss=8.6050205

Batch 177480, train_perplexity=5660.9873, train_loss=8.641354

Batch 177490, train_perplexity=5639.337, train_loss=8.637522

Batch 177500, train_perplexity=4617.3325, train_loss=8.4375725

Batch 177510, train_perplexity=5674.1865, train_loss=8.6436825

Batch 177520, train_perplexity=5151.4, train_loss=8.547024

Batch 177530, train_perplexity=4225.328, train_loss=8.348852

Batch 177540, train_perplexity=5151.5376, train_loss=8.54705

Batch 177550, train_perplexity=4983.8027, train_loss=8.513948

Batch 177560, train_perplexity=5425.0093, train_loss=8.598775

Batch 177570, train_perplexity=5202.2705, train_loss=8.55685

Batch 177580, train_perplexity=5887.1074, train_loss=8.68052

Batch 177590, train_perplexity=5202.509, train_loss=8.556896

Batch 177600, train_perplexity=4961.23, train_loss=8.509409

Batch 177610, train_perplexity=4516.2, train_loss=8.415426

Batch 177620, train_perplexity=5605.289, train_loss=8.631466

Batch 177630, train_perplexity=5526.698, train_loss=8.617346

Batch 177640, train_perplexity=5334.2188, train_loss=8.581898

Batch 177650, train_perplexity=5602.435, train_loss=8.630957

Batch 177660, train_perplexity=4317.0557, train_loss=8.370329

Batch 177670, train_perplexity=6056.002, train_loss=8.708805

Batch 177680, train_perplexity=7403.6826, train_loss=8.909733

Batch 177690, train_perplexity=5166.406, train_loss=8.5499325

Batch 177700, train_perplexity=6002.6074, train_loss=8.699949

Batch 177710, train_perplexity=5527.4307, train_loss=8.617478

Batch 177720, train_perplexity=5281.826, train_loss=8.572027

Batch 177730, train_perplexity=4658.6313, train_loss=8.446477

Batch 177740, train_perplexity=4801.7974, train_loss=8.476746

Batch 177750, train_perplexity=5418.0137, train_loss=8.597485

Batch 177760, train_perplexity=5197.1577, train_loss=8.555867

Batch 177770, train_perplexity=6468.578, train_loss=8.774712

Batch 177780, train_perplexity=5011.2593, train_loss=8.519443

Batch 177790, train_perplexity=7095.8794, train_loss=8.8672695

Batch 177800, train_perplexity=4360.6104, train_loss=8.380367

Batch 177810, train_perplexity=6280.668, train_loss=8.745232

Batch 177820, train_perplexity=5132.951, train_loss=8.543436

Batch 177830, train_perplexity=4798.14, train_loss=8.475984

Batch 177840, train_perplexity=5657.4253, train_loss=8.640724

Batch 177850, train_perplexity=5668.6104, train_loss=8.642699

Batch 177860, train_perplexity=5568.239, train_loss=8.624834

Batch 177870, train_perplexity=4720.1265, train_loss=8.459591

Batch 177880, train_perplexity=6052.728, train_loss=8.708264

Batch 177890, train_perplexity=5608.3423, train_loss=8.63201

Batch 177900, train_perplexity=4604.967, train_loss=8.434891

Batch 177910, train_perplexity=5158.504, train_loss=8.548402

Batch 177920, train_perplexity=5640.7676, train_loss=8.637775

Batch 177930, train_perplexity=5939.293, train_loss=8.689345

Batch 177940, train_perplexity=5775.041, train_loss=8.661301

Batch 177950, train_perplexity=6365.663, train_loss=8.758674

Batch 177960, train_perplexity=6425.8706, train_loss=8.768087

Batch 177970, train_perplexity=4357.0684, train_loss=8.379555

Batch 177980, train_perplexity=4585.054, train_loss=8.430557

Batch 177990, train_perplexity=5008.025, train_loss=8.518797

Batch 178000, train_perplexity=7384.362, train_loss=8.90712

Batch 178010, train_perplexity=5030.8203, train_loss=8.523338

Batch 178020, train_perplexity=6058.7285, train_loss=8.709255

Batch 178030, train_perplexity=6900.412, train_loss=8.839336

Batch 178040, train_perplexity=5331.8486, train_loss=8.581453

Batch 178050, train_perplexity=5085.037, train_loss=8.534058
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 178060, train_perplexity=5860.8906, train_loss=8.676057

Batch 178070, train_perplexity=4870.401, train_loss=8.4909315

Batch 178080, train_perplexity=5421.2803, train_loss=8.598087

Batch 178090, train_perplexity=4756.117, train_loss=8.467187

Batch 178100, train_perplexity=5661.4194, train_loss=8.64143

Batch 178110, train_perplexity=5034.6406, train_loss=8.524097

Batch 178120, train_perplexity=5390.708, train_loss=8.592432

Batch 178130, train_perplexity=5344.0664, train_loss=8.583742

Batch 178140, train_perplexity=6070.7935, train_loss=8.711245

Batch 178150, train_perplexity=5478.2163, train_loss=8.608535

Batch 178160, train_perplexity=4759.393, train_loss=8.4678755

Batch 178170, train_perplexity=5106.4585, train_loss=8.538261

Batch 178180, train_perplexity=6245.559, train_loss=8.739626

Batch 178190, train_perplexity=5620.0034, train_loss=8.634088

Batch 178200, train_perplexity=5517.419, train_loss=8.615665

Batch 178210, train_perplexity=5878.4004, train_loss=8.67904

Batch 178220, train_perplexity=6050.7656, train_loss=8.70794

Batch 178230, train_perplexity=5033.3013, train_loss=8.523831

Batch 178240, train_perplexity=5676.238, train_loss=8.644044

Batch 178250, train_perplexity=5991.855, train_loss=8.698156

Batch 178260, train_perplexity=4865.2524, train_loss=8.489874

Batch 178270, train_perplexity=6154.5854, train_loss=8.724953

Batch 178280, train_perplexity=5167.1597, train_loss=8.550078

Batch 178290, train_perplexity=5624.6094, train_loss=8.634907

Batch 178300, train_perplexity=4755.7046, train_loss=8.4671

Batch 178310, train_perplexity=4912.079, train_loss=8.499453

Batch 178320, train_perplexity=5920.557, train_loss=8.686186

Batch 178330, train_perplexity=5190.159, train_loss=8.55452

Batch 178340, train_perplexity=4988.824, train_loss=8.5149555

Batch 178350, train_perplexity=4547.2173, train_loss=8.422271

Batch 178360, train_perplexity=5173.7915, train_loss=8.551361

Batch 178370, train_perplexity=5129.3203, train_loss=8.542728

Batch 178380, train_perplexity=4530.764, train_loss=8.418646

Batch 178390, train_perplexity=5501.8237, train_loss=8.612835

Batch 178400, train_perplexity=6351.225, train_loss=8.756403

Batch 178410, train_perplexity=5495.1226, train_loss=8.611616

Batch 178420, train_perplexity=5440.8486, train_loss=8.60169

Batch 178430, train_perplexity=4708.4014, train_loss=8.457104

Batch 178440, train_perplexity=4569.889, train_loss=8.427244

Batch 178450, train_perplexity=5004.783, train_loss=8.518149

Batch 178460, train_perplexity=6286.331, train_loss=8.746133

Batch 178470, train_perplexity=5316.474, train_loss=8.578566

Batch 178480, train_perplexity=5270.374, train_loss=8.569857

Batch 178490, train_perplexity=5053.478, train_loss=8.527832

Batch 178500, train_perplexity=5171.241, train_loss=8.550868

Batch 178510, train_perplexity=5749.6084, train_loss=8.656887

Batch 178520, train_perplexity=7170.7646, train_loss=8.877768

Batch 178530, train_perplexity=6090.684, train_loss=8.714516

Batch 178540, train_perplexity=4647.2935, train_loss=8.44404

Batch 178550, train_perplexity=4557.5283, train_loss=8.424536

Batch 178560, train_perplexity=4684.7256, train_loss=8.452063

Batch 178570, train_perplexity=5874.4214, train_loss=8.678363

Batch 178580, train_perplexity=6234.627, train_loss=8.737874

Batch 178590, train_perplexity=6734.7812, train_loss=8.815041

Batch 178600, train_perplexity=6499.583, train_loss=8.779493

Batch 178610, train_perplexity=6340.1987, train_loss=8.754665

Batch 178620, train_perplexity=4788.289, train_loss=8.473928

Batch 178630, train_perplexity=5974.036, train_loss=8.695178

Batch 178640, train_perplexity=4882.3945, train_loss=8.493391

Batch 178650, train_perplexity=4760.651, train_loss=8.46814

Batch 178660, train_perplexity=5765.2515, train_loss=8.659604

Batch 178670, train_perplexity=5798.617, train_loss=8.665375

Batch 178680, train_perplexity=5108.758, train_loss=8.538712

Batch 178690, train_perplexity=5675.989, train_loss=8.644

Batch 178700, train_perplexity=5245.023, train_loss=8.565035

Batch 178710, train_perplexity=5543.257, train_loss=8.6203375

Batch 178720, train_perplexity=5443.5474, train_loss=8.602186

Batch 178730, train_perplexity=5040.3, train_loss=8.525221

Batch 178740, train_perplexity=5503.2354, train_loss=8.613091

Batch 178750, train_perplexity=5010.81, train_loss=8.519353

Batch 178760, train_perplexity=5154.147, train_loss=8.547557

Batch 178770, train_perplexity=3921.401, train_loss=8.274204

Batch 178780, train_perplexity=5019.644, train_loss=8.521114

Batch 178790, train_perplexity=5870.686, train_loss=8.677727

Batch 178800, train_perplexity=5386.124, train_loss=8.591581

Batch 178810, train_perplexity=4620.698, train_loss=8.438301

Batch 178820, train_perplexity=5329.485, train_loss=8.58101

Batch 178830, train_perplexity=4722.1978, train_loss=8.46003

Batch 178840, train_perplexity=5401.35, train_loss=8.594404

Batch 178850, train_perplexity=5311.4365, train_loss=8.577618

Batch 178860, train_perplexity=5674.8794, train_loss=8.643805

Batch 178870, train_perplexity=6086.1494, train_loss=8.713771

Batch 178880, train_perplexity=5723.5566, train_loss=8.652346

Batch 178890, train_perplexity=4993.3174, train_loss=8.515856

Batch 178900, train_perplexity=6194.604, train_loss=8.731434

Batch 178910, train_perplexity=5523.3784, train_loss=8.616745

Batch 178920, train_perplexity=5944.07, train_loss=8.690149

Batch 178930, train_perplexity=5281.5996, train_loss=8.571984

Batch 178940, train_perplexity=5402.968, train_loss=8.594704

Batch 178950, train_perplexity=4555.3643, train_loss=8.424061

Batch 178960, train_perplexity=4905.7964, train_loss=8.498173

Batch 178970, train_perplexity=4793.461, train_loss=8.475008

Batch 178980, train_perplexity=4134.3267, train_loss=8.32708

Batch 178990, train_perplexity=4905.7217, train_loss=8.4981575

Batch 179000, train_perplexity=5463.154, train_loss=8.605782

Batch 179010, train_perplexity=4367.486, train_loss=8.381943

Batch 179020, train_perplexity=5602.136, train_loss=8.630903

Batch 179030, train_perplexity=5530.721, train_loss=8.618073

Batch 179040, train_perplexity=4595.245, train_loss=8.432777

Batch 179050, train_perplexity=5287.9297, train_loss=8.573182

Batch 179060, train_perplexity=5679.7197, train_loss=8.644657

Batch 179070, train_perplexity=6013.523, train_loss=8.701766

Batch 179080, train_perplexity=5119.5854, train_loss=8.540829

Batch 179090, train_perplexity=5523.363, train_loss=8.616742

Batch 179100, train_perplexity=5376.1323, train_loss=8.589725

Batch 179110, train_perplexity=4811.3823, train_loss=8.47874

Batch 179120, train_perplexity=4609.6333, train_loss=8.435904

Batch 179130, train_perplexity=4851.7183, train_loss=8.487088

Batch 179140, train_perplexity=4879.611, train_loss=8.492821

Batch 179150, train_perplexity=5115.7197, train_loss=8.540073

Batch 179160, train_perplexity=5046.331, train_loss=8.526417

Batch 179170, train_perplexity=5068.7207, train_loss=8.530844

Batch 179180, train_perplexity=4962.73, train_loss=8.509711

Batch 179190, train_perplexity=4095.2854, train_loss=8.317592

Batch 179200, train_perplexity=5480.2803, train_loss=8.6089115

Batch 179210, train_perplexity=5677.207, train_loss=8.644215

Batch 179220, train_perplexity=5332.9165, train_loss=8.581654

Batch 179230, train_perplexity=5243.5073, train_loss=8.564746

Batch 179240, train_perplexity=4705.677, train_loss=8.456525

Batch 179250, train_perplexity=5155.4985, train_loss=8.547819

Batch 179260, train_perplexity=5719.9443, train_loss=8.651714

Batch 179270, train_perplexity=5590.8374, train_loss=8.628884

Batch 179280, train_perplexity=6148.7188, train_loss=8.723999

Batch 179290, train_perplexity=5939.4004, train_loss=8.6893635

Batch 179300, train_perplexity=5903.44, train_loss=8.6832905

Batch 179310, train_perplexity=4464.266, train_loss=8.40386

Batch 179320, train_perplexity=5373.1445, train_loss=8.589169

Batch 179330, train_perplexity=5681.296, train_loss=8.644935

Batch 179340, train_perplexity=6696.3926, train_loss=8.809324

Batch 179350, train_perplexity=5496.9043, train_loss=8.61194

Batch 179360, train_perplexity=5394.39, train_loss=8.593115

Batch 179370, train_perplexity=4682.492, train_loss=8.451586
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 179380, train_perplexity=6717.859, train_loss=8.812525

Batch 179390, train_perplexity=5218.8076, train_loss=8.560024

Batch 179400, train_perplexity=6093.1763, train_loss=8.714925

Batch 179410, train_perplexity=5027.1655, train_loss=8.522612

Batch 179420, train_perplexity=4614.7837, train_loss=8.43702

Batch 179430, train_perplexity=4351.8403, train_loss=8.378354

Batch 179440, train_perplexity=5028.585, train_loss=8.522894

Batch 179450, train_perplexity=5125.7407, train_loss=8.54203

Batch 179460, train_perplexity=5076.122, train_loss=8.532303

Batch 179470, train_perplexity=6544.578, train_loss=8.786392

Batch 179480, train_perplexity=4927.097, train_loss=8.502505

Batch 179490, train_perplexity=4956.274, train_loss=8.5084095

Batch 179500, train_perplexity=4776.686, train_loss=8.471502

Batch 179510, train_perplexity=5874.4604, train_loss=8.6783695

Batch 179520, train_perplexity=6618.087, train_loss=8.797562

Batch 179530, train_perplexity=5518.7925, train_loss=8.615914

Batch 179540, train_perplexity=4465.6626, train_loss=8.404173

Batch 179550, train_perplexity=4629.078, train_loss=8.440113

Batch 179560, train_perplexity=4681.363, train_loss=8.4513445

Batch 179570, train_perplexity=6158.5957, train_loss=8.725604

Batch 179580, train_perplexity=7206.6333, train_loss=8.882757

Batch 179590, train_perplexity=6660.6367, train_loss=8.80397

Batch 179600, train_perplexity=6131.4097, train_loss=8.72118

Batch 179610, train_perplexity=5158.3413, train_loss=8.54837

Batch 179620, train_perplexity=4979.6978, train_loss=8.513124

Batch 179630, train_perplexity=4787.714, train_loss=8.473808

Batch 179640, train_perplexity=4986.056, train_loss=8.5144005

Batch 179650, train_perplexity=5254.3447, train_loss=8.566811

Batch 179660, train_perplexity=5921.297, train_loss=8.686311

Batch 179670, train_perplexity=5513.495, train_loss=8.614954

Batch 179680, train_perplexity=4807.736, train_loss=8.477982

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Batch 179690, train_perplexity=5693.744, train_loss=8.647123

Batch 179700, train_perplexity=5413.6704, train_loss=8.596683

Batch 179710, train_perplexity=5643.474, train_loss=8.638255

Batch 179720, train_perplexity=4611.251, train_loss=8.4362545

Batch 179730, train_perplexity=5198.62, train_loss=8.556149

Batch 179740, train_perplexity=5379.661, train_loss=8.590381

Batch 179750, train_perplexity=5741.7837, train_loss=8.655525

Batch 179760, train_perplexity=5353.1357, train_loss=8.585438

Batch 179770, train_perplexity=5211.0693, train_loss=8.55854

Batch 179780, train_perplexity=4397.494, train_loss=8.38879

Batch 179790, train_perplexity=4905.7217, train_loss=8.4981575

Batch 179800, train_perplexity=4957.6826, train_loss=8.508694

Batch 179810, train_perplexity=4316.5, train_loss=8.3702

Batch 179820, train_perplexity=4792.177, train_loss=8.47474

Batch 179830, train_perplexity=4983.5366, train_loss=8.513895

Batch 179840, train_perplexity=5170.5806, train_loss=8.55074

Batch 179850, train_perplexity=4638.1636, train_loss=8.442074

Batch 179860, train_perplexity=5285.2827, train_loss=8.572681

Batch 179870, train_perplexity=5254.996, train_loss=8.566935

Batch 179880, train_perplexity=6048.1523, train_loss=8.707508

Batch 179890, train_perplexity=4974.3057, train_loss=8.512041

Batch 179900, train_perplexity=5685.144, train_loss=8.645612

Batch 179910, train_perplexity=6852.7617, train_loss=8.832407

Batch 179920, train_perplexity=5440.729, train_loss=8.601668

Batch 179930, train_perplexity=6327.767, train_loss=8.752703

Batch 179940, train_perplexity=5842.4243, train_loss=8.672901

Batch 179950, train_perplexity=5006.936, train_loss=8.5185795

Batch 179960, train_perplexity=4787.709, train_loss=8.473807

Batch 179970, train_perplexity=5025.6606, train_loss=8.522312

Batch 179980, train_perplexity=5195.473, train_loss=8.555543

Batch 179990, train_perplexity=5257.723, train_loss=8.567453

Batch 180000, train_perplexity=5455.4434, train_loss=8.604369

Batch 180010, train_perplexity=5596.572, train_loss=8.6299095

Batch 180020, train_perplexity=6627.675, train_loss=8.799009

Batch 180030, train_perplexity=5534.0977, train_loss=8.618684

Batch 180040, train_perplexity=4821.161, train_loss=8.48077

Batch 180050, train_perplexity=5651.4287, train_loss=8.639664

Batch 180060, train_perplexity=4768.8755, train_loss=8.469866

Batch 180070, train_perplexity=4797.7144, train_loss=8.475895

Batch 180080, train_perplexity=5276.963, train_loss=8.571106

Batch 180090, train_perplexity=5193.412, train_loss=8.555146

Batch 180100, train_perplexity=4350.2344, train_loss=8.377985

Batch 180110, train_perplexity=5587.367, train_loss=8.628263

Batch 180120, train_perplexity=5440.9004, train_loss=8.6017

Batch 180130, train_perplexity=5893.0225, train_loss=8.681524

Batch 180140, train_perplexity=5477.469, train_loss=8.608398

Batch 180150, train_perplexity=6741.9785, train_loss=8.816109

Batch 180160, train_perplexity=4923.119, train_loss=8.501698

Batch 180170, train_perplexity=5472.2114, train_loss=8.607438

Batch 180180, train_perplexity=5911.8906, train_loss=8.684721

Batch 180190, train_perplexity=5215.4346, train_loss=8.559378

Batch 180200, train_perplexity=5848.7686, train_loss=8.673986

Batch 180210, train_perplexity=5466.265, train_loss=8.606351

Batch 180220, train_perplexity=4998.749, train_loss=8.516943

Batch 180230, train_perplexity=7075.0264, train_loss=8.8643265

Batch 180240, train_perplexity=5503.566, train_loss=8.613152

Batch 180250, train_perplexity=5457.858, train_loss=8.604812

Batch 180260, train_perplexity=5632.919, train_loss=8.636383

Batch 180270, train_perplexity=5272.581, train_loss=8.570275

Batch 180280, train_perplexity=4837.931, train_loss=8.484242

Batch 180290, train_perplexity=4643.829, train_loss=8.443295

Batch 180300, train_perplexity=5599.887, train_loss=8.630502

Batch 180310, train_perplexity=5554.7617, train_loss=8.622411

Batch 180320, train_perplexity=6197.6177, train_loss=8.73192

Batch 180330, train_perplexity=5010.074, train_loss=8.519206

Batch 180340, train_perplexity=5396.15, train_loss=8.593441

Batch 180350, train_perplexity=5800.896, train_loss=8.665768

Batch 180360, train_perplexity=5199.23, train_loss=8.556266

Batch 180370, train_perplexity=4842.6904, train_loss=8.485226

Batch 180380, train_perplexity=5072.2505, train_loss=8.53154

Batch 180390, train_perplexity=6107.7036, train_loss=8.717306

Batch 180400, train_perplexity=5665.7725, train_loss=8.642199

Batch 180410, train_perplexity=5205.784, train_loss=8.557526

Batch 180420, train_perplexity=4642.8726, train_loss=8.443089

Batch 180430, train_perplexity=4233.6733, train_loss=8.350825

Batch 180440, train_perplexity=5295.091, train_loss=8.574535

Batch 180450, train_perplexity=6313.3125, train_loss=8.750416

Batch 180460, train_perplexity=4821.8374, train_loss=8.48091

Batch 180470, train_perplexity=4915.7627, train_loss=8.500202

Batch 180480, train_perplexity=5397.2876, train_loss=8.593652

Batch 180490, train_perplexity=4837.908, train_loss=8.484238

Batch 180500, train_perplexity=4596.525, train_loss=8.433056

Batch 180510, train_perplexity=4853.375, train_loss=8.48743

Batch 180520, train_perplexity=6090.9863, train_loss=8.714565

Batch 180530, train_perplexity=5032.135, train_loss=8.5236

Batch 180540, train_perplexity=5074.181, train_loss=8.53192

Batch 180550, train_perplexity=5421.839, train_loss=8.59819

Batch 180560, train_perplexity=6034.3193, train_loss=8.705218

Batch 180570, train_perplexity=6373.7603, train_loss=8.759945

Batch 180580, train_perplexity=5948.4814, train_loss=8.690891

Batch 180590, train_perplexity=6433.9033, train_loss=8.769337

Batch 180600, train_perplexity=4729.905, train_loss=8.46166

Batch 180610, train_perplexity=5494.2944, train_loss=8.611465

Batch 180620, train_perplexity=4364.488, train_loss=8.381256

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 180630, train_perplexity=5327.808, train_loss=8.580695

Batch 180640, train_perplexity=6021.586, train_loss=8.703106

Batch 180650, train_perplexity=4940.201, train_loss=8.505161

Batch 180660, train_perplexity=5522.046, train_loss=8.616504

Batch 180670, train_perplexity=6041.972, train_loss=8.706486

Batch 180680, train_perplexity=5208.943, train_loss=8.558132

Batch 180690, train_perplexity=4883.065, train_loss=8.493528

Batch 180700, train_perplexity=5235.358, train_loss=8.56319

Batch 180710, train_perplexity=5568.239, train_loss=8.624834

Batch 180720, train_perplexity=5318.908, train_loss=8.579023

Batch 180730, train_perplexity=4800.4424, train_loss=8.476463

Batch 180740, train_perplexity=5829.7793, train_loss=8.670734

Batch 180750, train_perplexity=6006.41, train_loss=8.7005825

Batch 180760, train_perplexity=4692.8013, train_loss=8.453785

Batch 180770, train_perplexity=4882.9766, train_loss=8.49351

Batch 180780, train_perplexity=4530.66, train_loss=8.418623

Batch 180790, train_perplexity=5188.724, train_loss=8.554243

Batch 180800, train_perplexity=5083.631, train_loss=8.533781

Batch 180810, train_perplexity=6319.017, train_loss=8.751319

Batch 180820, train_perplexity=5662.834, train_loss=8.64168

Batch 180830, train_perplexity=5842.7256, train_loss=8.672953

Batch 180840, train_perplexity=5723.2725, train_loss=8.652296

Batch 180850, train_perplexity=6173.485, train_loss=8.728019

Batch 180860, train_perplexity=6341.88, train_loss=8.7549305

Batch 180870, train_perplexity=6058.694, train_loss=8.7092495

Batch 180880, train_perplexity=5779.052, train_loss=8.661995

Batch 180890, train_perplexity=6486.808, train_loss=8.777526

Batch 180900, train_perplexity=5571.096, train_loss=8.625347

Batch 180910, train_perplexity=4133.964, train_loss=8.326992

Batch 180920, train_perplexity=4617.183, train_loss=8.43754

Batch 180930, train_perplexity=5637.4707, train_loss=8.637191

Batch 180940, train_perplexity=5250.8584, train_loss=8.566147

Batch 180950, train_perplexity=5205.7197, train_loss=8.557513

Batch 180960, train_perplexity=5598.7554, train_loss=8.6303

Batch 180970, train_perplexity=5208.6494, train_loss=8.558076

Batch 180980, train_perplexity=5328.687, train_loss=8.58086

Batch 180990, train_perplexity=4821.3545, train_loss=8.48081

Batch 181000, train_perplexity=4999.5024, train_loss=8.517094

Batch 181010, train_perplexity=4912.9785, train_loss=8.499636

Batch 181020, train_perplexity=5323.0747, train_loss=8.579806

Batch 181030, train_perplexity=5535.813, train_loss=8.618994

Batch 181040, train_perplexity=5236.706, train_loss=8.563448

Batch 181050, train_perplexity=5030.6094, train_loss=8.523296

Batch 181060, train_perplexity=5807.422, train_loss=8.666892

Batch 181070, train_perplexity=5432.138, train_loss=8.600088

Batch 181080, train_perplexity=5748.9614, train_loss=8.6567745

Batch 181090, train_perplexity=4751.09, train_loss=8.466129

Batch 181100, train_perplexity=6116.838, train_loss=8.718801

Batch 181110, train_perplexity=5167.421, train_loss=8.550129

Batch 181120, train_perplexity=5097.6274, train_loss=8.5365305

Batch 181130, train_perplexity=6341.6196, train_loss=8.7548895

Batch 181140, train_perplexity=5302.7515, train_loss=8.575981

Batch 181150, train_perplexity=6187.0703, train_loss=8.730217

Batch 181160, train_perplexity=5331.9604, train_loss=8.581474

Batch 181170, train_perplexity=5483.5737, train_loss=8.609512

Batch 181180, train_perplexity=5162.899, train_loss=8.549253

Batch 181190, train_perplexity=6142.1665, train_loss=8.722933

Batch 181200, train_perplexity=5492.576, train_loss=8.611153

Batch 181210, train_perplexity=4948.4575, train_loss=8.506831

Batch 181220, train_perplexity=6962.323, train_loss=8.8482685

Batch 181230, train_perplexity=4850.007, train_loss=8.486735

Batch 181240, train_perplexity=5264.216, train_loss=8.568687

Batch 181250, train_perplexity=4516.579, train_loss=8.41551

Batch 181260, train_perplexity=6074.998, train_loss=8.711937

Batch 181270, train_perplexity=4408.752, train_loss=8.391347

Batch 181280, train_perplexity=5982.5537, train_loss=8.696603

Batch 181290, train_perplexity=5029.842, train_loss=8.523144

Batch 181300, train_perplexity=5335.725, train_loss=8.58218

Batch 181310, train_perplexity=5637.5625, train_loss=8.637207

Batch 181320, train_perplexity=5481.1846, train_loss=8.6090765

Batch 181330, train_perplexity=5833.817, train_loss=8.671427

Batch 181340, train_perplexity=5726.21, train_loss=8.652809

Batch 181350, train_perplexity=5165.7354, train_loss=8.549803

Batch 181360, train_perplexity=5460.1226, train_loss=8.6052265

Batch 181370, train_perplexity=4651.47, train_loss=8.444939

Batch 181380, train_perplexity=6713.248, train_loss=8.811838

Batch 181390, train_perplexity=4279.173, train_loss=8.361515

Batch 181400, train_perplexity=5809.6104, train_loss=8.667269

Batch 181410, train_perplexity=5509.2114, train_loss=8.614177

Batch 181420, train_perplexity=5724.61, train_loss=8.65253

Batch 181430, train_perplexity=4394.9116, train_loss=8.388203

Batch 181440, train_perplexity=5386.6274, train_loss=8.591675

Batch 181450, train_perplexity=5173.249, train_loss=8.551256

Batch 181460, train_perplexity=5347.7627, train_loss=8.584434

Batch 181470, train_perplexity=4403.2183, train_loss=8.390091

Batch 181480, train_perplexity=5659.109, train_loss=8.641022

Batch 181490, train_perplexity=4375.636, train_loss=8.383807

Batch 181500, train_perplexity=5060.003, train_loss=8.529122

Batch 181510, train_perplexity=4470.124, train_loss=8.405171

Batch 181520, train_perplexity=5245.823, train_loss=8.565187

Batch 181530, train_perplexity=5837.2505, train_loss=8.672015

Batch 181540, train_perplexity=4411.33, train_loss=8.391932

Batch 181550, train_perplexity=6972.716, train_loss=8.84976

Batch 181560, train_perplexity=5273.974, train_loss=8.570539

Batch 181570, train_perplexity=5772.91, train_loss=8.660932

Batch 181580, train_perplexity=5216.529, train_loss=8.5595875

Batch 181590, train_perplexity=4444.4956, train_loss=8.399422

Batch 181600, train_perplexity=4827.727, train_loss=8.482131

Batch 181610, train_perplexity=5470.4736, train_loss=8.6071205

Batch 181620, train_perplexity=4825.5312, train_loss=8.481676

Batch 181630, train_perplexity=5266.5757, train_loss=8.569136

Batch 181640, train_perplexity=4796.6895, train_loss=8.475681

Batch 181650, train_perplexity=4654.3013, train_loss=8.445547

Batch 181660, train_perplexity=4585.632, train_loss=8.430683

Batch 181670, train_perplexity=5787.5786, train_loss=8.663469

Batch 181680, train_perplexity=5690.639, train_loss=8.646578

Batch 181690, train_perplexity=6242.9565, train_loss=8.739209

Batch 181700, train_perplexity=5382.8584, train_loss=8.590975

Batch 181710, train_perplexity=5549.3716, train_loss=8.62144

Batch 181720, train_perplexity=5118.15, train_loss=8.540548

Batch 181730, train_perplexity=5155.41, train_loss=8.547802

Batch 181740, train_perplexity=5450.352, train_loss=8.6034355

Batch 181750, train_perplexity=5555.911, train_loss=8.622618

Batch 181760, train_perplexity=4677.7656, train_loss=8.450576

Batch 181770, train_perplexity=4703.133, train_loss=8.455984

Batch 181780, train_perplexity=5081.92, train_loss=8.533444

Batch 181790, train_perplexity=4626.426, train_loss=8.43954

Batch 181800, train_perplexity=6333.1284, train_loss=8.75355

Batch 181810, train_perplexity=5128.8555, train_loss=8.542638

Batch 181820, train_perplexity=5285.681, train_loss=8.572757

Batch 181830, train_perplexity=4869.8853, train_loss=8.490826

Batch 181840, train_perplexity=5232.353, train_loss=8.562616

Batch 181850, train_perplexity=5056.79, train_loss=8.528487

Batch 181860, train_perplexity=4298.835, train_loss=8.366099

Batch 181870, train_perplexity=5060.0703, train_loss=8.529136

Batch 181880, train_perplexity=6008.5986, train_loss=8.700947

Batch 181890, train_perplexity=5412.9214, train_loss=8.596544

Batch 181900, train_perplexity=4897.149, train_loss=8.496408

Batch 181910, train_perplexity=5017.414, train_loss=8.52067

Batch 181920, train_perplexity=5608.54, train_loss=8.632046

Batch 181930, train_perplexity=5447.494, train_loss=8.602911

Batch 181940, train_perplexity=4606.627, train_loss=8.435251
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 181950, train_perplexity=4899.765, train_loss=8.4969425

Batch 181960, train_perplexity=6127.926, train_loss=8.720612

Batch 181970, train_perplexity=6143.268, train_loss=8.723112

Batch 181980, train_perplexity=5762.7944, train_loss=8.659178

Batch 181990, train_perplexity=5896.1533, train_loss=8.682055

Batch 182000, train_perplexity=5904.0254, train_loss=8.68339

Batch 182010, train_perplexity=5375.666, train_loss=8.589638

Batch 182020, train_perplexity=4423.2563, train_loss=8.394631

Batch 182030, train_perplexity=4800.3413, train_loss=8.476442

Batch 182040, train_perplexity=4491.3955, train_loss=8.409919

Batch 182050, train_perplexity=5712.4756, train_loss=8.650408

Batch 182060, train_perplexity=5938.755, train_loss=8.689255

Batch 182070, train_perplexity=4804.2803, train_loss=8.4772625

Batch 182080, train_perplexity=6179.982, train_loss=8.729071

Batch 182090, train_perplexity=5352.901, train_loss=8.585394

Batch 182100, train_perplexity=5678.1978, train_loss=8.644389

Batch 182110, train_perplexity=4772.142, train_loss=8.470551

Batch 182120, train_perplexity=4869.8296, train_loss=8.490814

Batch 182130, train_perplexity=5093.3853, train_loss=8.535698

Batch 182140, train_perplexity=4967.4224, train_loss=8.510656

Batch 182150, train_perplexity=5275.7397, train_loss=8.570874

Batch 182160, train_perplexity=4288.929, train_loss=8.363792

Batch 182170, train_perplexity=4443.6313, train_loss=8.399227

Batch 182180, train_perplexity=4444.6567, train_loss=8.399458

Batch 182190, train_perplexity=4948.651, train_loss=8.50687

Batch 182200, train_perplexity=5855.0024, train_loss=8.675052

Batch 182210, train_perplexity=5959.611, train_loss=8.69276

Batch 182220, train_perplexity=5085.406, train_loss=8.53413

Batch 182230, train_perplexity=5171.3154, train_loss=8.550882

Batch 182240, train_perplexity=4872.059, train_loss=8.491272

Batch 182250, train_perplexity=5487.8794, train_loss=8.610297

Batch 182260, train_perplexity=4785.897, train_loss=8.473429

Batch 182270, train_perplexity=5869.04, train_loss=8.677446

Batch 182280, train_perplexity=4754.9927, train_loss=8.46695

Batch 182290, train_perplexity=5470.2183, train_loss=8.607074

Batch 182300, train_perplexity=4898.8213, train_loss=8.49675

Batch 182310, train_perplexity=5363.5605, train_loss=8.587383

Batch 182320, train_perplexity=4894.4688, train_loss=8.495861

Batch 182330, train_perplexity=5245.663, train_loss=8.565157

Batch 182340, train_perplexity=5732.0996, train_loss=8.653837

Batch 182350, train_perplexity=6005.127, train_loss=8.700369

Batch 182360, train_perplexity=4776.595, train_loss=8.471483

Batch 182370, train_perplexity=6548.5737, train_loss=8.787003

Batch 182380, train_perplexity=5090.1416, train_loss=8.535061

Batch 182390, train_perplexity=5306.394, train_loss=8.576668

Batch 182400, train_perplexity=6093.496, train_loss=8.714977

Batch 182410, train_perplexity=6289.203, train_loss=8.74659

Batch 182420, train_perplexity=6026.078, train_loss=8.703852

Batch 182430, train_perplexity=4618.024, train_loss=8.437722

Batch 182440, train_perplexity=4653.0586, train_loss=8.44528

Batch 182450, train_perplexity=5433.8896, train_loss=8.60041

Batch 182460, train_perplexity=5681.773, train_loss=8.645019

Batch 182470, train_perplexity=6616.5977, train_loss=8.797337

Batch 182480, train_perplexity=5345.3306, train_loss=8.583979

Batch 182490, train_perplexity=5501.4146, train_loss=8.612761

Batch 182500, train_perplexity=5461.0024, train_loss=8.605388

Batch 182510, train_perplexity=4521.0264, train_loss=8.416494

Batch 182520, train_perplexity=5011.312, train_loss=8.519453

Batch 182530, train_perplexity=4980.3247, train_loss=8.51325

Batch 182540, train_perplexity=5147.5547, train_loss=8.546277

Batch 182550, train_perplexity=5265.8525, train_loss=8.568998

Batch 182560, train_perplexity=5514.1416, train_loss=8.615071

Batch 182570, train_perplexity=5770.7026, train_loss=8.660549

Batch 182580, train_perplexity=5109.0503, train_loss=8.538769

Batch 182590, train_perplexity=5162.5244, train_loss=8.549181

Batch 182600, train_perplexity=4737.1187, train_loss=8.463184

Batch 182610, train_perplexity=5179.4937, train_loss=8.552463

Batch 182620, train_perplexity=6985.9746, train_loss=8.85166

Batch 182630, train_perplexity=4357.5337, train_loss=8.379662

Batch 182640, train_perplexity=5425.6304, train_loss=8.598889

Batch 182650, train_perplexity=5946.315, train_loss=8.690527

Batch 182660, train_perplexity=4859.688, train_loss=8.4887295

Batch 182670, train_perplexity=4382.0796, train_loss=8.385279

Batch 182680, train_perplexity=4577.505, train_loss=8.428909

Batch 182690, train_perplexity=4900.8726, train_loss=8.497169

Batch 182700, train_perplexity=4942.774, train_loss=8.505682

Batch 182710, train_perplexity=6755.8486, train_loss=8.818164

Batch 182720, train_perplexity=5154.3286, train_loss=8.547592

Batch 182730, train_perplexity=4337.657, train_loss=8.37509

Batch 182740, train_perplexity=5856.136, train_loss=8.675245

Batch 182750, train_perplexity=5251.915, train_loss=8.566348

Batch 182760, train_perplexity=5071.283, train_loss=8.531349

Batch 182770, train_perplexity=6963.26, train_loss=8.848403

Batch 182780, train_perplexity=4542.3286, train_loss=8.421195

Batch 182790, train_perplexity=6259.703, train_loss=8.741888

Batch 182800, train_perplexity=4983.584, train_loss=8.513905

Batch 182810, train_perplexity=4883.81, train_loss=8.493681

Batch 182820, train_perplexity=5396.6953, train_loss=8.593542

Batch 182830, train_perplexity=6633.9165, train_loss=8.799951

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Batch 182840, train_perplexity=4646.8945, train_loss=8.443954

Batch 182850, train_perplexity=5108.1826, train_loss=8.538599

Batch 182860, train_perplexity=5124.646, train_loss=8.541817

Batch 182870, train_perplexity=4705.8516, train_loss=8.456562

Batch 182880, train_perplexity=5594.0, train_loss=8.62945

Batch 182890, train_perplexity=5217.1157, train_loss=8.5597

Batch 182900, train_perplexity=4893.0547, train_loss=8.495572

Batch 182910, train_perplexity=4745.1265, train_loss=8.464873

Batch 182920, train_perplexity=5294.515, train_loss=8.574427

Batch 182930, train_perplexity=5073.238, train_loss=8.531734

Batch 182940, train_perplexity=6039.1265, train_loss=8.706015

Batch 182950, train_perplexity=3999.6895, train_loss=8.293972

Batch 182960, train_perplexity=5425.4287, train_loss=8.598852

Batch 182970, train_perplexity=4734.9326, train_loss=8.462723

Batch 182980, train_perplexity=6038.533, train_loss=8.705916

Batch 182990, train_perplexity=5219.39, train_loss=8.560136

Batch 183000, train_perplexity=4978.325, train_loss=8.512849

Batch 183010, train_perplexity=5671.925, train_loss=8.643284

Batch 183020, train_perplexity=4342.355, train_loss=8.376172

Batch 183030, train_perplexity=4635.9175, train_loss=8.441589

Batch 183040, train_perplexity=5361.4736, train_loss=8.586994

Batch 183050, train_perplexity=5161.8843, train_loss=8.549057

Batch 183060, train_perplexity=5193.7637, train_loss=8.555214

Batch 183070, train_perplexity=6241.802, train_loss=8.739024

Batch 183080, train_perplexity=5139.412, train_loss=8.544694

Batch 183090, train_perplexity=8272.68, train_loss=9.020714

Batch 183100, train_perplexity=5228.3574, train_loss=8.561852

Batch 183110, train_perplexity=5816.44, train_loss=8.668444

Batch 183120, train_perplexity=4756.4756, train_loss=8.467262

Batch 183130, train_perplexity=6144.932, train_loss=8.723383

Batch 183140, train_perplexity=4570.146, train_loss=8.4273

Batch 183150, train_perplexity=4692.394, train_loss=8.453698

Batch 183160, train_perplexity=5374.1694, train_loss=8.589359

Batch 183170, train_perplexity=6261.369, train_loss=8.742154

Batch 183180, train_perplexity=7087.3647, train_loss=8.866069

Batch 183190, train_perplexity=5926.206, train_loss=8.6871395
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 183200, train_perplexity=5196.33, train_loss=8.555708

Batch 183210, train_perplexity=4845.291, train_loss=8.485763

Batch 183220, train_perplexity=5590.6934, train_loss=8.628859

Batch 183230, train_perplexity=6653.602, train_loss=8.802914

Batch 183240, train_perplexity=5582.829, train_loss=8.627451

Batch 183250, train_perplexity=5088.525, train_loss=8.534743

Batch 183260, train_perplexity=5589.2163, train_loss=8.628594

Batch 183270, train_perplexity=5846.6104, train_loss=8.673617

Batch 183280, train_perplexity=4321.2734, train_loss=8.371305

Batch 183290, train_perplexity=6423.769, train_loss=8.76776

Batch 183300, train_perplexity=5412.5293, train_loss=8.596472

Batch 183310, train_perplexity=5165.553, train_loss=8.5497675

Batch 183320, train_perplexity=5953.646, train_loss=8.691759

Batch 183330, train_perplexity=5198.085, train_loss=8.556046

Batch 183340, train_perplexity=4945.6123, train_loss=8.506256

Batch 183350, train_perplexity=5224.574, train_loss=8.561129

Batch 183360, train_perplexity=5616.4404, train_loss=8.633453

Batch 183370, train_perplexity=5875.7383, train_loss=8.678587

Batch 183380, train_perplexity=5580.785, train_loss=8.627085

Batch 183390, train_perplexity=5834.4624, train_loss=8.671537

Batch 183400, train_perplexity=5121.5435, train_loss=8.541211

Batch 183410, train_perplexity=5820.102, train_loss=8.669073

Batch 183420, train_perplexity=5523.6943, train_loss=8.616802

Batch 183430, train_perplexity=6038.781, train_loss=8.705957

Batch 183440, train_perplexity=5041.271, train_loss=8.5254135

Batch 183450, train_perplexity=6011.946, train_loss=8.701504

Batch 183460, train_perplexity=4708.1006, train_loss=8.45704

Batch 183470, train_perplexity=4960.549, train_loss=8.509272

Batch 183480, train_perplexity=4951.899, train_loss=8.507526

Batch 183490, train_perplexity=5449.5464, train_loss=8.603288

Batch 183500, train_perplexity=6432.0996, train_loss=8.769056

Batch 183510, train_perplexity=5368.9185, train_loss=8.588382

Batch 183520, train_perplexity=4684.4126, train_loss=8.451996

Batch 183530, train_perplexity=3947.4219, train_loss=8.280818

Batch 183540, train_perplexity=4892.8633, train_loss=8.495533

Batch 183550, train_perplexity=5274.759, train_loss=8.570688

Batch 183560, train_perplexity=4604.01, train_loss=8.434683

Batch 183570, train_perplexity=6025.308, train_loss=8.703724

Batch 183580, train_perplexity=4986.465, train_loss=8.5144825

Batch 183590, train_perplexity=4927.0596, train_loss=8.502498

Batch 183600, train_perplexity=4796.7354, train_loss=8.475691

Batch 183610, train_perplexity=5947.041, train_loss=8.690649

Batch 183620, train_perplexity=4612.469, train_loss=8.436519

Batch 183630, train_perplexity=5116.9883, train_loss=8.540321

Batch 183640, train_perplexity=5403.9263, train_loss=8.594881

Batch 183650, train_perplexity=5354.994, train_loss=8.585785

Batch 183660, train_perplexity=5129.2075, train_loss=8.5427065

Batch 183670, train_perplexity=5527.109, train_loss=8.61742

Batch 183680, train_perplexity=5259.1675, train_loss=8.567728

Batch 183690, train_perplexity=5192.897, train_loss=8.555047

Batch 183700, train_perplexity=6017.303, train_loss=8.7023945

Batch 183710, train_perplexity=5145.93, train_loss=8.545961

Batch 183720, train_perplexity=5635.165, train_loss=8.636782

Batch 183730, train_perplexity=4193.1377, train_loss=8.341205

Batch 183740, train_perplexity=4775.201, train_loss=8.471191

Batch 183750, train_perplexity=5574.556, train_loss=8.625968

Batch 183760, train_perplexity=4502.5503, train_loss=8.412399

Batch 183770, train_perplexity=4951.7007, train_loss=8.507486

Batch 183780, train_perplexity=6290.9907, train_loss=8.746874

Batch 183790, train_perplexity=4359.3545, train_loss=8.380079

Batch 183800, train_perplexity=5140.7944, train_loss=8.544963

Batch 183810, train_perplexity=5097.7783, train_loss=8.53656

Batch 183820, train_perplexity=4144.867, train_loss=8.329626

Batch 183830, train_perplexity=5650.6797, train_loss=8.639531

Batch 183840, train_perplexity=5647.399, train_loss=8.63895

Batch 183850, train_perplexity=7559.3037, train_loss=8.930534

Batch 183860, train_perplexity=5028.4893, train_loss=8.522875

Batch 183870, train_perplexity=5485.1846, train_loss=8.609806

Batch 183880, train_perplexity=4971.356, train_loss=8.511448

Batch 183890, train_perplexity=5363.161, train_loss=8.587309

Batch 183900, train_perplexity=4786.2573, train_loss=8.473504

Batch 183910, train_perplexity=4947.1504, train_loss=8.506567

Batch 183920, train_perplexity=4635.139, train_loss=8.4414215

Batch 183930, train_perplexity=4810.0107, train_loss=8.478455

Batch 183940, train_perplexity=5868.8164, train_loss=8.677408

Batch 183950, train_perplexity=5410.3926, train_loss=8.596077

Batch 183960, train_perplexity=4277.904, train_loss=8.361218

Batch 183970, train_perplexity=5018.725, train_loss=8.520931

Batch 183980, train_perplexity=6681.5166, train_loss=8.8071

Batch 183990, train_perplexity=5157.564, train_loss=8.54822

Batch 184000, train_perplexity=4686.692, train_loss=8.452482

Batch 184010, train_perplexity=5022.69, train_loss=8.521721

Batch 184020, train_perplexity=4939.113, train_loss=8.504941

Batch 184030, train_perplexity=3820.3286, train_loss=8.248092

Batch 184040, train_perplexity=6749.415, train_loss=8.817211

Batch 184050, train_perplexity=5249.211, train_loss=8.565833

Batch 184060, train_perplexity=5210.3833, train_loss=8.558409

Batch 184070, train_perplexity=5747.1416, train_loss=8.656458

Batch 184080, train_perplexity=4906.377, train_loss=8.498291

Batch 184090, train_perplexity=5728.5586, train_loss=8.653219

Batch 184100, train_perplexity=5082.2397, train_loss=8.533507

Batch 184110, train_perplexity=5472.316, train_loss=8.607457

Batch 184120, train_perplexity=6677.338, train_loss=8.806475

Batch 184130, train_perplexity=4465.288, train_loss=8.404089

Batch 184140, train_perplexity=5650.125, train_loss=8.639433

Batch 184150, train_perplexity=4822.2466, train_loss=8.480995

Batch 184160, train_perplexity=4500.4854, train_loss=8.411941

Batch 184170, train_perplexity=5760.311, train_loss=8.658747

Batch 184180, train_perplexity=5873.979, train_loss=8.6782875

Batch 184190, train_perplexity=5716.3887, train_loss=8.651093

Batch 184200, train_perplexity=5520.224, train_loss=8.616174

Batch 184210, train_perplexity=6183.808, train_loss=8.72969

Batch 184220, train_perplexity=5672.747, train_loss=8.643429

Batch 184230, train_perplexity=5236.4463, train_loss=8.563398

Batch 184240, train_perplexity=5126.64, train_loss=8.542206

Batch 184250, train_perplexity=5132.7017, train_loss=8.543387

Batch 184260, train_perplexity=5471.3296, train_loss=8.607277

Batch 184270, train_perplexity=5804.7144, train_loss=8.666426

Batch 184280, train_perplexity=5152.933, train_loss=8.547321

Batch 184290, train_perplexity=5122.369, train_loss=8.541372

Batch 184300, train_perplexity=5459.1855, train_loss=8.605055

Batch 184310, train_perplexity=4804.408, train_loss=8.477289

Batch 184320, train_perplexity=4722.6436, train_loss=8.460124

Batch 184330, train_perplexity=5177.3555, train_loss=8.55205

Batch 184340, train_perplexity=5391.464, train_loss=8.592572

Batch 184350, train_perplexity=5164.563, train_loss=8.549576

Batch 184360, train_perplexity=5259.2075, train_loss=8.567736

Batch 184370, train_perplexity=4712.0264, train_loss=8.457873

Batch 184380, train_perplexity=6429.438, train_loss=8.768642

Batch 184390, train_perplexity=5680.9873, train_loss=8.64488

Batch 184400, train_perplexity=6013.012, train_loss=8.701681

Batch 184410, train_perplexity=5837.045, train_loss=8.67198

Batch 184420, train_perplexity=5188.724, train_loss=8.554243

Batch 184430, train_perplexity=4935.1387, train_loss=8.504136

Batch 184440, train_perplexity=4808.103, train_loss=8.478058

Batch 184450, train_perplexity=4926.482, train_loss=8.50238

Batch 184460, train_perplexity=6074.349, train_loss=8.71183

Batch 184470, train_perplexity=5947.903, train_loss=8.690794

Batch 184480, train_perplexity=4626.3423, train_loss=8.439522

Batch 184490, train_perplexity=5878.5347, train_loss=8.679063

Batch 184500, train_perplexity=5882.9824, train_loss=8.679819

Batch 184510, train_perplexity=5036.3887, train_loss=8.524445
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 184520, train_perplexity=4950.888, train_loss=8.507322

Batch 184530, train_perplexity=4869.9224, train_loss=8.490833

Batch 184540, train_perplexity=5458.394, train_loss=8.60491

Batch 184550, train_perplexity=6370.576, train_loss=8.759445

Batch 184560, train_perplexity=4701.5273, train_loss=8.455643

Batch 184570, train_perplexity=5866.063, train_loss=8.676939

Batch 184580, train_perplexity=4866.7095, train_loss=8.490173

Batch 184590, train_perplexity=5597.9385, train_loss=8.630154

Batch 184600, train_perplexity=6415.4854, train_loss=8.76647

Batch 184610, train_perplexity=6052.48, train_loss=8.708223

Batch 184620, train_perplexity=5481.2944, train_loss=8.609097

Batch 184630, train_perplexity=5017.175, train_loss=8.520622

Batch 184640, train_perplexity=5822.056, train_loss=8.669409

Batch 184650, train_perplexity=4825.0757, train_loss=8.481582

Batch 184660, train_perplexity=5759.075, train_loss=8.658532

Batch 184670, train_perplexity=5183.9756, train_loss=8.553328

Batch 184680, train_perplexity=4451.24, train_loss=8.400938

Batch 184690, train_perplexity=5043.5503, train_loss=8.525866

Batch 184700, train_perplexity=4569.0522, train_loss=8.427061

Batch 184710, train_perplexity=5187.962, train_loss=8.554096

Batch 184720, train_perplexity=4621.5093, train_loss=8.438477

Batch 184730, train_perplexity=5129.5894, train_loss=8.542781

Batch 184740, train_perplexity=4759.398, train_loss=8.467876

Batch 184750, train_perplexity=5857.3203, train_loss=8.675447

Batch 184760, train_perplexity=5573.541, train_loss=8.625786

Batch 184770, train_perplexity=4509.3013, train_loss=8.4138975

Batch 184780, train_perplexity=4485.694, train_loss=8.4086485

Batch 184790, train_perplexity=4991.28, train_loss=8.515448

Batch 184800, train_perplexity=6290.895, train_loss=8.746859

Batch 184810, train_perplexity=5216.738, train_loss=8.559628

Batch 184820, train_perplexity=4827.7544, train_loss=8.482137

Batch 184830, train_perplexity=4627.745, train_loss=8.439825

Batch 184840, train_perplexity=5401.865, train_loss=8.5945

Batch 184850, train_perplexity=4874.081, train_loss=8.491687

Batch 184860, train_perplexity=5751.254, train_loss=8.657173

Batch 184870, train_perplexity=6211.4697, train_loss=8.734153

Batch 184880, train_perplexity=4952.2812, train_loss=8.507604

Batch 184890, train_perplexity=5395.677, train_loss=8.593353

Batch 184900, train_perplexity=6560.4756, train_loss=8.788818

Batch 184910, train_perplexity=7074.8647, train_loss=8.864304

Batch 184920, train_perplexity=4619.623, train_loss=8.438068

Batch 184930, train_perplexity=4849.78, train_loss=8.486689

Batch 184940, train_perplexity=5784.0474, train_loss=8.662859

Batch 184950, train_perplexity=5928.241, train_loss=8.687483

Batch 184960, train_perplexity=5728.111, train_loss=8.653141

Batch 184970, train_perplexity=5198.3525, train_loss=8.556097

Batch 184980, train_perplexity=5809.433, train_loss=8.667238

Batch 184990, train_perplexity=4810.176, train_loss=8.478489

Batch 185000, train_perplexity=5123.1997, train_loss=8.541534

Batch 185010, train_perplexity=4690.439, train_loss=8.453281

Batch 185020, train_perplexity=6669.8916, train_loss=8.805359

Batch 185030, train_perplexity=5659.163, train_loss=8.641031

Batch 185040, train_perplexity=5803.3086, train_loss=8.666183

Batch 185050, train_perplexity=5244.5024, train_loss=8.564936

Batch 185060, train_perplexity=5657.2417, train_loss=8.640692

Batch 185070, train_perplexity=4611.3037, train_loss=8.436266

Batch 185080, train_perplexity=5660.0156, train_loss=8.641182

Batch 185090, train_perplexity=5913.436, train_loss=8.684982

Batch 185100, train_perplexity=6393.528, train_loss=8.7630415

Batch 185110, train_perplexity=5355.0146, train_loss=8.585789

Batch 185120, train_perplexity=5217.3994, train_loss=8.559754

Batch 185130, train_perplexity=4981.7974, train_loss=8.513546

Batch 185140, train_perplexity=6052.7686, train_loss=8.708271

Batch 185150, train_perplexity=5143.977, train_loss=8.545582

Batch 185160, train_perplexity=4972.1997, train_loss=8.511618

Batch 185170, train_perplexity=4438.481, train_loss=8.398067

Batch 185180, train_perplexity=4828.404, train_loss=8.482271

Batch 185190, train_perplexity=5324.5723, train_loss=8.580088

Batch 185200, train_perplexity=5175.667, train_loss=8.5517235

Batch 185210, train_perplexity=4615.277, train_loss=8.437127

Batch 185220, train_perplexity=4780.815, train_loss=8.472366

Batch 185230, train_perplexity=4650.0776, train_loss=8.444639

Batch 185240, train_perplexity=5442.0005, train_loss=8.601902

Batch 185250, train_perplexity=5892.6177, train_loss=8.681456

Batch 185260, train_perplexity=4924.218, train_loss=8.501921

Batch 185270, train_perplexity=5071.201, train_loss=8.531333

Batch 185280, train_perplexity=4841.263, train_loss=8.484931

Batch 185290, train_perplexity=6388.286, train_loss=8.762221

Batch 185300, train_perplexity=4927.9526, train_loss=8.502679

Batch 185310, train_perplexity=5691.801, train_loss=8.646782

Batch 185320, train_perplexity=5202.553, train_loss=8.556905

Batch 185330, train_perplexity=5467.3965, train_loss=8.606558

Batch 185340, train_perplexity=5414.3257, train_loss=8.596804

Batch 185350, train_perplexity=5764.1797, train_loss=8.659418

Batch 185360, train_perplexity=5552.6113, train_loss=8.622024

Batch 185370, train_perplexity=6245.535, train_loss=8.739622

Batch 185380, train_perplexity=5677.981, train_loss=8.644351

Batch 185390, train_perplexity=5613.9023, train_loss=8.633001

Batch 185400, train_perplexity=4835.5693, train_loss=8.483754

Batch 185410, train_perplexity=4736.6353, train_loss=8.463082

Batch 185420, train_perplexity=4868.8125, train_loss=8.490605

Batch 185430, train_perplexity=6193.6177, train_loss=8.731275

Batch 185440, train_perplexity=5960.549, train_loss=8.692918

Batch 185450, train_perplexity=4844.2285, train_loss=8.485543

Batch 185460, train_perplexity=6164.3247, train_loss=8.726534

Batch 185470, train_perplexity=5320.562, train_loss=8.579334

Batch 185480, train_perplexity=5786.343, train_loss=8.663256

Batch 185490, train_perplexity=5154.211, train_loss=8.547569

Batch 185500, train_perplexity=5333.1606, train_loss=8.581699

Batch 185510, train_perplexity=5429.4917, train_loss=8.599601

Batch 185520, train_perplexity=5963.2266, train_loss=8.693367

Batch 185530, train_perplexity=5430.864, train_loss=8.5998535

Batch 185540, train_perplexity=4822.6973, train_loss=8.481089

Batch 185550, train_perplexity=5186.32, train_loss=8.55378

Batch 185560, train_perplexity=4828.1367, train_loss=8.482216

Batch 185570, train_perplexity=5718.777, train_loss=8.65151

Batch 185580, train_perplexity=5707.8145, train_loss=8.649591

Batch 185590, train_perplexity=5125.6235, train_loss=8.542007

Batch 185600, train_perplexity=6512.886, train_loss=8.781538

Batch 185610, train_perplexity=5759.943, train_loss=8.658683

Batch 185620, train_perplexity=5564.6396, train_loss=8.624187

Batch 185630, train_perplexity=5494.111, train_loss=8.611432

Batch 185640, train_perplexity=6033.4907, train_loss=8.705081

Batch 185650, train_perplexity=6276.71, train_loss=8.744601

Batch 185660, train_perplexity=4479.9185, train_loss=8.40736

Batch 185670, train_perplexity=5275.458, train_loss=8.570821

Batch 185680, train_perplexity=4882.9297, train_loss=8.493501

Batch 185690, train_perplexity=4633.866, train_loss=8.441147

Batch 185700, train_perplexity=5523.8633, train_loss=8.616833

Batch 185710, train_perplexity=4645.623, train_loss=8.443681

Batch 185720, train_perplexity=3992.6392, train_loss=8.292208

Batch 185730, train_perplexity=5780.518, train_loss=8.662249

Batch 185740, train_perplexity=4736.3145, train_loss=8.463015

Batch 185750, train_perplexity=5016.9165, train_loss=8.520571

Batch 185760, train_perplexity=4351.338, train_loss=8.378239

Batch 185770, train_perplexity=4903.86, train_loss=8.497778

Batch 185780, train_perplexity=5560.9473, train_loss=8.623524

Batch 185790, train_perplexity=4978.729, train_loss=8.51293

Batch 185800, train_perplexity=6265.365, train_loss=8.742792

Batch 185810, train_perplexity=5749.323, train_loss=8.656837

Batch 185820, train_perplexity=5939.82, train_loss=8.689434

Batch 185830, train_perplexity=5886.653, train_loss=8.680443
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 185840, train_perplexity=7396.4136, train_loss=8.908751

Batch 185850, train_perplexity=5477.119, train_loss=8.608335

Batch 185860, train_perplexity=5076.7803, train_loss=8.532433

Batch 185870, train_perplexity=4673.8994, train_loss=8.449749

Batch 185880, train_perplexity=6070.064, train_loss=8.711124

Batch 185890, train_perplexity=5417.9673, train_loss=8.597476

Batch 185900, train_perplexity=5317.9346, train_loss=8.57884

Batch 185910, train_perplexity=5597.9062, train_loss=8.630148

Batch 185920, train_perplexity=5321.633, train_loss=8.5795355

Batch 185930, train_perplexity=4690.4653, train_loss=8.453287

Batch 185940, train_perplexity=5320.339, train_loss=8.579292

Batch 185950, train_perplexity=4551.0874, train_loss=8.423121

Batch 185960, train_perplexity=5820.258, train_loss=8.6691

Batch 185970, train_perplexity=5410.9604, train_loss=8.596182

Batch 185980, train_perplexity=4955.7163, train_loss=8.508297

Batch 185990, train_perplexity=5182.6606, train_loss=8.553074

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Batch 186000, train_perplexity=5258.0444, train_loss=8.567514

Batch 186010, train_perplexity=4775.816, train_loss=8.47132

Batch 186020, train_perplexity=6300.129, train_loss=8.748325

Batch 186030, train_perplexity=4652.282, train_loss=8.445113

Batch 186040, train_perplexity=6757.659, train_loss=8.818432

Batch 186050, train_perplexity=5009.8306, train_loss=8.519157

Batch 186060, train_perplexity=5354.4224, train_loss=8.585678

Batch 186070, train_perplexity=5999.231, train_loss=8.699387

Batch 186080, train_perplexity=4904.88, train_loss=8.497986

Batch 186090, train_perplexity=5673.256, train_loss=8.643518

Batch 186100, train_perplexity=6151.898, train_loss=8.724516

Batch 186110, train_perplexity=5526.935, train_loss=8.617389

Batch 186120, train_perplexity=5571.479, train_loss=8.625416

Batch 186130, train_perplexity=5683.8267, train_loss=8.64538

Batch 186140, train_perplexity=4865.1924, train_loss=8.4898615

Batch 186150, train_perplexity=4922.4946, train_loss=8.501571

Batch 186160, train_perplexity=4925.993, train_loss=8.502281

Batch 186170, train_perplexity=5793.0513, train_loss=8.664414

Batch 186180, train_perplexity=5900.0405, train_loss=8.682714

Batch 186190, train_perplexity=4803.19, train_loss=8.4770355

Batch 186200, train_perplexity=5739.0464, train_loss=8.655048

Batch 186210, train_perplexity=4661.382, train_loss=8.447067

Batch 186220, train_perplexity=6883.2183, train_loss=8.836842

Batch 186230, train_perplexity=4557.8936, train_loss=8.424616

Batch 186240, train_perplexity=4751.253, train_loss=8.466164

Batch 186250, train_perplexity=5892.0107, train_loss=8.681353

Batch 186260, train_perplexity=4877.8193, train_loss=8.492454

Batch 186270, train_perplexity=5623.1826, train_loss=8.634653

Batch 186280, train_perplexity=5050.7705, train_loss=8.527296

Batch 186290, train_perplexity=5077.032, train_loss=8.532482

Batch 186300, train_perplexity=6435.9287, train_loss=8.769651

Batch 186310, train_perplexity=5504.343, train_loss=8.613293

Batch 186320, train_perplexity=5808.264, train_loss=8.667037

Batch 186330, train_perplexity=6502.708, train_loss=8.779974

Batch 186340, train_perplexity=5053.748, train_loss=8.527885

Batch 186350, train_perplexity=5176.9307, train_loss=8.551968

Batch 186360, train_perplexity=6030.022, train_loss=8.704506

Batch 186370, train_perplexity=6181.403, train_loss=8.7293005

Batch 186380, train_perplexity=4462.0186, train_loss=8.403357

Batch 186390, train_perplexity=4257.9155, train_loss=8.356535

Batch 186400, train_perplexity=5305.139, train_loss=8.576431

Batch 186410, train_perplexity=5731.0503, train_loss=8.653654

Batch 186420, train_perplexity=5747.8433, train_loss=8.65658

Batch 186430, train_perplexity=4900.667, train_loss=8.497127

Batch 186440, train_perplexity=5306.5156, train_loss=8.576691

Batch 186450, train_perplexity=4649.182, train_loss=8.444447

Batch 186460, train_perplexity=5085.173, train_loss=8.534084

Batch 186470, train_perplexity=5105.4946, train_loss=8.538073

Batch 186480, train_perplexity=5572.786, train_loss=8.62565

Batch 186490, train_perplexity=5959.463, train_loss=8.692736

Batch 186500, train_perplexity=4947.3013, train_loss=8.5065975

Batch 186510, train_perplexity=5162.347, train_loss=8.549147

Batch 186520, train_perplexity=5154.0483, train_loss=8.547538

Batch 186530, train_perplexity=4586.795, train_loss=8.430937

Batch 186540, train_perplexity=5114.5684, train_loss=8.539848

Batch 186550, train_perplexity=4455.8525, train_loss=8.401974

Batch 186560, train_perplexity=4715.6543, train_loss=8.458643

Batch 186570, train_perplexity=4958.5244, train_loss=8.508863

Batch 186580, train_perplexity=4986.0938, train_loss=8.514408

Batch 186590, train_perplexity=5917.3228, train_loss=8.685639

Batch 186600, train_perplexity=6392.6807, train_loss=8.762909

Batch 186610, train_perplexity=5709.295, train_loss=8.649851

Batch 186620, train_perplexity=5008.679, train_loss=8.518928

Batch 186630, train_perplexity=4249.514, train_loss=8.35456

Batch 186640, train_perplexity=4512.485, train_loss=8.414603

Batch 186650, train_perplexity=4867.7773, train_loss=8.490393

Batch 186660, train_perplexity=5703.233, train_loss=8.648788

Batch 186670, train_perplexity=6343.997, train_loss=8.755264

Batch 186680, train_perplexity=4288.095, train_loss=8.363598

Batch 186690, train_perplexity=5484.1074, train_loss=8.60961

Batch 186700, train_perplexity=5773.0425, train_loss=8.660954

Batch 186710, train_perplexity=4694.726, train_loss=8.454195

Batch 186720, train_perplexity=6523.3794, train_loss=8.783148

Batch 186730, train_perplexity=4124.1904, train_loss=8.324625

Batch 186740, train_perplexity=5408.02, train_loss=8.595638

Batch 186750, train_perplexity=5002.755, train_loss=8.517744

Batch 186760, train_perplexity=5607.4116, train_loss=8.6318445

Batch 186770, train_perplexity=5245.803, train_loss=8.565184

Batch 186780, train_perplexity=6023.981, train_loss=8.703504

Batch 186790, train_perplexity=6134.053, train_loss=8.721611

Batch 186800, train_perplexity=5123.9424, train_loss=8.541679

Batch 186810, train_perplexity=6380.7544, train_loss=8.761042

Batch 186820, train_perplexity=5654.836, train_loss=8.640266

Batch 186830, train_perplexity=6061.676, train_loss=8.709742

Batch 186840, train_perplexity=4970.6353, train_loss=8.511303

Batch 186850, train_perplexity=5038.272, train_loss=8.524818

Batch 186860, train_perplexity=4701.6304, train_loss=8.455665

Batch 186870, train_perplexity=4741.331, train_loss=8.464073

Batch 186880, train_perplexity=5980.095, train_loss=8.696192

Batch 186890, train_perplexity=5054.4517, train_loss=8.528025

Batch 186900, train_perplexity=5270.52, train_loss=8.569884

Batch 186910, train_perplexity=5096.3296, train_loss=8.536276

Batch 186920, train_perplexity=5488.759, train_loss=8.610457

Batch 186930, train_perplexity=5186.191, train_loss=8.553755

Batch 186940, train_perplexity=5085.2407, train_loss=8.534098

Batch 186950, train_perplexity=5532.821, train_loss=8.618453

Batch 186960, train_perplexity=5238.6143, train_loss=8.563812

Batch 186970, train_perplexity=4882.7295, train_loss=8.49346

Batch 186980, train_perplexity=6357.0117, train_loss=8.757314

Batch 186990, train_perplexity=4377.869, train_loss=8.384317

Batch 187000, train_perplexity=4952.768, train_loss=8.507702

Batch 187010, train_perplexity=5092.807, train_loss=8.535584

Batch 187020, train_perplexity=5903.9917, train_loss=8.683384

Batch 187030, train_perplexity=4701.774, train_loss=8.455695

Batch 187040, train_perplexity=4755.2827, train_loss=8.467011

Batch 187050, train_perplexity=5231.1655, train_loss=8.562389

Batch 187060, train_perplexity=5011.341, train_loss=8.519459

Batch 187070, train_perplexity=5009.353, train_loss=8.519062

Batch 187080, train_perplexity=5307.609, train_loss=8.576897
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 187090, train_perplexity=5433.957, train_loss=8.600423

Batch 187100, train_perplexity=5360.666, train_loss=8.5868435

Batch 187110, train_perplexity=5625.736, train_loss=8.635107

Batch 187120, train_perplexity=5321.76, train_loss=8.579559

Batch 187130, train_perplexity=4904.861, train_loss=8.497982

Batch 187140, train_perplexity=4888.7124, train_loss=8.494684

Batch 187150, train_perplexity=6240.165, train_loss=8.738762

Batch 187160, train_perplexity=5911.6655, train_loss=8.684683

Batch 187170, train_perplexity=5134.268, train_loss=8.543693

Batch 187180, train_perplexity=5190.8374, train_loss=8.55465

Batch 187190, train_perplexity=6261.9243, train_loss=8.742243

Batch 187200, train_perplexity=4960.2554, train_loss=8.5092125

Batch 187210, train_perplexity=5870.764, train_loss=8.67774

Batch 187220, train_perplexity=5340.169, train_loss=8.583013

Batch 187230, train_perplexity=5516.4297, train_loss=8.615486

Batch 187240, train_perplexity=6705.3906, train_loss=8.810667

Batch 187250, train_perplexity=6981.379, train_loss=8.851002

Batch 187260, train_perplexity=4414.3726, train_loss=8.392621

Batch 187270, train_perplexity=4429.022, train_loss=8.395934

Batch 187280, train_perplexity=5295.3433, train_loss=8.574583

Batch 187290, train_perplexity=5909.805, train_loss=8.684368

Batch 187300, train_perplexity=4953.533, train_loss=8.507856

Batch 187310, train_perplexity=5543.1245, train_loss=8.620314

Batch 187320, train_perplexity=5864.1724, train_loss=8.676617

Batch 187330, train_perplexity=6231.1855, train_loss=8.737322

Batch 187340, train_perplexity=5422.8213, train_loss=8.5983715

Batch 187350, train_perplexity=5568.095, train_loss=8.624808

Batch 187360, train_perplexity=5902.528, train_loss=8.683136

Batch 187370, train_perplexity=4744.7734, train_loss=8.464799

Batch 187380, train_perplexity=5614.9087, train_loss=8.633181

Batch 187390, train_perplexity=5602.168, train_loss=8.630909

Batch 187400, train_perplexity=6207.2476, train_loss=8.733473

Batch 187410, train_perplexity=5674.4136, train_loss=8.643723

Batch 187420, train_perplexity=5056.4863, train_loss=8.528427

Batch 187430, train_perplexity=5963.0957, train_loss=8.693345

Batch 187440, train_perplexity=5451.028, train_loss=8.6035595

Batch 187450, train_perplexity=6062.127, train_loss=8.709816

Batch 187460, train_perplexity=6538.352, train_loss=8.78544

Batch 187470, train_perplexity=5146.0625, train_loss=8.545987

Batch 187480, train_perplexity=5716.029, train_loss=8.65103

Batch 187490, train_perplexity=5079.444, train_loss=8.532957

Batch 187500, train_perplexity=5924.121, train_loss=8.686788

Batch 187510, train_perplexity=6321.9165, train_loss=8.751778

Batch 187520, train_perplexity=4411.8394, train_loss=8.392047

Batch 187530, train_perplexity=5723.1855, train_loss=8.652281

Batch 187540, train_perplexity=5730.5693, train_loss=8.65357

Batch 187550, train_perplexity=5388.8677, train_loss=8.592091

Batch 187560, train_perplexity=4590.125, train_loss=8.431663

Batch 187570, train_perplexity=5853.59, train_loss=8.67481

Batch 187580, train_perplexity=5709.2515, train_loss=8.649843

Batch 187590, train_perplexity=5743.7446, train_loss=8.655867

Batch 187600, train_perplexity=5278.649, train_loss=8.571425

Batch 187610, train_perplexity=5659.8213, train_loss=8.641148

Batch 187620, train_perplexity=4736.3013, train_loss=8.463012

Batch 187630, train_perplexity=4613.653, train_loss=8.436775

Batch 187640, train_perplexity=4997.1143, train_loss=8.516616

Batch 187650, train_perplexity=5162.495, train_loss=8.549175

Batch 187660, train_perplexity=5561.552, train_loss=8.623632

Batch 187670, train_perplexity=5086.1333, train_loss=8.534273

Batch 187680, train_perplexity=6298.4707, train_loss=8.748062

Batch 187690, train_perplexity=6331.8477, train_loss=8.753347

Batch 187700, train_perplexity=5196.6274, train_loss=8.555765

Batch 187710, train_perplexity=4968.4175, train_loss=8.510857

Batch 187720, train_perplexity=6156.3877, train_loss=8.725245

Batch 187730, train_perplexity=4872.496, train_loss=8.491362

Batch 187740, train_perplexity=4712.3813, train_loss=8.457949

Batch 187750, train_perplexity=6385.856, train_loss=8.761841

Batch 187760, train_perplexity=4761.3135, train_loss=8.468279

Batch 187770, train_perplexity=5988.422, train_loss=8.697583

Batch 187780, train_perplexity=5222.069, train_loss=8.560649

Batch 187790, train_perplexity=4226.924, train_loss=8.34923

Batch 187800, train_perplexity=5162.2783, train_loss=8.549133

Batch 187810, train_perplexity=5901.8413, train_loss=8.68302

Batch 187820, train_perplexity=5763.9707, train_loss=8.659382

Batch 187830, train_perplexity=4937.446, train_loss=8.504603

Batch 187840, train_perplexity=5893.5396, train_loss=8.681612

Batch 187850, train_perplexity=6368.8022, train_loss=8.759167

Batch 187860, train_perplexity=5890.269, train_loss=8.681057

Batch 187870, train_perplexity=5325.984, train_loss=8.580353

Batch 187880, train_perplexity=4768.084, train_loss=8.4697

Batch 187890, train_perplexity=5538.3267, train_loss=8.619448

Batch 187900, train_perplexity=4677.141, train_loss=8.450442

Batch 187910, train_perplexity=5332.7744, train_loss=8.581627

Batch 187920, train_perplexity=6175.893, train_loss=8.728409

Batch 187930, train_perplexity=4638.8184, train_loss=8.442215

Batch 187940, train_perplexity=4122.1025, train_loss=8.324119

Batch 187950, train_perplexity=4491.7812, train_loss=8.410005

Batch 187960, train_perplexity=4856.343, train_loss=8.488041

Batch 187970, train_perplexity=5377.5527, train_loss=8.589989

Batch 187980, train_perplexity=4816.759, train_loss=8.4798565

Batch 187990, train_perplexity=5199.2847, train_loss=8.556276

Batch 188000, train_perplexity=5379.584, train_loss=8.590366

Batch 188010, train_perplexity=5469.472, train_loss=8.606937

Batch 188020, train_perplexity=4682.8765, train_loss=8.451668

Batch 188030, train_perplexity=5082.342, train_loss=8.533527

Batch 188040, train_perplexity=4866.598, train_loss=8.49015

Batch 188050, train_perplexity=5040.2324, train_loss=8.5252075

Batch 188060, train_perplexity=5855.254, train_loss=8.675095

Batch 188070, train_perplexity=5193.447, train_loss=8.555153

Batch 188080, train_perplexity=6549.036, train_loss=8.787073

Batch 188090, train_perplexity=5449.6504, train_loss=8.603307

Batch 188100, train_perplexity=5011.25, train_loss=8.519441

Batch 188110, train_perplexity=5822.434, train_loss=8.669474

Batch 188120, train_perplexity=5714.328, train_loss=8.650732

Batch 188130, train_perplexity=5561.2866, train_loss=8.623585

Batch 188140, train_perplexity=3822.3147, train_loss=8.248611

Batch 188150, train_perplexity=5781.4336, train_loss=8.662407

Batch 188160, train_perplexity=5906.571, train_loss=8.683821

Batch 188170, train_perplexity=4844.903, train_loss=8.4856825

Batch 188180, train_perplexity=5492.9375, train_loss=8.611218

Batch 188190, train_perplexity=5790.7036, train_loss=8.664009

Batch 188200, train_perplexity=5099.8496, train_loss=8.536966

Batch 188210, train_perplexity=6258.557, train_loss=8.741705

Batch 188220, train_perplexity=4905.151, train_loss=8.498041

Batch 188230, train_perplexity=6648.363, train_loss=8.802126

Batch 188240, train_perplexity=5761.459, train_loss=8.658946

Batch 188250, train_perplexity=4725.027, train_loss=8.4606285

Batch 188260, train_perplexity=6217.064, train_loss=8.735053

Batch 188270, train_perplexity=4979.4507, train_loss=8.513075

Batch 188280, train_perplexity=6555.1973, train_loss=8.788013

Batch 188290, train_perplexity=5662.9473, train_loss=8.6417

Batch 188300, train_perplexity=4012.4346, train_loss=8.297153

Batch 188310, train_perplexity=4842.561, train_loss=8.485199

Batch 188320, train_perplexity=5508.8696, train_loss=8.614115

Batch 188330, train_perplexity=6511.508, train_loss=8.781326

Batch 188340, train_perplexity=4885.9766, train_loss=8.494124

Batch 188350, train_perplexity=5085.6436, train_loss=8.534177

Batch 188360, train_perplexity=4751.96, train_loss=8.466312

Batch 188370, train_perplexity=4796.0723, train_loss=8.475553

Batch 188380, train_perplexity=4106.1924, train_loss=8.320251

Batch 188390, train_perplexity=4601.604, train_loss=8.43416

Batch 188400, train_perplexity=5455.1987, train_loss=8.604324
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 188410, train_perplexity=5589.3926, train_loss=8.628626

Batch 188420, train_perplexity=5754.0845, train_loss=8.657665

Batch 188430, train_perplexity=6431.781, train_loss=8.769007

Batch 188440, train_perplexity=4840.266, train_loss=8.484725

Batch 188450, train_perplexity=5472.3315, train_loss=8.60746

Batch 188460, train_perplexity=5492.513, train_loss=8.611141

Batch 188470, train_perplexity=6095.862, train_loss=8.715365

Batch 188480, train_perplexity=5209.673, train_loss=8.558272

Batch 188490, train_perplexity=6414.3413, train_loss=8.766292

Batch 188500, train_perplexity=5024.606, train_loss=8.522102

Batch 188510, train_perplexity=5673.6235, train_loss=8.643583

Batch 188520, train_perplexity=4582.0293, train_loss=8.429897

Batch 188530, train_perplexity=4467.9927, train_loss=8.404695

Batch 188540, train_perplexity=5390.8364, train_loss=8.592456

Batch 188550, train_perplexity=6097.6235, train_loss=8.715654

Batch 188560, train_perplexity=5014.276, train_loss=8.520044

Batch 188570, train_perplexity=5436.0767, train_loss=8.600813

Batch 188580, train_perplexity=5090.2383, train_loss=8.53508

Batch 188590, train_perplexity=5242.3823, train_loss=8.564531

Batch 188600, train_perplexity=4592.7305, train_loss=8.43223

Batch 188610, train_perplexity=5768.249, train_loss=8.660124

Batch 188620, train_perplexity=4532.994, train_loss=8.419138

Batch 188630, train_perplexity=4443.3135, train_loss=8.399156

Batch 188640, train_perplexity=5082.952, train_loss=8.533648

Batch 188650, train_perplexity=6174.574, train_loss=8.728195

Batch 188660, train_perplexity=5309.087, train_loss=8.577175

Batch 188670, train_perplexity=5009.1665, train_loss=8.519025

Batch 188680, train_perplexity=4512.4976, train_loss=8.414606

Batch 188690, train_perplexity=5065.802, train_loss=8.530268

Batch 188700, train_perplexity=4683.6934, train_loss=8.451842

Batch 188710, train_perplexity=5765.477, train_loss=8.659643

Batch 188720, train_perplexity=5555.1167, train_loss=8.622475

Batch 188730, train_perplexity=6743.6763, train_loss=8.81636

Batch 188740, train_perplexity=5908.7905, train_loss=8.684196

Batch 188750, train_perplexity=5766.6206, train_loss=8.659842

Batch 188760, train_perplexity=4999.8125, train_loss=8.517156

Batch 188770, train_perplexity=5351.747, train_loss=8.585178

Batch 188780, train_perplexity=5335.2466, train_loss=8.58209

Batch 188790, train_perplexity=5055.2856, train_loss=8.52819

Batch 188800, train_perplexity=4553.319, train_loss=8.423612

Batch 188810, train_perplexity=5394.3696, train_loss=8.593111

Batch 188820, train_perplexity=5570.023, train_loss=8.6251545

Batch 188830, train_perplexity=5372.806, train_loss=8.589106

Batch 188840, train_perplexity=5402.123, train_loss=8.594547

Batch 188850, train_perplexity=4750.075, train_loss=8.465916

Batch 188860, train_perplexity=5720.1406, train_loss=8.651749

Batch 188870, train_perplexity=5605.3745, train_loss=8.631481

Batch 188880, train_perplexity=5321.4756, train_loss=8.579506

Batch 188890, train_perplexity=5061.2046, train_loss=8.52936

Batch 188900, train_perplexity=5761.2446, train_loss=8.658909

Batch 188910, train_perplexity=4725.09, train_loss=8.460642

Batch 188920, train_perplexity=5004.4585, train_loss=8.518085

Batch 188930, train_perplexity=5448.4556, train_loss=8.603087

Batch 188940, train_perplexity=4509.65, train_loss=8.413975

Batch 188950, train_perplexity=5216.708, train_loss=8.559622

Batch 188960, train_perplexity=5305.5894, train_loss=8.576516

Batch 188970, train_perplexity=5654.5557, train_loss=8.640217

Batch 188980, train_perplexity=4588.952, train_loss=8.431407

Batch 188990, train_perplexity=5657.7114, train_loss=8.640775

Batch 189000, train_perplexity=5806.6025, train_loss=8.666751

Batch 189010, train_perplexity=4866.287, train_loss=8.490087

Batch 189020, train_perplexity=5047.0293, train_loss=8.526555

Batch 189030, train_perplexity=5100.91, train_loss=8.537174

Batch 189040, train_perplexity=6166.418, train_loss=8.726873

Batch 189050, train_perplexity=4558.3193, train_loss=8.424709

Batch 189060, train_perplexity=4715.834, train_loss=8.458681

Batch 189070, train_perplexity=4914.928, train_loss=8.500032

Batch 189080, train_perplexity=6246.0176, train_loss=8.739699

Batch 189090, train_perplexity=5218.499, train_loss=8.559965

Batch 189100, train_perplexity=4914.272, train_loss=8.499899

Batch 189110, train_perplexity=5187.586, train_loss=8.554024

Batch 189120, train_perplexity=5157.884, train_loss=8.548282

Batch 189130, train_perplexity=4976.5547, train_loss=8.512493

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Batch 189140, train_perplexity=5385.426, train_loss=8.591452

Batch 189150, train_perplexity=5973.4834, train_loss=8.695086

Batch 189160, train_perplexity=5931.374, train_loss=8.688011

Batch 189170, train_perplexity=4713.3164, train_loss=8.458147

Batch 189180, train_perplexity=5549.176, train_loss=8.621405

Batch 189190, train_perplexity=5186.1616, train_loss=8.553749

Batch 189200, train_perplexity=4803.602, train_loss=8.477121

Batch 189210, train_perplexity=5047.828, train_loss=8.526713

Batch 189220, train_perplexity=5758.191, train_loss=8.658379

Batch 189230, train_perplexity=4616.7866, train_loss=8.437454

Batch 189240, train_perplexity=4992.8555, train_loss=8.515763

Batch 189250, train_perplexity=5206.0522, train_loss=8.557577

Batch 189260, train_perplexity=4410.3667, train_loss=8.391713

Batch 189270, train_perplexity=4231.6753, train_loss=8.350353

Batch 189280, train_perplexity=5273.8135, train_loss=8.570509

Batch 189290, train_perplexity=5725.4346, train_loss=8.652674

Batch 189300, train_perplexity=5655.3, train_loss=8.640348

Batch 189310, train_perplexity=6381.047, train_loss=8.761087

Batch 189320, train_perplexity=6417.6455, train_loss=8.766807

Batch 189330, train_perplexity=4942.241, train_loss=8.505574

Batch 189340, train_perplexity=5671.33, train_loss=8.643179

Batch 189350, train_perplexity=5832.9155, train_loss=8.671272

Batch 189360, train_perplexity=6377.5913, train_loss=8.760546

Batch 189370, train_perplexity=5831.9146, train_loss=8.671101

Batch 189380, train_perplexity=5720.7407, train_loss=8.651854

Batch 189390, train_perplexity=6122.6216, train_loss=8.719746

Batch 189400, train_perplexity=6081.334, train_loss=8.712979

Batch 189410, train_perplexity=5105.9375, train_loss=8.538159

Batch 189420, train_perplexity=6083.6367, train_loss=8.713358

Batch 189430, train_perplexity=4853.3887, train_loss=8.4874325

Batch 189440, train_perplexity=5507.9136, train_loss=8.613941

Batch 189450, train_perplexity=5879.185, train_loss=8.679173

Batch 189460, train_perplexity=6840.0034, train_loss=8.8305435

Batch 189470, train_perplexity=5971.7236, train_loss=8.694791

Batch 189480, train_perplexity=4637.757, train_loss=8.441986

Batch 189490, train_perplexity=5674.6626, train_loss=8.643766

Batch 189500, train_perplexity=5642.78, train_loss=8.638132

Batch 189510, train_perplexity=5778.457, train_loss=8.661892

Batch 189520, train_perplexity=5056.978, train_loss=8.528524

Batch 189530, train_perplexity=5025.622, train_loss=8.522305

Batch 189540, train_perplexity=5665.9673, train_loss=8.642233

Batch 189550, train_perplexity=4827.5747, train_loss=8.4821

Batch 189560, train_perplexity=5070.9546, train_loss=8.531284

Batch 189570, train_perplexity=5290.6787, train_loss=8.573702

Batch 189580, train_perplexity=6828.337, train_loss=8.828836

Batch 189590, train_perplexity=3871.873, train_loss=8.261494

Batch 189600, train_perplexity=4857.802, train_loss=8.488341

Batch 189610, train_perplexity=5534.3667, train_loss=8.618732

Batch 189620, train_perplexity=5039.2188, train_loss=8.525006

Batch 189630, train_perplexity=5731.0396, train_loss=8.653652

Batch 189640, train_perplexity=5033.4355, train_loss=8.523858

Batch 189650, train_perplexity=5383.4126, train_loss=8.591078
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 189660, train_perplexity=4839.4956, train_loss=8.484566

Batch 189670, train_perplexity=6080.76, train_loss=8.712885

Batch 189680, train_perplexity=5799.259, train_loss=8.665485

Batch 189690, train_perplexity=5253.7686, train_loss=8.566701

Batch 189700, train_perplexity=4636.9697, train_loss=8.441816

Batch 189710, train_perplexity=4169.018, train_loss=8.335436

Batch 189720, train_perplexity=4959.9434, train_loss=8.50915

Batch 189730, train_perplexity=4965.6226, train_loss=8.510294

Batch 189740, train_perplexity=5420.0654, train_loss=8.597863

Batch 189750, train_perplexity=5360.3696, train_loss=8.586788

Batch 189760, train_perplexity=5701.46, train_loss=8.648478

Batch 189770, train_perplexity=5655.2354, train_loss=8.640337

Batch 189780, train_perplexity=6364.103, train_loss=8.758429

Batch 189790, train_perplexity=4197.8394, train_loss=8.342325

Batch 189800, train_perplexity=6079.681, train_loss=8.7127075

Batch 189810, train_perplexity=6384.638, train_loss=8.76165

Batch 189820, train_perplexity=6260.2046, train_loss=8.741968

Batch 189830, train_perplexity=4123.3096, train_loss=8.324411

Batch 189840, train_perplexity=5500.575, train_loss=8.612608

Batch 189850, train_perplexity=6803.0264, train_loss=8.825123

Batch 189860, train_perplexity=5783.9814, train_loss=8.6628475

Batch 189870, train_perplexity=5070.2676, train_loss=8.531149

Batch 189880, train_perplexity=6251.667, train_loss=8.740603

Batch 189890, train_perplexity=5552.415, train_loss=8.621988

Batch 189900, train_perplexity=5519.4976, train_loss=8.616042

Batch 189910, train_perplexity=5517.156, train_loss=8.615618

Batch 189920, train_perplexity=5454.9854, train_loss=8.604285

Batch 189930, train_perplexity=4641.0884, train_loss=8.442704

Batch 189940, train_perplexity=4720.397, train_loss=8.459648

Batch 189950, train_perplexity=5500.565, train_loss=8.612606

Batch 189960, train_perplexity=4602.157, train_loss=8.43428

Batch 189970, train_perplexity=5282.5063, train_loss=8.572156

Batch 189980, train_perplexity=4669.141, train_loss=8.44873

Batch 189990, train_perplexity=4948.6367, train_loss=8.506867

Batch 190000, train_perplexity=5039.771, train_loss=8.525116

Batch 190010, train_perplexity=4597.0864, train_loss=8.433178

Batch 190020, train_perplexity=5348.456, train_loss=8.584563

Batch 190030, train_perplexity=5446.4604, train_loss=8.602721

Batch 190040, train_perplexity=4909.536, train_loss=8.498935

Batch 190050, train_perplexity=5589.936, train_loss=8.628723

Batch 190060, train_perplexity=4677.4043, train_loss=8.450499

Batch 190070, train_perplexity=6969.365, train_loss=8.849279

Batch 190080, train_perplexity=5885.777, train_loss=8.680294

Batch 190090, train_perplexity=5779.6196, train_loss=8.662093

Batch 190100, train_perplexity=5628.022, train_loss=8.635513

Batch 190110, train_perplexity=5212.67, train_loss=8.558847

Batch 190120, train_perplexity=5690.4546, train_loss=8.646545

Batch 190130, train_perplexity=4740.2007, train_loss=8.463835

Batch 190140, train_perplexity=5934.599, train_loss=8.688555

Batch 190150, train_perplexity=4819.5615, train_loss=8.480438

Batch 190160, train_perplexity=6029.562, train_loss=8.70443

Batch 190170, train_perplexity=5334.3916, train_loss=8.58193

Batch 190180, train_perplexity=5354.1313, train_loss=8.585624

Batch 190190, train_perplexity=5531.3384, train_loss=8.618185

Batch 190200, train_perplexity=5308.7983, train_loss=8.577121

Batch 190210, train_perplexity=4652.065, train_loss=8.445066

Batch 190220, train_perplexity=5009.8213, train_loss=8.5191555

Batch 190230, train_perplexity=4923.551, train_loss=8.501785

Batch 190240, train_perplexity=5241.7476, train_loss=8.56441

Batch 190250, train_perplexity=6301.427, train_loss=8.748531

Batch 190260, train_perplexity=5371.126, train_loss=8.588793

Batch 190270, train_perplexity=6140.667, train_loss=8.722689

Batch 190280, train_perplexity=4855.185, train_loss=8.4878025

Batch 190290, train_perplexity=6304.1436, train_loss=8.748962

Batch 190300, train_perplexity=5185.998, train_loss=8.553718

Batch 190310, train_perplexity=5687.46, train_loss=8.646019

Batch 190320, train_perplexity=5178.955, train_loss=8.552359

Batch 190330, train_perplexity=5607.3955, train_loss=8.631842

Batch 190340, train_perplexity=4579.151, train_loss=8.429269

Batch 190350, train_perplexity=4309.257, train_loss=8.368521

Batch 190360, train_perplexity=4932.2544, train_loss=8.5035515

Batch 190370, train_perplexity=5100.438, train_loss=8.537082

Batch 190380, train_perplexity=5041.8145, train_loss=8.525521

Batch 190390, train_perplexity=5625.682, train_loss=8.6350975

Batch 190400, train_perplexity=4577.3433, train_loss=8.428874

Batch 190410, train_perplexity=4833.9004, train_loss=8.483409

Batch 190420, train_perplexity=5799.137, train_loss=8.665464

Batch 190430, train_perplexity=5592.7407, train_loss=8.629225

Batch 190440, train_perplexity=6085.4473, train_loss=8.713655

Batch 190450, train_perplexity=4809.635, train_loss=8.478376

Batch 190460, train_perplexity=5680.272, train_loss=8.644754

Batch 190470, train_perplexity=5806.464, train_loss=8.666727

Batch 190480, train_perplexity=4487.598, train_loss=8.409073

Batch 190490, train_perplexity=6950.203, train_loss=8.846526

Batch 190500, train_perplexity=5135.669, train_loss=8.543965

Batch 190510, train_perplexity=4932.979, train_loss=8.503698

Batch 190520, train_perplexity=5802.0854, train_loss=8.665973

Batch 190530, train_perplexity=7100.239, train_loss=8.867884

Batch 190540, train_perplexity=5466.5312, train_loss=8.6064

Batch 190550, train_perplexity=5988.519, train_loss=8.697599

Batch 190560, train_perplexity=4655.691, train_loss=8.445846

Batch 190570, train_perplexity=5093.1567, train_loss=8.535653

Batch 190580, train_perplexity=4896.6074, train_loss=8.496298

Batch 190590, train_perplexity=7187.525, train_loss=8.880102

Batch 190600, train_perplexity=5426.21, train_loss=8.598996

Batch 190610, train_perplexity=5116.6714, train_loss=8.540259

Batch 190620, train_perplexity=5709.6055, train_loss=8.649905

Batch 190630, train_perplexity=5327.2695, train_loss=8.580594

Batch 190640, train_perplexity=4978.843, train_loss=8.512953

Batch 190650, train_perplexity=5030.0815, train_loss=8.523191

Batch 190660, train_perplexity=4706.2466, train_loss=8.456646

Batch 190670, train_perplexity=5298.1367, train_loss=8.57511

Batch 190680, train_perplexity=4892.1543, train_loss=8.495388

Batch 190690, train_perplexity=5537.64, train_loss=8.619324

Batch 190700, train_perplexity=6760.212, train_loss=8.8188095

Batch 190710, train_perplexity=5281.524, train_loss=8.57197

Batch 190720, train_perplexity=4973.7363, train_loss=8.511927

Batch 190730, train_perplexity=5635.181, train_loss=8.636785

Batch 190740, train_perplexity=4534.7236, train_loss=8.419519

Batch 190750, train_perplexity=4275.094, train_loss=8.360561

Batch 190760, train_perplexity=5119.3413, train_loss=8.540781

Batch 190770, train_perplexity=5615.4175, train_loss=8.633271

Batch 190780, train_perplexity=5218.2456, train_loss=8.5599165

Batch 190790, train_perplexity=5074.6553, train_loss=8.532014

Batch 190800, train_perplexity=5691.594, train_loss=8.646746

Batch 190810, train_perplexity=5064.522, train_loss=8.530015

Batch 190820, train_perplexity=4707.05, train_loss=8.456817

Batch 190830, train_perplexity=5338.453, train_loss=8.582691

Batch 190840, train_perplexity=5907.506, train_loss=8.683979

Batch 190850, train_perplexity=4552.798, train_loss=8.423497

Batch 190860, train_perplexity=5243.0425, train_loss=8.564657

Batch 190870, train_perplexity=5550.975, train_loss=8.621729

Batch 190880, train_perplexity=5692.7725, train_loss=8.646953

Batch 190890, train_perplexity=6358.8857, train_loss=8.757608

Batch 190900, train_perplexity=5557.92, train_loss=8.622979

Batch 190910, train_perplexity=5305.594, train_loss=8.576517

Batch 190920, train_perplexity=5489.047, train_loss=8.61051

Batch 190930, train_perplexity=5481.2, train_loss=8.609079

Batch 190940, train_perplexity=4776.167, train_loss=8.471394

Batch 190950, train_perplexity=5088.4863, train_loss=8.534736

Batch 190960, train_perplexity=5430.667, train_loss=8.599817

Batch 190970, train_perplexity=5283.4487, train_loss=8.572334
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 190980, train_perplexity=5807.86, train_loss=8.666967

Batch 190990, train_perplexity=5944.9653, train_loss=8.6903

Batch 191000, train_perplexity=4229.069, train_loss=8.349737

Batch 191010, train_perplexity=5607.2725, train_loss=8.63182

Batch 191020, train_perplexity=6351.806, train_loss=8.7564945

Batch 191030, train_perplexity=4480.828, train_loss=8.407563

Batch 191040, train_perplexity=6938.408, train_loss=8.844828

Batch 191050, train_perplexity=4508.5015, train_loss=8.41372

Batch 191060, train_perplexity=5921.427, train_loss=8.686333

Batch 191070, train_perplexity=5712.0728, train_loss=8.650337

Batch 191080, train_perplexity=5077.434, train_loss=8.532561

Batch 191090, train_perplexity=4736.5674, train_loss=8.463068

Batch 191100, train_perplexity=4600.564, train_loss=8.433934

Batch 191110, train_perplexity=5556.256, train_loss=8.62268

Batch 191120, train_perplexity=6038.4062, train_loss=8.705895

Batch 191130, train_perplexity=5189.9116, train_loss=8.554472

Batch 191140, train_perplexity=5606.6255, train_loss=8.631704

Batch 191150, train_perplexity=5260.5723, train_loss=8.567995

Batch 191160, train_perplexity=5892.281, train_loss=8.681398

Batch 191170, train_perplexity=4942.1895, train_loss=8.505564

Batch 191180, train_perplexity=5543.273, train_loss=8.62034

Batch 191190, train_perplexity=4242.78, train_loss=8.352974

Batch 191200, train_perplexity=4817.7695, train_loss=8.480066

Batch 191210, train_perplexity=5492.136, train_loss=8.611073

Batch 191220, train_perplexity=4713.0376, train_loss=8.458088

Batch 191230, train_perplexity=5986.657, train_loss=8.6972885

Batch 191240, train_perplexity=7246.544, train_loss=8.88828

Batch 191250, train_perplexity=4890.7173, train_loss=8.495094

Batch 191260, train_perplexity=5767.611, train_loss=8.660013

Batch 191270, train_perplexity=7053.475, train_loss=8.861276

Batch 191280, train_perplexity=6437.9116, train_loss=8.769959

Batch 191290, train_perplexity=5549.3237, train_loss=8.621431

Batch 191300, train_perplexity=4399.7593, train_loss=8.389305

Batch 191310, train_perplexity=6104.0293, train_loss=8.716704

Batch 191320, train_perplexity=5103.323, train_loss=8.537647

Batch 191330, train_perplexity=5660.696, train_loss=8.641302

Batch 191340, train_perplexity=4252.299, train_loss=8.355215

Batch 191350, train_perplexity=6011.619, train_loss=8.701449

Batch 191360, train_perplexity=5585.076, train_loss=8.627853

Batch 191370, train_perplexity=5163.3516, train_loss=8.549341

Batch 191380, train_perplexity=5303.7427, train_loss=8.576168

Batch 191390, train_perplexity=5975.979, train_loss=8.695503

Batch 191400, train_perplexity=4930.778, train_loss=8.503252

Batch 191410, train_perplexity=4962.7393, train_loss=8.509713

Batch 191420, train_perplexity=4532.938, train_loss=8.419126

Batch 191430, train_perplexity=6502.1997, train_loss=8.779896

Batch 191440, train_perplexity=5033.7285, train_loss=8.523916

Batch 191450, train_perplexity=5556.505, train_loss=8.622725

Batch 191460, train_perplexity=5314.948, train_loss=8.578279

Batch 191470, train_perplexity=5051.3965, train_loss=8.52742

Batch 191480, train_perplexity=5816.207, train_loss=8.668404

Batch 191490, train_perplexity=5294.8784, train_loss=8.574495

Batch 191500, train_perplexity=5840.4746, train_loss=8.672567

Batch 191510, train_perplexity=5087.21, train_loss=8.534485

Batch 191520, train_perplexity=5688.1865, train_loss=8.646147

Batch 191530, train_perplexity=5288.2524, train_loss=8.573243

Batch 191540, train_perplexity=5552.7593, train_loss=8.62205

Batch 191550, train_perplexity=4618.407, train_loss=8.437805

Batch 191560, train_perplexity=4950.7373, train_loss=8.507292

Batch 191570, train_perplexity=6715.662, train_loss=8.812198

Batch 191580, train_perplexity=4977.3047, train_loss=8.512644

Batch 191590, train_perplexity=5639.8745, train_loss=8.637617

Batch 191600, train_perplexity=5102.214, train_loss=8.53743

Batch 191610, train_perplexity=5985.6187, train_loss=8.697115

Batch 191620, train_perplexity=5313.362, train_loss=8.57798

Batch 191630, train_perplexity=5638.756, train_loss=8.637419

Batch 191640, train_perplexity=4619.1475, train_loss=8.437965

Batch 191650, train_perplexity=4588.8735, train_loss=8.43139

Batch 191660, train_perplexity=5319.8975, train_loss=8.579209

Batch 191670, train_perplexity=5408.4683, train_loss=8.595721

Batch 191680, train_perplexity=4935.2754, train_loss=8.504164

Batch 191690, train_perplexity=5317.7217, train_loss=8.5788

Batch 191700, train_perplexity=5550.398, train_loss=8.621625

Batch 191710, train_perplexity=5402.8594, train_loss=8.594684

Batch 191720, train_perplexity=4904.482, train_loss=8.497905

Batch 191730, train_perplexity=4996.5947, train_loss=8.516512

Batch 191740, train_perplexity=5423.835, train_loss=8.598558

Batch 191750, train_perplexity=5853.6514, train_loss=8.674821

Batch 191760, train_perplexity=4320.565, train_loss=8.371141

Batch 191770, train_perplexity=5189.318, train_loss=8.554358

Batch 191780, train_perplexity=4967.683, train_loss=8.510709

Batch 191790, train_perplexity=4919.9365, train_loss=8.501051

Batch 191800, train_perplexity=4910.2246, train_loss=8.499075

Batch 191810, train_perplexity=5468.4707, train_loss=8.606754

Batch 191820, train_perplexity=5616.8477, train_loss=8.633526

Batch 191830, train_perplexity=5287.9854, train_loss=8.573193

Batch 191840, train_perplexity=5234.0947, train_loss=8.562949

Batch 191850, train_perplexity=5804.969, train_loss=8.66647

Batch 191860, train_perplexity=4732.54, train_loss=8.462217

Batch 191870, train_perplexity=5635.4443, train_loss=8.636831

Batch 191880, train_perplexity=5046.933, train_loss=8.526536

Batch 191890, train_perplexity=4267.733, train_loss=8.358838

Batch 191900, train_perplexity=5021.483, train_loss=8.521481

Batch 191910, train_perplexity=4853.861, train_loss=8.48753

Batch 191920, train_perplexity=5205.511, train_loss=8.557473

Batch 191930, train_perplexity=5412.4985, train_loss=8.596466

Batch 191940, train_perplexity=5738.3076, train_loss=8.65492

Batch 191950, train_perplexity=4373.15, train_loss=8.383239

Batch 191960, train_perplexity=5964.023, train_loss=8.6935005

Batch 191970, train_perplexity=4839.925, train_loss=8.484654

Batch 191980, train_perplexity=4880.7183, train_loss=8.493048

Batch 191990, train_perplexity=5336.478, train_loss=8.582321

Batch 192000, train_perplexity=6289.833, train_loss=8.74669

Batch 192010, train_perplexity=5421.7563, train_loss=8.598175

Batch 192020, train_perplexity=5066.71, train_loss=8.530447

Batch 192030, train_perplexity=5359.848, train_loss=8.586691

Batch 192040, train_perplexity=5673.6455, train_loss=8.643587

Batch 192050, train_perplexity=5873.3516, train_loss=8.678181

Batch 192060, train_perplexity=5913.1763, train_loss=8.684938

Batch 192070, train_perplexity=6202.0283, train_loss=8.732632

Batch 192080, train_perplexity=6565.4326, train_loss=8.789574

Batch 192090, train_perplexity=6097.3325, train_loss=8.715607

Batch 192100, train_perplexity=5922.579, train_loss=8.686527

Batch 192110, train_perplexity=4965.509, train_loss=8.510271

Batch 192120, train_perplexity=5330.5776, train_loss=8.581215

Batch 192130, train_perplexity=4551.0264, train_loss=8.423108

Batch 192140, train_perplexity=5302.893, train_loss=8.576008

Batch 192150, train_perplexity=5330.5166, train_loss=8.581203

Batch 192160, train_perplexity=5482.272, train_loss=8.609275

Batch 192170, train_perplexity=4708.455, train_loss=8.457115

Batch 192180, train_perplexity=5796.5215, train_loss=8.665013

Batch 192190, train_perplexity=4817.365, train_loss=8.479982

Batch 192200, train_perplexity=4688.422, train_loss=8.452851

Batch 192210, train_perplexity=5544.8115, train_loss=8.620618

Batch 192220, train_perplexity=4584.263, train_loss=8.430385

Batch 192230, train_perplexity=6110.943, train_loss=8.717836

Batch 192240, train_perplexity=4950.6333, train_loss=8.507271

Batch 192250, train_perplexity=4877.587, train_loss=8.492406

Batch 192260, train_perplexity=5259.6943, train_loss=8.567828

Batch 192270, train_perplexity=4911.1562, train_loss=8.499265

Batch 192280, train_perplexity=5208.9478, train_loss=8.558133

Batch 192290, train_perplexity=4591.723, train_loss=8.432011
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 192300, train_perplexity=5426.5464, train_loss=8.599058

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Batch 192310, train_perplexity=5259.097, train_loss=8.567715

Batch 192320, train_perplexity=6161.6445, train_loss=8.726099

Batch 192330, train_perplexity=4980.41, train_loss=8.5132675

Batch 192340, train_perplexity=5060.746, train_loss=8.529269

Batch 192350, train_perplexity=6250.3613, train_loss=8.740395

Batch 192360, train_perplexity=4472.8315, train_loss=8.405777

Batch 192370, train_perplexity=5024.5293, train_loss=8.522087

Batch 192380, train_perplexity=4475.2256, train_loss=8.406312

Batch 192390, train_perplexity=4724.964, train_loss=8.460615

Batch 192400, train_perplexity=5763.399, train_loss=8.659283

Batch 192410, train_perplexity=5276.208, train_loss=8.570963

Batch 192420, train_perplexity=5218.6235, train_loss=8.559989

Batch 192430, train_perplexity=5885.5747, train_loss=8.68026

Batch 192440, train_perplexity=4915.922, train_loss=8.500235

Batch 192450, train_perplexity=5597.698, train_loss=8.630111

Batch 192460, train_perplexity=5075.2505, train_loss=8.532131

Batch 192470, train_perplexity=4976.84, train_loss=8.51255

Batch 192480, train_perplexity=5197.857, train_loss=8.556002

Batch 192490, train_perplexity=5496.223, train_loss=8.611816

Batch 192500, train_perplexity=6916.6724, train_loss=8.84169

Batch 192510, train_perplexity=5091.6807, train_loss=8.535363

Batch 192520, train_perplexity=5859.857, train_loss=8.67588

Batch 192530, train_perplexity=4066.265, train_loss=8.31048

Batch 192540, train_perplexity=4987.987, train_loss=8.514788

Batch 192550, train_perplexity=4549.6943, train_loss=8.422815

Batch 192560, train_perplexity=5540.3657, train_loss=8.619816

Batch 192570, train_perplexity=6000.3584, train_loss=8.699574

Batch 192580, train_perplexity=6264.66, train_loss=8.74268

Batch 192590, train_perplexity=6516.291, train_loss=8.782061

Batch 192600, train_perplexity=5812.0376, train_loss=8.667686

Batch 192610, train_perplexity=4888.055, train_loss=8.49455

Batch 192620, train_perplexity=5228.193, train_loss=8.561821

Batch 192630, train_perplexity=5738.9756, train_loss=8.655036

Batch 192640, train_perplexity=5232.4927, train_loss=8.562643

Batch 192650, train_perplexity=5069.736, train_loss=8.531044

Batch 192660, train_perplexity=5268.4795, train_loss=8.569497

Batch 192670, train_perplexity=5539.9854, train_loss=8.619747

Batch 192680, train_perplexity=6350.2197, train_loss=8.756245

Batch 192690, train_perplexity=5265.993, train_loss=8.569025

Batch 192700, train_perplexity=5400.3613, train_loss=8.594221

Batch 192710, train_perplexity=5681.54, train_loss=8.644978

Batch 192720, train_perplexity=5442.4624, train_loss=8.601987

Batch 192730, train_perplexity=5457.249, train_loss=8.6047

Batch 192740, train_perplexity=4500.5713, train_loss=8.41196

Batch 192750, train_perplexity=4955.688, train_loss=8.508291

Batch 192760, train_perplexity=5286.624, train_loss=8.572935

Batch 192770, train_perplexity=5167.5684, train_loss=8.550158

Batch 192780, train_perplexity=5008.713, train_loss=8.518934

Batch 192790, train_perplexity=4729.03, train_loss=8.461475

Batch 192800, train_perplexity=5110.1416, train_loss=8.538982

Batch 192810, train_perplexity=4843.7896, train_loss=8.485453

Batch 192820, train_perplexity=5552.5796, train_loss=8.622018

Batch 192830, train_perplexity=4870.1177, train_loss=8.490873

Batch 192840, train_perplexity=5962.1577, train_loss=8.693188

Batch 192850, train_perplexity=5853.4673, train_loss=8.674789

Batch 192860, train_perplexity=5236.3965, train_loss=8.563389

Batch 192870, train_perplexity=6784.2764, train_loss=8.822363

Batch 192880, train_perplexity=6257.25, train_loss=8.741496

Batch 192890, train_perplexity=5911.6597, train_loss=8.684682

Batch 192900, train_perplexity=5130.8857, train_loss=8.543034

Batch 192910, train_perplexity=5605.15, train_loss=8.631441

Batch 192920, train_perplexity=5397.272, train_loss=8.593649

Batch 192930, train_perplexity=4965.85, train_loss=8.51034

Batch 192940, train_perplexity=5389.7314, train_loss=8.592251

Batch 192950, train_perplexity=6118.629, train_loss=8.719093

Batch 192960, train_perplexity=6030.597, train_loss=8.704601

Batch 192970, train_perplexity=6137.4062, train_loss=8.7221575

Batch 192980, train_perplexity=6216.7856, train_loss=8.735008

Batch 192990, train_perplexity=6217.39, train_loss=8.7351055

Batch 193000, train_perplexity=5679.671, train_loss=8.644649

Batch 193010, train_perplexity=5947.9824, train_loss=8.690807

Batch 193020, train_perplexity=6183.1714, train_loss=8.729587

Batch 193030, train_perplexity=6830.1084, train_loss=8.829096

Batch 193040, train_perplexity=5171.1724, train_loss=8.550855

Batch 193050, train_perplexity=4530.5566, train_loss=8.4186

Batch 193060, train_perplexity=5915.387, train_loss=8.685312

Batch 193070, train_perplexity=4688.3457, train_loss=8.452835

Batch 193080, train_perplexity=5656.206, train_loss=8.640509

Batch 193090, train_perplexity=5064.71, train_loss=8.530052

Batch 193100, train_perplexity=7555.693, train_loss=8.930057

Batch 193110, train_perplexity=4845.5176, train_loss=8.485809

Batch 193120, train_perplexity=5764.806, train_loss=8.659527

Batch 193130, train_perplexity=5277.325, train_loss=8.571175

Batch 193140, train_perplexity=4934.7246, train_loss=8.504052

Batch 193150, train_perplexity=4980.5005, train_loss=8.513286

Batch 193160, train_perplexity=5729.662, train_loss=8.653412

Batch 193170, train_perplexity=5407.22, train_loss=8.59549

Batch 193180, train_perplexity=4738.745, train_loss=8.463528

Batch 193190, train_perplexity=5305.635, train_loss=8.576525

Batch 193200, train_perplexity=4902.546, train_loss=8.49751

Batch 193210, train_perplexity=6442.573, train_loss=8.770683

Batch 193220, train_perplexity=4765.2295, train_loss=8.469101

Batch 193230, train_perplexity=5106.892, train_loss=8.538346

Batch 193240, train_perplexity=4890.382, train_loss=8.495026

Batch 193250, train_perplexity=5032.9414, train_loss=8.52376

Batch 193260, train_perplexity=5307.2847, train_loss=8.576836

Batch 193270, train_perplexity=4990.3945, train_loss=8.51527

Batch 193280, train_perplexity=4904.636, train_loss=8.497936

Batch 193290, train_perplexity=5585.609, train_loss=8.627949

Batch 193300, train_perplexity=4450.693, train_loss=8.400815

Batch 193310, train_perplexity=4867.2573, train_loss=8.490286

Batch 193320, train_perplexity=5007.1416, train_loss=8.5186205

Batch 193330, train_perplexity=5908.047, train_loss=8.684071

Batch 193340, train_perplexity=4690.6177, train_loss=8.45332

Batch 193350, train_perplexity=6009.6875, train_loss=8.701128

Batch 193360, train_perplexity=3652.0046, train_loss=8.203032

Batch 193370, train_perplexity=5618.4224, train_loss=8.633806

Batch 193380, train_perplexity=4625.6274, train_loss=8.439367

Batch 193390, train_perplexity=5186.755, train_loss=8.553864

Batch 193400, train_perplexity=6192.5957, train_loss=8.73111

Batch 193410, train_perplexity=4968.3037, train_loss=8.510834

Batch 193420, train_perplexity=5511.5815, train_loss=8.614607

Batch 193430, train_perplexity=4399.13, train_loss=8.389162

Batch 193440, train_perplexity=5739.025, train_loss=8.655045

Batch 193450, train_perplexity=4587.858, train_loss=8.431169

Batch 193460, train_perplexity=4564.7754, train_loss=8.426125

Batch 193470, train_perplexity=5478.1167, train_loss=8.608517

Batch 193480, train_perplexity=5327.0815, train_loss=8.580559

Batch 193490, train_perplexity=4723.9272, train_loss=8.460396

Batch 193500, train_perplexity=6092.0317, train_loss=8.714737

Batch 193510, train_perplexity=5735.933, train_loss=8.654506

Batch 193520, train_perplexity=5557.4956, train_loss=8.622903

Batch 193530, train_perplexity=5173.8506, train_loss=8.551373

Batch 193540, train_perplexity=5041.5884, train_loss=8.525476

Batch 193550, train_perplexity=5829.201, train_loss=8.670635
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 193560, train_perplexity=5619.816, train_loss=8.634054

Batch 193570, train_perplexity=6652.016, train_loss=8.802675

Batch 193580, train_perplexity=4684.529, train_loss=8.452021

Batch 193590, train_perplexity=6009.057, train_loss=8.701023

Batch 193600, train_perplexity=4957.314, train_loss=8.508619

Batch 193610, train_perplexity=4812.204, train_loss=8.47891

Batch 193620, train_perplexity=4626.43, train_loss=8.439541

Batch 193630, train_perplexity=5787.435, train_loss=8.6634445

Batch 193640, train_perplexity=4342.09, train_loss=8.376111

Batch 193650, train_perplexity=5941.6777, train_loss=8.689747

Batch 193660, train_perplexity=5190.4067, train_loss=8.554567

Batch 193670, train_perplexity=5928.8403, train_loss=8.687584

Batch 193680, train_perplexity=5419.461, train_loss=8.597752

Batch 193690, train_perplexity=4523.407, train_loss=8.417021

Batch 193700, train_perplexity=5198.2036, train_loss=8.556068

Batch 193710, train_perplexity=5026.432, train_loss=8.522466

Batch 193720, train_perplexity=6263.3936, train_loss=8.742477

Batch 193730, train_perplexity=5599.4175, train_loss=8.630418

Batch 193740, train_perplexity=6030.1714, train_loss=8.704531

Batch 193750, train_perplexity=6058.694, train_loss=8.7092495

Batch 193760, train_perplexity=4605.0195, train_loss=8.434902

Batch 193770, train_perplexity=4641.009, train_loss=8.442687

Batch 193780, train_perplexity=6058.89, train_loss=8.709282

Batch 193790, train_perplexity=6668.3906, train_loss=8.805134

Batch 193800, train_perplexity=5268.334, train_loss=8.569469

Batch 193810, train_perplexity=4934.5835, train_loss=8.504024

Batch 193820, train_perplexity=5006.621, train_loss=8.518517

Batch 193830, train_perplexity=4839.5737, train_loss=8.484582

Batch 193840, train_perplexity=5052.818, train_loss=8.527701

Batch 193850, train_perplexity=6130.8135, train_loss=8.721083

Batch 193860, train_perplexity=4994.8794, train_loss=8.516169

Batch 193870, train_perplexity=4850.46, train_loss=8.486829

Batch 193880, train_perplexity=5658.2456, train_loss=8.640869

Batch 193890, train_perplexity=5385.3022, train_loss=8.591429

Batch 193900, train_perplexity=5810.342, train_loss=8.667395

Batch 193910, train_perplexity=5854.8403, train_loss=8.675024

Batch 193920, train_perplexity=4873.709, train_loss=8.491611

Batch 193930, train_perplexity=4120.1255, train_loss=8.323639

Batch 193940, train_perplexity=5198.9575, train_loss=8.556213

Batch 193950, train_perplexity=4326.073, train_loss=8.372416

Batch 193960, train_perplexity=4655.167, train_loss=8.445733

Batch 193970, train_perplexity=4681.992, train_loss=8.451479

Batch 193980, train_perplexity=4545.5396, train_loss=8.421902

Batch 193990, train_perplexity=6181.4497, train_loss=8.729308

Batch 194000, train_perplexity=5076.674, train_loss=8.532412

Batch 194010, train_perplexity=5208.466, train_loss=8.558041

Batch 194020, train_perplexity=4804.5186, train_loss=8.477312

Batch 194030, train_perplexity=5077.555, train_loss=8.532585

Batch 194040, train_perplexity=4888.069, train_loss=8.494553

Batch 194050, train_perplexity=4381.929, train_loss=8.385244

Batch 194060, train_perplexity=4868.13, train_loss=8.490465

Batch 194070, train_perplexity=5647.5605, train_loss=8.638979

Batch 194080, train_perplexity=4699.434, train_loss=8.455197

Batch 194090, train_perplexity=5412.989, train_loss=8.596557

Batch 194100, train_perplexity=4798.8584, train_loss=8.476133

Batch 194110, train_perplexity=5909.546, train_loss=8.684324

Batch 194120, train_perplexity=5494.3677, train_loss=8.611479

Batch 194130, train_perplexity=5205.0493, train_loss=8.5573845

Batch 194140, train_perplexity=4886.997, train_loss=8.494333

Batch 194150, train_perplexity=4649.2485, train_loss=8.444461

Batch 194160, train_perplexity=5120.181, train_loss=8.540945

Batch 194170, train_perplexity=5367.9766, train_loss=8.588206

Batch 194180, train_perplexity=6266.261, train_loss=8.742935

Batch 194190, train_perplexity=5401.597, train_loss=8.59445

Batch 194200, train_perplexity=6182.464, train_loss=8.729472

Batch 194210, train_perplexity=5109.854, train_loss=8.538926

Batch 194220, train_perplexity=5418.4585, train_loss=8.597567

Batch 194230, train_perplexity=4659.258, train_loss=8.446611

Batch 194240, train_perplexity=6444.2935, train_loss=8.77095

Batch 194250, train_perplexity=4722.441, train_loss=8.460081

Batch 194260, train_perplexity=5410.5527, train_loss=8.596107

Batch 194270, train_perplexity=5483.3125, train_loss=8.609465

Batch 194280, train_perplexity=5796.9365, train_loss=8.665085

Batch 194290, train_perplexity=4628.394, train_loss=8.439965

Batch 194300, train_perplexity=6877.5356, train_loss=8.836016

Batch 194310, train_perplexity=6887.46, train_loss=8.837458

Batch 194320, train_perplexity=5165.011, train_loss=8.549663

Batch 194330, train_perplexity=5454.564, train_loss=8.604208

Batch 194340, train_perplexity=5436.7197, train_loss=8.600931

Batch 194350, train_perplexity=5120.8843, train_loss=8.541082

Batch 194360, train_perplexity=6212.0146, train_loss=8.734241

Batch 194370, train_perplexity=5911.3496, train_loss=8.684629

Batch 194380, train_perplexity=5929.819, train_loss=8.687749

Batch 194390, train_perplexity=4763.766, train_loss=8.468794

Batch 194400, train_perplexity=4242.5693, train_loss=8.352924

Batch 194410, train_perplexity=5147.206, train_loss=8.546209

Batch 194420, train_perplexity=4210.3926, train_loss=8.345311

Batch 194430, train_perplexity=4430.357, train_loss=8.396235

Batch 194440, train_perplexity=5653.758, train_loss=8.640076

Batch 194450, train_perplexity=6570.193, train_loss=8.790298

Batch 194460, train_perplexity=4857.802, train_loss=8.488341

Batch 194470, train_perplexity=5298.859, train_loss=8.575247

Batch 194480, train_perplexity=5641.8325, train_loss=8.637964

Batch 194490, train_perplexity=5721.794, train_loss=8.652038

Batch 194500, train_perplexity=5008.092, train_loss=8.51881

Batch 194510, train_perplexity=4900.4194, train_loss=8.497076

Batch 194520, train_perplexity=5246.8237, train_loss=8.565378

Batch 194530, train_perplexity=5544.626, train_loss=8.6205845

Batch 194540, train_perplexity=4255.918, train_loss=8.356066

Batch 194550, train_perplexity=6052.145, train_loss=8.708168

Batch 194560, train_perplexity=5751.298, train_loss=8.657181

Batch 194570, train_perplexity=6326.802, train_loss=8.75255

Batch 194580, train_perplexity=5213.008, train_loss=8.558912

Batch 194590, train_perplexity=4649.492, train_loss=8.444513

Batch 194600, train_perplexity=6500.0913, train_loss=8.779572

Batch 194610, train_perplexity=5300.6685, train_loss=8.575588

Batch 194620, train_perplexity=5107.5786, train_loss=8.538481

Batch 194630, train_perplexity=4849.6826, train_loss=8.486669

Batch 194640, train_perplexity=5153.2275, train_loss=8.547379

Batch 194650, train_perplexity=4804.303, train_loss=8.477267

Batch 194660, train_perplexity=5009.157, train_loss=8.519023

Batch 194670, train_perplexity=5489.1045, train_loss=8.61052

Batch 194680, train_perplexity=4987.302, train_loss=8.51465

Batch 194690, train_perplexity=5925.6523, train_loss=8.687046

Batch 194700, train_perplexity=5114.4272, train_loss=8.539821

Batch 194710, train_perplexity=4802.6494, train_loss=8.476923

Batch 194720, train_perplexity=3869.8132, train_loss=8.260962

Batch 194730, train_perplexity=5977.3013, train_loss=8.6957245

Batch 194740, train_perplexity=5891.213, train_loss=8.681217

Batch 194750, train_perplexity=5074.66, train_loss=8.532015

Batch 194760, train_perplexity=5168.6035, train_loss=8.550358

Batch 194770, train_perplexity=6067.118, train_loss=8.710639

Batch 194780, train_perplexity=5049.9805, train_loss=8.52714

Batch 194790, train_perplexity=5342.823, train_loss=8.583509

Batch 194800, train_perplexity=4777.834, train_loss=8.471743

Batch 194810, train_perplexity=4562.051, train_loss=8.425528

Batch 194820, train_perplexity=5647.0327, train_loss=8.6388855

Batch 194830, train_perplexity=5044.137, train_loss=8.525982

Batch 194840, train_perplexity=5574.434, train_loss=8.625946

Batch 194850, train_perplexity=5369.8916, train_loss=8.588563

Batch 194860, train_perplexity=5041.492, train_loss=8.525457

Batch 194870, train_perplexity=5174.3296, train_loss=8.551465
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 194880, train_perplexity=4884.3223, train_loss=8.493786

Batch 194890, train_perplexity=5442.255, train_loss=8.601949

Batch 194900, train_perplexity=4082.8806, train_loss=8.314558

Batch 194910, train_perplexity=5201.263, train_loss=8.556657

Batch 194920, train_perplexity=5028.571, train_loss=8.522891

Batch 194930, train_perplexity=5329.81, train_loss=8.581071

Batch 194940, train_perplexity=4703.312, train_loss=8.456022

Batch 194950, train_perplexity=4334.9526, train_loss=8.374466

Batch 194960, train_perplexity=6497.1167, train_loss=8.779114

Batch 194970, train_perplexity=5316.7886, train_loss=8.578625

Batch 194980, train_perplexity=6061.699, train_loss=8.709745

Batch 194990, train_perplexity=5065.8164, train_loss=8.530271

Batch 195000, train_perplexity=5667.34, train_loss=8.642475

Batch 195010, train_perplexity=5215.0615, train_loss=8.559306

Batch 195020, train_perplexity=5896.3447, train_loss=8.682088

Batch 195030, train_perplexity=5524.8745, train_loss=8.617016

Batch 195040, train_perplexity=4855.0737, train_loss=8.48778

Batch 195050, train_perplexity=4978.8335, train_loss=8.512951

Batch 195060, train_perplexity=5107.1357, train_loss=8.538394

Batch 195070, train_perplexity=4368.077, train_loss=8.382078

Batch 195080, train_perplexity=6111.52, train_loss=8.717931

Batch 195090, train_perplexity=6251.6313, train_loss=8.740598

Batch 195100, train_perplexity=6829.418, train_loss=8.828995

Batch 195110, train_perplexity=5026.3794, train_loss=8.522455

Batch 195120, train_perplexity=5368.5854, train_loss=8.58832

Batch 195130, train_perplexity=5043.8965, train_loss=8.525934

Batch 195140, train_perplexity=4885.8784, train_loss=8.494104

Batch 195150, train_perplexity=5883.016, train_loss=8.679825

Batch 195160, train_perplexity=5102.418, train_loss=8.53747

Batch 195170, train_perplexity=5970.351, train_loss=8.694561

Batch 195180, train_perplexity=5144.5464, train_loss=8.545692

Batch 195190, train_perplexity=5183.501, train_loss=8.553236

Batch 195200, train_perplexity=5367.439, train_loss=8.588106

Batch 195210, train_perplexity=4831.324, train_loss=8.482876

Batch 195220, train_perplexity=6355.369, train_loss=8.757055

Batch 195230, train_perplexity=5860.863, train_loss=8.676052

Batch 195240, train_perplexity=6291.3867, train_loss=8.746937

Batch 195250, train_perplexity=5722.547, train_loss=8.652169

Batch 195260, train_perplexity=4645.92, train_loss=8.443745

Batch 195270, train_perplexity=5806.3423, train_loss=8.666706

Batch 195280, train_perplexity=6083.5034, train_loss=8.713336

Batch 195290, train_perplexity=5004.1343, train_loss=8.51802

Batch 195300, train_perplexity=4678.1626, train_loss=8.450661

Batch 195310, train_perplexity=5476.5396, train_loss=8.608229

Batch 195320, train_perplexity=5156.054, train_loss=8.547927

Batch 195330, train_perplexity=5966.179, train_loss=8.693862

Batch 195340, train_perplexity=5664.271, train_loss=8.641933

Batch 195350, train_perplexity=5234.779, train_loss=8.56308

Batch 195360, train_perplexity=4515.0806, train_loss=8.415178

Batch 195370, train_perplexity=5336.264, train_loss=8.582281

Batch 195380, train_perplexity=5120.6743, train_loss=8.541041

Batch 195390, train_perplexity=5521.588, train_loss=8.616421

Batch 195400, train_perplexity=6070.747, train_loss=8.711237

Batch 195410, train_perplexity=5280.3257, train_loss=8.571743

Batch 195420, train_perplexity=5272.1436, train_loss=8.570192

Batch 195430, train_perplexity=6129.036, train_loss=8.720793

Batch 195440, train_perplexity=4685.6997, train_loss=8.4522705

Batch 195450, train_perplexity=5844.3306, train_loss=8.673227

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Batch 195460, train_perplexity=6007.418, train_loss=8.70075

Batch 195470, train_perplexity=6609.837, train_loss=8.796314

Batch 195480, train_perplexity=5206.986, train_loss=8.557756

Batch 195490, train_perplexity=6000.2324, train_loss=8.6995535

Batch 195500, train_perplexity=4833.8125, train_loss=8.483391

Batch 195510, train_perplexity=5133.7734, train_loss=8.543596

Batch 195520, train_perplexity=5328.829, train_loss=8.580887

Batch 195530, train_perplexity=5565.743, train_loss=8.624386

Batch 195540, train_perplexity=4112.392, train_loss=8.32176

Batch 195550, train_perplexity=4769.1895, train_loss=8.469932

Batch 195560, train_perplexity=4852.6763, train_loss=8.487286

Batch 195570, train_perplexity=4868.8545, train_loss=8.490614

Batch 195580, train_perplexity=4989.281, train_loss=8.515047

Batch 195590, train_perplexity=4673.948, train_loss=8.4497595

Batch 195600, train_perplexity=5992.4497, train_loss=8.698256

Batch 195610, train_perplexity=4977.7417, train_loss=8.512732

Batch 195620, train_perplexity=5291.739, train_loss=8.573902

Batch 195630, train_perplexity=5158.794, train_loss=8.548458

Batch 195640, train_perplexity=6076.91, train_loss=8.712252

Batch 195650, train_perplexity=5610.5464, train_loss=8.632403

Batch 195660, train_perplexity=6057.1685, train_loss=8.708998

Batch 195670, train_perplexity=5567.0654, train_loss=8.624623

Batch 195680, train_perplexity=5568.9077, train_loss=8.624954

Batch 195690, train_perplexity=6455.538, train_loss=8.772694

Batch 195700, train_perplexity=4854.375, train_loss=8.487636

Batch 195710, train_perplexity=5356.5776, train_loss=8.586081

Batch 195720, train_perplexity=5901.9595, train_loss=8.68304

Batch 195730, train_perplexity=5903.147, train_loss=8.683241

Batch 195740, train_perplexity=5010.834, train_loss=8.519358

Batch 195750, train_perplexity=6248.955, train_loss=8.74017

Batch 195760, train_perplexity=5331.6963, train_loss=8.581425

Batch 195770, train_perplexity=6043.9546, train_loss=8.706814

Batch 195780, train_perplexity=5697.2476, train_loss=8.647738

Batch 195790, train_perplexity=4980.586, train_loss=8.513303

Batch 195800, train_perplexity=5062.711, train_loss=8.529657

Batch 195810, train_perplexity=4743.6333, train_loss=8.464559

Batch 195820, train_perplexity=5586.4507, train_loss=8.628099

Batch 195830, train_perplexity=4289.629, train_loss=8.3639555

Batch 195840, train_perplexity=5852.2505, train_loss=8.674582

Batch 195850, train_perplexity=5236.876, train_loss=8.56348

Batch 195860, train_perplexity=5755.962, train_loss=8.657991

Batch 195870, train_perplexity=5345.188, train_loss=8.583952

Batch 195880, train_perplexity=5542.924, train_loss=8.620277

Batch 195890, train_perplexity=6510.5015, train_loss=8.781172

Batch 195900, train_perplexity=6726.0264, train_loss=8.81374

Batch 195910, train_perplexity=4768.048, train_loss=8.469692

Batch 195920, train_perplexity=6108.566, train_loss=8.717447

Batch 195930, train_perplexity=5966.0195, train_loss=8.693835

Batch 195940, train_perplexity=4228.577, train_loss=8.349621

Batch 195950, train_perplexity=4083.924, train_loss=8.314814

Batch 195960, train_perplexity=5720.4023, train_loss=8.651794

Batch 195970, train_perplexity=5908.847, train_loss=8.684206

Batch 195980, train_perplexity=5890.932, train_loss=8.6811695

Batch 195990, train_perplexity=5128.4644, train_loss=8.542562

Batch 196000, train_perplexity=4960.0425, train_loss=8.50917

Batch 196010, train_perplexity=5739.1997, train_loss=8.655075

Batch 196020, train_perplexity=7147.1284, train_loss=8.874466

Batch 196030, train_perplexity=5697.2964, train_loss=8.647747

Batch 196040, train_perplexity=5055.691, train_loss=8.52827

Batch 196050, train_perplexity=5470.067, train_loss=8.607046

Batch 196060, train_perplexity=5846.638, train_loss=8.673622

Batch 196070, train_perplexity=6316.2275, train_loss=8.750877

Batch 196080, train_perplexity=4837.447, train_loss=8.484142

Batch 196090, train_perplexity=5610.905, train_loss=8.632467

Batch 196100, train_perplexity=5669.881, train_loss=8.642923

Batch 196110, train_perplexity=5386.751, train_loss=8.591698

Batch 196120, train_perplexity=6694.9624, train_loss=8.809111
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 196130, train_perplexity=5013.9414, train_loss=8.519978

Batch 196140, train_perplexity=5813.6895, train_loss=8.667971

Batch 196150, train_perplexity=4918.004, train_loss=8.500658

Batch 196160, train_perplexity=5003.4995, train_loss=8.517893

Batch 196170, train_perplexity=6559.1367, train_loss=8.788614

Batch 196180, train_perplexity=5479.2246, train_loss=8.608719

Batch 196190, train_perplexity=4612.157, train_loss=8.436451

Batch 196200, train_perplexity=5745.695, train_loss=8.656206

Batch 196210, train_perplexity=4690.6846, train_loss=8.453334

Batch 196220, train_perplexity=5487.979, train_loss=8.610315

Batch 196230, train_perplexity=5569.6724, train_loss=8.625092

Batch 196240, train_perplexity=5329.7134, train_loss=8.581053

Batch 196250, train_perplexity=5246.4136, train_loss=8.5653

Batch 196260, train_perplexity=5353.9014, train_loss=8.585581

Batch 196270, train_perplexity=5315.5005, train_loss=8.5783825

Batch 196280, train_perplexity=5788.23, train_loss=8.663582

Batch 196290, train_perplexity=4550.007, train_loss=8.422884

Batch 196300, train_perplexity=5630.2656, train_loss=8.635912

Batch 196310, train_perplexity=7095.331, train_loss=8.867192

Batch 196320, train_perplexity=4773.016, train_loss=8.470734

Batch 196330, train_perplexity=5117.33, train_loss=8.540388

Batch 196340, train_perplexity=5147.4614, train_loss=8.546259

Batch 196350, train_perplexity=4959.877, train_loss=8.509136

Batch 196360, train_perplexity=4761.759, train_loss=8.468372

Batch 196370, train_perplexity=5987.5938, train_loss=8.697445

Batch 196380, train_perplexity=4217.691, train_loss=8.347043

Batch 196390, train_perplexity=5477.7197, train_loss=8.608444

Batch 196400, train_perplexity=5073.0005, train_loss=8.531688

Batch 196410, train_perplexity=4245.2485, train_loss=8.353556

Batch 196420, train_perplexity=5958.2695, train_loss=8.692535

Batch 196430, train_perplexity=4702.061, train_loss=8.455756

Batch 196440, train_perplexity=5287.9097, train_loss=8.573178

Batch 196450, train_perplexity=5102.8804, train_loss=8.53756

Batch 196460, train_perplexity=4622.351, train_loss=8.438659

Batch 196470, train_perplexity=4936.9277, train_loss=8.5044985

Batch 196480, train_perplexity=5382.7197, train_loss=8.590949

Batch 196490, train_perplexity=5327.676, train_loss=8.58067

Batch 196500, train_perplexity=5394.349, train_loss=8.593107

Batch 196510, train_perplexity=5211.944, train_loss=8.558708

Batch 196520, train_perplexity=5213.4653, train_loss=8.559

Batch 196530, train_perplexity=5698.7583, train_loss=8.648004

Batch 196540, train_perplexity=5195.1855, train_loss=8.555488

Batch 196550, train_perplexity=5624.121, train_loss=8.63482

Batch 196560, train_perplexity=4637.213, train_loss=8.441869

Batch 196570, train_perplexity=5005.6997, train_loss=8.5183325

Batch 196580, train_perplexity=5407.803, train_loss=8.595598

Batch 196590, train_perplexity=4600.4326, train_loss=8.433906

Batch 196600, train_perplexity=4610.763, train_loss=8.436149

Batch 196610, train_perplexity=5379.1685, train_loss=8.590289

Batch 196620, train_perplexity=5557.798, train_loss=8.622957

Batch 196630, train_perplexity=5085.8374, train_loss=8.534215

Batch 196640, train_perplexity=4806.1455, train_loss=8.477651

Batch 196650, train_perplexity=5809.333, train_loss=8.667221

Batch 196660, train_perplexity=5994.5244, train_loss=8.698602

Batch 196670, train_perplexity=6124.315, train_loss=8.720022

Batch 196680, train_perplexity=5117.0127, train_loss=8.540326

Batch 196690, train_perplexity=5315.2373, train_loss=8.578333

Batch 196700, train_perplexity=5431.9517, train_loss=8.600054

Batch 196710, train_perplexity=5351.1655, train_loss=8.58507

Batch 196720, train_perplexity=4827.059, train_loss=8.481993

Batch 196730, train_perplexity=6848.6978, train_loss=8.831814

Batch 196740, train_perplexity=4980.591, train_loss=8.513304

Batch 196750, train_perplexity=4444.2925, train_loss=8.399376

Batch 196760, train_perplexity=6032.087, train_loss=8.704848

Batch 196770, train_perplexity=5024.8364, train_loss=8.522148

Batch 196780, train_perplexity=5877.251, train_loss=8.678844

Batch 196790, train_perplexity=5984.791, train_loss=8.696977

Batch 196800, train_perplexity=4949.463, train_loss=8.507034

Batch 196810, train_perplexity=5263.352, train_loss=8.568523

Batch 196820, train_perplexity=3840.687, train_loss=8.253407

Batch 196830, train_perplexity=5854.757, train_loss=8.67501

Batch 196840, train_perplexity=5975.153, train_loss=8.695365

Batch 196850, train_perplexity=6390.876, train_loss=8.762627

Batch 196860, train_perplexity=5217.101, train_loss=8.559697

Batch 196870, train_perplexity=5275.101, train_loss=8.570753

Batch 196880, train_perplexity=6966.574, train_loss=8.848879

Batch 196890, train_perplexity=4932.1274, train_loss=8.503526

Batch 196900, train_perplexity=4500.2236, train_loss=8.411882

Batch 196910, train_perplexity=4562.1206, train_loss=8.425543

Batch 196920, train_perplexity=5466.208, train_loss=8.60634

Batch 196930, train_perplexity=5102.3306, train_loss=8.537453

Batch 196940, train_perplexity=5252.3257, train_loss=8.566426

Batch 196950, train_perplexity=5526.2344, train_loss=8.617262

Batch 196960, train_perplexity=7075.5396, train_loss=8.864399

Batch 196970, train_perplexity=4852.135, train_loss=8.487174

Batch 196980, train_perplexity=5985.002, train_loss=8.697012

Batch 196990, train_perplexity=4892.467, train_loss=8.495452

Batch 197000, train_perplexity=4989.823, train_loss=8.515156

Batch 197010, train_perplexity=4751.688, train_loss=8.466255

Batch 197020, train_perplexity=4567.157, train_loss=8.426646

Batch 197030, train_perplexity=4968.9814, train_loss=8.51097

Batch 197040, train_perplexity=5166.8193, train_loss=8.550013

Batch 197050, train_perplexity=4640.491, train_loss=8.442575

Batch 197060, train_perplexity=5069.736, train_loss=8.531044

Batch 197070, train_perplexity=4906.1895, train_loss=8.498253

Batch 197080, train_perplexity=5771.567, train_loss=8.660699

Batch 197090, train_perplexity=5352.258, train_loss=8.585274

Batch 197100, train_perplexity=5550.742, train_loss=8.621687

Batch 197110, train_perplexity=4637.6685, train_loss=8.441967

Batch 197120, train_perplexity=5299.405, train_loss=8.57535

Batch 197130, train_perplexity=4807.4473, train_loss=8.4779215

Batch 197140, train_perplexity=6253.0864, train_loss=8.74083

Batch 197150, train_perplexity=4493.654, train_loss=8.410421

Batch 197160, train_perplexity=5219.445, train_loss=8.560146

Batch 197170, train_perplexity=4615.774, train_loss=8.437235

Batch 197180, train_perplexity=4836.464, train_loss=8.483939

Batch 197190, train_perplexity=5130.866, train_loss=8.54303

Batch 197200, train_perplexity=4734.978, train_loss=8.462732

Batch 197210, train_perplexity=5748.0625, train_loss=8.656618

Batch 197220, train_perplexity=6161.327, train_loss=8.7260475

Batch 197230, train_perplexity=5217.4146, train_loss=8.559757

Batch 197240, train_perplexity=5277.003, train_loss=8.571114

Batch 197250, train_perplexity=5650.5884, train_loss=8.639515

Batch 197260, train_perplexity=5818.504, train_loss=8.668798

Batch 197270, train_perplexity=4464.8535, train_loss=8.403992

Batch 197280, train_perplexity=5303.207, train_loss=8.576067

Batch 197290, train_perplexity=6440.718, train_loss=8.770395

Batch 197300, train_perplexity=5625.58, train_loss=8.635079

Batch 197310, train_perplexity=6595.8574, train_loss=8.794197

Batch 197320, train_perplexity=5572.7544, train_loss=8.625645

Batch 197330, train_perplexity=5941.2925, train_loss=8.689682

Batch 197340, train_perplexity=5667.167, train_loss=8.642445

Batch 197350, train_perplexity=5448.31, train_loss=8.603061

Batch 197360, train_perplexity=5239.8433, train_loss=8.564047

Batch 197370, train_perplexity=4873.918, train_loss=8.491653

Batch 197380, train_perplexity=5038.5938, train_loss=8.524882

Batch 197390, train_perplexity=5301.0527, train_loss=8.575661

Batch 197400, train_perplexity=5365.5557, train_loss=8.587755

Batch 197410, train_perplexity=6157.891, train_loss=8.72549

Batch 197420, train_perplexity=4433.878, train_loss=8.39703

Batch 197430, train_perplexity=5096.009, train_loss=8.536213

Batch 197440, train_perplexity=4692.6265, train_loss=8.453748
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 197450, train_perplexity=4823.295, train_loss=8.481213

Batch 197460, train_perplexity=4506.52, train_loss=8.4132805

Batch 197470, train_perplexity=5439.6553, train_loss=8.601471

Batch 197480, train_perplexity=5073.649, train_loss=8.531816

Batch 197490, train_perplexity=5845.5566, train_loss=8.673437

Batch 197500, train_perplexity=5843.4556, train_loss=8.673078

Batch 197510, train_perplexity=5548.403, train_loss=8.621265

Batch 197520, train_perplexity=5480.1235, train_loss=8.608883

Batch 197530, train_perplexity=5095.892, train_loss=8.53619

Batch 197540, train_perplexity=4777.6885, train_loss=8.471712

Batch 197550, train_perplexity=5246.2437, train_loss=8.565268

Batch 197560, train_perplexity=5214.3555, train_loss=8.559171

Batch 197570, train_perplexity=5461.4556, train_loss=8.605471

Batch 197580, train_perplexity=4797.1333, train_loss=8.475774

Batch 197590, train_perplexity=6096.3325, train_loss=8.715443

Batch 197600, train_perplexity=5840.213, train_loss=8.672523

Batch 197610, train_perplexity=4696.4185, train_loss=8.4545555

Batch 197620, train_perplexity=6626.2466, train_loss=8.798794

Batch 197630, train_perplexity=5057.8174, train_loss=8.52869

Batch 197640, train_perplexity=5160.526, train_loss=8.548794

Batch 197650, train_perplexity=6038.5967, train_loss=8.705927

Batch 197660, train_perplexity=5954.2026, train_loss=8.691853

Batch 197670, train_perplexity=4524.9346, train_loss=8.417358

Batch 197680, train_perplexity=4680.7954, train_loss=8.451223

Batch 197690, train_perplexity=5471.825, train_loss=8.6073675

Batch 197700, train_perplexity=5021.3965, train_loss=8.521463

Batch 197710, train_perplexity=4924.969, train_loss=8.502073

Batch 197720, train_perplexity=5628.564, train_loss=8.63561

Batch 197730, train_perplexity=4987.426, train_loss=8.514675

Batch 197740, train_perplexity=4524.1147, train_loss=8.417177

Batch 197750, train_perplexity=4996.4517, train_loss=8.516483

Batch 197760, train_perplexity=5155.6953, train_loss=8.547857

Batch 197770, train_perplexity=5580.684, train_loss=8.627067

Batch 197780, train_perplexity=4607.431, train_loss=8.435426

Batch 197790, train_perplexity=5790.7314, train_loss=8.664014

Batch 197800, train_perplexity=5022.45, train_loss=8.521673

Batch 197810, train_perplexity=6043.4243, train_loss=8.706726

Batch 197820, train_perplexity=4948.4053, train_loss=8.506821

Batch 197830, train_perplexity=6838.7383, train_loss=8.8303585

Batch 197840, train_perplexity=5974.0303, train_loss=8.695177

Batch 197850, train_perplexity=6081.5713, train_loss=8.713018

Batch 197860, train_perplexity=5070.51, train_loss=8.531197

Batch 197870, train_perplexity=5867.7583, train_loss=8.677228

Batch 197880, train_perplexity=5074.8296, train_loss=8.532048

Batch 197890, train_perplexity=5942.29, train_loss=8.68985

Batch 197900, train_perplexity=5532.045, train_loss=8.618313

Batch 197910, train_perplexity=5738.357, train_loss=8.654928

Batch 197920, train_perplexity=5425.8164, train_loss=8.598924

Batch 197930, train_perplexity=4844.6167, train_loss=8.485623

Batch 197940, train_perplexity=5014.118, train_loss=8.520013

Batch 197950, train_perplexity=5715.522, train_loss=8.650941

Batch 197960, train_perplexity=4840.52, train_loss=8.484777

Batch 197970, train_perplexity=5036.2925, train_loss=8.5244255

Batch 197980, train_perplexity=4916.4985, train_loss=8.500352

Batch 197990, train_perplexity=4428.0635, train_loss=8.395718

Batch 198000, train_perplexity=5079.919, train_loss=8.533051

Batch 198010, train_perplexity=5607.497, train_loss=8.63186

Batch 198020, train_perplexity=5649.2305, train_loss=8.639275

Batch 198030, train_perplexity=5522.3516, train_loss=8.616559

Batch 198040, train_perplexity=5262.9507, train_loss=8.568447

Batch 198050, train_perplexity=5780.5073, train_loss=8.662247

Batch 198060, train_perplexity=5358.647, train_loss=8.586467

Batch 198070, train_perplexity=4928.634, train_loss=8.502817

Batch 198080, train_perplexity=6856.8145, train_loss=8.832998

Batch 198090, train_perplexity=4787.5405, train_loss=8.473772

Batch 198100, train_perplexity=5608.583, train_loss=8.632053

Batch 198110, train_perplexity=5742.885, train_loss=8.655717

Batch 198120, train_perplexity=5726.2207, train_loss=8.652811

Batch 198130, train_perplexity=5697.644, train_loss=8.647808

Batch 198140, train_perplexity=5146.617, train_loss=8.546095

Batch 198150, train_perplexity=4642.988, train_loss=8.443113

Batch 198160, train_perplexity=4812.5894, train_loss=8.478991

Batch 198170, train_perplexity=4561.5117, train_loss=8.425409

Batch 198180, train_perplexity=5413.34, train_loss=8.5966215

Batch 198190, train_perplexity=5421.6216, train_loss=8.59815

Batch 198200, train_perplexity=5640.5737, train_loss=8.637741

Batch 198210, train_perplexity=4954.3457, train_loss=8.50802

Batch 198220, train_perplexity=4512.188, train_loss=8.414537

Batch 198230, train_perplexity=4810.341, train_loss=8.478523

Batch 198240, train_perplexity=5802.2905, train_loss=8.666008

Batch 198250, train_perplexity=5547.456, train_loss=8.621095

Batch 198260, train_perplexity=4666.871, train_loss=8.448244

Batch 198270, train_perplexity=4950.9214, train_loss=8.507329

Batch 198280, train_perplexity=4985.7993, train_loss=8.514349

Batch 198290, train_perplexity=5653.1216, train_loss=8.639963

Batch 198300, train_perplexity=5213.4604, train_loss=8.558999

Batch 198310, train_perplexity=5113.8223, train_loss=8.539702

Batch 198320, train_perplexity=4379.193, train_loss=8.38462

Batch 198330, train_perplexity=5843.2046, train_loss=8.673035

Batch 198340, train_perplexity=5511.7495, train_loss=8.614637

Batch 198350, train_perplexity=5313.595, train_loss=8.578024

Batch 198360, train_perplexity=4772.479, train_loss=8.470621

Batch 198370, train_perplexity=4859.8545, train_loss=8.488764

Batch 198380, train_perplexity=6145.3657, train_loss=8.7234535

Batch 198390, train_perplexity=5559.6533, train_loss=8.623291

Batch 198400, train_perplexity=4991.356, train_loss=8.515463

Batch 198410, train_perplexity=5396.078, train_loss=8.593428

Batch 198420, train_perplexity=4950.9873, train_loss=8.507342

Batch 198430, train_perplexity=5994.0728, train_loss=8.698526

Batch 198440, train_perplexity=5419.528, train_loss=8.597764

Batch 198450, train_perplexity=5071.5737, train_loss=8.531406

Batch 198460, train_perplexity=4607.9453, train_loss=8.435537

Batch 198470, train_perplexity=4765.593, train_loss=8.469177

Batch 198480, train_perplexity=5065.8696, train_loss=8.530281

Batch 198490, train_perplexity=5314.3706, train_loss=8.57817

Batch 198500, train_perplexity=5890.286, train_loss=8.68106

Batch 198510, train_perplexity=6668.022, train_loss=8.8050785

Batch 198520, train_perplexity=4933.906, train_loss=8.503886

Batch 198530, train_perplexity=5294.717, train_loss=8.574465

Batch 198540, train_perplexity=6870.05, train_loss=8.834927

Batch 198550, train_perplexity=5049.3496, train_loss=8.527015

Batch 198560, train_perplexity=4945.037, train_loss=8.50614

Batch 198570, train_perplexity=5355.0864, train_loss=8.585802

Batch 198580, train_perplexity=6121.092, train_loss=8.719496

Batch 198590, train_perplexity=5907.6753, train_loss=8.684008

Batch 198600, train_perplexity=6203.815, train_loss=8.73292

Batch 198610, train_perplexity=4859.6367, train_loss=8.488719

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Batch 198620, train_perplexity=5632.9727, train_loss=8.636393

Batch 198630, train_perplexity=5727.7446, train_loss=8.653077

Batch 198640, train_perplexity=5239.5034, train_loss=8.563982

Batch 198650, train_perplexity=5565.4517, train_loss=8.624333

Batch 198660, train_perplexity=5462.946, train_loss=8.605743

Batch 198670, train_perplexity=4988.824, train_loss=8.5149555

Batch 198680, train_perplexity=5081.634, train_loss=8.533388

Batch 198690, train_perplexity=5169.8755, train_loss=8.550604
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 198700, train_perplexity=5354.9688, train_loss=8.58578

Batch 198710, train_perplexity=4617.5, train_loss=8.437609

Batch 198720, train_perplexity=5765.9277, train_loss=8.659721

Batch 198730, train_perplexity=6235.287, train_loss=8.73798

Batch 198740, train_perplexity=5311.7915, train_loss=8.577684

Batch 198750, train_perplexity=4525.336, train_loss=8.417447

Batch 198760, train_perplexity=5730.8755, train_loss=8.653624

Batch 198770, train_perplexity=5579.8164, train_loss=8.626911

Batch 198780, train_perplexity=7255.125, train_loss=8.889463

Batch 198790, train_perplexity=5270.334, train_loss=8.569849

Batch 198800, train_perplexity=5013.219, train_loss=8.519834

Batch 198810, train_perplexity=5632.656, train_loss=8.636336

Batch 198820, train_perplexity=5878.7256, train_loss=8.679095

Batch 198830, train_perplexity=5708.391, train_loss=8.649693

Batch 198840, train_perplexity=6510.5576, train_loss=8.78118

Batch 198850, train_perplexity=5515.9985, train_loss=8.615408

Batch 198860, train_perplexity=5461.9087, train_loss=8.605554

Batch 198870, train_perplexity=4762.935, train_loss=8.468619

Batch 198880, train_perplexity=5793.007, train_loss=8.664407

Batch 198890, train_perplexity=5177.681, train_loss=8.552113

Batch 198900, train_perplexity=5439.9614, train_loss=8.601527

Batch 198910, train_perplexity=5694.4937, train_loss=8.647255

Batch 198920, train_perplexity=5614.9946, train_loss=8.633196

Batch 198930, train_perplexity=5677.391, train_loss=8.644247

Batch 198940, train_perplexity=4643.559, train_loss=8.443236

Batch 198950, train_perplexity=6483.8765, train_loss=8.777074

Batch 198960, train_perplexity=5091.86, train_loss=8.5353985

Batch 198970, train_perplexity=4243.1846, train_loss=8.353069

Batch 198980, train_perplexity=5874.326, train_loss=8.678347

Batch 198990, train_perplexity=5781.8965, train_loss=8.662487

Batch 199000, train_perplexity=5735.5176, train_loss=8.654433

Batch 199010, train_perplexity=5517.324, train_loss=8.615648

Batch 199020, train_perplexity=5306.323, train_loss=8.576654

Batch 199030, train_perplexity=6402.589, train_loss=8.764458

Batch 199040, train_perplexity=5264.2104, train_loss=8.5686865

Batch 199050, train_perplexity=5320.014, train_loss=8.579231

Batch 199060, train_perplexity=4837.2114, train_loss=8.484094

Batch 199070, train_perplexity=6011.3613, train_loss=8.7014065

Batch 199080, train_perplexity=5750.475, train_loss=8.657038

Batch 199090, train_perplexity=5472.8584, train_loss=8.607556

Batch 199100, train_perplexity=4921.8843, train_loss=8.501447

Batch 199110, train_perplexity=4959.546, train_loss=8.509069

Batch 199120, train_perplexity=5108.7676, train_loss=8.538713

Batch 199130, train_perplexity=5654.696, train_loss=8.640242

Batch 199140, train_perplexity=5771.358, train_loss=8.660663

Batch 199150, train_perplexity=5979.5195, train_loss=8.696095

Batch 199160, train_perplexity=5488.743, train_loss=8.610455

Batch 199170, train_perplexity=3932.3286, train_loss=8.276987

Batch 199180, train_perplexity=5905.8613, train_loss=8.683701

Batch 199190, train_perplexity=4390.2153, train_loss=8.387134

Batch 199200, train_perplexity=4373.7837, train_loss=8.383384

Batch 199210, train_perplexity=5513.4316, train_loss=8.614943

Batch 199220, train_perplexity=5561.074, train_loss=8.623547

Batch 199230, train_perplexity=6076.7246, train_loss=8.712221

Batch 199240, train_perplexity=5401.654, train_loss=8.5944605

Batch 199250, train_perplexity=5007.1177, train_loss=8.518616

Batch 199260, train_perplexity=4413.8047, train_loss=8.392492

Batch 199270, train_perplexity=5019.472, train_loss=8.52108

Batch 199280, train_perplexity=4977.6562, train_loss=8.512714

Batch 199290, train_perplexity=4719.717, train_loss=8.459504

Batch 199300, train_perplexity=5227.0015, train_loss=8.561593

Batch 199310, train_perplexity=5061.721, train_loss=8.529462

Batch 199320, train_perplexity=5191.2188, train_loss=8.554724

Batch 199330, train_perplexity=6604.254, train_loss=8.795469

Batch 199340, train_perplexity=5218.972, train_loss=8.560056

Batch 199350, train_perplexity=5602.5103, train_loss=8.63097

Batch 199360, train_perplexity=5360.6045, train_loss=8.586832

Batch 199370, train_perplexity=4900.0825, train_loss=8.497007

Batch 199380, train_perplexity=5249.6665, train_loss=8.56592

Batch 199390, train_perplexity=6206.7207, train_loss=8.733388

Batch 199400, train_perplexity=4813.5854, train_loss=8.4791975

Batch 199410, train_perplexity=5184.287, train_loss=8.553388

Batch 199420, train_perplexity=5141.358, train_loss=8.545073

Batch 199430, train_perplexity=4401.5723, train_loss=8.389717

Batch 199440, train_perplexity=4473.8813, train_loss=8.406012

Batch 199450, train_perplexity=5526.925, train_loss=8.617387

Batch 199460, train_perplexity=5376.4556, train_loss=8.589785

Batch 199470, train_perplexity=5415.1626, train_loss=8.596958

Batch 199480, train_perplexity=5527.879, train_loss=8.617559

Batch 199490, train_perplexity=5461.7476, train_loss=8.605524

Batch 199500, train_perplexity=5791.963, train_loss=8.664227

Batch 199510, train_perplexity=5408.1797, train_loss=8.595668

Batch 199520, train_perplexity=5489.9053, train_loss=8.610666

Batch 199530, train_perplexity=5667.9937, train_loss=8.6425905

Batch 199540, train_perplexity=5816.9946, train_loss=8.668539

Batch 199550, train_perplexity=5378.948, train_loss=8.590248

Batch 199560, train_perplexity=6134.1997, train_loss=8.721635

Batch 199570, train_perplexity=5637.3257, train_loss=8.637165

Batch 199580, train_perplexity=5001.181, train_loss=8.517429

Batch 199590, train_perplexity=6039.1323, train_loss=8.706016

Batch 199600, train_perplexity=5259.82, train_loss=8.567852

Batch 199610, train_perplexity=6537.5415, train_loss=8.785316

Batch 199620, train_perplexity=5754.6006, train_loss=8.657755

Batch 199630, train_perplexity=5619.0337, train_loss=8.633915

Batch 199640, train_perplexity=5840.7197, train_loss=8.672609

Batch 199650, train_perplexity=4527.1016, train_loss=8.417837

Batch 199660, train_perplexity=5206.618, train_loss=8.557686

Batch 199670, train_perplexity=5452.359, train_loss=8.603804

Batch 199680, train_perplexity=5334.178, train_loss=8.58189

Batch 199690, train_perplexity=6357.1455, train_loss=8.757335

Batch 199700, train_perplexity=6168.9004, train_loss=8.727276

Batch 199710, train_perplexity=5115.7344, train_loss=8.540076

Batch 199720, train_perplexity=4604.8, train_loss=8.4348545

Batch 199730, train_perplexity=4864.436, train_loss=8.489706

Batch 199740, train_perplexity=5385.323, train_loss=8.591433

Batch 199750, train_perplexity=5201.08, train_loss=8.556622

Batch 199760, train_perplexity=5673.797, train_loss=8.643614

Batch 199770, train_perplexity=4999.922, train_loss=8.517178

Batch 199780, train_perplexity=5795.024, train_loss=8.664755

Batch 199790, train_perplexity=4862.0522, train_loss=8.489216

Batch 199800, train_perplexity=6320.09, train_loss=8.751489

Batch 199810, train_perplexity=5266.4253, train_loss=8.569107

Batch 199820, train_perplexity=5460.81, train_loss=8.605352

Batch 199830, train_perplexity=5245.123, train_loss=8.565054

Batch 199840, train_perplexity=4912.6416, train_loss=8.499567

Batch 199850, train_perplexity=5059.742, train_loss=8.529071

Batch 199860, train_perplexity=5261.5454, train_loss=8.56818

Batch 199870, train_perplexity=5975.375, train_loss=8.695402

Batch 199880, train_perplexity=5607.904, train_loss=8.631932

Batch 199890, train_perplexity=5064.9272, train_loss=8.530095

Batch 199900, train_perplexity=5809.743, train_loss=8.667292

Batch 199910, train_perplexity=5992.6094, train_loss=8.698282

Batch 199920, train_perplexity=5608.4707, train_loss=8.632033

Batch 199930, train_perplexity=5177.2764, train_loss=8.552034

Batch 199940, train_perplexity=5603.0977, train_loss=8.631075

Batch 199950, train_perplexity=5118.189, train_loss=8.540556

Batch 199960, train_perplexity=5120.6597, train_loss=8.5410385

Batch 199970, train_perplexity=6041.546, train_loss=8.706415

Batch 199980, train_perplexity=5428.4614, train_loss=8.599411

Batch 199990, train_perplexity=5005.0933, train_loss=8.518211

Batch 200000, train_perplexity=5183.2837, train_loss=8.553194

Batch 200010, train_perplexity=5740.021, train_loss=8.655218
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 200020, train_perplexity=5496.684, train_loss=8.6119

Batch 200030, train_perplexity=5324.369, train_loss=8.5800495

Batch 200040, train_perplexity=4805.151, train_loss=8.477444

Batch 200050, train_perplexity=5097.6714, train_loss=8.536539

Batch 200060, train_perplexity=4874.8105, train_loss=8.491837

Batch 200070, train_perplexity=6130.7314, train_loss=8.721069

Batch 200080, train_perplexity=5494.63, train_loss=8.6115265

Batch 200090, train_perplexity=6109.8706, train_loss=8.717661

Batch 200100, train_perplexity=4531.9136, train_loss=8.4189

Batch 200110, train_perplexity=6248.109, train_loss=8.740034

Batch 200120, train_perplexity=5672.017, train_loss=8.6433

Batch 200130, train_perplexity=5120.0054, train_loss=8.540911

Batch 200140, train_perplexity=5510.2095, train_loss=8.614358

Batch 200150, train_perplexity=5664.076, train_loss=8.641899

Batch 200160, train_perplexity=5735.9385, train_loss=8.654507

Batch 200170, train_perplexity=5408.5044, train_loss=8.595728

Batch 200180, train_perplexity=4681.407, train_loss=8.451354

Batch 200190, train_perplexity=5706.655, train_loss=8.649388

Batch 200200, train_perplexity=4670.758, train_loss=8.449077

Batch 200210, train_perplexity=4614.7354, train_loss=8.43701

Batch 200220, train_perplexity=5953.9414, train_loss=8.691809

Batch 200230, train_perplexity=4931.2295, train_loss=8.503344

Batch 200240, train_perplexity=5224.33, train_loss=8.561082

Batch 200250, train_perplexity=6414.8857, train_loss=8.7663765

Batch 200260, train_perplexity=5233.331, train_loss=8.562803

Batch 200270, train_perplexity=5798.949, train_loss=8.665432

Batch 200280, train_perplexity=5871.117, train_loss=8.6778

Batch 200290, train_perplexity=4583.3843, train_loss=8.430193

Batch 200300, train_perplexity=5300.0317, train_loss=8.575468

Batch 200310, train_perplexity=5594.256, train_loss=8.629496

Batch 200320, train_perplexity=4305.6665, train_loss=8.367687

Batch 200330, train_perplexity=4714.2197, train_loss=8.458339

Batch 200340, train_perplexity=5528.2583, train_loss=8.617628

Batch 200350, train_perplexity=5087.084, train_loss=8.53446

Batch 200360, train_perplexity=4748.78, train_loss=8.465643

Batch 200370, train_perplexity=4606.8643, train_loss=8.435303

Batch 200380, train_perplexity=6183.897, train_loss=8.729704

Batch 200390, train_perplexity=6451.1865, train_loss=8.772019

Batch 200400, train_perplexity=6275.6924, train_loss=8.744439

Batch 200410, train_perplexity=4998.487, train_loss=8.516891

Batch 200420, train_perplexity=5130.568, train_loss=8.542972

Batch 200430, train_perplexity=6066.5166, train_loss=8.71054

Batch 200440, train_perplexity=4935.812, train_loss=8.504272

Batch 200450, train_perplexity=5064.594, train_loss=8.530029

Batch 200460, train_perplexity=5300.0215, train_loss=8.575466

Batch 200470, train_perplexity=5058.6763, train_loss=8.52886

Batch 200480, train_perplexity=5726.385, train_loss=8.65284

Batch 200490, train_perplexity=4526.9717, train_loss=8.417809

Batch 200500, train_perplexity=6081.0146, train_loss=8.712927

Batch 200510, train_perplexity=4487.076, train_loss=8.408957

Batch 200520, train_perplexity=5441.393, train_loss=8.60179

Batch 200530, train_perplexity=5748.9614, train_loss=8.6567745

Batch 200540, train_perplexity=4935.2563, train_loss=8.50416

Batch 200550, train_perplexity=5609.316, train_loss=8.632184

Batch 200560, train_perplexity=5229.4, train_loss=8.562052

Batch 200570, train_perplexity=4767.243, train_loss=8.469523

Batch 200580, train_perplexity=5468.846, train_loss=8.606823

Batch 200590, train_perplexity=5302.185, train_loss=8.575874

Batch 200600, train_perplexity=5836.0093, train_loss=8.6718025

Batch 200610, train_perplexity=5258.215, train_loss=8.567547

Batch 200620, train_perplexity=6045.3784, train_loss=8.707049

Batch 200630, train_perplexity=5377.476, train_loss=8.589974

Batch 200640, train_perplexity=6602.4595, train_loss=8.7951975

Batch 200650, train_perplexity=4902.013, train_loss=8.497401

Batch 200660, train_perplexity=5033.642, train_loss=8.523899

Batch 200670, train_perplexity=4903.3735, train_loss=8.497679

Batch 200680, train_perplexity=5962.3965, train_loss=8.693228

Batch 200690, train_perplexity=5248.235, train_loss=8.565647

Batch 200700, train_perplexity=5239.0737, train_loss=8.5639

Batch 200710, train_perplexity=5976.469, train_loss=8.695585

Batch 200720, train_perplexity=4753.4736, train_loss=8.466631

Batch 200730, train_perplexity=5164.859, train_loss=8.549633

Batch 200740, train_perplexity=5273.954, train_loss=8.570536

Batch 200750, train_perplexity=5569.5347, train_loss=8.625067

Batch 200760, train_perplexity=5686.6084, train_loss=8.645869

Batch 200770, train_perplexity=5881.0356, train_loss=8.679488

Batch 200780, train_perplexity=4405.7803, train_loss=8.390673

Batch 200790, train_perplexity=5043.089, train_loss=8.525774

Batch 200800, train_perplexity=5119.4683, train_loss=8.540806

Batch 200810, train_perplexity=5556.9497, train_loss=8.622805

Batch 200820, train_perplexity=4744.5786, train_loss=8.464758

Batch 200830, train_perplexity=6415.963, train_loss=8.766544

Batch 200840, train_perplexity=5835.9316, train_loss=8.671789

Batch 200850, train_perplexity=5408.319, train_loss=8.595694

Batch 200860, train_perplexity=6037.065, train_loss=8.705673

Batch 200870, train_perplexity=4436.3564, train_loss=8.397589

Batch 200880, train_perplexity=4853.0, train_loss=8.487352

Batch 200890, train_perplexity=5633.177, train_loss=8.636429

Batch 200900, train_perplexity=5919.9077, train_loss=8.686076

Batch 200910, train_perplexity=5722.323, train_loss=8.65213

Batch 200920, train_perplexity=5061.4653, train_loss=8.529411

Batch 200930, train_perplexity=5549.959, train_loss=8.621546

Batch 200940, train_perplexity=4734.3096, train_loss=8.462591

Batch 200950, train_perplexity=5717.8335, train_loss=8.651345

Batch 200960, train_perplexity=6324.865, train_loss=8.752244

Batch 200970, train_perplexity=5345.1113, train_loss=8.583938

Batch 200980, train_perplexity=5246.7036, train_loss=8.565355

Batch 200990, train_perplexity=5340.7344, train_loss=8.583118

Batch 201000, train_perplexity=5049.114, train_loss=8.526968

Batch 201010, train_perplexity=6806.076, train_loss=8.825571

Batch 201020, train_perplexity=5758.9595, train_loss=8.658512

Batch 201030, train_perplexity=5985.2646, train_loss=8.697056

Batch 201040, train_perplexity=6545.5396, train_loss=8.786539

Batch 201050, train_perplexity=5309.431, train_loss=8.57724

Batch 201060, train_perplexity=6243.2007, train_loss=8.739248

Batch 201070, train_perplexity=6049.681, train_loss=8.707761

Batch 201080, train_perplexity=5346.87, train_loss=8.584267

Batch 201090, train_perplexity=5658.019, train_loss=8.640829

Batch 201100, train_perplexity=5705.235, train_loss=8.649139

Batch 201110, train_perplexity=4890.102, train_loss=8.494968

Batch 201120, train_perplexity=4930.7026, train_loss=8.503237

Batch 201130, train_perplexity=6407.8057, train_loss=8.765272

Batch 201140, train_perplexity=6374.806, train_loss=8.760109

Batch 201150, train_perplexity=5682.7373, train_loss=8.645188

Batch 201160, train_perplexity=4398.207, train_loss=8.388952

Batch 201170, train_perplexity=6217.5205, train_loss=8.7351265

Batch 201180, train_perplexity=4886.8247, train_loss=8.494298

Batch 201190, train_perplexity=4633.5835, train_loss=8.441086

Batch 201200, train_perplexity=5095.547, train_loss=8.536122

Batch 201210, train_perplexity=4467.571, train_loss=8.4046

Batch 201220, train_perplexity=5470.9487, train_loss=8.607207

Batch 201230, train_perplexity=6004.016, train_loss=8.700184

Batch 201240, train_perplexity=4670.531, train_loss=8.449028

Batch 201250, train_perplexity=5354.9126, train_loss=8.58577

Batch 201260, train_perplexity=5316.2, train_loss=8.578514

Batch 201270, train_perplexity=5792.1284, train_loss=8.664255

Batch 201280, train_perplexity=5362.844, train_loss=8.58725

Batch 201290, train_perplexity=5794.4214, train_loss=8.664651

Batch 201300, train_perplexity=5818.1265, train_loss=8.668734

Batch 201310, train_perplexity=5052.8564, train_loss=8.527709

Batch 201320, train_perplexity=5625.693, train_loss=8.635099

Batch 201330, train_perplexity=5656.8535, train_loss=8.640623
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 201340, train_perplexity=5646.494, train_loss=8.63879

Batch 201350, train_perplexity=3880.079, train_loss=8.263611

Batch 201360, train_perplexity=5267.731, train_loss=8.569355

Batch 201370, train_perplexity=5449.9053, train_loss=8.6033535

Batch 201380, train_perplexity=4857.306, train_loss=8.488239

Batch 201390, train_perplexity=5991.3696, train_loss=8.698075

Batch 201400, train_perplexity=6021.1265, train_loss=8.70303

Batch 201410, train_perplexity=5385.77, train_loss=8.591516

Batch 201420, train_perplexity=4948.802, train_loss=8.506901

Batch 201430, train_perplexity=5250.543, train_loss=8.566087

Batch 201440, train_perplexity=4775.6294, train_loss=8.471281

Batch 201450, train_perplexity=5648.4224, train_loss=8.639132

Batch 201460, train_perplexity=5024.338, train_loss=8.522049

Batch 201470, train_perplexity=5537.2334, train_loss=8.61925

Batch 201480, train_perplexity=5013.1763, train_loss=8.519825

Batch 201490, train_perplexity=5293.495, train_loss=8.574234

Batch 201500, train_perplexity=5347.8594, train_loss=8.584452

Batch 201510, train_perplexity=5977.0166, train_loss=8.695677

Batch 201520, train_perplexity=5031.386, train_loss=8.523451

Batch 201530, train_perplexity=5497.649, train_loss=8.612076

Batch 201540, train_perplexity=5267.56, train_loss=8.569323

Batch 201550, train_perplexity=5398.43, train_loss=8.5938635

Batch 201560, train_perplexity=5492.534, train_loss=8.611145

Batch 201570, train_perplexity=6216.643, train_loss=8.734985

Batch 201580, train_perplexity=4861.176, train_loss=8.489036

Batch 201590, train_perplexity=5406.21, train_loss=8.595304

Batch 201600, train_perplexity=4178.894, train_loss=8.337802

Batch 201610, train_perplexity=6550.31, train_loss=8.787268

Batch 201620, train_perplexity=5385.518, train_loss=8.591469

Batch 201630, train_perplexity=5216.7974, train_loss=8.559639

Batch 201640, train_perplexity=5468.1685, train_loss=8.606699

Batch 201650, train_perplexity=5120.4253, train_loss=8.540993

Batch 201660, train_perplexity=5368.4014, train_loss=8.588285

Batch 201670, train_perplexity=5385.5283, train_loss=8.591471

Batch 201680, train_perplexity=6259.214, train_loss=8.74181

Batch 201690, train_perplexity=6195.7974, train_loss=8.7316265

Batch 201700, train_perplexity=5545.9907, train_loss=8.620831

Batch 201710, train_perplexity=5451.4126, train_loss=8.60363

Batch 201720, train_perplexity=6234.032, train_loss=8.737779

Batch 201730, train_perplexity=5027.971, train_loss=8.522772

Batch 201740, train_perplexity=4743.674, train_loss=8.464567

Batch 201750, train_perplexity=5306.5054, train_loss=8.576689

Batch 201760, train_perplexity=6525.9307, train_loss=8.783539

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Batch 201770, train_perplexity=6162.6436, train_loss=8.726261

Batch 201780, train_perplexity=6022.3955, train_loss=8.70324

Batch 201790, train_perplexity=4781.5996, train_loss=8.47253

Batch 201800, train_perplexity=5080.994, train_loss=8.533262

Batch 201810, train_perplexity=5767.985, train_loss=8.660078

Batch 201820, train_perplexity=4575.667, train_loss=8.428508

Batch 201830, train_perplexity=5846.4155, train_loss=8.673584

Batch 201840, train_perplexity=5505.8867, train_loss=8.613573

Batch 201850, train_perplexity=4621.725, train_loss=8.438523

Batch 201860, train_perplexity=6614.4526, train_loss=8.797012

Batch 201870, train_perplexity=5921.777, train_loss=8.686392

Batch 201880, train_perplexity=5537.3604, train_loss=8.619273

Batch 201890, train_perplexity=5086.3516, train_loss=8.534316

Batch 201900, train_perplexity=5100.7886, train_loss=8.53715

Batch 201910, train_perplexity=5280.9097, train_loss=8.571854

Batch 201920, train_perplexity=5558.6406, train_loss=8.623109

Batch 201930, train_perplexity=4532.017, train_loss=8.418922

Batch 201940, train_perplexity=6119.3936, train_loss=8.719218

Batch 201950, train_perplexity=5504.28, train_loss=8.613281

Batch 201960, train_perplexity=4897.541, train_loss=8.496489

Batch 201970, train_perplexity=4339.726, train_loss=8.3755665

Batch 201980, train_perplexity=4993.3604, train_loss=8.515864

Batch 201990, train_perplexity=4703.599, train_loss=8.456083

Batch 202000, train_perplexity=5011.417, train_loss=8.519474

Batch 202010, train_perplexity=5446.4707, train_loss=8.602723

Batch 202020, train_perplexity=4980.833, train_loss=8.513352

Batch 202030, train_perplexity=4810.63, train_loss=8.478583

Batch 202040, train_perplexity=5505.9023, train_loss=8.613576

Batch 202050, train_perplexity=5215.007, train_loss=8.559296

Batch 202060, train_perplexity=4779.288, train_loss=8.472047

Batch 202070, train_perplexity=4587.915, train_loss=8.431181

Batch 202080, train_perplexity=5273.4663, train_loss=8.570443

Batch 202090, train_perplexity=7043.654, train_loss=8.859882

Batch 202100, train_perplexity=5050.0864, train_loss=8.527161

Batch 202110, train_perplexity=6058.578, train_loss=8.70923

Batch 202120, train_perplexity=6034.037, train_loss=8.705172

Batch 202130, train_perplexity=5325.781, train_loss=8.580315

Batch 202140, train_perplexity=5549.8105, train_loss=8.621519

Batch 202150, train_perplexity=5277.4204, train_loss=8.571193

Batch 202160, train_perplexity=6357.515, train_loss=8.757393

Batch 202170, train_perplexity=4991.6846, train_loss=8.515529

Batch 202180, train_perplexity=6061.3115, train_loss=8.7096815

Batch 202190, train_perplexity=6294.7837, train_loss=8.747477

Batch 202200, train_perplexity=5719.8296, train_loss=8.651694

Batch 202210, train_perplexity=4930.458, train_loss=8.503187

Batch 202220, train_perplexity=7052.9033, train_loss=8.861195

Batch 202230, train_perplexity=5225.4067, train_loss=8.561288

Batch 202240, train_perplexity=8261.012, train_loss=9.019302

Batch 202250, train_perplexity=4025.0208, train_loss=8.300285

Batch 202260, train_perplexity=4343.825, train_loss=8.376511

Batch 202270, train_perplexity=5689.8306, train_loss=8.646436

Batch 202280, train_perplexity=5848.9863, train_loss=8.674024

Batch 202290, train_perplexity=5316.4336, train_loss=8.578558

Batch 202300, train_perplexity=4922.1895, train_loss=8.501509

Batch 202310, train_perplexity=5045.6626, train_loss=8.526284

Batch 202320, train_perplexity=5306.89, train_loss=8.576761

Batch 202330, train_perplexity=5036.821, train_loss=8.52453

Batch 202340, train_perplexity=5927.4272, train_loss=8.6873455

Batch 202350, train_perplexity=5000.971, train_loss=8.517387

Batch 202360, train_perplexity=4297.7734, train_loss=8.365852

Batch 202370, train_perplexity=5817.3496, train_loss=8.6686

Batch 202380, train_perplexity=5075.13, train_loss=8.532107

Batch 202390, train_perplexity=4943.0283, train_loss=8.5057335

Batch 202400, train_perplexity=4825.697, train_loss=8.48171

Batch 202410, train_perplexity=5053.276, train_loss=8.527792

Batch 202420, train_perplexity=5523.21, train_loss=8.6167145

Batch 202430, train_perplexity=4895.44, train_loss=8.496059

Batch 202440, train_perplexity=4986.3696, train_loss=8.514463

Batch 202450, train_perplexity=4693.3696, train_loss=8.453906

Batch 202460, train_perplexity=4428.376, train_loss=8.395788

Batch 202470, train_perplexity=4183.771, train_loss=8.338968

Batch 202480, train_perplexity=5514.599, train_loss=8.615154

Batch 202490, train_perplexity=6319.8306, train_loss=8.751448

Batch 202500, train_perplexity=5089.632, train_loss=8.534961

Batch 202510, train_perplexity=6079.2983, train_loss=8.712645

Batch 202520, train_perplexity=5794.869, train_loss=8.664728

Batch 202530, train_perplexity=4587.4336, train_loss=8.431076

Batch 202540, train_perplexity=5167.943, train_loss=8.55023

Batch 202550, train_perplexity=4774.901, train_loss=8.471128

Batch 202560, train_perplexity=5663.4443, train_loss=8.641788

Batch 202570, train_perplexity=5243.9224, train_loss=8.564825

Batch 202580, train_perplexity=6555.297, train_loss=8.788029

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 202590, train_perplexity=5256.48, train_loss=8.567217

Batch 202600, train_perplexity=6163.3667, train_loss=8.726378

Batch 202610, train_perplexity=4353.044, train_loss=8.378631

Batch 202620, train_perplexity=6011.797, train_loss=8.701479

Batch 202630, train_perplexity=5873.374, train_loss=8.6781845

Batch 202640, train_perplexity=6111.363, train_loss=8.717905

Batch 202650, train_perplexity=5424.6475, train_loss=8.598708

Batch 202660, train_perplexity=5072.812, train_loss=8.531651

Batch 202670, train_perplexity=5224.988, train_loss=8.561208

Batch 202680, train_perplexity=6609.2695, train_loss=8.796228

Batch 202690, train_perplexity=5087.812, train_loss=8.534603

Batch 202700, train_perplexity=7095.825, train_loss=8.867262

Batch 202710, train_perplexity=5846.739, train_loss=8.673639

Batch 202720, train_perplexity=6570.669, train_loss=8.790371

Batch 202730, train_perplexity=4899.709, train_loss=8.496931

Batch 202740, train_perplexity=4872.9326, train_loss=8.491451

Batch 202750, train_perplexity=5904.6562, train_loss=8.683496

Batch 202760, train_perplexity=4749.047, train_loss=8.465699

Batch 202770, train_perplexity=5706.791, train_loss=8.649412

Batch 202780, train_perplexity=6783.2993, train_loss=8.822219

Batch 202790, train_perplexity=5674.403, train_loss=8.643721

Batch 202800, train_perplexity=6148.3145, train_loss=8.723933

Batch 202810, train_perplexity=5252.4263, train_loss=8.566445

Batch 202820, train_perplexity=4319.3784, train_loss=8.370867

Batch 202830, train_perplexity=4105.237, train_loss=8.320019

Batch 202840, train_perplexity=5187.7046, train_loss=8.554047

Batch 202850, train_perplexity=5102.0776, train_loss=8.537403

Batch 202860, train_perplexity=4830.6973, train_loss=8.482746

Batch 202870, train_perplexity=6254.13, train_loss=8.740997

Batch 202880, train_perplexity=5346.396, train_loss=8.584178

Batch 202890, train_perplexity=4857.612, train_loss=8.488302

Batch 202900, train_perplexity=6105.8926, train_loss=8.71701

Batch 202910, train_perplexity=4480.7515, train_loss=8.407546

Batch 202920, train_perplexity=6050.702, train_loss=8.70793

Batch 202930, train_perplexity=4311.789, train_loss=8.369108

Batch 202940, train_perplexity=6184.687, train_loss=8.729832

Batch 202950, train_perplexity=5208.0586, train_loss=8.557962

Batch 202960, train_perplexity=5519.3555, train_loss=8.616016

Batch 202970, train_perplexity=5252.7866, train_loss=8.566514

Batch 202980, train_perplexity=5859.2866, train_loss=8.675783

Batch 202990, train_perplexity=6209.0947, train_loss=8.73377

Batch 203000, train_perplexity=6000.6445, train_loss=8.699622

Batch 203010, train_perplexity=5066.6426, train_loss=8.530434

Batch 203020, train_perplexity=4335.358, train_loss=8.374559

Batch 203030, train_perplexity=5873.374, train_loss=8.6781845

Batch 203040, train_perplexity=6154.9614, train_loss=8.725014

Batch 203050, train_perplexity=6346.284, train_loss=8.755625

Batch 203060, train_perplexity=6098.938, train_loss=8.71587

Batch 203070, train_perplexity=5379.553, train_loss=8.590361

Batch 203080, train_perplexity=4100.0645, train_loss=8.318758

Batch 203090, train_perplexity=5827.067, train_loss=8.670269

Batch 203100, train_perplexity=4852.112, train_loss=8.487169

Batch 203110, train_perplexity=4527.7837, train_loss=8.417988

Batch 203120, train_perplexity=6318.806, train_loss=8.751286

Batch 203130, train_perplexity=4854.56, train_loss=8.487674

Batch 203140, train_perplexity=4286.627, train_loss=8.3632555

Batch 203150, train_perplexity=6580.0, train_loss=8.79179

Batch 203160, train_perplexity=4642.505, train_loss=8.443009

Batch 203170, train_perplexity=4314.8086, train_loss=8.369808

Batch 203180, train_perplexity=5712.4976, train_loss=8.650412

Batch 203190, train_perplexity=5501.3833, train_loss=8.612755

Batch 203200, train_perplexity=4939.89, train_loss=8.505098

Batch 203210, train_perplexity=5763.773, train_loss=8.659348

Batch 203220, train_perplexity=5705.7026, train_loss=8.649221

Batch 203230, train_perplexity=5274.9097, train_loss=8.570717

Batch 203240, train_perplexity=5378.4966, train_loss=8.590164

Batch 203250, train_perplexity=5895.569, train_loss=8.681956

Batch 203260, train_perplexity=5298.354, train_loss=8.575151

Batch 203270, train_perplexity=6451.0205, train_loss=8.771994

Batch 203280, train_perplexity=4686.3564, train_loss=8.452411

Batch 203290, train_perplexity=5969.121, train_loss=8.694355

Batch 203300, train_perplexity=5371.8633, train_loss=8.58893

Batch 203310, train_perplexity=5537.7144, train_loss=8.619337

Batch 203320, train_perplexity=5630.1694, train_loss=8.635895

Batch 203330, train_perplexity=5475.8394, train_loss=8.608101

Batch 203340, train_perplexity=5572.239, train_loss=8.625552

Batch 203350, train_perplexity=6532.537, train_loss=8.784551

Batch 203360, train_perplexity=5284.199, train_loss=8.572476

Batch 203370, train_perplexity=5648.2715, train_loss=8.639105

Batch 203380, train_perplexity=4841.9927, train_loss=8.485082

Batch 203390, train_perplexity=5370.296, train_loss=8.588638

Batch 203400, train_perplexity=6244.308, train_loss=8.739426

Batch 203410, train_perplexity=6847.078, train_loss=8.831577

Batch 203420, train_perplexity=5752.093, train_loss=8.657319

Batch 203430, train_perplexity=5616.392, train_loss=8.633445

Batch 203440, train_perplexity=4893.199, train_loss=8.495602

Batch 203450, train_perplexity=6272.288, train_loss=8.7438965

Batch 203460, train_perplexity=5105.6455, train_loss=8.538102

Batch 203470, train_perplexity=4947.4995, train_loss=8.506638

Batch 203480, train_perplexity=6013.6777, train_loss=8.701792

Batch 203490, train_perplexity=4718.1196, train_loss=8.459166

Batch 203500, train_perplexity=5432.9673, train_loss=8.600241

Batch 203510, train_perplexity=5259.945, train_loss=8.567876

Batch 203520, train_perplexity=5611.247, train_loss=8.632528

Batch 203530, train_perplexity=4875.6753, train_loss=8.492014

Batch 203540, train_perplexity=4953.2495, train_loss=8.507799

Batch 203550, train_perplexity=4813.627, train_loss=8.479206

Batch 203560, train_perplexity=5646.6826, train_loss=8.6388235

Batch 203570, train_perplexity=5314.239, train_loss=8.578145

Batch 203580, train_perplexity=5038.695, train_loss=8.524902

Batch 203590, train_perplexity=5310.991, train_loss=8.577534

Batch 203600, train_perplexity=4761.359, train_loss=8.468288

Batch 203610, train_perplexity=5836.9336, train_loss=8.671961

Batch 203620, train_perplexity=5884.2783, train_loss=8.680039

Batch 203630, train_perplexity=5795.1455, train_loss=8.664776

Batch 203640, train_perplexity=5379.718, train_loss=8.590391

Batch 203650, train_perplexity=5732.537, train_loss=8.6539135

Batch 203660, train_perplexity=4899.919, train_loss=8.496974

Batch 203670, train_perplexity=4901.2275, train_loss=8.497241

Batch 203680, train_perplexity=5556.4355, train_loss=8.622712

Batch 203690, train_perplexity=5046.331, train_loss=8.526417

Batch 203700, train_perplexity=5058.7534, train_loss=8.528875

Batch 203710, train_perplexity=5729.5747, train_loss=8.653397

Batch 203720, train_perplexity=5534.1978, train_loss=8.618702

Batch 203730, train_perplexity=5184.48, train_loss=8.553425

Batch 203740, train_perplexity=5217.663, train_loss=8.559805

Batch 203750, train_perplexity=6096.9956, train_loss=8.715551

Batch 203760, train_perplexity=5755.9453, train_loss=8.657989

Batch 203770, train_perplexity=5508.0977, train_loss=8.613975

Batch 203780, train_perplexity=5634.95, train_loss=8.636744

Batch 203790, train_perplexity=5265.8477, train_loss=8.568997

Batch 203800, train_perplexity=5612.2534, train_loss=8.632708

Batch 203810, train_perplexity=5348.543, train_loss=8.584579

Batch 203820, train_perplexity=5151.9404, train_loss=8.547129

Batch 203830, train_perplexity=6052.9067, train_loss=8.708294

Batch 203840, train_perplexity=5322.8765, train_loss=8.579769

Batch 203850, train_perplexity=5375.6045, train_loss=8.589626

Batch 203860, train_perplexity=4961.2393, train_loss=8.509411

Batch 203870, train_perplexity=5164.317, train_loss=8.549528

Batch 203880, train_perplexity=5303.7124, train_loss=8.576162

Batch 203890, train_perplexity=5762.0747, train_loss=8.659053

Batch 203900, train_perplexity=5474.2627, train_loss=8.607813
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 203910, train_perplexity=5116.5444, train_loss=8.540235

Batch 203920, train_perplexity=5129.3105, train_loss=8.5427265

Batch 203930, train_perplexity=4642.607, train_loss=8.443031

Batch 203940, train_perplexity=5361.1416, train_loss=8.586932

Batch 203950, train_perplexity=5700.5684, train_loss=8.648321

Batch 203960, train_perplexity=5720.1953, train_loss=8.651758

Batch 203970, train_perplexity=6672.131, train_loss=8.805695

Batch 203980, train_perplexity=6065.6313, train_loss=8.710394

Batch 203990, train_perplexity=6546.3257, train_loss=8.786659

Batch 204000, train_perplexity=5024.132, train_loss=8.522008

Batch 204010, train_perplexity=4874.578, train_loss=8.491789

Batch 204020, train_perplexity=5071.085, train_loss=8.53131

Batch 204030, train_perplexity=4774.336, train_loss=8.47101

Batch 204040, train_perplexity=6266.65, train_loss=8.742997

Batch 204050, train_perplexity=5491.298, train_loss=8.61092

Batch 204060, train_perplexity=4709.2637, train_loss=8.457287

Batch 204070, train_perplexity=5736.7046, train_loss=8.65464

Batch 204080, train_perplexity=5768.1167, train_loss=8.660101

Batch 204090, train_perplexity=4519.5825, train_loss=8.416175

Batch 204100, train_perplexity=5056.9204, train_loss=8.528513

Batch 204110, train_perplexity=4631.4893, train_loss=8.440634

Batch 204120, train_perplexity=5723.0327, train_loss=8.652254

Batch 204130, train_perplexity=4486.896, train_loss=8.408916

Batch 204140, train_perplexity=6261.954, train_loss=8.742248

Batch 204150, train_perplexity=4092.3064, train_loss=8.316864

Batch 204160, train_perplexity=5376.384, train_loss=8.589771

Batch 204170, train_perplexity=5849.8174, train_loss=8.674166

Batch 204180, train_perplexity=6268.796, train_loss=8.74334

Batch 204190, train_perplexity=5935.952, train_loss=8.688783

Batch 204200, train_perplexity=5310.8945, train_loss=8.577516

Batch 204210, train_perplexity=4813.168, train_loss=8.479111

Batch 204220, train_perplexity=5618.487, train_loss=8.633818

Batch 204230, train_perplexity=5847.6753, train_loss=8.6737995

Batch 204240, train_perplexity=5098.7993, train_loss=8.53676

Batch 204250, train_perplexity=5443.8486, train_loss=8.6022415

Batch 204260, train_perplexity=5269.711, train_loss=8.569731

Batch 204270, train_perplexity=6732.1484, train_loss=8.81465

Batch 204280, train_perplexity=5603.6484, train_loss=8.631173

Batch 204290, train_perplexity=5882.0845, train_loss=8.6796665

Batch 204300, train_perplexity=5297.333, train_loss=8.574959

Batch 204310, train_perplexity=5496.737, train_loss=8.61191

Batch 204320, train_perplexity=5134.4785, train_loss=8.543734

Batch 204330, train_perplexity=5619.178, train_loss=8.633941

Batch 204340, train_perplexity=4345.8555, train_loss=8.376978

Batch 204350, train_perplexity=6357.9214, train_loss=8.757457

Batch 204360, train_perplexity=4250.742, train_loss=8.354849

Batch 204370, train_perplexity=5050.1733, train_loss=8.527178

Batch 204380, train_perplexity=4144.338, train_loss=8.329498

Batch 204390, train_perplexity=4722.0806, train_loss=8.460005

Batch 204400, train_perplexity=4509.8345, train_loss=8.414016

Batch 204410, train_perplexity=5088.0156, train_loss=8.534643

Batch 204420, train_perplexity=5519.6606, train_loss=8.616072

Batch 204430, train_perplexity=5300.962, train_loss=8.575644

Batch 204440, train_perplexity=5732.4497, train_loss=8.653898

Batch 204450, train_perplexity=4515.0376, train_loss=8.415169

Batch 204460, train_perplexity=5056.308, train_loss=8.528392

Batch 204470, train_perplexity=4716.401, train_loss=8.458801

Batch 204480, train_perplexity=4746.593, train_loss=8.465182

Batch 204490, train_perplexity=4940.724, train_loss=8.505267

Batch 204500, train_perplexity=4400.859, train_loss=8.389555

Batch 204510, train_perplexity=4723.247, train_loss=8.460252

Batch 204520, train_perplexity=6717.2954, train_loss=8.812441

Batch 204530, train_perplexity=4107.845, train_loss=8.320654

Batch 204540, train_perplexity=5376.8555, train_loss=8.589859

Batch 204550, train_perplexity=4167.643, train_loss=8.335106

Batch 204560, train_perplexity=5665.654, train_loss=8.642178

Batch 204570, train_perplexity=5475.9546, train_loss=8.608122

Batch 204580, train_perplexity=5842.7197, train_loss=8.672952

Batch 204590, train_perplexity=5458.597, train_loss=8.604947

Batch 204600, train_perplexity=6487.501, train_loss=8.777633

Batch 204610, train_perplexity=5131.0767, train_loss=8.543071

Batch 204620, train_perplexity=6139.6836, train_loss=8.722528

Batch 204630, train_perplexity=4777.1416, train_loss=8.471598

Batch 204640, train_perplexity=5473.5684, train_loss=8.607686

Batch 204650, train_perplexity=5365.8574, train_loss=8.587811

Batch 204660, train_perplexity=4951.5493, train_loss=8.507456

Batch 204670, train_perplexity=6659.7983, train_loss=8.803844

Batch 204680, train_perplexity=4948.302, train_loss=8.5068

Batch 204690, train_perplexity=4898.6997, train_loss=8.496725

Batch 204700, train_perplexity=5784.246, train_loss=8.662893

Batch 204710, train_perplexity=4934.019, train_loss=8.503909

Batch 204720, train_perplexity=5747.8213, train_loss=8.656576

Batch 204730, train_perplexity=5472.2793, train_loss=8.6074505

Batch 204740, train_perplexity=5825.672, train_loss=8.67003

Batch 204750, train_perplexity=7585.055, train_loss=8.933935

Batch 204760, train_perplexity=5425.9253, train_loss=8.598944

Batch 204770, train_perplexity=5688.7617, train_loss=8.646248

Batch 204780, train_perplexity=5747.257, train_loss=8.656478

Batch 204790, train_perplexity=6395.4487, train_loss=8.763342

Batch 204800, train_perplexity=5370.5522, train_loss=8.588686

Batch 204810, train_perplexity=5052.8516, train_loss=8.527708

Batch 204820, train_perplexity=5272.5864, train_loss=8.570276

Batch 204830, train_perplexity=5564.0557, train_loss=8.624083

Batch 204840, train_perplexity=5681.6914, train_loss=8.645004

Batch 204850, train_perplexity=5319.989, train_loss=8.5792265

Batch 204860, train_perplexity=4800.607, train_loss=8.476498

Batch 204870, train_perplexity=4975.8286, train_loss=8.512347

Batch 204880, train_perplexity=5559.314, train_loss=8.62323

Batch 204890, train_perplexity=5197.5894, train_loss=8.55595

Batch 204900, train_perplexity=4984.021, train_loss=8.513992

Batch 204910, train_perplexity=4839.3525, train_loss=8.484536

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Batch 204920, train_perplexity=5539.14, train_loss=8.619595

Batch 204930, train_perplexity=5164.2183, train_loss=8.549509

Batch 204940, train_perplexity=5204.0864, train_loss=8.5571995

Batch 204950, train_perplexity=5954.941, train_loss=8.691977

Batch 204960, train_perplexity=6090.9805, train_loss=8.714564

Batch 204970, train_perplexity=4955.083, train_loss=8.508169

Batch 204980, train_perplexity=5358.0493, train_loss=8.586355

Batch 204990, train_perplexity=5363.5757, train_loss=8.587386

Batch 205000, train_perplexity=4252.5464, train_loss=8.355273

Batch 205010, train_perplexity=5527.636, train_loss=8.617516

Batch 205020, train_perplexity=6314.601, train_loss=8.75062

Batch 205030, train_perplexity=6136.3174, train_loss=8.72198

Batch 205040, train_perplexity=5841.834, train_loss=8.6728

Batch 205050, train_perplexity=5294.212, train_loss=8.574369

Batch 205060, train_perplexity=5801.305, train_loss=8.665838

Batch 205070, train_perplexity=5642.263, train_loss=8.638041

Batch 205080, train_perplexity=5126.958, train_loss=8.542268

Batch 205090, train_perplexity=6106.294, train_loss=8.717075

Batch 205100, train_perplexity=5995.2734, train_loss=8.698727

Batch 205110, train_perplexity=5732.4497, train_loss=8.653898

Batch 205120, train_perplexity=5877.038, train_loss=8.678808

Batch 205130, train_perplexity=6406.608, train_loss=8.765085

Batch 205140, train_perplexity=5735.5176, train_loss=8.654433

Batch 205150, train_perplexity=5985.367, train_loss=8.697073
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 205160, train_perplexity=5445.2607, train_loss=8.602501

Batch 205170, train_perplexity=5821.6787, train_loss=8.669344

Batch 205180, train_perplexity=5263.407, train_loss=8.568534

Batch 205190, train_perplexity=5964.6826, train_loss=8.693611

Batch 205200, train_perplexity=5352.0435, train_loss=8.585234

Batch 205210, train_perplexity=5214.788, train_loss=8.559254

Batch 205220, train_perplexity=5006.459, train_loss=8.518484

Batch 205230, train_perplexity=5497.4707, train_loss=8.612043

Batch 205240, train_perplexity=5105.154, train_loss=8.538006

Batch 205250, train_perplexity=5580.4604, train_loss=8.627027

Batch 205260, train_perplexity=5368.673, train_loss=8.588336

Batch 205270, train_perplexity=4837.179, train_loss=8.484087

Batch 205280, train_perplexity=5630.0083, train_loss=8.635866

Batch 205290, train_perplexity=5289.105, train_loss=8.573404

Batch 205300, train_perplexity=5305.2607, train_loss=8.576454

Batch 205310, train_perplexity=5392.8984, train_loss=8.592838

Batch 205320, train_perplexity=5703.14, train_loss=8.648772

Batch 205330, train_perplexity=4335.978, train_loss=8.374702

Batch 205340, train_perplexity=5686.2505, train_loss=8.645806

Batch 205350, train_perplexity=5139.24, train_loss=8.544661

Batch 205360, train_perplexity=5971.41, train_loss=8.694738

Batch 205370, train_perplexity=4513.2637, train_loss=8.414776

Batch 205380, train_perplexity=5726.1333, train_loss=8.652796

Batch 205390, train_perplexity=4550.8877, train_loss=8.423078

Batch 205400, train_perplexity=5417.7607, train_loss=8.597438

Batch 205410, train_perplexity=5138.52, train_loss=8.54452

Batch 205420, train_perplexity=5807.073, train_loss=8.666832

Batch 205430, train_perplexity=7033.25, train_loss=8.858404

Batch 205440, train_perplexity=4479.995, train_loss=8.407377

Batch 205450, train_perplexity=5482.596, train_loss=8.609334

Batch 205460, train_perplexity=5569.9756, train_loss=8.625146

Batch 205470, train_perplexity=5157.2983, train_loss=8.548168

Batch 205480, train_perplexity=5462.5186, train_loss=8.605665

Batch 205490, train_perplexity=5409.134, train_loss=8.595844

Batch 205500, train_perplexity=5339.563, train_loss=8.582899

Batch 205510, train_perplexity=5688.9243, train_loss=8.646276

Batch 205520, train_perplexity=5471.914, train_loss=8.607384

Batch 205530, train_perplexity=5519.0396, train_loss=8.615959

Batch 205540, train_perplexity=5543.8066, train_loss=8.620437

Batch 205550, train_perplexity=5148.2075, train_loss=8.546404

Batch 205560, train_perplexity=5026.4272, train_loss=8.522465

Batch 205570, train_perplexity=5876.1753, train_loss=8.678661

Batch 205580, train_perplexity=5281.5996, train_loss=8.571984

Batch 205590, train_perplexity=4959.432, train_loss=8.509047

Batch 205600, train_perplexity=5326.5835, train_loss=8.580465

Batch 205610, train_perplexity=5282.219, train_loss=8.572102

Batch 205620, train_perplexity=5798.529, train_loss=8.6653595

Batch 205630, train_perplexity=5296.9946, train_loss=8.574895

Batch 205640, train_perplexity=5265.993, train_loss=8.569025

Batch 205650, train_perplexity=4665.1, train_loss=8.447865

Batch 205660, train_perplexity=5455.823, train_loss=8.604439

Batch 205670, train_perplexity=5998.47, train_loss=8.69926

Batch 205680, train_perplexity=5448.897, train_loss=8.6031685

Batch 205690, train_perplexity=6760.2246, train_loss=8.818811

Batch 205700, train_perplexity=4680.635, train_loss=8.451189

Batch 205710, train_perplexity=4930.8203, train_loss=8.503261

Batch 205720, train_perplexity=4972.859, train_loss=8.51175

Batch 205730, train_perplexity=4817.622, train_loss=8.480036

Batch 205740, train_perplexity=5165.903, train_loss=8.549835

Batch 205750, train_perplexity=5802.4673, train_loss=8.6660385

Batch 205760, train_perplexity=6066.019, train_loss=8.710458

Batch 205770, train_perplexity=4853.8516, train_loss=8.487528

Batch 205780, train_perplexity=4886.8945, train_loss=8.494312

Batch 205790, train_perplexity=5209.832, train_loss=8.558303

Batch 205800, train_perplexity=5614.984, train_loss=8.633194

Batch 205810, train_perplexity=5667.194, train_loss=8.642449

Batch 205820, train_perplexity=5245.0576, train_loss=8.565042

Batch 205830, train_perplexity=5495.4893, train_loss=8.611683

Batch 205840, train_perplexity=4753.737, train_loss=8.466686

Batch 205850, train_perplexity=5070.9155, train_loss=8.531277

Batch 205860, train_perplexity=5448.1953, train_loss=8.60304

Batch 205870, train_perplexity=5335.745, train_loss=8.582184

Batch 205880, train_perplexity=4872.38, train_loss=8.491338

Batch 205890, train_perplexity=5968.8423, train_loss=8.694308

Batch 205900, train_perplexity=4387.901, train_loss=8.386606

Batch 205910, train_perplexity=4851.987, train_loss=8.4871435

Batch 205920, train_perplexity=5395.3677, train_loss=8.593296

Batch 205930, train_perplexity=4487.448, train_loss=8.4090395

Batch 205940, train_perplexity=5719.2896, train_loss=8.6516

Batch 205950, train_perplexity=6879.8057, train_loss=8.836346

Batch 205960, train_perplexity=5467.5894, train_loss=8.606593

Batch 205970, train_perplexity=5911.079, train_loss=8.684584

Batch 205980, train_perplexity=5680.391, train_loss=8.644775

Batch 205990, train_perplexity=5454.5435, train_loss=8.604204

Batch 206000, train_perplexity=5879.9424, train_loss=8.679302

Batch 206010, train_perplexity=5858.0464, train_loss=8.675571

Batch 206020, train_perplexity=4989.0146, train_loss=8.514994

Batch 206030, train_perplexity=5877.5596, train_loss=8.678897

Batch 206040, train_perplexity=5010.767, train_loss=8.519344

Batch 206050, train_perplexity=5982.8564, train_loss=8.696653

Batch 206060, train_perplexity=5986.2065, train_loss=8.697213

Batch 206070, train_perplexity=4648.065, train_loss=8.444206

Batch 206080, train_perplexity=5610.5996, train_loss=8.632413

Batch 206090, train_perplexity=4869.95, train_loss=8.490839

Batch 206100, train_perplexity=5491.7695, train_loss=8.611006

Batch 206110, train_perplexity=5805.29, train_loss=8.666525

Batch 206120, train_perplexity=5245.8633, train_loss=8.565195

Batch 206130, train_perplexity=5155.548, train_loss=8.547829

Batch 206140, train_perplexity=4280.418, train_loss=8.361806

Batch 206150, train_perplexity=5466.0723, train_loss=8.606316

Batch 206160, train_perplexity=5384.948, train_loss=8.591363

Batch 206170, train_perplexity=4346.5063, train_loss=8.377128

Batch 206180, train_perplexity=5071.085, train_loss=8.53131

Batch 206190, train_perplexity=5444.783, train_loss=8.602413

Batch 206200, train_perplexity=6274.5557, train_loss=8.744258

Batch 206210, train_perplexity=5493.781, train_loss=8.611372

Batch 206220, train_perplexity=4382.836, train_loss=8.385451

Batch 206230, train_perplexity=5751.945, train_loss=8.657293

Batch 206240, train_perplexity=5556.9287, train_loss=8.622801

Batch 206250, train_perplexity=5292.4097, train_loss=8.574029

Batch 206260, train_perplexity=6150.185, train_loss=8.724237

Batch 206270, train_perplexity=4718.8574, train_loss=8.459322

Batch 206280, train_perplexity=6322.519, train_loss=8.751873

Batch 206290, train_perplexity=6511.1724, train_loss=8.781275

Batch 206300, train_perplexity=5814.876, train_loss=8.668175

Batch 206310, train_perplexity=6841.3735, train_loss=8.830744

Batch 206320, train_perplexity=5477.84, train_loss=8.608466

Batch 206330, train_perplexity=5910.707, train_loss=8.684521

Batch 206340, train_perplexity=5794.25, train_loss=8.664621

Batch 206350, train_perplexity=5770.8296, train_loss=8.660571

Batch 206360, train_perplexity=6070.84, train_loss=8.711252

Batch 206370, train_perplexity=4605.6123, train_loss=8.435031

Batch 206380, train_perplexity=5968.313, train_loss=8.69422

Batch 206390, train_perplexity=4774.14, train_loss=8.470969

Batch 206400, train_perplexity=6035.027, train_loss=8.705336

Batch 206410, train_perplexity=6482.226, train_loss=8.776819

Batch 206420, train_perplexity=5271.3994, train_loss=8.570051

Batch 206430, train_perplexity=5766.2026, train_loss=8.659769

Batch 206440, train_perplexity=5099.0376, train_loss=8.536807

Batch 206450, train_perplexity=6761.3726, train_loss=8.818981

Batch 206460, train_perplexity=5206.4443, train_loss=8.557652

Batch 206470, train_perplexity=5036.048, train_loss=8.524377
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 206480, train_perplexity=7073.023, train_loss=8.864043

Batch 206490, train_perplexity=5133.514, train_loss=8.543546

Batch 206500, train_perplexity=6137.3066, train_loss=8.722141

Batch 206510, train_perplexity=4807.0576, train_loss=8.47784

Batch 206520, train_perplexity=4615.9106, train_loss=8.437264

Batch 206530, train_perplexity=4768.903, train_loss=8.4698715

Batch 206540, train_perplexity=5110.7705, train_loss=8.539105

Batch 206550, train_perplexity=5298.5913, train_loss=8.575196

Batch 206560, train_perplexity=5981.875, train_loss=8.696489

Batch 206570, train_perplexity=5748.671, train_loss=8.656724

Batch 206580, train_perplexity=5765.9663, train_loss=8.659728

Batch 206590, train_perplexity=6194.025, train_loss=8.73134

Batch 206600, train_perplexity=4960.4917, train_loss=8.50926

Batch 206610, train_perplexity=4974.2773, train_loss=8.512035

Batch 206620, train_perplexity=5693.581, train_loss=8.647095

Batch 206630, train_perplexity=5201.447, train_loss=8.556692

Batch 206640, train_perplexity=4230.602, train_loss=8.3501

Batch 206650, train_perplexity=5123.18, train_loss=8.541531

Batch 206660, train_perplexity=5306.824, train_loss=8.576749

Batch 206670, train_perplexity=5136.673, train_loss=8.544161

Batch 206680, train_perplexity=5185.7163, train_loss=8.553663

Batch 206690, train_perplexity=5314.0713, train_loss=8.578114

Batch 206700, train_perplexity=6454.479, train_loss=8.77253

Batch 206710, train_perplexity=5825.122, train_loss=8.669935

Batch 206720, train_perplexity=4968.01, train_loss=8.510775

Batch 206730, train_perplexity=5583.186, train_loss=8.627515

Batch 206740, train_perplexity=4850.039, train_loss=8.486742

Batch 206750, train_perplexity=6624.288, train_loss=8.798498

Batch 206760, train_perplexity=5303.3384, train_loss=8.576092

Batch 206770, train_perplexity=5058.7485, train_loss=8.528874

Batch 206780, train_perplexity=4965.8643, train_loss=8.510343

Batch 206790, train_perplexity=5673.1475, train_loss=8.643499

Batch 206800, train_perplexity=4883.5864, train_loss=8.493635

Batch 206810, train_perplexity=5092.5205, train_loss=8.535528

Batch 206820, train_perplexity=6247.954, train_loss=8.740009

Batch 206830, train_perplexity=5184.51, train_loss=8.553431

Batch 206840, train_perplexity=5416.6294, train_loss=8.597229

Batch 206850, train_perplexity=5167.6377, train_loss=8.550171

Batch 206860, train_perplexity=4650.3525, train_loss=8.444698

Batch 206870, train_perplexity=6901.215, train_loss=8.839453

Batch 206880, train_perplexity=5652.938, train_loss=8.639931

Batch 206890, train_perplexity=5449.0894, train_loss=8.603204

Batch 206900, train_perplexity=5505.12, train_loss=8.613434

Batch 206910, train_perplexity=5641.2515, train_loss=8.637861

Batch 206920, train_perplexity=5421.1978, train_loss=8.598072

Batch 206930, train_perplexity=5547.821, train_loss=8.6211605

Batch 206940, train_perplexity=6022.2173, train_loss=8.703211

Batch 206950, train_perplexity=5225.86, train_loss=8.561375

Batch 206960, train_perplexity=4277.7285, train_loss=8.361177

Batch 206970, train_perplexity=6052.5776, train_loss=8.70824

Batch 206980, train_perplexity=5905.197, train_loss=8.683588

Batch 206990, train_perplexity=4976.8774, train_loss=8.512558

Batch 207000, train_perplexity=4416.0654, train_loss=8.393004

Batch 207010, train_perplexity=4665.385, train_loss=8.447926

Batch 207020, train_perplexity=4562.9736, train_loss=8.42573

Batch 207030, train_perplexity=6892.789, train_loss=8.838231

Batch 207040, train_perplexity=5570.6924, train_loss=8.625275

Batch 207050, train_perplexity=5647.7866, train_loss=8.639019

Batch 207060, train_perplexity=4685.9053, train_loss=8.452314

Batch 207070, train_perplexity=5479.4805, train_loss=8.608766

Batch 207080, train_perplexity=5316.008, train_loss=8.578478

Batch 207090, train_perplexity=5248.275, train_loss=8.565655

Batch 207100, train_perplexity=5984.9106, train_loss=8.696997

Batch 207110, train_perplexity=5258.661, train_loss=8.567632

Batch 207120, train_perplexity=5707.199, train_loss=8.649484

Batch 207130, train_perplexity=5726.9907, train_loss=8.6529455

Batch 207140, train_perplexity=4503.6885, train_loss=8.412652

Batch 207150, train_perplexity=4771.35, train_loss=8.470385

Batch 207160, train_perplexity=5284.0786, train_loss=8.5724535

Batch 207170, train_perplexity=5320.207, train_loss=8.5792675

Batch 207180, train_perplexity=4339.3286, train_loss=8.375475

Batch 207190, train_perplexity=6328.0327, train_loss=8.752745

Batch 207200, train_perplexity=5615.9585, train_loss=8.633368

Batch 207210, train_perplexity=5611.986, train_loss=8.63266

Batch 207220, train_perplexity=4688.6855, train_loss=8.452908

Batch 207230, train_perplexity=5212.3716, train_loss=8.55879

Batch 207240, train_perplexity=5504.663, train_loss=8.613351

Batch 207250, train_perplexity=5415.7617, train_loss=8.597069

Batch 207260, train_perplexity=5679.4487, train_loss=8.644609

Batch 207270, train_perplexity=5829.1733, train_loss=8.67063

Batch 207280, train_perplexity=5973.4263, train_loss=8.695076

Batch 207290, train_perplexity=5846.6606, train_loss=8.673626

Batch 207300, train_perplexity=4663.9346, train_loss=8.447615

Batch 207310, train_perplexity=4983.137, train_loss=8.513815

Batch 207320, train_perplexity=6023.929, train_loss=8.703495

Batch 207330, train_perplexity=5490.67, train_loss=8.6108055

Batch 207340, train_perplexity=6060.855, train_loss=8.709606

Batch 207350, train_perplexity=6513.315, train_loss=8.781604

Batch 207360, train_perplexity=5209.087, train_loss=8.55816

Batch 207370, train_perplexity=5585.662, train_loss=8.627958

Batch 207380, train_perplexity=5724.1406, train_loss=8.652448

Batch 207390, train_perplexity=5425.646, train_loss=8.598892

Batch 207400, train_perplexity=4371.6943, train_loss=8.382906

Batch 207410, train_perplexity=4997.9194, train_loss=8.516777

Batch 207420, train_perplexity=4823.0835, train_loss=8.481169

Batch 207430, train_perplexity=5197.153, train_loss=8.555866

Batch 207440, train_perplexity=5985.5386, train_loss=8.697102

Batch 207450, train_perplexity=6010.7075, train_loss=8.701298

Batch 207460, train_perplexity=5306.7886, train_loss=8.576742

Batch 207470, train_perplexity=5613.0347, train_loss=8.632847

Batch 207480, train_perplexity=5204.3945, train_loss=8.557259

Batch 207490, train_perplexity=6707.706, train_loss=8.811012

Batch 207500, train_perplexity=5815.974, train_loss=8.668364

Batch 207510, train_perplexity=5268.8364, train_loss=8.569565

Batch 207520, train_perplexity=5502.926, train_loss=8.613035

Batch 207530, train_perplexity=4970.5073, train_loss=8.511277

Batch 207540, train_perplexity=5216.7676, train_loss=8.559633

Batch 207550, train_perplexity=5806.2095, train_loss=8.666683

Batch 207560, train_perplexity=5682.6343, train_loss=8.64517

Batch 207570, train_perplexity=6568.263, train_loss=8.790005

Batch 207580, train_perplexity=4931.0835, train_loss=8.503314

Batch 207590, train_perplexity=5796.726, train_loss=8.665049

Batch 207600, train_perplexity=5136.134, train_loss=8.544056

Batch 207610, train_perplexity=4644.1035, train_loss=8.443354

Batch 207620, train_perplexity=5837.0557, train_loss=8.671982

Batch 207630, train_perplexity=5541.824, train_loss=8.620079

Batch 207640, train_perplexity=5501.656, train_loss=8.612804

Batch 207650, train_perplexity=6134.4863, train_loss=8.721682

Batch 207660, train_perplexity=5491.4395, train_loss=8.610946

Batch 207670, train_perplexity=5719.857, train_loss=8.651699

Batch 207680, train_perplexity=4604.0273, train_loss=8.434687

Batch 207690, train_perplexity=5104.4136, train_loss=8.537861

Batch 207700, train_perplexity=5197.966, train_loss=8.556023

Batch 207710, train_perplexity=5429.512, train_loss=8.599605

Batch 207720, train_perplexity=4343.1543, train_loss=8.376356

Batch 207730, train_perplexity=4953.76, train_loss=8.507902

Batch 207740, train_perplexity=5835.731, train_loss=8.671755

Batch 207750, train_perplexity=5073.7744, train_loss=8.53184

Batch 207760, train_perplexity=5002.5166, train_loss=8.517696

Batch 207770, train_perplexity=4702.173, train_loss=8.45578

Batch 207780, train_perplexity=4368.2104, train_loss=8.382109

Batch 207790, train_perplexity=5833.8057, train_loss=8.671425
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 207800, train_perplexity=4412.87, train_loss=8.392281

Batch 207810, train_perplexity=5266.852, train_loss=8.569188

Batch 207820, train_perplexity=4815.5093, train_loss=8.479597

Batch 207830, train_perplexity=4329.7344, train_loss=8.373261

Batch 207840, train_perplexity=5225.511, train_loss=8.561308

Batch 207850, train_perplexity=5158.7104, train_loss=8.548442

Batch 207860, train_perplexity=6052.624, train_loss=8.708247

Batch 207870, train_perplexity=4537.146, train_loss=8.4200535

Batch 207880, train_perplexity=5266.154, train_loss=8.569056

Batch 207890, train_perplexity=5996.154, train_loss=8.6988735

Batch 207900, train_perplexity=5600.325, train_loss=8.63058

Batch 207910, train_perplexity=6126.6167, train_loss=8.720398

Batch 207920, train_perplexity=5884.3965, train_loss=8.680059

Batch 207930, train_perplexity=5606.15, train_loss=8.631619

Batch 207940, train_perplexity=5314.467, train_loss=8.578188

Batch 207950, train_perplexity=4871.1953, train_loss=8.491095

Batch 207960, train_perplexity=5884.4976, train_loss=8.680077

Batch 207970, train_perplexity=4645.858, train_loss=8.443731

Batch 207980, train_perplexity=5195.458, train_loss=8.55554

Batch 207990, train_perplexity=4878.4707, train_loss=8.492587

Batch 208000, train_perplexity=6025.199, train_loss=8.703706

Batch 208010, train_perplexity=6007.097, train_loss=8.700697

Batch 208020, train_perplexity=5473.8555, train_loss=8.6077385

Batch 208030, train_perplexity=4465.6113, train_loss=8.404161

Batch 208040, train_perplexity=4928.8926, train_loss=8.50287

Batch 208050, train_perplexity=5838.882, train_loss=8.672295

Batch 208060, train_perplexity=5799.292, train_loss=8.665491

Batch 208070, train_perplexity=5124.167, train_loss=8.541723

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Batch 208080, train_perplexity=5349.446, train_loss=8.584748

Batch 208090, train_perplexity=5644.19, train_loss=8.638382

Batch 208100, train_perplexity=5181.7466, train_loss=8.552897

Batch 208110, train_perplexity=5662.013, train_loss=8.641535

Batch 208120, train_perplexity=6238.9272, train_loss=8.738564

Batch 208130, train_perplexity=6042.479, train_loss=8.70657

Batch 208140, train_perplexity=4565.15, train_loss=8.426207

Batch 208150, train_perplexity=5413.2314, train_loss=8.5966015

Batch 208160, train_perplexity=5625.2744, train_loss=8.635025

Batch 208170, train_perplexity=6069.9365, train_loss=8.711103

Batch 208180, train_perplexity=5326.5786, train_loss=8.580464

Batch 208190, train_perplexity=4617.923, train_loss=8.4377

Batch 208200, train_perplexity=5562.878, train_loss=8.623871

Batch 208210, train_perplexity=5715.3477, train_loss=8.65091

Batch 208220, train_perplexity=4271.508, train_loss=8.359722

Batch 208230, train_perplexity=4636.8677, train_loss=8.441794

Batch 208240, train_perplexity=5976.612, train_loss=8.695609

Batch 208250, train_perplexity=5345.993, train_loss=8.584103

Batch 208260, train_perplexity=5709.927, train_loss=8.649961

Batch 208270, train_perplexity=5625.2637, train_loss=8.635023

Batch 208280, train_perplexity=4718.2275, train_loss=8.459188

Batch 208290, train_perplexity=5574.902, train_loss=8.62603

Batch 208300, train_perplexity=6286.229, train_loss=8.746117

Batch 208310, train_perplexity=5165.4253, train_loss=8.549743

Batch 208320, train_perplexity=6062.786, train_loss=8.709925

Batch 208330, train_perplexity=4134.496, train_loss=8.327121

Batch 208340, train_perplexity=5274.4673, train_loss=8.570633

Batch 208350, train_perplexity=4784.9385, train_loss=8.473228

Batch 208360, train_perplexity=5165.0605, train_loss=8.549672

Batch 208370, train_perplexity=5397.5293, train_loss=8.593697

Batch 208380, train_perplexity=5011.6753, train_loss=8.519526

Batch 208390, train_perplexity=5189.2236, train_loss=8.554339

Batch 208400, train_perplexity=4640.73, train_loss=8.442627

Batch 208410, train_perplexity=4692.2017, train_loss=8.453657

Batch 208420, train_perplexity=6244.0938, train_loss=8.739391

Batch 208430, train_perplexity=4889.0806, train_loss=8.49476

Batch 208440, train_perplexity=4806.012, train_loss=8.477623

Batch 208450, train_perplexity=5767.0444, train_loss=8.659915

Batch 208460, train_perplexity=5227.176, train_loss=8.561626

Batch 208470, train_perplexity=4509.5205, train_loss=8.413946

Batch 208480, train_perplexity=5385.0044, train_loss=8.591373

Batch 208490, train_perplexity=6253.0923, train_loss=8.740831

Batch 208500, train_perplexity=5041.8286, train_loss=8.525524

Batch 208510, train_perplexity=5636.9385, train_loss=8.637096

Batch 208520, train_perplexity=6431.922, train_loss=8.769029

Batch 208530, train_perplexity=4936.0474, train_loss=8.50432

Batch 208540, train_perplexity=5161.151, train_loss=8.548915

Batch 208550, train_perplexity=5224.6294, train_loss=8.561139

Batch 208560, train_perplexity=5345.7334, train_loss=8.584054

Batch 208570, train_perplexity=4876.922, train_loss=8.4922695

Batch 208580, train_perplexity=4832.771, train_loss=8.483175

Batch 208590, train_perplexity=5476.017, train_loss=8.608133

Batch 208600, train_perplexity=5120.1616, train_loss=8.540941

Batch 208610, train_perplexity=5615.2676, train_loss=8.6332445

Batch 208620, train_perplexity=6068.576, train_loss=8.710879

Batch 208630, train_perplexity=4760.147, train_loss=8.468034

Batch 208640, train_perplexity=4933.026, train_loss=8.503708

Batch 208650, train_perplexity=4785.518, train_loss=8.47335

Batch 208660, train_perplexity=4638.5796, train_loss=8.442163

Batch 208670, train_perplexity=4611.638, train_loss=8.436338

Batch 208680, train_perplexity=5980.4717, train_loss=8.696255

Batch 208690, train_perplexity=6490.292, train_loss=8.778063

Batch 208700, train_perplexity=5301.892, train_loss=8.575819

Batch 208710, train_perplexity=5587.122, train_loss=8.62822

Batch 208720, train_perplexity=5266.32, train_loss=8.569087

Batch 208730, train_perplexity=5728.9956, train_loss=8.6532955

Batch 208740, train_perplexity=5107.311, train_loss=8.538428

Batch 208750, train_perplexity=4566.047, train_loss=8.426403

Batch 208760, train_perplexity=6113.1987, train_loss=8.718205

Batch 208770, train_perplexity=5556.3403, train_loss=8.622695

Batch 208780, train_perplexity=5451.0386, train_loss=8.603561

Batch 208790, train_perplexity=5184.312, train_loss=8.553392

Batch 208800, train_perplexity=5553.681, train_loss=8.622216

Batch 208810, train_perplexity=5266.0938, train_loss=8.569044

Batch 208820, train_perplexity=4574.354, train_loss=8.428221

Batch 208830, train_perplexity=5369.7427, train_loss=8.588535

Batch 208840, train_perplexity=5724.883, train_loss=8.652577

Batch 208850, train_perplexity=5622.228, train_loss=8.634483

Batch 208860, train_perplexity=6039.8867, train_loss=8.7061405

Batch 208870, train_perplexity=5682.916, train_loss=8.64522

Batch 208880, train_perplexity=5016.7583, train_loss=8.520539

Batch 208890, train_perplexity=5460.164, train_loss=8.605234

Batch 208900, train_perplexity=6349.481, train_loss=8.756128

Batch 208910, train_perplexity=5424.0522, train_loss=8.5985985

Batch 208920, train_perplexity=4849.2803, train_loss=8.486586

Batch 208930, train_perplexity=5410.408, train_loss=8.59608

Batch 208940, train_perplexity=6137.986, train_loss=8.722252

Batch 208950, train_perplexity=5422.9766, train_loss=8.5984

Batch 208960, train_perplexity=6137.494, train_loss=8.722172

Batch 208970, train_perplexity=5498.75, train_loss=8.612276

Batch 208980, train_perplexity=5372.299, train_loss=8.589011

Batch 208990, train_perplexity=6398.267, train_loss=8.7637825

Batch 209000, train_perplexity=5277.365, train_loss=8.571182

Batch 209010, train_perplexity=5018.156, train_loss=8.520818

Batch 209020, train_perplexity=6148.754, train_loss=8.724005

Batch 209030, train_perplexity=5226.428, train_loss=8.561483

Batch 209040, train_perplexity=4640.646, train_loss=8.442609

Batch 209050, train_perplexity=4495.737, train_loss=8.410885
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 209060, train_perplexity=5457.7227, train_loss=8.604787

Batch 209070, train_perplexity=6003.1113, train_loss=8.700033

Batch 209080, train_perplexity=4353.1313, train_loss=8.378651

Batch 209090, train_perplexity=5453.4614, train_loss=8.604006

Batch 209100, train_perplexity=5203.9077, train_loss=8.557165

Batch 209110, train_perplexity=5534.372, train_loss=8.618733

Batch 209120, train_perplexity=4877.9917, train_loss=8.492489

Batch 209130, train_perplexity=4748.8066, train_loss=8.465649

Batch 209140, train_perplexity=4999.4785, train_loss=8.517089

Batch 209150, train_perplexity=5976.88, train_loss=8.695654

Batch 209160, train_perplexity=4459.228, train_loss=8.402731

Batch 209170, train_perplexity=5850.9277, train_loss=8.6743555

Batch 209180, train_perplexity=5664.1304, train_loss=8.641909

Batch 209190, train_perplexity=6507.8325, train_loss=8.780762

Batch 209200, train_perplexity=5708.9575, train_loss=8.649792

Batch 209210, train_perplexity=5612.944, train_loss=8.632831

Batch 209220, train_perplexity=4436.915, train_loss=8.397715

Batch 209230, train_perplexity=5227.505, train_loss=8.561689

Batch 209240, train_perplexity=5040.8, train_loss=8.52532

Batch 209250, train_perplexity=5081.9976, train_loss=8.53346

Batch 209260, train_perplexity=6401.0625, train_loss=8.764219

Batch 209270, train_perplexity=5662.618, train_loss=8.641642

Batch 209280, train_perplexity=4818.3853, train_loss=8.480194

Batch 209290, train_perplexity=5090.345, train_loss=8.535101

Batch 209300, train_perplexity=4399.298, train_loss=8.3892

Batch 209310, train_perplexity=5521.193, train_loss=8.616349

Batch 209320, train_perplexity=4663.801, train_loss=8.447586

Batch 209330, train_perplexity=5303.222, train_loss=8.57607

Batch 209340, train_perplexity=5147.844, train_loss=8.546333

Batch 209350, train_perplexity=5875.279, train_loss=8.678509

Batch 209360, train_perplexity=6060.7046, train_loss=8.709581

Batch 209370, train_perplexity=5900.0293, train_loss=8.682713

Batch 209380, train_perplexity=5680.2397, train_loss=8.644749

Batch 209390, train_perplexity=4958.392, train_loss=8.508837

Batch 209400, train_perplexity=4250.8477, train_loss=8.354874

Batch 209410, train_perplexity=5951.4604, train_loss=8.691392

Batch 209420, train_perplexity=4836.2563, train_loss=8.483896

Batch 209430, train_perplexity=4721.7837, train_loss=8.459942

Batch 209440, train_perplexity=5336.763, train_loss=8.582375

Batch 209450, train_perplexity=5274.11, train_loss=8.570565

Batch 209460, train_perplexity=4848.522, train_loss=8.486429

Batch 209470, train_perplexity=5361.423, train_loss=8.586985

Batch 209480, train_perplexity=6967.3315, train_loss=8.848988

Batch 209490, train_perplexity=3794.8918, train_loss=8.241411

Batch 209500, train_perplexity=4846.243, train_loss=8.485959

Batch 209510, train_perplexity=5032.711, train_loss=8.523714

Batch 209520, train_perplexity=6386.0083, train_loss=8.761865

Batch 209530, train_perplexity=6587.811, train_loss=8.792976

Batch 209540, train_perplexity=5822.3896, train_loss=8.669466

Batch 209550, train_perplexity=5159.5317, train_loss=8.548601

Batch 209560, train_perplexity=4768.848, train_loss=8.46986

Batch 209570, train_perplexity=4920.2275, train_loss=8.50111

Batch 209580, train_perplexity=5905.771, train_loss=8.683685

Batch 209590, train_perplexity=5067.266, train_loss=8.530557

Batch 209600, train_perplexity=5163.332, train_loss=8.549337

Batch 209610, train_perplexity=4941.148, train_loss=8.505353

Batch 209620, train_perplexity=4969.299, train_loss=8.511034

Batch 209630, train_perplexity=5291.809, train_loss=8.5739155

Batch 209640, train_perplexity=4662.734, train_loss=8.447357

Batch 209650, train_perplexity=5216.927, train_loss=8.559664

Batch 209660, train_perplexity=5503.713, train_loss=8.613178

Batch 209670, train_perplexity=6885.4766, train_loss=8.83717

Batch 209680, train_perplexity=5315.146, train_loss=8.578316

Batch 209690, train_perplexity=4949.189, train_loss=8.506979

Batch 209700, train_perplexity=5405.6274, train_loss=8.595196

Batch 209710, train_perplexity=5744.0894, train_loss=8.655927

Batch 209720, train_perplexity=4838.18, train_loss=8.484294

Batch 209730, train_perplexity=4889.8267, train_loss=8.494912

Batch 209740, train_perplexity=5391.952, train_loss=8.592663

Batch 209750, train_perplexity=5281.9067, train_loss=8.572042

Batch 209760, train_perplexity=4494.3735, train_loss=8.410582

Batch 209770, train_perplexity=6332.3794, train_loss=8.753431

Batch 209780, train_perplexity=5507.982, train_loss=8.613954

Batch 209790, train_perplexity=5162.131, train_loss=8.549105

Batch 209800, train_perplexity=4898.849, train_loss=8.496756

Batch 209810, train_perplexity=5355.5, train_loss=8.585879

Batch 209820, train_perplexity=6037.1226, train_loss=8.705683

Batch 209830, train_perplexity=5911.981, train_loss=8.684736

Batch 209840, train_perplexity=5015.1943, train_loss=8.520227

Batch 209850, train_perplexity=5819.564, train_loss=8.668981

Batch 209860, train_perplexity=4888.1807, train_loss=8.4945755

Batch 209870, train_perplexity=6228.5356, train_loss=8.7368965

Batch 209880, train_perplexity=5497.481, train_loss=8.612045

Batch 209890, train_perplexity=5285.1167, train_loss=8.57265

Batch 209900, train_perplexity=4865.545, train_loss=8.489934

Batch 209910, train_perplexity=5264.2656, train_loss=8.568697

Batch 209920, train_perplexity=4594.7544, train_loss=8.432671

Batch 209930, train_perplexity=5881.995, train_loss=8.679651

Batch 209940, train_perplexity=5407.4424, train_loss=8.595531

Batch 209950, train_perplexity=5459.1543, train_loss=8.605049

Batch 209960, train_perplexity=5493.6133, train_loss=8.611341

Batch 209970, train_perplexity=5072.618, train_loss=8.531612

Batch 209980, train_perplexity=5778.997, train_loss=8.661985

Batch 209990, train_perplexity=5892.4995, train_loss=8.681436

Batch 210000, train_perplexity=6025.078, train_loss=8.703686

Batch 210010, train_perplexity=4499.211, train_loss=8.411657

Batch 210020, train_perplexity=6582.2847, train_loss=8.792137

Batch 210030, train_perplexity=4973.8027, train_loss=8.51194

Batch 210040, train_perplexity=6163.8486, train_loss=8.726457

Batch 210050, train_perplexity=5523.9473, train_loss=8.616848

Batch 210060, train_perplexity=5744.2153, train_loss=8.655949

Batch 210070, train_perplexity=5123.776, train_loss=8.541647

Batch 210080, train_perplexity=5264.4414, train_loss=8.56873

Batch 210090, train_perplexity=7147.7554, train_loss=8.874554

Batch 210100, train_perplexity=5734.6914, train_loss=8.654289

Batch 210110, train_perplexity=5472.864, train_loss=8.607557

Batch 210120, train_perplexity=6586.4917, train_loss=8.792776

Batch 210130, train_perplexity=6171.884, train_loss=8.727759

Batch 210140, train_perplexity=4909.11, train_loss=8.498848

Batch 210150, train_perplexity=4636.744, train_loss=8.441768

Batch 210160, train_perplexity=5353.442, train_loss=8.585495

Batch 210170, train_perplexity=5367.173, train_loss=8.588057

Batch 210180, train_perplexity=5440.423, train_loss=8.601612

Batch 210190, train_perplexity=6631.0005, train_loss=8.799511

Batch 210200, train_perplexity=5050.2935, train_loss=8.527202

Batch 210210, train_perplexity=5333.2627, train_loss=8.581718

Batch 210220, train_perplexity=5276.4595, train_loss=8.571011

Batch 210230, train_perplexity=4839.071, train_loss=8.484478

Batch 210240, train_perplexity=5272.938, train_loss=8.570343

Batch 210250, train_perplexity=5556.303, train_loss=8.622688

Batch 210260, train_perplexity=4487.196, train_loss=8.408983

Batch 210270, train_perplexity=5940.6016, train_loss=8.689566

Batch 210280, train_perplexity=5131.3604, train_loss=8.543126

Batch 210290, train_perplexity=5598.547, train_loss=8.630262

Batch 210300, train_perplexity=5118.2964, train_loss=8.540577

Batch 210310, train_perplexity=6080.371, train_loss=8.712821

Batch 210320, train_perplexity=5256.6353, train_loss=8.567246

Batch 210330, train_perplexity=4649.958, train_loss=8.444613

Batch 210340, train_perplexity=4722.36, train_loss=8.460064

Batch 210350, train_perplexity=5554.841, train_loss=8.622425

Batch 210360, train_perplexity=5214.1416, train_loss=8.55913

Batch 210370, train_perplexity=4753.972, train_loss=8.466736
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 210380, train_perplexity=5019.204, train_loss=8.521027

Batch 210390, train_perplexity=6022.9985, train_loss=8.703341

Batch 210400, train_perplexity=5215.335, train_loss=8.559359

Batch 210410, train_perplexity=5112.55, train_loss=8.5394535

Batch 210420, train_perplexity=5174.0825, train_loss=8.551417

Batch 210430, train_perplexity=4844.6626, train_loss=8.485633

Batch 210440, train_perplexity=6215.422, train_loss=8.734789

Batch 210450, train_perplexity=5768.238, train_loss=8.660122

Batch 210460, train_perplexity=5015.768, train_loss=8.520342

Batch 210470, train_perplexity=5437.1294, train_loss=8.6010065

Batch 210480, train_perplexity=4353.783, train_loss=8.3788

Batch 210490, train_perplexity=5753.909, train_loss=8.657635

Batch 210500, train_perplexity=5508.7803, train_loss=8.614099

Batch 210510, train_perplexity=6272.826, train_loss=8.743982

Batch 210520, train_perplexity=5037.325, train_loss=8.524631

Batch 210530, train_perplexity=5735.851, train_loss=8.654491

Batch 210540, train_perplexity=5280.0835, train_loss=8.571697

Batch 210550, train_perplexity=4291.5317, train_loss=8.364399

Batch 210560, train_perplexity=5176.664, train_loss=8.551916

Batch 210570, train_perplexity=4753.1294, train_loss=8.466558

Batch 210580, train_perplexity=5987.377, train_loss=8.697409

Batch 210590, train_perplexity=4943.726, train_loss=8.505875

Batch 210600, train_perplexity=6539.7617, train_loss=8.785656

Batch 210610, train_perplexity=4817.604, train_loss=8.480032

Batch 210620, train_perplexity=4602.864, train_loss=8.434434

Batch 210630, train_perplexity=4629.7847, train_loss=8.440266

Batch 210640, train_perplexity=5777.8726, train_loss=8.661791

Batch 210650, train_perplexity=5414.269, train_loss=8.596793

Batch 210660, train_perplexity=5367.265, train_loss=8.588074

Batch 210670, train_perplexity=4670.6465, train_loss=8.449053

Batch 210680, train_perplexity=6832.3887, train_loss=8.82943

Batch 210690, train_perplexity=5277.2344, train_loss=8.571157

Batch 210700, train_perplexity=6243.909, train_loss=8.739362

Batch 210710, train_perplexity=4825.706, train_loss=8.481712

Batch 210720, train_perplexity=5569.6196, train_loss=8.625082

Batch 210730, train_perplexity=4870.777, train_loss=8.491009

Batch 210740, train_perplexity=5461.685, train_loss=8.605513

Batch 210750, train_perplexity=5345.3203, train_loss=8.583977

Batch 210760, train_perplexity=5634.042, train_loss=8.636582

Batch 210770, train_perplexity=4875.8335, train_loss=8.492046

Batch 210780, train_perplexity=5638.3364, train_loss=8.637344

Batch 210790, train_perplexity=5974.5376, train_loss=8.695262

Batch 210800, train_perplexity=5248.6704, train_loss=8.56573

Batch 210810, train_perplexity=5600.2505, train_loss=8.630567

Batch 210820, train_perplexity=5774.606, train_loss=8.661225

Batch 210830, train_perplexity=4966.7876, train_loss=8.510529

Batch 210840, train_perplexity=5590.741, train_loss=8.628867

Batch 210850, train_perplexity=7148.6416, train_loss=8.874678

Batch 210860, train_perplexity=5497.0566, train_loss=8.611968

Batch 210870, train_perplexity=4566.9263, train_loss=8.426596

Batch 210880, train_perplexity=4591.0664, train_loss=8.431868

Batch 210890, train_perplexity=4995.937, train_loss=8.51638

Batch 210900, train_perplexity=4827.5747, train_loss=8.4821

Batch 210910, train_perplexity=5428.7515, train_loss=8.599464

Batch 210920, train_perplexity=6023.8433, train_loss=8.703481

Batch 210930, train_perplexity=4860.309, train_loss=8.488857

Batch 210940, train_perplexity=4844.3345, train_loss=8.485565

Batch 210950, train_perplexity=5422.3765, train_loss=8.5982895

Batch 210960, train_perplexity=5097.861, train_loss=8.536576

Batch 210970, train_perplexity=6786.1655, train_loss=8.822641

Batch 210980, train_perplexity=5238.0947, train_loss=8.563713

Batch 210990, train_perplexity=5240.843, train_loss=8.564238

Batch 211000, train_perplexity=5782.084, train_loss=8.662519

Batch 211010, train_perplexity=5546.911, train_loss=8.620996

Batch 211020, train_perplexity=5483.035, train_loss=8.609414

Batch 211030, train_perplexity=5608.1978, train_loss=8.631985

Batch 211040, train_perplexity=5503.0625, train_loss=8.61306

Batch 211050, train_perplexity=6306.489, train_loss=8.749334

Batch 211060, train_perplexity=4747.9375, train_loss=8.465466

Batch 211070, train_perplexity=4447.9727, train_loss=8.400204

Batch 211080, train_perplexity=4423.353, train_loss=8.394653

Batch 211090, train_perplexity=5808.8237, train_loss=8.667133

Batch 211100, train_perplexity=5757.3066, train_loss=8.658225

Batch 211110, train_perplexity=5870.171, train_loss=8.677639

Batch 211120, train_perplexity=5198.8286, train_loss=8.556189

Batch 211130, train_perplexity=5078.6304, train_loss=8.532797

Batch 211140, train_perplexity=4816.1616, train_loss=8.4797325

Batch 211150, train_perplexity=7001.1143, train_loss=8.853825

Batch 211160, train_perplexity=4907.4575, train_loss=8.498511

Batch 211170, train_perplexity=5676.9414, train_loss=8.644168

Batch 211180, train_perplexity=6409.804, train_loss=8.765584

Batch 211190, train_perplexity=4972.0576, train_loss=8.511589

Batch 211200, train_perplexity=5571.3516, train_loss=8.625393

Batch 211210, train_perplexity=5248.475, train_loss=8.565693

Batch 211220, train_perplexity=5350.9204, train_loss=8.585024

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Batch 211230, train_perplexity=5234.2495, train_loss=8.562979

Batch 211240, train_perplexity=5009.9165, train_loss=8.519175

Batch 211250, train_perplexity=5384.953, train_loss=8.591364

Batch 211260, train_perplexity=5777.0957, train_loss=8.661656

Batch 211270, train_perplexity=4657.3296, train_loss=8.4461975

Batch 211280, train_perplexity=5143.7515, train_loss=8.545538

Batch 211290, train_perplexity=5719.704, train_loss=8.651672

Batch 211300, train_perplexity=6130.9653, train_loss=8.7211075

Batch 211310, train_perplexity=5040.997, train_loss=8.525359

Batch 211320, train_perplexity=5358.3506, train_loss=8.586411

Batch 211330, train_perplexity=5856.3037, train_loss=8.675274

Batch 211340, train_perplexity=5989.947, train_loss=8.697838

Batch 211350, train_perplexity=6361.469, train_loss=8.758015

Batch 211360, train_perplexity=5420.8, train_loss=8.597999

Batch 211370, train_perplexity=6038.585, train_loss=8.705925

Batch 211380, train_perplexity=5534.1978, train_loss=8.618702

Batch 211390, train_perplexity=4987.9155, train_loss=8.514773

Batch 211400, train_perplexity=4957.8955, train_loss=8.508737

Batch 211410, train_perplexity=5056.9443, train_loss=8.528518

Batch 211420, train_perplexity=5101.586, train_loss=8.537307

Batch 211430, train_perplexity=5397.859, train_loss=8.593758

Batch 211440, train_perplexity=6302.929, train_loss=8.74877

Batch 211450, train_perplexity=4935.2188, train_loss=8.504152

Batch 211460, train_perplexity=5260.9233, train_loss=8.568062

Batch 211470, train_perplexity=4382.765, train_loss=8.385435

Batch 211480, train_perplexity=5193.5557, train_loss=8.555174

Batch 211490, train_perplexity=4146.698, train_loss=8.330068

Batch 211500, train_perplexity=5361.3457, train_loss=8.58697

Batch 211510, train_perplexity=5594.2344, train_loss=8.629492

Batch 211520, train_perplexity=4617.24, train_loss=8.437552

Batch 211530, train_perplexity=5461.7837, train_loss=8.605531

Batch 211540, train_perplexity=6998.4043, train_loss=8.853437

Batch 211550, train_perplexity=5934.984, train_loss=8.68862

Batch 211560, train_perplexity=4484.586, train_loss=8.4084015

Batch 211570, train_perplexity=5342.487, train_loss=8.5834465

Batch 211580, train_perplexity=5326.0347, train_loss=8.580362

Batch 211590, train_perplexity=6151.8276, train_loss=8.724504

Batch 211600, train_perplexity=6384.023, train_loss=8.761554

Batch 211610, train_perplexity=4610.2134, train_loss=8.436029

Batch 211620, train_perplexity=5095.1826, train_loss=8.536051
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 211630, train_perplexity=5179.4194, train_loss=8.552448

Batch 211640, train_perplexity=5514.084, train_loss=8.615061

Batch 211650, train_perplexity=5447.8784, train_loss=8.602982

Batch 211660, train_perplexity=5960.719, train_loss=8.692946

Batch 211670, train_perplexity=5996.8857, train_loss=8.698996

Batch 211680, train_perplexity=4693.983, train_loss=8.454037

Batch 211690, train_perplexity=4939.221, train_loss=8.504963

Batch 211700, train_perplexity=4448.49, train_loss=8.40032

Batch 211710, train_perplexity=4406.772, train_loss=8.390898

Batch 211720, train_perplexity=5696.9487, train_loss=8.647686

Batch 211730, train_perplexity=4966.191, train_loss=8.510408

Batch 211740, train_perplexity=4536.8174, train_loss=8.419981

Batch 211750, train_perplexity=5412.927, train_loss=8.596545

Batch 211760, train_perplexity=5896.564, train_loss=8.682125

Batch 211770, train_perplexity=5886.1587, train_loss=8.680359

Batch 211780, train_perplexity=5532.3984, train_loss=8.618377

Batch 211790, train_perplexity=4574.773, train_loss=8.428312

Batch 211800, train_perplexity=5939.9785, train_loss=8.689461

Batch 211810, train_perplexity=4827.6763, train_loss=8.4821205

Batch 211820, train_perplexity=5192.3623, train_loss=8.554944

Batch 211830, train_perplexity=4862.2, train_loss=8.489246

Batch 211840, train_perplexity=5964.893, train_loss=8.693646

Batch 211850, train_perplexity=5716.3887, train_loss=8.651093

Batch 211860, train_perplexity=5686.017, train_loss=8.645765

Batch 211870, train_perplexity=5800.321, train_loss=8.6656685

Batch 211880, train_perplexity=4968.5596, train_loss=8.510885

Batch 211890, train_perplexity=4657.8184, train_loss=8.446302

Batch 211900, train_perplexity=3896.8186, train_loss=8.267916

Batch 211910, train_perplexity=4752.839, train_loss=8.466497

Batch 211920, train_perplexity=5508.707, train_loss=8.614085

Batch 211930, train_perplexity=5073.078, train_loss=8.531703

Batch 211940, train_perplexity=6111.794, train_loss=8.717976

Batch 211950, train_perplexity=6402.009, train_loss=8.764367

Batch 211960, train_perplexity=4894.404, train_loss=8.495848

Batch 211970, train_perplexity=5709.1045, train_loss=8.649817

Batch 211980, train_perplexity=5381.1494, train_loss=8.590657

Batch 211990, train_perplexity=6530.027, train_loss=8.784166

Batch 212000, train_perplexity=5525.0854, train_loss=8.617054

Batch 212010, train_perplexity=5235.148, train_loss=8.56315

Batch 212020, train_perplexity=4630.465, train_loss=8.4404125

Batch 212030, train_perplexity=5382.3657, train_loss=8.590883

Batch 212040, train_perplexity=4908.7124, train_loss=8.498767

Batch 212050, train_perplexity=6245.273, train_loss=8.73958

Batch 212060, train_perplexity=5501.3623, train_loss=8.612751

Batch 212070, train_perplexity=5195.1953, train_loss=8.55549

Batch 212080, train_perplexity=5513.6157, train_loss=8.614976

Batch 212090, train_perplexity=5352.8906, train_loss=8.585392

Batch 212100, train_perplexity=5779.967, train_loss=8.662153

Batch 212110, train_perplexity=6245.4756, train_loss=8.739613

Batch 212120, train_perplexity=5035.8794, train_loss=8.5243435

Batch 212130, train_perplexity=5630.6685, train_loss=8.635983

Batch 212140, train_perplexity=7032.619, train_loss=8.8583145

Batch 212150, train_perplexity=5134.7188, train_loss=8.54378

Batch 212160, train_perplexity=4721.7026, train_loss=8.459925

Batch 212170, train_perplexity=6245.803, train_loss=8.739665

Batch 212180, train_perplexity=5915.1055, train_loss=8.685265

Batch 212190, train_perplexity=5725.063, train_loss=8.652609

Batch 212200, train_perplexity=5291.214, train_loss=8.573803

Batch 212210, train_perplexity=4932.6357, train_loss=8.503629

Batch 212220, train_perplexity=5920.8677, train_loss=8.686238

Batch 212230, train_perplexity=5679.221, train_loss=8.644569

Batch 212240, train_perplexity=4748.6167, train_loss=8.465609

Batch 212250, train_perplexity=5307.3555, train_loss=8.576849

Batch 212260, train_perplexity=5924.6465, train_loss=8.686876

Batch 212270, train_perplexity=4748.4897, train_loss=8.465582

Batch 212280, train_perplexity=4751.928, train_loss=8.466306

Batch 212290, train_perplexity=5391.9727, train_loss=8.592667

Batch 212300, train_perplexity=5163.076, train_loss=8.549288

Batch 212310, train_perplexity=5280.27, train_loss=8.5717325

Batch 212320, train_perplexity=5248.656, train_loss=8.565727

Batch 212330, train_perplexity=4731.863, train_loss=8.462074

Batch 212340, train_perplexity=4553.1973, train_loss=8.423585

Batch 212350, train_perplexity=5517.4927, train_loss=8.615679

Batch 212360, train_perplexity=5453.893, train_loss=8.604085

Batch 212370, train_perplexity=5503.2515, train_loss=8.613094

Batch 212380, train_perplexity=4613.0327, train_loss=8.436641

Batch 212390, train_perplexity=5086.143, train_loss=8.534275

Batch 212400, train_perplexity=5468.815, train_loss=8.606817

Batch 212410, train_perplexity=5189.199, train_loss=8.554335

Batch 212420, train_perplexity=5261.651, train_loss=8.5682

Batch 212430, train_perplexity=6383.183, train_loss=8.761422

Batch 212440, train_perplexity=5090.923, train_loss=8.535214

Batch 212450, train_perplexity=4498.25, train_loss=8.411444

Batch 212460, train_perplexity=4720.46, train_loss=8.4596615

Batch 212470, train_perplexity=5551.2876, train_loss=8.621785

Batch 212480, train_perplexity=5017.194, train_loss=8.520626

Batch 212490, train_perplexity=5594.672, train_loss=8.62957

Batch 212500, train_perplexity=6083.6597, train_loss=8.713362

Batch 212510, train_perplexity=5684.6943, train_loss=8.645533

Batch 212520, train_perplexity=4581.488, train_loss=8.429779

Batch 212530, train_perplexity=5305.994, train_loss=8.576592

Batch 212540, train_perplexity=5802.3623, train_loss=8.66602

Batch 212550, train_perplexity=4895.72, train_loss=8.496117

Batch 212560, train_perplexity=5181.5786, train_loss=8.552865

Batch 212570, train_perplexity=5229.998, train_loss=8.562166

Batch 212580, train_perplexity=5628.8486, train_loss=8.63566

Batch 212590, train_perplexity=5106.4634, train_loss=8.538262

Batch 212600, train_perplexity=5125.052, train_loss=8.541896

Batch 212610, train_perplexity=5643.695, train_loss=8.638294

Batch 212620, train_perplexity=4927.5625, train_loss=8.5026

Batch 212630, train_perplexity=5989.604, train_loss=8.697781

Batch 212640, train_perplexity=4870.3267, train_loss=8.490916

Batch 212650, train_perplexity=5583.9365, train_loss=8.627649

Batch 212660, train_perplexity=4707.957, train_loss=8.457009

Batch 212670, train_perplexity=4755.0244, train_loss=8.466957

Batch 212680, train_perplexity=5559.181, train_loss=8.623206

Batch 212690, train_perplexity=4457.604, train_loss=8.402367

Batch 212700, train_perplexity=5065.6377, train_loss=8.530235

Batch 212710, train_perplexity=5354.3, train_loss=8.585655

Batch 212720, train_perplexity=5608.043, train_loss=8.631957

Batch 212730, train_perplexity=5080.0884, train_loss=8.533084

Batch 212740, train_perplexity=4690.5996, train_loss=8.453316

Batch 212750, train_perplexity=4589.6084, train_loss=8.43155

Batch 212760, train_perplexity=5270.334, train_loss=8.569849

Batch 212770, train_perplexity=5557.469, train_loss=8.622898

Batch 212780, train_perplexity=5017.108, train_loss=8.520609

Batch 212790, train_perplexity=4687.7065, train_loss=8.452699

Batch 212800, train_perplexity=5064.2563, train_loss=8.529963

Batch 212810, train_perplexity=5332.627, train_loss=8.581599

Batch 212820, train_perplexity=5947.4604, train_loss=8.69072

Batch 212830, train_perplexity=6148.2617, train_loss=8.723925

Batch 212840, train_perplexity=5172.7803, train_loss=8.551166

Batch 212850, train_perplexity=6641.582, train_loss=8.8011055

Batch 212860, train_perplexity=4306.159, train_loss=8.367802

Batch 212870, train_perplexity=5693.8525, train_loss=8.647142

Batch 212880, train_perplexity=5171.567, train_loss=8.550931

Batch 212890, train_perplexity=5781.213, train_loss=8.662369

Batch 212900, train_perplexity=5286.5835, train_loss=8.572927

Batch 212910, train_perplexity=5437.736, train_loss=8.601118

Batch 212920, train_perplexity=4343.4604, train_loss=8.376427

Batch 212930, train_perplexity=5259.1875, train_loss=8.567732

Batch 212940, train_perplexity=5338.092, train_loss=8.5826235
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 212950, train_perplexity=5495.4473, train_loss=8.611675

Batch 212960, train_perplexity=4977.808, train_loss=8.512745

Batch 212970, train_perplexity=4651.222, train_loss=8.444885

Batch 212980, train_perplexity=5328.3467, train_loss=8.580796

Batch 212990, train_perplexity=5272.0933, train_loss=8.570183

Batch 213000, train_perplexity=6335.182, train_loss=8.753874

Batch 213010, train_perplexity=6019.668, train_loss=8.702787

Batch 213020, train_perplexity=3938.4612, train_loss=8.278545

Batch 213030, train_perplexity=4670.575, train_loss=8.449038

Batch 213040, train_perplexity=5945.4473, train_loss=8.690381

Batch 213050, train_perplexity=4523.998, train_loss=8.417151

Batch 213060, train_perplexity=4127.952, train_loss=8.325537

Batch 213070, train_perplexity=5674.2515, train_loss=8.643694

Batch 213080, train_perplexity=4399.8433, train_loss=8.389324

Batch 213090, train_perplexity=5777.487, train_loss=8.661724

Batch 213100, train_perplexity=6746.661, train_loss=8.816803

Batch 213110, train_perplexity=5454.195, train_loss=8.60414

Batch 213120, train_perplexity=4510.2603, train_loss=8.41411

Batch 213130, train_perplexity=5367.255, train_loss=8.588072

Batch 213140, train_perplexity=6858.685, train_loss=8.833271

Batch 213150, train_perplexity=5300.37, train_loss=8.575532

Batch 213160, train_perplexity=3828.9695, train_loss=8.250351

Batch 213170, train_perplexity=5547.8843, train_loss=8.621172

Batch 213180, train_perplexity=5087.2925, train_loss=8.534501

Batch 213190, train_perplexity=4811.5156, train_loss=8.478767

Batch 213200, train_perplexity=6083.8804, train_loss=8.713398

Batch 213210, train_perplexity=5035.7114, train_loss=8.52431

Batch 213220, train_perplexity=4878.866, train_loss=8.492668

Batch 213230, train_perplexity=5262.6396, train_loss=8.568388

Batch 213240, train_perplexity=4775.1055, train_loss=8.471171

Batch 213250, train_perplexity=6441.504, train_loss=8.770517

Batch 213260, train_perplexity=5047.8765, train_loss=8.526723

Batch 213270, train_perplexity=6040.0767, train_loss=8.706172

Batch 213280, train_perplexity=5180.1406, train_loss=8.5525875

Batch 213290, train_perplexity=4800.8267, train_loss=8.476543

Batch 213300, train_perplexity=6522.857, train_loss=8.783068

Batch 213310, train_perplexity=5362.3584, train_loss=8.587159

Batch 213320, train_perplexity=5410.31, train_loss=8.596062

Batch 213330, train_perplexity=6024.4634, train_loss=8.703584

Batch 213340, train_perplexity=5033.215, train_loss=8.523814

Batch 213350, train_perplexity=5753.1626, train_loss=8.657505

Batch 213360, train_perplexity=4586.353, train_loss=8.4308405

Batch 213370, train_perplexity=4856.8154, train_loss=8.488138

Batch 213380, train_perplexity=5346.9673, train_loss=8.584285

Batch 213390, train_perplexity=5009.1235, train_loss=8.519016

Batch 213400, train_perplexity=5144.595, train_loss=8.545702

Batch 213410, train_perplexity=6262.163, train_loss=8.742281

Batch 213420, train_perplexity=5818.659, train_loss=8.668825

Batch 213430, train_perplexity=6765.81, train_loss=8.819637

Batch 213440, train_perplexity=5766.9453, train_loss=8.659898

Batch 213450, train_perplexity=6567.7373, train_loss=8.789925

Batch 213460, train_perplexity=5167.406, train_loss=8.550126

Batch 213470, train_perplexity=5721.848, train_loss=8.652047

Batch 213480, train_perplexity=6213.342, train_loss=8.734454

Batch 213490, train_perplexity=5660.199, train_loss=8.641214

Batch 213500, train_perplexity=5402.7925, train_loss=8.594671

Batch 213510, train_perplexity=5703.1294, train_loss=8.64877

Batch 213520, train_perplexity=5792.151, train_loss=8.664259

Batch 213530, train_perplexity=4736.649, train_loss=8.463085

Batch 213540, train_perplexity=4428.79, train_loss=8.395882

Batch 213550, train_perplexity=5209.7524, train_loss=8.558288

Batch 213560, train_perplexity=5479.6533, train_loss=8.608797

Batch 213570, train_perplexity=4576.7974, train_loss=8.428755

Batch 213580, train_perplexity=5529.6924, train_loss=8.6178875

Batch 213590, train_perplexity=4939.3057, train_loss=8.50498

Batch 213600, train_perplexity=5783.7607, train_loss=8.662809

Batch 213610, train_perplexity=5673.115, train_loss=8.643494

Batch 213620, train_perplexity=5973.979, train_loss=8.6951685

Batch 213630, train_perplexity=5697.4976, train_loss=8.647782

Batch 213640, train_perplexity=4552.8193, train_loss=8.423502

Batch 213650, train_perplexity=4964.9976, train_loss=8.510168

Batch 213660, train_perplexity=4811.5615, train_loss=8.478777

Batch 213670, train_perplexity=4305.527, train_loss=8.367655

Batch 213680, train_perplexity=5379.9126, train_loss=8.590427

Batch 213690, train_perplexity=5407.638, train_loss=8.595568

Batch 213700, train_perplexity=4665.269, train_loss=8.447901

Batch 213710, train_perplexity=5360.323, train_loss=8.58678

Batch 213720, train_perplexity=5260.9434, train_loss=8.568066

Batch 213730, train_perplexity=5297.6665, train_loss=8.575022

Batch 213740, train_perplexity=5333.446, train_loss=8.581753

Batch 213750, train_perplexity=5431.8276, train_loss=8.600031

Batch 213760, train_perplexity=5861.444, train_loss=8.676151

Batch 213770, train_perplexity=5501.3936, train_loss=8.612757

Batch 213780, train_perplexity=4722.441, train_loss=8.460081

Batch 213790, train_perplexity=5194.6753, train_loss=8.555389

Batch 213800, train_perplexity=5598.7285, train_loss=8.630295

Batch 213810, train_perplexity=5074.6987, train_loss=8.532022

Batch 213820, train_perplexity=5763.542, train_loss=8.6593075

Batch 213830, train_perplexity=6098.1934, train_loss=8.715748

Batch 213840, train_perplexity=5925.2114, train_loss=8.686972

Batch 213850, train_perplexity=5601.623, train_loss=8.630812

Batch 213860, train_perplexity=4933.186, train_loss=8.50374

Batch 213870, train_perplexity=5499.5103, train_loss=8.612414

Batch 213880, train_perplexity=5267.4497, train_loss=8.569302

Batch 213890, train_perplexity=5603.7285, train_loss=8.631187

Batch 213900, train_perplexity=5508.4653, train_loss=8.614041

Batch 213910, train_perplexity=4497.894, train_loss=8.411365

Batch 213920, train_perplexity=5109.537, train_loss=8.538864

Batch 213930, train_perplexity=5196.023, train_loss=8.555649

Batch 213940, train_perplexity=4993.0747, train_loss=8.515807

Batch 213950, train_perplexity=6391.644, train_loss=8.762747

Batch 213960, train_perplexity=6671.444, train_loss=8.805592

Batch 213970, train_perplexity=4756.0083, train_loss=8.467164

Batch 213980, train_perplexity=5754.008, train_loss=8.657652

Batch 213990, train_perplexity=4920.669, train_loss=8.5012

Batch 214000, train_perplexity=5345.6772, train_loss=8.5840435

Batch 214010, train_perplexity=5134.6646, train_loss=8.54377

Batch 214020, train_perplexity=4716.3965, train_loss=8.4588

Batch 214030, train_perplexity=5628.446, train_loss=8.635589

Batch 214040, train_perplexity=5399.398, train_loss=8.594043

Batch 214050, train_perplexity=5086.5503, train_loss=8.534355

Batch 214060, train_perplexity=4978.582, train_loss=8.5129

Batch 214070, train_perplexity=5330.09, train_loss=8.581123

Batch 214080, train_perplexity=5031.554, train_loss=8.523484

Batch 214090, train_perplexity=5612.489, train_loss=8.63275

Batch 214100, train_perplexity=6065.7817, train_loss=8.710419

Batch 214110, train_perplexity=5399.2593, train_loss=8.594017

Batch 214120, train_perplexity=5419.239, train_loss=8.597711

Batch 214130, train_perplexity=5771.5063, train_loss=8.660688

Batch 214140, train_perplexity=4733.325, train_loss=8.462383

Batch 214150, train_perplexity=6140.304, train_loss=8.72263

Batch 214160, train_perplexity=4873.89, train_loss=8.491648

Batch 214170, train_perplexity=5180.714, train_loss=8.552698

Batch 214180, train_perplexity=4892.9287, train_loss=8.495546

Batch 214190, train_perplexity=4394.0396, train_loss=8.388004

Batch 214200, train_perplexity=4422.3916, train_loss=8.394436

Batch 214210, train_perplexity=5165.174, train_loss=8.549694

Batch 214220, train_perplexity=4083.6438, train_loss=8.314745

Batch 214230, train_perplexity=5942.8, train_loss=8.689936

Batch 214240, train_perplexity=5323.11, train_loss=8.579813

Batch 214250, train_perplexity=5685.8545, train_loss=8.645737

Batch 214260, train_perplexity=4639.4907, train_loss=8.44236
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 214270, train_perplexity=4756.1855, train_loss=8.467201

Batch 214280, train_perplexity=5185.3306, train_loss=8.553589

Batch 214290, train_perplexity=5153.405, train_loss=8.547413

Batch 214300, train_perplexity=4443.0464, train_loss=8.399096

Batch 214310, train_perplexity=5626.6265, train_loss=8.635265

Batch 214320, train_perplexity=5528.933, train_loss=8.61775

Batch 214330, train_perplexity=4663.312, train_loss=8.447481

Batch 214340, train_perplexity=4495.2524, train_loss=8.410777

Batch 214350, train_perplexity=5698.682, train_loss=8.64799

Batch 214360, train_perplexity=5206.3403, train_loss=8.557632

Batch 214370, train_perplexity=4979.5933, train_loss=8.5131035

Batch 214380, train_perplexity=5962.1465, train_loss=8.693186

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Batch 214390, train_perplexity=5117.6084, train_loss=8.540442

Batch 214400, train_perplexity=6633.2456, train_loss=8.7998495

Batch 214410, train_perplexity=5679.281, train_loss=8.64458

Batch 214420, train_perplexity=4975.558, train_loss=8.512293

Batch 214430, train_perplexity=5868.2676, train_loss=8.677315

Batch 214440, train_perplexity=6322.8447, train_loss=8.7519245

Batch 214450, train_perplexity=4088.5615, train_loss=8.3159485

Batch 214460, train_perplexity=4506.6445, train_loss=8.413308

Batch 214470, train_perplexity=4507.332, train_loss=8.413461

Batch 214480, train_perplexity=4909.541, train_loss=8.498936

Batch 214490, train_perplexity=5706.165, train_loss=8.6493025

Batch 214500, train_perplexity=6019.4097, train_loss=8.7027445

Batch 214510, train_perplexity=6851.1343, train_loss=8.83217

Batch 214520, train_perplexity=4413.3076, train_loss=8.39238

Batch 214530, train_perplexity=4486.601, train_loss=8.408851

Batch 214540, train_perplexity=5865.3413, train_loss=8.676816

Batch 214550, train_perplexity=5734.123, train_loss=8.65419

Batch 214560, train_perplexity=5480.275, train_loss=8.608911

Batch 214570, train_perplexity=7071.559, train_loss=8.863836

Batch 214580, train_perplexity=5612.098, train_loss=8.63268

Batch 214590, train_perplexity=4958.3447, train_loss=8.508827

Batch 214600, train_perplexity=4567.7017, train_loss=8.426765

Batch 214610, train_perplexity=5825.3, train_loss=8.669966

Batch 214620, train_perplexity=5266.9023, train_loss=8.569198

Batch 214630, train_perplexity=6274.2744, train_loss=8.744213

Batch 214640, train_perplexity=5594.5015, train_loss=8.6295395

Batch 214650, train_perplexity=4541.3496, train_loss=8.4209795

Batch 214660, train_perplexity=4430.9106, train_loss=8.39636

Batch 214670, train_perplexity=6620.8394, train_loss=8.797977

Batch 214680, train_perplexity=4536.575, train_loss=8.419928

Batch 214690, train_perplexity=5654.653, train_loss=8.640234

Batch 214700, train_perplexity=5207.5815, train_loss=8.557871

Batch 214710, train_perplexity=5504.2485, train_loss=8.613276

Batch 214720, train_perplexity=4935.7837, train_loss=8.504267

Batch 214730, train_perplexity=5821.6953, train_loss=8.669347

Batch 214740, train_perplexity=5137.5547, train_loss=8.5443325

Batch 214750, train_perplexity=5534.0293, train_loss=8.618671

Batch 214760, train_perplexity=4665.705, train_loss=8.447994

Batch 214770, train_perplexity=4631.728, train_loss=8.440685

Batch 214780, train_perplexity=4968.19, train_loss=8.510811

Batch 214790, train_perplexity=5398.214, train_loss=8.593823

Batch 214800, train_perplexity=5352.1963, train_loss=8.585262

Batch 214810, train_perplexity=5683.816, train_loss=8.645378

Batch 214820, train_perplexity=5035.294, train_loss=8.524227

Batch 214830, train_perplexity=6196.873, train_loss=8.7318

Batch 214840, train_perplexity=4857.3525, train_loss=8.488249

Batch 214850, train_perplexity=4723.3237, train_loss=8.460268

Batch 214860, train_perplexity=5802.877, train_loss=8.666109

Batch 214870, train_perplexity=4869.7183, train_loss=8.490791

Batch 214880, train_perplexity=4670.62, train_loss=8.449047

Batch 214890, train_perplexity=6267.672, train_loss=8.74316

Batch 214900, train_perplexity=5431.993, train_loss=8.600061

Batch 214910, train_perplexity=6184.8286, train_loss=8.729855

Batch 214920, train_perplexity=5772.0513, train_loss=8.660783

Batch 214930, train_perplexity=5352.4927, train_loss=8.585318

Batch 214940, train_perplexity=4208.526, train_loss=8.344868

Batch 214950, train_perplexity=6473.108, train_loss=8.775412

Batch 214960, train_perplexity=5699.6006, train_loss=8.648151

Batch 214970, train_perplexity=5145.459, train_loss=8.54587

Batch 214980, train_perplexity=4544.4126, train_loss=8.421654

Batch 214990, train_perplexity=4442.97, train_loss=8.399078

Batch 215000, train_perplexity=5293.823, train_loss=8.574296

Batch 215010, train_perplexity=4809.098, train_loss=8.478265

Batch 215020, train_perplexity=5487.6387, train_loss=8.610253

Batch 215030, train_perplexity=5811.1006, train_loss=8.667525

Batch 215040, train_perplexity=6409.7246, train_loss=8.765572

Batch 215050, train_perplexity=5028.988, train_loss=8.522974

Batch 215060, train_perplexity=6332.3916, train_loss=8.753433

Batch 215070, train_perplexity=5846.817, train_loss=8.673653

Batch 215080, train_perplexity=5721.9956, train_loss=8.652073

Batch 215090, train_perplexity=7203.8984, train_loss=8.882378

Batch 215100, train_perplexity=5266.9272, train_loss=8.569202

Batch 215110, train_perplexity=4950.256, train_loss=8.5071945

Batch 215120, train_perplexity=5658.1914, train_loss=8.64086

Batch 215130, train_perplexity=5598.1094, train_loss=8.630184

Batch 215140, train_perplexity=24644.652, train_loss=10.112315

Batch 215150, train_perplexity=7495.9575, train_loss=8.922119

Batch 215160, train_perplexity=4721.0044, train_loss=8.459777

Batch 215170, train_perplexity=4799.614, train_loss=8.476291

Batch 215180, train_perplexity=5868.514, train_loss=8.677357

Batch 215190, train_perplexity=5094.9443, train_loss=8.536004

Batch 215200, train_perplexity=5578.1616, train_loss=8.626615

Batch 215210, train_perplexity=5975.9565, train_loss=8.695499

Batch 215220, train_perplexity=4961.4287, train_loss=8.509449

Batch 215230, train_perplexity=4265.125, train_loss=8.358227

Batch 215240, train_perplexity=3825.3926, train_loss=8.249416

Batch 215250, train_perplexity=5495.6675, train_loss=8.611715

Batch 215260, train_perplexity=5481.0537, train_loss=8.609053

Batch 215270, train_perplexity=5688.219, train_loss=8.6461525

Batch 215280, train_perplexity=4455.64, train_loss=8.401926

Batch 215290, train_perplexity=5681.0195, train_loss=8.644886

Batch 215300, train_perplexity=5215.5537, train_loss=8.559401

Batch 215310, train_perplexity=5333.517, train_loss=8.581766

Batch 215320, train_perplexity=4651.75, train_loss=8.444999

Batch 215330, train_perplexity=5430.688, train_loss=8.599821

Batch 215340, train_perplexity=5526.803, train_loss=8.617365

Batch 215350, train_perplexity=5374.313, train_loss=8.589386

Batch 215360, train_perplexity=5652.097, train_loss=8.639782

Batch 215370, train_perplexity=6546.975, train_loss=8.786758

Batch 215380, train_perplexity=5709.66, train_loss=8.649915

Batch 215390, train_perplexity=5084.8867, train_loss=8.534028

Batch 215400, train_perplexity=5425.573, train_loss=8.598879

Batch 215410, train_perplexity=5428.3423, train_loss=8.599389

Batch 215420, train_perplexity=6923.021, train_loss=8.8426075

Batch 215430, train_perplexity=5555.625, train_loss=8.622566

Batch 215440, train_perplexity=5049.7397, train_loss=8.527092

Batch 215450, train_perplexity=5129.9023, train_loss=8.542842

Batch 215460, train_perplexity=5354.136, train_loss=8.585625

Batch 215470, train_perplexity=5355.6226, train_loss=8.585902

Batch 215480, train_perplexity=5796.6655, train_loss=8.665038

Batch 215490, train_perplexity=5630.5933, train_loss=8.63597

Batch 215500, train_perplexity=5550.319, train_loss=8.621611

Batch 215510, train_perplexity=5429.3623, train_loss=8.599577
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 215520, train_perplexity=5092.9478, train_loss=8.535612

Batch 215530, train_perplexity=5403.8438, train_loss=8.594866

Batch 215540, train_perplexity=5064.0244, train_loss=8.529917

Batch 215550, train_perplexity=4902.574, train_loss=8.497516

Batch 215560, train_perplexity=4971.953, train_loss=8.511568

Batch 215570, train_perplexity=4676.093, train_loss=8.450218

Batch 215580, train_perplexity=5364.701, train_loss=8.587596

Batch 215590, train_perplexity=5971.4785, train_loss=8.69475

Batch 215600, train_perplexity=5238.8237, train_loss=8.563852

Batch 215610, train_perplexity=5799.8174, train_loss=8.665582

Batch 215620, train_perplexity=5227.769, train_loss=8.56174

Batch 215630, train_perplexity=4803.373, train_loss=8.477074

Batch 215640, train_perplexity=5649.8984, train_loss=8.639393

Batch 215650, train_perplexity=5716.345, train_loss=8.651085

Batch 215660, train_perplexity=6953.604, train_loss=8.847015

Batch 215670, train_perplexity=5517.377, train_loss=8.615658

Batch 215680, train_perplexity=4595.118, train_loss=8.43275

Batch 215690, train_perplexity=5251.0234, train_loss=8.566178

Batch 215700, train_perplexity=4928.3096, train_loss=8.502751

Batch 215710, train_perplexity=4571.4014, train_loss=8.427575

Batch 215720, train_perplexity=5256.931, train_loss=8.567303

Batch 215730, train_perplexity=5271.571, train_loss=8.570084

Batch 215740, train_perplexity=4294.4424, train_loss=8.365077

Batch 215750, train_perplexity=5789.3784, train_loss=8.66378

Batch 215760, train_perplexity=6100.398, train_loss=8.716109

Batch 215770, train_perplexity=5879.6, train_loss=8.679244

Batch 215780, train_perplexity=5623.44, train_loss=8.634699

Batch 215790, train_perplexity=6536.507, train_loss=8.785158

Batch 215800, train_perplexity=6415.131, train_loss=8.766415

Batch 215810, train_perplexity=4676.7485, train_loss=8.450358

Batch 215820, train_perplexity=5662.883, train_loss=8.641688

Batch 215830, train_perplexity=6505.487, train_loss=8.780401

Batch 215840, train_perplexity=5146.5093, train_loss=8.546074

Batch 215850, train_perplexity=6197.7, train_loss=8.731934

Batch 215860, train_perplexity=5722.563, train_loss=8.652172

Batch 215870, train_perplexity=4849.2437, train_loss=8.486578

Batch 215880, train_perplexity=5354.5654, train_loss=8.585705

Batch 215890, train_perplexity=5418.717, train_loss=8.597614

Batch 215900, train_perplexity=5150.2554, train_loss=8.546802

Batch 215910, train_perplexity=5747.5474, train_loss=8.656528

Batch 215920, train_perplexity=4386.0264, train_loss=8.386179

Batch 215930, train_perplexity=5509.1797, train_loss=8.614171

Batch 215940, train_perplexity=6050.0327, train_loss=8.707819

Batch 215950, train_perplexity=5037.196, train_loss=8.524605

Batch 215960, train_perplexity=5738.707, train_loss=8.654989

Batch 215970, train_perplexity=5724.5063, train_loss=8.652512

Batch 215980, train_perplexity=4866.3477, train_loss=8.490099

Batch 215990, train_perplexity=5102.5156, train_loss=8.537489

Batch 216000, train_perplexity=4602.6133, train_loss=8.43438

Batch 216010, train_perplexity=4766.966, train_loss=8.469465

Batch 216020, train_perplexity=5781.8525, train_loss=8.662479

Batch 216030, train_perplexity=5518.135, train_loss=8.615795

Batch 216040, train_perplexity=4442.788, train_loss=8.399037

Batch 216050, train_perplexity=6577.2144, train_loss=8.791367

Batch 216060, train_perplexity=5628.29, train_loss=8.635561

Batch 216070, train_perplexity=6392.461, train_loss=8.762875

Batch 216080, train_perplexity=5138.8384, train_loss=8.544582

Batch 216090, train_perplexity=4924.969, train_loss=8.502073

Batch 216100, train_perplexity=4756.5166, train_loss=8.467271

Batch 216110, train_perplexity=6320.9277, train_loss=8.751621

Batch 216120, train_perplexity=5443.942, train_loss=8.602259

Batch 216130, train_perplexity=5040.1367, train_loss=8.525188

Batch 216140, train_perplexity=4301.0825, train_loss=8.366622

Batch 216150, train_perplexity=6421.4844, train_loss=8.767405

Batch 216160, train_perplexity=5072.149, train_loss=8.53152

Batch 216170, train_perplexity=4596.7266, train_loss=8.4331

Batch 216180, train_perplexity=5131.248, train_loss=8.543104

Batch 216190, train_perplexity=5554.942, train_loss=8.622443

Batch 216200, train_perplexity=5354.851, train_loss=8.585758

Batch 216210, train_perplexity=4492.3, train_loss=8.41012

Batch 216220, train_perplexity=5608.733, train_loss=8.63208

Batch 216230, train_perplexity=4088.3706, train_loss=8.315902

Batch 216240, train_perplexity=6552.9346, train_loss=8.787668

Batch 216250, train_perplexity=5424.854, train_loss=8.598746

Batch 216260, train_perplexity=4930.2183, train_loss=8.503139

Batch 216270, train_perplexity=5609.1445, train_loss=8.6321535

Batch 216280, train_perplexity=4735.1855, train_loss=8.462776

Batch 216290, train_perplexity=5870.434, train_loss=8.677684

Batch 216300, train_perplexity=4873.212, train_loss=8.4915085

Batch 216310, train_perplexity=5453.992, train_loss=8.604103

Batch 216320, train_perplexity=5719.5186, train_loss=8.65164

Batch 216330, train_perplexity=5817.5605, train_loss=8.668636

Batch 216340, train_perplexity=5647.356, train_loss=8.638943

Batch 216350, train_perplexity=5000.017, train_loss=8.517197

Batch 216360, train_perplexity=3986.6008, train_loss=8.290694

Batch 216370, train_perplexity=4901.354, train_loss=8.497267

Batch 216380, train_perplexity=5100.9487, train_loss=8.537182

Batch 216390, train_perplexity=5454.179, train_loss=8.604137

Batch 216400, train_perplexity=5491.701, train_loss=8.610993

Batch 216410, train_perplexity=5019.1226, train_loss=8.52101

Batch 216420, train_perplexity=4947.7354, train_loss=8.506685

Batch 216430, train_perplexity=6736.4126, train_loss=8.815283

Batch 216440, train_perplexity=5518.9136, train_loss=8.615936

Batch 216450, train_perplexity=5573.5566, train_loss=8.625789

Batch 216460, train_perplexity=5117.803, train_loss=8.540481

Batch 216470, train_perplexity=3525.33, train_loss=8.167729

Batch 216480, train_perplexity=5957.44, train_loss=8.692396

Batch 216490, train_perplexity=5238.774, train_loss=8.563843

Batch 216500, train_perplexity=5040.1606, train_loss=8.525193

Batch 216510, train_perplexity=5003.3706, train_loss=8.517867

Batch 216520, train_perplexity=5478.54, train_loss=8.608594

Batch 216530, train_perplexity=5331.7065, train_loss=8.581427

Batch 216540, train_perplexity=4844.529, train_loss=8.485605

Batch 216550, train_perplexity=5572.2812, train_loss=8.62556

Batch 216560, train_perplexity=4600.696, train_loss=8.433963

Batch 216570, train_perplexity=5308.2314, train_loss=8.577014

Batch 216580, train_perplexity=5436.678, train_loss=8.600924

Batch 216590, train_perplexity=4944.2354, train_loss=8.505978

Batch 216600, train_perplexity=5115.842, train_loss=8.540097

Batch 216610, train_perplexity=5296.9644, train_loss=8.574889

Batch 216620, train_perplexity=6277.686, train_loss=8.744757

Batch 216630, train_perplexity=5157.697, train_loss=8.548245

Batch 216640, train_perplexity=5661.317, train_loss=8.641412

Batch 216650, train_perplexity=4261.7017, train_loss=8.357424

Batch 216660, train_perplexity=5500.544, train_loss=8.612602

Batch 216670, train_perplexity=5215.2754, train_loss=8.559347

Batch 216680, train_perplexity=5350.9717, train_loss=8.585033

Batch 216690, train_perplexity=4796.571, train_loss=8.4756565

Batch 216700, train_perplexity=5780.1104, train_loss=8.662178

Batch 216710, train_perplexity=5052.389, train_loss=8.5276165

Batch 216720, train_perplexity=5467.7563, train_loss=8.606624

Batch 216730, train_perplexity=5421.3735, train_loss=8.5981045

Batch 216740, train_perplexity=4825.3564, train_loss=8.48164

Batch 216750, train_perplexity=5883.0273, train_loss=8.679827

Batch 216760, train_perplexity=5312.46, train_loss=8.57781

Batch 216770, train_perplexity=5789.9526, train_loss=8.663879

Batch 216780, train_perplexity=5126.1904, train_loss=8.542118

Batch 216790, train_perplexity=5627.979, train_loss=8.635506

Batch 216800, train_perplexity=4892.1587, train_loss=8.495389

Batch 216810, train_perplexity=4596.8496, train_loss=8.433126

Batch 216820, train_perplexity=5418.975, train_loss=8.597662

Batch 216830, train_perplexity=4901.597, train_loss=8.497316
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 216840, train_perplexity=4519.9746, train_loss=8.416262

Batch 216850, train_perplexity=4754.299, train_loss=8.4668045

Batch 216860, train_perplexity=5208.386, train_loss=8.558025

Batch 216870, train_perplexity=5033.762, train_loss=8.523923

Batch 216880, train_perplexity=5665.2056, train_loss=8.642098

Batch 216890, train_perplexity=5740.229, train_loss=8.655254

Batch 216900, train_perplexity=4919.965, train_loss=8.501057

Batch 216910, train_perplexity=5006.8027, train_loss=8.518553

Batch 216920, train_perplexity=5337.9233, train_loss=8.582592

Batch 216930, train_perplexity=5828.9565, train_loss=8.670593

Batch 216940, train_perplexity=4641.465, train_loss=8.442785

Batch 216950, train_perplexity=4879.704, train_loss=8.49284

Batch 216960, train_perplexity=5883.145, train_loss=8.679847

Batch 216970, train_perplexity=5178.9155, train_loss=8.552351

Batch 216980, train_perplexity=5206.8022, train_loss=8.557721

Batch 216990, train_perplexity=5226.767, train_loss=8.561548

Batch 217000, train_perplexity=4640.4155, train_loss=8.442559

Batch 217010, train_perplexity=6105.3623, train_loss=8.716923

Batch 217020, train_perplexity=5395.3574, train_loss=8.593294

Batch 217030, train_perplexity=5763.146, train_loss=8.659239

Batch 217040, train_perplexity=6671.9336, train_loss=8.805665

Batch 217050, train_perplexity=5217.3247, train_loss=8.55974

Batch 217060, train_perplexity=5977.9287, train_loss=8.695829

Batch 217070, train_perplexity=4590.02, train_loss=8.43164

Batch 217080, train_perplexity=5101.961, train_loss=8.53738

Batch 217090, train_perplexity=4696.8486, train_loss=8.454647

Batch 217100, train_perplexity=5103.6006, train_loss=8.537702

Batch 217110, train_perplexity=6279.65, train_loss=8.7450695

Batch 217120, train_perplexity=3997.5312, train_loss=8.293432

Batch 217130, train_perplexity=6743.9463, train_loss=8.816401

Batch 217140, train_perplexity=5425.527, train_loss=8.59887

Batch 217150, train_perplexity=6435.499, train_loss=8.769585

Batch 217160, train_perplexity=4558.828, train_loss=8.424821

Batch 217170, train_perplexity=6013.643, train_loss=8.701786

Batch 217180, train_perplexity=5216.489, train_loss=8.55958

Batch 217190, train_perplexity=5634.0474, train_loss=8.636583

Batch 217200, train_perplexity=4377.8105, train_loss=8.384304

Batch 217210, train_perplexity=4567.475, train_loss=8.426716

Batch 217220, train_perplexity=5163.4995, train_loss=8.54937

Batch 217230, train_perplexity=5954.918, train_loss=8.691973

Batch 217240, train_perplexity=5802.3457, train_loss=8.666018

Batch 217250, train_perplexity=5630.4053, train_loss=8.635937

Batch 217260, train_perplexity=4331.25, train_loss=8.373611

Batch 217270, train_perplexity=5405.174, train_loss=8.595112

Batch 217280, train_perplexity=4766.2065, train_loss=8.469306

Batch 217290, train_perplexity=4462.2866, train_loss=8.403417

Batch 217300, train_perplexity=5370.1577, train_loss=8.588613

Batch 217310, train_perplexity=4985.129, train_loss=8.5142145

Batch 217320, train_perplexity=5734.8555, train_loss=8.654318

Batch 217330, train_perplexity=5428.306, train_loss=8.599382

Batch 217340, train_perplexity=6628.408, train_loss=8.79912

Batch 217350, train_perplexity=5037.705, train_loss=8.524706

Batch 217360, train_perplexity=5690.7583, train_loss=8.646599

Batch 217370, train_perplexity=5118.56, train_loss=8.540628

Batch 217380, train_perplexity=4520.578, train_loss=8.416395

Batch 217390, train_perplexity=4998.3584, train_loss=8.516865

Batch 217400, train_perplexity=6023.2344, train_loss=8.70338

Batch 217410, train_perplexity=5175.721, train_loss=8.551734

Batch 217420, train_perplexity=6650.8486, train_loss=8.8025

Batch 217430, train_perplexity=5652.0757, train_loss=8.639778

Batch 217440, train_perplexity=5522.7466, train_loss=8.616631

Batch 217450, train_perplexity=5728.5806, train_loss=8.653223

Batch 217460, train_perplexity=5318.888, train_loss=8.57902

Batch 217470, train_perplexity=5999.786, train_loss=8.699479

Batch 217480, train_perplexity=4834.209, train_loss=8.483473

Batch 217490, train_perplexity=6641.329, train_loss=8.801067

Batch 217500, train_perplexity=6421.974, train_loss=8.767481

Batch 217510, train_perplexity=4750.854, train_loss=8.46608

Batch 217520, train_perplexity=5529.2705, train_loss=8.617811

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Batch 217530, train_perplexity=5028.729, train_loss=8.5229225

Batch 217540, train_perplexity=5322.0137, train_loss=8.579607

Batch 217550, train_perplexity=5169.0176, train_loss=8.550438

Batch 217560, train_perplexity=5504.9624, train_loss=8.613405

Batch 217570, train_perplexity=7332.586, train_loss=8.900084

Batch 217580, train_perplexity=5257.4224, train_loss=8.567396

Batch 217590, train_perplexity=5695.895, train_loss=8.647501

Batch 217600, train_perplexity=4801.651, train_loss=8.476715

Batch 217610, train_perplexity=4820.1685, train_loss=8.480564

Batch 217620, train_perplexity=5492.9585, train_loss=8.611222

Batch 217630, train_perplexity=5101.0557, train_loss=8.537203

Batch 217640, train_perplexity=4925.984, train_loss=8.502279

Batch 217650, train_perplexity=5073.0103, train_loss=8.53169

Batch 217660, train_perplexity=6322.073, train_loss=8.751802

Batch 217670, train_perplexity=5248.966, train_loss=8.565786

Batch 217680, train_perplexity=5324.054, train_loss=8.57999

Batch 217690, train_perplexity=4360.3525, train_loss=8.380308

Batch 217700, train_perplexity=5348.6655, train_loss=8.584602

Batch 217710, train_perplexity=4580.4565, train_loss=8.429554

Batch 217720, train_perplexity=5049.942, train_loss=8.527132

Batch 217730, train_perplexity=5747.262, train_loss=8.656479

Batch 217740, train_perplexity=5572.887, train_loss=8.625669

Batch 217750, train_perplexity=5199.9043, train_loss=8.556396

Batch 217760, train_perplexity=5680.207, train_loss=8.644743

Batch 217770, train_perplexity=4602.8857, train_loss=8.434439

Batch 217780, train_perplexity=5034.698, train_loss=8.524109

Batch 217790, train_perplexity=5614.3945, train_loss=8.633089

Batch 217800, train_perplexity=6774.3325, train_loss=8.820896

Batch 217810, train_perplexity=5818.1157, train_loss=8.668732

Batch 217820, train_perplexity=4877.5127, train_loss=8.492391

Batch 217830, train_perplexity=6037.249, train_loss=8.705704

Batch 217840, train_perplexity=4941.85, train_loss=8.505495

Batch 217850, train_perplexity=4796.484, train_loss=8.475638

Batch 217860, train_perplexity=5194.9824, train_loss=8.555449

Batch 217870, train_perplexity=4809.809, train_loss=8.478413

Batch 217880, train_perplexity=5337.74, train_loss=8.582558

Batch 217890, train_perplexity=5317.0166, train_loss=8.578668

Batch 217900, train_perplexity=5260.211, train_loss=8.567926

Batch 217910, train_perplexity=5746.188, train_loss=8.656292

Batch 217920, train_perplexity=5913.6387, train_loss=8.685017

Batch 217930, train_perplexity=5481.7437, train_loss=8.609179

Batch 217940, train_perplexity=4878.6616, train_loss=8.492626

Batch 217950, train_perplexity=5419.239, train_loss=8.597711

Batch 217960, train_perplexity=5889.382, train_loss=8.680906

Batch 217970, train_perplexity=5079.434, train_loss=8.532955

Batch 217980, train_perplexity=4351.695, train_loss=8.378321

Batch 217990, train_perplexity=6362.804, train_loss=8.7582245

Batch 218000, train_perplexity=4512.713, train_loss=8.414654

Batch 218010, train_perplexity=6019.7427, train_loss=8.7028

Batch 218020, train_perplexity=4969.593, train_loss=8.511093

Batch 218030, train_perplexity=5781.312, train_loss=8.662386

Batch 218040, train_perplexity=6095.984, train_loss=8.715385

Batch 218050, train_perplexity=5345.0503, train_loss=8.583926

Batch 218060, train_perplexity=4947.632, train_loss=8.506664

Batch 218070, train_perplexity=6699.101, train_loss=8.809729

Batch 218080, train_perplexity=6903.486, train_loss=8.839782

Batch 218090, train_perplexity=5450.1025, train_loss=8.60339
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 218100, train_perplexity=5413.0146, train_loss=8.596561

Batch 218110, train_perplexity=6131.515, train_loss=8.721197

Batch 218120, train_perplexity=5510.4463, train_loss=8.614401

Batch 218130, train_perplexity=5898.915, train_loss=8.682524

Batch 218140, train_perplexity=5673.0396, train_loss=8.64348

Batch 218150, train_perplexity=6097.1235, train_loss=8.715572

Batch 218160, train_perplexity=6263.7163, train_loss=8.742529

Batch 218170, train_perplexity=4539.8037, train_loss=8.420639

Batch 218180, train_perplexity=5163.4307, train_loss=8.549356

Batch 218190, train_perplexity=5065.2705, train_loss=8.530163

Batch 218200, train_perplexity=5528.132, train_loss=8.617605

Batch 218210, train_perplexity=5263.603, train_loss=8.568571

Batch 218220, train_perplexity=4567.414, train_loss=8.4267025

Batch 218230, train_perplexity=4355.3276, train_loss=8.379155

Batch 218240, train_perplexity=5482.7476, train_loss=8.609362

Batch 218250, train_perplexity=4696.956, train_loss=8.45467

Batch 218260, train_perplexity=4050.0798, train_loss=8.306492

Batch 218270, train_perplexity=5304.8657, train_loss=8.57638

Batch 218280, train_perplexity=4984.592, train_loss=8.514107

Batch 218290, train_perplexity=6290.103, train_loss=8.746733

Batch 218300, train_perplexity=6770.8447, train_loss=8.820381

Batch 218310, train_perplexity=5950.666, train_loss=8.691258

Batch 218320, train_perplexity=4559.8066, train_loss=8.425035

Batch 218330, train_perplexity=5661.981, train_loss=8.641529

Batch 218340, train_perplexity=4794.9746, train_loss=8.475324

Batch 218350, train_perplexity=4539.284, train_loss=8.420525

Batch 218360, train_perplexity=5986.315, train_loss=8.697231

Batch 218370, train_perplexity=4614.8853, train_loss=8.437042

Batch 218380, train_perplexity=5177.089, train_loss=8.551998

Batch 218390, train_perplexity=6069.3, train_loss=8.710999

Batch 218400, train_perplexity=4629.67, train_loss=8.440241

Batch 218410, train_perplexity=5726.7505, train_loss=8.652904

Batch 218420, train_perplexity=5422.956, train_loss=8.598396

Batch 218430, train_perplexity=4773.1753, train_loss=8.470767

Batch 218440, train_perplexity=6189.307, train_loss=8.730578

Batch 218450, train_perplexity=5319.279, train_loss=8.579093

Batch 218460, train_perplexity=5408.138, train_loss=8.59566

Batch 218470, train_perplexity=5360.4565, train_loss=8.586804

Batch 218480, train_perplexity=5187.2446, train_loss=8.553958

Batch 218490, train_perplexity=6114.3477, train_loss=8.718393

Batch 218500, train_perplexity=6416.2075, train_loss=8.7665825

Batch 218510, train_perplexity=5348.4004, train_loss=8.584553

Batch 218520, train_perplexity=6526.0674, train_loss=8.78356

Batch 218530, train_perplexity=5847.1123, train_loss=8.673703

Batch 218540, train_perplexity=5738.9424, train_loss=8.65503

Batch 218550, train_perplexity=4944.8013, train_loss=8.506092

Batch 218560, train_perplexity=4685.2573, train_loss=8.452176

Batch 218570, train_perplexity=5919.7554, train_loss=8.68605

Batch 218580, train_perplexity=5145.9053, train_loss=8.545957

Batch 218590, train_perplexity=5009.5728, train_loss=8.519106

Batch 218600, train_perplexity=4113.604, train_loss=8.322055

Batch 218610, train_perplexity=5466.4478, train_loss=8.606384

Batch 218620, train_perplexity=6333.69, train_loss=8.753638

Batch 218630, train_perplexity=4609.3516, train_loss=8.4358425

Batch 218640, train_perplexity=4611.612, train_loss=8.436333

Batch 218650, train_perplexity=5370.926, train_loss=8.588756

Batch 218660, train_perplexity=5579.8804, train_loss=8.626923

Batch 218670, train_perplexity=4937.662, train_loss=8.504647

Batch 218680, train_perplexity=4970.8726, train_loss=8.511351

Batch 218690, train_perplexity=5123.1016, train_loss=8.541515

Batch 218700, train_perplexity=5779.3496, train_loss=8.662046

Batch 218710, train_perplexity=5525.676, train_loss=8.617161

Batch 218720, train_perplexity=5984.477, train_loss=8.696924

Batch 218730, train_perplexity=4692.7207, train_loss=8.453768

Batch 218740, train_perplexity=5081.5757, train_loss=8.533377

Batch 218750, train_perplexity=4623.4004, train_loss=8.438886

Batch 218760, train_perplexity=5316.444, train_loss=8.57856

Batch 218770, train_perplexity=5087.38, train_loss=8.534518

Batch 218780, train_perplexity=4793.42, train_loss=8.474999

Batch 218790, train_perplexity=5807.699, train_loss=8.66694

Batch 218800, train_perplexity=5881.148, train_loss=8.679507

Batch 218810, train_perplexity=4412.0957, train_loss=8.392105

Batch 218820, train_perplexity=5315.4097, train_loss=8.578365

Batch 218830, train_perplexity=5341.0654, train_loss=8.58318

Batch 218840, train_perplexity=5042.377, train_loss=8.525633

Batch 218850, train_perplexity=4939.711, train_loss=8.505062

Batch 218860, train_perplexity=5197.272, train_loss=8.555889

Batch 218870, train_perplexity=5431.33, train_loss=8.599939

Batch 218880, train_perplexity=5911.564, train_loss=8.684666

Batch 218890, train_perplexity=4820.536, train_loss=8.48064

Batch 218900, train_perplexity=5374.9946, train_loss=8.589513

Batch 218910, train_perplexity=4716.383, train_loss=8.458797

Batch 218920, train_perplexity=5750.2666, train_loss=8.6570015

Batch 218930, train_perplexity=5506.795, train_loss=8.613738

Batch 218940, train_perplexity=4951.993, train_loss=8.507545

Batch 218950, train_perplexity=6250.439, train_loss=8.740407

Batch 218960, train_perplexity=6076.377, train_loss=8.712164

Batch 218970, train_perplexity=4256.762, train_loss=8.356264

Batch 218980, train_perplexity=5741.395, train_loss=8.6554575

Batch 218990, train_perplexity=4916.9487, train_loss=8.500443

Batch 219000, train_perplexity=5503.5557, train_loss=8.61315

Batch 219010, train_perplexity=4526.946, train_loss=8.417803

Batch 219020, train_perplexity=4684.3145, train_loss=8.451975

Batch 219030, train_perplexity=6278.105, train_loss=8.744823

Batch 219040, train_perplexity=5790.2285, train_loss=8.663927

Batch 219050, train_perplexity=6217.6094, train_loss=8.735141

Batch 219060, train_perplexity=5779.261, train_loss=8.662031

Batch 219070, train_perplexity=6449.606, train_loss=8.771774

Batch 219080, train_perplexity=5440.444, train_loss=8.601616

Batch 219090, train_perplexity=5936.105, train_loss=8.688808

Batch 219100, train_perplexity=5319.7046, train_loss=8.579173

Batch 219110, train_perplexity=6419.2983, train_loss=8.767064

Batch 219120, train_perplexity=5619.923, train_loss=8.634073

Batch 219130, train_perplexity=4864.306, train_loss=8.489679

Batch 219140, train_perplexity=4828.62, train_loss=8.482316

Batch 219150, train_perplexity=4275.196, train_loss=8.360585

Batch 219160, train_perplexity=5170.5166, train_loss=8.550728

Batch 219170, train_perplexity=5728.7773, train_loss=8.653257

Batch 219180, train_perplexity=4800.451, train_loss=8.476465

Batch 219190, train_perplexity=5228.4023, train_loss=8.561861

Batch 219200, train_perplexity=5478.7856, train_loss=8.608639

Batch 219210, train_perplexity=5314.72, train_loss=8.578236

Batch 219220, train_perplexity=4935.8496, train_loss=8.50428

Batch 219230, train_perplexity=5395.8105, train_loss=8.593378

Batch 219240, train_perplexity=4798.8906, train_loss=8.47614

Batch 219250, train_perplexity=5433.392, train_loss=8.600319

Batch 219260, train_perplexity=4823.309, train_loss=8.4812155

Batch 219270, train_perplexity=5195.488, train_loss=8.555546

Batch 219280, train_perplexity=5849.0864, train_loss=8.674041

Batch 219290, train_perplexity=4452.4673, train_loss=8.401214

Batch 219300, train_perplexity=4921.3633, train_loss=8.501341

Batch 219310, train_perplexity=4643.6343, train_loss=8.443253

Batch 219320, train_perplexity=5024.7593, train_loss=8.522133

Batch 219330, train_perplexity=6439.035, train_loss=8.770134

Batch 219340, train_perplexity=4696.105, train_loss=8.454489

Batch 219350, train_perplexity=6660.103, train_loss=8.80389

Batch 219360, train_perplexity=7434.808, train_loss=8.913928

Batch 219370, train_perplexity=5089.1367, train_loss=8.534863

Batch 219380, train_perplexity=6017.705, train_loss=8.702461

Batch 219390, train_perplexity=5607.9946, train_loss=8.631948

Batch 219400, train_perplexity=6432.7993, train_loss=8.769165

Batch 219410, train_perplexity=4652.473, train_loss=8.445154
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 219420, train_perplexity=6204.9863, train_loss=8.7331085

Batch 219430, train_perplexity=4953.325, train_loss=8.507814

Batch 219440, train_perplexity=6584.149, train_loss=8.79242

Batch 219450, train_perplexity=6346.6836, train_loss=8.755688

Batch 219460, train_perplexity=4829.113, train_loss=8.482418

Batch 219470, train_perplexity=5043.483, train_loss=8.525852

Batch 219480, train_perplexity=6003.8154, train_loss=8.7001505

Batch 219490, train_perplexity=5634.7456, train_loss=8.636707

Batch 219500, train_perplexity=5227.6494, train_loss=8.561717

Batch 219510, train_perplexity=5521.7983, train_loss=8.616459

Batch 219520, train_perplexity=5152.8003, train_loss=8.547296

Batch 219530, train_perplexity=5491.853, train_loss=8.611021

Batch 219540, train_perplexity=5157.584, train_loss=8.5482235

Batch 219550, train_perplexity=5646.085, train_loss=8.638718

Batch 219560, train_perplexity=5989.804, train_loss=8.697814

Batch 219570, train_perplexity=6289.911, train_loss=8.746702

Batch 219580, train_perplexity=6040.9004, train_loss=8.706308

Batch 219590, train_perplexity=6042.3926, train_loss=8.706555

Batch 219600, train_perplexity=4783.387, train_loss=8.472904

Batch 219610, train_perplexity=7000.0464, train_loss=8.853672

Batch 219620, train_perplexity=6357.7334, train_loss=8.757427

Batch 219630, train_perplexity=5528.564, train_loss=8.617683

Batch 219640, train_perplexity=6534.4746, train_loss=8.784847

Batch 219650, train_perplexity=6048.989, train_loss=8.707646

Batch 219660, train_perplexity=5535.169, train_loss=8.618877

Batch 219670, train_perplexity=4544.369, train_loss=8.421644

Batch 219680, train_perplexity=5367.101, train_loss=8.588043

Batch 219690, train_perplexity=4174.919, train_loss=8.33685

Batch 219700, train_perplexity=5437.819, train_loss=8.601133

Batch 219710, train_perplexity=5137.6724, train_loss=8.544355

Batch 219720, train_perplexity=5064.536, train_loss=8.530018

Batch 219730, train_perplexity=5043.6416, train_loss=8.525884

Batch 219740, train_perplexity=5323.7397, train_loss=8.579931

Batch 219750, train_perplexity=5252.376, train_loss=8.566436

Batch 219760, train_perplexity=4572.8403, train_loss=8.42789

Batch 219770, train_perplexity=4924.3633, train_loss=8.50195

Batch 219780, train_perplexity=6101.9805, train_loss=8.716369

Batch 219790, train_perplexity=5654.782, train_loss=8.640257

Batch 219800, train_perplexity=6772.3174, train_loss=8.820599

Batch 219810, train_perplexity=5674.901, train_loss=8.643808

Batch 219820, train_perplexity=5434.781, train_loss=8.6005745

Batch 219830, train_perplexity=4814.242, train_loss=8.479334

Batch 219840, train_perplexity=5240.9727, train_loss=8.564262

Batch 219850, train_perplexity=4604.4443, train_loss=8.434777

Batch 219860, train_perplexity=5119.571, train_loss=8.540826

Batch 219870, train_perplexity=5460.461, train_loss=8.6052885

Batch 219880, train_perplexity=4927.7314, train_loss=8.502634

Batch 219890, train_perplexity=6425.809, train_loss=8.768078

Batch 219900, train_perplexity=4827.879, train_loss=8.482162

Batch 219910, train_perplexity=4894.87, train_loss=8.495943

Batch 219920, train_perplexity=5624.148, train_loss=8.634825

Batch 219930, train_perplexity=5594.8003, train_loss=8.629593

Batch 219940, train_perplexity=4354.78, train_loss=8.379029

Batch 219950, train_perplexity=5458.847, train_loss=8.604993

Batch 219960, train_perplexity=31585.535, train_loss=10.360455

Batch 219970, train_perplexity=6087.49, train_loss=8.713991

Batch 219980, train_perplexity=6644.4204, train_loss=8.801533

Batch 219990, train_perplexity=5345.2285, train_loss=8.58396

Batch 220000, train_perplexity=6218.3745, train_loss=8.735264

Batch 220010, train_perplexity=5560.5547, train_loss=8.623453

Batch 220020, train_perplexity=4870.294, train_loss=8.49091

Batch 220030, train_perplexity=4970.55, train_loss=8.511286

Batch 220040, train_perplexity=5315.9517, train_loss=8.578467

Batch 220050, train_perplexity=5932.7656, train_loss=8.688246

Batch 220060, train_perplexity=5484.4683, train_loss=8.609675

Batch 220070, train_perplexity=5772.7837, train_loss=8.66091

Batch 220080, train_perplexity=5630.2495, train_loss=8.635909

Batch 220090, train_perplexity=5462.206, train_loss=8.605608

Batch 220100, train_perplexity=5890.404, train_loss=8.68108

Batch 220110, train_perplexity=4574.668, train_loss=8.428289

Batch 220120, train_perplexity=6300.4834, train_loss=8.748382

Batch 220130, train_perplexity=4695.8633, train_loss=8.454437

Batch 220140, train_perplexity=5446.3823, train_loss=8.602707

Batch 220150, train_perplexity=4921.983, train_loss=8.501467

Batch 220160, train_perplexity=5980.837, train_loss=8.696316

Batch 220170, train_perplexity=6543.517, train_loss=8.78623

Batch 220180, train_perplexity=5287.723, train_loss=8.573143

Batch 220190, train_perplexity=5520.8506, train_loss=8.616287

Batch 220200, train_perplexity=4673.6587, train_loss=8.4496975

Batch 220210, train_perplexity=4747.303, train_loss=8.465332

Batch 220220, train_perplexity=6408.6426, train_loss=8.765403

Batch 220230, train_perplexity=4766.561, train_loss=8.46938

Batch 220240, train_perplexity=4818.068, train_loss=8.480128

Batch 220250, train_perplexity=5484.7036, train_loss=8.609718

Batch 220260, train_perplexity=5571.0005, train_loss=8.62533

Batch 220270, train_perplexity=5094.318, train_loss=8.535881

Batch 220280, train_perplexity=4579.7974, train_loss=8.42941

Batch 220290, train_perplexity=7984.1245, train_loss=8.98521

Batch 220300, train_perplexity=4947.9995, train_loss=8.506739

Batch 220310, train_perplexity=5258.0645, train_loss=8.567518

Batch 220320, train_perplexity=5317.204, train_loss=8.578703

Batch 220330, train_perplexity=5364.0513, train_loss=8.587475

Batch 220340, train_perplexity=6092.7056, train_loss=8.714848

Batch 220350, train_perplexity=4139.6133, train_loss=8.328358

Batch 220360, train_perplexity=7204.036, train_loss=8.882397

Batch 220370, train_perplexity=5304.001, train_loss=8.576217

Batch 220380, train_perplexity=5194.294, train_loss=8.555316

Batch 220390, train_perplexity=4594.7104, train_loss=8.432661

Batch 220400, train_perplexity=5796.754, train_loss=8.665053

Batch 220410, train_perplexity=5017.0024, train_loss=8.520588

Batch 220420, train_perplexity=4546.9487, train_loss=8.422212

Batch 220430, train_perplexity=5429.352, train_loss=8.599575

Batch 220440, train_perplexity=4920.8145, train_loss=8.501229

Batch 220450, train_perplexity=5979.8784, train_loss=8.696156

Batch 220460, train_perplexity=5534.2876, train_loss=8.618718

Batch 220470, train_perplexity=6386.3555, train_loss=8.761919

Batch 220480, train_perplexity=4541.3276, train_loss=8.420975

Batch 220490, train_perplexity=4855.315, train_loss=8.487829

Batch 220500, train_perplexity=5582.6963, train_loss=8.627427

Batch 220510, train_perplexity=4662.0576, train_loss=8.447212

Batch 220520, train_perplexity=5744.117, train_loss=8.655931

Batch 220530, train_perplexity=4950.0054, train_loss=8.507144

Batch 220540, train_perplexity=4695.5767, train_loss=8.454376

Batch 220550, train_perplexity=4890.447, train_loss=8.495039

Batch 220560, train_perplexity=5905.2305, train_loss=8.683594

Batch 220570, train_perplexity=5781.169, train_loss=8.662361

Batch 220580, train_perplexity=4810.327, train_loss=8.47852

Batch 220590, train_perplexity=6133.8076, train_loss=8.721571

Batch 220600, train_perplexity=5470.6045, train_loss=8.607144

Batch 220610, train_perplexity=4878.922, train_loss=8.49268

Batch 220620, train_perplexity=4858.979, train_loss=8.488584

Batch 220630, train_perplexity=6049.606, train_loss=8.707748

Batch 220640, train_perplexity=5998.15, train_loss=8.699206

Batch 220650, train_perplexity=5214.3003, train_loss=8.55916

Batch 220660, train_perplexity=4665.5093, train_loss=8.447952

Batch 220670, train_perplexity=5260.151, train_loss=8.567915

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305446 sentences.
Finished loading
Batch 220680, train_perplexity=5766.918, train_loss=8.659893

Batch 220690, train_perplexity=4988.453, train_loss=8.514881

Batch 220700, train_perplexity=4762.213, train_loss=8.468468

Batch 220710, train_perplexity=4790.902, train_loss=8.474474

Batch 220720, train_perplexity=4889.8643, train_loss=8.49492

Batch 220730, train_perplexity=4864.2876, train_loss=8.4896755

Batch 220740, train_perplexity=5281.645, train_loss=8.571993

Batch 220750, train_perplexity=5593.5835, train_loss=8.629375

Batch 220760, train_perplexity=5749.905, train_loss=8.656939

Batch 220770, train_perplexity=6125.5884, train_loss=8.72023

Batch 220780, train_perplexity=4626.3335, train_loss=8.43952

Batch 220790, train_perplexity=6516.757, train_loss=8.782132

Batch 220800, train_perplexity=4775.784, train_loss=8.471313

Batch 220810, train_perplexity=4403.294, train_loss=8.390108

Batch 220820, train_perplexity=4337.008, train_loss=8.37494

Batch 220830, train_perplexity=5271.9526, train_loss=8.570156

Batch 220840, train_perplexity=6261.4106, train_loss=8.742161

Batch 220850, train_perplexity=5939.378, train_loss=8.68936

Batch 220860, train_perplexity=5643.8022, train_loss=8.638313

Batch 220870, train_perplexity=4987.121, train_loss=8.514614

Batch 220880, train_perplexity=5577.518, train_loss=8.626499

Batch 220890, train_perplexity=5629.015, train_loss=8.63569

Batch 220900, train_perplexity=5974.9307, train_loss=8.695328

Batch 220910, train_perplexity=5608.1016, train_loss=8.631968

Batch 220920, train_perplexity=5317.4272, train_loss=8.578745

Batch 220930, train_perplexity=4800.451, train_loss=8.476465

Batch 220940, train_perplexity=6386.4956, train_loss=8.761941

Batch 220950, train_perplexity=5619.4463, train_loss=8.633988

Batch 220960, train_perplexity=5209.2905, train_loss=8.558199

Batch 220970, train_perplexity=5130.5923, train_loss=8.542976

Batch 220980, train_perplexity=5197.2124, train_loss=8.555878

Batch 220990, train_perplexity=5842.358, train_loss=8.67289

Batch 221000, train_perplexity=4898.1016, train_loss=8.496603

Batch 221010, train_perplexity=4792.049, train_loss=8.474713

Batch 221020, train_perplexity=5904.78, train_loss=8.683517

Batch 221030, train_perplexity=5993.198, train_loss=8.69838

Batch 221040, train_perplexity=5314.9736, train_loss=8.578283

Batch 221050, train_perplexity=4730.7935, train_loss=8.461848

Batch 221060, train_perplexity=4966.077, train_loss=8.5103855

Batch 221070, train_perplexity=5478.253, train_loss=8.6085415

Batch 221080, train_perplexity=5284.053, train_loss=8.572449

Batch 221090, train_perplexity=5781.505, train_loss=8.662419

Batch 221100, train_perplexity=5061.9, train_loss=8.529497

Batch 221110, train_perplexity=5051.811, train_loss=8.527502

Batch 221120, train_perplexity=6236.054, train_loss=8.738103

Batch 221130, train_perplexity=4945.032, train_loss=8.506139

Batch 221140, train_perplexity=6119.586, train_loss=8.71925

Batch 221150, train_perplexity=6217.248, train_loss=8.735083

Batch 221160, train_perplexity=5563.5356, train_loss=8.623989

Batch 221170, train_perplexity=5198.387, train_loss=8.556104

Batch 221180, train_perplexity=4300.8115, train_loss=8.366559

Batch 221190, train_perplexity=6121.2437, train_loss=8.719521

Batch 221200, train_perplexity=5925.8955, train_loss=8.687087

Batch 221210, train_perplexity=5235.143, train_loss=8.563149

Batch 221220, train_perplexity=5517.387, train_loss=8.61566

Batch 221230, train_perplexity=5544.3193, train_loss=8.620529

Batch 221240, train_perplexity=6063.1504, train_loss=8.709985

Batch 221250, train_perplexity=4488.1074, train_loss=8.409186

Batch 221260, train_perplexity=5635.2134, train_loss=8.63679

Batch 221270, train_perplexity=4883.037, train_loss=8.493523

Batch 221280, train_perplexity=5120.4546, train_loss=8.540998

Batch 221290, train_perplexity=6264.0747, train_loss=8.742586

Batch 221300, train_perplexity=5911.677, train_loss=8.684685

Batch 221310, train_perplexity=5574.184, train_loss=8.625901

Batch 221320, train_perplexity=4832.605, train_loss=8.483141

Batch 221330, train_perplexity=6907.852, train_loss=8.840414

Batch 221340, train_perplexity=5118.7944, train_loss=8.540674

Batch 221350, train_perplexity=5100.511, train_loss=8.537096

Batch 221360, train_perplexity=4867.299, train_loss=8.490294

Batch 221370, train_perplexity=4892.6304, train_loss=8.495485

Batch 221380, train_perplexity=4955.7065, train_loss=8.508295

Batch 221390, train_perplexity=6380.286, train_loss=8.760968

Batch 221400, train_perplexity=5475.3906, train_loss=8.608019

Batch 221410, train_perplexity=5108.436, train_loss=8.538649

Batch 221420, train_perplexity=5623.4507, train_loss=8.634701

Batch 221430, train_perplexity=5601.623, train_loss=8.630812

Batch 221440, train_perplexity=5035.265, train_loss=8.524221

Batch 221450, train_perplexity=5964.791, train_loss=8.693629

Batch 221460, train_perplexity=5497.523, train_loss=8.612053

Batch 221470, train_perplexity=5021.2915, train_loss=8.521442

Batch 221480, train_perplexity=4797.321, train_loss=8.475813

Batch 221490, train_perplexity=4539.6997, train_loss=8.420616

Batch 221500, train_perplexity=5006.0293, train_loss=8.518398

Batch 221510, train_perplexity=5497.13, train_loss=8.611981

Batch 221520, train_perplexity=5259.659, train_loss=8.5678215

Batch 221530, train_perplexity=5349.298, train_loss=8.584721

Batch 221540, train_perplexity=4942.227, train_loss=8.505571

Batch 221550, train_perplexity=4913.6724, train_loss=8.499777

Batch 221560, train_perplexity=4821.7637, train_loss=8.480895

Batch 221570, train_perplexity=5802.5615, train_loss=8.666055

Batch 221580, train_perplexity=5488.314, train_loss=8.610376

Batch 221590, train_perplexity=4465.8286, train_loss=8.40421

Batch 221600, train_perplexity=5947.9595, train_loss=8.690804

Batch 221610, train_perplexity=5236.941, train_loss=8.563493

Batch 221620, train_perplexity=5607.8237, train_loss=8.631918

Batch 221630, train_perplexity=4660.244, train_loss=8.446823

Batch 221640, train_perplexity=4488.5483, train_loss=8.409285

Batch 221650, train_perplexity=5181.3857, train_loss=8.552828

Batch 221660, train_perplexity=5804.205, train_loss=8.666338

Batch 221670, train_perplexity=4452.9473, train_loss=8.401321

Batch 221680, train_perplexity=5567.1343, train_loss=8.624636

Batch 221690, train_perplexity=5540.213, train_loss=8.619788

Batch 221700, train_perplexity=5532.5938, train_loss=8.618412

Batch 221710, train_perplexity=5836.7607, train_loss=8.671931

Batch 221720, train_perplexity=4524.27, train_loss=8.417212

Batch 221730, train_perplexity=5679.8604, train_loss=8.644682

Batch 221740, train_perplexity=5326.2075, train_loss=8.580395

Batch 221750, train_perplexity=4805.041, train_loss=8.477421

Batch 221760, train_perplexity=5772.431, train_loss=8.660849

Batch 221770, train_perplexity=5427.711, train_loss=8.599273

Batch 221780, train_perplexity=5617.6724, train_loss=8.633673

Batch 221790, train_perplexity=5642.7207, train_loss=8.638122

Batch 221800, train_perplexity=5531.5967, train_loss=8.618232

Batch 221810, train_perplexity=5659.6704, train_loss=8.641121

Batch 221820, train_perplexity=5309.74, train_loss=8.577298

Batch 221830, train_perplexity=4592.135, train_loss=8.4321

Batch 221840, train_perplexity=4933.591, train_loss=8.503822

Batch 221850, train_perplexity=5474.1895, train_loss=8.6078

Batch 221860, train_perplexity=4903.5513, train_loss=8.497715

Batch 221870, train_perplexity=5650.8794, train_loss=8.639566

Batch 221880, train_perplexity=5622.4907, train_loss=8.63453

Batch 221890, train_perplexity=6070.0117, train_loss=8.711116

Batch 221900, train_perplexity=6089.139, train_loss=8.714262

Batch 221910, train_perplexity=5567.4155, train_loss=8.624686

Batch 221920, train_perplexity=5010.9536, train_loss=8.5193815

Batch 221930, train_perplexity=5915.5, train_loss=8.685331

Batch 221940, train_perplexity=5455.979, train_loss=8.604467

Batch 221950, train_perplexity=5492.33, train_loss=8.611108

Batch 221960, train_perplexity=5776.33, train_loss=8.661524

Batch 221970, train_perplexity=4229.061, train_loss=8.349735

Batch 221980, train_perplexity=4754.8066, train_loss=8.466911
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 221990, train_perplexity=5621.9814, train_loss=8.634439

Batch 222000, train_perplexity=5186.8145, train_loss=8.553875

Batch 222010, train_perplexity=4533.461, train_loss=8.419241

Batch 222020, train_perplexity=5415.8027, train_loss=8.597076

Batch 222030, train_perplexity=5614.887, train_loss=8.633177

Batch 222040, train_perplexity=6405.2515, train_loss=8.7648735

Batch 222050, train_perplexity=6500.9717, train_loss=8.779707

Batch 222060, train_perplexity=5398.132, train_loss=8.593808

Batch 222070, train_perplexity=5563.0054, train_loss=8.623894

Batch 222080, train_perplexity=5751.4844, train_loss=8.657213

Batch 222090, train_perplexity=4798.465, train_loss=8.476051

Batch 222100, train_perplexity=4532.0605, train_loss=8.418932

Batch 222110, train_perplexity=4978.0166, train_loss=8.512787

Batch 222120, train_perplexity=5195.28, train_loss=8.555506

Batch 222130, train_perplexity=5239.2485, train_loss=8.563933

Batch 222140, train_perplexity=6179.8584, train_loss=8.729051

Batch 222150, train_perplexity=5710.4604, train_loss=8.650055

Batch 222160, train_perplexity=5036.3213, train_loss=8.524431

Batch 222170, train_perplexity=5510.8037, train_loss=8.614466

Batch 222180, train_perplexity=5367.9404, train_loss=8.5882

Batch 222190, train_perplexity=6067.3555, train_loss=8.710678

Batch 222200, train_perplexity=5353.5596, train_loss=8.585517

Batch 222210, train_perplexity=5737.9956, train_loss=8.654865

Batch 222220, train_perplexity=4635.9175, train_loss=8.441589

Batch 222230, train_perplexity=4666.6973, train_loss=8.448207

Batch 222240, train_perplexity=5291.3853, train_loss=8.573835

Batch 222250, train_perplexity=6281.0874, train_loss=8.745298

Batch 222260, train_perplexity=5843.433, train_loss=8.673074

Batch 222270, train_perplexity=5593.6265, train_loss=8.629383

Batch 222280, train_perplexity=5131.825, train_loss=8.543217

Batch 222290, train_perplexity=5727.8267, train_loss=8.653091

Batch 222300, train_perplexity=4089.5754, train_loss=8.316196

Batch 222310, train_perplexity=4099.963, train_loss=8.318733

Batch 222320, train_perplexity=4690.922, train_loss=8.453384

Batch 222330, train_perplexity=5768.9478, train_loss=8.660245

Batch 222340, train_perplexity=4682.653, train_loss=8.45162

Batch 222350, train_perplexity=6119.329, train_loss=8.719208

Batch 222360, train_perplexity=5412.545, train_loss=8.596475

Batch 222370, train_perplexity=5349.0938, train_loss=8.584682

Batch 222380, train_perplexity=4766.3384, train_loss=8.469334

Batch 222390, train_perplexity=5529.513, train_loss=8.617855

Batch 222400, train_perplexity=5163.9624, train_loss=8.549459

Batch 222410, train_perplexity=4573.447, train_loss=8.428022

Batch 222420, train_perplexity=5659.9673, train_loss=8.641173

Batch 222430, train_perplexity=4802.6904, train_loss=8.476932

Batch 222440, train_perplexity=5422.8677, train_loss=8.59838

Batch 222450, train_perplexity=4794.595, train_loss=8.4752445

Batch 222460, train_perplexity=5112.4375, train_loss=8.539432

Batch 222470, train_perplexity=5763.2837, train_loss=8.659263

Batch 222480, train_perplexity=5436.9478, train_loss=8.600973

Batch 222490, train_perplexity=5190.3374, train_loss=8.554554

Batch 222500, train_perplexity=5183.076, train_loss=8.553154

Batch 222510, train_perplexity=4812.0522, train_loss=8.478879

Batch 222520, train_perplexity=5694.803, train_loss=8.647309

Batch 222530, train_perplexity=6272.2163, train_loss=8.743885

Batch 222540, train_perplexity=5913.0073, train_loss=8.68491

Batch 222550, train_perplexity=7197.0454, train_loss=8.881426

Batch 222560, train_perplexity=4871.678, train_loss=8.491194

Batch 222570, train_perplexity=5974.8735, train_loss=8.695318

Batch 222580, train_perplexity=5908.295, train_loss=8.684113

Batch 222590, train_perplexity=6287.038, train_loss=8.746245

Batch 222600, train_perplexity=4702.747, train_loss=8.455902

Batch 222610, train_perplexity=4577.221, train_loss=8.428847

Batch 222620, train_perplexity=5245.418, train_loss=8.56511

Batch 222630, train_perplexity=5665.4214, train_loss=8.642137

Batch 222640, train_perplexity=5245.893, train_loss=8.565201

Batch 222650, train_perplexity=7120.7856, train_loss=8.870773

Batch 222660, train_perplexity=5277.0933, train_loss=8.571131

Batch 222670, train_perplexity=5127.071, train_loss=8.54229

Batch 222680, train_perplexity=5135.409, train_loss=8.543915

Batch 222690, train_perplexity=5495.5103, train_loss=8.611687

Batch 222700, train_perplexity=4796.086, train_loss=8.475555

Batch 222710, train_perplexity=5521.9355, train_loss=8.616484

Batch 222720, train_perplexity=5710.341, train_loss=8.650034

Batch 222730, train_perplexity=5166.588, train_loss=8.549968

Batch 222740, train_perplexity=5879.8804, train_loss=8.679292

Batch 222750, train_perplexity=5175.835, train_loss=8.551756

Batch 222760, train_perplexity=4760.4873, train_loss=8.468105

Batch 222770, train_perplexity=5361.1567, train_loss=8.586935

Batch 222780, train_perplexity=5208.1084, train_loss=8.557972

Batch 222790, train_perplexity=5799.4355, train_loss=8.665516

Batch 222800, train_perplexity=5375.989, train_loss=8.589698

Batch 222810, train_perplexity=5500.2866, train_loss=8.6125555

Batch 222820, train_perplexity=5748.8574, train_loss=8.656756

Batch 222830, train_perplexity=5356.2305, train_loss=8.586016

Batch 222840, train_perplexity=4621.8486, train_loss=8.43855

Batch 222850, train_perplexity=5351.1807, train_loss=8.5850725

Batch 222860, train_perplexity=5990.7065, train_loss=8.697965

Batch 222870, train_perplexity=4985.452, train_loss=8.514279

Batch 222880, train_perplexity=5430.9053, train_loss=8.599861

Batch 222890, train_perplexity=5088.4233, train_loss=8.534723

Batch 222900, train_perplexity=5610.022, train_loss=8.63231

Batch 222910, train_perplexity=5591.568, train_loss=8.629015

Batch 222920, train_perplexity=4662.698, train_loss=8.44735

Batch 222930, train_perplexity=4419.9795, train_loss=8.39389

Batch 222940, train_perplexity=6089.5806, train_loss=8.7143345

Batch 222950, train_perplexity=5993.2153, train_loss=8.698383

Batch 222960, train_perplexity=4417.1353, train_loss=8.393247

Batch 222970, train_perplexity=4560.5674, train_loss=8.425202

Batch 222980, train_perplexity=4495.9126, train_loss=8.410924

Batch 222990, train_perplexity=5340.047, train_loss=8.58299

Batch 223000, train_perplexity=5912.308, train_loss=8.684792

Batch 223010, train_perplexity=5420.303, train_loss=8.597907

Batch 223020, train_perplexity=5619.762, train_loss=8.634045

Batch 223030, train_perplexity=5536.204, train_loss=8.619064

Batch 223040, train_perplexity=5314.421, train_loss=8.578179

Batch 223050, train_perplexity=6086.4917, train_loss=8.713827

Batch 223060, train_perplexity=5111.526, train_loss=8.539253

Batch 223070, train_perplexity=5164.524, train_loss=8.549568

Batch 223080, train_perplexity=4884.518, train_loss=8.493826

Batch 223090, train_perplexity=4986.598, train_loss=8.514509

Batch 223100, train_perplexity=5420.319, train_loss=8.59791

Batch 223110, train_perplexity=4777.0234, train_loss=8.471573

Batch 223120, train_perplexity=5363.5703, train_loss=8.587385

Batch 223130, train_perplexity=6061.757, train_loss=8.709755

Batch 223140, train_perplexity=6183.5723, train_loss=8.729651

Batch 223150, train_perplexity=5378.948, train_loss=8.590248

Batch 223160, train_perplexity=5120.601, train_loss=8.541027

Batch 223170, train_perplexity=6403.273, train_loss=8.7645645

Batch 223180, train_perplexity=5914.0337, train_loss=8.685083

Batch 223190, train_perplexity=5423.504, train_loss=8.598497

Batch 223200, train_perplexity=4406.3687, train_loss=8.390806

Batch 223210, train_perplexity=4573.5903, train_loss=8.428054

Batch 223220, train_perplexity=5317.1943, train_loss=8.578701

Batch 223230, train_perplexity=4928.8223, train_loss=8.502855

Batch 223240, train_perplexity=5218.24, train_loss=8.559916

Batch 223250, train_perplexity=5542.49, train_loss=8.620199

Batch 223260, train_perplexity=6092.5894, train_loss=8.7148285

Batch 223270, train_perplexity=4674.8, train_loss=8.449942

Batch 223280, train_perplexity=6005.8945, train_loss=8.700497

Batch 223290, train_perplexity=6298.765, train_loss=8.748109

Batch 223300, train_perplexity=5455.8022, train_loss=8.604435
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 223310, train_perplexity=4320.4, train_loss=8.371103

Batch 223320, train_perplexity=6250.4688, train_loss=8.740412

Batch 223330, train_perplexity=5673.472, train_loss=8.643557

Batch 223340, train_perplexity=5824.3555, train_loss=8.669804

Batch 223350, train_perplexity=5206.4443, train_loss=8.557652

Batch 223360, train_perplexity=5619.5747, train_loss=8.634011

Batch 223370, train_perplexity=5087.9526, train_loss=8.534631

Batch 223380, train_perplexity=5421.4976, train_loss=8.598127

Batch 223390, train_perplexity=5308.378, train_loss=8.577042

Batch 223400, train_perplexity=5319.984, train_loss=8.579226

Batch 223410, train_perplexity=4902.6816, train_loss=8.497538

Batch 223420, train_perplexity=5223.947, train_loss=8.561008

Batch 223430, train_perplexity=5926.5337, train_loss=8.687195

Batch 223440, train_perplexity=5183.5557, train_loss=8.5532465

Batch 223450, train_perplexity=4879.611, train_loss=8.492821

Batch 223460, train_perplexity=5945.487, train_loss=8.690388

Batch 223470, train_perplexity=4921.927, train_loss=8.501455

Batch 223480, train_perplexity=5508.0293, train_loss=8.613962

Batch 223490, train_perplexity=5744.303, train_loss=8.655964

Batch 223500, train_perplexity=4459.5044, train_loss=8.402793

Batch 223510, train_perplexity=6282.4653, train_loss=8.745518

Batch 223520, train_perplexity=4307.026, train_loss=8.368003

Batch 223530, train_perplexity=5739.3423, train_loss=8.6551

Batch 223540, train_perplexity=5965.6216, train_loss=8.6937685

Batch 223550, train_perplexity=5659.055, train_loss=8.641012

Batch 223560, train_perplexity=5763.355, train_loss=8.659275

Batch 223570, train_perplexity=5174.818, train_loss=8.551559

Batch 223580, train_perplexity=4839.329, train_loss=8.484531

Batch 223590, train_perplexity=5032.807, train_loss=8.523733

Batch 223600, train_perplexity=5356.8384, train_loss=8.586129

Batch 223610, train_perplexity=5629.1704, train_loss=8.635717

Batch 223620, train_perplexity=5766.1147, train_loss=8.659754

Batch 223630, train_perplexity=5704.2554, train_loss=8.648968

Batch 223640, train_perplexity=4803.5747, train_loss=8.477116

Batch 223650, train_perplexity=4366.0366, train_loss=8.381611

Batch 223660, train_perplexity=5705.1587, train_loss=8.649126

Batch 223670, train_perplexity=5030.9595, train_loss=8.523366

Batch 223680, train_perplexity=6513.4824, train_loss=8.78163

Batch 223690, train_perplexity=5225.3716, train_loss=8.561281

Batch 223700, train_perplexity=4800.7534, train_loss=8.476528

Batch 223710, train_perplexity=6326.2646, train_loss=8.752465

Batch 223720, train_perplexity=6240.647, train_loss=8.738839

Batch 223730, train_perplexity=5696.9272, train_loss=8.647682

Batch 223740, train_perplexity=5808.2695, train_loss=8.667038

Batch 223750, train_perplexity=5628.3867, train_loss=8.635578

Batch 223760, train_perplexity=5005.781, train_loss=8.518349

Batch 223770, train_perplexity=5252.0903, train_loss=8.566381

Batch 223780, train_perplexity=5247.94, train_loss=8.565591

Batch 223790, train_perplexity=4619.3145, train_loss=8.438002

Batch 223800, train_perplexity=5035.6875, train_loss=8.524305

Batch 223810, train_perplexity=5555.6196, train_loss=8.622565

Batch 223820, train_perplexity=5679.47, train_loss=8.644613

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Batch 223830, train_perplexity=5665.13, train_loss=8.642085

Batch 223840, train_perplexity=6024.9575, train_loss=8.703666

Batch 223850, train_perplexity=4869.6157, train_loss=8.49077

Batch 223860, train_perplexity=4343.196, train_loss=8.376366

Batch 223870, train_perplexity=5239.2236, train_loss=8.563929

Batch 223880, train_perplexity=5452.4526, train_loss=8.603821

Batch 223890, train_perplexity=5080.064, train_loss=8.533079

Batch 223900, train_perplexity=5546.213, train_loss=8.620871

Batch 223910, train_perplexity=5465.171, train_loss=8.606151

Batch 223920, train_perplexity=5713.3364, train_loss=8.650558

Batch 223930, train_perplexity=6285.2456, train_loss=8.74596

Batch 223940, train_perplexity=4951.6626, train_loss=8.507479

Batch 223950, train_perplexity=5216.504, train_loss=8.559583

Batch 223960, train_perplexity=5168.83, train_loss=8.550402

Batch 223970, train_perplexity=5573.5835, train_loss=8.625793

Batch 223980, train_perplexity=5271.7314, train_loss=8.570114

Batch 223990, train_perplexity=6399.8784, train_loss=8.764034

Batch 224000, train_perplexity=5846.488, train_loss=8.673596

Batch 224010, train_perplexity=5560.7563, train_loss=8.623489

Batch 224020, train_perplexity=5139.0103, train_loss=8.544616

Batch 224030, train_perplexity=4705.2773, train_loss=8.45644

Batch 224040, train_perplexity=4865.1226, train_loss=8.489847

Batch 224050, train_perplexity=5188.12, train_loss=8.554127

Batch 224060, train_perplexity=4988.653, train_loss=8.514921

Batch 224070, train_perplexity=6391.3213, train_loss=8.762696

Batch 224080, train_perplexity=4422.29, train_loss=8.394413

Batch 224090, train_perplexity=5096.6064, train_loss=8.53633

Batch 224100, train_perplexity=5135.7715, train_loss=8.543985

Batch 224110, train_perplexity=5294.939, train_loss=8.574507

Batch 224120, train_perplexity=5745.925, train_loss=8.656246

Batch 224130, train_perplexity=4516.8853, train_loss=8.415578

Batch 224140, train_perplexity=4922.377, train_loss=8.501547

Batch 224150, train_perplexity=4896.92, train_loss=8.496362

Batch 224160, train_perplexity=6182.788, train_loss=8.729525

Batch 224170, train_perplexity=4457.298, train_loss=8.402298

Batch 224180, train_perplexity=5399.9233, train_loss=8.59414

Batch 224190, train_perplexity=6422.519, train_loss=8.767566

Batch 224200, train_perplexity=5679.9795, train_loss=8.644703

Batch 224210, train_perplexity=5033.2676, train_loss=8.523825

Batch 224220, train_perplexity=4900.84, train_loss=8.497162

Batch 224230, train_perplexity=5474.555, train_loss=8.607866

Batch 224240, train_perplexity=4949.732, train_loss=8.507089

Batch 224250, train_perplexity=5942.1655, train_loss=8.689829

Batch 224260, train_perplexity=5752.735, train_loss=8.657431

Batch 224270, train_perplexity=5463.185, train_loss=8.605787

Batch 224280, train_perplexity=5645.374, train_loss=8.638592

Batch 224290, train_perplexity=5018.4717, train_loss=8.520881

Batch 224300, train_perplexity=5142.403, train_loss=8.545276

Batch 224310, train_perplexity=5480.479, train_loss=8.608948

Batch 224320, train_perplexity=4948.6416, train_loss=8.506868

Batch 224330, train_perplexity=4842.8936, train_loss=8.485268

Batch 224340, train_perplexity=4677.0654, train_loss=8.450426

Batch 224350, train_perplexity=5409.304, train_loss=8.595876

Batch 224360, train_perplexity=5804.7534, train_loss=8.666432

Batch 224370, train_perplexity=5248.29, train_loss=8.565658

Batch 224380, train_perplexity=4640.69, train_loss=8.442618

Batch 224390, train_perplexity=5915.004, train_loss=8.685247

Batch 224400, train_perplexity=5013.7837, train_loss=8.519946

Batch 224410, train_perplexity=6252.496, train_loss=8.740736

Batch 224420, train_perplexity=4950.1143, train_loss=8.507166

Batch 224430, train_perplexity=5343.0317, train_loss=8.583549

Batch 224440, train_perplexity=5052.49, train_loss=8.527637

Batch 224450, train_perplexity=5214.1963, train_loss=8.55914

Batch 224460, train_perplexity=6142.2075, train_loss=8.7229395

Batch 224470, train_perplexity=4792.396, train_loss=8.474786

Batch 224480, train_perplexity=5467.376, train_loss=8.606554

Batch 224490, train_perplexity=4516.924, train_loss=8.415586

Batch 224500, train_perplexity=5319.4463, train_loss=8.579124

Batch 224510, train_perplexity=5209.648, train_loss=8.558268

Batch 224520, train_perplexity=4878.373, train_loss=8.492567

Batch 224530, train_perplexity=5342.0283, train_loss=8.583361

Batch 224540, train_perplexity=6380.919, train_loss=8.761067

Batch 224550, train_perplexity=5084.2563, train_loss=8.533904
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 224560, train_perplexity=5594.176, train_loss=8.629481

Batch 224570, train_perplexity=5577.002, train_loss=8.626407

Batch 224580, train_perplexity=5700.8457, train_loss=8.64837

Batch 224590, train_perplexity=4963.3594, train_loss=8.509838

Batch 224600, train_perplexity=5742.3315, train_loss=8.655621

Batch 224610, train_perplexity=5082.1865, train_loss=8.533497

Batch 224620, train_perplexity=4969.7964, train_loss=8.511134

Batch 224630, train_perplexity=5326.4717, train_loss=8.580444

Batch 224640, train_perplexity=5239.6733, train_loss=8.564014

Batch 224650, train_perplexity=5237.615, train_loss=8.5636215

Batch 224660, train_perplexity=5700.1226, train_loss=8.648243

Batch 224670, train_perplexity=5437.114, train_loss=8.601004

Batch 224680, train_perplexity=5353.6157, train_loss=8.585527

Batch 224690, train_perplexity=5275.715, train_loss=8.570869

Batch 224700, train_perplexity=4437.262, train_loss=8.397793

Batch 224710, train_perplexity=4767.4385, train_loss=8.469564

Batch 224720, train_perplexity=5546.779, train_loss=8.620973

Batch 224730, train_perplexity=5508.213, train_loss=8.613996

Batch 224740, train_perplexity=6370.3027, train_loss=8.759402

Batch 224750, train_perplexity=5817.0723, train_loss=8.668552

Batch 224760, train_perplexity=4796.5615, train_loss=8.475655

Batch 224770, train_perplexity=6221.2275, train_loss=8.735723

Batch 224780, train_perplexity=5939.5933, train_loss=8.689396

Batch 224790, train_perplexity=5595.408, train_loss=8.629702

Batch 224800, train_perplexity=5261.199, train_loss=8.568114

Batch 224810, train_perplexity=6312.554, train_loss=8.750296

Batch 224820, train_perplexity=5634.928, train_loss=8.63674

Batch 224830, train_perplexity=8050.967, train_loss=8.993547

Batch 224840, train_perplexity=5040.6846, train_loss=8.525297

Batch 224850, train_perplexity=4944.3394, train_loss=8.505999

Batch 224860, train_perplexity=6230.71, train_loss=8.737246

Batch 224870, train_perplexity=4759.026, train_loss=8.467798

Batch 224880, train_perplexity=4862.4927, train_loss=8.489306

Batch 224890, train_perplexity=5354.167, train_loss=8.58563

Batch 224900, train_perplexity=5575.359, train_loss=8.626112

Batch 224910, train_perplexity=6378.3696, train_loss=8.760668

Batch 224920, train_perplexity=5757.356, train_loss=8.658234

Batch 224930, train_perplexity=5286.856, train_loss=8.572979

Batch 224940, train_perplexity=6287.356, train_loss=8.746296

Batch 224950, train_perplexity=5357.15, train_loss=8.586187

Batch 224960, train_perplexity=5379.569, train_loss=8.5903635

Batch 224970, train_perplexity=4729.6973, train_loss=8.4616165

Batch 224980, train_perplexity=5290.3965, train_loss=8.573648

Batch 224990, train_perplexity=5415.6997, train_loss=8.597057

Batch 225000, train_perplexity=5532.14, train_loss=8.61833

Batch 225010, train_perplexity=5999.969, train_loss=8.69951

Batch 225020, train_perplexity=4908.3423, train_loss=8.498692

Batch 225030, train_perplexity=5429.595, train_loss=8.59962

Batch 225040, train_perplexity=5024.544, train_loss=8.52209

Batch 225050, train_perplexity=6483.957, train_loss=8.777086

Batch 225060, train_perplexity=5296.818, train_loss=8.574862

Batch 225070, train_perplexity=5476.9834, train_loss=8.60831

Batch 225080, train_perplexity=5175.8296, train_loss=8.551755

Batch 225090, train_perplexity=5800.686, train_loss=8.665731

Batch 225100, train_perplexity=5776.0547, train_loss=8.661476

Batch 225110, train_perplexity=5511.981, train_loss=8.614679

Batch 225120, train_perplexity=5982.902, train_loss=8.696661

Batch 225130, train_perplexity=4971.1187, train_loss=8.5114

Batch 225140, train_perplexity=5116.0713, train_loss=8.540142

Batch 225150, train_perplexity=5530.5996, train_loss=8.618052

Batch 225160, train_perplexity=4713.7886, train_loss=8.458247

Batch 225170, train_perplexity=5128.3813, train_loss=8.542545

Batch 225180, train_perplexity=4218.218, train_loss=8.347168

Batch 225190, train_perplexity=5040.4634, train_loss=8.525253

Batch 225200, train_perplexity=4886.8477, train_loss=8.494303

Batch 225210, train_perplexity=5665.481, train_loss=8.642147

Batch 225220, train_perplexity=5139.3823, train_loss=8.544688

Batch 225230, train_perplexity=5621.1665, train_loss=8.6342945

Batch 225240, train_perplexity=5193.0806, train_loss=8.555082

Batch 225250, train_perplexity=4914.797, train_loss=8.500006

Batch 225260, train_perplexity=4432.2544, train_loss=8.396664

Batch 225270, train_perplexity=5363.044, train_loss=8.587287

Batch 225280, train_perplexity=4891.823, train_loss=8.49532

Batch 225290, train_perplexity=6035.9824, train_loss=8.705494

Batch 225300, train_perplexity=4848.744, train_loss=8.486475

Batch 225310, train_perplexity=5599.604, train_loss=8.630451

Batch 225320, train_perplexity=5709.981, train_loss=8.649971

Batch 225330, train_perplexity=5534.2085, train_loss=8.618704

Batch 225340, train_perplexity=4427.9243, train_loss=8.395686

Batch 225350, train_perplexity=6075.4614, train_loss=8.712013

Batch 225360, train_perplexity=5733.855, train_loss=8.654143

Batch 225370, train_perplexity=4819.9614, train_loss=8.480521

Batch 225380, train_perplexity=4426.919, train_loss=8.395459

Batch 225390, train_perplexity=5376.066, train_loss=8.589712

Batch 225400, train_perplexity=5328.25, train_loss=8.580778

Batch 225410, train_perplexity=5920.693, train_loss=8.686209

Batch 225420, train_perplexity=5035.327, train_loss=8.524234

Batch 225430, train_perplexity=5838.9824, train_loss=8.672312

Batch 225440, train_perplexity=5800.846, train_loss=8.665759

Batch 225450, train_perplexity=5336.341, train_loss=8.582295

Batch 225460, train_perplexity=4549.104, train_loss=8.422686

Batch 225470, train_perplexity=4412.929, train_loss=8.392294

Batch 225480, train_perplexity=5594.9707, train_loss=8.629623

Batch 225490, train_perplexity=4974.652, train_loss=8.512111

Batch 225500, train_perplexity=6025.5493, train_loss=8.703764

Batch 225510, train_perplexity=3900.213, train_loss=8.268786

Batch 225520, train_perplexity=6438.0464, train_loss=8.76998

Batch 225530, train_perplexity=4941.153, train_loss=8.505354

Batch 225540, train_perplexity=5481.8643, train_loss=8.6092005

Batch 225550, train_perplexity=5467.5737, train_loss=8.60659

Batch 225560, train_perplexity=4368.623, train_loss=8.382203

Batch 225570, train_perplexity=5278.5127, train_loss=8.5714

Batch 225580, train_perplexity=5371.059, train_loss=8.58878

Batch 225590, train_perplexity=5271.6206, train_loss=8.570093

Batch 225600, train_perplexity=5018.668, train_loss=8.52092

Batch 225610, train_perplexity=5552.9395, train_loss=8.622083

Batch 225620, train_perplexity=5803.978, train_loss=8.666299

Batch 225630, train_perplexity=5032.7686, train_loss=8.5237255

Batch 225640, train_perplexity=5228.916, train_loss=8.561959

Batch 225650, train_perplexity=5238.454, train_loss=8.563782

Batch 225660, train_perplexity=5301.25, train_loss=8.575698

Batch 225670, train_perplexity=4595.245, train_loss=8.432777

Batch 225680, train_perplexity=5128.342, train_loss=8.542538

Batch 225690, train_perplexity=6445.7627, train_loss=8.771178

Batch 225700, train_perplexity=5941.4287, train_loss=8.689705

Batch 225710, train_perplexity=4722.8867, train_loss=8.4601755

Batch 225720, train_perplexity=6597.0967, train_loss=8.794385

Batch 225730, train_perplexity=6768.8438, train_loss=8.820086

Batch 225740, train_perplexity=7000.113, train_loss=8.853682

Batch 225750, train_perplexity=5379.215, train_loss=8.590298

Batch 225760, train_perplexity=6297.63, train_loss=8.747929

Batch 225770, train_perplexity=5589.2324, train_loss=8.628597

Batch 225780, train_perplexity=6650.5444, train_loss=8.802454

Batch 225790, train_perplexity=7001.074, train_loss=8.853819

Batch 225800, train_perplexity=5995.319, train_loss=8.698734

Batch 225810, train_perplexity=6611.9175, train_loss=8.796629

Batch 225820, train_perplexity=5420.588, train_loss=8.5979595

Batch 225830, train_perplexity=5220.127, train_loss=8.560277

Batch 225840, train_perplexity=4353.4053, train_loss=8.378714

Batch 225850, train_perplexity=4902.1675, train_loss=8.497433

Batch 225860, train_perplexity=4798.053, train_loss=8.4759655

Batch 225870, train_perplexity=5973.6772, train_loss=8.695118
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 225880, train_perplexity=5243.7124, train_loss=8.564785

Batch 225890, train_perplexity=4975.7246, train_loss=8.512326

Batch 225900, train_perplexity=5241.7676, train_loss=8.564414

Batch 225910, train_perplexity=6016.121, train_loss=8.702198

Batch 225920, train_perplexity=5470.0566, train_loss=8.607044

Batch 225930, train_perplexity=5578.0073, train_loss=8.626587

Batch 225940, train_perplexity=5339.9243, train_loss=8.582967

Batch 225950, train_perplexity=6569.867, train_loss=8.790249

Batch 225960, train_perplexity=5121.861, train_loss=8.541273

Batch 225970, train_perplexity=5251.1987, train_loss=8.566212

Batch 225980, train_perplexity=5925.7256, train_loss=8.687058

Batch 225990, train_perplexity=4631.0654, train_loss=8.440542

Batch 226000, train_perplexity=5188.15, train_loss=8.554132

Batch 226010, train_perplexity=5669.632, train_loss=8.6428795

Batch 226020, train_perplexity=4791.5415, train_loss=8.474607

Batch 226030, train_perplexity=5578.4546, train_loss=8.626667

Batch 226040, train_perplexity=6440.5703, train_loss=8.770372

Batch 226050, train_perplexity=5157.013, train_loss=8.548113

Batch 226060, train_perplexity=6172.4663, train_loss=8.727854

Batch 226070, train_perplexity=6692.894, train_loss=8.808802

Batch 226080, train_perplexity=5902.984, train_loss=8.683213

Batch 226090, train_perplexity=7006.3774, train_loss=8.854576

Batch 226100, train_perplexity=5521.1455, train_loss=8.616341

Batch 226110, train_perplexity=5039.1177, train_loss=8.524986

Batch 226120, train_perplexity=5147.1567, train_loss=8.5462

Batch 226130, train_perplexity=6709.305, train_loss=8.811251

Batch 226140, train_perplexity=5294.0503, train_loss=8.574339

Batch 226150, train_perplexity=4225.5737, train_loss=8.34891

Batch 226160, train_perplexity=4279.136, train_loss=8.361506

Batch 226170, train_perplexity=4801.633, train_loss=8.476711

Batch 226180, train_perplexity=6246.715, train_loss=8.739811

Batch 226190, train_perplexity=5711.408, train_loss=8.650221

Batch 226200, train_perplexity=5373.2056, train_loss=8.58918

Batch 226210, train_perplexity=4783.912, train_loss=8.473014

Batch 226220, train_perplexity=5459.9976, train_loss=8.605204

Batch 226230, train_perplexity=5085.8423, train_loss=8.534216

Batch 226240, train_perplexity=5196.836, train_loss=8.555805

Batch 226250, train_perplexity=5083.7476, train_loss=8.533804

Batch 226260, train_perplexity=3899.577, train_loss=8.268623

Batch 226270, train_perplexity=5364.0513, train_loss=8.587475

Batch 226280, train_perplexity=4775.6567, train_loss=8.471287

Batch 226290, train_perplexity=6050.6675, train_loss=8.707924

Batch 226300, train_perplexity=6558.8115, train_loss=8.788565

Batch 226310, train_perplexity=5133.7393, train_loss=8.54359

Batch 226320, train_perplexity=5089.462, train_loss=8.534927

Batch 226330, train_perplexity=4746.511, train_loss=8.465165

Batch 226340, train_perplexity=5249.9272, train_loss=8.565969

Batch 226350, train_perplexity=4888.74, train_loss=8.49469

Batch 226360, train_perplexity=5495.112, train_loss=8.611614

Batch 226370, train_perplexity=4744.588, train_loss=8.46476

Batch 226380, train_perplexity=5036.451, train_loss=8.524457

Batch 226390, train_perplexity=4599.3623, train_loss=8.433673

Batch 226400, train_perplexity=4465.961, train_loss=8.40424

Batch 226410, train_perplexity=5216.3794, train_loss=8.559559

Batch 226420, train_perplexity=6170.5947, train_loss=8.7275505

Batch 226430, train_perplexity=5087.3604, train_loss=8.534514

Batch 226440, train_perplexity=5030.384, train_loss=8.523252

Batch 226450, train_perplexity=5433.309, train_loss=8.600304

Batch 226460, train_perplexity=5808.314, train_loss=8.667046

Batch 226470, train_perplexity=5306.4697, train_loss=8.576682

Batch 226480, train_perplexity=5981.5557, train_loss=8.696436

Batch 226490, train_perplexity=4580.588, train_loss=8.429583

Batch 226500, train_perplexity=6266.4287, train_loss=8.742962

Batch 226510, train_perplexity=4892.5464, train_loss=8.495468

Batch 226520, train_perplexity=5077.8457, train_loss=8.532642

Batch 226530, train_perplexity=5523.452, train_loss=8.616758

Batch 226540, train_perplexity=5390.811, train_loss=8.592451

Batch 226550, train_perplexity=4736.482, train_loss=8.46305

Batch 226560, train_perplexity=5342.4717, train_loss=8.583444

Batch 226570, train_perplexity=4528.4487, train_loss=8.418135

Batch 226580, train_perplexity=5503.897, train_loss=8.613212

Batch 226590, train_perplexity=5427.7886, train_loss=8.599287

Batch 226600, train_perplexity=5156.6787, train_loss=8.548048

Batch 226610, train_perplexity=6411.07, train_loss=8.765781

Batch 226620, train_perplexity=4891.6226, train_loss=8.495279

Batch 226630, train_perplexity=5850.7993, train_loss=8.674334

Batch 226640, train_perplexity=6204.661, train_loss=8.733056

Batch 226650, train_perplexity=6121.489, train_loss=8.719561

Batch 226660, train_perplexity=5690.7583, train_loss=8.646599

Batch 226670, train_perplexity=5120.8696, train_loss=8.5410795

Batch 226680, train_perplexity=6265.126, train_loss=8.742754

Batch 226690, train_perplexity=5776.0435, train_loss=8.661474

Batch 226700, train_perplexity=6393.8755, train_loss=8.763096

Batch 226710, train_perplexity=5428.4146, train_loss=8.599402

Batch 226720, train_perplexity=4977.504, train_loss=8.512684

Batch 226730, train_perplexity=5826.144, train_loss=8.670111

Batch 226740, train_perplexity=4603.5396, train_loss=8.434581

Batch 226750, train_perplexity=5200.133, train_loss=8.556439

Batch 226760, train_perplexity=5223.6577, train_loss=8.560953

Batch 226770, train_perplexity=5472.77, train_loss=8.60754

Batch 226780, train_perplexity=4573.49, train_loss=8.428032

Batch 226790, train_perplexity=4612.179, train_loss=8.436456

Batch 226800, train_perplexity=4732.3867, train_loss=8.462185

Batch 226810, train_perplexity=4751.194, train_loss=8.466151

Batch 226820, train_perplexity=5864.5864, train_loss=8.676687

Batch 226830, train_perplexity=4893.1294, train_loss=8.495587

Batch 226840, train_perplexity=6557.348, train_loss=8.7883415

Batch 226850, train_perplexity=5323.9224, train_loss=8.579966

Batch 226860, train_perplexity=4697.2876, train_loss=8.454741

Batch 226870, train_perplexity=5040.521, train_loss=8.525265

Batch 226880, train_perplexity=5357.5737, train_loss=8.5862665

Batch 226890, train_perplexity=4570.0894, train_loss=8.427288

Batch 226900, train_perplexity=4658.7954, train_loss=8.446512

Batch 226910, train_perplexity=5869.7563, train_loss=8.677568

Batch 226920, train_perplexity=4597.2354, train_loss=8.43321

Batch 226930, train_perplexity=4713.4873, train_loss=8.458183

Batch 226940, train_perplexity=5783.6836, train_loss=8.662796

Batch 226950, train_perplexity=4765.6655, train_loss=8.4691925

Batch 226960, train_perplexity=5293.061, train_loss=8.574152

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Batch 226970, train_perplexity=4597.3755, train_loss=8.433241

Batch 226980, train_perplexity=6416.5625, train_loss=8.766638

Batch 226990, train_perplexity=5079.362, train_loss=8.532941

Batch 227000, train_perplexity=5170.842, train_loss=8.550791

Batch 227010, train_perplexity=5669.6426, train_loss=8.642881

Batch 227020, train_perplexity=5211.387, train_loss=8.558601

Batch 227030, train_perplexity=5448.58, train_loss=8.60311

Batch 227040, train_perplexity=5341.386, train_loss=8.5832405

Batch 227050, train_perplexity=6134.8022, train_loss=8.721733

Batch 227060, train_perplexity=5017.854, train_loss=8.520758

Batch 227070, train_perplexity=5942.641, train_loss=8.689909

Batch 227080, train_perplexity=6008.289, train_loss=8.700895

Batch 227090, train_perplexity=5804.172, train_loss=8.666332

Batch 227100, train_perplexity=6140.515, train_loss=8.722664

Batch 227110, train_perplexity=6248.365, train_loss=8.740075

Batch 227120, train_perplexity=6134.381, train_loss=8.721664
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 227130, train_perplexity=6216.6904, train_loss=8.734993

Batch 227140, train_perplexity=4704.7837, train_loss=8.456335

Batch 227150, train_perplexity=5867.7754, train_loss=8.677231

Batch 227160, train_perplexity=4952.442, train_loss=8.507636

Batch 227170, train_perplexity=5527.278, train_loss=8.617451

Batch 227180, train_perplexity=4696.28, train_loss=8.454526

Batch 227190, train_perplexity=5284.5674, train_loss=8.572546

Batch 227200, train_perplexity=4912.482, train_loss=8.499535

Batch 227210, train_perplexity=4265.6133, train_loss=8.358341

Batch 227220, train_perplexity=6050.7485, train_loss=8.707937

Batch 227230, train_perplexity=4581.7324, train_loss=8.429832

Batch 227240, train_perplexity=4832.1074, train_loss=8.483038

Batch 227250, train_perplexity=5392.6826, train_loss=8.592798

Batch 227260, train_perplexity=5762.3604, train_loss=8.659102

Batch 227270, train_perplexity=5695.0474, train_loss=8.647352

Batch 227280, train_perplexity=5124.1377, train_loss=8.541718

Batch 227290, train_perplexity=4782.4565, train_loss=8.47271

Batch 227300, train_perplexity=5377.163, train_loss=8.589916

Batch 227310, train_perplexity=5962.3394, train_loss=8.693218

Batch 227320, train_perplexity=5673.3857, train_loss=8.643541

Batch 227330, train_perplexity=5621.2417, train_loss=8.634308

Batch 227340, train_perplexity=5336.0913, train_loss=8.582249

Batch 227350, train_perplexity=5260.3916, train_loss=8.567961

Batch 227360, train_perplexity=5865.6436, train_loss=8.6768675

Batch 227370, train_perplexity=4638.5884, train_loss=8.442165

Batch 227380, train_perplexity=5989.5757, train_loss=8.697776

Batch 227390, train_perplexity=5279.4746, train_loss=8.571582

Batch 227400, train_perplexity=4515.046, train_loss=8.415171

Batch 227410, train_perplexity=5553.6543, train_loss=8.622211

Batch 227420, train_perplexity=5992.6553, train_loss=8.69829

Batch 227430, train_perplexity=5450.872, train_loss=8.603531

Batch 227440, train_perplexity=4914.525, train_loss=8.49995

Batch 227450, train_perplexity=5363.5605, train_loss=8.587383

Batch 227460, train_perplexity=4867.3364, train_loss=8.490302

Batch 227470, train_perplexity=5383.444, train_loss=8.591084

Batch 227480, train_perplexity=4767.1655, train_loss=8.469507

Batch 227490, train_perplexity=5168.9536, train_loss=8.550426

Batch 227500, train_perplexity=6089.749, train_loss=8.714362

Batch 227510, train_perplexity=4436.3145, train_loss=8.397579

Batch 227520, train_perplexity=3965.8018, train_loss=8.285463

Batch 227530, train_perplexity=5650.933, train_loss=8.639576

Batch 227540, train_perplexity=5000.122, train_loss=8.517218

Batch 227550, train_perplexity=7268.838, train_loss=8.891352

Batch 227560, train_perplexity=4753.6187, train_loss=8.466661

Batch 227570, train_perplexity=4758.8896, train_loss=8.46777

Batch 227580, train_perplexity=5537.8354, train_loss=8.619359

Batch 227590, train_perplexity=5222.6416, train_loss=8.560759

Batch 227600, train_perplexity=5496.8413, train_loss=8.611929

Batch 227610, train_perplexity=5283.035, train_loss=8.572256

Batch 227620, train_perplexity=6040.6123, train_loss=8.706261

Batch 227630, train_perplexity=5637.745, train_loss=8.637239

Batch 227640, train_perplexity=5619.3013, train_loss=8.633963

Batch 227650, train_perplexity=5248.205, train_loss=8.565641

Batch 227660, train_perplexity=5126.875, train_loss=8.542252

Batch 227670, train_perplexity=4034.1091, train_loss=8.302541

Batch 227680, train_perplexity=5503.65, train_loss=8.613167

Batch 227690, train_perplexity=5676.557, train_loss=8.6441

Batch 227700, train_perplexity=6341.9644, train_loss=8.754944

Batch 227710, train_perplexity=6070.4, train_loss=8.71118

Batch 227720, train_perplexity=5452.6606, train_loss=8.603859

Batch 227730, train_perplexity=4850.7236, train_loss=8.486883

Batch 227740, train_perplexity=5802.4287, train_loss=8.666032

Batch 227750, train_perplexity=5011.7803, train_loss=8.5195465

Batch 227760, train_perplexity=4752.608, train_loss=8.466449

Batch 227770, train_perplexity=5482.5176, train_loss=8.60932

Batch 227780, train_perplexity=4819.272, train_loss=8.480378

Batch 227790, train_perplexity=5077.8506, train_loss=8.532643

Batch 227800, train_perplexity=4902.892, train_loss=8.497581

Batch 227810, train_perplexity=5471.194, train_loss=8.607252

Batch 227820, train_perplexity=6402.308, train_loss=8.764414

Batch 227830, train_perplexity=5241.4277, train_loss=8.564349

Batch 227840, train_perplexity=5326.1416, train_loss=8.580382

Batch 227850, train_perplexity=5343.613, train_loss=8.583657

Batch 227860, train_perplexity=5310.0796, train_loss=8.577362

Batch 227870, train_perplexity=4966.5273, train_loss=8.510476

Batch 227880, train_perplexity=6148.8477, train_loss=8.72402

Batch 227890, train_perplexity=5054.124, train_loss=8.52796

Batch 227900, train_perplexity=5453.95, train_loss=8.604095

Batch 227910, train_perplexity=5143.6875, train_loss=8.545526

Batch 227920, train_perplexity=4921.875, train_loss=8.501445

Batch 227930, train_perplexity=4536.393, train_loss=8.419888

Batch 227940, train_perplexity=5682.8296, train_loss=8.645205

Batch 227950, train_perplexity=5798.053, train_loss=8.6652775

Batch 227960, train_perplexity=5275.3774, train_loss=8.570806

Batch 227970, train_perplexity=5470.3066, train_loss=8.60709

Batch 227980, train_perplexity=5064.623, train_loss=8.530035

Batch 227990, train_perplexity=4885.273, train_loss=8.49398

Batch 228000, train_perplexity=5768.59, train_loss=8.660183

Batch 228010, train_perplexity=5496.1704, train_loss=8.611807

Batch 228020, train_perplexity=5580.519, train_loss=8.627037

Batch 228030, train_perplexity=5858.728, train_loss=8.675688

Batch 228040, train_perplexity=5405.256, train_loss=8.595127

Batch 228050, train_perplexity=5257.307, train_loss=8.567374

Batch 228060, train_perplexity=5803.5796, train_loss=8.66623

Batch 228070, train_perplexity=5546.308, train_loss=8.620888

Batch 228080, train_perplexity=5151.159, train_loss=8.546977

Batch 228090, train_perplexity=6577.503, train_loss=8.79141

Batch 228100, train_perplexity=5328.951, train_loss=8.58091

Batch 228110, train_perplexity=5935.85, train_loss=8.688766

Batch 228120, train_perplexity=4867.4336, train_loss=8.490322

Batch 228130, train_perplexity=5046.8364, train_loss=8.526517

Batch 228140, train_perplexity=5926.014, train_loss=8.687107

Batch 228150, train_perplexity=5073.5474, train_loss=8.5317955

Batch 228160, train_perplexity=5189.7285, train_loss=8.554437

Batch 228170, train_perplexity=5889.7974, train_loss=8.680977

Batch 228180, train_perplexity=6971.705, train_loss=8.849615

Batch 228190, train_perplexity=5459.7476, train_loss=8.605158

Batch 228200, train_perplexity=5635.7344, train_loss=8.636883

Batch 228210, train_perplexity=5629.3745, train_loss=8.635754

Batch 228220, train_perplexity=5586.014, train_loss=8.628021

Batch 228230, train_perplexity=4749.599, train_loss=8.465816

Batch 228240, train_perplexity=6385.8926, train_loss=8.761847

Batch 228250, train_perplexity=5097.253, train_loss=8.536457

Batch 228260, train_perplexity=4622.452, train_loss=8.438681

Batch 228270, train_perplexity=5362.394, train_loss=8.587166

Batch 228280, train_perplexity=6145.266, train_loss=8.723437

Batch 228290, train_perplexity=4651.7363, train_loss=8.444996

Batch 228300, train_perplexity=5147.5986, train_loss=8.546286

Batch 228310, train_perplexity=5444.056, train_loss=8.60228

Batch 228320, train_perplexity=5539.119, train_loss=8.619591

Batch 228330, train_perplexity=5148.183, train_loss=8.546399

Batch 228340, train_perplexity=5162.17, train_loss=8.549112

Batch 228350, train_perplexity=6192.584, train_loss=8.731108

Batch 228360, train_perplexity=5456.1455, train_loss=8.604498

Batch 228370, train_perplexity=5269.3994, train_loss=8.569672

Batch 228380, train_perplexity=5563.2812, train_loss=8.623943

Batch 228390, train_perplexity=5010.4424, train_loss=8.5192795

Batch 228400, train_perplexity=5796.439, train_loss=8.664999

Batch 228410, train_perplexity=5509.758, train_loss=8.614276

Batch 228420, train_perplexity=4719.8657, train_loss=8.459536

Batch 228430, train_perplexity=5468.095, train_loss=8.606686

Batch 228440, train_perplexity=5786.7397, train_loss=8.663324
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 228450, train_perplexity=5352.8496, train_loss=8.585384

Batch 228460, train_perplexity=5614.984, train_loss=8.633194

Batch 228470, train_perplexity=5105.5674, train_loss=8.538087

Batch 228480, train_perplexity=4693.741, train_loss=8.453985

Batch 228490, train_perplexity=5695.395, train_loss=8.647413

Batch 228500, train_perplexity=5593.8667, train_loss=8.629426

Batch 228510, train_perplexity=5113.5884, train_loss=8.539657

Batch 228520, train_perplexity=5487.0894, train_loss=8.610153

Batch 228530, train_perplexity=5647.3345, train_loss=8.638939

Batch 228540, train_perplexity=6838.4185, train_loss=8.830312

Batch 228550, train_perplexity=5511.6865, train_loss=8.614626

Batch 228560, train_perplexity=4160.987, train_loss=8.333508

Batch 228570, train_perplexity=5216.5933, train_loss=8.5596

Batch 228580, train_perplexity=5002.0015, train_loss=8.517593

Batch 228590, train_perplexity=4751.9736, train_loss=8.466315

Batch 228600, train_perplexity=5059.9883, train_loss=8.5291195

Batch 228610, train_perplexity=4341.187, train_loss=8.375903

Batch 228620, train_perplexity=5976.7544, train_loss=8.695633

Batch 228630, train_perplexity=5019.534, train_loss=8.521092

Batch 228640, train_perplexity=4815.84, train_loss=8.479666

Batch 228650, train_perplexity=5745.914, train_loss=8.656244

Batch 228660, train_perplexity=5785.0957, train_loss=8.66304

Batch 228670, train_perplexity=4921.523, train_loss=8.501373

Batch 228680, train_perplexity=5119.444, train_loss=8.540801

Batch 228690, train_perplexity=4768.821, train_loss=8.469854

Batch 228700, train_perplexity=7698.2446, train_loss=8.948748

Batch 228710, train_perplexity=5900.817, train_loss=8.682846

Batch 228720, train_perplexity=3986.5742, train_loss=8.290688

Batch 228730, train_perplexity=5174.428, train_loss=8.551484

Batch 228740, train_perplexity=4237.434, train_loss=8.351713

Batch 228750, train_perplexity=5788.6167, train_loss=8.663649

Batch 228760, train_perplexity=5781.764, train_loss=8.662464

Batch 228770, train_perplexity=6309.545, train_loss=8.749819

Batch 228780, train_perplexity=5580.4233, train_loss=8.62702

Batch 228790, train_perplexity=5251.94, train_loss=8.566353

Batch 228800, train_perplexity=5145.189, train_loss=8.545817

Batch 228810, train_perplexity=6496.559, train_loss=8.779028

Batch 228820, train_perplexity=4592.529, train_loss=8.432186

Batch 228830, train_perplexity=5417.363, train_loss=8.597364

Batch 228840, train_perplexity=6105.945, train_loss=8.717018

Batch 228850, train_perplexity=5509.248, train_loss=8.614183

Batch 228860, train_perplexity=4576.885, train_loss=8.428774

Batch 228870, train_perplexity=5055.3965, train_loss=8.528212

Batch 228880, train_perplexity=4753.501, train_loss=8.466637

Batch 228890, train_perplexity=6533.7144, train_loss=8.784731

Batch 228900, train_perplexity=4897.3076, train_loss=8.496441

Batch 228910, train_perplexity=6464.7793, train_loss=8.774124

Batch 228920, train_perplexity=6198.2915, train_loss=8.732029

Batch 228930, train_perplexity=5270.0728, train_loss=8.569799

Batch 228940, train_perplexity=5711.969, train_loss=8.650319

Batch 228950, train_perplexity=6227.894, train_loss=8.7367935

Batch 228960, train_perplexity=4289.1914, train_loss=8.363853

Batch 228970, train_perplexity=4349.3467, train_loss=8.377781

Batch 228980, train_perplexity=5496.0083, train_loss=8.611777

Batch 228990, train_perplexity=6008.392, train_loss=8.700912

Batch 229000, train_perplexity=5919.1685, train_loss=8.685951

Batch 229010, train_perplexity=5674.322, train_loss=8.643706

Batch 229020, train_perplexity=6425.0005, train_loss=8.767952

Batch 229030, train_perplexity=6105.199, train_loss=8.716896

Batch 229040, train_perplexity=5687.975, train_loss=8.64611

Batch 229050, train_perplexity=5000.451, train_loss=8.517283

Batch 229060, train_perplexity=4988.7812, train_loss=8.514947

Batch 229070, train_perplexity=5780.2314, train_loss=8.662199

Batch 229080, train_perplexity=5478.9634, train_loss=8.608671

Batch 229090, train_perplexity=5734.746, train_loss=8.654299

Batch 229100, train_perplexity=5347.6147, train_loss=8.584406

Batch 229110, train_perplexity=6321.856, train_loss=8.751768

Batch 229120, train_perplexity=6099.7812, train_loss=8.716008

Batch 229130, train_perplexity=5976.817, train_loss=8.695643

Batch 229140, train_perplexity=5399.28, train_loss=8.594021

Batch 229150, train_perplexity=6280.428, train_loss=8.7451935

Batch 229160, train_perplexity=5154.2007, train_loss=8.547567

Batch 229170, train_perplexity=5756.379, train_loss=8.658064

Batch 229180, train_perplexity=5694.711, train_loss=8.647293

Batch 229190, train_perplexity=5532.541, train_loss=8.6184025

Batch 229200, train_perplexity=5424.497, train_loss=8.5986805

Batch 229210, train_perplexity=5104.2334, train_loss=8.537826

Batch 229220, train_perplexity=4787.965, train_loss=8.473861

Batch 229230, train_perplexity=5184.342, train_loss=8.553398

Batch 229240, train_perplexity=5841.5444, train_loss=8.67275

Batch 229250, train_perplexity=6190.5645, train_loss=8.730782

Batch 229260, train_perplexity=5259.1978, train_loss=8.567734

Batch 229270, train_perplexity=4679.604, train_loss=8.450969

Batch 229280, train_perplexity=5320.735, train_loss=8.579367

Batch 229290, train_perplexity=4764.493, train_loss=8.468946

Batch 229300, train_perplexity=6219.347, train_loss=8.73542

Batch 229310, train_perplexity=4690.9263, train_loss=8.453385

Batch 229320, train_perplexity=5779.1733, train_loss=8.662016

Batch 229330, train_perplexity=4721.414, train_loss=8.459864

Batch 229340, train_perplexity=5083.2383, train_loss=8.533704

Batch 229350, train_perplexity=6174.9395, train_loss=8.728254

Batch 229360, train_perplexity=6250.63, train_loss=8.7404375

Batch 229370, train_perplexity=5519.103, train_loss=8.615971

Batch 229380, train_perplexity=4514.555, train_loss=8.415062

Batch 229390, train_perplexity=5117.623, train_loss=8.540445

Batch 229400, train_perplexity=4604.317, train_loss=8.43475

Batch 229410, train_perplexity=6008.364, train_loss=8.700908

Batch 229420, train_perplexity=4664.5215, train_loss=8.447741

Batch 229430, train_perplexity=5642.43, train_loss=8.63807

Batch 229440, train_perplexity=6117.5205, train_loss=8.718912

Batch 229450, train_perplexity=5412.901, train_loss=8.59654

Batch 229460, train_perplexity=5377.435, train_loss=8.589967

Batch 229470, train_perplexity=5739.517, train_loss=8.65513

Batch 229480, train_perplexity=5805.545, train_loss=8.666569

Batch 229490, train_perplexity=5360.2876, train_loss=8.586773

Batch 229500, train_perplexity=5451.2617, train_loss=8.603602

Batch 229510, train_perplexity=4690.6895, train_loss=8.453335

Batch 229520, train_perplexity=5243.3574, train_loss=8.564717

Batch 229530, train_perplexity=4865.1504, train_loss=8.489853

Batch 229540, train_perplexity=5178.1353, train_loss=8.5522

Batch 229550, train_perplexity=5340.123, train_loss=8.583004

Batch 229560, train_perplexity=4021.2493, train_loss=8.299348

Batch 229570, train_perplexity=4713.9097, train_loss=8.458273

Batch 229580, train_perplexity=5488.4707, train_loss=8.610405

Batch 229590, train_perplexity=4012.8289, train_loss=8.297252

Batch 229600, train_perplexity=5138.5103, train_loss=8.544518

Batch 229610, train_perplexity=4709.888, train_loss=8.457419

Batch 229620, train_perplexity=5443.6406, train_loss=8.602203

Batch 229630, train_perplexity=6342.648, train_loss=8.755052

Batch 229640, train_perplexity=5567.6387, train_loss=8.624726

Batch 229650, train_perplexity=5032.807, train_loss=8.523733

Batch 229660, train_perplexity=6165.1245, train_loss=8.726664

Batch 229670, train_perplexity=5650.125, train_loss=8.639433

Batch 229680, train_perplexity=5720.6206, train_loss=8.651833

Batch 229690, train_perplexity=6255.43, train_loss=8.741205

Batch 229700, train_perplexity=5306.2573, train_loss=8.576642

Batch 229710, train_perplexity=6267.445, train_loss=8.743124

Batch 229720, train_perplexity=5728.209, train_loss=8.653158

Batch 229730, train_perplexity=5668.7563, train_loss=8.642725

Batch 229740, train_perplexity=5382.3555, train_loss=8.590881

Batch 229750, train_perplexity=5457.884, train_loss=8.604816

Batch 229760, train_perplexity=5258.937, train_loss=8.567684
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 229770, train_perplexity=6037.8594, train_loss=8.705805

Batch 229780, train_perplexity=6778.559, train_loss=8.82152

Batch 229790, train_perplexity=5028.4985, train_loss=8.522877

Batch 229800, train_perplexity=5124.05, train_loss=8.5417

Batch 229810, train_perplexity=4989.338, train_loss=8.5150585

Batch 229820, train_perplexity=5512.012, train_loss=8.614685

Batch 229830, train_perplexity=4991.8794, train_loss=8.515568

Batch 229840, train_perplexity=5514.189, train_loss=8.61508

Batch 229850, train_perplexity=5503.3877, train_loss=8.613119

Batch 229860, train_perplexity=4930.59, train_loss=8.503214

Batch 229870, train_perplexity=5480.155, train_loss=8.608889

Batch 229880, train_perplexity=6004.119, train_loss=8.700201

Batch 229890, train_perplexity=4376.909, train_loss=8.384098

Batch 229900, train_perplexity=5397.637, train_loss=8.593717

Batch 229910, train_perplexity=5408.216, train_loss=8.5956745

Batch 229920, train_perplexity=6032.904, train_loss=8.704984

Batch 229930, train_perplexity=5491.4185, train_loss=8.610942

Batch 229940, train_perplexity=4817.9346, train_loss=8.480101

Batch 229950, train_perplexity=5458.186, train_loss=8.604872

Batch 229960, train_perplexity=4712.849, train_loss=8.458048

Batch 229970, train_perplexity=5239.9434, train_loss=8.564066

Batch 229980, train_perplexity=5759.542, train_loss=8.658613

Batch 229990, train_perplexity=5510.7666, train_loss=8.614459

Batch 230000, train_perplexity=5126.2686, train_loss=8.542133

Batch 230010, train_perplexity=5783.1924, train_loss=8.662711

Batch 230020, train_perplexity=5451.4233, train_loss=8.603632

Batch 230030, train_perplexity=5248.876, train_loss=8.565769

Batch 230040, train_perplexity=6541.6826, train_loss=8.78595

Batch 230050, train_perplexity=4542.337, train_loss=8.421197

Batch 230060, train_perplexity=5512.2754, train_loss=8.614733

Batch 230070, train_perplexity=5612.628, train_loss=8.632774

Batch 230080, train_perplexity=4968.6777, train_loss=8.510909

Batch 230090, train_perplexity=4688.1313, train_loss=8.452789

Batch 230100, train_perplexity=4954.681, train_loss=8.508088

Batch 230110, train_perplexity=5676.8604, train_loss=8.644154

Batch 230120, train_perplexity=6639.5874, train_loss=8.800805

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Batch 230130, train_perplexity=6150.543, train_loss=8.724296

Batch 230140, train_perplexity=5191.3867, train_loss=8.554756

Batch 230150, train_perplexity=5187.299, train_loss=8.553968

Batch 230160, train_perplexity=6399.4756, train_loss=8.763971

Batch 230170, train_perplexity=7245.348, train_loss=8.888115

Batch 230180, train_perplexity=4431.409, train_loss=8.396473

Batch 230190, train_perplexity=5128.9287, train_loss=8.542652

Batch 230200, train_perplexity=7125.9893, train_loss=8.871504

Batch 230210, train_perplexity=4952.5225, train_loss=8.507652

Batch 230220, train_perplexity=5304.881, train_loss=8.576383

Batch 230230, train_perplexity=5173.7915, train_loss=8.551361

Batch 230240, train_perplexity=5265.4507, train_loss=8.568922

Batch 230250, train_perplexity=5998.0127, train_loss=8.699183

Batch 230260, train_perplexity=4478.4614, train_loss=8.407035

Batch 230270, train_perplexity=5435.5015, train_loss=8.600707

Batch 230280, train_perplexity=4618.275, train_loss=8.437777

Batch 230290, train_perplexity=6226.457, train_loss=8.736563

Batch 230300, train_perplexity=6063.526, train_loss=8.710047

Batch 230310, train_perplexity=6104.6753, train_loss=8.71681

Batch 230320, train_perplexity=6140.7725, train_loss=8.722706

Batch 230330, train_perplexity=5414.7754, train_loss=8.596887

Batch 230340, train_perplexity=5059.038, train_loss=8.528932

Batch 230350, train_perplexity=5183.1157, train_loss=8.553162

Batch 230360, train_perplexity=5543.315, train_loss=8.620348

Batch 230370, train_perplexity=5433.983, train_loss=8.600428

Batch 230380, train_perplexity=5046.4565, train_loss=8.526442

Batch 230390, train_perplexity=5672.8013, train_loss=8.643438

Batch 230400, train_perplexity=6305.8633, train_loss=8.749235

Batch 230410, train_perplexity=4823.2217, train_loss=8.481197

Batch 230420, train_perplexity=5230.6567, train_loss=8.562292

Batch 230430, train_perplexity=4927.647, train_loss=8.502617

Batch 230440, train_perplexity=4658.0225, train_loss=8.446346

Batch 230450, train_perplexity=4715.6274, train_loss=8.458637

Batch 230460, train_perplexity=6329.9644, train_loss=8.75305

Batch 230470, train_perplexity=5642.6074, train_loss=8.638102

Batch 230480, train_perplexity=5743.8813, train_loss=8.65589

Batch 230490, train_perplexity=5537.038, train_loss=8.619215

Batch 230500, train_perplexity=5589.1313, train_loss=8.628579

Batch 230510, train_perplexity=5899.832, train_loss=8.682679

Batch 230520, train_perplexity=4836.4688, train_loss=8.48394

Batch 230530, train_perplexity=5213.505, train_loss=8.559008

Batch 230540, train_perplexity=6107.785, train_loss=8.7173195

Batch 230550, train_perplexity=4491.1, train_loss=8.409853

Batch 230560, train_perplexity=5239.0337, train_loss=8.563892

Batch 230570, train_perplexity=5173.2734, train_loss=8.551261

Batch 230580, train_perplexity=6223.382, train_loss=8.736069

Batch 230590, train_perplexity=5965.462, train_loss=8.693742

Batch 230600, train_perplexity=6107.6455, train_loss=8.717297

Batch 230610, train_perplexity=5486.0796, train_loss=8.609969

Batch 230620, train_perplexity=5775.0303, train_loss=8.661299

Batch 230630, train_perplexity=4523.1655, train_loss=8.416967

Batch 230640, train_perplexity=5697.65, train_loss=8.647809

Batch 230650, train_perplexity=4610.4683, train_loss=8.436085

Batch 230660, train_perplexity=5063.4736, train_loss=8.529808

Batch 230670, train_perplexity=4816.9517, train_loss=8.479897

Batch 230680, train_perplexity=6022.838, train_loss=8.703314

Batch 230690, train_perplexity=5299.7383, train_loss=8.575413

Batch 230700, train_perplexity=5836.7666, train_loss=8.671932

Batch 230710, train_perplexity=4790.436, train_loss=8.474377

Batch 230720, train_perplexity=6291.6025, train_loss=8.746971

Batch 230730, train_perplexity=4831.771, train_loss=8.482968

Batch 230740, train_perplexity=5010.834, train_loss=8.519358

Batch 230750, train_perplexity=4965.4995, train_loss=8.510269

Batch 230760, train_perplexity=4746.1265, train_loss=8.465084

Batch 230770, train_perplexity=5439.7275, train_loss=8.601484

Batch 230780, train_perplexity=6580.427, train_loss=8.791855

Batch 230790, train_perplexity=4776.6133, train_loss=8.471487

Batch 230800, train_perplexity=3983.7847, train_loss=8.289988

Batch 230810, train_perplexity=5257.974, train_loss=8.567501

Batch 230820, train_perplexity=5090.952, train_loss=8.53522

Batch 230830, train_perplexity=4362.615, train_loss=8.380827

Batch 230840, train_perplexity=4715.0293, train_loss=8.45851

Batch 230850, train_perplexity=5497.2397, train_loss=8.612001

Batch 230860, train_perplexity=4918.7734, train_loss=8.500814

Batch 230870, train_perplexity=5520.4507, train_loss=8.616215

Batch 230880, train_perplexity=5314.8467, train_loss=8.578259

Batch 230890, train_perplexity=5495.652, train_loss=8.611712

Batch 230900, train_perplexity=5181.712, train_loss=8.552891

Batch 230910, train_perplexity=4905.4927, train_loss=8.498111

Batch 230920, train_perplexity=4179.097, train_loss=8.337851

Batch 230930, train_perplexity=5619.677, train_loss=8.634029

Batch 230940, train_perplexity=5690.183, train_loss=8.646498

Batch 230950, train_perplexity=5373.821, train_loss=8.589294

Batch 230960, train_perplexity=5807.799, train_loss=8.666957

Batch 230970, train_perplexity=5287.945, train_loss=8.573185

Batch 230980, train_perplexity=5416.552, train_loss=8.597215

Batch 230990, train_perplexity=5519.829, train_loss=8.616102

Batch 231000, train_perplexity=5385.0767, train_loss=8.591387

Batch 231010, train_perplexity=5253.483, train_loss=8.566647
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 231020, train_perplexity=5152.0386, train_loss=8.547148

Batch 231030, train_perplexity=4556.681, train_loss=8.42435

Batch 231040, train_perplexity=5038.262, train_loss=8.5248165

Batch 231050, train_perplexity=4165.6245, train_loss=8.334621

Batch 231060, train_perplexity=4065.6753, train_loss=8.310335

Batch 231070, train_perplexity=5400.454, train_loss=8.594238

Batch 231080, train_perplexity=5100.0537, train_loss=8.537006

Batch 231090, train_perplexity=5422.8936, train_loss=8.598385

Batch 231100, train_perplexity=5772.1777, train_loss=8.660805

Batch 231110, train_perplexity=5617.308, train_loss=8.633608

Batch 231120, train_perplexity=5154.8057, train_loss=8.547685

Batch 231130, train_perplexity=6229.5337, train_loss=8.737057

Batch 231140, train_perplexity=5101.562, train_loss=8.537302

Batch 231150, train_perplexity=5773.103, train_loss=8.660965

Batch 231160, train_perplexity=5681.193, train_loss=8.644917

Batch 231170, train_perplexity=7315.404, train_loss=8.8977375

Batch 231180, train_perplexity=5619.8374, train_loss=8.634058

Batch 231190, train_perplexity=4236.65, train_loss=8.351528

Batch 231200, train_perplexity=4743.113, train_loss=8.464449

Batch 231210, train_perplexity=6787.8745, train_loss=8.822893

Batch 231220, train_perplexity=5301.6494, train_loss=8.575773

Batch 231230, train_perplexity=5077.2017, train_loss=8.532516

Batch 231240, train_perplexity=4716.6616, train_loss=8.458857

Batch 231250, train_perplexity=5584.288, train_loss=8.627712

Batch 231260, train_perplexity=5765.7026, train_loss=8.659682

Batch 231270, train_perplexity=5287.7583, train_loss=8.57315

Batch 231280, train_perplexity=5586.2485, train_loss=8.628063

Batch 231290, train_perplexity=4846.7007, train_loss=8.486053

Batch 231300, train_perplexity=6020.931, train_loss=8.702997

Batch 231310, train_perplexity=5517.0293, train_loss=8.615595

Batch 231320, train_perplexity=5810.7627, train_loss=8.667467

Batch 231330, train_perplexity=5205.7544, train_loss=8.55752

Batch 231340, train_perplexity=6735.6035, train_loss=8.815163

Batch 231350, train_perplexity=5491.649, train_loss=8.610984

Batch 231360, train_perplexity=5572.0527, train_loss=8.625519

Batch 231370, train_perplexity=5852.6855, train_loss=8.674656

Batch 231380, train_perplexity=5341.779, train_loss=8.583314

Batch 231390, train_perplexity=6138.594, train_loss=8.722351

Batch 231400, train_perplexity=5308.51, train_loss=8.577066

Batch 231410, train_perplexity=6444.4043, train_loss=8.7709675

Batch 231420, train_perplexity=4622.814, train_loss=8.438759

Batch 231430, train_perplexity=5299.5815, train_loss=8.575383

Batch 231440, train_perplexity=6045.338, train_loss=8.707043

Batch 231450, train_perplexity=4189.257, train_loss=8.340279

Batch 231460, train_perplexity=5831.0693, train_loss=8.670956

Batch 231470, train_perplexity=4848.9106, train_loss=8.486509

Batch 231480, train_perplexity=5555.0264, train_loss=8.622458

Batch 231490, train_perplexity=5897.97, train_loss=8.6823635

Batch 231500, train_perplexity=7269.198, train_loss=8.891401

Batch 231510, train_perplexity=5360.4565, train_loss=8.586804

Batch 231520, train_perplexity=5816.8506, train_loss=8.668514

Batch 231530, train_perplexity=5773.6313, train_loss=8.6610565

Batch 231540, train_perplexity=4764.661, train_loss=8.468982

Batch 231550, train_perplexity=5724.774, train_loss=8.652558

Batch 231560, train_perplexity=6022.8955, train_loss=8.703323

Batch 231570, train_perplexity=4358.4023, train_loss=8.379861

Batch 231580, train_perplexity=5559.4624, train_loss=8.623257

Batch 231590, train_perplexity=4656.9033, train_loss=8.446106

Batch 231600, train_perplexity=4605.841, train_loss=8.435081

Batch 231610, train_perplexity=5250.9785, train_loss=8.56617

Batch 231620, train_perplexity=5812.913, train_loss=8.667837

Batch 231630, train_perplexity=5137.1626, train_loss=8.544256

Batch 231640, train_perplexity=5331.02, train_loss=8.581298

Batch 231650, train_perplexity=5459.149, train_loss=8.605048

Batch 231660, train_perplexity=5567.06, train_loss=8.624622

Batch 231670, train_perplexity=6004.44, train_loss=8.700254

Batch 231680, train_perplexity=4612.131, train_loss=8.436445

Batch 231690, train_perplexity=5310.768, train_loss=8.577492

Batch 231700, train_perplexity=4911.1426, train_loss=8.499262

Batch 231710, train_perplexity=5815.3306, train_loss=8.668253

Batch 231720, train_perplexity=6191.698, train_loss=8.730965

Batch 231730, train_perplexity=5492.597, train_loss=8.611156

Batch 231740, train_perplexity=5011.5845, train_loss=8.519507

Batch 231750, train_perplexity=4524.922, train_loss=8.417356

Batch 231760, train_perplexity=5464.1855, train_loss=8.60597

Batch 231770, train_perplexity=5396.7676, train_loss=8.593555

Batch 231780, train_perplexity=5353.1763, train_loss=8.585445

Batch 231790, train_perplexity=5737.733, train_loss=8.6548195

Batch 231800, train_perplexity=5026.7485, train_loss=8.522529

Batch 231810, train_perplexity=5778.0713, train_loss=8.661825

Batch 231820, train_perplexity=4798.4556, train_loss=8.476049

Batch 231830, train_perplexity=5018.529, train_loss=8.520892

Batch 231840, train_perplexity=6113.065, train_loss=8.7181835

Batch 231850, train_perplexity=6335.031, train_loss=8.75385

Batch 231860, train_perplexity=5260.868, train_loss=8.568051

Batch 231870, train_perplexity=4960.293, train_loss=8.50922

Batch 231880, train_perplexity=5177.997, train_loss=8.552174

Batch 231890, train_perplexity=4618.808, train_loss=8.437892

Batch 231900, train_perplexity=5678.972, train_loss=8.644526

Batch 231910, train_perplexity=4984.791, train_loss=8.514147

Batch 231920, train_perplexity=5362.4146, train_loss=8.58717

Batch 231930, train_perplexity=5714.9824, train_loss=8.6508465

Batch 231940, train_perplexity=5600.448, train_loss=8.630602

Batch 231950, train_perplexity=6687.828, train_loss=8.808044

Batch 231960, train_perplexity=5534.9897, train_loss=8.618845

Batch 231970, train_perplexity=5669.8213, train_loss=8.642913

Batch 231980, train_perplexity=5431.7236, train_loss=8.600012

Batch 231990, train_perplexity=5722.5303, train_loss=8.652166

Batch 232000, train_perplexity=6145.4595, train_loss=8.723469

Batch 232010, train_perplexity=5921.4155, train_loss=8.686331

Batch 232020, train_perplexity=5339.675, train_loss=8.58292

Batch 232030, train_perplexity=5838.3584, train_loss=8.672205

Batch 232040, train_perplexity=5241.4775, train_loss=8.564359

Batch 232050, train_perplexity=5706.1597, train_loss=8.649302

Batch 232060, train_perplexity=5116.237, train_loss=8.5401745

Batch 232070, train_perplexity=6423.922, train_loss=8.767784

Batch 232080, train_perplexity=4880.7417, train_loss=8.4930525

Batch 232090, train_perplexity=5115.754, train_loss=8.54008

Batch 232100, train_perplexity=4971.4033, train_loss=8.511457

Batch 232110, train_perplexity=4359.579, train_loss=8.380131

Batch 232120, train_perplexity=5235.033, train_loss=8.563128

Batch 232130, train_perplexity=5274.1855, train_loss=8.57058

Batch 232140, train_perplexity=4638.854, train_loss=8.442223

Batch 232150, train_perplexity=5186.6265, train_loss=8.553839

Batch 232160, train_perplexity=4614.7046, train_loss=8.437003

Batch 232170, train_perplexity=4668.5356, train_loss=8.448601

Batch 232180, train_perplexity=4891.683, train_loss=8.495292

Batch 232190, train_perplexity=4966.9297, train_loss=8.510557

Batch 232200, train_perplexity=5198.724, train_loss=8.556169

Batch 232210, train_perplexity=4961.575, train_loss=8.509479

Batch 232220, train_perplexity=5430.9365, train_loss=8.599867

Batch 232230, train_perplexity=5511.0664, train_loss=8.614513

Batch 232240, train_perplexity=6068.6284, train_loss=8.710888

Batch 232250, train_perplexity=4921.3916, train_loss=8.501347

Batch 232260, train_perplexity=4942.953, train_loss=8.505718

Batch 232270, train_perplexity=6233.408, train_loss=8.737679

Batch 232280, train_perplexity=5198.263, train_loss=8.55608

Batch 232290, train_perplexity=5974.549, train_loss=8.695264

Batch 232300, train_perplexity=5348.3745, train_loss=8.584548

Batch 232310, train_perplexity=5781.963, train_loss=8.662498

Batch 232320, train_perplexity=4837.2437, train_loss=8.4841

Batch 232330, train_perplexity=4836.6304, train_loss=8.4839735
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 232340, train_perplexity=4974.23, train_loss=8.512026

Batch 232350, train_perplexity=5368.857, train_loss=8.58837

Batch 232360, train_perplexity=5924.4546, train_loss=8.686844

Batch 232370, train_perplexity=6015.685, train_loss=8.702126

Batch 232380, train_perplexity=5593.386, train_loss=8.62934

Batch 232390, train_perplexity=5839.9067, train_loss=8.67247

Batch 232400, train_perplexity=5047.005, train_loss=8.52655

Batch 232410, train_perplexity=5192.5654, train_loss=8.554983

Batch 232420, train_perplexity=4879.4526, train_loss=8.492788

Batch 232430, train_perplexity=4836.6855, train_loss=8.483985

Batch 232440, train_perplexity=5686.348, train_loss=8.6458235

Batch 232450, train_perplexity=5056.356, train_loss=8.528401

Batch 232460, train_perplexity=5545.1763, train_loss=8.620684

Batch 232470, train_perplexity=5901.475, train_loss=8.682958

Batch 232480, train_perplexity=4891.7812, train_loss=8.495312

Batch 232490, train_perplexity=4916.6626, train_loss=8.500385

Batch 232500, train_perplexity=4846.183, train_loss=8.485947

Batch 232510, train_perplexity=5450.3315, train_loss=8.603432

Batch 232520, train_perplexity=4231.8003, train_loss=8.350383

Batch 232530, train_perplexity=5513.737, train_loss=8.614998

Batch 232540, train_perplexity=5176.8223, train_loss=8.551947

Batch 232550, train_perplexity=6261.136, train_loss=8.742117

Batch 232560, train_perplexity=4412.382, train_loss=8.39217

Batch 232570, train_perplexity=5470.4736, train_loss=8.6071205

Batch 232580, train_perplexity=5037.1187, train_loss=8.52459

Batch 232590, train_perplexity=5380.087, train_loss=8.59046

Batch 232600, train_perplexity=4094.547, train_loss=8.317411

Batch 232610, train_perplexity=4936.631, train_loss=8.504438

Batch 232620, train_perplexity=6479.741, train_loss=8.776436

Batch 232630, train_perplexity=7078.394, train_loss=8.864802

Batch 232640, train_perplexity=5327.3203, train_loss=8.580604

Batch 232650, train_perplexity=6491.4062, train_loss=8.7782345

Batch 232660, train_perplexity=5042.492, train_loss=8.525656

Batch 232670, train_perplexity=5135.654, train_loss=8.5439625

Batch 232680, train_perplexity=4439.277, train_loss=8.398247

Batch 232690, train_perplexity=4381.6953, train_loss=8.385191

Batch 232700, train_perplexity=4987.6826, train_loss=8.514727

Batch 232710, train_perplexity=5978.4644, train_loss=8.695919

Batch 232720, train_perplexity=4871.2554, train_loss=8.491107

Batch 232730, train_perplexity=5314.188, train_loss=8.5781355

Batch 232740, train_perplexity=5461.133, train_loss=8.605412

Batch 232750, train_perplexity=5436.025, train_loss=8.600803

Batch 232760, train_perplexity=4670.954, train_loss=8.449119

Batch 232770, train_perplexity=5637.2827, train_loss=8.637157

Batch 232780, train_perplexity=6223.4175, train_loss=8.736074

Batch 232790, train_perplexity=5623.6763, train_loss=8.634741

Batch 232800, train_perplexity=5571.4683, train_loss=8.625414

Batch 232810, train_perplexity=5147.373, train_loss=8.546242

Batch 232820, train_perplexity=4779.119, train_loss=8.472012

Batch 232830, train_perplexity=5722.394, train_loss=8.652143

Batch 232840, train_perplexity=4754.5347, train_loss=8.466854

Batch 232850, train_perplexity=5455.022, train_loss=8.604292

Batch 232860, train_perplexity=5394.8843, train_loss=8.593206

Batch 232870, train_perplexity=5499.925, train_loss=8.61249

Batch 232880, train_perplexity=4660.111, train_loss=8.4467945

Batch 232890, train_perplexity=5329.7744, train_loss=8.581064

Batch 232900, train_perplexity=5099.519, train_loss=8.536901

Batch 232910, train_perplexity=5324.1255, train_loss=8.580004

Batch 232920, train_perplexity=5018.17, train_loss=8.520821

Batch 232930, train_perplexity=5369.999, train_loss=8.588583

Batch 232940, train_perplexity=4830.4395, train_loss=8.482693

Batch 232950, train_perplexity=5635.8154, train_loss=8.636897

Batch 232960, train_perplexity=6007.4985, train_loss=8.700764

Batch 232970, train_perplexity=5382.74, train_loss=8.590953

Batch 232980, train_perplexity=5238.3193, train_loss=8.563756

Batch 232990, train_perplexity=5469.7695, train_loss=8.606992

Batch 233000, train_perplexity=5798.772, train_loss=8.665401

Batch 233010, train_perplexity=5234.3545, train_loss=8.562999

Batch 233020, train_perplexity=5346.2686, train_loss=8.584154

Batch 233030, train_perplexity=5815.8633, train_loss=8.6683445

Batch 233040, train_perplexity=6711.0845, train_loss=8.811516

Batch 233050, train_perplexity=6109.673, train_loss=8.7176285

Batch 233060, train_perplexity=5641.295, train_loss=8.637869

Batch 233070, train_perplexity=6463.3, train_loss=8.773895

Batch 233080, train_perplexity=4792.547, train_loss=8.474817

Batch 233090, train_perplexity=5063.6187, train_loss=8.529837

Batch 233100, train_perplexity=5468.9507, train_loss=8.606842

Batch 233110, train_perplexity=5419.947, train_loss=8.597841

Batch 233120, train_perplexity=5760.6187, train_loss=8.6588

Batch 233130, train_perplexity=5119.5073, train_loss=8.540813

Batch 233140, train_perplexity=5200.6284, train_loss=8.556535

Batch 233150, train_perplexity=5983.792, train_loss=8.69681

Batch 233160, train_perplexity=5252.7417, train_loss=8.566505

Batch 233170, train_perplexity=5030.403, train_loss=8.523255

Batch 233180, train_perplexity=5163.12, train_loss=8.549296

Batch 233190, train_perplexity=5223.807, train_loss=8.560982

Batch 233200, train_perplexity=4726.8433, train_loss=8.461013

Batch 233210, train_perplexity=5004.5205, train_loss=8.518097

Batch 233220, train_perplexity=5670.2485, train_loss=8.642988

Batch 233230, train_perplexity=5306.8394, train_loss=8.576752

Batch 233240, train_perplexity=5979.776, train_loss=8.696138

Batch 233250, train_perplexity=5305.3516, train_loss=8.576471

Batch 233260, train_perplexity=4987.5684, train_loss=8.514704

Batch 233270, train_perplexity=5273.823, train_loss=8.570511

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Batch 233280, train_perplexity=4917.8447, train_loss=8.500626

Batch 233290, train_perplexity=5087.613, train_loss=8.534564

Batch 233300, train_perplexity=5392.4355, train_loss=8.592752

Batch 233310, train_perplexity=5039.4106, train_loss=8.525044

Batch 233320, train_perplexity=4913.222, train_loss=8.499685

Batch 233330, train_perplexity=5027.209, train_loss=8.52262

Batch 233340, train_perplexity=6048.948, train_loss=8.70764

Batch 233350, train_perplexity=5242.932, train_loss=8.564636

Batch 233360, train_perplexity=5898.7856, train_loss=8.682502

Batch 233370, train_perplexity=5851.6143, train_loss=8.674473

Batch 233380, train_perplexity=6381.1685, train_loss=8.7611065

Batch 233390, train_perplexity=4826.2124, train_loss=8.481817

Batch 233400, train_perplexity=5745.8535, train_loss=8.656234

Batch 233410, train_perplexity=4906.62, train_loss=8.498341

Batch 233420, train_perplexity=5511.166, train_loss=8.6145315

Batch 233430, train_perplexity=6335.774, train_loss=8.753967

Batch 233440, train_perplexity=5899.4775, train_loss=8.682619

Batch 233450, train_perplexity=4635.537, train_loss=8.441507

Batch 233460, train_perplexity=6210.0835, train_loss=8.73393

Batch 233470, train_perplexity=4227.904, train_loss=8.349462

Batch 233480, train_perplexity=4595.8896, train_loss=8.432918

Batch 233490, train_perplexity=4636.8945, train_loss=8.4418

Batch 233500, train_perplexity=5363.6475, train_loss=8.5873995

Batch 233510, train_perplexity=5570.937, train_loss=8.625319

Batch 233520, train_perplexity=4959.8296, train_loss=8.509127

Batch 233530, train_perplexity=5381.8936, train_loss=8.5907955

Batch 233540, train_perplexity=5555.9375, train_loss=8.6226225

Batch 233550, train_perplexity=5829.09, train_loss=8.670616

Batch 233560, train_perplexity=4515.791, train_loss=8.415336

Batch 233570, train_perplexity=5563.3555, train_loss=8.623957

Batch 233580, train_perplexity=6530.743, train_loss=8.784276
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 233590, train_perplexity=5644.658, train_loss=8.638465

Batch 233600, train_perplexity=6071.3027, train_loss=8.7113285

Batch 233610, train_perplexity=4775.1377, train_loss=8.471178

Batch 233620, train_perplexity=5234.03, train_loss=8.562937

Batch 233630, train_perplexity=5056.935, train_loss=8.528516

Batch 233640, train_perplexity=6784.8975, train_loss=8.822454

Batch 233650, train_perplexity=4707.557, train_loss=8.456924

Batch 233660, train_perplexity=4750.7183, train_loss=8.466051

Batch 233670, train_perplexity=4930.4863, train_loss=8.503193

Batch 233680, train_perplexity=4698.1973, train_loss=8.454934

Batch 233690, train_perplexity=5616.339, train_loss=8.633435

Batch 233700, train_perplexity=5394.308, train_loss=8.5931

Batch 233710, train_perplexity=5647.4473, train_loss=8.638959

Batch 233720, train_perplexity=6257.3457, train_loss=8.741511

Batch 233730, train_perplexity=4726.947, train_loss=8.461035

Batch 233740, train_perplexity=6723.025, train_loss=8.813293

Batch 233750, train_perplexity=4907.5747, train_loss=8.498535

Batch 233760, train_perplexity=5608.1123, train_loss=8.631969

Batch 233770, train_perplexity=5995.159, train_loss=8.698708

Batch 233780, train_perplexity=5102.89, train_loss=8.537562

Batch 233790, train_perplexity=4051.0261, train_loss=8.3067255

Batch 233800, train_perplexity=6149.927, train_loss=8.7241955

Batch 233810, train_perplexity=6060.8896, train_loss=8.709612

Batch 233820, train_perplexity=5741.455, train_loss=8.655468

Batch 233830, train_perplexity=5251.249, train_loss=8.566221

Batch 233840, train_perplexity=6019.3984, train_loss=8.702743

Batch 233850, train_perplexity=5420.9907, train_loss=8.598034

Batch 233860, train_perplexity=4623.114, train_loss=8.438824

Batch 233870, train_perplexity=5531.823, train_loss=8.618273

Batch 233880, train_perplexity=5467.3184, train_loss=8.606544

Batch 233890, train_perplexity=5136.3887, train_loss=8.544106

Batch 233900, train_perplexity=4737.3716, train_loss=8.463238

Batch 233910, train_perplexity=6465.8584, train_loss=8.774291

Batch 233920, train_perplexity=5284.522, train_loss=8.572537

Batch 233930, train_perplexity=5820.846, train_loss=8.669201

Batch 233940, train_perplexity=5741.581, train_loss=8.65549

Batch 233950, train_perplexity=4699.1245, train_loss=8.455132

Batch 233960, train_perplexity=5290.709, train_loss=8.573708

Batch 233970, train_perplexity=5231.9937, train_loss=8.562548

Batch 233980, train_perplexity=5436.7197, train_loss=8.600931

Batch 233990, train_perplexity=6755.0884, train_loss=8.818051

Batch 234000, train_perplexity=5497.8794, train_loss=8.612118

Batch 234010, train_perplexity=4258.4473, train_loss=8.35666

Batch 234020, train_perplexity=5537.471, train_loss=8.619293

Batch 234030, train_perplexity=3924.6147, train_loss=8.275023

Batch 234040, train_perplexity=6083.776, train_loss=8.713381

Batch 234050, train_perplexity=6385.1133, train_loss=8.761724

Batch 234060, train_perplexity=5432.2056, train_loss=8.6001005

Batch 234070, train_perplexity=4523.64, train_loss=8.417072

Batch 234080, train_perplexity=6381.138, train_loss=8.761102

Batch 234090, train_perplexity=5208.575, train_loss=8.558062

Batch 234100, train_perplexity=5160.5063, train_loss=8.54879

Batch 234110, train_perplexity=5029.9663, train_loss=8.523169

Batch 234120, train_perplexity=5252.962, train_loss=8.566547

Batch 234130, train_perplexity=5185.9785, train_loss=8.553714

Batch 234140, train_perplexity=5450.7266, train_loss=8.603504

Batch 234150, train_perplexity=5338.84, train_loss=8.582764

Batch 234160, train_perplexity=6258.33, train_loss=8.741669

Batch 234170, train_perplexity=4810.607, train_loss=8.478579

Batch 234180, train_perplexity=6756.2095, train_loss=8.818217

Batch 234190, train_perplexity=6038.723, train_loss=8.705948

Batch 234200, train_perplexity=6335.8706, train_loss=8.753983

Batch 234210, train_perplexity=5779.2944, train_loss=8.662037

Batch 234220, train_perplexity=4658.3735, train_loss=8.446422

Batch 234230, train_perplexity=4725.3423, train_loss=8.460695

Batch 234240, train_perplexity=5403.2153, train_loss=8.594749

Batch 234250, train_perplexity=4920.5747, train_loss=8.501181

Batch 234260, train_perplexity=5599.7964, train_loss=8.630486

Batch 234270, train_perplexity=5485.038, train_loss=8.609779

Batch 234280, train_perplexity=4889.9434, train_loss=8.494936

Batch 234290, train_perplexity=4310.297, train_loss=8.368762

Batch 234300, train_perplexity=5489.3296, train_loss=8.610561

Batch 234310, train_perplexity=4894.413, train_loss=8.49585

Batch 234320, train_perplexity=7471.0713, train_loss=8.918794

Batch 234330, train_perplexity=4680.5723, train_loss=8.451176

Batch 234340, train_perplexity=4578.535, train_loss=8.429134

Batch 234350, train_perplexity=5458.0815, train_loss=8.604853

Batch 234360, train_perplexity=4439.023, train_loss=8.39819

Batch 234370, train_perplexity=5141.177, train_loss=8.545037

Batch 234380, train_perplexity=4813.14, train_loss=8.479105

Batch 234390, train_perplexity=6008.301, train_loss=8.700897

Batch 234400, train_perplexity=5693.7114, train_loss=8.647118

Batch 234410, train_perplexity=4898.812, train_loss=8.496748

Batch 234420, train_perplexity=4803.327, train_loss=8.477064

Batch 234430, train_perplexity=5960.691, train_loss=8.692942

Batch 234440, train_perplexity=5873.0264, train_loss=8.678125

Batch 234450, train_perplexity=5867.4897, train_loss=8.677182

Batch 234460, train_perplexity=5429.331, train_loss=8.599571

Batch 234470, train_perplexity=5451.4854, train_loss=8.603643

Batch 234480, train_perplexity=4950.8223, train_loss=8.507309

Batch 234490, train_perplexity=5883.7344, train_loss=8.679947

Batch 234500, train_perplexity=6122.2827, train_loss=8.71969

Batch 234510, train_perplexity=6157.4624, train_loss=8.72542

Batch 234520, train_perplexity=5991.101, train_loss=8.69803

Batch 234530, train_perplexity=5548.7734, train_loss=8.621332

Batch 234540, train_perplexity=5898.859, train_loss=8.682514

Batch 234550, train_perplexity=4979.8877, train_loss=8.513163

Batch 234560, train_perplexity=5211.01, train_loss=8.558529

Batch 234570, train_perplexity=5986.692, train_loss=8.697294

Batch 234580, train_perplexity=5377.9224, train_loss=8.590057

Batch 234590, train_perplexity=4299.7373, train_loss=8.366309

Batch 234600, train_perplexity=5385.9854, train_loss=8.591556

Batch 234610, train_perplexity=5596.4116, train_loss=8.629881

Batch 234620, train_perplexity=5271.847, train_loss=8.570136

Batch 234630, train_perplexity=4430.9653, train_loss=8.396373

Batch 234640, train_perplexity=5328.6465, train_loss=8.5808525

Batch 234650, train_perplexity=4937.469, train_loss=8.504608

Batch 234660, train_perplexity=5017.055, train_loss=8.520598

Batch 234670, train_perplexity=5574.4287, train_loss=8.625945

Batch 234680, train_perplexity=4971.1475, train_loss=8.511406

Batch 234690, train_perplexity=4421.3877, train_loss=8.394209

Batch 234700, train_perplexity=5111.297, train_loss=8.539208

Batch 234710, train_perplexity=5764.35, train_loss=8.659448

Batch 234720, train_perplexity=5026.159, train_loss=8.522411

Batch 234730, train_perplexity=6069.462, train_loss=8.711025

Batch 234740, train_perplexity=5678.4414, train_loss=8.644432

Batch 234750, train_perplexity=7205.046, train_loss=8.882537

Batch 234760, train_perplexity=5561.647, train_loss=8.62365

Batch 234770, train_perplexity=5445.6763, train_loss=8.602577

Batch 234780, train_perplexity=4729.8916, train_loss=8.461658

Batch 234790, train_perplexity=5486.294, train_loss=8.610008

Batch 234800, train_perplexity=4696.795, train_loss=8.454636

Batch 234810, train_perplexity=5608.337, train_loss=8.6320095

Batch 234820, train_perplexity=5185.5483, train_loss=8.553631

Batch 234830, train_perplexity=4937.959, train_loss=8.504707

Batch 234840, train_perplexity=5493.0005, train_loss=8.61123

Batch 234850, train_perplexity=5285.9683, train_loss=8.572811

Batch 234860, train_perplexity=5945.033, train_loss=8.690311

Batch 234870, train_perplexity=5615.878, train_loss=8.633353

Batch 234880, train_perplexity=5151.891, train_loss=8.547119

Batch 234890, train_perplexity=4944.9004, train_loss=8.506112

Batch 234900, train_perplexity=5384.6553, train_loss=8.591309
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 234910, train_perplexity=5402.7207, train_loss=8.594658

Batch 234920, train_perplexity=4317.661, train_loss=8.370469

Batch 234930, train_perplexity=4470.3496, train_loss=8.405222

Batch 234940, train_perplexity=5913.4697, train_loss=8.684988

Batch 234950, train_perplexity=4911.8965, train_loss=8.499415

Batch 234960, train_perplexity=5302.7817, train_loss=8.575987

Batch 234970, train_perplexity=4944.9478, train_loss=8.506122

Batch 234980, train_perplexity=5490.1616, train_loss=8.610713

Batch 234990, train_perplexity=6000.5474, train_loss=8.699606

Batch 235000, train_perplexity=5043.8296, train_loss=8.525921

Batch 235010, train_perplexity=5218.8574, train_loss=8.560034

Batch 235020, train_perplexity=5237.43, train_loss=8.563586

Batch 235030, train_perplexity=4926.2607, train_loss=8.502336

Batch 235040, train_perplexity=5073.731, train_loss=8.531832

Batch 235050, train_perplexity=6425.111, train_loss=8.767969

Batch 235060, train_perplexity=5166.1987, train_loss=8.549892

Batch 235070, train_perplexity=5878.703, train_loss=8.679091

Batch 235080, train_perplexity=4778.8823, train_loss=8.471962

Batch 235090, train_perplexity=4903.1494, train_loss=8.497633

Batch 235100, train_perplexity=5340.434, train_loss=8.583062

Batch 235110, train_perplexity=5856.572, train_loss=8.67532

Batch 235120, train_perplexity=5185.073, train_loss=8.553539

Batch 235130, train_perplexity=5200.9014, train_loss=8.556587

Batch 235140, train_perplexity=5470.4946, train_loss=8.607124

Batch 235150, train_perplexity=5287.844, train_loss=8.573166

Batch 235160, train_perplexity=5370.5728, train_loss=8.58869

Batch 235170, train_perplexity=5683.1006, train_loss=8.645252

Batch 235180, train_perplexity=6145.0024, train_loss=8.723394

Batch 235190, train_perplexity=5407.628, train_loss=8.595566

Batch 235200, train_perplexity=5317.9243, train_loss=8.578838

Batch 235210, train_perplexity=5721.87, train_loss=8.652051

Batch 235220, train_perplexity=6646.626, train_loss=8.801865

Batch 235230, train_perplexity=6050.46, train_loss=8.70789

Batch 235240, train_perplexity=5227.2256, train_loss=8.561636

Batch 235250, train_perplexity=6303.7046, train_loss=8.748893

Batch 235260, train_perplexity=6013.3623, train_loss=8.701739

Batch 235270, train_perplexity=5038.983, train_loss=8.52496

Batch 235280, train_perplexity=5284.184, train_loss=8.572474

Batch 235290, train_perplexity=5726.1772, train_loss=8.652803

Batch 235300, train_perplexity=5770.5376, train_loss=8.660521

Batch 235310, train_perplexity=6453.3403, train_loss=8.772353

Batch 235320, train_perplexity=6630.7095, train_loss=8.799467

Batch 235330, train_perplexity=4252.1245, train_loss=8.355174

Batch 235340, train_perplexity=5144.5464, train_loss=8.545692

Batch 235350, train_perplexity=4696.983, train_loss=8.454676

Batch 235360, train_perplexity=4946.2163, train_loss=8.506378

Batch 235370, train_perplexity=5304.2485, train_loss=8.576263

Batch 235380, train_perplexity=5918.8975, train_loss=8.685905

Batch 235390, train_perplexity=5666.54, train_loss=8.642334

Batch 235400, train_perplexity=6240.82, train_loss=8.738867

Batch 235410, train_perplexity=5271.8823, train_loss=8.570143

Batch 235420, train_perplexity=4990.594, train_loss=8.51531

Batch 235430, train_perplexity=6040.837, train_loss=8.706298

Batch 235440, train_perplexity=5311.234, train_loss=8.5775795

Batch 235450, train_perplexity=5492.461, train_loss=8.611132

Batch 235460, train_perplexity=5956.3154, train_loss=8.692207

Batch 235470, train_perplexity=5149.0127, train_loss=8.54656

Batch 235480, train_perplexity=6402.247, train_loss=8.764404

Batch 235490, train_perplexity=6188.9707, train_loss=8.730524

Batch 235500, train_perplexity=5200.827, train_loss=8.556573

Batch 235510, train_perplexity=4235.729, train_loss=8.351311

Batch 235520, train_perplexity=5781.643, train_loss=8.662443

Batch 235530, train_perplexity=6902.584, train_loss=8.839651

Batch 235540, train_perplexity=4600.077, train_loss=8.433828

Batch 235550, train_perplexity=5096.0864, train_loss=8.536228

Batch 235560, train_perplexity=4498.1987, train_loss=8.411432

Batch 235570, train_perplexity=6456.72, train_loss=8.772877

Batch 235580, train_perplexity=5001.4814, train_loss=8.517489

Batch 235590, train_perplexity=5802.827, train_loss=8.6661005

Batch 235600, train_perplexity=5921.1216, train_loss=8.686281

Batch 235610, train_perplexity=4713.739, train_loss=8.458237

Batch 235620, train_perplexity=5919.8286, train_loss=8.686063

Batch 235630, train_perplexity=5128.7236, train_loss=8.542612

Batch 235640, train_perplexity=4908.782, train_loss=8.498781

Batch 235650, train_perplexity=5855.935, train_loss=8.675211

Batch 235660, train_perplexity=5980.055, train_loss=8.696185

Batch 235670, train_perplexity=5002.4404, train_loss=8.517681

Batch 235680, train_perplexity=5425.7285, train_loss=8.598907

Batch 235690, train_perplexity=6143.3853, train_loss=8.723131

Batch 235700, train_perplexity=5280.3657, train_loss=8.571751

Batch 235710, train_perplexity=5481.2944, train_loss=8.609097

Batch 235720, train_perplexity=5121.8706, train_loss=8.541275

Batch 235730, train_perplexity=6638.467, train_loss=8.800636

Batch 235740, train_perplexity=4580.754, train_loss=8.429619

Batch 235750, train_perplexity=5748.9287, train_loss=8.656769

Batch 235760, train_perplexity=5907.6523, train_loss=8.684004

Batch 235770, train_perplexity=5338.0103, train_loss=8.582608

Batch 235780, train_perplexity=5245.7383, train_loss=8.565171

Batch 235790, train_perplexity=6012.909, train_loss=8.701664

Batch 235800, train_perplexity=4651.834, train_loss=8.445017

Batch 235810, train_perplexity=4523.0664, train_loss=8.416945

Batch 235820, train_perplexity=4857.2275, train_loss=8.488223

Batch 235830, train_perplexity=4463.317, train_loss=8.403647

Batch 235840, train_perplexity=5331.2896, train_loss=8.581348

Batch 235850, train_perplexity=4852.75, train_loss=8.487301

Batch 235860, train_perplexity=4685.8247, train_loss=8.452297

Batch 235870, train_perplexity=5447.3125, train_loss=8.602878

Batch 235880, train_perplexity=5220.8237, train_loss=8.5604105

Batch 235890, train_perplexity=4937.9307, train_loss=8.504702

Batch 235900, train_perplexity=6250.481, train_loss=8.740414

Batch 235910, train_perplexity=4708.087, train_loss=8.457037

Batch 235920, train_perplexity=5923.6406, train_loss=8.686707

Batch 235930, train_perplexity=5703.777, train_loss=8.648884

Batch 235940, train_perplexity=5207.4575, train_loss=8.557847

Batch 235950, train_perplexity=4965.698, train_loss=8.510309

Batch 235960, train_perplexity=6912.1357, train_loss=8.841034

Batch 235970, train_perplexity=4881.761, train_loss=8.493261

Batch 235980, train_perplexity=5497.497, train_loss=8.612048

Batch 235990, train_perplexity=5780.6724, train_loss=8.662275

Batch 236000, train_perplexity=5376.2866, train_loss=8.589753

Batch 236010, train_perplexity=5431.9, train_loss=8.600044

Batch 236020, train_perplexity=5064.1787, train_loss=8.529947

Batch 236030, train_perplexity=6543.1426, train_loss=8.786173

Batch 236040, train_perplexity=5788.186, train_loss=8.663574

Batch 236050, train_perplexity=5376.4966, train_loss=8.589792

Batch 236060, train_perplexity=5348.492, train_loss=8.58457

Batch 236070, train_perplexity=5786.089, train_loss=8.663212

Batch 236080, train_perplexity=4731.935, train_loss=8.46209

Batch 236090, train_perplexity=4667.1514, train_loss=8.448304

Batch 236100, train_perplexity=5802.2017, train_loss=8.665993

Batch 236110, train_perplexity=5058.2373, train_loss=8.528773

Batch 236120, train_perplexity=4897.6255, train_loss=8.496506

Batch 236130, train_perplexity=5134.743, train_loss=8.543785

Batch 236140, train_perplexity=5911.4116, train_loss=8.68464

Batch 236150, train_perplexity=4428.849, train_loss=8.395895

Batch 236160, train_perplexity=5436.1597, train_loss=8.600828

Batch 236170, train_perplexity=4504.3887, train_loss=8.412807

Batch 236180, train_perplexity=4943.8677, train_loss=8.505903

Batch 236190, train_perplexity=5893.365, train_loss=8.681582

Batch 236200, train_perplexity=4730.825, train_loss=8.461855

Batch 236210, train_perplexity=5528.8276, train_loss=8.617731

Batch 236220, train_perplexity=5024.108, train_loss=8.522003
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 236230, train_perplexity=5225.2124, train_loss=8.561251

Batch 236240, train_perplexity=5082.8457, train_loss=8.533627

Batch 236250, train_perplexity=4982.5146, train_loss=8.51369

Batch 236260, train_perplexity=5013.5493, train_loss=8.519899

Batch 236270, train_perplexity=6143.309, train_loss=8.723119

Batch 236280, train_perplexity=4328.3594, train_loss=8.372944

Batch 236290, train_perplexity=5557.522, train_loss=8.622908

Batch 236300, train_perplexity=5896.0635, train_loss=8.68204

Batch 236310, train_perplexity=4564.7363, train_loss=8.426116

Batch 236320, train_perplexity=5218.32, train_loss=8.559931

Batch 236330, train_perplexity=5955.429, train_loss=8.692059

Batch 236340, train_perplexity=5027.252, train_loss=8.522629

Batch 236350, train_perplexity=5054.857, train_loss=8.528105

Batch 236360, train_perplexity=4411.33, train_loss=8.391932

Batch 236370, train_perplexity=5350.1855, train_loss=8.584887

Batch 236380, train_perplexity=6275.7466, train_loss=8.744448

Batch 236390, train_perplexity=5213.813, train_loss=8.559067

Batch 236400, train_perplexity=5257.523, train_loss=8.567415

Batch 236410, train_perplexity=5214.534, train_loss=8.559205

Batch 236420, train_perplexity=5804.0444, train_loss=8.66631

Batch 236430, train_perplexity=4901.6577, train_loss=8.497329

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Batch 236440, train_perplexity=5829.312, train_loss=8.670654

Batch 236450, train_perplexity=4760.469, train_loss=8.4681015

Batch 236460, train_perplexity=4995.9565, train_loss=8.516384

Batch 236470, train_perplexity=5209.832, train_loss=8.558303

Batch 236480, train_perplexity=5906.661, train_loss=8.683836

Batch 236490, train_perplexity=4621.3506, train_loss=8.438442

Batch 236500, train_perplexity=5662.332, train_loss=8.641591

Batch 236510, train_perplexity=5107.9927, train_loss=8.538562

Batch 236520, train_perplexity=6525.9116, train_loss=8.783536

Batch 236530, train_perplexity=5206.8564, train_loss=8.557732

Batch 236540, train_perplexity=4479.423, train_loss=8.407249

Batch 236550, train_perplexity=4614.4624, train_loss=8.436951

Batch 236560, train_perplexity=5617.8066, train_loss=8.633697

Batch 236570, train_perplexity=6402.357, train_loss=8.764421

Batch 236580, train_perplexity=4093.1455, train_loss=8.317069

Batch 236590, train_perplexity=5205.6055, train_loss=8.557491

Batch 236600, train_perplexity=5551.5522, train_loss=8.621833

Batch 236610, train_perplexity=4653.05, train_loss=8.445278

Batch 236620, train_perplexity=5077.6616, train_loss=8.532606

Batch 236630, train_perplexity=5203.868, train_loss=8.5571575

Batch 236640, train_perplexity=5989.924, train_loss=8.697834

Batch 236650, train_perplexity=4885.152, train_loss=8.493956

Batch 236660, train_perplexity=4865.3823, train_loss=8.489901

Batch 236670, train_perplexity=5644.4375, train_loss=8.638426

Batch 236680, train_perplexity=4961.571, train_loss=8.509478

Batch 236690, train_perplexity=5348.053, train_loss=8.584488

Batch 236700, train_perplexity=5370.793, train_loss=8.588731

Batch 236710, train_perplexity=5275.6743, train_loss=8.570862

Batch 236720, train_perplexity=5559.8228, train_loss=8.623322

Batch 236730, train_perplexity=4812.08, train_loss=8.478885

Batch 236740, train_perplexity=5622.2495, train_loss=8.634487

Batch 236750, train_perplexity=5575.3057, train_loss=8.626102

Batch 236760, train_perplexity=5874.9087, train_loss=8.678446

Batch 236770, train_perplexity=5408.1953, train_loss=8.595671

Batch 236780, train_perplexity=4773.5576, train_loss=8.470847

Batch 236790, train_perplexity=5899.804, train_loss=8.682674

Batch 236800, train_perplexity=4834.343, train_loss=8.4835005

Batch 236810, train_perplexity=5979.816, train_loss=8.696145

Batch 236820, train_perplexity=4652.85, train_loss=8.445235

Batch 236830, train_perplexity=5191.8525, train_loss=8.554846

Batch 236840, train_perplexity=6352.7393, train_loss=8.756641

Batch 236850, train_perplexity=5773.1416, train_loss=8.660972

Batch 236860, train_perplexity=5089.423, train_loss=8.53492

Batch 236870, train_perplexity=5781.7534, train_loss=8.662462

Batch 236880, train_perplexity=5500.7534, train_loss=8.61264

Batch 236890, train_perplexity=6156.4287, train_loss=8.725252

Batch 236900, train_perplexity=5941.474, train_loss=8.689713

Batch 236910, train_perplexity=5905.129, train_loss=8.683577

Batch 236920, train_perplexity=5905.9854, train_loss=8.683722

Batch 236930, train_perplexity=6241.3076, train_loss=8.738945

Batch 236940, train_perplexity=4897.093, train_loss=8.496397

Batch 236950, train_perplexity=6210.0776, train_loss=8.733929

Batch 236960, train_perplexity=5624.009, train_loss=8.6348

Batch 236970, train_perplexity=4836.9854, train_loss=8.484047

Batch 236980, train_perplexity=5405.3384, train_loss=8.595142

Batch 236990, train_perplexity=4769.08, train_loss=8.469909

Batch 237000, train_perplexity=5542.3633, train_loss=8.620176

Batch 237010, train_perplexity=5123.5415, train_loss=8.541601

Batch 237020, train_perplexity=5723.4365, train_loss=8.652325

Batch 237030, train_perplexity=5012.818, train_loss=8.519753

Batch 237040, train_perplexity=4974.2583, train_loss=8.512032

Batch 237050, train_perplexity=4370.9775, train_loss=8.382742

Batch 237060, train_perplexity=6024.9233, train_loss=8.70366

Batch 237070, train_perplexity=5002.8696, train_loss=8.517767

Batch 237080, train_perplexity=5676.9146, train_loss=8.644163

Batch 237090, train_perplexity=4349.081, train_loss=8.37772

Batch 237100, train_perplexity=5239.4136, train_loss=8.563965

Batch 237110, train_perplexity=5304.3193, train_loss=8.576277

Batch 237120, train_perplexity=5353.207, train_loss=8.585451

Batch 237130, train_perplexity=5143.3247, train_loss=8.545455

Batch 237140, train_perplexity=4131.174, train_loss=8.326317

Batch 237150, train_perplexity=4704.38, train_loss=8.456249

Batch 237160, train_perplexity=4813.3926, train_loss=8.479157

Batch 237170, train_perplexity=5174.2754, train_loss=8.551455

Batch 237180, train_perplexity=5697.237, train_loss=8.647737

Batch 237190, train_perplexity=6271.3667, train_loss=8.74375

Batch 237200, train_perplexity=6467.9365, train_loss=8.774612

Batch 237210, train_perplexity=5326.949, train_loss=8.580534

Batch 237220, train_perplexity=4917.4224, train_loss=8.50054

Batch 237230, train_perplexity=5581.0776, train_loss=8.627137

Batch 237240, train_perplexity=5899.461, train_loss=8.682616

Batch 237250, train_perplexity=6958.5664, train_loss=8.847729

Batch 237260, train_perplexity=8228.036, train_loss=9.015303

Batch 237270, train_perplexity=5591.4023, train_loss=8.628985

Batch 237280, train_perplexity=5529.4077, train_loss=8.617836

Batch 237290, train_perplexity=4681.519, train_loss=8.451378

Batch 237300, train_perplexity=5801.5156, train_loss=8.6658745

Batch 237310, train_perplexity=7132.591, train_loss=8.87243

Batch 237320, train_perplexity=5620.2124, train_loss=8.634125

Batch 237330, train_perplexity=6067.1875, train_loss=8.71065

Batch 237340, train_perplexity=4862.6084, train_loss=8.48933

Batch 237350, train_perplexity=5291.6025, train_loss=8.573876

Batch 237360, train_perplexity=5201.4966, train_loss=8.556702

Batch 237370, train_perplexity=5513.032, train_loss=8.61487

Batch 237380, train_perplexity=4984.4775, train_loss=8.514084

Batch 237390, train_perplexity=5859.611, train_loss=8.675838

Batch 237400, train_perplexity=4830.2964, train_loss=8.482663

Batch 237410, train_perplexity=6126.5933, train_loss=8.720394

Batch 237420, train_perplexity=6677.4146, train_loss=8.806486

Batch 237430, train_perplexity=5132.5303, train_loss=8.543354

Batch 237440, train_perplexity=5420.329, train_loss=8.597912

Batch 237450, train_perplexity=5907.878, train_loss=8.684042

Batch 237460, train_perplexity=5753.5415, train_loss=8.657571

Batch 237470, train_perplexity=4973.8027, train_loss=8.51194
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 237480, train_perplexity=5663.3633, train_loss=8.641773

Batch 237490, train_perplexity=4734.6934, train_loss=8.462672

Batch 237500, train_perplexity=5412.3433, train_loss=8.596437

Batch 237510, train_perplexity=4591.189, train_loss=8.431894

Batch 237520, train_perplexity=5322.4146, train_loss=8.579682

Batch 237530, train_perplexity=4679.412, train_loss=8.450928

Batch 237540, train_perplexity=5841.6167, train_loss=8.672763

Batch 237550, train_perplexity=5774.16, train_loss=8.661148

Batch 237560, train_perplexity=5089.292, train_loss=8.534894

Batch 237570, train_perplexity=5420.8823, train_loss=8.598014

Batch 237580, train_perplexity=6231.7974, train_loss=8.73742

Batch 237590, train_perplexity=5188.407, train_loss=8.554182

Batch 237600, train_perplexity=4586.8345, train_loss=8.430945

Batch 237610, train_perplexity=7260.87, train_loss=8.890255

Batch 237620, train_perplexity=6059.728, train_loss=8.70942

Batch 237630, train_perplexity=4951.172, train_loss=8.50738

Batch 237640, train_perplexity=6010.169, train_loss=8.701208

Batch 237650, train_perplexity=6146.6787, train_loss=8.723667

Batch 237660, train_perplexity=6292.443, train_loss=8.747105

Batch 237670, train_perplexity=5377.63, train_loss=8.590003

Batch 237680, train_perplexity=4661.009, train_loss=8.446987

Batch 237690, train_perplexity=5231.525, train_loss=8.562458

Batch 237700, train_perplexity=5267.279, train_loss=8.569269

Batch 237710, train_perplexity=6367.867, train_loss=8.75902

Batch 237720, train_perplexity=7037.3965, train_loss=8.858994

Batch 237730, train_perplexity=6099.153, train_loss=8.715905

Batch 237740, train_perplexity=5135.7227, train_loss=8.543976

Batch 237750, train_perplexity=5542.739, train_loss=8.620244

Batch 237760, train_perplexity=5829.09, train_loss=8.670616

Batch 237770, train_perplexity=4985.5806, train_loss=8.514305

Batch 237780, train_perplexity=6277.7935, train_loss=8.744774

Batch 237790, train_perplexity=5953.8223, train_loss=8.691789

Batch 237800, train_perplexity=5262.9507, train_loss=8.568447

Batch 237810, train_perplexity=5326.9087, train_loss=8.580526

Batch 237820, train_perplexity=4395.5903, train_loss=8.388357

Batch 237830, train_perplexity=6264.803, train_loss=8.7427025

Batch 237840, train_perplexity=5864.2734, train_loss=8.676634

Batch 237850, train_perplexity=5670.5244, train_loss=8.643037

Batch 237860, train_perplexity=7041.881, train_loss=8.859631

Batch 237870, train_perplexity=5101.1826, train_loss=8.537228

Batch 237880, train_perplexity=6173.726, train_loss=8.728058

Batch 237890, train_perplexity=5559.9927, train_loss=8.623352

Batch 237900, train_perplexity=5754.913, train_loss=8.657809

Batch 237910, train_perplexity=4501.8633, train_loss=8.412247

Batch 237920, train_perplexity=4747.838, train_loss=8.465445

Batch 237930, train_perplexity=5695.5796, train_loss=8.647446

Batch 237940, train_perplexity=6622.3423, train_loss=8.798204

Batch 237950, train_perplexity=5035.438, train_loss=8.524256

Batch 237960, train_perplexity=6315.4863, train_loss=8.75076

Batch 237970, train_perplexity=5124.807, train_loss=8.541848

Batch 237980, train_perplexity=5370.363, train_loss=8.588651

Batch 237990, train_perplexity=5156.571, train_loss=8.548027

Batch 238000, train_perplexity=4272.7056, train_loss=8.3600025

Batch 238010, train_perplexity=5060.2637, train_loss=8.529174

Batch 238020, train_perplexity=5558.847, train_loss=8.623146

Batch 238030, train_perplexity=5031.041, train_loss=8.523382

Batch 238040, train_perplexity=4609.009, train_loss=8.435768

Batch 238050, train_perplexity=5111.243, train_loss=8.539198

Batch 238060, train_perplexity=6233.902, train_loss=8.737758

Batch 238070, train_perplexity=5139.917, train_loss=8.544792

Batch 238080, train_perplexity=4762.558, train_loss=8.46854

Batch 238090, train_perplexity=6747.536, train_loss=8.816933

Batch 238100, train_perplexity=5125.4375, train_loss=8.541971

Batch 238110, train_perplexity=5247.8047, train_loss=8.565565

Batch 238120, train_perplexity=5479.695, train_loss=8.608805

Batch 238130, train_perplexity=5772.756, train_loss=8.660905

Batch 238140, train_perplexity=5126.088, train_loss=8.542098

Batch 238150, train_perplexity=5893.202, train_loss=8.681555

Batch 238160, train_perplexity=6356.557, train_loss=8.757242

Batch 238170, train_perplexity=4531.944, train_loss=8.418906

Batch 238180, train_perplexity=5597.1055, train_loss=8.630005

Batch 238190, train_perplexity=5482.6533, train_loss=8.6093445

Batch 238200, train_perplexity=6561.5894, train_loss=8.788988

Batch 238210, train_perplexity=4675.638, train_loss=8.450121

Batch 238220, train_perplexity=6373.3105, train_loss=8.759874

Batch 238230, train_perplexity=4877.3774, train_loss=8.492363

Batch 238240, train_perplexity=5348.6807, train_loss=8.584605

Batch 238250, train_perplexity=4967.7637, train_loss=8.510725

Batch 238260, train_perplexity=4973.4, train_loss=8.511859

Batch 238270, train_perplexity=7550.917, train_loss=8.929424

Batch 238280, train_perplexity=5306.829, train_loss=8.57675

Batch 238290, train_perplexity=5162.6133, train_loss=8.549198

Batch 238300, train_perplexity=6832.174, train_loss=8.829398

Batch 238310, train_perplexity=5468.429, train_loss=8.606747

Batch 238320, train_perplexity=4599.2173, train_loss=8.433641

Batch 238330, train_perplexity=5322.389, train_loss=8.579678

Batch 238340, train_perplexity=5450.035, train_loss=8.603377

Batch 238350, train_perplexity=5979.6846, train_loss=8.696123

Batch 238360, train_perplexity=5589.6963, train_loss=8.62868

Batch 238370, train_perplexity=5056.626, train_loss=8.528455

Batch 238380, train_perplexity=5612.1787, train_loss=8.632694

Batch 238390, train_perplexity=5119.0044, train_loss=8.540715

Batch 238400, train_perplexity=5533.3853, train_loss=8.618555

Batch 238410, train_perplexity=5514.8833, train_loss=8.615206

Batch 238420, train_perplexity=4839.5737, train_loss=8.484582

Batch 238430, train_perplexity=4906.2363, train_loss=8.498262

Batch 238440, train_perplexity=6005.161, train_loss=8.700375

Batch 238450, train_perplexity=5274.658, train_loss=8.570669

Batch 238460, train_perplexity=4304.837, train_loss=8.367495

Batch 238470, train_perplexity=6036.5005, train_loss=8.70558

Batch 238480, train_perplexity=5423.2456, train_loss=8.59845

Batch 238490, train_perplexity=5417.311, train_loss=8.597355

Batch 238500, train_perplexity=5759.399, train_loss=8.658588

Batch 238510, train_perplexity=5731.302, train_loss=8.653698

Batch 238520, train_perplexity=4466.412, train_loss=8.404341

Batch 238530, train_perplexity=5679.3945, train_loss=8.6446

Batch 238540, train_perplexity=6465.513, train_loss=8.774238

Batch 238550, train_perplexity=5489.853, train_loss=8.610657

Batch 238560, train_perplexity=5420.712, train_loss=8.597982

Batch 238570, train_perplexity=5819.8857, train_loss=8.669036

Batch 238580, train_perplexity=5391.757, train_loss=8.592627

Batch 238590, train_perplexity=5580.54, train_loss=8.627041

Batch 238600, train_perplexity=5725.1504, train_loss=8.652624

Batch 238610, train_perplexity=3778.4104, train_loss=8.237059

Batch 238620, train_perplexity=6996.909, train_loss=8.853224

Batch 238630, train_perplexity=5064.604, train_loss=8.530031

Batch 238640, train_perplexity=5932.9126, train_loss=8.688271

Batch 238650, train_perplexity=5215.9316, train_loss=8.559473

Batch 238660, train_perplexity=5176.0024, train_loss=8.551788

Batch 238670, train_perplexity=6846.4385, train_loss=8.831484

Batch 238680, train_perplexity=4825.3745, train_loss=8.481644

Batch 238690, train_perplexity=5928.23, train_loss=8.687481

Batch 238700, train_perplexity=4670.21, train_loss=8.448959

Batch 238710, train_perplexity=5091.7725, train_loss=8.535381

Batch 238720, train_perplexity=5094.25, train_loss=8.535868

Batch 238730, train_perplexity=5668.183, train_loss=8.642624

Batch 238740, train_perplexity=5112.34, train_loss=8.5394125

Batch 238750, train_perplexity=4693.522, train_loss=8.4539385

Batch 238760, train_perplexity=6017.774, train_loss=8.702473

Batch 238770, train_perplexity=5476.8423, train_loss=8.608284

Batch 238780, train_perplexity=5962.021, train_loss=8.693165

Batch 238790, train_perplexity=5869.2974, train_loss=8.67749
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 238800, train_perplexity=4862.993, train_loss=8.489409

Batch 238810, train_perplexity=6385.6123, train_loss=8.761803

Batch 238820, train_perplexity=5481.6704, train_loss=8.609165

Batch 238830, train_perplexity=6015.2036, train_loss=8.702045

Batch 238840, train_perplexity=5362.89, train_loss=8.587258

Batch 238850, train_perplexity=5160.093, train_loss=8.54871

Batch 238860, train_perplexity=5938.7437, train_loss=8.689253

Batch 238870, train_perplexity=5857.5938, train_loss=8.675494

Batch 238880, train_perplexity=4327.625, train_loss=8.372774

Batch 238890, train_perplexity=4256.8027, train_loss=8.356274

Batch 238900, train_perplexity=4827.9478, train_loss=8.482177

Batch 238910, train_perplexity=5242.2925, train_loss=8.564514

Batch 238920, train_perplexity=5139.0933, train_loss=8.544632

Batch 238930, train_perplexity=5040.05, train_loss=8.525171

Batch 238940, train_perplexity=5315.8047, train_loss=8.57844

Batch 238950, train_perplexity=5049.6865, train_loss=8.5270815

Batch 238960, train_perplexity=4624.723, train_loss=8.439172

Batch 238970, train_perplexity=4956.071, train_loss=8.5083685

Batch 238980, train_perplexity=5781.0034, train_loss=8.662333

Batch 238990, train_perplexity=5348.13, train_loss=8.584502

Batch 239000, train_perplexity=6113.4844, train_loss=8.718252

Batch 239010, train_perplexity=5532.182, train_loss=8.618338

Batch 239020, train_perplexity=5864.7207, train_loss=8.67671

Batch 239030, train_perplexity=5465.197, train_loss=8.606155

Batch 239040, train_perplexity=5682.8945, train_loss=8.645216

Batch 239050, train_perplexity=5259.8096, train_loss=8.56785

Batch 239060, train_perplexity=5326.8984, train_loss=8.580524

Batch 239070, train_perplexity=7684.792, train_loss=8.946999

Batch 239080, train_perplexity=6508.304, train_loss=8.780834

Batch 239090, train_perplexity=4380.9766, train_loss=8.385027

Batch 239100, train_perplexity=5182.4336, train_loss=8.55303

Batch 239110, train_perplexity=5554.1045, train_loss=8.6222925

Batch 239120, train_perplexity=6463.2134, train_loss=8.773882

Batch 239130, train_perplexity=4787.673, train_loss=8.4738

Batch 239140, train_perplexity=6115.7236, train_loss=8.718618

Batch 239150, train_perplexity=5123.7466, train_loss=8.541641

Batch 239160, train_perplexity=4559.102, train_loss=8.424881

Batch 239170, train_perplexity=5970.875, train_loss=8.694649

Batch 239180, train_perplexity=4651.0044, train_loss=8.444839

Batch 239190, train_perplexity=5471.1313, train_loss=8.607241

Batch 239200, train_perplexity=5883.139, train_loss=8.679846

Batch 239210, train_perplexity=5243.2124, train_loss=8.56469

Batch 239220, train_perplexity=5196.152, train_loss=8.555674

Batch 239230, train_perplexity=4834.1816, train_loss=8.483467

Batch 239240, train_perplexity=5463.2896, train_loss=8.605806

Batch 239250, train_perplexity=5429.6416, train_loss=8.599628

Batch 239260, train_perplexity=6495.89, train_loss=8.778925

Batch 239270, train_perplexity=5532.5303, train_loss=8.618401

Batch 239280, train_perplexity=4980.733, train_loss=8.513332

Batch 239290, train_perplexity=6913.23, train_loss=8.841192

Batch 239300, train_perplexity=6181.597, train_loss=8.729332

Batch 239310, train_perplexity=5734.3306, train_loss=8.654226

Batch 239320, train_perplexity=5414.6255, train_loss=8.596859

Batch 239330, train_perplexity=5011.427, train_loss=8.519476

Batch 239340, train_perplexity=4596.5425, train_loss=8.43306

Batch 239350, train_perplexity=6180.4067, train_loss=8.729139

Batch 239360, train_perplexity=5490.811, train_loss=8.610831

Batch 239370, train_perplexity=6044.9404, train_loss=8.706977

Batch 239380, train_perplexity=7717.283, train_loss=8.951218

Batch 239390, train_perplexity=4064.1558, train_loss=8.309961

Batch 239400, train_perplexity=5454.2104, train_loss=8.604143

Batch 239410, train_perplexity=6638.3525, train_loss=8.800619

Batch 239420, train_perplexity=5212.6, train_loss=8.558834

Batch 239430, train_perplexity=4987.844, train_loss=8.514759

Batch 239440, train_perplexity=5436.139, train_loss=8.600824

Batch 239450, train_perplexity=4441.945, train_loss=8.398848

Batch 239460, train_perplexity=5212.8833, train_loss=8.558888

Batch 239470, train_perplexity=5090.7725, train_loss=8.535185

Batch 239480, train_perplexity=4629.5156, train_loss=8.4402075

Batch 239490, train_perplexity=5452.889, train_loss=8.603901

Batch 239500, train_perplexity=5882.2754, train_loss=8.679699

Batch 239510, train_perplexity=4618.76, train_loss=8.437881

Batch 239520, train_perplexity=5911.5527, train_loss=8.684664

Batch 239530, train_perplexity=4354.9, train_loss=8.379057

Batch 239540, train_perplexity=4726.1353, train_loss=8.460863

Batch 239550, train_perplexity=5795.1123, train_loss=8.66477

Batch 239560, train_perplexity=5162.367, train_loss=8.54915

Batch 239570, train_perplexity=5398.353, train_loss=8.593849

Batch 239580, train_perplexity=5873.9453, train_loss=8.678282

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Batch 239590, train_perplexity=4817.186, train_loss=8.479945

Batch 239600, train_perplexity=5543.1562, train_loss=8.620319

Batch 239610, train_perplexity=5196.097, train_loss=8.555663

Batch 239620, train_perplexity=5753.3164, train_loss=8.657532

Batch 239630, train_perplexity=5293.909, train_loss=8.574312

Batch 239640, train_perplexity=5442.6753, train_loss=8.602026

Batch 239650, train_perplexity=5522.462, train_loss=8.616579

Batch 239660, train_perplexity=4555.634, train_loss=8.42412

Batch 239670, train_perplexity=5398.4097, train_loss=8.59386

Batch 239680, train_perplexity=4689.236, train_loss=8.453025

Batch 239690, train_perplexity=5056.5635, train_loss=8.528442

Batch 239700, train_perplexity=5764.0034, train_loss=8.659388

Batch 239710, train_perplexity=5342.925, train_loss=8.5835285

Batch 239720, train_perplexity=5435.04, train_loss=8.600622

Batch 239730, train_perplexity=6171.884, train_loss=8.727759

Batch 239740, train_perplexity=4869.5186, train_loss=8.49075

Batch 239750, train_perplexity=5478.65, train_loss=8.608614

Batch 239760, train_perplexity=4847.149, train_loss=8.486146

Batch 239770, train_perplexity=5977.735, train_loss=8.695797

Batch 239780, train_perplexity=4481.2515, train_loss=8.407658

Batch 239790, train_perplexity=5490.743, train_loss=8.610819

Batch 239800, train_perplexity=5590.597, train_loss=8.628841

Batch 239810, train_perplexity=5954.5093, train_loss=8.691904

Batch 239820, train_perplexity=4945.433, train_loss=8.50622

Batch 239830, train_perplexity=4713.613, train_loss=8.45821

Batch 239840, train_perplexity=4564.967, train_loss=8.426167

Batch 239850, train_perplexity=4856.287, train_loss=8.4880295

Batch 239860, train_perplexity=6464.742, train_loss=8.774118

Batch 239870, train_perplexity=6939.639, train_loss=8.845005

Batch 239880, train_perplexity=5634.961, train_loss=8.636745

Batch 239890, train_perplexity=5259.99, train_loss=8.567884

Batch 239900, train_perplexity=6479.5557, train_loss=8.776407

Batch 239910, train_perplexity=4956.458, train_loss=8.508447

Batch 239920, train_perplexity=5627.163, train_loss=8.635361

Batch 239930, train_perplexity=5142.4565, train_loss=8.545286

Batch 239940, train_perplexity=4632.081, train_loss=8.440762

Batch 239950, train_perplexity=5294.222, train_loss=8.574371

Batch 239960, train_perplexity=5126.6353, train_loss=8.542205

Batch 239970, train_perplexity=5032.999, train_loss=8.523771

Batch 239980, train_perplexity=5176.467, train_loss=8.551878

Batch 239990, train_perplexity=5306.49, train_loss=8.576686

Batch 240000, train_perplexity=6139.022, train_loss=8.722421

Batch 240010, train_perplexity=5660.734, train_loss=8.641309

Batch 240020, train_perplexity=6257.835, train_loss=8.74159

Batch 240030, train_perplexity=6015.8975, train_loss=8.702161

Batch 240040, train_perplexity=5216.0063, train_loss=8.559487

Batch 240050, train_perplexity=4256.957, train_loss=8.35631
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 240060, train_perplexity=4750.152, train_loss=8.465932

Batch 240070, train_perplexity=5657.733, train_loss=8.640779

Batch 240080, train_perplexity=5879.281, train_loss=8.67919

Batch 240090, train_perplexity=5610.279, train_loss=8.632356

Batch 240100, train_perplexity=4911.6855, train_loss=8.4993725

Batch 240110, train_perplexity=5197.698, train_loss=8.555971

Batch 240120, train_perplexity=5573.1475, train_loss=8.625715

Batch 240130, train_perplexity=5428.177, train_loss=8.599359

Batch 240140, train_perplexity=5375.7993, train_loss=8.589663

Batch 240150, train_perplexity=4720.217, train_loss=8.45961

Batch 240160, train_perplexity=5130.954, train_loss=8.543047

Batch 240170, train_perplexity=5152.2646, train_loss=8.547192

Batch 240180, train_perplexity=5786.1772, train_loss=8.663227

Batch 240190, train_perplexity=5677.6724, train_loss=8.644297

Batch 240200, train_perplexity=5267.475, train_loss=8.569306

Batch 240210, train_perplexity=4963.317, train_loss=8.5098295

Batch 240220, train_perplexity=4854.4907, train_loss=8.487659

Batch 240230, train_perplexity=4576.0645, train_loss=8.428595

Batch 240240, train_perplexity=4438.5146, train_loss=8.398075

Batch 240250, train_perplexity=5494.787, train_loss=8.611555

Batch 240260, train_perplexity=5453.3677, train_loss=8.603989

Batch 240270, train_perplexity=5202.6475, train_loss=8.556923

Batch 240280, train_perplexity=6205.5903, train_loss=8.733206

Batch 240290, train_perplexity=5036.7344, train_loss=8.524513

Batch 240300, train_perplexity=4701.2764, train_loss=8.455589

Batch 240310, train_perplexity=4540.219, train_loss=8.420731

Batch 240320, train_perplexity=5371.9966, train_loss=8.588955

Batch 240330, train_perplexity=4753.211, train_loss=8.466576

Batch 240340, train_perplexity=4851.9263, train_loss=8.487131

Batch 240350, train_perplexity=4898.41, train_loss=8.496666

Batch 240360, train_perplexity=5751.0234, train_loss=8.657133

Batch 240370, train_perplexity=6064.174, train_loss=8.710154

Batch 240380, train_perplexity=4793.05, train_loss=8.474922

Batch 240390, train_perplexity=5471.4497, train_loss=8.607299

Batch 240400, train_perplexity=5570.342, train_loss=8.625212

Batch 240410, train_perplexity=5757.834, train_loss=8.658317

Batch 240420, train_perplexity=4209.706, train_loss=8.345148

Batch 240430, train_perplexity=5151.4785, train_loss=8.547039

Batch 240440, train_perplexity=5181.0303, train_loss=8.552759

Batch 240450, train_perplexity=5437.041, train_loss=8.60099

Batch 240460, train_perplexity=5255.728, train_loss=8.567074

Batch 240470, train_perplexity=4576.802, train_loss=8.428756

Batch 240480, train_perplexity=4648.322, train_loss=8.444262

Batch 240490, train_perplexity=6295.342, train_loss=8.747565

Batch 240500, train_perplexity=5335.4297, train_loss=8.582125

Batch 240510, train_perplexity=5087.6274, train_loss=8.534567

Batch 240520, train_perplexity=5443.3604, train_loss=8.602152

Batch 240530, train_perplexity=4382.5854, train_loss=8.385394

Batch 240540, train_perplexity=5429.621, train_loss=8.599625

Batch 240550, train_perplexity=5556.155, train_loss=8.622662

Batch 240560, train_perplexity=5089.782, train_loss=8.53499

Batch 240570, train_perplexity=5418.5205, train_loss=8.597578

Batch 240580, train_perplexity=5513.826, train_loss=8.615014

Batch 240590, train_perplexity=5065.908, train_loss=8.530289

Batch 240600, train_perplexity=5260.0303, train_loss=8.567892

Batch 240610, train_perplexity=6719.371, train_loss=8.81275

Batch 240620, train_perplexity=6364.546, train_loss=8.758498

Batch 240630, train_perplexity=5246.2134, train_loss=8.565262

Batch 240640, train_perplexity=6265.7476, train_loss=8.742853

Batch 240650, train_perplexity=4950.152, train_loss=8.507174

Batch 240660, train_perplexity=7056.7783, train_loss=8.861744

Batch 240670, train_perplexity=5177.928, train_loss=8.55216

Batch 240680, train_perplexity=6450.282, train_loss=8.771879

Batch 240690, train_perplexity=5677.0986, train_loss=8.644196

Batch 240700, train_perplexity=5595.462, train_loss=8.629711

Batch 240710, train_perplexity=5004.979, train_loss=8.518188

Batch 240720, train_perplexity=6050.431, train_loss=8.707885

Batch 240730, train_perplexity=5420.577, train_loss=8.597958

Batch 240740, train_perplexity=6224.747, train_loss=8.736288

Batch 240750, train_perplexity=5282.0527, train_loss=8.57207

Batch 240760, train_perplexity=5499.4634, train_loss=8.612406

Batch 240770, train_perplexity=5218.678, train_loss=8.559999

Batch 240780, train_perplexity=4714.629, train_loss=8.4584255

Batch 240790, train_perplexity=5436.305, train_loss=8.600855

Batch 240800, train_perplexity=5430.7085, train_loss=8.599825

Batch 240810, train_perplexity=4348.6665, train_loss=8.3776245

Batch 240820, train_perplexity=6530.737, train_loss=8.784275

Batch 240830, train_perplexity=5482.7163, train_loss=8.609356

Batch 240840, train_perplexity=5630.4536, train_loss=8.635945

Batch 240850, train_perplexity=4982.7334, train_loss=8.513734

Batch 240860, train_perplexity=5006.678, train_loss=8.518528

Batch 240870, train_perplexity=6825.5503, train_loss=8.828428

Batch 240880, train_perplexity=12565.464, train_loss=9.438707

Batch 240890, train_perplexity=5321.9985, train_loss=8.579604

Batch 240900, train_perplexity=11613.299, train_loss=9.359906

Batch 240910, train_perplexity=4782.616, train_loss=8.472743

Batch 240920, train_perplexity=4658.3335, train_loss=8.446413

Batch 240930, train_perplexity=6642.6846, train_loss=8.801271

Batch 240940, train_perplexity=5226.448, train_loss=8.561487

Batch 240950, train_perplexity=5028.575, train_loss=8.522892

Batch 240960, train_perplexity=5199.701, train_loss=8.556356

Batch 240970, train_perplexity=5929.191, train_loss=8.687643

Batch 240980, train_perplexity=4121.4854, train_loss=8.323969

Batch 240990, train_perplexity=6918.8496, train_loss=8.842005

Batch 241000, train_perplexity=4589.2104, train_loss=8.431463

Batch 241010, train_perplexity=5229.7188, train_loss=8.562113

Batch 241020, train_perplexity=4917.469, train_loss=8.500549

Batch 241030, train_perplexity=4069.6555, train_loss=8.311314

Batch 241040, train_perplexity=6421.3003, train_loss=8.767376

Batch 241050, train_perplexity=6153.928, train_loss=8.724846

Batch 241060, train_perplexity=5291.996, train_loss=8.573951

Batch 241070, train_perplexity=4829.1313, train_loss=8.482422

Batch 241080, train_perplexity=6143.1685, train_loss=8.723096

Batch 241090, train_perplexity=4718.5786, train_loss=8.459263

Batch 241100, train_perplexity=5884.4414, train_loss=8.680067

Batch 241110, train_perplexity=5384.9175, train_loss=8.591357

Batch 241120, train_perplexity=5558.7095, train_loss=8.623121

Batch 241130, train_perplexity=5276.429, train_loss=8.571005

Batch 241140, train_perplexity=5469.4307, train_loss=8.60693

Batch 241150, train_perplexity=6183.484, train_loss=8.729637

Batch 241160, train_perplexity=5776.8037, train_loss=8.661606

Batch 241170, train_perplexity=5115.5635, train_loss=8.540043

Batch 241180, train_perplexity=5417.2803, train_loss=8.597349

Batch 241190, train_perplexity=4648.3086, train_loss=8.444259

Batch 241200, train_perplexity=6446.9985, train_loss=8.77137

Batch 241210, train_perplexity=4918.905, train_loss=8.500841

Batch 241220, train_perplexity=5420.588, train_loss=8.5979595

Batch 241230, train_perplexity=5025.435, train_loss=8.522267

Batch 241240, train_perplexity=6408.1235, train_loss=8.765322

Batch 241250, train_perplexity=4626.479, train_loss=8.439551

Batch 241260, train_perplexity=6836.2666, train_loss=8.829997

Batch 241270, train_perplexity=5276.4795, train_loss=8.571014

Batch 241280, train_perplexity=6441.166, train_loss=8.770465

Batch 241290, train_perplexity=6141.2236, train_loss=8.722779

Batch 241300, train_perplexity=5614.566, train_loss=8.63312

Batch 241310, train_perplexity=4615.7563, train_loss=8.437231

Batch 241320, train_perplexity=5290.669, train_loss=8.5737

Batch 241330, train_perplexity=4345.897, train_loss=8.376987

Batch 241340, train_perplexity=4939.8525, train_loss=8.505091

Batch 241350, train_perplexity=5816.1074, train_loss=8.668386

Batch 241360, train_perplexity=6344.0815, train_loss=8.755278

Batch 241370, train_perplexity=5178.866, train_loss=8.552341
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 241380, train_perplexity=4819.534, train_loss=8.4804325

Batch 241390, train_perplexity=6212.85, train_loss=8.734375

Batch 241400, train_perplexity=5384.6655, train_loss=8.5913105

Batch 241410, train_perplexity=5245.413, train_loss=8.565109

Batch 241420, train_perplexity=5371.448, train_loss=8.588853

Batch 241430, train_perplexity=5679.3945, train_loss=8.6446

Batch 241440, train_perplexity=5254.8057, train_loss=8.566898

Batch 241450, train_perplexity=5558.8896, train_loss=8.623154

Batch 241460, train_perplexity=6097.612, train_loss=8.715652

Batch 241470, train_perplexity=5949.8657, train_loss=8.691124

Batch 241480, train_perplexity=5203.9575, train_loss=8.557175

Batch 241490, train_perplexity=5792.1064, train_loss=8.664251

Batch 241500, train_perplexity=5586.5146, train_loss=8.628111

Batch 241510, train_perplexity=5294.6665, train_loss=8.574455

Batch 241520, train_perplexity=5085.0664, train_loss=8.534063

Batch 241530, train_perplexity=4472.034, train_loss=8.405599

Batch 241540, train_perplexity=5206.221, train_loss=8.55761

Batch 241550, train_perplexity=5532.9316, train_loss=8.618473

Batch 241560, train_perplexity=5405.5137, train_loss=8.595175

Batch 241570, train_perplexity=6842.848, train_loss=8.830959

Batch 241580, train_perplexity=5563.939, train_loss=8.624062

Batch 241590, train_perplexity=5502.469, train_loss=8.612952

Batch 241600, train_perplexity=4991.6606, train_loss=8.515524

Batch 241610, train_perplexity=6443.7036, train_loss=8.770859

Batch 241620, train_perplexity=5815.3364, train_loss=8.668254

Batch 241630, train_perplexity=5765.5264, train_loss=8.659652

Batch 241640, train_perplexity=4729.057, train_loss=8.461481

Batch 241650, train_perplexity=5581.5195, train_loss=8.627216

Batch 241660, train_perplexity=5383.2075, train_loss=8.59104

Batch 241670, train_perplexity=4576.448, train_loss=8.4286785

Batch 241680, train_perplexity=4852.815, train_loss=8.487314

Batch 241690, train_perplexity=5405.7305, train_loss=8.595215

Batch 241700, train_perplexity=4645.1577, train_loss=8.443581

Batch 241710, train_perplexity=5257.8184, train_loss=8.5674715

Batch 241720, train_perplexity=5721.035, train_loss=8.651905

Batch 241730, train_perplexity=5741.4663, train_loss=8.65547

Batch 241740, train_perplexity=5299.0864, train_loss=8.57529

Batch 241750, train_perplexity=5108.6113, train_loss=8.538683

Batch 241760, train_perplexity=5734.522, train_loss=8.65426

Batch 241770, train_perplexity=4220.7607, train_loss=8.347771

Batch 241780, train_perplexity=4607.717, train_loss=8.435488

Batch 241790, train_perplexity=5248.275, train_loss=8.565655

Batch 241800, train_perplexity=5432.3765, train_loss=8.600132

Batch 241810, train_perplexity=5102.311, train_loss=8.537449

Batch 241820, train_perplexity=5997.2803, train_loss=8.699061

Batch 241830, train_perplexity=4440.5513, train_loss=8.398534

Batch 241840, train_perplexity=5123.0186, train_loss=8.541499

Batch 241850, train_perplexity=4775.0146, train_loss=8.471152

Batch 241860, train_perplexity=5110.483, train_loss=8.539049

Batch 241870, train_perplexity=5319.588, train_loss=8.579151

Batch 241880, train_perplexity=4397.7627, train_loss=8.388851

Batch 241890, train_perplexity=4979.9873, train_loss=8.513183

Batch 241900, train_perplexity=6610.48, train_loss=8.7964115

Batch 241910, train_perplexity=6393.1074, train_loss=8.762976

Batch 241920, train_perplexity=5283.227, train_loss=8.572292

Batch 241930, train_perplexity=4973.9546, train_loss=8.5119705

Batch 241940, train_perplexity=6032.334, train_loss=8.704889

Batch 241950, train_perplexity=5298.733, train_loss=8.575223

Batch 241960, train_perplexity=6246.965, train_loss=8.739851

Batch 241970, train_perplexity=5643.4795, train_loss=8.638256

Batch 241980, train_perplexity=5293.0005, train_loss=8.574141

Batch 241990, train_perplexity=4669.8716, train_loss=8.448887

Batch 242000, train_perplexity=5382.191, train_loss=8.590851

Batch 242010, train_perplexity=5826.6167, train_loss=8.670192

Batch 242020, train_perplexity=5125.3496, train_loss=8.541954

Batch 242030, train_perplexity=5513.658, train_loss=8.614984

Batch 242040, train_perplexity=5244.1123, train_loss=8.564861

Batch 242050, train_perplexity=5367.8896, train_loss=8.58819

Batch 242060, train_perplexity=5783.959, train_loss=8.662844

Batch 242070, train_perplexity=6396.0225, train_loss=8.763432

Batch 242080, train_perplexity=4949.7554, train_loss=8.507093

Batch 242090, train_perplexity=5427.799, train_loss=8.599289

Batch 242100, train_perplexity=4969.0713, train_loss=8.510988

Batch 242110, train_perplexity=5303.7026, train_loss=8.57616

Batch 242120, train_perplexity=5468.0117, train_loss=8.60667

Batch 242130, train_perplexity=6050.362, train_loss=8.707873

Batch 242140, train_perplexity=5959.0938, train_loss=8.692674

Batch 242150, train_perplexity=4689.58, train_loss=8.453098

Batch 242160, train_perplexity=5172.07, train_loss=8.551028

Batch 242170, train_perplexity=4648.065, train_loss=8.444206

Batch 242180, train_perplexity=5324.8364, train_loss=8.580137

Batch 242190, train_perplexity=5878.0977, train_loss=8.678988

Batch 242200, train_perplexity=5781.9404, train_loss=8.662495

Batch 242210, train_perplexity=5585.332, train_loss=8.627899

Batch 242220, train_perplexity=5273.924, train_loss=8.57053

Batch 242230, train_perplexity=5217.8125, train_loss=8.559834

Batch 242240, train_perplexity=6079.2754, train_loss=8.712641

Batch 242250, train_perplexity=5847.8315, train_loss=8.673826

Batch 242260, train_perplexity=4505.8496, train_loss=8.413132

Batch 242270, train_perplexity=4660.866, train_loss=8.446957

Batch 242280, train_perplexity=6760.0054, train_loss=8.818779

Batch 242290, train_perplexity=4701.689, train_loss=8.455677

Batch 242300, train_perplexity=4734.165, train_loss=8.462561

Batch 242310, train_perplexity=5832.037, train_loss=8.671122

Batch 242320, train_perplexity=6223.3105, train_loss=8.736057

Batch 242330, train_perplexity=5553.771, train_loss=8.622232

Batch 242340, train_perplexity=4838.1616, train_loss=8.48429

Batch 242350, train_perplexity=5367.101, train_loss=8.588043

Batch 242360, train_perplexity=4832.384, train_loss=8.483095

Batch 242370, train_perplexity=5142.756, train_loss=8.545344

Batch 242380, train_perplexity=4168.541, train_loss=8.335321

Batch 242390, train_perplexity=5587.5483, train_loss=8.628296

Batch 242400, train_perplexity=5250.9287, train_loss=8.56616

Batch 242410, train_perplexity=5045.3257, train_loss=8.526217

Batch 242420, train_perplexity=5026.48, train_loss=8.522475

Batch 242430, train_perplexity=4551.621, train_loss=8.423239

Batch 242440, train_perplexity=4486.1475, train_loss=8.40875

Batch 242450, train_perplexity=6042.9746, train_loss=8.706652

Batch 242460, train_perplexity=5357.1704, train_loss=8.586191

Batch 242470, train_perplexity=4591.618, train_loss=8.431988

Batch 242480, train_perplexity=6071.442, train_loss=8.711351

Batch 242490, train_perplexity=4239.835, train_loss=8.35228

Batch 242500, train_perplexity=5527.7734, train_loss=8.61754

Batch 242510, train_perplexity=4888.2275, train_loss=8.494585

Batch 242520, train_perplexity=5049.8647, train_loss=8.527117

Batch 242530, train_perplexity=4647.8345, train_loss=8.444157

Batch 242540, train_perplexity=5121.597, train_loss=8.541222

Batch 242550, train_perplexity=4845.296, train_loss=8.485764

Batch 242560, train_perplexity=5549.3345, train_loss=8.621433

Batch 242570, train_perplexity=4968.465, train_loss=8.510866

Batch 242580, train_perplexity=5285.2427, train_loss=8.572674

Batch 242590, train_perplexity=4453.7495, train_loss=8.401502

Batch 242600, train_perplexity=7121.2744, train_loss=8.870842

Batch 242610, train_perplexity=5915.015, train_loss=8.685249

Batch 242620, train_perplexity=4405.2007, train_loss=8.390541

Batch 242630, train_perplexity=5316.0635, train_loss=8.578488

Batch 242640, train_perplexity=4897.5225, train_loss=8.496485

Batch 242650, train_perplexity=4763.203, train_loss=8.468676

Batch 242660, train_perplexity=6132.854, train_loss=8.7214155

Batch 242670, train_perplexity=5712.5576, train_loss=8.650422

Batch 242680, train_perplexity=5766.984, train_loss=8.6599045

Batch 242690, train_perplexity=5035.87, train_loss=8.524342
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 242700, train_perplexity=6821.1387, train_loss=8.827782

Batch 242710, train_perplexity=4690.6445, train_loss=8.453325

Batch 242720, train_perplexity=5007.848, train_loss=8.518762

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Batch 242730, train_perplexity=5866.6787, train_loss=8.677044

Batch 242740, train_perplexity=5820.746, train_loss=8.669184

Batch 242750, train_perplexity=5252.2505, train_loss=8.566412

Batch 242760, train_perplexity=5893.3203, train_loss=8.681575

Batch 242770, train_perplexity=5186.0425, train_loss=8.553726

Batch 242780, train_perplexity=4353.3306, train_loss=8.378696

Batch 242790, train_perplexity=5265.782, train_loss=8.568985

Batch 242800, train_perplexity=5643.4043, train_loss=8.638243

Batch 242810, train_perplexity=5661.3438, train_loss=8.641417

Batch 242820, train_perplexity=6395.2417, train_loss=8.7633095

Batch 242830, train_perplexity=5165.253, train_loss=8.549709

Batch 242840, train_perplexity=4744.5605, train_loss=8.464754

Batch 242850, train_perplexity=4622.8228, train_loss=8.438761

Batch 242860, train_perplexity=6798.9272, train_loss=8.82452

Batch 242870, train_perplexity=6594.927, train_loss=8.794056

Batch 242880, train_perplexity=5657.965, train_loss=8.64082

Batch 242890, train_perplexity=5630.62, train_loss=8.635975

Batch 242900, train_perplexity=6269.2383, train_loss=8.74341

Batch 242910, train_perplexity=5853.7573, train_loss=8.674839

Batch 242920, train_perplexity=5145.184, train_loss=8.545816

Batch 242930, train_perplexity=5030.072, train_loss=8.52319

Batch 242940, train_perplexity=5073.8037, train_loss=8.531846

Batch 242950, train_perplexity=4712.521, train_loss=8.457978

Batch 242960, train_perplexity=5255.282, train_loss=8.566989

Batch 242970, train_perplexity=6203.578, train_loss=8.732882

Batch 242980, train_perplexity=4961.329, train_loss=8.509429

Batch 242990, train_perplexity=4854.463, train_loss=8.487654

Batch 243000, train_perplexity=5416.624, train_loss=8.597228

Batch 243010, train_perplexity=6266.6855, train_loss=8.743003

Batch 243020, train_perplexity=4423.362, train_loss=8.394655

Batch 243030, train_perplexity=4850.27, train_loss=8.48679

Batch 243040, train_perplexity=6596.3735, train_loss=8.794275

Batch 243050, train_perplexity=4970.133, train_loss=8.511202

Batch 243060, train_perplexity=5331.3555, train_loss=8.581361

Batch 243070, train_perplexity=5363.1665, train_loss=8.58731

Batch 243080, train_perplexity=4777.2373, train_loss=8.471618

Batch 243090, train_perplexity=6808.2573, train_loss=8.8258915

Batch 243100, train_perplexity=5851.927, train_loss=8.674526

Batch 243110, train_perplexity=4912.168, train_loss=8.499471

Batch 243120, train_perplexity=6245.2373, train_loss=8.739574

Batch 243130, train_perplexity=5580.8594, train_loss=8.627098

Batch 243140, train_perplexity=5095.8726, train_loss=8.536186

Batch 243150, train_perplexity=5527.1353, train_loss=8.617425

Batch 243160, train_perplexity=6998.8047, train_loss=8.853495

Batch 243170, train_perplexity=5771.6055, train_loss=8.660706

Batch 243180, train_perplexity=4967.5977, train_loss=8.510692

Batch 243190, train_perplexity=5300.861, train_loss=8.575624

Batch 243200, train_perplexity=7147.4897, train_loss=8.8745165

Batch 243210, train_perplexity=5987.046, train_loss=8.697353

Batch 243220, train_perplexity=5036.043, train_loss=8.524376

Batch 243230, train_perplexity=5657.5874, train_loss=8.640753

Batch 243240, train_perplexity=4785.5684, train_loss=8.47336

Batch 243250, train_perplexity=5255.8735, train_loss=8.5671015

Batch 243260, train_perplexity=5171.1675, train_loss=8.550854

Batch 243270, train_perplexity=5960.543, train_loss=8.692917

Batch 243280, train_perplexity=5744.2046, train_loss=8.655947

Batch 243290, train_perplexity=7409.6445, train_loss=8.910538

Batch 243300, train_perplexity=5196.127, train_loss=8.555669

Batch 243310, train_perplexity=4493.7266, train_loss=8.410438

Batch 243320, train_perplexity=4484.9624, train_loss=8.408485

Batch 243330, train_perplexity=5069.5522, train_loss=8.531008

Batch 243340, train_perplexity=5374.149, train_loss=8.589355

Batch 243350, train_perplexity=5181.5093, train_loss=8.552852

Batch 243360, train_perplexity=4648.1934, train_loss=8.444234

Batch 243370, train_perplexity=5512.517, train_loss=8.614777

Batch 243380, train_perplexity=5193.155, train_loss=8.555097

Batch 243390, train_perplexity=4359.579, train_loss=8.380131

Batch 243400, train_perplexity=5104.1606, train_loss=8.537811

Batch 243410, train_perplexity=5009.6685, train_loss=8.519125

Batch 243420, train_perplexity=4833.5405, train_loss=8.483335

Batch 243430, train_perplexity=5711.604, train_loss=8.650255

Batch 243440, train_perplexity=4812.9106, train_loss=8.479057

Batch 243450, train_perplexity=5968.939, train_loss=8.6943245

Batch 243460, train_perplexity=5802.932, train_loss=8.666119

Batch 243470, train_perplexity=5256.38, train_loss=8.567198

Batch 243480, train_perplexity=6142.776, train_loss=8.723032

Batch 243490, train_perplexity=5221.725, train_loss=8.560583

Batch 243500, train_perplexity=5406.1426, train_loss=8.595291

Batch 243510, train_perplexity=5028.2974, train_loss=8.522837

Batch 243520, train_perplexity=4391.995, train_loss=8.387539

Batch 243530, train_perplexity=6038.6885, train_loss=8.705942

Batch 243540, train_perplexity=4459.6616, train_loss=8.402828

Batch 243550, train_perplexity=5412.3433, train_loss=8.596437

Batch 243560, train_perplexity=5127.0654, train_loss=8.542289

Batch 243570, train_perplexity=6135.0947, train_loss=8.721781

Batch 243580, train_perplexity=5716.116, train_loss=8.651045

Batch 243590, train_perplexity=5254.6504, train_loss=8.566869

Batch 243600, train_perplexity=4868.761, train_loss=8.490595

Batch 243610, train_perplexity=6186.492, train_loss=8.7301235

Batch 243620, train_perplexity=4562.9214, train_loss=8.425718

Batch 243630, train_perplexity=5084.989, train_loss=8.534048

Batch 243640, train_perplexity=5619.843, train_loss=8.634059

Batch 243650, train_perplexity=4625.2393, train_loss=8.439283

Batch 243660, train_perplexity=5236.8706, train_loss=8.563479

Batch 243670, train_perplexity=4490.0425, train_loss=8.409617

Batch 243680, train_perplexity=4956.964, train_loss=8.508549

Batch 243690, train_perplexity=5559.998, train_loss=8.623353

Batch 243700, train_perplexity=6046.4336, train_loss=8.707224

Batch 243710, train_perplexity=4936.094, train_loss=8.50433

Batch 243720, train_perplexity=4985.7754, train_loss=8.514344

Batch 243730, train_perplexity=4664.633, train_loss=8.447764

Batch 243740, train_perplexity=4981.4883, train_loss=8.513484

Batch 243750, train_perplexity=5858.1914, train_loss=8.675596

Batch 243760, train_perplexity=4746.8237, train_loss=8.465231

Batch 243770, train_perplexity=5375.7686, train_loss=8.589657

Batch 243780, train_perplexity=5575.8853, train_loss=8.626206

Batch 243790, train_perplexity=4949.647, train_loss=8.5070715

Batch 243800, train_perplexity=5215.504, train_loss=8.559391

Batch 243810, train_perplexity=4764.216, train_loss=8.468888

Batch 243820, train_perplexity=5211.467, train_loss=8.558617

Batch 243830, train_perplexity=5587.021, train_loss=8.6282015

Batch 243840, train_perplexity=4878.5127, train_loss=8.492596

Batch 243850, train_perplexity=5239.9634, train_loss=8.56407

Batch 243860, train_perplexity=5656.001, train_loss=8.640472

Batch 243870, train_perplexity=5172.302, train_loss=8.551073

Batch 243880, train_perplexity=4885.9346, train_loss=8.494116

Batch 243890, train_perplexity=5152.972, train_loss=8.547329

Batch 243900, train_perplexity=5508.4966, train_loss=8.614047

Batch 243910, train_perplexity=5068.2373, train_loss=8.530748

Batch 243920, train_perplexity=5524.3477, train_loss=8.61692

Batch 243930, train_perplexity=4402.6094, train_loss=8.389953

Batch 243940, train_perplexity=5642.4033, train_loss=8.638065
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 243950, train_perplexity=5187.883, train_loss=8.554081

Batch 243960, train_perplexity=6621.515, train_loss=8.7980795

Batch 243970, train_perplexity=5606.957, train_loss=8.631763

Batch 243980, train_perplexity=4893.4795, train_loss=8.495659

Batch 243990, train_perplexity=5509.2637, train_loss=8.614186

Batch 244000, train_perplexity=5315.7847, train_loss=8.578436

Batch 244010, train_perplexity=4929.875, train_loss=8.503069

Batch 244020, train_perplexity=4674.9517, train_loss=8.449974

Batch 244030, train_perplexity=5468.9507, train_loss=8.606842

Batch 244040, train_perplexity=5358.1104, train_loss=8.586367

Batch 244050, train_perplexity=5736.387, train_loss=8.654585

Batch 244060, train_perplexity=5198.04, train_loss=8.556037

Batch 244070, train_perplexity=5494.64, train_loss=8.611528

Batch 244080, train_perplexity=5031.545, train_loss=8.523482

Batch 244090, train_perplexity=6611.4004, train_loss=8.796551

Batch 244100, train_perplexity=5924.4546, train_loss=8.686844

Batch 244110, train_perplexity=4818.3853, train_loss=8.480194

Batch 244120, train_perplexity=5316.8545, train_loss=8.578637

Batch 244130, train_perplexity=5691.03, train_loss=8.6466465

Batch 244140, train_perplexity=5906.9087, train_loss=8.683878

Batch 244150, train_perplexity=5266.6157, train_loss=8.569143

Batch 244160, train_perplexity=5208.2026, train_loss=8.55799

Batch 244170, train_perplexity=5304.5117, train_loss=8.576313

Batch 244180, train_perplexity=4162.872, train_loss=8.333961

Batch 244190, train_perplexity=5824.911, train_loss=8.669899

Batch 244200, train_perplexity=5124.4062, train_loss=8.54177

Batch 244210, train_perplexity=4873.779, train_loss=8.491625

Batch 244220, train_perplexity=4296.3394, train_loss=8.365519

Batch 244230, train_perplexity=5068.4404, train_loss=8.530788

Batch 244240, train_perplexity=5035.14, train_loss=8.524197

Batch 244250, train_perplexity=6129.1294, train_loss=8.720808

Batch 244260, train_perplexity=5644.5396, train_loss=8.638444

Batch 244270, train_perplexity=4376.166, train_loss=8.383928

Batch 244280, train_perplexity=4432.8887, train_loss=8.396807

Batch 244290, train_perplexity=5638.4546, train_loss=8.637365

Batch 244300, train_perplexity=5594.928, train_loss=8.629616

Batch 244310, train_perplexity=5556.8545, train_loss=8.622787

Batch 244320, train_perplexity=4661.089, train_loss=8.447004

Batch 244330, train_perplexity=5379.9844, train_loss=8.590441

Batch 244340, train_perplexity=5166.9375, train_loss=8.550035

Batch 244350, train_perplexity=5742.3696, train_loss=8.655627

Batch 244360, train_perplexity=5487.4136, train_loss=8.610212

Batch 244370, train_perplexity=4033.2974, train_loss=8.30234

Batch 244380, train_perplexity=5411.0996, train_loss=8.596208

Batch 244390, train_perplexity=6093.938, train_loss=8.71505

Batch 244400, train_perplexity=5907.63, train_loss=8.684

Batch 244410, train_perplexity=4443.9194, train_loss=8.399292

Batch 244420, train_perplexity=5181.05, train_loss=8.552763

Batch 244430, train_perplexity=4667.5254, train_loss=8.448384

Batch 244440, train_perplexity=5288.308, train_loss=8.573254

Batch 244450, train_perplexity=6283.2144, train_loss=8.745637

Batch 244460, train_perplexity=6646.9746, train_loss=8.801917

Batch 244470, train_perplexity=6903.7954, train_loss=8.839827

Batch 244480, train_perplexity=6493.5425, train_loss=8.7785635

Batch 244490, train_perplexity=6830.851, train_loss=8.829205

Batch 244500, train_perplexity=6032.53, train_loss=8.704922

Batch 244510, train_perplexity=4903.2476, train_loss=8.497653

Batch 244520, train_perplexity=4825.7246, train_loss=8.481716

Batch 244530, train_perplexity=6671.991, train_loss=8.805674

Batch 244540, train_perplexity=6724.0767, train_loss=8.81345

Batch 244550, train_perplexity=5029.0454, train_loss=8.522985

Batch 244560, train_perplexity=5157.712, train_loss=8.548248

Batch 244570, train_perplexity=5634.606, train_loss=8.6366825

Batch 244580, train_perplexity=5162.17, train_loss=8.549112

Batch 244590, train_perplexity=5108.4653, train_loss=8.538654

Batch 244600, train_perplexity=5098.206, train_loss=8.536644

Batch 244610, train_perplexity=4740.7476, train_loss=8.46395

Batch 244620, train_perplexity=4608.385, train_loss=8.435633

Batch 244630, train_perplexity=5877.487, train_loss=8.6788845

Batch 244640, train_perplexity=6238.487, train_loss=8.738493

Batch 244650, train_perplexity=4810.2217, train_loss=8.478498

Batch 244660, train_perplexity=6691.822, train_loss=8.808641

Batch 244670, train_perplexity=5316.0938, train_loss=8.578494

Batch 244680, train_perplexity=5749.1646, train_loss=8.65681

Batch 244690, train_perplexity=6378.151, train_loss=8.760633

Batch 244700, train_perplexity=5395.085, train_loss=8.593244

Batch 244710, train_perplexity=5230.7964, train_loss=8.562319

Batch 244720, train_perplexity=4598.3755, train_loss=8.433458

Batch 244730, train_perplexity=4900.625, train_loss=8.497118

Batch 244740, train_perplexity=5069.939, train_loss=8.531084

Batch 244750, train_perplexity=5585.705, train_loss=8.627966

Batch 244760, train_perplexity=5580.8384, train_loss=8.627094

Batch 244770, train_perplexity=5219.803, train_loss=8.560215

Batch 244780, train_perplexity=4317.562, train_loss=8.370446

Batch 244790, train_perplexity=5716.7705, train_loss=8.651159

Batch 244800, train_perplexity=5213.7935, train_loss=8.559063

Batch 244810, train_perplexity=5462.414, train_loss=8.605646

Batch 244820, train_perplexity=3793.2131, train_loss=8.240969

Batch 244830, train_perplexity=5105.874, train_loss=8.538147

Batch 244840, train_perplexity=4461.0317, train_loss=8.403135

Batch 244850, train_perplexity=4838.6694, train_loss=8.484395

Batch 244860, train_perplexity=4594.7896, train_loss=8.432678

Batch 244870, train_perplexity=5836.2153, train_loss=8.671838

Batch 244880, train_perplexity=4342.9717, train_loss=8.376314

Batch 244890, train_perplexity=4878.3125, train_loss=8.492555

Batch 244900, train_perplexity=6869.2764, train_loss=8.834814

Batch 244910, train_perplexity=5175.3706, train_loss=8.551666

Batch 244920, train_perplexity=5829.957, train_loss=8.670765

Batch 244930, train_perplexity=5325.7656, train_loss=8.580312

Batch 244940, train_perplexity=5147.815, train_loss=8.546328

Batch 244950, train_perplexity=5019.4434, train_loss=8.521074

Batch 244960, train_perplexity=6052.549, train_loss=8.708235

Batch 244970, train_perplexity=4970.095, train_loss=8.511194

Batch 244980, train_perplexity=4973.542, train_loss=8.511888

Batch 244990, train_perplexity=5460.008, train_loss=8.605206

Batch 245000, train_perplexity=5654.588, train_loss=8.640223

Batch 245010, train_perplexity=4675.9814, train_loss=8.450194

Batch 245020, train_perplexity=4911.592, train_loss=8.499353

Batch 245030, train_perplexity=5966.6226, train_loss=8.693936

Batch 245040, train_perplexity=5185.321, train_loss=8.553587

Batch 245050, train_perplexity=4769.376, train_loss=8.469971

Batch 245060, train_perplexity=5441.2896, train_loss=8.601771

Batch 245070, train_perplexity=4180.313, train_loss=8.338141

Batch 245080, train_perplexity=4982.757, train_loss=8.513739

Batch 245090, train_perplexity=5075.0474, train_loss=8.532091

Batch 245100, train_perplexity=5347.0996, train_loss=8.58431

Batch 245110, train_perplexity=5770.0093, train_loss=8.660429

Batch 245120, train_perplexity=5772.222, train_loss=8.660812

Batch 245130, train_perplexity=5159.286, train_loss=8.548553

Batch 245140, train_perplexity=4081.444, train_loss=8.314206

Batch 245150, train_perplexity=4715.7847, train_loss=8.458671

Batch 245160, train_perplexity=5332.2607, train_loss=8.581531

Batch 245170, train_perplexity=5279.067, train_loss=8.571505

Batch 245180, train_perplexity=5599.7856, train_loss=8.630484

Batch 245190, train_perplexity=4418.551, train_loss=8.393567

Batch 245200, train_perplexity=5312.323, train_loss=8.577785

Batch 245210, train_perplexity=5209.9263, train_loss=8.558321

Batch 245220, train_perplexity=4928.22, train_loss=8.502733

Batch 245230, train_perplexity=4449.042, train_loss=8.400444

Batch 245240, train_perplexity=7135.387, train_loss=8.872822

Batch 245250, train_perplexity=4754.707, train_loss=8.46689

Batch 245260, train_perplexity=5082.0605, train_loss=8.533472
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 245270, train_perplexity=4653.1875, train_loss=8.445308

Batch 245280, train_perplexity=5316.606, train_loss=8.57859

Batch 245290, train_perplexity=5357.3237, train_loss=8.58622

Batch 245300, train_perplexity=6433.848, train_loss=8.769328

Batch 245310, train_perplexity=7520.001, train_loss=8.925322

Batch 245320, train_perplexity=5274.97, train_loss=8.570728

Batch 245330, train_perplexity=5026.859, train_loss=8.522551

Batch 245340, train_perplexity=4924.1567, train_loss=8.501908

Batch 245350, train_perplexity=5631.6675, train_loss=8.636161

Batch 245360, train_perplexity=5229.051, train_loss=8.561985

Batch 245370, train_perplexity=5901.5205, train_loss=8.682965

Batch 245380, train_perplexity=5036.648, train_loss=8.524496

Batch 245390, train_perplexity=5311.1226, train_loss=8.5775585

Batch 245400, train_perplexity=6208.485, train_loss=8.733672

Batch 245410, train_perplexity=5442.997, train_loss=8.602085

Batch 245420, train_perplexity=5561.2334, train_loss=8.623575

Batch 245430, train_perplexity=6160.5635, train_loss=8.725924

Batch 245440, train_perplexity=4532.8213, train_loss=8.4191

Batch 245450, train_perplexity=4530.4355, train_loss=8.418573

Batch 245460, train_perplexity=5638.299, train_loss=8.637338

Batch 245470, train_perplexity=7816.798, train_loss=8.96403

Batch 245480, train_perplexity=6881.787, train_loss=8.836634

Batch 245490, train_perplexity=5532.889, train_loss=8.618465

Batch 245500, train_perplexity=5740.4043, train_loss=8.655285

Batch 245510, train_perplexity=5384.2705, train_loss=8.591237

Batch 245520, train_perplexity=5997.212, train_loss=8.69905

Batch 245530, train_perplexity=6177.79, train_loss=8.728716

Batch 245540, train_perplexity=4358.274, train_loss=8.379831

Batch 245550, train_perplexity=5900.2427, train_loss=8.682749

Batch 245560, train_perplexity=4587.2896, train_loss=8.431045

Batch 245570, train_perplexity=4725.4595, train_loss=8.46072

Batch 245580, train_perplexity=5247.094, train_loss=8.56543

Batch 245590, train_perplexity=4690.3584, train_loss=8.453264

Batch 245600, train_perplexity=7273.484, train_loss=8.891991

Batch 245610, train_perplexity=5537.3076, train_loss=8.619264

Batch 245620, train_perplexity=5070.6255, train_loss=8.5312195

Batch 245630, train_perplexity=4513.647, train_loss=8.414861

Batch 245640, train_perplexity=5562.6763, train_loss=8.623835

Batch 245650, train_perplexity=5107.9395, train_loss=8.538551

Batch 245660, train_perplexity=5676.043, train_loss=8.64401

Batch 245670, train_perplexity=4026.3875, train_loss=8.300625

Batch 245680, train_perplexity=5147.412, train_loss=8.546249

Batch 245690, train_perplexity=4605.8364, train_loss=8.43508

Batch 245700, train_perplexity=4503.074, train_loss=8.412516

Batch 245710, train_perplexity=5463.5396, train_loss=8.605852

Batch 245720, train_perplexity=4481.1616, train_loss=8.407638

Batch 245730, train_perplexity=5743.2188, train_loss=8.655775

Batch 245740, train_perplexity=4961.8164, train_loss=8.509527

Batch 245750, train_perplexity=6299.3896, train_loss=8.748208

Batch 245760, train_perplexity=5898.651, train_loss=8.682479

Batch 245770, train_perplexity=5374.236, train_loss=8.589372

Batch 245780, train_perplexity=5008.603, train_loss=8.518912

Batch 245790, train_perplexity=5477.166, train_loss=8.608343

Batch 245800, train_perplexity=4964.2114, train_loss=8.51001

Batch 245810, train_perplexity=5123.8394, train_loss=8.541659

Batch 245820, train_perplexity=5281.413, train_loss=8.571949

Batch 245830, train_perplexity=6279.71, train_loss=8.745079

Batch 245840, train_perplexity=6177.154, train_loss=8.728613

Batch 245850, train_perplexity=7183.6807, train_loss=8.879567

Batch 245860, train_perplexity=4925.1616, train_loss=8.502112

Batch 245870, train_perplexity=5033.815, train_loss=8.523933

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Batch 245880, train_perplexity=6292.401, train_loss=8.747098

Batch 245890, train_perplexity=4355.9385, train_loss=8.379295

Batch 245900, train_perplexity=5106.658, train_loss=8.5383005

Batch 245910, train_perplexity=5483.093, train_loss=8.609425

Batch 245920, train_perplexity=5264.5317, train_loss=8.5687475

Batch 245930, train_perplexity=7105.766, train_loss=8.868662

Batch 245940, train_perplexity=5012.784, train_loss=8.519747

Batch 245950, train_perplexity=6981.046, train_loss=8.850954

Batch 245960, train_perplexity=5303.581, train_loss=8.576138

Batch 245970, train_perplexity=5113.7, train_loss=8.539679

Batch 245980, train_perplexity=5553.4214, train_loss=8.6221695

Batch 245990, train_perplexity=5842.6865, train_loss=8.672946

Batch 246000, train_perplexity=6568.3823, train_loss=8.790023

Batch 246010, train_perplexity=6548.9736, train_loss=8.787064

Batch 246020, train_perplexity=5439.1157, train_loss=8.601372

Batch 246030, train_perplexity=5529.2393, train_loss=8.6178055

Batch 246040, train_perplexity=5406.5244, train_loss=8.595362

Batch 246050, train_perplexity=5210.8306, train_loss=8.558495

Batch 246060, train_perplexity=5369.6304, train_loss=8.588514

Batch 246070, train_perplexity=5012.5835, train_loss=8.519707

Batch 246080, train_perplexity=5885.317, train_loss=8.680216

Batch 246090, train_perplexity=5135.8257, train_loss=8.543996

Batch 246100, train_perplexity=5004.4346, train_loss=8.51808

Batch 246110, train_perplexity=5851.268, train_loss=8.674414

Batch 246120, train_perplexity=4843.7666, train_loss=8.485448

Batch 246130, train_perplexity=5821.3623, train_loss=8.66929

Batch 246140, train_perplexity=4579.9717, train_loss=8.429448

Batch 246150, train_perplexity=5173.017, train_loss=8.551211

Batch 246160, train_perplexity=5689.304, train_loss=8.646343

Batch 246170, train_perplexity=6075.485, train_loss=8.712017

Batch 246180, train_perplexity=6424.2163, train_loss=8.76783

Batch 246190, train_perplexity=6160.364, train_loss=8.725891

Batch 246200, train_perplexity=4555.7295, train_loss=8.424141

Batch 246210, train_perplexity=6565.8584, train_loss=8.7896385

Batch 246220, train_perplexity=6543.0054, train_loss=8.786152

Batch 246230, train_perplexity=4869.8994, train_loss=8.4908285

Batch 246240, train_perplexity=6562.678, train_loss=8.789154

Batch 246250, train_perplexity=6591.638, train_loss=8.793557

Batch 246260, train_perplexity=5356.312, train_loss=8.586031

Batch 246270, train_perplexity=5265.983, train_loss=8.569023

Batch 246280, train_perplexity=6268.0425, train_loss=8.743219

Batch 246290, train_perplexity=5904.115, train_loss=8.683405

Batch 246300, train_perplexity=4640.9424, train_loss=8.442673

Batch 246310, train_perplexity=5222.8306, train_loss=8.560795

Batch 246320, train_perplexity=5429.176, train_loss=8.599543

Batch 246330, train_perplexity=6128.054, train_loss=8.720633

Batch 246340, train_perplexity=6762.7524, train_loss=8.819185

Batch 246350, train_perplexity=5961.555, train_loss=8.693087

Batch 246360, train_perplexity=5349.5635, train_loss=8.58477

Batch 246370, train_perplexity=4694.135, train_loss=8.454069

Batch 246380, train_perplexity=4555.1387, train_loss=8.424011

Batch 246390, train_perplexity=5593.4023, train_loss=8.629343

Batch 246400, train_perplexity=5619.9443, train_loss=8.634077

Batch 246410, train_perplexity=5857.3203, train_loss=8.675447

Batch 246420, train_perplexity=5172.9478, train_loss=8.551198

Batch 246430, train_perplexity=5229.574, train_loss=8.562085

Batch 246440, train_perplexity=5731.444, train_loss=8.653723

Batch 246450, train_perplexity=4266.1543, train_loss=8.358468

Batch 246460, train_perplexity=5830.335, train_loss=8.67083

Batch 246470, train_perplexity=5544.2773, train_loss=8.620522

Batch 246480, train_perplexity=6754.721, train_loss=8.817997

Batch 246490, train_perplexity=5933.2183, train_loss=8.688322

Batch 246500, train_perplexity=6138.29, train_loss=8.7223015

Batch 246510, train_perplexity=5558.805, train_loss=8.623138
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 246520, train_perplexity=6207.3066, train_loss=8.733482

Batch 246530, train_perplexity=5364.42, train_loss=8.5875435

Batch 246540, train_perplexity=5972.68, train_loss=8.694951

Batch 246550, train_perplexity=5121.7876, train_loss=8.541259

Batch 246560, train_perplexity=6167.053, train_loss=8.726976

Batch 246570, train_perplexity=5226.0044, train_loss=8.561402

Batch 246580, train_perplexity=5179.6074, train_loss=8.5524845

Batch 246590, train_perplexity=5266.033, train_loss=8.569033

Batch 246600, train_perplexity=5588.8276, train_loss=8.628525

Batch 246610, train_perplexity=5409.65, train_loss=8.59594

Batch 246620, train_perplexity=5588.2144, train_loss=8.628415

Batch 246630, train_perplexity=6686.2466, train_loss=8.807808

Batch 246640, train_perplexity=5447.1616, train_loss=8.60285

Batch 246650, train_perplexity=6108.053, train_loss=8.717363

Batch 246660, train_perplexity=4470.597, train_loss=8.405277

Batch 246670, train_perplexity=4414.2, train_loss=8.392582

Batch 246680, train_perplexity=4795.9395, train_loss=8.475525

Batch 246690, train_perplexity=4828.574, train_loss=8.4823065

Batch 246700, train_perplexity=5578.5503, train_loss=8.626684

Batch 246710, train_perplexity=4984.62, train_loss=8.514112

Batch 246720, train_perplexity=5532.652, train_loss=8.6184225

Batch 246730, train_perplexity=5513.653, train_loss=8.614983

Batch 246740, train_perplexity=5268.5146, train_loss=8.569504

Batch 246750, train_perplexity=5534.214, train_loss=8.618705

Batch 246760, train_perplexity=5220.172, train_loss=8.560286

Batch 246770, train_perplexity=5980.05, train_loss=8.696184

Batch 246780, train_perplexity=5080.3545, train_loss=8.533136

Batch 246790, train_perplexity=4618.3896, train_loss=8.437801

Batch 246800, train_perplexity=5672.52, train_loss=8.643389

Batch 246810, train_perplexity=5417.833, train_loss=8.597451

Batch 246820, train_perplexity=5412.075, train_loss=8.596388

Batch 246830, train_perplexity=5329.7134, train_loss=8.581053

Batch 246840, train_perplexity=4873.0537, train_loss=8.491476

Batch 246850, train_perplexity=5703.8477, train_loss=8.648896

Batch 246860, train_perplexity=4804.6465, train_loss=8.477339

Batch 246870, train_perplexity=5562.708, train_loss=8.62384

Batch 246880, train_perplexity=5892.8423, train_loss=8.681494

Batch 246890, train_perplexity=6794.8823, train_loss=8.823925

Batch 246900, train_perplexity=6598.6636, train_loss=8.794622

Batch 246910, train_perplexity=5686.5054, train_loss=8.645851

Batch 246920, train_perplexity=5160.585, train_loss=8.548805

Batch 246930, train_perplexity=5727.2695, train_loss=8.652994

Batch 246940, train_perplexity=4421.455, train_loss=8.394224

Batch 246950, train_perplexity=5334.102, train_loss=8.581876

Batch 246960, train_perplexity=5387.0693, train_loss=8.591757

Batch 246970, train_perplexity=5811.3613, train_loss=8.66757

Batch 246980, train_perplexity=5078.5723, train_loss=8.532785

Batch 246990, train_perplexity=6102.0967, train_loss=8.716388

Batch 247000, train_perplexity=4704.6357, train_loss=8.456304

Batch 247010, train_perplexity=4931.869, train_loss=8.503473

Batch 247020, train_perplexity=5349.196, train_loss=8.584702

Batch 247030, train_perplexity=7775.704, train_loss=8.958759

Batch 247040, train_perplexity=5636.154, train_loss=8.636957

Batch 247050, train_perplexity=6513.2344, train_loss=8.781591

Batch 247060, train_perplexity=5218.5093, train_loss=8.559967

Batch 247070, train_perplexity=5569.4653, train_loss=8.625054

Batch 247080, train_perplexity=5616.992, train_loss=8.633552

Batch 247090, train_perplexity=5337.15, train_loss=8.582447

Batch 247100, train_perplexity=5056.877, train_loss=8.528504

Batch 247110, train_perplexity=4579.666, train_loss=8.429381

Batch 247120, train_perplexity=5202.633, train_loss=8.55692

Batch 247130, train_perplexity=4705.0303, train_loss=8.4563875

Batch 247140, train_perplexity=5884.4526, train_loss=8.680069

Batch 247150, train_perplexity=6717.199, train_loss=8.812427

Batch 247160, train_perplexity=5879.062, train_loss=8.6791525

Batch 247170, train_perplexity=4618.0063, train_loss=8.437718

Batch 247180, train_perplexity=5265.3203, train_loss=8.568897

Batch 247190, train_perplexity=5498.299, train_loss=8.612194

Batch 247200, train_perplexity=5460.763, train_loss=8.605344

Batch 247210, train_perplexity=6249.956, train_loss=8.74033

Batch 247220, train_perplexity=5075.9673, train_loss=8.532272

Batch 247230, train_perplexity=5732.6025, train_loss=8.653925

Batch 247240, train_perplexity=4932.2544, train_loss=8.5035515

Batch 247250, train_perplexity=6217.734, train_loss=8.735161

Batch 247260, train_perplexity=5810.1807, train_loss=8.667367

Batch 247270, train_perplexity=4705.6226, train_loss=8.456513

Batch 247280, train_perplexity=4762.4033, train_loss=8.468508

Batch 247290, train_perplexity=5119.0483, train_loss=8.540724

Batch 247300, train_perplexity=6288.4834, train_loss=8.746475

Batch 247310, train_perplexity=5487.529, train_loss=8.610233

Batch 247320, train_perplexity=4992.703, train_loss=8.515733

Batch 247330, train_perplexity=5232.203, train_loss=8.562588

Batch 247340, train_perplexity=5203.1436, train_loss=8.557018

Batch 247350, train_perplexity=4847.9023, train_loss=8.486301

Batch 247360, train_perplexity=6526.0425, train_loss=8.783556

Batch 247370, train_perplexity=4627.2246, train_loss=8.439713

Batch 247380, train_perplexity=4563.422, train_loss=8.425828

Batch 247390, train_perplexity=6039.725, train_loss=8.706114

Batch 247400, train_perplexity=5091.287, train_loss=8.535286

Batch 247410, train_perplexity=5107.5933, train_loss=8.538484

Batch 247420, train_perplexity=5917.1816, train_loss=8.685616

Batch 247430, train_perplexity=5670.5728, train_loss=8.643045

Batch 247440, train_perplexity=6787.732, train_loss=8.822872

Batch 247450, train_perplexity=5099.689, train_loss=8.536935

Batch 247460, train_perplexity=5357.87, train_loss=8.586322

Batch 247470, train_perplexity=5836.2935, train_loss=8.671851

Batch 247480, train_perplexity=6209.675, train_loss=8.733864

Batch 247490, train_perplexity=6208.9526, train_loss=8.7337475

Batch 247500, train_perplexity=5109.3276, train_loss=8.538823

Batch 247510, train_perplexity=5154.2695, train_loss=8.547581

Batch 247520, train_perplexity=4650.574, train_loss=8.444746

Batch 247530, train_perplexity=6064.937, train_loss=8.710279

Batch 247540, train_perplexity=5598.5522, train_loss=8.630263

Batch 247550, train_perplexity=5228.3125, train_loss=8.561844

Batch 247560, train_perplexity=6455.2114, train_loss=8.772643

Batch 247570, train_perplexity=6122.7793, train_loss=8.719771

Batch 247580, train_perplexity=4715.2856, train_loss=8.458565

Batch 247590, train_perplexity=5361.341, train_loss=8.586969

Batch 247600, train_perplexity=5773.576, train_loss=8.661047

Batch 247610, train_perplexity=4695.335, train_loss=8.454325

Batch 247620, train_perplexity=5663.5684, train_loss=8.641809

Batch 247630, train_perplexity=4980.125, train_loss=8.51321

Batch 247640, train_perplexity=4486.391, train_loss=8.408804

Batch 247650, train_perplexity=5751.3086, train_loss=8.657183

Batch 247660, train_perplexity=5888.438, train_loss=8.680746

Batch 247670, train_perplexity=5938.2227, train_loss=8.689165

Batch 247680, train_perplexity=5265.1646, train_loss=8.568868

Batch 247690, train_perplexity=5881.0977, train_loss=8.679499

Batch 247700, train_perplexity=5884.363, train_loss=8.680054

Batch 247710, train_perplexity=6038.5215, train_loss=8.7059145

Batch 247720, train_perplexity=5112.818, train_loss=8.539506

Batch 247730, train_perplexity=4890.8433, train_loss=8.49512

Batch 247740, train_perplexity=5350.349, train_loss=8.584917

Batch 247750, train_perplexity=5565.648, train_loss=8.624369

Batch 247760, train_perplexity=5915.145, train_loss=8.685271

Batch 247770, train_perplexity=5196.5435, train_loss=8.555749

Batch 247780, train_perplexity=5546.4136, train_loss=8.620907

Batch 247790, train_perplexity=6093.159, train_loss=8.714922

Batch 247800, train_perplexity=4790.075, train_loss=8.474301

Batch 247810, train_perplexity=4446.319, train_loss=8.399832

Batch 247820, train_perplexity=4959.2764, train_loss=8.509015

Batch 247830, train_perplexity=4711.029, train_loss=8.457662
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 247840, train_perplexity=5939.9785, train_loss=8.689461

Batch 247850, train_perplexity=5472.5244, train_loss=8.607495

Batch 247860, train_perplexity=5263.2466, train_loss=8.568503

Batch 247870, train_perplexity=4825.6875, train_loss=8.481709

Batch 247880, train_perplexity=4688.5024, train_loss=8.452868

Batch 247890, train_perplexity=4605.7397, train_loss=8.435059

Batch 247900, train_perplexity=4442.6353, train_loss=8.399003

Batch 247910, train_perplexity=5490.099, train_loss=8.610702

Batch 247920, train_perplexity=5959.304, train_loss=8.692709

Batch 247930, train_perplexity=5454.9595, train_loss=8.60428

Batch 247940, train_perplexity=5147.3438, train_loss=8.546236

Batch 247950, train_perplexity=5104.871, train_loss=8.5379505

Batch 247960, train_perplexity=5636.9385, train_loss=8.637096

Batch 247970, train_perplexity=5689.5156, train_loss=8.64638

Batch 247980, train_perplexity=5883.24, train_loss=8.679863

Batch 247990, train_perplexity=4686.6245, train_loss=8.452468

Batch 248000, train_perplexity=5979.137, train_loss=8.696032

Batch 248010, train_perplexity=5677.391, train_loss=8.644247

Batch 248020, train_perplexity=4801.9717, train_loss=8.476782

Batch 248030, train_perplexity=5084.5864, train_loss=8.533969

Batch 248040, train_perplexity=4671.0522, train_loss=8.44914

Batch 248050, train_perplexity=4640.155, train_loss=8.442503

Batch 248060, train_perplexity=5279.142, train_loss=8.571519

Batch 248070, train_perplexity=4501.0522, train_loss=8.412066

Batch 248080, train_perplexity=6028.0093, train_loss=8.704172

Batch 248090, train_perplexity=5312.612, train_loss=8.577839

Batch 248100, train_perplexity=4947.632, train_loss=8.506664

Batch 248110, train_perplexity=5794.388, train_loss=8.664645

Batch 248120, train_perplexity=4416.478, train_loss=8.393098

Batch 248130, train_perplexity=4958.2783, train_loss=8.508814

Batch 248140, train_perplexity=6336.04, train_loss=8.754009

Batch 248150, train_perplexity=6328.0327, train_loss=8.752745

Batch 248160, train_perplexity=5371.633, train_loss=8.588887

Batch 248170, train_perplexity=5473.9546, train_loss=8.607757

Batch 248180, train_perplexity=5004.664, train_loss=8.518126

Batch 248190, train_perplexity=5451.7817, train_loss=8.603698

Batch 248200, train_perplexity=5845.083, train_loss=8.673356

Batch 248210, train_perplexity=5143.5356, train_loss=8.545496

Batch 248220, train_perplexity=4903.603, train_loss=8.4977255

Batch 248230, train_perplexity=4882.6367, train_loss=8.493441

Batch 248240, train_perplexity=5036.802, train_loss=8.524527

Batch 248250, train_perplexity=5266.2593, train_loss=8.569076

Batch 248260, train_perplexity=5655.613, train_loss=8.640404

Batch 248270, train_perplexity=5570.937, train_loss=8.625319

Batch 248280, train_perplexity=6268.3115, train_loss=8.743262

Batch 248290, train_perplexity=4571.1006, train_loss=8.427509

Batch 248300, train_perplexity=5647.5176, train_loss=8.638971

Batch 248310, train_perplexity=4764.661, train_loss=8.468982

Batch 248320, train_perplexity=5949.049, train_loss=8.690987

Batch 248330, train_perplexity=5187.487, train_loss=8.554005

Batch 248340, train_perplexity=6083.7236, train_loss=8.713372

Batch 248350, train_perplexity=4963.6245, train_loss=8.5098915

Batch 248360, train_perplexity=4930.9517, train_loss=8.503287

Batch 248370, train_perplexity=5302.1396, train_loss=8.575866

Batch 248380, train_perplexity=5223.6875, train_loss=8.560959

Batch 248390, train_perplexity=4446.166, train_loss=8.399797

Batch 248400, train_perplexity=4576.8237, train_loss=8.428761

Batch 248410, train_perplexity=6788.5737, train_loss=8.822996

Batch 248420, train_perplexity=6046.48, train_loss=8.7072315

Batch 248430, train_perplexity=5467.7666, train_loss=8.606626

Batch 248440, train_perplexity=5573.934, train_loss=8.625856

Batch 248450, train_perplexity=5557.0664, train_loss=8.622826

Batch 248460, train_perplexity=4590.0156, train_loss=8.431639

Batch 248470, train_perplexity=4652.6685, train_loss=8.445196

Batch 248480, train_perplexity=4845.998, train_loss=8.4859085

Batch 248490, train_perplexity=4776.49, train_loss=8.471461

Batch 248500, train_perplexity=6047.8867, train_loss=8.707464

Batch 248510, train_perplexity=5608.283, train_loss=8.632

Batch 248520, train_perplexity=5040.223, train_loss=8.525206

Batch 248530, train_perplexity=5198.749, train_loss=8.556173

Batch 248540, train_perplexity=5579.9014, train_loss=8.626926

Batch 248550, train_perplexity=4549.9893, train_loss=8.42288

Batch 248560, train_perplexity=5676.839, train_loss=8.64415

Batch 248570, train_perplexity=4926.2373, train_loss=8.502331

Batch 248580, train_perplexity=4847.214, train_loss=8.486159

Batch 248590, train_perplexity=4451.9194, train_loss=8.401091

Batch 248600, train_perplexity=4732.8467, train_loss=8.462282

Batch 248610, train_perplexity=4647.7456, train_loss=8.444138

Batch 248620, train_perplexity=5632.3066, train_loss=8.636274

Batch 248630, train_perplexity=5266.475, train_loss=8.569117

Batch 248640, train_perplexity=5258.6763, train_loss=8.567635

Batch 248650, train_perplexity=5927.2236, train_loss=8.687311

Batch 248660, train_perplexity=4691.4585, train_loss=8.453499

Batch 248670, train_perplexity=6348.972, train_loss=8.756048

Batch 248680, train_perplexity=5742.89, train_loss=8.655718

Batch 248690, train_perplexity=5492.9272, train_loss=8.611217

Batch 248700, train_perplexity=4763.162, train_loss=8.468667

Batch 248710, train_perplexity=6459.7563, train_loss=8.773347

Batch 248720, train_perplexity=5665.3027, train_loss=8.642116

Batch 248730, train_perplexity=6745.606, train_loss=8.816647

Batch 248740, train_perplexity=5620.4697, train_loss=8.634171

Batch 248750, train_perplexity=5339.247, train_loss=8.58284

Batch 248760, train_perplexity=4643.705, train_loss=8.443268

Batch 248770, train_perplexity=4423.6104, train_loss=8.3947115

Batch 248780, train_perplexity=5840.909, train_loss=8.672642

Batch 248790, train_perplexity=5820.6904, train_loss=8.669174

Batch 248800, train_perplexity=6383.9316, train_loss=8.761539

Batch 248810, train_perplexity=5612.189, train_loss=8.632696

Batch 248820, train_perplexity=6574.3296, train_loss=8.790928

Batch 248830, train_perplexity=5086.7686, train_loss=8.534398

Batch 248840, train_perplexity=6474.719, train_loss=8.7756605

Batch 248850, train_perplexity=5533.7334, train_loss=8.618618

Batch 248860, train_perplexity=5291.2793, train_loss=8.573815

Batch 248870, train_perplexity=5203.397, train_loss=8.557067

Batch 248880, train_perplexity=5774.518, train_loss=8.66121

Batch 248890, train_perplexity=4439.721, train_loss=8.398347

Batch 248900, train_perplexity=5224.5596, train_loss=8.561126

Batch 248910, train_perplexity=4849.669, train_loss=8.486666

Batch 248920, train_perplexity=4845.268, train_loss=8.485758

Batch 248930, train_perplexity=5977.746, train_loss=8.695799

Batch 248940, train_perplexity=5855.5273, train_loss=8.675141

Batch 248950, train_perplexity=5032.6724, train_loss=8.523706

Batch 248960, train_perplexity=6284.7183, train_loss=8.745876

Batch 248970, train_perplexity=5409.1235, train_loss=8.595842

Batch 248980, train_perplexity=5093.769, train_loss=8.535773

Batch 248990, train_perplexity=5078.9404, train_loss=8.532858

Batch 249000, train_perplexity=5990.3184, train_loss=8.6979

Batch 249010, train_perplexity=5698.4863, train_loss=8.647956

Batch 249020, train_perplexity=4412.1, train_loss=8.392106

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Batch 249030, train_perplexity=5564.8677, train_loss=8.6242285

Batch 249040, train_perplexity=4767.025, train_loss=8.469478

Batch 249050, train_perplexity=6464.7236, train_loss=8.774116

Batch 249060, train_perplexity=5101.221, train_loss=8.537235

Batch 249070, train_perplexity=5058.1553, train_loss=8.528757

Batch 249080, train_perplexity=4584.722, train_loss=8.430485
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 249090, train_perplexity=5481.1323, train_loss=8.609067

Batch 249100, train_perplexity=6270.8945, train_loss=8.743674

Batch 249110, train_perplexity=6098.5015, train_loss=8.715798

Batch 249120, train_perplexity=5263.2017, train_loss=8.568495

Batch 249130, train_perplexity=4742.823, train_loss=8.464388

Batch 249140, train_perplexity=5826.1055, train_loss=8.670104

Batch 249150, train_perplexity=6257.7573, train_loss=8.741577

Batch 249160, train_perplexity=6069.1494, train_loss=8.710974

Batch 249170, train_perplexity=6032.0923, train_loss=8.704849

Batch 249180, train_perplexity=5703.048, train_loss=8.648756

Batch 249190, train_perplexity=5381.642, train_loss=8.590749

Batch 249200, train_perplexity=5421.2183, train_loss=8.598076

Batch 249210, train_perplexity=4742.941, train_loss=8.464413

Batch 249220, train_perplexity=5686.142, train_loss=8.645787

Batch 249230, train_perplexity=4477.898, train_loss=8.406909

Batch 249240, train_perplexity=5197.7676, train_loss=8.5559845

Batch 249250, train_perplexity=4646.0703, train_loss=8.443777

Batch 249260, train_perplexity=5660.037, train_loss=8.641186

Batch 249270, train_perplexity=6220.498, train_loss=8.735605

Batch 249280, train_perplexity=5721.5537, train_loss=8.651996

Batch 249290, train_perplexity=6061.959, train_loss=8.709788

Batch 249300, train_perplexity=6135.668, train_loss=8.721874

Batch 249310, train_perplexity=6659.1123, train_loss=8.803741

Batch 249320, train_perplexity=4524.3047, train_loss=8.417219

Batch 249330, train_perplexity=6346.163, train_loss=8.755606

Batch 249340, train_perplexity=5015.644, train_loss=8.520317

Batch 249350, train_perplexity=5459.274, train_loss=8.605071

Batch 249360, train_perplexity=5228.432, train_loss=8.561867

Batch 249370, train_perplexity=5076.054, train_loss=8.5322895

Batch 249380, train_perplexity=5296.924, train_loss=8.574882

Batch 249390, train_perplexity=4959.56, train_loss=8.509072

Batch 249400, train_perplexity=5774.072, train_loss=8.661133

Batch 249410, train_perplexity=4348.828, train_loss=8.377662

Batch 249420, train_perplexity=6266.1416, train_loss=8.742916

Batch 249430, train_perplexity=4515.046, train_loss=8.415171

Batch 249440, train_perplexity=4833.988, train_loss=8.483427

Batch 249450, train_perplexity=4810.3457, train_loss=8.478524

Batch 249460, train_perplexity=5233.311, train_loss=8.562799

Batch 249470, train_perplexity=6085.4297, train_loss=8.713653

Batch 249480, train_perplexity=5230.916, train_loss=8.562342

Batch 249490, train_perplexity=6703.7793, train_loss=8.810427

Batch 249500, train_perplexity=5871.1787, train_loss=8.677811

Batch 249510, train_perplexity=5455.4536, train_loss=8.604371

Batch 249520, train_perplexity=5733.3027, train_loss=8.654047

Batch 249530, train_perplexity=5008.574, train_loss=8.518907

Batch 249540, train_perplexity=5533.744, train_loss=8.61862

Batch 249550, train_perplexity=5347.9155, train_loss=8.584462

Batch 249560, train_perplexity=5933.841, train_loss=8.688427

Batch 249570, train_perplexity=5359.347, train_loss=8.586597

Batch 249580, train_perplexity=5168.5737, train_loss=8.550352

Batch 249590, train_perplexity=5099.7036, train_loss=8.536938

Batch 249600, train_perplexity=5332.657, train_loss=8.581605

Batch 249610, train_perplexity=5701.8027, train_loss=8.648538

Batch 249620, train_perplexity=5858.102, train_loss=8.675581

Batch 249630, train_perplexity=5270.676, train_loss=8.569914

Batch 249640, train_perplexity=5579.593, train_loss=8.626871

Batch 249650, train_perplexity=5665.481, train_loss=8.642147

Batch 249660, train_perplexity=5948.6743, train_loss=8.690924

Batch 249670, train_perplexity=5408.644, train_loss=8.595754

Batch 249680, train_perplexity=4002.8071, train_loss=8.294751

Batch 249690, train_perplexity=5096.7183, train_loss=8.536352

Batch 249700, train_perplexity=4365.0664, train_loss=8.381389

Batch 249710, train_perplexity=6248.109, train_loss=8.740034

Batch 249720, train_perplexity=5634.2085, train_loss=8.636612

Batch 249730, train_perplexity=4402.647, train_loss=8.389961

Batch 249740, train_perplexity=5377.8403, train_loss=8.590042

Batch 249750, train_perplexity=6165.865, train_loss=8.726784

Batch 249760, train_perplexity=4915.9595, train_loss=8.500242

Batch 249770, train_perplexity=5777.862, train_loss=8.661789

Batch 249780, train_perplexity=5369.4663, train_loss=8.588484

Batch 249790, train_perplexity=6152.367, train_loss=8.724592

Batch 249800, train_perplexity=5590.032, train_loss=8.62874

Batch 249810, train_perplexity=5671.1895, train_loss=8.643154

Batch 249820, train_perplexity=4727.001, train_loss=8.461046

Batch 249830, train_perplexity=5263.2266, train_loss=8.5685

Batch 249840, train_perplexity=4864.733, train_loss=8.489767

Batch 249850, train_perplexity=5436.346, train_loss=8.6008625

Batch 249860, train_perplexity=4487.6577, train_loss=8.409086

Batch 249870, train_perplexity=5332.2046, train_loss=8.58152

Batch 249880, train_perplexity=6514.0415, train_loss=8.781715

Batch 249890, train_perplexity=5998.6133, train_loss=8.699284

Batch 249900, train_perplexity=4779.2197, train_loss=8.472033

Batch 249910, train_perplexity=6027.285, train_loss=8.704052

Batch 249920, train_perplexity=5900.1475, train_loss=8.682733

Batch 249930, train_perplexity=4779.265, train_loss=8.472042

Batch 249940, train_perplexity=4108.3857, train_loss=8.3207855

Batch 249950, train_perplexity=5301.993, train_loss=8.575838

Batch 249960, train_perplexity=4397.3516, train_loss=8.388758

Batch 249970, train_perplexity=4075.1704, train_loss=8.312668

Batch 249980, train_perplexity=5638.643, train_loss=8.637399

Batch 249990, train_perplexity=5491.6387, train_loss=8.610982

Batch 250000, train_perplexity=5792.1343, train_loss=8.664256

Batch 250010, train_perplexity=5514.0522, train_loss=8.615055

Batch 250020, train_perplexity=5100.511, train_loss=8.537096

Batch 250030, train_perplexity=5106.2734, train_loss=8.538225

Batch 250040, train_perplexity=6203.4956, train_loss=8.732868

Batch 250050, train_perplexity=5411.146, train_loss=8.596216

Batch 250060, train_perplexity=5320.7295, train_loss=8.579366

Batch 250070, train_perplexity=6664.8555, train_loss=8.804604

Batch 250080, train_perplexity=6057.238, train_loss=8.709009

Batch 250090, train_perplexity=4233.205, train_loss=8.350715

Batch 250100, train_perplexity=5803.2256, train_loss=8.666169

Batch 250110, train_perplexity=5515.0728, train_loss=8.61524

Batch 250120, train_perplexity=5047.188, train_loss=8.526587

Batch 250130, train_perplexity=4413.8843, train_loss=8.39251

Batch 250140, train_perplexity=5997.921, train_loss=8.699168

Batch 250150, train_perplexity=4831.854, train_loss=8.4829855

Batch 250160, train_perplexity=4278.0225, train_loss=8.361246

Batch 250170, train_perplexity=5847.285, train_loss=8.673733

Batch 250180, train_perplexity=4639.429, train_loss=8.442347

Batch 250190, train_perplexity=5001.2095, train_loss=8.517435

Batch 250200, train_perplexity=5425.0195, train_loss=8.598777

Batch 250210, train_perplexity=4503.637, train_loss=8.412641

Batch 250220, train_perplexity=5526.329, train_loss=8.617279

Batch 250230, train_perplexity=4820.426, train_loss=8.480618

Batch 250240, train_perplexity=4892.649, train_loss=8.495489

Batch 250250, train_perplexity=5615.6743, train_loss=8.633317

Batch 250260, train_perplexity=6021.919, train_loss=8.703161

Batch 250270, train_perplexity=6244.1177, train_loss=8.739395

Batch 250280, train_perplexity=4321.488, train_loss=8.371355

Batch 250290, train_perplexity=5680.9004, train_loss=8.644865

Batch 250300, train_perplexity=5081.479, train_loss=8.533358

Batch 250310, train_perplexity=4428.621, train_loss=8.3958435

Batch 250320, train_perplexity=4500.8506, train_loss=8.412022

Batch 250330, train_perplexity=5748.589, train_loss=8.65671

Batch 250340, train_perplexity=4396.6094, train_loss=8.388589

Batch 250350, train_perplexity=5536.088, train_loss=8.619043

Batch 250360, train_perplexity=4585.5747, train_loss=8.430671

Batch 250370, train_perplexity=5332.8965, train_loss=8.58165

Batch 250380, train_perplexity=6193.3755, train_loss=8.7312355

Batch 250390, train_perplexity=5744.884, train_loss=8.656065

Batch 250400, train_perplexity=5110.6436, train_loss=8.539081
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 250410, train_perplexity=5651.5044, train_loss=8.639677

Batch 250420, train_perplexity=5331.005, train_loss=8.581295

Batch 250430, train_perplexity=5217.897, train_loss=8.55985

Batch 250440, train_perplexity=4869.4116, train_loss=8.490728

Batch 250450, train_perplexity=5225.7603, train_loss=8.561356

Batch 250460, train_perplexity=4897.8403, train_loss=8.49655

Batch 250470, train_perplexity=4840.5107, train_loss=8.484776

Batch 250480, train_perplexity=5418.019, train_loss=8.597486

Batch 250490, train_perplexity=5014.4336, train_loss=8.520076

Batch 250500, train_perplexity=5607.23, train_loss=8.631812

Batch 250510, train_perplexity=5907.303, train_loss=8.683945

Batch 250520, train_perplexity=6060.1846, train_loss=8.709496

Batch 250530, train_perplexity=5751.967, train_loss=8.657297

Batch 250540, train_perplexity=4846.1416, train_loss=8.485938

Batch 250550, train_perplexity=5728.526, train_loss=8.6532135

Batch 250560, train_perplexity=5049.942, train_loss=8.527132

Batch 250570, train_perplexity=5603.194, train_loss=8.631092

Batch 250580, train_perplexity=5582.318, train_loss=8.627359

Batch 250590, train_perplexity=4521.587, train_loss=8.416618

Batch 250600, train_perplexity=4963.7427, train_loss=8.509915

Batch 250610, train_perplexity=5749.3013, train_loss=8.656834

Batch 250620, train_perplexity=4936.579, train_loss=8.504428

Batch 250630, train_perplexity=5413.83, train_loss=8.596712

Batch 250640, train_perplexity=4723.5537, train_loss=8.460317

Batch 250650, train_perplexity=5314.0513, train_loss=8.57811

Batch 250660, train_perplexity=6525.2646, train_loss=8.783437

Batch 250670, train_perplexity=4631.8955, train_loss=8.4407215

Batch 250680, train_perplexity=4521.6387, train_loss=8.41663

Batch 250690, train_perplexity=4308.353, train_loss=8.368311

Batch 250700, train_perplexity=5635.208, train_loss=8.636789

Batch 250710, train_perplexity=5138.03, train_loss=8.544425

Batch 250720, train_perplexity=6391.1445, train_loss=8.762669

Batch 250730, train_perplexity=5248.5757, train_loss=8.565712

Batch 250740, train_perplexity=5075.3184, train_loss=8.532145

Batch 250750, train_perplexity=5436.44, train_loss=8.60088

Batch 250760, train_perplexity=4158.666, train_loss=8.33295

Batch 250770, train_perplexity=4981.498, train_loss=8.513486

Batch 250780, train_perplexity=5002.3926, train_loss=8.517672

Batch 250790, train_perplexity=5073.446, train_loss=8.531775

Batch 250800, train_perplexity=6044.2314, train_loss=8.70686

Batch 250810, train_perplexity=5021.0137, train_loss=8.521387

Batch 250820, train_perplexity=5112.949, train_loss=8.539532

Batch 250830, train_perplexity=5101.805, train_loss=8.53735

Batch 250840, train_perplexity=4524.6626, train_loss=8.417298

Batch 250850, train_perplexity=3981.8171, train_loss=8.289494

Batch 250860, train_perplexity=5064.633, train_loss=8.530037

Batch 250870, train_perplexity=6450.6577, train_loss=8.771937

Batch 250880, train_perplexity=4918.0977, train_loss=8.500677

Batch 250890, train_perplexity=4868.0835, train_loss=8.490456

Batch 250900, train_perplexity=6049.779, train_loss=8.707777

Batch 250910, train_perplexity=5071.907, train_loss=8.531472

Batch 250920, train_perplexity=5568.568, train_loss=8.624893

Batch 250930, train_perplexity=5585.716, train_loss=8.627968

Batch 250940, train_perplexity=6640.4043, train_loss=8.800928

Batch 250950, train_perplexity=5046.0713, train_loss=8.526365

Batch 250960, train_perplexity=5389.464, train_loss=8.592201

Batch 250970, train_perplexity=5387.47, train_loss=8.591831

Batch 250980, train_perplexity=4543.2515, train_loss=8.421398

Batch 250990, train_perplexity=5699.829, train_loss=8.648191

Batch 251000, train_perplexity=4642.5713, train_loss=8.443024

Batch 251010, train_perplexity=5507.22, train_loss=8.613815

Batch 251020, train_perplexity=5134.038, train_loss=8.543648

Batch 251030, train_perplexity=5580.567, train_loss=8.627046

Batch 251040, train_perplexity=5423.39, train_loss=8.598476

Batch 251050, train_perplexity=5834.518, train_loss=8.671547

Batch 251060, train_perplexity=6240.7124, train_loss=8.73885

Batch 251070, train_perplexity=5616.2153, train_loss=8.633413

Batch 251080, train_perplexity=5119.9907, train_loss=8.540908

Batch 251090, train_perplexity=7557.3433, train_loss=8.930275

Batch 251100, train_perplexity=4145.413, train_loss=8.329758

Batch 251110, train_perplexity=5649.1655, train_loss=8.639263

Batch 251120, train_perplexity=6277.3867, train_loss=8.744709

Batch 251130, train_perplexity=4707.5664, train_loss=8.456926

Batch 251140, train_perplexity=6527.3, train_loss=8.783749

Batch 251150, train_perplexity=5116.569, train_loss=8.540239

Batch 251160, train_perplexity=4347.9033, train_loss=8.377449

Batch 251170, train_perplexity=5115.9443, train_loss=8.540117

Batch 251180, train_perplexity=4816.8506, train_loss=8.479876

Batch 251190, train_perplexity=5270.8066, train_loss=8.569939

Batch 251200, train_perplexity=4738.9893, train_loss=8.463579

Batch 251210, train_perplexity=4786.317, train_loss=8.473516

Batch 251220, train_perplexity=7223.34, train_loss=8.885073

Batch 251230, train_perplexity=5622.8716, train_loss=8.634598

Batch 251240, train_perplexity=5873.9565, train_loss=8.678284

Batch 251250, train_perplexity=4222.3633, train_loss=8.34815

Batch 251260, train_perplexity=4954.3125, train_loss=8.508014

Batch 251270, train_perplexity=4325.591, train_loss=8.372304

Batch 251280, train_perplexity=5516.1826, train_loss=8.615441

Batch 251290, train_perplexity=4897.102, train_loss=8.496399

Batch 251300, train_perplexity=4215.9736, train_loss=8.346636

Batch 251310, train_perplexity=4906.0303, train_loss=8.49822

Batch 251320, train_perplexity=5417.2183, train_loss=8.597338

Batch 251330, train_perplexity=4486.682, train_loss=8.408869

Batch 251340, train_perplexity=4964.5996, train_loss=8.510088

Batch 251350, train_perplexity=5229.4497, train_loss=8.562061

Batch 251360, train_perplexity=6215.9316, train_loss=8.734871

Batch 251370, train_perplexity=5678.1816, train_loss=8.644386

Batch 251380, train_perplexity=5156.374, train_loss=8.547989

Batch 251390, train_perplexity=5161.353, train_loss=8.548954

Batch 251400, train_perplexity=5057.687, train_loss=8.528665

Batch 251410, train_perplexity=5027.7603, train_loss=8.52273

Batch 251420, train_perplexity=4649.3193, train_loss=8.444476

Batch 251430, train_perplexity=5402.782, train_loss=8.594669

Batch 251440, train_perplexity=6015.278, train_loss=8.702058

Batch 251450, train_perplexity=5450.29, train_loss=8.603424

Batch 251460, train_perplexity=3815.435, train_loss=8.24681

Batch 251470, train_perplexity=5766.7637, train_loss=8.659866

Batch 251480, train_perplexity=5402.4473, train_loss=8.594607

Batch 251490, train_perplexity=6032.875, train_loss=8.704979

Batch 251500, train_perplexity=4950.426, train_loss=8.507229

Batch 251510, train_perplexity=4947.4854, train_loss=8.506635

Batch 251520, train_perplexity=6403.6636, train_loss=8.764626

Batch 251530, train_perplexity=4878.452, train_loss=8.492583

Batch 251540, train_perplexity=5411.0225, train_loss=8.596193

Batch 251550, train_perplexity=5281.413, train_loss=8.571949

Batch 251560, train_perplexity=5322.775, train_loss=8.57975

Batch 251570, train_perplexity=5276.399, train_loss=8.570999

Batch 251580, train_perplexity=5964.296, train_loss=8.693546

Batch 251590, train_perplexity=4867.2017, train_loss=8.490274

Batch 251600, train_perplexity=6202.987, train_loss=8.732786

Batch 251610, train_perplexity=5077.2983, train_loss=8.532535

Batch 251620, train_perplexity=4873.463, train_loss=8.49156

Batch 251630, train_perplexity=4926.51, train_loss=8.502386

Batch 251640, train_perplexity=4715.5376, train_loss=8.458618

Batch 251650, train_perplexity=5651.192, train_loss=8.639622

Batch 251660, train_perplexity=5003.1655, train_loss=8.517826

Batch 251670, train_perplexity=5246.1836, train_loss=8.565256

Batch 251680, train_perplexity=4454.816, train_loss=8.401741

Batch 251690, train_perplexity=5960.793, train_loss=8.692959

Batch 251700, train_perplexity=4832.338, train_loss=8.483086

Batch 251710, train_perplexity=5337.771, train_loss=8.582563

Batch 251720, train_perplexity=5432.708, train_loss=8.600193
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 251730, train_perplexity=5664.422, train_loss=8.64196

Batch 251740, train_perplexity=5435.3354, train_loss=8.600677

Batch 251750, train_perplexity=5322.242, train_loss=8.57965

Batch 251760, train_perplexity=6135.481, train_loss=8.721844

Batch 251770, train_perplexity=6753.7935, train_loss=8.81786

Batch 251780, train_perplexity=5255.0767, train_loss=8.56695

Batch 251790, train_perplexity=5907.4946, train_loss=8.683977

Batch 251800, train_perplexity=4734.0747, train_loss=8.462542

Batch 251810, train_perplexity=6036.3394, train_loss=8.705553

Batch 251820, train_perplexity=6190.9067, train_loss=8.730837

Batch 251830, train_perplexity=5551.2026, train_loss=8.62177

Batch 251840, train_perplexity=5627.5337, train_loss=8.6354265

Batch 251850, train_perplexity=4926.087, train_loss=8.5023

Batch 251860, train_perplexity=4736.17, train_loss=8.462984

Batch 251870, train_perplexity=5624.99, train_loss=8.6349745

Batch 251880, train_perplexity=6130.4272, train_loss=8.72102

Batch 251890, train_perplexity=5538.8125, train_loss=8.619535

Batch 251900, train_perplexity=5674.9443, train_loss=8.643816

Batch 251910, train_perplexity=5481.0327, train_loss=8.609049

Batch 251920, train_perplexity=5572.818, train_loss=8.625656

Batch 251930, train_perplexity=4303.909, train_loss=8.367279

Batch 251940, train_perplexity=5069.5327, train_loss=8.531004

Batch 251950, train_perplexity=4635.272, train_loss=8.44145

Batch 251960, train_perplexity=6195.7856, train_loss=8.731625

Batch 251970, train_perplexity=5508.5073, train_loss=8.614049

Batch 251980, train_perplexity=5892.1006, train_loss=8.681368

Batch 251990, train_perplexity=5003.6807, train_loss=8.517929

Batch 252000, train_perplexity=5643.135, train_loss=8.638195

Batch 252010, train_perplexity=5353.7993, train_loss=8.585562

Batch 252020, train_perplexity=5409.99, train_loss=8.596003

Batch 252030, train_perplexity=5542.5645, train_loss=8.620213

Batch 252040, train_perplexity=4958.7275, train_loss=8.508904

Batch 252050, train_perplexity=5994.85, train_loss=8.698656

Batch 252060, train_perplexity=5840.664, train_loss=8.6726

Batch 252070, train_perplexity=5577.034, train_loss=8.626412

Batch 252080, train_perplexity=4989.495, train_loss=8.51509

Batch 252090, train_perplexity=5874.797, train_loss=8.678427

Batch 252100, train_perplexity=4712.0356, train_loss=8.457875

Batch 252110, train_perplexity=5167.7754, train_loss=8.550198

Batch 252120, train_perplexity=4657.1787, train_loss=8.446165

Batch 252130, train_perplexity=6191.651, train_loss=8.730957

Batch 252140, train_perplexity=4735.547, train_loss=8.4628525

Batch 252150, train_perplexity=5338.066, train_loss=8.582619

Batch 252160, train_perplexity=5208.635, train_loss=8.558073

Batch 252170, train_perplexity=5918.536, train_loss=8.685844

Batch 252180, train_perplexity=6802.4424, train_loss=8.825037

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Batch 252190, train_perplexity=5832.6265, train_loss=8.671223

Batch 252200, train_perplexity=5710.602, train_loss=8.65008

Batch 252210, train_perplexity=4261.287, train_loss=8.3573265

Batch 252220, train_perplexity=5170.6543, train_loss=8.550755

Batch 252230, train_perplexity=5557.31, train_loss=8.6228695

Batch 252240, train_perplexity=6147.3584, train_loss=8.723778

Batch 252250, train_perplexity=4592.809, train_loss=8.432247

Batch 252260, train_perplexity=5264.366, train_loss=8.568716

Batch 252270, train_perplexity=5982.377, train_loss=8.696573

Batch 252280, train_perplexity=5642.0586, train_loss=8.638004

Batch 252290, train_perplexity=5822.1895, train_loss=8.669432

Batch 252300, train_perplexity=4798.5244, train_loss=8.476064

Batch 252310, train_perplexity=5764.0093, train_loss=8.659389

Batch 252320, train_perplexity=5816.8726, train_loss=8.668518

Batch 252330, train_perplexity=5814.854, train_loss=8.668171

Batch 252340, train_perplexity=5636.745, train_loss=8.637062

Batch 252350, train_perplexity=5256.119, train_loss=8.567148

Batch 252360, train_perplexity=5466.5205, train_loss=8.606398

Batch 252370, train_perplexity=5268.0728, train_loss=8.56942

Batch 252380, train_perplexity=5743.9473, train_loss=8.655902

Batch 252390, train_perplexity=5024.2036, train_loss=8.522022

Batch 252400, train_perplexity=4725.9644, train_loss=8.460827

Batch 252410, train_perplexity=5431.0503, train_loss=8.599888

Batch 252420, train_perplexity=6041.7993, train_loss=8.706457

Batch 252430, train_perplexity=4771.787, train_loss=8.470476

Batch 252440, train_perplexity=6924.1035, train_loss=8.842764

Batch 252450, train_perplexity=5406.71, train_loss=8.595396

Batch 252460, train_perplexity=5115.017, train_loss=8.539936

Batch 252470, train_perplexity=6684.429, train_loss=8.807536

Batch 252480, train_perplexity=4849.5347, train_loss=8.486638

Batch 252490, train_perplexity=4949.538, train_loss=8.50705

Batch 252500, train_perplexity=5570.99, train_loss=8.625328

Batch 252510, train_perplexity=5459.956, train_loss=8.605196

Batch 252520, train_perplexity=5048.574, train_loss=8.526861

Batch 252530, train_perplexity=5530.8843, train_loss=8.618103

Batch 252540, train_perplexity=5400.4385, train_loss=8.594235

Batch 252550, train_perplexity=6252.174, train_loss=8.7406845

Batch 252560, train_perplexity=4900.1436, train_loss=8.49702

Batch 252570, train_perplexity=5076.209, train_loss=8.53232

Batch 252580, train_perplexity=4723.234, train_loss=8.460249

Batch 252590, train_perplexity=6520.8853, train_loss=8.782765

Batch 252600, train_perplexity=7702.034, train_loss=8.94924

Batch 252610, train_perplexity=5685.1714, train_loss=8.645617

Batch 252620, train_perplexity=6733.176, train_loss=8.814802

Batch 252630, train_perplexity=5951.2617, train_loss=8.691359

Batch 252640, train_perplexity=5414.6357, train_loss=8.596861

Batch 252650, train_perplexity=4367.4897, train_loss=8.381944

Batch 252660, train_perplexity=5096.5044, train_loss=8.53631

Batch 252670, train_perplexity=5010.084, train_loss=8.519208

Batch 252680, train_perplexity=5284.0835, train_loss=8.572454

Batch 252690, train_perplexity=4748.349, train_loss=8.465552

Batch 252700, train_perplexity=5480.5938, train_loss=8.608969

Batch 252710, train_perplexity=5189.506, train_loss=8.554394

Batch 252720, train_perplexity=6744.8916, train_loss=8.816541

Batch 252730, train_perplexity=4760.9688, train_loss=8.468206

Batch 252740, train_perplexity=5721.8647, train_loss=8.65205

Batch 252750, train_perplexity=4624.128, train_loss=8.439043

Batch 252760, train_perplexity=5916.9844, train_loss=8.685582

Batch 252770, train_perplexity=5480.829, train_loss=8.609012

Batch 252780, train_perplexity=5262.3936, train_loss=8.568341

Batch 252790, train_perplexity=5071.98, train_loss=8.5314865

Batch 252800, train_perplexity=4934.061, train_loss=8.503918

Batch 252810, train_perplexity=4500.1377, train_loss=8.411863

Batch 252820, train_perplexity=4615.444, train_loss=8.437163

Batch 252830, train_perplexity=5931.6284, train_loss=8.688054

Batch 252840, train_perplexity=5002.049, train_loss=8.517603

Batch 252850, train_perplexity=5773.4165, train_loss=8.661019

Batch 252860, train_perplexity=4893.881, train_loss=8.495741

Batch 252870, train_perplexity=5555.3286, train_loss=8.622513

Batch 252880, train_perplexity=5374.2515, train_loss=8.589375

Batch 252890, train_perplexity=5309.153, train_loss=8.577188

Batch 252900, train_perplexity=5421.999, train_loss=8.59822

Batch 252910, train_perplexity=6110.7217, train_loss=8.7178

Batch 252920, train_perplexity=5685.6157, train_loss=8.645695

Batch 252930, train_perplexity=5241.1924, train_loss=8.564304

Batch 252940, train_perplexity=6082.2676, train_loss=8.713133

Batch 252950, train_perplexity=5520.061, train_loss=8.616144

Batch 252960, train_perplexity=5061.137, train_loss=8.529346

Batch 252970, train_perplexity=5103.333, train_loss=8.537649

Batch 252980, train_perplexity=5709.2407, train_loss=8.649841
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 252990, train_perplexity=5499.2324, train_loss=8.612364

Batch 253000, train_perplexity=5566.476, train_loss=8.624517

Batch 253010, train_perplexity=4718.196, train_loss=8.459182

Batch 253020, train_perplexity=4852.792, train_loss=8.487309

Batch 253030, train_perplexity=5044.503, train_loss=8.526054

Batch 253040, train_perplexity=5271.9224, train_loss=8.57015

Batch 253050, train_perplexity=5063.696, train_loss=8.529852

Batch 253060, train_perplexity=5174.2407, train_loss=8.551448

Batch 253070, train_perplexity=5904.5996, train_loss=8.683487

Batch 253080, train_perplexity=4820.0674, train_loss=8.480543

Batch 253090, train_perplexity=6024.538, train_loss=8.703596

Batch 253100, train_perplexity=5503.897, train_loss=8.613212

Batch 253110, train_perplexity=5204.7666, train_loss=8.55733

Batch 253120, train_perplexity=5730.34, train_loss=8.65353

Batch 253130, train_perplexity=5565.8125, train_loss=8.624398

Batch 253140, train_perplexity=5754.8804, train_loss=8.657804

Batch 253150, train_perplexity=5871.4585, train_loss=8.677858

Batch 253160, train_perplexity=6116.628, train_loss=8.718766

Batch 253170, train_perplexity=5848.9585, train_loss=8.674019

Batch 253180, train_perplexity=5500.334, train_loss=8.612564

Batch 253190, train_perplexity=6307.836, train_loss=8.749548

Batch 253200, train_perplexity=5306.9707, train_loss=8.5767765

Batch 253210, train_perplexity=5798.8716, train_loss=8.665419

Batch 253220, train_perplexity=5702.3516, train_loss=8.648634

Batch 253230, train_perplexity=5140.157, train_loss=8.544839

Batch 253240, train_perplexity=4840.862, train_loss=8.484848

Batch 253250, train_perplexity=4973.765, train_loss=8.511932

Batch 253260, train_perplexity=4436.6104, train_loss=8.397646

Batch 253270, train_perplexity=5896.744, train_loss=8.682156

Batch 253280, train_perplexity=5247.124, train_loss=8.565435

Batch 253290, train_perplexity=5167.692, train_loss=8.550181

Batch 253300, train_perplexity=4961.807, train_loss=8.509525

Batch 253310, train_perplexity=5720.206, train_loss=8.65176

Batch 253320, train_perplexity=6068.3853, train_loss=8.710848

Batch 253330, train_perplexity=5279.2227, train_loss=8.571534

Batch 253340, train_perplexity=6338.7476, train_loss=8.7544365

Batch 253350, train_perplexity=6421.38, train_loss=8.767388

Batch 253360, train_perplexity=4579.0156, train_loss=8.429239

Batch 253370, train_perplexity=5821.268, train_loss=8.669273

Batch 253380, train_perplexity=5343.0166, train_loss=8.583546

Batch 253390, train_perplexity=4814.343, train_loss=8.479355

Batch 253400, train_perplexity=5735.9277, train_loss=8.654505

Batch 253410, train_perplexity=6144.3228, train_loss=8.723284

Batch 253420, train_perplexity=5785.0957, train_loss=8.66304

Batch 253430, train_perplexity=6026.067, train_loss=8.70385

Batch 253440, train_perplexity=5183.175, train_loss=8.553173

Batch 253450, train_perplexity=5678.658, train_loss=8.64447

Batch 253460, train_perplexity=5227.525, train_loss=8.561693

Batch 253470, train_perplexity=5496.8677, train_loss=8.611934

Batch 253480, train_perplexity=5796.284, train_loss=8.664972

Batch 253490, train_perplexity=7298.749, train_loss=8.895458

Batch 253500, train_perplexity=4915.233, train_loss=8.500094

Batch 253510, train_perplexity=5643.366, train_loss=8.638236

Batch 253520, train_perplexity=4863.3135, train_loss=8.489475

Batch 253530, train_perplexity=6244.8267, train_loss=8.739509

Batch 253540, train_perplexity=4701.191, train_loss=8.455571

Batch 253550, train_perplexity=5162.564, train_loss=8.549189

Batch 253560, train_perplexity=5413.221, train_loss=8.5966

Batch 253570, train_perplexity=4917.141, train_loss=8.500483

Batch 253580, train_perplexity=5936.0254, train_loss=8.688795

Batch 253590, train_perplexity=4637.2173, train_loss=8.44187

Batch 253600, train_perplexity=4853.468, train_loss=8.487449

Batch 253610, train_perplexity=5232.263, train_loss=8.562599

Batch 253620, train_perplexity=4385.006, train_loss=8.385946

Batch 253630, train_perplexity=4945.853, train_loss=8.506305

Batch 253640, train_perplexity=4709.349, train_loss=8.457305

Batch 253650, train_perplexity=5496.569, train_loss=8.611879

Batch 253660, train_perplexity=5742.879, train_loss=8.655716

Batch 253670, train_perplexity=5561.6367, train_loss=8.623648

Batch 253680, train_perplexity=5667.1562, train_loss=8.642443

Batch 253690, train_perplexity=4973.5327, train_loss=8.511886

Batch 253700, train_perplexity=5080.936, train_loss=8.533251

Batch 253710, train_perplexity=4796.443, train_loss=8.47563

Batch 253720, train_perplexity=4793.0312, train_loss=8.474918

Batch 253730, train_perplexity=6549.142, train_loss=8.787089

Batch 253740, train_perplexity=6427.6357, train_loss=8.768362

Batch 253750, train_perplexity=5084.7124, train_loss=8.533994

Batch 253760, train_perplexity=4588.536, train_loss=8.431316

Batch 253770, train_perplexity=4536.0386, train_loss=8.419809

Batch 253780, train_perplexity=4609.2285, train_loss=8.435816

Batch 253790, train_perplexity=5824.122, train_loss=8.669764

Batch 253800, train_perplexity=6334.034, train_loss=8.753693

Batch 253810, train_perplexity=5675.0415, train_loss=8.643833

Batch 253820, train_perplexity=6289.1074, train_loss=8.746574

Batch 253830, train_perplexity=4455.322, train_loss=8.4018545

Batch 253840, train_perplexity=5198.9277, train_loss=8.556208

Batch 253850, train_perplexity=4938.1377, train_loss=8.504744

Batch 253860, train_perplexity=5941.576, train_loss=8.68973

Batch 253870, train_perplexity=5146.3325, train_loss=8.54604

Batch 253880, train_perplexity=5833.3384, train_loss=8.671345

Batch 253890, train_perplexity=5743.5967, train_loss=8.655841

Batch 253900, train_perplexity=6272.7905, train_loss=8.743977

Batch 253910, train_perplexity=6163.9604, train_loss=8.726475

Batch 253920, train_perplexity=6293.763, train_loss=8.747314

Batch 253930, train_perplexity=4827.7915, train_loss=8.482144

Batch 253940, train_perplexity=4773.007, train_loss=8.470732

Batch 253950, train_perplexity=4910.735, train_loss=8.499179

Batch 253960, train_perplexity=5925.3867, train_loss=8.687001

Batch 253970, train_perplexity=4942.0103, train_loss=8.5055275

Batch 253980, train_perplexity=4559.724, train_loss=8.425017

Batch 253990, train_perplexity=5009.386, train_loss=8.519069

Batch 254000, train_perplexity=4902.0317, train_loss=8.497405

Batch 254010, train_perplexity=4980.4194, train_loss=8.513269

Batch 254020, train_perplexity=5200.0137, train_loss=8.5564165

Batch 254030, train_perplexity=5419.709, train_loss=8.597797

Batch 254040, train_perplexity=4705.982, train_loss=8.45659

Batch 254050, train_perplexity=5080.7373, train_loss=8.533212

Batch 254060, train_perplexity=4442.4956, train_loss=8.398972

Batch 254070, train_perplexity=4550.2715, train_loss=8.422942

Batch 254080, train_perplexity=5712.051, train_loss=8.650333

Batch 254090, train_perplexity=6304.9795, train_loss=8.749095

Batch 254100, train_perplexity=5744.7417, train_loss=8.65604

Batch 254110, train_perplexity=5373.3184, train_loss=8.589201

Batch 254120, train_perplexity=5458.4565, train_loss=8.604921

Batch 254130, train_perplexity=5394.771, train_loss=8.593185

Batch 254140, train_perplexity=5077.0903, train_loss=8.532494

Batch 254150, train_perplexity=5045.667, train_loss=8.526285

Batch 254160, train_perplexity=5519.777, train_loss=8.616093

Batch 254170, train_perplexity=5170.842, train_loss=8.550791

Batch 254180, train_perplexity=6730.492, train_loss=8.814404

Batch 254190, train_perplexity=5383.6025, train_loss=8.591113

Batch 254200, train_perplexity=5081.382, train_loss=8.533339

Batch 254210, train_perplexity=5679.703, train_loss=8.644654

Batch 254220, train_perplexity=5202.092, train_loss=8.556816

Batch 254230, train_perplexity=4462.1123, train_loss=8.403378

Batch 254240, train_perplexity=5207.055, train_loss=8.55777

Batch 254250, train_perplexity=4649.115, train_loss=8.444432

Batch 254260, train_perplexity=5374.2925, train_loss=8.589382

Batch 254270, train_perplexity=5449.76, train_loss=8.603327

Batch 254280, train_perplexity=5400.2583, train_loss=8.594202

Batch 254290, train_perplexity=5116.4224, train_loss=8.540211

Batch 254300, train_perplexity=5093.4434, train_loss=8.535709
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 254310, train_perplexity=4968.8154, train_loss=8.510937

Batch 254320, train_perplexity=5279.3887, train_loss=8.571566

Batch 254330, train_perplexity=4403.403, train_loss=8.390133

Batch 254340, train_perplexity=5776.3906, train_loss=8.661534

Batch 254350, train_perplexity=4959.896, train_loss=8.50914

Batch 254360, train_perplexity=5465.567, train_loss=8.606223

Batch 254370, train_perplexity=6674.021, train_loss=8.805978

Batch 254380, train_perplexity=5680.326, train_loss=8.644764

Batch 254390, train_perplexity=6148.1206, train_loss=8.723902

Batch 254400, train_perplexity=5795.687, train_loss=8.664869

Batch 254410, train_perplexity=5114.91, train_loss=8.539915

Batch 254420, train_perplexity=5841.9233, train_loss=8.672815

Batch 254430, train_perplexity=6342.539, train_loss=8.755034

Batch 254440, train_perplexity=5097.394, train_loss=8.536485

Batch 254450, train_perplexity=4796.241, train_loss=8.475588

Batch 254460, train_perplexity=5345.6772, train_loss=8.5840435

Batch 254470, train_perplexity=4937.224, train_loss=8.504559

Batch 254480, train_perplexity=6245.4277, train_loss=8.739605

Batch 254490, train_perplexity=4995.0225, train_loss=8.516197

Batch 254500, train_perplexity=5464.144, train_loss=8.605963

Batch 254510, train_perplexity=5183.5405, train_loss=8.553244

Batch 254520, train_perplexity=5057.0894, train_loss=8.528546

Batch 254530, train_perplexity=4706.6597, train_loss=8.456734

Batch 254540, train_perplexity=4894.436, train_loss=8.495854

Batch 254550, train_perplexity=4720.095, train_loss=8.459584

Batch 254560, train_perplexity=6867.141, train_loss=8.834503

Batch 254570, train_perplexity=5085.1343, train_loss=8.534077

Batch 254580, train_perplexity=5029.851, train_loss=8.523146

Batch 254590, train_perplexity=4828.2104, train_loss=8.482231

Batch 254600, train_perplexity=5980.061, train_loss=8.696186

Batch 254610, train_perplexity=6148.7188, train_loss=8.723999

Batch 254620, train_perplexity=4871.107, train_loss=8.491076

Batch 254630, train_perplexity=5028.6426, train_loss=8.522905

Batch 254640, train_perplexity=4940.1587, train_loss=8.505153

Batch 254650, train_perplexity=6020.7817, train_loss=8.702972

Batch 254660, train_perplexity=6197.789, train_loss=8.731948

Batch 254670, train_perplexity=4687.635, train_loss=8.452683

Batch 254680, train_perplexity=5894.7144, train_loss=8.681811

Batch 254690, train_perplexity=5036.47, train_loss=8.524461

Batch 254700, train_perplexity=5930.8647, train_loss=8.687925

Batch 254710, train_perplexity=4771.76, train_loss=8.47047

Batch 254720, train_perplexity=5066.01, train_loss=8.530309

Batch 254730, train_perplexity=4658.467, train_loss=8.446442

Batch 254740, train_perplexity=5247.7446, train_loss=8.565554

Batch 254750, train_perplexity=5480.249, train_loss=8.608906

Batch 254760, train_perplexity=5782.536, train_loss=8.662598

Batch 254770, train_perplexity=5243.6025, train_loss=8.564764

Batch 254780, train_perplexity=6647.4565, train_loss=8.80199

Batch 254790, train_perplexity=4622.8843, train_loss=8.438774

Batch 254800, train_perplexity=5371.1, train_loss=8.588788

Batch 254810, train_perplexity=5816.185, train_loss=8.6684

Batch 254820, train_perplexity=5944.166, train_loss=8.6901655

Batch 254830, train_perplexity=5606.187, train_loss=8.631626

Batch 254840, train_perplexity=5911.8906, train_loss=8.684721

Batch 254850, train_perplexity=6417.921, train_loss=8.7668495

Batch 254860, train_perplexity=5538.7495, train_loss=8.619524

Batch 254870, train_perplexity=4820.729, train_loss=8.48068

Batch 254880, train_perplexity=4718.7583, train_loss=8.459301

Batch 254890, train_perplexity=6388.6216, train_loss=8.762274

Batch 254900, train_perplexity=5604.145, train_loss=8.631262

Batch 254910, train_perplexity=5250.8184, train_loss=8.566139

Batch 254920, train_perplexity=5434.8223, train_loss=8.600582

Batch 254930, train_perplexity=5301.2046, train_loss=8.575689

Batch 254940, train_perplexity=6040.52, train_loss=8.706245

Batch 254950, train_perplexity=5623.88, train_loss=8.634777

Batch 254960, train_perplexity=5482.1514, train_loss=8.609253

Batch 254970, train_perplexity=5341.4424, train_loss=8.583251

Batch 254980, train_perplexity=5770.9395, train_loss=8.66059

Batch 254990, train_perplexity=5393.9272, train_loss=8.593029

Batch 255000, train_perplexity=5949.6274, train_loss=8.691084

Batch 255010, train_perplexity=5170.0825, train_loss=8.550644

Batch 255020, train_perplexity=6362.586, train_loss=8.75819

Batch 255030, train_perplexity=5780.992, train_loss=8.662331

Batch 255040, train_perplexity=6077.1245, train_loss=8.712287

Batch 255050, train_perplexity=5251.3843, train_loss=8.566247

Batch 255060, train_perplexity=6132.3687, train_loss=8.721336

Batch 255070, train_perplexity=5107.871, train_loss=8.538538

Batch 255080, train_perplexity=4855.2407, train_loss=8.487814

Batch 255090, train_perplexity=5799.187, train_loss=8.665473

Batch 255100, train_perplexity=5662.8667, train_loss=8.6416855

Batch 255110, train_perplexity=5123.8936, train_loss=8.54167

Batch 255120, train_perplexity=5187.21, train_loss=8.553951

Batch 255130, train_perplexity=5036.206, train_loss=8.524408

Batch 255140, train_perplexity=4517.7207, train_loss=8.415763

Batch 255150, train_perplexity=5599.1343, train_loss=8.630367

Batch 255160, train_perplexity=4206.5796, train_loss=8.344405

Batch 255170, train_perplexity=5428.073, train_loss=8.5993395

Batch 255180, train_perplexity=6225.228, train_loss=8.736365

Batch 255190, train_perplexity=5587.863, train_loss=8.628352

Batch 255200, train_perplexity=5325.568, train_loss=8.580275

Batch 255210, train_perplexity=5067.4688, train_loss=8.530597

Batch 255220, train_perplexity=4778.4175, train_loss=8.471865

Batch 255230, train_perplexity=4839.726, train_loss=8.484613

Batch 255240, train_perplexity=6389.0356, train_loss=8.762339

Batch 255250, train_perplexity=5207.949, train_loss=8.557941

Batch 255260, train_perplexity=4556.668, train_loss=8.424347

Batch 255270, train_perplexity=4341.7915, train_loss=8.376042

Batch 255280, train_perplexity=5903.5186, train_loss=8.683304

Batch 255290, train_perplexity=5258.014, train_loss=8.567509

Batch 255300, train_perplexity=5738.237, train_loss=8.654907

Batch 255310, train_perplexity=4881.868, train_loss=8.493283

Batch 255320, train_perplexity=5329.4087, train_loss=8.580996

Batch 255330, train_perplexity=5203.6, train_loss=8.557106

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Batch 255340, train_perplexity=5325.6284, train_loss=8.580286

Batch 255350, train_perplexity=6416.8623, train_loss=8.766685

Batch 255360, train_perplexity=5380.518, train_loss=8.59054

Batch 255370, train_perplexity=4824.022, train_loss=8.481363

Batch 255380, train_perplexity=5580.2314, train_loss=8.626986

Batch 255390, train_perplexity=5340.113, train_loss=8.583002

Batch 255400, train_perplexity=6650.557, train_loss=8.802456

Batch 255410, train_perplexity=6155.02, train_loss=8.725023

Batch 255420, train_perplexity=5358.259, train_loss=8.586394

Batch 255430, train_perplexity=6150.83, train_loss=8.724342

Batch 255440, train_perplexity=4940.6157, train_loss=8.505245

Batch 255450, train_perplexity=5051.199, train_loss=8.527381

Batch 255460, train_perplexity=5132.7554, train_loss=8.543398

Batch 255470, train_perplexity=4513.066, train_loss=8.414732

Batch 255480, train_perplexity=5262.1177, train_loss=8.568289

Batch 255490, train_perplexity=5318.969, train_loss=8.579035

Batch 255500, train_perplexity=4249.3804, train_loss=8.354528

Batch 255510, train_perplexity=4987.8013, train_loss=8.5147505

Batch 255520, train_perplexity=5199.463, train_loss=8.556311

Batch 255530, train_perplexity=6873.51, train_loss=8.83543

Batch 255540, train_perplexity=6075.8325, train_loss=8.712074

Batch 255550, train_perplexity=5262.1475, train_loss=8.568295

Batch 255560, train_perplexity=5570.958, train_loss=8.625322
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 255570, train_perplexity=5049.8457, train_loss=8.527113

Batch 255580, train_perplexity=5210.821, train_loss=8.558493

Batch 255590, train_perplexity=4815.826, train_loss=8.479663

Batch 255600, train_perplexity=4216.283, train_loss=8.346709

Batch 255610, train_perplexity=5364.803, train_loss=8.587615

Batch 255620, train_perplexity=5266.405, train_loss=8.569103

Batch 255630, train_perplexity=5853.4673, train_loss=8.674789

Batch 255640, train_perplexity=4765.843, train_loss=8.46923

Batch 255650, train_perplexity=5663.617, train_loss=8.641818

Batch 255660, train_perplexity=5765.499, train_loss=8.659647

Batch 255670, train_perplexity=4971.6973, train_loss=8.511517

Batch 255680, train_perplexity=5181.786, train_loss=8.552905

Batch 255690, train_perplexity=5510.173, train_loss=8.614351

Batch 255700, train_perplexity=5018.8496, train_loss=8.520956

Batch 255710, train_perplexity=5109.6055, train_loss=8.5388775

Batch 255720, train_perplexity=5338.0103, train_loss=8.582608

Batch 255730, train_perplexity=5663.1904, train_loss=8.641743

Batch 255740, train_perplexity=4585.072, train_loss=8.430561

Batch 255750, train_perplexity=5199.5127, train_loss=8.55632

Batch 255760, train_perplexity=4901.2607, train_loss=8.497248

Batch 255770, train_perplexity=5616.3013, train_loss=8.633429

Batch 255780, train_perplexity=5094.007, train_loss=8.53582

Batch 255790, train_perplexity=5258.8564, train_loss=8.567669

Batch 255800, train_perplexity=5012.4307, train_loss=8.519676

Batch 255810, train_perplexity=5672.4497, train_loss=8.643376

Batch 255820, train_perplexity=5404.699, train_loss=8.595024

Batch 255830, train_perplexity=4408.7056, train_loss=8.391336

Batch 255840, train_perplexity=5075.0767, train_loss=8.532097

Batch 255850, train_perplexity=6353.745, train_loss=8.7568

Batch 255860, train_perplexity=6169.1416, train_loss=8.727315

Batch 255870, train_perplexity=5504.9414, train_loss=8.613401

Batch 255880, train_perplexity=4607.2554, train_loss=8.435388

Batch 255890, train_perplexity=6076.678, train_loss=8.7122135

Batch 255900, train_perplexity=5472.702, train_loss=8.607528

Batch 255910, train_perplexity=5119.2046, train_loss=8.540754

Batch 255920, train_perplexity=5444.8765, train_loss=8.60243

Batch 255930, train_perplexity=5306.485, train_loss=8.576685

Batch 255940, train_perplexity=6916.1577, train_loss=8.841616

Batch 255950, train_perplexity=5250.1025, train_loss=8.566003

Batch 255960, train_perplexity=5763.564, train_loss=8.659311

Batch 255970, train_perplexity=4643.014, train_loss=8.443119

Batch 255980, train_perplexity=5153.4097, train_loss=8.547414

Batch 255990, train_perplexity=6182.859, train_loss=8.729536

Batch 256000, train_perplexity=5832.159, train_loss=8.671143

Batch 256010, train_perplexity=5988.542, train_loss=8.697603

Batch 256020, train_perplexity=5481.8066, train_loss=8.60919

Batch 256030, train_perplexity=5895.1753, train_loss=8.68189

Batch 256040, train_perplexity=4623.4575, train_loss=8.438898

Batch 256050, train_perplexity=3906.2917, train_loss=8.270344

Batch 256060, train_perplexity=4402.6387, train_loss=8.389959

Batch 256070, train_perplexity=4908.9883, train_loss=8.498823

Batch 256080, train_perplexity=4820.9956, train_loss=8.480736

Batch 256090, train_perplexity=5422.3765, train_loss=8.5982895

Batch 256100, train_perplexity=5427.8506, train_loss=8.5992985

Batch 256110, train_perplexity=5907.889, train_loss=8.684044

Batch 256120, train_perplexity=6432.0264, train_loss=8.769045

Batch 256130, train_perplexity=5390.888, train_loss=8.592465

Batch 256140, train_perplexity=5123.0527, train_loss=8.541506

Batch 256150, train_perplexity=4982.928, train_loss=8.513773

Batch 256160, train_perplexity=5580.0347, train_loss=8.62695

Batch 256170, train_perplexity=5919.326, train_loss=8.685978

Batch 256180, train_perplexity=4590.296, train_loss=8.4317

Batch 256190, train_perplexity=4311.9497, train_loss=8.369145

Batch 256200, train_perplexity=4802.8877, train_loss=8.476973

Batch 256210, train_perplexity=6682.3833, train_loss=8.80723

Batch 256220, train_perplexity=4929.3345, train_loss=8.502959

Batch 256230, train_perplexity=4975.5205, train_loss=8.512285

Batch 256240, train_perplexity=4464.479, train_loss=8.403908

Batch 256250, train_perplexity=5279.671, train_loss=8.571619

Batch 256260, train_perplexity=5220.112, train_loss=8.560274

Batch 256270, train_perplexity=4097.1255, train_loss=8.318041

Batch 256280, train_perplexity=4833.1074, train_loss=8.483245

Batch 256290, train_perplexity=6413.7725, train_loss=8.766203

Batch 256300, train_perplexity=5838.325, train_loss=8.672199

Batch 256310, train_perplexity=5967.539, train_loss=8.69409

Batch 256320, train_perplexity=6153.6934, train_loss=8.724808

Batch 256330, train_perplexity=4892.593, train_loss=8.495478

Batch 256340, train_perplexity=5920.2183, train_loss=8.686129

Batch 256350, train_perplexity=6003.993, train_loss=8.70018

Batch 256360, train_perplexity=5072.067, train_loss=8.531504

Batch 256370, train_perplexity=5075.8994, train_loss=8.532259

Batch 256380, train_perplexity=4700.8506, train_loss=8.455499

Batch 256390, train_perplexity=5836.377, train_loss=8.671865

Batch 256400, train_perplexity=5340.531, train_loss=8.58308

Batch 256410, train_perplexity=4751.004, train_loss=8.466111

Batch 256420, train_perplexity=5426.898, train_loss=8.599123

Batch 256430, train_perplexity=5412.488, train_loss=8.596464

Batch 256440, train_perplexity=4916.991, train_loss=8.500452

Batch 256450, train_perplexity=5334.926, train_loss=8.58203

Batch 256460, train_perplexity=5433.646, train_loss=8.600366

Batch 256470, train_perplexity=5186.265, train_loss=8.553769

Batch 256480, train_perplexity=5135.517, train_loss=8.543936

Batch 256490, train_perplexity=4671.088, train_loss=8.449147

Batch 256500, train_perplexity=5637.573, train_loss=8.637209

Batch 256510, train_perplexity=5848.8467, train_loss=8.674

Batch 256520, train_perplexity=4508.2266, train_loss=8.413659

Batch 256530, train_perplexity=4685.99, train_loss=8.4523325

Batch 256540, train_perplexity=5778.501, train_loss=8.6619

Batch 256550, train_perplexity=5855.7, train_loss=8.675171

Batch 256560, train_perplexity=5298.5913, train_loss=8.575196

Batch 256570, train_perplexity=4956.784, train_loss=8.5085125

Batch 256580, train_perplexity=5264.8784, train_loss=8.568813

Batch 256590, train_perplexity=4738.212, train_loss=8.463415

Batch 256600, train_perplexity=6116.739, train_loss=8.718784

Batch 256610, train_perplexity=5531.3066, train_loss=8.618179

Batch 256620, train_perplexity=5220.3706, train_loss=8.560324

Batch 256630, train_perplexity=5681.5776, train_loss=8.644984

Batch 256640, train_perplexity=5728.548, train_loss=8.653217

Batch 256650, train_perplexity=4775.338, train_loss=8.47122

Batch 256660, train_perplexity=5212.6597, train_loss=8.5588455

Batch 256670, train_perplexity=5307.9375, train_loss=8.576959

Batch 256680, train_perplexity=5045.441, train_loss=8.52624

Batch 256690, train_perplexity=5404.153, train_loss=8.594923

Batch 256700, train_perplexity=5642.952, train_loss=8.638163

Batch 256710, train_perplexity=5005.9766, train_loss=8.518388

Batch 256720, train_perplexity=4547.8765, train_loss=8.422416

Batch 256730, train_perplexity=7030.3394, train_loss=8.85799

Batch 256740, train_perplexity=4733.3613, train_loss=8.462391

Batch 256750, train_perplexity=6053.097, train_loss=8.708325

Batch 256760, train_perplexity=5875.6875, train_loss=8.678578

Batch 256770, train_perplexity=6443.716, train_loss=8.770861

Batch 256780, train_perplexity=4743.8413, train_loss=8.464602

Batch 256790, train_perplexity=4761.155, train_loss=8.4682455

Batch 256800, train_perplexity=4919.0923, train_loss=8.500879

Batch 256810, train_perplexity=5199.1807, train_loss=8.556256

Batch 256820, train_perplexity=4852.454, train_loss=8.48724

Batch 256830, train_perplexity=5783.3247, train_loss=8.662734

Batch 256840, train_perplexity=4902.0273, train_loss=8.497404

Batch 256850, train_perplexity=5149.2485, train_loss=8.546606

Batch 256860, train_perplexity=5209.191, train_loss=8.55818

Batch 256870, train_perplexity=5662.791, train_loss=8.641672

Batch 256880, train_perplexity=5501.4883, train_loss=8.612774
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 256890, train_perplexity=5111.1943, train_loss=8.539188

Batch 256900, train_perplexity=4793.845, train_loss=8.475088

Batch 256910, train_perplexity=5942.0747, train_loss=8.689814

Batch 256920, train_perplexity=5810.3306, train_loss=8.667393

Batch 256930, train_perplexity=5105.178, train_loss=8.538011

Batch 256940, train_perplexity=4391.857, train_loss=8.387507

Batch 256950, train_perplexity=5020.324, train_loss=8.52125

Batch 256960, train_perplexity=5595.0454, train_loss=8.629637

Batch 256970, train_perplexity=6747.5874, train_loss=8.81694

Batch 256980, train_perplexity=5366.267, train_loss=8.587888

Batch 256990, train_perplexity=5952.8965, train_loss=8.691633

Batch 257000, train_perplexity=4815.381, train_loss=8.47957

Batch 257010, train_perplexity=5557.077, train_loss=8.622828

Batch 257020, train_perplexity=6034.083, train_loss=8.705179

Batch 257030, train_perplexity=5643.7217, train_loss=8.638299

Batch 257040, train_perplexity=5118.101, train_loss=8.540539

Batch 257050, train_perplexity=5100.9927, train_loss=8.53719

Batch 257060, train_perplexity=5507.9243, train_loss=8.613943

Batch 257070, train_perplexity=4830.2275, train_loss=8.482649

Batch 257080, train_perplexity=6245.6006, train_loss=8.739633

Batch 257090, train_perplexity=5077.633, train_loss=8.5326

Batch 257100, train_perplexity=5638.229, train_loss=8.637325

Batch 257110, train_perplexity=4671.034, train_loss=8.449136

Batch 257120, train_perplexity=5396.994, train_loss=8.593597

Batch 257130, train_perplexity=5584.1016, train_loss=8.627679

Batch 257140, train_perplexity=4979.2515, train_loss=8.513035

Batch 257150, train_perplexity=5160.142, train_loss=8.548719

Batch 257160, train_perplexity=4817.3237, train_loss=8.479974

Batch 257170, train_perplexity=4627.8335, train_loss=8.439844

Batch 257180, train_perplexity=7403.4004, train_loss=8.909695

Batch 257190, train_perplexity=5365.8423, train_loss=8.587809

Batch 257200, train_perplexity=5657.8247, train_loss=8.640795

Batch 257210, train_perplexity=4930.6836, train_loss=8.503233

Batch 257220, train_perplexity=4293.7134, train_loss=8.364907

Batch 257230, train_perplexity=4560.537, train_loss=8.425196

Batch 257240, train_perplexity=4576.073, train_loss=8.4285965

Batch 257250, train_perplexity=5033.6997, train_loss=8.5239105

Batch 257260, train_perplexity=4831.5635, train_loss=8.482925

Batch 257270, train_perplexity=5983.769, train_loss=8.696806

Batch 257280, train_perplexity=6266.196, train_loss=8.742925

Batch 257290, train_perplexity=6225.317, train_loss=8.73638

Batch 257300, train_perplexity=5703.924, train_loss=8.64891

Batch 257310, train_perplexity=6468.4053, train_loss=8.774685

Batch 257320, train_perplexity=6053.461, train_loss=8.708385

Batch 257330, train_perplexity=6661.386, train_loss=8.804083

Batch 257340, train_perplexity=5396.0366, train_loss=8.59342

Batch 257350, train_perplexity=5327.0815, train_loss=8.580559

Batch 257360, train_perplexity=5807.605, train_loss=8.6669235

Batch 257370, train_perplexity=5831.9146, train_loss=8.671101

Batch 257380, train_perplexity=5678.5986, train_loss=8.64446

Batch 257390, train_perplexity=5986.1553, train_loss=8.697205

Batch 257400, train_perplexity=5219.7935, train_loss=8.560213

Batch 257410, train_perplexity=5992.3867, train_loss=8.698245

Batch 257420, train_perplexity=5499.563, train_loss=8.612424

Batch 257430, train_perplexity=5321.5264, train_loss=8.579515

Batch 257440, train_perplexity=4924.875, train_loss=8.502054

Batch 257450, train_perplexity=5372.7856, train_loss=8.589102

Batch 257460, train_perplexity=5819.475, train_loss=8.668965

Batch 257470, train_perplexity=4631.4277, train_loss=8.44062

Batch 257480, train_perplexity=5047.3804, train_loss=8.526625

Batch 257490, train_perplexity=4923.814, train_loss=8.501839

Batch 257500, train_perplexity=5317.169, train_loss=8.578696

Batch 257510, train_perplexity=6495.382, train_loss=8.778847

Batch 257520, train_perplexity=5090.1416, train_loss=8.535061

Batch 257530, train_perplexity=5268.8413, train_loss=8.569566

Batch 257540, train_perplexity=4957.328, train_loss=8.508622

Batch 257550, train_perplexity=5501.5195, train_loss=8.61278

Batch 257560, train_perplexity=5624.3145, train_loss=8.634854

Batch 257570, train_perplexity=5724.457, train_loss=8.652503

Batch 257580, train_perplexity=5208.3267, train_loss=8.558014

Batch 257590, train_perplexity=5651.3643, train_loss=8.639652

Batch 257600, train_perplexity=7889.775, train_loss=8.973323

Batch 257610, train_perplexity=5092.3022, train_loss=8.535485

Batch 257620, train_perplexity=5096.247, train_loss=8.53626

Batch 257630, train_perplexity=4984.2065, train_loss=8.5140295

Batch 257640, train_perplexity=5200.4253, train_loss=8.556496

Batch 257650, train_perplexity=5787.612, train_loss=8.663475

Batch 257660, train_perplexity=4720.802, train_loss=8.459734

Batch 257670, train_perplexity=4420.081, train_loss=8.393913

Batch 257680, train_perplexity=4820.784, train_loss=8.480692

Batch 257690, train_perplexity=4700.7026, train_loss=8.455467

Batch 257700, train_perplexity=4647.9673, train_loss=8.444185

Batch 257710, train_perplexity=4729.1606, train_loss=8.461503

Batch 257720, train_perplexity=5330.4556, train_loss=8.581192

Batch 257730, train_perplexity=6356.2544, train_loss=8.7571945

Batch 257740, train_perplexity=4999.1113, train_loss=8.517015

Batch 257750, train_perplexity=4531.542, train_loss=8.4188175

Batch 257760, train_perplexity=5825.05, train_loss=8.669923

Batch 257770, train_perplexity=5078.8726, train_loss=8.532845

Batch 257780, train_perplexity=5500.428, train_loss=8.612581

Batch 257790, train_perplexity=5080.9507, train_loss=8.533254

Batch 257800, train_perplexity=5381.349, train_loss=8.590694

Batch 257810, train_perplexity=5611.868, train_loss=8.632639

Batch 257820, train_perplexity=5181.811, train_loss=8.55291

Batch 257830, train_perplexity=4538.964, train_loss=8.420454

Batch 257840, train_perplexity=6305.2256, train_loss=8.749134

Batch 257850, train_perplexity=5139.5347, train_loss=8.544718

Batch 257860, train_perplexity=4741.8467, train_loss=8.464182

Batch 257870, train_perplexity=5549.3027, train_loss=8.621428

Batch 257880, train_perplexity=5865.9287, train_loss=8.676916

Batch 257890, train_perplexity=4538.029, train_loss=8.420248

Batch 257900, train_perplexity=4612.0737, train_loss=8.436433

Batch 257910, train_perplexity=5144.35, train_loss=8.545654

Batch 257920, train_perplexity=4825.163, train_loss=8.4816

Batch 257930, train_perplexity=6537.9404, train_loss=8.7853775

Batch 257940, train_perplexity=5211.75, train_loss=8.558671

Batch 257950, train_perplexity=5534.599, train_loss=8.618774

Batch 257960, train_perplexity=5519.882, train_loss=8.616112

Batch 257970, train_perplexity=5346.3555, train_loss=8.58417

Batch 257980, train_perplexity=6077.3335, train_loss=8.712321

Batch 257990, train_perplexity=5388.899, train_loss=8.592096

Batch 258000, train_perplexity=5323.3535, train_loss=8.579859

Batch 258010, train_perplexity=4143.6147, train_loss=8.329324

Batch 258020, train_perplexity=5901.5146, train_loss=8.682964

Batch 258030, train_perplexity=5905.912, train_loss=8.683709

Batch 258040, train_perplexity=6618.977, train_loss=8.797696

Batch 258050, train_perplexity=4728.967, train_loss=8.461462

Batch 258060, train_perplexity=5535.6973, train_loss=8.618973

Batch 258070, train_perplexity=5621.4346, train_loss=8.634342

Batch 258080, train_perplexity=4613.094, train_loss=8.436654

Batch 258090, train_perplexity=5452.9985, train_loss=8.603921

Batch 258100, train_perplexity=5486.001, train_loss=8.609955

Batch 258110, train_perplexity=6512.899, train_loss=8.78154

Batch 258120, train_perplexity=5208.0933, train_loss=8.557969

Batch 258130, train_perplexity=5154.152, train_loss=8.547558

Batch 258140, train_perplexity=6038.1816, train_loss=8.705858

Batch 258150, train_perplexity=5036.8496, train_loss=8.524536

Batch 258160, train_perplexity=5119.3657, train_loss=8.540786

Batch 258170, train_perplexity=4825.6553, train_loss=8.481702

Batch 258180, train_perplexity=6118.617, train_loss=8.719091

Batch 258190, train_perplexity=5136.301, train_loss=8.544088

Batch 258200, train_perplexity=5321.0137, train_loss=8.579419
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 258210, train_perplexity=5345.927, train_loss=8.58409

Batch 258220, train_perplexity=5161.973, train_loss=8.549074

Batch 258230, train_perplexity=5557.31, train_loss=8.6228695

Batch 258240, train_perplexity=5672.4985, train_loss=8.643385

Batch 258250, train_perplexity=4535.831, train_loss=8.419764

Batch 258260, train_perplexity=5092.2485, train_loss=8.535475

Batch 258270, train_perplexity=5378.6606, train_loss=8.590195

Batch 258280, train_perplexity=5688.8374, train_loss=8.646261

Batch 258290, train_perplexity=4662.538, train_loss=8.447315

Batch 258300, train_perplexity=4986.9263, train_loss=8.514575

Batch 258310, train_perplexity=5952.2607, train_loss=8.691526

Batch 258320, train_perplexity=4598.3228, train_loss=8.433447

Batch 258330, train_perplexity=4976.2607, train_loss=8.512434

Batch 258340, train_perplexity=5097.8364, train_loss=8.5365715

Batch 258350, train_perplexity=6185.7607, train_loss=8.730005

Batch 258360, train_perplexity=5269.54, train_loss=8.569698

Batch 258370, train_perplexity=4537.882, train_loss=8.420216

Batch 258380, train_perplexity=4304.164, train_loss=8.367338

Batch 258390, train_perplexity=5276.3086, train_loss=8.570982

Batch 258400, train_perplexity=5519.9243, train_loss=8.616119

Batch 258410, train_perplexity=6546.5005, train_loss=8.786686

Batch 258420, train_perplexity=5213.0874, train_loss=8.558928

Batch 258430, train_perplexity=5613.19, train_loss=8.6328745

Batch 258440, train_perplexity=4612.7905, train_loss=8.436588

Batch 258450, train_perplexity=5797.4673, train_loss=8.665176

Batch 258460, train_perplexity=5005.4277, train_loss=8.518278

Batch 258470, train_perplexity=5879.101, train_loss=8.679159

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Batch 258480, train_perplexity=4879.7085, train_loss=8.492841

Batch 258490, train_perplexity=4622.7964, train_loss=8.438755

Batch 258500, train_perplexity=5387.799, train_loss=8.591892

Batch 258510, train_perplexity=4929.612, train_loss=8.5030155

Batch 258520, train_perplexity=5910.696, train_loss=8.684519

Batch 258530, train_perplexity=5376.902, train_loss=8.589868

Batch 258540, train_perplexity=5514.636, train_loss=8.615161

Batch 258550, train_perplexity=5351.492, train_loss=8.585131

Batch 258560, train_perplexity=4853.7637, train_loss=8.48751

Batch 258570, train_perplexity=5483.621, train_loss=8.609521

Batch 258580, train_perplexity=4615.422, train_loss=8.437159

Batch 258590, train_perplexity=5109.5615, train_loss=8.538869

Batch 258600, train_perplexity=5403.6895, train_loss=8.594837

Batch 258610, train_perplexity=5428.016, train_loss=8.599329

Batch 258620, train_perplexity=5661.754, train_loss=8.641489

Batch 258630, train_perplexity=4844.436, train_loss=8.485586

Batch 258640, train_perplexity=5393.459, train_loss=8.592942

Batch 258650, train_perplexity=5553.3525, train_loss=8.622157

Batch 258660, train_perplexity=6144.5103, train_loss=8.723314

Batch 258670, train_perplexity=5953.635, train_loss=8.691757

Batch 258680, train_perplexity=4898.2515, train_loss=8.496634

Batch 258690, train_perplexity=5309.5933, train_loss=8.5772705

Batch 258700, train_perplexity=5201.035, train_loss=8.556613

Batch 258710, train_perplexity=5505.792, train_loss=8.613556

Batch 258720, train_perplexity=5344.046, train_loss=8.583738

Batch 258730, train_perplexity=6146.362, train_loss=8.723616

Batch 258740, train_perplexity=4746.4976, train_loss=8.465162

Batch 258750, train_perplexity=6517.6333, train_loss=8.782267

Batch 258760, train_perplexity=4896.3926, train_loss=8.496254

Batch 258770, train_perplexity=5707.0903, train_loss=8.649465

Batch 258780, train_perplexity=5426.7896, train_loss=8.599103

Batch 258790, train_perplexity=4563.5303, train_loss=8.425852

Batch 258800, train_perplexity=4744.674, train_loss=8.464778

Batch 258810, train_perplexity=5514.8516, train_loss=8.6152

Batch 258820, train_perplexity=5196.0825, train_loss=8.55566

Batch 258830, train_perplexity=5624.6523, train_loss=8.634914

Batch 258840, train_perplexity=5191.9316, train_loss=8.554861

Batch 258850, train_perplexity=4897.242, train_loss=8.496428

Batch 258860, train_perplexity=5241.8726, train_loss=8.564434

Batch 258870, train_perplexity=6300.862, train_loss=8.748442

Batch 258880, train_perplexity=5294.7876, train_loss=8.574478

Batch 258890, train_perplexity=4495.3765, train_loss=8.410805

Batch 258900, train_perplexity=5104.394, train_loss=8.537857

Batch 258910, train_perplexity=5013.855, train_loss=8.51996

Batch 258920, train_perplexity=5905.3486, train_loss=8.683614

Batch 258930, train_perplexity=5183.076, train_loss=8.553154

Batch 258940, train_perplexity=5521.377, train_loss=8.616383

Batch 258950, train_perplexity=4857.89, train_loss=8.488359

Batch 258960, train_perplexity=5157.4463, train_loss=8.548197

Batch 258970, train_perplexity=5502.2017, train_loss=8.612904

Batch 258980, train_perplexity=5777.6855, train_loss=8.661758

Batch 258990, train_perplexity=5131.6494, train_loss=8.543182

Batch 259000, train_perplexity=4620.1562, train_loss=8.438184

Batch 259010, train_perplexity=5221.222, train_loss=8.560487

Batch 259020, train_perplexity=5317.1333, train_loss=8.57869

Batch 259030, train_perplexity=4800.973, train_loss=8.476574

Batch 259040, train_perplexity=5549.5513, train_loss=8.621472

Batch 259050, train_perplexity=5593.349, train_loss=8.6293335

Batch 259060, train_perplexity=5337.6436, train_loss=8.58254

Batch 259070, train_perplexity=4347.472, train_loss=8.37735

Batch 259080, train_perplexity=5368.5854, train_loss=8.58832

Batch 259090, train_perplexity=4238.465, train_loss=8.351956

Batch 259100, train_perplexity=5273.079, train_loss=8.57037

Batch 259110, train_perplexity=5814.1553, train_loss=8.668051

Batch 259120, train_perplexity=5744.621, train_loss=8.656019

Batch 259130, train_perplexity=5937.781, train_loss=8.689091

Batch 259140, train_perplexity=4951.2095, train_loss=8.507387

Batch 259150, train_perplexity=4171.7227, train_loss=8.336084

Batch 259160, train_perplexity=4371.4775, train_loss=8.382856

Batch 259170, train_perplexity=5730.941, train_loss=8.653635

Batch 259180, train_perplexity=5997.6924, train_loss=8.69913

Batch 259190, train_perplexity=5518.408, train_loss=8.615845

Batch 259200, train_perplexity=4507.9297, train_loss=8.413593

Batch 259210, train_perplexity=5274.1855, train_loss=8.57058

Batch 259220, train_perplexity=5594.779, train_loss=8.629589

Batch 259230, train_perplexity=4116.9795, train_loss=8.322875

Batch 259240, train_perplexity=4515.735, train_loss=8.415323

Batch 259250, train_perplexity=5591.8345, train_loss=8.629063

Batch 259260, train_perplexity=4397.9136, train_loss=8.3888855

Batch 259270, train_perplexity=4853.273, train_loss=8.487409

Batch 259280, train_perplexity=4233.766, train_loss=8.350847

Batch 259290, train_perplexity=5855.963, train_loss=8.675216

Batch 259300, train_perplexity=5098.867, train_loss=8.536774

Batch 259310, train_perplexity=4984.2256, train_loss=8.514033

Batch 259320, train_perplexity=5115.881, train_loss=8.540105

Batch 259330, train_perplexity=5478.53, train_loss=8.608592

Batch 259340, train_perplexity=4419.0103, train_loss=8.393671

Batch 259350, train_perplexity=6203.7676, train_loss=8.732912

Batch 259360, train_perplexity=5878.933, train_loss=8.679131

Batch 259370, train_perplexity=5788.771, train_loss=8.663675

Batch 259380, train_perplexity=4733.276, train_loss=8.462373

Batch 259390, train_perplexity=5888.247, train_loss=8.680714

Batch 259400, train_perplexity=6030.9077, train_loss=8.704653

Batch 259410, train_perplexity=6111.9224, train_loss=8.717997

Batch 259420, train_perplexity=4825.7104, train_loss=8.481713

Batch 259430, train_perplexity=5073.0537, train_loss=8.531698

Batch 259440, train_perplexity=5139.7354, train_loss=8.544757

Batch 259450, train_perplexity=4848.166, train_loss=8.486356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 259460, train_perplexity=5563.8115, train_loss=8.624039

Batch 259470, train_perplexity=5363.5806, train_loss=8.587387

Batch 259480, train_perplexity=4622.338, train_loss=8.438656

Batch 259490, train_perplexity=6237.6543, train_loss=8.738359

Batch 259500, train_perplexity=4861.913, train_loss=8.489187

Batch 259510, train_perplexity=4819.07, train_loss=8.480336

Batch 259520, train_perplexity=4739.0527, train_loss=8.463593

Batch 259530, train_perplexity=5207.522, train_loss=8.557859

Batch 259540, train_perplexity=5733.4883, train_loss=8.654079

Batch 259550, train_perplexity=5672.4443, train_loss=8.643375

Batch 259560, train_perplexity=5401.896, train_loss=8.594505

Batch 259570, train_perplexity=5373.739, train_loss=8.589279

Batch 259580, train_perplexity=4902.892, train_loss=8.497581

Batch 259590, train_perplexity=5078.64, train_loss=8.532799

Batch 259600, train_perplexity=6472.5894, train_loss=8.7753315

Batch 259610, train_perplexity=4567.8716, train_loss=8.426803

Batch 259620, train_perplexity=5233.0464, train_loss=8.562749

Batch 259630, train_perplexity=4520.5654, train_loss=8.416392

Batch 259640, train_perplexity=5252.6313, train_loss=8.566484

Batch 259650, train_perplexity=5464.4097, train_loss=8.606011

Batch 259660, train_perplexity=5001.9585, train_loss=8.517585

Batch 259670, train_perplexity=5242.3325, train_loss=8.564522

Batch 259680, train_perplexity=4940.842, train_loss=8.505291

Batch 259690, train_perplexity=4986.1655, train_loss=8.514422

Batch 259700, train_perplexity=5060.886, train_loss=8.529297

Batch 259710, train_perplexity=4706.588, train_loss=8.456718

Batch 259720, train_perplexity=5724.19, train_loss=8.652456

Batch 259730, train_perplexity=5109.245, train_loss=8.538807

Batch 259740, train_perplexity=5070.1807, train_loss=8.531132

Batch 259750, train_perplexity=4886.778, train_loss=8.494288

Batch 259760, train_perplexity=5189.8423, train_loss=8.554459

Batch 259770, train_perplexity=4466.936, train_loss=8.404458

Batch 259780, train_perplexity=5383.0947, train_loss=8.591019

Batch 259790, train_perplexity=5380.1997, train_loss=8.590481

Batch 259800, train_perplexity=5318.5786, train_loss=8.578961

Batch 259810, train_perplexity=5942.1084, train_loss=8.689819

Batch 259820, train_perplexity=5010.103, train_loss=8.519212

Batch 259830, train_perplexity=5612.6978, train_loss=8.632787

Batch 259840, train_perplexity=7084.3843, train_loss=8.865648

Batch 259850, train_perplexity=5133.6855, train_loss=8.543579

Batch 259860, train_perplexity=5250.9937, train_loss=8.566173

Batch 259870, train_perplexity=6653.818, train_loss=8.802946

Batch 259880, train_perplexity=5055.609, train_loss=8.528254

Batch 259890, train_perplexity=5207.969, train_loss=8.557945

Batch 259900, train_perplexity=5363.2637, train_loss=8.587328

Batch 259910, train_perplexity=5391.911, train_loss=8.592655

Batch 259920, train_perplexity=4979.6787, train_loss=8.513121

Batch 259930, train_perplexity=6152.9424, train_loss=8.724686

Batch 259940, train_perplexity=5707.945, train_loss=8.649614

Batch 259950, train_perplexity=5568.86, train_loss=8.624946

Batch 259960, train_perplexity=5706.508, train_loss=8.649363

Batch 259970, train_perplexity=4791.295, train_loss=8.474556

Batch 259980, train_perplexity=4503.3706, train_loss=8.412581

Batch 259990, train_perplexity=5428.637, train_loss=8.599443

Batch 260000, train_perplexity=5336.9106, train_loss=8.582402

Batch 260010, train_perplexity=5919.4844, train_loss=8.686005

Batch 260020, train_perplexity=5337.516, train_loss=8.582516

Batch 260030, train_perplexity=6261.309, train_loss=8.742145

Batch 260040, train_perplexity=5403.483, train_loss=8.594799

Batch 260050, train_perplexity=5960.4463, train_loss=8.692901

Batch 260060, train_perplexity=4410.5854, train_loss=8.391763

Batch 260070, train_perplexity=5247.1543, train_loss=8.565441

Batch 260080, train_perplexity=4792.4556, train_loss=8.474798

Batch 260090, train_perplexity=5388.6006, train_loss=8.592041

Batch 260100, train_perplexity=4330.589, train_loss=8.373459

Batch 260110, train_perplexity=5959.2983, train_loss=8.692708

Batch 260120, train_perplexity=4228.1494, train_loss=8.34952

Batch 260130, train_perplexity=5447.8784, train_loss=8.602982

Batch 260140, train_perplexity=5780.452, train_loss=8.662237

Batch 260150, train_perplexity=5222.273, train_loss=8.560688

Batch 260160, train_perplexity=4928.8643, train_loss=8.502864

Batch 260170, train_perplexity=5303.1055, train_loss=8.576048

Batch 260180, train_perplexity=5383.3203, train_loss=8.591061

Batch 260190, train_perplexity=4279.373, train_loss=8.361562

Batch 260200, train_perplexity=4732.134, train_loss=8.4621315

Batch 260210, train_perplexity=4765.716, train_loss=8.469203

Batch 260220, train_perplexity=5834.7627, train_loss=8.671589

Batch 260230, train_perplexity=6231.3994, train_loss=8.737356

Batch 260240, train_perplexity=6412.641, train_loss=8.7660265

Batch 260250, train_perplexity=5374.2104, train_loss=8.589367

Batch 260260, train_perplexity=5470.646, train_loss=8.607152

Batch 260270, train_perplexity=5494.8813, train_loss=8.611572

Batch 260280, train_perplexity=5562.4536, train_loss=8.623795

Batch 260290, train_perplexity=5242.6123, train_loss=8.564575

Batch 260300, train_perplexity=4816.6436, train_loss=8.479833

Batch 260310, train_perplexity=5000.16, train_loss=8.517225

Batch 260320, train_perplexity=4644.83, train_loss=8.44351

Batch 260330, train_perplexity=4519.6772, train_loss=8.416196

Batch 260340, train_perplexity=4969.365, train_loss=8.511047

Batch 260350, train_perplexity=5155.292, train_loss=8.547779

Batch 260360, train_perplexity=5021.138, train_loss=8.521412

Batch 260370, train_perplexity=5800.0, train_loss=8.665613

Batch 260380, train_perplexity=4584.53, train_loss=8.430443

Batch 260390, train_perplexity=4674.314, train_loss=8.449838

Batch 260400, train_perplexity=6403.1997, train_loss=8.764553

Batch 260410, train_perplexity=5469.5454, train_loss=8.606951

Batch 260420, train_perplexity=4996.4043, train_loss=8.516474

Batch 260430, train_perplexity=5293.0107, train_loss=8.574142

Batch 260440, train_perplexity=5123.3657, train_loss=8.541567

Batch 260450, train_perplexity=4954.1143, train_loss=8.507974

Batch 260460, train_perplexity=4564.44, train_loss=8.426051

Batch 260470, train_perplexity=6771.749, train_loss=8.820515

Batch 260480, train_perplexity=7875.9946, train_loss=8.971575

Batch 260490, train_perplexity=5016.9214, train_loss=8.520572

Batch 260500, train_perplexity=5318.0664, train_loss=8.578865

Batch 260510, train_perplexity=6191.7925, train_loss=8.73098

Batch 260520, train_perplexity=5187.8433, train_loss=8.554073

Batch 260530, train_perplexity=5240.9478, train_loss=8.564258

Batch 260540, train_perplexity=5289.5083, train_loss=8.573481

Batch 260550, train_perplexity=5109.3813, train_loss=8.538834

Batch 260560, train_perplexity=4843.337, train_loss=8.485359

Batch 260570, train_perplexity=4818.725, train_loss=8.480265

Batch 260580, train_perplexity=4331.6465, train_loss=8.373703

Batch 260590, train_perplexity=5070.9736, train_loss=8.531288

Batch 260600, train_perplexity=5602.489, train_loss=8.630966

Batch 260610, train_perplexity=5307.1074, train_loss=8.576802

Batch 260620, train_perplexity=4328.372, train_loss=8.372947

Batch 260630, train_perplexity=4657.3164, train_loss=8.446195

Batch 260640, train_perplexity=5698.753, train_loss=8.648003

Batch 260650, train_perplexity=5760.4316, train_loss=8.658768

Batch 260660, train_perplexity=5254.846, train_loss=8.566906

Batch 260670, train_perplexity=4958.931, train_loss=8.508945

Batch 260680, train_perplexity=5796.6377, train_loss=8.665033

Batch 260690, train_perplexity=5642.4893, train_loss=8.638081

Batch 260700, train_perplexity=4825.5356, train_loss=8.481677

Batch 260710, train_perplexity=6075.172, train_loss=8.711966

Batch 260720, train_perplexity=5728.2637, train_loss=8.653168

Batch 260730, train_perplexity=5900.6255, train_loss=8.682814

Batch 260740, train_perplexity=4998.611, train_loss=8.516915

Batch 260750, train_perplexity=4264.157, train_loss=8.358

Batch 260760, train_perplexity=5187.952, train_loss=8.554094

Batch 260770, train_perplexity=5480.6147, train_loss=8.608973
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 260780, train_perplexity=6282.7227, train_loss=8.745559

Batch 260790, train_perplexity=5287.4307, train_loss=8.573088

Batch 260800, train_perplexity=5242.2876, train_loss=8.564513

Batch 260810, train_perplexity=5175.445, train_loss=8.551681

Batch 260820, train_perplexity=5239.0137, train_loss=8.563889

Batch 260830, train_perplexity=5159.8027, train_loss=8.548654

Batch 260840, train_perplexity=5411.11, train_loss=8.59621

Batch 260850, train_perplexity=5535.676, train_loss=8.618969

Batch 260860, train_perplexity=5717.937, train_loss=8.651363

Batch 260870, train_perplexity=5824.022, train_loss=8.669746

Batch 260880, train_perplexity=5162.3867, train_loss=8.549154

Batch 260890, train_perplexity=4750.247, train_loss=8.465952

Batch 260900, train_perplexity=5864.2676, train_loss=8.676633

Batch 260910, train_perplexity=5490.4604, train_loss=8.610767

Batch 260920, train_perplexity=4916.635, train_loss=8.50038

Batch 260930, train_perplexity=5956.054, train_loss=8.692163

Batch 260940, train_perplexity=5111.8623, train_loss=8.539319

Batch 260950, train_perplexity=5788.291, train_loss=8.663592

Batch 260960, train_perplexity=6629.2554, train_loss=8.799248

Batch 260970, train_perplexity=5048.921, train_loss=8.52693

Batch 260980, train_perplexity=5174.8228, train_loss=8.55156

Batch 260990, train_perplexity=4956.9736, train_loss=8.508551

Batch 261000, train_perplexity=5515.472, train_loss=8.615313

Batch 261010, train_perplexity=5171.3843, train_loss=8.550896

Batch 261020, train_perplexity=6184.8286, train_loss=8.729855

Batch 261030, train_perplexity=5433.755, train_loss=8.600386

Batch 261040, train_perplexity=5558.073, train_loss=8.623007

Batch 261050, train_perplexity=4652.89, train_loss=8.445244

Batch 261060, train_perplexity=4332.167, train_loss=8.373823

Batch 261070, train_perplexity=5825.244, train_loss=8.669956

Batch 261080, train_perplexity=5384.6655, train_loss=8.5913105

Batch 261090, train_perplexity=5812.8467, train_loss=8.667826

Batch 261100, train_perplexity=4976.294, train_loss=8.512441

Batch 261110, train_perplexity=4766.5977, train_loss=8.469388

Batch 261120, train_perplexity=5617.276, train_loss=8.633602

Batch 261130, train_perplexity=5590.9277, train_loss=8.628901

Batch 261140, train_perplexity=6593.732, train_loss=8.793875

Batch 261150, train_perplexity=5184.9546, train_loss=8.553516

Batch 261160, train_perplexity=5338.305, train_loss=8.582664

Batch 261170, train_perplexity=5008.5503, train_loss=8.518902

Batch 261180, train_perplexity=5290.457, train_loss=8.57366

Batch 261190, train_perplexity=5323.8105, train_loss=8.579945

Batch 261200, train_perplexity=4890.046, train_loss=8.494957

Batch 261210, train_perplexity=4639.057, train_loss=8.442266

Batch 261220, train_perplexity=5637.4976, train_loss=8.637196

Batch 261230, train_perplexity=6465.5684, train_loss=8.774246

Batch 261240, train_perplexity=4726.3564, train_loss=8.46091

Batch 261250, train_perplexity=5917.8022, train_loss=8.68572

Batch 261260, train_perplexity=4974.382, train_loss=8.512056

Batch 261270, train_perplexity=4457.8716, train_loss=8.402427

Batch 261280, train_perplexity=5038.4595, train_loss=8.524856

Batch 261290, train_perplexity=4613.512, train_loss=8.436745

Batch 261300, train_perplexity=5603.3438, train_loss=8.631119

Batch 261310, train_perplexity=6119.878, train_loss=8.719297

Batch 261320, train_perplexity=6814.0254, train_loss=8.826738

Batch 261330, train_perplexity=5540.8574, train_loss=8.6199045

Batch 261340, train_perplexity=4974.733, train_loss=8.512127

Batch 261350, train_perplexity=5638.9497, train_loss=8.637453

Batch 261360, train_perplexity=5383.8237, train_loss=8.591154

Batch 261370, train_perplexity=5856.2144, train_loss=8.675259

Batch 261380, train_perplexity=5859.8286, train_loss=8.675876

Batch 261390, train_perplexity=4926.6885, train_loss=8.502422

Batch 261400, train_perplexity=4896.8315, train_loss=8.496344

Batch 261410, train_perplexity=5905.422, train_loss=8.683626

Batch 261420, train_perplexity=5959.8154, train_loss=8.692795

Batch 261430, train_perplexity=5263.1714, train_loss=8.568489

Batch 261440, train_perplexity=6680.663, train_loss=8.8069725

Batch 261450, train_perplexity=5053.9023, train_loss=8.527916

Batch 261460, train_perplexity=5524.8955, train_loss=8.61702

Batch 261470, train_perplexity=6043.2227, train_loss=8.706693

Batch 261480, train_perplexity=5802.3677, train_loss=8.666021

Batch 261490, train_perplexity=5343.934, train_loss=8.583717

Batch 261500, train_perplexity=5753.931, train_loss=8.657639

Batch 261510, train_perplexity=5117.969, train_loss=8.540513

Batch 261520, train_perplexity=5457.676, train_loss=8.604778

Batch 261530, train_perplexity=6930.7695, train_loss=8.843726

Batch 261540, train_perplexity=5065.5166, train_loss=8.530211

Batch 261550, train_perplexity=4755.8545, train_loss=8.467132

Batch 261560, train_perplexity=6934.869, train_loss=8.844317

Batch 261570, train_perplexity=5647.69, train_loss=8.639002

Batch 261580, train_perplexity=5815.386, train_loss=8.6682625

Batch 261590, train_perplexity=4634.989, train_loss=8.441389

Batch 261600, train_perplexity=5036.139, train_loss=8.524395

Batch 261610, train_perplexity=5131.5073, train_loss=8.543155

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Batch 261620, train_perplexity=5681.4204, train_loss=8.644957

Batch 261630, train_perplexity=5377.9736, train_loss=8.590067

Batch 261640, train_perplexity=4797.55, train_loss=8.475861

Batch 261650, train_perplexity=4596.9106, train_loss=8.43314

Batch 261660, train_perplexity=4480.3413, train_loss=8.4074545

Batch 261670, train_perplexity=4578.8584, train_loss=8.429205

Batch 261680, train_perplexity=4733.1855, train_loss=8.462354

Batch 261690, train_perplexity=5124.46, train_loss=8.54178

Batch 261700, train_perplexity=5619.918, train_loss=8.634072

Batch 261710, train_perplexity=4133.2227, train_loss=8.326813

Batch 261720, train_perplexity=5464.8633, train_loss=8.606094

Batch 261730, train_perplexity=6840.4927, train_loss=8.830615

Batch 261740, train_perplexity=5651.984, train_loss=8.639762

Batch 261750, train_perplexity=4408.378, train_loss=8.391262

Batch 261760, train_perplexity=7012.0327, train_loss=8.855383

Batch 261770, train_perplexity=4484.599, train_loss=8.408404

Batch 261780, train_perplexity=7037.7183, train_loss=8.859039

Batch 261790, train_perplexity=4742.4526, train_loss=8.46431

Batch 261800, train_perplexity=5886.1587, train_loss=8.680359

Batch 261810, train_perplexity=5913.966, train_loss=8.685072

Batch 261820, train_perplexity=7768.1587, train_loss=8.957788

Batch 261830, train_perplexity=5165.8047, train_loss=8.549816

Batch 261840, train_perplexity=4872.0127, train_loss=8.491262

Batch 261850, train_perplexity=5390.137, train_loss=8.592326

Batch 261860, train_perplexity=5325.3745, train_loss=8.580238

Batch 261870, train_perplexity=5801.9917, train_loss=8.6659565

Batch 261880, train_perplexity=5279.6455, train_loss=8.571614

Batch 261890, train_perplexity=5102.5156, train_loss=8.537489

Batch 261900, train_perplexity=5555.1694, train_loss=8.622484

Batch 261910, train_perplexity=4874.6245, train_loss=8.491798

Batch 261920, train_perplexity=4883.093, train_loss=8.493534

Batch 261930, train_perplexity=6679.4653, train_loss=8.806793

Batch 261940, train_perplexity=5415.71, train_loss=8.597059

Batch 261950, train_perplexity=5616.199, train_loss=8.63341

Batch 261960, train_perplexity=5684.889, train_loss=8.645567

Batch 261970, train_perplexity=5385.544, train_loss=8.591474

Batch 261980, train_perplexity=5201.2236, train_loss=8.556649

Batch 261990, train_perplexity=5155.366, train_loss=8.547793

Batch 262000, train_perplexity=5554.232, train_loss=8.622315

Batch 262010, train_perplexity=5925.8896, train_loss=8.687086

Batch 262020, train_perplexity=5709.3657, train_loss=8.649863
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 262030, train_perplexity=5755.2373, train_loss=8.657866

Batch 262040, train_perplexity=5020.2476, train_loss=8.5212345

Batch 262050, train_perplexity=5881.984, train_loss=8.679649

Batch 262060, train_perplexity=6266.261, train_loss=8.742935

Batch 262070, train_perplexity=4588.637, train_loss=8.431338

Batch 262080, train_perplexity=4862.4414, train_loss=8.489296

Batch 262090, train_perplexity=5129.1245, train_loss=8.54269

Batch 262100, train_perplexity=5660.415, train_loss=8.6412525

Batch 262110, train_perplexity=6054.9043, train_loss=8.708624

Batch 262120, train_perplexity=5888.64, train_loss=8.68078

Batch 262130, train_perplexity=5763.4595, train_loss=8.659293

Batch 262140, train_perplexity=5775.889, train_loss=8.661448

Batch 262150, train_perplexity=5557.999, train_loss=8.622993

Batch 262160, train_perplexity=6931.173, train_loss=8.843784

Batch 262170, train_perplexity=6477.6714, train_loss=8.776116

Batch 262180, train_perplexity=6390.73, train_loss=8.762604

Batch 262190, train_perplexity=5753.635, train_loss=8.657587

Batch 262200, train_perplexity=5557.4585, train_loss=8.622896

Batch 262210, train_perplexity=5839.133, train_loss=8.672338

Batch 262220, train_perplexity=5027.06, train_loss=8.522591

Batch 262230, train_perplexity=5473.683, train_loss=8.607707

Batch 262240, train_perplexity=6406.4673, train_loss=8.765063

Batch 262250, train_perplexity=6031.8623, train_loss=8.704811

Batch 262260, train_perplexity=5691.8984, train_loss=8.646799

Batch 262270, train_perplexity=4912.29, train_loss=8.4994955

Batch 262280, train_perplexity=5999.3457, train_loss=8.699406

Batch 262290, train_perplexity=4910.079, train_loss=8.499045

Batch 262300, train_perplexity=5794.0513, train_loss=8.664587

Batch 262310, train_perplexity=5269.731, train_loss=8.569735

Batch 262320, train_perplexity=4238.9053, train_loss=8.35206

Batch 262330, train_perplexity=5601.1636, train_loss=8.63073

Batch 262340, train_perplexity=4945.782, train_loss=8.50629

Batch 262350, train_perplexity=4777.6885, train_loss=8.471712

Batch 262360, train_perplexity=4891.0674, train_loss=8.495166

Batch 262370, train_perplexity=5287.748, train_loss=8.573148

Batch 262380, train_perplexity=5647.264, train_loss=8.6389265

Batch 262390, train_perplexity=5103.4497, train_loss=8.537672

Batch 262400, train_perplexity=6489.6235, train_loss=8.77796

Batch 262410, train_perplexity=5203.5806, train_loss=8.557102

Batch 262420, train_perplexity=6128.0425, train_loss=8.720631

Batch 262430, train_perplexity=5600.5283, train_loss=8.630616

Batch 262440, train_perplexity=5234.2695, train_loss=8.562983

Batch 262450, train_perplexity=5151.444, train_loss=8.547032

Batch 262460, train_perplexity=5551.1235, train_loss=8.621756

Batch 262470, train_perplexity=6953.989, train_loss=8.847071

Batch 262480, train_perplexity=5433.1484, train_loss=8.600274

Batch 262490, train_perplexity=5890.471, train_loss=8.681091

Batch 262500, train_perplexity=4632.9956, train_loss=8.440959

Batch 262510, train_perplexity=4917.249, train_loss=8.5005045

Batch 262520, train_perplexity=4932.424, train_loss=8.503586

Batch 262530, train_perplexity=5299.4556, train_loss=8.575359

Batch 262540, train_perplexity=5753.2505, train_loss=8.65752

Batch 262550, train_perplexity=4262.604, train_loss=8.3576355

Batch 262560, train_perplexity=6524.3438, train_loss=8.783296

Batch 262570, train_perplexity=5545.25, train_loss=8.620697

Batch 262580, train_perplexity=5593.5996, train_loss=8.629378

Batch 262590, train_perplexity=5271.7515, train_loss=8.570118

Batch 262600, train_perplexity=5846.2036, train_loss=8.673548

Batch 262610, train_perplexity=5447.9307, train_loss=8.602991

Batch 262620, train_perplexity=4411.187, train_loss=8.391899

Batch 262630, train_perplexity=5732.0234, train_loss=8.653824

Batch 262640, train_perplexity=5479.737, train_loss=8.608812

Batch 262650, train_perplexity=5509.4478, train_loss=8.61422

Batch 262660, train_perplexity=5838.0747, train_loss=8.672156

Batch 262670, train_perplexity=6075.6816, train_loss=8.7120495

Batch 262680, train_perplexity=5338.687, train_loss=8.582735

Batch 262690, train_perplexity=5249.717, train_loss=8.565929

Batch 262700, train_perplexity=5233.356, train_loss=8.562808

Batch 262710, train_perplexity=5731.012, train_loss=8.653647

Batch 262720, train_perplexity=5955.054, train_loss=8.691996

Batch 262730, train_perplexity=4533.7637, train_loss=8.419308

Batch 262740, train_perplexity=5141.422, train_loss=8.545085

Batch 262750, train_perplexity=5096.8643, train_loss=8.536381

Batch 262760, train_perplexity=5413.4067, train_loss=8.596634

Batch 262770, train_perplexity=6191.013, train_loss=8.730854

Batch 262780, train_perplexity=5681.8105, train_loss=8.645025

Batch 262790, train_perplexity=6094.8154, train_loss=8.715194

Batch 262800, train_perplexity=4825.0156, train_loss=8.481569

Batch 262810, train_perplexity=5505.9077, train_loss=8.613577

Batch 262820, train_perplexity=4621.2754, train_loss=8.438426

Batch 262830, train_perplexity=6372.411, train_loss=8.759733

Batch 262840, train_perplexity=4387.323, train_loss=8.386475

Batch 262850, train_perplexity=4847.625, train_loss=8.486244

Batch 262860, train_perplexity=5857.2476, train_loss=8.675435

Batch 262870, train_perplexity=5503.713, train_loss=8.613178

Batch 262880, train_perplexity=4369.4688, train_loss=8.382397

Batch 262890, train_perplexity=4964.775, train_loss=8.510123

Batch 262900, train_perplexity=5800.73, train_loss=8.665739

Batch 262910, train_perplexity=5797.611, train_loss=8.665201

Batch 262920, train_perplexity=5579.6514, train_loss=8.626882

Batch 262930, train_perplexity=5663.471, train_loss=8.641792

Batch 262940, train_perplexity=5005.7046, train_loss=8.518333

Batch 262950, train_perplexity=5991.5864, train_loss=8.698112

Batch 262960, train_perplexity=5426.8257, train_loss=8.59911

Batch 262970, train_perplexity=5133.01, train_loss=8.5434475

Batch 262980, train_perplexity=4881.5376, train_loss=8.493216

Batch 262990, train_perplexity=6296.8613, train_loss=8.747807

Batch 263000, train_perplexity=5254.455, train_loss=8.566832

Batch 263010, train_perplexity=6235.168, train_loss=8.737961

Batch 263020, train_perplexity=4501.1294, train_loss=8.412084

Batch 263030, train_perplexity=5074.404, train_loss=8.531964

Batch 263040, train_perplexity=5910.346, train_loss=8.68446

Batch 263050, train_perplexity=5374.605, train_loss=8.58944

Batch 263060, train_perplexity=5553.75, train_loss=8.622229

Batch 263070, train_perplexity=5479.308, train_loss=8.608734

Batch 263080, train_perplexity=5305.4023, train_loss=8.576481

Batch 263090, train_perplexity=5431.9727, train_loss=8.600058

Batch 263100, train_perplexity=5336.188, train_loss=8.582267

Batch 263110, train_perplexity=5117.984, train_loss=8.540516

Batch 263120, train_perplexity=5143.8447, train_loss=8.545556

Batch 263130, train_perplexity=5626.9272, train_loss=8.635319

Batch 263140, train_perplexity=5522.757, train_loss=8.616632

Batch 263150, train_perplexity=5152.471, train_loss=8.547232

Batch 263160, train_perplexity=4892.35, train_loss=8.495428

Batch 263170, train_perplexity=6906.8906, train_loss=8.840275

Batch 263180, train_perplexity=5495.8667, train_loss=8.611752

Batch 263190, train_perplexity=5199.6514, train_loss=8.556347

Batch 263200, train_perplexity=5083.7183, train_loss=8.533798

Batch 263210, train_perplexity=4839.698, train_loss=8.484608

Batch 263220, train_perplexity=5248.5356, train_loss=8.565704

Batch 263230, train_perplexity=5958.2812, train_loss=8.692537

Batch 263240, train_perplexity=6225.578, train_loss=8.736422

Batch 263250, train_perplexity=5835.197, train_loss=8.671663

Batch 263260, train_perplexity=5240.073, train_loss=8.564091

Batch 263270, train_perplexity=4406.146, train_loss=8.390756

Batch 263280, train_perplexity=5653.6606, train_loss=8.6400585

Batch 263290, train_perplexity=5481.7227, train_loss=8.609175

Batch 263300, train_perplexity=6442.8066, train_loss=8.77072

Batch 263310, train_perplexity=6224.7944, train_loss=8.736296

Batch 263320, train_perplexity=6271.618, train_loss=8.74379

Batch 263330, train_perplexity=5630.663, train_loss=8.6359825

Batch 263340, train_perplexity=4732.757, train_loss=8.462263
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 263350, train_perplexity=5646.4404, train_loss=8.638781

Batch 263360, train_perplexity=4608.987, train_loss=8.435763

Batch 263370, train_perplexity=4784.2314, train_loss=8.473081

Batch 263380, train_perplexity=6145.2134, train_loss=8.723429

Batch 263390, train_perplexity=5688.002, train_loss=8.646114

Batch 263400, train_perplexity=4908.7354, train_loss=8.498772

Batch 263410, train_perplexity=5096.971, train_loss=8.536402

Batch 263420, train_perplexity=5749.236, train_loss=8.656822

Batch 263430, train_perplexity=5081.3237, train_loss=8.533327

Batch 263440, train_perplexity=5373.462, train_loss=8.589228

Batch 263450, train_perplexity=6033.1167, train_loss=8.705019

Batch 263460, train_perplexity=5748.2324, train_loss=8.656648

Batch 263470, train_perplexity=5107.355, train_loss=8.538437

Batch 263480, train_perplexity=5061.1904, train_loss=8.529357

Batch 263490, train_perplexity=4951.7383, train_loss=8.507494

Batch 263500, train_perplexity=5601.345, train_loss=8.630762

Batch 263510, train_perplexity=6030.896, train_loss=8.704651

Batch 263520, train_perplexity=5404.7305, train_loss=8.59503

Batch 263530, train_perplexity=4868.1855, train_loss=8.490477

Batch 263540, train_perplexity=4737.5522, train_loss=8.463276

Batch 263550, train_perplexity=5214.554, train_loss=8.559209

Batch 263560, train_perplexity=6574.511, train_loss=8.790956

Batch 263570, train_perplexity=5643.8833, train_loss=8.638328

Batch 263580, train_perplexity=5860.427, train_loss=8.675978

Batch 263590, train_perplexity=5879.6953, train_loss=8.67926

Batch 263600, train_perplexity=5557.9146, train_loss=8.622978

Batch 263610, train_perplexity=6068.9644, train_loss=8.710943

Batch 263620, train_perplexity=4566.1685, train_loss=8.42643

Batch 263630, train_perplexity=5481.467, train_loss=8.609128

Batch 263640, train_perplexity=4717.6333, train_loss=8.459063

Batch 263650, train_perplexity=5431.175, train_loss=8.599911

Batch 263660, train_perplexity=4793.1685, train_loss=8.474947

Batch 263670, train_perplexity=4716.3423, train_loss=8.458789

Batch 263680, train_perplexity=4812.7544, train_loss=8.479025

Batch 263690, train_perplexity=5331.6304, train_loss=8.581412

Batch 263700, train_perplexity=5856.4487, train_loss=8.675299

Batch 263710, train_perplexity=5241.7227, train_loss=8.564405

Batch 263720, train_perplexity=4977.3145, train_loss=8.512646

Batch 263730, train_perplexity=5029.669, train_loss=8.523109

Batch 263740, train_perplexity=6353.539, train_loss=8.756767

Batch 263750, train_perplexity=5266.0283, train_loss=8.569032

Batch 263760, train_perplexity=4998.1772, train_loss=8.516829

Batch 263770, train_perplexity=6144.897, train_loss=8.723377

Batch 263780, train_perplexity=5220.749, train_loss=8.560396

Batch 263790, train_perplexity=5523.031, train_loss=8.616682

Batch 263800, train_perplexity=5409.093, train_loss=8.595837

Batch 263810, train_perplexity=5744.884, train_loss=8.656065

Batch 263820, train_perplexity=5120.899, train_loss=8.541085

Batch 263830, train_perplexity=5256.219, train_loss=8.567167

Batch 263840, train_perplexity=5780.2427, train_loss=8.662201

Batch 263850, train_perplexity=5404.38, train_loss=8.594965

Batch 263860, train_perplexity=4986.822, train_loss=8.514554

Batch 263870, train_perplexity=5182.2456, train_loss=8.552994

Batch 263880, train_perplexity=5707.172, train_loss=8.649479

Batch 263890, train_perplexity=6139.9995, train_loss=8.72258

Batch 263900, train_perplexity=5183.353, train_loss=8.553207

Batch 263910, train_perplexity=5381.3594, train_loss=8.590696

Batch 263920, train_perplexity=4373.4707, train_loss=8.383312

Batch 263930, train_perplexity=4598.586, train_loss=8.433504

Batch 263940, train_perplexity=5452.806, train_loss=8.603886

Batch 263950, train_perplexity=4675.473, train_loss=8.450086

Batch 263960, train_perplexity=5824.8223, train_loss=8.669884

Batch 263970, train_perplexity=5939.112, train_loss=8.689315

Batch 263980, train_perplexity=5494.3047, train_loss=8.611467

Batch 263990, train_perplexity=7500.4336, train_loss=8.922716

Batch 264000, train_perplexity=5397.828, train_loss=8.593752

Batch 264010, train_perplexity=5413.3037, train_loss=8.596615

Batch 264020, train_perplexity=6482.448, train_loss=8.776854

Batch 264030, train_perplexity=6066.025, train_loss=8.710459

Batch 264040, train_perplexity=5058.218, train_loss=8.5287695

Batch 264050, train_perplexity=5113.876, train_loss=8.539713

Batch 264060, train_perplexity=4988.9004, train_loss=8.514971

Batch 264070, train_perplexity=6720.73, train_loss=8.812952

Batch 264080, train_perplexity=4839.9614, train_loss=8.484662

Batch 264090, train_perplexity=5462.852, train_loss=8.605726

Batch 264100, train_perplexity=6510.2656, train_loss=8.781136

Batch 264110, train_perplexity=4511.336, train_loss=8.414349

Batch 264120, train_perplexity=5616.601, train_loss=8.633482

Batch 264130, train_perplexity=4939.942, train_loss=8.505109

Batch 264140, train_perplexity=5004.511, train_loss=8.518095

Batch 264150, train_perplexity=6152.966, train_loss=8.7246895

Batch 264160, train_perplexity=6049.3403, train_loss=8.707705

Batch 264170, train_perplexity=5709.11, train_loss=8.649818

Batch 264180, train_perplexity=6221.655, train_loss=8.735791

Batch 264190, train_perplexity=6095.4434, train_loss=8.715297

Batch 264200, train_perplexity=5785.4155, train_loss=8.663095

Batch 264210, train_perplexity=5294.8584, train_loss=8.5744915

Batch 264220, train_perplexity=5946.939, train_loss=8.690632

Batch 264230, train_perplexity=5203.923, train_loss=8.557168

Batch 264240, train_perplexity=5204.484, train_loss=8.557276

Batch 264250, train_perplexity=4958.0137, train_loss=8.50876

Batch 264260, train_perplexity=4793.379, train_loss=8.474991

Batch 264270, train_perplexity=7845.05, train_loss=8.967638

Batch 264280, train_perplexity=6699.2544, train_loss=8.8097515

Batch 264290, train_perplexity=5231.5747, train_loss=8.562468

Batch 264300, train_perplexity=5173.1206, train_loss=8.551231

Batch 264310, train_perplexity=6704.022, train_loss=8.810463

Batch 264320, train_perplexity=5190.184, train_loss=8.554524

Batch 264330, train_perplexity=4672.161, train_loss=8.449377

Batch 264340, train_perplexity=5033.8726, train_loss=8.523945

Batch 264350, train_perplexity=4943.0806, train_loss=8.505744

Batch 264360, train_perplexity=5015.687, train_loss=8.520326

Batch 264370, train_perplexity=5290.179, train_loss=8.573607

Batch 264380, train_perplexity=4921.8794, train_loss=8.501446

Batch 264390, train_perplexity=6665.0083, train_loss=8.804626

Batch 264400, train_perplexity=5476.973, train_loss=8.608308

Batch 264410, train_perplexity=5258.5054, train_loss=8.567602

Batch 264420, train_perplexity=5556.4517, train_loss=8.622715

Batch 264430, train_perplexity=4444.7417, train_loss=8.399477

Batch 264440, train_perplexity=4918.576, train_loss=8.500774

Batch 264450, train_perplexity=6421.509, train_loss=8.767408

Batch 264460, train_perplexity=5494.619, train_loss=8.611525

Batch 264470, train_perplexity=5058.044, train_loss=8.528735

Batch 264480, train_perplexity=4523.407, train_loss=8.417021

Batch 264490, train_perplexity=4673.993, train_loss=8.449769

Batch 264500, train_perplexity=4965.073, train_loss=8.510183

Batch 264510, train_perplexity=4756.267, train_loss=8.467218

Batch 264520, train_perplexity=4650.0156, train_loss=8.444626

Batch 264530, train_perplexity=5173.7275, train_loss=8.551349

Batch 264540, train_perplexity=5394.8843, train_loss=8.593206

Batch 264550, train_perplexity=5702.1885, train_loss=8.648605

Batch 264560, train_perplexity=5141.819, train_loss=8.545162

Batch 264570, train_perplexity=5730.318, train_loss=8.653526

Batch 264580, train_perplexity=4942.793, train_loss=8.505686

Batch 264590, train_perplexity=6298.753, train_loss=8.748107

Batch 264600, train_perplexity=6592.9146, train_loss=8.793751

Batch 264610, train_perplexity=5713.8325, train_loss=8.650645

Batch 264620, train_perplexity=6266.0103, train_loss=8.742895

Batch 264630, train_perplexity=5360.083, train_loss=8.586735

Batch 264640, train_perplexity=5975.9907, train_loss=8.695505

Batch 264650, train_perplexity=6669.65, train_loss=8.805323

Batch 264660, train_perplexity=5252.1104, train_loss=8.566385
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 264670, train_perplexity=4960.2314, train_loss=8.509208

Batch 264680, train_perplexity=7326.0854, train_loss=8.899197

Batch 264690, train_perplexity=5227.0913, train_loss=8.56161

Batch 264700, train_perplexity=6613.5757, train_loss=8.79688

Batch 264710, train_perplexity=5589.0244, train_loss=8.62856

Batch 264720, train_perplexity=6455.624, train_loss=8.772707

Batch 264730, train_perplexity=5659.843, train_loss=8.641151

Batch 264740, train_perplexity=5890.7188, train_loss=8.681133

Batch 264750, train_perplexity=4743.656, train_loss=8.464563

Batch 264760, train_perplexity=4653.2896, train_loss=8.44533

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Batch 264770, train_perplexity=5504.0176, train_loss=8.613234

Batch 264780, train_perplexity=6169.5005, train_loss=8.727373

Batch 264790, train_perplexity=4936.2217, train_loss=8.504355

Batch 264800, train_perplexity=5826.9834, train_loss=8.670255

Batch 264810, train_perplexity=4994.218, train_loss=8.516036

Batch 264820, train_perplexity=4914.178, train_loss=8.49988

Batch 264830, train_perplexity=5093.1665, train_loss=8.535655

Batch 264840, train_perplexity=3982.5576, train_loss=8.28968

Batch 264850, train_perplexity=6191.202, train_loss=8.730885

Batch 264860, train_perplexity=5097.1753, train_loss=8.536442

Batch 264870, train_perplexity=4848.0044, train_loss=8.486322

Batch 264880, train_perplexity=4761.364, train_loss=8.468289

Batch 264890, train_perplexity=6176.8477, train_loss=8.728563

Batch 264900, train_perplexity=6563.905, train_loss=8.789341

Batch 264910, train_perplexity=4654.8164, train_loss=8.445658

Batch 264920, train_perplexity=4620.46, train_loss=8.43825

Batch 264930, train_perplexity=5923.4316, train_loss=8.686671

Batch 264940, train_perplexity=4962.3657, train_loss=8.509638

Batch 264950, train_perplexity=5271.329, train_loss=8.570038

Batch 264960, train_perplexity=4262.5713, train_loss=8.357628

Batch 264970, train_perplexity=5326.0503, train_loss=8.580365

Batch 264980, train_perplexity=4299.856, train_loss=8.366337

Batch 264990, train_perplexity=4278.4834, train_loss=8.361354

Batch 265000, train_perplexity=4320.136, train_loss=8.371042

Batch 265010, train_perplexity=5788.9478, train_loss=8.663706

Batch 265020, train_perplexity=5486.456, train_loss=8.610038

Batch 265030, train_perplexity=4878.2896, train_loss=8.49255

Batch 265040, train_perplexity=4523.1094, train_loss=8.416955

Batch 265050, train_perplexity=5134.4346, train_loss=8.543725

Batch 265060, train_perplexity=6031.333, train_loss=8.704723

Batch 265070, train_perplexity=5083.2866, train_loss=8.533713

Batch 265080, train_perplexity=5413.701, train_loss=8.596688

Batch 265090, train_perplexity=4879.769, train_loss=8.492853

Batch 265100, train_perplexity=5461.971, train_loss=8.605565

Batch 265110, train_perplexity=5763.487, train_loss=8.659298

Batch 265120, train_perplexity=5272.506, train_loss=8.570261

Batch 265130, train_perplexity=6383.828, train_loss=8.761523

Batch 265140, train_perplexity=5410.3774, train_loss=8.596074

Batch 265150, train_perplexity=5012.383, train_loss=8.519667

Batch 265160, train_perplexity=5738.215, train_loss=8.654903

Batch 265170, train_perplexity=6318.7095, train_loss=8.75127

Batch 265180, train_perplexity=5460.8516, train_loss=8.60536

Batch 265190, train_perplexity=5470.6514, train_loss=8.607153

Batch 265200, train_perplexity=5326.9697, train_loss=8.580538

Batch 265210, train_perplexity=5131.1206, train_loss=8.543079

Batch 265220, train_perplexity=5525.18, train_loss=8.617071

Batch 265230, train_perplexity=5794.9463, train_loss=8.6647415

Batch 265240, train_perplexity=5286.029, train_loss=8.572823

Batch 265250, train_perplexity=5600.213, train_loss=8.63056

Batch 265260, train_perplexity=5168.0615, train_loss=8.550253

Batch 265270, train_perplexity=4519.061, train_loss=8.4160595

Batch 265280, train_perplexity=5254.996, train_loss=8.566935

Batch 265290, train_perplexity=5670.8755, train_loss=8.643099

Batch 265300, train_perplexity=5686.2666, train_loss=8.645809

Batch 265310, train_perplexity=5690.617, train_loss=8.646574

Batch 265320, train_perplexity=5985.8125, train_loss=8.697147

Batch 265330, train_perplexity=5624.8506, train_loss=8.63495

Batch 265340, train_perplexity=5886.8213, train_loss=8.680471

Batch 265350, train_perplexity=6614.4844, train_loss=8.797017

Batch 265360, train_perplexity=5951.682, train_loss=8.691429

Batch 265370, train_perplexity=4971.342, train_loss=8.511445

Batch 265380, train_perplexity=6175.198, train_loss=8.728296

Batch 265390, train_perplexity=4874.75, train_loss=8.491824

Batch 265400, train_perplexity=5347.258, train_loss=8.584339

Batch 265410, train_perplexity=4657.969, train_loss=8.446335

Batch 265420, train_perplexity=5186.7896, train_loss=8.55387

Batch 265430, train_perplexity=4459.032, train_loss=8.402687

Batch 265440, train_perplexity=6004.806, train_loss=8.700315

Batch 265450, train_perplexity=5161.9287, train_loss=8.549066

Batch 265460, train_perplexity=4379.9946, train_loss=8.384803

Batch 265470, train_perplexity=5191.476, train_loss=8.554773

Batch 265480, train_perplexity=5667.459, train_loss=8.642496

Batch 265490, train_perplexity=5083.8203, train_loss=8.533818

Batch 265500, train_perplexity=4852.2227, train_loss=8.487192

Batch 265510, train_perplexity=5024.793, train_loss=8.52214

Batch 265520, train_perplexity=7257.4087, train_loss=8.889778

Batch 265530, train_perplexity=4767.3794, train_loss=8.469552

Batch 265540, train_perplexity=6477.7393, train_loss=8.776127

Batch 265550, train_perplexity=5269.0073, train_loss=8.569597

Batch 265560, train_perplexity=5389.778, train_loss=8.592259

Batch 265570, train_perplexity=4582.4663, train_loss=8.429993

Batch 265580, train_perplexity=4580.0244, train_loss=8.42946

Batch 265590, train_perplexity=5842.7925, train_loss=8.672964

Batch 265600, train_perplexity=4615.5366, train_loss=8.437183

Batch 265610, train_perplexity=5303.6113, train_loss=8.576143

Batch 265620, train_perplexity=4923.6636, train_loss=8.501808

Batch 265630, train_perplexity=5067.0195, train_loss=8.530508

Batch 265640, train_perplexity=5871.559, train_loss=8.6778755

Batch 265650, train_perplexity=5696.0903, train_loss=8.647535

Batch 265660, train_perplexity=7144.13, train_loss=8.874046

Batch 265670, train_perplexity=5493.2363, train_loss=8.611273

Batch 265680, train_perplexity=6437.488, train_loss=8.769894

Batch 265690, train_perplexity=5766.032, train_loss=8.6597395

Batch 265700, train_perplexity=5394.3286, train_loss=8.593103

Batch 265710, train_perplexity=4684.4175, train_loss=8.451997

Batch 265720, train_perplexity=4205.0034, train_loss=8.34403

Batch 265730, train_perplexity=5307.9224, train_loss=8.576956

Batch 265740, train_perplexity=4430.0615, train_loss=8.396169

Batch 265750, train_perplexity=5571.0005, train_loss=8.62533

Batch 265760, train_perplexity=4798.4604, train_loss=8.47605

Batch 265770, train_perplexity=5954.157, train_loss=8.691845

Batch 265780, train_perplexity=5038.7427, train_loss=8.524912

Batch 265790, train_perplexity=6029.9414, train_loss=8.704493

Batch 265800, train_perplexity=6363.2046, train_loss=8.758287

Batch 265810, train_perplexity=4419.6553, train_loss=8.393817

Batch 265820, train_perplexity=5635.0576, train_loss=8.636763

Batch 265830, train_perplexity=6203.697, train_loss=8.732901

Batch 265840, train_perplexity=5240.193, train_loss=8.564114

Batch 265850, train_perplexity=5004.96, train_loss=8.518185

Batch 265860, train_perplexity=5656.616, train_loss=8.640581

Batch 265870, train_perplexity=5308.773, train_loss=8.577116

Batch 265880, train_perplexity=6049.121, train_loss=8.707668

Batch 265890, train_perplexity=4337.1855, train_loss=8.374981

Batch 265900, train_perplexity=5670.665, train_loss=8.643062

Batch 265910, train_perplexity=5887.6855, train_loss=8.680618
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 265920, train_perplexity=4806.4434, train_loss=8.477713

Batch 265930, train_perplexity=6070.0757, train_loss=8.711126

Batch 265940, train_perplexity=4767.4565, train_loss=8.469568

Batch 265950, train_perplexity=4652.8813, train_loss=8.445242

Batch 265960, train_perplexity=5490.9683, train_loss=8.61086

Batch 265970, train_perplexity=4453.0957, train_loss=8.401355

Batch 265980, train_perplexity=4745.5835, train_loss=8.46497

Batch 265990, train_perplexity=4286.451, train_loss=8.3632145

Batch 266000, train_perplexity=4788.947, train_loss=8.474066

Batch 266010, train_perplexity=5078.6494, train_loss=8.532801

Batch 266020, train_perplexity=5959.645, train_loss=8.692766

Batch 266030, train_perplexity=5059.2935, train_loss=8.528982

Batch 266040, train_perplexity=5514.526, train_loss=8.615141

Batch 266050, train_perplexity=6539.375, train_loss=8.785597

Batch 266060, train_perplexity=4751.688, train_loss=8.466255

Batch 266070, train_perplexity=5392.045, train_loss=8.59268

Batch 266080, train_perplexity=5432.107, train_loss=8.600082

Batch 266090, train_perplexity=4997.3857, train_loss=8.51667

Batch 266100, train_perplexity=5068.257, train_loss=8.530752

Batch 266110, train_perplexity=6368.875, train_loss=8.759178

Batch 266120, train_perplexity=6301.4146, train_loss=8.748529

Batch 266130, train_perplexity=4863.9673, train_loss=8.48961

Batch 266140, train_perplexity=4503.0615, train_loss=8.412513

Batch 266150, train_perplexity=6123.398, train_loss=8.719872

Batch 266160, train_perplexity=5442.0005, train_loss=8.601902

Batch 266170, train_perplexity=5024.5967, train_loss=8.5221

Batch 266180, train_perplexity=6340.8394, train_loss=8.754766

Batch 266190, train_perplexity=4778.367, train_loss=8.471854

Batch 266200, train_perplexity=5480.416, train_loss=8.608936

Batch 266210, train_perplexity=6109.0957, train_loss=8.717534

Batch 266220, train_perplexity=5387.6245, train_loss=8.59186

Batch 266230, train_perplexity=5045.6143, train_loss=8.526275

Batch 266240, train_perplexity=4898.499, train_loss=8.496684

Batch 266250, train_perplexity=5547.1543, train_loss=8.62104

Batch 266260, train_perplexity=4842.6855, train_loss=8.485225

Batch 266270, train_perplexity=5366.85, train_loss=8.5879965

Batch 266280, train_perplexity=7553.7837, train_loss=8.929804

Batch 266290, train_perplexity=6495.667, train_loss=8.778891

Batch 266300, train_perplexity=6205.6787, train_loss=8.73322

Batch 266310, train_perplexity=4786.4946, train_loss=8.473554

Batch 266320, train_perplexity=5111.2627, train_loss=8.539202

Batch 266330, train_perplexity=4417.8135, train_loss=8.3934

Batch 266340, train_perplexity=5535.565, train_loss=8.618949

Batch 266350, train_perplexity=5043.1465, train_loss=8.525785

Batch 266360, train_perplexity=5568.5996, train_loss=8.624899

Batch 266370, train_perplexity=5926.014, train_loss=8.687107

Batch 266380, train_perplexity=5774.2866, train_loss=8.66117

Batch 266390, train_perplexity=4845.5083, train_loss=8.485807

Batch 266400, train_perplexity=6536.519, train_loss=8.78516

Batch 266410, train_perplexity=5453.139, train_loss=8.603947

Batch 266420, train_perplexity=4877.1357, train_loss=8.492313

Batch 266430, train_perplexity=4761.141, train_loss=8.468243

Batch 266440, train_perplexity=5108.061, train_loss=8.538575

Batch 266450, train_perplexity=5672.3955, train_loss=8.643367

Batch 266460, train_perplexity=4995.423, train_loss=8.516277

Batch 266470, train_perplexity=4881.026, train_loss=8.493111

Batch 266480, train_perplexity=6099.5894, train_loss=8.715977

Batch 266490, train_perplexity=5293.5454, train_loss=8.574244

Batch 266500, train_perplexity=5814.693, train_loss=8.668143

Batch 266510, train_perplexity=6065.0874, train_loss=8.710304

Batch 266520, train_perplexity=6675.498, train_loss=8.806199

Batch 266530, train_perplexity=5024.3716, train_loss=8.522056

Batch 266540, train_perplexity=5256.044, train_loss=8.567134

Batch 266550, train_perplexity=5166.77, train_loss=8.550003

Batch 266560, train_perplexity=5474.273, train_loss=8.607815

Batch 266570, train_perplexity=4156.1206, train_loss=8.332337

Batch 266580, train_perplexity=5113.1934, train_loss=8.539579

Batch 266590, train_perplexity=4252.3516, train_loss=8.355227

Batch 266600, train_perplexity=4937.6904, train_loss=8.504653

Batch 266610, train_perplexity=5089.258, train_loss=8.534887

Batch 266620, train_perplexity=6432.0137, train_loss=8.769043

Batch 266630, train_perplexity=5678.241, train_loss=8.644397

Batch 266640, train_perplexity=5637.261, train_loss=8.637154

Batch 266650, train_perplexity=6369.835, train_loss=8.759329

Batch 266660, train_perplexity=3998.942, train_loss=8.293785

Batch 266670, train_perplexity=4495.205, train_loss=8.410767

Batch 266680, train_perplexity=6463.768, train_loss=8.773968

Batch 266690, train_perplexity=7205.4927, train_loss=8.882599

Batch 266700, train_perplexity=5059.0137, train_loss=8.528927

Batch 266710, train_perplexity=5835.0913, train_loss=8.671645

Batch 266720, train_perplexity=4857.547, train_loss=8.488289

Batch 266730, train_perplexity=5925.867, train_loss=8.687082

Batch 266740, train_perplexity=5580.631, train_loss=8.627057

Batch 266750, train_perplexity=5200.326, train_loss=8.556477

Batch 266760, train_perplexity=7301.4917, train_loss=8.895834

Batch 266770, train_perplexity=5536.8374, train_loss=8.619179

Batch 266780, train_perplexity=6324.5815, train_loss=8.752199

Batch 266790, train_perplexity=5292.7075, train_loss=8.574085

Batch 266800, train_perplexity=4917.507, train_loss=8.500557

Batch 266810, train_perplexity=4782.124, train_loss=8.47264

Batch 266820, train_perplexity=4610.8643, train_loss=8.436171

Batch 266830, train_perplexity=5319.72, train_loss=8.579176

Batch 266840, train_perplexity=5070.7803, train_loss=8.53125

Batch 266850, train_perplexity=4686.96, train_loss=8.452539

Batch 266860, train_perplexity=5574.987, train_loss=8.626045

Batch 266870, train_perplexity=5549.0273, train_loss=8.621378

Batch 266880, train_perplexity=5982.879, train_loss=8.696657

Batch 266890, train_perplexity=5073.8955, train_loss=8.531864

Batch 266900, train_perplexity=5698.981, train_loss=8.648043

Batch 266910, train_perplexity=5252.1304, train_loss=8.566389

Batch 266920, train_perplexity=4510.742, train_loss=8.414217

Batch 266930, train_perplexity=6193.6587, train_loss=8.731281

Batch 266940, train_perplexity=5327.5234, train_loss=8.580642

Batch 266950, train_perplexity=4449.9585, train_loss=8.40065

Batch 266960, train_perplexity=5455.974, train_loss=8.604466

Batch 266970, train_perplexity=4634.392, train_loss=8.44126

Batch 266980, train_perplexity=4702.716, train_loss=8.455895

Batch 266990, train_perplexity=5109.4595, train_loss=8.538849

Batch 267000, train_perplexity=4855.2314, train_loss=8.487812

Batch 267010, train_perplexity=5886.1587, train_loss=8.680359

Batch 267020, train_perplexity=5876.6514, train_loss=8.678742

Batch 267030, train_perplexity=4375.106, train_loss=8.383686

Batch 267040, train_perplexity=4808.1074, train_loss=8.478059

Batch 267050, train_perplexity=5410.1914, train_loss=8.59604

Batch 267060, train_perplexity=6073.944, train_loss=8.711763

Batch 267070, train_perplexity=4666.96, train_loss=8.448263

Batch 267080, train_perplexity=5167.9233, train_loss=8.550226

Batch 267090, train_perplexity=5697.0464, train_loss=8.647703

Batch 267100, train_perplexity=6216.003, train_loss=8.734882

Batch 267110, train_perplexity=5246.4336, train_loss=8.565304

Batch 267120, train_perplexity=5908.34, train_loss=8.68412

Batch 267130, train_perplexity=4864.83, train_loss=8.489787

Batch 267140, train_perplexity=6033.7036, train_loss=8.705116

Batch 267150, train_perplexity=6274.029, train_loss=8.744174

Batch 267160, train_perplexity=5631.9414, train_loss=8.6362095

Batch 267170, train_perplexity=4622.964, train_loss=8.438791

Batch 267180, train_perplexity=5833.127, train_loss=8.6713085

Batch 267190, train_perplexity=5015.6245, train_loss=8.520313

Batch 267200, train_perplexity=4730.992, train_loss=8.46189

Batch 267210, train_perplexity=5847.709, train_loss=8.673805

Batch 267220, train_perplexity=4670.8916, train_loss=8.449105

Batch 267230, train_perplexity=5301.452, train_loss=8.575736
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 267240, train_perplexity=4488.06, train_loss=8.409176

Batch 267250, train_perplexity=6939.0303, train_loss=8.844917

Batch 267260, train_perplexity=5676.5303, train_loss=8.644095

Batch 267270, train_perplexity=5169.5894, train_loss=8.550549

Batch 267280, train_perplexity=5051.5024, train_loss=8.527441

Batch 267290, train_perplexity=6182.028, train_loss=8.729402

Batch 267300, train_perplexity=6040.941, train_loss=8.706315

Batch 267310, train_perplexity=5611.654, train_loss=8.632601

Batch 267320, train_perplexity=5674.0835, train_loss=8.643664

Batch 267330, train_perplexity=5343.404, train_loss=8.583618

Batch 267340, train_perplexity=5075.8267, train_loss=8.532245

Batch 267350, train_perplexity=5070.5386, train_loss=8.531202

Batch 267360, train_perplexity=6863.684, train_loss=8.834

Batch 267370, train_perplexity=5980.038, train_loss=8.696182

Batch 267380, train_perplexity=5082.6807, train_loss=8.533594

Batch 267390, train_perplexity=5065.2124, train_loss=8.530151

Batch 267400, train_perplexity=6242.1353, train_loss=8.739078

Batch 267410, train_perplexity=5900.7886, train_loss=8.682841

Batch 267420, train_perplexity=6179.982, train_loss=8.729071

Batch 267430, train_perplexity=5704.6035, train_loss=8.649029

Batch 267440, train_perplexity=4631.286, train_loss=8.44059

Batch 267450, train_perplexity=4717.71, train_loss=8.459079

Batch 267460, train_perplexity=4710.737, train_loss=8.4576

Batch 267470, train_perplexity=5649.2573, train_loss=8.639279

Batch 267480, train_perplexity=5774.0166, train_loss=8.661123

Batch 267490, train_perplexity=6021.988, train_loss=8.703173

Batch 267500, train_perplexity=5029.9087, train_loss=8.523157

Batch 267510, train_perplexity=5514.8833, train_loss=8.615206

Batch 267520, train_perplexity=5507.8823, train_loss=8.613935

Batch 267530, train_perplexity=5031.18, train_loss=8.52341

Batch 267540, train_perplexity=5458.5347, train_loss=8.604936

Batch 267550, train_perplexity=5886.8325, train_loss=8.680473

Batch 267560, train_perplexity=5842.2354, train_loss=8.672869

Batch 267570, train_perplexity=4897.2188, train_loss=8.496423

Batch 267580, train_perplexity=5349.9614, train_loss=8.584845

Batch 267590, train_perplexity=5877.24, train_loss=8.678843

Batch 267600, train_perplexity=5118.9214, train_loss=8.540699

Batch 267610, train_perplexity=5654.9277, train_loss=8.640283

Batch 267620, train_perplexity=5197.4897, train_loss=8.555931

Batch 267630, train_perplexity=5719.6714, train_loss=8.651667

Batch 267640, train_perplexity=5126.7285, train_loss=8.542223

Batch 267650, train_perplexity=6457.527, train_loss=8.773002

Batch 267660, train_perplexity=4645.623, train_loss=8.443681

Batch 267670, train_perplexity=5649.2305, train_loss=8.639275

Batch 267680, train_perplexity=4955.735, train_loss=8.508301

Batch 267690, train_perplexity=4807.227, train_loss=8.477876

Batch 267700, train_perplexity=5929.1006, train_loss=8.687628

Batch 267710, train_perplexity=6036.4546, train_loss=8.705572

Batch 267720, train_perplexity=6741.6636, train_loss=8.816062

Batch 267730, train_perplexity=5132.359, train_loss=8.543321

Batch 267740, train_perplexity=4622.8315, train_loss=8.438763

Batch 267750, train_perplexity=5372.6523, train_loss=8.589077

Batch 267760, train_perplexity=6026.7563, train_loss=8.703964

Batch 267770, train_perplexity=5903.8735, train_loss=8.683364

Batch 267780, train_perplexity=5627.979, train_loss=8.635506

Batch 267790, train_perplexity=5521.1455, train_loss=8.616341

Batch 267800, train_perplexity=5032.4087, train_loss=8.523654

Batch 267810, train_perplexity=5306.8193, train_loss=8.576748

Batch 267820, train_perplexity=5132.6626, train_loss=8.54338

Batch 267830, train_perplexity=5476.592, train_loss=8.608238

Batch 267840, train_perplexity=5313.0527, train_loss=8.577922

Batch 267850, train_perplexity=5242.0625, train_loss=8.56447

Batch 267860, train_perplexity=6616.6797, train_loss=8.797349

Batch 267870, train_perplexity=5736.65, train_loss=8.654631

Batch 267880, train_perplexity=4954.119, train_loss=8.507975

Batch 267890, train_perplexity=5640.488, train_loss=8.637726

Batch 267900, train_perplexity=5066.594, train_loss=8.530424

Batch 267910, train_perplexity=5415.359, train_loss=8.596994

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Batch 267920, train_perplexity=6377.3296, train_loss=8.760505

Batch 267930, train_perplexity=5385.138, train_loss=8.591398

Batch 267940, train_perplexity=5575.625, train_loss=8.62616

Batch 267950, train_perplexity=5018.443, train_loss=8.520875

Batch 267960, train_perplexity=7115.782, train_loss=8.87007

Batch 267970, train_perplexity=5162.49, train_loss=8.549174

Batch 267980, train_perplexity=6486.165, train_loss=8.777427

Batch 267990, train_perplexity=6583.3267, train_loss=8.792295

Batch 268000, train_perplexity=4844.9307, train_loss=8.485688

Batch 268010, train_perplexity=5296.247, train_loss=8.574754

Batch 268020, train_perplexity=4173.1514, train_loss=8.336427

Batch 268030, train_perplexity=5085.5317, train_loss=8.534155

Batch 268040, train_perplexity=5140.2695, train_loss=8.544861

Batch 268050, train_perplexity=6133.784, train_loss=8.721567

Batch 268060, train_perplexity=5610.0273, train_loss=8.632311

Batch 268070, train_perplexity=4619.116, train_loss=8.437959

Batch 268080, train_perplexity=6014.3145, train_loss=8.701898

Batch 268090, train_perplexity=6038.8267, train_loss=8.705965

Batch 268100, train_perplexity=6007.8022, train_loss=8.700814

Batch 268110, train_perplexity=6016.098, train_loss=8.702194

Batch 268120, train_perplexity=5128.7725, train_loss=8.542622

Batch 268130, train_perplexity=5229.7837, train_loss=8.562125

Batch 268140, train_perplexity=5195.9487, train_loss=8.5556345

Batch 268150, train_perplexity=5550.4883, train_loss=8.621641

Batch 268160, train_perplexity=4398.2236, train_loss=8.388956

Batch 268170, train_perplexity=5398.0234, train_loss=8.593788

Batch 268180, train_perplexity=4825.0527, train_loss=8.481577

Batch 268190, train_perplexity=5761.459, train_loss=8.658946

Batch 268200, train_perplexity=5767.2476, train_loss=8.65995

Batch 268210, train_perplexity=5969.3716, train_loss=8.694397

Batch 268220, train_perplexity=5015.5195, train_loss=8.520292

Batch 268230, train_perplexity=4541.2324, train_loss=8.420954

Batch 268240, train_perplexity=5178.0957, train_loss=8.552193

Batch 268250, train_perplexity=6187.873, train_loss=8.730347

Batch 268260, train_perplexity=6390.218, train_loss=8.762524

Batch 268270, train_perplexity=5909.918, train_loss=8.684387

Batch 268280, train_perplexity=5360.0625, train_loss=8.586731

Batch 268290, train_perplexity=4862.627, train_loss=8.489334

Batch 268300, train_perplexity=5791.8525, train_loss=8.664207

Batch 268310, train_perplexity=4513.2295, train_loss=8.414768

Batch 268320, train_perplexity=4786.988, train_loss=8.473657

Batch 268330, train_perplexity=6051.0137, train_loss=8.707981

Batch 268340, train_perplexity=5351.6147, train_loss=8.585154

Batch 268350, train_perplexity=6210.066, train_loss=8.733927

Batch 268360, train_perplexity=5771.8364, train_loss=8.660746

Batch 268370, train_perplexity=4843.143, train_loss=8.485319

Batch 268380, train_perplexity=5883.184, train_loss=8.679853

Batch 268390, train_perplexity=5075.0425, train_loss=8.53209

Batch 268400, train_perplexity=5278.533, train_loss=8.5714035

Batch 268410, train_perplexity=5212.461, train_loss=8.558807

Batch 268420, train_perplexity=5249.3213, train_loss=8.565854

Batch 268430, train_perplexity=4981.512, train_loss=8.513489

Batch 268440, train_perplexity=5664.2275, train_loss=8.641926

Batch 268450, train_perplexity=5030.35, train_loss=8.523245

Batch 268460, train_perplexity=5772.1504, train_loss=8.6608

Batch 268470, train_perplexity=4920.819, train_loss=8.50123

Batch 268480, train_perplexity=5110.1416, train_loss=8.538982
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 268490, train_perplexity=4996.5234, train_loss=8.516498

Batch 268500, train_perplexity=6548.4116, train_loss=8.786978

Batch 268510, train_perplexity=4919.6646, train_loss=8.500996

Batch 268520, train_perplexity=5312.9263, train_loss=8.577898

Batch 268530, train_perplexity=5238.699, train_loss=8.563828

Batch 268540, train_perplexity=5708.375, train_loss=8.64969

Batch 268550, train_perplexity=5850.5986, train_loss=8.674299

Batch 268560, train_perplexity=5386.5454, train_loss=8.59166

Batch 268570, train_perplexity=5176.926, train_loss=8.551967

Batch 268580, train_perplexity=5926.206, train_loss=8.6871395

Batch 268590, train_perplexity=4334.2373, train_loss=8.374301

Batch 268600, train_perplexity=5023.878, train_loss=8.521957

Batch 268610, train_perplexity=5647.022, train_loss=8.638884

Batch 268620, train_perplexity=5361.024, train_loss=8.58691

Batch 268630, train_perplexity=4086.2305, train_loss=8.315378

Batch 268640, train_perplexity=5972.9707, train_loss=8.695

Batch 268650, train_perplexity=5069.2476, train_loss=8.530948

Batch 268660, train_perplexity=5174.443, train_loss=8.551487

Batch 268670, train_perplexity=6710.201, train_loss=8.811384

Batch 268680, train_perplexity=5634.5415, train_loss=8.636671

Batch 268690, train_perplexity=6234.0977, train_loss=8.737789

Batch 268700, train_perplexity=5709.023, train_loss=8.649803

Batch 268710, train_perplexity=5569.5664, train_loss=8.6250725

Batch 268720, train_perplexity=4989.2144, train_loss=8.515034

Batch 268730, train_perplexity=4850.312, train_loss=8.486798

Batch 268740, train_perplexity=6058.2544, train_loss=8.709177

Batch 268750, train_perplexity=4691.28, train_loss=8.453461

Batch 268760, train_perplexity=5669.135, train_loss=8.642792

Batch 268770, train_perplexity=4979.783, train_loss=8.513142

Batch 268780, train_perplexity=5394.7505, train_loss=8.593182

Batch 268790, train_perplexity=7241.9634, train_loss=8.887648

Batch 268800, train_perplexity=5504.926, train_loss=8.613399

Batch 268810, train_perplexity=6131.363, train_loss=8.721172

Batch 268820, train_perplexity=5111.9014, train_loss=8.539327

Batch 268830, train_perplexity=5992.941, train_loss=8.698338

Batch 268840, train_perplexity=6034.7275, train_loss=8.705286

Batch 268850, train_perplexity=5261.631, train_loss=8.568196

Batch 268860, train_perplexity=5525.855, train_loss=8.617193

Batch 268870, train_perplexity=5294.111, train_loss=8.57435

Batch 268880, train_perplexity=5363.8364, train_loss=8.587435

Batch 268890, train_perplexity=5062.5273, train_loss=8.529621

Batch 268900, train_perplexity=4656.9434, train_loss=8.446115

Batch 268910, train_perplexity=6048.36, train_loss=8.707542

Batch 268920, train_perplexity=4685.843, train_loss=8.452301

Batch 268930, train_perplexity=5716.9883, train_loss=8.651197

Batch 268940, train_perplexity=6493.939, train_loss=8.778625

Batch 268950, train_perplexity=5142.653, train_loss=8.545324

Batch 268960, train_perplexity=4909.232, train_loss=8.498873

Batch 268970, train_perplexity=5385.138, train_loss=8.591398

Batch 268980, train_perplexity=4101.5156, train_loss=8.319112

Batch 268990, train_perplexity=5671.6543, train_loss=8.643236

Batch 269000, train_perplexity=4450.107, train_loss=8.400683

Batch 269010, train_perplexity=4627.7363, train_loss=8.439823

Batch 269020, train_perplexity=4970.891, train_loss=8.511354

Batch 269030, train_perplexity=5372.1963, train_loss=8.588992

Batch 269040, train_perplexity=5936.592, train_loss=8.68889

Batch 269050, train_perplexity=5774.3306, train_loss=8.661178

Batch 269060, train_perplexity=5112.891, train_loss=8.53952

Batch 269070, train_perplexity=7166.7993, train_loss=8.877214

Batch 269080, train_perplexity=5803.5854, train_loss=8.666231

Batch 269090, train_perplexity=5234.2295, train_loss=8.562975

Batch 269100, train_perplexity=6062.8555, train_loss=8.709936

Batch 269110, train_perplexity=6112.989, train_loss=8.718171

Batch 269120, train_perplexity=4805.82, train_loss=8.477583

Batch 269130, train_perplexity=6891.5996, train_loss=8.838058

Batch 269140, train_perplexity=4576.9023, train_loss=8.428778

Batch 269150, train_perplexity=5423.685, train_loss=8.598531

Batch 269160, train_perplexity=5892.2695, train_loss=8.6813965

Batch 269170, train_perplexity=6246.53, train_loss=8.739781

Batch 269180, train_perplexity=4966.224, train_loss=8.510415

Batch 269190, train_perplexity=5026.135, train_loss=8.522407

Batch 269200, train_perplexity=6263.8955, train_loss=8.742558

Batch 269210, train_perplexity=6272.4194, train_loss=8.743917

Batch 269220, train_perplexity=5778.3467, train_loss=8.661873

Batch 269230, train_perplexity=3906.545, train_loss=8.270409

Batch 269240, train_perplexity=5448.2007, train_loss=8.603041

Batch 269250, train_perplexity=5085.4883, train_loss=8.534146

Batch 269260, train_perplexity=5763.3003, train_loss=8.6592655

Batch 269270, train_perplexity=5157.49, train_loss=8.548205

Batch 269280, train_perplexity=4889.076, train_loss=8.494759

Batch 269290, train_perplexity=6559.9497, train_loss=8.788738

Batch 269300, train_perplexity=5984.42, train_loss=8.696915

Batch 269310, train_perplexity=6343.882, train_loss=8.755246

Batch 269320, train_perplexity=4334.969, train_loss=8.37447

Batch 269330, train_perplexity=5635.3745, train_loss=8.636819

Batch 269340, train_perplexity=4984.625, train_loss=8.514113

Batch 269350, train_perplexity=4346.22, train_loss=8.377062

Batch 269360, train_perplexity=5766.5327, train_loss=8.659826

Batch 269370, train_perplexity=6497.061, train_loss=8.779105

Batch 269380, train_perplexity=4703.451, train_loss=8.456052

Batch 269390, train_perplexity=5423.871, train_loss=8.598565

Batch 269400, train_perplexity=5674.7114, train_loss=8.643775

Batch 269410, train_perplexity=4889.2812, train_loss=8.494801

Batch 269420, train_perplexity=5172.9233, train_loss=8.551193

Batch 269430, train_perplexity=5346.0083, train_loss=8.5841055

Batch 269440, train_perplexity=5199.3145, train_loss=8.556282

Batch 269450, train_perplexity=5052.5625, train_loss=8.527651

Batch 269460, train_perplexity=5498.729, train_loss=8.612272

Batch 269470, train_perplexity=5464.8477, train_loss=8.6060915

Batch 269480, train_perplexity=5099.3926, train_loss=8.536877

Batch 269490, train_perplexity=5710.733, train_loss=8.650103

Batch 269500, train_perplexity=5391.0884, train_loss=8.592503

Batch 269510, train_perplexity=6051.441, train_loss=8.708052

Batch 269520, train_perplexity=5411.0225, train_loss=8.596193

Batch 269530, train_perplexity=5632.731, train_loss=8.63635

Batch 269540, train_perplexity=5079.9863, train_loss=8.533064

Batch 269550, train_perplexity=5727.4277, train_loss=8.653022

Batch 269560, train_perplexity=4916.0156, train_loss=8.500254

Batch 269570, train_perplexity=4893.428, train_loss=8.495648

Batch 269580, train_perplexity=5499.5786, train_loss=8.612427

Batch 269590, train_perplexity=5853.891, train_loss=8.674862

Batch 269600, train_perplexity=5096.6797, train_loss=8.536345

Batch 269610, train_perplexity=4791.2856, train_loss=8.474554

Batch 269620, train_perplexity=4744.5107, train_loss=8.464744

Batch 269630, train_perplexity=5552.971, train_loss=8.622088

Batch 269640, train_perplexity=4616.0513, train_loss=8.437295

Batch 269650, train_perplexity=5494.96, train_loss=8.611587

Batch 269660, train_perplexity=4493.4824, train_loss=8.410383

Batch 269670, train_perplexity=5469.17, train_loss=8.606882

Batch 269680, train_perplexity=5980.2495, train_loss=8.696218

Batch 269690, train_perplexity=5837.9297, train_loss=8.672132

Batch 269700, train_perplexity=5760.19, train_loss=8.658726

Batch 269710, train_perplexity=6033.8877, train_loss=8.705147

Batch 269720, train_perplexity=5677.954, train_loss=8.644346

Batch 269730, train_perplexity=5022.412, train_loss=8.521666

Batch 269740, train_perplexity=4956.9165, train_loss=8.508539

Batch 269750, train_perplexity=5477.7305, train_loss=8.608446

Batch 269760, train_perplexity=5059.448, train_loss=8.529013

Batch 269770, train_perplexity=4797.225, train_loss=8.475793

Batch 269780, train_perplexity=5032.855, train_loss=8.523743

Batch 269790, train_perplexity=4192.2383, train_loss=8.34099

Batch 269800, train_perplexity=5983.541, train_loss=8.696768
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 269810, train_perplexity=5386.8228, train_loss=8.591711

Batch 269820, train_perplexity=5543.8174, train_loss=8.620439

Batch 269830, train_perplexity=5103.1094, train_loss=8.537605

Batch 269840, train_perplexity=4681.519, train_loss=8.451378

Batch 269850, train_perplexity=4573.8784, train_loss=8.428117

Batch 269860, train_perplexity=5112.0815, train_loss=8.539362

Batch 269870, train_perplexity=5651.052, train_loss=8.639597

Batch 269880, train_perplexity=6144.8735, train_loss=8.723373

Batch 269890, train_perplexity=5475.067, train_loss=8.60796

Batch 269900, train_perplexity=5675.821, train_loss=8.6439705

Batch 269910, train_perplexity=5442.6284, train_loss=8.602017

Batch 269920, train_perplexity=5491.466, train_loss=8.61095

Batch 269930, train_perplexity=5877.4307, train_loss=8.678875

Batch 269940, train_perplexity=6107.727, train_loss=8.71731

Batch 269950, train_perplexity=5945.056, train_loss=8.690315

Batch 269960, train_perplexity=4732.4995, train_loss=8.462209

Batch 269970, train_perplexity=5765.51, train_loss=8.659649

Batch 269980, train_perplexity=6623.4795, train_loss=8.798376

Batch 269990, train_perplexity=5890.46, train_loss=8.681089

Batch 270000, train_perplexity=5004.75, train_loss=8.518143

Batch 270010, train_perplexity=5948.459, train_loss=8.690887

Batch 270020, train_perplexity=5030.427, train_loss=8.52326

Batch 270030, train_perplexity=6004.577, train_loss=8.700277

Batch 270040, train_perplexity=5619.773, train_loss=8.634047

Batch 270050, train_perplexity=4866.4404, train_loss=8.490118

Batch 270060, train_perplexity=5520.377, train_loss=8.616201

Batch 270070, train_perplexity=5793.7197, train_loss=8.66453

Batch 270080, train_perplexity=5079.124, train_loss=8.532894

Batch 270090, train_perplexity=5073.494, train_loss=8.531785

Batch 270100, train_perplexity=6142.2954, train_loss=8.722954

Batch 270110, train_perplexity=5338.153, train_loss=8.582635

Batch 270120, train_perplexity=5107.9004, train_loss=8.538544

Batch 270130, train_perplexity=5273.215, train_loss=8.570395

Batch 270140, train_perplexity=6239.415, train_loss=8.738642

Batch 270150, train_perplexity=4821.4463, train_loss=8.480829

Batch 270160, train_perplexity=5082.6035, train_loss=8.533579

Batch 270170, train_perplexity=4500.6743, train_loss=8.411983

Batch 270180, train_perplexity=5437.5596, train_loss=8.601086

Batch 270190, train_perplexity=5634.681, train_loss=8.636696

Batch 270200, train_perplexity=5413.5874, train_loss=8.596667

Batch 270210, train_perplexity=4443.5845, train_loss=8.399217

Batch 270220, train_perplexity=4930.7686, train_loss=8.50325

Batch 270230, train_perplexity=4789.9697, train_loss=8.474279

Batch 270240, train_perplexity=5343.205, train_loss=8.583581

Batch 270250, train_perplexity=5065.1255, train_loss=8.530134

Batch 270260, train_perplexity=5029.707, train_loss=8.523117

Batch 270270, train_perplexity=5516.756, train_loss=8.615545

Batch 270280, train_perplexity=5560.4805, train_loss=8.62344

Batch 270290, train_perplexity=4696.974, train_loss=8.454674

Batch 270300, train_perplexity=4921.2603, train_loss=8.50132

Batch 270310, train_perplexity=5949.724, train_loss=8.6911

Batch 270320, train_perplexity=4344.202, train_loss=8.376597

Batch 270330, train_perplexity=6097.0303, train_loss=8.715557

Batch 270340, train_perplexity=5305.129, train_loss=8.576429

Batch 270350, train_perplexity=5215.3647, train_loss=8.559364

Batch 270360, train_perplexity=5033.4214, train_loss=8.523855

Batch 270370, train_perplexity=6286.6846, train_loss=8.746189

Batch 270380, train_perplexity=4867.3315, train_loss=8.490301

Batch 270390, train_perplexity=4848.3696, train_loss=8.486398

Batch 270400, train_perplexity=4665.3667, train_loss=8.447922

Batch 270410, train_perplexity=5562.464, train_loss=8.623796

Batch 270420, train_perplexity=5466.2705, train_loss=8.606352

Batch 270430, train_perplexity=4775.0645, train_loss=8.471163

Batch 270440, train_perplexity=5731.8047, train_loss=8.653786

Batch 270450, train_perplexity=5083.3306, train_loss=8.533722

Batch 270460, train_perplexity=6104.6465, train_loss=8.716805

Batch 270470, train_perplexity=5323.0645, train_loss=8.579804

Batch 270480, train_perplexity=5435.7764, train_loss=8.600758

Batch 270490, train_perplexity=5179.1333, train_loss=8.552393

Batch 270500, train_perplexity=4423.674, train_loss=8.394726

Batch 270510, train_perplexity=5938.3584, train_loss=8.689188

Batch 270520, train_perplexity=5246.5386, train_loss=8.565324

Batch 270530, train_perplexity=5464.2793, train_loss=8.605988

Batch 270540, train_perplexity=4623.6914, train_loss=8.438949

Batch 270550, train_perplexity=5740.579, train_loss=8.655315

Batch 270560, train_perplexity=4788.586, train_loss=8.47399

Batch 270570, train_perplexity=5395.754, train_loss=8.593368

Batch 270580, train_perplexity=4895.015, train_loss=8.495973

Batch 270590, train_perplexity=5346.962, train_loss=8.584284

Batch 270600, train_perplexity=6836.756, train_loss=8.830069

Batch 270610, train_perplexity=5164.573, train_loss=8.549578

Batch 270620, train_perplexity=5304.709, train_loss=8.57635

Batch 270630, train_perplexity=5354.749, train_loss=8.585739

Batch 270640, train_perplexity=4843.286, train_loss=8.485349

Batch 270650, train_perplexity=5656.368, train_loss=8.640537

Batch 270660, train_perplexity=5123.307, train_loss=8.541555

Batch 270670, train_perplexity=5306.5005, train_loss=8.576688

Batch 270680, train_perplexity=5532.2246, train_loss=8.618345

Batch 270690, train_perplexity=5238.704, train_loss=8.563829

Batch 270700, train_perplexity=6070.238, train_loss=8.711153

Batch 270710, train_perplexity=6213.9106, train_loss=8.734546

Batch 270720, train_perplexity=4960.3784, train_loss=8.509237

Batch 270730, train_perplexity=4420.3677, train_loss=8.393978

Batch 270740, train_perplexity=4996.628, train_loss=8.516519

Batch 270750, train_perplexity=6006.5015, train_loss=8.700598

Batch 270760, train_perplexity=5950.0815, train_loss=8.69116

Batch 270770, train_perplexity=5655.3433, train_loss=8.640356

Batch 270780, train_perplexity=5777.0684, train_loss=8.661652

Batch 270790, train_perplexity=6545.4707, train_loss=8.786529

Batch 270800, train_perplexity=4964.3535, train_loss=8.510038

Batch 270810, train_perplexity=5923.2456, train_loss=8.68664

Batch 270820, train_perplexity=6554.3096, train_loss=8.787878

Batch 270830, train_perplexity=5185.0835, train_loss=8.553541

Batch 270840, train_perplexity=5123.033, train_loss=8.541502

Batch 270850, train_perplexity=4584.9277, train_loss=8.43053

Batch 270860, train_perplexity=5552.733, train_loss=8.6220455

Batch 270870, train_perplexity=4776.5767, train_loss=8.471479

Batch 270880, train_perplexity=5217.9814, train_loss=8.559866

Batch 270890, train_perplexity=4538.6865, train_loss=8.420393

Batch 270900, train_perplexity=4998.1055, train_loss=8.516814

Batch 270910, train_perplexity=5246.6587, train_loss=8.565347

Batch 270920, train_perplexity=5571.7656, train_loss=8.625467

Batch 270930, train_perplexity=5926.285, train_loss=8.687153

Batch 270940, train_perplexity=4943.2876, train_loss=8.505786

Batch 270950, train_perplexity=5270.4297, train_loss=8.569867

Batch 270960, train_perplexity=4797.8564, train_loss=8.4759245

Batch 270970, train_perplexity=5087.259, train_loss=8.534494

Batch 270980, train_perplexity=5307.7607, train_loss=8.576925

Batch 270990, train_perplexity=6536.918, train_loss=8.785221

Batch 271000, train_perplexity=7224.897, train_loss=8.885288

Batch 271010, train_perplexity=6303.855, train_loss=8.748917

Batch 271020, train_perplexity=5134.802, train_loss=8.543797

Batch 271030, train_perplexity=5611.697, train_loss=8.632608

Batch 271040, train_perplexity=5094.843, train_loss=8.535984

Batch 271050, train_perplexity=4943.665, train_loss=8.505862

Batch 271060, train_perplexity=6128.0776, train_loss=8.720636

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305532 sentences.
Finished loading
Batch 271070, train_perplexity=5116.2563, train_loss=8.540178

Batch 271080, train_perplexity=5096.6846, train_loss=8.5363455

Batch 271090, train_perplexity=6380.341, train_loss=8.760977

Batch 271100, train_perplexity=5372.7036, train_loss=8.589087

Batch 271110, train_perplexity=5591.76, train_loss=8.629049

Batch 271120, train_perplexity=5130.4453, train_loss=8.542948

Batch 271130, train_perplexity=5410.31, train_loss=8.596062

Batch 271140, train_perplexity=6034.6587, train_loss=8.705275

Batch 271150, train_perplexity=4740.427, train_loss=8.463882

Batch 271160, train_perplexity=5456.125, train_loss=8.604494

Batch 271170, train_perplexity=5288.0156, train_loss=8.573198

Batch 271180, train_perplexity=5849.661, train_loss=8.674139

Batch 271190, train_perplexity=5117.989, train_loss=8.540517

Batch 271200, train_perplexity=6658.1978, train_loss=8.803604

Batch 271210, train_perplexity=6274.9326, train_loss=8.744318

Batch 271220, train_perplexity=5868.962, train_loss=8.677433

Batch 271230, train_perplexity=4517.057, train_loss=8.415616

Batch 271240, train_perplexity=5360.927, train_loss=8.586892

Batch 271250, train_perplexity=5223.045, train_loss=8.560836

Batch 271260, train_perplexity=5257.3975, train_loss=8.567391

Batch 271270, train_perplexity=4541.0205, train_loss=8.420907

Batch 271280, train_perplexity=5629.9546, train_loss=8.635857

Batch 271290, train_perplexity=5478.911, train_loss=8.608662

Batch 271300, train_perplexity=4786.49, train_loss=8.473553

Batch 271310, train_perplexity=5322.44, train_loss=8.579687

Batch 271320, train_perplexity=5250.7134, train_loss=8.566119

Batch 271330, train_perplexity=5998.007, train_loss=8.6991825

Batch 271340, train_perplexity=6286.085, train_loss=8.746094

Batch 271350, train_perplexity=5638.6484, train_loss=8.6374

Batch 271360, train_perplexity=6700.839, train_loss=8.809988

Batch 271370, train_perplexity=5530.4204, train_loss=8.618019

Batch 271380, train_perplexity=5298.6567, train_loss=8.575209

Batch 271390, train_perplexity=5796.218, train_loss=8.664961

Batch 271400, train_perplexity=5909.856, train_loss=8.684377

Batch 271410, train_perplexity=6414.776, train_loss=8.766359

Batch 271420, train_perplexity=5313.0273, train_loss=8.577917

Batch 271430, train_perplexity=5576.008, train_loss=8.626228

Batch 271440, train_perplexity=5304.5015, train_loss=8.576311

Batch 271450, train_perplexity=5495.3843, train_loss=8.611664

Batch 271460, train_perplexity=5469.2637, train_loss=8.606899

Batch 271470, train_perplexity=4951.658, train_loss=8.507478

Batch 271480, train_perplexity=5009.0474, train_loss=8.519001

Batch 271490, train_perplexity=5115.0366, train_loss=8.53994

Batch 271500, train_perplexity=5209.8765, train_loss=8.558311

Batch 271510, train_perplexity=5202.509, train_loss=8.556896

Batch 271520, train_perplexity=5223.105, train_loss=8.560847

Batch 271530, train_perplexity=5617.63, train_loss=8.633665

Batch 271540, train_perplexity=5711.5605, train_loss=8.650248

Batch 271550, train_perplexity=5475.2915, train_loss=8.608001

Batch 271560, train_perplexity=5273.3306, train_loss=8.570417

Batch 271570, train_perplexity=6352.2666, train_loss=8.756567

Batch 271580, train_perplexity=4342.9634, train_loss=8.376312

Batch 271590, train_perplexity=5660.1885, train_loss=8.641212

Batch 271600, train_perplexity=4910.4116, train_loss=8.499113

Batch 271610, train_perplexity=5103.8003, train_loss=8.537741

Batch 271620, train_perplexity=5129.105, train_loss=8.542686

Batch 271630, train_perplexity=5129.643, train_loss=8.542791

Batch 271640, train_perplexity=4975.383, train_loss=8.512258

Batch 271650, train_perplexity=5489.0938, train_loss=8.610518

Batch 271660, train_perplexity=5691.312, train_loss=8.646696

Batch 271670, train_perplexity=6105.118, train_loss=8.716883

Batch 271680, train_perplexity=5523.8315, train_loss=8.616827

Batch 271690, train_perplexity=5081.925, train_loss=8.533445

Batch 271700, train_perplexity=5754.5454, train_loss=8.657745

Batch 271710, train_perplexity=4448.0405, train_loss=8.400219

Batch 271720, train_perplexity=5694.1353, train_loss=8.647192

Batch 271730, train_perplexity=5684.206, train_loss=8.645447

Batch 271740, train_perplexity=6892.033, train_loss=8.838121

Batch 271750, train_perplexity=6754.3027, train_loss=8.817935

Batch 271760, train_perplexity=4478.321, train_loss=8.407003

Batch 271770, train_perplexity=5307.897, train_loss=8.576951

Batch 271780, train_perplexity=4909.26, train_loss=8.4988785

Batch 271790, train_perplexity=6249.7476, train_loss=8.740296

Batch 271800, train_perplexity=6202.596, train_loss=8.732723

Batch 271810, train_perplexity=6331.908, train_loss=8.753357

Batch 271820, train_perplexity=5721.188, train_loss=8.651932

Batch 271830, train_perplexity=5751.5225, train_loss=8.65722

Batch 271840, train_perplexity=5464.957, train_loss=8.606112

Batch 271850, train_perplexity=5411.9927, train_loss=8.596373

Batch 271860, train_perplexity=4711.483, train_loss=8.457758

Batch 271870, train_perplexity=5347.2783, train_loss=8.584343

Batch 271880, train_perplexity=4288.283, train_loss=8.363642

Batch 271890, train_perplexity=4235.8623, train_loss=8.351342

Batch 271900, train_perplexity=4941.6333, train_loss=8.505451

Batch 271910, train_perplexity=5887.1187, train_loss=8.680522

Batch 271920, train_perplexity=6965.013, train_loss=8.848655

Batch 271930, train_perplexity=5626.53, train_loss=8.635248

Batch 271940, train_perplexity=5754.573, train_loss=8.65775

Batch 271950, train_perplexity=5477.647, train_loss=8.608431

Batch 271960, train_perplexity=5375.3843, train_loss=8.589585

Batch 271970, train_perplexity=4298.5767, train_loss=8.366039

Batch 271980, train_perplexity=4385.253, train_loss=8.386003

Batch 271990, train_perplexity=5670.7676, train_loss=8.64308

Batch 272000, train_perplexity=4658.3823, train_loss=8.446424

Batch 272010, train_perplexity=6102.807, train_loss=8.716504

Batch 272020, train_perplexity=6182.7646, train_loss=8.729521

Batch 272030, train_perplexity=5257.984, train_loss=8.567503

Batch 272040, train_perplexity=5574.4766, train_loss=8.625954

Batch 272050, train_perplexity=5344.4434, train_loss=8.583813

Batch 272060, train_perplexity=4725.45, train_loss=8.460718

Batch 272070, train_perplexity=4557.4067, train_loss=8.424509

Batch 272080, train_perplexity=6393.1924, train_loss=8.762989

Batch 272090, train_perplexity=6627.852, train_loss=8.799036

Batch 272100, train_perplexity=5376.748, train_loss=8.589839

Batch 272110, train_perplexity=4642.102, train_loss=8.442923

Batch 272120, train_perplexity=5743.7007, train_loss=8.655859

Batch 272130, train_perplexity=4787.449, train_loss=8.473753

Batch 272140, train_perplexity=5177.039, train_loss=8.551989

Batch 272150, train_perplexity=5561.2866, train_loss=8.623585

Batch 272160, train_perplexity=6140.263, train_loss=8.722623

Batch 272170, train_perplexity=5044.984, train_loss=8.52615

Batch 272180, train_perplexity=5600.918, train_loss=8.630686

Batch 272190, train_perplexity=5089.1416, train_loss=8.534864

Batch 272200, train_perplexity=5906.881, train_loss=8.683873

Batch 272210, train_perplexity=5197.1924, train_loss=8.555874

Batch 272220, train_perplexity=5743.2134, train_loss=8.655774

Batch 272230, train_perplexity=5573.4985, train_loss=8.625778

Batch 272240, train_perplexity=4964.5522, train_loss=8.510078

Batch 272250, train_perplexity=5534.863, train_loss=8.618822

Batch 272260, train_perplexity=6173.95, train_loss=8.728094

Batch 272270, train_perplexity=4942.878, train_loss=8.505703

Batch 272280, train_perplexity=5212.6646, train_loss=8.558846

Batch 272290, train_perplexity=6028.5156, train_loss=8.704256

Batch 272300, train_perplexity=5747.35, train_loss=8.656494

Batch 272310, train_perplexity=5152.9526, train_loss=8.547325

Batch 272320, train_perplexity=5185.746, train_loss=8.553669

Batch 272330, train_perplexity=5401.7, train_loss=8.594469

Batch 272340, train_perplexity=5012.488, train_loss=8.519688

Batch 272350, train_perplexity=5718.9243, train_loss=8.651536

Batch 272360, train_perplexity=4866.5796, train_loss=8.490147

Batch 272370, train_perplexity=5702.2593, train_loss=8.648618
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 272380, train_perplexity=5658.3965, train_loss=8.640896

Batch 272390, train_perplexity=5387.09, train_loss=8.591761

Batch 272400, train_perplexity=6213.988, train_loss=8.734558

Batch 272410, train_perplexity=5866.5947, train_loss=8.67703

Batch 272420, train_perplexity=7150.1147, train_loss=8.874884

Batch 272430, train_perplexity=4985.3047, train_loss=8.51425

Batch 272440, train_perplexity=4880.2715, train_loss=8.492956

Batch 272450, train_perplexity=5202.608, train_loss=8.556915

Batch 272460, train_perplexity=5606.706, train_loss=8.631719

Batch 272470, train_perplexity=5813.8447, train_loss=8.667997

Batch 272480, train_perplexity=5086.104, train_loss=8.534267

Batch 272490, train_perplexity=5413.701, train_loss=8.596688

Batch 272500, train_perplexity=7273.574, train_loss=8.892003

Batch 272510, train_perplexity=4943.175, train_loss=8.505763

Batch 272520, train_perplexity=5880.245, train_loss=8.679354

Batch 272530, train_perplexity=4756.6846, train_loss=8.467306

Batch 272540, train_perplexity=5328.1074, train_loss=8.580751

Batch 272550, train_perplexity=6252.12, train_loss=8.740676

Batch 272560, train_perplexity=6301.523, train_loss=8.748547

Batch 272570, train_perplexity=5971.2734, train_loss=8.6947155

Batch 272580, train_perplexity=4288.561, train_loss=8.363707

Batch 272590, train_perplexity=6029.815, train_loss=8.704472

Batch 272600, train_perplexity=4757.0654, train_loss=8.467386

Batch 272610, train_perplexity=5723.6875, train_loss=8.652369

Batch 272620, train_perplexity=4397.515, train_loss=8.388795

Batch 272630, train_perplexity=5260.3013, train_loss=8.567944

Batch 272640, train_perplexity=5567.75, train_loss=8.624746

Batch 272650, train_perplexity=4455.4746, train_loss=8.401889

Batch 272660, train_perplexity=4941.464, train_loss=8.505417

Batch 272670, train_perplexity=5121.236, train_loss=8.541151

Batch 272680, train_perplexity=5913.5938, train_loss=8.685009

Batch 272690, train_perplexity=5254.3247, train_loss=8.566807

Batch 272700, train_perplexity=5416.1904, train_loss=8.597148

Batch 272710, train_perplexity=5254.6855, train_loss=8.566875

Batch 272720, train_perplexity=7064.158, train_loss=8.862789

Batch 272730, train_perplexity=5394.2207, train_loss=8.593083

Batch 272740, train_perplexity=5308.4893, train_loss=8.577063

Batch 272750, train_perplexity=5914.2534, train_loss=8.685121

Batch 272760, train_perplexity=5273.1694, train_loss=8.570387

Batch 272770, train_perplexity=5124.7095, train_loss=8.541829

Batch 272780, train_perplexity=5129.8877, train_loss=8.542839

Batch 272790, train_perplexity=5366.057, train_loss=8.587849

Batch 272800, train_perplexity=5200.7227, train_loss=8.556553

Batch 272810, train_perplexity=4148.4106, train_loss=8.330481

Batch 272820, train_perplexity=4861.3706, train_loss=8.489076

Batch 272830, train_perplexity=4628.222, train_loss=8.439928

Batch 272840, train_perplexity=4998.2056, train_loss=8.516834

Batch 272850, train_perplexity=5113.5933, train_loss=8.539658

Batch 272860, train_perplexity=5505.5767, train_loss=8.613517

Batch 272870, train_perplexity=5510.304, train_loss=8.614375

Batch 272880, train_perplexity=4814.3706, train_loss=8.479361

Batch 272890, train_perplexity=4197.519, train_loss=8.342249

Batch 272900, train_perplexity=5535.613, train_loss=8.6189575

Batch 272910, train_perplexity=4437.918, train_loss=8.397941

Batch 272920, train_perplexity=5323.6074, train_loss=8.579906

Batch 272930, train_perplexity=5052.6973, train_loss=8.527678

Batch 272940, train_perplexity=6126.687, train_loss=8.720409

Batch 272950, train_perplexity=6378.2847, train_loss=8.760654

Batch 272960, train_perplexity=6185.466, train_loss=8.729958

Batch 272970, train_perplexity=5862.8027, train_loss=8.676383

Batch 272980, train_perplexity=5452.2656, train_loss=8.603786

Batch 272990, train_perplexity=6012.508, train_loss=8.701597

Batch 273000, train_perplexity=4685.4585, train_loss=8.452219

Batch 273010, train_perplexity=5504.4478, train_loss=8.613312

Batch 273020, train_perplexity=5733.248, train_loss=8.654037

Batch 273030, train_perplexity=5361.3306, train_loss=8.586967

Batch 273040, train_perplexity=4802.4297, train_loss=8.476877

Batch 273050, train_perplexity=4993.9175, train_loss=8.515976

Batch 273060, train_perplexity=4474.4873, train_loss=8.406147

Batch 273070, train_perplexity=5860.628, train_loss=8.676012

Batch 273080, train_perplexity=5431.962, train_loss=8.600056

Batch 273090, train_perplexity=4799.87, train_loss=8.476344

Batch 273100, train_perplexity=4582.257, train_loss=8.429947

Batch 273110, train_perplexity=5090.088, train_loss=8.53505

Batch 273120, train_perplexity=4798.5977, train_loss=8.476079

Batch 273130, train_perplexity=5793.5376, train_loss=8.664498

Batch 273140, train_perplexity=6161.3037, train_loss=8.726044

Batch 273150, train_perplexity=5683.632, train_loss=8.645346

Batch 273160, train_perplexity=5856.7837, train_loss=8.675356

Batch 273170, train_perplexity=5520.7505, train_loss=8.616269

Batch 273180, train_perplexity=4855.81, train_loss=8.487931

Batch 273190, train_perplexity=5323.181, train_loss=8.579826

Batch 273200, train_perplexity=4530.112, train_loss=8.418502

Batch 273210, train_perplexity=6015.2954, train_loss=8.702061

Batch 273220, train_perplexity=5627.9146, train_loss=8.635494

Batch 273230, train_perplexity=5389.726, train_loss=8.59225

Batch 273240, train_perplexity=5199.6665, train_loss=8.55635

Batch 273250, train_perplexity=4588.2827, train_loss=8.431261

Batch 273260, train_perplexity=5425.0664, train_loss=8.598785

Batch 273270, train_perplexity=5731.34, train_loss=8.653705

Batch 273280, train_perplexity=5729.083, train_loss=8.653311

Batch 273290, train_perplexity=5229.27, train_loss=8.562027

Batch 273300, train_perplexity=5989.0103, train_loss=8.697681

Batch 273310, train_perplexity=6512.34, train_loss=8.781454

Batch 273320, train_perplexity=5172.5137, train_loss=8.551114

Batch 273330, train_perplexity=6159.741, train_loss=8.72579

Batch 273340, train_perplexity=4755.197, train_loss=8.466993

Batch 273350, train_perplexity=5191.783, train_loss=8.554832

Batch 273360, train_perplexity=4354.41, train_loss=8.378944

Batch 273370, train_perplexity=4627.242, train_loss=8.439716

Batch 273380, train_perplexity=5713.8926, train_loss=8.650656

Batch 273390, train_perplexity=4775.5293, train_loss=8.47126

Batch 273400, train_perplexity=5204.2505, train_loss=8.557231

Batch 273410, train_perplexity=5188.0015, train_loss=8.554104

Batch 273420, train_perplexity=5337.333, train_loss=8.582481

Batch 273430, train_perplexity=5038.3105, train_loss=8.524826

Batch 273440, train_perplexity=5831.959, train_loss=8.671108

Batch 273450, train_perplexity=4970.057, train_loss=8.511187

Batch 273460, train_perplexity=6233.824, train_loss=8.737745

Batch 273470, train_perplexity=6075.392, train_loss=8.712002

Batch 273480, train_perplexity=6520.0703, train_loss=8.78264

Batch 273490, train_perplexity=5080.994, train_loss=8.533262

Batch 273500, train_perplexity=4337.1606, train_loss=8.374975

Batch 273510, train_perplexity=4690.1973, train_loss=8.45323

Batch 273520, train_perplexity=5409.526, train_loss=8.595917

Batch 273530, train_perplexity=4968.863, train_loss=8.510946

Batch 273540, train_perplexity=4294.07, train_loss=8.36499

Batch 273550, train_perplexity=4855.4814, train_loss=8.487864

Batch 273560, train_perplexity=5803.657, train_loss=8.666244

Batch 273570, train_perplexity=4676.3115, train_loss=8.450265

Batch 273580, train_perplexity=5446.055, train_loss=8.602647

Batch 273590, train_perplexity=4959.404, train_loss=8.509041

Batch 273600, train_perplexity=4452.272, train_loss=8.40117

Batch 273610, train_perplexity=4482.4653, train_loss=8.407928

Batch 273620, train_perplexity=5025.258, train_loss=8.522232

Batch 273630, train_perplexity=6137.1543, train_loss=8.722116

Batch 273640, train_perplexity=5301.862, train_loss=8.575813

Batch 273650, train_perplexity=6243.6416, train_loss=8.739319

Batch 273660, train_perplexity=6181.3853, train_loss=8.729298

Batch 273670, train_perplexity=4960.416, train_loss=8.509245

Batch 273680, train_perplexity=5119.024, train_loss=8.540719

Batch 273690, train_perplexity=5149.9214, train_loss=8.546737
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 273700, train_perplexity=5353.299, train_loss=8.585468

Batch 273710, train_perplexity=5100.268, train_loss=8.537048

Batch 273720, train_perplexity=5285.2827, train_loss=8.572681

Batch 273730, train_perplexity=5195.1807, train_loss=8.555487

Batch 273740, train_perplexity=6341.4927, train_loss=8.754869

Batch 273750, train_perplexity=4367.0522, train_loss=8.381844

Batch 273760, train_perplexity=5377.4707, train_loss=8.589973

Batch 273770, train_perplexity=5207.6216, train_loss=8.5578785

Batch 273780, train_perplexity=6104.9897, train_loss=8.716862

Batch 273790, train_perplexity=5911.992, train_loss=8.684738

Batch 273800, train_perplexity=4848.2446, train_loss=8.486372

Batch 273810, train_perplexity=5936.8916, train_loss=8.688941

Batch 273820, train_perplexity=5059.2983, train_loss=8.528983

Batch 273830, train_perplexity=6120.041, train_loss=8.719324

Batch 273840, train_perplexity=5447.5615, train_loss=8.602923

Batch 273850, train_perplexity=5342.3745, train_loss=8.5834255

Batch 273860, train_perplexity=5375.1226, train_loss=8.589537

Batch 273870, train_perplexity=5488.759, train_loss=8.610457

Batch 273880, train_perplexity=5375.9736, train_loss=8.589695

Batch 273890, train_perplexity=5203.0146, train_loss=8.5569935

Batch 273900, train_perplexity=4764.2524, train_loss=8.468896

Batch 273910, train_perplexity=5411.812, train_loss=8.596339

Batch 273920, train_perplexity=5702.515, train_loss=8.648663

Batch 273930, train_perplexity=5059.7183, train_loss=8.529066

Batch 273940, train_perplexity=5330.852, train_loss=8.581266

Batch 273950, train_perplexity=6185.6074, train_loss=8.72998

Batch 273960, train_perplexity=5543.5107, train_loss=8.620383

Batch 273970, train_perplexity=6919.417, train_loss=8.842087

Batch 273980, train_perplexity=5734.976, train_loss=8.654339

Batch 273990, train_perplexity=5064.1885, train_loss=8.529949

Batch 274000, train_perplexity=4977.4854, train_loss=8.51268

Batch 274010, train_perplexity=5372.509, train_loss=8.58905

Batch 274020, train_perplexity=5985.1733, train_loss=8.697041

Batch 274030, train_perplexity=4826.4194, train_loss=8.48186

Batch 274040, train_perplexity=4434.9014, train_loss=8.397261

Batch 274050, train_perplexity=6016.54, train_loss=8.702268

Batch 274060, train_perplexity=6376.363, train_loss=8.760353

Batch 274070, train_perplexity=5602.713, train_loss=8.631006

Batch 274080, train_perplexity=4643.014, train_loss=8.443119

Batch 274090, train_perplexity=5619.5156, train_loss=8.634001

Batch 274100, train_perplexity=4534.304, train_loss=8.419427

Batch 274110, train_perplexity=4069.2559, train_loss=8.311215

Batch 274120, train_perplexity=5428.5234, train_loss=8.599422

Batch 274130, train_perplexity=4807.314, train_loss=8.477894

Batch 274140, train_perplexity=5638.799, train_loss=8.637426

Batch 274150, train_perplexity=5306.3335, train_loss=8.576656

Batch 274160, train_perplexity=5021.5645, train_loss=8.521497

Batch 274170, train_perplexity=5057.1616, train_loss=8.528561

Batch 274180, train_perplexity=5296.434, train_loss=8.574789

Batch 274190, train_perplexity=5746.303, train_loss=8.656312

Batch 274200, train_perplexity=6574.574, train_loss=8.790965

Batch 274210, train_perplexity=4913.6724, train_loss=8.499777

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Batch 274220, train_perplexity=5455.8076, train_loss=8.604436

Batch 274230, train_perplexity=5772.7725, train_loss=8.660908

Batch 274240, train_perplexity=5074.8975, train_loss=8.532062

Batch 274250, train_perplexity=5823.783, train_loss=8.669705

Batch 274260, train_perplexity=5341.03, train_loss=8.583174

Batch 274270, train_perplexity=5611.033, train_loss=8.63249

Batch 274280, train_perplexity=4522.743, train_loss=8.416874

Batch 274290, train_perplexity=5678.452, train_loss=8.644434

Batch 274300, train_perplexity=4643.187, train_loss=8.443156

Batch 274310, train_perplexity=5540.7676, train_loss=8.619888

Batch 274320, train_perplexity=4678.1445, train_loss=8.450657

Batch 274330, train_perplexity=5357.467, train_loss=8.5862465

Batch 274340, train_perplexity=4497.358, train_loss=8.411245

Batch 274350, train_perplexity=4039.079, train_loss=8.303772

Batch 274360, train_perplexity=6005.7056, train_loss=8.700465

Batch 274370, train_perplexity=5703.668, train_loss=8.648865

Batch 274380, train_perplexity=5470.839, train_loss=8.607187

Batch 274390, train_perplexity=5916.5044, train_loss=8.685501

Batch 274400, train_perplexity=3646.1443, train_loss=8.201426

Batch 274410, train_perplexity=5411.3833, train_loss=8.59626

Batch 274420, train_perplexity=5251.73, train_loss=8.566313

Batch 274430, train_perplexity=6497.2407, train_loss=8.779133

Batch 274440, train_perplexity=6203.46, train_loss=8.732862

Batch 274450, train_perplexity=5510.9717, train_loss=8.614496

Batch 274460, train_perplexity=5496.3174, train_loss=8.611834

Batch 274470, train_perplexity=4633.65, train_loss=8.4411

Batch 274480, train_perplexity=5162.2144, train_loss=8.549121

Batch 274490, train_perplexity=4512.855, train_loss=8.414685

Batch 274500, train_perplexity=5505.3403, train_loss=8.613474

Batch 274510, train_perplexity=5499.5996, train_loss=8.612431

Batch 274520, train_perplexity=6130.3516, train_loss=8.721007

Batch 274530, train_perplexity=5380.898, train_loss=8.5906105

Batch 274540, train_perplexity=5677.039, train_loss=8.644185

Batch 274550, train_perplexity=6190.895, train_loss=8.730835

Batch 274560, train_perplexity=5407.6074, train_loss=8.595562

Batch 274570, train_perplexity=5988.039, train_loss=8.697519

Batch 274580, train_perplexity=5564.884, train_loss=8.624231

Batch 274590, train_perplexity=5737.8535, train_loss=8.65484

Batch 274600, train_perplexity=6598.7896, train_loss=8.7946415

Batch 274610, train_perplexity=5489.874, train_loss=8.610661

Batch 274620, train_perplexity=5661.711, train_loss=8.641481

Batch 274630, train_perplexity=6162.2324, train_loss=8.726194

Batch 274640, train_perplexity=5197.634, train_loss=8.555959

Batch 274650, train_perplexity=5595.2803, train_loss=8.629679

Batch 274660, train_perplexity=4535.5366, train_loss=8.419699

Batch 274670, train_perplexity=5783.744, train_loss=8.6628065

Batch 274680, train_perplexity=4452.9727, train_loss=8.401327

Batch 274690, train_perplexity=4865.939, train_loss=8.490015

Batch 274700, train_perplexity=5356.3784, train_loss=8.586043

Batch 274710, train_perplexity=5390.163, train_loss=8.592331

Batch 274720, train_perplexity=6809.952, train_loss=8.82614

Batch 274730, train_perplexity=5684.651, train_loss=8.645525

Batch 274740, train_perplexity=5154.2007, train_loss=8.547567

Batch 274750, train_perplexity=5001.906, train_loss=8.517574

Batch 274760, train_perplexity=5203.248, train_loss=8.557038

Batch 274770, train_perplexity=4903.112, train_loss=8.497625

Batch 274780, train_perplexity=5249.021, train_loss=8.565797

Batch 274790, train_perplexity=5452.91, train_loss=8.603905

Batch 274800, train_perplexity=6288.1777, train_loss=8.746427

Batch 274810, train_perplexity=4732.5986, train_loss=8.46223

Batch 274820, train_perplexity=6983.61, train_loss=8.851321

Batch 274830, train_perplexity=5362.6396, train_loss=8.587212

Batch 274840, train_perplexity=5113.047, train_loss=8.539551

Batch 274850, train_perplexity=6457.5454, train_loss=8.773005

Batch 274860, train_perplexity=4632.0195, train_loss=8.440748

Batch 274870, train_perplexity=5424.5024, train_loss=8.598681

Batch 274880, train_perplexity=4714.854, train_loss=8.458473

Batch 274890, train_perplexity=5279.711, train_loss=8.571627

Batch 274900, train_perplexity=4962.99, train_loss=8.509764

Batch 274910, train_perplexity=5967.7437, train_loss=8.694124

Batch 274920, train_perplexity=4797.5176, train_loss=8.475854

Batch 274930, train_perplexity=4730.1665, train_loss=8.461716

Batch 274940, train_perplexity=5329.063, train_loss=8.580931
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 274950, train_perplexity=5152.294, train_loss=8.547197

Batch 274960, train_perplexity=5054.3555, train_loss=8.528006

Batch 274970, train_perplexity=4469.233, train_loss=8.404972

Batch 274980, train_perplexity=4404.7974, train_loss=8.39045

Batch 274990, train_perplexity=5104.7495, train_loss=8.537927

Batch 275000, train_perplexity=4847.154, train_loss=8.486147

Batch 275010, train_perplexity=5512.4644, train_loss=8.614767

Batch 275020, train_perplexity=5088.9863, train_loss=8.534834

Batch 275030, train_perplexity=5607.7485, train_loss=8.631905

Batch 275040, train_perplexity=5245.783, train_loss=8.56518

Batch 275050, train_perplexity=5044.835, train_loss=8.52612

Batch 275060, train_perplexity=4473.476, train_loss=8.405921

Batch 275070, train_perplexity=4997.929, train_loss=8.516779

Batch 275080, train_perplexity=5170.605, train_loss=8.550745

Batch 275090, train_perplexity=4380.851, train_loss=8.384998

Batch 275100, train_perplexity=5266.907, train_loss=8.569199

Batch 275110, train_perplexity=4790.0522, train_loss=8.474297

Batch 275120, train_perplexity=4503.564, train_loss=8.412624

Batch 275130, train_perplexity=7237.641, train_loss=8.887051

Batch 275140, train_perplexity=5693.755, train_loss=8.647125

Batch 275150, train_perplexity=6337.6836, train_loss=8.754269

Batch 275160, train_perplexity=5436.02, train_loss=8.600802

Batch 275170, train_perplexity=4917.413, train_loss=8.500538

Batch 275180, train_perplexity=5330.9185, train_loss=8.581279

Batch 275190, train_perplexity=4745.2573, train_loss=8.464901

Batch 275200, train_perplexity=5525.2437, train_loss=8.617083

Batch 275210, train_perplexity=6024.1016, train_loss=8.703524

Batch 275220, train_perplexity=4867.5684, train_loss=8.49035

Batch 275230, train_perplexity=6193.8716, train_loss=8.731316

Batch 275240, train_perplexity=3895.466, train_loss=8.267569

Batch 275250, train_perplexity=5364.297, train_loss=8.587521

Batch 275260, train_perplexity=5582.137, train_loss=8.627327

Batch 275270, train_perplexity=4820.4165, train_loss=8.480616

Batch 275280, train_perplexity=5321.7954, train_loss=8.579566

Batch 275290, train_perplexity=4899.111, train_loss=8.496809

Batch 275300, train_perplexity=6049.479, train_loss=8.707727

Batch 275310, train_perplexity=5520.656, train_loss=8.616252

Batch 275320, train_perplexity=5132.5596, train_loss=8.54336

Batch 275330, train_perplexity=5562.034, train_loss=8.623719

Batch 275340, train_perplexity=6003.707, train_loss=8.700132

Batch 275350, train_perplexity=5290.3457, train_loss=8.573639

Batch 275360, train_perplexity=5391.4795, train_loss=8.592575

Batch 275370, train_perplexity=4889.7754, train_loss=8.494902

Batch 275380, train_perplexity=5228.298, train_loss=8.561841

Batch 275390, train_perplexity=4785.3037, train_loss=8.473305

Batch 275400, train_perplexity=5012.005, train_loss=8.519591

Batch 275410, train_perplexity=4786.595, train_loss=8.473575

Batch 275420, train_perplexity=4904.936, train_loss=8.497997

Batch 275430, train_perplexity=5442.4155, train_loss=8.601978

Batch 275440, train_perplexity=5807.793, train_loss=8.666956

Batch 275450, train_perplexity=4462.2397, train_loss=8.403406

Batch 275460, train_perplexity=4785.5454, train_loss=8.473355

Batch 275470, train_perplexity=5226.792, train_loss=8.561553

Batch 275480, train_perplexity=7080.3047, train_loss=8.865072

Batch 275490, train_perplexity=4237.317, train_loss=8.351686

Batch 275500, train_perplexity=5212.6997, train_loss=8.558853

Batch 275510, train_perplexity=5593.152, train_loss=8.629298

Batch 275520, train_perplexity=4564.266, train_loss=8.426013

Batch 275530, train_perplexity=5668.7236, train_loss=8.642719

Batch 275540, train_perplexity=4870.7305, train_loss=8.490999

Batch 275550, train_perplexity=5217.947, train_loss=8.559859

Batch 275560, train_perplexity=5902.466, train_loss=8.6831255

Batch 275570, train_perplexity=5493.0215, train_loss=8.611234

Batch 275580, train_perplexity=4691.3647, train_loss=8.453479

Batch 275590, train_perplexity=6251.083, train_loss=8.74051

Batch 275600, train_perplexity=4835.565, train_loss=8.483753

Batch 275610, train_perplexity=5910.2505, train_loss=8.684443

Batch 275620, train_perplexity=5570.9795, train_loss=8.625326

Batch 275630, train_perplexity=5056.4478, train_loss=8.5284195

Batch 275640, train_perplexity=5352.7173, train_loss=8.58536

Batch 275650, train_perplexity=5504.0176, train_loss=8.613234

Batch 275660, train_perplexity=5562.114, train_loss=8.6237335

Batch 275670, train_perplexity=6899.4775, train_loss=8.839201

Batch 275680, train_perplexity=6362.3916, train_loss=8.75816

Batch 275690, train_perplexity=4958.1367, train_loss=8.508785

Batch 275700, train_perplexity=5027.434, train_loss=8.522665

Batch 275710, train_perplexity=4882.781, train_loss=8.49347

Batch 275720, train_perplexity=5281.0356, train_loss=8.5718775

Batch 275730, train_perplexity=5930.175, train_loss=8.687809

Batch 275740, train_perplexity=5378.4043, train_loss=8.590147

Batch 275750, train_perplexity=5215.519, train_loss=8.559394

Batch 275760, train_perplexity=4730.1577, train_loss=8.461714

Batch 275770, train_perplexity=4569.022, train_loss=8.427054

Batch 275780, train_perplexity=4805.563, train_loss=8.47753

Batch 275790, train_perplexity=6119.8193, train_loss=8.719288

Batch 275800, train_perplexity=7038.752, train_loss=8.859186

Batch 275810, train_perplexity=5663.3633, train_loss=8.641773

Batch 275820, train_perplexity=5444.8193, train_loss=8.60242

Batch 275830, train_perplexity=5092.5786, train_loss=8.53554

Batch 275840, train_perplexity=5022.029, train_loss=8.521589

Batch 275850, train_perplexity=4914.0894, train_loss=8.499862

Batch 275860, train_perplexity=4847.995, train_loss=8.4863205

Batch 275870, train_perplexity=4978.2544, train_loss=8.512835

Batch 275880, train_perplexity=6001.915, train_loss=8.699834

Batch 275890, train_perplexity=5228.3823, train_loss=8.561857

Batch 275900, train_perplexity=4768.38, train_loss=8.469762

Batch 275910, train_perplexity=5500.2607, train_loss=8.612551

Batch 275920, train_perplexity=5624.009, train_loss=8.6348

Batch 275930, train_perplexity=5271.661, train_loss=8.570101

Batch 275940, train_perplexity=4488.728, train_loss=8.409325

Batch 275950, train_perplexity=5501.1577, train_loss=8.612714

Batch 275960, train_perplexity=6069.763, train_loss=8.711075

Batch 275970, train_perplexity=5135.8647, train_loss=8.5440035

Batch 275980, train_perplexity=5060.447, train_loss=8.52921

Batch 275990, train_perplexity=4832.416, train_loss=8.483102

Batch 276000, train_perplexity=5090.588, train_loss=8.535149

Batch 276010, train_perplexity=5686.5703, train_loss=8.645863

Batch 276020, train_perplexity=6191.4146, train_loss=8.730919

Batch 276030, train_perplexity=5532.6675, train_loss=8.618425

Batch 276040, train_perplexity=5231.485, train_loss=8.56245

Batch 276050, train_perplexity=5486.0166, train_loss=8.609958

Batch 276060, train_perplexity=5411.363, train_loss=8.596256

Batch 276070, train_perplexity=5228.0835, train_loss=8.5618

Batch 276080, train_perplexity=5167.692, train_loss=8.550181

Batch 276090, train_perplexity=4462.8613, train_loss=8.403545

Batch 276100, train_perplexity=6948.1484, train_loss=8.8462305

Batch 276110, train_perplexity=4819.989, train_loss=8.480527

Batch 276120, train_perplexity=4700.5454, train_loss=8.455434

Batch 276130, train_perplexity=5153.139, train_loss=8.547361

Batch 276140, train_perplexity=5064.5747, train_loss=8.5300255

Batch 276150, train_perplexity=4818.6104, train_loss=8.480241

Batch 276160, train_perplexity=5354.2183, train_loss=8.58564

Batch 276170, train_perplexity=6145.0317, train_loss=8.723399

Batch 276180, train_perplexity=5866.2925, train_loss=8.676978

Batch 276190, train_perplexity=4733.271, train_loss=8.462372

Batch 276200, train_perplexity=5665.6646, train_loss=8.6421795

Batch 276210, train_perplexity=5202.2256, train_loss=8.556842

Batch 276220, train_perplexity=4716.689, train_loss=8.458862

Batch 276230, train_perplexity=6029.2573, train_loss=8.704379

Batch 276240, train_perplexity=5544.5415, train_loss=8.620569

Batch 276250, train_perplexity=4716.446, train_loss=8.458811

Batch 276260, train_perplexity=5517.082, train_loss=8.615604
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 276270, train_perplexity=5476.6646, train_loss=8.608252

Batch 276280, train_perplexity=5101.2554, train_loss=8.537242

Batch 276290, train_perplexity=5859.89, train_loss=8.675886

Batch 276300, train_perplexity=4948.769, train_loss=8.506894

Batch 276310, train_perplexity=5664.6167, train_loss=8.641994

Batch 276320, train_perplexity=5190.0405, train_loss=8.554497

Batch 276330, train_perplexity=5304.6436, train_loss=8.576338

Batch 276340, train_perplexity=5396.5615, train_loss=8.593517

Batch 276350, train_perplexity=5002.2974, train_loss=8.5176525

Batch 276360, train_perplexity=5517.6187, train_loss=8.615702

Batch 276370, train_perplexity=5426.857, train_loss=8.599115

Batch 276380, train_perplexity=4332.13, train_loss=8.373815

Batch 276390, train_perplexity=5496.4536, train_loss=8.611858

Batch 276400, train_perplexity=5250.0522, train_loss=8.565993

Batch 276410, train_perplexity=6194.994, train_loss=8.731497

Batch 276420, train_perplexity=5634.5576, train_loss=8.636674

Batch 276430, train_perplexity=5419.409, train_loss=8.597742

Batch 276440, train_perplexity=5892.438, train_loss=8.681425

Batch 276450, train_perplexity=5156.925, train_loss=8.548096

Batch 276460, train_perplexity=5175.134, train_loss=8.5516205

Batch 276470, train_perplexity=4140.1187, train_loss=8.32848

Batch 276480, train_perplexity=5798.1304, train_loss=8.665291

Batch 276490, train_perplexity=5553.9087, train_loss=8.622257

Batch 276500, train_perplexity=5336.712, train_loss=8.582365

Batch 276510, train_perplexity=4885.9062, train_loss=8.49411

Batch 276520, train_perplexity=5113.4272, train_loss=8.539625

Batch 276530, train_perplexity=5549.8423, train_loss=8.621525

Batch 276540, train_perplexity=5390.497, train_loss=8.592393

Batch 276550, train_perplexity=4879.4897, train_loss=8.492796

Batch 276560, train_perplexity=4616.06, train_loss=8.437297

Batch 276570, train_perplexity=4904.777, train_loss=8.497965

Batch 276580, train_perplexity=5642.904, train_loss=8.638154

Batch 276590, train_perplexity=4713.793, train_loss=8.458248

Batch 276600, train_perplexity=4748.0957, train_loss=8.465499

Batch 276610, train_perplexity=5581.3174, train_loss=8.62718

Batch 276620, train_perplexity=4799.0234, train_loss=8.476168

Batch 276630, train_perplexity=5086.798, train_loss=8.534404

Batch 276640, train_perplexity=7464.512, train_loss=8.917915

Batch 276650, train_perplexity=5452.4316, train_loss=8.603817

Batch 276660, train_perplexity=4052.6143, train_loss=8.307117

Batch 276670, train_perplexity=6259.1064, train_loss=8.741793

Batch 276680, train_perplexity=5570.6714, train_loss=8.625271

Batch 276690, train_perplexity=5934.509, train_loss=8.6885395

Batch 276700, train_perplexity=5305.7666, train_loss=8.57655

Batch 276710, train_perplexity=5099.8594, train_loss=8.536968

Batch 276720, train_perplexity=4789.4673, train_loss=8.4741745

Batch 276730, train_perplexity=5788.219, train_loss=8.66358

Batch 276740, train_perplexity=5827.378, train_loss=8.670322

Batch 276750, train_perplexity=4311.02, train_loss=8.36893

Batch 276760, train_perplexity=5525.096, train_loss=8.617056

Batch 276770, train_perplexity=5284.93, train_loss=8.572615

Batch 276780, train_perplexity=4557.885, train_loss=8.424614

Batch 276790, train_perplexity=6934.6045, train_loss=8.844279

Batch 276800, train_perplexity=5868.318, train_loss=8.677323

Batch 276810, train_perplexity=7094.2417, train_loss=8.867039

Batch 276820, train_perplexity=5801.499, train_loss=8.665872

Batch 276830, train_perplexity=5726.86, train_loss=8.652923

Batch 276840, train_perplexity=6116.575, train_loss=8.718758

Batch 276850, train_perplexity=4655.8687, train_loss=8.445884

Batch 276860, train_perplexity=5219.943, train_loss=8.560242

Batch 276870, train_perplexity=4877.2476, train_loss=8.492336

Batch 276880, train_perplexity=4830.559, train_loss=8.4827175

Batch 276890, train_perplexity=4933.9575, train_loss=8.503897

Batch 276900, train_perplexity=5053.3433, train_loss=8.527805

Batch 276910, train_perplexity=5011.236, train_loss=8.519438

Batch 276920, train_perplexity=5603.536, train_loss=8.631153

Batch 276930, train_perplexity=5620.6465, train_loss=8.634202

Batch 276940, train_perplexity=5628.709, train_loss=8.635635

Batch 276950, train_perplexity=6610.152, train_loss=8.796362

Batch 276960, train_perplexity=4686.6426, train_loss=8.452472

Batch 276970, train_perplexity=5827.8115, train_loss=8.670397

Batch 276980, train_perplexity=4702.7246, train_loss=8.455897

Batch 276990, train_perplexity=4787.1294, train_loss=8.473686

Batch 277000, train_perplexity=4979.166, train_loss=8.513018

Batch 277010, train_perplexity=4726.821, train_loss=8.461008

Batch 277020, train_perplexity=5608.9683, train_loss=8.632122

Batch 277030, train_perplexity=4936.226, train_loss=8.504356

Batch 277040, train_perplexity=4446.365, train_loss=8.399842

Batch 277050, train_perplexity=5815.6914, train_loss=8.668315

Batch 277060, train_perplexity=5311.1025, train_loss=8.577555

Batch 277070, train_perplexity=5732.067, train_loss=8.6538315

Batch 277080, train_perplexity=6051.637, train_loss=8.708084

Batch 277090, train_perplexity=5612.029, train_loss=8.632668

Batch 277100, train_perplexity=4967.337, train_loss=8.510639

Batch 277110, train_perplexity=5137.6284, train_loss=8.544347

Batch 277120, train_perplexity=4713.087, train_loss=8.458098

Batch 277130, train_perplexity=5275.141, train_loss=8.570761

Batch 277140, train_perplexity=5299.1523, train_loss=8.575302

Batch 277150, train_perplexity=5266.636, train_loss=8.569147

Batch 277160, train_perplexity=4725.1216, train_loss=8.460649

Batch 277170, train_perplexity=6047.933, train_loss=8.707472

Batch 277180, train_perplexity=4674.069, train_loss=8.449785

Batch 277190, train_perplexity=4189.0093, train_loss=8.3402195

Batch 277200, train_perplexity=5199.508, train_loss=8.556319

Batch 277210, train_perplexity=5604.161, train_loss=8.631265

Batch 277220, train_perplexity=6409.2603, train_loss=8.765499

Batch 277230, train_perplexity=4340.0073, train_loss=8.375631

Batch 277240, train_perplexity=5723.6, train_loss=8.652353

Batch 277250, train_perplexity=5819.5137, train_loss=8.668972

Batch 277260, train_perplexity=5419.1973, train_loss=8.597703

Batch 277270, train_perplexity=6331.105, train_loss=8.75323

Batch 277280, train_perplexity=4928.2344, train_loss=8.502736

Batch 277290, train_perplexity=4982.396, train_loss=8.513666

Batch 277300, train_perplexity=5994.084, train_loss=8.698528

Batch 277310, train_perplexity=5161.141, train_loss=8.548913

Batch 277320, train_perplexity=5421.0425, train_loss=8.598043

Batch 277330, train_perplexity=4647.36, train_loss=8.444055

Batch 277340, train_perplexity=5223.07, train_loss=8.560841

Batch 277350, train_perplexity=4548.6963, train_loss=8.422596

Batch 277360, train_perplexity=5144.041, train_loss=8.545594

Batch 277370, train_perplexity=4770.058, train_loss=8.470114

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Batch 277380, train_perplexity=4689.732, train_loss=8.453131

Batch 277390, train_perplexity=5304.6533, train_loss=8.57634

Batch 277400, train_perplexity=5401.963, train_loss=8.594518

Batch 277410, train_perplexity=5572.6055, train_loss=8.625618

Batch 277420, train_perplexity=6130.3223, train_loss=8.721003

Batch 277430, train_perplexity=4947.9146, train_loss=8.5067215

Batch 277440, train_perplexity=4867.3037, train_loss=8.490295

Batch 277450, train_perplexity=5029.803, train_loss=8.523136

Batch 277460, train_perplexity=6315.059, train_loss=8.750692

Batch 277470, train_perplexity=6231.34, train_loss=8.737347

Batch 277480, train_perplexity=5957.5312, train_loss=8.692411

Batch 277490, train_perplexity=5248.1147, train_loss=8.565624

Batch 277500, train_perplexity=5168.318, train_loss=8.5503025

Batch 277510, train_perplexity=6521.7744, train_loss=8.782902
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 277520, train_perplexity=6297.2935, train_loss=8.747875

Batch 277530, train_perplexity=4815.941, train_loss=8.479687

Batch 277540, train_perplexity=5392.5796, train_loss=8.592779

Batch 277550, train_perplexity=5377.235, train_loss=8.58993

Batch 277560, train_perplexity=5874.4326, train_loss=8.678365

Batch 277570, train_perplexity=5222.1333, train_loss=8.560661

Batch 277580, train_perplexity=4976.9536, train_loss=8.512573

Batch 277590, train_perplexity=5516.698, train_loss=8.615535

Batch 277600, train_perplexity=4468.598, train_loss=8.40483

Batch 277610, train_perplexity=5183.6987, train_loss=8.553274

Batch 277620, train_perplexity=4572.7534, train_loss=8.427871

Batch 277630, train_perplexity=5070.8384, train_loss=8.531261

Batch 277640, train_perplexity=6279.8477, train_loss=8.745101

Batch 277650, train_perplexity=6379.209, train_loss=8.760799

Batch 277660, train_perplexity=4947.9243, train_loss=8.506723

Batch 277670, train_perplexity=4745.04, train_loss=8.464855

Batch 277680, train_perplexity=6526.727, train_loss=8.783661

Batch 277690, train_perplexity=4734.436, train_loss=8.462618

Batch 277700, train_perplexity=5480.625, train_loss=8.608974

Batch 277710, train_perplexity=5772.123, train_loss=8.660795

Batch 277720, train_perplexity=4791.1484, train_loss=8.474525

Batch 277730, train_perplexity=4586.7383, train_loss=8.430924

Batch 277740, train_perplexity=5090.481, train_loss=8.535128

Batch 277750, train_perplexity=4702.7876, train_loss=8.455911

Batch 277760, train_perplexity=5645.7295, train_loss=8.638655

Batch 277770, train_perplexity=4805.1006, train_loss=8.477433

Batch 277780, train_perplexity=6426.575, train_loss=8.768197

Batch 277790, train_perplexity=4787.951, train_loss=8.473858

Batch 277800, train_perplexity=5183.9014, train_loss=8.553313

Batch 277810, train_perplexity=5244.713, train_loss=8.564976

Batch 277820, train_perplexity=4739.6494, train_loss=8.463718

Batch 277830, train_perplexity=5709.001, train_loss=8.649799

Batch 277840, train_perplexity=4749.314, train_loss=8.465755

Batch 277850, train_perplexity=5498.2783, train_loss=8.61219

Batch 277860, train_perplexity=5385.559, train_loss=8.591476

Batch 277870, train_perplexity=5486.043, train_loss=8.609962

Batch 277880, train_perplexity=5550.5835, train_loss=8.621658

Batch 277890, train_perplexity=4589.3984, train_loss=8.431504

Batch 277900, train_perplexity=4981.075, train_loss=8.513401

Batch 277910, train_perplexity=5852.5684, train_loss=8.674636

Batch 277920, train_perplexity=5683.5884, train_loss=8.645338

Batch 277930, train_perplexity=5492.0786, train_loss=8.611062

Batch 277940, train_perplexity=5890.1455, train_loss=8.681036

Batch 277950, train_perplexity=5107.569, train_loss=8.538479

Batch 277960, train_perplexity=6204.637, train_loss=8.733052

Batch 277970, train_perplexity=6317.4204, train_loss=8.751066

Batch 277980, train_perplexity=7056.2534, train_loss=8.86167

Batch 277990, train_perplexity=5260.838, train_loss=8.568046

Batch 278000, train_perplexity=5310.748, train_loss=8.577488

Batch 278010, train_perplexity=5110.673, train_loss=8.539086

Batch 278020, train_perplexity=5524.527, train_loss=8.616953

Batch 278030, train_perplexity=5924.8384, train_loss=8.686909

Batch 278040, train_perplexity=8853.007, train_loss=9.088512

Batch 278050, train_perplexity=4518.781, train_loss=8.4159975

Batch 278060, train_perplexity=5212.312, train_loss=8.558779

Batch 278070, train_perplexity=5599.0273, train_loss=8.630348

Batch 278080, train_perplexity=5306.662, train_loss=8.576718

Batch 278090, train_perplexity=5423.1626, train_loss=8.598434

Batch 278100, train_perplexity=5572.6797, train_loss=8.625631

Batch 278110, train_perplexity=5653.3804, train_loss=8.640009

Batch 278120, train_perplexity=4535.0396, train_loss=8.419589

Batch 278130, train_perplexity=5338.2188, train_loss=8.582647

Batch 278140, train_perplexity=6404.7876, train_loss=8.764801

Batch 278150, train_perplexity=6707.9106, train_loss=8.811043

Batch 278160, train_perplexity=6773.9707, train_loss=8.820843

Batch 278170, train_perplexity=4610.3804, train_loss=8.436066

Batch 278180, train_perplexity=4939.0894, train_loss=8.504936

Batch 278190, train_perplexity=5184.8853, train_loss=8.553503

Batch 278200, train_perplexity=5393.7114, train_loss=8.592989

Batch 278210, train_perplexity=5522.4355, train_loss=8.616574

Batch 278220, train_perplexity=4485.035, train_loss=8.408502

Batch 278230, train_perplexity=6400.971, train_loss=8.764205

Batch 278240, train_perplexity=6426.2505, train_loss=8.7681465

Batch 278250, train_perplexity=4734.63, train_loss=8.462659

Batch 278260, train_perplexity=5063.7827, train_loss=8.529869

Batch 278270, train_perplexity=5553.056, train_loss=8.622104

Batch 278280, train_perplexity=5858.1133, train_loss=8.675583

Batch 278290, train_perplexity=5100.5986, train_loss=8.537113

Batch 278300, train_perplexity=5657.6304, train_loss=8.64076

Batch 278310, train_perplexity=4593.891, train_loss=8.432483

Batch 278320, train_perplexity=5742.4795, train_loss=8.655646

Batch 278330, train_perplexity=5458.956, train_loss=8.605013

Batch 278340, train_perplexity=6360.0864, train_loss=8.757797

Batch 278350, train_perplexity=5128.484, train_loss=8.542565

Batch 278360, train_perplexity=4868.4087, train_loss=8.490522

Batch 278370, train_perplexity=5845.3896, train_loss=8.6734085

Batch 278380, train_perplexity=5128.0483, train_loss=8.54248

Batch 278390, train_perplexity=4442.9067, train_loss=8.399064

Batch 278400, train_perplexity=4861.7134, train_loss=8.489146

Batch 278410, train_perplexity=5367.8896, train_loss=8.58819

Batch 278420, train_perplexity=6366.6953, train_loss=8.758836

Batch 278430, train_perplexity=4718.9834, train_loss=8.459349

Batch 278440, train_perplexity=4809.6895, train_loss=8.478388

Batch 278450, train_perplexity=4190.6475, train_loss=8.3406105

Batch 278460, train_perplexity=5245.293, train_loss=8.565086

Batch 278470, train_perplexity=5470.1763, train_loss=8.607066

Batch 278480, train_perplexity=5195.0864, train_loss=8.555469

Batch 278490, train_perplexity=5373.221, train_loss=8.589183

Batch 278500, train_perplexity=5424.756, train_loss=8.598728

Batch 278510, train_perplexity=5048.5166, train_loss=8.52685

Batch 278520, train_perplexity=5322.8154, train_loss=8.579758

Batch 278530, train_perplexity=5563.5356, train_loss=8.623989

Batch 278540, train_perplexity=5050.221, train_loss=8.527187

Batch 278550, train_perplexity=4539.8813, train_loss=8.420656

Batch 278560, train_perplexity=4250.5396, train_loss=8.354801

Batch 278570, train_perplexity=4404.898, train_loss=8.390472

Batch 278580, train_perplexity=5874.612, train_loss=8.678395

Batch 278590, train_perplexity=4697.0996, train_loss=8.4547

Batch 278600, train_perplexity=5116.8374, train_loss=8.540292

Batch 278610, train_perplexity=4761.0957, train_loss=8.468233

Batch 278620, train_perplexity=5635.439, train_loss=8.63683

Batch 278630, train_perplexity=5921.7544, train_loss=8.686388

Batch 278640, train_perplexity=5532.177, train_loss=8.618337

Batch 278650, train_perplexity=5787.623, train_loss=8.663477

Batch 278660, train_perplexity=4614.2603, train_loss=8.436907

Batch 278670, train_perplexity=6149.305, train_loss=8.724094

Batch 278680, train_perplexity=4690.054, train_loss=8.453199

Batch 278690, train_perplexity=4934.334, train_loss=8.503973

Batch 278700, train_perplexity=5469.5244, train_loss=8.606947

Batch 278710, train_perplexity=5630.674, train_loss=8.635984

Batch 278720, train_perplexity=5992.621, train_loss=8.698284

Batch 278730, train_perplexity=5925.9067, train_loss=8.687089

Batch 278740, train_perplexity=5330.654, train_loss=8.581229

Batch 278750, train_perplexity=6381.497, train_loss=8.761158

Batch 278760, train_perplexity=5044.1274, train_loss=8.52598

Batch 278770, train_perplexity=4864.2734, train_loss=8.489673

Batch 278780, train_perplexity=5697.101, train_loss=8.647713

Batch 278790, train_perplexity=4932.264, train_loss=8.503553

Batch 278800, train_perplexity=5142.29, train_loss=8.545254

Batch 278810, train_perplexity=4982.8047, train_loss=8.513748

Batch 278820, train_perplexity=7854.3926, train_loss=8.968828

Batch 278830, train_perplexity=4982.7666, train_loss=8.513741
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 278840, train_perplexity=4751.7783, train_loss=8.466274

Batch 278850, train_perplexity=5230.8364, train_loss=8.562326

Batch 278860, train_perplexity=5081.7163, train_loss=8.533404

Batch 278870, train_perplexity=6506.3433, train_loss=8.780533

Batch 278880, train_perplexity=4237.5146, train_loss=8.351732

Batch 278890, train_perplexity=5183.8325, train_loss=8.5533

Batch 278900, train_perplexity=5115.7295, train_loss=8.540075

Batch 278910, train_perplexity=6103.162, train_loss=8.716562

Batch 278920, train_perplexity=4974.0684, train_loss=8.511993

Batch 278930, train_perplexity=5263.081, train_loss=8.568472

Batch 278940, train_perplexity=5359.879, train_loss=8.586697

Batch 278950, train_perplexity=4591.316, train_loss=8.431922

Batch 278960, train_perplexity=5838.6704, train_loss=8.672258

Batch 278970, train_perplexity=5547.678, train_loss=8.621135

Batch 278980, train_perplexity=4771.3594, train_loss=8.4703865

Batch 278990, train_perplexity=5097.6323, train_loss=8.536531

Batch 279000, train_perplexity=5002.6885, train_loss=8.517731

Batch 279010, train_perplexity=4911.6855, train_loss=8.4993725

Batch 279020, train_perplexity=4927.572, train_loss=8.502602

Batch 279030, train_perplexity=5328.433, train_loss=8.580812

Batch 279040, train_perplexity=5606.5938, train_loss=8.631699

Batch 279050, train_perplexity=4751.792, train_loss=8.466277

Batch 279060, train_perplexity=5606.647, train_loss=8.631708

Batch 279070, train_perplexity=5139.4023, train_loss=8.544692

Batch 279080, train_perplexity=4994.737, train_loss=8.51614

Batch 279090, train_perplexity=5494.2734, train_loss=8.611462

Batch 279100, train_perplexity=5600.6455, train_loss=8.630637

Batch 279110, train_perplexity=5480.3223, train_loss=8.608919

Batch 279120, train_perplexity=6037.0996, train_loss=8.705679

Batch 279130, train_perplexity=5049.711, train_loss=8.527086

Batch 279140, train_perplexity=5612.66, train_loss=8.63278

Batch 279150, train_perplexity=5012.794, train_loss=8.519749

Batch 279160, train_perplexity=5945.249, train_loss=8.690348

Batch 279170, train_perplexity=4842.6904, train_loss=8.485226

Batch 279180, train_perplexity=6592.952, train_loss=8.7937565

Batch 279190, train_perplexity=5416.9185, train_loss=8.597282

Batch 279200, train_perplexity=5371.505, train_loss=8.588863

Batch 279210, train_perplexity=5191.372, train_loss=8.554753

Batch 279220, train_perplexity=5569.46, train_loss=8.625053

Batch 279230, train_perplexity=4977.452, train_loss=8.512673

Batch 279240, train_perplexity=4212.614, train_loss=8.345839

Batch 279250, train_perplexity=4850.5664, train_loss=8.486851

Batch 279260, train_perplexity=4968.4033, train_loss=8.510854

Batch 279270, train_perplexity=4956.983, train_loss=8.508553

Batch 279280, train_perplexity=6525.7437, train_loss=8.78351

Batch 279290, train_perplexity=5342.456, train_loss=8.583441

Batch 279300, train_perplexity=4862.9795, train_loss=8.489407

Batch 279310, train_perplexity=5783.286, train_loss=8.662727

Batch 279320, train_perplexity=5816.5566, train_loss=8.668464

Batch 279330, train_perplexity=5466.792, train_loss=8.606447

Batch 279340, train_perplexity=5033.5127, train_loss=8.523873

Batch 279350, train_perplexity=5536.7954, train_loss=8.619171

Batch 279360, train_perplexity=5004.602, train_loss=8.518113

Batch 279370, train_perplexity=5737.558, train_loss=8.654789

Batch 279380, train_perplexity=5366.7427, train_loss=8.587976

Batch 279390, train_perplexity=5140.677, train_loss=8.54494

Batch 279400, train_perplexity=5255.0464, train_loss=8.566944

Batch 279410, train_perplexity=5947.8804, train_loss=8.69079

Batch 279420, train_perplexity=4127.051, train_loss=8.325318

Batch 279430, train_perplexity=5443.459, train_loss=8.60217

Batch 279440, train_perplexity=5691.019, train_loss=8.646645

Batch 279450, train_perplexity=4914.947, train_loss=8.500036

Batch 279460, train_perplexity=5913.278, train_loss=8.684956

Batch 279470, train_perplexity=5536.447, train_loss=8.619108

Batch 279480, train_perplexity=5145.773, train_loss=8.545931

Batch 279490, train_perplexity=4571.449, train_loss=8.427586

Batch 279500, train_perplexity=5023.8013, train_loss=8.521942

Batch 279510, train_perplexity=6492.069, train_loss=8.778337

Batch 279520, train_perplexity=5259.1777, train_loss=8.56773

Batch 279530, train_perplexity=4776.6226, train_loss=8.471489

Batch 279540, train_perplexity=5455.688, train_loss=8.604414

Batch 279550, train_perplexity=4175.6475, train_loss=8.337025

Batch 279560, train_perplexity=5353.3755, train_loss=8.585483

Batch 279570, train_perplexity=5439.157, train_loss=8.601379

Batch 279580, train_perplexity=5444.2534, train_loss=8.602316

Batch 279590, train_perplexity=4703.4106, train_loss=8.456043

Batch 279600, train_perplexity=5346.0747, train_loss=8.584118

Batch 279610, train_perplexity=5522.599, train_loss=8.616604

Batch 279620, train_perplexity=5130.113, train_loss=8.542883

Batch 279630, train_perplexity=5671.5464, train_loss=8.643217

Batch 279640, train_perplexity=5059.4434, train_loss=8.529012

Batch 279650, train_perplexity=5605.439, train_loss=8.631493

Batch 279660, train_perplexity=5491.738, train_loss=8.611

Batch 279670, train_perplexity=5145.4, train_loss=8.545858

Batch 279680, train_perplexity=5332.5405, train_loss=8.581583

Batch 279690, train_perplexity=5785.708, train_loss=8.663146

Batch 279700, train_perplexity=5828.645, train_loss=8.67054

Batch 279710, train_perplexity=6116.552, train_loss=8.718754

Batch 279720, train_perplexity=6057.446, train_loss=8.7090435

Batch 279730, train_perplexity=5134.557, train_loss=8.543749

Batch 279740, train_perplexity=5933.8184, train_loss=8.688423

Batch 279750, train_perplexity=4950.7466, train_loss=8.507294

Batch 279760, train_perplexity=5312.673, train_loss=8.57785

Batch 279770, train_perplexity=5408.8555, train_loss=8.595793

Batch 279780, train_perplexity=4717.44, train_loss=8.459022

Batch 279790, train_perplexity=4506.829, train_loss=8.413349

Batch 279800, train_perplexity=6095.048, train_loss=8.715232

Batch 279810, train_perplexity=5206.38, train_loss=8.55764

Batch 279820, train_perplexity=5469.5713, train_loss=8.606956

Batch 279830, train_perplexity=4862.3765, train_loss=8.489283

Batch 279840, train_perplexity=6077.0728, train_loss=8.712278

Batch 279850, train_perplexity=5080.306, train_loss=8.533127

Batch 279860, train_perplexity=5039.305, train_loss=8.525023

Batch 279870, train_perplexity=5118.8726, train_loss=8.540689

Batch 279880, train_perplexity=4689.459, train_loss=8.453073

Batch 279890, train_perplexity=4309.117, train_loss=8.368488

Batch 279900, train_perplexity=6840.669, train_loss=8.830641

Batch 279910, train_perplexity=5594.416, train_loss=8.629524

Batch 279920, train_perplexity=4444.301, train_loss=8.399378

Batch 279930, train_perplexity=4983.893, train_loss=8.513967

Batch 279940, train_perplexity=5827.139, train_loss=8.670281

Batch 279950, train_perplexity=4993.8794, train_loss=8.515968

Batch 279960, train_perplexity=6226.255, train_loss=8.73653

Batch 279970, train_perplexity=5261.1743, train_loss=8.5681095

Batch 279980, train_perplexity=5505.9497, train_loss=8.6135845

Batch 279990, train_perplexity=5245.9634, train_loss=8.565214

Batch 280000, train_perplexity=5502.0073, train_loss=8.612868

Batch 280010, train_perplexity=5824.1333, train_loss=8.669765

Batch 280020, train_perplexity=4464.2446, train_loss=8.403855

Batch 280030, train_perplexity=5381.7803, train_loss=8.590775

Batch 280040, train_perplexity=5353.963, train_loss=8.585592

Batch 280050, train_perplexity=5321.973, train_loss=8.579599

Batch 280060, train_perplexity=6547.4937, train_loss=8.786838

Batch 280070, train_perplexity=6188.5337, train_loss=8.7304535

Batch 280080, train_perplexity=4957.3896, train_loss=8.508635

Batch 280090, train_perplexity=5291.5063, train_loss=8.573858

Batch 280100, train_perplexity=4969.9526, train_loss=8.511166

Batch 280110, train_perplexity=5502.3066, train_loss=8.612923

Batch 280120, train_perplexity=4907.238, train_loss=8.4984665

Batch 280130, train_perplexity=4957.404, train_loss=8.508637

Batch 280140, train_perplexity=6157.3623, train_loss=8.725404

Batch 280150, train_perplexity=5114.237, train_loss=8.5397835
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 280160, train_perplexity=4691.083, train_loss=8.453419

Batch 280170, train_perplexity=5849.706, train_loss=8.674147

Batch 280180, train_perplexity=6613.885, train_loss=8.7969265

Batch 280190, train_perplexity=6110.436, train_loss=8.717753

Batch 280200, train_perplexity=5955.384, train_loss=8.692051

Batch 280210, train_perplexity=4537.423, train_loss=8.4201145

Batch 280220, train_perplexity=5924.3696, train_loss=8.68683

Batch 280230, train_perplexity=4339.6763, train_loss=8.375555

Batch 280240, train_perplexity=5986.703, train_loss=8.697296

Batch 280250, train_perplexity=4921.0815, train_loss=8.501284

Batch 280260, train_perplexity=4375.056, train_loss=8.383675

Batch 280270, train_perplexity=5460.6123, train_loss=8.605316

Batch 280280, train_perplexity=5048.2373, train_loss=8.526794

Batch 280290, train_perplexity=5073.731, train_loss=8.531832

Batch 280300, train_perplexity=5365.878, train_loss=8.587815

Batch 280310, train_perplexity=5940.4316, train_loss=8.689537

Batch 280320, train_perplexity=5310.368, train_loss=8.577416

Batch 280330, train_perplexity=3933.9265, train_loss=8.277393

Batch 280340, train_perplexity=6492.88, train_loss=8.778461

Batch 280350, train_perplexity=4772.2783, train_loss=8.470579

Batch 280360, train_perplexity=5678.1816, train_loss=8.644386

Batch 280370, train_perplexity=5325.222, train_loss=8.58021

Batch 280380, train_perplexity=6261.3154, train_loss=8.742146

Batch 280390, train_perplexity=5376.076, train_loss=8.589714

Batch 280400, train_perplexity=5472.1333, train_loss=8.607424

Batch 280410, train_perplexity=5346.666, train_loss=8.5842285

Batch 280420, train_perplexity=5526.3184, train_loss=8.617277

Batch 280430, train_perplexity=5215.9517, train_loss=8.559477

Batch 280440, train_perplexity=4943.2783, train_loss=8.505784

Batch 280450, train_perplexity=5855.544, train_loss=8.675144

Batch 280460, train_perplexity=5230.6963, train_loss=8.5623

Batch 280470, train_perplexity=5530.457, train_loss=8.618026

Batch 280480, train_perplexity=5358.412, train_loss=8.586423

Batch 280490, train_perplexity=5686.8633, train_loss=8.645914

Batch 280500, train_perplexity=6021.046, train_loss=8.703016

Batch 280510, train_perplexity=4002.6392, train_loss=8.294709

Batch 280520, train_perplexity=4929.292, train_loss=8.502951

Batch 280530, train_perplexity=5771.9907, train_loss=8.660772

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Batch 280540, train_perplexity=7883.645, train_loss=8.972546

Batch 280550, train_perplexity=5252.7217, train_loss=8.566502

Batch 280560, train_perplexity=5668.075, train_loss=8.642605

Batch 280570, train_perplexity=6114.079, train_loss=8.718349

Batch 280580, train_perplexity=5832.905, train_loss=8.67127

Batch 280590, train_perplexity=6294.3633, train_loss=8.74741

Batch 280600, train_perplexity=5207.8696, train_loss=8.557926

Batch 280610, train_perplexity=5381.2725, train_loss=8.59068

Batch 280620, train_perplexity=5240.9375, train_loss=8.564256

Batch 280630, train_perplexity=4077.0051, train_loss=8.313118

Batch 280640, train_perplexity=5908.65, train_loss=8.684173

Batch 280650, train_perplexity=5405.5396, train_loss=8.59518

Batch 280660, train_perplexity=4606.065, train_loss=8.435129

Batch 280670, train_perplexity=4339.933, train_loss=8.375614

Batch 280680, train_perplexity=5846.276, train_loss=8.67356

Batch 280690, train_perplexity=5342.451, train_loss=8.58344

Batch 280700, train_perplexity=5270.0625, train_loss=8.5697975

Batch 280710, train_perplexity=4131.237, train_loss=8.326332

Batch 280720, train_perplexity=5033.652, train_loss=8.523901

Batch 280730, train_perplexity=5342.069, train_loss=8.583368

Batch 280740, train_perplexity=5007.4136, train_loss=8.518675

Batch 280750, train_perplexity=4667.156, train_loss=8.448305

Batch 280760, train_perplexity=3801.72, train_loss=8.243209

Batch 280770, train_perplexity=5920.2524, train_loss=8.686134

Batch 280780, train_perplexity=4910.9736, train_loss=8.499228

Batch 280790, train_perplexity=4615.937, train_loss=8.43727

Batch 280800, train_perplexity=6335.315, train_loss=8.753895

Batch 280810, train_perplexity=4358.8804, train_loss=8.379971

Batch 280820, train_perplexity=6804.181, train_loss=8.825293

Batch 280830, train_perplexity=4806.7734, train_loss=8.477781

Batch 280840, train_perplexity=4900.5503, train_loss=8.497103

Batch 280850, train_perplexity=5249.5166, train_loss=8.565891

Batch 280860, train_perplexity=6243.5938, train_loss=8.739311

Batch 280870, train_perplexity=5264.3213, train_loss=8.568707

Batch 280880, train_perplexity=5227.48, train_loss=8.561685

Batch 280890, train_perplexity=5214.728, train_loss=8.559242

Batch 280900, train_perplexity=5392.9756, train_loss=8.592853

Batch 280910, train_perplexity=4816.6025, train_loss=8.479824

Batch 280920, train_perplexity=5223.578, train_loss=8.560938

Batch 280930, train_perplexity=5263.543, train_loss=8.56856

Batch 280940, train_perplexity=5517.245, train_loss=8.615634

Batch 280950, train_perplexity=4448.9316, train_loss=8.400419

Batch 280960, train_perplexity=5280.7183, train_loss=8.571817

Batch 280970, train_perplexity=4792.4146, train_loss=8.47479

Batch 280980, train_perplexity=4957.0254, train_loss=8.508561

Batch 280990, train_perplexity=5245.373, train_loss=8.565102

Batch 281000, train_perplexity=5110.7607, train_loss=8.5391035

Batch 281010, train_perplexity=6451.2607, train_loss=8.772031

Batch 281020, train_perplexity=4494.4165, train_loss=8.410591

Batch 281030, train_perplexity=6271.5225, train_loss=8.743774

Batch 281040, train_perplexity=6221.7974, train_loss=8.735814

Batch 281050, train_perplexity=5134.9194, train_loss=8.543819

Batch 281060, train_perplexity=5728.5312, train_loss=8.653214

Batch 281070, train_perplexity=5690.965, train_loss=8.646635

Batch 281080, train_perplexity=4788.7734, train_loss=8.47403

Batch 281090, train_perplexity=5624.3735, train_loss=8.634865

Batch 281100, train_perplexity=5681.968, train_loss=8.645053

Batch 281110, train_perplexity=4790.7056, train_loss=8.474433

Batch 281120, train_perplexity=6002.35, train_loss=8.699906

Batch 281130, train_perplexity=5481.848, train_loss=8.609198

Batch 281140, train_perplexity=5558.0205, train_loss=8.622997

Batch 281150, train_perplexity=5062.4214, train_loss=8.5296

Batch 281160, train_perplexity=5252.7515, train_loss=8.566507

Batch 281170, train_perplexity=4842.441, train_loss=8.485174

Batch 281180, train_perplexity=5704.1196, train_loss=8.648944

Batch 281190, train_perplexity=4717.62, train_loss=8.45906

Batch 281200, train_perplexity=5039.531, train_loss=8.525068

Batch 281210, train_perplexity=4999.531, train_loss=8.517099

Batch 281220, train_perplexity=5754.381, train_loss=8.657717

Batch 281230, train_perplexity=5583.4043, train_loss=8.627554

Batch 281240, train_perplexity=6076.528, train_loss=8.712189

Batch 281250, train_perplexity=6223.536, train_loss=8.7360935

Batch 281260, train_perplexity=4056.9607, train_loss=8.308189

Batch 281270, train_perplexity=5337.486, train_loss=8.58251

Batch 281280, train_perplexity=4935.1904, train_loss=8.504147

Batch 281290, train_perplexity=5900.6143, train_loss=8.682812

Batch 281300, train_perplexity=5071.283, train_loss=8.531349

Batch 281310, train_perplexity=5531.1377, train_loss=8.618149

Batch 281320, train_perplexity=5794.5376, train_loss=8.664671

Batch 281330, train_perplexity=5874.886, train_loss=8.678442

Batch 281340, train_perplexity=5104.696, train_loss=8.537916

Batch 281350, train_perplexity=4976.346, train_loss=8.512451

Batch 281360, train_perplexity=4917.582, train_loss=8.500572

Batch 281370, train_perplexity=6528.987, train_loss=8.784007

Batch 281380, train_perplexity=6253.4263, train_loss=8.740885

Batch 281390, train_perplexity=5010.729, train_loss=8.519337

Batch 281400, train_perplexity=5224.6143, train_loss=8.561136
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 281410, train_perplexity=4978.8335, train_loss=8.512951

Batch 281420, train_perplexity=5147.9326, train_loss=8.5463505

Batch 281430, train_perplexity=5279.052, train_loss=8.571502

Batch 281440, train_perplexity=6145.436, train_loss=8.723465

Batch 281450, train_perplexity=5283.7305, train_loss=8.572388

Batch 281460, train_perplexity=5394.7554, train_loss=8.593183

Batch 281470, train_perplexity=5294.2827, train_loss=8.574383

Batch 281480, train_perplexity=5337.089, train_loss=8.582436

Batch 281490, train_perplexity=5771.831, train_loss=8.660745

Batch 281500, train_perplexity=6138.501, train_loss=8.722336

Batch 281510, train_perplexity=5547.546, train_loss=8.621111

Batch 281520, train_perplexity=5802.268, train_loss=8.666004

Batch 281530, train_perplexity=4774.2314, train_loss=8.470988

Batch 281540, train_perplexity=5915.658, train_loss=8.685358

Batch 281550, train_perplexity=5409.0, train_loss=8.595819

Batch 281560, train_perplexity=4984.9243, train_loss=8.5141735

Batch 281570, train_perplexity=5217.8823, train_loss=8.559847

Batch 281580, train_perplexity=4866.1714, train_loss=8.490063

Batch 281590, train_perplexity=4929.245, train_loss=8.502941

Batch 281600, train_perplexity=6091.0093, train_loss=8.714569

Batch 281610, train_perplexity=5899.5903, train_loss=8.682638

Batch 281620, train_perplexity=6834.6045, train_loss=8.829754

Batch 281630, train_perplexity=4868.52, train_loss=8.490545

Batch 281640, train_perplexity=5385.1484, train_loss=8.5914

Batch 281650, train_perplexity=5064.3477, train_loss=8.529981

Batch 281660, train_perplexity=5330.2627, train_loss=8.581156

Batch 281670, train_perplexity=5890.4375, train_loss=8.681086

Batch 281680, train_perplexity=5187.8086, train_loss=8.554067

Batch 281690, train_perplexity=4818.596, train_loss=8.480238

Batch 281700, train_perplexity=5425.9253, train_loss=8.598944

Batch 281710, train_perplexity=5199.8696, train_loss=8.556389

Batch 281720, train_perplexity=5452.1196, train_loss=8.60376

Batch 281730, train_perplexity=5654.3184, train_loss=8.640175

Batch 281740, train_perplexity=5494.981, train_loss=8.61159

Batch 281750, train_perplexity=4443.2114, train_loss=8.399133

Batch 281760, train_perplexity=5081.1494, train_loss=8.533293

Batch 281770, train_perplexity=5152.977, train_loss=8.54733

Batch 281780, train_perplexity=5549.0483, train_loss=8.621382

Batch 281790, train_perplexity=6223.109, train_loss=8.736025

Batch 281800, train_perplexity=6210.504, train_loss=8.733997

Batch 281810, train_perplexity=5808.347, train_loss=8.667051

Batch 281820, train_perplexity=5624.985, train_loss=8.634974

Batch 281830, train_perplexity=5900.9746, train_loss=8.682873

Batch 281840, train_perplexity=5083.5195, train_loss=8.533759

Batch 281850, train_perplexity=6027.6187, train_loss=8.704107

Batch 281860, train_perplexity=5218.569, train_loss=8.5599785

Batch 281870, train_perplexity=6072.9473, train_loss=8.711599

Batch 281880, train_perplexity=4361.0425, train_loss=8.380466

Batch 281890, train_perplexity=5556.155, train_loss=8.622662

Batch 281900, train_perplexity=5465.838, train_loss=8.606273

Batch 281910, train_perplexity=5507.147, train_loss=8.613802

Batch 281920, train_perplexity=5214.534, train_loss=8.559205

Batch 281930, train_perplexity=4874.6245, train_loss=8.491798

Batch 281940, train_perplexity=6114.7207, train_loss=8.718454

Batch 281950, train_perplexity=5200.0137, train_loss=8.5564165

Batch 281960, train_perplexity=5217.3896, train_loss=8.559752

Batch 281970, train_perplexity=5209.4443, train_loss=8.5582285

Batch 281980, train_perplexity=4016.5618, train_loss=8.298182

Batch 281990, train_perplexity=5259.6895, train_loss=8.567827

Batch 282000, train_perplexity=5329.175, train_loss=8.580952

Batch 282010, train_perplexity=5038.551, train_loss=8.524874

Batch 282020, train_perplexity=5755.5444, train_loss=8.657919

Batch 282030, train_perplexity=6836.3516, train_loss=8.830009

Batch 282040, train_perplexity=4433.6836, train_loss=8.396986

Batch 282050, train_perplexity=4762.5215, train_loss=8.468533

Batch 282060, train_perplexity=5930.175, train_loss=8.687809

Batch 282070, train_perplexity=5792.46, train_loss=8.664312

Batch 282080, train_perplexity=5441.4194, train_loss=8.601795

Batch 282090, train_perplexity=5873.1665, train_loss=8.678149

Batch 282100, train_perplexity=6096.7803, train_loss=8.715516

Batch 282110, train_perplexity=5885.0923, train_loss=8.680178

Batch 282120, train_perplexity=4633.3184, train_loss=8.441029

Batch 282130, train_perplexity=4778.486, train_loss=8.471879

Batch 282140, train_perplexity=5788.1416, train_loss=8.663567

Batch 282150, train_perplexity=6159.083, train_loss=8.725683

Batch 282160, train_perplexity=4971.707, train_loss=8.5115185

Batch 282170, train_perplexity=4637.1685, train_loss=8.441859

Batch 282180, train_perplexity=4999.3784, train_loss=8.517069

Batch 282190, train_perplexity=5504.8784, train_loss=8.61339

Batch 282200, train_perplexity=5150.4663, train_loss=8.546843

Batch 282210, train_perplexity=5487.4033, train_loss=8.61021

Batch 282220, train_perplexity=5145.12, train_loss=8.545804

Batch 282230, train_perplexity=5574.285, train_loss=8.625919

Batch 282240, train_perplexity=4797.1196, train_loss=8.475771

Batch 282250, train_perplexity=6377.9497, train_loss=8.760602

Batch 282260, train_perplexity=5310.2974, train_loss=8.577403

Batch 282270, train_perplexity=5730.438, train_loss=8.653547

Batch 282280, train_perplexity=5952.318, train_loss=8.691536

Batch 282290, train_perplexity=4596.595, train_loss=8.433071

Batch 282300, train_perplexity=5388.2563, train_loss=8.591977

Batch 282310, train_perplexity=5661.7593, train_loss=8.64149

Batch 282320, train_perplexity=5341.2437, train_loss=8.583214

Batch 282330, train_perplexity=6613.6074, train_loss=8.796885

Batch 282340, train_perplexity=5712.977, train_loss=8.650496

Batch 282350, train_perplexity=5497.287, train_loss=8.61201

Batch 282360, train_perplexity=6575.9536, train_loss=8.791175

Batch 282370, train_perplexity=4694.7036, train_loss=8.45419

Batch 282380, train_perplexity=5320.943, train_loss=8.579406

Batch 282390, train_perplexity=4861.1895, train_loss=8.489038

Batch 282400, train_perplexity=5769.707, train_loss=8.660377

Batch 282410, train_perplexity=5658.5747, train_loss=8.640927

Batch 282420, train_perplexity=5150.039, train_loss=8.54676

Batch 282430, train_perplexity=5579.5396, train_loss=8.626862

Batch 282440, train_perplexity=5189.318, train_loss=8.554358

Batch 282450, train_perplexity=4930.745, train_loss=8.503245

Batch 282460, train_perplexity=5124.211, train_loss=8.541732

Batch 282470, train_perplexity=4849.345, train_loss=8.486599

Batch 282480, train_perplexity=5058.4688, train_loss=8.528819

Batch 282490, train_perplexity=5040.2183, train_loss=8.525205

Batch 282500, train_perplexity=6093.2754, train_loss=8.714941

Batch 282510, train_perplexity=5737.5034, train_loss=8.654779

Batch 282520, train_perplexity=5555.1694, train_loss=8.622484

Batch 282530, train_perplexity=5569.8955, train_loss=8.625132

Batch 282540, train_perplexity=5800.265, train_loss=8.665659

Batch 282550, train_perplexity=6011.625, train_loss=8.70145

Batch 282560, train_perplexity=6276.8657, train_loss=8.744626

Batch 282570, train_perplexity=5557.0454, train_loss=8.622822

Batch 282580, train_perplexity=4671.4663, train_loss=8.449228

Batch 282590, train_perplexity=5474.926, train_loss=8.607934

Batch 282600, train_perplexity=5994.17, train_loss=8.698543

Batch 282610, train_perplexity=6588.1, train_loss=8.79302

Batch 282620, train_perplexity=5344.4946, train_loss=8.583822

Batch 282630, train_perplexity=7548.4546, train_loss=8.929098

Batch 282640, train_perplexity=5209.062, train_loss=8.558155

Batch 282650, train_perplexity=6425.123, train_loss=8.767971

Batch 282660, train_perplexity=4659.115, train_loss=8.446581

Batch 282670, train_perplexity=5236.5464, train_loss=8.563417

Batch 282680, train_perplexity=5563.5835, train_loss=8.623998

Batch 282690, train_perplexity=5241.113, train_loss=8.564289

Batch 282700, train_perplexity=5210.3735, train_loss=8.558407

Batch 282710, train_perplexity=5791.6646, train_loss=8.664175

Batch 282720, train_perplexity=6145.225, train_loss=8.723431
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 282730, train_perplexity=4593.7466, train_loss=8.432451

Batch 282740, train_perplexity=5295.0957, train_loss=8.574536

Batch 282750, train_perplexity=5376.5376, train_loss=8.5898

Batch 282760, train_perplexity=5779.096, train_loss=8.662003

Batch 282770, train_perplexity=5144.5464, train_loss=8.545692

Batch 282780, train_perplexity=5396.068, train_loss=8.593426

Batch 282790, train_perplexity=5170.487, train_loss=8.550722

Batch 282800, train_perplexity=5053.449, train_loss=8.527826

Batch 282810, train_perplexity=5416.0923, train_loss=8.59713

Batch 282820, train_perplexity=5066.8066, train_loss=8.530466

Batch 282830, train_perplexity=6416.954, train_loss=8.766699

Batch 282840, train_perplexity=5335.8877, train_loss=8.582211

Batch 282850, train_perplexity=6455.827, train_loss=8.772738

Batch 282860, train_perplexity=5902.652, train_loss=8.683157

Batch 282870, train_perplexity=5531.053, train_loss=8.618134

Batch 282880, train_perplexity=5658.8066, train_loss=8.640968

Batch 282890, train_perplexity=6895.662, train_loss=8.838648

Batch 282900, train_perplexity=5470.7554, train_loss=8.607172

Batch 282910, train_perplexity=5295.868, train_loss=8.574682

Batch 282920, train_perplexity=6778.2427, train_loss=8.821473

Batch 282930, train_perplexity=4674.532, train_loss=8.449884

Batch 282940, train_perplexity=5504.894, train_loss=8.613393

Batch 282950, train_perplexity=5576.0186, train_loss=8.62623

Batch 282960, train_perplexity=5301.366, train_loss=8.57572

Batch 282970, train_perplexity=5354.4478, train_loss=8.585683

Batch 282980, train_perplexity=5246.5537, train_loss=8.565327

Batch 282990, train_perplexity=4189.996, train_loss=8.340455

Batch 283000, train_perplexity=6382.319, train_loss=8.761287

Batch 283010, train_perplexity=5619.505, train_loss=8.633999

Batch 283020, train_perplexity=5465.379, train_loss=8.606189

Batch 283030, train_perplexity=4803.8496, train_loss=8.477173

Batch 283040, train_perplexity=6223.8506, train_loss=8.736144

Batch 283050, train_perplexity=4483.235, train_loss=8.4081

Batch 283060, train_perplexity=5386.551, train_loss=8.5916605

Batch 283070, train_perplexity=5292.7983, train_loss=8.574102

Batch 283080, train_perplexity=5998.8193, train_loss=8.699318

Batch 283090, train_perplexity=5011.5366, train_loss=8.519498

Batch 283100, train_perplexity=5204.315, train_loss=8.557243

Batch 283110, train_perplexity=4942.237, train_loss=8.505573

Batch 283120, train_perplexity=5952.789, train_loss=8.691615

Batch 283130, train_perplexity=4922.213, train_loss=8.5015135

Batch 283140, train_perplexity=4912.913, train_loss=8.499622

Batch 283150, train_perplexity=5146.9854, train_loss=8.546166

Batch 283160, train_perplexity=5339.731, train_loss=8.582931

Batch 283170, train_perplexity=6719.602, train_loss=8.812784

Batch 283180, train_perplexity=5811.8213, train_loss=8.667649

Batch 283190, train_perplexity=4397.6533, train_loss=8.388826

Batch 283200, train_perplexity=4660.3955, train_loss=8.446856

Batch 283210, train_perplexity=5174.8276, train_loss=8.551561

Batch 283220, train_perplexity=7560.688, train_loss=8.930717

Batch 283230, train_perplexity=5038.2046, train_loss=8.524805

Batch 283240, train_perplexity=5794.2666, train_loss=8.664624

Batch 283250, train_perplexity=5896.4854, train_loss=8.682112

Batch 283260, train_perplexity=5566.391, train_loss=8.624502

Batch 283270, train_perplexity=4458.65, train_loss=8.402601

Batch 283280, train_perplexity=5354.5757, train_loss=8.585707

Batch 283290, train_perplexity=4549.2256, train_loss=8.422712

Batch 283300, train_perplexity=5901.582, train_loss=8.682976

Batch 283310, train_perplexity=4025.9343, train_loss=8.300512

Batch 283320, train_perplexity=5800.564, train_loss=8.66571

Batch 283330, train_perplexity=4531.9785, train_loss=8.418914

Batch 283340, train_perplexity=4987.5205, train_loss=8.514694

Batch 283350, train_perplexity=5488.131, train_loss=8.610343

Batch 283360, train_perplexity=6061.4795, train_loss=8.709709

Batch 283370, train_perplexity=6142.723, train_loss=8.723023

Batch 283380, train_perplexity=5293.813, train_loss=8.574294

Batch 283390, train_perplexity=5627.2812, train_loss=8.635382

Batch 283400, train_perplexity=5011.7563, train_loss=8.519542

Batch 283410, train_perplexity=5103.226, train_loss=8.537628

Batch 283420, train_perplexity=5456.2656, train_loss=8.60452

Batch 283430, train_perplexity=4842.413, train_loss=8.485168

Batch 283440, train_perplexity=5624.985, train_loss=8.634974

Batch 283450, train_perplexity=5967.055, train_loss=8.694009

Batch 283460, train_perplexity=4789.1934, train_loss=8.474117

Batch 283470, train_perplexity=6097.0303, train_loss=8.715557

Batch 283480, train_perplexity=4914.2393, train_loss=8.499892

Batch 283490, train_perplexity=4894.8843, train_loss=8.495946

Batch 283500, train_perplexity=4264.283, train_loss=8.358029

Batch 283510, train_perplexity=4955.073, train_loss=8.508167

Batch 283520, train_perplexity=5164.4546, train_loss=8.549555

Batch 283530, train_perplexity=6091.0967, train_loss=8.714583

Batch 283540, train_perplexity=5966.253, train_loss=8.693874

Batch 283550, train_perplexity=5084.441, train_loss=8.53394

Batch 283560, train_perplexity=5915.0376, train_loss=8.685253

Batch 283570, train_perplexity=4890.5215, train_loss=8.495054

Batch 283580, train_perplexity=5462.4453, train_loss=8.605652

Batch 283590, train_perplexity=5650.75, train_loss=8.639544

Batch 283600, train_perplexity=4713.6807, train_loss=8.458224

Batch 283610, train_perplexity=4879.08, train_loss=8.492712

Batch 283620, train_perplexity=4905.3286, train_loss=8.498077

Batch 283630, train_perplexity=5034.8906, train_loss=8.524147

Batch 283640, train_perplexity=5489.1616, train_loss=8.610531

Batch 283650, train_perplexity=4890.582, train_loss=8.495067

Batch 283660, train_perplexity=4907.116, train_loss=8.498442

Batch 283670, train_perplexity=5409.309, train_loss=8.595877

Batch 283680, train_perplexity=5299.2734, train_loss=8.575325

Batch 283690, train_perplexity=5567.4155, train_loss=8.624686

Batch 283700, train_perplexity=4761.686, train_loss=8.468357

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Batch 283710, train_perplexity=4587.416, train_loss=8.431072

Batch 283720, train_perplexity=4581.4966, train_loss=8.429781

Batch 283730, train_perplexity=6363.763, train_loss=8.758375

Batch 283740, train_perplexity=5415.6533, train_loss=8.597049

Batch 283750, train_perplexity=4643.0454, train_loss=8.443126

Batch 283760, train_perplexity=4661.751, train_loss=8.447146

Batch 283770, train_perplexity=5347.268, train_loss=8.584341

Batch 283780, train_perplexity=5294.722, train_loss=8.574466

Batch 283790, train_perplexity=5425.511, train_loss=8.598867

Batch 283800, train_perplexity=5409.712, train_loss=8.595951

Batch 283810, train_perplexity=5955.202, train_loss=8.69202

Batch 283820, train_perplexity=4776.3765, train_loss=8.471437

Batch 283830, train_perplexity=5890.73, train_loss=8.681135

Batch 283840, train_perplexity=4265.865, train_loss=8.3584

Batch 283850, train_perplexity=4878.192, train_loss=8.49253

Batch 283860, train_perplexity=5136.4575, train_loss=8.544119

Batch 283870, train_perplexity=4807.91, train_loss=8.478018

Batch 283880, train_perplexity=5649.527, train_loss=8.639327

Batch 283890, train_perplexity=6182.458, train_loss=8.729471

Batch 283900, train_perplexity=4368.7603, train_loss=8.382235

Batch 283910, train_perplexity=4799.618, train_loss=8.476292

Batch 283920, train_perplexity=5511.0293, train_loss=8.614507

Batch 283930, train_perplexity=6552.878, train_loss=8.78766

Batch 283940, train_perplexity=4922.2646, train_loss=8.501524

Batch 283950, train_perplexity=4689.8843, train_loss=8.453163

Batch 283960, train_perplexity=4819.6353, train_loss=8.4804535

Batch 283970, train_perplexity=6589.394, train_loss=8.793217

Batch 283980, train_perplexity=5311.9736, train_loss=8.577719
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 283990, train_perplexity=5844.1187, train_loss=8.673191

Batch 284000, train_perplexity=6610.1455, train_loss=8.796361

Batch 284010, train_perplexity=5992.3467, train_loss=8.698238

Batch 284020, train_perplexity=5692.305, train_loss=8.646871

Batch 284030, train_perplexity=6020.5693, train_loss=8.702937

Batch 284040, train_perplexity=5696.3184, train_loss=8.647575

Batch 284050, train_perplexity=5828.0337, train_loss=8.670435

Batch 284060, train_perplexity=4673.427, train_loss=8.449648

Batch 284070, train_perplexity=5310.2314, train_loss=8.577391

Batch 284080, train_perplexity=5733.5596, train_loss=8.654092

Batch 284090, train_perplexity=6122.703, train_loss=8.719759

Batch 284100, train_perplexity=5742.879, train_loss=8.655716

Batch 284110, train_perplexity=5227.789, train_loss=8.561744

Batch 284120, train_perplexity=5202.7964, train_loss=8.5569515

Batch 284130, train_perplexity=6175.575, train_loss=8.728357

Batch 284140, train_perplexity=4825.177, train_loss=8.481603

Batch 284150, train_perplexity=5007.4185, train_loss=8.518676

Batch 284160, train_perplexity=5894.8774, train_loss=8.681839

Batch 284170, train_perplexity=4886.9736, train_loss=8.4943285

Batch 284180, train_perplexity=6136.2593, train_loss=8.721971

Batch 284190, train_perplexity=4955.253, train_loss=8.5082035

Batch 284200, train_perplexity=4697.7627, train_loss=8.454842

Batch 284210, train_perplexity=5490.0728, train_loss=8.610697

Batch 284220, train_perplexity=5436.953, train_loss=8.600974

Batch 284230, train_perplexity=5756.291, train_loss=8.658049

Batch 284240, train_perplexity=4446.806, train_loss=8.399941

Batch 284250, train_perplexity=5236.8657, train_loss=8.563478

Batch 284260, train_perplexity=5733.997, train_loss=8.654168

Batch 284270, train_perplexity=5669.059, train_loss=8.642778

Batch 284280, train_perplexity=4615.334, train_loss=8.4371395

Batch 284290, train_perplexity=5624.1157, train_loss=8.634819

Batch 284300, train_perplexity=5307.71, train_loss=8.576916

Batch 284310, train_perplexity=5204.4043, train_loss=8.5572605

Batch 284320, train_perplexity=5076.3447, train_loss=8.532347

Batch 284330, train_perplexity=5513.837, train_loss=8.615016

Batch 284340, train_perplexity=4735.1855, train_loss=8.462776

Batch 284350, train_perplexity=4947.632, train_loss=8.506664

Batch 284360, train_perplexity=5152.7017, train_loss=8.5472765

Batch 284370, train_perplexity=4854.208, train_loss=8.487601

Batch 284380, train_perplexity=5795.875, train_loss=8.664902

Batch 284390, train_perplexity=5783.656, train_loss=8.662791

Batch 284400, train_perplexity=4570.944, train_loss=8.427475

Batch 284410, train_perplexity=4839.0615, train_loss=8.484476

Batch 284420, train_perplexity=6335.279, train_loss=8.753889

Batch 284430, train_perplexity=5011.4795, train_loss=8.519486

Batch 284440, train_perplexity=5998.104, train_loss=8.699199

Batch 284450, train_perplexity=4855.676, train_loss=8.487904

Batch 284460, train_perplexity=5393.0835, train_loss=8.592873

Batch 284470, train_perplexity=5215.1855, train_loss=8.55933

Batch 284480, train_perplexity=5904.273, train_loss=8.683432

Batch 284490, train_perplexity=5289.796, train_loss=8.573535

Batch 284500, train_perplexity=5315.8506, train_loss=8.578448

Batch 284510, train_perplexity=5371.2383, train_loss=8.588814

Batch 284520, train_perplexity=4433.709, train_loss=8.396992

Batch 284530, train_perplexity=4813.9526, train_loss=8.479274

Batch 284540, train_perplexity=6120.882, train_loss=8.719461

Batch 284550, train_perplexity=4607.1147, train_loss=8.435357

Batch 284560, train_perplexity=4713.159, train_loss=8.458114

Batch 284570, train_perplexity=5392.7134, train_loss=8.592804

Batch 284580, train_perplexity=6815.3906, train_loss=8.826939

Batch 284590, train_perplexity=6231.5537, train_loss=8.737381

Batch 284600, train_perplexity=5229.679, train_loss=8.562105

Batch 284610, train_perplexity=5434.5894, train_loss=8.600539

Batch 284620, train_perplexity=6165.336, train_loss=8.726698

Batch 284630, train_perplexity=6139.5547, train_loss=8.722507

Batch 284640, train_perplexity=5033.5366, train_loss=8.523878

Batch 284650, train_perplexity=6166.712, train_loss=8.726921

Batch 284660, train_perplexity=5029.1123, train_loss=8.522999

Batch 284670, train_perplexity=4767.1885, train_loss=8.469512

Batch 284680, train_perplexity=5511.0137, train_loss=8.614504

Batch 284690, train_perplexity=4950.983, train_loss=8.507341

Batch 284700, train_perplexity=5741.817, train_loss=8.655531

Batch 284710, train_perplexity=5237.455, train_loss=8.563591

Batch 284720, train_perplexity=4664.846, train_loss=8.44781

Batch 284730, train_perplexity=4843.041, train_loss=8.485298

Batch 284740, train_perplexity=5026.058, train_loss=8.522391

Batch 284750, train_perplexity=5220.2163, train_loss=8.560294

Batch 284760, train_perplexity=4861.5513, train_loss=8.489113

Batch 284770, train_perplexity=5141.2993, train_loss=8.545061

Batch 284780, train_perplexity=5586.5146, train_loss=8.628111

Batch 284790, train_perplexity=6051.683, train_loss=8.708092

Batch 284800, train_perplexity=4547.7466, train_loss=8.422387

Batch 284810, train_perplexity=5293.0356, train_loss=8.574147

Batch 284820, train_perplexity=5454.517, train_loss=8.604199

Batch 284830, train_perplexity=4978.2876, train_loss=8.512841

Batch 284840, train_perplexity=5430.0923, train_loss=8.599711

Batch 284850, train_perplexity=4476.1943, train_loss=8.406528

Batch 284860, train_perplexity=5828.6284, train_loss=8.670537

Batch 284870, train_perplexity=6369.185, train_loss=8.759227

Batch 284880, train_perplexity=6411.4487, train_loss=8.765841

Batch 284890, train_perplexity=5910.1265, train_loss=8.6844225

Batch 284900, train_perplexity=5443.2256, train_loss=8.602127

Batch 284910, train_perplexity=5171.779, train_loss=8.550972

Batch 284920, train_perplexity=6765.681, train_loss=8.819618

Batch 284930, train_perplexity=4808.896, train_loss=8.478223

Batch 284940, train_perplexity=4817.1904, train_loss=8.479946

Batch 284950, train_perplexity=5260.9785, train_loss=8.568072

Batch 284960, train_perplexity=5392.405, train_loss=8.592747

Batch 284970, train_perplexity=5020.5537, train_loss=8.521296

Batch 284980, train_perplexity=5608.8823, train_loss=8.632107

Batch 284990, train_perplexity=5282.0527, train_loss=8.57207

Batch 285000, train_perplexity=4382.8154, train_loss=8.385447

Batch 285010, train_perplexity=6419.7085, train_loss=8.767128

Batch 285020, train_perplexity=5367.92, train_loss=8.588196

Batch 285030, train_perplexity=5778.743, train_loss=8.661942

Batch 285040, train_perplexity=5728.8755, train_loss=8.653275

Batch 285050, train_perplexity=5040.396, train_loss=8.52524

Batch 285060, train_perplexity=4971.5503, train_loss=8.511487

Batch 285070, train_perplexity=5612.5317, train_loss=8.632757

Batch 285080, train_perplexity=4849.216, train_loss=8.486572

Batch 285090, train_perplexity=5894.6523, train_loss=8.681801

Batch 285100, train_perplexity=6383.6763, train_loss=8.761499

Batch 285110, train_perplexity=6099.188, train_loss=8.715911

Batch 285120, train_perplexity=5475.944, train_loss=8.60812

Batch 285130, train_perplexity=6188.133, train_loss=8.730389

Batch 285140, train_perplexity=5918.446, train_loss=8.685829

Batch 285150, train_perplexity=5723.3438, train_loss=8.652308

Batch 285160, train_perplexity=5691.475, train_loss=8.646725

Batch 285170, train_perplexity=5561.8755, train_loss=8.623691

Batch 285180, train_perplexity=4732.8647, train_loss=8.462286

Batch 285190, train_perplexity=5662.391, train_loss=8.641602

Batch 285200, train_perplexity=4815.895, train_loss=8.479677

Batch 285210, train_perplexity=6072.548, train_loss=8.711534

Batch 285220, train_perplexity=5469.7124, train_loss=8.606981

Batch 285230, train_perplexity=4907.6777, train_loss=8.498556

Batch 285240, train_perplexity=5183.412, train_loss=8.553219

Batch 285250, train_perplexity=6354.157, train_loss=8.756865

Batch 285260, train_perplexity=5960.168, train_loss=8.692854

Batch 285270, train_perplexity=5477.4795, train_loss=8.6084

Batch 285280, train_perplexity=4867.907, train_loss=8.490419

Batch 285290, train_perplexity=5230.6167, train_loss=8.562284

Batch 285300, train_perplexity=4582.248, train_loss=8.429945
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 285310, train_perplexity=5624.234, train_loss=8.63484

Batch 285320, train_perplexity=5831.503, train_loss=8.67103

Batch 285330, train_perplexity=4817.2456, train_loss=8.479958

Batch 285340, train_perplexity=7296.856, train_loss=8.895199

Batch 285350, train_perplexity=5626.5137, train_loss=8.635245

Batch 285360, train_perplexity=5742.4355, train_loss=8.655639

Batch 285370, train_perplexity=5789.9414, train_loss=8.6638775

Batch 285380, train_perplexity=5687.921, train_loss=8.6461

Batch 285390, train_perplexity=4698.82, train_loss=8.455067

Batch 285400, train_perplexity=4434.8716, train_loss=8.397254

Batch 285410, train_perplexity=4051.8994, train_loss=8.306941

Batch 285420, train_perplexity=4631.26, train_loss=8.440584

Batch 285430, train_perplexity=4999.021, train_loss=8.516997

Batch 285440, train_perplexity=6466.6045, train_loss=8.774406

Batch 285450, train_perplexity=6142.3423, train_loss=8.722961

Batch 285460, train_perplexity=5314.5376, train_loss=8.578201

Batch 285470, train_perplexity=4339.126, train_loss=8.375428

Batch 285480, train_perplexity=6326.0654, train_loss=8.752434

Batch 285490, train_perplexity=5287.6426, train_loss=8.573128

Batch 285500, train_perplexity=5603.787, train_loss=8.631198

Batch 285510, train_perplexity=5212.998, train_loss=8.55891

Batch 285520, train_perplexity=5473.1562, train_loss=8.607611

Batch 285530, train_perplexity=5295.697, train_loss=8.57465

Batch 285540, train_perplexity=5304.3447, train_loss=8.576282

Batch 285550, train_perplexity=6278.8716, train_loss=8.744946

Batch 285560, train_perplexity=5378.9326, train_loss=8.590245

Batch 285570, train_perplexity=4980.505, train_loss=8.513287

Batch 285580, train_perplexity=5520.103, train_loss=8.616152

Batch 285590, train_perplexity=5716.8086, train_loss=8.651166

Batch 285600, train_perplexity=5925.8896, train_loss=8.687086

Batch 285610, train_perplexity=4954.1377, train_loss=8.507978

Batch 285620, train_perplexity=5748.4844, train_loss=8.656692

Batch 285630, train_perplexity=4697.839, train_loss=8.454858

Batch 285640, train_perplexity=4192.646, train_loss=8.341087

Batch 285650, train_perplexity=5955.1567, train_loss=8.692013

Batch 285660, train_perplexity=5590.032, train_loss=8.62874

Batch 285670, train_perplexity=5593.072, train_loss=8.629284

Batch 285680, train_perplexity=5073.8184, train_loss=8.531849

Batch 285690, train_perplexity=4941.69, train_loss=8.505463

Batch 285700, train_perplexity=4125.5044, train_loss=8.324944

Batch 285710, train_perplexity=5133.367, train_loss=8.543517

Batch 285720, train_perplexity=5940.7603, train_loss=8.689592

Batch 285730, train_perplexity=4861.514, train_loss=8.489105

Batch 285740, train_perplexity=5275.8457, train_loss=8.570894

Batch 285750, train_perplexity=4827.2896, train_loss=8.48204

Batch 285760, train_perplexity=6272.7544, train_loss=8.743971

Batch 285770, train_perplexity=5277.3955, train_loss=8.571188

Batch 285780, train_perplexity=5301.508, train_loss=8.575747

Batch 285790, train_perplexity=5492.749, train_loss=8.611184

Batch 285800, train_perplexity=5650.125, train_loss=8.639433

Batch 285810, train_perplexity=4722.432, train_loss=8.460079

Batch 285820, train_perplexity=5664.584, train_loss=8.641989

Batch 285830, train_perplexity=5307.3, train_loss=8.5768385

Batch 285840, train_perplexity=5059.405, train_loss=8.529004

Batch 285850, train_perplexity=4450.7563, train_loss=8.400829

Batch 285860, train_perplexity=5189.5356, train_loss=8.5543995

Batch 285870, train_perplexity=5071.999, train_loss=8.53149

Batch 285880, train_perplexity=5500.68, train_loss=8.612627

Batch 285890, train_perplexity=4618.037, train_loss=8.437725

Batch 285900, train_perplexity=5831.7197, train_loss=8.671067

Batch 285910, train_perplexity=5317.9346, train_loss=8.57884

Batch 285920, train_perplexity=4384.9893, train_loss=8.385942

Batch 285930, train_perplexity=5237.4854, train_loss=8.563597

Batch 285940, train_perplexity=5147.108, train_loss=8.54619

Batch 285950, train_perplexity=5570.331, train_loss=8.62521

Batch 285960, train_perplexity=5967.9033, train_loss=8.694151

Batch 285970, train_perplexity=5690.634, train_loss=8.646577

Batch 285980, train_perplexity=6149.622, train_loss=8.724146

Batch 285990, train_perplexity=6521.1772, train_loss=8.78281

Batch 286000, train_perplexity=5068.7886, train_loss=8.530857

Batch 286010, train_perplexity=5727.5645, train_loss=8.653046

Batch 286020, train_perplexity=6069.971, train_loss=8.711109

Batch 286030, train_perplexity=5001.2812, train_loss=8.517449

Batch 286040, train_perplexity=5263.578, train_loss=8.568566

Batch 286050, train_perplexity=5089.986, train_loss=8.53503

Batch 286060, train_perplexity=5571.2876, train_loss=8.625381

Batch 286070, train_perplexity=4916.091, train_loss=8.500269

Batch 286080, train_perplexity=4842.5146, train_loss=8.485189

Batch 286090, train_perplexity=5733.9204, train_loss=8.654155

Batch 286100, train_perplexity=5061.4365, train_loss=8.529406

Batch 286110, train_perplexity=5891.2354, train_loss=8.681221

Batch 286120, train_perplexity=5157.3774, train_loss=8.548183

Batch 286130, train_perplexity=4850.594, train_loss=8.486856

Batch 286140, train_perplexity=5231.4546, train_loss=8.562445

Batch 286150, train_perplexity=4363.314, train_loss=8.380987

Batch 286160, train_perplexity=4490.1235, train_loss=8.409636

Batch 286170, train_perplexity=5109.82, train_loss=8.538919

Batch 286180, train_perplexity=5698.3726, train_loss=8.647936

Batch 286190, train_perplexity=4682.4116, train_loss=8.451569

Batch 286200, train_perplexity=4906.068, train_loss=8.498228

Batch 286210, train_perplexity=5338.2036, train_loss=8.582644

Batch 286220, train_perplexity=6362.149, train_loss=8.7581215

Batch 286230, train_perplexity=5647.1943, train_loss=8.638914

Batch 286240, train_perplexity=5084.2954, train_loss=8.533912

Batch 286250, train_perplexity=6892.546, train_loss=8.838196

Batch 286260, train_perplexity=5860.427, train_loss=8.675978

Batch 286270, train_perplexity=4456.1416, train_loss=8.402039

Batch 286280, train_perplexity=4429.9937, train_loss=8.396153

Batch 286290, train_perplexity=4774.2905, train_loss=8.471001

Batch 286300, train_perplexity=4792.355, train_loss=8.474777

Batch 286310, train_perplexity=4732.725, train_loss=8.462256

Batch 286320, train_perplexity=4971.773, train_loss=8.511532

Batch 286330, train_perplexity=5192.1396, train_loss=8.554901

Batch 286340, train_perplexity=4881.6587, train_loss=8.49324

Batch 286350, train_perplexity=6573.007, train_loss=8.790727

Batch 286360, train_perplexity=4916.7285, train_loss=8.500399

Batch 286370, train_perplexity=4644.002, train_loss=8.443332

Batch 286380, train_perplexity=6448.997, train_loss=8.77168

Batch 286390, train_perplexity=5697.253, train_loss=8.647739

Batch 286400, train_perplexity=6628.6104, train_loss=8.79915

Batch 286410, train_perplexity=4614.845, train_loss=8.437034

Batch 286420, train_perplexity=4121.7603, train_loss=8.324036

Batch 286430, train_perplexity=6519.567, train_loss=8.782563

Batch 286440, train_perplexity=4901.597, train_loss=8.497316

Batch 286450, train_perplexity=4746.3755, train_loss=8.465137

Batch 286460, train_perplexity=7157.0186, train_loss=8.875849

Batch 286470, train_perplexity=4686.8125, train_loss=8.452508

Batch 286480, train_perplexity=5524.464, train_loss=8.616941

Batch 286490, train_perplexity=5222.766, train_loss=8.560782

Batch 286500, train_perplexity=4896.9106, train_loss=8.49636

Batch 286510, train_perplexity=5010.208, train_loss=8.519233

Batch 286520, train_perplexity=6817.5747, train_loss=8.827259

Batch 286530, train_perplexity=5118.931, train_loss=8.540701

Batch 286540, train_perplexity=4860.0493, train_loss=8.488804

Batch 286550, train_perplexity=5076.4365, train_loss=8.532365

Batch 286560, train_perplexity=5299.5464, train_loss=8.5753765

Batch 286570, train_perplexity=5355.428, train_loss=8.585866

Batch 286580, train_perplexity=4837.3267, train_loss=8.4841175

Batch 286590, train_perplexity=5676.2866, train_loss=8.6440525

Batch 286600, train_perplexity=5159.98, train_loss=8.548688

Batch 286610, train_perplexity=5276.912, train_loss=8.571096

Batch 286620, train_perplexity=6015.066, train_loss=8.702023
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 286630, train_perplexity=5341.9517, train_loss=8.583346

Batch 286640, train_perplexity=6228.844, train_loss=8.736946

Batch 286650, train_perplexity=5443.2563, train_loss=8.602133

Batch 286660, train_perplexity=5009.0234, train_loss=8.518996

Batch 286670, train_perplexity=5169.4907, train_loss=8.5505295

Batch 286680, train_perplexity=4725.135, train_loss=8.460651

Batch 286690, train_perplexity=5313.215, train_loss=8.577952

Batch 286700, train_perplexity=5495.6777, train_loss=8.611717

Batch 286710, train_perplexity=4967.034, train_loss=8.510578

Batch 286720, train_perplexity=5058.961, train_loss=8.528916

Batch 286730, train_perplexity=4846.5156, train_loss=8.486015

Batch 286740, train_perplexity=4968.6543, train_loss=8.510904

Batch 286750, train_perplexity=5359.475, train_loss=8.586621

Batch 286760, train_perplexity=6029.826, train_loss=8.7044735

Batch 286770, train_perplexity=5458.1025, train_loss=8.6048565

Batch 286780, train_perplexity=5079.943, train_loss=8.533055

Batch 286790, train_perplexity=6014.033, train_loss=8.701851

Batch 286800, train_perplexity=5351.7676, train_loss=8.585182

Batch 286810, train_perplexity=5431.8687, train_loss=8.600039

Batch 286820, train_perplexity=4359.454, train_loss=8.380102

Batch 286830, train_perplexity=5159.6597, train_loss=8.548626

Batch 286840, train_perplexity=5282.511, train_loss=8.572157

Batch 286850, train_perplexity=5268.957, train_loss=8.569588

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Batch 286860, train_perplexity=5440.776, train_loss=8.601677

Batch 286870, train_perplexity=4525.772, train_loss=8.417543

Batch 286880, train_perplexity=6237.5947, train_loss=8.73835

Batch 286890, train_perplexity=5597.639, train_loss=8.6301

Batch 286900, train_perplexity=5178.753, train_loss=8.55232

Batch 286910, train_perplexity=5764.2124, train_loss=8.659424

Batch 286920, train_perplexity=5369.538, train_loss=8.588497

Batch 286930, train_perplexity=5686.5054, train_loss=8.645851

Batch 286940, train_perplexity=6479.2593, train_loss=8.776361

Batch 286950, train_perplexity=5354.78, train_loss=8.585745

Batch 286960, train_perplexity=4773.844, train_loss=8.470907

Batch 286970, train_perplexity=5306.141, train_loss=8.57662

Batch 286980, train_perplexity=6056.048, train_loss=8.708813

Batch 286990, train_perplexity=5275.9463, train_loss=8.570913

Batch 287000, train_perplexity=6121.5356, train_loss=8.719568

Batch 287010, train_perplexity=4466.5786, train_loss=8.404378

Batch 287020, train_perplexity=5581.679, train_loss=8.627245

Batch 287030, train_perplexity=4742.0366, train_loss=8.464222

Batch 287040, train_perplexity=5343.8525, train_loss=8.583702

Batch 287050, train_perplexity=5004.0386, train_loss=8.518001

Batch 287060, train_perplexity=3884.1702, train_loss=8.264665

Batch 287070, train_perplexity=5090.4717, train_loss=8.535126

Batch 287080, train_perplexity=4433.7554, train_loss=8.397002

Batch 287090, train_perplexity=4836.2427, train_loss=8.483893

Batch 287100, train_perplexity=5993.3296, train_loss=8.698402

Batch 287110, train_perplexity=4789.0654, train_loss=8.474091

Batch 287120, train_perplexity=5764.8833, train_loss=8.65954

Batch 287130, train_perplexity=4930.9375, train_loss=8.503284

Batch 287140, train_perplexity=5540.6143, train_loss=8.619861

Batch 287150, train_perplexity=4635.0728, train_loss=8.441407

Batch 287160, train_perplexity=5730.8374, train_loss=8.653617

Batch 287170, train_perplexity=5998.9565, train_loss=8.699341

Batch 287180, train_perplexity=6117.5786, train_loss=8.718922

Batch 287190, train_perplexity=4432.8125, train_loss=8.39679

Batch 287200, train_perplexity=4522.096, train_loss=8.416731

Batch 287210, train_perplexity=5750.497, train_loss=8.657042

Batch 287220, train_perplexity=4615.9766, train_loss=8.437279

Batch 287230, train_perplexity=4735.4653, train_loss=8.462835

Batch 287240, train_perplexity=5006.631, train_loss=8.518518

Batch 287250, train_perplexity=5240.863, train_loss=8.564241

Batch 287260, train_perplexity=6153.6113, train_loss=8.724794

Batch 287270, train_perplexity=5904.453, train_loss=8.683462

Batch 287280, train_perplexity=6036.4316, train_loss=8.705568

Batch 287290, train_perplexity=4769.9214, train_loss=8.470085

Batch 287300, train_perplexity=4620.8657, train_loss=8.438337

Batch 287310, train_perplexity=4530.1763, train_loss=8.418516

Batch 287320, train_perplexity=5169.634, train_loss=8.550557

Batch 287330, train_perplexity=5028.916, train_loss=8.52296

Batch 287340, train_perplexity=4606.4297, train_loss=8.435208

Batch 287350, train_perplexity=4928.3003, train_loss=8.502749

Batch 287360, train_perplexity=6257.28, train_loss=8.741501

Batch 287370, train_perplexity=4942.5005, train_loss=8.505627

Batch 287380, train_perplexity=6309.7373, train_loss=8.749849

Batch 287390, train_perplexity=5851.0894, train_loss=8.674383

Batch 287400, train_perplexity=5207.7207, train_loss=8.557898

Batch 287410, train_perplexity=5707.5854, train_loss=8.649551

Batch 287420, train_perplexity=4681.4116, train_loss=8.451355

Batch 287430, train_perplexity=5151.955, train_loss=8.547132

Batch 287440, train_perplexity=5436.502, train_loss=8.600891

Batch 287450, train_perplexity=5865.179, train_loss=8.676788

Batch 287460, train_perplexity=4791.0938, train_loss=8.474514

Batch 287470, train_perplexity=5862.562, train_loss=8.676342

Batch 287480, train_perplexity=6063.0405, train_loss=8.709967

Batch 287490, train_perplexity=5883.611, train_loss=8.679926

Batch 287500, train_perplexity=5766.984, train_loss=8.6599045

Batch 287510, train_perplexity=5084.2515, train_loss=8.533903

Batch 287520, train_perplexity=6620.4355, train_loss=8.797916

Batch 287530, train_perplexity=6818.394, train_loss=8.827379

Batch 287540, train_perplexity=4726.3066, train_loss=8.460899

Batch 287550, train_perplexity=5989.678, train_loss=8.697793

Batch 287560, train_perplexity=5212.456, train_loss=8.558806

Batch 287570, train_perplexity=5379.3174, train_loss=8.590317

Batch 287580, train_perplexity=4765.716, train_loss=8.469203

Batch 287590, train_perplexity=5146.8477, train_loss=8.54614

Batch 287600, train_perplexity=4644.1655, train_loss=8.443367

Batch 287610, train_perplexity=5097.924, train_loss=8.536589

Batch 287620, train_perplexity=4865.0947, train_loss=8.489841

Batch 287630, train_perplexity=5234.649, train_loss=8.563055

Batch 287640, train_perplexity=5305.1997, train_loss=8.576443

Batch 287650, train_perplexity=5205.01, train_loss=8.557377

Batch 287660, train_perplexity=5233.5205, train_loss=8.5628395

Batch 287670, train_perplexity=5465.322, train_loss=8.606178

Batch 287680, train_perplexity=5414.295, train_loss=8.596798

Batch 287690, train_perplexity=4965.3193, train_loss=8.510233

Batch 287700, train_perplexity=4837.396, train_loss=8.484132

Batch 287710, train_perplexity=4729.6704, train_loss=8.461611

Batch 287720, train_perplexity=5283.6904, train_loss=8.57238

Batch 287730, train_perplexity=5103.922, train_loss=8.537765

Batch 287740, train_perplexity=4490.2734, train_loss=8.409669

Batch 287750, train_perplexity=4988.8906, train_loss=8.514969

Batch 287760, train_perplexity=3883.5813, train_loss=8.264513

Batch 287770, train_perplexity=5767.9077, train_loss=8.660065

Batch 287780, train_perplexity=5748.095, train_loss=8.656624

Batch 287790, train_perplexity=4964.311, train_loss=8.51003

Batch 287800, train_perplexity=5287.6523, train_loss=8.57313

Batch 287810, train_perplexity=4821.8555, train_loss=8.480914

Batch 287820, train_perplexity=6067.315, train_loss=8.710671

Batch 287830, train_perplexity=6016.523, train_loss=8.702265

Batch 287840, train_perplexity=5912.787, train_loss=8.684873

Batch 287850, train_perplexity=7219.3525, train_loss=8.884521

Batch 287860, train_perplexity=5804.515, train_loss=8.666391

Batch 287870, train_perplexity=5833.9727, train_loss=8.671453
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

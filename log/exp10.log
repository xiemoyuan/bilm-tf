nohup: ignoring input
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-30 12:25:55.581459: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-30 12:25:55.807266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0e:00.0
totalMemory: 22.38GiB freeMemory: 21.80GiB
2020-10-30 12:25:55.807321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-30 12:25:56.362747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-30 12:25:56.362799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-30 12:25:56.362811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-30 12:25:56.364214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21153 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:0e:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
Found 99 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
final_state:  (LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/rnn/multi_rnn_cell/cell_1/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>))
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_high_0/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_0/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_0/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/W_carry:0', TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/W_transform:0',
  TensorShape([Dimension(2048), Dimension(2048)])],
 ['lm/CNN_high_1/b_carry:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_high_1/b_transform:0', TensorShape([Dimension(2048)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0',
  TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=150423.7, train_loss=11.921211

Batch 10, train_perplexity=108388.086, train_loss=11.593473

Batch 20, train_perplexity=35263.98, train_loss=10.470617

Batch 30, train_perplexity=13472.639, train_loss=9.508416

Batch 40, train_perplexity=10190.555, train_loss=9.229217

Batch 50, train_perplexity=7513.9224, train_loss=8.924513

Batch 60, train_perplexity=7031.9355, train_loss=8.858217

Batch 70, train_perplexity=6704.5977, train_loss=8.810549

Batch 80, train_perplexity=6051.868, train_loss=8.708122

Batch 90, train_perplexity=5702.1997, train_loss=8.648607

Batch 100, train_perplexity=4921.2036, train_loss=8.501308

Batch 110, train_perplexity=4647.6304, train_loss=8.444113

Batch 120, train_perplexity=4340.0156, train_loss=8.375633

Batch 130, train_perplexity=4284.1055, train_loss=8.362667

Batch 140, train_perplexity=4286.157, train_loss=8.363146

Batch 150, train_perplexity=4034.0862, train_loss=8.302535

Batch 160, train_perplexity=3915.3772, train_loss=8.272667

Batch 170, train_perplexity=3855.4617, train_loss=8.257246

Batch 180, train_perplexity=3707.45, train_loss=8.2181

Batch 190, train_perplexity=3785.725, train_loss=8.238993

Batch 200, train_perplexity=3300.2805, train_loss=8.101763

Batch 210, train_perplexity=3559.325, train_loss=8.177326

Batch 220, train_perplexity=3344.04, train_loss=8.114935

Batch 230, train_perplexity=3367.6458, train_loss=8.121969

Batch 240, train_perplexity=3031.5015, train_loss=8.016813

Batch 250, train_perplexity=3566.3447, train_loss=8.1792965

Batch 260, train_perplexity=3191.386, train_loss=8.068211

Batch 270, train_perplexity=2975.3345, train_loss=7.9981117

Batch 280, train_perplexity=2934.7678, train_loss=7.9843836

Batch 290, train_perplexity=3080.436, train_loss=8.032826

Batch 300, train_perplexity=2668.58, train_loss=7.889302

Batch 310, train_perplexity=2617.0012, train_loss=7.8697844

Batch 320, train_perplexity=2933.4834, train_loss=7.983946

Batch 330, train_perplexity=2641.8489, train_loss=7.8792343

Batch 340, train_perplexity=2838.7048, train_loss=7.951103

Batch 350, train_perplexity=2764.17, train_loss=7.9244957

Batch 360, train_perplexity=3044.5044, train_loss=8.021093

Batch 370, train_perplexity=2523.9058, train_loss=7.833563

Batch 380, train_perplexity=2716.5212, train_loss=7.9071074

Batch 390, train_perplexity=2750.8923, train_loss=7.9196806

Batch 400, train_perplexity=2757.2432, train_loss=7.9219866

Batch 410, train_perplexity=2577.7998, train_loss=7.8546915

Batch 420, train_perplexity=2344.3484, train_loss=7.759763

Batch 430, train_perplexity=2435.45, train_loss=7.797887

Batch 440, train_perplexity=2366.465, train_loss=7.7691526

Batch 450, train_perplexity=2445.3691, train_loss=7.8019514

Batch 460, train_perplexity=2753.2544, train_loss=7.920539

Batch 470, train_perplexity=2328.4983, train_loss=7.752979

Batch 480, train_perplexity=2438.6992, train_loss=7.79922

Batch 490, train_perplexity=2507.6736, train_loss=7.827111

Batch 500, train_perplexity=2824.9963, train_loss=7.9462624

Batch 510, train_perplexity=2490.2441, train_loss=7.820136

Batch 520, train_perplexity=2575.366, train_loss=7.853747

Batch 530, train_perplexity=2450.9026, train_loss=7.8042116

Batch 540, train_perplexity=2481.0752, train_loss=7.8164473

Batch 550, train_perplexity=2364.5093, train_loss=7.768326

Batch 560, train_perplexity=2253.278, train_loss=7.7201414

Batch 570, train_perplexity=2229.2217, train_loss=7.709408

Batch 580, train_perplexity=2066.9238, train_loss=7.6338167

Batch 590, train_perplexity=2291.433, train_loss=7.7369328

Batch 600, train_perplexity=2296.5679, train_loss=7.739171
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 610, train_perplexity=2485.5227, train_loss=7.8182383

Batch 620, train_perplexity=2182.7288, train_loss=7.688331

Batch 630, train_perplexity=2246.9373, train_loss=7.7173233

Batch 640, train_perplexity=2166.7104, train_loss=7.6809654

Batch 650, train_perplexity=2091.0747, train_loss=7.6454334

Batch 660, train_perplexity=2009.9987, train_loss=7.6058893

Batch 670, train_perplexity=2010.6486, train_loss=7.6062126

Batch 680, train_perplexity=2166.915, train_loss=7.68106

Batch 690, train_perplexity=2284.732, train_loss=7.734004

Batch 700, train_perplexity=2241.4539, train_loss=7.71488

Batch 710, train_perplexity=2010.8883, train_loss=7.606332

Batch 720, train_perplexity=1915.0535, train_loss=7.557501

Batch 730, train_perplexity=1983.525, train_loss=7.592631

Batch 740, train_perplexity=2235.6875, train_loss=7.712304

Batch 750, train_perplexity=2337.1714, train_loss=7.7566967

Batch 760, train_perplexity=2116.2563, train_loss=7.657404

Batch 770, train_perplexity=1961.2625, train_loss=7.5813437

Batch 780, train_perplexity=2176.5022, train_loss=7.6854744

Batch 790, train_perplexity=2010.8192, train_loss=7.6062975

Batch 800, train_perplexity=2308.9229, train_loss=7.7445364

Batch 810, train_perplexity=2144.9014, train_loss=7.670849

Batch 820, train_perplexity=2173.4988, train_loss=7.6840935

Batch 830, train_perplexity=2113.2192, train_loss=7.6559677

Batch 840, train_perplexity=2101.6167, train_loss=7.650462

Batch 850, train_perplexity=2194.4033, train_loss=7.6936655

Batch 860, train_perplexity=1981.3688, train_loss=7.591543

Batch 870, train_perplexity=1963.6805, train_loss=7.582576

Batch 880, train_perplexity=2004.0269, train_loss=7.602914

Batch 890, train_perplexity=1922.8844, train_loss=7.5615816

Batch 900, train_perplexity=2086.4055, train_loss=7.643198

Batch 910, train_perplexity=1922.1803, train_loss=7.5612154

Batch 920, train_perplexity=1981.7391, train_loss=7.59173

Batch 930, train_perplexity=1899.4619, train_loss=7.549326

Batch 940, train_perplexity=2043.4475, train_loss=7.6223936

Batch 950, train_perplexity=2069.6912, train_loss=7.6351547

Batch 960, train_perplexity=1940.9186, train_loss=7.5709167

Batch 970, train_perplexity=1895.932, train_loss=7.547466

Batch 980, train_perplexity=1983.3463, train_loss=7.5925407

Batch 990, train_perplexity=1939.3209, train_loss=7.570093

Batch 1000, train_perplexity=2048.291, train_loss=7.624761

Batch 1010, train_perplexity=2025.9487, train_loss=7.6137934

Batch 1020, train_perplexity=1942.9882, train_loss=7.5719824

Batch 1030, train_perplexity=2066.3818, train_loss=7.6335545

Batch 1040, train_perplexity=1935.5867, train_loss=7.568166

Batch 1050, train_perplexity=2044.4669, train_loss=7.6228924

Batch 1060, train_perplexity=1883.0624, train_loss=7.5406547

Batch 1070, train_perplexity=1849.2921, train_loss=7.522558

Batch 1080, train_perplexity=1870.6672, train_loss=7.5340505

Batch 1090, train_perplexity=1857.0985, train_loss=7.5267706

Batch 1100, train_perplexity=1915.9614, train_loss=7.557975

Batch 1110, train_perplexity=1873.4371, train_loss=7.53553

Batch 1120, train_perplexity=1873.6115, train_loss=7.535623

Batch 1130, train_perplexity=1868.5607, train_loss=7.5329237

Batch 1140, train_perplexity=1845.8677, train_loss=7.5207047

Batch 1150, train_perplexity=1736.98, train_loss=7.4599032

Batch 1160, train_perplexity=2060.1472, train_loss=7.6305327

Batch 1170, train_perplexity=1741.3811, train_loss=7.462434

Batch 1180, train_perplexity=1724.228, train_loss=7.4525347

Batch 1190, train_perplexity=1848.0245, train_loss=7.5218725

Batch 1200, train_perplexity=1762.6819, train_loss=7.4745917

Batch 1210, train_perplexity=1688.9138, train_loss=7.431841

Batch 1220, train_perplexity=1742.4808, train_loss=7.463065

Batch 1230, train_perplexity=1970.3512, train_loss=7.585967

Batch 1240, train_perplexity=2003.2167, train_loss=7.6025095

Batch 1250, train_perplexity=1798.8925, train_loss=7.4949265

Batch 1260, train_perplexity=1881.3939, train_loss=7.539768

Batch 1270, train_perplexity=1763.2611, train_loss=7.4749203

Batch 1280, train_perplexity=1879.4966, train_loss=7.538759

Batch 1290, train_perplexity=1769.2273, train_loss=7.478298

Batch 1300, train_perplexity=1868.092, train_loss=7.532673

Batch 1310, train_perplexity=1859.5548, train_loss=7.5280924

Batch 1320, train_perplexity=1806.4498, train_loss=7.499119

Batch 1330, train_perplexity=1829.5706, train_loss=7.5118365

Batch 1340, train_perplexity=1871.3462, train_loss=7.5344133

Batch 1350, train_perplexity=1749.4609, train_loss=7.467063

Batch 1360, train_perplexity=1710.7936, train_loss=7.4447126

Batch 1370, train_perplexity=1705.2847, train_loss=7.4414873

Batch 1380, train_perplexity=1850.1688, train_loss=7.523032

Batch 1390, train_perplexity=1656.4779, train_loss=7.412449

Batch 1400, train_perplexity=1786.1322, train_loss=7.4878078

Batch 1410, train_perplexity=1778.6945, train_loss=7.483635

Batch 1420, train_perplexity=1900.589, train_loss=7.549919

Batch 1430, train_perplexity=1864.319, train_loss=7.530651

Batch 1440, train_perplexity=1864.9858, train_loss=7.5310087

Batch 1450, train_perplexity=1775.3085, train_loss=7.4817295

Batch 1460, train_perplexity=1777.7058, train_loss=7.483079

Batch 1470, train_perplexity=1779.9893, train_loss=7.4843626

Batch 1480, train_perplexity=1782.5731, train_loss=7.485813

Batch 1490, train_perplexity=1811.4718, train_loss=7.501895

Batch 1500, train_perplexity=1792.4766, train_loss=7.4913535

Batch 1510, train_perplexity=1647.0203, train_loss=7.406723

Batch 1520, train_perplexity=1591.3474, train_loss=7.3723364

Batch 1530, train_perplexity=1642.2068, train_loss=7.403796

Batch 1540, train_perplexity=1785.039, train_loss=7.4871955

Batch 1550, train_perplexity=1817.0397, train_loss=7.504964

Batch 1560, train_perplexity=1779.7101, train_loss=7.4842057

Batch 1570, train_perplexity=1619.6804, train_loss=7.389984

Batch 1580, train_perplexity=1715.5071, train_loss=7.447464

Batch 1590, train_perplexity=1666.6382, train_loss=7.418564

Batch 1600, train_perplexity=1767.8342, train_loss=7.4775105

Batch 1610, train_perplexity=1616.2657, train_loss=7.3878736

Batch 1620, train_perplexity=1783.7694, train_loss=7.486484

Batch 1630, train_perplexity=1681.7295, train_loss=7.427578

Batch 1640, train_perplexity=1738.1921, train_loss=7.460601

Batch 1650, train_perplexity=1678.2799, train_loss=7.4255247

Batch 1660, train_perplexity=1833.275, train_loss=7.5138593

Batch 1670, train_perplexity=1740.3427, train_loss=7.4618373

Batch 1680, train_perplexity=1663.8281, train_loss=7.4168763

Batch 1690, train_perplexity=1728.8276, train_loss=7.455199

Batch 1700, train_perplexity=1439.6451, train_loss=7.272152

Batch 1710, train_perplexity=1645.4833, train_loss=7.4057894

Batch 1720, train_perplexity=1748.4167, train_loss=7.466466

Batch 1730, train_perplexity=1649.194, train_loss=7.408042

Batch 1740, train_perplexity=1661.403, train_loss=7.4154177

Batch 1750, train_perplexity=1545.1605, train_loss=7.342883

Batch 1760, train_perplexity=1541.8854, train_loss=7.340761

Batch 1770, train_perplexity=1549.7495, train_loss=7.3458486

Batch 1780, train_perplexity=1457.777, train_loss=7.284668

Batch 1790, train_perplexity=1497.0826, train_loss=7.3112736

Batch 1800, train_perplexity=1626.4958, train_loss=7.394183

Batch 1810, train_perplexity=1464.9991, train_loss=7.28961

Batch 1820, train_perplexity=1703.4747, train_loss=7.4404254

Batch 1830, train_perplexity=1632.0078, train_loss=7.3975663

Batch 1840, train_perplexity=1662.8755, train_loss=7.4163036

Batch 1850, train_perplexity=1497.0198, train_loss=7.3112316

Batch 1860, train_perplexity=1632.4811, train_loss=7.397856

Batch 1870, train_perplexity=1535.3323, train_loss=7.336502

Batch 1880, train_perplexity=1663.6512, train_loss=7.41677

Batch 1890, train_perplexity=1381.6007, train_loss=7.230998

Batch 1900, train_perplexity=1594.6708, train_loss=7.3744226

Batch 1910, train_perplexity=1508.7373, train_loss=7.3190284

Batch 1920, train_perplexity=1557.2021, train_loss=7.350646

Batch 1930, train_perplexity=1419.1433, train_loss=7.2578087

Batch 1940, train_perplexity=1642.2632, train_loss=7.4038305

Batch 1950, train_perplexity=1467.5958, train_loss=7.291381
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 1960, train_perplexity=1523.2622, train_loss=7.3286095

Batch 1970, train_perplexity=1458.03, train_loss=7.2848415

Batch 1980, train_perplexity=1571.0066, train_loss=7.359472

Batch 1990, train_perplexity=1535.5826, train_loss=7.336665

Batch 2000, train_perplexity=1758.7115, train_loss=7.472337

Batch 2010, train_perplexity=1508.0879, train_loss=7.318598

Batch 2020, train_perplexity=1522.3407, train_loss=7.3280044

Batch 2030, train_perplexity=1388.6361, train_loss=7.2360773

Batch 2040, train_perplexity=1458.7046, train_loss=7.285304

Batch 2050, train_perplexity=1472.7822, train_loss=7.2949085

Batch 2060, train_perplexity=1430.3871, train_loss=7.2657003

Batch 2070, train_perplexity=1515.9502, train_loss=7.3237977

Batch 2080, train_perplexity=1382.762, train_loss=7.231838

Batch 2090, train_perplexity=1500.4988, train_loss=7.313553

Batch 2100, train_perplexity=1572.7026, train_loss=7.360551

Batch 2110, train_perplexity=1634.0012, train_loss=7.398787

Batch 2120, train_perplexity=1482.6759, train_loss=7.301604

Batch 2130, train_perplexity=1577.4885, train_loss=7.3635893

Batch 2140, train_perplexity=1537.2809, train_loss=7.3377705

Batch 2150, train_perplexity=1461.0498, train_loss=7.2869105

Batch 2160, train_perplexity=1258.6313, train_loss=7.13778

Batch 2170, train_perplexity=1462.4572, train_loss=7.2878733

Batch 2180, train_perplexity=1400.5741, train_loss=7.2446375

Batch 2190, train_perplexity=1444.4757, train_loss=7.2755017

Batch 2200, train_perplexity=1360.4651, train_loss=7.215582

Batch 2210, train_perplexity=1442.7004, train_loss=7.274272

Batch 2220, train_perplexity=1336.7622, train_loss=7.1980057

Batch 2230, train_perplexity=1422.6055, train_loss=7.2602453

Batch 2240, train_perplexity=1411.112, train_loss=7.2521334

Batch 2250, train_perplexity=1410.8241, train_loss=7.2519293

Batch 2260, train_perplexity=1455.4058, train_loss=7.28304

Batch 2270, train_perplexity=1267.8462, train_loss=7.145075

Batch 2280, train_perplexity=1492.3401, train_loss=7.3081007

Batch 2290, train_perplexity=1450.8175, train_loss=7.2798824

Batch 2300, train_perplexity=1370.6163, train_loss=7.223016

Batch 2310, train_perplexity=1364.6395, train_loss=7.2186456

Batch 2320, train_perplexity=1342.0376, train_loss=7.2019444

Batch 2330, train_perplexity=1387.4294, train_loss=7.235208

Batch 2340, train_perplexity=1384.6622, train_loss=7.2332115

Batch 2350, train_perplexity=1395.5928, train_loss=7.2410746

Batch 2360, train_perplexity=1326.8528, train_loss=7.190565

Batch 2370, train_perplexity=1365.5201, train_loss=7.2192907

Batch 2380, train_perplexity=1322.8955, train_loss=7.187578

Batch 2390, train_perplexity=1363.6488, train_loss=7.2179193

Batch 2400, train_perplexity=1365.3041, train_loss=7.2191324

Batch 2410, train_perplexity=1388.352, train_loss=7.2358727

Batch 2420, train_perplexity=1362.7876, train_loss=7.2172875

Batch 2430, train_perplexity=1362.8259, train_loss=7.2173157

Batch 2440, train_perplexity=1347.7876, train_loss=7.2062197

Batch 2450, train_perplexity=1192.0085, train_loss=7.083395

Batch 2460, train_perplexity=1411.2056, train_loss=7.2521996

Batch 2470, train_perplexity=1303.9388, train_loss=7.173145

Batch 2480, train_perplexity=1272.0427, train_loss=7.1483793

Batch 2490, train_perplexity=1430.5377, train_loss=7.2658057

Batch 2500, train_perplexity=1416.8296, train_loss=7.256177

Batch 2510, train_perplexity=1296.2152, train_loss=7.167204

Batch 2520, train_perplexity=1358.6045, train_loss=7.2142134

Batch 2530, train_perplexity=1342.7725, train_loss=7.2024918

Batch 2540, train_perplexity=1262.7842, train_loss=7.141074

Batch 2550, train_perplexity=1287.2126, train_loss=7.1602345

Batch 2560, train_perplexity=1245.7184, train_loss=7.1274676

Batch 2570, train_perplexity=1283.3987, train_loss=7.157267

Batch 2580, train_perplexity=1337.5056, train_loss=7.1985617

Batch 2590, train_perplexity=1252.1036, train_loss=7.1325803

Batch 2600, train_perplexity=1281.775, train_loss=7.156001

Batch 2610, train_perplexity=1330.2115, train_loss=7.1930933

Batch 2620, train_perplexity=1232.7603, train_loss=7.117011

Batch 2630, train_perplexity=1224.849, train_loss=7.110573

Batch 2640, train_perplexity=1279.2892, train_loss=7.15406

Batch 2650, train_perplexity=1301.9066, train_loss=7.171585

Batch 2660, train_perplexity=1278.5032, train_loss=7.1534452

Batch 2670, train_perplexity=1387.718, train_loss=7.235416

Batch 2680, train_perplexity=1328.4691, train_loss=7.1917825

Batch 2690, train_perplexity=1269.1866, train_loss=7.1461315

Batch 2700, train_perplexity=1351.4886, train_loss=7.208962

Batch 2710, train_perplexity=1212.2693, train_loss=7.1002493

Batch 2720, train_perplexity=1276.7017, train_loss=7.152035

Batch 2730, train_perplexity=1301.2959, train_loss=7.171116

Batch 2740, train_perplexity=1317.0546, train_loss=7.183153

Batch 2750, train_perplexity=1320.915, train_loss=7.18608

Batch 2760, train_perplexity=1277.6962, train_loss=7.152814

Batch 2770, train_perplexity=1269.0208, train_loss=7.146001

Batch 2780, train_perplexity=1346.4386, train_loss=7.2052183

Batch 2790, train_perplexity=1248.3151, train_loss=7.12955

Batch 2800, train_perplexity=1249.6455, train_loss=7.130615

Batch 2810, train_perplexity=1181.5739, train_loss=7.0746026

Batch 2820, train_perplexity=1335.8287, train_loss=7.197307

Batch 2830, train_perplexity=1304.019, train_loss=7.1732063

Batch 2840, train_perplexity=1345.8076, train_loss=7.2047496

Batch 2850, train_perplexity=1296.4198, train_loss=7.1673617

Batch 2860, train_perplexity=1381.6317, train_loss=7.2310205

Batch 2870, train_perplexity=1324.3687, train_loss=7.188691

Batch 2880, train_perplexity=1333.1274, train_loss=7.195283

Batch 2890, train_perplexity=1261.6232, train_loss=7.1401544

Batch 2900, train_perplexity=1298.9872, train_loss=7.16934

Batch 2910, train_perplexity=1269.4541, train_loss=7.1463423

Batch 2920, train_perplexity=1316.4028, train_loss=7.182658

Batch 2930, train_perplexity=1322.3575, train_loss=7.1871715

Batch 2940, train_perplexity=1356.685, train_loss=7.2127995

Batch 2950, train_perplexity=1362.0787, train_loss=7.2167673

Batch 2960, train_perplexity=1298.7964, train_loss=7.1691933

Batch 2970, train_perplexity=1177.4563, train_loss=7.0711117

Batch 2980, train_perplexity=1197.9847, train_loss=7.088396

Batch 2990, train_perplexity=1217.1237, train_loss=7.1042457

Batch 3000, train_perplexity=1262.7661, train_loss=7.14106

Batch 3010, train_perplexity=1121.6694, train_loss=7.0225735

Batch 3020, train_perplexity=1202.7969, train_loss=7.092405

Batch 3030, train_perplexity=1282.3435, train_loss=7.1564445

Batch 3040, train_perplexity=1343.2509, train_loss=7.202848

Batch 3050, train_perplexity=1395.129, train_loss=7.240742

Batch 3060, train_perplexity=1215.1062, train_loss=7.1025867

Batch 3070, train_perplexity=1161.7253, train_loss=7.0576615

Batch 3080, train_perplexity=1250.6542, train_loss=7.131422

Batch 3090, train_perplexity=1192.5725, train_loss=7.083868

Batch 3100, train_perplexity=1181.934, train_loss=7.0749073

Batch 3110, train_perplexity=1145.4506, train_loss=7.0435534

Batch 3120, train_perplexity=1165.2021, train_loss=7.06065

Batch 3130, train_perplexity=1253.4183, train_loss=7.13363

Batch 3140, train_perplexity=1281.8312, train_loss=7.156045

Batch 3150, train_perplexity=1165.8679, train_loss=7.061221

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Batch 3160, train_perplexity=1206.0509, train_loss=7.0951066

Batch 3170, train_perplexity=1108.6566, train_loss=7.0109043

Batch 3180, train_perplexity=1211.3124, train_loss=7.0994596

Batch 3190, train_perplexity=1408.845, train_loss=7.2505255

Batch 3200, train_perplexity=1265.4545, train_loss=7.1431866

Batch 3210, train_perplexity=1204.053, train_loss=7.0934486

Batch 3220, train_perplexity=1204.9203, train_loss=7.0941687

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 3230, train_perplexity=1201.3232, train_loss=7.091179

Batch 3240, train_perplexity=1206.5939, train_loss=7.0955567

Batch 3250, train_perplexity=1312.6746, train_loss=7.179822

Batch 3260, train_perplexity=1328.2493, train_loss=7.191617

Batch 3270, train_perplexity=1228.0256, train_loss=7.113163

Batch 3280, train_perplexity=1239.8333, train_loss=7.122732

Batch 3290, train_perplexity=1220.2786, train_loss=7.1068344

Batch 3300, train_perplexity=1258.0242, train_loss=7.1372976

Batch 3310, train_perplexity=1097.4398, train_loss=7.0007353

Batch 3320, train_perplexity=1112.0049, train_loss=7.01392

Batch 3330, train_perplexity=1199.1381, train_loss=7.0893583

Batch 3340, train_perplexity=1320.5636, train_loss=7.185814

Batch 3350, train_perplexity=1239.1819, train_loss=7.1222067

Batch 3360, train_perplexity=1254.874, train_loss=7.1347904

Batch 3370, train_perplexity=1241.3121, train_loss=7.1239243

Batch 3380, train_perplexity=1227.9905, train_loss=7.1131344

Batch 3390, train_perplexity=1320.7903, train_loss=7.1859856

Batch 3400, train_perplexity=1262.0365, train_loss=7.140482

Batch 3410, train_perplexity=1279.2935, train_loss=7.154063

Batch 3420, train_perplexity=1179.5321, train_loss=7.072873

Batch 3430, train_perplexity=1232.2466, train_loss=7.1165943

Batch 3440, train_perplexity=1185.2604, train_loss=7.077718

Batch 3450, train_perplexity=1262.9021, train_loss=7.1411676

Batch 3460, train_perplexity=1388.6719, train_loss=7.236103

Batch 3470, train_perplexity=1092.8402, train_loss=6.9965353

Batch 3480, train_perplexity=1156.5502, train_loss=7.053197

Batch 3490, train_perplexity=1263.3213, train_loss=7.1414995

Batch 3500, train_perplexity=1153.8291, train_loss=7.0508413

Batch 3510, train_perplexity=1300.1311, train_loss=7.1702204

Batch 3520, train_perplexity=1300.7574, train_loss=7.170702

Batch 3530, train_perplexity=1343.6686, train_loss=7.203159

Batch 3540, train_perplexity=1204.6628, train_loss=7.093955

Batch 3550, train_perplexity=1143.0367, train_loss=7.041444

Batch 3560, train_perplexity=1195.6952, train_loss=7.086483

Batch 3570, train_perplexity=1245.1488, train_loss=7.1270103

Batch 3580, train_perplexity=1135.0104, train_loss=7.034397

Batch 3590, train_perplexity=1190.9495, train_loss=7.082506

Batch 3600, train_perplexity=1243.6868, train_loss=7.1258354

Batch 3610, train_perplexity=1072.0698, train_loss=6.9773464

Batch 3620, train_perplexity=1201.8859, train_loss=7.091647

Batch 3630, train_perplexity=1182.6652, train_loss=7.0755258

Batch 3640, train_perplexity=1207.5166, train_loss=7.096321

Batch 3650, train_perplexity=1104.6646, train_loss=7.007297

Batch 3660, train_perplexity=1155.298, train_loss=7.0521135

Batch 3670, train_perplexity=1153.2856, train_loss=7.05037

Batch 3680, train_perplexity=1352.3905, train_loss=7.209629

Batch 3690, train_perplexity=1251.6362, train_loss=7.132207

Batch 3700, train_perplexity=1227.0703, train_loss=7.112385

Batch 3710, train_perplexity=1215.0969, train_loss=7.102579

Batch 3720, train_perplexity=1067.6743, train_loss=6.973238

Batch 3730, train_perplexity=1085.5846, train_loss=6.989874

Batch 3740, train_perplexity=1182.4344, train_loss=7.0753307

Batch 3750, train_perplexity=1149.6564, train_loss=7.0472183

Batch 3760, train_perplexity=1234.2543, train_loss=7.118222

Batch 3770, train_perplexity=1162.0472, train_loss=7.0579386

Batch 3780, train_perplexity=1237.9304, train_loss=7.1211963

Batch 3790, train_perplexity=1173.8528, train_loss=7.0680466

Batch 3800, train_perplexity=1170.0204, train_loss=7.0647764

Batch 3810, train_perplexity=1164.7811, train_loss=7.0602884

Batch 3820, train_perplexity=1258.9327, train_loss=7.1380196

Batch 3830, train_perplexity=1159.9795, train_loss=7.0561576

Batch 3840, train_perplexity=1207.2633, train_loss=7.0961113

Batch 3850, train_perplexity=1275.6812, train_loss=7.1512356

Batch 3860, train_perplexity=1222.0243, train_loss=7.108264

Batch 3870, train_perplexity=1175.8292, train_loss=7.069729

Batch 3880, train_perplexity=1104.0475, train_loss=7.006738

Batch 3890, train_perplexity=1163.0673, train_loss=7.058816

Batch 3900, train_perplexity=1197.1156, train_loss=7.0876703

Batch 3910, train_perplexity=1063.1954, train_loss=6.969034

Batch 3920, train_perplexity=1188.696, train_loss=7.080612

Batch 3930, train_perplexity=1139.4207, train_loss=7.0382752

Batch 3940, train_perplexity=1127.6361, train_loss=7.0278788

Batch 3950, train_perplexity=1207.8862, train_loss=7.096627

Batch 3960, train_perplexity=1132.8973, train_loss=7.0325336

Batch 3970, train_perplexity=1151.76, train_loss=7.0490465

Batch 3980, train_perplexity=1043.9895, train_loss=6.9508047

Batch 3990, train_perplexity=1231.6157, train_loss=7.116082

Batch 4000, train_perplexity=1137.5244, train_loss=7.0366096

Batch 4010, train_perplexity=1223.4364, train_loss=7.109419

Batch 4020, train_perplexity=1145.6871, train_loss=7.04376

Batch 4030, train_perplexity=1105.9785, train_loss=7.008486

Batch 4040, train_perplexity=1226.1322, train_loss=7.11162

Batch 4050, train_perplexity=1165.0988, train_loss=7.060561

Batch 4060, train_perplexity=1192.6874, train_loss=7.0839643

Batch 4070, train_perplexity=1090.4999, train_loss=6.9943914

Batch 4080, train_perplexity=1127.3522, train_loss=7.027627

Batch 4090, train_perplexity=1233.1907, train_loss=7.11736

Batch 4100, train_perplexity=1051.1216, train_loss=6.957613

Batch 4110, train_perplexity=1125.4976, train_loss=7.0259805

Batch 4120, train_perplexity=1155.7222, train_loss=7.0524807

Batch 4130, train_perplexity=1167.8866, train_loss=7.062951

Batch 4140, train_perplexity=1135.4012, train_loss=7.0347414

Batch 4150, train_perplexity=1121.1829, train_loss=7.0221395

Batch 4160, train_perplexity=1136.206, train_loss=7.03545

Batch 4170, train_perplexity=1141.8002, train_loss=7.0403614

Batch 4180, train_perplexity=1295.6831, train_loss=7.1667933

Batch 4190, train_perplexity=1121.8342, train_loss=7.0227203

Batch 4200, train_perplexity=1153.2582, train_loss=7.0503464

Batch 4210, train_perplexity=1069.2058, train_loss=6.9746714

Batch 4220, train_perplexity=1181.0359, train_loss=7.074147

Batch 4230, train_perplexity=1111.3889, train_loss=7.0133657

Batch 4240, train_perplexity=978.27966, train_loss=6.8857956

Batch 4250, train_perplexity=1132.1327, train_loss=7.0318584

Batch 4260, train_perplexity=1085.3755, train_loss=6.9896812

Batch 4270, train_perplexity=1087.347, train_loss=6.991496

Batch 4280, train_perplexity=1206.4893, train_loss=7.09547

Batch 4290, train_perplexity=1054.1292, train_loss=6.96047

Batch 4300, train_perplexity=1245.3038, train_loss=7.127135

Batch 4310, train_perplexity=1202.8422, train_loss=7.0924425

Batch 4320, train_perplexity=1043.9218, train_loss=6.95074

Batch 4330, train_perplexity=1048.3505, train_loss=6.954973

Batch 4340, train_perplexity=1229.1288, train_loss=7.114061

Batch 4350, train_perplexity=1090.5524, train_loss=6.9944396

Batch 4360, train_perplexity=1089.8865, train_loss=6.993829

Batch 4370, train_perplexity=1043.0889, train_loss=6.9499416

Batch 4380, train_perplexity=1075.2439, train_loss=6.980303

Batch 4390, train_perplexity=1163.7163, train_loss=7.059374

Batch 4400, train_perplexity=1013.62616, train_loss=6.9212894

Batch 4410, train_perplexity=1110.3845, train_loss=7.0124617

Batch 4420, train_perplexity=1217.6815, train_loss=7.104704

Batch 4430, train_perplexity=1201.7151, train_loss=7.091505

Batch 4440, train_perplexity=1118.2896, train_loss=7.0195556

Batch 4450, train_perplexity=1110.3898, train_loss=7.0124664

Batch 4460, train_perplexity=1110.3782, train_loss=7.012456

Batch 4470, train_perplexity=1095.3549, train_loss=6.9988337

Batch 4480, train_perplexity=1153.7031, train_loss=7.050732

Batch 4490, train_perplexity=1048.2104, train_loss=6.9548397

Batch 4500, train_perplexity=1181.9609, train_loss=7.07493

Batch 4510, train_perplexity=1064.7205, train_loss=6.9704676

Batch 4520, train_perplexity=1216.3473, train_loss=7.1036077

Batch 4530, train_perplexity=1053.8325, train_loss=6.960189

Batch 4540, train_perplexity=1102.5885, train_loss=7.005416

Batch 4550, train_perplexity=1172.9855, train_loss=7.0673075

Batch 4560, train_perplexity=1130.2621, train_loss=7.030205
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4570, train_perplexity=1066.9761, train_loss=6.972584

Batch 4580, train_perplexity=1116.2123, train_loss=7.0176964

Batch 4590, train_perplexity=1029.2852, train_loss=6.9366198

Batch 4600, train_perplexity=1093.2875, train_loss=6.9969444

Batch 4610, train_perplexity=1186.9095, train_loss=7.079108

Batch 4620, train_perplexity=1089.9696, train_loss=6.993905

Batch 4630, train_perplexity=990.39484, train_loss=6.8981037

Batch 4640, train_perplexity=1089.45, train_loss=6.993428

Batch 4650, train_perplexity=1024.2402, train_loss=6.9317064

Batch 4660, train_perplexity=1042.4738, train_loss=6.949352

Batch 4670, train_perplexity=1131.5304, train_loss=7.0313263

Batch 4680, train_perplexity=1112.2137, train_loss=7.0141077

Batch 4690, train_perplexity=1066.5273, train_loss=6.972163

Batch 4700, train_perplexity=1122.6198, train_loss=7.0234203

Batch 4710, train_perplexity=1158.2699, train_loss=7.0546827

Batch 4720, train_perplexity=1130.2157, train_loss=7.030164

Batch 4730, train_perplexity=1025.2, train_loss=6.932643

Batch 4740, train_perplexity=1033.773, train_loss=6.9409704

Batch 4750, train_perplexity=1074.6996, train_loss=6.9797964

Batch 4760, train_perplexity=1167.6683, train_loss=7.062764

Batch 4770, train_perplexity=1247.6879, train_loss=7.1290474

Batch 4780, train_perplexity=1186.3732, train_loss=7.078656

Batch 4790, train_perplexity=1107.1262, train_loss=7.009523

Batch 4800, train_perplexity=1123.7124, train_loss=7.024393

Batch 4810, train_perplexity=1108.3754, train_loss=7.0106506

Batch 4820, train_perplexity=1184.3564, train_loss=7.076955

Batch 4830, train_perplexity=1009.8025, train_loss=6.91751

Batch 4840, train_perplexity=1049.4358, train_loss=6.956008

Batch 4850, train_perplexity=1133.1848, train_loss=7.0327873

Batch 4860, train_perplexity=1088.3046, train_loss=6.9923763

Batch 4870, train_perplexity=1106.6929, train_loss=7.0091314

Batch 4880, train_perplexity=1063.1173, train_loss=6.968961

Batch 4890, train_perplexity=1184.9767, train_loss=7.0774784

Batch 4900, train_perplexity=1038.7032, train_loss=6.9457283

Batch 4910, train_perplexity=1142.9244, train_loss=7.0413456

Batch 4920, train_perplexity=1108.9739, train_loss=7.0111904

Batch 4930, train_perplexity=1163.0262, train_loss=7.0587807

Batch 4940, train_perplexity=1087.319, train_loss=6.9914703

Batch 4950, train_perplexity=1059.7289, train_loss=6.9657683

Batch 4960, train_perplexity=1104.0448, train_loss=7.006736

Batch 4970, train_perplexity=1082.4758, train_loss=6.987006

Batch 4980, train_perplexity=1217.2345, train_loss=7.1043367

Batch 4990, train_perplexity=1088.0037, train_loss=6.9921

Batch 5000, train_perplexity=1130.5197, train_loss=7.0304327

Batch 5010, train_perplexity=1204.2643, train_loss=7.093624

Batch 5020, train_perplexity=1110.5455, train_loss=7.0126066

Batch 5030, train_perplexity=1136.7155, train_loss=7.035898

Batch 5040, train_perplexity=1125.4788, train_loss=7.025964

Batch 5050, train_perplexity=1099.0486, train_loss=7.0022

Batch 5060, train_perplexity=1116.5594, train_loss=7.0180073

Batch 5070, train_perplexity=1111.2183, train_loss=7.013212

Batch 5080, train_perplexity=1066.7344, train_loss=6.9723573

Batch 5090, train_perplexity=1130.1499, train_loss=7.0301056

Batch 5100, train_perplexity=1164.8966, train_loss=7.0603876

Batch 5110, train_perplexity=1060.8811, train_loss=6.966855

Batch 5120, train_perplexity=1064.819, train_loss=6.97056

Batch 5130, train_perplexity=1048.5585, train_loss=6.9551716

Batch 5140, train_perplexity=1124.8381, train_loss=7.0253944

Batch 5150, train_perplexity=1141.4252, train_loss=7.040033

Batch 5160, train_perplexity=1126.3389, train_loss=7.0267277

Batch 5170, train_perplexity=1064.5234, train_loss=6.9702826

Batch 5180, train_perplexity=1060.2732, train_loss=6.966282

Batch 5190, train_perplexity=1156.7753, train_loss=7.0533915

Batch 5200, train_perplexity=1093.8444, train_loss=6.9974537

Batch 5210, train_perplexity=1043.1237, train_loss=6.949975

Batch 5220, train_perplexity=1133.2312, train_loss=7.0328283

Batch 5230, train_perplexity=1041.3435, train_loss=6.948267

Batch 5240, train_perplexity=1028.7571, train_loss=6.9361067

Batch 5250, train_perplexity=1074.0356, train_loss=6.9791784

Batch 5260, train_perplexity=1023.5001, train_loss=6.9309835

Batch 5270, train_perplexity=1091.9589, train_loss=6.9957285

Batch 5280, train_perplexity=1072.3264, train_loss=6.977586

Batch 5290, train_perplexity=1089.1436, train_loss=6.993147

Batch 5300, train_perplexity=1061.4731, train_loss=6.967413

Batch 5310, train_perplexity=1131.88, train_loss=7.0316353

Batch 5320, train_perplexity=1022.7171, train_loss=6.930218

Batch 5330, train_perplexity=1153.7675, train_loss=7.050788

Batch 5340, train_perplexity=981.11066, train_loss=6.888685

Batch 5350, train_perplexity=1068.2446, train_loss=6.973772

Batch 5360, train_perplexity=989.3744, train_loss=6.897073

Batch 5370, train_perplexity=1042.4027, train_loss=6.9492836

Batch 5380, train_perplexity=1123.2988, train_loss=7.024025

Batch 5390, train_perplexity=1043.9387, train_loss=6.950756

Batch 5400, train_perplexity=1048.6604, train_loss=6.955269

Batch 5410, train_perplexity=1034.1989, train_loss=6.9413824

Batch 5420, train_perplexity=1086.1448, train_loss=6.99039

Batch 5430, train_perplexity=1127.8065, train_loss=7.02803

Batch 5440, train_perplexity=1095.8716, train_loss=6.9993052

Batch 5450, train_perplexity=1078.8523, train_loss=6.983653

Batch 5460, train_perplexity=1063.599, train_loss=6.9694138

Batch 5470, train_perplexity=1074.0623, train_loss=6.979203

Batch 5480, train_perplexity=1049.3447, train_loss=6.955921

Batch 5490, train_perplexity=1019.5799, train_loss=6.927146

Batch 5500, train_perplexity=1098.0889, train_loss=7.0013266

Batch 5510, train_perplexity=1108.4811, train_loss=7.010746

Batch 5520, train_perplexity=1152.3962, train_loss=7.0495987

Batch 5530, train_perplexity=1147.2654, train_loss=7.0451365

Batch 5540, train_perplexity=989.8047, train_loss=6.8975077

Batch 5550, train_perplexity=1104.9191, train_loss=7.0075274

Batch 5560, train_perplexity=1111.6782, train_loss=7.013626

Batch 5570, train_perplexity=1061.675, train_loss=6.967603

Batch 5580, train_perplexity=1042.2064, train_loss=6.9490952

Batch 5590, train_perplexity=1053.6135, train_loss=6.959981

Batch 5600, train_perplexity=1006.582, train_loss=6.9143157

Batch 5610, train_perplexity=1024.739, train_loss=6.9321933

Batch 5620, train_perplexity=1041.3385, train_loss=6.948262

Batch 5630, train_perplexity=984.8501, train_loss=6.8924894

Batch 5640, train_perplexity=1002.3475, train_loss=6.9101

Batch 5650, train_perplexity=1054.4197, train_loss=6.960746

Batch 5660, train_perplexity=1016.8736, train_loss=6.924488

Batch 5670, train_perplexity=972.1304, train_loss=6.87949

Batch 5680, train_perplexity=1048.988, train_loss=6.955581

Batch 5690, train_perplexity=1100.3732, train_loss=7.0034046

Batch 5700, train_perplexity=1115.9558, train_loss=7.0174665

Batch 5710, train_perplexity=982.1975, train_loss=6.8897924

Batch 5720, train_perplexity=944.3963, train_loss=6.850546

Batch 5730, train_perplexity=1072.5887, train_loss=6.9778304

Batch 5740, train_perplexity=1083.7086, train_loss=6.9881444

Batch 5750, train_perplexity=1111.0153, train_loss=7.0130296

Batch 5760, train_perplexity=1128.0642, train_loss=7.0282583

Batch 5770, train_perplexity=1043.9373, train_loss=6.9507546

Batch 5780, train_perplexity=1059.6803, train_loss=6.9657226

Batch 5790, train_perplexity=983.703, train_loss=6.891324

Batch 5800, train_perplexity=1118.5834, train_loss=7.0198183

Batch 5810, train_perplexity=1020.9041, train_loss=6.928444

Batch 5820, train_perplexity=972.179, train_loss=6.87954

Batch 5830, train_perplexity=1034.1876, train_loss=6.9413714

Batch 5840, train_perplexity=990.41565, train_loss=6.8981247

Batch 5850, train_perplexity=1108.0942, train_loss=7.010397

Batch 5860, train_perplexity=979.285, train_loss=6.8868227

Batch 5870, train_perplexity=1099.1869, train_loss=7.002326

Batch 5880, train_perplexity=891.2878, train_loss=6.7926674

Batch 5890, train_perplexity=1067.1235, train_loss=6.972722

Batch 5900, train_perplexity=995.3741, train_loss=6.9031186
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 5910, train_perplexity=974.7743, train_loss=6.882206

Batch 5920, train_perplexity=1020.2452, train_loss=6.9277983

Batch 5930, train_perplexity=1123.2645, train_loss=7.0239944

Batch 5940, train_perplexity=1069.4454, train_loss=6.9748955

Batch 5950, train_perplexity=982.9814, train_loss=6.89059

Batch 5960, train_perplexity=1075.9814, train_loss=6.9809885

Batch 5970, train_perplexity=987.15295, train_loss=6.894825

Batch 5980, train_perplexity=1076.317, train_loss=6.9813004

Batch 5990, train_perplexity=1003.01874, train_loss=6.9107695

Batch 6000, train_perplexity=1114.9686, train_loss=7.0165815

Batch 6010, train_perplexity=1082.8765, train_loss=6.987376

Batch 6020, train_perplexity=928.03845, train_loss=6.833073

Batch 6030, train_perplexity=1048.1595, train_loss=6.954791

Batch 6040, train_perplexity=983.73773, train_loss=6.8913593

Batch 6050, train_perplexity=1010.7284, train_loss=6.9184265

Batch 6060, train_perplexity=967.8174, train_loss=6.8750434

Batch 6070, train_perplexity=1085.8258, train_loss=6.990096

Batch 6080, train_perplexity=990.40906, train_loss=6.898118

Batch 6090, train_perplexity=1053.907, train_loss=6.9602594

Batch 6100, train_perplexity=1026.5814, train_loss=6.9339895

Batch 6110, train_perplexity=1011.823, train_loss=6.919509

Batch 6120, train_perplexity=1027.2797, train_loss=6.9346695

Batch 6130, train_perplexity=1051.8666, train_loss=6.9583216

Batch 6140, train_perplexity=981.2655, train_loss=6.888843

Batch 6150, train_perplexity=1005.3617, train_loss=6.9131026

Batch 6160, train_perplexity=965.41504, train_loss=6.872558

Batch 6170, train_perplexity=956.97766, train_loss=6.86378

Batch 6180, train_perplexity=975.8579, train_loss=6.883317

Batch 6190, train_perplexity=1044.6737, train_loss=6.95146

Batch 6200, train_perplexity=996.26013, train_loss=6.9040084

Batch 6210, train_perplexity=1095.59, train_loss=6.999048

Batch 6220, train_perplexity=1011.11786, train_loss=6.918812

Batch 6230, train_perplexity=1052.8964, train_loss=6.9593

Batch 6240, train_perplexity=935.0331, train_loss=6.840582

Batch 6250, train_perplexity=894.5386, train_loss=6.796308

Batch 6260, train_perplexity=1069.2241, train_loss=6.9746885

Batch 6270, train_perplexity=1019.42725, train_loss=6.926996

Batch 6280, train_perplexity=960.7195, train_loss=6.8676825

Batch 6290, train_perplexity=1034.485, train_loss=6.941659

Batch 6300, train_perplexity=1102.4182, train_loss=7.0052614

Batch 6310, train_perplexity=976.9623, train_loss=6.884448

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Batch 6320, train_perplexity=994.854, train_loss=6.902596

Batch 6330, train_perplexity=997.781, train_loss=6.905534

Batch 6340, train_perplexity=1061.8933, train_loss=6.9678087

Batch 6350, train_perplexity=988.27246, train_loss=6.8959584

Batch 6360, train_perplexity=894.18805, train_loss=6.795916

Batch 6370, train_perplexity=1054.5238, train_loss=6.9608445

Batch 6380, train_perplexity=911.384, train_loss=6.8149643

Batch 6390, train_perplexity=985.48193, train_loss=6.893131

Batch 6400, train_perplexity=1016.8745, train_loss=6.924489

Batch 6410, train_perplexity=944.76654, train_loss=6.850938

Batch 6420, train_perplexity=973.4004, train_loss=6.8807955

Batch 6430, train_perplexity=1030.1847, train_loss=6.9374933

Batch 6440, train_perplexity=1036.121, train_loss=6.943239

Batch 6450, train_perplexity=1037.8011, train_loss=6.9448595

Batch 6460, train_perplexity=987.36383, train_loss=6.8950386

Batch 6470, train_perplexity=973.02356, train_loss=6.8804083

Batch 6480, train_perplexity=1035.9047, train_loss=6.9430304

Batch 6490, train_perplexity=1020.9002, train_loss=6.92844

Batch 6500, train_perplexity=971.5418, train_loss=6.8788843

Batch 6510, train_perplexity=1002.19025, train_loss=6.909943

Batch 6520, train_perplexity=982.8703, train_loss=6.890477

Batch 6530, train_perplexity=998.85205, train_loss=6.9066067

Batch 6540, train_perplexity=1042.8925, train_loss=6.9497533

Batch 6550, train_perplexity=1003.6287, train_loss=6.9113774

Batch 6560, train_perplexity=960.42267, train_loss=6.8673735

Batch 6570, train_perplexity=908.07806, train_loss=6.8113303

Batch 6580, train_perplexity=1044.2355, train_loss=6.9510403

Batch 6590, train_perplexity=1031.7661, train_loss=6.9390273

Batch 6600, train_perplexity=995.5601, train_loss=6.9033055

Batch 6610, train_perplexity=912.4989, train_loss=6.816187

Batch 6620, train_perplexity=959.1426, train_loss=6.8660398

Batch 6630, train_perplexity=1037.8486, train_loss=6.9449053

Batch 6640, train_perplexity=936.0073, train_loss=6.8416233

Batch 6650, train_perplexity=906.69476, train_loss=6.809806

Batch 6660, train_perplexity=973.7142, train_loss=6.881118

Batch 6670, train_perplexity=1059.8345, train_loss=6.965868

Batch 6680, train_perplexity=1061.8407, train_loss=6.967759

Batch 6690, train_perplexity=872.2809, train_loss=6.7711115

Batch 6700, train_perplexity=1021.43976, train_loss=6.9289684

Batch 6710, train_perplexity=984.03705, train_loss=6.8916636

Batch 6720, train_perplexity=992.6425, train_loss=6.9003706

Batch 6730, train_perplexity=1013.37726, train_loss=6.921044

Batch 6740, train_perplexity=937.6065, train_loss=6.8433304

Batch 6750, train_perplexity=990.41516, train_loss=6.898124

Batch 6760, train_perplexity=944.2954, train_loss=6.850439

Batch 6770, train_perplexity=1057.1462, train_loss=6.9633284

Batch 6780, train_perplexity=919.4032, train_loss=6.8237247

Batch 6790, train_perplexity=949.59467, train_loss=6.856035

Batch 6800, train_perplexity=983.28, train_loss=6.890894

Batch 6810, train_perplexity=975.0058, train_loss=6.8824434

Batch 6820, train_perplexity=1049.4548, train_loss=6.956026

Batch 6830, train_perplexity=943.3418, train_loss=6.8494287

Batch 6840, train_perplexity=946.6596, train_loss=6.8529396

Batch 6850, train_perplexity=1076.6455, train_loss=6.9816055

Batch 6860, train_perplexity=1034.5314, train_loss=6.941704

Batch 6870, train_perplexity=988.3078, train_loss=6.895994

Batch 6880, train_perplexity=952.93317, train_loss=6.8595448

Batch 6890, train_perplexity=996.8879, train_loss=6.9046383

Batch 6900, train_perplexity=1032.2833, train_loss=6.9395285

Batch 6910, train_perplexity=998.0646, train_loss=6.905818

Batch 6920, train_perplexity=944.426, train_loss=6.8505774

Batch 6930, train_perplexity=965.4054, train_loss=6.872548

Batch 6940, train_perplexity=987.7566, train_loss=6.8954363

Batch 6950, train_perplexity=938.1808, train_loss=6.8439426

Batch 6960, train_perplexity=992.9076, train_loss=6.9006376

Batch 6970, train_perplexity=914.2707, train_loss=6.8181267

Batch 6980, train_perplexity=1018.62354, train_loss=6.9262075

Batch 6990, train_perplexity=844.8524, train_loss=6.739162

Batch 7000, train_perplexity=956.2537, train_loss=6.8630233

Batch 7010, train_perplexity=915.2137, train_loss=6.8191576

Batch 7020, train_perplexity=966.36615, train_loss=6.873543

Batch 7030, train_perplexity=1025.2723, train_loss=6.9327135

Batch 7040, train_perplexity=1073.6567, train_loss=6.9788256

Batch 7050, train_perplexity=967.6309, train_loss=6.8748507

Batch 7060, train_perplexity=914.32385, train_loss=6.818185

Batch 7070, train_perplexity=1121.308, train_loss=7.022251

Batch 7080, train_perplexity=1016.6515, train_loss=6.9242697

Batch 7090, train_perplexity=924.953, train_loss=6.829743

Batch 7100, train_perplexity=1015.91296, train_loss=6.923543

Batch 7110, train_perplexity=1041.9763, train_loss=6.9488745

Batch 7120, train_perplexity=921.50903, train_loss=6.8260126

Batch 7130, train_perplexity=977.66504, train_loss=6.885167

Batch 7140, train_perplexity=1005.8925, train_loss=6.9136305

Batch 7150, train_perplexity=1015.83984, train_loss=6.923471

Batch 7160, train_perplexity=1061.8842, train_loss=6.9678

Batch 7170, train_perplexity=900.1263, train_loss=6.802535

Batch 7180, train_perplexity=988.8065, train_loss=6.8964987
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7190, train_perplexity=906.1476, train_loss=6.809202

Batch 7200, train_perplexity=945.0089, train_loss=6.8511944

Batch 7210, train_perplexity=953.73413, train_loss=6.860385

Batch 7220, train_perplexity=980.04456, train_loss=6.887598

Batch 7230, train_perplexity=962.66705, train_loss=6.8697076

Batch 7240, train_perplexity=1009.1002, train_loss=6.9168143

Batch 7250, train_perplexity=941.4778, train_loss=6.8474507

Batch 7260, train_perplexity=921.93146, train_loss=6.826471

Batch 7270, train_perplexity=1049.2651, train_loss=6.9558454

Batch 7280, train_perplexity=971.29724, train_loss=6.8786325

Batch 7290, train_perplexity=1025.709, train_loss=6.9331393

Batch 7300, train_perplexity=962.617, train_loss=6.8696556

Batch 7310, train_perplexity=912.485, train_loss=6.8161716

Batch 7320, train_perplexity=996.3556, train_loss=6.904104

Batch 7330, train_perplexity=966.20807, train_loss=6.873379

Batch 7340, train_perplexity=924.47546, train_loss=6.8292265

Batch 7350, train_perplexity=988.3644, train_loss=6.8960514

Batch 7360, train_perplexity=1022.02, train_loss=6.9295363

Batch 7370, train_perplexity=851.6082, train_loss=6.7471266

Batch 7380, train_perplexity=978.1808, train_loss=6.8856945

Batch 7390, train_perplexity=994.2484, train_loss=6.901987

Batch 7400, train_perplexity=909.5961, train_loss=6.8130007

Batch 7410, train_perplexity=980.60364, train_loss=6.8881683

Batch 7420, train_perplexity=929.09973, train_loss=6.834216

Batch 7430, train_perplexity=919.1007, train_loss=6.8233957

Batch 7440, train_perplexity=971.9162, train_loss=6.8792696

Batch 7450, train_perplexity=973.9129, train_loss=6.881322

Batch 7460, train_perplexity=966.8658, train_loss=6.8740597

Batch 7470, train_perplexity=1019.54004, train_loss=6.927107

Batch 7480, train_perplexity=972.0775, train_loss=6.8794355

Batch 7490, train_perplexity=926.8333, train_loss=6.8317738

Batch 7500, train_perplexity=1031.7283, train_loss=6.9389906

Batch 7510, train_perplexity=921.771, train_loss=6.826297

Batch 7520, train_perplexity=915.16486, train_loss=6.819104

Batch 7530, train_perplexity=920.0672, train_loss=6.8244467

Batch 7540, train_perplexity=940.9854, train_loss=6.8469276

Batch 7550, train_perplexity=890.761, train_loss=6.792076

Batch 7560, train_perplexity=967.5982, train_loss=6.874817

Batch 7570, train_perplexity=912.08215, train_loss=6.81573

Batch 7580, train_perplexity=1014.82263, train_loss=6.922469

Batch 7590, train_perplexity=949.97736, train_loss=6.856438

Batch 7600, train_perplexity=977.9793, train_loss=6.8854885

Batch 7610, train_perplexity=976.9101, train_loss=6.8843946

Batch 7620, train_perplexity=1051.9514, train_loss=6.958402

Batch 7630, train_perplexity=1010.3217, train_loss=6.918024

Batch 7640, train_perplexity=985.44385, train_loss=6.893092

Batch 7650, train_perplexity=1048.2994, train_loss=6.9549246

Batch 7660, train_perplexity=979.25696, train_loss=6.886794

Batch 7670, train_perplexity=929.8825, train_loss=6.835058

Batch 7680, train_perplexity=966.1164, train_loss=6.8732843

Batch 7690, train_perplexity=975.68994, train_loss=6.883145

Batch 7700, train_perplexity=941.5743, train_loss=6.8475533

Batch 7710, train_perplexity=955.41644, train_loss=6.8621473

Batch 7720, train_perplexity=939.56146, train_loss=6.845413

Batch 7730, train_perplexity=926.8802, train_loss=6.8318243

Batch 7740, train_perplexity=978.3492, train_loss=6.8858666

Batch 7750, train_perplexity=902.5193, train_loss=6.80519

Batch 7760, train_perplexity=950.87427, train_loss=6.857382

Batch 7770, train_perplexity=864.244, train_loss=6.761855

Batch 7780, train_perplexity=901.70886, train_loss=6.8042917

Batch 7790, train_perplexity=920.4573, train_loss=6.8248706

Batch 7800, train_perplexity=884.6453, train_loss=6.785187

Batch 7810, train_perplexity=996.7358, train_loss=6.9044857

Batch 7820, train_perplexity=922.07605, train_loss=6.8266277

Batch 7830, train_perplexity=925.81036, train_loss=6.8306694

Batch 7840, train_perplexity=909.3893, train_loss=6.812773

Batch 7850, train_perplexity=883.09265, train_loss=6.78343

Batch 7860, train_perplexity=921.9662, train_loss=6.8265085

Batch 7870, train_perplexity=984.0788, train_loss=6.891706

Batch 7880, train_perplexity=973.5452, train_loss=6.8809443

Batch 7890, train_perplexity=929.605, train_loss=6.8347597

Batch 7900, train_perplexity=1029.13, train_loss=6.936469

Batch 7910, train_perplexity=970.39496, train_loss=6.877703

Batch 7920, train_perplexity=1008.1119, train_loss=6.9158344

Batch 7930, train_perplexity=956.9471, train_loss=6.863748

Batch 7940, train_perplexity=986.1466, train_loss=6.893805

Batch 7950, train_perplexity=921.8914, train_loss=6.8264275

Batch 7960, train_perplexity=909.3602, train_loss=6.8127413

Batch 7970, train_perplexity=1045.1874, train_loss=6.9519515

Batch 7980, train_perplexity=848.4297, train_loss=6.743387

Batch 7990, train_perplexity=931.3104, train_loss=6.8365927

Batch 8000, train_perplexity=925.5976, train_loss=6.8304396

Batch 8010, train_perplexity=928.39294, train_loss=6.833455

Batch 8020, train_perplexity=929.88916, train_loss=6.8350654

Batch 8030, train_perplexity=952.02026, train_loss=6.8585863

Batch 8040, train_perplexity=1016.8794, train_loss=6.924494

Batch 8050, train_perplexity=947.30853, train_loss=6.853625

Batch 8060, train_perplexity=895.67096, train_loss=6.797573

Batch 8070, train_perplexity=939.4279, train_loss=6.845271

Batch 8080, train_perplexity=928.2854, train_loss=6.833339

Batch 8090, train_perplexity=927.4505, train_loss=6.8324394

Batch 8100, train_perplexity=880.988, train_loss=6.781044

Batch 8110, train_perplexity=904.2747, train_loss=6.807133

Batch 8120, train_perplexity=989.1206, train_loss=6.8968163

Batch 8130, train_perplexity=865.076, train_loss=6.7628174

Batch 8140, train_perplexity=966.8764, train_loss=6.8740706

Batch 8150, train_perplexity=1109.4451, train_loss=7.0116153

Batch 8160, train_perplexity=985.9299, train_loss=6.893585

Batch 8170, train_perplexity=857.8996, train_loss=6.754487

Batch 8180, train_perplexity=903.58636, train_loss=6.8063717

Batch 8190, train_perplexity=1019.194, train_loss=6.9267673

Batch 8200, train_perplexity=834.9427, train_loss=6.727363

Batch 8210, train_perplexity=961.11444, train_loss=6.8680935

Batch 8220, train_perplexity=910.2643, train_loss=6.813735

Batch 8230, train_perplexity=886.87366, train_loss=6.7877026

Batch 8240, train_perplexity=936.2917, train_loss=6.841927

Batch 8250, train_perplexity=938.9774, train_loss=6.8447914

Batch 8260, train_perplexity=861.70215, train_loss=6.7589097

Batch 8270, train_perplexity=845.1349, train_loss=6.739496

Batch 8280, train_perplexity=900.0164, train_loss=6.802413

Batch 8290, train_perplexity=915.57996, train_loss=6.8195577

Batch 8300, train_perplexity=985.53125, train_loss=6.893181

Batch 8310, train_perplexity=949.24786, train_loss=6.85567

Batch 8320, train_perplexity=970.59906, train_loss=6.8779135

Batch 8330, train_perplexity=989.3734, train_loss=6.897072

Batch 8340, train_perplexity=896.6957, train_loss=6.7987165

Batch 8350, train_perplexity=905.2696, train_loss=6.808233

Batch 8360, train_perplexity=844.85767, train_loss=6.739168

Batch 8370, train_perplexity=919.5483, train_loss=6.8238826

Batch 8380, train_perplexity=848.2052, train_loss=6.7431226

Batch 8390, train_perplexity=1065.5544, train_loss=6.9712505

Batch 8400, train_perplexity=942.47534, train_loss=6.84851

Batch 8410, train_perplexity=869.2175, train_loss=6.7675934

Batch 8420, train_perplexity=824.80426, train_loss=6.715146

Batch 8430, train_perplexity=923.52814, train_loss=6.8282013

Batch 8440, train_perplexity=975.09045, train_loss=6.88253

Batch 8450, train_perplexity=857.1415, train_loss=6.753603

Batch 8460, train_perplexity=921.6831, train_loss=6.8262014

Batch 8470, train_perplexity=866.0381, train_loss=6.763929

Batch 8480, train_perplexity=910.7349, train_loss=6.814252

Batch 8490, train_perplexity=878.93616, train_loss=6.7787123

Batch 8500, train_perplexity=986.58545, train_loss=6.89425

Batch 8510, train_perplexity=933.9235, train_loss=6.8393946

Batch 8520, train_perplexity=934.9631, train_loss=6.840507

Batch 8530, train_perplexity=830.8294, train_loss=6.7224245
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 8540, train_perplexity=879.8885, train_loss=6.779795

Batch 8550, train_perplexity=886.4213, train_loss=6.7871923

Batch 8560, train_perplexity=833.7034, train_loss=6.725878

Batch 8570, train_perplexity=916.1848, train_loss=6.820218

Batch 8580, train_perplexity=966.97363, train_loss=6.8741713

Batch 8590, train_perplexity=873.52625, train_loss=6.772538

Batch 8600, train_perplexity=964.2161, train_loss=6.8713155

Batch 8610, train_perplexity=866.50073, train_loss=6.764463

Batch 8620, train_perplexity=876.99615, train_loss=6.7765026

Batch 8630, train_perplexity=899.4535, train_loss=6.8017874

Batch 8640, train_perplexity=910.8939, train_loss=6.8144264

Batch 8650, train_perplexity=884.9254, train_loss=6.7855034

Batch 8660, train_perplexity=832.2739, train_loss=6.7241616

Batch 8670, train_perplexity=964.33704, train_loss=6.871441

Batch 8680, train_perplexity=871.54083, train_loss=6.7702627

Batch 8690, train_perplexity=880.76874, train_loss=6.780795

Batch 8700, train_perplexity=906.1355, train_loss=6.809189

Batch 8710, train_perplexity=887.4473, train_loss=6.788349

Batch 8720, train_perplexity=860.9436, train_loss=6.758029

Batch 8730, train_perplexity=931.03424, train_loss=6.836296

Batch 8740, train_perplexity=903.62384, train_loss=6.806413

Batch 8750, train_perplexity=870.01117, train_loss=6.768506

Batch 8760, train_perplexity=993.2751, train_loss=6.9010077

Batch 8770, train_perplexity=909.01337, train_loss=6.81236

Batch 8780, train_perplexity=850.35114, train_loss=6.7456493

Batch 8790, train_perplexity=918.9951, train_loss=6.823281

Batch 8800, train_perplexity=816.5963, train_loss=6.705145

Batch 8810, train_perplexity=953.997, train_loss=6.8606606

Batch 8820, train_perplexity=972.62646, train_loss=6.88

Batch 8830, train_perplexity=957.3756, train_loss=6.864196

Batch 8840, train_perplexity=860.4659, train_loss=6.757474

Batch 8850, train_perplexity=866.58624, train_loss=6.7645617

Batch 8860, train_perplexity=924.8591, train_loss=6.8296413

Batch 8870, train_perplexity=867.44037, train_loss=6.765547

Batch 8880, train_perplexity=847.29486, train_loss=6.7420487

Batch 8890, train_perplexity=970.8805, train_loss=6.8782034

Batch 8900, train_perplexity=875.1105, train_loss=6.77435

Batch 8910, train_perplexity=894.73737, train_loss=6.7965302

Batch 8920, train_perplexity=860.37726, train_loss=6.757371

Batch 8930, train_perplexity=891.5573, train_loss=6.7929697

Batch 8940, train_perplexity=962.82404, train_loss=6.8698707

Batch 8950, train_perplexity=809.19055, train_loss=6.6960344

Batch 8960, train_perplexity=899.83405, train_loss=6.8022103

Batch 8970, train_perplexity=965.37866, train_loss=6.8725204

Batch 8980, train_perplexity=865.54803, train_loss=6.763363

Batch 8990, train_perplexity=875.72076, train_loss=6.7750473

Batch 9000, train_perplexity=871.604, train_loss=6.770335

Batch 9010, train_perplexity=892.9477, train_loss=6.794528

Batch 9020, train_perplexity=896.87787, train_loss=6.7989197

Batch 9030, train_perplexity=959.8898, train_loss=6.8668184

Batch 9040, train_perplexity=869.23535, train_loss=6.767614

Batch 9050, train_perplexity=900.50964, train_loss=6.802961

Batch 9060, train_perplexity=863.74634, train_loss=6.761279

Batch 9070, train_perplexity=989.0352, train_loss=6.89673

Batch 9080, train_perplexity=877.71655, train_loss=6.7773237

Batch 9090, train_perplexity=855.824, train_loss=6.7520647

Batch 9100, train_perplexity=973.9761, train_loss=6.8813868

Batch 9110, train_perplexity=889.4049, train_loss=6.7905526

Batch 9120, train_perplexity=862.9357, train_loss=6.76034

Batch 9130, train_perplexity=924.8736, train_loss=6.829657

Batch 9140, train_perplexity=873.3509, train_loss=6.7723374

Batch 9150, train_perplexity=911.62305, train_loss=6.8152266

Batch 9160, train_perplexity=783.30023, train_loss=6.663516

Batch 9170, train_perplexity=813.48254, train_loss=6.7013245

Batch 9180, train_perplexity=938.88965, train_loss=6.844698

Batch 9190, train_perplexity=938.8655, train_loss=6.844672

Batch 9200, train_perplexity=857.24036, train_loss=6.7537184

Batch 9210, train_perplexity=886.0114, train_loss=6.78673

Batch 9220, train_perplexity=948.18567, train_loss=6.8545504

Batch 9230, train_perplexity=856.2146, train_loss=6.752521

Batch 9240, train_perplexity=877.0158, train_loss=6.776525

Batch 9250, train_perplexity=806.07654, train_loss=6.6921787

Batch 9260, train_perplexity=851.6407, train_loss=6.7471647

Batch 9270, train_perplexity=929.6701, train_loss=6.83483

Batch 9280, train_perplexity=900.5668, train_loss=6.8030243

Batch 9290, train_perplexity=968.538, train_loss=6.8757877

Batch 9300, train_perplexity=839.60626, train_loss=6.732933

Batch 9310, train_perplexity=808.4589, train_loss=6.69513

Batch 9320, train_perplexity=918.5951, train_loss=6.8228455

Batch 9330, train_perplexity=912.7191, train_loss=6.816428

Batch 9340, train_perplexity=850.1731, train_loss=6.74544

Batch 9350, train_perplexity=846.6402, train_loss=6.741276

Batch 9360, train_perplexity=957.335, train_loss=6.8641534

Batch 9370, train_perplexity=843.5144, train_loss=6.737577

Batch 9380, train_perplexity=854.8688, train_loss=6.750948

Batch 9390, train_perplexity=917.2133, train_loss=6.82134

Batch 9400, train_perplexity=871.1669, train_loss=6.7698336

Batch 9410, train_perplexity=793.0858, train_loss=6.6759315

Batch 9420, train_perplexity=917.27325, train_loss=6.8214054

Batch 9430, train_perplexity=812.5327, train_loss=6.700156

Batch 9440, train_perplexity=840.7168, train_loss=6.734255

Batch 9450, train_perplexity=852.3484, train_loss=6.7479954

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00053-of-00100
Loaded 306875 sentences.
Finished loading
Batch 9460, train_perplexity=885.65405, train_loss=6.7863264

Batch 9470, train_perplexity=983.51636, train_loss=6.8911343

Batch 9480, train_perplexity=862.80615, train_loss=6.76019

Batch 9490, train_perplexity=914.1443, train_loss=6.8179884

Batch 9500, train_perplexity=930.6592, train_loss=6.835893

Batch 9510, train_perplexity=826.1417, train_loss=6.7167664

Batch 9520, train_perplexity=877.3475, train_loss=6.776903

Batch 9530, train_perplexity=937.31683, train_loss=6.8430214

Batch 9540, train_perplexity=860.0327, train_loss=6.7569704

Batch 9550, train_perplexity=809.2608, train_loss=6.696121

Batch 9560, train_perplexity=874.5365, train_loss=6.773694

Batch 9570, train_perplexity=775.295, train_loss=6.6532435

Batch 9580, train_perplexity=864.59186, train_loss=6.7622576

Batch 9590, train_perplexity=926.79443, train_loss=6.831732

Batch 9600, train_perplexity=874.6537, train_loss=6.773828

Batch 9610, train_perplexity=912.90063, train_loss=6.816627

Batch 9620, train_perplexity=825.8818, train_loss=6.7164516

Batch 9630, train_perplexity=841.98773, train_loss=6.7357655

Batch 9640, train_perplexity=957.9076, train_loss=6.8647513

Batch 9650, train_perplexity=889.2281, train_loss=6.790354

Batch 9660, train_perplexity=781.4424, train_loss=6.6611414

Batch 9670, train_perplexity=830.65515, train_loss=6.7222147

Batch 9680, train_perplexity=831.9855, train_loss=6.723815

Batch 9690, train_perplexity=868.77826, train_loss=6.767088

Batch 9700, train_perplexity=862.8617, train_loss=6.7602544

Batch 9710, train_perplexity=862.56055, train_loss=6.7599053

Batch 9720, train_perplexity=920.1716, train_loss=6.82456

Batch 9730, train_perplexity=749.053, train_loss=6.6188097

Batch 9740, train_perplexity=896.889, train_loss=6.798932

Batch 9750, train_perplexity=857.11816, train_loss=6.753576

Batch 9760, train_perplexity=890.3117, train_loss=6.7915716

Batch 9770, train_perplexity=830.09644, train_loss=6.721542

Batch 9780, train_perplexity=856.29584, train_loss=6.752616

Batch 9790, train_perplexity=814.32086, train_loss=6.7023544

Batch 9800, train_perplexity=827.4601, train_loss=6.718361

Batch 9810, train_perplexity=849.9109, train_loss=6.7451315

Batch 9820, train_perplexity=835.9247, train_loss=6.7285385
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9830, train_perplexity=891.69507, train_loss=6.793124

Batch 9840, train_perplexity=903.52, train_loss=6.8062983

Batch 9850, train_perplexity=752.81525, train_loss=6.62382

Batch 9860, train_perplexity=889.0034, train_loss=6.790101

Batch 9870, train_perplexity=851.9843, train_loss=6.747568

Batch 9880, train_perplexity=954.4566, train_loss=6.861142

Batch 9890, train_perplexity=853.55676, train_loss=6.749412

Batch 9900, train_perplexity=890.68835, train_loss=6.7919946

Batch 9910, train_perplexity=831.59674, train_loss=6.7233477

Batch 9920, train_perplexity=828.22906, train_loss=6.71929

Batch 9930, train_perplexity=818.8415, train_loss=6.7078905

Batch 9940, train_perplexity=916.93915, train_loss=6.821041

Batch 9950, train_perplexity=812.7753, train_loss=6.7004547

Batch 9960, train_perplexity=857.2747, train_loss=6.7537584

Batch 9970, train_perplexity=833.81793, train_loss=6.726015

Batch 9980, train_perplexity=802.4701, train_loss=6.6876945

Batch 9990, train_perplexity=847.78265, train_loss=6.7426243

Batch 10000, train_perplexity=938.3982, train_loss=6.8441744

Batch 10010, train_perplexity=763.25073, train_loss=6.6375866

Batch 10020, train_perplexity=826.5066, train_loss=6.717208

Batch 10030, train_perplexity=772.3427, train_loss=6.6494284

Batch 10040, train_perplexity=801.77356, train_loss=6.686826

Batch 10050, train_perplexity=815.951, train_loss=6.7043543

Batch 10060, train_perplexity=940.3736, train_loss=6.846277

Batch 10070, train_perplexity=788.9925, train_loss=6.670757

Batch 10080, train_perplexity=830.5862, train_loss=6.7221317

Batch 10090, train_perplexity=847.22174, train_loss=6.7419624

Batch 10100, train_perplexity=746.658, train_loss=6.6156073

Batch 10110, train_perplexity=811.4924, train_loss=6.698875

Batch 10120, train_perplexity=867.6158, train_loss=6.765749

Batch 10130, train_perplexity=885.9945, train_loss=6.7867107

Batch 10140, train_perplexity=827.3891, train_loss=6.718275

Batch 10150, train_perplexity=892.2352, train_loss=6.79373

Batch 10160, train_perplexity=869.9141, train_loss=6.7683945

Batch 10170, train_perplexity=939.1888, train_loss=6.8450165

Batch 10180, train_perplexity=892.64545, train_loss=6.7941895

Batch 10190, train_perplexity=760.95844, train_loss=6.6345787

Batch 10200, train_perplexity=739.37726, train_loss=6.6058083

Batch 10210, train_perplexity=798.79285, train_loss=6.6831017

Batch 10220, train_perplexity=770.0811, train_loss=6.646496

Batch 10230, train_perplexity=818.9985, train_loss=6.708082

Batch 10240, train_perplexity=805.81866, train_loss=6.691859

Batch 10250, train_perplexity=772.2823, train_loss=6.64935

Batch 10260, train_perplexity=832.1021, train_loss=6.723955

Batch 10270, train_perplexity=847.20154, train_loss=6.7419386

Batch 10280, train_perplexity=819.67944, train_loss=6.7089133

Batch 10290, train_perplexity=845.5525, train_loss=6.73999

Batch 10300, train_perplexity=779.8313, train_loss=6.6590776

Batch 10310, train_perplexity=804.49567, train_loss=6.6902156

Batch 10320, train_perplexity=868.2871, train_loss=6.7665224

Batch 10330, train_perplexity=912.9725, train_loss=6.8167057

Batch 10340, train_perplexity=870.1506, train_loss=6.7686663

Batch 10350, train_perplexity=813.3243, train_loss=6.70113

Batch 10360, train_perplexity=809.5811, train_loss=6.696517

Batch 10370, train_perplexity=878.57495, train_loss=6.7783012

Batch 10380, train_perplexity=819.11487, train_loss=6.7082243

Batch 10390, train_perplexity=935.4179, train_loss=6.8409934

Batch 10400, train_perplexity=789.09937, train_loss=6.6708922

Batch 10410, train_perplexity=835.7868, train_loss=6.7283735

Batch 10420, train_perplexity=865.3467, train_loss=6.76313

Batch 10430, train_perplexity=809.6093, train_loss=6.696552

Batch 10440, train_perplexity=817.28076, train_loss=6.7059827

Batch 10450, train_perplexity=754.65796, train_loss=6.6262646

Batch 10460, train_perplexity=812.1493, train_loss=6.699684

Batch 10470, train_perplexity=781.638, train_loss=6.6613917

Batch 10480, train_perplexity=844.73035, train_loss=6.7390175

Batch 10490, train_perplexity=923.7312, train_loss=6.828421

Batch 10500, train_perplexity=809.29626, train_loss=6.696165

Batch 10510, train_perplexity=778.0359, train_loss=6.6567726

Batch 10520, train_perplexity=709.94226, train_loss=6.5651836

Batch 10530, train_perplexity=878.7518, train_loss=6.7785025

Batch 10540, train_perplexity=934.3752, train_loss=6.839878

Batch 10550, train_perplexity=874.0687, train_loss=6.773159

Batch 10560, train_perplexity=820.20764, train_loss=6.7095575

Batch 10570, train_perplexity=793.8043, train_loss=6.676837

Batch 10580, train_perplexity=825.4936, train_loss=6.7159815

Batch 10590, train_perplexity=827.07117, train_loss=6.7178907

Batch 10600, train_perplexity=843.88007, train_loss=6.7380104

Batch 10610, train_perplexity=835.2943, train_loss=6.727784

Batch 10620, train_perplexity=844.53143, train_loss=6.738782

Batch 10630, train_perplexity=827.74585, train_loss=6.718706

Batch 10640, train_perplexity=762.8207, train_loss=6.637023

Batch 10650, train_perplexity=808.8256, train_loss=6.6955833

Batch 10660, train_perplexity=790.9958, train_loss=6.6732926

Batch 10670, train_perplexity=762.2309, train_loss=6.6362495

Batch 10680, train_perplexity=740.90326, train_loss=6.60787

Batch 10690, train_perplexity=831.50714, train_loss=6.72324

Batch 10700, train_perplexity=846.1579, train_loss=6.740706

Batch 10710, train_perplexity=833.8013, train_loss=6.725995

Batch 10720, train_perplexity=870.2751, train_loss=6.7688093

Batch 10730, train_perplexity=769.7081, train_loss=6.6460114

Batch 10740, train_perplexity=792.34985, train_loss=6.675003

Batch 10750, train_perplexity=810.587, train_loss=6.6977587

Batch 10760, train_perplexity=864.0528, train_loss=6.761634

Batch 10770, train_perplexity=801.47424, train_loss=6.686453

Batch 10780, train_perplexity=848.7016, train_loss=6.7437077

Batch 10790, train_perplexity=921.8145, train_loss=6.826344

Batch 10800, train_perplexity=897.8346, train_loss=6.799986

Batch 10810, train_perplexity=814.81104, train_loss=6.702956

Batch 10820, train_perplexity=798.19965, train_loss=6.6823587

Batch 10830, train_perplexity=763.2307, train_loss=6.6375604

Batch 10840, train_perplexity=850.24365, train_loss=6.745523

Batch 10850, train_perplexity=772.63074, train_loss=6.6498013

Batch 10860, train_perplexity=863.6005, train_loss=6.7611103

Batch 10870, train_perplexity=796.90967, train_loss=6.6807413

Batch 10880, train_perplexity=743.18585, train_loss=6.610946

Batch 10890, train_perplexity=774.7015, train_loss=6.6524777

Batch 10900, train_perplexity=818.40234, train_loss=6.707354

Batch 10910, train_perplexity=746.5786, train_loss=6.615501

Batch 10920, train_perplexity=778.0997, train_loss=6.6568546

Batch 10930, train_perplexity=877.5475, train_loss=6.777131

Batch 10940, train_perplexity=896.4208, train_loss=6.79841

Batch 10950, train_perplexity=777.12744, train_loss=6.6556044

Batch 10960, train_perplexity=806.67065, train_loss=6.6929154

Batch 10970, train_perplexity=743.76764, train_loss=6.6117287

Batch 10980, train_perplexity=792.7214, train_loss=6.675472

Batch 10990, train_perplexity=816.7334, train_loss=6.7053127

Batch 11000, train_perplexity=801.0028, train_loss=6.6858644

Batch 11010, train_perplexity=848.7591, train_loss=6.7437754

Batch 11020, train_perplexity=792.538, train_loss=6.6752405

Batch 11030, train_perplexity=820.1365, train_loss=6.7094707

Batch 11040, train_perplexity=709.88947, train_loss=6.5651093

Batch 11050, train_perplexity=826.39746, train_loss=6.717076

Batch 11060, train_perplexity=775.5771, train_loss=6.6536074

Batch 11070, train_perplexity=736.45605, train_loss=6.6018496

Batch 11080, train_perplexity=889.41724, train_loss=6.7905664

Batch 11090, train_perplexity=759.696, train_loss=6.6329184

Batch 11100, train_perplexity=765.0051, train_loss=6.6398826

Batch 11110, train_perplexity=863.8114, train_loss=6.7613544

Batch 11120, train_perplexity=759.05725, train_loss=6.632077

Batch 11130, train_perplexity=759.28784, train_loss=6.632381

Batch 11140, train_perplexity=812.80475, train_loss=6.700491

Batch 11150, train_perplexity=869.7283, train_loss=6.768181
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11160, train_perplexity=737.7667, train_loss=6.6036277

Batch 11170, train_perplexity=800.2694, train_loss=6.6849484

Batch 11180, train_perplexity=729.2289, train_loss=6.5919876

Batch 11190, train_perplexity=801.67834, train_loss=6.6867075

Batch 11200, train_perplexity=957.5272, train_loss=6.864354

Batch 11210, train_perplexity=828.4092, train_loss=6.719507

Batch 11220, train_perplexity=698.85895, train_loss=6.549449

Batch 11230, train_perplexity=819.8143, train_loss=6.709078

Batch 11240, train_perplexity=818.8747, train_loss=6.707931

Batch 11250, train_perplexity=791.0475, train_loss=6.673358

Batch 11260, train_perplexity=755.58295, train_loss=6.6274896

Batch 11270, train_perplexity=828.51385, train_loss=6.7196336

Batch 11280, train_perplexity=743.5329, train_loss=6.611413

Batch 11290, train_perplexity=720.7424, train_loss=6.5802817

Batch 11300, train_perplexity=732.54846, train_loss=6.5965295

Batch 11310, train_perplexity=823.7438, train_loss=6.7138596

Batch 11320, train_perplexity=738.30835, train_loss=6.6043615

Batch 11330, train_perplexity=759.4403, train_loss=6.6325817

Batch 11340, train_perplexity=736.39355, train_loss=6.6017647

Batch 11350, train_perplexity=860.47736, train_loss=6.7574873

Batch 11360, train_perplexity=795.36383, train_loss=6.6787996

Batch 11370, train_perplexity=765.20105, train_loss=6.6401386

Batch 11380, train_perplexity=798.2004, train_loss=6.6823597

Batch 11390, train_perplexity=736.71844, train_loss=6.6022058

Batch 11400, train_perplexity=878.8205, train_loss=6.7785807

Batch 11410, train_perplexity=758.79346, train_loss=6.6317296

Batch 11420, train_perplexity=785.25085, train_loss=6.666003

Batch 11430, train_perplexity=735.34296, train_loss=6.600337

Batch 11440, train_perplexity=730.7639, train_loss=6.5940905

Batch 11450, train_perplexity=786.5936, train_loss=6.6677117

Batch 11460, train_perplexity=773.16296, train_loss=6.65049

Batch 11470, train_perplexity=746.23834, train_loss=6.615045

Batch 11480, train_perplexity=725.1559, train_loss=6.5863867

Batch 11490, train_perplexity=869.8606, train_loss=6.768333

Batch 11500, train_perplexity=752.8691, train_loss=6.6238914

Batch 11510, train_perplexity=790.09973, train_loss=6.672159

Batch 11520, train_perplexity=758.3663, train_loss=6.6311665

Batch 11530, train_perplexity=887.92773, train_loss=6.7888904

Batch 11540, train_perplexity=731.2666, train_loss=6.594778

Batch 11550, train_perplexity=745.355, train_loss=6.6138606

Batch 11560, train_perplexity=796.2316, train_loss=6.67989

Batch 11570, train_perplexity=751.7187, train_loss=6.622362

Batch 11580, train_perplexity=841.6425, train_loss=6.7353554

Batch 11590, train_perplexity=704.72217, train_loss=6.5578036

Batch 11600, train_perplexity=872.99536, train_loss=6.77193

Batch 11610, train_perplexity=798.2103, train_loss=6.682372

Batch 11620, train_perplexity=761.70624, train_loss=6.635561

Batch 11630, train_perplexity=740.1215, train_loss=6.6068144

Batch 11640, train_perplexity=763.8544, train_loss=6.638377

Batch 11650, train_perplexity=743.6513, train_loss=6.6115723

Batch 11660, train_perplexity=791.27795, train_loss=6.6736493

Batch 11670, train_perplexity=751.7735, train_loss=6.622435

Batch 11680, train_perplexity=725.87274, train_loss=6.5873747

Batch 11690, train_perplexity=758.0774, train_loss=6.6307855

Batch 11700, train_perplexity=808.6328, train_loss=6.695345

Batch 11710, train_perplexity=706.9533, train_loss=6.5609646

Batch 11720, train_perplexity=831.7617, train_loss=6.723546

Batch 11730, train_perplexity=757.8999, train_loss=6.6305513

Batch 11740, train_perplexity=711.412, train_loss=6.5672517

Batch 11750, train_perplexity=739.01276, train_loss=6.605315

Batch 11760, train_perplexity=758.9168, train_loss=6.631892

Batch 11770, train_perplexity=842.79755, train_loss=6.7367268

Batch 11780, train_perplexity=786.9504, train_loss=6.668165

Batch 11790, train_perplexity=818.28485, train_loss=6.7072105

Batch 11800, train_perplexity=758.67804, train_loss=6.6315775

Batch 11810, train_perplexity=826.02594, train_loss=6.716626

Batch 11820, train_perplexity=805.0429, train_loss=6.6908956

Batch 11830, train_perplexity=693.85504, train_loss=6.542263

Batch 11840, train_perplexity=806.48566, train_loss=6.692686

Batch 11850, train_perplexity=883.04297, train_loss=6.783374

Batch 11860, train_perplexity=757.01935, train_loss=6.629389

Batch 11870, train_perplexity=762.0107, train_loss=6.6359606

Batch 11880, train_perplexity=779.95404, train_loss=6.659235

Batch 11890, train_perplexity=680.39636, train_loss=6.5226755

Batch 11900, train_perplexity=691.35333, train_loss=6.538651

Batch 11910, train_perplexity=745.7869, train_loss=6.61444

Batch 11920, train_perplexity=826.28436, train_loss=6.716939

Batch 11930, train_perplexity=711.0732, train_loss=6.5667753

Batch 11940, train_perplexity=820.782, train_loss=6.7102575

Batch 11950, train_perplexity=713.2955, train_loss=6.5698957

Batch 11960, train_perplexity=786.9643, train_loss=6.668183

Batch 11970, train_perplexity=759.1807, train_loss=6.63224

Batch 11980, train_perplexity=768.09564, train_loss=6.643914

Batch 11990, train_perplexity=721.2766, train_loss=6.5810227

Batch 12000, train_perplexity=830.3391, train_loss=6.721834

Batch 12010, train_perplexity=746.1348, train_loss=6.6149063

Batch 12020, train_perplexity=727.37854, train_loss=6.589447

Batch 12030, train_perplexity=818.7708, train_loss=6.707804

Batch 12040, train_perplexity=831.9593, train_loss=6.7237835

Batch 12050, train_perplexity=743.9819, train_loss=6.6120167

Batch 12060, train_perplexity=849.9117, train_loss=6.7451324

Batch 12070, train_perplexity=781.9116, train_loss=6.6617417

Batch 12080, train_perplexity=793.3351, train_loss=6.6762457

Batch 12090, train_perplexity=712.4589, train_loss=6.5687222

Batch 12100, train_perplexity=726.2459, train_loss=6.5878887

Batch 12110, train_perplexity=778.6237, train_loss=6.657528

Batch 12120, train_perplexity=738.2689, train_loss=6.604308

Batch 12130, train_perplexity=823.2506, train_loss=6.7132607

Batch 12140, train_perplexity=775.89813, train_loss=6.6540213

Batch 12150, train_perplexity=760.23596, train_loss=6.633629

Batch 12160, train_perplexity=718.3852, train_loss=6.577006

Batch 12170, train_perplexity=854.3309, train_loss=6.7503185

Batch 12180, train_perplexity=806.11426, train_loss=6.6922255

Batch 12190, train_perplexity=750.22687, train_loss=6.6203756

Batch 12200, train_perplexity=700.70624, train_loss=6.5520887

Batch 12210, train_perplexity=713.4846, train_loss=6.570161

Batch 12220, train_perplexity=787.3028, train_loss=6.668613

Batch 12230, train_perplexity=724.5144, train_loss=6.5855017

Batch 12240, train_perplexity=797.5, train_loss=6.681482

Batch 12250, train_perplexity=755.12445, train_loss=6.6268826

Batch 12260, train_perplexity=745.894, train_loss=6.6145835

Batch 12270, train_perplexity=720.2493, train_loss=6.5795975

Batch 12280, train_perplexity=685.6393, train_loss=6.5303516

Batch 12290, train_perplexity=778.45184, train_loss=6.657307

Batch 12300, train_perplexity=805.29974, train_loss=6.6912146

Batch 12310, train_perplexity=768.3029, train_loss=6.644184

Batch 12320, train_perplexity=733.3897, train_loss=6.597677

Batch 12330, train_perplexity=700.1668, train_loss=6.5513186

Batch 12340, train_perplexity=768.5521, train_loss=6.6445084

Batch 12350, train_perplexity=817.1284, train_loss=6.7057962

Batch 12360, train_perplexity=729.5697, train_loss=6.592455

Batch 12370, train_perplexity=718.2574, train_loss=6.576828

Batch 12380, train_perplexity=810.68134, train_loss=6.697875

Batch 12390, train_perplexity=788.76715, train_loss=6.670471

Batch 12400, train_perplexity=712.56525, train_loss=6.5688715

Batch 12410, train_perplexity=785.38904, train_loss=6.666179

Batch 12420, train_perplexity=771.888, train_loss=6.6488395

Batch 12430, train_perplexity=739.28455, train_loss=6.605683

Batch 12440, train_perplexity=782.0261, train_loss=6.661888

Batch 12450, train_perplexity=806.27954, train_loss=6.6924305

Batch 12460, train_perplexity=796.40704, train_loss=6.6801105

Batch 12470, train_perplexity=697.38025, train_loss=6.547331

Batch 12480, train_perplexity=726.6651, train_loss=6.5884657
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 12490, train_perplexity=785.4295, train_loss=6.6662307

Batch 12500, train_perplexity=768.5213, train_loss=6.6444683

Batch 12510, train_perplexity=756.7768, train_loss=6.6290684

Batch 12520, train_perplexity=768.77496, train_loss=6.6447983

Batch 12530, train_perplexity=727.0678, train_loss=6.58902

Batch 12540, train_perplexity=712.0653, train_loss=6.5681696

Batch 12550, train_perplexity=768.76324, train_loss=6.644783

Batch 12560, train_perplexity=694.6827, train_loss=6.543455

Batch 12570, train_perplexity=776.07056, train_loss=6.6542435

Batch 12580, train_perplexity=734.18536, train_loss=6.5987616

Batch 12590, train_perplexity=715.0977, train_loss=6.572419

Batch 12600, train_perplexity=739.71014, train_loss=6.6062584

Batch 12610, train_perplexity=739.1238, train_loss=6.6054654

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100
Loaded 305432 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00083-of-00100
Loaded 305432 sentences.
Finished loading
Batch 12620, train_perplexity=693.7647, train_loss=6.542133

Batch 12630, train_perplexity=778.73065, train_loss=6.6576653

Batch 12640, train_perplexity=718.0341, train_loss=6.576517

Batch 12650, train_perplexity=722.0299, train_loss=6.5820665

Batch 12660, train_perplexity=787.2086, train_loss=6.6684933

Batch 12670, train_perplexity=735.47974, train_loss=6.600523

Batch 12680, train_perplexity=712.159, train_loss=6.568301

Batch 12690, train_perplexity=684.5704, train_loss=6.5287914

Batch 12700, train_perplexity=745.2342, train_loss=6.6136985

Batch 12710, train_perplexity=813.3914, train_loss=6.7012124

Batch 12720, train_perplexity=728.4483, train_loss=6.5909166

Batch 12730, train_perplexity=774.33325, train_loss=6.6520023

Batch 12740, train_perplexity=692.37665, train_loss=6.54013

Batch 12750, train_perplexity=789.73737, train_loss=6.6717005

Batch 12760, train_perplexity=821.60236, train_loss=6.7112565

Batch 12770, train_perplexity=671.8343, train_loss=6.5100117

Batch 12780, train_perplexity=760.03046, train_loss=6.6333585

Batch 12790, train_perplexity=735.09265, train_loss=6.5999966

Batch 12800, train_perplexity=761.4056, train_loss=6.635166

Batch 12810, train_perplexity=711.35974, train_loss=6.5671782

Batch 12820, train_perplexity=732.3532, train_loss=6.596263

Batch 12830, train_perplexity=697.68994, train_loss=6.547775

Batch 12840, train_perplexity=664.8851, train_loss=6.4996142

Batch 12850, train_perplexity=754.1155, train_loss=6.6255455

Batch 12860, train_perplexity=682.0335, train_loss=6.525079

Batch 12870, train_perplexity=671.1503, train_loss=6.508993

Batch 12880, train_perplexity=766.097, train_loss=6.641309

Batch 12890, train_perplexity=703.7265, train_loss=6.55639

Batch 12900, train_perplexity=725.0024, train_loss=6.586175

Batch 12910, train_perplexity=706.0275, train_loss=6.559654

Batch 12920, train_perplexity=717.51904, train_loss=6.5757995

Batch 12930, train_perplexity=698.84564, train_loss=6.54943

Batch 12940, train_perplexity=686.9843, train_loss=6.5323114

Batch 12950, train_perplexity=700.7844, train_loss=6.5522003

Batch 12960, train_perplexity=677.7069, train_loss=6.518715

Batch 12970, train_perplexity=757.427, train_loss=6.629927

Batch 12980, train_perplexity=745.93384, train_loss=6.614637

Batch 12990, train_perplexity=771.2081, train_loss=6.6479583

Batch 13000, train_perplexity=757.2475, train_loss=6.62969

Batch 13010, train_perplexity=636.2261, train_loss=6.455554

Batch 13020, train_perplexity=667.9042, train_loss=6.5041447

Batch 13030, train_perplexity=796.40515, train_loss=6.680108

Batch 13040, train_perplexity=776.6162, train_loss=6.6549463

Batch 13050, train_perplexity=720.6403, train_loss=6.58014

Batch 13060, train_perplexity=701.4959, train_loss=6.553215

Batch 13070, train_perplexity=672.53235, train_loss=6.51105

Batch 13080, train_perplexity=716.696, train_loss=6.5746517

Batch 13090, train_perplexity=742.3518, train_loss=6.609823

Batch 13100, train_perplexity=736.9422, train_loss=6.6025095

Batch 13110, train_perplexity=763.46625, train_loss=6.637869

Batch 13120, train_perplexity=758.8597, train_loss=6.631817

Batch 13130, train_perplexity=643.7871, train_loss=6.467368

Batch 13140, train_perplexity=850.0621, train_loss=6.7453094

Batch 13150, train_perplexity=689.4544, train_loss=6.5359006

Batch 13160, train_perplexity=707.82794, train_loss=6.562201

Batch 13170, train_perplexity=709.73175, train_loss=6.564887

Batch 13180, train_perplexity=713.09143, train_loss=6.5696096

Batch 13190, train_perplexity=633.42456, train_loss=6.451141

Batch 13200, train_perplexity=743.80804, train_loss=6.611783

Batch 13210, train_perplexity=721.7325, train_loss=6.5816545

Batch 13220, train_perplexity=649.83777, train_loss=6.4767227

Batch 13230, train_perplexity=707.4473, train_loss=6.561663

Batch 13240, train_perplexity=770.3242, train_loss=6.6468115

Batch 13250, train_perplexity=739.9832, train_loss=6.6066275

Batch 13260, train_perplexity=746.5017, train_loss=6.615398

Batch 13270, train_perplexity=637.55994, train_loss=6.4576483

Batch 13280, train_perplexity=648.0742, train_loss=6.474005

Batch 13290, train_perplexity=817.6218, train_loss=6.7064

Batch 13300, train_perplexity=699.20496, train_loss=6.549944

Batch 13310, train_perplexity=689.1908, train_loss=6.535518

Batch 13320, train_perplexity=689.45575, train_loss=6.5359025

Batch 13330, train_perplexity=694.85095, train_loss=6.5436974

Batch 13340, train_perplexity=657.7779, train_loss=6.4888673

Batch 13350, train_perplexity=620.4587, train_loss=6.430459

Batch 13360, train_perplexity=701.69257, train_loss=6.5534954

Batch 13370, train_perplexity=622.1432, train_loss=6.4331703

Batch 13380, train_perplexity=676.56036, train_loss=6.5170217

Batch 13390, train_perplexity=707.39435, train_loss=6.5615883

Batch 13400, train_perplexity=703.16833, train_loss=6.5555964

Batch 13410, train_perplexity=765.631, train_loss=6.6407003

Batch 13420, train_perplexity=687.2346, train_loss=6.5326757

Batch 13430, train_perplexity=733.49884, train_loss=6.597826

Batch 13440, train_perplexity=769.9963, train_loss=6.6463857

Batch 13450, train_perplexity=685.75433, train_loss=6.5305195

Batch 13460, train_perplexity=739.3173, train_loss=6.605727

Batch 13470, train_perplexity=787.7268, train_loss=6.6691513

Batch 13480, train_perplexity=630.58026, train_loss=6.4466405

Batch 13490, train_perplexity=658.6771, train_loss=6.4902334

Batch 13500, train_perplexity=618.4218, train_loss=6.4271708

Batch 13510, train_perplexity=730.6601, train_loss=6.5939484

Batch 13520, train_perplexity=773.4609, train_loss=6.650875

Batch 13530, train_perplexity=712.25104, train_loss=6.5684304

Batch 13540, train_perplexity=706.7497, train_loss=6.5606766

Batch 13550, train_perplexity=688.19543, train_loss=6.534073

Batch 13560, train_perplexity=697.6344, train_loss=6.547695

Batch 13570, train_perplexity=772.54126, train_loss=6.6496854

Batch 13580, train_perplexity=726.17114, train_loss=6.5877857

Batch 13590, train_perplexity=745.91785, train_loss=6.6146154

Batch 13600, train_perplexity=694.3803, train_loss=6.54302

Batch 13610, train_perplexity=644.19617, train_loss=6.4680033

Batch 13620, train_perplexity=731.086, train_loss=6.594531

Batch 13630, train_perplexity=772.9768, train_loss=6.650249

Batch 13640, train_perplexity=700.96893, train_loss=6.5524635

Batch 13650, train_perplexity=713.1683, train_loss=6.5697174

Batch 13660, train_perplexity=685.398, train_loss=6.5299997

Batch 13670, train_perplexity=749.6597, train_loss=6.6196194

Batch 13680, train_perplexity=725.71246, train_loss=6.587154

Batch 13690, train_perplexity=710.7281, train_loss=6.56629

Batch 13700, train_perplexity=793.39185, train_loss=6.676317

Batch 13710, train_perplexity=756.9515, train_loss=6.629299

Batch 13720, train_perplexity=718.3869, train_loss=6.5770082

Batch 13730, train_perplexity=652.1497, train_loss=6.480274

Batch 13740, train_perplexity=779.8975, train_loss=6.6591625

Batch 13750, train_perplexity=731.11597, train_loss=6.594572
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13760, train_perplexity=690.60767, train_loss=6.537572

Batch 13770, train_perplexity=710.72675, train_loss=6.566288

Batch 13780, train_perplexity=668.9949, train_loss=6.5057764

Batch 13790, train_perplexity=698.8823, train_loss=6.5494823

Batch 13800, train_perplexity=649.1149, train_loss=6.47561

Batch 13810, train_perplexity=656.75616, train_loss=6.487313

Batch 13820, train_perplexity=727.7185, train_loss=6.5899143

Batch 13830, train_perplexity=656.43243, train_loss=6.4868197

Batch 13840, train_perplexity=702.9578, train_loss=6.555297

Batch 13850, train_perplexity=724.6098, train_loss=6.5856333

Batch 13860, train_perplexity=671.7452, train_loss=6.509879

Batch 13870, train_perplexity=711.2502, train_loss=6.567024

Batch 13880, train_perplexity=691.17566, train_loss=6.538394

Batch 13890, train_perplexity=684.63434, train_loss=6.528885

Batch 13900, train_perplexity=672.1092, train_loss=6.510421

Batch 13910, train_perplexity=756.5091, train_loss=6.6287146

Batch 13920, train_perplexity=690.1843, train_loss=6.5369587

Batch 13930, train_perplexity=680.8014, train_loss=6.5232706

Batch 13940, train_perplexity=670.1983, train_loss=6.5075736

Batch 13950, train_perplexity=653.17737, train_loss=6.4818487

Batch 13960, train_perplexity=723.08417, train_loss=6.5835257

Batch 13970, train_perplexity=614.4847, train_loss=6.420784

Batch 13980, train_perplexity=684.5028, train_loss=6.5286927

Batch 13990, train_perplexity=699.1229, train_loss=6.5498266

Batch 14000, train_perplexity=701.8663, train_loss=6.553743

Batch 14010, train_perplexity=714.49646, train_loss=6.571578

Batch 14020, train_perplexity=662.87585, train_loss=6.4965878

Batch 14030, train_perplexity=685.72687, train_loss=6.5304794

Batch 14040, train_perplexity=662.3479, train_loss=6.495791

Batch 14050, train_perplexity=681.0053, train_loss=6.52357

Batch 14060, train_perplexity=766.1737, train_loss=6.641409

Batch 14070, train_perplexity=633.6487, train_loss=6.4514947

Batch 14080, train_perplexity=694.75616, train_loss=6.543561

Batch 14090, train_perplexity=640.3886, train_loss=6.462075

Batch 14100, train_perplexity=754.8897, train_loss=6.6265717

Batch 14110, train_perplexity=662.1644, train_loss=6.495514

Batch 14120, train_perplexity=640.93304, train_loss=6.462925

Batch 14130, train_perplexity=671.2258, train_loss=6.5091057

Batch 14140, train_perplexity=658.16724, train_loss=6.489459

Batch 14150, train_perplexity=718.4924, train_loss=6.577155

Batch 14160, train_perplexity=682.9483, train_loss=6.526419

Batch 14170, train_perplexity=679.5789, train_loss=6.5214734

Batch 14180, train_perplexity=747.5803, train_loss=6.616842

Batch 14190, train_perplexity=627.48846, train_loss=6.4417253

Batch 14200, train_perplexity=686.9627, train_loss=6.53228

Batch 14210, train_perplexity=765.29083, train_loss=6.640256

Batch 14220, train_perplexity=739.86255, train_loss=6.6064644

Batch 14230, train_perplexity=714.16296, train_loss=6.571111

Batch 14240, train_perplexity=648.17035, train_loss=6.4741535

Batch 14250, train_perplexity=674.88196, train_loss=6.514538

Batch 14260, train_perplexity=597.20337, train_loss=6.3922577

Batch 14270, train_perplexity=691.715, train_loss=6.539174

Batch 14280, train_perplexity=630.13513, train_loss=6.4459343

Batch 14290, train_perplexity=663.877, train_loss=6.498097

Batch 14300, train_perplexity=715.6455, train_loss=6.573185

Batch 14310, train_perplexity=680.81696, train_loss=6.5232935

Batch 14320, train_perplexity=736.1906, train_loss=6.601489

Batch 14330, train_perplexity=632.037, train_loss=6.448948

Batch 14340, train_perplexity=613.4816, train_loss=6.4191504

Batch 14350, train_perplexity=701.6029, train_loss=6.5533676

Batch 14360, train_perplexity=694.2469, train_loss=6.5428276

Batch 14370, train_perplexity=728.2007, train_loss=6.5905766

Batch 14380, train_perplexity=780.96704, train_loss=6.660533

Batch 14390, train_perplexity=695.13196, train_loss=6.5441017

Batch 14400, train_perplexity=753.36035, train_loss=6.6245437

Batch 14410, train_perplexity=681.359, train_loss=6.5240893

Batch 14420, train_perplexity=629.3802, train_loss=6.4447355

Batch 14430, train_perplexity=736.02637, train_loss=6.601266

Batch 14440, train_perplexity=713.70105, train_loss=6.570464

Batch 14450, train_perplexity=661.4566, train_loss=6.4944444

Batch 14460, train_perplexity=636.41693, train_loss=6.455854

Batch 14470, train_perplexity=639.7269, train_loss=6.4610415

Batch 14480, train_perplexity=687.41583, train_loss=6.5329394

Batch 14490, train_perplexity=769.98486, train_loss=6.646371

Batch 14500, train_perplexity=636.5429, train_loss=6.456052

Batch 14510, train_perplexity=685.6494, train_loss=6.5303664

Batch 14520, train_perplexity=699.7887, train_loss=6.5507784

Batch 14530, train_perplexity=630.2313, train_loss=6.446087

Batch 14540, train_perplexity=625.5293, train_loss=6.438598

Batch 14550, train_perplexity=659.3263, train_loss=6.4912186

Batch 14560, train_perplexity=694.045, train_loss=6.5425367

Batch 14570, train_perplexity=667.63983, train_loss=6.503749

Batch 14580, train_perplexity=631.35504, train_loss=6.4478683

Batch 14590, train_perplexity=657.8685, train_loss=6.489005

Batch 14600, train_perplexity=582.7903, train_loss=6.3678274

Batch 14610, train_perplexity=682.4102, train_loss=6.525631

Batch 14620, train_perplexity=655.8764, train_loss=6.4859724

Batch 14630, train_perplexity=558.44666, train_loss=6.325159

Batch 14640, train_perplexity=674.13934, train_loss=6.513437

Batch 14650, train_perplexity=636.015, train_loss=6.455222

Batch 14660, train_perplexity=698.6817, train_loss=6.5491953

Batch 14670, train_perplexity=619.9003, train_loss=6.4295588

Batch 14680, train_perplexity=624.2465, train_loss=6.4365454

Batch 14690, train_perplexity=717.0378, train_loss=6.5751286

Batch 14700, train_perplexity=612.593, train_loss=6.417701

Batch 14710, train_perplexity=648.1314, train_loss=6.4740934

Batch 14720, train_perplexity=648.2767, train_loss=6.4743176

Batch 14730, train_perplexity=666.29614, train_loss=6.5017343

Batch 14740, train_perplexity=752.999, train_loss=6.624064

Batch 14750, train_perplexity=684.96545, train_loss=6.5293684

Batch 14760, train_perplexity=702.9317, train_loss=6.5552597

Batch 14770, train_perplexity=704.1998, train_loss=6.557062

Batch 14780, train_perplexity=662.4013, train_loss=6.4958715

Batch 14790, train_perplexity=697.085, train_loss=6.5469074

Batch 14800, train_perplexity=608.62103, train_loss=6.4111958

Batch 14810, train_perplexity=630.9349, train_loss=6.4472027

Batch 14820, train_perplexity=674.7127, train_loss=6.514287

Batch 14830, train_perplexity=740.29767, train_loss=6.6070523

Batch 14840, train_perplexity=691.9143, train_loss=6.539462

Batch 14850, train_perplexity=640.1798, train_loss=6.461749

Batch 14860, train_perplexity=656.46875, train_loss=6.486875

Batch 14870, train_perplexity=660.5284, train_loss=6.49304

Batch 14880, train_perplexity=631.6417, train_loss=6.4483223

Batch 14890, train_perplexity=675.14075, train_loss=6.514921

Batch 14900, train_perplexity=564.8527, train_loss=6.336565

Batch 14910, train_perplexity=713.79016, train_loss=6.570589

Batch 14920, train_perplexity=730.6789, train_loss=6.593974

Batch 14930, train_perplexity=656.74896, train_loss=6.487302

Batch 14940, train_perplexity=666.1894, train_loss=6.501574

Batch 14950, train_perplexity=627.18005, train_loss=6.4412336

Batch 14960, train_perplexity=718.4622, train_loss=6.577113

Batch 14970, train_perplexity=633.5614, train_loss=6.451357

Batch 14980, train_perplexity=673.61975, train_loss=6.5126657

Batch 14990, train_perplexity=612.9521, train_loss=6.418287

Batch 15000, train_perplexity=680.7105, train_loss=6.523137

Batch 15010, train_perplexity=738.0971, train_loss=6.6040754

Batch 15020, train_perplexity=655.2987, train_loss=6.485091

Batch 15030, train_perplexity=635.9046, train_loss=6.4550486

Batch 15040, train_perplexity=596.5692, train_loss=6.3911953

Batch 15050, train_perplexity=624.7462, train_loss=6.4373455

Batch 15060, train_perplexity=660.4345, train_loss=6.492898

Batch 15070, train_perplexity=687.9579, train_loss=6.5337276

Batch 15080, train_perplexity=656.9284, train_loss=6.487575
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15090, train_perplexity=688.61365, train_loss=6.5346804

Batch 15100, train_perplexity=675.75433, train_loss=6.5158296

Batch 15110, train_perplexity=744.6211, train_loss=6.6128755

Batch 15120, train_perplexity=602.93976, train_loss=6.4018173

Batch 15130, train_perplexity=620.686, train_loss=6.430825

Batch 15140, train_perplexity=620.05646, train_loss=6.4298105

Batch 15150, train_perplexity=621.5923, train_loss=6.4322844

Batch 15160, train_perplexity=724.85376, train_loss=6.58597

Batch 15170, train_perplexity=677.3309, train_loss=6.51816

Batch 15180, train_perplexity=622.7689, train_loss=6.4341755

Batch 15190, train_perplexity=653.5861, train_loss=6.4824743

Batch 15200, train_perplexity=658.39764, train_loss=6.489809

Batch 15210, train_perplexity=631.40894, train_loss=6.4479537

Batch 15220, train_perplexity=644.2668, train_loss=6.468113

Batch 15230, train_perplexity=611.9851, train_loss=6.416708

Batch 15240, train_perplexity=679.33594, train_loss=6.521116

Batch 15250, train_perplexity=631.9966, train_loss=6.448884

Batch 15260, train_perplexity=598.7151, train_loss=6.394786

Batch 15270, train_perplexity=635.19727, train_loss=6.4539356

Batch 15280, train_perplexity=639.4662, train_loss=6.4606338

Batch 15290, train_perplexity=617.1304, train_loss=6.4250803

Batch 15300, train_perplexity=694.9649, train_loss=6.5438614

Batch 15310, train_perplexity=766.5318, train_loss=6.641876

Batch 15320, train_perplexity=695.02655, train_loss=6.54395

Batch 15330, train_perplexity=591.0377, train_loss=6.38188

Batch 15340, train_perplexity=628.67566, train_loss=6.4436154

Batch 15350, train_perplexity=627.9006, train_loss=6.442382

Batch 15360, train_perplexity=579.0502, train_loss=6.361389

Batch 15370, train_perplexity=644.11847, train_loss=6.4678826

Batch 15380, train_perplexity=588.00684, train_loss=6.3767385

Batch 15390, train_perplexity=599.5016, train_loss=6.3960986

Batch 15400, train_perplexity=635.56445, train_loss=6.4545135

Batch 15410, train_perplexity=753.51483, train_loss=6.6247487

Batch 15420, train_perplexity=663.353, train_loss=6.4973073

Batch 15430, train_perplexity=613.0515, train_loss=6.418449

Batch 15440, train_perplexity=752.8931, train_loss=6.6239233

Batch 15450, train_perplexity=647.6099, train_loss=6.4732885

Batch 15460, train_perplexity=623.93225, train_loss=6.436042

Batch 15470, train_perplexity=662.1534, train_loss=6.495497

Batch 15480, train_perplexity=643.50354, train_loss=6.4669275

Batch 15490, train_perplexity=676.3181, train_loss=6.5166636

Batch 15500, train_perplexity=678.7347, train_loss=6.5202303

Batch 15510, train_perplexity=653.1356, train_loss=6.481785

Batch 15520, train_perplexity=579.97815, train_loss=6.3629904

Batch 15530, train_perplexity=645.8093, train_loss=6.4705043

Batch 15540, train_perplexity=667.1596, train_loss=6.5030293

Batch 15550, train_perplexity=629.88367, train_loss=6.445535

Batch 15560, train_perplexity=665.14166, train_loss=6.5

Batch 15570, train_perplexity=646.86273, train_loss=6.472134

Batch 15580, train_perplexity=622.8158, train_loss=6.434251

Batch 15590, train_perplexity=669.19586, train_loss=6.506077

Batch 15600, train_perplexity=643.29675, train_loss=6.466606

Batch 15610, train_perplexity=686.851, train_loss=6.5321174

Batch 15620, train_perplexity=643.8258, train_loss=6.467428

Batch 15630, train_perplexity=653.1945, train_loss=6.481875

Batch 15640, train_perplexity=636.8866, train_loss=6.4565916

Batch 15650, train_perplexity=622.3652, train_loss=6.433527

Batch 15660, train_perplexity=623.92334, train_loss=6.4360275

Batch 15670, train_perplexity=658.3719, train_loss=6.48977

Batch 15680, train_perplexity=603.2878, train_loss=6.4023943

Batch 15690, train_perplexity=589.014, train_loss=6.37845

Batch 15700, train_perplexity=628.16534, train_loss=6.4428034

Batch 15710, train_perplexity=627.64197, train_loss=6.44197

Batch 15720, train_perplexity=712.19434, train_loss=6.568351

Batch 15730, train_perplexity=656.3044, train_loss=6.4866247

Batch 15740, train_perplexity=725.29803, train_loss=6.5865827

Batch 15750, train_perplexity=608.3077, train_loss=6.410681

Batch 15760, train_perplexity=655.34686, train_loss=6.4851646

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Batch 15770, train_perplexity=557.88135, train_loss=6.3241463

Batch 15780, train_perplexity=626.9516, train_loss=6.4408693

Batch 15790, train_perplexity=631.70526, train_loss=6.448423

Batch 15800, train_perplexity=582.0538, train_loss=6.366563

Batch 15810, train_perplexity=694.5992, train_loss=6.543335

Batch 15820, train_perplexity=693.762, train_loss=6.542129

Batch 15830, train_perplexity=661.175, train_loss=6.4940186

Batch 15840, train_perplexity=633.6327, train_loss=6.4514694

Batch 15850, train_perplexity=654.8146, train_loss=6.484352

Batch 15860, train_perplexity=605.56464, train_loss=6.4061613

Batch 15870, train_perplexity=676.5865, train_loss=6.5170603

Batch 15880, train_perplexity=668.5032, train_loss=6.505041

Batch 15890, train_perplexity=579.81274, train_loss=6.362705

Batch 15900, train_perplexity=574.4585, train_loss=6.353428

Batch 15910, train_perplexity=633.67285, train_loss=6.451533

Batch 15920, train_perplexity=625.0755, train_loss=6.4378724

Batch 15930, train_perplexity=647.82025, train_loss=6.4736133

Batch 15940, train_perplexity=606.9864, train_loss=6.4085064

Batch 15950, train_perplexity=673.91046, train_loss=6.5130973

Batch 15960, train_perplexity=664.232, train_loss=6.4986315

Batch 15970, train_perplexity=637.1223, train_loss=6.4569616

Batch 15980, train_perplexity=638.36, train_loss=6.4589024

Batch 15990, train_perplexity=727.3494, train_loss=6.589407

Batch 16000, train_perplexity=606.9951, train_loss=6.4085207

Batch 16010, train_perplexity=620.409, train_loss=6.430379

Batch 16020, train_perplexity=698.9223, train_loss=6.5495396

Batch 16030, train_perplexity=605.9404, train_loss=6.4067817

Batch 16040, train_perplexity=576.0302, train_loss=6.35616

Batch 16050, train_perplexity=619.58386, train_loss=6.429048

Batch 16060, train_perplexity=656.00653, train_loss=6.486171

Batch 16070, train_perplexity=586.7498, train_loss=6.3745985

Batch 16080, train_perplexity=654.11707, train_loss=6.4832864

Batch 16090, train_perplexity=720.1766, train_loss=6.5794964

Batch 16100, train_perplexity=556.3276, train_loss=6.3213573

Batch 16110, train_perplexity=606.5605, train_loss=6.4078045

Batch 16120, train_perplexity=679.11993, train_loss=6.5207977

Batch 16130, train_perplexity=623.70355, train_loss=6.435675

Batch 16140, train_perplexity=647.25183, train_loss=6.4727354

Batch 16150, train_perplexity=653.19794, train_loss=6.48188

Batch 16160, train_perplexity=639.44025, train_loss=6.460593

Batch 16170, train_perplexity=583.945, train_loss=6.369807

Batch 16180, train_perplexity=626.74774, train_loss=6.440544

Batch 16190, train_perplexity=657.7785, train_loss=6.488868

Batch 16200, train_perplexity=669.4819, train_loss=6.506504

Batch 16210, train_perplexity=682.4031, train_loss=6.5256205

Batch 16220, train_perplexity=664.80524, train_loss=6.499494

Batch 16230, train_perplexity=647.7341, train_loss=6.47348

Batch 16240, train_perplexity=639.2391, train_loss=6.4602785

Batch 16250, train_perplexity=682.3227, train_loss=6.5255027

Batch 16260, train_perplexity=648.33105, train_loss=6.4744015

Batch 16270, train_perplexity=650.7385, train_loss=6.478108

Batch 16280, train_perplexity=569.3329, train_loss=6.3444653

Batch 16290, train_perplexity=664.64514, train_loss=6.4992533

Batch 16300, train_perplexity=607.1989, train_loss=6.4088564

Batch 16310, train_perplexity=661.93585, train_loss=6.4951687

Batch 16320, train_perplexity=592.0012, train_loss=6.3835087

Batch 16330, train_perplexity=619.22144, train_loss=6.428463

Batch 16340, train_perplexity=592.6248, train_loss=6.3845615

Batch 16350, train_perplexity=604.9424, train_loss=6.4051332
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 16360, train_perplexity=652.96747, train_loss=6.4815273

Batch 16370, train_perplexity=607.74927, train_loss=6.4097624

Batch 16380, train_perplexity=644.3166, train_loss=6.46819

Batch 16390, train_perplexity=632.5049, train_loss=6.449688

Batch 16400, train_perplexity=603.0174, train_loss=6.401946

Batch 16410, train_perplexity=596.3747, train_loss=6.390869

Batch 16420, train_perplexity=646.20917, train_loss=6.471123

Batch 16430, train_perplexity=614.9194, train_loss=6.421491

Batch 16440, train_perplexity=564.11896, train_loss=6.335265

Batch 16450, train_perplexity=660.0844, train_loss=6.4923677

Batch 16460, train_perplexity=590.71704, train_loss=6.381337

Batch 16470, train_perplexity=598.34155, train_loss=6.3941617

Batch 16480, train_perplexity=688.16266, train_loss=6.534025

Batch 16490, train_perplexity=637.1612, train_loss=6.4570227

Batch 16500, train_perplexity=535.0747, train_loss=6.2824063

Batch 16510, train_perplexity=631.8676, train_loss=6.44868

Batch 16520, train_perplexity=769.3929, train_loss=6.6456017

Batch 16530, train_perplexity=636.98254, train_loss=6.4567423

Batch 16540, train_perplexity=622.55927, train_loss=6.433839

Batch 16550, train_perplexity=589.8237, train_loss=6.3798237

Batch 16560, train_perplexity=676.44196, train_loss=6.5168467

Batch 16570, train_perplexity=620.0526, train_loss=6.4298043

Batch 16580, train_perplexity=588.60657, train_loss=6.377758

Batch 16590, train_perplexity=610.98676, train_loss=6.4150753

Batch 16600, train_perplexity=615.20294, train_loss=6.4219522

Batch 16610, train_perplexity=702.9682, train_loss=6.5553117

Batch 16620, train_perplexity=684.9246, train_loss=6.529309

Batch 16630, train_perplexity=709.87115, train_loss=6.5650835

Batch 16640, train_perplexity=631.21655, train_loss=6.447649

Batch 16650, train_perplexity=607.3732, train_loss=6.4091434

Batch 16660, train_perplexity=707.8809, train_loss=6.562276

Batch 16670, train_perplexity=639.2732, train_loss=6.460332

Batch 16680, train_perplexity=619.6208, train_loss=6.4291077

Batch 16690, train_perplexity=670.10657, train_loss=6.5074368

Batch 16700, train_perplexity=626.8365, train_loss=6.4406857

Batch 16710, train_perplexity=644.6387, train_loss=6.46869

Batch 16720, train_perplexity=631.21234, train_loss=6.4476423

Batch 16730, train_perplexity=657.1794, train_loss=6.487957

Batch 16740, train_perplexity=642.17566, train_loss=6.464862

Batch 16750, train_perplexity=670.4933, train_loss=6.5080137

Batch 16760, train_perplexity=595.01776, train_loss=6.3885913

Batch 16770, train_perplexity=618.7563, train_loss=6.4277115

Batch 16780, train_perplexity=715.13654, train_loss=6.5724735

Batch 16790, train_perplexity=611.31085, train_loss=6.4156055

Batch 16800, train_perplexity=636.19275, train_loss=6.4555016

Batch 16810, train_perplexity=628.1968, train_loss=6.4428535

Batch 16820, train_perplexity=610.6952, train_loss=6.414598

Batch 16830, train_perplexity=614.3956, train_loss=6.420639

Batch 16840, train_perplexity=719.22284, train_loss=6.5781713

Batch 16850, train_perplexity=598.9947, train_loss=6.3952527

Batch 16860, train_perplexity=620.173, train_loss=6.4299984

Batch 16870, train_perplexity=702.6824, train_loss=6.554905

Batch 16880, train_perplexity=624.26526, train_loss=6.4365754

Batch 16890, train_perplexity=616.5127, train_loss=6.424079

Batch 16900, train_perplexity=654.01196, train_loss=6.4831257

Batch 16910, train_perplexity=707.78406, train_loss=6.562139

Batch 16920, train_perplexity=625.46814, train_loss=6.4385004

Batch 16930, train_perplexity=601.02057, train_loss=6.398629

Batch 16940, train_perplexity=655.9997, train_loss=6.4861603

Batch 16950, train_perplexity=612.0855, train_loss=6.416872

Batch 16960, train_perplexity=646.32874, train_loss=6.471308

Batch 16970, train_perplexity=636.8769, train_loss=6.4565763

Batch 16980, train_perplexity=614.1736, train_loss=6.4202776

Batch 16990, train_perplexity=637.46265, train_loss=6.4574957

Batch 17000, train_perplexity=638.0009, train_loss=6.4583397

Batch 17010, train_perplexity=592.5819, train_loss=6.384489

Batch 17020, train_perplexity=595.1946, train_loss=6.3888884

Batch 17030, train_perplexity=641.02045, train_loss=6.4630613

Batch 17040, train_perplexity=559.8715, train_loss=6.3277073

Batch 17050, train_perplexity=580.89923, train_loss=6.3645773

Batch 17060, train_perplexity=575.5536, train_loss=6.3553324

Batch 17070, train_perplexity=678.03595, train_loss=6.5192003

Batch 17080, train_perplexity=634.7389, train_loss=6.4532137

Batch 17090, train_perplexity=634.8282, train_loss=6.4533544

Batch 17100, train_perplexity=665.101, train_loss=6.499939

Batch 17110, train_perplexity=585.5633, train_loss=6.3725743

Batch 17120, train_perplexity=584.1541, train_loss=6.370165

Batch 17130, train_perplexity=554.2345, train_loss=6.317588

Batch 17140, train_perplexity=642.58887, train_loss=6.465505

Batch 17150, train_perplexity=578.81885, train_loss=6.3609896

Batch 17160, train_perplexity=585.0651, train_loss=6.371723

Batch 17170, train_perplexity=646.69684, train_loss=6.4718776

Batch 17180, train_perplexity=635.08765, train_loss=6.453763

Batch 17190, train_perplexity=609.12616, train_loss=6.4120255

Batch 17200, train_perplexity=584.5676, train_loss=6.3708725

Batch 17210, train_perplexity=601.07074, train_loss=6.3987126

Batch 17220, train_perplexity=561.79535, train_loss=6.3311377

Batch 17230, train_perplexity=599.0218, train_loss=6.395298

Batch 17240, train_perplexity=617.602, train_loss=6.425844

Batch 17250, train_perplexity=547.1802, train_loss=6.304778

Batch 17260, train_perplexity=575.9391, train_loss=6.356002

Batch 17270, train_perplexity=605.3198, train_loss=6.405757

Batch 17280, train_perplexity=626.11237, train_loss=6.43953

Batch 17290, train_perplexity=691.0544, train_loss=6.5382185

Batch 17300, train_perplexity=613.9938, train_loss=6.419985

Batch 17310, train_perplexity=607.0463, train_loss=6.408605

Batch 17320, train_perplexity=575.68506, train_loss=6.355561

Batch 17330, train_perplexity=536.9361, train_loss=6.285879

Batch 17340, train_perplexity=589.2036, train_loss=6.378772

Batch 17350, train_perplexity=635.1724, train_loss=6.4538965

Batch 17360, train_perplexity=571.87714, train_loss=6.348924

Batch 17370, train_perplexity=558.89044, train_loss=6.3259535

Batch 17380, train_perplexity=622.74396, train_loss=6.4341354

Batch 17390, train_perplexity=623.76807, train_loss=6.4357786

Batch 17400, train_perplexity=634.04614, train_loss=6.4521217

Batch 17410, train_perplexity=673.7881, train_loss=6.5129156

Batch 17420, train_perplexity=576.6607, train_loss=6.357254

Batch 17430, train_perplexity=532.2691, train_loss=6.277149

Batch 17440, train_perplexity=587.257, train_loss=6.3754625

Batch 17450, train_perplexity=647.61237, train_loss=6.4732924

Batch 17460, train_perplexity=600.32513, train_loss=6.3974714

Batch 17470, train_perplexity=624.6604, train_loss=6.437208

Batch 17480, train_perplexity=581.99493, train_loss=6.3664618

Batch 17490, train_perplexity=615.4471, train_loss=6.422349

Batch 17500, train_perplexity=584.7472, train_loss=6.3711796

Batch 17510, train_perplexity=535.4838, train_loss=6.2831707

Batch 17520, train_perplexity=577.01605, train_loss=6.35787

Batch 17530, train_perplexity=546.2901, train_loss=6.30315

Batch 17540, train_perplexity=625.5848, train_loss=6.438687

Batch 17550, train_perplexity=590.8579, train_loss=6.3815756

Batch 17560, train_perplexity=596.87427, train_loss=6.3917065

Batch 17570, train_perplexity=558.6075, train_loss=6.325447

Batch 17580, train_perplexity=682.0511, train_loss=6.5251045

Batch 17590, train_perplexity=607.4685, train_loss=6.4093003

Batch 17600, train_perplexity=570.2512, train_loss=6.346077

Batch 17610, train_perplexity=651.1225, train_loss=6.478698

Batch 17620, train_perplexity=617.4966, train_loss=6.4256735

Batch 17630, train_perplexity=643.7795, train_loss=6.467356

Batch 17640, train_perplexity=602.2852, train_loss=6.400731

Batch 17650, train_perplexity=605.2407, train_loss=6.4056263

Batch 17660, train_perplexity=628.6244, train_loss=6.443534

Batch 17670, train_perplexity=653.6478, train_loss=6.4825687

Batch 17680, train_perplexity=640.7176, train_loss=6.462589
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17690, train_perplexity=647.2944, train_loss=6.472801

Batch 17700, train_perplexity=559.91046, train_loss=6.327777

Batch 17710, train_perplexity=516.7354, train_loss=6.247531

Batch 17720, train_perplexity=627.1214, train_loss=6.44114

Batch 17730, train_perplexity=598.60266, train_loss=6.394598

Batch 17740, train_perplexity=637.33014, train_loss=6.457288

Batch 17750, train_perplexity=549.0887, train_loss=6.30826

Batch 17760, train_perplexity=649.6983, train_loss=6.476508

Batch 17770, train_perplexity=582.7386, train_loss=6.3677387

Batch 17780, train_perplexity=647.37744, train_loss=6.4729295

Batch 17790, train_perplexity=613.54425, train_loss=6.4192524

Batch 17800, train_perplexity=719.64105, train_loss=6.5787525

Batch 17810, train_perplexity=592.808, train_loss=6.3848705

Batch 17820, train_perplexity=562.68866, train_loss=6.3327265

Batch 17830, train_perplexity=593.4684, train_loss=6.385984

Batch 17840, train_perplexity=704.6311, train_loss=6.5576744

Batch 17850, train_perplexity=539.33124, train_loss=6.29033

Batch 17860, train_perplexity=610.84406, train_loss=6.4148417

Batch 17870, train_perplexity=567.43835, train_loss=6.341132

Batch 17880, train_perplexity=580.59045, train_loss=6.3640456

Batch 17890, train_perplexity=620.93695, train_loss=6.4312296

Batch 17900, train_perplexity=555.31805, train_loss=6.319541

Batch 17910, train_perplexity=617.61816, train_loss=6.4258704

Batch 17920, train_perplexity=641.2901, train_loss=6.463482

Batch 17930, train_perplexity=599.6148, train_loss=6.3962874

Batch 17940, train_perplexity=612.2478, train_loss=6.417137

Batch 17950, train_perplexity=622.3901, train_loss=6.433567

Batch 17960, train_perplexity=617.92957, train_loss=6.4263744

Batch 17970, train_perplexity=618.61884, train_loss=6.4274893

Batch 17980, train_perplexity=630.5421, train_loss=6.44658

Batch 17990, train_perplexity=531.5278, train_loss=6.2757554

Batch 18000, train_perplexity=546.73206, train_loss=6.303959

Batch 18010, train_perplexity=568.46454, train_loss=6.342939

Batch 18020, train_perplexity=596.058, train_loss=6.390338

Batch 18030, train_perplexity=609.4353, train_loss=6.412533

Batch 18040, train_perplexity=617.3785, train_loss=6.4254823

Batch 18050, train_perplexity=580.49, train_loss=6.3638725

Batch 18060, train_perplexity=573.2269, train_loss=6.3512816

Batch 18070, train_perplexity=516.9062, train_loss=6.2478614

Batch 18080, train_perplexity=555.83435, train_loss=6.3204703

Batch 18090, train_perplexity=595.14233, train_loss=6.3888006

Batch 18100, train_perplexity=596.10376, train_loss=6.3904147

Batch 18110, train_perplexity=591.17017, train_loss=6.382104

Batch 18120, train_perplexity=567.2098, train_loss=6.340729

Batch 18130, train_perplexity=615.96497, train_loss=6.42319

Batch 18140, train_perplexity=590.449, train_loss=6.380883

Batch 18150, train_perplexity=587.4071, train_loss=6.375718

Batch 18160, train_perplexity=543.87396, train_loss=6.2987175

Batch 18170, train_perplexity=593.73956, train_loss=6.3864408

Batch 18180, train_perplexity=550.73596, train_loss=6.3112555

Batch 18190, train_perplexity=592.49286, train_loss=6.384339

Batch 18200, train_perplexity=621.4714, train_loss=6.43209

Batch 18210, train_perplexity=643.2139, train_loss=6.4664774

Batch 18220, train_perplexity=631.49414, train_loss=6.4480886

Batch 18230, train_perplexity=592.6983, train_loss=6.3846855

Batch 18240, train_perplexity=603.1658, train_loss=6.402192

Batch 18250, train_perplexity=608.78937, train_loss=6.4114723

Batch 18260, train_perplexity=632.6762, train_loss=6.449959

Batch 18270, train_perplexity=626.3402, train_loss=6.4398937

Batch 18280, train_perplexity=576.5892, train_loss=6.35713

Batch 18290, train_perplexity=608.3065, train_loss=6.410679

Batch 18300, train_perplexity=495.91696, train_loss=6.2064085

Batch 18310, train_perplexity=678.55896, train_loss=6.5199714

Batch 18320, train_perplexity=633.96875, train_loss=6.4519997

Batch 18330, train_perplexity=533.45264, train_loss=6.2793703

Batch 18340, train_perplexity=618.9611, train_loss=6.4280424

Batch 18350, train_perplexity=564.17816, train_loss=6.33537

Batch 18360, train_perplexity=705.3848, train_loss=6.5587435

Batch 18370, train_perplexity=605.4001, train_loss=6.4058895

Batch 18380, train_perplexity=549.2715, train_loss=6.308593

Batch 18390, train_perplexity=690.47003, train_loss=6.5373726

Batch 18400, train_perplexity=680.6339, train_loss=6.5230246

Batch 18410, train_perplexity=560.13, train_loss=6.328169

Batch 18420, train_perplexity=584.48126, train_loss=6.3707247

Batch 18430, train_perplexity=531.0163, train_loss=6.2747927

Batch 18440, train_perplexity=516.4957, train_loss=6.247067

Batch 18450, train_perplexity=584.5094, train_loss=6.370773

Batch 18460, train_perplexity=592.8221, train_loss=6.3848944

Batch 18470, train_perplexity=520.534, train_loss=6.254855

Batch 18480, train_perplexity=568.4247, train_loss=6.342869

Batch 18490, train_perplexity=639.09216, train_loss=6.4600487

Batch 18500, train_perplexity=616.31635, train_loss=6.4237604

Batch 18510, train_perplexity=583.0132, train_loss=6.36821

Batch 18520, train_perplexity=574.33307, train_loss=6.3532095

Batch 18530, train_perplexity=580.6865, train_loss=6.364211

Batch 18540, train_perplexity=652.5856, train_loss=6.4809422

Batch 18550, train_perplexity=653.0634, train_loss=6.481674

Batch 18560, train_perplexity=549.1023, train_loss=6.3082848

Batch 18570, train_perplexity=544.20703, train_loss=6.2993298

Batch 18580, train_perplexity=582.61804, train_loss=6.367532

Batch 18590, train_perplexity=585.64374, train_loss=6.3727117

Batch 18600, train_perplexity=639.94293, train_loss=6.461379

Batch 18610, train_perplexity=615.01587, train_loss=6.421648

Batch 18620, train_perplexity=592.84357, train_loss=6.3849306

Batch 18630, train_perplexity=599.4104, train_loss=6.3959465

Batch 18640, train_perplexity=590.43713, train_loss=6.380863

Batch 18650, train_perplexity=549.8417, train_loss=6.3096304

Batch 18660, train_perplexity=571.4603, train_loss=6.348195

Batch 18670, train_perplexity=546.7847, train_loss=6.304055

Batch 18680, train_perplexity=592.4446, train_loss=6.3842573

Batch 18690, train_perplexity=567.68414, train_loss=6.341565

Batch 18700, train_perplexity=588.77783, train_loss=6.378049

Batch 18710, train_perplexity=549.45825, train_loss=6.308933

Batch 18720, train_perplexity=577.14484, train_loss=6.3580933

Batch 18730, train_perplexity=530.6447, train_loss=6.2740927

Batch 18740, train_perplexity=625.2287, train_loss=6.4381175

Batch 18750, train_perplexity=609.6324, train_loss=6.412856

Batch 18760, train_perplexity=638.6425, train_loss=6.459345

Batch 18770, train_perplexity=634.07697, train_loss=6.4521704

Batch 18780, train_perplexity=681.84717, train_loss=6.5248055

Batch 18790, train_perplexity=556.65845, train_loss=6.321952

Batch 18800, train_perplexity=610.87085, train_loss=6.4148855

Batch 18810, train_perplexity=549.8975, train_loss=6.309732

Batch 18820, train_perplexity=520.4687, train_loss=6.2547297

Batch 18830, train_perplexity=538.6733, train_loss=6.289109

Batch 18840, train_perplexity=648.6703, train_loss=6.4749246

Batch 18850, train_perplexity=594.69415, train_loss=6.388047

Batch 18860, train_perplexity=537.20654, train_loss=6.2863827

Batch 18870, train_perplexity=552.6764, train_loss=6.3147726

Batch 18880, train_perplexity=589.1232, train_loss=6.3786354

Batch 18890, train_perplexity=602.2622, train_loss=6.400693

Batch 18900, train_perplexity=610.99286, train_loss=6.4150853

Batch 18910, train_perplexity=571.3516, train_loss=6.348005

Batch 18920, train_perplexity=578.6282, train_loss=6.36066

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100
Loaded 306074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00058-of-00100
Loaded 306074 sentences.
Finished loading
Batch 18930, train_perplexity=628.6268, train_loss=6.4435377

Batch 18940, train_perplexity=605.17206, train_loss=6.405513

Batch 18950, train_perplexity=602.2691, train_loss=6.4007044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 18960, train_perplexity=546.6867, train_loss=6.303876

Batch 18970, train_perplexity=583.03436, train_loss=6.368246

Batch 18980, train_perplexity=551.61694, train_loss=6.312854

Batch 18990, train_perplexity=582.40857, train_loss=6.3671722

Batch 19000, train_perplexity=549.5675, train_loss=6.3091316

Batch 19010, train_perplexity=623.08673, train_loss=6.4346857

Batch 19020, train_perplexity=563.628, train_loss=6.3343945

Batch 19030, train_perplexity=591.8657, train_loss=6.38328

Batch 19040, train_perplexity=527.0125, train_loss=6.2672243

Batch 19050, train_perplexity=541.79645, train_loss=6.2948904

Batch 19060, train_perplexity=587.9771, train_loss=6.376688

Batch 19070, train_perplexity=603.71075, train_loss=6.4030952

Batch 19080, train_perplexity=575.97284, train_loss=6.3560605

Batch 19090, train_perplexity=628.98926, train_loss=6.444114

Batch 19100, train_perplexity=574.9883, train_loss=6.3543496

Batch 19110, train_perplexity=563.32654, train_loss=6.3338594

Batch 19120, train_perplexity=533.43176, train_loss=6.279331

Batch 19130, train_perplexity=567.22034, train_loss=6.340748

Batch 19140, train_perplexity=617.4742, train_loss=6.4256372

Batch 19150, train_perplexity=508.11346, train_loss=6.230705

Batch 19160, train_perplexity=588.8707, train_loss=6.3782067

Batch 19170, train_perplexity=619.47546, train_loss=6.428873

Batch 19180, train_perplexity=578.0846, train_loss=6.35972

Batch 19190, train_perplexity=558.09393, train_loss=6.3245273

Batch 19200, train_perplexity=632.5631, train_loss=6.44978

Batch 19210, train_perplexity=602.6104, train_loss=6.401271

Batch 19220, train_perplexity=620.7212, train_loss=6.430882

Batch 19230, train_perplexity=567.15436, train_loss=6.3406315

Batch 19240, train_perplexity=559.69855, train_loss=6.3273983

Batch 19250, train_perplexity=606.4619, train_loss=6.407642

Batch 19260, train_perplexity=564.27795, train_loss=6.335547

Batch 19270, train_perplexity=570.0881, train_loss=6.345791

Batch 19280, train_perplexity=624.6515, train_loss=6.437194

Batch 19290, train_perplexity=593.65265, train_loss=6.3862944

Batch 19300, train_perplexity=539.6387, train_loss=6.2908998

Batch 19310, train_perplexity=575.61426, train_loss=6.3554378

Batch 19320, train_perplexity=597.9488, train_loss=6.393505

Batch 19330, train_perplexity=607.9434, train_loss=6.410082

Batch 19340, train_perplexity=526.23004, train_loss=6.2657385

Batch 19350, train_perplexity=540.8236, train_loss=6.293093

Batch 19360, train_perplexity=620.0148, train_loss=6.4297433

Batch 19370, train_perplexity=565.7347, train_loss=6.338125

Batch 19380, train_perplexity=650.19604, train_loss=6.477274

Batch 19390, train_perplexity=549.47107, train_loss=6.308956

Batch 19400, train_perplexity=577.38983, train_loss=6.3585176

Batch 19410, train_perplexity=549.88837, train_loss=6.3097153

Batch 19420, train_perplexity=599.3973, train_loss=6.3959246

Batch 19430, train_perplexity=545.70953, train_loss=6.302087

Batch 19440, train_perplexity=598.4394, train_loss=6.3943253

Batch 19450, train_perplexity=555.79224, train_loss=6.3203945

Batch 19460, train_perplexity=545.41974, train_loss=6.3015556

Batch 19470, train_perplexity=596.3684, train_loss=6.3908587

Batch 19480, train_perplexity=525.08856, train_loss=6.263567

Batch 19490, train_perplexity=549.7489, train_loss=6.3094616

Batch 19500, train_perplexity=546.95374, train_loss=6.304364

Batch 19510, train_perplexity=524.3725, train_loss=6.2622023

Batch 19520, train_perplexity=573.35596, train_loss=6.3515067

Batch 19530, train_perplexity=548.6278, train_loss=6.3074203

Batch 19540, train_perplexity=572.0307, train_loss=6.3491926

Batch 19550, train_perplexity=603.6285, train_loss=6.402959

Batch 19560, train_perplexity=624.9673, train_loss=6.4376993

Batch 19570, train_perplexity=556.14056, train_loss=6.321021

Batch 19580, train_perplexity=535.8974, train_loss=6.2839427

Batch 19590, train_perplexity=563.8269, train_loss=6.3347473

Batch 19600, train_perplexity=565.64136, train_loss=6.3379602

Batch 19610, train_perplexity=631.8327, train_loss=6.4486246

Batch 19620, train_perplexity=658.55273, train_loss=6.4900446

Batch 19630, train_perplexity=545.01337, train_loss=6.3008103

Batch 19640, train_perplexity=611.0695, train_loss=6.4152107

Batch 19650, train_perplexity=621.27344, train_loss=6.4317713

Batch 19660, train_perplexity=524.512, train_loss=6.2624683

Batch 19670, train_perplexity=520.7241, train_loss=6.2552204

Batch 19680, train_perplexity=579.9629, train_loss=6.362964

Batch 19690, train_perplexity=618.42004, train_loss=6.427168

Batch 19700, train_perplexity=541.3998, train_loss=6.294158

Batch 19710, train_perplexity=632.9433, train_loss=6.450381

Batch 19720, train_perplexity=567.9308, train_loss=6.3419995

Batch 19730, train_perplexity=559.72253, train_loss=6.327441

Batch 19740, train_perplexity=587.34436, train_loss=6.3756113

Batch 19750, train_perplexity=539.124, train_loss=6.2899456

Batch 19760, train_perplexity=537.61145, train_loss=6.287136

Batch 19770, train_perplexity=559.58966, train_loss=6.3272038

Batch 19780, train_perplexity=564.76117, train_loss=6.336403

Batch 19790, train_perplexity=554.07355, train_loss=6.3172975

Batch 19800, train_perplexity=500.22803, train_loss=6.215064

Batch 19810, train_perplexity=566.5019, train_loss=6.3394804

Batch 19820, train_perplexity=656.17236, train_loss=6.4864235

Batch 19830, train_perplexity=574.8356, train_loss=6.354084

Batch 19840, train_perplexity=631.26654, train_loss=6.447728

Batch 19850, train_perplexity=548.91724, train_loss=6.3079476

Batch 19860, train_perplexity=549.59296, train_loss=6.309178

Batch 19870, train_perplexity=594.00775, train_loss=6.3868923

Batch 19880, train_perplexity=581.7832, train_loss=6.366098

Batch 19890, train_perplexity=586.03845, train_loss=6.3733854

Batch 19900, train_perplexity=512.2869, train_loss=6.238885

Batch 19910, train_perplexity=532.83234, train_loss=6.278207

Batch 19920, train_perplexity=634.449, train_loss=6.452757

Batch 19930, train_perplexity=506.0261, train_loss=6.2265882

Batch 19940, train_perplexity=550.7816, train_loss=6.3113384

Batch 19950, train_perplexity=571.317, train_loss=6.3479443

Batch 19960, train_perplexity=548.9429, train_loss=6.3079944

Batch 19970, train_perplexity=536.0474, train_loss=6.2842226

Batch 19980, train_perplexity=559.7343, train_loss=6.327462

Batch 19990, train_perplexity=547.4419, train_loss=6.3052564

Batch 20000, train_perplexity=609.8734, train_loss=6.4132514

Batch 20010, train_perplexity=557.6978, train_loss=6.3238173

Batch 20020, train_perplexity=588.73175, train_loss=6.3779707

Batch 20030, train_perplexity=543.4369, train_loss=6.2979136

Batch 20040, train_perplexity=574.8131, train_loss=6.354045

Batch 20050, train_perplexity=522.1102, train_loss=6.257879

Batch 20060, train_perplexity=568.42194, train_loss=6.342864

Batch 20070, train_perplexity=592.48413, train_loss=6.384324

Batch 20080, train_perplexity=458.76852, train_loss=6.1285458

Batch 20090, train_perplexity=528.22595, train_loss=6.269524

Batch 20100, train_perplexity=524.6799, train_loss=6.2627883

Batch 20110, train_perplexity=541.4778, train_loss=6.294302

Batch 20120, train_perplexity=560.364, train_loss=6.3285866

Batch 20130, train_perplexity=601.81036, train_loss=6.3999424

Batch 20140, train_perplexity=539.8134, train_loss=6.2912235

Batch 20150, train_perplexity=630.84106, train_loss=6.447054

Batch 20160, train_perplexity=553.5578, train_loss=6.316366

Batch 20170, train_perplexity=535.0408, train_loss=6.282343

Batch 20180, train_perplexity=596.9585, train_loss=6.3918476

Batch 20190, train_perplexity=553.2232, train_loss=6.3157616

Batch 20200, train_perplexity=571.0975, train_loss=6.34756

Batch 20210, train_perplexity=548.96875, train_loss=6.3080416

Batch 20220, train_perplexity=551.5404, train_loss=6.312715

Batch 20230, train_perplexity=566.7799, train_loss=6.339971

Batch 20240, train_perplexity=609.042, train_loss=6.411887

Batch 20250, train_perplexity=497.1799, train_loss=6.208952

Batch 20260, train_perplexity=562.9785, train_loss=6.3332415

Batch 20270, train_perplexity=613.9546, train_loss=6.419921

Batch 20280, train_perplexity=654.784, train_loss=6.4843054
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 20290, train_perplexity=543.1063, train_loss=6.297305

Batch 20300, train_perplexity=575.5171, train_loss=6.355269

Batch 20310, train_perplexity=529.1085, train_loss=6.2711935

Batch 20320, train_perplexity=555.67035, train_loss=6.320175

Batch 20330, train_perplexity=566.5548, train_loss=6.339574

Batch 20340, train_perplexity=549.96075, train_loss=6.309847

Batch 20350, train_perplexity=587.4934, train_loss=6.375865

Batch 20360, train_perplexity=566.47754, train_loss=6.3394375

Batch 20370, train_perplexity=538.5187, train_loss=6.288822

Batch 20380, train_perplexity=521.25006, train_loss=6.25623

Batch 20390, train_perplexity=606.53796, train_loss=6.4077673

Batch 20400, train_perplexity=569.0009, train_loss=6.343882

Batch 20410, train_perplexity=599.74774, train_loss=6.396509

Batch 20420, train_perplexity=594.04456, train_loss=6.3869543

Batch 20430, train_perplexity=542.51465, train_loss=6.296215

Batch 20440, train_perplexity=549.665, train_loss=6.309309

Batch 20450, train_perplexity=602.12555, train_loss=6.400466

Batch 20460, train_perplexity=583.103, train_loss=6.368364

Batch 20470, train_perplexity=603.7476, train_loss=6.4031563

Batch 20480, train_perplexity=604.6275, train_loss=6.4046125

Batch 20490, train_perplexity=570.8971, train_loss=6.347209

Batch 20500, train_perplexity=540.0356, train_loss=6.291635

Batch 20510, train_perplexity=571.3127, train_loss=6.3479366

Batch 20520, train_perplexity=540.3261, train_loss=6.292173

Batch 20530, train_perplexity=538.87164, train_loss=6.2894773

Batch 20540, train_perplexity=516.7216, train_loss=6.247504

Batch 20550, train_perplexity=624.7671, train_loss=6.437379

Batch 20560, train_perplexity=523.7353, train_loss=6.2609863

Batch 20570, train_perplexity=613.6894, train_loss=6.419489

Batch 20580, train_perplexity=568.67303, train_loss=6.3433056

Batch 20590, train_perplexity=588.704, train_loss=6.3779235

Batch 20600, train_perplexity=595.8579, train_loss=6.3900023

Batch 20610, train_perplexity=539.5581, train_loss=6.2907505

Batch 20620, train_perplexity=554.33124, train_loss=6.3177624

Batch 20630, train_perplexity=518.2312, train_loss=6.2504215

Batch 20640, train_perplexity=519.82654, train_loss=6.253495

Batch 20650, train_perplexity=534.5399, train_loss=6.2814064

Batch 20660, train_perplexity=536.3552, train_loss=6.2847967

Batch 20670, train_perplexity=550.31934, train_loss=6.3104987

Batch 20680, train_perplexity=497.0609, train_loss=6.2087126

Batch 20690, train_perplexity=576.89557, train_loss=6.3576612

Batch 20700, train_perplexity=530.0249, train_loss=6.272924

Batch 20710, train_perplexity=543.1084, train_loss=6.297309

Batch 20720, train_perplexity=585.3154, train_loss=6.372151

Batch 20730, train_perplexity=594.5841, train_loss=6.387862

Batch 20740, train_perplexity=577.1074, train_loss=6.3580284

Batch 20750, train_perplexity=577.0416, train_loss=6.3579144

Batch 20760, train_perplexity=527.3821, train_loss=6.2679253

Batch 20770, train_perplexity=574.29034, train_loss=6.353135

Batch 20780, train_perplexity=564.9723, train_loss=6.3367767

Batch 20790, train_perplexity=561.3656, train_loss=6.3303723

Batch 20800, train_perplexity=527.9351, train_loss=6.2689734

Batch 20810, train_perplexity=559.6676, train_loss=6.327343

Batch 20820, train_perplexity=551.21436, train_loss=6.312124

Batch 20830, train_perplexity=492.31046, train_loss=6.1991096

Batch 20840, train_perplexity=617.8665, train_loss=6.4262724

Batch 20850, train_perplexity=631.517, train_loss=6.448125

Batch 20860, train_perplexity=574.44507, train_loss=6.3534045

Batch 20870, train_perplexity=604.6456, train_loss=6.4046426

Batch 20880, train_perplexity=541.54645, train_loss=6.294429

Batch 20890, train_perplexity=515.642, train_loss=6.245413

Batch 20900, train_perplexity=612.15497, train_loss=6.4169855

Batch 20910, train_perplexity=576.455, train_loss=6.3568974

Batch 20920, train_perplexity=540.0142, train_loss=6.2915955

Batch 20930, train_perplexity=632.96136, train_loss=6.4504094

Batch 20940, train_perplexity=539.1631, train_loss=6.290018

Batch 20950, train_perplexity=548.05493, train_loss=6.3063755

Batch 20960, train_perplexity=525.1832, train_loss=6.263747

Batch 20970, train_perplexity=566.3733, train_loss=6.3392534

Batch 20980, train_perplexity=551.0233, train_loss=6.311777

Batch 20990, train_perplexity=520.98096, train_loss=6.2557135

Batch 21000, train_perplexity=530.03625, train_loss=6.2729454

Batch 21010, train_perplexity=534.51544, train_loss=6.2813606

Batch 21020, train_perplexity=620.68005, train_loss=6.4308157

Batch 21030, train_perplexity=576.80725, train_loss=6.357508

Batch 21040, train_perplexity=516.3797, train_loss=6.2468424

Batch 21050, train_perplexity=588.3316, train_loss=6.3772907

Batch 21060, train_perplexity=580.4186, train_loss=6.3637495

Batch 21070, train_perplexity=584.6889, train_loss=6.37108

Batch 21080, train_perplexity=587.40265, train_loss=6.3757105

Batch 21090, train_perplexity=536.9587, train_loss=6.285921

Batch 21100, train_perplexity=621.83124, train_loss=6.4326687

Batch 21110, train_perplexity=549.06854, train_loss=6.3082232

Batch 21120, train_perplexity=536.91974, train_loss=6.2858486

Batch 21130, train_perplexity=572.7698, train_loss=6.350484

Batch 21140, train_perplexity=542.821, train_loss=6.2967796

Batch 21150, train_perplexity=593.7092, train_loss=6.3863897

Batch 21160, train_perplexity=575.2987, train_loss=6.3548894

Batch 21170, train_perplexity=618.9056, train_loss=6.427953

Batch 21180, train_perplexity=480.57806, train_loss=6.1749897

Batch 21190, train_perplexity=539.8335, train_loss=6.2912607

Batch 21200, train_perplexity=532.34576, train_loss=6.277293

Batch 21210, train_perplexity=570.26587, train_loss=6.3461027

Batch 21220, train_perplexity=547.2381, train_loss=6.304884

Batch 21230, train_perplexity=537.5912, train_loss=6.2870984

Batch 21240, train_perplexity=583.6265, train_loss=6.3692613

Batch 21250, train_perplexity=521.6668, train_loss=6.257029

Batch 21260, train_perplexity=554.09076, train_loss=6.3173285

Batch 21270, train_perplexity=538.06335, train_loss=6.2879763

Batch 21280, train_perplexity=574.60095, train_loss=6.353676

Batch 21290, train_perplexity=577.9473, train_loss=6.359483

Batch 21300, train_perplexity=506.5234, train_loss=6.2275705

Batch 21310, train_perplexity=513.1603, train_loss=6.240588

Batch 21320, train_perplexity=554.9305, train_loss=6.318843

Batch 21330, train_perplexity=570.0011, train_loss=6.3456383

Batch 21340, train_perplexity=556.0255, train_loss=6.320814

Batch 21350, train_perplexity=584.3129, train_loss=6.3704367

Batch 21360, train_perplexity=581.0521, train_loss=6.3648405

Batch 21370, train_perplexity=549.3666, train_loss=6.308766

Batch 21380, train_perplexity=512.08887, train_loss=6.238498

Batch 21390, train_perplexity=525.9481, train_loss=6.2652025

Batch 21400, train_perplexity=513.5622, train_loss=6.241371

Batch 21410, train_perplexity=506.93466, train_loss=6.228382

Batch 21420, train_perplexity=566.6929, train_loss=6.3398175

Batch 21430, train_perplexity=560.75, train_loss=6.329275

Batch 21440, train_perplexity=485.34235, train_loss=6.1848545

Batch 21450, train_perplexity=549.59607, train_loss=6.3091836

Batch 21460, train_perplexity=575.6845, train_loss=6.35556

Batch 21470, train_perplexity=611.93085, train_loss=6.4166193

Batch 21480, train_perplexity=524.0485, train_loss=6.2615843

Batch 21490, train_perplexity=503.70297, train_loss=6.221987

Batch 21500, train_perplexity=544.9544, train_loss=6.300702

Batch 21510, train_perplexity=552.124, train_loss=6.3137727

Batch 21520, train_perplexity=588.87836, train_loss=6.3782196

Batch 21530, train_perplexity=544.8743, train_loss=6.300555

Batch 21540, train_perplexity=503.33948, train_loss=6.221265

Batch 21550, train_perplexity=555.13403, train_loss=6.3192096

Batch 21560, train_perplexity=499.98764, train_loss=6.2145834

Batch 21570, train_perplexity=531.2242, train_loss=6.275184

Batch 21580, train_perplexity=534.4742, train_loss=6.2812834

Batch 21590, train_perplexity=628.2246, train_loss=6.442898

Batch 21600, train_perplexity=498.71994, train_loss=6.2120447

Batch 21610, train_perplexity=537.47614, train_loss=6.2868843
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21620, train_perplexity=605.8431, train_loss=6.406621

Batch 21630, train_perplexity=631.38965, train_loss=6.447923

Batch 21640, train_perplexity=565.75006, train_loss=6.3381524

Batch 21650, train_perplexity=532.4443, train_loss=6.277478

Batch 21660, train_perplexity=554.31537, train_loss=6.317734

Batch 21670, train_perplexity=557.6308, train_loss=6.323697

Batch 21680, train_perplexity=491.0147, train_loss=6.196474

Batch 21690, train_perplexity=537.78424, train_loss=6.2874575

Batch 21700, train_perplexity=534.4094, train_loss=6.2811623

Batch 21710, train_perplexity=595.8994, train_loss=6.390072

Batch 21720, train_perplexity=543.5328, train_loss=6.29809

Batch 21730, train_perplexity=536.8895, train_loss=6.2857924

Batch 21740, train_perplexity=543.1674, train_loss=6.2974176

Batch 21750, train_perplexity=505.5317, train_loss=6.2256107

Batch 21760, train_perplexity=539.5633, train_loss=6.29076

Batch 21770, train_perplexity=537.4884, train_loss=6.286907

Batch 21780, train_perplexity=526.80347, train_loss=6.2668276

Batch 21790, train_perplexity=498.10962, train_loss=6.21082

Batch 21800, train_perplexity=528.0071, train_loss=6.2691097

Batch 21810, train_perplexity=545.57135, train_loss=6.3018336

Batch 21820, train_perplexity=579.93555, train_loss=6.362917

Batch 21830, train_perplexity=609.5254, train_loss=6.4126806

Batch 21840, train_perplexity=550.1695, train_loss=6.3102264

Batch 21850, train_perplexity=505.73782, train_loss=6.2260184

Batch 21860, train_perplexity=518.6426, train_loss=6.251215

Batch 21870, train_perplexity=512.6479, train_loss=6.239589

Batch 21880, train_perplexity=554.7675, train_loss=6.318549

Batch 21890, train_perplexity=564.25726, train_loss=6.3355103

Batch 21900, train_perplexity=531.2599, train_loss=6.2752514

Batch 21910, train_perplexity=532.24524, train_loss=6.2771044

Batch 21920, train_perplexity=587.93506, train_loss=6.3766165

Batch 21930, train_perplexity=573.10693, train_loss=6.3510723

Batch 21940, train_perplexity=540.9717, train_loss=6.293367

Batch 21950, train_perplexity=479.17264, train_loss=6.172061

Batch 21960, train_perplexity=510.21674, train_loss=6.2348356

Batch 21970, train_perplexity=566.1724, train_loss=6.3388987

Batch 21980, train_perplexity=515.6895, train_loss=6.245505

Batch 21990, train_perplexity=535.2515, train_loss=6.282737

Batch 22000, train_perplexity=522.0943, train_loss=6.2578483

Batch 22010, train_perplexity=595.871, train_loss=6.390024

Batch 22020, train_perplexity=495.3266, train_loss=6.2052174

Batch 22030, train_perplexity=596.402, train_loss=6.390915

Batch 22040, train_perplexity=489.0006, train_loss=6.1923637

Batch 22050, train_perplexity=571.85834, train_loss=6.3488913

Batch 22060, train_perplexity=542.01404, train_loss=6.295292

Batch 22070, train_perplexity=502.4697, train_loss=6.2195354

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Batch 22080, train_perplexity=518.8219, train_loss=6.2515607

Batch 22090, train_perplexity=562.7659, train_loss=6.332864

Batch 22100, train_perplexity=471.78674, train_loss=6.156527

Batch 22110, train_perplexity=528.39374, train_loss=6.2698417

Batch 22120, train_perplexity=522.96466, train_loss=6.259514

Batch 22130, train_perplexity=537.5866, train_loss=6.28709

Batch 22140, train_perplexity=602.3162, train_loss=6.4007826

Batch 22150, train_perplexity=547.2537, train_loss=6.3049126

Batch 22160, train_perplexity=562.0097, train_loss=6.331519

Batch 22170, train_perplexity=512.74664, train_loss=6.239782

Batch 22180, train_perplexity=507.8606, train_loss=6.230207

Batch 22190, train_perplexity=572.54755, train_loss=6.3500957

Batch 22200, train_perplexity=502.7072, train_loss=6.220008

Batch 22210, train_perplexity=554.03156, train_loss=6.3172216

Batch 22220, train_perplexity=518.6886, train_loss=6.2513037

Batch 22230, train_perplexity=492.02158, train_loss=6.1985226

Batch 22240, train_perplexity=478.72525, train_loss=6.171127

Batch 22250, train_perplexity=538.3369, train_loss=6.2884846

Batch 22260, train_perplexity=517.11304, train_loss=6.2482615

Batch 22270, train_perplexity=515.86017, train_loss=6.245836

Batch 22280, train_perplexity=539.07776, train_loss=6.28986

Batch 22290, train_perplexity=515.042, train_loss=6.2442484

Batch 22300, train_perplexity=527.915, train_loss=6.268935

Batch 22310, train_perplexity=526.4891, train_loss=6.2662306

Batch 22320, train_perplexity=606.53735, train_loss=6.4077663

Batch 22330, train_perplexity=556.2212, train_loss=6.321166

Batch 22340, train_perplexity=578.661, train_loss=6.360717

Batch 22350, train_perplexity=579.9444, train_loss=6.362932

Batch 22360, train_perplexity=522.8048, train_loss=6.259208

Batch 22370, train_perplexity=562.4, train_loss=6.3322134

Batch 22380, train_perplexity=509.79797, train_loss=6.2340145

Batch 22390, train_perplexity=513.2283, train_loss=6.2407207

Batch 22400, train_perplexity=562.48346, train_loss=6.3323617

Batch 22410, train_perplexity=474.69025, train_loss=6.1626625

Batch 22420, train_perplexity=548.80023, train_loss=6.3077345

Batch 22430, train_perplexity=539.1752, train_loss=6.2900405

Batch 22440, train_perplexity=553.6708, train_loss=6.3165703

Batch 22450, train_perplexity=537.9961, train_loss=6.2878513

Batch 22460, train_perplexity=598.0446, train_loss=6.3936653

Batch 22470, train_perplexity=504.83792, train_loss=6.2242374

Batch 22480, train_perplexity=516.75214, train_loss=6.2475634

Batch 22490, train_perplexity=535.8008, train_loss=6.2837625

Batch 22500, train_perplexity=547.09406, train_loss=6.3046207

Batch 22510, train_perplexity=538.2465, train_loss=6.2883167

Batch 22520, train_perplexity=535.74536, train_loss=6.283659

Batch 22530, train_perplexity=478.70084, train_loss=6.171076

Batch 22540, train_perplexity=515.1191, train_loss=6.244398

Batch 22550, train_perplexity=524.0633, train_loss=6.2616124

Batch 22560, train_perplexity=491.50662, train_loss=6.1974754

Batch 22570, train_perplexity=550.16034, train_loss=6.3102098

Batch 22580, train_perplexity=555.4054, train_loss=6.3196983

Batch 22590, train_perplexity=554.4983, train_loss=6.3180637

Batch 22600, train_perplexity=556.3199, train_loss=6.3213434

Batch 22610, train_perplexity=538.616, train_loss=6.289003

Batch 22620, train_perplexity=500.94244, train_loss=6.216491

Batch 22630, train_perplexity=626.88464, train_loss=6.4407625

Batch 22640, train_perplexity=511.69833, train_loss=6.2377353

Batch 22650, train_perplexity=545.6976, train_loss=6.302065

Batch 22660, train_perplexity=535.7648, train_loss=6.283695

Batch 22670, train_perplexity=595.64996, train_loss=6.389653

Batch 22680, train_perplexity=513.66504, train_loss=6.2415714

Batch 22690, train_perplexity=539.5365, train_loss=6.2907104

Batch 22700, train_perplexity=491.87167, train_loss=6.198218

Batch 22710, train_perplexity=538.0038, train_loss=6.2878656

Batch 22720, train_perplexity=572.1406, train_loss=6.349385

Batch 22730, train_perplexity=499.16986, train_loss=6.2129464

Batch 22740, train_perplexity=504.1605, train_loss=6.2228947

Batch 22750, train_perplexity=516.197, train_loss=6.2464886

Batch 22760, train_perplexity=578.24945, train_loss=6.3600054

Batch 22770, train_perplexity=561.09235, train_loss=6.3298855

Batch 22780, train_perplexity=571.46497, train_loss=6.348203

Batch 22790, train_perplexity=524.59656, train_loss=6.2626295

Batch 22800, train_perplexity=517.618, train_loss=6.2492375

Batch 22810, train_perplexity=544.1157, train_loss=6.299162

Batch 22820, train_perplexity=475.94498, train_loss=6.1653023

Batch 22830, train_perplexity=565.32074, train_loss=6.3373933

Batch 22840, train_perplexity=530.2615, train_loss=6.2733703

Batch 22850, train_perplexity=540.52844, train_loss=6.292547

Batch 22860, train_perplexity=488.19495, train_loss=6.190715

Batch 22870, train_perplexity=607.2678, train_loss=6.40897
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22880, train_perplexity=603.4529, train_loss=6.402668

Batch 22890, train_perplexity=564.4139, train_loss=6.335788

Batch 22900, train_perplexity=485.1466, train_loss=6.184451

Batch 22910, train_perplexity=541.0834, train_loss=6.2935734

Batch 22920, train_perplexity=465.58218, train_loss=6.1432886

Batch 22930, train_perplexity=552.8672, train_loss=6.315118

Batch 22940, train_perplexity=532.18994, train_loss=6.2770004

Batch 22950, train_perplexity=480.76373, train_loss=6.175376

Batch 22960, train_perplexity=474.03204, train_loss=6.161275

Batch 22970, train_perplexity=500.66187, train_loss=6.215931

Batch 22980, train_perplexity=507.59937, train_loss=6.2296925

Batch 22990, train_perplexity=503.35605, train_loss=6.2212977

Batch 23000, train_perplexity=523.25, train_loss=6.2600594

Batch 23010, train_perplexity=562.4703, train_loss=6.3323383

Batch 23020, train_perplexity=529.6247, train_loss=6.2721686

Batch 23030, train_perplexity=532.2224, train_loss=6.2770615

Batch 23040, train_perplexity=545.2991, train_loss=6.3013344

Batch 23050, train_perplexity=604.8668, train_loss=6.4050083

Batch 23060, train_perplexity=551.93555, train_loss=6.3134313

Batch 23070, train_perplexity=510.17343, train_loss=6.2347507

Batch 23080, train_perplexity=517.6484, train_loss=6.249296

Batch 23090, train_perplexity=501.17538, train_loss=6.216956

Batch 23100, train_perplexity=483.01605, train_loss=6.18005

Batch 23110, train_perplexity=525.27765, train_loss=6.263927

Batch 23120, train_perplexity=522.8684, train_loss=6.25933

Batch 23130, train_perplexity=517.998, train_loss=6.2499714

Batch 23140, train_perplexity=517.16876, train_loss=6.248369

Batch 23150, train_perplexity=543.01984, train_loss=6.297146

Batch 23160, train_perplexity=496.1329, train_loss=6.206844

Batch 23170, train_perplexity=495.81647, train_loss=6.206206

Batch 23180, train_perplexity=549.18085, train_loss=6.308428

Batch 23190, train_perplexity=530.9039, train_loss=6.274581

Batch 23200, train_perplexity=528.1496, train_loss=6.2693796

Batch 23210, train_perplexity=520.8259, train_loss=6.255416

Batch 23220, train_perplexity=503.69843, train_loss=6.2219777

Batch 23230, train_perplexity=571.30505, train_loss=6.3479233

Batch 23240, train_perplexity=540.3784, train_loss=6.2922697

Batch 23250, train_perplexity=551.9016, train_loss=6.3133698

Batch 23260, train_perplexity=516.49054, train_loss=6.247057

Batch 23270, train_perplexity=507.30948, train_loss=6.229121

Batch 23280, train_perplexity=525.4643, train_loss=6.264282

Batch 23290, train_perplexity=522.2146, train_loss=6.2580786

Batch 23300, train_perplexity=521.8255, train_loss=6.2573333

Batch 23310, train_perplexity=559.3122, train_loss=6.326708

Batch 23320, train_perplexity=486.09137, train_loss=6.1863966

Batch 23330, train_perplexity=525.81744, train_loss=6.264954

Batch 23340, train_perplexity=496.81946, train_loss=6.2082267

Batch 23350, train_perplexity=570.0092, train_loss=6.3456526

Batch 23360, train_perplexity=495.57184, train_loss=6.2057123

Batch 23370, train_perplexity=562.807, train_loss=6.332937

Batch 23380, train_perplexity=481.41843, train_loss=6.176737

Batch 23390, train_perplexity=531.0411, train_loss=6.2748394

Batch 23400, train_perplexity=508.7916, train_loss=6.2320385

Batch 23410, train_perplexity=538.177, train_loss=6.2881875

Batch 23420, train_perplexity=499.96024, train_loss=6.2145286

Batch 23430, train_perplexity=467.76828, train_loss=6.147973

Batch 23440, train_perplexity=506.12262, train_loss=6.226779

Batch 23450, train_perplexity=489.76462, train_loss=6.193925

Batch 23460, train_perplexity=437.92493, train_loss=6.0820475

Batch 23470, train_perplexity=538.64526, train_loss=6.2890573

Batch 23480, train_perplexity=496.92062, train_loss=6.2084303

Batch 23490, train_perplexity=558.5585, train_loss=6.3253593

Batch 23500, train_perplexity=486.53058, train_loss=6.1872997

Batch 23510, train_perplexity=458.62897, train_loss=6.1282415

Batch 23520, train_perplexity=538.60675, train_loss=6.2889857

Batch 23530, train_perplexity=506.2199, train_loss=6.226971

Batch 23540, train_perplexity=500.39383, train_loss=6.2153955

Batch 23550, train_perplexity=549.96936, train_loss=6.3098626

Batch 23560, train_perplexity=532.51434, train_loss=6.27761

Batch 23570, train_perplexity=489.7123, train_loss=6.193818

Batch 23580, train_perplexity=544.9388, train_loss=6.3006735

Batch 23590, train_perplexity=521.21, train_loss=6.256153

Batch 23600, train_perplexity=525.0665, train_loss=6.263525

Batch 23610, train_perplexity=483.96082, train_loss=6.182004

Batch 23620, train_perplexity=427.0422, train_loss=6.056883

Batch 23630, train_perplexity=530.33356, train_loss=6.273506

Batch 23640, train_perplexity=527.52545, train_loss=6.268197

Batch 23650, train_perplexity=551.2851, train_loss=6.312252

Batch 23660, train_perplexity=540.20996, train_loss=6.291958

Batch 23670, train_perplexity=511.78693, train_loss=6.2379084

Batch 23680, train_perplexity=541.2862, train_loss=6.293948

Batch 23690, train_perplexity=485.9127, train_loss=6.186029

Batch 23700, train_perplexity=598.1923, train_loss=6.3939123

Batch 23710, train_perplexity=486.829, train_loss=6.187913

Batch 23720, train_perplexity=545.43744, train_loss=6.301588

Batch 23730, train_perplexity=539.55145, train_loss=6.290738

Batch 23740, train_perplexity=560.17993, train_loss=6.328258

Batch 23750, train_perplexity=464.42474, train_loss=6.1407995

Batch 23760, train_perplexity=553.04175, train_loss=6.3154335

Batch 23770, train_perplexity=550.88824, train_loss=6.311532

Batch 23780, train_perplexity=521.7511, train_loss=6.2571907

Batch 23790, train_perplexity=535.95386, train_loss=6.284048

Batch 23800, train_perplexity=544.28955, train_loss=6.2994814

Batch 23810, train_perplexity=568.83954, train_loss=6.3435984

Batch 23820, train_perplexity=565.1237, train_loss=6.3370447

Batch 23830, train_perplexity=502.88223, train_loss=6.220356

Batch 23840, train_perplexity=516.8034, train_loss=6.2476625

Batch 23850, train_perplexity=531.6195, train_loss=6.275928

Batch 23860, train_perplexity=553.113, train_loss=6.3155622

Batch 23870, train_perplexity=481.07056, train_loss=6.176014

Batch 23880, train_perplexity=546.52515, train_loss=6.3035803

Batch 23890, train_perplexity=506.54562, train_loss=6.2276144

Batch 23900, train_perplexity=547.4581, train_loss=6.305286

Batch 23910, train_perplexity=492.06897, train_loss=6.198619

Batch 23920, train_perplexity=468.57864, train_loss=6.149704

Batch 23930, train_perplexity=502.9628, train_loss=6.220516

Batch 23940, train_perplexity=490.94797, train_loss=6.196338

Batch 23950, train_perplexity=556.1268, train_loss=6.3209963

Batch 23960, train_perplexity=577.68976, train_loss=6.359037

Batch 23970, train_perplexity=493.27835, train_loss=6.2010736

Batch 23980, train_perplexity=508.9731, train_loss=6.232395

Batch 23990, train_perplexity=490.03162, train_loss=6.19447

Batch 24000, train_perplexity=473.53503, train_loss=6.160226

Batch 24010, train_perplexity=516.197, train_loss=6.2464886

Batch 24020, train_perplexity=489.57642, train_loss=6.1935406

Batch 24030, train_perplexity=544.5185, train_loss=6.299902

Batch 24040, train_perplexity=527.65375, train_loss=6.2684402

Batch 24050, train_perplexity=498.37524, train_loss=6.2113533

Batch 24060, train_perplexity=482.14047, train_loss=6.1782355

Batch 24070, train_perplexity=537.6176, train_loss=6.2871475

Batch 24080, train_perplexity=485.16464, train_loss=6.1844883

Batch 24090, train_perplexity=498.0384, train_loss=6.210677

Batch 24100, train_perplexity=531.75995, train_loss=6.276192

Batch 24110, train_perplexity=488.28995, train_loss=6.1909094

Batch 24120, train_perplexity=489.64227, train_loss=6.193675

Batch 24130, train_perplexity=548.80206, train_loss=6.307738

Batch 24140, train_perplexity=480.3453, train_loss=6.174505

Batch 24150, train_perplexity=501.0502, train_loss=6.2167063

Batch 24160, train_perplexity=518.57904, train_loss=6.2510924

Batch 24170, train_perplexity=536.76715, train_loss=6.2855644

Batch 24180, train_perplexity=499.33554, train_loss=6.2132783

Batch 24190, train_perplexity=484.22168, train_loss=6.182543

Batch 24200, train_perplexity=501.5345, train_loss=6.2176723
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24210, train_perplexity=471.01642, train_loss=6.154893

Batch 24220, train_perplexity=538.57825, train_loss=6.288933

Batch 24230, train_perplexity=490.8705, train_loss=6.1961803

Batch 24240, train_perplexity=483.75458, train_loss=6.1815777

Batch 24250, train_perplexity=522.81055, train_loss=6.259219

Batch 24260, train_perplexity=485.55347, train_loss=6.1852894

Batch 24270, train_perplexity=443.049, train_loss=6.0936804

Batch 24280, train_perplexity=567.73395, train_loss=6.341653

Batch 24290, train_perplexity=463.83426, train_loss=6.1395273

Batch 24300, train_perplexity=514.8711, train_loss=6.2439165

Batch 24310, train_perplexity=501.3604, train_loss=6.217325

Batch 24320, train_perplexity=433.92438, train_loss=6.0728703

Batch 24330, train_perplexity=537.0631, train_loss=6.2861156

Batch 24340, train_perplexity=512.2806, train_loss=6.2388725

Batch 24350, train_perplexity=505.17505, train_loss=6.224905

Batch 24360, train_perplexity=507.78577, train_loss=6.2300596

Batch 24370, train_perplexity=518.99536, train_loss=6.251895

Batch 24380, train_perplexity=513.10864, train_loss=6.2404876

Batch 24390, train_perplexity=508.23535, train_loss=6.2309446

Batch 24400, train_perplexity=525.535, train_loss=6.2644167

Batch 24410, train_perplexity=481.9975, train_loss=6.177939

Batch 24420, train_perplexity=503.8106, train_loss=6.2222004

Batch 24430, train_perplexity=503.42444, train_loss=6.2214336

Batch 24440, train_perplexity=482.88687, train_loss=6.1797824

Batch 24450, train_perplexity=503.15662, train_loss=6.2209015

Batch 24460, train_perplexity=501.7043, train_loss=6.218011

Batch 24470, train_perplexity=518.7225, train_loss=6.251369

Batch 24480, train_perplexity=587.89526, train_loss=6.376549

Batch 24490, train_perplexity=510.81973, train_loss=6.2360168

Batch 24500, train_perplexity=486.1338, train_loss=6.186484

Batch 24510, train_perplexity=538.99084, train_loss=6.2896986

Batch 24520, train_perplexity=499.7169, train_loss=6.2140417

Batch 24530, train_perplexity=483.0863, train_loss=6.1801953

Batch 24540, train_perplexity=496.5033, train_loss=6.20759

Batch 24550, train_perplexity=543.27106, train_loss=6.2976084

Batch 24560, train_perplexity=539.3194, train_loss=6.290308

Batch 24570, train_perplexity=569.97174, train_loss=6.345587

Batch 24580, train_perplexity=485.7063, train_loss=6.185604

Batch 24590, train_perplexity=477.52878, train_loss=6.1686244

Batch 24600, train_perplexity=457.60226, train_loss=6.1260004

Batch 24610, train_perplexity=520.1484, train_loss=6.254114

Batch 24620, train_perplexity=571.0934, train_loss=6.347553

Batch 24630, train_perplexity=472.8678, train_loss=6.158816

Batch 24640, train_perplexity=493.6932, train_loss=6.2019143

Batch 24650, train_perplexity=509.03207, train_loss=6.232511

Batch 24660, train_perplexity=585.32574, train_loss=6.3721685

Batch 24670, train_perplexity=495.19626, train_loss=6.204954

Batch 24680, train_perplexity=472.37448, train_loss=6.157772

Batch 24690, train_perplexity=505.5828, train_loss=6.225712

Batch 24700, train_perplexity=535.0663, train_loss=6.2823906

Batch 24710, train_perplexity=516.6691, train_loss=6.2474027

Batch 24720, train_perplexity=495.0088, train_loss=6.2045755

Batch 24730, train_perplexity=574.06335, train_loss=6.35274

Batch 24740, train_perplexity=502.26706, train_loss=6.219132

Batch 24750, train_perplexity=466.19354, train_loss=6.144601

Batch 24760, train_perplexity=519.6756, train_loss=6.253205

Batch 24770, train_perplexity=499.57275, train_loss=6.213753

Batch 24780, train_perplexity=618.04504, train_loss=6.4265614

Batch 24790, train_perplexity=582.49664, train_loss=6.3673234

Batch 24800, train_perplexity=502.11307, train_loss=6.2188253

Batch 24810, train_perplexity=523.0878, train_loss=6.2597494

Batch 24820, train_perplexity=481.14948, train_loss=6.176178

Batch 24830, train_perplexity=481.739, train_loss=6.1774025

Batch 24840, train_perplexity=500.6984, train_loss=6.216004

Batch 24850, train_perplexity=485.2602, train_loss=6.184685

Batch 24860, train_perplexity=485.03003, train_loss=6.184211

Batch 24870, train_perplexity=457.71335, train_loss=6.126243

Batch 24880, train_perplexity=497.12823, train_loss=6.208848

Batch 24890, train_perplexity=508.87265, train_loss=6.2321978

Batch 24900, train_perplexity=499.25244, train_loss=6.213112

Batch 24910, train_perplexity=531.44666, train_loss=6.275603

Batch 24920, train_perplexity=530.3887, train_loss=6.27361

Batch 24930, train_perplexity=486.21817, train_loss=6.1866574

Batch 24940, train_perplexity=584.93567, train_loss=6.371502

Batch 24950, train_perplexity=561.23065, train_loss=6.330132

Batch 24960, train_perplexity=499.18295, train_loss=6.2129726

Batch 24970, train_perplexity=515.40704, train_loss=6.244957

Batch 24980, train_perplexity=440.1633, train_loss=6.087146

Batch 24990, train_perplexity=475.62375, train_loss=6.164627

Batch 25000, train_perplexity=474.3703, train_loss=6.1619883

Batch 25010, train_perplexity=505.15964, train_loss=6.2248745

Batch 25020, train_perplexity=534.60565, train_loss=6.2815294

Batch 25030, train_perplexity=566.50024, train_loss=6.3394775

Batch 25040, train_perplexity=520.97424, train_loss=6.2557006

Batch 25050, train_perplexity=512.00854, train_loss=6.2383413

Batch 25060, train_perplexity=520.0239, train_loss=6.253875

Batch 25070, train_perplexity=473.2885, train_loss=6.159705

Batch 25080, train_perplexity=527.12714, train_loss=6.2674417

Batch 25090, train_perplexity=515.23895, train_loss=6.244631

Batch 25100, train_perplexity=514.06836, train_loss=6.2423563

Batch 25110, train_perplexity=526.3041, train_loss=6.265879

Batch 25120, train_perplexity=534.3335, train_loss=6.28102

Batch 25130, train_perplexity=520.6611, train_loss=6.2550993

Batch 25140, train_perplexity=479.1763, train_loss=6.1720686

Batch 25150, train_perplexity=451.51688, train_loss=6.1126127

Batch 25160, train_perplexity=453.00922, train_loss=6.1159124

Batch 25170, train_perplexity=524.429, train_loss=6.26231

Batch 25180, train_perplexity=455.58334, train_loss=6.1215787

Batch 25190, train_perplexity=508.73822, train_loss=6.2319336

Batch 25200, train_perplexity=482.64975, train_loss=6.1792912

Batch 25210, train_perplexity=479.45056, train_loss=6.172641

Batch 25220, train_perplexity=487.6575, train_loss=6.1896133

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Batch 25230, train_perplexity=467.64072, train_loss=6.1477003

Batch 25240, train_perplexity=471.37814, train_loss=6.1556606

Batch 25250, train_perplexity=537.96484, train_loss=6.287793

Batch 25260, train_perplexity=484.10855, train_loss=6.182309

Batch 25270, train_perplexity=511.49854, train_loss=6.2373447

Batch 25280, train_perplexity=452.47296, train_loss=6.114728

Batch 25290, train_perplexity=495.75266, train_loss=6.206077

Batch 25300, train_perplexity=501.66888, train_loss=6.2179403

Batch 25310, train_perplexity=478.89352, train_loss=6.1714783

Batch 25320, train_perplexity=513.48926, train_loss=6.241229

Batch 25330, train_perplexity=454.52078, train_loss=6.1192436

Batch 25340, train_perplexity=514.3141, train_loss=6.242834

Batch 25350, train_perplexity=492.80862, train_loss=6.200121

Batch 25360, train_perplexity=507.12784, train_loss=6.228763

Batch 25370, train_perplexity=553.87177, train_loss=6.316933

Batch 25380, train_perplexity=566.1368, train_loss=6.3388357

Batch 25390, train_perplexity=559.4813, train_loss=6.32701

Batch 25400, train_perplexity=504.62756, train_loss=6.2238207

Batch 25410, train_perplexity=522.01666, train_loss=6.2576995

Batch 25420, train_perplexity=481.2369, train_loss=6.1763597

Batch 25430, train_perplexity=501.0005, train_loss=6.216607

Batch 25440, train_perplexity=505.94455, train_loss=6.226427

Batch 25450, train_perplexity=504.5583, train_loss=6.2236834

Batch 25460, train_perplexity=493.5953, train_loss=6.201716
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 25470, train_perplexity=506.50385, train_loss=6.227532

Batch 25480, train_perplexity=451.37653, train_loss=6.112302

Batch 25490, train_perplexity=495.1854, train_loss=6.204932

Batch 25500, train_perplexity=492.33606, train_loss=6.1991615

Batch 25510, train_perplexity=534.79535, train_loss=6.281884

Batch 25520, train_perplexity=486.77005, train_loss=6.187792

Batch 25530, train_perplexity=538.2979, train_loss=6.288412

Batch 25540, train_perplexity=494.1843, train_loss=6.2029085

Batch 25550, train_perplexity=448.2202, train_loss=6.1052847

Batch 25560, train_perplexity=524.467, train_loss=6.2623825

Batch 25570, train_perplexity=522.02484, train_loss=6.257715

Batch 25580, train_perplexity=540.5171, train_loss=6.2925262

Batch 25590, train_perplexity=467.75626, train_loss=6.1479473

Batch 25600, train_perplexity=463.7018, train_loss=6.1392417

Batch 25610, train_perplexity=462.49942, train_loss=6.1366453

Batch 25620, train_perplexity=526.1663, train_loss=6.2656174

Batch 25630, train_perplexity=492.51968, train_loss=6.1995344

Batch 25640, train_perplexity=515.0159, train_loss=6.244198

Batch 25650, train_perplexity=463.54727, train_loss=6.1389084

Batch 25660, train_perplexity=527.41626, train_loss=6.26799

Batch 25670, train_perplexity=470.92075, train_loss=6.15469

Batch 25680, train_perplexity=491.68948, train_loss=6.1978474

Batch 25690, train_perplexity=540.2429, train_loss=6.292019

Batch 25700, train_perplexity=540.1051, train_loss=6.291764

Batch 25710, train_perplexity=469.59818, train_loss=6.1518774

Batch 25720, train_perplexity=546.0703, train_loss=6.3027477

Batch 25730, train_perplexity=488.21732, train_loss=6.1907606

Batch 25740, train_perplexity=508.78458, train_loss=6.2320247

Batch 25750, train_perplexity=474.68936, train_loss=6.1626606

Batch 25760, train_perplexity=496.9744, train_loss=6.2085385

Batch 25770, train_perplexity=485.5294, train_loss=6.18524

Batch 25780, train_perplexity=510.30652, train_loss=6.2350116

Batch 25790, train_perplexity=469.55206, train_loss=6.151779

Batch 25800, train_perplexity=506.95352, train_loss=6.2284193

Batch 25810, train_perplexity=483.2773, train_loss=6.1805906

Batch 25820, train_perplexity=499.36862, train_loss=6.2133446

Batch 25830, train_perplexity=477.66653, train_loss=6.168913

Batch 25840, train_perplexity=480.9671, train_loss=6.175799

Batch 25850, train_perplexity=457.79303, train_loss=6.126417

Batch 25860, train_perplexity=541.3301, train_loss=6.294029

Batch 25870, train_perplexity=490.59906, train_loss=6.195627

Batch 25880, train_perplexity=489.02393, train_loss=6.1924114

Batch 25890, train_perplexity=560.2967, train_loss=6.3284664

Batch 25900, train_perplexity=514.18677, train_loss=6.2425866

Batch 25910, train_perplexity=569.05383, train_loss=6.343975

Batch 25920, train_perplexity=513.8929, train_loss=6.242015

Batch 25930, train_perplexity=489.63666, train_loss=6.1936636

Batch 25940, train_perplexity=482.65598, train_loss=6.179304

Batch 25950, train_perplexity=536.32965, train_loss=6.284749

Batch 25960, train_perplexity=496.24292, train_loss=6.2070656

Batch 25970, train_perplexity=473.53864, train_loss=6.1602335

Batch 25980, train_perplexity=437.1934, train_loss=6.0803757

Batch 25990, train_perplexity=492.65967, train_loss=6.1998186

Batch 26000, train_perplexity=497.1358, train_loss=6.2088633

Batch 26010, train_perplexity=522.4716, train_loss=6.2585707

Batch 26020, train_perplexity=530.06506, train_loss=6.273

Batch 26030, train_perplexity=409.75336, train_loss=6.0155554

Batch 26040, train_perplexity=491.67096, train_loss=6.1978097

Batch 26050, train_perplexity=499.73572, train_loss=6.2140794

Batch 26060, train_perplexity=492.93835, train_loss=6.200384

Batch 26070, train_perplexity=495.93872, train_loss=6.2064524

Batch 26080, train_perplexity=519.95447, train_loss=6.2537413

Batch 26090, train_perplexity=484.118, train_loss=6.1823287

Batch 26100, train_perplexity=504.2206, train_loss=6.223014

Batch 26110, train_perplexity=467.80954, train_loss=6.1480613

Batch 26120, train_perplexity=475.2406, train_loss=6.163821

Batch 26130, train_perplexity=491.28683, train_loss=6.197028

Batch 26140, train_perplexity=560.4757, train_loss=6.328786

Batch 26150, train_perplexity=518.1981, train_loss=6.2503576

Batch 26160, train_perplexity=461.38135, train_loss=6.134225

Batch 26170, train_perplexity=516.0425, train_loss=6.246189

Batch 26180, train_perplexity=472.65073, train_loss=6.1583567

Batch 26190, train_perplexity=493.1998, train_loss=6.2009144

Batch 26200, train_perplexity=474.5047, train_loss=6.1622715

Batch 26210, train_perplexity=456.05002, train_loss=6.1226025

Batch 26220, train_perplexity=542.9463, train_loss=6.2970104

Batch 26230, train_perplexity=470.96655, train_loss=6.154787

Batch 26240, train_perplexity=415.71216, train_loss=6.029993

Batch 26250, train_perplexity=486.15488, train_loss=6.1865273

Batch 26260, train_perplexity=475.724, train_loss=6.164838

Batch 26270, train_perplexity=450.13266, train_loss=6.1095424

Batch 26280, train_perplexity=448.81992, train_loss=6.1066217

Batch 26290, train_perplexity=565.5313, train_loss=6.3377657

Batch 26300, train_perplexity=486.7199, train_loss=6.187689

Batch 26310, train_perplexity=522.09656, train_loss=6.2578526

Batch 26320, train_perplexity=529.7603, train_loss=6.2724247

Batch 26330, train_perplexity=444.87637, train_loss=6.0977964

Batch 26340, train_perplexity=464.09268, train_loss=6.1400843

Batch 26350, train_perplexity=454.05505, train_loss=6.1182184

Batch 26360, train_perplexity=475.4054, train_loss=6.164168

Batch 26370, train_perplexity=464.3862, train_loss=6.1407166

Batch 26380, train_perplexity=483.50043, train_loss=6.181052

Batch 26390, train_perplexity=516.7019, train_loss=6.247466

Batch 26400, train_perplexity=484.7849, train_loss=6.1837053

Batch 26410, train_perplexity=546.1935, train_loss=6.3029733

Batch 26420, train_perplexity=523.16016, train_loss=6.2598877

Batch 26430, train_perplexity=453.96887, train_loss=6.1180286

Batch 26440, train_perplexity=486.17413, train_loss=6.186567

Batch 26450, train_perplexity=514.7348, train_loss=6.243652

Batch 26460, train_perplexity=551.91003, train_loss=6.313385

Batch 26470, train_perplexity=459.761, train_loss=6.130707

Batch 26480, train_perplexity=528.37506, train_loss=6.2698064

Batch 26490, train_perplexity=530.0019, train_loss=6.2728806

Batch 26500, train_perplexity=510.10922, train_loss=6.234625

Batch 26510, train_perplexity=507.64536, train_loss=6.229783

Batch 26520, train_perplexity=502.60126, train_loss=6.219797

Batch 26530, train_perplexity=460.9332, train_loss=6.133253

Batch 26540, train_perplexity=491.60016, train_loss=6.1976657

Batch 26550, train_perplexity=440.35202, train_loss=6.0875745

Batch 26560, train_perplexity=534.83746, train_loss=6.281963

Batch 26570, train_perplexity=409.07358, train_loss=6.013895

Batch 26580, train_perplexity=463.1805, train_loss=6.138117

Batch 26590, train_perplexity=462.08102, train_loss=6.1357403

Batch 26600, train_perplexity=501.97733, train_loss=6.218555

Batch 26610, train_perplexity=467.27426, train_loss=6.1469164

Batch 26620, train_perplexity=497.37103, train_loss=6.2093363

Batch 26630, train_perplexity=450.67368, train_loss=6.1107435

Batch 26640, train_perplexity=490.5565, train_loss=6.1955404

Batch 26650, train_perplexity=458.5131, train_loss=6.127989

Batch 26660, train_perplexity=472.84457, train_loss=6.1587667

Batch 26670, train_perplexity=475.07928, train_loss=6.1634817

Batch 26680, train_perplexity=506.01813, train_loss=6.2265725

Batch 26690, train_perplexity=470.6343, train_loss=6.1540813

Batch 26700, train_perplexity=507.3351, train_loss=6.2291718

Batch 26710, train_perplexity=487.80542, train_loss=6.1899166

Batch 26720, train_perplexity=504.85043, train_loss=6.224262

Batch 26730, train_perplexity=481.17725, train_loss=6.1762357

Batch 26740, train_perplexity=513.67194, train_loss=6.241585

Batch 26750, train_perplexity=441.43115, train_loss=6.090022

Batch 26760, train_perplexity=426.2545, train_loss=6.0550365

Batch 26770, train_perplexity=477.52808, train_loss=6.168623

Batch 26780, train_perplexity=510.7167, train_loss=6.235815
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 26790, train_perplexity=479.1626, train_loss=6.17204

Batch 26800, train_perplexity=591.9194, train_loss=6.3833704

Batch 26810, train_perplexity=496.85214, train_loss=6.2082925

Batch 26820, train_perplexity=467.0025, train_loss=6.1463346

Batch 26830, train_perplexity=497.6797, train_loss=6.2099566

Batch 26840, train_perplexity=521.5024, train_loss=6.256714

Batch 26850, train_perplexity=505.48203, train_loss=6.2255125

Batch 26860, train_perplexity=514.2768, train_loss=6.2427616

Batch 26870, train_perplexity=455.4908, train_loss=6.1213756

Batch 26880, train_perplexity=547.7148, train_loss=6.3057547

Batch 26890, train_perplexity=499.55917, train_loss=6.213726

Batch 26900, train_perplexity=525.7247, train_loss=6.2647777

Batch 26910, train_perplexity=515.03705, train_loss=6.244239

Batch 26920, train_perplexity=439.58984, train_loss=6.085842

Batch 26930, train_perplexity=538.00946, train_loss=6.287876

Batch 26940, train_perplexity=510.33743, train_loss=6.235072

Batch 26950, train_perplexity=516.1276, train_loss=6.246354

Batch 26960, train_perplexity=504.24274, train_loss=6.2230577

Batch 26970, train_perplexity=468.35126, train_loss=6.1492186

Batch 26980, train_perplexity=515.57715, train_loss=6.245287

Batch 26990, train_perplexity=472.9163, train_loss=6.1589184

Batch 27000, train_perplexity=475.93002, train_loss=6.165271

Batch 27010, train_perplexity=516.1161, train_loss=6.2463317

Batch 27020, train_perplexity=459.33456, train_loss=6.129779

Batch 27030, train_perplexity=512.06665, train_loss=6.238455

Batch 27040, train_perplexity=434.7443, train_loss=6.074758

Batch 27050, train_perplexity=466.1262, train_loss=6.1444564

Batch 27060, train_perplexity=483.79172, train_loss=6.1816545

Batch 27070, train_perplexity=471.13028, train_loss=6.1551347

Batch 27080, train_perplexity=482.0122, train_loss=6.1779695

Batch 27090, train_perplexity=482.94742, train_loss=6.179908

Batch 27100, train_perplexity=492.39545, train_loss=6.199282

Batch 27110, train_perplexity=500.12857, train_loss=6.214865

Batch 27120, train_perplexity=453.67715, train_loss=6.117386

Batch 27130, train_perplexity=509.7321, train_loss=6.2338853

Batch 27140, train_perplexity=496.54614, train_loss=6.2076764

Batch 27150, train_perplexity=498.4204, train_loss=6.211444

Batch 27160, train_perplexity=462.00217, train_loss=6.1355696

Batch 27170, train_perplexity=500.03247, train_loss=6.214673

Batch 27180, train_perplexity=441.40463, train_loss=6.089962

Batch 27190, train_perplexity=497.68417, train_loss=6.2099657

Batch 27200, train_perplexity=495.77982, train_loss=6.206132

Batch 27210, train_perplexity=509.59186, train_loss=6.23361

Batch 27220, train_perplexity=524.41296, train_loss=6.2622795

Batch 27230, train_perplexity=421.9127, train_loss=6.0447984

Batch 27240, train_perplexity=519.1434, train_loss=6.25218

Batch 27250, train_perplexity=487.49384, train_loss=6.1892776

Batch 27260, train_perplexity=532.05164, train_loss=6.2767406

Batch 27270, train_perplexity=501.3542, train_loss=6.217313

Batch 27280, train_perplexity=476.52905, train_loss=6.1665287

Batch 27290, train_perplexity=478.53375, train_loss=6.170727

Batch 27300, train_perplexity=420.46222, train_loss=6.0413547

Batch 27310, train_perplexity=450.9467, train_loss=6.111349

Batch 27320, train_perplexity=560.5137, train_loss=6.3288536

Batch 27330, train_perplexity=498.0764, train_loss=6.2107534

Batch 27340, train_perplexity=487.8971, train_loss=6.1901045

Batch 27350, train_perplexity=513.14606, train_loss=6.2405605

Batch 27360, train_perplexity=482.68152, train_loss=6.179357

Batch 27370, train_perplexity=422.6845, train_loss=6.046626

Batch 27380, train_perplexity=428.6954, train_loss=6.0607467

Batch 27390, train_perplexity=468.70917, train_loss=6.1499825

Batch 27400, train_perplexity=446.2778, train_loss=6.1009417

Batch 27410, train_perplexity=518.652, train_loss=6.251233

Batch 27420, train_perplexity=533.208, train_loss=6.2789116

Batch 27430, train_perplexity=474.302, train_loss=6.1618443

Batch 27440, train_perplexity=481.81897, train_loss=6.1775684

Batch 27450, train_perplexity=487.0236, train_loss=6.1883125

Batch 27460, train_perplexity=460.75168, train_loss=6.132859

Batch 27470, train_perplexity=470.07806, train_loss=6.152899

Batch 27480, train_perplexity=533.30414, train_loss=6.279092

Batch 27490, train_perplexity=525.5505, train_loss=6.2644463

Batch 27500, train_perplexity=481.70868, train_loss=6.1773396

Batch 27510, train_perplexity=457.6529, train_loss=6.126111

Batch 27520, train_perplexity=509.36398, train_loss=6.233163

Batch 27530, train_perplexity=431.5946, train_loss=6.067487

Batch 27540, train_perplexity=478.32935, train_loss=6.1702995

Batch 27550, train_perplexity=457.8179, train_loss=6.1264715

Batch 27560, train_perplexity=502.8664, train_loss=6.2203245

Batch 27570, train_perplexity=480.57968, train_loss=6.174993

Batch 27580, train_perplexity=463.02814, train_loss=6.137788

Batch 27590, train_perplexity=470.892, train_loss=6.1546288

Batch 27600, train_perplexity=471.8459, train_loss=6.1566525

Batch 27610, train_perplexity=473.81555, train_loss=6.160818

Batch 27620, train_perplexity=437.2818, train_loss=6.080578

Batch 27630, train_perplexity=457.08716, train_loss=6.124874

Batch 27640, train_perplexity=497.4341, train_loss=6.209463

Batch 27650, train_perplexity=477.76404, train_loss=6.169117

Batch 27660, train_perplexity=483.12018, train_loss=6.1802654

Batch 27670, train_perplexity=472.662, train_loss=6.1583805

Batch 27680, train_perplexity=498.39474, train_loss=6.2113924

Batch 27690, train_perplexity=503.2958, train_loss=6.221178

Batch 27700, train_perplexity=483.9636, train_loss=6.1820097

Batch 27710, train_perplexity=459.1563, train_loss=6.1293907

Batch 27720, train_perplexity=513.16565, train_loss=6.2405987

Batch 27730, train_perplexity=424.33563, train_loss=6.0505247

Batch 27740, train_perplexity=446.17313, train_loss=6.100707

Batch 27750, train_perplexity=519.8181, train_loss=6.253479

Batch 27760, train_perplexity=458.59244, train_loss=6.128162

Batch 27770, train_perplexity=496.4299, train_loss=6.2074423

Batch 27780, train_perplexity=504.6146, train_loss=6.223795

Batch 27790, train_perplexity=490.89743, train_loss=6.196235

Batch 27800, train_perplexity=520.3657, train_loss=6.254532

Batch 27810, train_perplexity=426.731, train_loss=6.056154

Batch 27820, train_perplexity=432.2446, train_loss=6.0689917

Batch 27830, train_perplexity=450.8366, train_loss=6.111105

Batch 27840, train_perplexity=497.37576, train_loss=6.209346

Batch 27850, train_perplexity=496.84668, train_loss=6.2082815

Batch 27860, train_perplexity=468.4243, train_loss=6.1493745

Batch 27870, train_perplexity=451.91837, train_loss=6.1135015

Batch 27880, train_perplexity=532.29425, train_loss=6.2771964

Batch 27890, train_perplexity=510.15836, train_loss=6.234721

Batch 27900, train_perplexity=476.2399, train_loss=6.1659217

Batch 27910, train_perplexity=461.3939, train_loss=6.134252

Batch 27920, train_perplexity=451.07507, train_loss=6.111634

Batch 27930, train_perplexity=437.8276, train_loss=6.0818253

Batch 27940, train_perplexity=476.22058, train_loss=6.165881

Batch 27950, train_perplexity=500.65137, train_loss=6.21591

Batch 27960, train_perplexity=463.19177, train_loss=6.138141

Batch 27970, train_perplexity=514.9857, train_loss=6.244139

Batch 27980, train_perplexity=519.61566, train_loss=6.2530894

Batch 27990, train_perplexity=436.4109, train_loss=6.078584

Batch 28000, train_perplexity=521.6484, train_loss=6.256994

Batch 28010, train_perplexity=488.60718, train_loss=6.191559

Batch 28020, train_perplexity=430.61508, train_loss=6.0652146

Batch 28030, train_perplexity=518.33203, train_loss=6.250616

Batch 28040, train_perplexity=479.32574, train_loss=6.1723804

Batch 28050, train_perplexity=486.246, train_loss=6.1867146

Batch 28060, train_perplexity=473.95383, train_loss=6.16111

Batch 28070, train_perplexity=490.79514, train_loss=6.196027

Batch 28080, train_perplexity=480.15707, train_loss=6.1741133

Batch 28090, train_perplexity=492.5934, train_loss=6.199684

Batch 28100, train_perplexity=514.3734, train_loss=6.2429495

Batch 28110, train_perplexity=520.03033, train_loss=6.253887
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28120, train_perplexity=539.93054, train_loss=6.2914405

Batch 28130, train_perplexity=520.50073, train_loss=6.2547913

Batch 28140, train_perplexity=426.38498, train_loss=6.0553427

Batch 28150, train_perplexity=453.11722, train_loss=6.116151

Batch 28160, train_perplexity=488.32535, train_loss=6.190982

Batch 28170, train_perplexity=492.5737, train_loss=6.199644

Batch 28180, train_perplexity=505.93008, train_loss=6.2263985

Batch 28190, train_perplexity=469.25482, train_loss=6.151146

Batch 28200, train_perplexity=461.4139, train_loss=6.1342955

Batch 28210, train_perplexity=490.88126, train_loss=6.1962023

Batch 28220, train_perplexity=508.71204, train_loss=6.231882

Batch 28230, train_perplexity=488.6272, train_loss=6.1916

Batch 28240, train_perplexity=527.1211, train_loss=6.2674303

Batch 28250, train_perplexity=447.3753, train_loss=6.103398

Batch 28260, train_perplexity=472.84143, train_loss=6.15876

Batch 28270, train_perplexity=519.8102, train_loss=6.2534637

Batch 28280, train_perplexity=525.59607, train_loss=6.264533

Batch 28290, train_perplexity=525.915, train_loss=6.2651396

Batch 28300, train_perplexity=461.0752, train_loss=6.133561

Batch 28310, train_perplexity=532.51636, train_loss=6.2776136

Batch 28320, train_perplexity=450.09125, train_loss=6.1094503

Batch 28330, train_perplexity=494.67703, train_loss=6.203905

Batch 28340, train_perplexity=455.69916, train_loss=6.121833

Batch 28350, train_perplexity=460.5408, train_loss=6.1324015

Batch 28360, train_perplexity=449.33813, train_loss=6.1077757

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Batch 28370, train_perplexity=557.99304, train_loss=6.3243465

Batch 28380, train_perplexity=469.84076, train_loss=6.152394

Batch 28390, train_perplexity=488.81317, train_loss=6.1919804

Batch 28400, train_perplexity=456.5892, train_loss=6.123784

Batch 28410, train_perplexity=407.781, train_loss=6.0107303

Batch 28420, train_perplexity=528.56433, train_loss=6.2701645

Batch 28430, train_perplexity=493.86133, train_loss=6.202255

Batch 28440, train_perplexity=487.52243, train_loss=6.1893363

Batch 28450, train_perplexity=516.1365, train_loss=6.2463713

Batch 28460, train_perplexity=474.326, train_loss=6.161895

Batch 28470, train_perplexity=430.30536, train_loss=6.064495

Batch 28480, train_perplexity=493.8187, train_loss=6.2021685

Batch 28490, train_perplexity=506.6645, train_loss=6.227849

Batch 28500, train_perplexity=496.66003, train_loss=6.207906

Batch 28510, train_perplexity=481.98486, train_loss=6.1779127

Batch 28520, train_perplexity=475.145, train_loss=6.16362

Batch 28530, train_perplexity=498.20132, train_loss=6.2110043

Batch 28540, train_perplexity=412.75684, train_loss=6.0228586

Batch 28550, train_perplexity=451.53625, train_loss=6.1126556

Batch 28560, train_perplexity=477.8317, train_loss=6.1692586

Batch 28570, train_perplexity=503.32004, train_loss=6.221226

Batch 28580, train_perplexity=457.702, train_loss=6.1262183

Batch 28590, train_perplexity=493.52634, train_loss=6.201576

Batch 28600, train_perplexity=492.97925, train_loss=6.200467

Batch 28610, train_perplexity=472.9226, train_loss=6.1589317

Batch 28620, train_perplexity=434.19177, train_loss=6.0734863

Batch 28630, train_perplexity=484.28217, train_loss=6.1826677

Batch 28640, train_perplexity=444.33533, train_loss=6.0965796

Batch 28650, train_perplexity=485.72018, train_loss=6.1856327

Batch 28660, train_perplexity=429.96262, train_loss=6.0636983

Batch 28670, train_perplexity=473.22983, train_loss=6.159581

Batch 28680, train_perplexity=463.49643, train_loss=6.1387987

Batch 28690, train_perplexity=471.4112, train_loss=6.1557307

Batch 28700, train_perplexity=489.48538, train_loss=6.1933546

Batch 28710, train_perplexity=478.3134, train_loss=6.170266

Batch 28720, train_perplexity=493.26285, train_loss=6.201042

Batch 28730, train_perplexity=447.9493, train_loss=6.10468

Batch 28740, train_perplexity=496.13928, train_loss=6.2068567

Batch 28750, train_perplexity=417.70312, train_loss=6.034771

Batch 28760, train_perplexity=486.0654, train_loss=6.186343

Batch 28770, train_perplexity=455.60226, train_loss=6.12162

Batch 28780, train_perplexity=414.43002, train_loss=6.026904

Batch 28790, train_perplexity=393.61725, train_loss=5.975379

Batch 28800, train_perplexity=508.88672, train_loss=6.2322254

Batch 28810, train_perplexity=433.83395, train_loss=6.072662

Batch 28820, train_perplexity=454.45468, train_loss=6.119098

Batch 28830, train_perplexity=448.55634, train_loss=6.1060343

Batch 28840, train_perplexity=431.2601, train_loss=6.0667114

Batch 28850, train_perplexity=421.74573, train_loss=6.0444026

Batch 28860, train_perplexity=436.9883, train_loss=6.0799065

Batch 28870, train_perplexity=521.8972, train_loss=6.2574706

Batch 28880, train_perplexity=459.10223, train_loss=6.129273

Batch 28890, train_perplexity=484.0991, train_loss=6.1822896

Batch 28900, train_perplexity=433.38385, train_loss=6.071624

Batch 28910, train_perplexity=439.8981, train_loss=6.086543

Batch 28920, train_perplexity=467.94498, train_loss=6.1483507

Batch 28930, train_perplexity=458.73047, train_loss=6.128463

Batch 28940, train_perplexity=500.2106, train_loss=6.2150292

Batch 28950, train_perplexity=424.69046, train_loss=6.0513606

Batch 28960, train_perplexity=456.1605, train_loss=6.1228447

Batch 28970, train_perplexity=476.92734, train_loss=6.167364

Batch 28980, train_perplexity=404.16046, train_loss=6.001812

Batch 28990, train_perplexity=446.97058, train_loss=6.102493

Batch 29000, train_perplexity=464.94812, train_loss=6.141926

Batch 29010, train_perplexity=525.39966, train_loss=6.264159

Batch 29020, train_perplexity=462.79922, train_loss=6.1372933

Batch 29030, train_perplexity=551.08374, train_loss=6.311887

Batch 29040, train_perplexity=487.9918, train_loss=6.1902986

Batch 29050, train_perplexity=462.4355, train_loss=6.136507

Batch 29060, train_perplexity=361.55002, train_loss=5.8904004

Batch 29070, train_perplexity=491.59756, train_loss=6.1976604

Batch 29080, train_perplexity=515.92194, train_loss=6.2459555

Batch 29090, train_perplexity=427.01575, train_loss=6.056821

Batch 29100, train_perplexity=447.83356, train_loss=6.1044216

Batch 29110, train_perplexity=427.83936, train_loss=6.058748

Batch 29120, train_perplexity=447.60297, train_loss=6.1039066

Batch 29130, train_perplexity=471.62726, train_loss=6.156189

Batch 29140, train_perplexity=478.1442, train_loss=6.1699123

Batch 29150, train_perplexity=465.82333, train_loss=6.1438065

Batch 29160, train_perplexity=412.05597, train_loss=6.021159

Batch 29170, train_perplexity=420.8387, train_loss=6.0422497

Batch 29180, train_perplexity=437.84683, train_loss=6.081869

Batch 29190, train_perplexity=423.17136, train_loss=6.047777

Batch 29200, train_perplexity=443.2117, train_loss=6.0940475

Batch 29210, train_perplexity=438.31683, train_loss=6.082942

Batch 29220, train_perplexity=496.7375, train_loss=6.2080617

Batch 29230, train_perplexity=417.08316, train_loss=6.0332856

Batch 29240, train_perplexity=489.72702, train_loss=6.193848

Batch 29250, train_perplexity=458.04282, train_loss=6.1269627

Batch 29260, train_perplexity=441.90967, train_loss=6.0911055

Batch 29270, train_perplexity=514.17084, train_loss=6.2425556

Batch 29280, train_perplexity=467.52032, train_loss=6.147443

Batch 29290, train_perplexity=490.2235, train_loss=6.1948614

Batch 29300, train_perplexity=489.9627, train_loss=6.1943293

Batch 29310, train_perplexity=495.27087, train_loss=6.205105

Batch 29320, train_perplexity=461.0066, train_loss=6.1334124

Batch 29330, train_perplexity=463.99533, train_loss=6.1398745

Batch 29340, train_perplexity=413.36917, train_loss=6.024341

Batch 29350, train_perplexity=516.52057, train_loss=6.247115

Batch 29360, train_perplexity=569.0696, train_loss=6.3440027

Batch 29370, train_perplexity=475.3562, train_loss=6.1640644
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 29380, train_perplexity=433.94714, train_loss=6.0729227

Batch 29390, train_perplexity=485.22293, train_loss=6.1846085

Batch 29400, train_perplexity=453.50674, train_loss=6.11701

Batch 29410, train_perplexity=465.31763, train_loss=6.14272

Batch 29420, train_perplexity=465.96152, train_loss=6.144103

Batch 29430, train_perplexity=458.32248, train_loss=6.127573

Batch 29440, train_perplexity=456.46338, train_loss=6.1235085

Batch 29450, train_perplexity=496.3111, train_loss=6.207203

Batch 29460, train_perplexity=472.81277, train_loss=6.1586995

Batch 29470, train_perplexity=413.07816, train_loss=6.023637

Batch 29480, train_perplexity=447.35056, train_loss=6.1033425

Batch 29490, train_perplexity=439.43225, train_loss=6.0854836

Batch 29500, train_perplexity=460.07196, train_loss=6.131383

Batch 29510, train_perplexity=438.3396, train_loss=6.082994

Batch 29520, train_perplexity=499.70782, train_loss=6.2140236

Batch 29530, train_perplexity=528.6195, train_loss=6.270269

Batch 29540, train_perplexity=473.82684, train_loss=6.160842

Batch 29550, train_perplexity=514.3518, train_loss=6.2429075

Batch 29560, train_perplexity=490.50946, train_loss=6.1954446

Batch 29570, train_perplexity=444.98755, train_loss=6.0980463

Batch 29580, train_perplexity=447.6873, train_loss=6.104095

Batch 29590, train_perplexity=481.73718, train_loss=6.1773987

Batch 29600, train_perplexity=457.62976, train_loss=6.1260605

Batch 29610, train_perplexity=474.70203, train_loss=6.1626873

Batch 29620, train_perplexity=471.35925, train_loss=6.1556206

Batch 29630, train_perplexity=475.88644, train_loss=6.1651793

Batch 29640, train_perplexity=532.0019, train_loss=6.276647

Batch 29650, train_perplexity=427.34695, train_loss=6.057596

Batch 29660, train_perplexity=428.99008, train_loss=6.061434

Batch 29670, train_perplexity=441.42316, train_loss=6.090004

Batch 29680, train_perplexity=477.80347, train_loss=6.1691995

Batch 29690, train_perplexity=458.89542, train_loss=6.1288223

Batch 29700, train_perplexity=430.7981, train_loss=6.0656395

Batch 29710, train_perplexity=495.60397, train_loss=6.205777

Batch 29720, train_perplexity=515.9195, train_loss=6.2459507

Batch 29730, train_perplexity=453.10208, train_loss=6.1161175

Batch 29740, train_perplexity=467.8073, train_loss=6.1480565

Batch 29750, train_perplexity=458.8217, train_loss=6.1286616

Batch 29760, train_perplexity=459.36612, train_loss=6.1298475

Batch 29770, train_perplexity=430.565, train_loss=6.0650983

Batch 29780, train_perplexity=476.0914, train_loss=6.16561

Batch 29790, train_perplexity=460.22116, train_loss=6.131707

Batch 29800, train_perplexity=457.60446, train_loss=6.126005

Batch 29810, train_perplexity=482.24097, train_loss=6.178444

Batch 29820, train_perplexity=476.71497, train_loss=6.1669188

Batch 29830, train_perplexity=486.377, train_loss=6.186984

Batch 29840, train_perplexity=405.50845, train_loss=6.0051417

Batch 29850, train_perplexity=463.5243, train_loss=6.138859

Batch 29860, train_perplexity=438.4619, train_loss=6.083273

Batch 29870, train_perplexity=468.00568, train_loss=6.1484804

Batch 29880, train_perplexity=379.4937, train_loss=5.938838

Batch 29890, train_perplexity=535.29694, train_loss=6.2828217

Batch 29900, train_perplexity=473.2689, train_loss=6.1596637

Batch 29910, train_perplexity=442.9945, train_loss=6.0935574

Batch 29920, train_perplexity=457.90042, train_loss=6.126652

Batch 29930, train_perplexity=512.5051, train_loss=6.2393107

Batch 29940, train_perplexity=472.8942, train_loss=6.1588717

Batch 29950, train_perplexity=453.64343, train_loss=6.1173115

Batch 29960, train_perplexity=441.8001, train_loss=6.0908575

Batch 29970, train_perplexity=478.0849, train_loss=6.1697884

Batch 29980, train_perplexity=476.2578, train_loss=6.1659594

Batch 29990, train_perplexity=441.51874, train_loss=6.0902205

Batch 30000, train_perplexity=453.32684, train_loss=6.1166134

Batch 30010, train_perplexity=460.0597, train_loss=6.1313562

Batch 30020, train_perplexity=455.95868, train_loss=6.122402

Batch 30030, train_perplexity=462.11343, train_loss=6.1358104

Batch 30040, train_perplexity=468.58267, train_loss=6.1497126

Batch 30050, train_perplexity=402.52338, train_loss=5.997753

Batch 30060, train_perplexity=414.1491, train_loss=6.026226

Batch 30070, train_perplexity=472.27853, train_loss=6.157569

Batch 30080, train_perplexity=487.7938, train_loss=6.189893

Batch 30090, train_perplexity=449.54773, train_loss=6.108242

Batch 30100, train_perplexity=502.8983, train_loss=6.220388

Batch 30110, train_perplexity=449.97452, train_loss=6.109191

Batch 30120, train_perplexity=456.589, train_loss=6.1237836

Batch 30130, train_perplexity=544.08014, train_loss=6.2990966

Batch 30140, train_perplexity=494.16922, train_loss=6.202878

Batch 30150, train_perplexity=456.5239, train_loss=6.123641

Batch 30160, train_perplexity=415.90704, train_loss=6.030462

Batch 30170, train_perplexity=495.4452, train_loss=6.2054567

Batch 30180, train_perplexity=459.8805, train_loss=6.1309667

Batch 30190, train_perplexity=401.4065, train_loss=5.9949746

Batch 30200, train_perplexity=462.1304, train_loss=6.135847

Batch 30210, train_perplexity=385.09656, train_loss=5.953494

Batch 30220, train_perplexity=483.43036, train_loss=6.1809072

Batch 30230, train_perplexity=468.60413, train_loss=6.1497583

Batch 30240, train_perplexity=478.5721, train_loss=6.170807

Batch 30250, train_perplexity=433.52686, train_loss=6.071954

Batch 30260, train_perplexity=461.13193, train_loss=6.133684

Batch 30270, train_perplexity=419.65942, train_loss=6.0394435

Batch 30280, train_perplexity=443.8995, train_loss=6.095598

Batch 30290, train_perplexity=449.10852, train_loss=6.1072645

Batch 30300, train_perplexity=420.4817, train_loss=6.041401

Batch 30310, train_perplexity=489.80923, train_loss=6.194016

Batch 30320, train_perplexity=402.8183, train_loss=5.9984856

Batch 30330, train_perplexity=455.7348, train_loss=6.121911

Batch 30340, train_perplexity=486.71643, train_loss=6.1876817

Batch 30350, train_perplexity=474.13513, train_loss=6.1614923

Batch 30360, train_perplexity=444.4273, train_loss=6.0967865

Batch 30370, train_perplexity=494.03067, train_loss=6.2025976

Batch 30380, train_perplexity=474.7113, train_loss=6.162707

Batch 30390, train_perplexity=459.968, train_loss=6.131157

Batch 30400, train_perplexity=391.73004, train_loss=5.970573

Batch 30410, train_perplexity=446.42084, train_loss=6.101262

Batch 30420, train_perplexity=365.6474, train_loss=5.9016695

Batch 30430, train_perplexity=425.40005, train_loss=6.05303

Batch 30440, train_perplexity=447.45404, train_loss=6.103574

Batch 30450, train_perplexity=475.6505, train_loss=6.1646833

Batch 30460, train_perplexity=449.38614, train_loss=6.1078825

Batch 30470, train_perplexity=444.34235, train_loss=6.0965953

Batch 30480, train_perplexity=430.52167, train_loss=6.0649977

Batch 30490, train_perplexity=499.66495, train_loss=6.2139378

Batch 30500, train_perplexity=463.22403, train_loss=6.138211

Batch 30510, train_perplexity=422.02597, train_loss=6.045067

Batch 30520, train_perplexity=481.45126, train_loss=6.176805

Batch 30530, train_perplexity=451.91318, train_loss=6.11349

Batch 30540, train_perplexity=497.98874, train_loss=6.2105775

Batch 30550, train_perplexity=448.05014, train_loss=6.104905

Batch 30560, train_perplexity=435.50555, train_loss=6.0765076

Batch 30570, train_perplexity=449.327, train_loss=6.107751

Batch 30580, train_perplexity=442.03864, train_loss=6.0913973

Batch 30590, train_perplexity=491.62054, train_loss=6.197707

Batch 30600, train_perplexity=448.06638, train_loss=6.1049414

Batch 30610, train_perplexity=495.35236, train_loss=6.2052693

Batch 30620, train_perplexity=367.55246, train_loss=5.906866

Batch 30630, train_perplexity=468.35483, train_loss=6.149226

Batch 30640, train_perplexity=452.56186, train_loss=6.1149244

Batch 30650, train_perplexity=452.28162, train_loss=6.114305

Batch 30660, train_perplexity=483.39276, train_loss=6.1808295

Batch 30670, train_perplexity=428.2547, train_loss=6.059718

Batch 30680, train_perplexity=438.14172, train_loss=6.0825424

Batch 30690, train_perplexity=510.80292, train_loss=6.235984
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 30700, train_perplexity=472.80173, train_loss=6.158676

Batch 30710, train_perplexity=483.96567, train_loss=6.182014

Batch 30720, train_perplexity=460.2078, train_loss=6.131678

Batch 30730, train_perplexity=459.7268, train_loss=6.1306324

Batch 30740, train_perplexity=440.84387, train_loss=6.0886908

Batch 30750, train_perplexity=440.1969, train_loss=6.087222

Batch 30760, train_perplexity=439.5186, train_loss=6.08568

Batch 30770, train_perplexity=479.5521, train_loss=6.1728525

Batch 30780, train_perplexity=506.41376, train_loss=6.227354

Batch 30790, train_perplexity=488.6025, train_loss=6.1915493

Batch 30800, train_perplexity=444.80426, train_loss=6.0976343

Batch 30810, train_perplexity=456.51996, train_loss=6.1236324

Batch 30820, train_perplexity=476.38184, train_loss=6.1662197

Batch 30830, train_perplexity=463.4947, train_loss=6.138795

Batch 30840, train_perplexity=435.44617, train_loss=6.076371

Batch 30850, train_perplexity=460.8831, train_loss=6.1331444

Batch 30860, train_perplexity=462.9153, train_loss=6.137544

Batch 30870, train_perplexity=484.32513, train_loss=6.1827564

Batch 30880, train_perplexity=435.85873, train_loss=6.077318

Batch 30890, train_perplexity=457.9454, train_loss=6.12675

Batch 30900, train_perplexity=410.16504, train_loss=6.0165596

Batch 30910, train_perplexity=453.73212, train_loss=6.117507

Batch 30920, train_perplexity=461.74823, train_loss=6.13502

Batch 30930, train_perplexity=444.1036, train_loss=6.096058

Batch 30940, train_perplexity=483.6005, train_loss=6.181259

Batch 30950, train_perplexity=432.99344, train_loss=6.0707226

Batch 30960, train_perplexity=410.6956, train_loss=6.0178523

Batch 30970, train_perplexity=477.14523, train_loss=6.167821

Batch 30980, train_perplexity=471.23926, train_loss=6.155366

Batch 30990, train_perplexity=487.32974, train_loss=6.188941

Batch 31000, train_perplexity=444.0714, train_loss=6.0959854

Batch 31010, train_perplexity=398.94476, train_loss=5.988823

Batch 31020, train_perplexity=452.63177, train_loss=6.115079

Batch 31030, train_perplexity=469.84995, train_loss=6.1524134

Batch 31040, train_perplexity=435.46527, train_loss=6.076415

Batch 31050, train_perplexity=464.19492, train_loss=6.1403046

Batch 31060, train_perplexity=444.65158, train_loss=6.097291

Batch 31070, train_perplexity=381.2765, train_loss=5.943525

Batch 31080, train_perplexity=448.99072, train_loss=6.1070023

Batch 31090, train_perplexity=483.23444, train_loss=6.180502

Batch 31100, train_perplexity=453.67218, train_loss=6.117375

Batch 31110, train_perplexity=437.41528, train_loss=6.080883

Batch 31120, train_perplexity=447.17758, train_loss=6.102956

Batch 31130, train_perplexity=420.95673, train_loss=6.04253

Batch 31140, train_perplexity=423.2335, train_loss=6.047924

Batch 31150, train_perplexity=407.32916, train_loss=6.0096216

Batch 31160, train_perplexity=475.69565, train_loss=6.164778

Batch 31170, train_perplexity=437.24573, train_loss=6.0804954

Batch 31180, train_perplexity=450.0848, train_loss=6.109436

Batch 31190, train_perplexity=449.5533, train_loss=6.1082544

Batch 31200, train_perplexity=426.4952, train_loss=6.055601

Batch 31210, train_perplexity=482.9152, train_loss=6.179841

Batch 31220, train_perplexity=376.9565, train_loss=5.93213

Batch 31230, train_perplexity=400.20456, train_loss=5.991976

Batch 31240, train_perplexity=423.38046, train_loss=6.048271

Batch 31250, train_perplexity=485.44165, train_loss=6.185059

Batch 31260, train_perplexity=423.63126, train_loss=6.0488634

Batch 31270, train_perplexity=466.4337, train_loss=6.145116

Batch 31280, train_perplexity=457.36188, train_loss=6.125475

Batch 31290, train_perplexity=491.1224, train_loss=6.1966934

Batch 31300, train_perplexity=468.15253, train_loss=6.148794

Batch 31310, train_perplexity=470.7609, train_loss=6.1543503

Batch 31320, train_perplexity=450.473, train_loss=6.110298

Batch 31330, train_perplexity=478.95996, train_loss=6.171617

Batch 31340, train_perplexity=461.03033, train_loss=6.133464

Batch 31350, train_perplexity=413.82892, train_loss=6.0254526

Batch 31360, train_perplexity=527.5121, train_loss=6.268172

Batch 31370, train_perplexity=428.08362, train_loss=6.0593185

Batch 31380, train_perplexity=456.7682, train_loss=6.124176

Batch 31390, train_perplexity=455.56705, train_loss=6.121543

Batch 31400, train_perplexity=422.9345, train_loss=6.0472174

Batch 31410, train_perplexity=483.0165, train_loss=6.180051

Batch 31420, train_perplexity=462.06827, train_loss=6.1357126

Batch 31430, train_perplexity=436.64755, train_loss=6.0791264

Batch 31440, train_perplexity=492.0305, train_loss=6.1985407

Batch 31450, train_perplexity=415.33212, train_loss=6.0290785

Batch 31460, train_perplexity=430.4533, train_loss=6.064839

Batch 31470, train_perplexity=448.10568, train_loss=6.105029

Batch 31480, train_perplexity=455.018, train_loss=6.120337

Batch 31490, train_perplexity=466.55646, train_loss=6.145379

Batch 31500, train_perplexity=469.0452, train_loss=6.150699

Batch 31510, train_perplexity=512.86865, train_loss=6.24002

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Batch 31520, train_perplexity=439.55463, train_loss=6.085762

Batch 31530, train_perplexity=488.3803, train_loss=6.1910944

Batch 31540, train_perplexity=403.7375, train_loss=6.000765

Batch 31550, train_perplexity=516.103, train_loss=6.2463064

Batch 31560, train_perplexity=470.094, train_loss=6.1529326

Batch 31570, train_perplexity=450.90024, train_loss=6.111246

Batch 31580, train_perplexity=490.4096, train_loss=6.195241

Batch 31590, train_perplexity=437.55463, train_loss=6.0812016

Batch 31600, train_perplexity=456.96338, train_loss=6.1246033

Batch 31610, train_perplexity=451.0314, train_loss=6.111537

Batch 31620, train_perplexity=452.88632, train_loss=6.115641

Batch 31630, train_perplexity=485.2653, train_loss=6.1846957

Batch 31640, train_perplexity=390.19073, train_loss=5.9666357

Batch 31650, train_perplexity=436.6255, train_loss=6.079076

Batch 31660, train_perplexity=464.2009, train_loss=6.1403174

Batch 31670, train_perplexity=428.48633, train_loss=6.060259

Batch 31680, train_perplexity=446.71002, train_loss=6.1019096

Batch 31690, train_perplexity=463.51016, train_loss=6.1388283

Batch 31700, train_perplexity=475.4734, train_loss=6.164311

Batch 31710, train_perplexity=462.00525, train_loss=6.1355762

Batch 31720, train_perplexity=493.5513, train_loss=6.201627

Batch 31730, train_perplexity=453.96237, train_loss=6.1180143

Batch 31740, train_perplexity=435.0566, train_loss=6.075476

Batch 31750, train_perplexity=470.7676, train_loss=6.1543646

Batch 31760, train_perplexity=431.10632, train_loss=6.0663548

Batch 31770, train_perplexity=458.94443, train_loss=6.128929

Batch 31780, train_perplexity=452.09964, train_loss=6.1139026

Batch 31790, train_perplexity=415.39273, train_loss=6.0292244

Batch 31800, train_perplexity=437.51166, train_loss=6.0811033

Batch 31810, train_perplexity=440.06256, train_loss=6.086917

Batch 31820, train_perplexity=455.76175, train_loss=6.12197

Batch 31830, train_perplexity=462.5025, train_loss=6.136652

Batch 31840, train_perplexity=416.11237, train_loss=6.0309553

Batch 31850, train_perplexity=448.4969, train_loss=6.1059017

Batch 31860, train_perplexity=434.70346, train_loss=6.074664

Batch 31870, train_perplexity=485.95462, train_loss=6.1861153

Batch 31880, train_perplexity=444.5672, train_loss=6.097101

Batch 31890, train_perplexity=439.16956, train_loss=6.0848856

Batch 31900, train_perplexity=433.96698, train_loss=6.0729685

Batch 31910, train_perplexity=443.53854, train_loss=6.0947847

Batch 31920, train_perplexity=454.09747, train_loss=6.118312

Batch 31930, train_perplexity=456.22662, train_loss=6.1229897

Batch 31940, train_perplexity=401.50354, train_loss=5.9952164

Batch 31950, train_perplexity=393.27692, train_loss=5.974514
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 31960, train_perplexity=398.3114, train_loss=5.987234

Batch 31970, train_perplexity=437.2603, train_loss=6.0805287

Batch 31980, train_perplexity=445.2036, train_loss=6.0985317

Batch 31990, train_perplexity=451.1695, train_loss=6.111843

Batch 32000, train_perplexity=473.2883, train_loss=6.1597047

Batch 32010, train_perplexity=463.69983, train_loss=6.1392374

Batch 32020, train_perplexity=412.65567, train_loss=6.0226135

Batch 32030, train_perplexity=439.2018, train_loss=6.084959

Batch 32040, train_perplexity=541.10144, train_loss=6.2936068

Batch 32050, train_perplexity=403.20493, train_loss=5.999445

Batch 32060, train_perplexity=438.43515, train_loss=6.083212

Batch 32070, train_perplexity=384.10074, train_loss=5.950905

Batch 32080, train_perplexity=432.37656, train_loss=6.069297

Batch 32090, train_perplexity=479.68655, train_loss=6.173133

Batch 32100, train_perplexity=447.11853, train_loss=6.1028237

Batch 32110, train_perplexity=400.26944, train_loss=5.992138

Batch 32120, train_perplexity=448.39465, train_loss=6.105674

Batch 32130, train_perplexity=445.4546, train_loss=6.0990953

Batch 32140, train_perplexity=419.06332, train_loss=6.038022

Batch 32150, train_perplexity=387.14798, train_loss=5.958807

Batch 32160, train_perplexity=462.52148, train_loss=6.136693

Batch 32170, train_perplexity=393.09167, train_loss=5.974043

Batch 32180, train_perplexity=408.7655, train_loss=6.0131416

Batch 32190, train_perplexity=468.17487, train_loss=6.148842

Batch 32200, train_perplexity=425.18753, train_loss=6.0525303

Batch 32210, train_perplexity=458.0166, train_loss=6.1269054

Batch 32220, train_perplexity=440.49756, train_loss=6.087905

Batch 32230, train_perplexity=429.44647, train_loss=6.062497

Batch 32240, train_perplexity=496.97842, train_loss=6.2085466

Batch 32250, train_perplexity=452.16992, train_loss=6.114058

Batch 32260, train_perplexity=492.59836, train_loss=6.199694

Batch 32270, train_perplexity=393.86807, train_loss=5.976016

Batch 32280, train_perplexity=412.0933, train_loss=6.02125

Batch 32290, train_perplexity=453.01505, train_loss=6.1159253

Batch 32300, train_perplexity=480.41174, train_loss=6.1746435

Batch 32310, train_perplexity=443.65045, train_loss=6.095037

Batch 32320, train_perplexity=434.63754, train_loss=6.0745125

Batch 32330, train_perplexity=452.94138, train_loss=6.1157627

Batch 32340, train_perplexity=426.90317, train_loss=6.056557

Batch 32350, train_perplexity=477.03717, train_loss=6.1675944

Batch 32360, train_perplexity=477.26947, train_loss=6.1680813

Batch 32370, train_perplexity=439.09628, train_loss=6.0847187

Batch 32380, train_perplexity=477.2811, train_loss=6.1681056

Batch 32390, train_perplexity=399.19443, train_loss=5.9894485

Batch 32400, train_perplexity=444.18536, train_loss=6.096242

Batch 32410, train_perplexity=408.68793, train_loss=6.012952

Batch 32420, train_perplexity=439.46976, train_loss=6.085569

Batch 32430, train_perplexity=434.61932, train_loss=6.0744705

Batch 32440, train_perplexity=494.49545, train_loss=6.203538

Batch 32450, train_perplexity=424.32086, train_loss=6.05049

Batch 32460, train_perplexity=440.49695, train_loss=6.0879035

Batch 32470, train_perplexity=451.27234, train_loss=6.112071

Batch 32480, train_perplexity=440.92438, train_loss=6.0888734

Batch 32490, train_perplexity=438.04938, train_loss=6.0823317

Batch 32500, train_perplexity=432.86462, train_loss=6.070425

Batch 32510, train_perplexity=500.458, train_loss=6.2155237

Batch 32520, train_perplexity=451.05292, train_loss=6.1115847

Batch 32530, train_perplexity=438.90118, train_loss=6.0842743

Batch 32540, train_perplexity=433.85693, train_loss=6.072715

Batch 32550, train_perplexity=485.151, train_loss=6.18446

Batch 32560, train_perplexity=469.7908, train_loss=6.1522875

Batch 32570, train_perplexity=444.14343, train_loss=6.0961475

Batch 32580, train_perplexity=461.87396, train_loss=6.135292

Batch 32590, train_perplexity=399.5471, train_loss=5.9903316

Batch 32600, train_perplexity=402.71918, train_loss=5.9982395

Batch 32610, train_perplexity=430.70935, train_loss=6.0654335

Batch 32620, train_perplexity=404.873, train_loss=6.0035734

Batch 32630, train_perplexity=398.96338, train_loss=5.9888697

Batch 32640, train_perplexity=493.5447, train_loss=6.2016134

Batch 32650, train_perplexity=495.70514, train_loss=6.2059813

Batch 32660, train_perplexity=433.31937, train_loss=6.071475

Batch 32670, train_perplexity=434.8579, train_loss=6.0750194

Batch 32680, train_perplexity=441.94864, train_loss=6.0911937

Batch 32690, train_perplexity=413.47977, train_loss=6.0246086

Batch 32700, train_perplexity=428.11035, train_loss=6.059381

Batch 32710, train_perplexity=463.82544, train_loss=6.1395082

Batch 32720, train_perplexity=463.20636, train_loss=6.1381726

Batch 32730, train_perplexity=425.76086, train_loss=6.053878

Batch 32740, train_perplexity=430.78125, train_loss=6.0656004

Batch 32750, train_perplexity=442.52432, train_loss=6.0924954

Batch 32760, train_perplexity=420.23633, train_loss=6.0408173

Batch 32770, train_perplexity=453.8836, train_loss=6.117841

Batch 32780, train_perplexity=470.07358, train_loss=6.1528893

Batch 32790, train_perplexity=439.05963, train_loss=6.0846353

Batch 32800, train_perplexity=461.75858, train_loss=6.135042

Batch 32810, train_perplexity=473.30093, train_loss=6.1597314

Batch 32820, train_perplexity=414.25653, train_loss=6.0264854

Batch 32830, train_perplexity=478.37955, train_loss=6.1704044

Batch 32840, train_perplexity=398.86258, train_loss=5.988617

Batch 32850, train_perplexity=433.55148, train_loss=6.0720105

Batch 32860, train_perplexity=444.51505, train_loss=6.096984

Batch 32870, train_perplexity=412.85406, train_loss=6.023094

Batch 32880, train_perplexity=443.65744, train_loss=6.0950527

Batch 32890, train_perplexity=446.07144, train_loss=6.100479

Batch 32900, train_perplexity=409.28665, train_loss=6.0144157

Batch 32910, train_perplexity=460.01056, train_loss=6.1312494

Batch 32920, train_perplexity=425.01077, train_loss=6.0521145

Batch 32930, train_perplexity=405.07266, train_loss=6.0040665

Batch 32940, train_perplexity=494.5846, train_loss=6.203718

Batch 32950, train_perplexity=412.00174, train_loss=6.0210276

Batch 32960, train_perplexity=424.93536, train_loss=6.051937

Batch 32970, train_perplexity=450.3506, train_loss=6.1100264

Batch 32980, train_perplexity=405.3478, train_loss=6.0047455

Batch 32990, train_perplexity=428.46143, train_loss=6.0602007

Batch 33000, train_perplexity=405.86963, train_loss=6.006032

Batch 33010, train_perplexity=387.75174, train_loss=5.9603653

Batch 33020, train_perplexity=452.49927, train_loss=6.114786

Batch 33030, train_perplexity=444.41415, train_loss=6.096757

Batch 33040, train_perplexity=398.4497, train_loss=5.9875813

Batch 33050, train_perplexity=476.13702, train_loss=6.1657057

Batch 33060, train_perplexity=435.80096, train_loss=6.0771856

Batch 33070, train_perplexity=469.7254, train_loss=6.1521482

Batch 33080, train_perplexity=410.8186, train_loss=6.0181518

Batch 33090, train_perplexity=438.27127, train_loss=6.082838

Batch 33100, train_perplexity=389.79614, train_loss=5.965624

Batch 33110, train_perplexity=418.01794, train_loss=6.0355244

Batch 33120, train_perplexity=474.53094, train_loss=6.162327

Batch 33130, train_perplexity=440.74927, train_loss=6.088476

Batch 33140, train_perplexity=427.93176, train_loss=6.058964

Batch 33150, train_perplexity=456.7059, train_loss=6.1240396

Batch 33160, train_perplexity=422.8952, train_loss=6.0471244

Batch 33170, train_perplexity=468.429, train_loss=6.1493845

Batch 33180, train_perplexity=423.64096, train_loss=6.0488863

Batch 33190, train_perplexity=424.76743, train_loss=6.051542

Batch 33200, train_perplexity=388.3522, train_loss=5.9619126

Batch 33210, train_perplexity=472.84955, train_loss=6.158777

Batch 33220, train_perplexity=421.78275, train_loss=6.0444903

Batch 33230, train_perplexity=369.55115, train_loss=5.912289

Batch 33240, train_perplexity=425.1196, train_loss=6.0523705

Batch 33250, train_perplexity=402.47385, train_loss=5.99763

Batch 33260, train_perplexity=415.15747, train_loss=6.028658

Batch 33270, train_perplexity=471.43658, train_loss=6.1557846
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33280, train_perplexity=412.26862, train_loss=6.021675

Batch 33290, train_perplexity=454.99698, train_loss=6.1202908

Batch 33300, train_perplexity=401.85254, train_loss=5.996085

Batch 33310, train_perplexity=448.2429, train_loss=6.105335

Batch 33320, train_perplexity=431.66296, train_loss=6.067645

Batch 33330, train_perplexity=483.99084, train_loss=6.182066

Batch 33340, train_perplexity=380.92722, train_loss=5.9426084

Batch 33350, train_perplexity=384.83734, train_loss=5.952821

Batch 33360, train_perplexity=462.7145, train_loss=6.13711

Batch 33370, train_perplexity=428.61835, train_loss=6.060567

Batch 33380, train_perplexity=457.57346, train_loss=6.1259375

Batch 33390, train_perplexity=460.66513, train_loss=6.1326714

Batch 33400, train_perplexity=444.29614, train_loss=6.0964913

Batch 33410, train_perplexity=424.2468, train_loss=6.0503154

Batch 33420, train_perplexity=425.37915, train_loss=6.052981

Batch 33430, train_perplexity=443.4821, train_loss=6.0946574

Batch 33440, train_perplexity=430.00198, train_loss=6.06379

Batch 33450, train_perplexity=430.44983, train_loss=6.064831

Batch 33460, train_perplexity=446.03275, train_loss=6.1003923

Batch 33470, train_perplexity=489.919, train_loss=6.19424

Batch 33480, train_perplexity=398.19614, train_loss=5.9869447

Batch 33490, train_perplexity=418.53394, train_loss=6.036758

Batch 33500, train_perplexity=425.37146, train_loss=6.052963

Batch 33510, train_perplexity=429.04654, train_loss=6.0615654

Batch 33520, train_perplexity=425.19156, train_loss=6.05254

Batch 33530, train_perplexity=415.92667, train_loss=6.030509

Batch 33540, train_perplexity=480.70184, train_loss=6.175247

Batch 33550, train_perplexity=407.114, train_loss=6.0090933

Batch 33560, train_perplexity=431.26196, train_loss=6.0667157

Batch 33570, train_perplexity=457.27203, train_loss=6.1252785

Batch 33580, train_perplexity=442.43762, train_loss=6.0922995

Batch 33590, train_perplexity=395.0659, train_loss=5.9790525

Batch 33600, train_perplexity=479.20853, train_loss=6.172136

Batch 33610, train_perplexity=423.3229, train_loss=6.0481353

Batch 33620, train_perplexity=427.84363, train_loss=6.058758

Batch 33630, train_perplexity=427.3712, train_loss=6.057653

Batch 33640, train_perplexity=448.61752, train_loss=6.1061707

Batch 33650, train_perplexity=426.45493, train_loss=6.0555067

Batch 33660, train_perplexity=432.7664, train_loss=6.070198

Batch 33670, train_perplexity=425.67538, train_loss=6.053677

Batch 33680, train_perplexity=356.4665, train_loss=5.8762403

Batch 33690, train_perplexity=432.13498, train_loss=6.068738

Batch 33700, train_perplexity=423.62964, train_loss=6.0488596

Batch 33710, train_perplexity=400.02368, train_loss=5.9915237

Batch 33720, train_perplexity=423.68762, train_loss=6.0489964

Batch 33730, train_perplexity=403.82007, train_loss=6.0009694

Batch 33740, train_perplexity=509.50513, train_loss=6.23344

Batch 33750, train_perplexity=406.40646, train_loss=6.007354

Batch 33760, train_perplexity=423.36954, train_loss=6.0482454

Batch 33770, train_perplexity=432.3683, train_loss=6.069278

Batch 33780, train_perplexity=439.51355, train_loss=6.0856686

Batch 33790, train_perplexity=421.69687, train_loss=6.0442867

Batch 33800, train_perplexity=503.5301, train_loss=6.2216434

Batch 33810, train_perplexity=424.4793, train_loss=6.0508633

Batch 33820, train_perplexity=441.22263, train_loss=6.0895495

Batch 33830, train_perplexity=421.48257, train_loss=6.0437784

Batch 33840, train_perplexity=488.03436, train_loss=6.190386

Batch 33850, train_perplexity=431.39813, train_loss=6.0670314

Batch 33860, train_perplexity=405.45837, train_loss=6.005018

Batch 33870, train_perplexity=415.2535, train_loss=6.028889

Batch 33880, train_perplexity=446.67465, train_loss=6.1018305

Batch 33890, train_perplexity=414.29843, train_loss=6.0265865

Batch 33900, train_perplexity=380.74072, train_loss=5.9421186

Batch 33910, train_perplexity=455.48712, train_loss=6.1213675

Batch 33920, train_perplexity=443.9715, train_loss=6.0957603

Batch 33930, train_perplexity=453.88187, train_loss=6.117837

Batch 33940, train_perplexity=442.2735, train_loss=6.0919285

Batch 33950, train_perplexity=401.4174, train_loss=5.995002

Batch 33960, train_perplexity=416.57056, train_loss=6.032056

Batch 33970, train_perplexity=406.06552, train_loss=6.0065145

Batch 33980, train_perplexity=428.38214, train_loss=6.0600157

Batch 33990, train_perplexity=419.2528, train_loss=6.038474

Batch 34000, train_perplexity=416.0457, train_loss=6.030795

Batch 34010, train_perplexity=430.57587, train_loss=6.0651236

Batch 34020, train_perplexity=376.76425, train_loss=5.9316196

Batch 34030, train_perplexity=416.3144, train_loss=6.0314407

Batch 34040, train_perplexity=490.89603, train_loss=6.1962323

Batch 34050, train_perplexity=444.98926, train_loss=6.09805

Batch 34060, train_perplexity=446.0238, train_loss=6.1003723

Batch 34070, train_perplexity=412.7395, train_loss=6.0228167

Batch 34080, train_perplexity=417.5032, train_loss=6.034292

Batch 34090, train_perplexity=415.26834, train_loss=6.028925

Batch 34100, train_perplexity=445.1185, train_loss=6.0983405

Batch 34110, train_perplexity=412.4977, train_loss=6.0222306

Batch 34120, train_perplexity=376.29205, train_loss=5.9303656

Batch 34130, train_perplexity=355.93063, train_loss=5.874736

Batch 34140, train_perplexity=383.499, train_loss=5.949337

Batch 34150, train_perplexity=508.72754, train_loss=6.2319126

Batch 34160, train_perplexity=468.56033, train_loss=6.149665

Batch 34170, train_perplexity=388.205, train_loss=5.9615335

Batch 34180, train_perplexity=421.46088, train_loss=6.043727

Batch 34190, train_perplexity=465.29764, train_loss=6.1426773

Batch 34200, train_perplexity=470.1184, train_loss=6.1529846

Batch 34210, train_perplexity=446.4315, train_loss=6.101286

Batch 34220, train_perplexity=441.6175, train_loss=6.090444

Batch 34230, train_perplexity=516.9269, train_loss=6.2479014

Batch 34240, train_perplexity=413.06418, train_loss=6.023603

Batch 34250, train_perplexity=393.8418, train_loss=5.9759493

Batch 34260, train_perplexity=456.14136, train_loss=6.1228027

Batch 34270, train_perplexity=451.01636, train_loss=6.1115036

Batch 34280, train_perplexity=413.51053, train_loss=6.024683

Batch 34290, train_perplexity=393.56433, train_loss=5.9752445

Batch 34300, train_perplexity=415.8398, train_loss=6.0303

Batch 34310, train_perplexity=471.75635, train_loss=6.1564627

Batch 34320, train_perplexity=483.15244, train_loss=6.180332

Batch 34330, train_perplexity=391.44025, train_loss=5.969833

Batch 34340, train_perplexity=439.741, train_loss=6.086186

Batch 34350, train_perplexity=401.68777, train_loss=5.995675

Batch 34360, train_perplexity=408.53986, train_loss=6.0125895

Batch 34370, train_perplexity=442.70267, train_loss=6.0928984

Batch 34380, train_perplexity=460.51886, train_loss=6.132354

Batch 34390, train_perplexity=393.1603, train_loss=5.9742174

Batch 34400, train_perplexity=408.87485, train_loss=6.013409

Batch 34410, train_perplexity=439.45132, train_loss=6.085527

Batch 34420, train_perplexity=423.7448, train_loss=6.0491314

Batch 34430, train_perplexity=395.5326, train_loss=5.980233

Batch 34440, train_perplexity=423.9835, train_loss=6.0496945

Batch 34450, train_perplexity=401.4128, train_loss=5.9949903

Batch 34460, train_perplexity=426.93002, train_loss=6.05662

Batch 34470, train_perplexity=446.0021, train_loss=6.1003237

Batch 34480, train_perplexity=434.68625, train_loss=6.0746245

Batch 34490, train_perplexity=460.14526, train_loss=6.131542

Batch 34500, train_perplexity=412.61356, train_loss=6.0225115

Batch 34510, train_perplexity=371.90564, train_loss=5.91864

Batch 34520, train_perplexity=449.21088, train_loss=6.1074924

Batch 34530, train_perplexity=424.36374, train_loss=6.050591

Batch 34540, train_perplexity=391.5672, train_loss=5.970157

Batch 34550, train_perplexity=439.4641, train_loss=6.085556

Batch 34560, train_perplexity=434.0862, train_loss=6.073243

Batch 34570, train_perplexity=477.61096, train_loss=6.1687965

Batch 34580, train_perplexity=419.48077, train_loss=6.0390177

Batch 34590, train_perplexity=436.20013, train_loss=6.078101
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34600, train_perplexity=415.25647, train_loss=6.0288963

Batch 34610, train_perplexity=430.24628, train_loss=6.0643578

Batch 34620, train_perplexity=475.84967, train_loss=6.165102

Batch 34630, train_perplexity=400.58524, train_loss=5.9929266

Batch 34640, train_perplexity=437.09668, train_loss=6.0801544

Batch 34650, train_perplexity=451.73242, train_loss=6.11309

Batch 34660, train_perplexity=426.2238, train_loss=6.0549645

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Batch 34670, train_perplexity=428.66782, train_loss=6.0606823

Batch 34680, train_perplexity=439.54626, train_loss=6.085743

Batch 34690, train_perplexity=411.22174, train_loss=6.0191326

Batch 34700, train_perplexity=397.13065, train_loss=5.9842653

Batch 34710, train_perplexity=416.0689, train_loss=6.030851

Batch 34720, train_perplexity=403.0481, train_loss=5.999056

Batch 34730, train_perplexity=436.20908, train_loss=6.0781217

Batch 34740, train_perplexity=357.78232, train_loss=5.879925

Batch 34750, train_perplexity=424.0933, train_loss=6.0499535

Batch 34760, train_perplexity=416.12207, train_loss=6.0309787

Batch 34770, train_perplexity=407.44455, train_loss=6.009905

Batch 34780, train_perplexity=495.83563, train_loss=6.2062445

Batch 34790, train_perplexity=406.46924, train_loss=6.0075083

Batch 34800, train_perplexity=438.85535, train_loss=6.08417

Batch 34810, train_perplexity=381.84396, train_loss=5.945012

Batch 34820, train_perplexity=457.68127, train_loss=6.126173

Batch 34830, train_perplexity=425.92654, train_loss=6.054267

Batch 34840, train_perplexity=385.944, train_loss=5.9556923

Batch 34850, train_perplexity=440.01096, train_loss=6.0867996

Batch 34860, train_perplexity=395.4798, train_loss=5.9800997

Batch 34870, train_perplexity=415.37372, train_loss=6.0291786

Batch 34880, train_perplexity=450.66852, train_loss=6.110732

Batch 34890, train_perplexity=484.45538, train_loss=6.1830254

Batch 34900, train_perplexity=432.12158, train_loss=6.068707

Batch 34910, train_perplexity=388.0116, train_loss=5.9610353

Batch 34920, train_perplexity=409.79984, train_loss=6.015669

Batch 34930, train_perplexity=467.00296, train_loss=6.1463356

Batch 34940, train_perplexity=406.11432, train_loss=6.0066347

Batch 34950, train_perplexity=396.86374, train_loss=5.983593

Batch 34960, train_perplexity=440.36234, train_loss=6.087598

Batch 34970, train_perplexity=427.62784, train_loss=6.0582533

Batch 34980, train_perplexity=387.0819, train_loss=5.9586363

Batch 34990, train_perplexity=443.58127, train_loss=6.094881

Batch 35000, train_perplexity=391.55396, train_loss=5.9701233

Batch 35010, train_perplexity=458.0179, train_loss=6.1269083

Batch 35020, train_perplexity=419.38916, train_loss=6.0387993

Batch 35030, train_perplexity=407.57922, train_loss=6.0102353

Batch 35040, train_perplexity=431.5942, train_loss=6.067486

Batch 35050, train_perplexity=490.80823, train_loss=6.1960535

Batch 35060, train_perplexity=431.37054, train_loss=6.0669675

Batch 35070, train_perplexity=423.44446, train_loss=6.0484223

Batch 35080, train_perplexity=378.0434, train_loss=5.935009

Batch 35090, train_perplexity=439.32288, train_loss=6.0852346

Batch 35100, train_perplexity=419.2596, train_loss=6.0384903

Batch 35110, train_perplexity=430.0596, train_loss=6.063924

Batch 35120, train_perplexity=458.87616, train_loss=6.1287804

Batch 35130, train_perplexity=452.7153, train_loss=6.1152635

Batch 35140, train_perplexity=452.96298, train_loss=6.1158104

Batch 35150, train_perplexity=472.67618, train_loss=6.1584105

Batch 35160, train_perplexity=404.57773, train_loss=6.002844

Batch 35170, train_perplexity=437.55963, train_loss=6.081213

Batch 35180, train_perplexity=407.0286, train_loss=6.0088835

Batch 35190, train_perplexity=431.1536, train_loss=6.0664644

Batch 35200, train_perplexity=427.28317, train_loss=6.057447

Batch 35210, train_perplexity=386.49777, train_loss=5.957126

Batch 35220, train_perplexity=387.74582, train_loss=5.96035

Batch 35230, train_perplexity=403.65625, train_loss=6.0005636

Batch 35240, train_perplexity=362.4585, train_loss=5.89291

Batch 35250, train_perplexity=451.4297, train_loss=6.1124196

Batch 35260, train_perplexity=470.29553, train_loss=6.1533613

Batch 35270, train_perplexity=360.05615, train_loss=5.88626

Batch 35280, train_perplexity=418.72537, train_loss=6.037215

Batch 35290, train_perplexity=411.0861, train_loss=6.0188026

Batch 35300, train_perplexity=385.55847, train_loss=5.954693

Batch 35310, train_perplexity=475.73056, train_loss=6.1648517

Batch 35320, train_perplexity=403.40378, train_loss=5.999938

Batch 35330, train_perplexity=394.35934, train_loss=5.9772625

Batch 35340, train_perplexity=426.70514, train_loss=6.056093

Batch 35350, train_perplexity=419.39438, train_loss=6.0388117

Batch 35360, train_perplexity=396.2745, train_loss=5.982107

Batch 35370, train_perplexity=427.30988, train_loss=6.0575094

Batch 35380, train_perplexity=441.53305, train_loss=6.090253

Batch 35390, train_perplexity=388.40387, train_loss=5.9620457

Batch 35400, train_perplexity=437.3133, train_loss=6.08065

Batch 35410, train_perplexity=431.21713, train_loss=6.066612

Batch 35420, train_perplexity=443.61597, train_loss=6.0949593

Batch 35430, train_perplexity=411.36334, train_loss=6.019477

Batch 35440, train_perplexity=361.99786, train_loss=5.8916383

Batch 35450, train_perplexity=421.71014, train_loss=6.044318

Batch 35460, train_perplexity=379.4007, train_loss=5.938593

Batch 35470, train_perplexity=455.91956, train_loss=6.1223164

Batch 35480, train_perplexity=385.48602, train_loss=5.954505

Batch 35490, train_perplexity=451.86386, train_loss=6.113381

Batch 35500, train_perplexity=429.5915, train_loss=6.0628347

Batch 35510, train_perplexity=471.77975, train_loss=6.1565123

Batch 35520, train_perplexity=434.41336, train_loss=6.0739965

Batch 35530, train_perplexity=473.66147, train_loss=6.160493

Batch 35540, train_perplexity=415.34003, train_loss=6.0290976

Batch 35550, train_perplexity=395.97058, train_loss=5.98134

Batch 35560, train_perplexity=446.2395, train_loss=6.100856

Batch 35570, train_perplexity=465.2759, train_loss=6.1426306

Batch 35580, train_perplexity=464.44513, train_loss=6.1408434

Batch 35590, train_perplexity=417.14062, train_loss=6.0334234

Batch 35600, train_perplexity=392.3842, train_loss=5.9722414

Batch 35610, train_perplexity=376.9213, train_loss=5.9320364

Batch 35620, train_perplexity=427.1742, train_loss=6.057192

Batch 35630, train_perplexity=427.1705, train_loss=6.0571833

Batch 35640, train_perplexity=391.32996, train_loss=5.969551

Batch 35650, train_perplexity=392.60052, train_loss=5.9727926

Batch 35660, train_perplexity=415.20538, train_loss=6.0287733

Batch 35670, train_perplexity=432.64856, train_loss=6.069926

Batch 35680, train_perplexity=419.68204, train_loss=6.0394974

Batch 35690, train_perplexity=431.9619, train_loss=6.0683374

Batch 35700, train_perplexity=426.48322, train_loss=6.055573

Batch 35710, train_perplexity=414.187, train_loss=6.0263176

Batch 35720, train_perplexity=369.98895, train_loss=5.913473

Batch 35730, train_perplexity=438.0778, train_loss=6.0823965

Batch 35740, train_perplexity=425.44427, train_loss=6.053134

Batch 35750, train_perplexity=423.0281, train_loss=6.0474386

Batch 35760, train_perplexity=426.77594, train_loss=6.056259

Batch 35770, train_perplexity=388.09024, train_loss=5.961238

Batch 35780, train_perplexity=408.63803, train_loss=6.01283

Batch 35790, train_perplexity=420.1576, train_loss=6.04063

Batch 35800, train_perplexity=442.6212, train_loss=6.0927143

Batch 35810, train_perplexity=374.3781, train_loss=5.9252663

Batch 35820, train_perplexity=467.06754, train_loss=6.146474

Batch 35830, train_perplexity=399.4869, train_loss=5.990181

Batch 35840, train_perplexity=427.50388, train_loss=6.0579634

Batch 35850, train_perplexity=419.21982, train_loss=6.0383954
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 35860, train_perplexity=421.5501, train_loss=6.0439386

Batch 35870, train_perplexity=466.66037, train_loss=6.1456017

Batch 35880, train_perplexity=441.3657, train_loss=6.089874

Batch 35890, train_perplexity=426.50577, train_loss=6.055626

Batch 35900, train_perplexity=430.69806, train_loss=6.0654073

Batch 35910, train_perplexity=464.9583, train_loss=6.1419477

Batch 35920, train_perplexity=437.09897, train_loss=6.0801597

Batch 35930, train_perplexity=400.51077, train_loss=5.9927406

Batch 35940, train_perplexity=416.04074, train_loss=6.030783

Batch 35950, train_perplexity=430.99, train_loss=6.066085

Batch 35960, train_perplexity=428.83075, train_loss=6.0610623

Batch 35970, train_perplexity=462.31995, train_loss=6.136257

Batch 35980, train_perplexity=383.94235, train_loss=5.9504924

Batch 35990, train_perplexity=406.2315, train_loss=6.006923

Batch 36000, train_perplexity=383.04083, train_loss=5.9481416

Batch 36010, train_perplexity=456.57962, train_loss=6.123763

Batch 36020, train_perplexity=412.7962, train_loss=6.022954

Batch 36030, train_perplexity=420.7705, train_loss=6.0420876

Batch 36040, train_perplexity=438.57776, train_loss=6.083537

Batch 36050, train_perplexity=466.42477, train_loss=6.145097

Batch 36060, train_perplexity=420.76266, train_loss=6.042069

Batch 36070, train_perplexity=375.6882, train_loss=5.9287596

Batch 36080, train_perplexity=367.3669, train_loss=5.906361

Batch 36090, train_perplexity=425.20334, train_loss=6.0525675

Batch 36100, train_perplexity=353.79483, train_loss=5.868717

Batch 36110, train_perplexity=408.93472, train_loss=6.0135555

Batch 36120, train_perplexity=397.91635, train_loss=5.986242

Batch 36130, train_perplexity=425.73325, train_loss=6.053813

Batch 36140, train_perplexity=414.79102, train_loss=6.027775

Batch 36150, train_perplexity=417.9143, train_loss=6.0352764

Batch 36160, train_perplexity=422.40988, train_loss=6.045976

Batch 36170, train_perplexity=407.22742, train_loss=6.0093718

Batch 36180, train_perplexity=366.08774, train_loss=5.902873

Batch 36190, train_perplexity=376.45535, train_loss=5.9307995

Batch 36200, train_perplexity=486.2657, train_loss=6.186755

Batch 36210, train_perplexity=416.6057, train_loss=6.0321403

Batch 36220, train_perplexity=392.71286, train_loss=5.9730787

Batch 36230, train_perplexity=428.92584, train_loss=6.061284

Batch 36240, train_perplexity=426.74625, train_loss=6.0561895

Batch 36250, train_perplexity=390.53772, train_loss=5.9675245

Batch 36260, train_perplexity=417.0611, train_loss=6.0332327

Batch 36270, train_perplexity=437.5419, train_loss=6.0811725

Batch 36280, train_perplexity=438.49286, train_loss=6.0833435

Batch 36290, train_perplexity=413.41434, train_loss=6.0244503

Batch 36300, train_perplexity=414.24548, train_loss=6.0264587

Batch 36310, train_perplexity=404.66898, train_loss=6.0030694

Batch 36320, train_perplexity=412.54077, train_loss=6.022335

Batch 36330, train_perplexity=388.1206, train_loss=5.961316

Batch 36340, train_perplexity=407.73724, train_loss=6.010623

Batch 36350, train_perplexity=389.60175, train_loss=5.965125

Batch 36360, train_perplexity=455.8087, train_loss=6.122073

Batch 36370, train_perplexity=396.5535, train_loss=5.982811

Batch 36380, train_perplexity=435.91132, train_loss=6.077439

Batch 36390, train_perplexity=415.9453, train_loss=6.030554

Batch 36400, train_perplexity=408.09827, train_loss=6.011508

Batch 36410, train_perplexity=379.78354, train_loss=5.9396014

Batch 36420, train_perplexity=459.92566, train_loss=6.131065

Batch 36430, train_perplexity=426.61337, train_loss=6.055878

Batch 36440, train_perplexity=402.2369, train_loss=5.997041

Batch 36450, train_perplexity=403.17072, train_loss=5.99936

Batch 36460, train_perplexity=383.46317, train_loss=5.9492435

Batch 36470, train_perplexity=416.59143, train_loss=6.032106

Batch 36480, train_perplexity=415.3971, train_loss=6.029235

Batch 36490, train_perplexity=470.42517, train_loss=6.153637

Batch 36500, train_perplexity=422.64783, train_loss=6.0465393

Batch 36510, train_perplexity=399.9205, train_loss=5.991266

Batch 36520, train_perplexity=412.57382, train_loss=6.022415

Batch 36530, train_perplexity=413.83957, train_loss=6.0254784

Batch 36540, train_perplexity=369.08957, train_loss=5.9110394

Batch 36550, train_perplexity=442.51483, train_loss=6.092474

Batch 36560, train_perplexity=407.99048, train_loss=6.011244

Batch 36570, train_perplexity=480.79147, train_loss=6.1754336

Batch 36580, train_perplexity=418.33618, train_loss=6.0362854

Batch 36590, train_perplexity=402.932, train_loss=5.998768

Batch 36600, train_perplexity=385.2174, train_loss=5.953808

Batch 36610, train_perplexity=360.72086, train_loss=5.8881044

Batch 36620, train_perplexity=418.5443, train_loss=6.0367827

Batch 36630, train_perplexity=381.8345, train_loss=5.9449873

Batch 36640, train_perplexity=408.319, train_loss=6.0120487

Batch 36650, train_perplexity=373.1569, train_loss=5.921999

Batch 36660, train_perplexity=395.58408, train_loss=5.9803634

Batch 36670, train_perplexity=381.41888, train_loss=5.943898

Batch 36680, train_perplexity=418.36572, train_loss=6.036356

Batch 36690, train_perplexity=425.5009, train_loss=6.053267

Batch 36700, train_perplexity=379.11786, train_loss=5.937847

Batch 36710, train_perplexity=406.13873, train_loss=6.006695

Batch 36720, train_perplexity=373.72797, train_loss=5.923528

Batch 36730, train_perplexity=427.97708, train_loss=6.0590696

Batch 36740, train_perplexity=372.8316, train_loss=5.921127

Batch 36750, train_perplexity=421.9664, train_loss=6.0449257

Batch 36760, train_perplexity=436.56012, train_loss=6.078926

Batch 36770, train_perplexity=369.8196, train_loss=5.9130154

Batch 36780, train_perplexity=373.705, train_loss=5.9234667

Batch 36790, train_perplexity=474.69547, train_loss=6.1626735

Batch 36800, train_perplexity=440.95654, train_loss=6.0889463

Batch 36810, train_perplexity=431.4588, train_loss=6.067172

Batch 36820, train_perplexity=392.03146, train_loss=5.971342

Batch 36830, train_perplexity=430.08463, train_loss=6.063982

Batch 36840, train_perplexity=408.39456, train_loss=6.0122337

Batch 36850, train_perplexity=466.8244, train_loss=6.145953

Batch 36860, train_perplexity=440.79532, train_loss=6.0885806

Batch 36870, train_perplexity=462.5486, train_loss=6.1367517

Batch 36880, train_perplexity=393.09937, train_loss=5.9740624

Batch 36890, train_perplexity=428.64185, train_loss=6.0606217

Batch 36900, train_perplexity=417.598, train_loss=6.034519

Batch 36910, train_perplexity=388.33533, train_loss=5.9618692

Batch 36920, train_perplexity=461.41476, train_loss=6.1342974

Batch 36930, train_perplexity=423.62723, train_loss=6.048854

Batch 36940, train_perplexity=418.57764, train_loss=6.0368624

Batch 36950, train_perplexity=398.03192, train_loss=5.986532

Batch 36960, train_perplexity=439.05902, train_loss=6.084634

Batch 36970, train_perplexity=412.2289, train_loss=6.021579

Batch 36980, train_perplexity=431.24902, train_loss=6.0666857

Batch 36990, train_perplexity=408.70352, train_loss=6.01299

Batch 37000, train_perplexity=375.96185, train_loss=5.9294877

Batch 37010, train_perplexity=411.23743, train_loss=6.0191708

Batch 37020, train_perplexity=412.42236, train_loss=6.022048

Batch 37030, train_perplexity=436.2979, train_loss=6.0783253

Batch 37040, train_perplexity=424.69046, train_loss=6.0513606

Batch 37050, train_perplexity=408.17105, train_loss=6.0116863

Batch 37060, train_perplexity=439.7385, train_loss=6.08618

Batch 37070, train_perplexity=426.3053, train_loss=6.0551558

Batch 37080, train_perplexity=417.60754, train_loss=6.034542

Batch 37090, train_perplexity=425.5244, train_loss=6.0533223

Batch 37100, train_perplexity=400.81873, train_loss=5.9935093

Batch 37110, train_perplexity=432.48337, train_loss=6.069544

Batch 37120, train_perplexity=404.24988, train_loss=6.002033

Batch 37130, train_perplexity=430.75616, train_loss=6.065542

Batch 37140, train_perplexity=432.3646, train_loss=6.069269

Batch 37150, train_perplexity=393.81213, train_loss=5.975874

Batch 37160, train_perplexity=417.63004, train_loss=6.034596

Batch 37170, train_perplexity=354.9669, train_loss=5.8720245
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37180, train_perplexity=430.0055, train_loss=6.063798

Batch 37190, train_perplexity=463.64478, train_loss=6.1391187

Batch 37200, train_perplexity=404.0533, train_loss=6.001547

Batch 37210, train_perplexity=409.52267, train_loss=6.014992

Batch 37220, train_perplexity=461.6531, train_loss=6.134814

Batch 37230, train_perplexity=419.47656, train_loss=6.0390077

Batch 37240, train_perplexity=404.8201, train_loss=6.003443

Batch 37250, train_perplexity=419.6218, train_loss=6.039354

Batch 37260, train_perplexity=426.9526, train_loss=6.056673

Batch 37270, train_perplexity=422.53702, train_loss=6.046277

Batch 37280, train_perplexity=442.34882, train_loss=6.0920987

Batch 37290, train_perplexity=352.42123, train_loss=5.864827

Batch 37300, train_perplexity=453.6019, train_loss=6.11722

Batch 37310, train_perplexity=435.84274, train_loss=6.0772815

Batch 37320, train_perplexity=421.23944, train_loss=6.0432014

Batch 37330, train_perplexity=408.88675, train_loss=6.013438

Batch 37340, train_perplexity=368.66794, train_loss=5.9098964

Batch 37350, train_perplexity=380.52185, train_loss=5.9415436

Batch 37360, train_perplexity=401.3466, train_loss=5.9948254

Batch 37370, train_perplexity=410.85956, train_loss=6.0182514

Batch 37380, train_perplexity=384.79092, train_loss=5.9527

Batch 37390, train_perplexity=387.8514, train_loss=5.9606223

Batch 37400, train_perplexity=393.96277, train_loss=5.9762564

Batch 37410, train_perplexity=405.84293, train_loss=6.005966

Batch 37420, train_perplexity=428.7778, train_loss=6.060939

Batch 37430, train_perplexity=361.0771, train_loss=5.8890915

Batch 37440, train_perplexity=394.20477, train_loss=5.9768705

Batch 37450, train_perplexity=423.3314, train_loss=6.0481553

Batch 37460, train_perplexity=396.86545, train_loss=5.9835973

Batch 37470, train_perplexity=410.26556, train_loss=6.0168047

Batch 37480, train_perplexity=405.23108, train_loss=6.0044575

Batch 37490, train_perplexity=445.2121, train_loss=6.098551

Batch 37500, train_perplexity=425.4889, train_loss=6.053239

Batch 37510, train_perplexity=419.5352, train_loss=6.0391474

Batch 37520, train_perplexity=397.2693, train_loss=5.9846144

Batch 37530, train_perplexity=411.1539, train_loss=6.0189676

Batch 37540, train_perplexity=450.6554, train_loss=6.110703

Batch 37550, train_perplexity=465.3112, train_loss=6.1427064

Batch 37560, train_perplexity=435.05225, train_loss=6.075466

Batch 37570, train_perplexity=430.1956, train_loss=6.06424

Batch 37580, train_perplexity=429.4692, train_loss=6.06255

Batch 37590, train_perplexity=415.82196, train_loss=6.030257

Batch 37600, train_perplexity=370.31866, train_loss=5.914364

Batch 37610, train_perplexity=372.59808, train_loss=5.9205003

Batch 37620, train_perplexity=379.33792, train_loss=5.9384274

Batch 37630, train_perplexity=390.10776, train_loss=5.966423

Batch 37640, train_perplexity=398.323, train_loss=5.987263

Batch 37650, train_perplexity=401.87054, train_loss=5.99613

Batch 37660, train_perplexity=406.3894, train_loss=6.007312

Batch 37670, train_perplexity=399.68887, train_loss=5.9906864

Batch 37680, train_perplexity=370.55606, train_loss=5.9150047

Batch 37690, train_perplexity=391.39398, train_loss=5.9697146

Batch 37700, train_perplexity=441.73227, train_loss=6.090704

Batch 37710, train_perplexity=438.01764, train_loss=6.082259

Batch 37720, train_perplexity=358.96707, train_loss=5.8832307

Batch 37730, train_perplexity=360.8635, train_loss=5.8884997

Batch 37740, train_perplexity=416.1703, train_loss=6.0310946

Batch 37750, train_perplexity=422.80426, train_loss=6.0469093

Batch 37760, train_perplexity=417.23373, train_loss=6.0336466

Batch 37770, train_perplexity=368.8605, train_loss=5.9104185

Batch 37780, train_perplexity=409.9468, train_loss=6.0160275

Batch 37790, train_perplexity=363.77426, train_loss=5.8965335

Batch 37800, train_perplexity=397.40192, train_loss=5.984948

Batch 37810, train_perplexity=384.7104, train_loss=5.952491

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00043-of-00100
Loaded 306300 sentences.
Finished loading
Batch 37820, train_perplexity=429.71072, train_loss=6.0631123

Batch 37830, train_perplexity=433.92645, train_loss=6.072875

Batch 37840, train_perplexity=385.51288, train_loss=5.9545746

Batch 37850, train_perplexity=361.1191, train_loss=5.889208

Batch 37860, train_perplexity=421.3005, train_loss=6.0433464

Batch 37870, train_perplexity=380.3767, train_loss=5.941162

Batch 37880, train_perplexity=364.9959, train_loss=5.899886

Batch 37890, train_perplexity=377.41083, train_loss=5.9333344

Batch 37900, train_perplexity=359.8399, train_loss=5.885659

Batch 37910, train_perplexity=420.85757, train_loss=6.0422945

Batch 37920, train_perplexity=383.1667, train_loss=5.94847

Batch 37930, train_perplexity=385.35242, train_loss=5.9541583

Batch 37940, train_perplexity=380.48193, train_loss=5.9414387

Batch 37950, train_perplexity=388.8028, train_loss=5.9630723

Batch 37960, train_perplexity=428.75388, train_loss=6.060883

Batch 37970, train_perplexity=430.18698, train_loss=6.06422

Batch 37980, train_perplexity=451.87097, train_loss=6.1133966

Batch 37990, train_perplexity=399.5471, train_loss=5.9903316

Batch 38000, train_perplexity=398.11697, train_loss=5.986746

Batch 38010, train_perplexity=390.01813, train_loss=5.966193

Batch 38020, train_perplexity=381.40543, train_loss=5.943863

Batch 38030, train_perplexity=409.95856, train_loss=6.016056

Batch 38040, train_perplexity=407.58875, train_loss=6.0102587

Batch 38050, train_perplexity=408.02588, train_loss=6.0113306

Batch 38060, train_perplexity=437.35355, train_loss=6.080742

Batch 38070, train_perplexity=421.0061, train_loss=6.0426474

Batch 38080, train_perplexity=442.95987, train_loss=6.093479

Batch 38090, train_perplexity=399.1217, train_loss=5.9892664

Batch 38100, train_perplexity=423.28357, train_loss=6.0480423

Batch 38110, train_perplexity=378.96603, train_loss=5.9374466

Batch 38120, train_perplexity=417.32565, train_loss=6.033867

Batch 38130, train_perplexity=411.55856, train_loss=6.0199513

Batch 38140, train_perplexity=406.23248, train_loss=6.0069256

Batch 38150, train_perplexity=371.4179, train_loss=5.917328

Batch 38160, train_perplexity=426.30103, train_loss=6.0551457

Batch 38170, train_perplexity=403.37186, train_loss=5.999859

Batch 38180, train_perplexity=409.36218, train_loss=6.0146003

Batch 38190, train_perplexity=426.21222, train_loss=6.0549374

Batch 38200, train_perplexity=394.16513, train_loss=5.97677

Batch 38210, train_perplexity=373.76666, train_loss=5.9236317

Batch 38220, train_perplexity=395.22906, train_loss=5.9794655

Batch 38230, train_perplexity=392.84885, train_loss=5.973425

Batch 38240, train_perplexity=467.03903, train_loss=6.146413

Batch 38250, train_perplexity=444.49893, train_loss=6.0969477

Batch 38260, train_perplexity=389.61737, train_loss=5.965165

Batch 38270, train_perplexity=395.25055, train_loss=5.97952

Batch 38280, train_perplexity=455.4984, train_loss=6.1213923

Batch 38290, train_perplexity=409.85614, train_loss=6.015806

Batch 38300, train_perplexity=415.8194, train_loss=6.030251

Batch 38310, train_perplexity=389.9508, train_loss=5.9660206

Batch 38320, train_perplexity=432.31305, train_loss=6.06915

Batch 38330, train_perplexity=386.32404, train_loss=5.9566765

Batch 38340, train_perplexity=374.69028, train_loss=5.9261

Batch 38350, train_perplexity=358.79852, train_loss=5.882761

Batch 38360, train_perplexity=424.5275, train_loss=6.0509768

Batch 38370, train_perplexity=368.9122, train_loss=5.9105587

Batch 38380, train_perplexity=412.5809, train_loss=6.0224323

Batch 38390, train_perplexity=406.62045, train_loss=6.00788

Batch 38400, train_perplexity=434.541, train_loss=6.0742903

Batch 38410, train_perplexity=371.94235, train_loss=5.918739

Batch 38420, train_perplexity=392.7758, train_loss=5.973239

Batch 38430, train_perplexity=380.98392, train_loss=5.942757
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38440, train_perplexity=445.7087, train_loss=6.0996656

Batch 38450, train_perplexity=382.65106, train_loss=5.9471235

Batch 38460, train_perplexity=356.11633, train_loss=5.8752575

Batch 38470, train_perplexity=427.24997, train_loss=6.057369

Batch 38480, train_perplexity=425.267, train_loss=6.052717

Batch 38490, train_perplexity=384.9739, train_loss=5.9531755

Batch 38500, train_perplexity=395.92963, train_loss=5.9812365

Batch 38510, train_perplexity=438.84048, train_loss=6.084136

Batch 38520, train_perplexity=372.36026, train_loss=5.919862

Batch 38530, train_perplexity=397.7496, train_loss=5.9858227

Batch 38540, train_perplexity=411.22333, train_loss=6.0191364

Batch 38550, train_perplexity=427.22552, train_loss=6.057312

Batch 38560, train_perplexity=397.18198, train_loss=5.9843946

Batch 38570, train_perplexity=391.78665, train_loss=5.9707174

Batch 38580, train_perplexity=427.73227, train_loss=6.0584974

Batch 38590, train_perplexity=385.0242, train_loss=5.953306

Batch 38600, train_perplexity=355.4013, train_loss=5.8732476

Batch 38610, train_perplexity=393.44556, train_loss=5.9749427

Batch 38620, train_perplexity=365.28668, train_loss=5.9006824

Batch 38630, train_perplexity=375.96365, train_loss=5.9294925

Batch 38640, train_perplexity=403.47574, train_loss=6.0001163

Batch 38650, train_perplexity=435.7567, train_loss=6.077084

Batch 38660, train_perplexity=350.20966, train_loss=5.858532

Batch 38670, train_perplexity=415.70502, train_loss=6.029976

Batch 38680, train_perplexity=385.9256, train_loss=5.9556446

Batch 38690, train_perplexity=416.50342, train_loss=6.0318947

Batch 38700, train_perplexity=364.36746, train_loss=5.898163

Batch 38710, train_perplexity=396.5584, train_loss=5.9828234

Batch 38720, train_perplexity=392.09576, train_loss=5.971506

Batch 38730, train_perplexity=405.14838, train_loss=6.0042534

Batch 38740, train_perplexity=412.9722, train_loss=6.0233803

Batch 38750, train_perplexity=403.76828, train_loss=6.000841

Batch 38760, train_perplexity=377.2633, train_loss=5.9329433

Batch 38770, train_perplexity=373.84686, train_loss=5.9238462

Batch 38780, train_perplexity=404.77417, train_loss=6.0033293

Batch 38790, train_perplexity=397.54596, train_loss=5.9853106

Batch 38800, train_perplexity=405.28906, train_loss=6.0046005

Batch 38810, train_perplexity=439.31934, train_loss=6.0852265

Batch 38820, train_perplexity=420.18784, train_loss=6.040702

Batch 38830, train_perplexity=376.99426, train_loss=5.93223

Batch 38840, train_perplexity=413.85733, train_loss=6.0255213

Batch 38850, train_perplexity=411.84204, train_loss=6.02064

Batch 38860, train_perplexity=383.5736, train_loss=5.9495316

Batch 38870, train_perplexity=373.2749, train_loss=5.922315

Batch 38880, train_perplexity=405.15765, train_loss=6.0042763

Batch 38890, train_perplexity=417.54224, train_loss=6.0343857

Batch 38900, train_perplexity=415.9144, train_loss=6.0304794

Batch 38910, train_perplexity=407.74347, train_loss=6.010638

Batch 38920, train_perplexity=422.02878, train_loss=6.0450735

Batch 38930, train_perplexity=415.5558, train_loss=6.029617

Batch 38940, train_perplexity=391.41843, train_loss=5.969777

Batch 38950, train_perplexity=382.4661, train_loss=5.94664

Batch 38960, train_perplexity=366.97192, train_loss=5.9052854

Batch 38970, train_perplexity=395.89487, train_loss=5.9811487

Batch 38980, train_perplexity=411.81494, train_loss=6.020574

Batch 38990, train_perplexity=393.80368, train_loss=5.9758525

Batch 39000, train_perplexity=462.79504, train_loss=6.1372843

Batch 39010, train_perplexity=393.7158, train_loss=5.9756293

Batch 39020, train_perplexity=412.54865, train_loss=6.022354

Batch 39030, train_perplexity=339.33417, train_loss=5.8269854

Batch 39040, train_perplexity=379.74152, train_loss=5.939491

Batch 39050, train_perplexity=395.4979, train_loss=5.9801455

Batch 39060, train_perplexity=392.95975, train_loss=5.973707

Batch 39070, train_perplexity=377.13956, train_loss=5.9326153

Batch 39080, train_perplexity=408.23062, train_loss=6.011832

Batch 39090, train_perplexity=416.3553, train_loss=6.031539

Batch 39100, train_perplexity=383.38892, train_loss=5.94905

Batch 39110, train_perplexity=387.5826, train_loss=5.959929

Batch 39120, train_perplexity=402.05072, train_loss=5.996578

Batch 39130, train_perplexity=396.30624, train_loss=5.9821873

Batch 39140, train_perplexity=383.49716, train_loss=5.949332

Batch 39150, train_perplexity=381.57242, train_loss=5.9443007

Batch 39160, train_perplexity=397.23898, train_loss=5.984538

Batch 39170, train_perplexity=387.94925, train_loss=5.9608746

Batch 39180, train_perplexity=413.11026, train_loss=6.0237145

Batch 39190, train_perplexity=422.15378, train_loss=6.0453696

Batch 39200, train_perplexity=400.8493, train_loss=5.9935856

Batch 39210, train_perplexity=428.7647, train_loss=6.0609083

Batch 39220, train_perplexity=401.63873, train_loss=5.995553

Batch 39230, train_perplexity=392.14813, train_loss=5.9716396

Batch 39240, train_perplexity=413.55588, train_loss=6.0247927

Batch 39250, train_perplexity=411.82065, train_loss=6.020588

Batch 39260, train_perplexity=417.12274, train_loss=6.0333805

Batch 39270, train_perplexity=363.81918, train_loss=5.896657

Batch 39280, train_perplexity=426.11386, train_loss=6.0547066

Batch 39290, train_perplexity=400.55966, train_loss=5.9928627

Batch 39300, train_perplexity=377.0831, train_loss=5.9324656

Batch 39310, train_perplexity=426.52612, train_loss=6.0556736

Batch 39320, train_perplexity=367.57367, train_loss=5.906924

Batch 39330, train_perplexity=415.8664, train_loss=6.030364

Batch 39340, train_perplexity=404.50076, train_loss=6.0026536

Batch 39350, train_perplexity=371.6679, train_loss=5.9180007

Batch 39360, train_perplexity=450.4352, train_loss=6.110214

Batch 39370, train_perplexity=429.67117, train_loss=6.06302

Batch 39380, train_perplexity=378.88925, train_loss=5.937244

Batch 39390, train_perplexity=352.2969, train_loss=5.8644743

Batch 39400, train_perplexity=432.53616, train_loss=6.069666

Batch 39410, train_perplexity=423.66278, train_loss=6.048938

Batch 39420, train_perplexity=410.73145, train_loss=6.0179396

Batch 39430, train_perplexity=406.5898, train_loss=6.007805

Batch 39440, train_perplexity=423.6353, train_loss=6.048873

Batch 39450, train_perplexity=353.78506, train_loss=5.8686895

Batch 39460, train_perplexity=398.8523, train_loss=5.988591

Batch 39470, train_perplexity=415.62375, train_loss=6.0297804

Batch 39480, train_perplexity=437.71365, train_loss=6.081565

Batch 39490, train_perplexity=409.34247, train_loss=6.014552

Batch 39500, train_perplexity=361.45178, train_loss=5.8901286

Batch 39510, train_perplexity=386.0007, train_loss=5.955839

Batch 39520, train_perplexity=371.82513, train_loss=5.9184237

Batch 39530, train_perplexity=410.19162, train_loss=6.0166245

Batch 39540, train_perplexity=379.29868, train_loss=5.938324

Batch 39550, train_perplexity=471.97348, train_loss=6.156923

Batch 39560, train_perplexity=376.0133, train_loss=5.9296246

Batch 39570, train_perplexity=383.07242, train_loss=5.948224

Batch 39580, train_perplexity=389.07935, train_loss=5.9637833

Batch 39590, train_perplexity=390.4003, train_loss=5.9671726

Batch 39600, train_perplexity=355.99817, train_loss=5.8749256

Batch 39610, train_perplexity=409.19843, train_loss=6.0142

Batch 39620, train_perplexity=346.34024, train_loss=5.8474216

Batch 39630, train_perplexity=404.41803, train_loss=6.002449

Batch 39640, train_perplexity=388.18686, train_loss=5.961487

Batch 39650, train_perplexity=357.2242, train_loss=5.8783636

Batch 39660, train_perplexity=379.22073, train_loss=5.9381185

Batch 39670, train_perplexity=369.91733, train_loss=5.9132795

Batch 39680, train_perplexity=380.15985, train_loss=5.940592

Batch 39690, train_perplexity=417.95874, train_loss=6.0353827

Batch 39700, train_perplexity=372.75302, train_loss=5.920916

Batch 39710, train_perplexity=385.5101, train_loss=5.9545674

Batch 39720, train_perplexity=398.81293, train_loss=5.9884925

Batch 39730, train_perplexity=423.24197, train_loss=6.047944

Batch 39740, train_perplexity=364.5995, train_loss=5.8987994

Batch 39750, train_perplexity=404.81198, train_loss=6.0034227
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 39760, train_perplexity=370.21484, train_loss=5.9140835

Batch 39770, train_perplexity=418.94086, train_loss=6.0377297

Batch 39780, train_perplexity=383.01544, train_loss=5.9480753

Batch 39790, train_perplexity=397.29504, train_loss=5.984679

Batch 39800, train_perplexity=371.81876, train_loss=5.9184065

Batch 39810, train_perplexity=397.9636, train_loss=5.9863605

Batch 39820, train_perplexity=381.3254, train_loss=5.943653

Batch 39830, train_perplexity=457.61667, train_loss=6.126032

Batch 39840, train_perplexity=352.6956, train_loss=5.8656054

Batch 39850, train_perplexity=367.5784, train_loss=5.9069366

Batch 39860, train_perplexity=428.9876, train_loss=6.061428

Batch 39870, train_perplexity=432.70923, train_loss=6.070066

Batch 39880, train_perplexity=375.98965, train_loss=5.9295616

Batch 39890, train_perplexity=418.38705, train_loss=6.036407

Batch 39900, train_perplexity=382.94568, train_loss=5.947893

Batch 39910, train_perplexity=399.66144, train_loss=5.9906178

Batch 39920, train_perplexity=409.22125, train_loss=6.014256

Batch 39930, train_perplexity=447.30554, train_loss=6.103242

Batch 39940, train_perplexity=399.89285, train_loss=5.9911966

Batch 39950, train_perplexity=407.8065, train_loss=6.0107927

Batch 39960, train_perplexity=413.13824, train_loss=6.0237823

Batch 39970, train_perplexity=399.21497, train_loss=5.9895

Batch 39980, train_perplexity=382.90662, train_loss=5.947791

Batch 39990, train_perplexity=358.90393, train_loss=5.8830547

Batch 40000, train_perplexity=429.48868, train_loss=6.0625954

Batch 40010, train_perplexity=386.28442, train_loss=5.956574

Batch 40020, train_perplexity=399.27628, train_loss=5.9896536

Batch 40030, train_perplexity=354.97925, train_loss=5.8720593

Batch 40040, train_perplexity=375.8226, train_loss=5.929117

Batch 40050, train_perplexity=376.93207, train_loss=5.932065

Batch 40060, train_perplexity=379.24823, train_loss=5.938191

Batch 40070, train_perplexity=356.78467, train_loss=5.8771324

Batch 40080, train_perplexity=403.56192, train_loss=6.00033

Batch 40090, train_perplexity=408.90274, train_loss=6.0134773

Batch 40100, train_perplexity=364.4387, train_loss=5.8983583

Batch 40110, train_perplexity=411.69772, train_loss=6.0202894

Batch 40120, train_perplexity=373.56067, train_loss=5.9230804

Batch 40130, train_perplexity=405.805, train_loss=6.0058727

Batch 40140, train_perplexity=384.23776, train_loss=5.9512615

Batch 40150, train_perplexity=400.04257, train_loss=5.991571

Batch 40160, train_perplexity=395.87146, train_loss=5.9810896

Batch 40170, train_perplexity=353.5729, train_loss=5.8680897

Batch 40180, train_perplexity=442.7487, train_loss=6.0930023

Batch 40190, train_perplexity=464.22437, train_loss=6.140368

Batch 40200, train_perplexity=353.13278, train_loss=5.866844

Batch 40210, train_perplexity=403.24646, train_loss=5.999548

Batch 40220, train_perplexity=350.43515, train_loss=5.8591757

Batch 40230, train_perplexity=396.94095, train_loss=5.9837875

Batch 40240, train_perplexity=352.12408, train_loss=5.8639836

Batch 40250, train_perplexity=467.72122, train_loss=6.1478724

Batch 40260, train_perplexity=377.35504, train_loss=5.9331865

Batch 40270, train_perplexity=418.81082, train_loss=6.0374193

Batch 40280, train_perplexity=372.43448, train_loss=5.920061

Batch 40290, train_perplexity=441.73373, train_loss=6.0907073

Batch 40300, train_perplexity=334.18448, train_loss=5.811693

Batch 40310, train_perplexity=408.93433, train_loss=6.0135546

Batch 40320, train_perplexity=396.16605, train_loss=5.9818335

Batch 40330, train_perplexity=389.51874, train_loss=5.964912

Batch 40340, train_perplexity=352.8862, train_loss=5.8661456

Batch 40350, train_perplexity=401.236, train_loss=5.9945498

Batch 40360, train_perplexity=372.41245, train_loss=5.920002

Batch 40370, train_perplexity=373.38046, train_loss=5.922598

Batch 40380, train_perplexity=354.59793, train_loss=5.8709846

Batch 40390, train_perplexity=378.60226, train_loss=5.9364862

Batch 40400, train_perplexity=391.18332, train_loss=5.9691763

Batch 40410, train_perplexity=410.32034, train_loss=6.016938

Batch 40420, train_perplexity=336.80585, train_loss=5.8195066

Batch 40430, train_perplexity=381.5957, train_loss=5.9443617

Batch 40440, train_perplexity=382.8489, train_loss=5.9476404

Batch 40450, train_perplexity=407.97607, train_loss=6.0112085

Batch 40460, train_perplexity=360.41483, train_loss=5.8872557

Batch 40470, train_perplexity=390.5623, train_loss=5.9675875

Batch 40480, train_perplexity=400.0393, train_loss=5.991563

Batch 40490, train_perplexity=391.89352, train_loss=5.97099

Batch 40500, train_perplexity=339.31363, train_loss=5.826925

Batch 40510, train_perplexity=377.36404, train_loss=5.9332104

Batch 40520, train_perplexity=446.61502, train_loss=6.101697

Batch 40530, train_perplexity=386.67346, train_loss=5.9575806

Batch 40540, train_perplexity=348.26675, train_loss=5.8529687

Batch 40550, train_perplexity=375.81973, train_loss=5.9291096

Batch 40560, train_perplexity=351.84833, train_loss=5.8632

Batch 40570, train_perplexity=405.05258, train_loss=6.004017

Batch 40580, train_perplexity=382.1963, train_loss=5.9459343

Batch 40590, train_perplexity=403.9104, train_loss=6.001193

Batch 40600, train_perplexity=439.18423, train_loss=6.084919

Batch 40610, train_perplexity=372.16037, train_loss=5.919325

Batch 40620, train_perplexity=431.9683, train_loss=6.068352

Batch 40630, train_perplexity=413.9578, train_loss=6.025764

Batch 40640, train_perplexity=436.78415, train_loss=6.079439

Batch 40650, train_perplexity=307.32468, train_loss=5.727905

Batch 40660, train_perplexity=373.99094, train_loss=5.9242315

Batch 40670, train_perplexity=389.99115, train_loss=5.966124

Batch 40680, train_perplexity=409.34128, train_loss=6.0145493

Batch 40690, train_perplexity=379.08206, train_loss=5.9377527

Batch 40700, train_perplexity=416.28104, train_loss=6.0313606

Batch 40710, train_perplexity=366.22394, train_loss=5.903245

Batch 40720, train_perplexity=390.8352, train_loss=5.968286

Batch 40730, train_perplexity=409.0841, train_loss=6.013921

Batch 40740, train_perplexity=435.61377, train_loss=6.076756

Batch 40750, train_perplexity=345.35635, train_loss=5.844577

Batch 40760, train_perplexity=388.2583, train_loss=5.961671

Batch 40770, train_perplexity=393.2109, train_loss=5.974346

Batch 40780, train_perplexity=390.25922, train_loss=5.966811

Batch 40790, train_perplexity=370.72397, train_loss=5.9154577

Batch 40800, train_perplexity=434.2746, train_loss=6.073677

Batch 40810, train_perplexity=395.41528, train_loss=5.9799366

Batch 40820, train_perplexity=360.06183, train_loss=5.886276

Batch 40830, train_perplexity=407.8189, train_loss=6.0108232

Batch 40840, train_perplexity=408.39612, train_loss=6.0122375

Batch 40850, train_perplexity=425.29254, train_loss=6.0527773

Batch 40860, train_perplexity=421.8423, train_loss=6.0446315

Batch 40870, train_perplexity=378.55426, train_loss=5.9363594

Batch 40880, train_perplexity=399.06308, train_loss=5.9891195

Batch 40890, train_perplexity=340.45523, train_loss=5.8302836

Batch 40900, train_perplexity=371.6204, train_loss=5.917873

Batch 40910, train_perplexity=374.07474, train_loss=5.9244556

Batch 40920, train_perplexity=394.7375, train_loss=5.978221

Batch 40930, train_perplexity=384.0491, train_loss=5.9507704

Batch 40940, train_perplexity=409.80746, train_loss=6.0156875

Batch 40950, train_perplexity=377.64307, train_loss=5.9339495

Batch 40960, train_perplexity=384.5677, train_loss=5.95212

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Batch 40970, train_perplexity=376.2791, train_loss=5.930331

Batch 40980, train_perplexity=380.83243, train_loss=5.9423594

Batch 40990, train_perplexity=434.16693, train_loss=6.073429

Batch 41000, train_perplexity=397.97083, train_loss=5.9863787

Batch 41010, train_perplexity=379.7991, train_loss=5.9396424
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 41020, train_perplexity=392.10867, train_loss=5.971539

Batch 41030, train_perplexity=368.3824, train_loss=5.9091215

Batch 41040, train_perplexity=418.7473, train_loss=6.0372677

Batch 41050, train_perplexity=402.489, train_loss=5.997668

Batch 41060, train_perplexity=409.68457, train_loss=6.0153875

Batch 41070, train_perplexity=417.27872, train_loss=6.0337543

Batch 41080, train_perplexity=359.29913, train_loss=5.8841553

Batch 41090, train_perplexity=371.61685, train_loss=5.9178634

Batch 41100, train_perplexity=386.34393, train_loss=5.956728

Batch 41110, train_perplexity=415.8402, train_loss=6.030301

Batch 41120, train_perplexity=405.4553, train_loss=6.0050106

Batch 41130, train_perplexity=356.6979, train_loss=5.876889

Batch 41140, train_perplexity=430.25366, train_loss=6.064375

Batch 41150, train_perplexity=368.17905, train_loss=5.9085693

Batch 41160, train_perplexity=388.13208, train_loss=5.9613457

Batch 41170, train_perplexity=369.68878, train_loss=5.9126616

Batch 41180, train_perplexity=392.18964, train_loss=5.9717455

Batch 41190, train_perplexity=383.65536, train_loss=5.9497447

Batch 41200, train_perplexity=412.64465, train_loss=6.022587

Batch 41210, train_perplexity=411.68478, train_loss=6.020258

Batch 41220, train_perplexity=428.76022, train_loss=6.060898

Batch 41230, train_perplexity=434.14023, train_loss=6.0733676

Batch 41240, train_perplexity=356.2376, train_loss=5.875598

Batch 41250, train_perplexity=406.57587, train_loss=6.0077705

Batch 41260, train_perplexity=378.14978, train_loss=5.9352903

Batch 41270, train_perplexity=420.75845, train_loss=6.042059

Batch 41280, train_perplexity=450.68744, train_loss=6.110774

Batch 41290, train_perplexity=373.2797, train_loss=5.922328

Batch 41300, train_perplexity=399.31055, train_loss=5.9897394

Batch 41310, train_perplexity=390.91257, train_loss=5.968484

Batch 41320, train_perplexity=412.5105, train_loss=6.0222616

Batch 41330, train_perplexity=395.62918, train_loss=5.9804773

Batch 41340, train_perplexity=369.44174, train_loss=5.911993

Batch 41350, train_perplexity=385.4274, train_loss=5.954353

Batch 41360, train_perplexity=376.02228, train_loss=5.9296484

Batch 41370, train_perplexity=433.18262, train_loss=6.0711594

Batch 41380, train_perplexity=375.12433, train_loss=5.9272575

Batch 41390, train_perplexity=376.58087, train_loss=5.931133

Batch 41400, train_perplexity=356.6156, train_loss=5.8766584

Batch 41410, train_perplexity=373.48053, train_loss=5.922866

Batch 41420, train_perplexity=372.04965, train_loss=5.9190273

Batch 41430, train_perplexity=406.52197, train_loss=6.007638

Batch 41440, train_perplexity=406.0423, train_loss=6.0064573

Batch 41450, train_perplexity=354.4081, train_loss=5.870449

Batch 41460, train_perplexity=404.14484, train_loss=6.0017734

Batch 41470, train_perplexity=373.97684, train_loss=5.924194

Batch 41480, train_perplexity=377.81668, train_loss=5.934409

Batch 41490, train_perplexity=392.5773, train_loss=5.9727335

Batch 41500, train_perplexity=354.9813, train_loss=5.872065

Batch 41510, train_perplexity=397.18784, train_loss=5.9844093

Batch 41520, train_perplexity=367.01096, train_loss=5.9053917

Batch 41530, train_perplexity=402.42627, train_loss=5.997512

Batch 41540, train_perplexity=363.42245, train_loss=5.895566

Batch 41550, train_perplexity=430.36157, train_loss=6.0646257

Batch 41560, train_perplexity=365.78412, train_loss=5.9020433

Batch 41570, train_perplexity=359.41565, train_loss=5.8844795

Batch 41580, train_perplexity=363.819, train_loss=5.8966565

Batch 41590, train_perplexity=383.87698, train_loss=5.950322

Batch 41600, train_perplexity=366.17642, train_loss=5.9031153

Batch 41610, train_perplexity=369.14236, train_loss=5.9111824

Batch 41620, train_perplexity=442.54184, train_loss=6.092535

Batch 41630, train_perplexity=391.03003, train_loss=5.9687843

Batch 41640, train_perplexity=445.75717, train_loss=6.0997744

Batch 41650, train_perplexity=390.9148, train_loss=5.9684896

Batch 41660, train_perplexity=418.27396, train_loss=6.0361366

Batch 41670, train_perplexity=378.23813, train_loss=5.935524

Batch 41680, train_perplexity=386.1861, train_loss=5.9563193

Batch 41690, train_perplexity=388.36533, train_loss=5.9619465

Batch 41700, train_perplexity=366.13837, train_loss=5.9030113

Batch 41710, train_perplexity=499.11438, train_loss=6.2128353

Batch 41720, train_perplexity=373.00037, train_loss=5.9215794

Batch 41730, train_perplexity=359.07922, train_loss=5.883543

Batch 41740, train_perplexity=373.5785, train_loss=5.923128

Batch 41750, train_perplexity=368.37503, train_loss=5.9091015

Batch 41760, train_perplexity=351.41455, train_loss=5.8619666

Batch 41770, train_perplexity=422.0024, train_loss=6.045011

Batch 41780, train_perplexity=370.75558, train_loss=5.915543

Batch 41790, train_perplexity=342.42932, train_loss=5.8360653

Batch 41800, train_perplexity=417.4437, train_loss=6.0341496

Batch 41810, train_perplexity=350.30984, train_loss=5.858818

Batch 41820, train_perplexity=385.45883, train_loss=5.9544344

Batch 41830, train_perplexity=394.14145, train_loss=5.97671

Batch 41840, train_perplexity=366.39178, train_loss=5.903703

Batch 41850, train_perplexity=376.49377, train_loss=5.9309015

Batch 41860, train_perplexity=352.46747, train_loss=5.8649583

Batch 41870, train_perplexity=393.11945, train_loss=5.9741135

Batch 41880, train_perplexity=369.22934, train_loss=5.911418

Batch 41890, train_perplexity=342.64526, train_loss=5.8366957

Batch 41900, train_perplexity=418.87033, train_loss=6.0375614

Batch 41910, train_perplexity=346.11044, train_loss=5.846758

Batch 41920, train_perplexity=390.65988, train_loss=5.9678373

Batch 41930, train_perplexity=399.74643, train_loss=5.9908304

Batch 41940, train_perplexity=388.01532, train_loss=5.961045

Batch 41950, train_perplexity=350.54745, train_loss=5.859496

Batch 41960, train_perplexity=388.93817, train_loss=5.9634204

Batch 41970, train_perplexity=389.59283, train_loss=5.965102

Batch 41980, train_perplexity=397.59088, train_loss=5.9854236

Batch 41990, train_perplexity=403.48575, train_loss=6.000141

Batch 42000, train_perplexity=374.81717, train_loss=5.9264383

Batch 42010, train_perplexity=420.8917, train_loss=6.0423756

Batch 42020, train_perplexity=410.491, train_loss=6.017354

Batch 42030, train_perplexity=386.0156, train_loss=5.955878

Batch 42040, train_perplexity=390.1859, train_loss=5.9666233

Batch 42050, train_perplexity=364.06024, train_loss=5.8973193

Batch 42060, train_perplexity=378.85547, train_loss=5.937155

Batch 42070, train_perplexity=392.57358, train_loss=5.972724

Batch 42080, train_perplexity=364.35876, train_loss=5.898139

Batch 42090, train_perplexity=389.24634, train_loss=5.9642124

Batch 42100, train_perplexity=401.77588, train_loss=5.9958944

Batch 42110, train_perplexity=368.43546, train_loss=5.9092655

Batch 42120, train_perplexity=404.20593, train_loss=6.0019245

Batch 42130, train_perplexity=417.0364, train_loss=6.0331736

Batch 42140, train_perplexity=397.30908, train_loss=5.9847145

Batch 42150, train_perplexity=362.09332, train_loss=5.891902

Batch 42160, train_perplexity=381.3994, train_loss=5.943847

Batch 42170, train_perplexity=408.4771, train_loss=6.012436

Batch 42180, train_perplexity=351.3901, train_loss=5.861897

Batch 42190, train_perplexity=411.01218, train_loss=6.018623

Batch 42200, train_perplexity=392.73196, train_loss=5.9731274

Batch 42210, train_perplexity=370.4181, train_loss=5.9146323

Batch 42220, train_perplexity=396.85504, train_loss=5.983571

Batch 42230, train_perplexity=419.17163, train_loss=6.0382805

Batch 42240, train_perplexity=338.87155, train_loss=5.825621

Batch 42250, train_perplexity=419.6184, train_loss=6.0393457

Batch 42260, train_perplexity=393.61163, train_loss=5.9753647

Batch 42270, train_perplexity=437.51456, train_loss=6.08111

Batch 42280, train_perplexity=359.2241, train_loss=5.8839464

Batch 42290, train_perplexity=386.5223, train_loss=5.9571896

Batch 42300, train_perplexity=348.82983, train_loss=5.854584

Batch 42310, train_perplexity=349.45398, train_loss=5.856372

Batch 42320, train_perplexity=395.41058, train_loss=5.9799247

Batch 42330, train_perplexity=374.83502, train_loss=5.926486
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42340, train_perplexity=396.77707, train_loss=5.9833746

Batch 42350, train_perplexity=439.01715, train_loss=6.0845385

Batch 42360, train_perplexity=407.9247, train_loss=6.0110826

Batch 42370, train_perplexity=381.95215, train_loss=5.9452953

Batch 42380, train_perplexity=390.60886, train_loss=5.9677067

Batch 42390, train_perplexity=429.20615, train_loss=6.0619373

Batch 42400, train_perplexity=358.88885, train_loss=5.883013

Batch 42410, train_perplexity=361.13116, train_loss=5.889241

Batch 42420, train_perplexity=389.24356, train_loss=5.9642053

Batch 42430, train_perplexity=405.6564, train_loss=6.0055065

Batch 42440, train_perplexity=403.9913, train_loss=6.0013933

Batch 42450, train_perplexity=420.83432, train_loss=6.042239

Batch 42460, train_perplexity=386.60156, train_loss=5.9573946

Batch 42470, train_perplexity=403.34915, train_loss=5.9998026

Batch 42480, train_perplexity=390.31952, train_loss=5.9669657

Batch 42490, train_perplexity=442.7926, train_loss=6.0931015

Batch 42500, train_perplexity=394.88208, train_loss=5.978587

Batch 42510, train_perplexity=410.42505, train_loss=6.0171933

Batch 42520, train_perplexity=397.8414, train_loss=5.9860535

Batch 42530, train_perplexity=383.1243, train_loss=5.9483595

Batch 42540, train_perplexity=373.402, train_loss=5.9226556

Batch 42550, train_perplexity=405.79724, train_loss=6.0058537

Batch 42560, train_perplexity=356.08884, train_loss=5.8751802

Batch 42570, train_perplexity=347.88217, train_loss=5.851864

Batch 42580, train_perplexity=431.2054, train_loss=6.0665846

Batch 42590, train_perplexity=378.73264, train_loss=5.9368305

Batch 42600, train_perplexity=333.69373, train_loss=5.8102236

Batch 42610, train_perplexity=363.17682, train_loss=5.89489

Batch 42620, train_perplexity=420.01556, train_loss=6.040292

Batch 42630, train_perplexity=363.31366, train_loss=5.8952665

Batch 42640, train_perplexity=353.06006, train_loss=5.866638

Batch 42650, train_perplexity=395.33292, train_loss=5.979728

Batch 42660, train_perplexity=358.19696, train_loss=5.881083

Batch 42670, train_perplexity=365.31262, train_loss=5.9007535

Batch 42680, train_perplexity=442.1086, train_loss=6.0915556

Batch 42690, train_perplexity=349.95557, train_loss=5.857806

Batch 42700, train_perplexity=403.71725, train_loss=6.000715

Batch 42710, train_perplexity=405.37582, train_loss=6.0048146

Batch 42720, train_perplexity=407.10934, train_loss=6.009082

Batch 42730, train_perplexity=354.23865, train_loss=5.869971

Batch 42740, train_perplexity=424.44672, train_loss=6.0507865

Batch 42750, train_perplexity=352.51132, train_loss=5.8650827

Batch 42760, train_perplexity=327.02142, train_loss=5.7900257

Batch 42770, train_perplexity=402.6539, train_loss=5.9980774

Batch 42780, train_perplexity=375.18015, train_loss=5.9274063

Batch 42790, train_perplexity=349.9409, train_loss=5.8577642

Batch 42800, train_perplexity=391.93314, train_loss=5.9710913

Batch 42810, train_perplexity=371.07767, train_loss=5.9164114

Batch 42820, train_perplexity=348.75034, train_loss=5.8543563

Batch 42830, train_perplexity=385.65555, train_loss=5.9549446

Batch 42840, train_perplexity=380.16946, train_loss=5.940617

Batch 42850, train_perplexity=351.10605, train_loss=5.8610883

Batch 42860, train_perplexity=390.07056, train_loss=5.9663277

Batch 42870, train_perplexity=449.7287, train_loss=6.1086445

Batch 42880, train_perplexity=381.2127, train_loss=5.9433575

Batch 42890, train_perplexity=334.1617, train_loss=5.811625

Batch 42900, train_perplexity=380.9038, train_loss=5.942547

Batch 42910, train_perplexity=381.97473, train_loss=5.9453545

Batch 42920, train_perplexity=394.16907, train_loss=5.97678

Batch 42930, train_perplexity=411.10883, train_loss=6.018858

Batch 42940, train_perplexity=345.45074, train_loss=5.84485

Batch 42950, train_perplexity=392.88895, train_loss=5.973527

Batch 42960, train_perplexity=414.29782, train_loss=6.026585

Batch 42970, train_perplexity=415.19806, train_loss=6.0287557

Batch 42980, train_perplexity=332.82083, train_loss=5.8076043

Batch 42990, train_perplexity=372.356, train_loss=5.9198503

Batch 43000, train_perplexity=331.22922, train_loss=5.8028107

Batch 43010, train_perplexity=362.36053, train_loss=5.8926396

Batch 43020, train_perplexity=404.01248, train_loss=6.001446

Batch 43030, train_perplexity=373.40057, train_loss=5.922652

Batch 43040, train_perplexity=393.51984, train_loss=5.9751315

Batch 43050, train_perplexity=381.74057, train_loss=5.9447412

Batch 43060, train_perplexity=325.81577, train_loss=5.786332

Batch 43070, train_perplexity=394.6953, train_loss=5.978114

Batch 43080, train_perplexity=361.79578, train_loss=5.89108

Batch 43090, train_perplexity=407.12003, train_loss=6.009108

Batch 43100, train_perplexity=368.4235, train_loss=5.909233

Batch 43110, train_perplexity=404.81506, train_loss=6.0034304

Batch 43120, train_perplexity=392.8938, train_loss=5.9735394

Batch 43130, train_perplexity=401.88165, train_loss=5.9961576

Batch 43140, train_perplexity=361.85995, train_loss=5.8912573

Batch 43150, train_perplexity=366.8472, train_loss=5.9049454

Batch 43160, train_perplexity=333.7911, train_loss=5.8105154

Batch 43170, train_perplexity=367.62643, train_loss=5.9070673

Batch 43180, train_perplexity=405.9639, train_loss=6.006264

Batch 43190, train_perplexity=348.50446, train_loss=5.853651

Batch 43200, train_perplexity=427.03467, train_loss=6.056865

Batch 43210, train_perplexity=382.1941, train_loss=5.9459286

Batch 43220, train_perplexity=372.71802, train_loss=5.920822

Batch 43230, train_perplexity=380.45163, train_loss=5.941359

Batch 43240, train_perplexity=347.5211, train_loss=5.8508253

Batch 43250, train_perplexity=437.53375, train_loss=6.081154

Batch 43260, train_perplexity=394.82257, train_loss=5.9784365

Batch 43270, train_perplexity=391.9664, train_loss=5.971176

Batch 43280, train_perplexity=371.37363, train_loss=5.9172087

Batch 43290, train_perplexity=345.94016, train_loss=5.846266

Batch 43300, train_perplexity=379.4794, train_loss=5.9388003

Batch 43310, train_perplexity=383.9751, train_loss=5.9505777

Batch 43320, train_perplexity=370.4105, train_loss=5.914612

Batch 43330, train_perplexity=366.88498, train_loss=5.9050484

Batch 43340, train_perplexity=332.18204, train_loss=5.805683

Batch 43350, train_perplexity=387.30862, train_loss=5.959222

Batch 43360, train_perplexity=340.13055, train_loss=5.8293295

Batch 43370, train_perplexity=341.67743, train_loss=5.833867

Batch 43380, train_perplexity=390.45224, train_loss=5.9673057

Batch 43390, train_perplexity=364.64243, train_loss=5.898917

Batch 43400, train_perplexity=365.23914, train_loss=5.9005523

Batch 43410, train_perplexity=332.292, train_loss=5.806014

Batch 43420, train_perplexity=401.3535, train_loss=5.9948425

Batch 43430, train_perplexity=395.94, train_loss=5.9812627

Batch 43440, train_perplexity=344.1028, train_loss=5.8409405

Batch 43450, train_perplexity=386.4311, train_loss=5.9569535

Batch 43460, train_perplexity=404.16354, train_loss=6.0018196

Batch 43470, train_perplexity=414.71545, train_loss=6.0275927

Batch 43480, train_perplexity=396.6159, train_loss=5.9829683

Batch 43490, train_perplexity=377.55447, train_loss=5.933715

Batch 43500, train_perplexity=356.38202, train_loss=5.8760033

Batch 43510, train_perplexity=380.0618, train_loss=5.940334

Batch 43520, train_perplexity=404.16623, train_loss=6.0018263

Batch 43530, train_perplexity=374.94962, train_loss=5.9267917

Batch 43540, train_perplexity=397.6582, train_loss=5.985593

Batch 43550, train_perplexity=375.36374, train_loss=5.9278955

Batch 43560, train_perplexity=400.70294, train_loss=5.9932203

Batch 43570, train_perplexity=366.22357, train_loss=5.903244

Batch 43580, train_perplexity=382.69778, train_loss=5.9472456

Batch 43590, train_perplexity=357.00284, train_loss=5.8777437

Batch 43600, train_perplexity=383.47742, train_loss=5.9492807

Batch 43610, train_perplexity=416.48178, train_loss=6.0318427

Batch 43620, train_perplexity=378.0654, train_loss=5.935067

Batch 43630, train_perplexity=383.80524, train_loss=5.950135

Batch 43640, train_perplexity=353.28842, train_loss=5.867285

Batch 43650, train_perplexity=432.48254, train_loss=6.069542
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 43660, train_perplexity=362.00494, train_loss=5.891658

Batch 43670, train_perplexity=401.80347, train_loss=5.995963

Batch 43680, train_perplexity=391.27548, train_loss=5.969412

Batch 43690, train_perplexity=400.2366, train_loss=5.992056

Batch 43700, train_perplexity=412.74738, train_loss=6.0228357

Batch 43710, train_perplexity=377.5264, train_loss=5.9336405

Batch 43720, train_perplexity=371.53305, train_loss=5.917638

Batch 43730, train_perplexity=378.90677, train_loss=5.93729

Batch 43740, train_perplexity=384.27423, train_loss=5.9513564

Batch 43750, train_perplexity=410.55716, train_loss=6.017515

Batch 43760, train_perplexity=364.67615, train_loss=5.8990097

Batch 43770, train_perplexity=397.73462, train_loss=5.985785

Batch 43780, train_perplexity=361.52625, train_loss=5.8903346

Batch 43790, train_perplexity=408.2119, train_loss=6.0117865

Batch 43800, train_perplexity=393.2938, train_loss=5.974557

Batch 43810, train_perplexity=381.69324, train_loss=5.9446173

Batch 43820, train_perplexity=367.19983, train_loss=5.905906

Batch 43830, train_perplexity=384.19086, train_loss=5.9511395

Batch 43840, train_perplexity=400.35992, train_loss=5.992364

Batch 43850, train_perplexity=364.5329, train_loss=5.898617

Batch 43860, train_perplexity=398.19443, train_loss=5.9869404

Batch 43870, train_perplexity=395.53864, train_loss=5.9802485

Batch 43880, train_perplexity=361.0089, train_loss=5.8889027

Batch 43890, train_perplexity=387.6645, train_loss=5.96014

Batch 43900, train_perplexity=398.17865, train_loss=5.986901

Batch 43910, train_perplexity=363.58505, train_loss=5.8960133

Batch 43920, train_perplexity=396.91406, train_loss=5.98372

Batch 43930, train_perplexity=388.7598, train_loss=5.9629617

Batch 43940, train_perplexity=490.35815, train_loss=6.195136

Batch 43950, train_perplexity=366.62265, train_loss=5.904333

Batch 43960, train_perplexity=348.077, train_loss=5.8524237

Batch 43970, train_perplexity=381.4498, train_loss=5.9439793

Batch 43980, train_perplexity=363.12003, train_loss=5.8947334

Batch 43990, train_perplexity=359.88898, train_loss=5.8857956

Batch 44000, train_perplexity=370.30063, train_loss=5.914315

Batch 44010, train_perplexity=351.47842, train_loss=5.8621483

Batch 44020, train_perplexity=387.35294, train_loss=5.9593363

Batch 44030, train_perplexity=366.41327, train_loss=5.903762

Batch 44040, train_perplexity=360.84836, train_loss=5.888458

Batch 44050, train_perplexity=412.34195, train_loss=6.021853

Batch 44060, train_perplexity=386.93723, train_loss=5.9582624

Batch 44070, train_perplexity=380.8152, train_loss=5.942314

Batch 44080, train_perplexity=360.00122, train_loss=5.8861074

Batch 44090, train_perplexity=396.383, train_loss=5.982381

Batch 44100, train_perplexity=382.21887, train_loss=5.9459934

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100
Loaded 306206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00021-of-00100
Loaded 306206 sentences.
Finished loading
Batch 44110, train_perplexity=369.26666, train_loss=5.911519

Batch 44120, train_perplexity=381.79608, train_loss=5.9448867

Batch 44130, train_perplexity=358.8213, train_loss=5.8828244

Batch 44140, train_perplexity=370.7607, train_loss=5.915557

Batch 44150, train_perplexity=332.7407, train_loss=5.8073635

Batch 44160, train_perplexity=407.54443, train_loss=6.01015

Batch 44170, train_perplexity=365.4912, train_loss=5.9012423

Batch 44180, train_perplexity=377.0608, train_loss=5.9324064

Batch 44190, train_perplexity=400.77496, train_loss=5.9934

Batch 44200, train_perplexity=338.72583, train_loss=5.825191

Batch 44210, train_perplexity=380.51242, train_loss=5.941519

Batch 44220, train_perplexity=367.47482, train_loss=5.906655

Batch 44230, train_perplexity=410.89365, train_loss=6.0183344

Batch 44240, train_perplexity=370.9036, train_loss=5.915942

Batch 44250, train_perplexity=402.19568, train_loss=5.9969387

Batch 44260, train_perplexity=345.09528, train_loss=5.8438206

Batch 44270, train_perplexity=381.78372, train_loss=5.9448543

Batch 44280, train_perplexity=369.53952, train_loss=5.9122577

Batch 44290, train_perplexity=383.3032, train_loss=5.9488263

Batch 44300, train_perplexity=321.51636, train_loss=5.7730484

Batch 44310, train_perplexity=412.8883, train_loss=6.023177

Batch 44320, train_perplexity=369.3709, train_loss=5.9118013

Batch 44330, train_perplexity=363.90662, train_loss=5.8968973

Batch 44340, train_perplexity=358.5765, train_loss=5.882142

Batch 44350, train_perplexity=375.64255, train_loss=5.928638

Batch 44360, train_perplexity=379.13358, train_loss=5.9378886

Batch 44370, train_perplexity=341.0065, train_loss=5.8319016

Batch 44380, train_perplexity=388.61282, train_loss=5.9625835

Batch 44390, train_perplexity=416.71518, train_loss=6.032403

Batch 44400, train_perplexity=451.8296, train_loss=6.113305

Batch 44410, train_perplexity=370.67606, train_loss=5.9153285

Batch 44420, train_perplexity=370.95065, train_loss=5.916069

Batch 44430, train_perplexity=398.7867, train_loss=5.9884267

Batch 44440, train_perplexity=380.36038, train_loss=5.941119

Batch 44450, train_perplexity=421.81973, train_loss=6.044578

Batch 44460, train_perplexity=387.4466, train_loss=5.959578

Batch 44470, train_perplexity=362.15894, train_loss=5.892083

Batch 44480, train_perplexity=395.7888, train_loss=5.9808807

Batch 44490, train_perplexity=382.37582, train_loss=5.946404

Batch 44500, train_perplexity=371.79285, train_loss=5.918337

Batch 44510, train_perplexity=386.36014, train_loss=5.95677

Batch 44520, train_perplexity=381.4949, train_loss=5.9440975

Batch 44530, train_perplexity=381.62482, train_loss=5.944438

Batch 44540, train_perplexity=367.1622, train_loss=5.9058037

Batch 44550, train_perplexity=341.1275, train_loss=5.8322563

Batch 44560, train_perplexity=346.48395, train_loss=5.8478365

Batch 44570, train_perplexity=393.1151, train_loss=5.9741025

Batch 44580, train_perplexity=376.7982, train_loss=5.93171

Batch 44590, train_perplexity=341.33023, train_loss=5.8328505

Batch 44600, train_perplexity=341.18866, train_loss=5.8324356

Batch 44610, train_perplexity=390.03357, train_loss=5.966233

Batch 44620, train_perplexity=381.35413, train_loss=5.9437284

Batch 44630, train_perplexity=377.3324, train_loss=5.9331264

Batch 44640, train_perplexity=356.90683, train_loss=5.877475

Batch 44650, train_perplexity=350.85046, train_loss=5.86036

Batch 44660, train_perplexity=382.01245, train_loss=5.945453

Batch 44670, train_perplexity=404.094, train_loss=6.0016475

Batch 44680, train_perplexity=371.70068, train_loss=5.918089

Batch 44690, train_perplexity=401.23846, train_loss=5.994556

Batch 44700, train_perplexity=327.87567, train_loss=5.7926345

Batch 44710, train_perplexity=384.3609, train_loss=5.951582

Batch 44720, train_perplexity=357.86456, train_loss=5.8801546

Batch 44730, train_perplexity=355.95813, train_loss=5.874813

Batch 44740, train_perplexity=412.40665, train_loss=6.02201

Batch 44750, train_perplexity=373.46307, train_loss=5.922819

Batch 44760, train_perplexity=365.4208, train_loss=5.9010496

Batch 44770, train_perplexity=354.28375, train_loss=5.870098

Batch 44780, train_perplexity=348.64642, train_loss=5.8540583

Batch 44790, train_perplexity=359.87695, train_loss=5.885762

Batch 44800, train_perplexity=381.53748, train_loss=5.944209

Batch 44810, train_perplexity=381.07892, train_loss=5.9430065

Batch 44820, train_perplexity=355.52353, train_loss=5.8735914

Batch 44830, train_perplexity=378.7991, train_loss=5.937006

Batch 44840, train_perplexity=356.86667, train_loss=5.8773623

Batch 44850, train_perplexity=369.82333, train_loss=5.9130254

Batch 44860, train_perplexity=377.35883, train_loss=5.9331965

Batch 44870, train_perplexity=357.82224, train_loss=5.8800364

Batch 44880, train_perplexity=401.97977, train_loss=5.996402

Batch 44890, train_perplexity=348.08646, train_loss=5.852451

Batch 44900, train_perplexity=342.34738, train_loss=5.835826

Batch 44910, train_perplexity=403.208, train_loss=5.9994526
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44920, train_perplexity=348.85095, train_loss=5.854645

Batch 44930, train_perplexity=407.20566, train_loss=6.0093184

Batch 44940, train_perplexity=328.2043, train_loss=5.7936363

Batch 44950, train_perplexity=320.15356, train_loss=5.7688007

Batch 44960, train_perplexity=358.958, train_loss=5.8832054

Batch 44970, train_perplexity=445.0389, train_loss=6.0981617

Batch 44980, train_perplexity=401.66364, train_loss=5.995615

Batch 44990, train_perplexity=375.18964, train_loss=5.9274316

Batch 45000, train_perplexity=338.60214, train_loss=5.824826

Batch 45010, train_perplexity=362.877, train_loss=5.894064

Batch 45020, train_perplexity=340.33414, train_loss=5.829928

Batch 45030, train_perplexity=361.46124, train_loss=5.890155

Batch 45040, train_perplexity=434.31146, train_loss=6.073762

Batch 45050, train_perplexity=376.87457, train_loss=5.9319124

Batch 45060, train_perplexity=359.46365, train_loss=5.884613

Batch 45070, train_perplexity=410.14117, train_loss=6.0165014

Batch 45080, train_perplexity=368.80948, train_loss=5.91028

Batch 45090, train_perplexity=374.40594, train_loss=5.9253407

Batch 45100, train_perplexity=379.3345, train_loss=5.9384184

Batch 45110, train_perplexity=348.74234, train_loss=5.8543334

Batch 45120, train_perplexity=347.41702, train_loss=5.850526

Batch 45130, train_perplexity=381.5666, train_loss=5.9442854

Batch 45140, train_perplexity=380.70062, train_loss=5.9420133

Batch 45150, train_perplexity=365.91724, train_loss=5.902407

Batch 45160, train_perplexity=406.0423, train_loss=6.0064573

Batch 45170, train_perplexity=361.8353, train_loss=5.891189

Batch 45180, train_perplexity=380.87003, train_loss=5.942458

Batch 45190, train_perplexity=353.90585, train_loss=5.869031

Batch 45200, train_perplexity=394.84872, train_loss=5.9785028

Batch 45210, train_perplexity=345.8191, train_loss=5.845916

Batch 45220, train_perplexity=340.14255, train_loss=5.829365

Batch 45230, train_perplexity=384.40414, train_loss=5.9516945

Batch 45240, train_perplexity=363.86462, train_loss=5.896782

Batch 45250, train_perplexity=344.14417, train_loss=5.8410606

Batch 45260, train_perplexity=405.69876, train_loss=6.005611

Batch 45270, train_perplexity=393.81775, train_loss=5.9758883

Batch 45280, train_perplexity=384.4256, train_loss=5.9517503

Batch 45290, train_perplexity=370.41562, train_loss=5.9146256

Batch 45300, train_perplexity=386.3546, train_loss=5.9567556

Batch 45310, train_perplexity=341.99118, train_loss=5.834785

Batch 45320, train_perplexity=436.95374, train_loss=6.0798273

Batch 45330, train_perplexity=345.52637, train_loss=5.845069

Batch 45340, train_perplexity=357.7482, train_loss=5.8798294

Batch 45350, train_perplexity=345.21478, train_loss=5.8441668

Batch 45360, train_perplexity=383.4562, train_loss=5.9492254

Batch 45370, train_perplexity=375.48334, train_loss=5.928214

Batch 45380, train_perplexity=360.0294, train_loss=5.8861856

Batch 45390, train_perplexity=361.13116, train_loss=5.889241

Batch 45400, train_perplexity=341.30923, train_loss=5.832789

Batch 45410, train_perplexity=373.42532, train_loss=5.922718

Batch 45420, train_perplexity=396.6528, train_loss=5.9830613

Batch 45430, train_perplexity=355.5137, train_loss=5.873564

Batch 45440, train_perplexity=376.0994, train_loss=5.9298534

Batch 45450, train_perplexity=310.8846, train_loss=5.739422

Batch 45460, train_perplexity=372.50107, train_loss=5.92024

Batch 45470, train_perplexity=378.80307, train_loss=5.9370165

Batch 45480, train_perplexity=407.9578, train_loss=6.0111637

Batch 45490, train_perplexity=408.56866, train_loss=6.01266

Batch 45500, train_perplexity=348.78476, train_loss=5.854455

Batch 45510, train_perplexity=387.57114, train_loss=5.9598994

Batch 45520, train_perplexity=392.8387, train_loss=5.973399

Batch 45530, train_perplexity=379.82318, train_loss=5.939706

Batch 45540, train_perplexity=367.89545, train_loss=5.907799

Batch 45550, train_perplexity=357.38385, train_loss=5.8788104

Batch 45560, train_perplexity=343.15866, train_loss=5.838193

Batch 45570, train_perplexity=330.541, train_loss=5.8007307

Batch 45580, train_perplexity=395.79126, train_loss=5.980887

Batch 45590, train_perplexity=375.59006, train_loss=5.9284983

Batch 45600, train_perplexity=333.74973, train_loss=5.8103914

Batch 45610, train_perplexity=351.91275, train_loss=5.8633833

Batch 45620, train_perplexity=368.1341, train_loss=5.9084473

Batch 45630, train_perplexity=378.38425, train_loss=5.93591

Batch 45640, train_perplexity=411.55878, train_loss=6.019952

Batch 45650, train_perplexity=356.83673, train_loss=5.8772783

Batch 45660, train_perplexity=415.74982, train_loss=6.0300837

Batch 45670, train_perplexity=341.0595, train_loss=5.832057

Batch 45680, train_perplexity=332.26346, train_loss=5.805928

Batch 45690, train_perplexity=428.99335, train_loss=6.0614414

Batch 45700, train_perplexity=362.67166, train_loss=5.893498

Batch 45710, train_perplexity=358.39514, train_loss=5.881636

Batch 45720, train_perplexity=353.83466, train_loss=5.8688297

Batch 45730, train_perplexity=412.06342, train_loss=6.0211773

Batch 45740, train_perplexity=328.92093, train_loss=5.7958174

Batch 45750, train_perplexity=357.51526, train_loss=5.879178

Batch 45760, train_perplexity=385.6651, train_loss=5.9549694

Batch 45770, train_perplexity=386.27136, train_loss=5.95654

Batch 45780, train_perplexity=388.21536, train_loss=5.9615602

Batch 45790, train_perplexity=360.96243, train_loss=5.888774

Batch 45800, train_perplexity=386.62164, train_loss=5.9574466

Batch 45810, train_perplexity=359.11636, train_loss=5.8836465

Batch 45820, train_perplexity=369.95084, train_loss=5.91337

Batch 45830, train_perplexity=384.2678, train_loss=5.9513397

Batch 45840, train_perplexity=388.49167, train_loss=5.9622717

Batch 45850, train_perplexity=338.57614, train_loss=5.824749

Batch 45860, train_perplexity=350.9549, train_loss=5.8606577

Batch 45870, train_perplexity=382.95627, train_loss=5.947921

Batch 45880, train_perplexity=395.91678, train_loss=5.981204

Batch 45890, train_perplexity=347.03323, train_loss=5.8494205

Batch 45900, train_perplexity=412.83633, train_loss=6.0230513

Batch 45910, train_perplexity=371.80704, train_loss=5.918375

Batch 45920, train_perplexity=373.73065, train_loss=5.9235353

Batch 45930, train_perplexity=364.75128, train_loss=5.8992157

Batch 45940, train_perplexity=366.5758, train_loss=5.9042053

Batch 45950, train_perplexity=350.32922, train_loss=5.8588734

Batch 45960, train_perplexity=347.96512, train_loss=5.8521023

Batch 45970, train_perplexity=360.54047, train_loss=5.887604

Batch 45980, train_perplexity=375.17194, train_loss=5.9273844

Batch 45990, train_perplexity=351.18958, train_loss=5.861326

Batch 46000, train_perplexity=395.56354, train_loss=5.9803114

Batch 46010, train_perplexity=399.1868, train_loss=5.9894295

Batch 46020, train_perplexity=346.17612, train_loss=5.8469477

Batch 46030, train_perplexity=392.07446, train_loss=5.9714518

Batch 46040, train_perplexity=373.54892, train_loss=5.923049

Batch 46050, train_perplexity=353.66614, train_loss=5.8683534

Batch 46060, train_perplexity=353.47714, train_loss=5.867819

Batch 46070, train_perplexity=349.3207, train_loss=5.8559904

Batch 46080, train_perplexity=423.53754, train_loss=6.048642

Batch 46090, train_perplexity=363.9238, train_loss=5.8969445

Batch 46100, train_perplexity=346.3049, train_loss=5.8473196

Batch 46110, train_perplexity=356.3127, train_loss=5.8758087

Batch 46120, train_perplexity=385.70004, train_loss=5.95506

Batch 46130, train_perplexity=373.87573, train_loss=5.9239235

Batch 46140, train_perplexity=337.2269, train_loss=5.820756

Batch 46150, train_perplexity=339.11676, train_loss=5.8263445

Batch 46160, train_perplexity=322.09546, train_loss=5.774848

Batch 46170, train_perplexity=329.3189, train_loss=5.7970266

Batch 46180, train_perplexity=346.6165, train_loss=5.848219

Batch 46190, train_perplexity=378.73734, train_loss=5.936843

Batch 46200, train_perplexity=366.34427, train_loss=5.9035735

Batch 46210, train_perplexity=335.4907, train_loss=5.815594

Batch 46220, train_perplexity=340.75214, train_loss=5.8311553

Batch 46230, train_perplexity=353.15097, train_loss=5.8668957
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46240, train_perplexity=368.26822, train_loss=5.9088116

Batch 46250, train_perplexity=383.5599, train_loss=5.949496

Batch 46260, train_perplexity=376.03394, train_loss=5.9296794

Batch 46270, train_perplexity=369.95718, train_loss=5.9133873

Batch 46280, train_perplexity=357.414, train_loss=5.878895

Batch 46290, train_perplexity=354.36282, train_loss=5.8703213

Batch 46300, train_perplexity=344.93588, train_loss=5.8433585

Batch 46310, train_perplexity=327.23825, train_loss=5.7906885

Batch 46320, train_perplexity=340.38382, train_loss=5.830074

Batch 46330, train_perplexity=360.78244, train_loss=5.888275

Batch 46340, train_perplexity=389.45706, train_loss=5.9647536

Batch 46350, train_perplexity=354.92694, train_loss=5.871912

Batch 46360, train_perplexity=364.05832, train_loss=5.897314

Batch 46370, train_perplexity=353.58167, train_loss=5.8681145

Batch 46380, train_perplexity=425.93814, train_loss=6.054294

Batch 46390, train_perplexity=367.76968, train_loss=5.907457

Batch 46400, train_perplexity=386.00198, train_loss=5.9558425

Batch 46410, train_perplexity=321.38513, train_loss=5.77264

Batch 46420, train_perplexity=363.21075, train_loss=5.8949833

Batch 46430, train_perplexity=392.9515, train_loss=5.973686

Batch 46440, train_perplexity=376.67352, train_loss=5.931379

Batch 46450, train_perplexity=365.45776, train_loss=5.9011507

Batch 46460, train_perplexity=338.86298, train_loss=5.825596

Batch 46470, train_perplexity=318.74197, train_loss=5.764382

Batch 46480, train_perplexity=399.6561, train_loss=5.9906044

Batch 46490, train_perplexity=382.86862, train_loss=5.947692

Batch 46500, train_perplexity=381.18906, train_loss=5.9432955

Batch 46510, train_perplexity=315.84088, train_loss=5.7552385

Batch 46520, train_perplexity=363.86533, train_loss=5.896784

Batch 46530, train_perplexity=372.31445, train_loss=5.919739

Batch 46540, train_perplexity=337.90244, train_loss=5.8227572

Batch 46550, train_perplexity=346.2628, train_loss=5.847198

Batch 46560, train_perplexity=358.4784, train_loss=5.8818684

Batch 46570, train_perplexity=354.07736, train_loss=5.8695154

Batch 46580, train_perplexity=385.04697, train_loss=5.9533653

Batch 46590, train_perplexity=429.65274, train_loss=6.0629773

Batch 46600, train_perplexity=351.24484, train_loss=5.8614836

Batch 46610, train_perplexity=335.96054, train_loss=5.8169937

Batch 46620, train_perplexity=377.19675, train_loss=5.932767

Batch 46630, train_perplexity=368.90112, train_loss=5.9105287

Batch 46640, train_perplexity=375.1635, train_loss=5.927362

Batch 46650, train_perplexity=374.4622, train_loss=5.925491

Batch 46660, train_perplexity=383.5568, train_loss=5.9494877

Batch 46670, train_perplexity=355.42657, train_loss=5.8733187

Batch 46680, train_perplexity=434.46185, train_loss=6.074108

Batch 46690, train_perplexity=374.76587, train_loss=5.9263015

Batch 46700, train_perplexity=340.36887, train_loss=5.83003

Batch 46710, train_perplexity=395.1795, train_loss=5.97934

Batch 46720, train_perplexity=350.30753, train_loss=5.8588114

Batch 46730, train_perplexity=428.2776, train_loss=6.0597715

Batch 46740, train_perplexity=365.76007, train_loss=5.9019775

Batch 46750, train_perplexity=385.9876, train_loss=5.9558053

Batch 46760, train_perplexity=377.79346, train_loss=5.9343476

Batch 46770, train_perplexity=323.64426, train_loss=5.779645

Batch 46780, train_perplexity=351.73978, train_loss=5.8628917

Batch 46790, train_perplexity=370.90396, train_loss=5.915943

Batch 46800, train_perplexity=371.7207, train_loss=5.918143

Batch 46810, train_perplexity=348.95462, train_loss=5.854942

Batch 46820, train_perplexity=376.74753, train_loss=5.9315753

Batch 46830, train_perplexity=313.26526, train_loss=5.7470503

Batch 46840, train_perplexity=324.19458, train_loss=5.781344

Batch 46850, train_perplexity=383.0947, train_loss=5.9482822

Batch 46860, train_perplexity=329.2222, train_loss=5.796733

Batch 46870, train_perplexity=364.04688, train_loss=5.8972826

Batch 46880, train_perplexity=357.71854, train_loss=5.8797464

Batch 46890, train_perplexity=351.32224, train_loss=5.861704

Batch 46900, train_perplexity=372.71625, train_loss=5.9208174

Batch 46910, train_perplexity=352.06165, train_loss=5.8638062

Batch 46920, train_perplexity=375.19302, train_loss=5.9274406

Batch 46930, train_perplexity=372.26047, train_loss=5.919594

Batch 46940, train_perplexity=340.4437, train_loss=5.83025

Batch 46950, train_perplexity=381.7271, train_loss=5.944706

Batch 46960, train_perplexity=370.2657, train_loss=5.914221

Batch 46970, train_perplexity=359.42902, train_loss=5.8845167

Batch 46980, train_perplexity=363.38956, train_loss=5.8954754

Batch 46990, train_perplexity=365.44174, train_loss=5.901107

Batch 47000, train_perplexity=366.52023, train_loss=5.9040537

Batch 47010, train_perplexity=356.6974, train_loss=5.876888

Batch 47020, train_perplexity=374.061, train_loss=5.924419

Batch 47030, train_perplexity=370.6879, train_loss=5.9153605

Batch 47040, train_perplexity=368.95163, train_loss=5.9106655

Batch 47050, train_perplexity=408.38675, train_loss=6.0122147

Batch 47060, train_perplexity=384.49142, train_loss=5.9519215

Batch 47070, train_perplexity=365.69778, train_loss=5.9018073

Batch 47080, train_perplexity=373.63443, train_loss=5.923278

Batch 47090, train_perplexity=366.1548, train_loss=5.903056

Batch 47100, train_perplexity=360.9201, train_loss=5.8886566

Batch 47110, train_perplexity=398.6998, train_loss=5.988209

Batch 47120, train_perplexity=367.94174, train_loss=5.9079247

Batch 47130, train_perplexity=387.2691, train_loss=5.95912

Batch 47140, train_perplexity=340.5343, train_loss=5.830516

Batch 47150, train_perplexity=349.53864, train_loss=5.856614

Batch 47160, train_perplexity=335.00647, train_loss=5.81415

Batch 47170, train_perplexity=351.31622, train_loss=5.8616867

Batch 47180, train_perplexity=396.46765, train_loss=5.9825945

Batch 47190, train_perplexity=365.7297, train_loss=5.9018946

Batch 47200, train_perplexity=364.8654, train_loss=5.8995285

Batch 47210, train_perplexity=363.38123, train_loss=5.8954525

Batch 47220, train_perplexity=357.40173, train_loss=5.8788605

Batch 47230, train_perplexity=373.11703, train_loss=5.921892

Batch 47240, train_perplexity=338.3718, train_loss=5.8241453

Batch 47250, train_perplexity=339.85315, train_loss=5.8285136

Batch 47260, train_perplexity=361.24896, train_loss=5.8895674

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Batch 47270, train_perplexity=388.66843, train_loss=5.9627266

Batch 47280, train_perplexity=401.0036, train_loss=5.9939704

Batch 47290, train_perplexity=351.57227, train_loss=5.8624153

Batch 47300, train_perplexity=368.82214, train_loss=5.9103146

Batch 47310, train_perplexity=354.54263, train_loss=5.8708286

Batch 47320, train_perplexity=335.8601, train_loss=5.8166947

Batch 47330, train_perplexity=383.30447, train_loss=5.9488297

Batch 47340, train_perplexity=350.43933, train_loss=5.8591876

Batch 47350, train_perplexity=373.2272, train_loss=5.9221873

Batch 47360, train_perplexity=378.16348, train_loss=5.9353266

Batch 47370, train_perplexity=371.47815, train_loss=5.91749

Batch 47380, train_perplexity=386.43475, train_loss=5.956963

Batch 47390, train_perplexity=424.1418, train_loss=6.050068

Batch 47400, train_perplexity=357.5446, train_loss=5.87926

Batch 47410, train_perplexity=374.93817, train_loss=5.926761

Batch 47420, train_perplexity=404.76334, train_loss=6.0033026

Batch 47430, train_perplexity=398.91754, train_loss=5.9887547

Batch 47440, train_perplexity=379.69824, train_loss=5.939377

Batch 47450, train_perplexity=381.68088, train_loss=5.944585

Batch 47460, train_perplexity=347.26035, train_loss=5.850075

Batch 47470, train_perplexity=355.11792, train_loss=5.87245

Batch 47480, train_perplexity=393.59265, train_loss=5.9753165

Batch 47490, train_perplexity=350.38052, train_loss=5.8590198
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 47500, train_perplexity=366.20297, train_loss=5.9031878

Batch 47510, train_perplexity=362.55496, train_loss=5.893176

Batch 47520, train_perplexity=353.7901, train_loss=5.868704

Batch 47530, train_perplexity=337.07642, train_loss=5.8203096

Batch 47540, train_perplexity=369.28217, train_loss=5.911561

Batch 47550, train_perplexity=333.44113, train_loss=5.8094664

Batch 47560, train_perplexity=352.24332, train_loss=5.864322

Batch 47570, train_perplexity=340.6519, train_loss=5.830861

Batch 47580, train_perplexity=334.72992, train_loss=5.813324

Batch 47590, train_perplexity=356.95993, train_loss=5.8776236

Batch 47600, train_perplexity=322.06198, train_loss=5.774744

Batch 47610, train_perplexity=361.71747, train_loss=5.8908634

Batch 47620, train_perplexity=346.04938, train_loss=5.8465815

Batch 47630, train_perplexity=372.2468, train_loss=5.919557

Batch 47640, train_perplexity=375.1449, train_loss=5.9273124

Batch 47650, train_perplexity=352.53854, train_loss=5.86516

Batch 47660, train_perplexity=361.07504, train_loss=5.889086

Batch 47670, train_perplexity=342.89435, train_loss=5.8374224

Batch 47680, train_perplexity=346.43027, train_loss=5.8476815

Batch 47690, train_perplexity=359.35242, train_loss=5.8843036

Batch 47700, train_perplexity=343.09094, train_loss=5.8379955

Batch 47710, train_perplexity=379.23685, train_loss=5.938161

Batch 47720, train_perplexity=346.8913, train_loss=5.8490114

Batch 47730, train_perplexity=361.05817, train_loss=5.889039

Batch 47740, train_perplexity=336.67642, train_loss=5.8191223

Batch 47750, train_perplexity=353.7638, train_loss=5.8686295

Batch 47760, train_perplexity=366.57056, train_loss=5.904191

Batch 47770, train_perplexity=392.64584, train_loss=5.972908

Batch 47780, train_perplexity=394.68442, train_loss=5.9780865

Batch 47790, train_perplexity=386.29678, train_loss=5.956606

Batch 47800, train_perplexity=358.13855, train_loss=5.88092

Batch 47810, train_perplexity=303.42822, train_loss=5.715145

Batch 47820, train_perplexity=307.03278, train_loss=5.7269545

Batch 47830, train_perplexity=354.81528, train_loss=5.8715973

Batch 47840, train_perplexity=358.60867, train_loss=5.8822317

Batch 47850, train_perplexity=351.96747, train_loss=5.8635387

Batch 47860, train_perplexity=409.35028, train_loss=6.014571

Batch 47870, train_perplexity=384.60382, train_loss=5.952214

Batch 47880, train_perplexity=322.98935, train_loss=5.7776194

Batch 47890, train_perplexity=339.45618, train_loss=5.827345

Batch 47900, train_perplexity=379.89346, train_loss=5.939891

Batch 47910, train_perplexity=340.0167, train_loss=5.8289948

Batch 47920, train_perplexity=365.39084, train_loss=5.9009676

Batch 47930, train_perplexity=355.06796, train_loss=5.872309

Batch 47940, train_perplexity=383.43097, train_loss=5.9491596

Batch 47950, train_perplexity=331.65182, train_loss=5.8040857

Batch 47960, train_perplexity=376.68106, train_loss=5.931399

Batch 47970, train_perplexity=390.88013, train_loss=5.968401

Batch 47980, train_perplexity=356.8927, train_loss=5.877435

Batch 47990, train_perplexity=378.0252, train_loss=5.934961

Batch 48000, train_perplexity=302.97424, train_loss=5.713648

Batch 48010, train_perplexity=388.28165, train_loss=5.961731

Batch 48020, train_perplexity=370.55075, train_loss=5.9149904

Batch 48030, train_perplexity=323.5153, train_loss=5.7792463

Batch 48040, train_perplexity=370.74728, train_loss=5.9155207

Batch 48050, train_perplexity=380.46304, train_loss=5.941389

Batch 48060, train_perplexity=358.84387, train_loss=5.8828874

Batch 48070, train_perplexity=328.77243, train_loss=5.795366

Batch 48080, train_perplexity=372.38477, train_loss=5.9199276

Batch 48090, train_perplexity=348.53406, train_loss=5.853736

Batch 48100, train_perplexity=354.12177, train_loss=5.869641

Batch 48110, train_perplexity=355.69684, train_loss=5.8740788

Batch 48120, train_perplexity=332.95734, train_loss=5.8080144

Batch 48130, train_perplexity=376.35843, train_loss=5.930542

Batch 48140, train_perplexity=371.84036, train_loss=5.9184647

Batch 48150, train_perplexity=347.1494, train_loss=5.8497553

Batch 48160, train_perplexity=397.0356, train_loss=5.984026

Batch 48170, train_perplexity=352.394, train_loss=5.86475

Batch 48180, train_perplexity=344.20258, train_loss=5.8412304

Batch 48190, train_perplexity=347.68185, train_loss=5.851288

Batch 48200, train_perplexity=379.43274, train_loss=5.9386773

Batch 48210, train_perplexity=360.47653, train_loss=5.887427

Batch 48220, train_perplexity=374.39273, train_loss=5.9253054

Batch 48230, train_perplexity=355.61255, train_loss=5.873842

Batch 48240, train_perplexity=347.51727, train_loss=5.8508143

Batch 48250, train_perplexity=376.2655, train_loss=5.930295

Batch 48260, train_perplexity=423.67328, train_loss=6.0489626

Batch 48270, train_perplexity=361.29443, train_loss=5.8896933

Batch 48280, train_perplexity=391.1587, train_loss=5.9691133

Batch 48290, train_perplexity=375.45218, train_loss=5.928131

Batch 48300, train_perplexity=372.82394, train_loss=5.9211063

Batch 48310, train_perplexity=398.25842, train_loss=5.987101

Batch 48320, train_perplexity=341.65005, train_loss=5.833787

Batch 48330, train_perplexity=352.66736, train_loss=5.8655252

Batch 48340, train_perplexity=374.1336, train_loss=5.924613

Batch 48350, train_perplexity=338.17084, train_loss=5.823551

Batch 48360, train_perplexity=331.69992, train_loss=5.8042307

Batch 48370, train_perplexity=336.5286, train_loss=5.818683

Batch 48380, train_perplexity=351.5498, train_loss=5.8623514

Batch 48390, train_perplexity=327.14963, train_loss=5.7904177

Batch 48400, train_perplexity=373.8843, train_loss=5.9239464

Batch 48410, train_perplexity=328.9421, train_loss=5.7958817

Batch 48420, train_perplexity=329.92435, train_loss=5.7988634

Batch 48430, train_perplexity=363.48764, train_loss=5.8957453

Batch 48440, train_perplexity=352.14676, train_loss=5.864048

Batch 48450, train_perplexity=319.87048, train_loss=5.767916

Batch 48460, train_perplexity=406.99796, train_loss=6.008808

Batch 48470, train_perplexity=324.99667, train_loss=5.783815

Batch 48480, train_perplexity=377.43008, train_loss=5.9333854

Batch 48490, train_perplexity=353.57004, train_loss=5.8680816

Batch 48500, train_perplexity=341.5406, train_loss=5.8334665

Batch 48510, train_perplexity=351.88757, train_loss=5.863312

Batch 48520, train_perplexity=389.95676, train_loss=5.966036

Batch 48530, train_perplexity=310.91013, train_loss=5.739504

Batch 48540, train_perplexity=389.26993, train_loss=5.964273

Batch 48550, train_perplexity=369.68103, train_loss=5.9126406

Batch 48560, train_perplexity=348.20264, train_loss=5.8527846

Batch 48570, train_perplexity=363.36945, train_loss=5.89542

Batch 48580, train_perplexity=406.7826, train_loss=6.008279

Batch 48590, train_perplexity=341.56958, train_loss=5.8335514

Batch 48600, train_perplexity=376.73944, train_loss=5.931554

Batch 48610, train_perplexity=344.1742, train_loss=5.841148

Batch 48620, train_perplexity=360.42514, train_loss=5.8872843

Batch 48630, train_perplexity=370.18164, train_loss=5.913994

Batch 48640, train_perplexity=314.42056, train_loss=5.7507315

Batch 48650, train_perplexity=349.42065, train_loss=5.8562765

Batch 48660, train_perplexity=366.01636, train_loss=5.902678

Batch 48670, train_perplexity=356.11465, train_loss=5.8752527

Batch 48680, train_perplexity=375.21628, train_loss=5.9275026

Batch 48690, train_perplexity=345.4766, train_loss=5.844925

Batch 48700, train_perplexity=320.24976, train_loss=5.769101

Batch 48710, train_perplexity=385.86267, train_loss=5.9554815

Batch 48720, train_perplexity=364.0177, train_loss=5.8972025

Batch 48730, train_perplexity=331.2962, train_loss=5.803013

Batch 48740, train_perplexity=363.37256, train_loss=5.8954287

Batch 48750, train_perplexity=381.36615, train_loss=5.94376

Batch 48760, train_perplexity=346.75616, train_loss=5.848622

Batch 48770, train_perplexity=326.65924, train_loss=5.7889175

Batch 48780, train_perplexity=363.14774, train_loss=5.8948097

Batch 48790, train_perplexity=373.38153, train_loss=5.9226007

Batch 48800, train_perplexity=343.35214, train_loss=5.8387566

Batch 48810, train_perplexity=341.39697, train_loss=5.833046
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48820, train_perplexity=355.99545, train_loss=5.874918

Batch 48830, train_perplexity=344.48337, train_loss=5.842046

Batch 48840, train_perplexity=314.43512, train_loss=5.7507777

Batch 48850, train_perplexity=351.10318, train_loss=5.86108

Batch 48860, train_perplexity=403.96472, train_loss=6.0013275

Batch 48870, train_perplexity=354.46454, train_loss=5.8706083

Batch 48880, train_perplexity=342.12332, train_loss=5.835171

Batch 48890, train_perplexity=334.13635, train_loss=5.811549

Batch 48900, train_perplexity=390.27484, train_loss=5.966851

Batch 48910, train_perplexity=330.88794, train_loss=5.8017797

Batch 48920, train_perplexity=391.5306, train_loss=5.9700637

Batch 48930, train_perplexity=374.06406, train_loss=5.924427

Batch 48940, train_perplexity=373.7089, train_loss=5.923477

Batch 48950, train_perplexity=355.57794, train_loss=5.8737445

Batch 48960, train_perplexity=336.81677, train_loss=5.819539

Batch 48970, train_perplexity=389.2105, train_loss=5.9641204

Batch 48980, train_perplexity=326.57437, train_loss=5.7886577

Batch 48990, train_perplexity=338.23276, train_loss=5.8237343

Batch 49000, train_perplexity=361.23087, train_loss=5.8895173

Batch 49010, train_perplexity=351.9609, train_loss=5.86352

Batch 49020, train_perplexity=376.23608, train_loss=5.930217

Batch 49030, train_perplexity=338.64767, train_loss=5.82496

Batch 49040, train_perplexity=388.20853, train_loss=5.9615426

Batch 49050, train_perplexity=355.74838, train_loss=5.8742237

Batch 49060, train_perplexity=318.09683, train_loss=5.762356

Batch 49070, train_perplexity=362.28052, train_loss=5.892419

Batch 49080, train_perplexity=372.55295, train_loss=5.920379

Batch 49090, train_perplexity=396.94397, train_loss=5.983795

Batch 49100, train_perplexity=362.23285, train_loss=5.8922873

Batch 49110, train_perplexity=366.54224, train_loss=5.904114

Batch 49120, train_perplexity=350.8033, train_loss=5.8602257

Batch 49130, train_perplexity=361.06262, train_loss=5.8890514

Batch 49140, train_perplexity=356.85886, train_loss=5.8773403

Batch 49150, train_perplexity=372.011, train_loss=5.9189234

Batch 49160, train_perplexity=328.8124, train_loss=5.7954874

Batch 49170, train_perplexity=339.48987, train_loss=5.827444

Batch 49180, train_perplexity=386.01984, train_loss=5.9558887

Batch 49190, train_perplexity=400.49854, train_loss=5.99271

Batch 49200, train_perplexity=362.8047, train_loss=5.8938646

Batch 49210, train_perplexity=363.0667, train_loss=5.8945866

Batch 49220, train_perplexity=350.29916, train_loss=5.8587875

Batch 49230, train_perplexity=347.67654, train_loss=5.8512726

Batch 49240, train_perplexity=380.3876, train_loss=5.9411907

Batch 49250, train_perplexity=341.59482, train_loss=5.8336253

Batch 49260, train_perplexity=344.0795, train_loss=5.840873

Batch 49270, train_perplexity=356.64926, train_loss=5.876753

Batch 49280, train_perplexity=314.68094, train_loss=5.7515593

Batch 49290, train_perplexity=399.8223, train_loss=5.99102

Batch 49300, train_perplexity=384.30682, train_loss=5.9514413

Batch 49310, train_perplexity=345.24902, train_loss=5.844266

Batch 49320, train_perplexity=364.67215, train_loss=5.8989987

Batch 49330, train_perplexity=331.3259, train_loss=5.8031025

Batch 49340, train_perplexity=338.12503, train_loss=5.8234158

Batch 49350, train_perplexity=334.8969, train_loss=5.8138227

Batch 49360, train_perplexity=321.55252, train_loss=5.773161

Batch 49370, train_perplexity=354.60233, train_loss=5.870997

Batch 49380, train_perplexity=350.90018, train_loss=5.860502

Batch 49390, train_perplexity=353.05048, train_loss=5.866611

Batch 49400, train_perplexity=329.42368, train_loss=5.7973447

Batch 49410, train_perplexity=364.76068, train_loss=5.8992414

Batch 49420, train_perplexity=331.7597, train_loss=5.804411

Batch 49430, train_perplexity=352.63, train_loss=5.8654194

Batch 49440, train_perplexity=388.61523, train_loss=5.9625897

Batch 49450, train_perplexity=364.1979, train_loss=5.8976974

Batch 49460, train_perplexity=421.84628, train_loss=6.044641

Batch 49470, train_perplexity=348.09573, train_loss=5.8524776

Batch 49480, train_perplexity=362.804, train_loss=5.8938627

Batch 49490, train_perplexity=297.4544, train_loss=5.695261

Batch 49500, train_perplexity=343.8853, train_loss=5.840308

Batch 49510, train_perplexity=327.203, train_loss=5.7905807

Batch 49520, train_perplexity=353.68082, train_loss=5.868395

Batch 49530, train_perplexity=359.78088, train_loss=5.885495

Batch 49540, train_perplexity=376.51013, train_loss=5.930945

Batch 49550, train_perplexity=357.6288, train_loss=5.8794956

Batch 49560, train_perplexity=338.02896, train_loss=5.8231316

Batch 49570, train_perplexity=361.6026, train_loss=5.890546

Batch 49580, train_perplexity=355.4964, train_loss=5.873515

Batch 49590, train_perplexity=346.63864, train_loss=5.848283

Batch 49600, train_perplexity=362.71387, train_loss=5.8936143

Batch 49610, train_perplexity=331.22543, train_loss=5.802799

Batch 49620, train_perplexity=316.82028, train_loss=5.7583346

Batch 49630, train_perplexity=340.757, train_loss=5.8311696

Batch 49640, train_perplexity=345.5842, train_loss=5.8452363

Batch 49650, train_perplexity=332.66187, train_loss=5.8071265

Batch 49660, train_perplexity=393.04388, train_loss=5.9739213

Batch 49670, train_perplexity=327.9018, train_loss=5.792714

Batch 49680, train_perplexity=337.00153, train_loss=5.8200874

Batch 49690, train_perplexity=350.8224, train_loss=5.86028

Batch 49700, train_perplexity=363.54294, train_loss=5.8958974

Batch 49710, train_perplexity=405.57556, train_loss=6.005307

Batch 49720, train_perplexity=319.04547, train_loss=5.7653337

Batch 49730, train_perplexity=377.61407, train_loss=5.9338727

Batch 49740, train_perplexity=326.04642, train_loss=5.7870398

Batch 49750, train_perplexity=337.6731, train_loss=5.822078

Batch 49760, train_perplexity=353.7697, train_loss=5.868646

Batch 49770, train_perplexity=371.25217, train_loss=5.9168816

Batch 49780, train_perplexity=380.1216, train_loss=5.940491

Batch 49790, train_perplexity=343.73727, train_loss=5.8398776

Batch 49800, train_perplexity=322.20145, train_loss=5.775177

Batch 49810, train_perplexity=368.25436, train_loss=5.908774

Batch 49820, train_perplexity=345.85983, train_loss=5.8460336

Batch 49830, train_perplexity=342.23215, train_loss=5.8354893

Batch 49840, train_perplexity=339.93823, train_loss=5.828764

Batch 49850, train_perplexity=337.85394, train_loss=5.8226137

Batch 49860, train_perplexity=393.40372, train_loss=5.9748363

Batch 49870, train_perplexity=338.47766, train_loss=5.824458

Batch 49880, train_perplexity=357.04813, train_loss=5.8778706

Batch 49890, train_perplexity=392.0625, train_loss=5.9714212

Batch 49900, train_perplexity=350.73022, train_loss=5.8600173

Batch 49910, train_perplexity=343.63977, train_loss=5.839594

Batch 49920, train_perplexity=382.70325, train_loss=5.94726

Batch 49930, train_perplexity=345.92728, train_loss=5.8462286

Batch 49940, train_perplexity=331.59238, train_loss=5.8039064

Batch 49950, train_perplexity=345.4763, train_loss=5.844924

Batch 49960, train_perplexity=390.82498, train_loss=5.96826

Batch 49970, train_perplexity=323.39774, train_loss=5.778883

Batch 49980, train_perplexity=328.06866, train_loss=5.793223

Batch 49990, train_perplexity=355.27457, train_loss=5.872891

Batch 50000, train_perplexity=321.30576, train_loss=5.772393

Batch 50010, train_perplexity=339.03238, train_loss=5.8260956

Batch 50020, train_perplexity=384.09195, train_loss=5.950882

Batch 50030, train_perplexity=359.015, train_loss=5.883364

Batch 50040, train_perplexity=352.04214, train_loss=5.863751

Batch 50050, train_perplexity=324.0314, train_loss=5.7808404

Batch 50060, train_perplexity=315.43, train_loss=5.753937

Batch 50070, train_perplexity=365.67792, train_loss=5.901753

Batch 50080, train_perplexity=334.5574, train_loss=5.8128085

Batch 50090, train_perplexity=329.9025, train_loss=5.798797

Batch 50100, train_perplexity=362.06638, train_loss=5.8918276

Batch 50110, train_perplexity=360.89944, train_loss=5.8885994

Batch 50120, train_perplexity=347.8689, train_loss=5.8518257

Batch 50130, train_perplexity=301.81885, train_loss=5.709827
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 50140, train_perplexity=352.18604, train_loss=5.8641596

Batch 50150, train_perplexity=325.42792, train_loss=5.785141

Batch 50160, train_perplexity=377.82822, train_loss=5.9344397

Batch 50170, train_perplexity=371.54758, train_loss=5.917677

Batch 50180, train_perplexity=338.96158, train_loss=5.8258867

Batch 50190, train_perplexity=351.55417, train_loss=5.862364

Batch 50200, train_perplexity=300.12323, train_loss=5.704193

Batch 50210, train_perplexity=319.8804, train_loss=5.767947

Batch 50220, train_perplexity=347.364, train_loss=5.8503733

Batch 50230, train_perplexity=344.9505, train_loss=5.843401

Batch 50240, train_perplexity=369.07056, train_loss=5.910988

Batch 50250, train_perplexity=302.05884, train_loss=5.710622

Batch 50260, train_perplexity=338.6383, train_loss=5.8249326

Batch 50270, train_perplexity=387.5338, train_loss=5.959803

Batch 50280, train_perplexity=383.91214, train_loss=5.9504137

Batch 50290, train_perplexity=395.9976, train_loss=5.981408

Batch 50300, train_perplexity=369.1276, train_loss=5.9111423

Batch 50310, train_perplexity=316.33466, train_loss=5.7568007

Batch 50320, train_perplexity=290.9507, train_loss=5.673154

Batch 50330, train_perplexity=330.50287, train_loss=5.8006153

Batch 50340, train_perplexity=347.86227, train_loss=5.8518066

Batch 50350, train_perplexity=391.8386, train_loss=5.97085

Batch 50360, train_perplexity=359.1227, train_loss=5.883664

Batch 50370, train_perplexity=339.1766, train_loss=5.826521

Batch 50380, train_perplexity=326.87833, train_loss=5.789588

Batch 50390, train_perplexity=353.6105, train_loss=5.868196

Batch 50400, train_perplexity=345.6249, train_loss=5.845354

Batch 50410, train_perplexity=400.42044, train_loss=5.992515

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Batch 50420, train_perplexity=343.0337, train_loss=5.8378286

Batch 50430, train_perplexity=369.82175, train_loss=5.913021

Batch 50440, train_perplexity=332.61346, train_loss=5.806981

Batch 50450, train_perplexity=352.58597, train_loss=5.8652945

Batch 50460, train_perplexity=342.36972, train_loss=5.8358912

Batch 50470, train_perplexity=333.8774, train_loss=5.810774

Batch 50480, train_perplexity=345.90485, train_loss=5.8461637

Batch 50490, train_perplexity=338.32275, train_loss=5.8240004

Batch 50500, train_perplexity=371.43597, train_loss=5.9173765

Batch 50510, train_perplexity=345.4491, train_loss=5.8448453

Batch 50520, train_perplexity=426.81827, train_loss=6.0563583

Batch 50530, train_perplexity=333.10742, train_loss=5.808465

Batch 50540, train_perplexity=357.58994, train_loss=5.879387

Batch 50550, train_perplexity=352.87643, train_loss=5.866118

Batch 50560, train_perplexity=323.86395, train_loss=5.7803235

Batch 50570, train_perplexity=381.14835, train_loss=5.9431887

Batch 50580, train_perplexity=331.68332, train_loss=5.8041806

Batch 50590, train_perplexity=340.17758, train_loss=5.829468

Batch 50600, train_perplexity=322.06152, train_loss=5.7747426

Batch 50610, train_perplexity=355.5973, train_loss=5.873799

Batch 50620, train_perplexity=351.35626, train_loss=5.8618007

Batch 50630, train_perplexity=334.6345, train_loss=5.813039

Batch 50640, train_perplexity=379.86032, train_loss=5.9398036

Batch 50650, train_perplexity=355.21106, train_loss=5.872712

Batch 50660, train_perplexity=319.22034, train_loss=5.7658815

Batch 50670, train_perplexity=411.45203, train_loss=6.0196924

Batch 50680, train_perplexity=368.96393, train_loss=5.910699

Batch 50690, train_perplexity=363.67175, train_loss=5.8962517

Batch 50700, train_perplexity=330.36987, train_loss=5.800213

Batch 50710, train_perplexity=373.4645, train_loss=5.922823

Batch 50720, train_perplexity=359.01212, train_loss=5.883356

Batch 50730, train_perplexity=299.4288, train_loss=5.7018766

Batch 50740, train_perplexity=354.55667, train_loss=5.870868

Batch 50750, train_perplexity=297.92258, train_loss=5.6968336

Batch 50760, train_perplexity=364.07343, train_loss=5.8973556

Batch 50770, train_perplexity=336.07156, train_loss=5.817324

Batch 50780, train_perplexity=325.08453, train_loss=5.7840853

Batch 50790, train_perplexity=354.73846, train_loss=5.871381

Batch 50800, train_perplexity=352.50476, train_loss=5.865064

Batch 50810, train_perplexity=340.84653, train_loss=5.8314323

Batch 50820, train_perplexity=370.29675, train_loss=5.9143047

Batch 50830, train_perplexity=340.47733, train_loss=5.8303485

Batch 50840, train_perplexity=365.79843, train_loss=5.9020824

Batch 50850, train_perplexity=378.20676, train_loss=5.935441

Batch 50860, train_perplexity=371.45547, train_loss=5.917429

Batch 50870, train_perplexity=380.19974, train_loss=5.9406967

Batch 50880, train_perplexity=368.19272, train_loss=5.9086065

Batch 50890, train_perplexity=347.65964, train_loss=5.851224

Batch 50900, train_perplexity=336.77243, train_loss=5.8194075

Batch 50910, train_perplexity=363.28836, train_loss=5.895197

Batch 50920, train_perplexity=339.22287, train_loss=5.8266573

Batch 50930, train_perplexity=353.40012, train_loss=5.867601

Batch 50940, train_perplexity=332.46634, train_loss=5.8065386

Batch 50950, train_perplexity=312.138, train_loss=5.7434454

Batch 50960, train_perplexity=358.91452, train_loss=5.8830843

Batch 50970, train_perplexity=353.33847, train_loss=5.8674264

Batch 50980, train_perplexity=296.41714, train_loss=5.6917677

Batch 50990, train_perplexity=371.68082, train_loss=5.9180355

Batch 51000, train_perplexity=332.7207, train_loss=5.8073034

Batch 51010, train_perplexity=355.82065, train_loss=5.874427

Batch 51020, train_perplexity=364.82712, train_loss=5.8994236

Batch 51030, train_perplexity=357.50232, train_loss=5.879142

Batch 51040, train_perplexity=344.1609, train_loss=5.8411093

Batch 51050, train_perplexity=339.4986, train_loss=5.82747

Batch 51060, train_perplexity=358.28613, train_loss=5.881332

Batch 51070, train_perplexity=312.6081, train_loss=5.7449503

Batch 51080, train_perplexity=333.30188, train_loss=5.8090487

Batch 51090, train_perplexity=355.2727, train_loss=5.8728857

Batch 51100, train_perplexity=326.3541, train_loss=5.787983

Batch 51110, train_perplexity=365.50012, train_loss=5.9012666

Batch 51120, train_perplexity=366.66742, train_loss=5.904455

Batch 51130, train_perplexity=374.20856, train_loss=5.9248133

Batch 51140, train_perplexity=341.67172, train_loss=5.8338504

Batch 51150, train_perplexity=407.396, train_loss=6.0097857

Batch 51160, train_perplexity=349.3357, train_loss=5.8560333

Batch 51170, train_perplexity=372.011, train_loss=5.9189234

Batch 51180, train_perplexity=335.07916, train_loss=5.814367

Batch 51190, train_perplexity=333.695, train_loss=5.8102274

Batch 51200, train_perplexity=314.20984, train_loss=5.750061

Batch 51210, train_perplexity=387.26245, train_loss=5.9591026

Batch 51220, train_perplexity=331.73755, train_loss=5.804344

Batch 51230, train_perplexity=387.24677, train_loss=5.959062

Batch 51240, train_perplexity=346.6907, train_loss=5.848433

Batch 51250, train_perplexity=365.86786, train_loss=5.902272

Batch 51260, train_perplexity=334.93573, train_loss=5.8139386

Batch 51270, train_perplexity=349.0616, train_loss=5.8552485

Batch 51280, train_perplexity=347.71136, train_loss=5.8513727

Batch 51290, train_perplexity=370.58768, train_loss=5.91509

Batch 51300, train_perplexity=341.2056, train_loss=5.832485

Batch 51310, train_perplexity=307.01038, train_loss=5.7268815

Batch 51320, train_perplexity=329.13242, train_loss=5.79646

Batch 51330, train_perplexity=328.5635, train_loss=5.79473

Batch 51340, train_perplexity=301.75522, train_loss=5.709616

Batch 51350, train_perplexity=316.77057, train_loss=5.7581778

Batch 51360, train_perplexity=362.5904, train_loss=5.893274

Batch 51370, train_perplexity=370.48715, train_loss=5.914819

Batch 51380, train_perplexity=372.12347, train_loss=5.9192257

Batch 51390, train_perplexity=338.4612, train_loss=5.8244095
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 51400, train_perplexity=383.72363, train_loss=5.9499226

Batch 51410, train_perplexity=370.643, train_loss=5.9152393

Batch 51420, train_perplexity=346.73517, train_loss=5.8485613

Batch 51430, train_perplexity=310.09787, train_loss=5.736888

Batch 51440, train_perplexity=355.4781, train_loss=5.8734636

Batch 51450, train_perplexity=307.27032, train_loss=5.727728

Batch 51460, train_perplexity=333.72937, train_loss=5.8103304

Batch 51470, train_perplexity=387.00992, train_loss=5.9584503

Batch 51480, train_perplexity=419.70886, train_loss=6.0395613

Batch 51490, train_perplexity=392.90878, train_loss=5.9735775

Batch 51500, train_perplexity=356.19855, train_loss=5.8754883

Batch 51510, train_perplexity=308.1176, train_loss=5.7304816

Batch 51520, train_perplexity=372.47995, train_loss=5.920183

Batch 51530, train_perplexity=374.527, train_loss=5.925664

Batch 51540, train_perplexity=376.5965, train_loss=5.9311743

Batch 51550, train_perplexity=311.72687, train_loss=5.7421274

Batch 51560, train_perplexity=329.60327, train_loss=5.7978897

Batch 51570, train_perplexity=302.1656, train_loss=5.710975

Batch 51580, train_perplexity=326.14252, train_loss=5.7873344

Batch 51590, train_perplexity=347.22723, train_loss=5.8499794

Batch 51600, train_perplexity=348.47223, train_loss=5.8535585

Batch 51610, train_perplexity=327.20828, train_loss=5.790597

Batch 51620, train_perplexity=366.87448, train_loss=5.9050198

Batch 51630, train_perplexity=348.11615, train_loss=5.852536

Batch 51640, train_perplexity=387.3729, train_loss=5.959388

Batch 51650, train_perplexity=305.57016, train_loss=5.7221794

Batch 51660, train_perplexity=373.57565, train_loss=5.9231205

Batch 51670, train_perplexity=343.45987, train_loss=5.8390703

Batch 51680, train_perplexity=403.59927, train_loss=6.0004225

Batch 51690, train_perplexity=381.2996, train_loss=5.9435854

Batch 51700, train_perplexity=336.2641, train_loss=5.817897

Batch 51710, train_perplexity=365.40445, train_loss=5.901005

Batch 51720, train_perplexity=353.4035, train_loss=5.8676105

Batch 51730, train_perplexity=369.01425, train_loss=5.9108353

Batch 51740, train_perplexity=332.25934, train_loss=5.805916

Batch 51750, train_perplexity=342.63873, train_loss=5.8366766

Batch 51760, train_perplexity=342.89468, train_loss=5.8374233

Batch 51770, train_perplexity=335.8247, train_loss=5.8165894

Batch 51780, train_perplexity=386.80533, train_loss=5.9579215

Batch 51790, train_perplexity=415.5841, train_loss=6.029685

Batch 51800, train_perplexity=337.37115, train_loss=5.8211837

Batch 51810, train_perplexity=355.90704, train_loss=5.8746696

Batch 51820, train_perplexity=312.81894, train_loss=5.7456245

Batch 51830, train_perplexity=333.10837, train_loss=5.808468

Batch 51840, train_perplexity=311.91333, train_loss=5.7427254

Batch 51850, train_perplexity=349.16183, train_loss=5.8555355

Batch 51860, train_perplexity=338.9357, train_loss=5.8258104

Batch 51870, train_perplexity=358.0132, train_loss=5.88057

Batch 51880, train_perplexity=334.07742, train_loss=5.8113728

Batch 51890, train_perplexity=345.38898, train_loss=5.8446712

Batch 51900, train_perplexity=347.65118, train_loss=5.8511996

Batch 51910, train_perplexity=340.1393, train_loss=5.8293552

Batch 51920, train_perplexity=338.39053, train_loss=5.8242006

Batch 51930, train_perplexity=400.97244, train_loss=5.9938927

Batch 51940, train_perplexity=372.55756, train_loss=5.9203916

Batch 51950, train_perplexity=349.25708, train_loss=5.8558083

Batch 51960, train_perplexity=384.1709, train_loss=5.9510875

Batch 51970, train_perplexity=347.73904, train_loss=5.8514524

Batch 51980, train_perplexity=351.98056, train_loss=5.863576

Batch 51990, train_perplexity=331.2722, train_loss=5.8029404

Batch 52000, train_perplexity=365.85617, train_loss=5.9022403

Batch 52010, train_perplexity=333.40298, train_loss=5.809352

Batch 52020, train_perplexity=331.312, train_loss=5.8030605

Batch 52030, train_perplexity=376.41623, train_loss=5.9306955

Batch 52040, train_perplexity=420.24216, train_loss=6.040831

Batch 52050, train_perplexity=388.9903, train_loss=5.9635544

Batch 52060, train_perplexity=345.7439, train_loss=5.8456984

Batch 52070, train_perplexity=345.9819, train_loss=5.8463864

Batch 52080, train_perplexity=352.26196, train_loss=5.864375

Batch 52090, train_perplexity=360.5238, train_loss=5.887558

Batch 52100, train_perplexity=335.96582, train_loss=5.8170094

Batch 52110, train_perplexity=354.67438, train_loss=5.8712

Batch 52120, train_perplexity=334.6032, train_loss=5.8129454

Batch 52130, train_perplexity=337.32208, train_loss=5.8210382

Batch 52140, train_perplexity=318.25217, train_loss=5.762844

Batch 52150, train_perplexity=297.5591, train_loss=5.695613

Batch 52160, train_perplexity=320.236, train_loss=5.769058

Batch 52170, train_perplexity=370.4907, train_loss=5.9148283

Batch 52180, train_perplexity=321.9361, train_loss=5.774353

Batch 52190, train_perplexity=318.24762, train_loss=5.76283

Batch 52200, train_perplexity=377.06116, train_loss=5.9324074

Batch 52210, train_perplexity=347.31003, train_loss=5.850218

Batch 52220, train_perplexity=359.80078, train_loss=5.8855505

Batch 52230, train_perplexity=369.12723, train_loss=5.9111414

Batch 52240, train_perplexity=338.42053, train_loss=5.8242893

Batch 52250, train_perplexity=323.17206, train_loss=5.778185

Batch 52260, train_perplexity=321.61142, train_loss=5.773344

Batch 52270, train_perplexity=325.0582, train_loss=5.784004

Batch 52280, train_perplexity=362.09607, train_loss=5.8919096

Batch 52290, train_perplexity=345.9527, train_loss=5.846302

Batch 52300, train_perplexity=363.46198, train_loss=5.8956747

Batch 52310, train_perplexity=310.1443, train_loss=5.7370377

Batch 52320, train_perplexity=327.62796, train_loss=5.7918787

Batch 52330, train_perplexity=333.786, train_loss=5.8105

Batch 52340, train_perplexity=349.34735, train_loss=5.8560667

Batch 52350, train_perplexity=308.00525, train_loss=5.730117

Batch 52360, train_perplexity=346.45123, train_loss=5.847742

Batch 52370, train_perplexity=321.10696, train_loss=5.7717743

Batch 52380, train_perplexity=331.62686, train_loss=5.8040104

Batch 52390, train_perplexity=322.71118, train_loss=5.7767577

Batch 52400, train_perplexity=335.16434, train_loss=5.814621

Batch 52410, train_perplexity=338.57712, train_loss=5.824752

Batch 52420, train_perplexity=343.54767, train_loss=5.839326

Batch 52430, train_perplexity=330.43384, train_loss=5.8004065

Batch 52440, train_perplexity=375.9097, train_loss=5.929349

Batch 52450, train_perplexity=363.9632, train_loss=5.897053

Batch 52460, train_perplexity=340.90018, train_loss=5.8315897

Batch 52470, train_perplexity=331.2087, train_loss=5.8027487

Batch 52480, train_perplexity=335.62717, train_loss=5.816001

Batch 52490, train_perplexity=336.3045, train_loss=5.818017

Batch 52500, train_perplexity=335.65213, train_loss=5.8160753

Batch 52510, train_perplexity=355.5393, train_loss=5.873636

Batch 52520, train_perplexity=368.5124, train_loss=5.9094744

Batch 52530, train_perplexity=371.66168, train_loss=5.917984

Batch 52540, train_perplexity=356.519, train_loss=5.8763876

Batch 52550, train_perplexity=343.4725, train_loss=5.839107

Batch 52560, train_perplexity=351.39865, train_loss=5.8619213

Batch 52570, train_perplexity=332.5935, train_loss=5.806921

Batch 52580, train_perplexity=338.1315, train_loss=5.823435

Batch 52590, train_perplexity=314.54413, train_loss=5.7511244

Batch 52600, train_perplexity=363.54865, train_loss=5.895913

Batch 52610, train_perplexity=310.97876, train_loss=5.7397246

Batch 52620, train_perplexity=380.14407, train_loss=5.9405503

Batch 52630, train_perplexity=349.95807, train_loss=5.8578134

Batch 52640, train_perplexity=317.71664, train_loss=5.76116

Batch 52650, train_perplexity=381.74786, train_loss=5.9447603

Batch 52660, train_perplexity=322.76828, train_loss=5.7769346

Batch 52670, train_perplexity=332.69788, train_loss=5.807235

Batch 52680, train_perplexity=381.21143, train_loss=5.943354

Batch 52690, train_perplexity=345.50858, train_loss=5.8450174

Batch 52700, train_perplexity=350.8575, train_loss=5.86038

Batch 52710, train_perplexity=351.99634, train_loss=5.8636208
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52720, train_perplexity=359.50204, train_loss=5.88472

Batch 52730, train_perplexity=328.1054, train_loss=5.793335

Batch 52740, train_perplexity=319.8914, train_loss=5.7679815

Batch 52750, train_perplexity=354.38867, train_loss=5.870394

Batch 52760, train_perplexity=366.9665, train_loss=5.9052706

Batch 52770, train_perplexity=316.13364, train_loss=5.756165

Batch 52780, train_perplexity=369.3216, train_loss=5.911668

Batch 52790, train_perplexity=313.58926, train_loss=5.748084

Batch 52800, train_perplexity=320.16394, train_loss=5.768833

Batch 52810, train_perplexity=341.6924, train_loss=5.833911

Batch 52820, train_perplexity=316.74942, train_loss=5.758111

Batch 52830, train_perplexity=335.1525, train_loss=5.8145857

Batch 52840, train_perplexity=358.32697, train_loss=5.881446

Batch 52850, train_perplexity=328.02783, train_loss=5.7930984

Batch 52860, train_perplexity=332.5754, train_loss=5.8068666

Batch 52870, train_perplexity=325.98267, train_loss=5.7868443

Batch 52880, train_perplexity=347.23782, train_loss=5.85001

Batch 52890, train_perplexity=410.67838, train_loss=6.0178103

Batch 52900, train_perplexity=335.16275, train_loss=5.814616

Batch 52910, train_perplexity=404.2981, train_loss=6.0021524

Batch 52920, train_perplexity=311.57382, train_loss=5.7416363

Batch 52930, train_perplexity=336.4585, train_loss=5.818475

Batch 52940, train_perplexity=356.3885, train_loss=5.8760214

Batch 52950, train_perplexity=383.5045, train_loss=5.9493513

Batch 52960, train_perplexity=334.724, train_loss=5.8133063

Batch 52970, train_perplexity=369.16806, train_loss=5.911252

Batch 52980, train_perplexity=345.69016, train_loss=5.845543

Batch 52990, train_perplexity=322.03958, train_loss=5.7746744

Batch 53000, train_perplexity=346.59036, train_loss=5.8481436

Batch 53010, train_perplexity=305.74243, train_loss=5.722743

Batch 53020, train_perplexity=345.57547, train_loss=5.845211

Batch 53030, train_perplexity=317.96884, train_loss=5.7619534

Batch 53040, train_perplexity=365.8703, train_loss=5.902279

Batch 53050, train_perplexity=321.88666, train_loss=5.7741995

Batch 53060, train_perplexity=357.28314, train_loss=5.8785286

Batch 53070, train_perplexity=329.51212, train_loss=5.797613

Batch 53080, train_perplexity=334.81293, train_loss=5.813572

Batch 53090, train_perplexity=338.29953, train_loss=5.8239317

Batch 53100, train_perplexity=328.66617, train_loss=5.7950425

Batch 53110, train_perplexity=344.41635, train_loss=5.841851

Batch 53120, train_perplexity=375.12808, train_loss=5.9272676

Batch 53130, train_perplexity=350.37134, train_loss=5.8589935

Batch 53140, train_perplexity=333.02325, train_loss=5.8082123

Batch 53150, train_perplexity=339.9167, train_loss=5.8287005

Batch 53160, train_perplexity=329.83408, train_loss=5.7985897

Batch 53170, train_perplexity=303.81232, train_loss=5.71641

Batch 53180, train_perplexity=331.76636, train_loss=5.804431

Batch 53190, train_perplexity=339.92105, train_loss=5.8287134

Batch 53200, train_perplexity=339.1753, train_loss=5.826517

Batch 53210, train_perplexity=361.12738, train_loss=5.8892307

Batch 53220, train_perplexity=420.67963, train_loss=6.0418715

Batch 53230, train_perplexity=345.2072, train_loss=5.844145

Batch 53240, train_perplexity=337.3977, train_loss=5.8212624

Batch 53250, train_perplexity=321.64914, train_loss=5.7734613

Batch 53260, train_perplexity=378.8233, train_loss=5.93707

Batch 53270, train_perplexity=333.7913, train_loss=5.810516

Batch 53280, train_perplexity=341.41162, train_loss=5.833089

Batch 53290, train_perplexity=331.1913, train_loss=5.802696

Batch 53300, train_perplexity=311.30502, train_loss=5.740773

Batch 53310, train_perplexity=359.17715, train_loss=5.883816

Batch 53320, train_perplexity=342.5742, train_loss=5.8364882

Batch 53330, train_perplexity=332.48154, train_loss=5.8065844

Batch 53340, train_perplexity=324.98877, train_loss=5.7837906

Batch 53350, train_perplexity=340.55835, train_loss=5.8305864

Batch 53360, train_perplexity=359.6982, train_loss=5.8852654

Batch 53370, train_perplexity=300.56158, train_loss=5.7056527

Batch 53380, train_perplexity=351.00677, train_loss=5.8608055

Batch 53390, train_perplexity=342.6969, train_loss=5.8368464

Batch 53400, train_perplexity=379.33377, train_loss=5.9384165

Batch 53410, train_perplexity=373.7221, train_loss=5.9235125

Batch 53420, train_perplexity=320.61218, train_loss=5.770232

Batch 53430, train_perplexity=319.01886, train_loss=5.76525

Batch 53440, train_perplexity=332.1719, train_loss=5.8056526

Batch 53450, train_perplexity=347.2461, train_loss=5.8500338

Batch 53460, train_perplexity=290.72687, train_loss=5.6723843

Batch 53470, train_perplexity=331.4667, train_loss=5.8035274

Batch 53480, train_perplexity=356.02737, train_loss=5.8750076

Batch 53490, train_perplexity=348.70892, train_loss=5.8542376

Batch 53500, train_perplexity=358.44592, train_loss=5.881778

Batch 53510, train_perplexity=325.214, train_loss=5.7844834

Batch 53520, train_perplexity=354.5122, train_loss=5.870743

Batch 53530, train_perplexity=326.89423, train_loss=5.7896366

Batch 53540, train_perplexity=332.55765, train_loss=5.8068132

Batch 53550, train_perplexity=336.99347, train_loss=5.8200636

Batch 53560, train_perplexity=344.10214, train_loss=5.8409386

Batch 53570, train_perplexity=310.44168, train_loss=5.737996

Batch 53580, train_perplexity=298.81604, train_loss=5.699828

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00049-of-00100
Loaded 306055 sentences.
Finished loading
Batch 53590, train_perplexity=336.79428, train_loss=5.8194723

Batch 53600, train_perplexity=327.63046, train_loss=5.7918863

Batch 53610, train_perplexity=381.15253, train_loss=5.9431996

Batch 53620, train_perplexity=349.09424, train_loss=5.855342

Batch 53630, train_perplexity=359.36493, train_loss=5.8843384

Batch 53640, train_perplexity=322.15936, train_loss=5.7750463

Batch 53650, train_perplexity=340.39615, train_loss=5.83011

Batch 53660, train_perplexity=334.15005, train_loss=5.81159

Batch 53670, train_perplexity=383.2237, train_loss=5.948619

Batch 53680, train_perplexity=341.6349, train_loss=5.8337426

Batch 53690, train_perplexity=366.38638, train_loss=5.9036884

Batch 53700, train_perplexity=339.3874, train_loss=5.8271422

Batch 53710, train_perplexity=338.295, train_loss=5.8239183

Batch 53720, train_perplexity=330.42532, train_loss=5.8003807

Batch 53730, train_perplexity=360.97964, train_loss=5.8888216

Batch 53740, train_perplexity=358.16742, train_loss=5.8810005

Batch 53750, train_perplexity=360.03418, train_loss=5.886199

Batch 53760, train_perplexity=357.39935, train_loss=5.878854

Batch 53770, train_perplexity=326.4518, train_loss=5.7882824

Batch 53780, train_perplexity=338.87302, train_loss=5.8256254

Batch 53790, train_perplexity=347.12607, train_loss=5.849688

Batch 53800, train_perplexity=339.3463, train_loss=5.827021

Batch 53810, train_perplexity=323.21677, train_loss=5.778323

Batch 53820, train_perplexity=315.79285, train_loss=5.7550864

Batch 53830, train_perplexity=348.09955, train_loss=5.8524885

Batch 53840, train_perplexity=361.68106, train_loss=5.890763

Batch 53850, train_perplexity=327.18832, train_loss=5.790536

Batch 53860, train_perplexity=335.93347, train_loss=5.816913

Batch 53870, train_perplexity=353.7493, train_loss=5.8685884

Batch 53880, train_perplexity=340.5801, train_loss=5.8306503

Batch 53890, train_perplexity=356.80984, train_loss=5.877203

Batch 53900, train_perplexity=348.62497, train_loss=5.8539968

Batch 53910, train_perplexity=329.84854, train_loss=5.7986336

Batch 53920, train_perplexity=361.2338, train_loss=5.8895254

Batch 53930, train_perplexity=335.17346, train_loss=5.814648

Batch 53940, train_perplexity=322.17505, train_loss=5.775095

Batch 53950, train_perplexity=351.93307, train_loss=5.863441

Batch 53960, train_perplexity=346.41125, train_loss=5.8476267

Batch 53970, train_perplexity=338.1099, train_loss=5.823371
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53980, train_perplexity=337.32562, train_loss=5.8210487

Batch 53990, train_perplexity=341.1275, train_loss=5.8322563

Batch 54000, train_perplexity=374.33365, train_loss=5.9251475

Batch 54010, train_perplexity=340.39923, train_loss=5.830119

Batch 54020, train_perplexity=322.86185, train_loss=5.7772245

Batch 54030, train_perplexity=302.99103, train_loss=5.713703

Batch 54040, train_perplexity=352.3295, train_loss=5.864567

Batch 54050, train_perplexity=314.77548, train_loss=5.7518597

Batch 54060, train_perplexity=321.06592, train_loss=5.7716465

Batch 54070, train_perplexity=320.9972, train_loss=5.7714324

Batch 54080, train_perplexity=324.58487, train_loss=5.782547

Batch 54090, train_perplexity=326.11127, train_loss=5.7872386

Batch 54100, train_perplexity=336.25815, train_loss=5.817879

Batch 54110, train_perplexity=308.2183, train_loss=5.7308083

Batch 54120, train_perplexity=315.42923, train_loss=5.7539344

Batch 54130, train_perplexity=323.90286, train_loss=5.7804437

Batch 54140, train_perplexity=331.99423, train_loss=5.8051176

Batch 54150, train_perplexity=343.86038, train_loss=5.8402357

Batch 54160, train_perplexity=357.08728, train_loss=5.87798

Batch 54170, train_perplexity=370.83286, train_loss=5.9157515

Batch 54180, train_perplexity=344.91925, train_loss=5.8433104

Batch 54190, train_perplexity=339.923, train_loss=5.828719

Batch 54200, train_perplexity=288.18738, train_loss=5.663611

Batch 54210, train_perplexity=331.2796, train_loss=5.802963

Batch 54220, train_perplexity=332.03998, train_loss=5.8052554

Batch 54230, train_perplexity=337.3475, train_loss=5.8211136

Batch 54240, train_perplexity=358.46506, train_loss=5.881831

Batch 54250, train_perplexity=320.0296, train_loss=5.7684135

Batch 54260, train_perplexity=361.5545, train_loss=5.890413

Batch 54270, train_perplexity=312.11612, train_loss=5.7433753

Batch 54280, train_perplexity=340.29343, train_loss=5.829808

Batch 54290, train_perplexity=321.83478, train_loss=5.7740383

Batch 54300, train_perplexity=316.42245, train_loss=5.757078

Batch 54310, train_perplexity=347.5234, train_loss=5.850832

Batch 54320, train_perplexity=347.01767, train_loss=5.8493757

Batch 54330, train_perplexity=333.19604, train_loss=5.808731

Batch 54340, train_perplexity=294.42133, train_loss=5.685012

Batch 54350, train_perplexity=337.8024, train_loss=5.822461

Batch 54360, train_perplexity=328.00967, train_loss=5.793043

Batch 54370, train_perplexity=321.9975, train_loss=5.774544

Batch 54380, train_perplexity=299.74564, train_loss=5.7029343

Batch 54390, train_perplexity=339.16302, train_loss=5.826481

Batch 54400, train_perplexity=349.30603, train_loss=5.8559484

Batch 54410, train_perplexity=347.1443, train_loss=5.8497405

Batch 54420, train_perplexity=375.0063, train_loss=5.926943

Batch 54430, train_perplexity=331.6936, train_loss=5.8042116

Batch 54440, train_perplexity=317.5161, train_loss=5.7605286

Batch 54450, train_perplexity=315.60498, train_loss=5.7544913

Batch 54460, train_perplexity=340.08334, train_loss=5.8291907

Batch 54470, train_perplexity=349.77374, train_loss=5.8572865

Batch 54480, train_perplexity=370.3764, train_loss=5.91452

Batch 54490, train_perplexity=360.70297, train_loss=5.888055

Batch 54500, train_perplexity=358.31467, train_loss=5.8814116

Batch 54510, train_perplexity=312.7797, train_loss=5.745499

Batch 54520, train_perplexity=355.75415, train_loss=5.87424

Batch 54530, train_perplexity=410.49374, train_loss=6.0173607

Batch 54540, train_perplexity=330.16687, train_loss=5.799598

Batch 54550, train_perplexity=355.5171, train_loss=5.8735733

Batch 54560, train_perplexity=326.66452, train_loss=5.7889338

Batch 54570, train_perplexity=319.2692, train_loss=5.7660346

Batch 54580, train_perplexity=340.86508, train_loss=5.8314867

Batch 54590, train_perplexity=333.48184, train_loss=5.8095884

Batch 54600, train_perplexity=327.32846, train_loss=5.790964

Batch 54610, train_perplexity=304.55365, train_loss=5.7188473

Batch 54620, train_perplexity=306.11707, train_loss=5.7239676

Batch 54630, train_perplexity=350.32086, train_loss=5.8588495

Batch 54640, train_perplexity=365.74576, train_loss=5.9019384

Batch 54650, train_perplexity=339.23886, train_loss=5.8267045

Batch 54660, train_perplexity=348.2397, train_loss=5.852891

Batch 54670, train_perplexity=375.62857, train_loss=5.928601

Batch 54680, train_perplexity=310.13632, train_loss=5.737012

Batch 54690, train_perplexity=361.9269, train_loss=5.8914423

Batch 54700, train_perplexity=325.53873, train_loss=5.7854815

Batch 54710, train_perplexity=379.29416, train_loss=5.938312

Batch 54720, train_perplexity=323.08887, train_loss=5.7779274

Batch 54730, train_perplexity=374.51736, train_loss=5.925638

Batch 54740, train_perplexity=378.24966, train_loss=5.9355545

Batch 54750, train_perplexity=337.4199, train_loss=5.821328

Batch 54760, train_perplexity=326.58698, train_loss=5.7886963

Batch 54770, train_perplexity=336.16278, train_loss=5.8175955

Batch 54780, train_perplexity=350.1133, train_loss=5.858257

Batch 54790, train_perplexity=311.56818, train_loss=5.741618

Batch 54800, train_perplexity=373.14587, train_loss=5.9219694

Batch 54810, train_perplexity=382.5449, train_loss=5.946846

Batch 54820, train_perplexity=344.53165, train_loss=5.842186

Batch 54830, train_perplexity=335.79684, train_loss=5.8165064

Batch 54840, train_perplexity=393.37296, train_loss=5.974758

Batch 54850, train_perplexity=375.8948, train_loss=5.9293094

Batch 54860, train_perplexity=361.54605, train_loss=5.8903894

Batch 54870, train_perplexity=321.45642, train_loss=5.772862

Batch 54880, train_perplexity=329.6394, train_loss=5.7979994

Batch 54890, train_perplexity=336.8868, train_loss=5.819747

Batch 54900, train_perplexity=308.2621, train_loss=5.7309504

Batch 54910, train_perplexity=308.9748, train_loss=5.7332597

Batch 54920, train_perplexity=359.4794, train_loss=5.884657

Batch 54930, train_perplexity=303.45746, train_loss=5.7152414

Batch 54940, train_perplexity=326.7884, train_loss=5.789313

Batch 54950, train_perplexity=382.51132, train_loss=5.9467583

Batch 54960, train_perplexity=339.70816, train_loss=5.828087

Batch 54970, train_perplexity=307.97324, train_loss=5.730013

Batch 54980, train_perplexity=343.00278, train_loss=5.8377385

Batch 54990, train_perplexity=313.4807, train_loss=5.747738

Batch 55000, train_perplexity=290.91824, train_loss=5.6730423

Batch 55010, train_perplexity=355.14874, train_loss=5.8725367

Batch 55020, train_perplexity=357.32303, train_loss=5.87864

Batch 55030, train_perplexity=307.92007, train_loss=5.7298403

Batch 55040, train_perplexity=384.36053, train_loss=5.951581

Batch 55050, train_perplexity=307.5849, train_loss=5.728751

Batch 55060, train_perplexity=363.50046, train_loss=5.8957806

Batch 55070, train_perplexity=315.44293, train_loss=5.753978

Batch 55080, train_perplexity=344.66196, train_loss=5.842564

Batch 55090, train_perplexity=347.78317, train_loss=5.851579

Batch 55100, train_perplexity=354.19708, train_loss=5.8698535

Batch 55110, train_perplexity=333.32938, train_loss=5.809131

Batch 55120, train_perplexity=386.00125, train_loss=5.9558406

Batch 55130, train_perplexity=319.78555, train_loss=5.7676506

Batch 55140, train_perplexity=317.3142, train_loss=5.7598925

Batch 55150, train_perplexity=338.53644, train_loss=5.8246317

Batch 55160, train_perplexity=315.73398, train_loss=5.7549

Batch 55170, train_perplexity=330.6829, train_loss=5.80116

Batch 55180, train_perplexity=289.54318, train_loss=5.6683044

Batch 55190, train_perplexity=339.4204, train_loss=5.8272395

Batch 55200, train_perplexity=353.79703, train_loss=5.8687234

Batch 55210, train_perplexity=336.73712, train_loss=5.8193026

Batch 55220, train_perplexity=334.19086, train_loss=5.8117123

Batch 55230, train_perplexity=359.22888, train_loss=5.88396

Batch 55240, train_perplexity=348.09756, train_loss=5.852483

Batch 55250, train_perplexity=345.07654, train_loss=5.843766

Batch 55260, train_perplexity=363.35507, train_loss=5.8953805

Batch 55270, train_perplexity=363.07535, train_loss=5.8946104

Batch 55280, train_perplexity=339.3319, train_loss=5.8269787

Batch 55290, train_perplexity=299.5936, train_loss=5.702427
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 55300, train_perplexity=326.5904, train_loss=5.788707

Batch 55310, train_perplexity=351.30365, train_loss=5.861651

Batch 55320, train_perplexity=362.86453, train_loss=5.8940296

Batch 55330, train_perplexity=326.31674, train_loss=5.7878685

Batch 55340, train_perplexity=386.58127, train_loss=5.957342

Batch 55350, train_perplexity=321.74133, train_loss=5.773748

Batch 55360, train_perplexity=343.01047, train_loss=5.837761

Batch 55370, train_perplexity=352.3179, train_loss=5.864534

Batch 55380, train_perplexity=302.13965, train_loss=5.7108893

Batch 55390, train_perplexity=357.29184, train_loss=5.878553

Batch 55400, train_perplexity=327.73172, train_loss=5.7921953

Batch 55410, train_perplexity=327.7736, train_loss=5.792323

Batch 55420, train_perplexity=325.01355, train_loss=5.783867

Batch 55430, train_perplexity=309.55905, train_loss=5.735149

Batch 55440, train_perplexity=330.52115, train_loss=5.8006706

Batch 55450, train_perplexity=381.08438, train_loss=5.943021

Batch 55460, train_perplexity=337.06354, train_loss=5.8202715

Batch 55470, train_perplexity=316.40375, train_loss=5.757019

Batch 55480, train_perplexity=326.61032, train_loss=5.788768

Batch 55490, train_perplexity=317.19983, train_loss=5.759532

Batch 55500, train_perplexity=329.30478, train_loss=5.7969837

Batch 55510, train_perplexity=339.96222, train_loss=5.8288345

Batch 55520, train_perplexity=342.12054, train_loss=5.835163

Batch 55530, train_perplexity=316.2624, train_loss=5.7565722

Batch 55540, train_perplexity=349.2814, train_loss=5.855878

Batch 55550, train_perplexity=329.8885, train_loss=5.7987547

Batch 55560, train_perplexity=345.24506, train_loss=5.8442545

Batch 55570, train_perplexity=334.84915, train_loss=5.81368

Batch 55580, train_perplexity=347.21283, train_loss=5.849938

Batch 55590, train_perplexity=366.04446, train_loss=5.902755

Batch 55600, train_perplexity=309.3566, train_loss=5.7344947

Batch 55610, train_perplexity=329.1382, train_loss=5.796478

Batch 55620, train_perplexity=359.87695, train_loss=5.885762

Batch 55630, train_perplexity=335.95477, train_loss=5.8169765

Batch 55640, train_perplexity=333.4246, train_loss=5.809417

Batch 55650, train_perplexity=329.95456, train_loss=5.798955

Batch 55660, train_perplexity=339.53293, train_loss=5.827571

Batch 55670, train_perplexity=297.66556, train_loss=5.6959705

Batch 55680, train_perplexity=344.17896, train_loss=5.8411617

Batch 55690, train_perplexity=347.42233, train_loss=5.850541

Batch 55700, train_perplexity=325.87732, train_loss=5.786521

Batch 55710, train_perplexity=327.36, train_loss=5.7910604

Batch 55720, train_perplexity=348.24548, train_loss=5.8529077

Batch 55730, train_perplexity=331.14868, train_loss=5.8025675

Batch 55740, train_perplexity=293.48755, train_loss=5.681835

Batch 55750, train_perplexity=318.07846, train_loss=5.762298

Batch 55760, train_perplexity=324.13925, train_loss=5.781173

Batch 55770, train_perplexity=356.9962, train_loss=5.877725

Batch 55780, train_perplexity=308.0822, train_loss=5.7303667

Batch 55790, train_perplexity=319.95212, train_loss=5.7681713

Batch 55800, train_perplexity=336.93307, train_loss=5.8198843

Batch 55810, train_perplexity=337.94193, train_loss=5.822874

Batch 55820, train_perplexity=320.35025, train_loss=5.769415

Batch 55830, train_perplexity=360.0486, train_loss=5.886239

Batch 55840, train_perplexity=360.93387, train_loss=5.888695

Batch 55850, train_perplexity=324.24994, train_loss=5.7815146

Batch 55860, train_perplexity=329.44644, train_loss=5.797414

Batch 55870, train_perplexity=355.5203, train_loss=5.8735824

Batch 55880, train_perplexity=318.37936, train_loss=5.7632437

Batch 55890, train_perplexity=386.8769, train_loss=5.9581065

Batch 55900, train_perplexity=366.96194, train_loss=5.905258

Batch 55910, train_perplexity=339.84424, train_loss=5.8284874

Batch 55920, train_perplexity=352.3636, train_loss=5.8646636

Batch 55930, train_perplexity=362.1541, train_loss=5.89207

Batch 55940, train_perplexity=286.74387, train_loss=5.6585894

Batch 55950, train_perplexity=297.85837, train_loss=5.696618

Batch 55960, train_perplexity=352.60394, train_loss=5.8653455

Batch 55970, train_perplexity=351.87854, train_loss=5.863286

Batch 55980, train_perplexity=338.7625, train_loss=5.8252993

Batch 55990, train_perplexity=356.72513, train_loss=5.8769655

Batch 56000, train_perplexity=336.61447, train_loss=5.8189383

Batch 56010, train_perplexity=323.6506, train_loss=5.7796645

Batch 56020, train_perplexity=325.84933, train_loss=5.786435

Batch 56030, train_perplexity=337.10117, train_loss=5.820383

Batch 56040, train_perplexity=353.99567, train_loss=5.8692846

Batch 56050, train_perplexity=327.21268, train_loss=5.7906103

Batch 56060, train_perplexity=315.5443, train_loss=5.754299

Batch 56070, train_perplexity=322.5175, train_loss=5.7761574

Batch 56080, train_perplexity=355.7572, train_loss=5.8742485

Batch 56090, train_perplexity=342.40582, train_loss=5.8359966

Batch 56100, train_perplexity=356.88385, train_loss=5.8774104

Batch 56110, train_perplexity=333.27408, train_loss=5.808965

Batch 56120, train_perplexity=338.61053, train_loss=5.8248506

Batch 56130, train_perplexity=348.95078, train_loss=5.854931

Batch 56140, train_perplexity=378.8616, train_loss=5.937171

Batch 56150, train_perplexity=361.90656, train_loss=5.891386

Batch 56160, train_perplexity=380.20044, train_loss=5.9406986

Batch 56170, train_perplexity=327.53174, train_loss=5.791585

Batch 56180, train_perplexity=295.04562, train_loss=5.68713

Batch 56190, train_perplexity=335.93588, train_loss=5.8169203

Batch 56200, train_perplexity=345.00085, train_loss=5.843547

Batch 56210, train_perplexity=292.62784, train_loss=5.6789017

Batch 56220, train_perplexity=337.93356, train_loss=5.8228493

Batch 56230, train_perplexity=349.5303, train_loss=5.8565903

Batch 56240, train_perplexity=323.11014, train_loss=5.777993

Batch 56250, train_perplexity=365.80786, train_loss=5.902108

Batch 56260, train_perplexity=309.96066, train_loss=5.7364454

Batch 56270, train_perplexity=365.67984, train_loss=5.901758

Batch 56280, train_perplexity=315.04037, train_loss=5.752701

Batch 56290, train_perplexity=340.59927, train_loss=5.8307066

Batch 56300, train_perplexity=344.1074, train_loss=5.840954

Batch 56310, train_perplexity=332.8321, train_loss=5.807638

Batch 56320, train_perplexity=324.93173, train_loss=5.783615

Batch 56330, train_perplexity=383.42404, train_loss=5.9491415

Batch 56340, train_perplexity=315.90384, train_loss=5.755438

Batch 56350, train_perplexity=348.81186, train_loss=5.8545327

Batch 56360, train_perplexity=300.12494, train_loss=5.704199

Batch 56370, train_perplexity=352.87762, train_loss=5.8661213

Batch 56380, train_perplexity=336.01422, train_loss=5.8171535

Batch 56390, train_perplexity=326.55334, train_loss=5.7885933

Batch 56400, train_perplexity=364.90646, train_loss=5.899641

Batch 56410, train_perplexity=310.93414, train_loss=5.739581

Batch 56420, train_perplexity=326.41946, train_loss=5.788183

Batch 56430, train_perplexity=321.91675, train_loss=5.774293

Batch 56440, train_perplexity=334.45804, train_loss=5.8125114

Batch 56450, train_perplexity=357.48083, train_loss=5.8790817

Batch 56460, train_perplexity=337.75955, train_loss=5.8223343

Batch 56470, train_perplexity=320.63235, train_loss=5.770295

Batch 56480, train_perplexity=339.49472, train_loss=5.8274584

Batch 56490, train_perplexity=289.17093, train_loss=5.667018

Batch 56500, train_perplexity=325.1833, train_loss=5.784389

Batch 56510, train_perplexity=313.86273, train_loss=5.7489557

Batch 56520, train_perplexity=277.12598, train_loss=5.624472

Batch 56530, train_perplexity=335.58588, train_loss=5.815878

Batch 56540, train_perplexity=299.14252, train_loss=5.70092

Batch 56550, train_perplexity=328.4606, train_loss=5.794417

Batch 56560, train_perplexity=343.0317, train_loss=5.837823

Batch 56570, train_perplexity=392.15747, train_loss=5.9716635

Batch 56580, train_perplexity=311.638, train_loss=5.7418423

Batch 56590, train_perplexity=335.28262, train_loss=5.814974

Batch 56600, train_perplexity=325.11182, train_loss=5.784169

Batch 56610, train_perplexity=317.22223, train_loss=5.7596025
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 56620, train_perplexity=357.6752, train_loss=5.8796253

Batch 56630, train_perplexity=357.35898, train_loss=5.878741

Batch 56640, train_perplexity=317.71255, train_loss=5.761147

Batch 56650, train_perplexity=300.14038, train_loss=5.7042503

Batch 56660, train_perplexity=324.32803, train_loss=5.7817554

Batch 56670, train_perplexity=287.45834, train_loss=5.661078

Batch 56680, train_perplexity=329.76425, train_loss=5.798378

Batch 56690, train_perplexity=317.62604, train_loss=5.7608747

Batch 56700, train_perplexity=358.97067, train_loss=5.8832407

Batch 56710, train_perplexity=330.35065, train_loss=5.8001547

Batch 56720, train_perplexity=333.62564, train_loss=5.8100195

Batch 56730, train_perplexity=342.00668, train_loss=5.8348303

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00094-of-00100
Loaded 306835 sentences.
Finished loading
Batch 56740, train_perplexity=296.16763, train_loss=5.6909256

Batch 56750, train_perplexity=329.55737, train_loss=5.7977505

Batch 56760, train_perplexity=324.79016, train_loss=5.7831793

Batch 56770, train_perplexity=340.93936, train_loss=5.8317046

Batch 56780, train_perplexity=356.702, train_loss=5.8769007

Batch 56790, train_perplexity=319.30072, train_loss=5.7661333

Batch 56800, train_perplexity=327.18146, train_loss=5.790515

Batch 56810, train_perplexity=365.65543, train_loss=5.9016914

Batch 56820, train_perplexity=342.2749, train_loss=5.835614

Batch 56830, train_perplexity=321.28247, train_loss=5.7723207

Batch 56840, train_perplexity=337.91678, train_loss=5.8227997

Batch 56850, train_perplexity=349.87463, train_loss=5.857575

Batch 56860, train_perplexity=306.73102, train_loss=5.725971

Batch 56870, train_perplexity=364.64313, train_loss=5.898919

Batch 56880, train_perplexity=288.53955, train_loss=5.664832

Batch 56890, train_perplexity=303.68387, train_loss=5.715987

Batch 56900, train_perplexity=300.04422, train_loss=5.70393

Batch 56910, train_perplexity=288.1352, train_loss=5.6634297

Batch 56920, train_perplexity=303.81595, train_loss=5.716422

Batch 56930, train_perplexity=350.92776, train_loss=5.8605804

Batch 56940, train_perplexity=348.9072, train_loss=5.854806

Batch 56950, train_perplexity=392.22217, train_loss=5.9718285

Batch 56960, train_perplexity=287.11, train_loss=5.6598654

Batch 56970, train_perplexity=329.28595, train_loss=5.7969265

Batch 56980, train_perplexity=288.3206, train_loss=5.664073

Batch 56990, train_perplexity=329.9714, train_loss=5.799006

Batch 57000, train_perplexity=321.79764, train_loss=5.773923

Batch 57010, train_perplexity=314.41608, train_loss=5.750717

Batch 57020, train_perplexity=333.12457, train_loss=5.8085165

Batch 57030, train_perplexity=306.48935, train_loss=5.725183

Batch 57040, train_perplexity=346.48807, train_loss=5.8478484

Batch 57050, train_perplexity=325.6757, train_loss=5.785902

Batch 57060, train_perplexity=331.79578, train_loss=5.8045197

Batch 57070, train_perplexity=299.68906, train_loss=5.7027454

Batch 57080, train_perplexity=330.1088, train_loss=5.7994223

Batch 57090, train_perplexity=326.54337, train_loss=5.788563

Batch 57100, train_perplexity=332.41846, train_loss=5.8063946

Batch 57110, train_perplexity=306.57806, train_loss=5.7254725

Batch 57120, train_perplexity=322.71024, train_loss=5.776755

Batch 57130, train_perplexity=307.52655, train_loss=5.7285614

Batch 57140, train_perplexity=327.7242, train_loss=5.7921724

Batch 57150, train_perplexity=341.25473, train_loss=5.832629

Batch 57160, train_perplexity=313.80753, train_loss=5.74878

Batch 57170, train_perplexity=329.1952, train_loss=5.796651

Batch 57180, train_perplexity=280.01437, train_loss=5.634841

Batch 57190, train_perplexity=315.45316, train_loss=5.75401

Batch 57200, train_perplexity=342.81146, train_loss=5.8371806

Batch 57210, train_perplexity=346.93527, train_loss=5.8491383

Batch 57220, train_perplexity=358.9611, train_loss=5.883214

Batch 57230, train_perplexity=305.29404, train_loss=5.7212753

Batch 57240, train_perplexity=328.30762, train_loss=5.793951

Batch 57250, train_perplexity=349.33603, train_loss=5.8560343

Batch 57260, train_perplexity=333.00815, train_loss=5.808167

Batch 57270, train_perplexity=338.78284, train_loss=5.8253593

Batch 57280, train_perplexity=332.51422, train_loss=5.8066826

Batch 57290, train_perplexity=374.97018, train_loss=5.9268465

Batch 57300, train_perplexity=319.9532, train_loss=5.7681746

Batch 57310, train_perplexity=317.66983, train_loss=5.7610126

Batch 57320, train_perplexity=354.46927, train_loss=5.8706217

Batch 57330, train_perplexity=360.33508, train_loss=5.8870344

Batch 57340, train_perplexity=324.65048, train_loss=5.782749

Batch 57350, train_perplexity=304.5355, train_loss=5.7187877

Batch 57360, train_perplexity=317.03168, train_loss=5.7590017

Batch 57370, train_perplexity=350.9231, train_loss=5.860567

Batch 57380, train_perplexity=333.1163, train_loss=5.8084917

Batch 57390, train_perplexity=347.95898, train_loss=5.8520846

Batch 57400, train_perplexity=311.9023, train_loss=5.74269

Batch 57410, train_perplexity=353.1503, train_loss=5.866894

Batch 57420, train_perplexity=312.45117, train_loss=5.744448

Batch 57430, train_perplexity=326.54834, train_loss=5.788578

Batch 57440, train_perplexity=276.2424, train_loss=5.621279

Batch 57450, train_perplexity=333.92072, train_loss=5.8109035

Batch 57460, train_perplexity=334.9758, train_loss=5.8140583

Batch 57470, train_perplexity=320.19507, train_loss=5.7689304

Batch 57480, train_perplexity=296.8661, train_loss=5.693281

Batch 57490, train_perplexity=322.5032, train_loss=5.776113

Batch 57500, train_perplexity=318.93854, train_loss=5.7649984

Batch 57510, train_perplexity=341.94635, train_loss=5.834654

Batch 57520, train_perplexity=333.97263, train_loss=5.811059

Batch 57530, train_perplexity=324.8219, train_loss=5.783277

Batch 57540, train_perplexity=320.1166, train_loss=5.7686853

Batch 57550, train_perplexity=338.39084, train_loss=5.8242016

Batch 57560, train_perplexity=333.92197, train_loss=5.8109074

Batch 57570, train_perplexity=345.5435, train_loss=5.8451185

Batch 57580, train_perplexity=298.9165, train_loss=5.7001643

Batch 57590, train_perplexity=354.40607, train_loss=5.8704433

Batch 57600, train_perplexity=317.34082, train_loss=5.7599764

Batch 57610, train_perplexity=296.5475, train_loss=5.6922073

Batch 57620, train_perplexity=333.40125, train_loss=5.8093467

Batch 57630, train_perplexity=324.5875, train_loss=5.782555

Batch 57640, train_perplexity=329.01505, train_loss=5.7961035

Batch 57650, train_perplexity=340.8719, train_loss=5.8315067

Batch 57660, train_perplexity=319.83435, train_loss=5.767803

Batch 57670, train_perplexity=337.29764, train_loss=5.820966

Batch 57680, train_perplexity=344.3267, train_loss=5.841591

Batch 57690, train_perplexity=354.04663, train_loss=5.8694286

Batch 57700, train_perplexity=315.0189, train_loss=5.7526326

Batch 57710, train_perplexity=319.26187, train_loss=5.7660117

Batch 57720, train_perplexity=321.84277, train_loss=5.774063

Batch 57730, train_perplexity=310.2073, train_loss=5.737241

Batch 57740, train_perplexity=331.4041, train_loss=5.8033385

Batch 57750, train_perplexity=286.45523, train_loss=5.6575823

Batch 57760, train_perplexity=346.41623, train_loss=5.847641

Batch 57770, train_perplexity=311.25085, train_loss=5.740599

Batch 57780, train_perplexity=366.28156, train_loss=5.9034023

Batch 57790, train_perplexity=332.06833, train_loss=5.805341

Batch 57800, train_perplexity=321.8004, train_loss=5.7739315

Batch 57810, train_perplexity=354.08698, train_loss=5.8695426

Batch 57820, train_perplexity=344.25378, train_loss=5.841379

Batch 57830, train_perplexity=285.86548, train_loss=5.6555214

Batch 57840, train_perplexity=333.14554, train_loss=5.8085794

Batch 57850, train_perplexity=355.29407, train_loss=5.872946

Batch 57860, train_perplexity=313.90433, train_loss=5.7490883

Batch 57870, train_perplexity=325.9491, train_loss=5.7867413
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 57880, train_perplexity=294.90833, train_loss=5.6866646

Batch 57890, train_perplexity=327.46335, train_loss=5.791376

Batch 57900, train_perplexity=341.84543, train_loss=5.8343587

Batch 57910, train_perplexity=305.20538, train_loss=5.720985

Batch 57920, train_perplexity=325.09726, train_loss=5.7841244

Batch 57930, train_perplexity=336.66165, train_loss=5.8190784

Batch 57940, train_perplexity=358.21353, train_loss=5.8811293

Batch 57950, train_perplexity=309.77686, train_loss=5.7358522

Batch 57960, train_perplexity=320.47217, train_loss=5.7697954

Batch 57970, train_perplexity=299.28177, train_loss=5.7013855

Batch 57980, train_perplexity=352.32245, train_loss=5.864547

Batch 57990, train_perplexity=334.85156, train_loss=5.8136873

Batch 58000, train_perplexity=346.95282, train_loss=5.849189

Batch 58010, train_perplexity=320.81372, train_loss=5.7708607

Batch 58020, train_perplexity=356.8197, train_loss=5.8772306

Batch 58030, train_perplexity=337.8388, train_loss=5.822569

Batch 58040, train_perplexity=311.6725, train_loss=5.741953

Batch 58050, train_perplexity=337.9184, train_loss=5.8228045

Batch 58060, train_perplexity=359.91455, train_loss=5.8858666

Batch 58070, train_perplexity=353.35178, train_loss=5.867464

Batch 58080, train_perplexity=296.16946, train_loss=5.690932

Batch 58090, train_perplexity=317.56668, train_loss=5.760688

Batch 58100, train_perplexity=299.0076, train_loss=5.700469

Batch 58110, train_perplexity=342.2047, train_loss=5.835409

Batch 58120, train_perplexity=313.74408, train_loss=5.7485776

Batch 58130, train_perplexity=351.22275, train_loss=5.8614206

Batch 58140, train_perplexity=319.95822, train_loss=5.7681904

Batch 58150, train_perplexity=358.49292, train_loss=5.881909

Batch 58160, train_perplexity=373.81476, train_loss=5.9237604

Batch 58170, train_perplexity=342.40466, train_loss=5.8359933

Batch 58180, train_perplexity=321.8136, train_loss=5.7739725

Batch 58190, train_perplexity=321.70712, train_loss=5.7736416

Batch 58200, train_perplexity=331.65182, train_loss=5.8040857

Batch 58210, train_perplexity=301.78848, train_loss=5.7097263

Batch 58220, train_perplexity=361.06625, train_loss=5.8890615

Batch 58230, train_perplexity=274.25372, train_loss=5.6140537

Batch 58240, train_perplexity=339.2837, train_loss=5.8268366

Batch 58250, train_perplexity=336.77036, train_loss=5.8194013

Batch 58260, train_perplexity=316.12354, train_loss=5.756133

Batch 58270, train_perplexity=326.215, train_loss=5.7875566

Batch 58280, train_perplexity=288.18726, train_loss=5.6636105

Batch 58290, train_perplexity=347.78632, train_loss=5.8515882

Batch 58300, train_perplexity=304.08524, train_loss=5.717308

Batch 58310, train_perplexity=336.79492, train_loss=5.819474

Batch 58320, train_perplexity=306.6032, train_loss=5.7255545

Batch 58330, train_perplexity=357.82755, train_loss=5.880051

Batch 58340, train_perplexity=305.79727, train_loss=5.7229223

Batch 58350, train_perplexity=303.66415, train_loss=5.7159224

Batch 58360, train_perplexity=329.40607, train_loss=5.7972913

Batch 58370, train_perplexity=308.0051, train_loss=5.7301164

Batch 58380, train_perplexity=330.65625, train_loss=5.8010793

Batch 58390, train_perplexity=323.54303, train_loss=5.779332

Batch 58400, train_perplexity=301.9016, train_loss=5.710101

Batch 58410, train_perplexity=335.6752, train_loss=5.816144

Batch 58420, train_perplexity=312.1916, train_loss=5.743617

Batch 58430, train_perplexity=284.60928, train_loss=5.6511173

Batch 58440, train_perplexity=328.13483, train_loss=5.7934246

Batch 58450, train_perplexity=359.93222, train_loss=5.8859158

Batch 58460, train_perplexity=330.85544, train_loss=5.8016815

Batch 58470, train_perplexity=317.0063, train_loss=5.7589216

Batch 58480, train_perplexity=316.65277, train_loss=5.757806

Batch 58490, train_perplexity=317.88696, train_loss=5.761696

Batch 58500, train_perplexity=312.50375, train_loss=5.7446165

Batch 58510, train_perplexity=293.67484, train_loss=5.682473

Batch 58520, train_perplexity=293.56857, train_loss=5.6821113

Batch 58530, train_perplexity=332.12756, train_loss=5.805519

Batch 58540, train_perplexity=322.40985, train_loss=5.7758236

Batch 58550, train_perplexity=320.6834, train_loss=5.7704544

Batch 58560, train_perplexity=328.2882, train_loss=5.793892

Batch 58570, train_perplexity=342.71454, train_loss=5.836898

Batch 58580, train_perplexity=308.50015, train_loss=5.7317224

Batch 58590, train_perplexity=289.73474, train_loss=5.668966

Batch 58600, train_perplexity=351.3303, train_loss=5.8617268

Batch 58610, train_perplexity=324.65854, train_loss=5.782774

Batch 58620, train_perplexity=303.45544, train_loss=5.7152348

Batch 58630, train_perplexity=307.33612, train_loss=5.727942

Batch 58640, train_perplexity=370.34427, train_loss=5.914433

Batch 58650, train_perplexity=310.5181, train_loss=5.738242

Batch 58660, train_perplexity=334.94592, train_loss=5.813969

Batch 58670, train_perplexity=284.3155, train_loss=5.6500845

Batch 58680, train_perplexity=311.6166, train_loss=5.7417736

Batch 58690, train_perplexity=312.47723, train_loss=5.7445316

Batch 58700, train_perplexity=314.18765, train_loss=5.7499905

Batch 58710, train_perplexity=329.6111, train_loss=5.7979136

Batch 58720, train_perplexity=313.15714, train_loss=5.746705

Batch 58730, train_perplexity=326.0113, train_loss=5.786932

Batch 58740, train_perplexity=326.51938, train_loss=5.7884893

Batch 58750, train_perplexity=338.07602, train_loss=5.823271

Batch 58760, train_perplexity=299.54904, train_loss=5.702278

Batch 58770, train_perplexity=318.305, train_loss=5.76301

Batch 58780, train_perplexity=329.88376, train_loss=5.7987404

Batch 58790, train_perplexity=314.49612, train_loss=5.750972

Batch 58800, train_perplexity=329.763, train_loss=5.798374

Batch 58810, train_perplexity=307.6339, train_loss=5.7289104

Batch 58820, train_perplexity=329.2803, train_loss=5.7969093

Batch 58830, train_perplexity=291.818, train_loss=5.6761303

Batch 58840, train_perplexity=279.99316, train_loss=5.634765

Batch 58850, train_perplexity=312.5774, train_loss=5.744852

Batch 58860, train_perplexity=304.73438, train_loss=5.7194405

Batch 58870, train_perplexity=339.7723, train_loss=5.8282757

Batch 58880, train_perplexity=337.42215, train_loss=5.821335

Batch 58890, train_perplexity=354.53995, train_loss=5.870821

Batch 58900, train_perplexity=303.25986, train_loss=5.71459

Batch 58910, train_perplexity=371.83026, train_loss=5.9184375

Batch 58920, train_perplexity=349.7257, train_loss=5.857149

Batch 58930, train_perplexity=338.0019, train_loss=5.8230515

Batch 58940, train_perplexity=302.71634, train_loss=5.712796

Batch 58950, train_perplexity=319.55386, train_loss=5.766926

Batch 58960, train_perplexity=346.22333, train_loss=5.847084

Batch 58970, train_perplexity=316.14645, train_loss=5.7562056

Batch 58980, train_perplexity=330.66238, train_loss=5.801098

Batch 58990, train_perplexity=291.60516, train_loss=5.6754007

Batch 59000, train_perplexity=324.86636, train_loss=5.783414

Batch 59010, train_perplexity=326.04953, train_loss=5.7870493

Batch 59020, train_perplexity=327.8491, train_loss=5.7925534

Batch 59030, train_perplexity=376.76657, train_loss=5.931626

Batch 59040, train_perplexity=318.16174, train_loss=5.76256

Batch 59050, train_perplexity=318.57437, train_loss=5.763856

Batch 59060, train_perplexity=340.44565, train_loss=5.8302555

Batch 59070, train_perplexity=372.4018, train_loss=5.9199734

Batch 59080, train_perplexity=304.02637, train_loss=5.7171144

Batch 59090, train_perplexity=316.39377, train_loss=5.7569876

Batch 59100, train_perplexity=354.8369, train_loss=5.8716583

Batch 59110, train_perplexity=341.61322, train_loss=5.833679

Batch 59120, train_perplexity=395.75446, train_loss=5.980794

Batch 59130, train_perplexity=318.05164, train_loss=5.7622137

Batch 59140, train_perplexity=304.78333, train_loss=5.719601

Batch 59150, train_perplexity=306.3136, train_loss=5.7246094

Batch 59160, train_perplexity=317.99188, train_loss=5.762026

Batch 59170, train_perplexity=305.41663, train_loss=5.721677

Batch 59180, train_perplexity=328.56445, train_loss=5.794733

Batch 59190, train_perplexity=322.23343, train_loss=5.775276
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 59200, train_perplexity=291.3615, train_loss=5.674565

Batch 59210, train_perplexity=339.13358, train_loss=5.826394

Batch 59220, train_perplexity=307.7099, train_loss=5.7291574

Batch 59230, train_perplexity=318.0134, train_loss=5.7620935

Batch 59240, train_perplexity=290.74185, train_loss=5.6724358

Batch 59250, train_perplexity=294.23257, train_loss=5.6843705

Batch 59260, train_perplexity=350.5635, train_loss=5.859542

Batch 59270, train_perplexity=325.09818, train_loss=5.784127

Batch 59280, train_perplexity=321.13284, train_loss=5.771855

Batch 59290, train_perplexity=329.92844, train_loss=5.798876

Batch 59300, train_perplexity=328.71082, train_loss=5.7951784

Batch 59310, train_perplexity=315.8064, train_loss=5.7551293

Batch 59320, train_perplexity=287.4947, train_loss=5.6612043

Batch 59330, train_perplexity=358.38437, train_loss=5.881606

Batch 59340, train_perplexity=322.73563, train_loss=5.7768335

Batch 59350, train_perplexity=331.5647, train_loss=5.803823

Batch 59360, train_perplexity=323.81345, train_loss=5.7801676

Batch 59370, train_perplexity=311.50458, train_loss=5.741414

Batch 59380, train_perplexity=372.46927, train_loss=5.9201546

Batch 59390, train_perplexity=286.95312, train_loss=5.659319

Batch 59400, train_perplexity=303.31598, train_loss=5.714775

Batch 59410, train_perplexity=302.8275, train_loss=5.7131634

Batch 59420, train_perplexity=345.7657, train_loss=5.8457613

Batch 59430, train_perplexity=307.043, train_loss=5.726988

Batch 59440, train_perplexity=307.64212, train_loss=5.728937

Batch 59450, train_perplexity=336.49652, train_loss=5.818588

Batch 59460, train_perplexity=330.80984, train_loss=5.8015437

Batch 59470, train_perplexity=292.01538, train_loss=5.6768064

Batch 59480, train_perplexity=354.90427, train_loss=5.871848

Batch 59490, train_perplexity=331.50827, train_loss=5.803653

Batch 59500, train_perplexity=350.50534, train_loss=5.859376

Batch 59510, train_perplexity=305.28137, train_loss=5.721234

Batch 59520, train_perplexity=290.16513, train_loss=5.67045

Batch 59530, train_perplexity=327.6639, train_loss=5.7919884

Batch 59540, train_perplexity=352.53116, train_loss=5.865139

Batch 59550, train_perplexity=336.01245, train_loss=5.817148

Batch 59560, train_perplexity=326.56674, train_loss=5.7886343

Batch 59570, train_perplexity=296.00443, train_loss=5.6903744

Batch 59580, train_perplexity=339.55154, train_loss=5.8276258

Batch 59590, train_perplexity=309.945, train_loss=5.736395

Batch 59600, train_perplexity=310.8535, train_loss=5.7393217

Batch 59610, train_perplexity=291.65982, train_loss=5.675588

Batch 59620, train_perplexity=304.85498, train_loss=5.719836

Batch 59630, train_perplexity=346.58707, train_loss=5.848134

Batch 59640, train_perplexity=307.72015, train_loss=5.729191

Batch 59650, train_perplexity=331.57104, train_loss=5.803842

Batch 59660, train_perplexity=293.45242, train_loss=5.6817155

Batch 59670, train_perplexity=284.83792, train_loss=5.6519203

Batch 59680, train_perplexity=303.12137, train_loss=5.7141333

Batch 59690, train_perplexity=325.83722, train_loss=5.786398

Batch 59700, train_perplexity=351.15494, train_loss=5.8612275

Batch 59710, train_perplexity=286.87967, train_loss=5.659063

Batch 59720, train_perplexity=327.06915, train_loss=5.7901716

Batch 59730, train_perplexity=336.45334, train_loss=5.8184595

Batch 59740, train_perplexity=324.66968, train_loss=5.7828083

Batch 59750, train_perplexity=310.80103, train_loss=5.739153

Batch 59760, train_perplexity=292.56366, train_loss=5.6786823

Batch 59770, train_perplexity=307.99615, train_loss=5.7300873

Batch 59780, train_perplexity=312.75436, train_loss=5.745418

Batch 59790, train_perplexity=323.23773, train_loss=5.778388

Batch 59800, train_perplexity=307.5173, train_loss=5.7285314

Batch 59810, train_perplexity=319.229, train_loss=5.7659087

Batch 59820, train_perplexity=300.1839, train_loss=5.7043953

Batch 59830, train_perplexity=308.3297, train_loss=5.7311697

Batch 59840, train_perplexity=320.9545, train_loss=5.7712994

Batch 59850, train_perplexity=337.99963, train_loss=5.823045

Batch 59860, train_perplexity=312.31592, train_loss=5.744015

Batch 59870, train_perplexity=317.78662, train_loss=5.76138

Batch 59880, train_perplexity=309.60275, train_loss=5.73529

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100
Loaded 306116 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00024-of-00100
Loaded 306116 sentences.
Finished loading
Batch 59890, train_perplexity=348.34232, train_loss=5.8531857

Batch 59900, train_perplexity=335.22235, train_loss=5.814794

Batch 59910, train_perplexity=350.51202, train_loss=5.859395

Batch 59920, train_perplexity=319.19293, train_loss=5.7657957

Batch 59930, train_perplexity=352.7827, train_loss=5.8658524

Batch 59940, train_perplexity=319.81848, train_loss=5.7677536

Batch 59950, train_perplexity=329.32504, train_loss=5.797045

Batch 59960, train_perplexity=341.0403, train_loss=5.8320007

Batch 59970, train_perplexity=347.3289, train_loss=5.850272

Batch 59980, train_perplexity=316.30508, train_loss=5.756707

Batch 59990, train_perplexity=321.12808, train_loss=5.77184

Batch 60000, train_perplexity=338.38745, train_loss=5.8241916

Batch 60010, train_perplexity=312.66684, train_loss=5.745138

Batch 60020, train_perplexity=354.03583, train_loss=5.869398

Batch 60030, train_perplexity=312.81104, train_loss=5.7455993

Batch 60040, train_perplexity=357.56027, train_loss=5.879304

Batch 60050, train_perplexity=314.85355, train_loss=5.7521076

Batch 60060, train_perplexity=351.71716, train_loss=5.8628273

Batch 60070, train_perplexity=320.93552, train_loss=5.77124

Batch 60080, train_perplexity=312.04813, train_loss=5.7431574

Batch 60090, train_perplexity=312.73065, train_loss=5.7453423

Batch 60100, train_perplexity=315.8665, train_loss=5.7553196

Batch 60110, train_perplexity=293.5942, train_loss=5.6821985

Batch 60120, train_perplexity=332.09207, train_loss=5.8054123

Batch 60130, train_perplexity=303.23413, train_loss=5.714505

Batch 60140, train_perplexity=297.65277, train_loss=5.6959276

Batch 60150, train_perplexity=311.95618, train_loss=5.7428627

Batch 60160, train_perplexity=319.3919, train_loss=5.766419

Batch 60170, train_perplexity=315.49197, train_loss=5.754133

Batch 60180, train_perplexity=302.97974, train_loss=5.713666

Batch 60190, train_perplexity=295.46094, train_loss=5.6885366

Batch 60200, train_perplexity=315.99484, train_loss=5.755726

Batch 60210, train_perplexity=329.80765, train_loss=5.7985096

Batch 60220, train_perplexity=355.40793, train_loss=5.873266

Batch 60230, train_perplexity=333.7313, train_loss=5.810336

Batch 60240, train_perplexity=347.81732, train_loss=5.8516774

Batch 60250, train_perplexity=297.1023, train_loss=5.6940765

Batch 60260, train_perplexity=320.2134, train_loss=5.7689877

Batch 60270, train_perplexity=332.10632, train_loss=5.805455

Batch 60280, train_perplexity=290.74435, train_loss=5.6724443

Batch 60290, train_perplexity=297.95126, train_loss=5.69693

Batch 60300, train_perplexity=305.2684, train_loss=5.7211914

Batch 60310, train_perplexity=352.65808, train_loss=5.865499

Batch 60320, train_perplexity=316.16937, train_loss=5.756278

Batch 60330, train_perplexity=356.2429, train_loss=5.8756127

Batch 60340, train_perplexity=315.1245, train_loss=5.752968

Batch 60350, train_perplexity=337.60144, train_loss=5.821866

Batch 60360, train_perplexity=342.72287, train_loss=5.836922

Batch 60370, train_perplexity=286.2829, train_loss=5.6569805

Batch 60380, train_perplexity=345.78314, train_loss=5.845812

Batch 60390, train_perplexity=321.29044, train_loss=5.7723455

Batch 60400, train_perplexity=339.35422, train_loss=5.8270445

Batch 60410, train_perplexity=324.5027, train_loss=5.782294

Batch 60420, train_perplexity=304.2973, train_loss=5.718005

Batch 60430, train_perplexity=313.3183, train_loss=5.7472196

Batch 60440, train_perplexity=307.12207, train_loss=5.7272453

Batch 60450, train_perplexity=327.02768, train_loss=5.790045
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 60460, train_perplexity=289.4515, train_loss=5.667988

Batch 60470, train_perplexity=352.76675, train_loss=5.865807

Batch 60480, train_perplexity=312.08755, train_loss=5.7432837

Batch 60490, train_perplexity=304.9639, train_loss=5.7201934

Batch 60500, train_perplexity=322.39017, train_loss=5.7757626

Batch 60510, train_perplexity=339.39145, train_loss=5.827154

Batch 60520, train_perplexity=312.92947, train_loss=5.745978

Batch 60530, train_perplexity=309.26178, train_loss=5.734188

Batch 60540, train_perplexity=272.23785, train_loss=5.606676

Batch 60550, train_perplexity=334.73438, train_loss=5.8133373

Batch 60560, train_perplexity=310.64514, train_loss=5.7386513

Batch 60570, train_perplexity=288.07803, train_loss=5.6632314

Batch 60580, train_perplexity=303.3766, train_loss=5.714975

Batch 60590, train_perplexity=319.20966, train_loss=5.765848

Batch 60600, train_perplexity=336.6965, train_loss=5.819182

Batch 60610, train_perplexity=326.1288, train_loss=5.7872925

Batch 60620, train_perplexity=324.16537, train_loss=5.781254

Batch 60630, train_perplexity=339.2562, train_loss=5.8267555

Batch 60640, train_perplexity=324.63254, train_loss=5.782694

Batch 60650, train_perplexity=319.1296, train_loss=5.7655973

Batch 60660, train_perplexity=324.64893, train_loss=5.7827444

Batch 60670, train_perplexity=318.97443, train_loss=5.765111

Batch 60680, train_perplexity=304.89978, train_loss=5.719983

Batch 60690, train_perplexity=331.54004, train_loss=5.8037486

Batch 60700, train_perplexity=314.3261, train_loss=5.750431

Batch 60710, train_perplexity=299.71707, train_loss=5.702839

Batch 60720, train_perplexity=352.15918, train_loss=5.8640833

Batch 60730, train_perplexity=342.68414, train_loss=5.836809

Batch 60740, train_perplexity=297.4259, train_loss=5.695165

Batch 60750, train_perplexity=300.1899, train_loss=5.7044153

Batch 60760, train_perplexity=308.34912, train_loss=5.7312326

Batch 60770, train_perplexity=339.7347, train_loss=5.828165

Batch 60780, train_perplexity=319.645, train_loss=5.767211

Batch 60790, train_perplexity=326.28796, train_loss=5.7877803

Batch 60800, train_perplexity=292.99423, train_loss=5.680153

Batch 60810, train_perplexity=326.28345, train_loss=5.7877665

Batch 60820, train_perplexity=321.73383, train_loss=5.7737246

Batch 60830, train_perplexity=342.2617, train_loss=5.8355756

Batch 60840, train_perplexity=310.0949, train_loss=5.7368784

Batch 60850, train_perplexity=343.00507, train_loss=5.837745

Batch 60860, train_perplexity=324.08502, train_loss=5.781006

Batch 60870, train_perplexity=283.75452, train_loss=5.6481094

Batch 60880, train_perplexity=287.55295, train_loss=5.661407

Batch 60890, train_perplexity=312.79672, train_loss=5.7455535

Batch 60900, train_perplexity=302.98898, train_loss=5.7136965

Batch 60910, train_perplexity=330.2086, train_loss=5.7997246

Batch 60920, train_perplexity=293.10574, train_loss=5.6805334

Batch 60930, train_perplexity=314.121, train_loss=5.7497783

Batch 60940, train_perplexity=313.688, train_loss=5.748399

Batch 60950, train_perplexity=308.64642, train_loss=5.7321963

Batch 60960, train_perplexity=294.34274, train_loss=5.684745

Batch 60970, train_perplexity=353.02573, train_loss=5.866541

Batch 60980, train_perplexity=305.44022, train_loss=5.721754

Batch 60990, train_perplexity=315.91467, train_loss=5.755472

Batch 61000, train_perplexity=269.81906, train_loss=5.5977516

Batch 61010, train_perplexity=309.21487, train_loss=5.7340364

Batch 61020, train_perplexity=358.77832, train_loss=5.8827047

Batch 61030, train_perplexity=328.68762, train_loss=5.795108

Batch 61040, train_perplexity=304.77563, train_loss=5.719576

Batch 61050, train_perplexity=301.58463, train_loss=5.7090507

Batch 61060, train_perplexity=314.16818, train_loss=5.7499285

Batch 61070, train_perplexity=312.46204, train_loss=5.744483

Batch 61080, train_perplexity=308.41647, train_loss=5.731451

Batch 61090, train_perplexity=319.61588, train_loss=5.76712

Batch 61100, train_perplexity=316.30298, train_loss=5.7567005

Batch 61110, train_perplexity=338.8071, train_loss=5.825431

Batch 61120, train_perplexity=286.73785, train_loss=5.6585684

Batch 61130, train_perplexity=332.73373, train_loss=5.8073425

Batch 61140, train_perplexity=306.1454, train_loss=5.72406

Batch 61150, train_perplexity=310.1585, train_loss=5.7370834

Batch 61160, train_perplexity=299.99344, train_loss=5.7037606

Batch 61170, train_perplexity=316.74142, train_loss=5.7580857

Batch 61180, train_perplexity=303.1734, train_loss=5.714305

Batch 61190, train_perplexity=319.35065, train_loss=5.7662897

Batch 61200, train_perplexity=336.60196, train_loss=5.818901

Batch 61210, train_perplexity=340.9987, train_loss=5.8318787

Batch 61220, train_perplexity=347.11334, train_loss=5.8496513

Batch 61230, train_perplexity=339.35004, train_loss=5.827032

Batch 61240, train_perplexity=300.93015, train_loss=5.706878

Batch 61250, train_perplexity=327.13153, train_loss=5.7903624

Batch 61260, train_perplexity=331.83945, train_loss=5.8046513

Batch 61270, train_perplexity=332.02417, train_loss=5.8052077

Batch 61280, train_perplexity=358.85773, train_loss=5.882926

Batch 61290, train_perplexity=286.2013, train_loss=5.6566954

Batch 61300, train_perplexity=352.13904, train_loss=5.864026

Batch 61310, train_perplexity=311.0175, train_loss=5.739849

Batch 61320, train_perplexity=305.51758, train_loss=5.7220073

Batch 61330, train_perplexity=326.24362, train_loss=5.7876444

Batch 61340, train_perplexity=312.49988, train_loss=5.744604

Batch 61350, train_perplexity=292.1268, train_loss=5.677188

Batch 61360, train_perplexity=297.35388, train_loss=5.694923

Batch 61370, train_perplexity=288.88647, train_loss=5.6660337

Batch 61380, train_perplexity=324.88898, train_loss=5.7834835

Batch 61390, train_perplexity=310.50803, train_loss=5.7382097

Batch 61400, train_perplexity=312.06894, train_loss=5.743224

Batch 61410, train_perplexity=341.725, train_loss=5.8340063

Batch 61420, train_perplexity=329.30917, train_loss=5.796997

Batch 61430, train_perplexity=354.00647, train_loss=5.869315

Batch 61440, train_perplexity=314.47723, train_loss=5.7509117

Batch 61450, train_perplexity=320.0032, train_loss=5.768331

Batch 61460, train_perplexity=330.9594, train_loss=5.8019958

Batch 61470, train_perplexity=302.8772, train_loss=5.7133274

Batch 61480, train_perplexity=308.55975, train_loss=5.7319155

Batch 61490, train_perplexity=327.70328, train_loss=5.7921085

Batch 61500, train_perplexity=375.34262, train_loss=5.9278393

Batch 61510, train_perplexity=303.1611, train_loss=5.7142644

Batch 61520, train_perplexity=277.02292, train_loss=5.6241

Batch 61530, train_perplexity=300.0943, train_loss=5.704097

Batch 61540, train_perplexity=310.03696, train_loss=5.7366915

Batch 61550, train_perplexity=300.99158, train_loss=5.7070823

Batch 61560, train_perplexity=331.33334, train_loss=5.803125

Batch 61570, train_perplexity=341.15744, train_loss=5.832344

Batch 61580, train_perplexity=315.4076, train_loss=5.7538657

Batch 61590, train_perplexity=311.16626, train_loss=5.7403274

Batch 61600, train_perplexity=360.53857, train_loss=5.887599

Batch 61610, train_perplexity=349.55264, train_loss=5.856654

Batch 61620, train_perplexity=296.3261, train_loss=5.6914606

Batch 61630, train_perplexity=285.62814, train_loss=5.6546907

Batch 61640, train_perplexity=283.72366, train_loss=5.6480007

Batch 61650, train_perplexity=327.16165, train_loss=5.7904544

Batch 61660, train_perplexity=328.09073, train_loss=5.79329

Batch 61670, train_perplexity=309.1267, train_loss=5.7337513

Batch 61680, train_perplexity=302.7793, train_loss=5.713004

Batch 61690, train_perplexity=326.13754, train_loss=5.787319

Batch 61700, train_perplexity=313.1013, train_loss=5.7465267

Batch 61710, train_perplexity=314.3954, train_loss=5.7506514

Batch 61720, train_perplexity=322.8994, train_loss=5.777341

Batch 61730, train_perplexity=325.2405, train_loss=5.784565

Batch 61740, train_perplexity=330.0271, train_loss=5.799175

Batch 61750, train_perplexity=295.0172, train_loss=5.6870337

Batch 61760, train_perplexity=338.5529, train_loss=5.8246803

Batch 61770, train_perplexity=323.5327, train_loss=5.7793
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 61780, train_perplexity=311.27682, train_loss=5.7406826

Batch 61790, train_perplexity=319.70627, train_loss=5.7674026

Batch 61800, train_perplexity=290.18298, train_loss=5.6705117

Batch 61810, train_perplexity=328.39545, train_loss=5.7942185

Batch 61820, train_perplexity=312.40512, train_loss=5.744301

Batch 61830, train_perplexity=328.6179, train_loss=5.7948956

Batch 61840, train_perplexity=288.22052, train_loss=5.663726

Batch 61850, train_perplexity=334.00192, train_loss=5.8111467

Batch 61860, train_perplexity=312.14545, train_loss=5.743469

Batch 61870, train_perplexity=296.84146, train_loss=5.693198

Batch 61880, train_perplexity=303.07483, train_loss=5.7139797

Batch 61890, train_perplexity=297.24655, train_loss=5.694562

Batch 61900, train_perplexity=316.596, train_loss=5.7576265

Batch 61910, train_perplexity=313.05292, train_loss=5.746372

Batch 61920, train_perplexity=323.51868, train_loss=5.779257

Batch 61930, train_perplexity=305.0014, train_loss=5.7203164

Batch 61940, train_perplexity=292.51428, train_loss=5.6785135

Batch 61950, train_perplexity=317.84787, train_loss=5.761573

Batch 61960, train_perplexity=308.47046, train_loss=5.731626

Batch 61970, train_perplexity=283.52216, train_loss=5.64729

Batch 61980, train_perplexity=289.631, train_loss=5.6686077

Batch 61990, train_perplexity=343.27716, train_loss=5.838538

Batch 62000, train_perplexity=352.67307, train_loss=5.8655415

Batch 62010, train_perplexity=330.44077, train_loss=5.8004274

Batch 62020, train_perplexity=308.32678, train_loss=5.73116

Batch 62030, train_perplexity=317.62515, train_loss=5.760872

Batch 62040, train_perplexity=312.4108, train_loss=5.744319

Batch 62050, train_perplexity=345.61337, train_loss=5.8453207

Batch 62060, train_perplexity=294.5595, train_loss=5.685481

Batch 62070, train_perplexity=304.32806, train_loss=5.7181063

Batch 62080, train_perplexity=339.04984, train_loss=5.826147

Batch 62090, train_perplexity=294.20718, train_loss=5.684284

Batch 62100, train_perplexity=303.27258, train_loss=5.714632

Batch 62110, train_perplexity=322.34006, train_loss=5.775607

Batch 62120, train_perplexity=324.3806, train_loss=5.7819176

Batch 62130, train_perplexity=322.67346, train_loss=5.776641

Batch 62140, train_perplexity=324.9844, train_loss=5.783777

Batch 62150, train_perplexity=286.14493, train_loss=5.6564984

Batch 62160, train_perplexity=330.90625, train_loss=5.801835

Batch 62170, train_perplexity=293.113, train_loss=5.680558

Batch 62180, train_perplexity=299.68304, train_loss=5.7027254

Batch 62190, train_perplexity=292.5419, train_loss=5.678608

Batch 62200, train_perplexity=293.56396, train_loss=5.6820955

Batch 62210, train_perplexity=277.4752, train_loss=5.6257315

Batch 62220, train_perplexity=287.39557, train_loss=5.6608596

Batch 62230, train_perplexity=314.8339, train_loss=5.752045

Batch 62240, train_perplexity=319.61115, train_loss=5.767105

Batch 62250, train_perplexity=291.61545, train_loss=5.675436

Batch 62260, train_perplexity=320.79507, train_loss=5.7708025

Batch 62270, train_perplexity=286.9731, train_loss=5.6593885

Batch 62280, train_perplexity=323.5341, train_loss=5.7793045

Batch 62290, train_perplexity=319.11896, train_loss=5.765564

Batch 62300, train_perplexity=302.82233, train_loss=5.713146

Batch 62310, train_perplexity=293.32104, train_loss=5.6812677

Batch 62320, train_perplexity=292.23407, train_loss=5.677555

Batch 62330, train_perplexity=342.5247, train_loss=5.836344

Batch 62340, train_perplexity=358.6025, train_loss=5.8822145

Batch 62350, train_perplexity=297.26755, train_loss=5.6946325

Batch 62360, train_perplexity=339.39047, train_loss=5.8271513

Batch 62370, train_perplexity=329.89102, train_loss=5.7987623

Batch 62380, train_perplexity=277.4249, train_loss=5.6255503

Batch 62390, train_perplexity=316.99887, train_loss=5.7588983

Batch 62400, train_perplexity=340.25998, train_loss=5.82971

Batch 62410, train_perplexity=284.84946, train_loss=5.651961

Batch 62420, train_perplexity=310.5058, train_loss=5.7382026

Batch 62430, train_perplexity=317.9126, train_loss=5.7617764

Batch 62440, train_perplexity=305.92123, train_loss=5.7233276

Batch 62450, train_perplexity=370.87885, train_loss=5.9158754

Batch 62460, train_perplexity=299.35513, train_loss=5.7016306

Batch 62470, train_perplexity=309.3264, train_loss=5.734397

Batch 62480, train_perplexity=314.0316, train_loss=5.7494936

Batch 62490, train_perplexity=304.05466, train_loss=5.7172074

Batch 62500, train_perplexity=304.66055, train_loss=5.719198

Batch 62510, train_perplexity=261.69037, train_loss=5.567162

Batch 62520, train_perplexity=369.28146, train_loss=5.911559

Batch 62530, train_perplexity=329.5987, train_loss=5.797876

Batch 62540, train_perplexity=301.2565, train_loss=5.707962

Batch 62550, train_perplexity=328.99323, train_loss=5.796037

Batch 62560, train_perplexity=322.5941, train_loss=5.776395

Batch 62570, train_perplexity=327.75546, train_loss=5.792268

Batch 62580, train_perplexity=311.48737, train_loss=5.7413588

Batch 62590, train_perplexity=303.49768, train_loss=5.715374

Batch 62600, train_perplexity=329.67432, train_loss=5.7981052

Batch 62610, train_perplexity=315.0928, train_loss=5.752867

Batch 62620, train_perplexity=292.9976, train_loss=5.6801643

Batch 62630, train_perplexity=312.25815, train_loss=5.74383

Batch 62640, train_perplexity=319.6872, train_loss=5.767343

Batch 62650, train_perplexity=318.8248, train_loss=5.764642

Batch 62660, train_perplexity=319.32294, train_loss=5.766203

Batch 62670, train_perplexity=303.80667, train_loss=5.7163916

Batch 62680, train_perplexity=335.03442, train_loss=5.8142333

Batch 62690, train_perplexity=302.3861, train_loss=5.7117047

Batch 62700, train_perplexity=276.53815, train_loss=5.622349

Batch 62710, train_perplexity=307.2066, train_loss=5.7275205

Batch 62720, train_perplexity=336.93884, train_loss=5.8199015

Batch 62730, train_perplexity=286.9378, train_loss=5.6592655

Batch 62740, train_perplexity=312.79178, train_loss=5.7455378

Batch 62750, train_perplexity=277.66803, train_loss=5.626426

Batch 62760, train_perplexity=329.8004, train_loss=5.7984877

Batch 62770, train_perplexity=296.075, train_loss=5.690613

Batch 62780, train_perplexity=277.98184, train_loss=5.627556

Batch 62790, train_perplexity=323.07376, train_loss=5.7778807

Batch 62800, train_perplexity=332.3915, train_loss=5.8063135

Batch 62810, train_perplexity=275.99197, train_loss=5.620372

Batch 62820, train_perplexity=370.02777, train_loss=5.913578

Batch 62830, train_perplexity=280.56143, train_loss=5.6367927

Batch 62840, train_perplexity=305.3604, train_loss=5.721493

Batch 62850, train_perplexity=308.30017, train_loss=5.731074

Batch 62860, train_perplexity=299.60675, train_loss=5.702471

Batch 62870, train_perplexity=303.80246, train_loss=5.7163777

Batch 62880, train_perplexity=315.23032, train_loss=5.7533035

Batch 62890, train_perplexity=310.15732, train_loss=5.7370796

Batch 62900, train_perplexity=309.67422, train_loss=5.735521

Batch 62910, train_perplexity=313.8313, train_loss=5.7488556

Batch 62920, train_perplexity=271.9926, train_loss=5.605775

Batch 62930, train_perplexity=290.59494, train_loss=5.6719303

Batch 62940, train_perplexity=271.22433, train_loss=5.6029463

Batch 62950, train_perplexity=358.3263, train_loss=5.881444

Batch 62960, train_perplexity=293.02356, train_loss=5.680253

Batch 62970, train_perplexity=300.15643, train_loss=5.7043037

Batch 62980, train_perplexity=316.97183, train_loss=5.758813

Batch 62990, train_perplexity=287.68118, train_loss=5.661853

Batch 63000, train_perplexity=328.1023, train_loss=5.7933254

Batch 63010, train_perplexity=352.288, train_loss=5.864449

Batch 63020, train_perplexity=310.22223, train_loss=5.737289

Batch 63030, train_perplexity=311.96777, train_loss=5.7429

Batch 63040, train_perplexity=331.33255, train_loss=5.8031225

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306068 sentences.
Finished loading
Batch 63050, train_perplexity=353.22037, train_loss=5.867092

Batch 63060, train_perplexity=298.9905, train_loss=5.700412

Batch 63070, train_perplexity=336.32376, train_loss=5.818074

Batch 63080, train_perplexity=342.2083, train_loss=5.8354197

Batch 63090, train_perplexity=340.31744, train_loss=5.829879

Batch 63100, train_perplexity=326.68103, train_loss=5.7889843

Batch 63110, train_perplexity=320.35666, train_loss=5.769435

Batch 63120, train_perplexity=305.02075, train_loss=5.72038

Batch 63130, train_perplexity=322.96548, train_loss=5.7775455

Batch 63140, train_perplexity=346.83142, train_loss=5.848839

Batch 63150, train_perplexity=303.64578, train_loss=5.715862

Batch 63160, train_perplexity=355.2473, train_loss=5.872814

Batch 63170, train_perplexity=345.02667, train_loss=5.8436217

Batch 63180, train_perplexity=306.40826, train_loss=5.7249184

Batch 63190, train_perplexity=307.24234, train_loss=5.727637

Batch 63200, train_perplexity=327.3906, train_loss=5.791154

Batch 63210, train_perplexity=273.09647, train_loss=5.609825

Batch 63220, train_perplexity=304.6957, train_loss=5.7193136

Batch 63230, train_perplexity=309.1907, train_loss=5.7339582

Batch 63240, train_perplexity=301.81265, train_loss=5.7098064

Batch 63250, train_perplexity=306.06598, train_loss=5.7238007

Batch 63260, train_perplexity=329.91208, train_loss=5.798826

Batch 63270, train_perplexity=279.27698, train_loss=5.632204

Batch 63280, train_perplexity=322.0881, train_loss=5.774825

Batch 63290, train_perplexity=315.52808, train_loss=5.7542477

Batch 63300, train_perplexity=338.6039, train_loss=5.824831

Batch 63310, train_perplexity=298.62616, train_loss=5.6991925

Batch 63320, train_perplexity=306.1124, train_loss=5.7239523

Batch 63330, train_perplexity=280.7054, train_loss=5.6373057

Batch 63340, train_perplexity=313.74094, train_loss=5.7485676

Batch 63350, train_perplexity=300.77078, train_loss=5.7063484

Batch 63360, train_perplexity=315.80804, train_loss=5.7551346

Batch 63370, train_perplexity=288.43115, train_loss=5.6644564

Batch 63380, train_perplexity=275.47842, train_loss=5.6185093

Batch 63390, train_perplexity=322.6304, train_loss=5.7765074

Batch 63400, train_perplexity=310.9509, train_loss=5.739635

Batch 63410, train_perplexity=311.44666, train_loss=5.741228

Batch 63420, train_perplexity=302.4096, train_loss=5.7117825

Batch 63430, train_perplexity=313.7113, train_loss=5.748473

Batch 63440, train_perplexity=342.85577, train_loss=5.83731

Batch 63450, train_perplexity=307.54547, train_loss=5.728623

Batch 63460, train_perplexity=294.96024, train_loss=5.6868405

Batch 63470, train_perplexity=284.7407, train_loss=5.651579

Batch 63480, train_perplexity=345.0229, train_loss=5.843611

Batch 63490, train_perplexity=312.7059, train_loss=5.745263

Batch 63500, train_perplexity=323.98196, train_loss=5.780688

Batch 63510, train_perplexity=299.37326, train_loss=5.701691

Batch 63520, train_perplexity=281.38403, train_loss=5.6397204

Batch 63530, train_perplexity=303.34, train_loss=5.7148542

Batch 63540, train_perplexity=301.23438, train_loss=5.7078886

Batch 63550, train_perplexity=304.23462, train_loss=5.717799

Batch 63560, train_perplexity=292.9189, train_loss=5.679896

Batch 63570, train_perplexity=323.37277, train_loss=5.7788057

Batch 63580, train_perplexity=295.5972, train_loss=5.6889977

Batch 63590, train_perplexity=302.86823, train_loss=5.713298

Batch 63600, train_perplexity=312.89053, train_loss=5.7458534

Batch 63610, train_perplexity=330.9681, train_loss=5.802022

Batch 63620, train_perplexity=315.9763, train_loss=5.755667

Batch 63630, train_perplexity=304.56412, train_loss=5.7188816

Batch 63640, train_perplexity=294.4125, train_loss=5.684982

Batch 63650, train_perplexity=321.3064, train_loss=5.772395

Batch 63660, train_perplexity=291.16513, train_loss=5.6738906

Batch 63670, train_perplexity=309.11832, train_loss=5.733724

Batch 63680, train_perplexity=315.03827, train_loss=5.752694

Batch 63690, train_perplexity=319.36374, train_loss=5.7663307

Batch 63700, train_perplexity=348.7189, train_loss=5.854266

Batch 63710, train_perplexity=293.42932, train_loss=5.681637

Batch 63720, train_perplexity=292.75766, train_loss=5.679345

Batch 63730, train_perplexity=330.89062, train_loss=5.801788

Batch 63740, train_perplexity=313.91483, train_loss=5.7491217

Batch 63750, train_perplexity=296.37332, train_loss=5.69162

Batch 63760, train_perplexity=337.106, train_loss=5.8203974

Batch 63770, train_perplexity=284.92322, train_loss=5.65222

Batch 63780, train_perplexity=274.3907, train_loss=5.614553

Batch 63790, train_perplexity=296.34576, train_loss=5.691527

Batch 63800, train_perplexity=313.6368, train_loss=5.7482357

Batch 63810, train_perplexity=296.6544, train_loss=5.692568

Batch 63820, train_perplexity=292.49475, train_loss=5.678447

Batch 63830, train_perplexity=324.29153, train_loss=5.781643

Batch 63840, train_perplexity=318.21274, train_loss=5.76272

Batch 63850, train_perplexity=287.58545, train_loss=5.66152

Batch 63860, train_perplexity=307.31195, train_loss=5.7278633

Batch 63870, train_perplexity=307.52625, train_loss=5.7285604

Batch 63880, train_perplexity=311.80597, train_loss=5.742381

Batch 63890, train_perplexity=374.4474, train_loss=5.9254513

Batch 63900, train_perplexity=349.28372, train_loss=5.8558846

Batch 63910, train_perplexity=332.65503, train_loss=5.807106

Batch 63920, train_perplexity=309.18628, train_loss=5.733944

Batch 63930, train_perplexity=292.8565, train_loss=5.6796827

Batch 63940, train_perplexity=307.5855, train_loss=5.728753

Batch 63950, train_perplexity=335.70688, train_loss=5.8162384

Batch 63960, train_perplexity=315.3008, train_loss=5.753527

Batch 63970, train_perplexity=329.19238, train_loss=5.7966423

Batch 63980, train_perplexity=320.00793, train_loss=5.768346

Batch 63990, train_perplexity=316.9351, train_loss=5.758697

Batch 64000, train_perplexity=326.93005, train_loss=5.7897463

Batch 64010, train_perplexity=292.175, train_loss=5.677353

Batch 64020, train_perplexity=328.36084, train_loss=5.794113

Batch 64030, train_perplexity=318.0877, train_loss=5.762327

Batch 64040, train_perplexity=404.42612, train_loss=6.002469

Batch 64050, train_perplexity=331.09973, train_loss=5.8024197

Batch 64060, train_perplexity=315.37796, train_loss=5.753772

Batch 64070, train_perplexity=314.00763, train_loss=5.7494173

Batch 64080, train_perplexity=337.3504, train_loss=5.821122

Batch 64090, train_perplexity=283.22473, train_loss=5.6462407

Batch 64100, train_perplexity=314.56693, train_loss=5.751197

Batch 64110, train_perplexity=311.94843, train_loss=5.742838

Batch 64120, train_perplexity=308.45236, train_loss=5.7315674

Batch 64130, train_perplexity=338.40182, train_loss=5.824234

Batch 64140, train_perplexity=322.1305, train_loss=5.7749567

Batch 64150, train_perplexity=298.5981, train_loss=5.6990986

Batch 64160, train_perplexity=343.3829, train_loss=5.838846

Batch 64170, train_perplexity=326.6063, train_loss=5.7887554

Batch 64180, train_perplexity=314.80762, train_loss=5.7519617

Batch 64190, train_perplexity=289.7592, train_loss=5.66905

Batch 64200, train_perplexity=317.2785, train_loss=5.75978

Batch 64210, train_perplexity=316.87692, train_loss=5.7585135

Batch 64220, train_perplexity=305.5161, train_loss=5.7220025

Batch 64230, train_perplexity=327.5074, train_loss=5.7915106

Batch 64240, train_perplexity=339.95447, train_loss=5.8288116

Batch 64250, train_perplexity=313.68826, train_loss=5.7483997

Batch 64260, train_perplexity=273.17267, train_loss=5.610104

Batch 64270, train_perplexity=328.8237, train_loss=5.7955217

Batch 64280, train_perplexity=325.6794, train_loss=5.7859135

Batch 64290, train_perplexity=314.98163, train_loss=5.7525144

Batch 64300, train_perplexity=291.35776, train_loss=5.674552

Batch 64310, train_perplexity=289.11523, train_loss=5.6668253

Batch 64320, train_perplexity=289.92947, train_loss=5.6696377

Batch 64330, train_perplexity=265.0759, train_loss=5.580016

Batch 64340, train_perplexity=303.29343, train_loss=5.7147007

Batch 64350, train_perplexity=320.05496, train_loss=5.7684927

Batch 64360, train_perplexity=295.944, train_loss=5.6901703
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 64370, train_perplexity=330.57663, train_loss=5.8008385

Batch 64380, train_perplexity=275.741, train_loss=5.619462

Batch 64390, train_perplexity=317.37033, train_loss=5.7600694

Batch 64400, train_perplexity=327.2626, train_loss=5.790763

Batch 64410, train_perplexity=345.6193, train_loss=5.845338

Batch 64420, train_perplexity=296.57632, train_loss=5.6923046

Batch 64430, train_perplexity=303.9707, train_loss=5.7169313

Batch 64440, train_perplexity=308.9714, train_loss=5.7332487

Batch 64450, train_perplexity=320.05252, train_loss=5.768485

Batch 64460, train_perplexity=312.30087, train_loss=5.743967

Batch 64470, train_perplexity=323.7162, train_loss=5.779867

Batch 64480, train_perplexity=303.50232, train_loss=5.7153893

Batch 64490, train_perplexity=299.43893, train_loss=5.7019105

Batch 64500, train_perplexity=309.05362, train_loss=5.733515

Batch 64510, train_perplexity=281.19183, train_loss=5.639037

Batch 64520, train_perplexity=312.8586, train_loss=5.7457514

Batch 64530, train_perplexity=274.75336, train_loss=5.615874

Batch 64540, train_perplexity=300.2088, train_loss=5.7044783

Batch 64550, train_perplexity=328.94635, train_loss=5.7958946

Batch 64560, train_perplexity=309.6463, train_loss=5.7354307

Batch 64570, train_perplexity=288.23123, train_loss=5.663763

Batch 64580, train_perplexity=323.5088, train_loss=5.7792263

Batch 64590, train_perplexity=310.51334, train_loss=5.738227

Batch 64600, train_perplexity=303.79596, train_loss=5.7163563

Batch 64610, train_perplexity=297.7439, train_loss=5.6962337

Batch 64620, train_perplexity=313.44846, train_loss=5.747635

Batch 64630, train_perplexity=291.8219, train_loss=5.6761436

Batch 64640, train_perplexity=333.01166, train_loss=5.8081775

Batch 64650, train_perplexity=327.06854, train_loss=5.7901697

Batch 64660, train_perplexity=317.87164, train_loss=5.7616477

Batch 64670, train_perplexity=318.96988, train_loss=5.7650967

Batch 64680, train_perplexity=335.7919, train_loss=5.8164916

Batch 64690, train_perplexity=364.1939, train_loss=5.8976865

Batch 64700, train_perplexity=329.096, train_loss=5.7963495

Batch 64710, train_perplexity=303.00546, train_loss=5.713751

Batch 64720, train_perplexity=316.7147, train_loss=5.7580013

Batch 64730, train_perplexity=297.4276, train_loss=5.695171

Batch 64740, train_perplexity=323.8292, train_loss=5.780216

Batch 64750, train_perplexity=311.41638, train_loss=5.741131

Batch 64760, train_perplexity=306.06815, train_loss=5.723808

Batch 64770, train_perplexity=322.51672, train_loss=5.776155

Batch 64780, train_perplexity=338.48428, train_loss=5.8244777

Batch 64790, train_perplexity=295.1731, train_loss=5.687562

Batch 64800, train_perplexity=320.781, train_loss=5.7707586

Batch 64810, train_perplexity=268.51492, train_loss=5.5929065

Batch 64820, train_perplexity=336.3292, train_loss=5.8180904

Batch 64830, train_perplexity=290.78748, train_loss=5.6725926

Batch 64840, train_perplexity=321.80225, train_loss=5.773937

Batch 64850, train_perplexity=341.66992, train_loss=5.833845

Batch 64860, train_perplexity=293.87195, train_loss=5.683144

Batch 64870, train_perplexity=317.79117, train_loss=5.7613945

Batch 64880, train_perplexity=312.56113, train_loss=5.7448

Batch 64890, train_perplexity=298.84882, train_loss=5.699938

Batch 64900, train_perplexity=297.93622, train_loss=5.6968794

Batch 64910, train_perplexity=317.17807, train_loss=5.7594633

Batch 64920, train_perplexity=343.636, train_loss=5.839583

Batch 64930, train_perplexity=315.45212, train_loss=5.754007

Batch 64940, train_perplexity=282.6946, train_loss=5.644367

Batch 64950, train_perplexity=305.17847, train_loss=5.7208967

Batch 64960, train_perplexity=331.24976, train_loss=5.8028727

Batch 64970, train_perplexity=342.02463, train_loss=5.8348827

Batch 64980, train_perplexity=311.7263, train_loss=5.7421255

Batch 64990, train_perplexity=299.18433, train_loss=5.70106

Batch 65000, train_perplexity=375.26282, train_loss=5.9276266

Batch 65010, train_perplexity=341.25864, train_loss=5.8326406

Batch 65020, train_perplexity=291.2604, train_loss=5.6742177

Batch 65030, train_perplexity=298.7502, train_loss=5.699608

Batch 65040, train_perplexity=315.71982, train_loss=5.754855

Batch 65050, train_perplexity=289.4395, train_loss=5.6679463

Batch 65060, train_perplexity=317.28, train_loss=5.7597847

Batch 65070, train_perplexity=280.709, train_loss=5.6373186

Batch 65080, train_perplexity=311.33084, train_loss=5.740856

Batch 65090, train_perplexity=311.95142, train_loss=5.7428474

Batch 65100, train_perplexity=281.8921, train_loss=5.6415243

Batch 65110, train_perplexity=316.11722, train_loss=5.756113

Batch 65120, train_perplexity=305.66357, train_loss=5.722485

Batch 65130, train_perplexity=345.66116, train_loss=5.845459

Batch 65140, train_perplexity=320.9649, train_loss=5.771332

Batch 65150, train_perplexity=296.5281, train_loss=5.692142

Batch 65160, train_perplexity=292.7145, train_loss=5.679198

Batch 65170, train_perplexity=326.44778, train_loss=5.78827

Batch 65180, train_perplexity=294.68735, train_loss=5.685915

Batch 65190, train_perplexity=327.46085, train_loss=5.7913685

Batch 65200, train_perplexity=323.46313, train_loss=5.779085

Batch 65210, train_perplexity=316.6724, train_loss=5.757868

Batch 65220, train_perplexity=298.4723, train_loss=5.698677

Batch 65230, train_perplexity=289.88965, train_loss=5.6695004

Batch 65240, train_perplexity=282.77335, train_loss=5.6446457

Batch 65250, train_perplexity=326.35736, train_loss=5.787993

Batch 65260, train_perplexity=272.96237, train_loss=5.609334

Batch 65270, train_perplexity=309.08252, train_loss=5.7336082

Batch 65280, train_perplexity=282.7902, train_loss=5.6447053

Batch 65290, train_perplexity=342.11237, train_loss=5.8351393

Batch 65300, train_perplexity=297.6299, train_loss=5.695851

Batch 65310, train_perplexity=305.68878, train_loss=5.7225676

Batch 65320, train_perplexity=306.7243, train_loss=5.7259493

Batch 65330, train_perplexity=338.3279, train_loss=5.8240156

Batch 65340, train_perplexity=327.57172, train_loss=5.791707

Batch 65350, train_perplexity=303.88318, train_loss=5.7166433

Batch 65360, train_perplexity=315.8225, train_loss=5.7551804

Batch 65370, train_perplexity=330.36325, train_loss=5.800193

Batch 65380, train_perplexity=326.94128, train_loss=5.7897806

Batch 65390, train_perplexity=327.37296, train_loss=5.7911

Batch 65400, train_perplexity=310.46213, train_loss=5.738062

Batch 65410, train_perplexity=300.67786, train_loss=5.7060394

Batch 65420, train_perplexity=311.12665, train_loss=5.7402

Batch 65430, train_perplexity=281.73956, train_loss=5.640983

Batch 65440, train_perplexity=270.328, train_loss=5.599636

Batch 65450, train_perplexity=309.39896, train_loss=5.7346315

Batch 65460, train_perplexity=279.21347, train_loss=5.6319766

Batch 65470, train_perplexity=292.5782, train_loss=5.678732

Batch 65480, train_perplexity=277.2321, train_loss=5.624855

Batch 65490, train_perplexity=300.16385, train_loss=5.7043285

Batch 65500, train_perplexity=307.66794, train_loss=5.729021

Batch 65510, train_perplexity=355.11893, train_loss=5.8724527

Batch 65520, train_perplexity=302.56857, train_loss=5.712308

Batch 65530, train_perplexity=326.19696, train_loss=5.7875013

Batch 65540, train_perplexity=316.96487, train_loss=5.758791

Batch 65550, train_perplexity=285.91867, train_loss=5.6557074

Batch 65560, train_perplexity=297.36594, train_loss=5.6949635

Batch 65570, train_perplexity=297.77515, train_loss=5.6963387

Batch 65580, train_perplexity=308.9281, train_loss=5.7331085

Batch 65590, train_perplexity=323.21304, train_loss=5.7783117

Batch 65600, train_perplexity=308.3797, train_loss=5.731332

Batch 65610, train_perplexity=338.09537, train_loss=5.823328

Batch 65620, train_perplexity=265.84067, train_loss=5.582897

Batch 65630, train_perplexity=308.2578, train_loss=5.7309365

Batch 65640, train_perplexity=278.34076, train_loss=5.628846

Batch 65650, train_perplexity=324.20496, train_loss=5.781376

Batch 65660, train_perplexity=303.50638, train_loss=5.7154026

Batch 65670, train_perplexity=316.69052, train_loss=5.757925

Batch 65680, train_perplexity=267.80014, train_loss=5.590241

Batch 65690, train_perplexity=328.18555, train_loss=5.793579
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 65700, train_perplexity=310.5394, train_loss=5.738311

Batch 65710, train_perplexity=263.94803, train_loss=5.5757523

Batch 65720, train_perplexity=284.33286, train_loss=5.6501455

Batch 65730, train_perplexity=272.04034, train_loss=5.6059504

Batch 65740, train_perplexity=300.49368, train_loss=5.7054267

Batch 65750, train_perplexity=326.4487, train_loss=5.788273

Batch 65760, train_perplexity=297.09097, train_loss=5.6940384

Batch 65770, train_perplexity=281.09988, train_loss=5.63871

Batch 65780, train_perplexity=283.43347, train_loss=5.6469774

Batch 65790, train_perplexity=304.06885, train_loss=5.717254

Batch 65800, train_perplexity=312.64566, train_loss=5.7450705

Batch 65810, train_perplexity=300.73206, train_loss=5.7062197

Batch 65820, train_perplexity=302.11185, train_loss=5.7107973

Batch 65830, train_perplexity=311.58527, train_loss=5.741673

Batch 65840, train_perplexity=301.42404, train_loss=5.708518

Batch 65850, train_perplexity=301.16946, train_loss=5.707673

Batch 65860, train_perplexity=311.46063, train_loss=5.741273

Batch 65870, train_perplexity=311.213, train_loss=5.7404776

Batch 65880, train_perplexity=305.70163, train_loss=5.7226095

Batch 65890, train_perplexity=297.5536, train_loss=5.6955943

Batch 65900, train_perplexity=280.196, train_loss=5.6354895

Batch 65910, train_perplexity=338.70532, train_loss=5.8251305

Batch 65920, train_perplexity=307.52448, train_loss=5.7285547

Batch 65930, train_perplexity=318.30014, train_loss=5.762995

Batch 65940, train_perplexity=307.33774, train_loss=5.727947

Batch 65950, train_perplexity=298.77444, train_loss=5.699689

Batch 65960, train_perplexity=273.12277, train_loss=5.6099215

Batch 65970, train_perplexity=295.57846, train_loss=5.6889343

Batch 65980, train_perplexity=301.25763, train_loss=5.707966

Batch 65990, train_perplexity=326.01144, train_loss=5.7869325

Batch 66000, train_perplexity=273.63666, train_loss=5.611801

Batch 66010, train_perplexity=288.6934, train_loss=5.665365

Batch 66020, train_perplexity=315.74133, train_loss=5.7549233

Batch 66030, train_perplexity=322.06076, train_loss=5.77474

Batch 66040, train_perplexity=303.65735, train_loss=5.7159

Batch 66050, train_perplexity=333.45004, train_loss=5.809493

Batch 66060, train_perplexity=329.92184, train_loss=5.798856

Batch 66070, train_perplexity=315.73456, train_loss=5.754902

Batch 66080, train_perplexity=303.42795, train_loss=5.715144

Batch 66090, train_perplexity=318.42676, train_loss=5.7633924

Batch 66100, train_perplexity=283.166, train_loss=5.6460333

Batch 66110, train_perplexity=329.8432, train_loss=5.7986174

Batch 66120, train_perplexity=278.2703, train_loss=5.628593

Batch 66130, train_perplexity=319.23343, train_loss=5.7659225

Batch 66140, train_perplexity=299.22983, train_loss=5.701212

Batch 66150, train_perplexity=285.81424, train_loss=5.655342

Batch 66160, train_perplexity=333.90286, train_loss=5.81085

Batch 66170, train_perplexity=302.78592, train_loss=5.713026

Batch 66180, train_perplexity=309.2019, train_loss=5.7339945

Batch 66190, train_perplexity=326.78012, train_loss=5.7892876

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Batch 66200, train_perplexity=329.42947, train_loss=5.7973623

Batch 66210, train_perplexity=265.1401, train_loss=5.5802584

Batch 66220, train_perplexity=324.8701, train_loss=5.7834253

Batch 66230, train_perplexity=292.98865, train_loss=5.680134

Batch 66240, train_perplexity=315.79208, train_loss=5.755084

Batch 66250, train_perplexity=287.4623, train_loss=5.661092

Batch 66260, train_perplexity=293.51358, train_loss=5.681924

Batch 66270, train_perplexity=315.8228, train_loss=5.7551813

Batch 66280, train_perplexity=333.01468, train_loss=5.8081865

Batch 66290, train_perplexity=342.2612, train_loss=5.835574

Batch 66300, train_perplexity=269.7949, train_loss=5.597662

Batch 66310, train_perplexity=307.81937, train_loss=5.729513

Batch 66320, train_perplexity=292.694, train_loss=5.6791277

Batch 66330, train_perplexity=280.20483, train_loss=5.635521

Batch 66340, train_perplexity=308.1495, train_loss=5.730585

Batch 66350, train_perplexity=341.68182, train_loss=5.83388

Batch 66360, train_perplexity=312.26767, train_loss=5.7438607

Batch 66370, train_perplexity=296.60092, train_loss=5.6923876

Batch 66380, train_perplexity=293.28888, train_loss=5.681158

Batch 66390, train_perplexity=303.02136, train_loss=5.7138033

Batch 66400, train_perplexity=309.08398, train_loss=5.733613

Batch 66410, train_perplexity=303.95374, train_loss=5.7168756

Batch 66420, train_perplexity=293.18317, train_loss=5.6807976

Batch 66430, train_perplexity=300.75644, train_loss=5.7063007

Batch 66440, train_perplexity=302.0826, train_loss=5.7107005

Batch 66450, train_perplexity=300.72546, train_loss=5.7061977

Batch 66460, train_perplexity=348.755, train_loss=5.8543696

Batch 66470, train_perplexity=323.23926, train_loss=5.778393

Batch 66480, train_perplexity=298.40225, train_loss=5.6984425

Batch 66490, train_perplexity=326.98245, train_loss=5.7899065

Batch 66500, train_perplexity=318.259, train_loss=5.7628655

Batch 66510, train_perplexity=339.2075, train_loss=5.826612

Batch 66520, train_perplexity=297.8602, train_loss=5.6966243

Batch 66530, train_perplexity=332.45206, train_loss=5.8064957

Batch 66540, train_perplexity=285.1135, train_loss=5.6528873

Batch 66550, train_perplexity=315.34366, train_loss=5.753663

Batch 66560, train_perplexity=338.60263, train_loss=5.824827

Batch 66570, train_perplexity=344.15598, train_loss=5.841095

Batch 66580, train_perplexity=298.5981, train_loss=5.6990986

Batch 66590, train_perplexity=308.09662, train_loss=5.7304134

Batch 66600, train_perplexity=286.75534, train_loss=5.6586294

Batch 66610, train_perplexity=324.27606, train_loss=5.781595

Batch 66620, train_perplexity=322.17996, train_loss=5.7751102

Batch 66630, train_perplexity=297.06787, train_loss=5.6939607

Batch 66640, train_perplexity=309.73904, train_loss=5.73573

Batch 66650, train_perplexity=303.10388, train_loss=5.7140756

Batch 66660, train_perplexity=270.45822, train_loss=5.6001177

Batch 66670, train_perplexity=307.24512, train_loss=5.727646

Batch 66680, train_perplexity=331.60406, train_loss=5.8039417

Batch 66690, train_perplexity=290.07272, train_loss=5.6701317

Batch 66700, train_perplexity=308.88037, train_loss=5.732954

Batch 66710, train_perplexity=288.2069, train_loss=5.6636786

Batch 66720, train_perplexity=282.61725, train_loss=5.6440935

Batch 66730, train_perplexity=310.77258, train_loss=5.7390614

Batch 66740, train_perplexity=279.4643, train_loss=5.6328745

Batch 66750, train_perplexity=288.0654, train_loss=5.6631875

Batch 66760, train_perplexity=305.8909, train_loss=5.7232285

Batch 66770, train_perplexity=304.98788, train_loss=5.720272

Batch 66780, train_perplexity=282.44183, train_loss=5.6434727

Batch 66790, train_perplexity=302.5273, train_loss=5.7121716

Batch 66800, train_perplexity=299.68604, train_loss=5.7027354

Batch 66810, train_perplexity=312.7612, train_loss=5.74544

Batch 66820, train_perplexity=310.42007, train_loss=5.7379265

Batch 66830, train_perplexity=345.1789, train_loss=5.844063

Batch 66840, train_perplexity=270.87228, train_loss=5.6016474

Batch 66850, train_perplexity=306.41205, train_loss=5.724931

Batch 66860, train_perplexity=345.81613, train_loss=5.845907

Batch 66870, train_perplexity=286.80048, train_loss=5.658787

Batch 66880, train_perplexity=309.2333, train_loss=5.734096

Batch 66890, train_perplexity=313.43753, train_loss=5.7476

Batch 66900, train_perplexity=308.27545, train_loss=5.7309937

Batch 66910, train_perplexity=321.35266, train_loss=5.772539

Batch 66920, train_perplexity=317.74844, train_loss=5.76126

Batch 66930, train_perplexity=316.45413, train_loss=5.7571783

Batch 66940, train_perplexity=295.83762, train_loss=5.6898108

Batch 66950, train_perplexity=316.27792, train_loss=5.7566214
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66960, train_perplexity=323.04263, train_loss=5.7777843

Batch 66970, train_perplexity=329.73343, train_loss=5.7982845

Batch 66980, train_perplexity=315.36322, train_loss=5.753725

Batch 66990, train_perplexity=334.19818, train_loss=5.811734

Batch 67000, train_perplexity=288.0386, train_loss=5.6630945

Batch 67010, train_perplexity=325.88943, train_loss=5.786558

Batch 67020, train_perplexity=327.91055, train_loss=5.792741

Batch 67030, train_perplexity=354.30334, train_loss=5.8701534

Batch 67040, train_perplexity=322.50842, train_loss=5.7761292

Batch 67050, train_perplexity=279.57663, train_loss=5.6332765

Batch 67060, train_perplexity=277.1232, train_loss=5.624462

Batch 67070, train_perplexity=316.89142, train_loss=5.758559

Batch 67080, train_perplexity=311.9511, train_loss=5.7428465

Batch 67090, train_perplexity=293.84335, train_loss=5.683047

Batch 67100, train_perplexity=262.6561, train_loss=5.5708456

Batch 67110, train_perplexity=278.63187, train_loss=5.6298914

Batch 67120, train_perplexity=279.9415, train_loss=5.6345806

Batch 67130, train_perplexity=311.43982, train_loss=5.741206

Batch 67140, train_perplexity=320.87354, train_loss=5.771047

Batch 67150, train_perplexity=340.55313, train_loss=5.830571

Batch 67160, train_perplexity=302.72067, train_loss=5.7128105

Batch 67170, train_perplexity=282.93332, train_loss=5.645211

Batch 67180, train_perplexity=279.38022, train_loss=5.6325736

Batch 67190, train_perplexity=300.36545, train_loss=5.705

Batch 67200, train_perplexity=301.11804, train_loss=5.7075024

Batch 67210, train_perplexity=316.33344, train_loss=5.756797

Batch 67220, train_perplexity=300.06183, train_loss=5.7039886

Batch 67230, train_perplexity=301.19156, train_loss=5.7077465

Batch 67240, train_perplexity=305.65292, train_loss=5.7224503

Batch 67250, train_perplexity=308.98788, train_loss=5.733302

Batch 67260, train_perplexity=300.14526, train_loss=5.7042665

Batch 67270, train_perplexity=288.65375, train_loss=5.665228

Batch 67280, train_perplexity=298.6941, train_loss=5.69942

Batch 67290, train_perplexity=323.16666, train_loss=5.778168

Batch 67300, train_perplexity=372.5627, train_loss=5.9204054

Batch 67310, train_perplexity=283.39456, train_loss=5.64684

Batch 67320, train_perplexity=333.20224, train_loss=5.8087497

Batch 67330, train_perplexity=306.34894, train_loss=5.724725

Batch 67340, train_perplexity=319.5383, train_loss=5.766877

Batch 67350, train_perplexity=314.48984, train_loss=5.750952

Batch 67360, train_perplexity=316.23602, train_loss=5.756489

Batch 67370, train_perplexity=307.7947, train_loss=5.729433

Batch 67380, train_perplexity=309.9475, train_loss=5.736403

Batch 67390, train_perplexity=303.98346, train_loss=5.7169733

Batch 67400, train_perplexity=328.11966, train_loss=5.7933784

Batch 67410, train_perplexity=274.3202, train_loss=5.614296

Batch 67420, train_perplexity=272.38223, train_loss=5.6072063

Batch 67430, train_perplexity=303.736, train_loss=5.716159

Batch 67440, train_perplexity=281.27014, train_loss=5.6393156

Batch 67450, train_perplexity=316.63074, train_loss=5.757736

Batch 67460, train_perplexity=310.29428, train_loss=5.737521

Batch 67470, train_perplexity=244.01381, train_loss=5.497225

Batch 67480, train_perplexity=329.19238, train_loss=5.7966423

Batch 67490, train_perplexity=323.0294, train_loss=5.7777433

Batch 67500, train_perplexity=302.96445, train_loss=5.7136154

Batch 67510, train_perplexity=290.44296, train_loss=5.671407

Batch 67520, train_perplexity=317.65363, train_loss=5.7609615

Batch 67530, train_perplexity=297.28455, train_loss=5.6946898

Batch 67540, train_perplexity=278.57074, train_loss=5.629672

Batch 67550, train_perplexity=312.32858, train_loss=5.7440557

Batch 67560, train_perplexity=298.5793, train_loss=5.6990356

Batch 67570, train_perplexity=341.60263, train_loss=5.833648

Batch 67580, train_perplexity=290.34702, train_loss=5.671077

Batch 67590, train_perplexity=288.44214, train_loss=5.6644945

Batch 67600, train_perplexity=288.02103, train_loss=5.6630335

Batch 67610, train_perplexity=340.0206, train_loss=5.829006

Batch 67620, train_perplexity=274.54813, train_loss=5.6151266

Batch 67630, train_perplexity=321.19577, train_loss=5.772051

Batch 67640, train_perplexity=302.73395, train_loss=5.7128544

Batch 67650, train_perplexity=296.1021, train_loss=5.6907043

Batch 67660, train_perplexity=310.44733, train_loss=5.738014

Batch 67670, train_perplexity=298.23468, train_loss=5.6978807

Batch 67680, train_perplexity=323.4454, train_loss=5.7790303

Batch 67690, train_perplexity=335.45917, train_loss=5.8155003

Batch 67700, train_perplexity=308.0718, train_loss=5.730333

Batch 67710, train_perplexity=247.63957, train_loss=5.5119743

Batch 67720, train_perplexity=295.77386, train_loss=5.689595

Batch 67730, train_perplexity=315.73187, train_loss=5.7548933

Batch 67740, train_perplexity=319.2325, train_loss=5.7659197

Batch 67750, train_perplexity=296.38138, train_loss=5.691647

Batch 67760, train_perplexity=321.91092, train_loss=5.774275

Batch 67770, train_perplexity=341.54025, train_loss=5.8334656

Batch 67780, train_perplexity=321.20987, train_loss=5.7720947

Batch 67790, train_perplexity=313.76697, train_loss=5.7486506

Batch 67800, train_perplexity=303.6468, train_loss=5.715865

Batch 67810, train_perplexity=305.69055, train_loss=5.7225733

Batch 67820, train_perplexity=294.6643, train_loss=5.685837

Batch 67830, train_perplexity=296.08347, train_loss=5.6906414

Batch 67840, train_perplexity=291.47617, train_loss=5.674958

Batch 67850, train_perplexity=287.7753, train_loss=5.66218

Batch 67860, train_perplexity=335.91104, train_loss=5.8168464

Batch 67870, train_perplexity=326.5524, train_loss=5.7885904

Batch 67880, train_perplexity=311.62878, train_loss=5.7418127

Batch 67890, train_perplexity=286.98462, train_loss=5.6594286

Batch 67900, train_perplexity=335.18912, train_loss=5.814695

Batch 67910, train_perplexity=314.51984, train_loss=5.751047

Batch 67920, train_perplexity=292.02692, train_loss=5.676846

Batch 67930, train_perplexity=288.6334, train_loss=5.6651573

Batch 67940, train_perplexity=317.9855, train_loss=5.762006

Batch 67950, train_perplexity=308.48413, train_loss=5.7316704

Batch 67960, train_perplexity=302.7949, train_loss=5.7130556

Batch 67970, train_perplexity=349.2306, train_loss=5.8557324

Batch 67980, train_perplexity=306.42462, train_loss=5.724972

Batch 67990, train_perplexity=302.0234, train_loss=5.7105045

Batch 68000, train_perplexity=279.67596, train_loss=5.6336317

Batch 68010, train_perplexity=303.27606, train_loss=5.7146435

Batch 68020, train_perplexity=326.8152, train_loss=5.789395

Batch 68030, train_perplexity=250.34967, train_loss=5.5228586

Batch 68040, train_perplexity=323.222, train_loss=5.7783394

Batch 68050, train_perplexity=319.4664, train_loss=5.766652

Batch 68060, train_perplexity=331.60486, train_loss=5.803944

Batch 68070, train_perplexity=313.0677, train_loss=5.7464194

Batch 68080, train_perplexity=282.47754, train_loss=5.643599

Batch 68090, train_perplexity=288.91843, train_loss=5.6661444

Batch 68100, train_perplexity=271.90573, train_loss=5.6054554

Batch 68110, train_perplexity=308.6776, train_loss=5.7322974

Batch 68120, train_perplexity=285.02216, train_loss=5.652567

Batch 68130, train_perplexity=292.40817, train_loss=5.6781507

Batch 68140, train_perplexity=319.0499, train_loss=5.7653475

Batch 68150, train_perplexity=290.95807, train_loss=5.673179

Batch 68160, train_perplexity=300.42474, train_loss=5.7051973

Batch 68170, train_perplexity=297.49753, train_loss=5.695406

Batch 68180, train_perplexity=294.57806, train_loss=5.685544

Batch 68190, train_perplexity=291.77237, train_loss=5.675974

Batch 68200, train_perplexity=291.95718, train_loss=5.676607

Batch 68210, train_perplexity=319.66846, train_loss=5.7672844

Batch 68220, train_perplexity=291.48505, train_loss=5.6749887

Batch 68230, train_perplexity=346.80444, train_loss=5.848761

Batch 68240, train_perplexity=311.16492, train_loss=5.740323

Batch 68250, train_perplexity=299.7628, train_loss=5.7029915

Batch 68260, train_perplexity=293.4103, train_loss=5.681572

Batch 68270, train_perplexity=327.9839, train_loss=5.7929645
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 68280, train_perplexity=267.30655, train_loss=5.588396

Batch 68290, train_perplexity=299.01047, train_loss=5.7004786

Batch 68300, train_perplexity=304.1795, train_loss=5.717618

Batch 68310, train_perplexity=304.53012, train_loss=5.71877

Batch 68320, train_perplexity=281.8539, train_loss=5.641389

Batch 68330, train_perplexity=324.22366, train_loss=5.7814336

Batch 68340, train_perplexity=290.3999, train_loss=5.671259

Batch 68350, train_perplexity=325.62164, train_loss=5.785736

Batch 68360, train_perplexity=293.96515, train_loss=5.683461

Batch 68370, train_perplexity=299.7542, train_loss=5.702963

Batch 68380, train_perplexity=290.0535, train_loss=5.6700654

Batch 68390, train_perplexity=282.7689, train_loss=5.64463

Batch 68400, train_perplexity=320.1844, train_loss=5.768897

Batch 68410, train_perplexity=305.3792, train_loss=5.7215543

Batch 68420, train_perplexity=284.61282, train_loss=5.6511297

Batch 68430, train_perplexity=280.92407, train_loss=5.6380844

Batch 68440, train_perplexity=283.616, train_loss=5.647621

Batch 68450, train_perplexity=345.77393, train_loss=5.845785

Batch 68460, train_perplexity=283.05447, train_loss=5.6456394

Batch 68470, train_perplexity=302.9373, train_loss=5.713526

Batch 68480, train_perplexity=334.70694, train_loss=5.8132553

Batch 68490, train_perplexity=305.34512, train_loss=5.7214427

Batch 68500, train_perplexity=324.59692, train_loss=5.782584

Batch 68510, train_perplexity=286.36755, train_loss=5.657276

Batch 68520, train_perplexity=318.28586, train_loss=5.76295

Batch 68530, train_perplexity=275.0896, train_loss=5.617097

Batch 68540, train_perplexity=304.9402, train_loss=5.7201157

Batch 68550, train_perplexity=323.14218, train_loss=5.7780924

Batch 68560, train_perplexity=322.85263, train_loss=5.777196

Batch 68570, train_perplexity=280.5927, train_loss=5.6369042

Batch 68580, train_perplexity=302.4539, train_loss=5.711929

Batch 68590, train_perplexity=273.95627, train_loss=5.6129684

Batch 68600, train_perplexity=294.75705, train_loss=5.6861515

Batch 68610, train_perplexity=320.64795, train_loss=5.770344

Batch 68620, train_perplexity=306.72342, train_loss=5.7259464

Batch 68630, train_perplexity=281.7956, train_loss=5.641182

Batch 68640, train_perplexity=269.16138, train_loss=5.595311

Batch 68650, train_perplexity=330.25488, train_loss=5.799865

Batch 68660, train_perplexity=281.27954, train_loss=5.639349

Batch 68670, train_perplexity=303.7806, train_loss=5.7163057

Batch 68680, train_perplexity=295.7114, train_loss=5.689384

Batch 68690, train_perplexity=268.44925, train_loss=5.592662

Batch 68700, train_perplexity=302.44653, train_loss=5.7119045

Batch 68710, train_perplexity=280.8555, train_loss=5.6378403

Batch 68720, train_perplexity=271.23883, train_loss=5.6029997

Batch 68730, train_perplexity=315.08093, train_loss=5.7528296

Batch 68740, train_perplexity=270.7936, train_loss=5.601357

Batch 68750, train_perplexity=332.26346, train_loss=5.805928

Batch 68760, train_perplexity=335.06064, train_loss=5.8143115

Batch 68770, train_perplexity=278.92737, train_loss=5.6309514

Batch 68780, train_perplexity=269.77585, train_loss=5.5975914

Batch 68790, train_perplexity=316.2752, train_loss=5.756613

Batch 68800, train_perplexity=272.17593, train_loss=5.6064487

Batch 68810, train_perplexity=299.9979, train_loss=5.7037754

Batch 68820, train_perplexity=303.1078, train_loss=5.7140884

Batch 68830, train_perplexity=301.90936, train_loss=5.710127

Batch 68840, train_perplexity=331.55524, train_loss=5.8037944

Batch 68850, train_perplexity=279.27698, train_loss=5.632204

Batch 68860, train_perplexity=307.71783, train_loss=5.729183

Batch 68870, train_perplexity=283.42413, train_loss=5.6469445

Batch 68880, train_perplexity=324.68082, train_loss=5.7828426

Batch 68890, train_perplexity=294.13535, train_loss=5.68404

Batch 68900, train_perplexity=321.05966, train_loss=5.771627

Batch 68910, train_perplexity=293.41843, train_loss=5.6815996

Batch 68920, train_perplexity=279.61462, train_loss=5.6334124

Batch 68930, train_perplexity=321.38684, train_loss=5.7726455

Batch 68940, train_perplexity=261.89722, train_loss=5.567952

Batch 68950, train_perplexity=305.4047, train_loss=5.7216377

Batch 68960, train_perplexity=268.08313, train_loss=5.591297

Batch 68970, train_perplexity=269.14444, train_loss=5.595248

Batch 68980, train_perplexity=263.20425, train_loss=5.5729303

Batch 68990, train_perplexity=324.20944, train_loss=5.7813897

Batch 69000, train_perplexity=278.22003, train_loss=5.6284122

Batch 69010, train_perplexity=307.25156, train_loss=5.727667

Batch 69020, train_perplexity=317.509, train_loss=5.760506

Batch 69030, train_perplexity=306.07297, train_loss=5.7238235

Batch 69040, train_perplexity=343.6512, train_loss=5.8396273

Batch 69050, train_perplexity=282.5189, train_loss=5.6437454

Batch 69060, train_perplexity=313.76727, train_loss=5.7486515

Batch 69070, train_perplexity=276.25177, train_loss=5.6213126

Batch 69080, train_perplexity=310.61435, train_loss=5.738552

Batch 69090, train_perplexity=301.89468, train_loss=5.7100782

Batch 69100, train_perplexity=289.41013, train_loss=5.667845

Batch 69110, train_perplexity=306.91916, train_loss=5.7265844

Batch 69120, train_perplexity=272.99673, train_loss=5.60946

Batch 69130, train_perplexity=271.8449, train_loss=5.605232

Batch 69140, train_perplexity=271.26236, train_loss=5.6030865

Batch 69150, train_perplexity=317.00525, train_loss=5.7589183

Batch 69160, train_perplexity=300.67612, train_loss=5.7060337

Batch 69170, train_perplexity=287.7495, train_loss=5.6620903

Batch 69180, train_perplexity=311.14874, train_loss=5.740271

Batch 69190, train_perplexity=281.90875, train_loss=5.6415834

Batch 69200, train_perplexity=294.30203, train_loss=5.6846066

Batch 69210, train_perplexity=292.86768, train_loss=5.679721

Batch 69220, train_perplexity=295.08472, train_loss=5.6872625

Batch 69230, train_perplexity=342.63138, train_loss=5.836655

Batch 69240, train_perplexity=319.57397, train_loss=5.7669888

Batch 69250, train_perplexity=296.40018, train_loss=5.6917105

Batch 69260, train_perplexity=279.25726, train_loss=5.6321335

Batch 69270, train_perplexity=295.2086, train_loss=5.687682

Batch 69280, train_perplexity=303.8148, train_loss=5.7164183

Batch 69290, train_perplexity=297.45157, train_loss=5.6952515

Batch 69300, train_perplexity=289.82883, train_loss=5.6692905

Batch 69310, train_perplexity=350.08792, train_loss=5.8581843

Batch 69320, train_perplexity=291.35388, train_loss=5.6745386

Batch 69330, train_perplexity=301.41483, train_loss=5.7084875

Batch 69340, train_perplexity=321.63794, train_loss=5.7734265

Batch 69350, train_perplexity=264.70807, train_loss=5.5786276

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Batch 69360, train_perplexity=290.3401, train_loss=5.671053

Batch 69370, train_perplexity=291.4606, train_loss=5.674905

Batch 69380, train_perplexity=318.82025, train_loss=5.7646275

Batch 69390, train_perplexity=304.74423, train_loss=5.719473

Batch 69400, train_perplexity=304.78638, train_loss=5.719611

Batch 69410, train_perplexity=318.3138, train_loss=5.7630377

Batch 69420, train_perplexity=284.9364, train_loss=5.652266

Batch 69430, train_perplexity=311.21567, train_loss=5.740486

Batch 69440, train_perplexity=298.21167, train_loss=5.6978035

Batch 69450, train_perplexity=301.8181, train_loss=5.7098246

Batch 69460, train_perplexity=317.06766, train_loss=5.759115

Batch 69470, train_perplexity=304.31384, train_loss=5.7180595

Batch 69480, train_perplexity=284.22195, train_loss=5.6497555

Batch 69490, train_perplexity=311.10632, train_loss=5.7401347

Batch 69500, train_perplexity=286.75726, train_loss=5.658636

Batch 69510, train_perplexity=295.1185, train_loss=5.687377

Batch 69520, train_perplexity=277.19693, train_loss=5.624728

Batch 69530, train_perplexity=315.65253, train_loss=5.754642
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 69540, train_perplexity=288.3584, train_loss=5.664204

Batch 69550, train_perplexity=310.69315, train_loss=5.738806

Batch 69560, train_perplexity=318.36768, train_loss=5.763207

Batch 69570, train_perplexity=299.70477, train_loss=5.702798

Batch 69580, train_perplexity=290.34314, train_loss=5.6710634

Batch 69590, train_perplexity=309.71188, train_loss=5.7356424

Batch 69600, train_perplexity=289.7487, train_loss=5.669014

Batch 69610, train_perplexity=330.93274, train_loss=5.801915

Batch 69620, train_perplexity=259.08707, train_loss=5.557164

Batch 69630, train_perplexity=312.70618, train_loss=5.745264

Batch 69640, train_perplexity=330.9935, train_loss=5.8020988

Batch 69650, train_perplexity=250.1832, train_loss=5.5221934

Batch 69660, train_perplexity=310.41727, train_loss=5.7379174

Batch 69670, train_perplexity=289.33658, train_loss=5.6675906

Batch 69680, train_perplexity=322.38864, train_loss=5.775758

Batch 69690, train_perplexity=281.9757, train_loss=5.641821

Batch 69700, train_perplexity=274.44135, train_loss=5.6147375

Batch 69710, train_perplexity=322.14508, train_loss=5.775002

Batch 69720, train_perplexity=267.54742, train_loss=5.589297

Batch 69730, train_perplexity=285.3205, train_loss=5.653613

Batch 69740, train_perplexity=297.11334, train_loss=5.6941137

Batch 69750, train_perplexity=293.88498, train_loss=5.6831884

Batch 69760, train_perplexity=279.3657, train_loss=5.6325216

Batch 69770, train_perplexity=314.9139, train_loss=5.7522993

Batch 69780, train_perplexity=287.13354, train_loss=5.6599474

Batch 69790, train_perplexity=317.84875, train_loss=5.7615757

Batch 69800, train_perplexity=322.7007, train_loss=5.7767253

Batch 69810, train_perplexity=349.8481, train_loss=5.857499

Batch 69820, train_perplexity=318.97504, train_loss=5.765113

Batch 69830, train_perplexity=296.73434, train_loss=5.692837

Batch 69840, train_perplexity=288.30478, train_loss=5.664018

Batch 69850, train_perplexity=316.52628, train_loss=5.757406

Batch 69860, train_perplexity=330.36578, train_loss=5.8002005

Batch 69870, train_perplexity=296.2323, train_loss=5.691144

Batch 69880, train_perplexity=277.17618, train_loss=5.6246533

Batch 69890, train_perplexity=263.67908, train_loss=5.574733

Batch 69900, train_perplexity=324.7062, train_loss=5.782921

Batch 69910, train_perplexity=290.51706, train_loss=5.6716623

Batch 69920, train_perplexity=288.965, train_loss=5.6663055

Batch 69930, train_perplexity=277.4135, train_loss=5.6255093

Batch 69940, train_perplexity=325.5201, train_loss=5.785424

Batch 69950, train_perplexity=325.0898, train_loss=5.7841015

Batch 69960, train_perplexity=292.5458, train_loss=5.6786213

Batch 69970, train_perplexity=284.7779, train_loss=5.6517096

Batch 69980, train_perplexity=311.45172, train_loss=5.7412443

Batch 69990, train_perplexity=329.83374, train_loss=5.7985888

Batch 70000, train_perplexity=306.91318, train_loss=5.726565

Batch 70010, train_perplexity=258.8843, train_loss=5.556381

Batch 70020, train_perplexity=290.40778, train_loss=5.671286

Batch 70030, train_perplexity=275.23026, train_loss=5.617608

Batch 70040, train_perplexity=295.2801, train_loss=5.6879244

Batch 70050, train_perplexity=320.3188, train_loss=5.7693167

Batch 70060, train_perplexity=331.9667, train_loss=5.8050346

Batch 70070, train_perplexity=290.9123, train_loss=5.673022

Batch 70080, train_perplexity=315.29556, train_loss=5.7535105

Batch 70090, train_perplexity=279.21652, train_loss=5.6319876

Batch 70100, train_perplexity=324.81183, train_loss=5.783246

Batch 70110, train_perplexity=296.7775, train_loss=5.6929827

Batch 70120, train_perplexity=282.80328, train_loss=5.6447515

Batch 70130, train_perplexity=281.02643, train_loss=5.6384487

Batch 70140, train_perplexity=294.17462, train_loss=5.6841736

Batch 70150, train_perplexity=320.68832, train_loss=5.7704697

Batch 70160, train_perplexity=286.71735, train_loss=5.658497

Batch 70170, train_perplexity=266.26657, train_loss=5.584498

Batch 70180, train_perplexity=301.31943, train_loss=5.708171

Batch 70190, train_perplexity=279.30255, train_loss=5.6322956

Batch 70200, train_perplexity=258.48737, train_loss=5.554847

Batch 70210, train_perplexity=298.20483, train_loss=5.6977806

Batch 70220, train_perplexity=272.8377, train_loss=5.608877

Batch 70230, train_perplexity=271.75613, train_loss=5.604905

Batch 70240, train_perplexity=282.9371, train_loss=5.6452246

Batch 70250, train_perplexity=311.57648, train_loss=5.741645

Batch 70260, train_perplexity=280.2269, train_loss=5.6355996

Batch 70270, train_perplexity=308.94812, train_loss=5.7331734

Batch 70280, train_perplexity=271.89224, train_loss=5.605406

Batch 70290, train_perplexity=300.19937, train_loss=5.704447

Batch 70300, train_perplexity=295.75525, train_loss=5.6895323

Batch 70310, train_perplexity=295.06882, train_loss=5.6872087

Batch 70320, train_perplexity=247.50522, train_loss=5.5114317

Batch 70330, train_perplexity=285.64816, train_loss=5.654761

Batch 70340, train_perplexity=292.18753, train_loss=5.677396

Batch 70350, train_perplexity=312.7029, train_loss=5.7452536

Batch 70360, train_perplexity=292.1329, train_loss=5.677209

Batch 70370, train_perplexity=336.51913, train_loss=5.818655

Batch 70380, train_perplexity=306.22202, train_loss=5.7243104

Batch 70390, train_perplexity=305.9574, train_loss=5.723446

Batch 70400, train_perplexity=256.47064, train_loss=5.547014

Batch 70410, train_perplexity=318.71005, train_loss=5.7642817

Batch 70420, train_perplexity=324.73718, train_loss=5.783016

Batch 70430, train_perplexity=297.95837, train_loss=5.696954

Batch 70440, train_perplexity=318.84763, train_loss=5.7647133

Batch 70450, train_perplexity=276.65555, train_loss=5.622773

Batch 70460, train_perplexity=285.51837, train_loss=5.6543064

Batch 70470, train_perplexity=290.47635, train_loss=5.671522

Batch 70480, train_perplexity=303.83044, train_loss=5.71647

Batch 70490, train_perplexity=300.3388, train_loss=5.704911

Batch 70500, train_perplexity=325.08423, train_loss=5.7840843

Batch 70510, train_perplexity=261.12335, train_loss=5.564993

Batch 70520, train_perplexity=303.18613, train_loss=5.714347

Batch 70530, train_perplexity=289.98285, train_loss=5.6698217

Batch 70540, train_perplexity=313.47446, train_loss=5.747718

Batch 70550, train_perplexity=302.2747, train_loss=5.711336

Batch 70560, train_perplexity=274.28067, train_loss=5.614152

Batch 70570, train_perplexity=308.11368, train_loss=5.7304688

Batch 70580, train_perplexity=310.04834, train_loss=5.736728

Batch 70590, train_perplexity=274.884, train_loss=5.616349

Batch 70600, train_perplexity=344.30124, train_loss=5.841517

Batch 70610, train_perplexity=316.3467, train_loss=5.756839

Batch 70620, train_perplexity=308.88742, train_loss=5.732977

Batch 70630, train_perplexity=305.5786, train_loss=5.722207

Batch 70640, train_perplexity=308.04697, train_loss=5.7302523

Batch 70650, train_perplexity=286.3472, train_loss=5.657205

Batch 70660, train_perplexity=316.92197, train_loss=5.7586555

Batch 70670, train_perplexity=325.0424, train_loss=5.7839556

Batch 70680, train_perplexity=300.7101, train_loss=5.7061467

Batch 70690, train_perplexity=274.25766, train_loss=5.614068

Batch 70700, train_perplexity=287.35187, train_loss=5.6607075

Batch 70710, train_perplexity=288.957, train_loss=5.666278

Batch 70720, train_perplexity=267.66724, train_loss=5.5897446

Batch 70730, train_perplexity=282.17328, train_loss=5.6425214

Batch 70740, train_perplexity=259.63098, train_loss=5.5592613

Batch 70750, train_perplexity=276.3444, train_loss=5.621648

Batch 70760, train_perplexity=306.68744, train_loss=5.725829

Batch 70770, train_perplexity=300.0946, train_loss=5.7040977

Batch 70780, train_perplexity=292.27643, train_loss=5.6777

Batch 70790, train_perplexity=258.41193, train_loss=5.554555

Batch 70800, train_perplexity=302.66193, train_loss=5.7126164

Batch 70810, train_perplexity=280.6552, train_loss=5.637127

Batch 70820, train_perplexity=304.90295, train_loss=5.7199936

Batch 70830, train_perplexity=311.01154, train_loss=5.73983

Batch 70840, train_perplexity=284.86795, train_loss=5.6520257

Batch 70850, train_perplexity=265.7062, train_loss=5.5823913
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70860, train_perplexity=310.67123, train_loss=5.738735

Batch 70870, train_perplexity=288.36542, train_loss=5.6642284

Batch 70880, train_perplexity=307.3937, train_loss=5.7281294

Batch 70890, train_perplexity=303.07382, train_loss=5.7139764

Batch 70900, train_perplexity=310.24, train_loss=5.737346

Batch 70910, train_perplexity=319.4387, train_loss=5.7665653

Batch 70920, train_perplexity=295.95883, train_loss=5.6902204

Batch 70930, train_perplexity=325.97678, train_loss=5.786826

Batch 70940, train_perplexity=262.1866, train_loss=5.5690565

Batch 70950, train_perplexity=309.9329, train_loss=5.736356

Batch 70960, train_perplexity=265.4484, train_loss=5.5814204

Batch 70970, train_perplexity=283.8238, train_loss=5.6483536

Batch 70980, train_perplexity=262.8286, train_loss=5.571502

Batch 70990, train_perplexity=315.54913, train_loss=5.7543144

Batch 71000, train_perplexity=312.26007, train_loss=5.7438364

Batch 71010, train_perplexity=313.05142, train_loss=5.7463675

Batch 71020, train_perplexity=322.57578, train_loss=5.776338

Batch 71030, train_perplexity=297.67776, train_loss=5.6960115

Batch 71040, train_perplexity=281.80203, train_loss=5.641205

Batch 71050, train_perplexity=295.17453, train_loss=5.6875668

Batch 71060, train_perplexity=275.8599, train_loss=5.619893

Batch 71070, train_perplexity=311.41162, train_loss=5.7411156

Batch 71080, train_perplexity=277.32254, train_loss=5.625181

Batch 71090, train_perplexity=268.68936, train_loss=5.593556

Batch 71100, train_perplexity=311.94754, train_loss=5.742835

Batch 71110, train_perplexity=285.3152, train_loss=5.6535945

Batch 71120, train_perplexity=315.1591, train_loss=5.7530775

Batch 71130, train_perplexity=299.44278, train_loss=5.7019234

Batch 71140, train_perplexity=295.84073, train_loss=5.6898212

Batch 71150, train_perplexity=302.405, train_loss=5.711767

Batch 71160, train_perplexity=293.58804, train_loss=5.6821775

Batch 71170, train_perplexity=293.60693, train_loss=5.682242

Batch 71180, train_perplexity=266.49927, train_loss=5.5853715

Batch 71190, train_perplexity=288.49112, train_loss=5.6646643

Batch 71200, train_perplexity=294.6084, train_loss=5.685647

Batch 71210, train_perplexity=274.56412, train_loss=5.615185

Batch 71220, train_perplexity=280.77902, train_loss=5.637568

Batch 71230, train_perplexity=316.45593, train_loss=5.757184

Batch 71240, train_perplexity=307.37436, train_loss=5.7280664

Batch 71250, train_perplexity=313.42783, train_loss=5.747569

Batch 71260, train_perplexity=292.7825, train_loss=5.67943

Batch 71270, train_perplexity=287.993, train_loss=5.662936

Batch 71280, train_perplexity=344.31323, train_loss=5.841552

Batch 71290, train_perplexity=279.14636, train_loss=5.6317363

Batch 71300, train_perplexity=331.33002, train_loss=5.803115

Batch 71310, train_perplexity=308.78772, train_loss=5.732654

Batch 71320, train_perplexity=279.96872, train_loss=5.634678

Batch 71330, train_perplexity=287.54086, train_loss=5.661365

Batch 71340, train_perplexity=307.41394, train_loss=5.728195

Batch 71350, train_perplexity=308.52325, train_loss=5.731797

Batch 71360, train_perplexity=247.96971, train_loss=5.5133066

Batch 71370, train_perplexity=270.22595, train_loss=5.5992584

Batch 71380, train_perplexity=273.06314, train_loss=5.609703

Batch 71390, train_perplexity=300.42905, train_loss=5.7052116

Batch 71400, train_perplexity=329.12018, train_loss=5.796423

Batch 71410, train_perplexity=298.9175, train_loss=5.7001677

Batch 71420, train_perplexity=295.93555, train_loss=5.6901417

Batch 71430, train_perplexity=268.2188, train_loss=5.591803

Batch 71440, train_perplexity=259.16183, train_loss=5.5574527

Batch 71450, train_perplexity=290.47467, train_loss=5.6715164

Batch 71460, train_perplexity=293.64655, train_loss=5.682377

Batch 71470, train_perplexity=307.18036, train_loss=5.727435

Batch 71480, train_perplexity=301.1235, train_loss=5.7075205

Batch 71490, train_perplexity=329.323, train_loss=5.797039

Batch 71500, train_perplexity=293.14502, train_loss=5.6806674

Batch 71510, train_perplexity=292.91528, train_loss=5.6798835

Batch 71520, train_perplexity=287.95883, train_loss=5.6628175

Batch 71530, train_perplexity=276.2648, train_loss=5.62136

Batch 71540, train_perplexity=299.99274, train_loss=5.7037582

Batch 71550, train_perplexity=308.38675, train_loss=5.7313547

Batch 71560, train_perplexity=314.14197, train_loss=5.749845

Batch 71570, train_perplexity=300.75558, train_loss=5.706298

Batch 71580, train_perplexity=298.15366, train_loss=5.697609

Batch 71590, train_perplexity=282.47525, train_loss=5.643591

Batch 71600, train_perplexity=308.24298, train_loss=5.7308884

Batch 71610, train_perplexity=306.90848, train_loss=5.7265496

Batch 71620, train_perplexity=304.28452, train_loss=5.717963

Batch 71630, train_perplexity=271.2934, train_loss=5.603201

Batch 71640, train_perplexity=336.33466, train_loss=5.8181067

Batch 71650, train_perplexity=318.07254, train_loss=5.7622795

Batch 71660, train_perplexity=335.81384, train_loss=5.816557

Batch 71670, train_perplexity=302.44897, train_loss=5.7119126

Batch 71680, train_perplexity=282.1764, train_loss=5.6425323

Batch 71690, train_perplexity=281.3571, train_loss=5.6396246

Batch 71700, train_perplexity=326.83188, train_loss=5.789446

Batch 71710, train_perplexity=284.78766, train_loss=5.651744

Batch 71720, train_perplexity=297.98282, train_loss=5.697036

Batch 71730, train_perplexity=332.95242, train_loss=5.8079996

Batch 71740, train_perplexity=304.97742, train_loss=5.7202377

Batch 71750, train_perplexity=280.24426, train_loss=5.6356616

Batch 71760, train_perplexity=272.75604, train_loss=5.6085777

Batch 71770, train_perplexity=286.18655, train_loss=5.656644

Batch 71780, train_perplexity=294.9013, train_loss=5.6866407

Batch 71790, train_perplexity=294.01224, train_loss=5.6836214

Batch 71800, train_perplexity=272.18137, train_loss=5.6064687

Batch 71810, train_perplexity=290.10248, train_loss=5.670234

Batch 71820, train_perplexity=320.61002, train_loss=5.7702255

Batch 71830, train_perplexity=300.74997, train_loss=5.7062793

Batch 71840, train_perplexity=321.81927, train_loss=5.77399

Batch 71850, train_perplexity=286.1298, train_loss=5.6564455

Batch 71860, train_perplexity=328.91718, train_loss=5.795806

Batch 71870, train_perplexity=282.09433, train_loss=5.6422415

Batch 71880, train_perplexity=269.0531, train_loss=5.5949087

Batch 71890, train_perplexity=323.12692, train_loss=5.778045

Batch 71900, train_perplexity=305.59378, train_loss=5.7222567

Batch 71910, train_perplexity=271.43173, train_loss=5.6037107

Batch 71920, train_perplexity=317.08762, train_loss=5.759178

Batch 71930, train_perplexity=281.69604, train_loss=5.6408286

Batch 71940, train_perplexity=276.95633, train_loss=5.62386

Batch 71950, train_perplexity=312.42316, train_loss=5.7443585

Batch 71960, train_perplexity=309.41562, train_loss=5.7346854

Batch 71970, train_perplexity=288.4006, train_loss=5.6643505

Batch 71980, train_perplexity=295.94684, train_loss=5.69018

Batch 71990, train_perplexity=296.8276, train_loss=5.6931515

Batch 72000, train_perplexity=290.49823, train_loss=5.6715975

Batch 72010, train_perplexity=296.44455, train_loss=5.69186

Batch 72020, train_perplexity=330.02853, train_loss=5.799179

Batch 72030, train_perplexity=275.28986, train_loss=5.6178246

Batch 72040, train_perplexity=276.69934, train_loss=5.6229315

Batch 72050, train_perplexity=282.00745, train_loss=5.6419334

Batch 72060, train_perplexity=292.58124, train_loss=5.6787424

Batch 72070, train_perplexity=295.81647, train_loss=5.689739

Batch 72080, train_perplexity=296.7008, train_loss=5.692724

Batch 72090, train_perplexity=284.03308, train_loss=5.649091

Batch 72100, train_perplexity=264.8765, train_loss=5.5792637

Batch 72110, train_perplexity=273.85687, train_loss=5.6126056

Batch 72120, train_perplexity=306.66595, train_loss=5.725759

Batch 72130, train_perplexity=284.79825, train_loss=5.651781

Batch 72140, train_perplexity=359.09753, train_loss=5.883594

Batch 72150, train_perplexity=308.5615, train_loss=5.731921

Batch 72160, train_perplexity=282.00516, train_loss=5.6419253

Batch 72170, train_perplexity=306.38196, train_loss=5.7248325
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72180, train_perplexity=290.29703, train_loss=5.6709046

Batch 72190, train_perplexity=281.1511, train_loss=5.638892

Batch 72200, train_perplexity=279.81137, train_loss=5.6341157

Batch 72210, train_perplexity=293.13007, train_loss=5.6806164

Batch 72220, train_perplexity=277.1878, train_loss=5.6246953

Batch 72230, train_perplexity=310.24487, train_loss=5.737362

Batch 72240, train_perplexity=296.16736, train_loss=5.6909246

Batch 72250, train_perplexity=310.00177, train_loss=5.736578

Batch 72260, train_perplexity=280.07742, train_loss=5.635066

Batch 72270, train_perplexity=313.86154, train_loss=5.748952

Batch 72280, train_perplexity=290.80618, train_loss=5.672657

Batch 72290, train_perplexity=287.95813, train_loss=5.662815

Batch 72300, train_perplexity=283.55472, train_loss=5.647405

Batch 72310, train_perplexity=297.72916, train_loss=5.696184

Batch 72320, train_perplexity=315.06006, train_loss=5.7527633

Batch 72330, train_perplexity=295.75934, train_loss=5.689546

Batch 72340, train_perplexity=318.95132, train_loss=5.7650385

Batch 72350, train_perplexity=290.09625, train_loss=5.6702127

Batch 72360, train_perplexity=289.2807, train_loss=5.6673975

Batch 72370, train_perplexity=324.23804, train_loss=5.781478

Batch 72380, train_perplexity=280.88416, train_loss=5.6379423

Batch 72390, train_perplexity=284.26465, train_loss=5.6499057

Batch 72400, train_perplexity=272.99023, train_loss=5.609436

Batch 72410, train_perplexity=316.26752, train_loss=5.7565885

Batch 72420, train_perplexity=292.5923, train_loss=5.67878

Batch 72430, train_perplexity=305.15604, train_loss=5.7208233

Batch 72440, train_perplexity=297.34125, train_loss=5.6948805

Batch 72450, train_perplexity=301.51617, train_loss=5.7088237

Batch 72460, train_perplexity=272.62497, train_loss=5.608097

Batch 72470, train_perplexity=282.47363, train_loss=5.643585

Batch 72480, train_perplexity=279.14078, train_loss=5.6317163

Batch 72490, train_perplexity=261.2078, train_loss=5.565316

Batch 72500, train_perplexity=296.3716, train_loss=5.691614

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00066-of-00100
Loaded 305480 sentences.
Finished loading
Batch 72510, train_perplexity=267.44296, train_loss=5.5889063

Batch 72520, train_perplexity=307.20966, train_loss=5.7275305

Batch 72530, train_perplexity=305.9046, train_loss=5.7232733

Batch 72540, train_perplexity=283.52362, train_loss=5.6472955

Batch 72550, train_perplexity=301.0055, train_loss=5.7071285

Batch 72560, train_perplexity=292.82452, train_loss=5.6795735

Batch 72570, train_perplexity=304.9966, train_loss=5.7203007

Batch 72580, train_perplexity=256.90198, train_loss=5.5486946

Batch 72590, train_perplexity=308.19537, train_loss=5.730734

Batch 72600, train_perplexity=311.94724, train_loss=5.742834

Batch 72610, train_perplexity=264.0714, train_loss=5.5762196

Batch 72620, train_perplexity=291.26428, train_loss=5.674231

Batch 72630, train_perplexity=299.74564, train_loss=5.7029343

Batch 72640, train_perplexity=270.17157, train_loss=5.599057

Batch 72650, train_perplexity=313.6072, train_loss=5.7481413

Batch 72660, train_perplexity=279.5384, train_loss=5.6331396

Batch 72670, train_perplexity=307.2721, train_loss=5.7277336

Batch 72680, train_perplexity=278.44244, train_loss=5.6292114

Batch 72690, train_perplexity=280.1291, train_loss=5.6352506

Batch 72700, train_perplexity=307.74805, train_loss=5.7292814

Batch 72710, train_perplexity=278.13763, train_loss=5.628116

Batch 72720, train_perplexity=279.90225, train_loss=5.6344404

Batch 72730, train_perplexity=283.705, train_loss=5.647935

Batch 72740, train_perplexity=306.57077, train_loss=5.7254486

Batch 72750, train_perplexity=289.03156, train_loss=5.666536

Batch 72760, train_perplexity=265.8237, train_loss=5.5828333

Batch 72770, train_perplexity=292.04767, train_loss=5.676917

Batch 72780, train_perplexity=276.97522, train_loss=5.623928

Batch 72790, train_perplexity=285.56387, train_loss=5.6544657

Batch 72800, train_perplexity=285.68262, train_loss=5.6548815

Batch 72810, train_perplexity=309.28375, train_loss=5.734259

Batch 72820, train_perplexity=311.2642, train_loss=5.740642

Batch 72830, train_perplexity=320.53818, train_loss=5.7700014

Batch 72840, train_perplexity=301.66415, train_loss=5.7093143

Batch 72850, train_perplexity=309.81647, train_loss=5.73598

Batch 72860, train_perplexity=299.0076, train_loss=5.700469

Batch 72870, train_perplexity=302.30063, train_loss=5.711422

Batch 72880, train_perplexity=302.7071, train_loss=5.7127657

Batch 72890, train_perplexity=276.85083, train_loss=5.623479

Batch 72900, train_perplexity=293.3395, train_loss=5.6813307

Batch 72910, train_perplexity=302.87646, train_loss=5.713325

Batch 72920, train_perplexity=308.43692, train_loss=5.7315173

Batch 72930, train_perplexity=304.7531, train_loss=5.719502

Batch 72940, train_perplexity=257.85324, train_loss=5.5523906

Batch 72950, train_perplexity=291.25998, train_loss=5.6742163

Batch 72960, train_perplexity=303.51447, train_loss=5.7154293

Batch 72970, train_perplexity=277.0813, train_loss=5.624311

Batch 72980, train_perplexity=299.6442, train_loss=5.7025957

Batch 72990, train_perplexity=287.48398, train_loss=5.661167

Batch 73000, train_perplexity=311.02933, train_loss=5.739887

Batch 73010, train_perplexity=290.9485, train_loss=5.6731462

Batch 73020, train_perplexity=247.12643, train_loss=5.5099

Batch 73030, train_perplexity=303.08118, train_loss=5.7140007

Batch 73040, train_perplexity=250.24106, train_loss=5.5224247

Batch 73050, train_perplexity=316.02737, train_loss=5.755829

Batch 73060, train_perplexity=296.9247, train_loss=5.6934786

Batch 73070, train_perplexity=283.00293, train_loss=5.6454573

Batch 73080, train_perplexity=293.4839, train_loss=5.681823

Batch 73090, train_perplexity=292.3506, train_loss=5.6779537

Batch 73100, train_perplexity=295.77078, train_loss=5.6895847

Batch 73110, train_perplexity=300.8026, train_loss=5.7064543

Batch 73120, train_perplexity=281.85284, train_loss=5.641385

Batch 73130, train_perplexity=280.06244, train_loss=5.6350126

Batch 73140, train_perplexity=251.49016, train_loss=5.527404

Batch 73150, train_perplexity=293.78116, train_loss=5.682835

Batch 73160, train_perplexity=288.4563, train_loss=5.6645436

Batch 73170, train_perplexity=298.30197, train_loss=5.6981063

Batch 73180, train_perplexity=263.22934, train_loss=5.5730257

Batch 73190, train_perplexity=295.84976, train_loss=5.6898518

Batch 73200, train_perplexity=255.64697, train_loss=5.5437975

Batch 73210, train_perplexity=292.085, train_loss=5.677045

Batch 73220, train_perplexity=301.92203, train_loss=5.710169

Batch 73230, train_perplexity=309.75485, train_loss=5.735781

Batch 73240, train_perplexity=318.46683, train_loss=5.7635183

Batch 73250, train_perplexity=297.09976, train_loss=5.694068

Batch 73260, train_perplexity=290.12918, train_loss=5.670326

Batch 73270, train_perplexity=287.79807, train_loss=5.662259

Batch 73280, train_perplexity=281.84546, train_loss=5.641359

Batch 73290, train_perplexity=296.7554, train_loss=5.6929083

Batch 73300, train_perplexity=300.6846, train_loss=5.706062

Batch 73310, train_perplexity=290.16748, train_loss=5.6704583

Batch 73320, train_perplexity=305.98746, train_loss=5.723544

Batch 73330, train_perplexity=306.39423, train_loss=5.7248726

Batch 73340, train_perplexity=303.29834, train_loss=5.714717

Batch 73350, train_perplexity=286.28156, train_loss=5.6569757

Batch 73360, train_perplexity=323.32126, train_loss=5.7786465

Batch 73370, train_perplexity=316.972, train_loss=5.7588134

Batch 73380, train_perplexity=281.55423, train_loss=5.640325

Batch 73390, train_perplexity=306.47708, train_loss=5.725143

Batch 73400, train_perplexity=266.6246, train_loss=5.5858417

Batch 73410, train_perplexity=307.1082, train_loss=5.7272

Batch 73420, train_perplexity=279.9176, train_loss=5.6344953

Batch 73430, train_perplexity=266.00858, train_loss=5.5835285
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 73440, train_perplexity=270.7426, train_loss=5.6011686

Batch 73450, train_perplexity=279.40872, train_loss=5.6326756

Batch 73460, train_perplexity=272.4482, train_loss=5.6074486

Batch 73470, train_perplexity=290.08185, train_loss=5.670163

Batch 73480, train_perplexity=302.96445, train_loss=5.7136154

Batch 73490, train_perplexity=282.44803, train_loss=5.6434946

Batch 73500, train_perplexity=311.70935, train_loss=5.742071

Batch 73510, train_perplexity=296.6575, train_loss=5.6925783

Batch 73520, train_perplexity=281.15268, train_loss=5.638898

Batch 73530, train_perplexity=294.41937, train_loss=5.685005

Batch 73540, train_perplexity=326.01065, train_loss=5.78693

Batch 73550, train_perplexity=316.91864, train_loss=5.758645

Batch 73560, train_perplexity=264.0602, train_loss=5.576177

Batch 73570, train_perplexity=340.26907, train_loss=5.8297367

Batch 73580, train_perplexity=289.07965, train_loss=5.6667023

Batch 73590, train_perplexity=284.11273, train_loss=5.649371

Batch 73600, train_perplexity=289.80728, train_loss=5.669216

Batch 73610, train_perplexity=274.56488, train_loss=5.6151876

Batch 73620, train_perplexity=296.41855, train_loss=5.6917725

Batch 73630, train_perplexity=260.53333, train_loss=5.562731

Batch 73640, train_perplexity=259.64807, train_loss=5.559327

Batch 73650, train_perplexity=271.049, train_loss=5.6022997

Batch 73660, train_perplexity=279.14066, train_loss=5.631716

Batch 73670, train_perplexity=292.75946, train_loss=5.6793513

Batch 73680, train_perplexity=273.7524, train_loss=5.612224

Batch 73690, train_perplexity=263.78864, train_loss=5.575148

Batch 73700, train_perplexity=225.10799, train_loss=5.41658

Batch 73710, train_perplexity=313.2941, train_loss=5.7471423

Batch 73720, train_perplexity=304.15512, train_loss=5.717538

Batch 73730, train_perplexity=352.4535, train_loss=5.8649187

Batch 73740, train_perplexity=296.1494, train_loss=5.690864

Batch 73750, train_perplexity=317.20346, train_loss=5.7595434

Batch 73760, train_perplexity=306.29724, train_loss=5.724556

Batch 73770, train_perplexity=288.68433, train_loss=5.6653337

Batch 73780, train_perplexity=301.70172, train_loss=5.709439

Batch 73790, train_perplexity=264.91452, train_loss=5.579407

Batch 73800, train_perplexity=284.47647, train_loss=5.6506505

Batch 73810, train_perplexity=284.90475, train_loss=5.652155

Batch 73820, train_perplexity=261.62262, train_loss=5.566903

Batch 73830, train_perplexity=307.74146, train_loss=5.72926

Batch 73840, train_perplexity=276.8919, train_loss=5.623627

Batch 73850, train_perplexity=296.87177, train_loss=5.6933002

Batch 73860, train_perplexity=271.17676, train_loss=5.602771

Batch 73870, train_perplexity=302.5361, train_loss=5.7122006

Batch 73880, train_perplexity=312.96307, train_loss=5.746085

Batch 73890, train_perplexity=279.28592, train_loss=5.632236

Batch 73900, train_perplexity=284.8018, train_loss=5.6517935

Batch 73910, train_perplexity=247.10075, train_loss=5.509796

Batch 73920, train_perplexity=299.43195, train_loss=5.701887

Batch 73930, train_perplexity=272.0411, train_loss=5.605953

Batch 73940, train_perplexity=278.47006, train_loss=5.6293106

Batch 73950, train_perplexity=310.39078, train_loss=5.737832

Batch 73960, train_perplexity=311.18066, train_loss=5.7403736

Batch 73970, train_perplexity=274.21124, train_loss=5.6138988

Batch 73980, train_perplexity=324.2343, train_loss=5.7814665

Batch 73990, train_perplexity=306.89984, train_loss=5.7265215

Batch 74000, train_perplexity=264.95456, train_loss=5.5795584

Batch 74010, train_perplexity=258.37027, train_loss=5.554394

Batch 74020, train_perplexity=290.7061, train_loss=5.6723127

Batch 74030, train_perplexity=292.95874, train_loss=5.680032

Batch 74040, train_perplexity=297.64398, train_loss=5.695898

Batch 74050, train_perplexity=275.1232, train_loss=5.617219

Batch 74060, train_perplexity=303.03104, train_loss=5.7138352

Batch 74070, train_perplexity=295.96884, train_loss=5.690254

Batch 74080, train_perplexity=302.26025, train_loss=5.7112885

Batch 74090, train_perplexity=262.6912, train_loss=5.570979

Batch 74100, train_perplexity=285.08575, train_loss=5.65279

Batch 74110, train_perplexity=310.24783, train_loss=5.7373714

Batch 74120, train_perplexity=269.01794, train_loss=5.594778

Batch 74130, train_perplexity=290.15573, train_loss=5.670418

Batch 74140, train_perplexity=283.52188, train_loss=5.6472893

Batch 74150, train_perplexity=308.5006, train_loss=5.731724

Batch 74160, train_perplexity=279.9893, train_loss=5.6347513

Batch 74170, train_perplexity=285.1037, train_loss=5.652853

Batch 74180, train_perplexity=252.39441, train_loss=5.530993

Batch 74190, train_perplexity=300.89456, train_loss=5.70676

Batch 74200, train_perplexity=279.827, train_loss=5.6341715

Batch 74210, train_perplexity=305.56476, train_loss=5.722162

Batch 74220, train_perplexity=250.6674, train_loss=5.524127

Batch 74230, train_perplexity=254.57503, train_loss=5.5395956

Batch 74240, train_perplexity=308.0502, train_loss=5.7302628

Batch 74250, train_perplexity=267.80768, train_loss=5.590269

Batch 74260, train_perplexity=341.202, train_loss=5.8324747

Batch 74270, train_perplexity=308.65982, train_loss=5.7322397

Batch 74280, train_perplexity=257.29895, train_loss=5.5502386

Batch 74290, train_perplexity=272.55893, train_loss=5.607855

Batch 74300, train_perplexity=290.48355, train_loss=5.671547

Batch 74310, train_perplexity=302.69095, train_loss=5.7127123

Batch 74320, train_perplexity=316.89157, train_loss=5.7585597

Batch 74330, train_perplexity=255.07298, train_loss=5.5415497

Batch 74340, train_perplexity=278.27548, train_loss=5.6286116

Batch 74350, train_perplexity=268.24094, train_loss=5.5918856

Batch 74360, train_perplexity=293.5301, train_loss=5.68198

Batch 74370, train_perplexity=254.09889, train_loss=5.5377235

Batch 74380, train_perplexity=317.7539, train_loss=5.761277

Batch 74390, train_perplexity=274.71707, train_loss=5.6157417

Batch 74400, train_perplexity=326.85898, train_loss=5.789529

Batch 74410, train_perplexity=288.0253, train_loss=5.6630483

Batch 74420, train_perplexity=269.27243, train_loss=5.5957236

Batch 74430, train_perplexity=321.26608, train_loss=5.7722697

Batch 74440, train_perplexity=299.76422, train_loss=5.7029963

Batch 74450, train_perplexity=297.71808, train_loss=5.696147

Batch 74460, train_perplexity=313.105, train_loss=5.7465386

Batch 74470, train_perplexity=324.49524, train_loss=5.782271

Batch 74480, train_perplexity=309.34113, train_loss=5.7344446

Batch 74490, train_perplexity=257.80984, train_loss=5.5522223

Batch 74500, train_perplexity=277.00534, train_loss=5.624037

Batch 74510, train_perplexity=298.0422, train_loss=5.697235

Batch 74520, train_perplexity=286.8962, train_loss=5.6591206

Batch 74530, train_perplexity=267.3059, train_loss=5.5883937

Batch 74540, train_perplexity=255.37808, train_loss=5.542745

Batch 74550, train_perplexity=279.52133, train_loss=5.6330786

Batch 74560, train_perplexity=279.15143, train_loss=5.6317544

Batch 74570, train_perplexity=339.81555, train_loss=5.828403

Batch 74580, train_perplexity=290.54047, train_loss=5.671743

Batch 74590, train_perplexity=280.6389, train_loss=5.6370687

Batch 74600, train_perplexity=280.75345, train_loss=5.637477

Batch 74610, train_perplexity=262.95673, train_loss=5.5719895

Batch 74620, train_perplexity=261.38745, train_loss=5.566004

Batch 74630, train_perplexity=288.5116, train_loss=5.6647353

Batch 74640, train_perplexity=281.13474, train_loss=5.638834

Batch 74650, train_perplexity=289.7473, train_loss=5.669009

Batch 74660, train_perplexity=276.0599, train_loss=5.620618

Batch 74670, train_perplexity=263.34747, train_loss=5.5734744

Batch 74680, train_perplexity=282.81433, train_loss=5.6447906

Batch 74690, train_perplexity=272.3921, train_loss=5.6072426

Batch 74700, train_perplexity=298.55255, train_loss=5.698946

Batch 74710, train_perplexity=318.271, train_loss=5.762903

Batch 74720, train_perplexity=273.81442, train_loss=5.6124506

Batch 74730, train_perplexity=313.94833, train_loss=5.7492285

Batch 74740, train_perplexity=275.29025, train_loss=5.617826

Batch 74750, train_perplexity=254.63962, train_loss=5.5398493
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74760, train_perplexity=298.18237, train_loss=5.6977053

Batch 74770, train_perplexity=281.32596, train_loss=5.639514

Batch 74780, train_perplexity=294.83017, train_loss=5.6863995

Batch 74790, train_perplexity=272.03723, train_loss=5.605939

Batch 74800, train_perplexity=297.72986, train_loss=5.6961865

Batch 74810, train_perplexity=310.07568, train_loss=5.7368164

Batch 74820, train_perplexity=314.42117, train_loss=5.7507334

Batch 74830, train_perplexity=256.0889, train_loss=5.5455246

Batch 74840, train_perplexity=266.7234, train_loss=5.586212

Batch 74850, train_perplexity=243.07976, train_loss=5.4933896

Batch 74860, train_perplexity=266.3543, train_loss=5.5848274

Batch 74870, train_perplexity=293.9859, train_loss=5.6835318

Batch 74880, train_perplexity=259.33588, train_loss=5.558124

Batch 74890, train_perplexity=280.42953, train_loss=5.6363225

Batch 74900, train_perplexity=307.19415, train_loss=5.72748

Batch 74910, train_perplexity=277.81674, train_loss=5.6269617

Batch 74920, train_perplexity=278.4709, train_loss=5.6293135

Batch 74930, train_perplexity=279.66904, train_loss=5.633607

Batch 74940, train_perplexity=288.39484, train_loss=5.6643305

Batch 74950, train_perplexity=271.77286, train_loss=5.6049666

Batch 74960, train_perplexity=292.69092, train_loss=5.679117

Batch 74970, train_perplexity=258.41846, train_loss=5.55458

Batch 74980, train_perplexity=307.50162, train_loss=5.7284803

Batch 74990, train_perplexity=308.3112, train_loss=5.7311096

Batch 75000, train_perplexity=287.4289, train_loss=5.6609755

Batch 75010, train_perplexity=255.72256, train_loss=5.544093

Batch 75020, train_perplexity=312.7138, train_loss=5.7452884

Batch 75030, train_perplexity=323.85498, train_loss=5.780296

Batch 75040, train_perplexity=292.37485, train_loss=5.6780367

Batch 75050, train_perplexity=335.7357, train_loss=5.816324

Batch 75060, train_perplexity=271.27606, train_loss=5.603137

Batch 75070, train_perplexity=295.81082, train_loss=5.68972

Batch 75080, train_perplexity=298.31122, train_loss=5.6981373

Batch 75090, train_perplexity=315.41766, train_loss=5.7538977

Batch 75100, train_perplexity=307.2224, train_loss=5.727572

Batch 75110, train_perplexity=290.8679, train_loss=5.672869

Batch 75120, train_perplexity=297.68997, train_loss=5.6960526

Batch 75130, train_perplexity=305.55835, train_loss=5.722141

Batch 75140, train_perplexity=268.63492, train_loss=5.5933533

Batch 75150, train_perplexity=284.10245, train_loss=5.649335

Batch 75160, train_perplexity=278.11392, train_loss=5.628031

Batch 75170, train_perplexity=270.0738, train_loss=5.5986953

Batch 75180, train_perplexity=313.91272, train_loss=5.749115

Batch 75190, train_perplexity=315.824, train_loss=5.755185

Batch 75200, train_perplexity=271.04953, train_loss=5.6023016

Batch 75210, train_perplexity=274.80197, train_loss=5.6160507

Batch 75220, train_perplexity=309.25824, train_loss=5.7341766

Batch 75230, train_perplexity=303.35864, train_loss=5.7149158

Batch 75240, train_perplexity=271.40897, train_loss=5.6036267

Batch 75250, train_perplexity=265.98917, train_loss=5.5834556

Batch 75260, train_perplexity=290.57776, train_loss=5.671871

Batch 75270, train_perplexity=265.24936, train_loss=5.5806704

Batch 75280, train_perplexity=303.7322, train_loss=5.7161465

Batch 75290, train_perplexity=313.68738, train_loss=5.748397

Batch 75300, train_perplexity=286.1826, train_loss=5.65663

Batch 75310, train_perplexity=270.63406, train_loss=5.6007676

Batch 75320, train_perplexity=315.9885, train_loss=5.755706

Batch 75330, train_perplexity=281.06717, train_loss=5.6385937

Batch 75340, train_perplexity=268.01642, train_loss=5.5910482

Batch 75350, train_perplexity=329.73343, train_loss=5.7982845

Batch 75360, train_perplexity=297.74222, train_loss=5.696228

Batch 75370, train_perplexity=275.5508, train_loss=5.618772

Batch 75380, train_perplexity=259.87177, train_loss=5.5601883

Batch 75390, train_perplexity=270.72015, train_loss=5.6010857

Batch 75400, train_perplexity=268.93073, train_loss=5.594454

Batch 75410, train_perplexity=276.62848, train_loss=5.6226754

Batch 75420, train_perplexity=277.30322, train_loss=5.6251116

Batch 75430, train_perplexity=275.4482, train_loss=5.6183996

Batch 75440, train_perplexity=309.8336, train_loss=5.7360353

Batch 75450, train_perplexity=286.86078, train_loss=5.658997

Batch 75460, train_perplexity=266.5777, train_loss=5.5856657

Batch 75470, train_perplexity=314.04086, train_loss=5.749523

Batch 75480, train_perplexity=280.9474, train_loss=5.6381674

Batch 75490, train_perplexity=316.5833, train_loss=5.7575865

Batch 75500, train_perplexity=272.928, train_loss=5.609208

Batch 75510, train_perplexity=262.1441, train_loss=5.5688944

Batch 75520, train_perplexity=311.91452, train_loss=5.742729

Batch 75530, train_perplexity=279.97153, train_loss=5.634688

Batch 75540, train_perplexity=263.85693, train_loss=5.575407

Batch 75550, train_perplexity=260.1748, train_loss=5.5613537

Batch 75560, train_perplexity=314.0855, train_loss=5.7496653

Batch 75570, train_perplexity=260.6576, train_loss=5.5632076

Batch 75580, train_perplexity=290.73465, train_loss=5.672411

Batch 75590, train_perplexity=281.68488, train_loss=5.640789

Batch 75600, train_perplexity=274.93158, train_loss=5.6165223

Batch 75610, train_perplexity=275.97855, train_loss=5.620323

Batch 75620, train_perplexity=291.50784, train_loss=5.675067

Batch 75630, train_perplexity=292.11426, train_loss=5.677145

Batch 75640, train_perplexity=276.86972, train_loss=5.623547

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Batch 75650, train_perplexity=296.05975, train_loss=5.6905613

Batch 75660, train_perplexity=281.30515, train_loss=5.63944

Batch 75670, train_perplexity=297.5431, train_loss=5.695559

Batch 75680, train_perplexity=285.77896, train_loss=5.6552186

Batch 75690, train_perplexity=270.72223, train_loss=5.6010933

Batch 75700, train_perplexity=288.56897, train_loss=5.664934

Batch 75710, train_perplexity=283.9995, train_loss=5.6489725

Batch 75720, train_perplexity=319.20694, train_loss=5.7658396

Batch 75730, train_perplexity=283.09918, train_loss=5.6457973

Batch 75740, train_perplexity=289.08655, train_loss=5.666726

Batch 75750, train_perplexity=264.13312, train_loss=5.576453

Batch 75760, train_perplexity=294.33682, train_loss=5.684725

Batch 75770, train_perplexity=304.06754, train_loss=5.71725

Batch 75780, train_perplexity=273.56985, train_loss=5.611557

Batch 75790, train_perplexity=260.71826, train_loss=5.5634403

Batch 75800, train_perplexity=282.45935, train_loss=5.6435347

Batch 75810, train_perplexity=308.89127, train_loss=5.7329893

Batch 75820, train_perplexity=285.75525, train_loss=5.6551356

Batch 75830, train_perplexity=295.7956, train_loss=5.6896687

Batch 75840, train_perplexity=296.5752, train_loss=5.692301

Batch 75850, train_perplexity=305.59027, train_loss=5.722245

Batch 75860, train_perplexity=273.69513, train_loss=5.612015

Batch 75870, train_perplexity=298.3896, train_loss=5.6984

Batch 75880, train_perplexity=284.2667, train_loss=5.649913

Batch 75890, train_perplexity=292.4546, train_loss=5.6783094

Batch 75900, train_perplexity=285.78235, train_loss=5.6552305

Batch 75910, train_perplexity=293.0163, train_loss=5.680228

Batch 75920, train_perplexity=298.38504, train_loss=5.698385

Batch 75930, train_perplexity=271.5797, train_loss=5.6042557

Batch 75940, train_perplexity=284.17276, train_loss=5.6495824

Batch 75950, train_perplexity=276.93152, train_loss=5.62377

Batch 75960, train_perplexity=269.49863, train_loss=5.5965633

Batch 75970, train_perplexity=270.5297, train_loss=5.600382

Batch 75980, train_perplexity=276.66608, train_loss=5.6228113

Batch 75990, train_perplexity=260.1284, train_loss=5.5611753

Batch 76000, train_perplexity=256.79544, train_loss=5.54828

Batch 76010, train_perplexity=269.73956, train_loss=5.597457
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76020, train_perplexity=300.50873, train_loss=5.7054768

Batch 76030, train_perplexity=304.68207, train_loss=5.719269

Batch 76040, train_perplexity=270.99966, train_loss=5.6021175

Batch 76050, train_perplexity=273.09152, train_loss=5.609807

Batch 76060, train_perplexity=269.6583, train_loss=5.5971556

Batch 76070, train_perplexity=290.73215, train_loss=5.6724024

Batch 76080, train_perplexity=304.9652, train_loss=5.7201977

Batch 76090, train_perplexity=307.6603, train_loss=5.7289963

Batch 76100, train_perplexity=307.78018, train_loss=5.729386

Batch 76110, train_perplexity=266.54642, train_loss=5.5855484

Batch 76120, train_perplexity=270.39413, train_loss=5.5998807

Batch 76130, train_perplexity=253.06023, train_loss=5.5336275

Batch 76140, train_perplexity=282.52252, train_loss=5.6437583

Batch 76150, train_perplexity=271.96692, train_loss=5.6056805

Batch 76160, train_perplexity=303.98724, train_loss=5.7169857

Batch 76170, train_perplexity=287.8501, train_loss=5.66244

Batch 76180, train_perplexity=263.29086, train_loss=5.5732594

Batch 76190, train_perplexity=293.88947, train_loss=5.6832037

Batch 76200, train_perplexity=295.21518, train_loss=5.6877046

Batch 76210, train_perplexity=299.3417, train_loss=5.701586

Batch 76220, train_perplexity=259.48914, train_loss=5.558715

Batch 76230, train_perplexity=290.61795, train_loss=5.6720095

Batch 76240, train_perplexity=281.98672, train_loss=5.64186

Batch 76250, train_perplexity=294.1407, train_loss=5.684058

Batch 76260, train_perplexity=231.07071, train_loss=5.4427238

Batch 76270, train_perplexity=315.92297, train_loss=5.7554984

Batch 76280, train_perplexity=289.47748, train_loss=5.6680775

Batch 76290, train_perplexity=274.90524, train_loss=5.6164265

Batch 76300, train_perplexity=302.4503, train_loss=5.711917

Batch 76310, train_perplexity=261.95306, train_loss=5.5681653

Batch 76320, train_perplexity=291.10294, train_loss=5.673677

Batch 76330, train_perplexity=289.66678, train_loss=5.668731

Batch 76340, train_perplexity=263.63156, train_loss=5.5745525

Batch 76350, train_perplexity=284.82816, train_loss=5.651886

Batch 76360, train_perplexity=292.7984, train_loss=5.6794844

Batch 76370, train_perplexity=296.2959, train_loss=5.6913586

Batch 76380, train_perplexity=299.34943, train_loss=5.7016115

Batch 76390, train_perplexity=329.39713, train_loss=5.797264

Batch 76400, train_perplexity=325.409, train_loss=5.785083

Batch 76410, train_perplexity=295.7575, train_loss=5.68954

Batch 76420, train_perplexity=302.5752, train_loss=5.71233

Batch 76430, train_perplexity=308.32574, train_loss=5.731157

Batch 76440, train_perplexity=297.9365, train_loss=5.6968803

Batch 76450, train_perplexity=300.3755, train_loss=5.7050333

Batch 76460, train_perplexity=271.56558, train_loss=5.6042037

Batch 76470, train_perplexity=280.17545, train_loss=5.635416

Batch 76480, train_perplexity=295.91324, train_loss=5.6900663

Batch 76490, train_perplexity=263.51117, train_loss=5.5740957

Batch 76500, train_perplexity=296.77606, train_loss=5.692978

Batch 76510, train_perplexity=280.49976, train_loss=5.636573

Batch 76520, train_perplexity=269.27527, train_loss=5.595734

Batch 76530, train_perplexity=291.52023, train_loss=5.6751094

Batch 76540, train_perplexity=305.5588, train_loss=5.722142

Batch 76550, train_perplexity=281.9277, train_loss=5.6416507

Batch 76560, train_perplexity=292.26318, train_loss=5.6776547

Batch 76570, train_perplexity=247.4507, train_loss=5.5112114

Batch 76580, train_perplexity=287.202, train_loss=5.660186

Batch 76590, train_perplexity=303.71744, train_loss=5.716098

Batch 76600, train_perplexity=297.86972, train_loss=5.696656

Batch 76610, train_perplexity=300.29358, train_loss=5.7047606

Batch 76620, train_perplexity=254.89206, train_loss=5.54084

Batch 76630, train_perplexity=305.0292, train_loss=5.7204075

Batch 76640, train_perplexity=272.13272, train_loss=5.60629

Batch 76650, train_perplexity=270.36203, train_loss=5.599762

Batch 76660, train_perplexity=263.34122, train_loss=5.5734506

Batch 76670, train_perplexity=275.74152, train_loss=5.619464

Batch 76680, train_perplexity=281.3764, train_loss=5.6396933

Batch 76690, train_perplexity=307.7039, train_loss=5.729138

Batch 76700, train_perplexity=297.24655, train_loss=5.694562

Batch 76710, train_perplexity=258.28024, train_loss=5.554045

Batch 76720, train_perplexity=273.32526, train_loss=5.6106625

Batch 76730, train_perplexity=264.33875, train_loss=5.5772314

Batch 76740, train_perplexity=304.49585, train_loss=5.7186575

Batch 76750, train_perplexity=253.39397, train_loss=5.5349455

Batch 76760, train_perplexity=250.02303, train_loss=5.521553

Batch 76770, train_perplexity=287.51852, train_loss=5.6612873

Batch 76780, train_perplexity=285.775, train_loss=5.655205

Batch 76790, train_perplexity=264.81766, train_loss=5.5790415

Batch 76800, train_perplexity=281.13928, train_loss=5.63885

Batch 76810, train_perplexity=259.7994, train_loss=5.55991

Batch 76820, train_perplexity=286.65213, train_loss=5.6582694

Batch 76830, train_perplexity=277.5088, train_loss=5.6258526

Batch 76840, train_perplexity=257.321, train_loss=5.5503244

Batch 76850, train_perplexity=308.71515, train_loss=5.732419

Batch 76860, train_perplexity=266.6397, train_loss=5.5858984

Batch 76870, train_perplexity=273.64606, train_loss=5.6118355

Batch 76880, train_perplexity=296.72797, train_loss=5.692816

Batch 76890, train_perplexity=326.29712, train_loss=5.7878084

Batch 76900, train_perplexity=303.75266, train_loss=5.7162137

Batch 76910, train_perplexity=272.22214, train_loss=5.6066184

Batch 76920, train_perplexity=296.97696, train_loss=5.6936545

Batch 76930, train_perplexity=301.4775, train_loss=5.7086954

Batch 76940, train_perplexity=268.9592, train_loss=5.5945597

Batch 76950, train_perplexity=284.36673, train_loss=5.6502647

Batch 76960, train_perplexity=279.77414, train_loss=5.6339827

Batch 76970, train_perplexity=293.54102, train_loss=5.6820173

Batch 76980, train_perplexity=295.46658, train_loss=5.6885557

Batch 76990, train_perplexity=276.56348, train_loss=5.6224403

Batch 77000, train_perplexity=307.04858, train_loss=5.727006

Batch 77010, train_perplexity=271.58722, train_loss=5.6042833

Batch 77020, train_perplexity=266.5594, train_loss=5.585597

Batch 77030, train_perplexity=301.50827, train_loss=5.7087975

Batch 77040, train_perplexity=252.28516, train_loss=5.53056

Batch 77050, train_perplexity=273.0996, train_loss=5.6098366

Batch 77060, train_perplexity=263.34436, train_loss=5.5734625

Batch 77070, train_perplexity=311.57217, train_loss=5.741631

Batch 77080, train_perplexity=249.81746, train_loss=5.5207305

Batch 77090, train_perplexity=299.79297, train_loss=5.703092

Batch 77100, train_perplexity=294.87036, train_loss=5.686536

Batch 77110, train_perplexity=231.37004, train_loss=5.4440184

Batch 77120, train_perplexity=268.8725, train_loss=5.5942373

Batch 77130, train_perplexity=283.5197, train_loss=5.6472816

Batch 77140, train_perplexity=269.99255, train_loss=5.5983944

Batch 77150, train_perplexity=286.76575, train_loss=5.6586657

Batch 77160, train_perplexity=267.95355, train_loss=5.5908136

Batch 77170, train_perplexity=278.70016, train_loss=5.6301365

Batch 77180, train_perplexity=252.9092, train_loss=5.5330305

Batch 77190, train_perplexity=265.21573, train_loss=5.5805435

Batch 77200, train_perplexity=267.60968, train_loss=5.5895295

Batch 77210, train_perplexity=263.0419, train_loss=5.5723133

Batch 77220, train_perplexity=268.39752, train_loss=5.592469

Batch 77230, train_perplexity=291.25333, train_loss=5.6741934

Batch 77240, train_perplexity=271.70096, train_loss=5.604702

Batch 77250, train_perplexity=300.19836, train_loss=5.7044435

Batch 77260, train_perplexity=267.70093, train_loss=5.5898705

Batch 77270, train_perplexity=303.15808, train_loss=5.7142544

Batch 77280, train_perplexity=272.96732, train_loss=5.609352

Batch 77290, train_perplexity=285.7461, train_loss=5.6551037

Batch 77300, train_perplexity=274.82242, train_loss=5.616125

Batch 77310, train_perplexity=271.28784, train_loss=5.6031804

Batch 77320, train_perplexity=297.02722, train_loss=5.693824

Batch 77330, train_perplexity=279.73093, train_loss=5.633828
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 77340, train_perplexity=251.28685, train_loss=5.526595

Batch 77350, train_perplexity=280.40482, train_loss=5.6362343

Batch 77360, train_perplexity=295.35965, train_loss=5.688194

Batch 77370, train_perplexity=279.81577, train_loss=5.6341314

Batch 77380, train_perplexity=281.03876, train_loss=5.6384926

Batch 77390, train_perplexity=318.87878, train_loss=5.764811

Batch 77400, train_perplexity=248.79071, train_loss=5.516612

Batch 77410, train_perplexity=269.86102, train_loss=5.597907

Batch 77420, train_perplexity=295.8829, train_loss=5.689964

Batch 77430, train_perplexity=300.77393, train_loss=5.706359

Batch 77440, train_perplexity=269.12506, train_loss=5.595176

Batch 77450, train_perplexity=319.8191, train_loss=5.7677555

Batch 77460, train_perplexity=250.34096, train_loss=5.522824

Batch 77470, train_perplexity=286.91962, train_loss=5.659202

Batch 77480, train_perplexity=306.9044, train_loss=5.7265363

Batch 77490, train_perplexity=303.4857, train_loss=5.7153344

Batch 77500, train_perplexity=290.31876, train_loss=5.6709795

Batch 77510, train_perplexity=269.93103, train_loss=5.5981665

Batch 77520, train_perplexity=269.4327, train_loss=5.5963187

Batch 77530, train_perplexity=270.56995, train_loss=5.6005306

Batch 77540, train_perplexity=306.07516, train_loss=5.7238307

Batch 77550, train_perplexity=266.7492, train_loss=5.586309

Batch 77560, train_perplexity=288.98953, train_loss=5.6663904

Batch 77570, train_perplexity=268.72818, train_loss=5.5937004

Batch 77580, train_perplexity=295.25897, train_loss=5.687853

Batch 77590, train_perplexity=284.89035, train_loss=5.6521044

Batch 77600, train_perplexity=269.07465, train_loss=5.594989

Batch 77610, train_perplexity=299.7638, train_loss=5.702995

Batch 77620, train_perplexity=299.7451, train_loss=5.7029324

Batch 77630, train_perplexity=254.57625, train_loss=5.5396004

Batch 77640, train_perplexity=272.7672, train_loss=5.6086187

Batch 77650, train_perplexity=272.42847, train_loss=5.607376

Batch 77660, train_perplexity=261.2458, train_loss=5.5654616

Batch 77670, train_perplexity=254.32848, train_loss=5.5386267

Batch 77680, train_perplexity=295.84244, train_loss=5.689827

Batch 77690, train_perplexity=259.11896, train_loss=5.557287

Batch 77700, train_perplexity=262.51474, train_loss=5.5703073

Batch 77710, train_perplexity=289.631, train_loss=5.6686077

Batch 77720, train_perplexity=280.08356, train_loss=5.635088

Batch 77730, train_perplexity=310.68634, train_loss=5.738784

Batch 77740, train_perplexity=278.25745, train_loss=5.6285467

Batch 77750, train_perplexity=280.4115, train_loss=5.636258

Batch 77760, train_perplexity=287.7978, train_loss=5.662258

Batch 77770, train_perplexity=276.2918, train_loss=5.6214576

Batch 77780, train_perplexity=280.9096, train_loss=5.638033

Batch 77790, train_perplexity=281.16058, train_loss=5.638926

Batch 77800, train_perplexity=270.00967, train_loss=5.598458

Batch 77810, train_perplexity=270.80292, train_loss=5.6013913

Batch 77820, train_perplexity=345.4186, train_loss=5.844757

Batch 77830, train_perplexity=288.81705, train_loss=5.6657934

Batch 77840, train_perplexity=245.17172, train_loss=5.501959

Batch 77850, train_perplexity=295.3705, train_loss=5.6882305

Batch 77860, train_perplexity=277.5395, train_loss=5.625963

Batch 77870, train_perplexity=274.32437, train_loss=5.614311

Batch 77880, train_perplexity=280.0065, train_loss=5.634813

Batch 77890, train_perplexity=307.08722, train_loss=5.727132

Batch 77900, train_perplexity=263.3024, train_loss=5.573303

Batch 77910, train_perplexity=275.62454, train_loss=5.6190395

Batch 77920, train_perplexity=285.7615, train_loss=5.6551576

Batch 77930, train_perplexity=296.2785, train_loss=5.6913

Batch 77940, train_perplexity=262.1476, train_loss=5.5689077

Batch 77950, train_perplexity=278.14655, train_loss=5.628148

Batch 77960, train_perplexity=265.01282, train_loss=5.579778

Batch 77970, train_perplexity=257.28607, train_loss=5.5501885

Batch 77980, train_perplexity=291.93503, train_loss=5.6765313

Batch 77990, train_perplexity=277.0476, train_loss=5.6241894

Batch 78000, train_perplexity=280.68506, train_loss=5.6372333

Batch 78010, train_perplexity=249.86548, train_loss=5.5209227

Batch 78020, train_perplexity=271.68643, train_loss=5.6046486

Batch 78030, train_perplexity=282.63235, train_loss=5.644147

Batch 78040, train_perplexity=263.13284, train_loss=5.572659

Batch 78050, train_perplexity=286.1478, train_loss=5.6565084

Batch 78060, train_perplexity=281.1162, train_loss=5.638768

Batch 78070, train_perplexity=294.45306, train_loss=5.6851196

Batch 78080, train_perplexity=303.51752, train_loss=5.7154393

Batch 78090, train_perplexity=258.91095, train_loss=5.556484

Batch 78100, train_perplexity=274.10992, train_loss=5.613529

Batch 78110, train_perplexity=300.24545, train_loss=5.7046003

Batch 78120, train_perplexity=281.68933, train_loss=5.640805

Batch 78130, train_perplexity=264.06284, train_loss=5.576187

Batch 78140, train_perplexity=274.1758, train_loss=5.6137695

Batch 78150, train_perplexity=258.7298, train_loss=5.555784

Batch 78160, train_perplexity=270.77347, train_loss=5.6012826

Batch 78170, train_perplexity=326.30646, train_loss=5.787837

Batch 78180, train_perplexity=291.4727, train_loss=5.6749463

Batch 78190, train_perplexity=304.03073, train_loss=5.7171288

Batch 78200, train_perplexity=287.23322, train_loss=5.6602945

Batch 78210, train_perplexity=285.40732, train_loss=5.6539173

Batch 78220, train_perplexity=262.5638, train_loss=5.570494

Batch 78230, train_perplexity=276.60184, train_loss=5.622579

Batch 78240, train_perplexity=287.69037, train_loss=5.661885

Batch 78250, train_perplexity=274.87457, train_loss=5.616315

Batch 78260, train_perplexity=284.84662, train_loss=5.651951

Batch 78270, train_perplexity=275.92145, train_loss=5.620116

Batch 78280, train_perplexity=299.92206, train_loss=5.7035227

Batch 78290, train_perplexity=286.28647, train_loss=5.656993

Batch 78300, train_perplexity=261.05862, train_loss=5.564745

Batch 78310, train_perplexity=223.51353, train_loss=5.409472

Batch 78320, train_perplexity=291.65997, train_loss=5.6755886

Batch 78330, train_perplexity=304.76575, train_loss=5.7195435

Batch 78340, train_perplexity=292.37985, train_loss=5.678054

Batch 78350, train_perplexity=274.5162, train_loss=5.6150103

Batch 78360, train_perplexity=262.22388, train_loss=5.5691986

Batch 78370, train_perplexity=270.9759, train_loss=5.60203

Batch 78380, train_perplexity=262.66913, train_loss=5.570895

Batch 78390, train_perplexity=268.82278, train_loss=5.5940523

Batch 78400, train_perplexity=284.24255, train_loss=5.649828

Batch 78410, train_perplexity=250.36722, train_loss=5.5229287

Batch 78420, train_perplexity=269.05182, train_loss=5.594904

Batch 78430, train_perplexity=298.5735, train_loss=5.699016

Batch 78440, train_perplexity=267.03467, train_loss=5.5873785

Batch 78450, train_perplexity=305.38953, train_loss=5.721588

Batch 78460, train_perplexity=305.92545, train_loss=5.7233415

Batch 78470, train_perplexity=274.19397, train_loss=5.613836

Batch 78480, train_perplexity=273.8127, train_loss=5.6124444

Batch 78490, train_perplexity=302.0911, train_loss=5.7107286

Batch 78500, train_perplexity=282.68005, train_loss=5.6443157

Batch 78510, train_perplexity=276.56293, train_loss=5.6224384

Batch 78520, train_perplexity=279.2578, train_loss=5.6321354

Batch 78530, train_perplexity=267.87088, train_loss=5.590505

Batch 78540, train_perplexity=282.28027, train_loss=5.6429005

Batch 78550, train_perplexity=291.31958, train_loss=5.674421

Batch 78560, train_perplexity=278.38272, train_loss=5.628997

Batch 78570, train_perplexity=251.25665, train_loss=5.526475

Batch 78580, train_perplexity=306.89474, train_loss=5.726505

Batch 78590, train_perplexity=256.61743, train_loss=5.5475864

Batch 78600, train_perplexity=268.97342, train_loss=5.5946126

Batch 78610, train_perplexity=271.36197, train_loss=5.6034536

Batch 78620, train_perplexity=274.6428, train_loss=5.6154714

Batch 78630, train_perplexity=287.0548, train_loss=5.659673

Batch 78640, train_perplexity=261.22098, train_loss=5.5653667

Batch 78650, train_perplexity=263.97144, train_loss=5.575841
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78660, train_perplexity=322.38556, train_loss=5.7757483

Batch 78670, train_perplexity=294.19302, train_loss=5.684236

Batch 78680, train_perplexity=303.18005, train_loss=5.714327

Batch 78690, train_perplexity=312.0365, train_loss=5.74312

Batch 78700, train_perplexity=281.1535, train_loss=5.6389008

Batch 78710, train_perplexity=286.80786, train_loss=5.6588125

Batch 78720, train_perplexity=301.0665, train_loss=5.707331

Batch 78730, train_perplexity=298.98593, train_loss=5.7003965

Batch 78740, train_perplexity=285.00232, train_loss=5.6524973

Batch 78750, train_perplexity=265.93588, train_loss=5.5832553

Batch 78760, train_perplexity=286.35718, train_loss=5.65724

Batch 78770, train_perplexity=286.78955, train_loss=5.6587486

Batch 78780, train_perplexity=265.00714, train_loss=5.5797567

Batch 78790, train_perplexity=253.87326, train_loss=5.536835

Batch 78800, train_perplexity=304.72244, train_loss=5.7194014

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00085-of-00100
Loaded 305667 sentences.
Finished loading
Batch 78810, train_perplexity=287.78574, train_loss=5.662216

Batch 78820, train_perplexity=289.43137, train_loss=5.667918

Batch 78830, train_perplexity=271.07126, train_loss=5.6023817

Batch 78840, train_perplexity=282.74692, train_loss=5.644552

Batch 78850, train_perplexity=275.1678, train_loss=5.617381

Batch 78860, train_perplexity=323.59875, train_loss=5.7795043

Batch 78870, train_perplexity=271.82407, train_loss=5.605155

Batch 78880, train_perplexity=263.8051, train_loss=5.5752106

Batch 78890, train_perplexity=301.49908, train_loss=5.708767

Batch 78900, train_perplexity=294.32, train_loss=5.6846676

Batch 78910, train_perplexity=292.89587, train_loss=5.679817

Batch 78920, train_perplexity=249.7616, train_loss=5.520507

Batch 78930, train_perplexity=285.26093, train_loss=5.653404

Batch 78940, train_perplexity=264.81992, train_loss=5.57905

Batch 78950, train_perplexity=269.91583, train_loss=5.59811

Batch 78960, train_perplexity=274.5662, train_loss=5.6151924

Batch 78970, train_perplexity=285.8899, train_loss=5.6556067

Batch 78980, train_perplexity=257.7732, train_loss=5.55208

Batch 78990, train_perplexity=282.63773, train_loss=5.644166

Batch 79000, train_perplexity=281.3642, train_loss=5.63965

Batch 79010, train_perplexity=274.12103, train_loss=5.6135697

Batch 79020, train_perplexity=272.9072, train_loss=5.609132

Batch 79030, train_perplexity=261.46274, train_loss=5.566292

Batch 79040, train_perplexity=290.39188, train_loss=5.6712313

Batch 79050, train_perplexity=282.6833, train_loss=5.644327

Batch 79060, train_perplexity=287.46918, train_loss=5.6611156

Batch 79070, train_perplexity=259.30844, train_loss=5.558018

Batch 79080, train_perplexity=255.93678, train_loss=5.5449305

Batch 79090, train_perplexity=285.3847, train_loss=5.653838

Batch 79100, train_perplexity=262.05487, train_loss=5.568554

Batch 79110, train_perplexity=274.7692, train_loss=5.6159315

Batch 79120, train_perplexity=269.8237, train_loss=5.597769

Batch 79130, train_perplexity=265.5308, train_loss=5.581731

Batch 79140, train_perplexity=283.5319, train_loss=5.6473246

Batch 79150, train_perplexity=259.009, train_loss=5.556863

Batch 79160, train_perplexity=255.24818, train_loss=5.5422363

Batch 79170, train_perplexity=289.56943, train_loss=5.668395

Batch 79180, train_perplexity=278.98816, train_loss=5.6311693

Batch 79190, train_perplexity=256.88214, train_loss=5.5486174

Batch 79200, train_perplexity=240.33705, train_loss=5.4820423

Batch 79210, train_perplexity=286.9493, train_loss=5.6593056

Batch 79220, train_perplexity=276.5677, train_loss=5.6224556

Batch 79230, train_perplexity=276.04727, train_loss=5.620572

Batch 79240, train_perplexity=287.62097, train_loss=5.6616435

Batch 79250, train_perplexity=264.49384, train_loss=5.577818

Batch 79260, train_perplexity=255.46443, train_loss=5.543083

Batch 79270, train_perplexity=271.1351, train_loss=5.6026173

Batch 79280, train_perplexity=290.43173, train_loss=5.6713686

Batch 79290, train_perplexity=258.92307, train_loss=5.556531

Batch 79300, train_perplexity=304.46246, train_loss=5.718548

Batch 79310, train_perplexity=285.0026, train_loss=5.6524982

Batch 79320, train_perplexity=306.51743, train_loss=5.7252746

Batch 79330, train_perplexity=275.72232, train_loss=5.6193943

Batch 79340, train_perplexity=272.14517, train_loss=5.6063356

Batch 79350, train_perplexity=247.99466, train_loss=5.513407

Batch 79360, train_perplexity=240.61478, train_loss=5.483197

Batch 79370, train_perplexity=278.4252, train_loss=5.6291494

Batch 79380, train_perplexity=296.70676, train_loss=5.6927443

Batch 79390, train_perplexity=262.70032, train_loss=5.571014

Batch 79400, train_perplexity=278.41855, train_loss=5.6291256

Batch 79410, train_perplexity=296.52386, train_loss=5.6921277

Batch 79420, train_perplexity=310.46182, train_loss=5.738061

Batch 79430, train_perplexity=268.1376, train_loss=5.5915003

Batch 79440, train_perplexity=266.14, train_loss=5.5840225

Batch 79450, train_perplexity=281.98483, train_loss=5.6418533

Batch 79460, train_perplexity=294.5671, train_loss=5.685507

Batch 79470, train_perplexity=292.80093, train_loss=5.679493

Batch 79480, train_perplexity=260.90244, train_loss=5.5641465

Batch 79490, train_perplexity=284.5352, train_loss=5.650857

Batch 79500, train_perplexity=285.65918, train_loss=5.6547995

Batch 79510, train_perplexity=273.71286, train_loss=5.6120796

Batch 79520, train_perplexity=285.96338, train_loss=5.655864

Batch 79530, train_perplexity=279.69543, train_loss=5.6337013

Batch 79540, train_perplexity=293.26105, train_loss=5.681063

Batch 79550, train_perplexity=247.5718, train_loss=5.5117006

Batch 79560, train_perplexity=280.70526, train_loss=5.6373053

Batch 79570, train_perplexity=281.96494, train_loss=5.6417828

Batch 79580, train_perplexity=275.36075, train_loss=5.618082

Batch 79590, train_perplexity=278.48602, train_loss=5.629368

Batch 79600, train_perplexity=239.67934, train_loss=5.479302

Batch 79610, train_perplexity=295.7265, train_loss=5.689435

Batch 79620, train_perplexity=306.46173, train_loss=5.725093

Batch 79630, train_perplexity=284.1672, train_loss=5.649563

Batch 79640, train_perplexity=271.11237, train_loss=5.6025333

Batch 79650, train_perplexity=283.16263, train_loss=5.6460214

Batch 79660, train_perplexity=252.84674, train_loss=5.5327835

Batch 79670, train_perplexity=297.49896, train_loss=5.6954107

Batch 79680, train_perplexity=272.46146, train_loss=5.607497

Batch 79690, train_perplexity=276.48804, train_loss=5.6221676

Batch 79700, train_perplexity=281.9304, train_loss=5.64166

Batch 79710, train_perplexity=286.4096, train_loss=5.657423

Batch 79720, train_perplexity=286.07208, train_loss=5.656244

Batch 79730, train_perplexity=271.00314, train_loss=5.6021304

Batch 79740, train_perplexity=267.42078, train_loss=5.5888233

Batch 79750, train_perplexity=273.73245, train_loss=5.612151

Batch 79760, train_perplexity=291.24222, train_loss=5.674155

Batch 79770, train_perplexity=302.0554, train_loss=5.7106104

Batch 79780, train_perplexity=259.56674, train_loss=5.559014

Batch 79790, train_perplexity=310.89172, train_loss=5.7394447

Batch 79800, train_perplexity=282.5725, train_loss=5.643935

Batch 79810, train_perplexity=274.0323, train_loss=5.613246

Batch 79820, train_perplexity=266.89044, train_loss=5.5868382

Batch 79830, train_perplexity=311.30026, train_loss=5.740758

Batch 79840, train_perplexity=275.1014, train_loss=5.61714

Batch 79850, train_perplexity=301.5396, train_loss=5.7089014

Batch 79860, train_perplexity=244.64713, train_loss=5.499817

Batch 79870, train_perplexity=283.0183, train_loss=5.6455116

Batch 79880, train_perplexity=260.8901, train_loss=5.5640993

Batch 79890, train_perplexity=264.54364, train_loss=5.5780063

Batch 79900, train_perplexity=284.1557, train_loss=5.6495223

Batch 79910, train_perplexity=288.8545, train_loss=5.665923
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 79920, train_perplexity=300.677, train_loss=5.7060366

Batch 79930, train_perplexity=263.16837, train_loss=5.572794

Batch 79940, train_perplexity=280.7682, train_loss=5.6375294

Batch 79950, train_perplexity=279.1987, train_loss=5.6319237

Batch 79960, train_perplexity=285.43808, train_loss=5.654025

Batch 79970, train_perplexity=302.49124, train_loss=5.7120523

Batch 79980, train_perplexity=282.10724, train_loss=5.6422873

Batch 79990, train_perplexity=299.9677, train_loss=5.703675

Batch 80000, train_perplexity=261.09348, train_loss=5.5648785

Batch 80010, train_perplexity=262.88126, train_loss=5.5717025

Batch 80020, train_perplexity=271.1881, train_loss=5.602813

Batch 80030, train_perplexity=290.38715, train_loss=5.671215

Batch 80040, train_perplexity=291.7067, train_loss=5.675749

Batch 80050, train_perplexity=283.19867, train_loss=5.6461487

Batch 80060, train_perplexity=294.07983, train_loss=5.6838512

Batch 80070, train_perplexity=284.36118, train_loss=5.650245

Batch 80080, train_perplexity=246.35251, train_loss=5.5067635

Batch 80090, train_perplexity=275.8308, train_loss=5.6197877

Batch 80100, train_perplexity=256.962, train_loss=5.5489283

Batch 80110, train_perplexity=276.41922, train_loss=5.6219187

Batch 80120, train_perplexity=266.3241, train_loss=5.584714

Batch 80130, train_perplexity=301.88, train_loss=5.7100296

Batch 80140, train_perplexity=276.2337, train_loss=5.6212473

Batch 80150, train_perplexity=268.58932, train_loss=5.5931835

Batch 80160, train_perplexity=236.74098, train_loss=5.4669666

Batch 80170, train_perplexity=240.86778, train_loss=5.484248

Batch 80180, train_perplexity=276.6943, train_loss=5.6229134

Batch 80190, train_perplexity=252.74632, train_loss=5.5323863

Batch 80200, train_perplexity=261.51236, train_loss=5.5664816

Batch 80210, train_perplexity=296.31567, train_loss=5.6914253

Batch 80220, train_perplexity=292.87662, train_loss=5.6797514

Batch 80230, train_perplexity=270.81982, train_loss=5.601454

Batch 80240, train_perplexity=286.3132, train_loss=5.6570864

Batch 80250, train_perplexity=276.62097, train_loss=5.6226482

Batch 80260, train_perplexity=295.79715, train_loss=5.689674

Batch 80270, train_perplexity=295.2643, train_loss=5.687871

Batch 80280, train_perplexity=236.20966, train_loss=5.46472

Batch 80290, train_perplexity=291.51425, train_loss=5.675089

Batch 80300, train_perplexity=246.30833, train_loss=5.506584

Batch 80310, train_perplexity=250.4729, train_loss=5.5233507

Batch 80320, train_perplexity=271.59784, train_loss=5.6043224

Batch 80330, train_perplexity=289.7078, train_loss=5.668873

Batch 80340, train_perplexity=300.61505, train_loss=5.7058306

Batch 80350, train_perplexity=292.33594, train_loss=5.6779037

Batch 80360, train_perplexity=278.50754, train_loss=5.629445

Batch 80370, train_perplexity=276.27982, train_loss=5.621414

Batch 80380, train_perplexity=288.40707, train_loss=5.664373

Batch 80390, train_perplexity=276.32172, train_loss=5.621566

Batch 80400, train_perplexity=279.6377, train_loss=5.633495

Batch 80410, train_perplexity=277.3162, train_loss=5.6251583

Batch 80420, train_perplexity=296.01627, train_loss=5.6904144

Batch 80430, train_perplexity=285.02774, train_loss=5.6525865

Batch 80440, train_perplexity=278.1301, train_loss=5.628089

Batch 80450, train_perplexity=311.26077, train_loss=5.740631

Batch 80460, train_perplexity=266.37793, train_loss=5.584916

Batch 80470, train_perplexity=252.82455, train_loss=5.532696

Batch 80480, train_perplexity=278.1232, train_loss=5.628064

Batch 80490, train_perplexity=252.47916, train_loss=5.5313287

Batch 80500, train_perplexity=262.69144, train_loss=5.57098

Batch 80510, train_perplexity=282.40442, train_loss=5.64334

Batch 80520, train_perplexity=254.85924, train_loss=5.5407114

Batch 80530, train_perplexity=262.72025, train_loss=5.5710897

Batch 80540, train_perplexity=303.8148, train_loss=5.7164183

Batch 80550, train_perplexity=271.10474, train_loss=5.602505

Batch 80560, train_perplexity=251.45297, train_loss=5.527256

Batch 80570, train_perplexity=257.0788, train_loss=5.5493827

Batch 80580, train_perplexity=285.24893, train_loss=5.6533623

Batch 80590, train_perplexity=255.0596, train_loss=5.541497

Batch 80600, train_perplexity=280.517, train_loss=5.6366343

Batch 80610, train_perplexity=304.81342, train_loss=5.7197

Batch 80620, train_perplexity=298.6645, train_loss=5.699321

Batch 80630, train_perplexity=271.7664, train_loss=5.604943

Batch 80640, train_perplexity=275.842, train_loss=5.619828

Batch 80650, train_perplexity=257.95914, train_loss=5.552801

Batch 80660, train_perplexity=274.41882, train_loss=5.6146555

Batch 80670, train_perplexity=254.26955, train_loss=5.538395

Batch 80680, train_perplexity=312.38965, train_loss=5.7442513

Batch 80690, train_perplexity=285.0216, train_loss=5.652565

Batch 80700, train_perplexity=286.26953, train_loss=5.656934

Batch 80710, train_perplexity=255.96106, train_loss=5.5450253

Batch 80720, train_perplexity=258.78073, train_loss=5.555981

Batch 80730, train_perplexity=279.89557, train_loss=5.6344166

Batch 80740, train_perplexity=283.57108, train_loss=5.647463

Batch 80750, train_perplexity=311.78128, train_loss=5.742302

Batch 80760, train_perplexity=275.31454, train_loss=5.617914

Batch 80770, train_perplexity=263.01105, train_loss=5.572196

Batch 80780, train_perplexity=260.14154, train_loss=5.561226

Batch 80790, train_perplexity=270.1819, train_loss=5.5990953

Batch 80800, train_perplexity=273.15182, train_loss=5.610028

Batch 80810, train_perplexity=277.74786, train_loss=5.6267138

Batch 80820, train_perplexity=287.7112, train_loss=5.6619573

Batch 80830, train_perplexity=250.96078, train_loss=5.5252967

Batch 80840, train_perplexity=258.38678, train_loss=5.5544577

Batch 80850, train_perplexity=279.97272, train_loss=5.634692

Batch 80860, train_perplexity=259.64188, train_loss=5.5593033

Batch 80870, train_perplexity=258.8528, train_loss=5.5562596

Batch 80880, train_perplexity=283.3624, train_loss=5.6467266

Batch 80890, train_perplexity=278.8126, train_loss=5.63054

Batch 80900, train_perplexity=270.19553, train_loss=5.599146

Batch 80910, train_perplexity=293.7642, train_loss=5.6827774

Batch 80920, train_perplexity=261.4338, train_loss=5.566181

Batch 80930, train_perplexity=255.43788, train_loss=5.5429792

Batch 80940, train_perplexity=283.05045, train_loss=5.645625

Batch 80950, train_perplexity=315.09567, train_loss=5.7528763

Batch 80960, train_perplexity=261.5211, train_loss=5.566515

Batch 80970, train_perplexity=252.78525, train_loss=5.5325403

Batch 80980, train_perplexity=264.78558, train_loss=5.5789204

Batch 80990, train_perplexity=274.462, train_loss=5.614813

Batch 81000, train_perplexity=263.21805, train_loss=5.572983

Batch 81010, train_perplexity=241.35411, train_loss=5.486265

Batch 81020, train_perplexity=307.5569, train_loss=5.72866

Batch 81030, train_perplexity=259.00604, train_loss=5.5568514

Batch 81040, train_perplexity=295.7533, train_loss=5.6895256

Batch 81050, train_perplexity=272.57608, train_loss=5.607918

Batch 81060, train_perplexity=234.77547, train_loss=5.4586296

Batch 81070, train_perplexity=264.52322, train_loss=5.577929

Batch 81080, train_perplexity=308.325, train_loss=5.7311544

Batch 81090, train_perplexity=244.88815, train_loss=5.5008016

Batch 81100, train_perplexity=285.95096, train_loss=5.6558204

Batch 81110, train_perplexity=310.56815, train_loss=5.7384033

Batch 81120, train_perplexity=258.87292, train_loss=5.5563374

Batch 81130, train_perplexity=255.21739, train_loss=5.5421157

Batch 81140, train_perplexity=263.30542, train_loss=5.5733147

Batch 81150, train_perplexity=265.87555, train_loss=5.5830283

Batch 81160, train_perplexity=302.8795, train_loss=5.713335

Batch 81170, train_perplexity=294.41278, train_loss=5.684983

Batch 81180, train_perplexity=276.01752, train_loss=5.6204643

Batch 81190, train_perplexity=280.65173, train_loss=5.6371145

Batch 81200, train_perplexity=259.0374, train_loss=5.5569725

Batch 81210, train_perplexity=261.61438, train_loss=5.5668716

Batch 81220, train_perplexity=279.91266, train_loss=5.6344776

Batch 81230, train_perplexity=288.99863, train_loss=5.666422
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 81240, train_perplexity=278.71423, train_loss=5.630187

Batch 81250, train_perplexity=293.89703, train_loss=5.6832294

Batch 81260, train_perplexity=294.4104, train_loss=5.6849747

Batch 81270, train_perplexity=285.57938, train_loss=5.65452

Batch 81280, train_perplexity=293.27182, train_loss=5.6811

Batch 81290, train_perplexity=274.74512, train_loss=5.615844

Batch 81300, train_perplexity=267.9712, train_loss=5.5908794

Batch 81310, train_perplexity=256.92206, train_loss=5.548773

Batch 81320, train_perplexity=286.75864, train_loss=5.658641

Batch 81330, train_perplexity=281.80554, train_loss=5.641217

Batch 81340, train_perplexity=311.06775, train_loss=5.7400107

Batch 81350, train_perplexity=259.1454, train_loss=5.5573893

Batch 81360, train_perplexity=265.84854, train_loss=5.5829268

Batch 81370, train_perplexity=289.23892, train_loss=5.667253

Batch 81380, train_perplexity=281.61935, train_loss=5.6405563

Batch 81390, train_perplexity=259.092, train_loss=5.5571833

Batch 81400, train_perplexity=273.09256, train_loss=5.609811

Batch 81410, train_perplexity=235.88661, train_loss=5.4633512

Batch 81420, train_perplexity=249.9079, train_loss=5.5210924

Batch 81430, train_perplexity=284.9648, train_loss=5.6523657

Batch 81440, train_perplexity=262.12274, train_loss=5.568813

Batch 81450, train_perplexity=270.19644, train_loss=5.599149

Batch 81460, train_perplexity=300.6251, train_loss=5.705864

Batch 81470, train_perplexity=257.38705, train_loss=5.550581

Batch 81480, train_perplexity=331.27298, train_loss=5.8029428

Batch 81490, train_perplexity=270.34375, train_loss=5.5996943

Batch 81500, train_perplexity=264.13614, train_loss=5.5764647

Batch 81510, train_perplexity=288.90436, train_loss=5.6660957

Batch 81520, train_perplexity=250.63322, train_loss=5.5239906

Batch 81530, train_perplexity=283.23013, train_loss=5.64626

Batch 81540, train_perplexity=271.36884, train_loss=5.603479

Batch 81550, train_perplexity=274.63718, train_loss=5.615451

Batch 81560, train_perplexity=261.28253, train_loss=5.5656023

Batch 81570, train_perplexity=282.97864, train_loss=5.6453714

Batch 81580, train_perplexity=248.27425, train_loss=5.514534

Batch 81590, train_perplexity=293.77792, train_loss=5.682824

Batch 81600, train_perplexity=312.2303, train_loss=5.743741

Batch 81610, train_perplexity=265.0439, train_loss=5.5798955

Batch 81620, train_perplexity=284.55637, train_loss=5.6509314

Batch 81630, train_perplexity=277.1417, train_loss=5.624529

Batch 81640, train_perplexity=285.89056, train_loss=5.655609

Batch 81650, train_perplexity=245.67094, train_loss=5.503993

Batch 81660, train_perplexity=293.74124, train_loss=5.682699

Batch 81670, train_perplexity=275.32135, train_loss=5.617939

Batch 81680, train_perplexity=291.62518, train_loss=5.6754694

Batch 81690, train_perplexity=261.92084, train_loss=5.5680423

Batch 81700, train_perplexity=283.3913, train_loss=5.6468287

Batch 81710, train_perplexity=272.71442, train_loss=5.608425

Batch 81720, train_perplexity=271.91272, train_loss=5.605481

Batch 81730, train_perplexity=282.21243, train_loss=5.64266

Batch 81740, train_perplexity=267.7322, train_loss=5.5899873

Batch 81750, train_perplexity=276.5487, train_loss=5.622387

Batch 81760, train_perplexity=262.7844, train_loss=5.571334

Batch 81770, train_perplexity=279.51706, train_loss=5.6330633

Batch 81780, train_perplexity=238.61073, train_loss=5.4748335

Batch 81790, train_perplexity=265.39777, train_loss=5.5812297

Batch 81800, train_perplexity=285.07028, train_loss=5.6527357

Batch 81810, train_perplexity=279.37076, train_loss=5.6325397

Batch 81820, train_perplexity=279.4796, train_loss=5.6329293

Batch 81830, train_perplexity=282.83054, train_loss=5.644848

Batch 81840, train_perplexity=255.0967, train_loss=5.5416427

Batch 81850, train_perplexity=289.01282, train_loss=5.666471

Batch 81860, train_perplexity=274.08588, train_loss=5.6134415

Batch 81870, train_perplexity=329.56805, train_loss=5.797783

Batch 81880, train_perplexity=293.27545, train_loss=5.6811123

Batch 81890, train_perplexity=255.77939, train_loss=5.5443153

Batch 81900, train_perplexity=260.13077, train_loss=5.5611844

Batch 81910, train_perplexity=235.16818, train_loss=5.460301

Batch 81920, train_perplexity=284.77258, train_loss=5.651691

Batch 81930, train_perplexity=258.64764, train_loss=5.5554667

Batch 81940, train_perplexity=259.54593, train_loss=5.5589337

Batch 81950, train_perplexity=282.11047, train_loss=5.6422987

Batch 81960, train_perplexity=279.53598, train_loss=5.633131

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00078-of-00100
Loaded 306740 sentences.
Finished loading
Batch 81970, train_perplexity=253.25288, train_loss=5.5343885

Batch 81980, train_perplexity=281.65775, train_loss=5.6406927

Batch 81990, train_perplexity=304.93118, train_loss=5.720086

Batch 82000, train_perplexity=265.22382, train_loss=5.580574

Batch 82010, train_perplexity=277.9962, train_loss=5.6276073

Batch 82020, train_perplexity=291.15918, train_loss=5.67387

Batch 82030, train_perplexity=282.52844, train_loss=5.6437793

Batch 82040, train_perplexity=278.28345, train_loss=5.62864

Batch 82050, train_perplexity=272.67047, train_loss=5.608264

Batch 82060, train_perplexity=271.51703, train_loss=5.604025

Batch 82070, train_perplexity=261.28152, train_loss=5.5655985

Batch 82080, train_perplexity=267.7584, train_loss=5.590085

Batch 82090, train_perplexity=297.6925, train_loss=5.696061

Batch 82100, train_perplexity=266.25906, train_loss=5.58447

Batch 82110, train_perplexity=252.31873, train_loss=5.530693

Batch 82120, train_perplexity=291.25415, train_loss=5.6741962

Batch 82130, train_perplexity=292.0878, train_loss=5.6770544

Batch 82140, train_perplexity=280.06058, train_loss=5.635006

Batch 82150, train_perplexity=288.88205, train_loss=5.6660185

Batch 82160, train_perplexity=263.2133, train_loss=5.5729647

Batch 82170, train_perplexity=259.33502, train_loss=5.5581207

Batch 82180, train_perplexity=286.89224, train_loss=5.6591067

Batch 82190, train_perplexity=258.90466, train_loss=5.55646

Batch 82200, train_perplexity=267.94537, train_loss=5.590783

Batch 82210, train_perplexity=252.32281, train_loss=5.5307093

Batch 82220, train_perplexity=291.82272, train_loss=5.6761465

Batch 82230, train_perplexity=259.4973, train_loss=5.5587463

Batch 82240, train_perplexity=265.74716, train_loss=5.5825453

Batch 82250, train_perplexity=249.0294, train_loss=5.517571

Batch 82260, train_perplexity=281.62244, train_loss=5.6405673

Batch 82270, train_perplexity=281.5761, train_loss=5.640403

Batch 82280, train_perplexity=283.96918, train_loss=5.6488657

Batch 82290, train_perplexity=298.91907, train_loss=5.700173

Batch 82300, train_perplexity=292.16565, train_loss=5.677321

Batch 82310, train_perplexity=266.073, train_loss=5.5837708

Batch 82320, train_perplexity=253.31195, train_loss=5.5346217

Batch 82330, train_perplexity=256.47406, train_loss=5.5470276

Batch 82340, train_perplexity=287.04907, train_loss=5.659653

Batch 82350, train_perplexity=282.53992, train_loss=5.64382

Batch 82360, train_perplexity=287.23843, train_loss=5.6603127

Batch 82370, train_perplexity=280.29626, train_loss=5.635847

Batch 82380, train_perplexity=285.6412, train_loss=5.6547365

Batch 82390, train_perplexity=249.9105, train_loss=5.521103

Batch 82400, train_perplexity=253.09499, train_loss=5.533765

Batch 82410, train_perplexity=286.34708, train_loss=5.6572046

Batch 82420, train_perplexity=247.91734, train_loss=5.5130954

Batch 82430, train_perplexity=267.4228, train_loss=5.588831

Batch 82440, train_perplexity=251.73563, train_loss=5.5283794

Batch 82450, train_perplexity=302.89597, train_loss=5.7133894

Batch 82460, train_perplexity=266.5712, train_loss=5.5856414

Batch 82470, train_perplexity=267.56464, train_loss=5.589361

Batch 82480, train_perplexity=266.58188, train_loss=5.5856814

Batch 82490, train_perplexity=304.15485, train_loss=5.717537
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82500, train_perplexity=281.54227, train_loss=5.6402826

Batch 82510, train_perplexity=274.29926, train_loss=5.6142197

Batch 82520, train_perplexity=305.90533, train_loss=5.7232757

Batch 82530, train_perplexity=271.04178, train_loss=5.602273

Batch 82540, train_perplexity=278.86075, train_loss=5.6307125

Batch 82550, train_perplexity=276.4316, train_loss=5.6219635

Batch 82560, train_perplexity=281.22604, train_loss=5.6391587

Batch 82570, train_perplexity=275.39435, train_loss=5.618204

Batch 82580, train_perplexity=259.78702, train_loss=5.559862

Batch 82590, train_perplexity=262.61026, train_loss=5.570671

Batch 82600, train_perplexity=259.57156, train_loss=5.5590324

Batch 82610, train_perplexity=301.1291, train_loss=5.707539

Batch 82620, train_perplexity=263.8514, train_loss=5.575386

Batch 82630, train_perplexity=277.30945, train_loss=5.625134

Batch 82640, train_perplexity=264.76486, train_loss=5.578842

Batch 82650, train_perplexity=248.48247, train_loss=5.5153723

Batch 82660, train_perplexity=300.84967, train_loss=5.7066107

Batch 82670, train_perplexity=301.06924, train_loss=5.7073402

Batch 82680, train_perplexity=283.3797, train_loss=5.6467876

Batch 82690, train_perplexity=267.53708, train_loss=5.589258

Batch 82700, train_perplexity=262.08862, train_loss=5.5686827

Batch 82710, train_perplexity=274.61832, train_loss=5.615382

Batch 82720, train_perplexity=298.86407, train_loss=5.699989

Batch 82730, train_perplexity=266.8758, train_loss=5.5867834

Batch 82740, train_perplexity=278.20834, train_loss=5.6283703

Batch 82750, train_perplexity=276.25388, train_loss=5.6213202

Batch 82760, train_perplexity=315.65628, train_loss=5.754654

Batch 82770, train_perplexity=238.87689, train_loss=5.4759483

Batch 82780, train_perplexity=239.82841, train_loss=5.4799237

Batch 82790, train_perplexity=293.75818, train_loss=5.682757

Batch 82800, train_perplexity=297.6488, train_loss=5.6959143

Batch 82810, train_perplexity=279.07172, train_loss=5.631469

Batch 82820, train_perplexity=274.72913, train_loss=5.6157856

Batch 82830, train_perplexity=247.22026, train_loss=5.5102797

Batch 82840, train_perplexity=262.56796, train_loss=5.57051

Batch 82850, train_perplexity=289.1999, train_loss=5.667118

Batch 82860, train_perplexity=281.76953, train_loss=5.6410894

Batch 82870, train_perplexity=284.732, train_loss=5.6515484

Batch 82880, train_perplexity=259.71393, train_loss=5.559581

Batch 82890, train_perplexity=306.86942, train_loss=5.7264223

Batch 82900, train_perplexity=265.086, train_loss=5.5800543

Batch 82910, train_perplexity=284.66086, train_loss=5.6512985

Batch 82920, train_perplexity=261.25275, train_loss=5.5654883

Batch 82930, train_perplexity=255.87332, train_loss=5.5446825

Batch 82940, train_perplexity=277.9001, train_loss=5.6272616

Batch 82950, train_perplexity=296.09674, train_loss=5.690686

Batch 82960, train_perplexity=284.17587, train_loss=5.6495934

Batch 82970, train_perplexity=283.14682, train_loss=5.6459656

Batch 82980, train_perplexity=258.5665, train_loss=5.555153

Batch 82990, train_perplexity=288.09824, train_loss=5.6633015

Batch 83000, train_perplexity=290.5175, train_loss=5.6716638

Batch 83010, train_perplexity=257.65033, train_loss=5.5516033

Batch 83020, train_perplexity=272.46005, train_loss=5.607492

Batch 83030, train_perplexity=278.08395, train_loss=5.627923

Batch 83040, train_perplexity=280.30722, train_loss=5.635886

Batch 83050, train_perplexity=262.79968, train_loss=5.571392

Batch 83060, train_perplexity=280.87155, train_loss=5.6378975

Batch 83070, train_perplexity=286.85736, train_loss=5.658985

Batch 83080, train_perplexity=266.54413, train_loss=5.58554

Batch 83090, train_perplexity=256.44302, train_loss=5.5469065

Batch 83100, train_perplexity=280.54684, train_loss=5.6367407

Batch 83110, train_perplexity=304.21606, train_loss=5.717738

Batch 83120, train_perplexity=287.4027, train_loss=5.6608844

Batch 83130, train_perplexity=248.90083, train_loss=5.5170546

Batch 83140, train_perplexity=259.72955, train_loss=5.559641

Batch 83150, train_perplexity=263.8675, train_loss=5.575447

Batch 83160, train_perplexity=285.08957, train_loss=5.6528034

Batch 83170, train_perplexity=257.402, train_loss=5.550639

Batch 83180, train_perplexity=237.89001, train_loss=5.4718084

Batch 83190, train_perplexity=262.91864, train_loss=5.5718446

Batch 83200, train_perplexity=323.6367, train_loss=5.7796216

Batch 83210, train_perplexity=268.83917, train_loss=5.5941133

Batch 83220, train_perplexity=264.7203, train_loss=5.578674

Batch 83230, train_perplexity=328.20807, train_loss=5.793648

Batch 83240, train_perplexity=247.13504, train_loss=5.509935

Batch 83250, train_perplexity=285.93094, train_loss=5.6557503

Batch 83260, train_perplexity=299.43292, train_loss=5.7018905

Batch 83270, train_perplexity=244.56898, train_loss=5.4994974

Batch 83280, train_perplexity=275.31625, train_loss=5.6179204

Batch 83290, train_perplexity=261.72046, train_loss=5.567277

Batch 83300, train_perplexity=336.77823, train_loss=5.8194246

Batch 83310, train_perplexity=247.82019, train_loss=5.5127034

Batch 83320, train_perplexity=297.00684, train_loss=5.693755

Batch 83330, train_perplexity=275.3794, train_loss=5.6181498

Batch 83340, train_perplexity=288.93536, train_loss=5.666203

Batch 83350, train_perplexity=283.5516, train_loss=5.647394

Batch 83360, train_perplexity=271.91818, train_loss=5.605501

Batch 83370, train_perplexity=295.832, train_loss=5.6897917

Batch 83380, train_perplexity=251.0612, train_loss=5.5256968

Batch 83390, train_perplexity=267.85724, train_loss=5.590454

Batch 83400, train_perplexity=257.35416, train_loss=5.550453

Batch 83410, train_perplexity=284.8094, train_loss=5.65182

Batch 83420, train_perplexity=266.87643, train_loss=5.586786

Batch 83430, train_perplexity=272.91656, train_loss=5.609166

Batch 83440, train_perplexity=264.79947, train_loss=5.578973

Batch 83450, train_perplexity=266.82645, train_loss=5.5865984

Batch 83460, train_perplexity=268.30157, train_loss=5.5921116

Batch 83470, train_perplexity=301.67136, train_loss=5.709338

Batch 83480, train_perplexity=248.71931, train_loss=5.516325

Batch 83490, train_perplexity=248.36449, train_loss=5.5148973

Batch 83500, train_perplexity=273.04987, train_loss=5.6096544

Batch 83510, train_perplexity=269.0989, train_loss=5.595079

Batch 83520, train_perplexity=278.9235, train_loss=5.6309376

Batch 83530, train_perplexity=276.7228, train_loss=5.6230164

Batch 83540, train_perplexity=288.07486, train_loss=5.6632204

Batch 83550, train_perplexity=274.59763, train_loss=5.615307

Batch 83560, train_perplexity=292.6736, train_loss=5.679058

Batch 83570, train_perplexity=299.87402, train_loss=5.7033625

Batch 83580, train_perplexity=281.00995, train_loss=5.63839

Batch 83590, train_perplexity=249.61278, train_loss=5.519911

Batch 83600, train_perplexity=274.35785, train_loss=5.6144333

Batch 83610, train_perplexity=262.14874, train_loss=5.568912

Batch 83620, train_perplexity=285.18774, train_loss=5.6531477

Batch 83630, train_perplexity=269.88315, train_loss=5.597989

Batch 83640, train_perplexity=276.4307, train_loss=5.62196

Batch 83650, train_perplexity=305.01086, train_loss=5.7203474

Batch 83660, train_perplexity=249.60623, train_loss=5.5198846

Batch 83670, train_perplexity=298.9332, train_loss=5.70022

Batch 83680, train_perplexity=278.00015, train_loss=5.6276217

Batch 83690, train_perplexity=266.38544, train_loss=5.5849442

Batch 83700, train_perplexity=281.6681, train_loss=5.6407294

Batch 83710, train_perplexity=274.69662, train_loss=5.6156673

Batch 83720, train_perplexity=295.63174, train_loss=5.6891146

Batch 83730, train_perplexity=253.00195, train_loss=5.533397

Batch 83740, train_perplexity=277.19998, train_loss=5.624739

Batch 83750, train_perplexity=282.4921, train_loss=5.6436505

Batch 83760, train_perplexity=263.72562, train_loss=5.574909

Batch 83770, train_perplexity=253.00063, train_loss=5.533392

Batch 83780, train_perplexity=239.73409, train_loss=5.4795303

Batch 83790, train_perplexity=274.58612, train_loss=5.615265

Batch 83800, train_perplexity=266.10422, train_loss=5.583888

Batch 83810, train_perplexity=264.47516, train_loss=5.5777473
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 83820, train_perplexity=254.29163, train_loss=5.5384817

Batch 83830, train_perplexity=290.82144, train_loss=5.6727095

Batch 83840, train_perplexity=262.68042, train_loss=5.570938

Batch 83850, train_perplexity=293.45395, train_loss=5.6817207

Batch 83860, train_perplexity=272.63925, train_loss=5.6081495

Batch 83870, train_perplexity=260.6673, train_loss=5.563245

Batch 83880, train_perplexity=269.59363, train_loss=5.5969157

Batch 83890, train_perplexity=288.18808, train_loss=5.6636133

Batch 83900, train_perplexity=274.17255, train_loss=5.6137576

Batch 83910, train_perplexity=288.71268, train_loss=5.665432

Batch 83920, train_perplexity=255.5664, train_loss=5.5434823

Batch 83930, train_perplexity=278.5649, train_loss=5.629651

Batch 83940, train_perplexity=255.20876, train_loss=5.542082

Batch 83950, train_perplexity=285.60635, train_loss=5.6546144

Batch 83960, train_perplexity=279.2791, train_loss=5.6322117

Batch 83970, train_perplexity=244.45659, train_loss=5.4990377

Batch 83980, train_perplexity=285.9151, train_loss=5.655695

Batch 83990, train_perplexity=274.49106, train_loss=5.6149187

Batch 84000, train_perplexity=269.7352, train_loss=5.5974407

Batch 84010, train_perplexity=262.9862, train_loss=5.5721016

Batch 84020, train_perplexity=256.36108, train_loss=5.546587

Batch 84030, train_perplexity=315.67993, train_loss=5.754729

Batch 84040, train_perplexity=295.87488, train_loss=5.6899366

Batch 84050, train_perplexity=283.8277, train_loss=5.6483674

Batch 84060, train_perplexity=274.5103, train_loss=5.614989

Batch 84070, train_perplexity=250.6263, train_loss=5.523963

Batch 84080, train_perplexity=273.96826, train_loss=5.6130123

Batch 84090, train_perplexity=260.9155, train_loss=5.5641966

Batch 84100, train_perplexity=249.10861, train_loss=5.517889

Batch 84110, train_perplexity=278.7363, train_loss=5.630266

Batch 84120, train_perplexity=257.3122, train_loss=5.55029

Batch 84130, train_perplexity=280.60837, train_loss=5.63696

Batch 84140, train_perplexity=278.97644, train_loss=5.6311274

Batch 84150, train_perplexity=264.87146, train_loss=5.5792446

Batch 84160, train_perplexity=244.79918, train_loss=5.500438

Batch 84170, train_perplexity=282.24017, train_loss=5.6427584

Batch 84180, train_perplexity=246.67859, train_loss=5.508086

Batch 84190, train_perplexity=271.8448, train_loss=5.6052313

Batch 84200, train_perplexity=283.15573, train_loss=5.645997

Batch 84210, train_perplexity=262.2444, train_loss=5.569277

Batch 84220, train_perplexity=298.4273, train_loss=5.6985264

Batch 84230, train_perplexity=289.5755, train_loss=5.668416

Batch 84240, train_perplexity=279.54904, train_loss=5.6331778

Batch 84250, train_perplexity=243.99857, train_loss=5.4971623

Batch 84260, train_perplexity=267.96454, train_loss=5.5908546

Batch 84270, train_perplexity=274.92673, train_loss=5.6165047

Batch 84280, train_perplexity=278.0704, train_loss=5.6278744

Batch 84290, train_perplexity=262.24988, train_loss=5.569298

Batch 84300, train_perplexity=290.112, train_loss=5.670267

Batch 84310, train_perplexity=267.07364, train_loss=5.5875244

Batch 84320, train_perplexity=296.32852, train_loss=5.6914687

Batch 84330, train_perplexity=290.2443, train_loss=5.670723

Batch 84340, train_perplexity=264.99982, train_loss=5.579729

Batch 84350, train_perplexity=250.27663, train_loss=5.522567

Batch 84360, train_perplexity=285.00204, train_loss=5.6524963

Batch 84370, train_perplexity=275.04895, train_loss=5.616949

Batch 84380, train_perplexity=291.23608, train_loss=5.6741343

Batch 84390, train_perplexity=277.2001, train_loss=5.6247396

Batch 84400, train_perplexity=274.34607, train_loss=5.6143904

Batch 84410, train_perplexity=285.77444, train_loss=5.655203

Batch 84420, train_perplexity=281.08487, train_loss=5.6386566

Batch 84430, train_perplexity=285.1543, train_loss=5.6530304

Batch 84440, train_perplexity=281.301, train_loss=5.6394253

Batch 84450, train_perplexity=287.53745, train_loss=5.661353

Batch 84460, train_perplexity=308.36588, train_loss=5.731287

Batch 84470, train_perplexity=290.11893, train_loss=5.670291

Batch 84480, train_perplexity=277.15887, train_loss=5.624591

Batch 84490, train_perplexity=259.89804, train_loss=5.5602894

Batch 84500, train_perplexity=289.24704, train_loss=5.667281

Batch 84510, train_perplexity=290.05725, train_loss=5.6700783

Batch 84520, train_perplexity=258.26572, train_loss=5.553989

Batch 84530, train_perplexity=267.57703, train_loss=5.5894074

Batch 84540, train_perplexity=272.52762, train_loss=5.60774

Batch 84550, train_perplexity=263.65393, train_loss=5.5746374

Batch 84560, train_perplexity=285.69937, train_loss=5.65494

Batch 84570, train_perplexity=249.44072, train_loss=5.5192213

Batch 84580, train_perplexity=269.65726, train_loss=5.5971518

Batch 84590, train_perplexity=254.09222, train_loss=5.5376973

Batch 84600, train_perplexity=304.2484, train_loss=5.7178445

Batch 84610, train_perplexity=277.9246, train_loss=5.62735

Batch 84620, train_perplexity=279.58276, train_loss=5.6332984

Batch 84630, train_perplexity=272.1732, train_loss=5.6064386

Batch 84640, train_perplexity=274.50757, train_loss=5.614979

Batch 84650, train_perplexity=267.87064, train_loss=5.590504

Batch 84660, train_perplexity=277.89172, train_loss=5.6272316

Batch 84670, train_perplexity=239.09329, train_loss=5.476854

Batch 84680, train_perplexity=273.96985, train_loss=5.613018

Batch 84690, train_perplexity=267.5297, train_loss=5.5892305

Batch 84700, train_perplexity=272.0925, train_loss=5.606142

Batch 84710, train_perplexity=282.1196, train_loss=5.642331

Batch 84720, train_perplexity=250.46251, train_loss=5.523309

Batch 84730, train_perplexity=264.81525, train_loss=5.5790324

Batch 84740, train_perplexity=279.73306, train_loss=5.633836

Batch 84750, train_perplexity=297.95126, train_loss=5.69693

Batch 84760, train_perplexity=274.96884, train_loss=5.6166577

Batch 84770, train_perplexity=283.79184, train_loss=5.648241

Batch 84780, train_perplexity=280.59552, train_loss=5.6369143

Batch 84790, train_perplexity=294.31186, train_loss=5.68464

Batch 84800, train_perplexity=257.04364, train_loss=5.549246

Batch 84810, train_perplexity=270.2347, train_loss=5.599291

Batch 84820, train_perplexity=243.08707, train_loss=5.4934196

Batch 84830, train_perplexity=275.27753, train_loss=5.6177797

Batch 84840, train_perplexity=290.81616, train_loss=5.6726913

Batch 84850, train_perplexity=275.99066, train_loss=5.620367

Batch 84860, train_perplexity=254.81549, train_loss=5.5405397

Batch 84870, train_perplexity=283.08554, train_loss=5.645749

Batch 84880, train_perplexity=258.16205, train_loss=5.5535874

Batch 84890, train_perplexity=237.25438, train_loss=5.469133

Batch 84900, train_perplexity=271.96796, train_loss=5.6056843

Batch 84910, train_perplexity=301.8872, train_loss=5.7100534

Batch 84920, train_perplexity=312.71722, train_loss=5.7452993

Batch 84930, train_perplexity=247.62552, train_loss=5.5119176

Batch 84940, train_perplexity=293.04312, train_loss=5.68032

Batch 84950, train_perplexity=284.4161, train_loss=5.6504383

Batch 84960, train_perplexity=234.57426, train_loss=5.4577723

Batch 84970, train_perplexity=279.20242, train_loss=5.631937

Batch 84980, train_perplexity=290.79147, train_loss=5.6726065

Batch 84990, train_perplexity=274.5031, train_loss=5.6149626

Batch 85000, train_perplexity=249.39767, train_loss=5.5190487

Batch 85010, train_perplexity=284.0293, train_loss=5.6490774

Batch 85020, train_perplexity=244.21867, train_loss=5.498064

Batch 85030, train_perplexity=289.5149, train_loss=5.6682067

Batch 85040, train_perplexity=236.37144, train_loss=5.4654045

Batch 85050, train_perplexity=236.21935, train_loss=5.464761

Batch 85060, train_perplexity=271.94293, train_loss=5.6055923

Batch 85070, train_perplexity=255.58688, train_loss=5.5435624

Batch 85080, train_perplexity=245.83078, train_loss=5.5046434

Batch 85090, train_perplexity=281.31226, train_loss=5.6394653

Batch 85100, train_perplexity=239.04166, train_loss=5.476638

Batch 85110, train_perplexity=268.0017, train_loss=5.5909934

Batch 85120, train_perplexity=275.51022, train_loss=5.6186247

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305615 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Batch 85130, train_perplexity=260.6986, train_loss=5.563365

Batch 85140, train_perplexity=284.05286, train_loss=5.6491604

Batch 85150, train_perplexity=257.626, train_loss=5.551509

Batch 85160, train_perplexity=252.96274, train_loss=5.533242

Batch 85170, train_perplexity=262.35944, train_loss=5.5697155

Batch 85180, train_perplexity=281.1665, train_loss=5.638947

Batch 85190, train_perplexity=309.38773, train_loss=5.7345953

Batch 85200, train_perplexity=262.80884, train_loss=5.571427

Batch 85210, train_perplexity=261.8694, train_loss=5.567846

Batch 85220, train_perplexity=264.14093, train_loss=5.576483

Batch 85230, train_perplexity=283.3732, train_loss=5.6467648

Batch 85240, train_perplexity=270.52222, train_loss=5.600354

Batch 85250, train_perplexity=275.15704, train_loss=5.617342

Batch 85260, train_perplexity=242.1202, train_loss=5.4894342

Batch 85270, train_perplexity=281.56995, train_loss=5.640381

Batch 85280, train_perplexity=262.14136, train_loss=5.568884

Batch 85290, train_perplexity=246.36049, train_loss=5.506796

Batch 85300, train_perplexity=277.2588, train_loss=5.6249514

Batch 85310, train_perplexity=276.52814, train_loss=5.6223125

Batch 85320, train_perplexity=294.07843, train_loss=5.6838465

Batch 85330, train_perplexity=296.96606, train_loss=5.693618

Batch 85340, train_perplexity=273.2696, train_loss=5.610459

Batch 85350, train_perplexity=273.41754, train_loss=5.611

Batch 85360, train_perplexity=275.83304, train_loss=5.619796

Batch 85370, train_perplexity=264.87094, train_loss=5.5792427

Batch 85380, train_perplexity=291.80157, train_loss=5.676074

Batch 85390, train_perplexity=284.7867, train_loss=5.6517406

Batch 85400, train_perplexity=271.9144, train_loss=5.6054873

Batch 85410, train_perplexity=240.91246, train_loss=5.4844337

Batch 85420, train_perplexity=292.2597, train_loss=5.677643

Batch 85430, train_perplexity=261.4717, train_loss=5.566326

Batch 85440, train_perplexity=264.50897, train_loss=5.577875

Batch 85450, train_perplexity=289.23587, train_loss=5.6672425

Batch 85460, train_perplexity=277.24927, train_loss=5.624917

Batch 85470, train_perplexity=281.3399, train_loss=5.6395636

Batch 85480, train_perplexity=276.59012, train_loss=5.6225367

Batch 85490, train_perplexity=263.54608, train_loss=5.5742283

Batch 85500, train_perplexity=253.85596, train_loss=5.536767

Batch 85510, train_perplexity=261.00784, train_loss=5.5645504

Batch 85520, train_perplexity=259.75507, train_loss=5.559739

Batch 85530, train_perplexity=303.06152, train_loss=5.713936

Batch 85540, train_perplexity=255.58371, train_loss=5.54355

Batch 85550, train_perplexity=256.58148, train_loss=5.5474463

Batch 85560, train_perplexity=259.63495, train_loss=5.5592766

Batch 85570, train_perplexity=242.75577, train_loss=5.492056

Batch 85580, train_perplexity=273.23975, train_loss=5.6103497

Batch 85590, train_perplexity=251.46857, train_loss=5.527318

Batch 85600, train_perplexity=273.06183, train_loss=5.6096983

Batch 85610, train_perplexity=277.95508, train_loss=5.6274595

Batch 85620, train_perplexity=279.38593, train_loss=5.632594

Batch 85630, train_perplexity=277.12067, train_loss=5.624453

Batch 85640, train_perplexity=291.06088, train_loss=5.6735325

Batch 85650, train_perplexity=272.20395, train_loss=5.6065516

Batch 85660, train_perplexity=261.70746, train_loss=5.5672274

Batch 85670, train_perplexity=295.88107, train_loss=5.6899576

Batch 85680, train_perplexity=251.09053, train_loss=5.5258136

Batch 85690, train_perplexity=294.98315, train_loss=5.6869183

Batch 85700, train_perplexity=260.88016, train_loss=5.564061

Batch 85710, train_perplexity=289.6879, train_loss=5.668804

Batch 85720, train_perplexity=323.91614, train_loss=5.7804847

Batch 85730, train_perplexity=250.09291, train_loss=5.5218325

Batch 85740, train_perplexity=271.58386, train_loss=5.604271

Batch 85750, train_perplexity=229.90878, train_loss=5.4376826

Batch 85760, train_perplexity=267.3119, train_loss=5.588416

Batch 85770, train_perplexity=262.94006, train_loss=5.571926

Batch 85780, train_perplexity=270.84515, train_loss=5.6015472

Batch 85790, train_perplexity=242.6699, train_loss=5.491702

Batch 85800, train_perplexity=294.3287, train_loss=5.684697

Batch 85810, train_perplexity=259.88416, train_loss=5.560236

Batch 85820, train_perplexity=250.33391, train_loss=5.5227957

Batch 85830, train_perplexity=266.96005, train_loss=5.587099

Batch 85840, train_perplexity=268.1082, train_loss=5.5913906

Batch 85850, train_perplexity=220.53989, train_loss=5.3960786

Batch 85860, train_perplexity=268.5021, train_loss=5.592859

Batch 85870, train_perplexity=284.6849, train_loss=5.651383

Batch 85880, train_perplexity=254.90579, train_loss=5.540894

Batch 85890, train_perplexity=245.92528, train_loss=5.505028

Batch 85900, train_perplexity=282.75934, train_loss=5.644596

Batch 85910, train_perplexity=235.85017, train_loss=5.4631968

Batch 85920, train_perplexity=271.66144, train_loss=5.6045566

Batch 85930, train_perplexity=265.92993, train_loss=5.583233

Batch 85940, train_perplexity=269.73544, train_loss=5.5974417

Batch 85950, train_perplexity=252.17223, train_loss=5.5301123

Batch 85960, train_perplexity=253.54723, train_loss=5.53555

Batch 85970, train_perplexity=302.21243, train_loss=5.71113

Batch 85980, train_perplexity=238.27963, train_loss=5.473445

Batch 85990, train_perplexity=276.49518, train_loss=5.6221933

Batch 86000, train_perplexity=276.61835, train_loss=5.6226387

Batch 86010, train_perplexity=257.14096, train_loss=5.5496244

Batch 86020, train_perplexity=236.80702, train_loss=5.4672456

Batch 86030, train_perplexity=250.43492, train_loss=5.523199

Batch 86040, train_perplexity=264.54227, train_loss=5.578001

Batch 86050, train_perplexity=284.75778, train_loss=5.651639

Batch 86060, train_perplexity=299.36282, train_loss=5.7016563

Batch 86070, train_perplexity=248.53377, train_loss=5.5155787

Batch 86080, train_perplexity=263.74496, train_loss=5.5749826

Batch 86090, train_perplexity=265.4775, train_loss=5.58153

Batch 86100, train_perplexity=240.02646, train_loss=5.480749

Batch 86110, train_perplexity=243.42645, train_loss=5.494815

Batch 86120, train_perplexity=262.14435, train_loss=5.5688953

Batch 86130, train_perplexity=264.42447, train_loss=5.5775557

Batch 86140, train_perplexity=274.4412, train_loss=5.614737

Batch 86150, train_perplexity=275.59064, train_loss=5.6189165

Batch 86160, train_perplexity=262.5429, train_loss=5.5704145

Batch 86170, train_perplexity=279.02222, train_loss=5.6312914

Batch 86180, train_perplexity=263.92563, train_loss=5.5756674

Batch 86190, train_perplexity=247.56802, train_loss=5.5116854

Batch 86200, train_perplexity=272.11285, train_loss=5.606217

Batch 86210, train_perplexity=300.6813, train_loss=5.706051

Batch 86220, train_perplexity=258.81543, train_loss=5.556115

Batch 86230, train_perplexity=267.33612, train_loss=5.5885067

Batch 86240, train_perplexity=285.29028, train_loss=5.653507

Batch 86250, train_perplexity=277.76083, train_loss=5.6267605

Batch 86260, train_perplexity=264.15527, train_loss=5.576537

Batch 86270, train_perplexity=292.49826, train_loss=5.6784587

Batch 86280, train_perplexity=265.45206, train_loss=5.5814342

Batch 86290, train_perplexity=259.36755, train_loss=5.558246

Batch 86300, train_perplexity=299.08517, train_loss=5.7007284

Batch 86310, train_perplexity=263.5765, train_loss=5.5743437

Batch 86320, train_perplexity=248.66974, train_loss=5.5161257

Batch 86330, train_perplexity=276.0986, train_loss=5.620758

Batch 86340, train_perplexity=293.41394, train_loss=5.6815844

Batch 86350, train_perplexity=254.43765, train_loss=5.539056

Batch 86360, train_perplexity=285.9278, train_loss=5.6557393

Batch 86370, train_perplexity=265.2906, train_loss=5.580826

Batch 86380, train_perplexity=300.19565, train_loss=5.7044344

Batch 86390, train_perplexity=265.87592, train_loss=5.5830297

Batch 86400, train_perplexity=228.43932, train_loss=5.4312706
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86410, train_perplexity=260.21597, train_loss=5.561512

Batch 86420, train_perplexity=294.5865, train_loss=5.6855726

Batch 86430, train_perplexity=241.93703, train_loss=5.4886775

Batch 86440, train_perplexity=279.77386, train_loss=5.6339817

Batch 86450, train_perplexity=287.02225, train_loss=5.6595597

Batch 86460, train_perplexity=274.16272, train_loss=5.613722

Batch 86470, train_perplexity=279.11963, train_loss=5.6316404

Batch 86480, train_perplexity=294.33627, train_loss=5.684723

Batch 86490, train_perplexity=278.93362, train_loss=5.630974

Batch 86500, train_perplexity=262.52463, train_loss=5.570345

Batch 86510, train_perplexity=272.6715, train_loss=5.608268

Batch 86520, train_perplexity=308.60168, train_loss=5.7320514

Batch 86530, train_perplexity=277.38165, train_loss=5.6253943

Batch 86540, train_perplexity=277.4138, train_loss=5.62551

Batch 86550, train_perplexity=235.72066, train_loss=5.4626474

Batch 86560, train_perplexity=260.49704, train_loss=5.5625916

Batch 86570, train_perplexity=261.52322, train_loss=5.566523

Batch 86580, train_perplexity=293.65356, train_loss=5.6824007

Batch 86590, train_perplexity=269.45596, train_loss=5.596405

Batch 86600, train_perplexity=283.2381, train_loss=5.646288

Batch 86610, train_perplexity=251.04828, train_loss=5.5256453

Batch 86620, train_perplexity=264.13147, train_loss=5.576447

Batch 86630, train_perplexity=270.56516, train_loss=5.600513

Batch 86640, train_perplexity=271.4457, train_loss=5.603762

Batch 86650, train_perplexity=289.3287, train_loss=5.6675634

Batch 86660, train_perplexity=251.26959, train_loss=5.5265265

Batch 86670, train_perplexity=287.64194, train_loss=5.6617165

Batch 86680, train_perplexity=258.49475, train_loss=5.5548754

Batch 86690, train_perplexity=262.07974, train_loss=5.568649

Batch 86700, train_perplexity=266.06894, train_loss=5.5837555

Batch 86710, train_perplexity=309.85944, train_loss=5.736119

Batch 86720, train_perplexity=303.0173, train_loss=5.71379

Batch 86730, train_perplexity=266.83063, train_loss=5.586614

Batch 86740, train_perplexity=260.6086, train_loss=5.5630198

Batch 86750, train_perplexity=282.9569, train_loss=5.6452947

Batch 86760, train_perplexity=273.7656, train_loss=5.6122723

Batch 86770, train_perplexity=298.1295, train_loss=5.697528

Batch 86780, train_perplexity=293.09708, train_loss=5.680504

Batch 86790, train_perplexity=290.39368, train_loss=5.6712375

Batch 86800, train_perplexity=266.60056, train_loss=5.5857515

Batch 86810, train_perplexity=270.14865, train_loss=5.5989723

Batch 86820, train_perplexity=271.15942, train_loss=5.602707

Batch 86830, train_perplexity=247.67097, train_loss=5.512101

Batch 86840, train_perplexity=252.81467, train_loss=5.5326567

Batch 86850, train_perplexity=264.40152, train_loss=5.577469

Batch 86860, train_perplexity=260.12268, train_loss=5.5611534

Batch 86870, train_perplexity=276.58932, train_loss=5.622534

Batch 86880, train_perplexity=287.41272, train_loss=5.660919

Batch 86890, train_perplexity=239.23254, train_loss=5.477436

Batch 86900, train_perplexity=271.96448, train_loss=5.6056714

Batch 86910, train_perplexity=260.72, train_loss=5.563447

Batch 86920, train_perplexity=278.10648, train_loss=5.628004

Batch 86930, train_perplexity=262.3537, train_loss=5.5696936

Batch 86940, train_perplexity=271.77118, train_loss=5.6049604

Batch 86950, train_perplexity=299.31174, train_loss=5.7014856

Batch 86960, train_perplexity=272.79712, train_loss=5.6087284

Batch 86970, train_perplexity=299.76907, train_loss=5.7030125

Batch 86980, train_perplexity=272.79376, train_loss=5.608716

Batch 86990, train_perplexity=248.47914, train_loss=5.515359

Batch 87000, train_perplexity=277.91266, train_loss=5.627307

Batch 87010, train_perplexity=273.27832, train_loss=5.610491

Batch 87020, train_perplexity=271.77246, train_loss=5.604965

Batch 87030, train_perplexity=254.0158, train_loss=5.5373964

Batch 87040, train_perplexity=289.4852, train_loss=5.668104

Batch 87050, train_perplexity=209.24402, train_loss=5.343501

Batch 87060, train_perplexity=267.23212, train_loss=5.5881176

Batch 87070, train_perplexity=273.92294, train_loss=5.612847

Batch 87080, train_perplexity=259.92108, train_loss=5.560378

Batch 87090, train_perplexity=282.17853, train_loss=5.64254

Batch 87100, train_perplexity=286.4633, train_loss=5.6576104

Batch 87110, train_perplexity=268.70654, train_loss=5.59362

Batch 87120, train_perplexity=246.93738, train_loss=5.509135

Batch 87130, train_perplexity=272.7256, train_loss=5.608466

Batch 87140, train_perplexity=252.15466, train_loss=5.5300426

Batch 87150, train_perplexity=259.25613, train_loss=5.5578165

Batch 87160, train_perplexity=245.84204, train_loss=5.504689

Batch 87170, train_perplexity=266.22745, train_loss=5.584351

Batch 87180, train_perplexity=260.94287, train_loss=5.5643015

Batch 87190, train_perplexity=283.3575, train_loss=5.6467094

Batch 87200, train_perplexity=304.05798, train_loss=5.7172184

Batch 87210, train_perplexity=245.36118, train_loss=5.5027313

Batch 87220, train_perplexity=255.4028, train_loss=5.542842

Batch 87230, train_perplexity=298.5967, train_loss=5.699094

Batch 87240, train_perplexity=266.57437, train_loss=5.5856533

Batch 87250, train_perplexity=268.86432, train_loss=5.594207

Batch 87260, train_perplexity=250.55054, train_loss=5.5236607

Batch 87270, train_perplexity=272.2094, train_loss=5.6065717

Batch 87280, train_perplexity=267.0706, train_loss=5.587513

Batch 87290, train_perplexity=266.13962, train_loss=5.584021

Batch 87300, train_perplexity=274.73227, train_loss=5.615797

Batch 87310, train_perplexity=281.0271, train_loss=5.638451

Batch 87320, train_perplexity=255.50171, train_loss=5.543229

Batch 87330, train_perplexity=239.45624, train_loss=5.4783707

Batch 87340, train_perplexity=263.5984, train_loss=5.5744267

Batch 87350, train_perplexity=299.16107, train_loss=5.700982

Batch 87360, train_perplexity=278.54272, train_loss=5.6295714

Batch 87370, train_perplexity=270.9091, train_loss=5.6017833

Batch 87380, train_perplexity=271.3845, train_loss=5.6035366

Batch 87390, train_perplexity=283.67468, train_loss=5.647828

Batch 87400, train_perplexity=249.59753, train_loss=5.51985

Batch 87410, train_perplexity=252.36914, train_loss=5.530893

Batch 87420, train_perplexity=241.14969, train_loss=5.485418

Batch 87430, train_perplexity=256.56042, train_loss=5.547364

Batch 87440, train_perplexity=244.58624, train_loss=5.499568

Batch 87450, train_perplexity=255.79854, train_loss=5.54439

Batch 87460, train_perplexity=272.17426, train_loss=5.6064425

Batch 87470, train_perplexity=258.66318, train_loss=5.5555267

Batch 87480, train_perplexity=286.54922, train_loss=5.6579103

Batch 87490, train_perplexity=258.20364, train_loss=5.5537486

Batch 87500, train_perplexity=254.71466, train_loss=5.540144

Batch 87510, train_perplexity=265.52142, train_loss=5.5816956

Batch 87520, train_perplexity=267.51782, train_loss=5.589186

Batch 87530, train_perplexity=257.98767, train_loss=5.5529118

Batch 87540, train_perplexity=269.4046, train_loss=5.5962143

Batch 87550, train_perplexity=269.52127, train_loss=5.5966473

Batch 87560, train_perplexity=279.88382, train_loss=5.6343746

Batch 87570, train_perplexity=245.2089, train_loss=5.5021105

Batch 87580, train_perplexity=287.29214, train_loss=5.6604996

Batch 87590, train_perplexity=278.332, train_loss=5.6288147

Batch 87600, train_perplexity=244.59207, train_loss=5.499592

Batch 87610, train_perplexity=284.29596, train_loss=5.650016

Batch 87620, train_perplexity=258.3459, train_loss=5.5542994

Batch 87630, train_perplexity=266.54224, train_loss=5.5855327

Batch 87640, train_perplexity=265.91522, train_loss=5.5831776

Batch 87650, train_perplexity=260.29813, train_loss=5.5618277

Batch 87660, train_perplexity=294.20242, train_loss=5.684268

Batch 87670, train_perplexity=272.0559, train_loss=5.6060076

Batch 87680, train_perplexity=263.53455, train_loss=5.5741844

Batch 87690, train_perplexity=267.17032, train_loss=5.5878863

Batch 87700, train_perplexity=260.53854, train_loss=5.562751

Batch 87710, train_perplexity=284.4519, train_loss=5.650564

Batch 87720, train_perplexity=242.89241, train_loss=5.4926186
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 87730, train_perplexity=266.27405, train_loss=5.584526

Batch 87740, train_perplexity=288.79446, train_loss=5.665715

Batch 87750, train_perplexity=275.9429, train_loss=5.620194

Batch 87760, train_perplexity=270.58954, train_loss=5.600603

Batch 87770, train_perplexity=270.89346, train_loss=5.6017256

Batch 87780, train_perplexity=284.1492, train_loss=5.6494994

Batch 87790, train_perplexity=273.1504, train_loss=5.6100225

Batch 87800, train_perplexity=322.44092, train_loss=5.77592

Batch 87810, train_perplexity=275.52887, train_loss=5.6186924

Batch 87820, train_perplexity=250.49225, train_loss=5.523428

Batch 87830, train_perplexity=285.30704, train_loss=5.653566

Batch 87840, train_perplexity=256.50894, train_loss=5.5471635

Batch 87850, train_perplexity=259.383, train_loss=5.5583057

Batch 87860, train_perplexity=280.073, train_loss=5.6350503

Batch 87870, train_perplexity=256.36942, train_loss=5.5466194

Batch 87880, train_perplexity=242.44136, train_loss=5.49076

Batch 87890, train_perplexity=248.3337, train_loss=5.5147734

Batch 87900, train_perplexity=280.13458, train_loss=5.63527

Batch 87910, train_perplexity=283.72177, train_loss=5.647994

Batch 87920, train_perplexity=276.20288, train_loss=5.6211357

Batch 87930, train_perplexity=268.7628, train_loss=5.593829

Batch 87940, train_perplexity=262.22, train_loss=5.569184

Batch 87950, train_perplexity=278.94852, train_loss=5.631027

Batch 87960, train_perplexity=258.8843, train_loss=5.556381

Batch 87970, train_perplexity=261.216, train_loss=5.5653477

Batch 87980, train_perplexity=275.50574, train_loss=5.6186085

Batch 87990, train_perplexity=318.67038, train_loss=5.7641573

Batch 88000, train_perplexity=288.25378, train_loss=5.6638412

Batch 88010, train_perplexity=269.39264, train_loss=5.59617

Batch 88020, train_perplexity=244.02823, train_loss=5.497284

Batch 88030, train_perplexity=305.23404, train_loss=5.721079

Batch 88040, train_perplexity=283.00888, train_loss=5.6454782

Batch 88050, train_perplexity=243.76982, train_loss=5.4962244

Batch 88060, train_perplexity=281.64163, train_loss=5.6406355

Batch 88070, train_perplexity=231.73154, train_loss=5.4455795

Batch 88080, train_perplexity=262.49908, train_loss=5.5702477

Batch 88090, train_perplexity=262.09085, train_loss=5.5686913

Batch 88100, train_perplexity=252.21478, train_loss=5.530281

Batch 88110, train_perplexity=262.29816, train_loss=5.569482

Batch 88120, train_perplexity=271.0334, train_loss=5.602242

Batch 88130, train_perplexity=286.11478, train_loss=5.656393

Batch 88140, train_perplexity=269.46933, train_loss=5.5964546

Batch 88150, train_perplexity=260.5716, train_loss=5.5628777

Batch 88160, train_perplexity=244.81447, train_loss=5.5005007

Batch 88170, train_perplexity=282.1554, train_loss=5.642458

Batch 88180, train_perplexity=246.6981, train_loss=5.5081654

Batch 88190, train_perplexity=288.81705, train_loss=5.6657934

Batch 88200, train_perplexity=257.04572, train_loss=5.549254

Batch 88210, train_perplexity=294.23172, train_loss=5.6843677

Batch 88220, train_perplexity=287.99973, train_loss=5.6629596

Batch 88230, train_perplexity=277.027, train_loss=5.624115

Batch 88240, train_perplexity=261.64557, train_loss=5.566991

Batch 88250, train_perplexity=258.11786, train_loss=5.5534163

Batch 88260, train_perplexity=259.32562, train_loss=5.5580845

Batch 88270, train_perplexity=271.82678, train_loss=5.605165

Batch 88280, train_perplexity=265.93246, train_loss=5.5832424

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00040-of-00100
Loaded 305644 sentences.
Finished loading
Batch 88290, train_perplexity=271.6248, train_loss=5.6044216

Batch 88300, train_perplexity=262.23514, train_loss=5.5692415

Batch 88310, train_perplexity=258.01053, train_loss=5.5530005

Batch 88320, train_perplexity=279.56625, train_loss=5.6332393

Batch 88330, train_perplexity=276.33014, train_loss=5.6215963

Batch 88340, train_perplexity=245.49365, train_loss=5.503271

Batch 88350, train_perplexity=260.52338, train_loss=5.5626926

Batch 88360, train_perplexity=288.68378, train_loss=5.665332

Batch 88370, train_perplexity=271.56973, train_loss=5.604219

Batch 88380, train_perplexity=259.35135, train_loss=5.5581837

Batch 88390, train_perplexity=243.76633, train_loss=5.49621

Batch 88400, train_perplexity=252.18256, train_loss=5.5301533

Batch 88410, train_perplexity=233.55838, train_loss=5.453432

Batch 88420, train_perplexity=246.04962, train_loss=5.505533

Batch 88430, train_perplexity=251.20526, train_loss=5.5262704

Batch 88440, train_perplexity=253.7353, train_loss=5.5362916

Batch 88450, train_perplexity=297.71893, train_loss=5.69615

Batch 88460, train_perplexity=286.84918, train_loss=5.6589565

Batch 88470, train_perplexity=249.83617, train_loss=5.5208054

Batch 88480, train_perplexity=228.6564, train_loss=5.4322205

Batch 88490, train_perplexity=246.52548, train_loss=5.5074654

Batch 88500, train_perplexity=296.79193, train_loss=5.6930313

Batch 88510, train_perplexity=293.03894, train_loss=5.6803055

Batch 88520, train_perplexity=261.6244, train_loss=5.56691

Batch 88530, train_perplexity=265.78238, train_loss=5.582678

Batch 88540, train_perplexity=264.44983, train_loss=5.5776515

Batch 88550, train_perplexity=246.64742, train_loss=5.50796

Batch 88560, train_perplexity=275.6177, train_loss=5.6190147

Batch 88570, train_perplexity=285.86658, train_loss=5.655525

Batch 88580, train_perplexity=259.48962, train_loss=5.558717

Batch 88590, train_perplexity=261.5039, train_loss=5.566449

Batch 88600, train_perplexity=248.81113, train_loss=5.516694

Batch 88610, train_perplexity=269.6156, train_loss=5.5969973

Batch 88620, train_perplexity=255.25755, train_loss=5.542273

Batch 88630, train_perplexity=299.1548, train_loss=5.700961

Batch 88640, train_perplexity=256.5296, train_loss=5.547244

Batch 88650, train_perplexity=297.05344, train_loss=5.693912

Batch 88660, train_perplexity=273.23807, train_loss=5.6103435

Batch 88670, train_perplexity=260.90915, train_loss=5.5641723

Batch 88680, train_perplexity=292.9255, train_loss=5.6799183

Batch 88690, train_perplexity=288.7424, train_loss=5.665535

Batch 88700, train_perplexity=233.99893, train_loss=5.4553165

Batch 88710, train_perplexity=264.31128, train_loss=5.5771275

Batch 88720, train_perplexity=260.7501, train_loss=5.5635624

Batch 88730, train_perplexity=259.12216, train_loss=5.5572996

Batch 88740, train_perplexity=280.656, train_loss=5.63713

Batch 88750, train_perplexity=282.40656, train_loss=5.6433477

Batch 88760, train_perplexity=247.66602, train_loss=5.512081

Batch 88770, train_perplexity=298.3994, train_loss=5.698433

Batch 88780, train_perplexity=247.98273, train_loss=5.513359

Batch 88790, train_perplexity=273.21658, train_loss=5.610265

Batch 88800, train_perplexity=258.43484, train_loss=5.5546436

Batch 88810, train_perplexity=263.5926, train_loss=5.5744047

Batch 88820, train_perplexity=286.66513, train_loss=5.6583147

Batch 88830, train_perplexity=244.677, train_loss=5.499939

Batch 88840, train_perplexity=248.0487, train_loss=5.513625

Batch 88850, train_perplexity=264.9356, train_loss=5.579487

Batch 88860, train_perplexity=253.7347, train_loss=5.536289

Batch 88870, train_perplexity=265.82675, train_loss=5.5828447

Batch 88880, train_perplexity=232.88802, train_loss=5.4505577

Batch 88890, train_perplexity=264.76703, train_loss=5.5788503

Batch 88900, train_perplexity=269.18527, train_loss=5.5954

Batch 88910, train_perplexity=279.1038, train_loss=5.6315837

Batch 88920, train_perplexity=272.96396, train_loss=5.6093397

Batch 88930, train_perplexity=250.96306, train_loss=5.5253057

Batch 88940, train_perplexity=279.12363, train_loss=5.6316547

Batch 88950, train_perplexity=295.44827, train_loss=5.6884937

Batch 88960, train_perplexity=294.36377, train_loss=5.6848164

Batch 88970, train_perplexity=238.31737, train_loss=5.4736032

Batch 88980, train_perplexity=303.49478, train_loss=5.7153645
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88990, train_perplexity=282.53195, train_loss=5.6437917

Batch 89000, train_perplexity=266.12378, train_loss=5.5839615

Batch 89010, train_perplexity=300.94693, train_loss=5.706934

Batch 89020, train_perplexity=249.13808, train_loss=5.5180073

Batch 89030, train_perplexity=258.34775, train_loss=5.5543065

Batch 89040, train_perplexity=264.64017, train_loss=5.578371

Batch 89050, train_perplexity=277.63147, train_loss=5.6262946

Batch 89060, train_perplexity=253.43675, train_loss=5.5351143

Batch 89070, train_perplexity=274.14966, train_loss=5.613674

Batch 89080, train_perplexity=249.46011, train_loss=5.519299

Batch 89090, train_perplexity=234.94794, train_loss=5.459364

Batch 89100, train_perplexity=235.69514, train_loss=5.462539

Batch 89110, train_perplexity=271.5266, train_loss=5.60406

Batch 89120, train_perplexity=277.16177, train_loss=5.6246014

Batch 89130, train_perplexity=252.88531, train_loss=5.532936

Batch 89140, train_perplexity=240.61569, train_loss=5.483201

Batch 89150, train_perplexity=259.0248, train_loss=5.556924

Batch 89160, train_perplexity=283.8721, train_loss=5.648524

Batch 89170, train_perplexity=266.0154, train_loss=5.5835543

Batch 89180, train_perplexity=261.60605, train_loss=5.5668397

Batch 89190, train_perplexity=235.73415, train_loss=5.4627047

Batch 89200, train_perplexity=261.56912, train_loss=5.5666986

Batch 89210, train_perplexity=236.09097, train_loss=5.464217

Batch 89220, train_perplexity=243.72495, train_loss=5.4960403

Batch 89230, train_perplexity=244.57109, train_loss=5.499506

Batch 89240, train_perplexity=283.5873, train_loss=5.64752

Batch 89250, train_perplexity=274.19803, train_loss=5.6138506

Batch 89260, train_perplexity=271.71262, train_loss=5.604745

Batch 89270, train_perplexity=276.1023, train_loss=5.6207714

Batch 89280, train_perplexity=258.7594, train_loss=5.5558987

Batch 89290, train_perplexity=258.94034, train_loss=5.5565977

Batch 89300, train_perplexity=239.29324, train_loss=5.4776897

Batch 89310, train_perplexity=286.14508, train_loss=5.656499

Batch 89320, train_perplexity=262.7305, train_loss=5.571129

Batch 89330, train_perplexity=226.33337, train_loss=5.422009

Batch 89340, train_perplexity=265.54233, train_loss=5.581774

Batch 89350, train_perplexity=274.2587, train_loss=5.614072

Batch 89360, train_perplexity=303.4453, train_loss=5.7152014

Batch 89370, train_perplexity=252.71559, train_loss=5.5322647

Batch 89380, train_perplexity=237.63266, train_loss=5.470726

Batch 89390, train_perplexity=247.34206, train_loss=5.510772

Batch 89400, train_perplexity=281.67682, train_loss=5.6407604

Batch 89410, train_perplexity=246.73811, train_loss=5.5083275

Batch 89420, train_perplexity=255.03175, train_loss=5.541388

Batch 89430, train_perplexity=240.51062, train_loss=5.4827642

Batch 89440, train_perplexity=276.20053, train_loss=5.621127

Batch 89450, train_perplexity=276.9422, train_loss=5.623809

Batch 89460, train_perplexity=238.5709, train_loss=5.4746666

Batch 89470, train_perplexity=285.59872, train_loss=5.6545877

Batch 89480, train_perplexity=246.51831, train_loss=5.5074363

Batch 89490, train_perplexity=245.39534, train_loss=5.5028706

Batch 89500, train_perplexity=291.43335, train_loss=5.6748114

Batch 89510, train_perplexity=237.67798, train_loss=5.4709167

Batch 89520, train_perplexity=254.00804, train_loss=5.537366

Batch 89530, train_perplexity=251.65977, train_loss=5.528078

Batch 89540, train_perplexity=235.13902, train_loss=5.460177

Batch 89550, train_perplexity=268.99152, train_loss=5.59468

Batch 89560, train_perplexity=267.34222, train_loss=5.5885296

Batch 89570, train_perplexity=266.05005, train_loss=5.5836844

Batch 89580, train_perplexity=228.68944, train_loss=5.432365

Batch 89590, train_perplexity=260.5132, train_loss=5.5626535

Batch 89600, train_perplexity=247.23805, train_loss=5.5103517

Batch 89610, train_perplexity=256.39984, train_loss=5.546738

Batch 89620, train_perplexity=256.5039, train_loss=5.547144

Batch 89630, train_perplexity=259.2104, train_loss=5.55764

Batch 89640, train_perplexity=255.32816, train_loss=5.5425496

Batch 89650, train_perplexity=265.8569, train_loss=5.582958

Batch 89660, train_perplexity=254.13754, train_loss=5.5378757

Batch 89670, train_perplexity=278.31143, train_loss=5.628741

Batch 89680, train_perplexity=261.23456, train_loss=5.5654187

Batch 89690, train_perplexity=279.0946, train_loss=5.631551

Batch 89700, train_perplexity=260.2177, train_loss=5.5615187

Batch 89710, train_perplexity=264.76248, train_loss=5.578833

Batch 89720, train_perplexity=276.13864, train_loss=5.620903

Batch 89730, train_perplexity=258.90195, train_loss=5.5564494

Batch 89740, train_perplexity=287.39545, train_loss=5.660859

Batch 89750, train_perplexity=281.44055, train_loss=5.639921

Batch 89760, train_perplexity=256.05615, train_loss=5.545397

Batch 89770, train_perplexity=259.08237, train_loss=5.557146

Batch 89780, train_perplexity=269.28976, train_loss=5.595788

Batch 89790, train_perplexity=269.79886, train_loss=5.5976768

Batch 89800, train_perplexity=244.90706, train_loss=5.500879

Batch 89810, train_perplexity=248.14336, train_loss=5.5140066

Batch 89820, train_perplexity=258.51767, train_loss=5.554964

Batch 89830, train_perplexity=239.82819, train_loss=5.479923

Batch 89840, train_perplexity=278.96393, train_loss=5.6310825

Batch 89850, train_perplexity=237.72571, train_loss=5.4711175

Batch 89860, train_perplexity=280.28516, train_loss=5.6358075

Batch 89870, train_perplexity=252.32619, train_loss=5.5307226

Batch 89880, train_perplexity=251.43715, train_loss=5.527193

Batch 89890, train_perplexity=267.45264, train_loss=5.5889425

Batch 89900, train_perplexity=272.45172, train_loss=5.6074615

Batch 89910, train_perplexity=275.04553, train_loss=5.6169367

Batch 89920, train_perplexity=260.116, train_loss=5.5611277

Batch 89930, train_perplexity=263.0444, train_loss=5.572323

Batch 89940, train_perplexity=275.27805, train_loss=5.6177816

Batch 89950, train_perplexity=275.95657, train_loss=5.6202435

Batch 89960, train_perplexity=239.55925, train_loss=5.478801

Batch 89970, train_perplexity=262.94458, train_loss=5.5719433

Batch 89980, train_perplexity=279.49493, train_loss=5.632984

Batch 89990, train_perplexity=264.51022, train_loss=5.57788

Batch 90000, train_perplexity=239.74803, train_loss=5.4795885

Batch 90010, train_perplexity=225.51602, train_loss=5.418391

Batch 90020, train_perplexity=269.5418, train_loss=5.5967236

Batch 90030, train_perplexity=268.95984, train_loss=5.594562

Batch 90040, train_perplexity=247.02559, train_loss=5.509492

Batch 90050, train_perplexity=263.92752, train_loss=5.5756745

Batch 90060, train_perplexity=293.76196, train_loss=5.68277

Batch 90070, train_perplexity=272.62704, train_loss=5.6081047

Batch 90080, train_perplexity=229.85934, train_loss=5.4374676

Batch 90090, train_perplexity=250.74799, train_loss=5.5244484

Batch 90100, train_perplexity=260.13608, train_loss=5.561205

Batch 90110, train_perplexity=268.13364, train_loss=5.5914855

Batch 90120, train_perplexity=260.4702, train_loss=5.5624886

Batch 90130, train_perplexity=254.69208, train_loss=5.5400553

Batch 90140, train_perplexity=271.83234, train_loss=5.6051855

Batch 90150, train_perplexity=271.4878, train_loss=5.603917

Batch 90160, train_perplexity=260.2722, train_loss=5.561728

Batch 90170, train_perplexity=277.78232, train_loss=5.6268377

Batch 90180, train_perplexity=255.52632, train_loss=5.5433254

Batch 90190, train_perplexity=257.16745, train_loss=5.5497274

Batch 90200, train_perplexity=265.66962, train_loss=5.5822535

Batch 90210, train_perplexity=245.8118, train_loss=5.504566

Batch 90220, train_perplexity=246.57933, train_loss=5.5076838

Batch 90230, train_perplexity=261.3057, train_loss=5.565691

Batch 90240, train_perplexity=244.40717, train_loss=5.4988356

Batch 90250, train_perplexity=220.2386, train_loss=5.3947115

Batch 90260, train_perplexity=250.59679, train_loss=5.523845

Batch 90270, train_perplexity=326.65643, train_loss=5.788909

Batch 90280, train_perplexity=240.56865, train_loss=5.4830055

Batch 90290, train_perplexity=263.16208, train_loss=5.57277

Batch 90300, train_perplexity=261.39905, train_loss=5.566048
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 90310, train_perplexity=277.1249, train_loss=5.6244683

Batch 90320, train_perplexity=277.78204, train_loss=5.626837

Batch 90330, train_perplexity=263.25107, train_loss=5.573108

Batch 90340, train_perplexity=238.05228, train_loss=5.4724903

Batch 90350, train_perplexity=242.2977, train_loss=5.490167

Batch 90360, train_perplexity=290.95917, train_loss=5.673183

Batch 90370, train_perplexity=259.16678, train_loss=5.5574718

Batch 90380, train_perplexity=240.9397, train_loss=5.4845467

Batch 90390, train_perplexity=260.1511, train_loss=5.5612626

Batch 90400, train_perplexity=296.35013, train_loss=5.6915417

Batch 90410, train_perplexity=258.94824, train_loss=5.556628

Batch 90420, train_perplexity=277.30533, train_loss=5.625119

Batch 90430, train_perplexity=265.62234, train_loss=5.5820756

Batch 90440, train_perplexity=256.10648, train_loss=5.5455933

Batch 90450, train_perplexity=253.99423, train_loss=5.5373116

Batch 90460, train_perplexity=259.41406, train_loss=5.5584254

Batch 90470, train_perplexity=263.04315, train_loss=5.572318

Batch 90480, train_perplexity=228.43703, train_loss=5.4312606

Batch 90490, train_perplexity=235.67682, train_loss=5.4624615

Batch 90500, train_perplexity=236.86337, train_loss=5.4674835

Batch 90510, train_perplexity=260.9512, train_loss=5.5643334

Batch 90520, train_perplexity=266.86053, train_loss=5.586726

Batch 90530, train_perplexity=255.86052, train_loss=5.5446324

Batch 90540, train_perplexity=254.25197, train_loss=5.538326

Batch 90550, train_perplexity=250.56583, train_loss=5.5237217

Batch 90560, train_perplexity=278.12253, train_loss=5.628062

Batch 90570, train_perplexity=242.76167, train_loss=5.49208

Batch 90580, train_perplexity=237.28526, train_loss=5.469263

Batch 90590, train_perplexity=298.23584, train_loss=5.6978846

Batch 90600, train_perplexity=245.15582, train_loss=5.501894

Batch 90610, train_perplexity=340.5338, train_loss=5.8305144

Batch 90620, train_perplexity=307.20587, train_loss=5.727518

Batch 90630, train_perplexity=280.39758, train_loss=5.6362085

Batch 90640, train_perplexity=247.72224, train_loss=5.512308

Batch 90650, train_perplexity=266.3265, train_loss=5.584723

Batch 90660, train_perplexity=239.61592, train_loss=5.4790373

Batch 90670, train_perplexity=273.25565, train_loss=5.610408

Batch 90680, train_perplexity=268.19156, train_loss=5.5917015

Batch 90690, train_perplexity=246.60402, train_loss=5.507784

Batch 90700, train_perplexity=258.42856, train_loss=5.5546193

Batch 90710, train_perplexity=243.28662, train_loss=5.4942403

Batch 90720, train_perplexity=266.72595, train_loss=5.5862217

Batch 90730, train_perplexity=260.29517, train_loss=5.561816

Batch 90740, train_perplexity=260.94995, train_loss=5.5643287

Batch 90750, train_perplexity=247.6476, train_loss=5.5120068

Batch 90760, train_perplexity=277.09995, train_loss=5.624378

Batch 90770, train_perplexity=257.05222, train_loss=5.549279

Batch 90780, train_perplexity=269.58347, train_loss=5.596878

Batch 90790, train_perplexity=268.36197, train_loss=5.5923367

Batch 90800, train_perplexity=295.0881, train_loss=5.687274

Batch 90810, train_perplexity=258.46506, train_loss=5.5547605

Batch 90820, train_perplexity=241.96854, train_loss=5.4888077

Batch 90830, train_perplexity=272.32068, train_loss=5.6069803

Batch 90840, train_perplexity=307.88263, train_loss=5.7297187

Batch 90850, train_perplexity=288.3016, train_loss=5.664007

Batch 90860, train_perplexity=281.47034, train_loss=5.640027

Batch 90870, train_perplexity=262.5011, train_loss=5.5702553

Batch 90880, train_perplexity=281.211, train_loss=5.6391053

Batch 90890, train_perplexity=241.57114, train_loss=5.487164

Batch 90900, train_perplexity=241.1498, train_loss=5.4854183

Batch 90910, train_perplexity=265.71585, train_loss=5.5824275

Batch 90920, train_perplexity=244.11343, train_loss=5.497633

Batch 90930, train_perplexity=233.46463, train_loss=5.4530306

Batch 90940, train_perplexity=272.64224, train_loss=5.6081605

Batch 90950, train_perplexity=289.48438, train_loss=5.6681013

Batch 90960, train_perplexity=285.88486, train_loss=5.655589

Batch 90970, train_perplexity=261.65457, train_loss=5.567025

Batch 90980, train_perplexity=259.54187, train_loss=5.558918

Batch 90990, train_perplexity=250.47444, train_loss=5.523357

Batch 91000, train_perplexity=282.7445, train_loss=5.6445436

Batch 91010, train_perplexity=254.3969, train_loss=5.5388956

Batch 91020, train_perplexity=273.85934, train_loss=5.6126146

Batch 91030, train_perplexity=246.60637, train_loss=5.5077934

Batch 91040, train_perplexity=283.98273, train_loss=5.6489134

Batch 91050, train_perplexity=246.23341, train_loss=5.50628

Batch 91060, train_perplexity=253.97922, train_loss=5.5372524

Batch 91070, train_perplexity=258.1321, train_loss=5.5534716

Batch 91080, train_perplexity=243.65094, train_loss=5.4957366

Batch 91090, train_perplexity=248.46944, train_loss=5.51532

Batch 91100, train_perplexity=256.89426, train_loss=5.5486646

Batch 91110, train_perplexity=263.0504, train_loss=5.5723457

Batch 91120, train_perplexity=262.34595, train_loss=5.569664

Batch 91130, train_perplexity=255.5775, train_loss=5.5435257

Batch 91140, train_perplexity=246.88063, train_loss=5.508905

Batch 91150, train_perplexity=231.52919, train_loss=5.444706

Batch 91160, train_perplexity=311.12338, train_loss=5.7401896

Batch 91170, train_perplexity=274.98627, train_loss=5.616721

Batch 91180, train_perplexity=251.18814, train_loss=5.526202

Batch 91190, train_perplexity=272.75916, train_loss=5.608589

Batch 91200, train_perplexity=251.24396, train_loss=5.5264244

Batch 91210, train_perplexity=244.0209, train_loss=5.497254

Batch 91220, train_perplexity=244.15836, train_loss=5.497817

Batch 91230, train_perplexity=235.60176, train_loss=5.462143

Batch 91240, train_perplexity=269.55338, train_loss=5.5967665

Batch 91250, train_perplexity=247.75874, train_loss=5.5124555

Batch 91260, train_perplexity=273.67474, train_loss=5.6119404

Batch 91270, train_perplexity=272.9974, train_loss=5.6094623

Batch 91280, train_perplexity=267.87228, train_loss=5.5905104

Batch 91290, train_perplexity=245.02411, train_loss=5.5013566

Batch 91300, train_perplexity=275.972, train_loss=5.6202993

Batch 91310, train_perplexity=262.80368, train_loss=5.5714073

Batch 91320, train_perplexity=277.64804, train_loss=5.626354

Batch 91330, train_perplexity=295.48688, train_loss=5.6886244

Batch 91340, train_perplexity=226.21414, train_loss=5.421482

Batch 91350, train_perplexity=256.11087, train_loss=5.5456104

Batch 91360, train_perplexity=262.24164, train_loss=5.5692663

Batch 91370, train_perplexity=259.01062, train_loss=5.556869

Batch 91380, train_perplexity=273.80188, train_loss=5.612405

Batch 91390, train_perplexity=260.587, train_loss=5.562937

Batch 91400, train_perplexity=246.11252, train_loss=5.505789

Batch 91410, train_perplexity=251.00136, train_loss=5.5254583

Batch 91420, train_perplexity=267.011, train_loss=5.58729

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100
Loaded 306536 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00067-of-00100
Loaded 306536 sentences.
Finished loading
Batch 91430, train_perplexity=277.24768, train_loss=5.6249113

Batch 91440, train_perplexity=280.18723, train_loss=5.635458

Batch 91450, train_perplexity=215.37552, train_loss=5.372383

Batch 91460, train_perplexity=264.2576, train_loss=5.5769243

Batch 91470, train_perplexity=278.28662, train_loss=5.6286516

Batch 91480, train_perplexity=246.43028, train_loss=5.507079

Batch 91490, train_perplexity=272.16333, train_loss=5.6064024

Batch 91500, train_perplexity=274.4751, train_loss=5.6148605

Batch 91510, train_perplexity=253.57529, train_loss=5.5356607

Batch 91520, train_perplexity=232.71506, train_loss=5.449815

Batch 91530, train_perplexity=284.48447, train_loss=5.6506786

Batch 91540, train_perplexity=270.23755, train_loss=5.5993013

Batch 91550, train_perplexity=261.38632, train_loss=5.5659995

Batch 91560, train_perplexity=276.09332, train_loss=5.620739
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 91570, train_perplexity=240.2919, train_loss=5.4818544

Batch 91580, train_perplexity=275.41812, train_loss=5.6182904

Batch 91590, train_perplexity=258.6935, train_loss=5.555644

Batch 91600, train_perplexity=251.42036, train_loss=5.5271263

Batch 91610, train_perplexity=274.7501, train_loss=5.615862

Batch 91620, train_perplexity=219.80415, train_loss=5.392737

Batch 91630, train_perplexity=254.63525, train_loss=5.539832

Batch 91640, train_perplexity=251.59018, train_loss=5.5278015

Batch 91650, train_perplexity=276.3586, train_loss=5.6216993

Batch 91660, train_perplexity=265.51764, train_loss=5.5816813

Batch 91670, train_perplexity=243.36797, train_loss=5.4945745

Batch 91680, train_perplexity=274.87012, train_loss=5.6162987

Batch 91690, train_perplexity=278.31052, train_loss=5.6287374

Batch 91700, train_perplexity=249.71254, train_loss=5.5203104

Batch 91710, train_perplexity=282.6671, train_loss=5.64427

Batch 91720, train_perplexity=271.3172, train_loss=5.6032887

Batch 91730, train_perplexity=276.09833, train_loss=5.620757

Batch 91740, train_perplexity=270.27002, train_loss=5.5994215

Batch 91750, train_perplexity=224.81503, train_loss=5.415278

Batch 91760, train_perplexity=254.21475, train_loss=5.5381794

Batch 91770, train_perplexity=226.67996, train_loss=5.423539

Batch 91780, train_perplexity=266.72748, train_loss=5.5862274

Batch 91790, train_perplexity=231.31467, train_loss=5.443779

Batch 91800, train_perplexity=247.83827, train_loss=5.5127764

Batch 91810, train_perplexity=279.74506, train_loss=5.6338787

Batch 91820, train_perplexity=261.49365, train_loss=5.56641

Batch 91830, train_perplexity=242.93619, train_loss=5.492799

Batch 91840, train_perplexity=235.90405, train_loss=5.463425

Batch 91850, train_perplexity=236.03008, train_loss=5.463959

Batch 91860, train_perplexity=281.52054, train_loss=5.6402054

Batch 91870, train_perplexity=275.09695, train_loss=5.6171236

Batch 91880, train_perplexity=260.1723, train_loss=5.561344

Batch 91890, train_perplexity=254.16336, train_loss=5.537977

Batch 91900, train_perplexity=239.95001, train_loss=5.4804306

Batch 91910, train_perplexity=251.3645, train_loss=5.526904

Batch 91920, train_perplexity=264.1918, train_loss=5.5766754

Batch 91930, train_perplexity=276.65067, train_loss=5.6227555

Batch 91940, train_perplexity=267.28665, train_loss=5.5883217

Batch 91950, train_perplexity=281.8066, train_loss=5.641221

Batch 91960, train_perplexity=286.89923, train_loss=5.659131

Batch 91970, train_perplexity=246.05173, train_loss=5.505542

Batch 91980, train_perplexity=252.36769, train_loss=5.530887

Batch 91990, train_perplexity=220.4111, train_loss=5.3954945

Batch 92000, train_perplexity=303.3779, train_loss=5.714979

Batch 92010, train_perplexity=225.68771, train_loss=5.4191523

Batch 92020, train_perplexity=257.90894, train_loss=5.5526066

Batch 92030, train_perplexity=308.33002, train_loss=5.7311707

Batch 92040, train_perplexity=241.90323, train_loss=5.488538

Batch 92050, train_perplexity=257.95963, train_loss=5.552803

Batch 92060, train_perplexity=280.51352, train_loss=5.636622

Batch 92070, train_perplexity=246.45496, train_loss=5.5071793

Batch 92080, train_perplexity=277.40717, train_loss=5.6254864

Batch 92090, train_perplexity=259.4644, train_loss=5.5586195

Batch 92100, train_perplexity=238.32123, train_loss=5.4736195

Batch 92110, train_perplexity=278.23462, train_loss=5.6284647

Batch 92120, train_perplexity=238.41307, train_loss=5.4740047

Batch 92130, train_perplexity=276.06754, train_loss=5.6206455

Batch 92140, train_perplexity=227.75073, train_loss=5.4282517

Batch 92150, train_perplexity=249.34392, train_loss=5.518833

Batch 92160, train_perplexity=287.4175, train_loss=5.660936

Batch 92170, train_perplexity=260.5306, train_loss=5.5627203

Batch 92180, train_perplexity=227.33029, train_loss=5.426404

Batch 92190, train_perplexity=242.57874, train_loss=5.4913263

Batch 92200, train_perplexity=264.5829, train_loss=5.5781546

Batch 92210, train_perplexity=251.6125, train_loss=5.52789

Batch 92220, train_perplexity=252.32306, train_loss=5.53071

Batch 92230, train_perplexity=266.09967, train_loss=5.583871

Batch 92240, train_perplexity=250.42334, train_loss=5.523153

Batch 92250, train_perplexity=243.49634, train_loss=5.495102

Batch 92260, train_perplexity=257.60672, train_loss=5.551434

Batch 92270, train_perplexity=271.12244, train_loss=5.6025705

Batch 92280, train_perplexity=250.58662, train_loss=5.5238047

Batch 92290, train_perplexity=266.89618, train_loss=5.5868597

Batch 92300, train_perplexity=229.58987, train_loss=5.4362946

Batch 92310, train_perplexity=252.75801, train_loss=5.5324326

Batch 92320, train_perplexity=253.94882, train_loss=5.5371327

Batch 92330, train_perplexity=266.9327, train_loss=5.5869966

Batch 92340, train_perplexity=250.67793, train_loss=5.524169

Batch 92350, train_perplexity=281.81924, train_loss=5.641266

Batch 92360, train_perplexity=240.61018, train_loss=5.483178

Batch 92370, train_perplexity=251.30986, train_loss=5.5266867

Batch 92380, train_perplexity=282.12457, train_loss=5.642349

Batch 92390, train_perplexity=239.14381, train_loss=5.477065

Batch 92400, train_perplexity=268.077, train_loss=5.5912743

Batch 92410, train_perplexity=239.72357, train_loss=5.4794865

Batch 92420, train_perplexity=247.8177, train_loss=5.5126934

Batch 92430, train_perplexity=263.41016, train_loss=5.5737123

Batch 92440, train_perplexity=249.28235, train_loss=5.518586

Batch 92450, train_perplexity=267.47318, train_loss=5.5890193

Batch 92460, train_perplexity=269.7014, train_loss=5.5973153

Batch 92470, train_perplexity=263.3873, train_loss=5.5736256

Batch 92480, train_perplexity=262.7696, train_loss=5.5712776

Batch 92490, train_perplexity=266.72263, train_loss=5.5862093

Batch 92500, train_perplexity=243.35439, train_loss=5.4945188

Batch 92510, train_perplexity=240.20323, train_loss=5.4814854

Batch 92520, train_perplexity=305.0164, train_loss=5.7203655

Batch 92530, train_perplexity=230.77945, train_loss=5.4414625

Batch 92540, train_perplexity=270.77774, train_loss=5.6012983

Batch 92550, train_perplexity=258.8127, train_loss=5.5561047

Batch 92560, train_perplexity=278.69962, train_loss=5.6301346

Batch 92570, train_perplexity=235.67749, train_loss=5.4624643

Batch 92580, train_perplexity=268.5842, train_loss=5.5931644

Batch 92590, train_perplexity=258.45544, train_loss=5.5547233

Batch 92600, train_perplexity=244.999, train_loss=5.501254

Batch 92610, train_perplexity=246.74516, train_loss=5.508356

Batch 92620, train_perplexity=267.8281, train_loss=5.5903454

Batch 92630, train_perplexity=271.4312, train_loss=5.6037087

Batch 92640, train_perplexity=262.97205, train_loss=5.5720477

Batch 92650, train_perplexity=251.33623, train_loss=5.5267916

Batch 92660, train_perplexity=288.21405, train_loss=5.6637034

Batch 92670, train_perplexity=238.78511, train_loss=5.475564

Batch 92680, train_perplexity=279.89624, train_loss=5.634419

Batch 92690, train_perplexity=287.6063, train_loss=5.6615925

Batch 92700, train_perplexity=264.4338, train_loss=5.577591

Batch 92710, train_perplexity=258.4748, train_loss=5.554798

Batch 92720, train_perplexity=288.18875, train_loss=5.6636157

Batch 92730, train_perplexity=267.71753, train_loss=5.5899324

Batch 92740, train_perplexity=265.33032, train_loss=5.5809755

Batch 92750, train_perplexity=273.44922, train_loss=5.611116

Batch 92760, train_perplexity=250.56416, train_loss=5.523715

Batch 92770, train_perplexity=242.38126, train_loss=5.490512

Batch 92780, train_perplexity=232.42873, train_loss=5.4485836

Batch 92790, train_perplexity=240.66481, train_loss=5.483405

Batch 92800, train_perplexity=262.39948, train_loss=5.569868

Batch 92810, train_perplexity=258.08414, train_loss=5.5532856

Batch 92820, train_perplexity=298.1906, train_loss=5.697733

Batch 92830, train_perplexity=282.56525, train_loss=5.6439095

Batch 92840, train_perplexity=274.12, train_loss=5.613566

Batch 92850, train_perplexity=251.64586, train_loss=5.528023

Batch 92860, train_perplexity=260.676, train_loss=5.563278

Batch 92870, train_perplexity=248.43898, train_loss=5.5151973

Batch 92880, train_perplexity=277.09915, train_loss=5.6243753
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 92890, train_perplexity=242.13405, train_loss=5.4894915

Batch 92900, train_perplexity=265.87872, train_loss=5.58304

Batch 92910, train_perplexity=272.00922, train_loss=5.605836

Batch 92920, train_perplexity=234.24431, train_loss=5.4563646

Batch 92930, train_perplexity=264.7217, train_loss=5.578679

Batch 92940, train_perplexity=287.61725, train_loss=5.6616306

Batch 92950, train_perplexity=238.88748, train_loss=5.4759927

Batch 92960, train_perplexity=273.13022, train_loss=5.6099486

Batch 92970, train_perplexity=263.62604, train_loss=5.5745316

Batch 92980, train_perplexity=251.6593, train_loss=5.528076

Batch 92990, train_perplexity=261.2135, train_loss=5.565338

Batch 93000, train_perplexity=254.07321, train_loss=5.5376225

Batch 93010, train_perplexity=244.34622, train_loss=5.498586

Batch 93020, train_perplexity=293.94763, train_loss=5.6834016

Batch 93030, train_perplexity=280.77542, train_loss=5.637555

Batch 93040, train_perplexity=262.14297, train_loss=5.56889

Batch 93050, train_perplexity=250.6331, train_loss=5.52399

Batch 93060, train_perplexity=265.22104, train_loss=5.5805635

Batch 93070, train_perplexity=280.15247, train_loss=5.635334

Batch 93080, train_perplexity=267.68817, train_loss=5.589823

Batch 93090, train_perplexity=248.53033, train_loss=5.515565

Batch 93100, train_perplexity=251.45993, train_loss=5.5272837

Batch 93110, train_perplexity=271.9913, train_loss=5.60577

Batch 93120, train_perplexity=245.78015, train_loss=5.5044374

Batch 93130, train_perplexity=276.73416, train_loss=5.6230574

Batch 93140, train_perplexity=262.13797, train_loss=5.568871

Batch 93150, train_perplexity=269.12698, train_loss=5.5951834

Batch 93160, train_perplexity=259.09177, train_loss=5.5571823

Batch 93170, train_perplexity=279.88837, train_loss=5.634391

Batch 93180, train_perplexity=226.08624, train_loss=5.4209166

Batch 93190, train_perplexity=254.31477, train_loss=5.538573

Batch 93200, train_perplexity=271.4492, train_loss=5.603775

Batch 93210, train_perplexity=247.32495, train_loss=5.510703

Batch 93220, train_perplexity=257.64493, train_loss=5.5515823

Batch 93230, train_perplexity=245.64214, train_loss=5.5038757

Batch 93240, train_perplexity=263.3524, train_loss=5.573493

Batch 93250, train_perplexity=242.2523, train_loss=5.4899797

Batch 93260, train_perplexity=271.66235, train_loss=5.60456

Batch 93270, train_perplexity=272.26822, train_loss=5.6067877

Batch 93280, train_perplexity=263.55563, train_loss=5.5742645

Batch 93290, train_perplexity=260.04147, train_loss=5.560841

Batch 93300, train_perplexity=248.2108, train_loss=5.5142784

Batch 93310, train_perplexity=252.17053, train_loss=5.5301056

Batch 93320, train_perplexity=272.21265, train_loss=5.6065836

Batch 93330, train_perplexity=263.6675, train_loss=5.574689

Batch 93340, train_perplexity=256.36084, train_loss=5.546586

Batch 93350, train_perplexity=240.79199, train_loss=5.4839334

Batch 93360, train_perplexity=276.05554, train_loss=5.620602

Batch 93370, train_perplexity=230.50847, train_loss=5.4402876

Batch 93380, train_perplexity=272.75174, train_loss=5.608562

Batch 93390, train_perplexity=241.61711, train_loss=5.4873543

Batch 93400, train_perplexity=265.92587, train_loss=5.5832176

Batch 93410, train_perplexity=232.06482, train_loss=5.4470167

Batch 93420, train_perplexity=271.85333, train_loss=5.6052628

Batch 93430, train_perplexity=261.86725, train_loss=5.5678377

Batch 93440, train_perplexity=263.0133, train_loss=5.5722046

Batch 93450, train_perplexity=246.51866, train_loss=5.5074377

Batch 93460, train_perplexity=262.92227, train_loss=5.5718584

Batch 93470, train_perplexity=231.03975, train_loss=5.4425898

Batch 93480, train_perplexity=263.36017, train_loss=5.5735226

Batch 93490, train_perplexity=295.6443, train_loss=5.689157

Batch 93500, train_perplexity=250.10864, train_loss=5.5218954

Batch 93510, train_perplexity=263.0967, train_loss=5.5725217

Batch 93520, train_perplexity=225.45905, train_loss=5.4181385

Batch 93530, train_perplexity=261.4216, train_loss=5.5661345

Batch 93540, train_perplexity=243.38213, train_loss=5.4946327

Batch 93550, train_perplexity=288.74515, train_loss=5.6655445

Batch 93560, train_perplexity=298.12378, train_loss=5.697509

Batch 93570, train_perplexity=242.59677, train_loss=5.4914007

Batch 93580, train_perplexity=275.0058, train_loss=5.616792

Batch 93590, train_perplexity=276.70502, train_loss=5.622952

Batch 93600, train_perplexity=257.00833, train_loss=5.5491085

Batch 93610, train_perplexity=237.36131, train_loss=5.4695835

Batch 93620, train_perplexity=257.2717, train_loss=5.5501328

Batch 93630, train_perplexity=241.8006, train_loss=5.4881134

Batch 93640, train_perplexity=274.34412, train_loss=5.614383

Batch 93650, train_perplexity=260.5932, train_loss=5.5629606

Batch 93660, train_perplexity=253.45053, train_loss=5.5351686

Batch 93670, train_perplexity=259.12885, train_loss=5.5573254

Batch 93680, train_perplexity=273.0324, train_loss=5.6095905

Batch 93690, train_perplexity=255.8011, train_loss=5.5444

Batch 93700, train_perplexity=231.5897, train_loss=5.4449673

Batch 93710, train_perplexity=233.49246, train_loss=5.45315

Batch 93720, train_perplexity=253.81238, train_loss=5.5365953

Batch 93730, train_perplexity=253.56863, train_loss=5.5356345

Batch 93740, train_perplexity=249.4588, train_loss=5.519294

Batch 93750, train_perplexity=261.90533, train_loss=5.567983

Batch 93760, train_perplexity=258.5793, train_loss=5.5552025

Batch 93770, train_perplexity=260.813, train_loss=5.5638037

Batch 93780, train_perplexity=249.66383, train_loss=5.5201154

Batch 93790, train_perplexity=212.1659, train_loss=5.3573685

Batch 93800, train_perplexity=235.2237, train_loss=5.460537

Batch 93810, train_perplexity=235.5511, train_loss=5.461928

Batch 93820, train_perplexity=251.6149, train_loss=5.5278997

Batch 93830, train_perplexity=254.64143, train_loss=5.5398564

Batch 93840, train_perplexity=259.18283, train_loss=5.5575337

Batch 93850, train_perplexity=307.82907, train_loss=5.7295446

Batch 93860, train_perplexity=269.21182, train_loss=5.5954986

Batch 93870, train_perplexity=245.98322, train_loss=5.5052633

Batch 93880, train_perplexity=271.84998, train_loss=5.6052504

Batch 93890, train_perplexity=236.02478, train_loss=5.463937

Batch 93900, train_perplexity=287.06305, train_loss=5.659702

Batch 93910, train_perplexity=253.98322, train_loss=5.537268

Batch 93920, train_perplexity=246.89204, train_loss=5.508951

Batch 93930, train_perplexity=273.7391, train_loss=5.6121755

Batch 93940, train_perplexity=232.34317, train_loss=5.4482155

Batch 93950, train_perplexity=222.40308, train_loss=5.4044914

Batch 93960, train_perplexity=255.67258, train_loss=5.5438976

Batch 93970, train_perplexity=254.78026, train_loss=5.5404015

Batch 93980, train_perplexity=242.778, train_loss=5.4921474

Batch 93990, train_perplexity=297.97626, train_loss=5.697014

Batch 94000, train_perplexity=247.7923, train_loss=5.512591

Batch 94010, train_perplexity=265.3226, train_loss=5.5809464

Batch 94020, train_perplexity=265.11002, train_loss=5.580145

Batch 94030, train_perplexity=258.20685, train_loss=5.553761

Batch 94040, train_perplexity=264.04446, train_loss=5.5761175

Batch 94050, train_perplexity=237.79883, train_loss=5.471425

Batch 94060, train_perplexity=260.9221, train_loss=5.564222

Batch 94070, train_perplexity=254.35323, train_loss=5.538724

Batch 94080, train_perplexity=245.20177, train_loss=5.5020814

Batch 94090, train_perplexity=265.22937, train_loss=5.580595

Batch 94100, train_perplexity=237.06644, train_loss=5.4683404

Batch 94110, train_perplexity=245.61263, train_loss=5.5037556

Batch 94120, train_perplexity=247.57912, train_loss=5.51173

Batch 94130, train_perplexity=243.90108, train_loss=5.4967628

Batch 94140, train_perplexity=257.3397, train_loss=5.550397

Batch 94150, train_perplexity=260.46527, train_loss=5.5624695

Batch 94160, train_perplexity=254.90361, train_loss=5.5408854

Batch 94170, train_perplexity=262.29865, train_loss=5.5694838

Batch 94180, train_perplexity=256.54623, train_loss=5.547309

Batch 94190, train_perplexity=278.41486, train_loss=5.6291122

Batch 94200, train_perplexity=295.65936, train_loss=5.689208
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 94210, train_perplexity=283.77304, train_loss=5.648175

Batch 94220, train_perplexity=240.30702, train_loss=5.4819174

Batch 94230, train_perplexity=256.9445, train_loss=5.54886

Batch 94240, train_perplexity=247.29724, train_loss=5.510591

Batch 94250, train_perplexity=231.75584, train_loss=5.4456844

Batch 94260, train_perplexity=244.85593, train_loss=5.50067

Batch 94270, train_perplexity=287.60437, train_loss=5.661586

Batch 94280, train_perplexity=272.31964, train_loss=5.6069765

Batch 94290, train_perplexity=272.1205, train_loss=5.606245

Batch 94300, train_perplexity=261.86325, train_loss=5.5678225

Batch 94310, train_perplexity=245.33217, train_loss=5.502613

Batch 94320, train_perplexity=262.40936, train_loss=5.5699058

Batch 94330, train_perplexity=270.93686, train_loss=5.601886

Batch 94340, train_perplexity=249.50186, train_loss=5.5194664

Batch 94350, train_perplexity=278.004, train_loss=5.6276355

Batch 94360, train_perplexity=282.90067, train_loss=5.645096

Batch 94370, train_perplexity=261.0194, train_loss=5.5645947

Batch 94380, train_perplexity=280.99493, train_loss=5.6383367

Batch 94390, train_perplexity=249.92743, train_loss=5.5211706

Batch 94400, train_perplexity=241.77696, train_loss=5.4880157

Batch 94410, train_perplexity=247.06293, train_loss=5.509643

Batch 94420, train_perplexity=272.574, train_loss=5.60791

Batch 94430, train_perplexity=245.54822, train_loss=5.5034933

Batch 94440, train_perplexity=280.90356, train_loss=5.6380115

Batch 94450, train_perplexity=307.21875, train_loss=5.72756

Batch 94460, train_perplexity=238.60141, train_loss=5.4747944

Batch 94470, train_perplexity=255.15204, train_loss=5.5418596

Batch 94480, train_perplexity=270.02036, train_loss=5.5984974

Batch 94490, train_perplexity=274.9286, train_loss=5.6165113

Batch 94500, train_perplexity=227.37843, train_loss=5.4266157

Batch 94510, train_perplexity=245.85962, train_loss=5.5047607

Batch 94520, train_perplexity=283.07285, train_loss=5.6457043

Batch 94530, train_perplexity=258.68317, train_loss=5.555604

Batch 94540, train_perplexity=239.76106, train_loss=5.479643

Batch 94550, train_perplexity=239.92061, train_loss=5.480308

Batch 94560, train_perplexity=256.95453, train_loss=5.548899

Batch 94570, train_perplexity=268.35864, train_loss=5.5923243

Batch 94580, train_perplexity=243.97401, train_loss=5.4970617

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Batch 94590, train_perplexity=262.41687, train_loss=5.5699344

Batch 94600, train_perplexity=279.14224, train_loss=5.6317215

Batch 94610, train_perplexity=235.12659, train_loss=5.460124

Batch 94620, train_perplexity=238.75209, train_loss=5.4754257

Batch 94630, train_perplexity=229.86626, train_loss=5.4374976

Batch 94640, train_perplexity=229.40875, train_loss=5.4355054

Batch 94650, train_perplexity=251.84032, train_loss=5.5287952

Batch 94660, train_perplexity=249.83867, train_loss=5.5208154

Batch 94670, train_perplexity=243.0305, train_loss=5.493187

Batch 94680, train_perplexity=256.3704, train_loss=5.546623

Batch 94690, train_perplexity=240.09377, train_loss=5.4810295

Batch 94700, train_perplexity=267.43555, train_loss=5.5888786

Batch 94710, train_perplexity=260.3587, train_loss=5.5620604

Batch 94720, train_perplexity=256.39044, train_loss=5.5467014

Batch 94730, train_perplexity=326.60318, train_loss=5.788746

Batch 94740, train_perplexity=270.30637, train_loss=5.599556

Batch 94750, train_perplexity=244.6483, train_loss=5.4998217

Batch 94760, train_perplexity=263.85254, train_loss=5.5753903

Batch 94770, train_perplexity=254.66573, train_loss=5.539952

Batch 94780, train_perplexity=256.65024, train_loss=5.547714

Batch 94790, train_perplexity=260.46426, train_loss=5.5624657

Batch 94800, train_perplexity=252.90474, train_loss=5.533013

Batch 94810, train_perplexity=254.70216, train_loss=5.540095

Batch 94820, train_perplexity=253.59656, train_loss=5.5357447

Batch 94830, train_perplexity=249.71646, train_loss=5.520326

Batch 94840, train_perplexity=252.59596, train_loss=5.531791

Batch 94850, train_perplexity=257.15176, train_loss=5.5496664

Batch 94860, train_perplexity=250.45653, train_loss=5.5232854

Batch 94870, train_perplexity=265.34955, train_loss=5.581048

Batch 94880, train_perplexity=254.58669, train_loss=5.5396414

Batch 94890, train_perplexity=270.03375, train_loss=5.598547

Batch 94900, train_perplexity=220.91638, train_loss=5.397784

Batch 94910, train_perplexity=253.80875, train_loss=5.536581

Batch 94920, train_perplexity=222.46355, train_loss=5.404763

Batch 94930, train_perplexity=238.41557, train_loss=5.474015

Batch 94940, train_perplexity=259.9863, train_loss=5.560629

Batch 94950, train_perplexity=254.5579, train_loss=5.5395284

Batch 94960, train_perplexity=267.16406, train_loss=5.587863

Batch 94970, train_perplexity=230.68835, train_loss=5.4410677

Batch 94980, train_perplexity=245.61005, train_loss=5.503745

Batch 94990, train_perplexity=277.9441, train_loss=5.62742

Batch 95000, train_perplexity=290.91644, train_loss=5.673036

Batch 95010, train_perplexity=245.56824, train_loss=5.503575

Batch 95020, train_perplexity=220.24164, train_loss=5.3947253

Batch 95030, train_perplexity=260.45334, train_loss=5.5624237

Batch 95040, train_perplexity=247.17523, train_loss=5.5100975

Batch 95050, train_perplexity=251.2781, train_loss=5.5265603

Batch 95060, train_perplexity=259.06088, train_loss=5.557063

Batch 95070, train_perplexity=265.12393, train_loss=5.5801973

Batch 95080, train_perplexity=251.44112, train_loss=5.527209

Batch 95090, train_perplexity=220.33167, train_loss=5.395134

Batch 95100, train_perplexity=264.528, train_loss=5.577947

Batch 95110, train_perplexity=266.57184, train_loss=5.585644

Batch 95120, train_perplexity=269.7136, train_loss=5.5973606

Batch 95130, train_perplexity=261.38123, train_loss=5.56598

Batch 95140, train_perplexity=218.55074, train_loss=5.387018

Batch 95150, train_perplexity=232.43071, train_loss=5.448592

Batch 95160, train_perplexity=249.03415, train_loss=5.51759

Batch 95170, train_perplexity=229.94485, train_loss=5.4378395

Batch 95180, train_perplexity=263.6474, train_loss=5.5746126

Batch 95190, train_perplexity=269.692, train_loss=5.5972805

Batch 95200, train_perplexity=267.15057, train_loss=5.5878124

Batch 95210, train_perplexity=243.57635, train_loss=5.4954305

Batch 95220, train_perplexity=245.81895, train_loss=5.5045953

Batch 95230, train_perplexity=256.30597, train_loss=5.546372

Batch 95240, train_perplexity=233.36223, train_loss=5.452592

Batch 95250, train_perplexity=245.07143, train_loss=5.5015497

Batch 95260, train_perplexity=251.72302, train_loss=5.5283294

Batch 95270, train_perplexity=277.3286, train_loss=5.625203

Batch 95280, train_perplexity=289.51254, train_loss=5.6681986

Batch 95290, train_perplexity=270.91928, train_loss=5.601821

Batch 95300, train_perplexity=250.84605, train_loss=5.5248394

Batch 95310, train_perplexity=233.7294, train_loss=5.454164

Batch 95320, train_perplexity=262.10437, train_loss=5.5687428

Batch 95330, train_perplexity=243.03676, train_loss=5.4932127

Batch 95340, train_perplexity=292.0733, train_loss=5.677005

Batch 95350, train_perplexity=248.5979, train_loss=5.5158367

Batch 95360, train_perplexity=254.92973, train_loss=5.540988

Batch 95370, train_perplexity=219.84598, train_loss=5.392927

Batch 95380, train_perplexity=243.29741, train_loss=5.4942846

Batch 95390, train_perplexity=243.4462, train_loss=5.494896

Batch 95400, train_perplexity=252.03445, train_loss=5.529566

Batch 95410, train_perplexity=264.63513, train_loss=5.578352

Batch 95420, train_perplexity=269.3911, train_loss=5.596164

Batch 95430, train_perplexity=250.3979, train_loss=5.5230513

Batch 95440, train_perplexity=232.06438, train_loss=5.447015

Batch 95450, train_perplexity=261.6088, train_loss=5.56685

Batch 95460, train_perplexity=250.39384, train_loss=5.523035
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 95470, train_perplexity=256.23474, train_loss=5.546094

Batch 95480, train_perplexity=230.77803, train_loss=5.4414563

Batch 95490, train_perplexity=258.12918, train_loss=5.55346

Batch 95500, train_perplexity=257.8632, train_loss=5.552429

Batch 95510, train_perplexity=256.26602, train_loss=5.546216

Batch 95520, train_perplexity=255.28152, train_loss=5.542367

Batch 95530, train_perplexity=238.78807, train_loss=5.4755764

Batch 95540, train_perplexity=253.6897, train_loss=5.536112

Batch 95550, train_perplexity=272.32327, train_loss=5.60699

Batch 95560, train_perplexity=248.83105, train_loss=5.516774

Batch 95570, train_perplexity=248.47108, train_loss=5.5153265

Batch 95580, train_perplexity=271.25836, train_loss=5.6030717

Batch 95590, train_perplexity=242.05473, train_loss=5.489164

Batch 95600, train_perplexity=268.19974, train_loss=5.591732

Batch 95610, train_perplexity=247.5424, train_loss=5.511582

Batch 95620, train_perplexity=246.46942, train_loss=5.507238

Batch 95630, train_perplexity=255.8621, train_loss=5.5446386

Batch 95640, train_perplexity=261.08948, train_loss=5.564863

Batch 95650, train_perplexity=241.72336, train_loss=5.487794

Batch 95660, train_perplexity=260.68295, train_loss=5.563305

Batch 95670, train_perplexity=277.43323, train_loss=5.6255803

Batch 95680, train_perplexity=217.90349, train_loss=5.3840523

Batch 95690, train_perplexity=217.36769, train_loss=5.3815904

Batch 95700, train_perplexity=256.43518, train_loss=5.546876

Batch 95710, train_perplexity=251.25186, train_loss=5.526456

Batch 95720, train_perplexity=243.14607, train_loss=5.4936624

Batch 95730, train_perplexity=256.93457, train_loss=5.5488214

Batch 95740, train_perplexity=262.64722, train_loss=5.5708117

Batch 95750, train_perplexity=245.93349, train_loss=5.505061

Batch 95760, train_perplexity=244.23207, train_loss=5.498119

Batch 95770, train_perplexity=243.28848, train_loss=5.494248

Batch 95780, train_perplexity=228.71397, train_loss=5.432472

Batch 95790, train_perplexity=300.42575, train_loss=5.7052007

Batch 95800, train_perplexity=256.2934, train_loss=5.546323

Batch 95810, train_perplexity=252.51479, train_loss=5.53147

Batch 95820, train_perplexity=257.36337, train_loss=5.550489

Batch 95830, train_perplexity=250.59511, train_loss=5.5238385

Batch 95840, train_perplexity=257.51105, train_loss=5.5510626

Batch 95850, train_perplexity=262.6303, train_loss=5.5707474

Batch 95860, train_perplexity=244.45578, train_loss=5.4990344

Batch 95870, train_perplexity=255.73524, train_loss=5.5441427

Batch 95880, train_perplexity=257.966, train_loss=5.552828

Batch 95890, train_perplexity=265.0602, train_loss=5.579957

Batch 95900, train_perplexity=269.0218, train_loss=5.5947924

Batch 95910, train_perplexity=242.6766, train_loss=5.4917297

Batch 95920, train_perplexity=238.26634, train_loss=5.473389

Batch 95930, train_perplexity=295.66318, train_loss=5.689221

Batch 95940, train_perplexity=248.01915, train_loss=5.513506

Batch 95950, train_perplexity=235.0146, train_loss=5.4596477

Batch 95960, train_perplexity=256.54367, train_loss=5.547299

Batch 95970, train_perplexity=257.94485, train_loss=5.552746

Batch 95980, train_perplexity=258.95416, train_loss=5.556651

Batch 95990, train_perplexity=234.71368, train_loss=5.4583664

Batch 96000, train_perplexity=286.38776, train_loss=5.6573467

Batch 96010, train_perplexity=293.53876, train_loss=5.6820097

Batch 96020, train_perplexity=236.96562, train_loss=5.467915

Batch 96030, train_perplexity=284.72058, train_loss=5.6515083

Batch 96040, train_perplexity=275.1014, train_loss=5.61714

Batch 96050, train_perplexity=243.3241, train_loss=5.4943943

Batch 96060, train_perplexity=240.73665, train_loss=5.4837036

Batch 96070, train_perplexity=306.9139, train_loss=5.7265673

Batch 96080, train_perplexity=250.00705, train_loss=5.521489

Batch 96090, train_perplexity=256.2506, train_loss=5.546156

Batch 96100, train_perplexity=273.33566, train_loss=5.6107006

Batch 96110, train_perplexity=227.26765, train_loss=5.4261284

Batch 96120, train_perplexity=259.9212, train_loss=5.5603786

Batch 96130, train_perplexity=229.0129, train_loss=5.4337783

Batch 96140, train_perplexity=288.0614, train_loss=5.6631737

Batch 96150, train_perplexity=266.53397, train_loss=5.5855017

Batch 96160, train_perplexity=252.56212, train_loss=5.531657

Batch 96170, train_perplexity=262.96866, train_loss=5.572035

Batch 96180, train_perplexity=245.6891, train_loss=5.504067

Batch 96190, train_perplexity=237.56015, train_loss=5.470421

Batch 96200, train_perplexity=244.61424, train_loss=5.4996824

Batch 96210, train_perplexity=263.46643, train_loss=5.573926

Batch 96220, train_perplexity=240.42577, train_loss=5.4824114

Batch 96230, train_perplexity=261.67477, train_loss=5.5671024

Batch 96240, train_perplexity=290.55753, train_loss=5.6718016

Batch 96250, train_perplexity=236.07768, train_loss=5.464161

Batch 96260, train_perplexity=266.5918, train_loss=5.5857186

Batch 96270, train_perplexity=277.73343, train_loss=5.626662

Batch 96280, train_perplexity=243.12532, train_loss=5.493577

Batch 96290, train_perplexity=249.8955, train_loss=5.521043

Batch 96300, train_perplexity=285.2533, train_loss=5.6533775

Batch 96310, train_perplexity=238.67036, train_loss=5.4750834

Batch 96320, train_perplexity=245.39558, train_loss=5.5028715

Batch 96330, train_perplexity=247.68515, train_loss=5.5121584

Batch 96340, train_perplexity=272.91983, train_loss=5.609178

Batch 96350, train_perplexity=270.31976, train_loss=5.5996056

Batch 96360, train_perplexity=243.52444, train_loss=5.4952173

Batch 96370, train_perplexity=260.07333, train_loss=5.5609636

Batch 96380, train_perplexity=240.4519, train_loss=5.48252

Batch 96390, train_perplexity=252.98615, train_loss=5.5333347

Batch 96400, train_perplexity=247.12337, train_loss=5.5098877

Batch 96410, train_perplexity=279.41885, train_loss=5.632712

Batch 96420, train_perplexity=271.09363, train_loss=5.602464

Batch 96430, train_perplexity=262.8251, train_loss=5.571489

Batch 96440, train_perplexity=259.39105, train_loss=5.5583367

Batch 96450, train_perplexity=257.3954, train_loss=5.5506134

Batch 96460, train_perplexity=241.39209, train_loss=5.4864225

Batch 96470, train_perplexity=273.88116, train_loss=5.6126943

Batch 96480, train_perplexity=250.88696, train_loss=5.5250025

Batch 96490, train_perplexity=264.3027, train_loss=5.577095

Batch 96500, train_perplexity=223.76639, train_loss=5.4106026

Batch 96510, train_perplexity=260.1052, train_loss=5.561086

Batch 96520, train_perplexity=267.76324, train_loss=5.590103

Batch 96530, train_perplexity=246.71034, train_loss=5.508215

Batch 96540, train_perplexity=260.99115, train_loss=5.5644865

Batch 96550, train_perplexity=253.04237, train_loss=5.533557

Batch 96560, train_perplexity=255.71611, train_loss=5.544068

Batch 96570, train_perplexity=244.18619, train_loss=5.497931

Batch 96580, train_perplexity=247.92265, train_loss=5.513117

Batch 96590, train_perplexity=253.15123, train_loss=5.533987

Batch 96600, train_perplexity=257.05933, train_loss=5.549307

Batch 96610, train_perplexity=239.02411, train_loss=5.4765644

Batch 96620, train_perplexity=233.69185, train_loss=5.4540033

Batch 96630, train_perplexity=248.23744, train_loss=5.5143857

Batch 96640, train_perplexity=254.36511, train_loss=5.5387707

Batch 96650, train_perplexity=247.31482, train_loss=5.510662

Batch 96660, train_perplexity=243.02551, train_loss=5.4931664

Batch 96670, train_perplexity=252.58669, train_loss=5.5317545

Batch 96680, train_perplexity=243.53397, train_loss=5.4952564

Batch 96690, train_perplexity=234.59082, train_loss=5.457843

Batch 96700, train_perplexity=250.56248, train_loss=5.5237083

Batch 96710, train_perplexity=253.07314, train_loss=5.5336785

Batch 96720, train_perplexity=278.88495, train_loss=5.6307993

Batch 96730, train_perplexity=239.83424, train_loss=5.479948

Batch 96740, train_perplexity=267.41452, train_loss=5.5888

Batch 96750, train_perplexity=256.07645, train_loss=5.545476

Batch 96760, train_perplexity=271.334, train_loss=5.6033506

Batch 96770, train_perplexity=232.62753, train_loss=5.4494386

Batch 96780, train_perplexity=245.19861, train_loss=5.5020685
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 96790, train_perplexity=229.66432, train_loss=5.436619

Batch 96800, train_perplexity=258.08942, train_loss=5.553306

Batch 96810, train_perplexity=261.7846, train_loss=5.567522

Batch 96820, train_perplexity=234.829, train_loss=5.4588575

Batch 96830, train_perplexity=245.35533, train_loss=5.5027075

Batch 96840, train_perplexity=263.41907, train_loss=5.573746

Batch 96850, train_perplexity=220.12962, train_loss=5.3942165

Batch 96860, train_perplexity=261.23245, train_loss=5.5654106

Batch 96870, train_perplexity=270.32104, train_loss=5.5996103

Batch 96880, train_perplexity=278.86472, train_loss=5.630727

Batch 96890, train_perplexity=244.86853, train_loss=5.5007215

Batch 96900, train_perplexity=261.02588, train_loss=5.5646195

Batch 96910, train_perplexity=234.96272, train_loss=5.459427

Batch 96920, train_perplexity=229.07678, train_loss=5.434057

Batch 96930, train_perplexity=233.25722, train_loss=5.452142

Batch 96940, train_perplexity=275.02496, train_loss=5.616862

Batch 96950, train_perplexity=238.5112, train_loss=5.4744163

Batch 96960, train_perplexity=231.73, train_loss=5.445573

Batch 96970, train_perplexity=243.17865, train_loss=5.4937963

Batch 96980, train_perplexity=271.0136, train_loss=5.602169

Batch 96990, train_perplexity=241.24905, train_loss=5.48583

Batch 97000, train_perplexity=260.2227, train_loss=5.5615377

Batch 97010, train_perplexity=264.96317, train_loss=5.579591

Batch 97020, train_perplexity=248.22548, train_loss=5.5143375

Batch 97030, train_perplexity=247.13776, train_loss=5.509946

Batch 97040, train_perplexity=295.7987, train_loss=5.689679

Batch 97050, train_perplexity=287.64853, train_loss=5.6617393

Batch 97060, train_perplexity=268.156, train_loss=5.591569

Batch 97070, train_perplexity=232.92688, train_loss=5.4507246

Batch 97080, train_perplexity=253.94772, train_loss=5.5371284

Batch 97090, train_perplexity=256.82605, train_loss=5.548399

Batch 97100, train_perplexity=255.08708, train_loss=5.541605

Batch 97110, train_perplexity=257.34607, train_loss=5.5504217

Batch 97120, train_perplexity=243.63083, train_loss=5.495654

Batch 97130, train_perplexity=265.6592, train_loss=5.5822144

Batch 97140, train_perplexity=240.96129, train_loss=5.4846363

Batch 97150, train_perplexity=237.95535, train_loss=5.472083

Batch 97160, train_perplexity=244.15126, train_loss=5.497788

Batch 97170, train_perplexity=268.67566, train_loss=5.593505

Batch 97180, train_perplexity=220.01083, train_loss=5.3936768

Batch 97190, train_perplexity=262.79416, train_loss=5.571371

Batch 97200, train_perplexity=284.3719, train_loss=5.650283

Batch 97210, train_perplexity=219.44861, train_loss=5.391118

Batch 97220, train_perplexity=235.56447, train_loss=5.4619846

Batch 97230, train_perplexity=291.2658, train_loss=5.6742363

Batch 97240, train_perplexity=249.2865, train_loss=5.518603

Batch 97250, train_perplexity=242.29527, train_loss=5.490157

Batch 97260, train_perplexity=219.91518, train_loss=5.393242

Batch 97270, train_perplexity=295.6684, train_loss=5.6892385

Batch 97280, train_perplexity=253.01305, train_loss=5.533441

Batch 97290, train_perplexity=279.10977, train_loss=5.631605

Batch 97300, train_perplexity=245.66556, train_loss=5.503971

Batch 97310, train_perplexity=288.18423, train_loss=5.6636

Batch 97320, train_perplexity=262.19473, train_loss=5.5690875

Batch 97330, train_perplexity=254.71782, train_loss=5.5401564

Batch 97340, train_perplexity=270.55496, train_loss=5.6004753

Batch 97350, train_perplexity=264.30246, train_loss=5.577094

Batch 97360, train_perplexity=263.36847, train_loss=5.573554

Batch 97370, train_perplexity=234.71255, train_loss=5.4583616

Batch 97380, train_perplexity=236.01398, train_loss=5.463891

Batch 97390, train_perplexity=242.43199, train_loss=5.490721

Batch 97400, train_perplexity=222.00449, train_loss=5.4026976

Batch 97410, train_perplexity=240.73586, train_loss=5.4837003

Batch 97420, train_perplexity=217.08833, train_loss=5.3803043

Batch 97430, train_perplexity=265.6197, train_loss=5.5820656

Batch 97440, train_perplexity=251.49771, train_loss=5.527434

Batch 97450, train_perplexity=243.58948, train_loss=5.4954844

Batch 97460, train_perplexity=247.92656, train_loss=5.5131326

Batch 97470, train_perplexity=255.70439, train_loss=5.544022

Batch 97480, train_perplexity=253.09932, train_loss=5.533782

Batch 97490, train_perplexity=246.3034, train_loss=5.506564

Batch 97500, train_perplexity=253.2384, train_loss=5.5343313

Batch 97510, train_perplexity=265.36118, train_loss=5.581092

Batch 97520, train_perplexity=243.7661, train_loss=5.496209

Batch 97530, train_perplexity=242.92252, train_loss=5.4927425

Batch 97540, train_perplexity=265.90915, train_loss=5.5831547

Batch 97550, train_perplexity=266.29538, train_loss=5.584606

Batch 97560, train_perplexity=243.29997, train_loss=5.494295

Batch 97570, train_perplexity=275.45294, train_loss=5.618417

Batch 97580, train_perplexity=267.33203, train_loss=5.5884914

Batch 97590, train_perplexity=229.3883, train_loss=5.435416

Batch 97600, train_perplexity=217.76659, train_loss=5.383424

Batch 97610, train_perplexity=264.3652, train_loss=5.5773315

Batch 97620, train_perplexity=262.51688, train_loss=5.5703154

Batch 97630, train_perplexity=257.30334, train_loss=5.550256

Batch 97640, train_perplexity=244.95413, train_loss=5.501071

Batch 97650, train_perplexity=253.46793, train_loss=5.5352373

Batch 97660, train_perplexity=244.7113, train_loss=5.500079

Batch 97670, train_perplexity=265.8426, train_loss=5.5829043

Batch 97680, train_perplexity=238.12437, train_loss=5.472793

Batch 97690, train_perplexity=251.21173, train_loss=5.526296

Batch 97700, train_perplexity=225.7988, train_loss=5.4196444

Batch 97710, train_perplexity=257.24362, train_loss=5.5500236

Batch 97720, train_perplexity=268.5659, train_loss=5.5930963

Batch 97730, train_perplexity=252.1484, train_loss=5.530018

Batch 97740, train_perplexity=237.17633, train_loss=5.468804

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Batch 97750, train_perplexity=247.81416, train_loss=5.512679

Batch 97760, train_perplexity=264.1243, train_loss=5.57642

Batch 97770, train_perplexity=249.52519, train_loss=5.51956

Batch 97780, train_perplexity=229.92775, train_loss=5.437765

Batch 97790, train_perplexity=210.71808, train_loss=5.350521

Batch 97800, train_perplexity=279.8091, train_loss=5.6341076

Batch 97810, train_perplexity=276.2835, train_loss=5.6214275

Batch 97820, train_perplexity=243.10028, train_loss=5.493474

Batch 97830, train_perplexity=239.01909, train_loss=5.4765434

Batch 97840, train_perplexity=221.55736, train_loss=5.4006815

Batch 97850, train_perplexity=261.5544, train_loss=5.5666423

Batch 97860, train_perplexity=281.56616, train_loss=5.6403675

Batch 97870, train_perplexity=247.5411, train_loss=5.5115767

Batch 97880, train_perplexity=292.95062, train_loss=5.680004

Batch 97890, train_perplexity=245.48651, train_loss=5.503242

Batch 97900, train_perplexity=284.2587, train_loss=5.6498847

Batch 97910, train_perplexity=261.43307, train_loss=5.5661783

Batch 97920, train_perplexity=247.9884, train_loss=5.513382

Batch 97930, train_perplexity=212.68755, train_loss=5.359824

Batch 97940, train_perplexity=273.73688, train_loss=5.6121674

Batch 97950, train_perplexity=254.06897, train_loss=5.537606

Batch 97960, train_perplexity=266.8874, train_loss=5.586827

Batch 97970, train_perplexity=252.47133, train_loss=5.5312977

Batch 97980, train_perplexity=258.2976, train_loss=5.5541124

Batch 97990, train_perplexity=246.023, train_loss=5.505425

Batch 98000, train_perplexity=259.75406, train_loss=5.5597353

Batch 98010, train_perplexity=210.54733, train_loss=5.3497105

Batch 98020, train_perplexity=249.28543, train_loss=5.5185986

Batch 98030, train_perplexity=261.63324, train_loss=5.5669436

Batch 98040, train_perplexity=222.48476, train_loss=5.4048586
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 98050, train_perplexity=268.9846, train_loss=5.594654

Batch 98060, train_perplexity=273.71417, train_loss=5.6120844

Batch 98070, train_perplexity=256.7292, train_loss=5.548022

Batch 98080, train_perplexity=229.071, train_loss=5.434032

Batch 98090, train_perplexity=254.93095, train_loss=5.5409927

Batch 98100, train_perplexity=247.53296, train_loss=5.5115438

Batch 98110, train_perplexity=248.0836, train_loss=5.513766

Batch 98120, train_perplexity=247.60439, train_loss=5.511832

Batch 98130, train_perplexity=278.9907, train_loss=5.6311784

Batch 98140, train_perplexity=262.52576, train_loss=5.570349

Batch 98150, train_perplexity=264.09747, train_loss=5.5763183

Batch 98160, train_perplexity=256.66028, train_loss=5.5477533

Batch 98170, train_perplexity=252.34712, train_loss=5.5308056

Batch 98180, train_perplexity=245.2793, train_loss=5.5023975

Batch 98190, train_perplexity=269.38763, train_loss=5.5961514

Batch 98200, train_perplexity=250.9633, train_loss=5.5253067

Batch 98210, train_perplexity=238.43445, train_loss=5.4740944

Batch 98220, train_perplexity=267.7691, train_loss=5.590125

Batch 98230, train_perplexity=259.0537, train_loss=5.5570354

Batch 98240, train_perplexity=248.767, train_loss=5.5165167

Batch 98250, train_perplexity=249.61777, train_loss=5.519931

Batch 98260, train_perplexity=300.6357, train_loss=5.7058992

Batch 98270, train_perplexity=237.50307, train_loss=5.4701805

Batch 98280, train_perplexity=242.72406, train_loss=5.4919252

Batch 98290, train_perplexity=248.93787, train_loss=5.5172033

Batch 98300, train_perplexity=256.14212, train_loss=5.5457325

Batch 98310, train_perplexity=256.93884, train_loss=5.548838

Batch 98320, train_perplexity=250.90575, train_loss=5.5250773

Batch 98330, train_perplexity=277.7378, train_loss=5.6266775

Batch 98340, train_perplexity=250.7059, train_loss=5.5242805

Batch 98350, train_perplexity=232.83804, train_loss=5.450343

Batch 98360, train_perplexity=253.12805, train_loss=5.5338955

Batch 98370, train_perplexity=251.12155, train_loss=5.525937

Batch 98380, train_perplexity=258.38705, train_loss=5.5544586

Batch 98390, train_perplexity=258.92404, train_loss=5.556535

Batch 98400, train_perplexity=246.62166, train_loss=5.5078554

Batch 98410, train_perplexity=244.7575, train_loss=5.500268

Batch 98420, train_perplexity=255.99255, train_loss=5.5451484

Batch 98430, train_perplexity=285.4013, train_loss=5.6538963

Batch 98440, train_perplexity=252.79996, train_loss=5.5325985

Batch 98450, train_perplexity=254.71855, train_loss=5.540159

Batch 98460, train_perplexity=255.37637, train_loss=5.5427384

Batch 98470, train_perplexity=228.56223, train_loss=5.4318085

Batch 98480, train_perplexity=278.8299, train_loss=5.630602

Batch 98490, train_perplexity=249.27592, train_loss=5.5185604

Batch 98500, train_perplexity=319.90756, train_loss=5.768032

Batch 98510, train_perplexity=243.5479, train_loss=5.4953136

Batch 98520, train_perplexity=253.09293, train_loss=5.5337567

Batch 98530, train_perplexity=259.96707, train_loss=5.560555

Batch 98540, train_perplexity=240.76811, train_loss=5.4838343

Batch 98550, train_perplexity=268.42172, train_loss=5.5925593

Batch 98560, train_perplexity=248.10739, train_loss=5.5138617

Batch 98570, train_perplexity=240.21172, train_loss=5.4815207

Batch 98580, train_perplexity=236.56111, train_loss=5.4662066

Batch 98590, train_perplexity=272.6286, train_loss=5.6081104

Batch 98600, train_perplexity=287.16367, train_loss=5.6600523

Batch 98610, train_perplexity=277.95932, train_loss=5.627475

Batch 98620, train_perplexity=251.3705, train_loss=5.526928

Batch 98630, train_perplexity=238.52292, train_loss=5.4744654

Batch 98640, train_perplexity=247.35928, train_loss=5.510842

Batch 98650, train_perplexity=246.35861, train_loss=5.5067883

Batch 98660, train_perplexity=276.87488, train_loss=5.6235657

Batch 98670, train_perplexity=250.47934, train_loss=5.5233765

Batch 98680, train_perplexity=271.46887, train_loss=5.6038475

Batch 98690, train_perplexity=243.0874, train_loss=5.493421

Batch 98700, train_perplexity=277.2949, train_loss=5.6250815

Batch 98710, train_perplexity=250.89043, train_loss=5.5250163

Batch 98720, train_perplexity=257.1308, train_loss=5.549585

Batch 98730, train_perplexity=264.05655, train_loss=5.5761633

Batch 98740, train_perplexity=234.07828, train_loss=5.4556556

Batch 98750, train_perplexity=231.32228, train_loss=5.443812

Batch 98760, train_perplexity=249.26761, train_loss=5.518527

Batch 98770, train_perplexity=260.00787, train_loss=5.560712

Batch 98780, train_perplexity=254.37979, train_loss=5.5388284

Batch 98790, train_perplexity=254.08339, train_loss=5.5376625

Batch 98800, train_perplexity=241.60466, train_loss=5.487303

Batch 98810, train_perplexity=268.29977, train_loss=5.592105

Batch 98820, train_perplexity=251.82063, train_loss=5.528717

Batch 98830, train_perplexity=273.9526, train_loss=5.612955

Batch 98840, train_perplexity=251.00291, train_loss=5.5254645

Batch 98850, train_perplexity=263.32126, train_loss=5.5733747

Batch 98860, train_perplexity=254.01143, train_loss=5.5373793

Batch 98870, train_perplexity=262.228, train_loss=5.5692143

Batch 98880, train_perplexity=233.77365, train_loss=5.4543533

Batch 98890, train_perplexity=242.44528, train_loss=5.490776

Batch 98900, train_perplexity=228.52605, train_loss=5.43165

Batch 98910, train_perplexity=255.02408, train_loss=5.541358

Batch 98920, train_perplexity=263.1439, train_loss=5.572701

Batch 98930, train_perplexity=236.53787, train_loss=5.4661083

Batch 98940, train_perplexity=254.66682, train_loss=5.539956

Batch 98950, train_perplexity=224.04868, train_loss=5.4118633

Batch 98960, train_perplexity=252.81538, train_loss=5.5326595

Batch 98970, train_perplexity=282.98917, train_loss=5.6454086

Batch 98980, train_perplexity=280.0395, train_loss=5.6349306

Batch 98990, train_perplexity=239.5757, train_loss=5.4788694

Batch 99000, train_perplexity=210.20305, train_loss=5.348074

Batch 99010, train_perplexity=255.96082, train_loss=5.5450244

Batch 99020, train_perplexity=282.68735, train_loss=5.6443415

Batch 99030, train_perplexity=245.09059, train_loss=5.501628

Batch 99040, train_perplexity=276.1672, train_loss=5.6210065

Batch 99050, train_perplexity=244.64223, train_loss=5.499797

Batch 99060, train_perplexity=253.28357, train_loss=5.5345097

Batch 99070, train_perplexity=264.47668, train_loss=5.577753

Batch 99080, train_perplexity=255.28105, train_loss=5.542365

Batch 99090, train_perplexity=241.2195, train_loss=5.4857073

Batch 99100, train_perplexity=233.80441, train_loss=5.454485

Batch 99110, train_perplexity=268.66028, train_loss=5.5934477

Batch 99120, train_perplexity=248.02884, train_loss=5.513545

Batch 99130, train_perplexity=223.89713, train_loss=5.4111867

Batch 99140, train_perplexity=246.65529, train_loss=5.507992

Batch 99150, train_perplexity=237.15756, train_loss=5.4687247

Batch 99160, train_perplexity=251.21617, train_loss=5.526314

Batch 99170, train_perplexity=234.58679, train_loss=5.4578257

Batch 99180, train_perplexity=246.7174, train_loss=5.5082436

Batch 99190, train_perplexity=246.81001, train_loss=5.508619

Batch 99200, train_perplexity=264.45258, train_loss=5.577662

Batch 99210, train_perplexity=244.08527, train_loss=5.4975176

Batch 99220, train_perplexity=246.60849, train_loss=5.507802

Batch 99230, train_perplexity=210.41826, train_loss=5.3490973

Batch 99240, train_perplexity=248.35168, train_loss=5.514846

Batch 99250, train_perplexity=257.59628, train_loss=5.5513935

Batch 99260, train_perplexity=235.14317, train_loss=5.4601946

Batch 99270, train_perplexity=245.85541, train_loss=5.5047436

Batch 99280, train_perplexity=246.00386, train_loss=5.5053473

Batch 99290, train_perplexity=231.4815, train_loss=5.4445

Batch 99300, train_perplexity=244.69963, train_loss=5.5000315

Batch 99310, train_perplexity=258.0179, train_loss=5.553029

Batch 99320, train_perplexity=229.18823, train_loss=5.4345436

Batch 99330, train_perplexity=244.24535, train_loss=5.498173

Batch 99340, train_perplexity=235.7953, train_loss=5.462964

Batch 99350, train_perplexity=243.51074, train_loss=5.495161

Batch 99360, train_perplexity=261.37024, train_loss=5.565938
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 99370, train_perplexity=287.4607, train_loss=5.661086

Batch 99380, train_perplexity=257.32053, train_loss=5.5503225

Batch 99390, train_perplexity=270.1718, train_loss=5.599058

Batch 99400, train_perplexity=210.52184, train_loss=5.3495893

Batch 99410, train_perplexity=248.01158, train_loss=5.5134754

Batch 99420, train_perplexity=231.61288, train_loss=5.4450674

Batch 99430, train_perplexity=256.8443, train_loss=5.54847

Batch 99440, train_perplexity=230.93709, train_loss=5.4421453

Batch 99450, train_perplexity=261.24615, train_loss=5.565463

Batch 99460, train_perplexity=277.322, train_loss=5.6251793

Batch 99470, train_perplexity=254.81088, train_loss=5.5405216

Batch 99480, train_perplexity=248.10526, train_loss=5.513853

Batch 99490, train_perplexity=239.49118, train_loss=5.4785166

Batch 99500, train_perplexity=270.02087, train_loss=5.5984993

Batch 99510, train_perplexity=260.0794, train_loss=5.560987

Batch 99520, train_perplexity=265.24884, train_loss=5.5806684

Batch 99530, train_perplexity=240.3107, train_loss=5.4819326

Batch 99540, train_perplexity=257.95886, train_loss=5.5528

Batch 99550, train_perplexity=249.25809, train_loss=5.518489

Batch 99560, train_perplexity=244.67455, train_loss=5.499929

Batch 99570, train_perplexity=231.8341, train_loss=5.446022

Batch 99580, train_perplexity=238.0322, train_loss=5.472406

Batch 99590, train_perplexity=251.9763, train_loss=5.529335

Batch 99600, train_perplexity=249.58516, train_loss=5.5198

Batch 99610, train_perplexity=238.63144, train_loss=5.4749203

Batch 99620, train_perplexity=221.88976, train_loss=5.4021807

Batch 99630, train_perplexity=227.65898, train_loss=5.427849

Batch 99640, train_perplexity=251.96921, train_loss=5.529307

Batch 99650, train_perplexity=224.22235, train_loss=5.412638

Batch 99660, train_perplexity=252.52672, train_loss=5.531517

Batch 99670, train_perplexity=273.52786, train_loss=5.6114035

Batch 99680, train_perplexity=245.0345, train_loss=5.501399

Batch 99690, train_perplexity=272.59323, train_loss=5.6079807

Batch 99700, train_perplexity=238.51314, train_loss=5.4744244

Batch 99710, train_perplexity=233.3905, train_loss=5.452713

Batch 99720, train_perplexity=245.32375, train_loss=5.5025787

Batch 99730, train_perplexity=241.19327, train_loss=5.4855986

Batch 99740, train_perplexity=274.49866, train_loss=5.6149464

Batch 99750, train_perplexity=233.23897, train_loss=5.4520636

Batch 99760, train_perplexity=233.65909, train_loss=5.453863

Batch 99770, train_perplexity=254.60878, train_loss=5.539728

Batch 99780, train_perplexity=239.36057, train_loss=5.477971

Batch 99790, train_perplexity=236.57375, train_loss=5.46626

Batch 99800, train_perplexity=244.63266, train_loss=5.499758

Batch 99810, train_perplexity=280.80487, train_loss=5.63766

Batch 99820, train_perplexity=229.28595, train_loss=5.43497

Batch 99830, train_perplexity=256.7761, train_loss=5.5482044

Batch 99840, train_perplexity=225.90111, train_loss=5.4200974

Batch 99850, train_perplexity=248.31924, train_loss=5.514715

Batch 99860, train_perplexity=205.05934, train_loss=5.3232994

Batch 99870, train_perplexity=248.35146, train_loss=5.514845

Batch 99880, train_perplexity=267.36975, train_loss=5.5886326

Batch 99890, train_perplexity=229.26036, train_loss=5.4348583

Batch 99900, train_perplexity=259.5437, train_loss=5.558925

Batch 99910, train_perplexity=243.25438, train_loss=5.4941077

Batch 99920, train_perplexity=236.33966, train_loss=5.46527

Batch 99930, train_perplexity=250.10805, train_loss=5.521893

Batch 99940, train_perplexity=245.51567, train_loss=5.5033607

Batch 99950, train_perplexity=237.20958, train_loss=5.468944

Batch 99960, train_perplexity=252.79152, train_loss=5.532565

Batch 99970, train_perplexity=243.41833, train_loss=5.4947815

Batch 99980, train_perplexity=256.81528, train_loss=5.548357

Batch 99990, train_perplexity=252.99701, train_loss=5.5333776

Batch 100000, train_perplexity=227.14966, train_loss=5.425609

Batch 100010, train_perplexity=256.9473, train_loss=5.548871

Batch 100020, train_perplexity=275.4553, train_loss=5.6184254

Batch 100030, train_perplexity=235.40929, train_loss=5.4613256

Batch 100040, train_perplexity=238.85081, train_loss=5.475839

Batch 100050, train_perplexity=258.07205, train_loss=5.553239

Batch 100060, train_perplexity=255.50378, train_loss=5.543237

Batch 100070, train_perplexity=251.21317, train_loss=5.526302

Batch 100080, train_perplexity=235.97357, train_loss=5.46372

Batch 100090, train_perplexity=264.48828, train_loss=5.577797

Batch 100100, train_perplexity=191.42764, train_loss=5.25451

Batch 100110, train_perplexity=256.57584, train_loss=5.5474243

Batch 100120, train_perplexity=247.44788, train_loss=5.5112

Batch 100130, train_perplexity=247.98367, train_loss=5.513363

Batch 100140, train_perplexity=234.01678, train_loss=5.455393

Batch 100150, train_perplexity=235.46878, train_loss=5.4615784

Batch 100160, train_perplexity=250.62941, train_loss=5.5239754

Batch 100170, train_perplexity=248.80875, train_loss=5.5166845

Batch 100180, train_perplexity=254.12228, train_loss=5.5378156

Batch 100190, train_perplexity=233.56206, train_loss=5.453448

Batch 100200, train_perplexity=243.44177, train_loss=5.494878

Batch 100210, train_perplexity=243.05681, train_loss=5.493295

Batch 100220, train_perplexity=247.48846, train_loss=5.511364

Batch 100230, train_perplexity=269.19373, train_loss=5.5954313

Batch 100240, train_perplexity=252.67752, train_loss=5.532114

Batch 100250, train_perplexity=248.75157, train_loss=5.5164547

Batch 100260, train_perplexity=267.50647, train_loss=5.5891438

Batch 100270, train_perplexity=242.76364, train_loss=5.4920883

Batch 100280, train_perplexity=279.1143, train_loss=5.6316214

Batch 100290, train_perplexity=217.27992, train_loss=5.3811865

Batch 100300, train_perplexity=234.08519, train_loss=5.455685

Batch 100310, train_perplexity=260.41632, train_loss=5.5622816

Batch 100320, train_perplexity=262.23087, train_loss=5.5692253

Batch 100330, train_perplexity=220.52422, train_loss=5.3960075

Batch 100340, train_perplexity=237.87436, train_loss=5.4717426

Batch 100350, train_perplexity=300.80606, train_loss=5.7064657

Batch 100360, train_perplexity=278.46558, train_loss=5.6292944

Batch 100370, train_perplexity=239.57101, train_loss=5.47885

Batch 100380, train_perplexity=250.24858, train_loss=5.5224547

Batch 100390, train_perplexity=250.08157, train_loss=5.521787

Batch 100400, train_perplexity=315.22882, train_loss=5.7532988

Batch 100410, train_perplexity=245.13817, train_loss=5.501822

Batch 100420, train_perplexity=250.74165, train_loss=5.524423

Batch 100430, train_perplexity=252.73463, train_loss=5.53234

Batch 100440, train_perplexity=228.37941, train_loss=5.4310083

Batch 100450, train_perplexity=248.01938, train_loss=5.513507

Batch 100460, train_perplexity=239.735, train_loss=5.479534

Batch 100470, train_perplexity=250.73914, train_loss=5.524413

Batch 100480, train_perplexity=244.73325, train_loss=5.500169

Batch 100490, train_perplexity=270.92548, train_loss=5.601844

Batch 100500, train_perplexity=244.25897, train_loss=5.498229

Batch 100510, train_perplexity=278.69006, train_loss=5.6301003

Batch 100520, train_perplexity=229.99046, train_loss=5.438038

Batch 100530, train_perplexity=235.18422, train_loss=5.460369

Batch 100540, train_perplexity=264.9235, train_loss=5.579441

Batch 100550, train_perplexity=242.6051, train_loss=5.491435

Batch 100560, train_perplexity=245.25099, train_loss=5.502282

Batch 100570, train_perplexity=242.55965, train_loss=5.4912477

Batch 100580, train_perplexity=249.22363, train_loss=5.5183506

Batch 100590, train_perplexity=266.83243, train_loss=5.586621

Batch 100600, train_perplexity=249.10577, train_loss=5.5178776

Batch 100610, train_perplexity=257.0033, train_loss=5.549089

Batch 100620, train_perplexity=245.55301, train_loss=5.503513

Batch 100630, train_perplexity=255.5329, train_loss=5.543351

Batch 100640, train_perplexity=276.40897, train_loss=5.6218815

Batch 100650, train_perplexity=252.0794, train_loss=5.529744

Batch 100660, train_perplexity=266.74426, train_loss=5.5862904

Batch 100670, train_perplexity=246.88992, train_loss=5.5089426
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 100680, train_perplexity=241.79298, train_loss=5.488082

Batch 100690, train_perplexity=267.55188, train_loss=5.5893135

Batch 100700, train_perplexity=288.62677, train_loss=5.6651344

Batch 100710, train_perplexity=257.07233, train_loss=5.5493574

Batch 100720, train_perplexity=245.60349, train_loss=5.5037184

Batch 100730, train_perplexity=264.1175, train_loss=5.576394

Batch 100740, train_perplexity=234.96384, train_loss=5.4594316

Batch 100750, train_perplexity=245.68723, train_loss=5.5040593

Batch 100760, train_perplexity=259.54248, train_loss=5.5589204

Batch 100770, train_perplexity=255.85307, train_loss=5.5446033

Batch 100780, train_perplexity=259.11005, train_loss=5.557253

Batch 100790, train_perplexity=267.24512, train_loss=5.588166

Batch 100800, train_perplexity=224.08746, train_loss=5.4120364

Batch 100810, train_perplexity=281.47998, train_loss=5.6400614

Batch 100820, train_perplexity=254.27258, train_loss=5.538407

Batch 100830, train_perplexity=230.35298, train_loss=5.439613

Batch 100840, train_perplexity=233.99213, train_loss=5.4552875

Batch 100850, train_perplexity=245.38493, train_loss=5.502828

Batch 100860, train_perplexity=262.50287, train_loss=5.570262

Batch 100870, train_perplexity=249.00815, train_loss=5.5174856

Batch 100880, train_perplexity=242.0784, train_loss=5.4892616

Batch 100890, train_perplexity=226.61588, train_loss=5.4232564

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Batch 100900, train_perplexity=245.61731, train_loss=5.5037746

Batch 100910, train_perplexity=251.62257, train_loss=5.5279303

Batch 100920, train_perplexity=239.92084, train_loss=5.480309

Batch 100930, train_perplexity=228.43279, train_loss=5.431242

Batch 100940, train_perplexity=252.81804, train_loss=5.53267

Batch 100950, train_perplexity=271.07333, train_loss=5.6023893

Batch 100960, train_perplexity=212.67873, train_loss=5.3597827

Batch 100970, train_perplexity=247.39337, train_loss=5.5109797

Batch 100980, train_perplexity=237.31288, train_loss=5.4693794

Batch 100990, train_perplexity=263.30893, train_loss=5.573328

Batch 101000, train_perplexity=241.0483, train_loss=5.4849973

Batch 101010, train_perplexity=244.60654, train_loss=5.499651

Batch 101020, train_perplexity=252.86192, train_loss=5.5328436

Batch 101030, train_perplexity=246.57286, train_loss=5.5076575

Batch 101040, train_perplexity=284.05368, train_loss=5.6491632

Batch 101050, train_perplexity=243.15732, train_loss=5.4937086

Batch 101060, train_perplexity=214.9438, train_loss=5.3703766

Batch 101070, train_perplexity=251.08766, train_loss=5.525802

Batch 101080, train_perplexity=229.82635, train_loss=5.437324

Batch 101090, train_perplexity=270.3422, train_loss=5.5996885

Batch 101100, train_perplexity=230.00089, train_loss=5.438083

Batch 101110, train_perplexity=299.7102, train_loss=5.702816

Batch 101120, train_perplexity=240.59515, train_loss=5.4831157

Batch 101130, train_perplexity=240.34267, train_loss=5.4820657

Batch 101140, train_perplexity=236.21269, train_loss=5.4647326

Batch 101150, train_perplexity=250.28235, train_loss=5.5225897

Batch 101160, train_perplexity=242.69952, train_loss=5.491824

Batch 101170, train_perplexity=260.89883, train_loss=5.5641327

Batch 101180, train_perplexity=250.04353, train_loss=5.521635

Batch 101190, train_perplexity=246.09773, train_loss=5.5057287

Batch 101200, train_perplexity=280.236, train_loss=5.635632

Batch 101210, train_perplexity=220.9091, train_loss=5.3977513

Batch 101220, train_perplexity=232.03859, train_loss=5.4469037

Batch 101230, train_perplexity=249.04033, train_loss=5.517615

Batch 101240, train_perplexity=230.94722, train_loss=5.442189

Batch 101250, train_perplexity=264.7255, train_loss=5.5786934

Batch 101260, train_perplexity=251.64214, train_loss=5.528008

Batch 101270, train_perplexity=279.73572, train_loss=5.6338453

Batch 101280, train_perplexity=262.92026, train_loss=5.571851

Batch 101290, train_perplexity=250.36472, train_loss=5.5229187

Batch 101300, train_perplexity=248.31308, train_loss=5.5146904

Batch 101310, train_perplexity=234.57137, train_loss=5.45776

Batch 101320, train_perplexity=246.19197, train_loss=5.5061116

Batch 101330, train_perplexity=233.39963, train_loss=5.452752

Batch 101340, train_perplexity=272.7373, train_loss=5.608509

Batch 101350, train_perplexity=237.4906, train_loss=5.470128

Batch 101360, train_perplexity=244.65717, train_loss=5.499858

Batch 101370, train_perplexity=253.79036, train_loss=5.5365086

Batch 101380, train_perplexity=257.10736, train_loss=5.549494

Batch 101390, train_perplexity=260.36145, train_loss=5.562071

Batch 101400, train_perplexity=243.58472, train_loss=5.495465

Batch 101410, train_perplexity=226.18286, train_loss=5.421344

Batch 101420, train_perplexity=244.02649, train_loss=5.497277

Batch 101430, train_perplexity=235.531, train_loss=5.4618425

Batch 101440, train_perplexity=254.00488, train_loss=5.5373535

Batch 101450, train_perplexity=240.86652, train_loss=5.484243

Batch 101460, train_perplexity=302.525, train_loss=5.712164

Batch 101470, train_perplexity=241.86264, train_loss=5.48837

Batch 101480, train_perplexity=212.2519, train_loss=5.357774

Batch 101490, train_perplexity=271.34204, train_loss=5.60338

Batch 101500, train_perplexity=218.9895, train_loss=5.389024

Batch 101510, train_perplexity=245.61613, train_loss=5.50377

Batch 101520, train_perplexity=239.36285, train_loss=5.4779806

Batch 101530, train_perplexity=283.96106, train_loss=5.648837

Batch 101540, train_perplexity=257.5366, train_loss=5.551162

Batch 101550, train_perplexity=264.34467, train_loss=5.577254

Batch 101560, train_perplexity=230.05585, train_loss=5.438322

Batch 101570, train_perplexity=235.81767, train_loss=5.463059

Batch 101580, train_perplexity=255.98767, train_loss=5.5451293

Batch 101590, train_perplexity=265.1765, train_loss=5.5803957

Batch 101600, train_perplexity=229.90517, train_loss=5.437667

Batch 101610, train_perplexity=260.97635, train_loss=5.5644298

Batch 101620, train_perplexity=229.21796, train_loss=5.4346733

Batch 101630, train_perplexity=267.71436, train_loss=5.5899205

Batch 101640, train_perplexity=256.2048, train_loss=5.545977

Batch 101650, train_perplexity=257.46756, train_loss=5.550894

Batch 101660, train_perplexity=225.2321, train_loss=5.4171314

Batch 101670, train_perplexity=241.62944, train_loss=5.4874053

Batch 101680, train_perplexity=252.85759, train_loss=5.5328264

Batch 101690, train_perplexity=240.0102, train_loss=5.4806814

Batch 101700, train_perplexity=242.12146, train_loss=5.4894395

Batch 101710, train_perplexity=266.56903, train_loss=5.5856333

Batch 101720, train_perplexity=276.0483, train_loss=5.620576

Batch 101730, train_perplexity=258.80432, train_loss=5.556072

Batch 101740, train_perplexity=263.03888, train_loss=5.572302

Batch 101750, train_perplexity=248.32576, train_loss=5.5147414

Batch 101760, train_perplexity=243.7833, train_loss=5.4962797

Batch 101770, train_perplexity=259.72632, train_loss=5.5596285

Batch 101780, train_perplexity=292.5313, train_loss=5.6785717

Batch 101790, train_perplexity=253.26714, train_loss=5.534445

Batch 101800, train_perplexity=236.5407, train_loss=5.4661202

Batch 101810, train_perplexity=241.26895, train_loss=5.4859123

Batch 101820, train_perplexity=261.0281, train_loss=5.564628

Batch 101830, train_perplexity=262.35318, train_loss=5.5696917

Batch 101840, train_perplexity=255.58603, train_loss=5.543559

Batch 101850, train_perplexity=271.85464, train_loss=5.6052675

Batch 101860, train_perplexity=278.21936, train_loss=5.62841

Batch 101870, train_perplexity=258.83493, train_loss=5.5561905

Batch 101880, train_perplexity=261.7262, train_loss=5.567299

Batch 101890, train_perplexity=259.6949, train_loss=5.5595074

Batch 101900, train_perplexity=254.6214, train_loss=5.5397778

Batch 101910, train_perplexity=240.98703, train_loss=5.484743
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 101920, train_perplexity=254.75864, train_loss=5.5403166

Batch 101930, train_perplexity=238.91927, train_loss=5.4761257

Batch 101940, train_perplexity=262.79404, train_loss=5.5713706

Batch 101950, train_perplexity=238.06409, train_loss=5.47254

Batch 101960, train_perplexity=239.15259, train_loss=5.477102

Batch 101970, train_perplexity=272.70038, train_loss=5.6083736

Batch 101980, train_perplexity=255.44177, train_loss=5.5429945

Batch 101990, train_perplexity=260.93478, train_loss=5.5642705

Batch 102000, train_perplexity=217.54231, train_loss=5.3823934

Batch 102010, train_perplexity=253.14084, train_loss=5.533946

Batch 102020, train_perplexity=287.4711, train_loss=5.6611223

Batch 102030, train_perplexity=246.00891, train_loss=5.5053678

Batch 102040, train_perplexity=243.26703, train_loss=5.4941597

Batch 102050, train_perplexity=247.05598, train_loss=5.509615

Batch 102060, train_perplexity=231.0662, train_loss=5.442704

Batch 102070, train_perplexity=232.7255, train_loss=5.4498596

Batch 102080, train_perplexity=262.5494, train_loss=5.5704393

Batch 102090, train_perplexity=286.02176, train_loss=5.656068

Batch 102100, train_perplexity=257.3841, train_loss=5.5505695

Batch 102110, train_perplexity=229.97983, train_loss=5.4379916

Batch 102120, train_perplexity=250.86902, train_loss=5.524931

Batch 102130, train_perplexity=259.48322, train_loss=5.558692

Batch 102140, train_perplexity=246.1935, train_loss=5.506118

Batch 102150, train_perplexity=252.6621, train_loss=5.532053

Batch 102160, train_perplexity=223.41795, train_loss=5.4090443

Batch 102170, train_perplexity=244.65903, train_loss=5.4998655

Batch 102180, train_perplexity=230.91528, train_loss=5.442051

Batch 102190, train_perplexity=254.32436, train_loss=5.5386105

Batch 102200, train_perplexity=273.50958, train_loss=5.6113367

Batch 102210, train_perplexity=237.81413, train_loss=5.4714894

Batch 102220, train_perplexity=235.08084, train_loss=5.4599295

Batch 102230, train_perplexity=236.57329, train_loss=5.466258

Batch 102240, train_perplexity=253.05057, train_loss=5.5335894

Batch 102250, train_perplexity=241.15968, train_loss=5.4854593

Batch 102260, train_perplexity=260.50314, train_loss=5.562615

Batch 102270, train_perplexity=258.0204, train_loss=5.5530386

Batch 102280, train_perplexity=246.34381, train_loss=5.506728

Batch 102290, train_perplexity=260.89334, train_loss=5.5641117

Batch 102300, train_perplexity=244.42815, train_loss=5.4989214

Batch 102310, train_perplexity=248.06078, train_loss=5.513674

Batch 102320, train_perplexity=234.66534, train_loss=5.4581604

Batch 102330, train_perplexity=257.06522, train_loss=5.5493298

Batch 102340, train_perplexity=260.05127, train_loss=5.5608788

Batch 102350, train_perplexity=215.81676, train_loss=5.3744297

Batch 102360, train_perplexity=234.40028, train_loss=5.4570303

Batch 102370, train_perplexity=261.10193, train_loss=5.564911

Batch 102380, train_perplexity=266.0357, train_loss=5.5836306

Batch 102390, train_perplexity=290.65384, train_loss=5.672133

Batch 102400, train_perplexity=236.3017, train_loss=5.4651093

Batch 102410, train_perplexity=219.76883, train_loss=5.392576

Batch 102420, train_perplexity=227.44424, train_loss=5.426905

Batch 102430, train_perplexity=281.18445, train_loss=5.639011

Batch 102440, train_perplexity=246.70093, train_loss=5.508177

Batch 102450, train_perplexity=234.12672, train_loss=5.4558625

Batch 102460, train_perplexity=252.32474, train_loss=5.530717

Batch 102470, train_perplexity=223.43063, train_loss=5.409101

Batch 102480, train_perplexity=242.70103, train_loss=5.4918303

Batch 102490, train_perplexity=221.28284, train_loss=5.3994417

Batch 102500, train_perplexity=244.93346, train_loss=5.5009866

Batch 102510, train_perplexity=263.36758, train_loss=5.5735507

Batch 102520, train_perplexity=236.26958, train_loss=5.4649734

Batch 102530, train_perplexity=245.58638, train_loss=5.5036488

Batch 102540, train_perplexity=208.49464, train_loss=5.3399134

Batch 102550, train_perplexity=244.90251, train_loss=5.50086

Batch 102560, train_perplexity=283.0179, train_loss=5.64551

Batch 102570, train_perplexity=258.1203, train_loss=5.553426

Batch 102580, train_perplexity=272.51993, train_loss=5.607712

Batch 102590, train_perplexity=271.56625, train_loss=5.604206

Batch 102600, train_perplexity=250.41844, train_loss=5.5231333

Batch 102610, train_perplexity=247.19739, train_loss=5.510187

Batch 102620, train_perplexity=217.59013, train_loss=5.382613

Batch 102630, train_perplexity=241.90208, train_loss=5.488533

Batch 102640, train_perplexity=262.34256, train_loss=5.569651

Batch 102650, train_perplexity=248.15802, train_loss=5.5140657

Batch 102660, train_perplexity=240.37257, train_loss=5.48219

Batch 102670, train_perplexity=255.41522, train_loss=5.5428905

Batch 102680, train_perplexity=233.85414, train_loss=5.4546976

Batch 102690, train_perplexity=255.40901, train_loss=5.542866

Batch 102700, train_perplexity=227.13007, train_loss=5.425523

Batch 102710, train_perplexity=252.90932, train_loss=5.533031

Batch 102720, train_perplexity=264.58035, train_loss=5.578145

Batch 102730, train_perplexity=266.2668, train_loss=5.584499

Batch 102740, train_perplexity=267.07288, train_loss=5.5875216

Batch 102750, train_perplexity=233.97327, train_loss=5.455207

Batch 102760, train_perplexity=253.39543, train_loss=5.534951

Batch 102770, train_perplexity=278.2792, train_loss=5.628625

Batch 102780, train_perplexity=242.39755, train_loss=5.490579

Batch 102790, train_perplexity=239.46902, train_loss=5.478424

Batch 102800, train_perplexity=249.70969, train_loss=5.520299

Batch 102810, train_perplexity=254.35274, train_loss=5.538722

Batch 102820, train_perplexity=232.10344, train_loss=5.447183

Batch 102830, train_perplexity=258.2109, train_loss=5.5537767

Batch 102840, train_perplexity=218.854, train_loss=5.388405

Batch 102850, train_perplexity=256.26724, train_loss=5.546221

Batch 102860, train_perplexity=232.63962, train_loss=5.4494905

Batch 102870, train_perplexity=234.25325, train_loss=5.456403

Batch 102880, train_perplexity=227.37062, train_loss=5.4265814

Batch 102890, train_perplexity=288.31934, train_loss=5.6640687

Batch 102900, train_perplexity=261.8408, train_loss=5.5677366

Batch 102910, train_perplexity=279.1481, train_loss=5.6317425

Batch 102920, train_perplexity=254.37566, train_loss=5.538812

Batch 102930, train_perplexity=263.23248, train_loss=5.5730376

Batch 102940, train_perplexity=249.64813, train_loss=5.5200524

Batch 102950, train_perplexity=266.0243, train_loss=5.5835876

Batch 102960, train_perplexity=266.02304, train_loss=5.583583

Batch 102970, train_perplexity=272.68048, train_loss=5.6083007

Batch 102980, train_perplexity=227.47418, train_loss=5.427037

Batch 102990, train_perplexity=214.47449, train_loss=5.368191

Batch 103000, train_perplexity=228.27162, train_loss=5.4305363

Batch 103010, train_perplexity=242.60071, train_loss=5.491417

Batch 103020, train_perplexity=266.64685, train_loss=5.585925

Batch 103030, train_perplexity=255.06885, train_loss=5.5415335

Batch 103040, train_perplexity=233.5212, train_loss=5.453273

Batch 103050, train_perplexity=238.3518, train_loss=5.4737477

Batch 103060, train_perplexity=270.0026, train_loss=5.5984316

Batch 103070, train_perplexity=241.9412, train_loss=5.4886947

Batch 103080, train_perplexity=242.96445, train_loss=5.492915

Batch 103090, train_perplexity=262.33292, train_loss=5.5696144

Batch 103100, train_perplexity=270.194, train_loss=5.59914

Batch 103110, train_perplexity=251.18442, train_loss=5.5261874

Batch 103120, train_perplexity=221.66112, train_loss=5.4011497

Batch 103130, train_perplexity=242.28903, train_loss=5.4901314

Batch 103140, train_perplexity=247.42215, train_loss=5.511096

Batch 103150, train_perplexity=263.29236, train_loss=5.573265

Batch 103160, train_perplexity=254.00417, train_loss=5.5373507

Batch 103170, train_perplexity=249.61206, train_loss=5.519908

Batch 103180, train_perplexity=261.35904, train_loss=5.565895

Batch 103190, train_perplexity=237.03705, train_loss=5.4682164

Batch 103200, train_perplexity=241.68187, train_loss=5.4876223

Batch 103210, train_perplexity=242.26662, train_loss=5.490039
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 103220, train_perplexity=250.37212, train_loss=5.5229483

Batch 103230, train_perplexity=226.40881, train_loss=5.4223423

Batch 103240, train_perplexity=246.99284, train_loss=5.5093594

Batch 103250, train_perplexity=259.57565, train_loss=5.559048

Batch 103260, train_perplexity=249.23337, train_loss=5.5183897

Batch 103270, train_perplexity=247.38345, train_loss=5.5109396

Batch 103280, train_perplexity=240.36157, train_loss=5.4821444

Batch 103290, train_perplexity=243.2625, train_loss=5.494141

Batch 103300, train_perplexity=295.85794, train_loss=5.6898794

Batch 103310, train_perplexity=235.89798, train_loss=5.4633994

Batch 103320, train_perplexity=246.00856, train_loss=5.5053663

Batch 103330, train_perplexity=285.44623, train_loss=5.6540537

Batch 103340, train_perplexity=252.53706, train_loss=5.531558

Batch 103350, train_perplexity=242.87352, train_loss=5.492541

Batch 103360, train_perplexity=228.1591, train_loss=5.430043

Batch 103370, train_perplexity=259.05966, train_loss=5.5570583

Batch 103380, train_perplexity=248.49905, train_loss=5.515439

Batch 103390, train_perplexity=277.1734, train_loss=5.6246433

Batch 103400, train_perplexity=243.3343, train_loss=5.4944363

Batch 103410, train_perplexity=217.19063, train_loss=5.3807755

Batch 103420, train_perplexity=237.05185, train_loss=5.468279

Batch 103430, train_perplexity=267.7898, train_loss=5.5902023

Batch 103440, train_perplexity=244.81612, train_loss=5.5005074

Batch 103450, train_perplexity=253.42926, train_loss=5.5350847

Batch 103460, train_perplexity=255.58676, train_loss=5.543562

Batch 103470, train_perplexity=255.73439, train_loss=5.5441394

Batch 103480, train_perplexity=254.58377, train_loss=5.53963

Batch 103490, train_perplexity=229.77803, train_loss=5.437114

Batch 103500, train_perplexity=230.69243, train_loss=5.4410853

Batch 103510, train_perplexity=260.00092, train_loss=5.560685

Batch 103520, train_perplexity=223.07497, train_loss=5.407508

Batch 103530, train_perplexity=227.43135, train_loss=5.4268484

Batch 103540, train_perplexity=236.62282, train_loss=5.4664674

Batch 103550, train_perplexity=268.8743, train_loss=5.594244

Batch 103560, train_perplexity=245.36153, train_loss=5.5027328

Batch 103570, train_perplexity=238.96724, train_loss=5.4763265

Batch 103580, train_perplexity=243.33687, train_loss=5.4944468

Batch 103590, train_perplexity=277.11237, train_loss=5.624423

Batch 103600, train_perplexity=278.4122, train_loss=5.6291027

Batch 103610, train_perplexity=235.034, train_loss=5.45973

Batch 103620, train_perplexity=258.58597, train_loss=5.555228

Batch 103630, train_perplexity=238.7686, train_loss=5.475495

Batch 103640, train_perplexity=234.04178, train_loss=5.4554996

Batch 103650, train_perplexity=250.17389, train_loss=5.5221562

Batch 103660, train_perplexity=253.42732, train_loss=5.535077

Batch 103670, train_perplexity=234.40208, train_loss=5.457038

Batch 103680, train_perplexity=262.833, train_loss=5.571519

Batch 103690, train_perplexity=224.39948, train_loss=5.413428

Batch 103700, train_perplexity=249.51329, train_loss=5.519512

Batch 103710, train_perplexity=263.93445, train_loss=5.5757008

Batch 103720, train_perplexity=248.54575, train_loss=5.515627

Batch 103730, train_perplexity=256.08328, train_loss=5.5455027

Batch 103740, train_perplexity=255.045, train_loss=5.54144

Batch 103750, train_perplexity=244.33144, train_loss=5.4985256

Batch 103760, train_perplexity=242.5977, train_loss=5.4914045

Batch 103770, train_perplexity=223.53389, train_loss=5.409563

Batch 103780, train_perplexity=266.47067, train_loss=5.585264

Batch 103790, train_perplexity=216.98215, train_loss=5.379815

Batch 103800, train_perplexity=252.23296, train_loss=5.530353

Batch 103810, train_perplexity=262.7651, train_loss=5.5712605

Batch 103820, train_perplexity=241.34766, train_loss=5.4862385

Batch 103830, train_perplexity=230.38474, train_loss=5.4397507

Batch 103840, train_perplexity=239.38545, train_loss=5.478075

Batch 103850, train_perplexity=263.93143, train_loss=5.5756893

Batch 103860, train_perplexity=228.21068, train_loss=5.4302692

Batch 103870, train_perplexity=228.4866, train_loss=5.4314775

Batch 103880, train_perplexity=277.66125, train_loss=5.626402

Batch 103890, train_perplexity=225.96014, train_loss=5.4203587

Batch 103900, train_perplexity=246.46248, train_loss=5.50721

Batch 103910, train_perplexity=251.99095, train_loss=5.529393

Batch 103920, train_perplexity=235.72987, train_loss=5.4626865

Batch 103930, train_perplexity=245.50325, train_loss=5.50331

Batch 103940, train_perplexity=264.86185, train_loss=5.5792084

Batch 103950, train_perplexity=244.69614, train_loss=5.500017

Batch 103960, train_perplexity=274.42996, train_loss=5.614696

Batch 103970, train_perplexity=227.3369, train_loss=5.426433

Batch 103980, train_perplexity=261.9884, train_loss=5.5683002

Batch 103990, train_perplexity=257.81378, train_loss=5.5522375

Batch 104000, train_perplexity=249.2619, train_loss=5.518504

Batch 104010, train_perplexity=248.71718, train_loss=5.5163164

Batch 104020, train_perplexity=234.00987, train_loss=5.4553633

Batch 104030, train_perplexity=235.58582, train_loss=5.462075

Batch 104040, train_perplexity=283.07852, train_loss=5.6457243

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00050-of-00100
Loaded 305220 sentences.
Finished loading
Batch 104050, train_perplexity=264.16348, train_loss=5.576568

Batch 104060, train_perplexity=253.33237, train_loss=5.5347023

Batch 104070, train_perplexity=258.2987, train_loss=5.5541167

Batch 104080, train_perplexity=241.35365, train_loss=5.4862633

Batch 104090, train_perplexity=231.03865, train_loss=5.442585

Batch 104100, train_perplexity=240.91441, train_loss=5.4844418

Batch 104110, train_perplexity=275.01065, train_loss=5.61681

Batch 104120, train_perplexity=235.41176, train_loss=5.461336

Batch 104130, train_perplexity=235.53415, train_loss=5.461856

Batch 104140, train_perplexity=229.88774, train_loss=5.437591

Batch 104150, train_perplexity=259.14786, train_loss=5.557399

Batch 104160, train_perplexity=243.82922, train_loss=5.496468

Batch 104170, train_perplexity=265.3197, train_loss=5.5809355

Batch 104180, train_perplexity=250.66896, train_loss=5.524133

Batch 104190, train_perplexity=234.43628, train_loss=5.457184

Batch 104200, train_perplexity=291.56235, train_loss=5.675254

Batch 104210, train_perplexity=256.1773, train_loss=5.54587

Batch 104220, train_perplexity=248.88516, train_loss=5.5169916

Batch 104230, train_perplexity=237.51234, train_loss=5.4702196

Batch 104240, train_perplexity=248.93193, train_loss=5.5171795

Batch 104250, train_perplexity=253.42877, train_loss=5.535083

Batch 104260, train_perplexity=266.81754, train_loss=5.586565

Batch 104270, train_perplexity=244.38316, train_loss=5.4987373

Batch 104280, train_perplexity=234.69443, train_loss=5.4582844

Batch 104290, train_perplexity=268.45105, train_loss=5.5926685

Batch 104300, train_perplexity=247.21979, train_loss=5.5102777

Batch 104310, train_perplexity=281.9226, train_loss=5.6416326

Batch 104320, train_perplexity=241.99806, train_loss=5.4889297

Batch 104330, train_perplexity=268.08633, train_loss=5.591309

Batch 104340, train_perplexity=226.12247, train_loss=5.421077

Batch 104350, train_perplexity=219.96614, train_loss=5.3934736

Batch 104360, train_perplexity=261.7277, train_loss=5.5673046

Batch 104370, train_perplexity=293.7782, train_loss=5.682825

Batch 104380, train_perplexity=260.72174, train_loss=5.5634537

Batch 104390, train_perplexity=231.76048, train_loss=5.4457045

Batch 104400, train_perplexity=228.45326, train_loss=5.4313316

Batch 104410, train_perplexity=236.81775, train_loss=5.467291

Batch 104420, train_perplexity=254.75731, train_loss=5.5403113

Batch 104430, train_perplexity=253.95523, train_loss=5.537158

Batch 104440, train_perplexity=240.08528, train_loss=5.480994

Batch 104450, train_perplexity=220.9109, train_loss=5.3977594
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 104460, train_perplexity=242.35028, train_loss=5.490384

Batch 104470, train_perplexity=240.04042, train_loss=5.4808073

Batch 104480, train_perplexity=227.70457, train_loss=5.428049

Batch 104490, train_perplexity=234.45775, train_loss=5.4572754

Batch 104500, train_perplexity=254.07164, train_loss=5.5376163

Batch 104510, train_perplexity=244.48363, train_loss=5.4991484

Batch 104520, train_perplexity=245.13898, train_loss=5.5018253

Batch 104530, train_perplexity=250.87668, train_loss=5.5249615

Batch 104540, train_perplexity=266.17148, train_loss=5.584141

Batch 104550, train_perplexity=248.24277, train_loss=5.514407

Batch 104560, train_perplexity=226.10954, train_loss=5.4210196

Batch 104570, train_perplexity=233.22607, train_loss=5.4520082

Batch 104580, train_perplexity=235.82915, train_loss=5.4631076

Batch 104590, train_perplexity=261.2646, train_loss=5.5655336

Batch 104600, train_perplexity=227.05643, train_loss=5.4251986

Batch 104610, train_perplexity=250.37712, train_loss=5.5229683

Batch 104620, train_perplexity=242.80092, train_loss=5.492242

Batch 104630, train_perplexity=209.3512, train_loss=5.344013

Batch 104640, train_perplexity=231.55414, train_loss=5.4448137

Batch 104650, train_perplexity=245.63007, train_loss=5.5038266

Batch 104660, train_perplexity=225.25465, train_loss=5.4172316

Batch 104670, train_perplexity=260.68692, train_loss=5.56332

Batch 104680, train_perplexity=234.66176, train_loss=5.458145

Batch 104690, train_perplexity=255.34447, train_loss=5.5426135

Batch 104700, train_perplexity=256.64227, train_loss=5.5476832

Batch 104710, train_perplexity=207.45377, train_loss=5.3349085

Batch 104720, train_perplexity=253.97218, train_loss=5.537225

Batch 104730, train_perplexity=226.41098, train_loss=5.422352

Batch 104740, train_perplexity=239.80852, train_loss=5.4798408

Batch 104750, train_perplexity=271.20468, train_loss=5.602874

Batch 104760, train_perplexity=235.26566, train_loss=5.4607153

Batch 104770, train_perplexity=229.28322, train_loss=5.434958

Batch 104780, train_perplexity=230.84726, train_loss=5.4417562

Batch 104790, train_perplexity=238.17479, train_loss=5.473005

Batch 104800, train_perplexity=228.68355, train_loss=5.432339

Batch 104810, train_perplexity=260.85876, train_loss=5.563979

Batch 104820, train_perplexity=212.78067, train_loss=5.360262

Batch 104830, train_perplexity=260.1805, train_loss=5.5613756

Batch 104840, train_perplexity=250.84222, train_loss=5.524824

Batch 104850, train_perplexity=227.03911, train_loss=5.4251223

Batch 104860, train_perplexity=241.91661, train_loss=5.488593

Batch 104870, train_perplexity=267.60355, train_loss=5.5895066

Batch 104880, train_perplexity=205.72847, train_loss=5.326557

Batch 104890, train_perplexity=248.25436, train_loss=5.514454

Batch 104900, train_perplexity=215.02971, train_loss=5.370776

Batch 104910, train_perplexity=243.41833, train_loss=5.4947815

Batch 104920, train_perplexity=264.73584, train_loss=5.5787325

Batch 104930, train_perplexity=204.99931, train_loss=5.3230066

Batch 104940, train_perplexity=276.92426, train_loss=5.623744

Batch 104950, train_perplexity=248.26158, train_loss=5.514483

Batch 104960, train_perplexity=226.52933, train_loss=5.4228745

Batch 104970, train_perplexity=245.96281, train_loss=5.5051804

Batch 104980, train_perplexity=221.08351, train_loss=5.3985405

Batch 104990, train_perplexity=246.00668, train_loss=5.5053587

Batch 105000, train_perplexity=248.39125, train_loss=5.515005

Batch 105010, train_perplexity=242.56462, train_loss=5.491268

Batch 105020, train_perplexity=239.1625, train_loss=5.4771433

Batch 105030, train_perplexity=239.9293, train_loss=5.4803443

Batch 105040, train_perplexity=255.86101, train_loss=5.5446343

Batch 105050, train_perplexity=261.32214, train_loss=5.565754

Batch 105060, train_perplexity=239.74861, train_loss=5.479591

Batch 105070, train_perplexity=260.38443, train_loss=5.562159

Batch 105080, train_perplexity=247.05527, train_loss=5.509612

Batch 105090, train_perplexity=240.6227, train_loss=5.48323

Batch 105100, train_perplexity=231.34158, train_loss=5.4438953

Batch 105110, train_perplexity=245.09024, train_loss=5.5016265

Batch 105120, train_perplexity=239.1185, train_loss=5.476959

Batch 105130, train_perplexity=242.57803, train_loss=5.4913235

Batch 105140, train_perplexity=266.729, train_loss=5.586233

Batch 105150, train_perplexity=233.3165, train_loss=5.452396

Batch 105160, train_perplexity=251.79529, train_loss=5.5286164

Batch 105170, train_perplexity=250.47218, train_loss=5.523348

Batch 105180, train_perplexity=223.07167, train_loss=5.407493

Batch 105190, train_perplexity=240.9259, train_loss=5.4844894

Batch 105200, train_perplexity=249.23029, train_loss=5.5183773

Batch 105210, train_perplexity=249.99179, train_loss=5.521428

Batch 105220, train_perplexity=269.3898, train_loss=5.5961595

Batch 105230, train_perplexity=271.2123, train_loss=5.602902

Batch 105240, train_perplexity=230.00494, train_loss=5.438101

Batch 105250, train_perplexity=261.11563, train_loss=5.5649633

Batch 105260, train_perplexity=236.15154, train_loss=5.4644737

Batch 105270, train_perplexity=259.10068, train_loss=5.5572166

Batch 105280, train_perplexity=227.15616, train_loss=5.4256377

Batch 105290, train_perplexity=250.23497, train_loss=5.5224004

Batch 105300, train_perplexity=230.11772, train_loss=5.438591

Batch 105310, train_perplexity=254.98541, train_loss=5.5412064

Batch 105320, train_perplexity=237.10793, train_loss=5.4685154

Batch 105330, train_perplexity=283.5508, train_loss=5.6473913

Batch 105340, train_perplexity=226.22903, train_loss=5.421548

Batch 105350, train_perplexity=257.31012, train_loss=5.550282

Batch 105360, train_perplexity=255.1535, train_loss=5.5418653

Batch 105370, train_perplexity=237.38145, train_loss=5.4696684

Batch 105380, train_perplexity=254.0153, train_loss=5.5373945

Batch 105390, train_perplexity=243.90935, train_loss=5.4967966

Batch 105400, train_perplexity=269.26254, train_loss=5.595687

Batch 105410, train_perplexity=242.29135, train_loss=5.490141

Batch 105420, train_perplexity=240.89075, train_loss=5.4843435

Batch 105430, train_perplexity=245.44824, train_loss=5.503086

Batch 105440, train_perplexity=232.2189, train_loss=5.4476805

Batch 105450, train_perplexity=248.23969, train_loss=5.5143948

Batch 105460, train_perplexity=252.638, train_loss=5.5319576

Batch 105470, train_perplexity=232.71794, train_loss=5.449827

Batch 105480, train_perplexity=246.77153, train_loss=5.508463

Batch 105490, train_perplexity=260.73416, train_loss=5.5635014

Batch 105500, train_perplexity=231.32736, train_loss=5.443834

Batch 105510, train_perplexity=247.29712, train_loss=5.5105906

Batch 105520, train_perplexity=244.81027, train_loss=5.5004835

Batch 105530, train_perplexity=285.2121, train_loss=5.653233

Batch 105540, train_perplexity=254.61485, train_loss=5.539752

Batch 105550, train_perplexity=256.22128, train_loss=5.5460415

Batch 105560, train_perplexity=272.75824, train_loss=5.608586

Batch 105570, train_perplexity=277.4228, train_loss=5.6255426

Batch 105580, train_perplexity=248.46611, train_loss=5.5153065

Batch 105590, train_perplexity=230.71718, train_loss=5.4411926

Batch 105600, train_perplexity=268.0839, train_loss=5.5913

Batch 105610, train_perplexity=241.0753, train_loss=5.4851093

Batch 105620, train_perplexity=235.71481, train_loss=5.4626226

Batch 105630, train_perplexity=242.09041, train_loss=5.489311

Batch 105640, train_perplexity=230.7777, train_loss=5.441455

Batch 105650, train_perplexity=246.40138, train_loss=5.506962

Batch 105660, train_perplexity=261.96967, train_loss=5.5682287

Batch 105670, train_perplexity=225.39175, train_loss=5.41784

Batch 105680, train_perplexity=248.01749, train_loss=5.5134993

Batch 105690, train_perplexity=231.67961, train_loss=5.4453554

Batch 105700, train_perplexity=223.88997, train_loss=5.4111547

Batch 105710, train_perplexity=245.36375, train_loss=5.502742

Batch 105720, train_perplexity=259.989, train_loss=5.5606394

Batch 105730, train_perplexity=219.40247, train_loss=5.390908

Batch 105740, train_perplexity=245.01768, train_loss=5.5013304

Batch 105750, train_perplexity=252.31644, train_loss=5.530684
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 105760, train_perplexity=235.45554, train_loss=5.461522

Batch 105770, train_perplexity=231.56651, train_loss=5.444867

Batch 105780, train_perplexity=261.6884, train_loss=5.5671544

Batch 105790, train_perplexity=225.7368, train_loss=5.4193697

Batch 105800, train_perplexity=204.8758, train_loss=5.322404

Batch 105810, train_perplexity=230.12222, train_loss=5.4386106

Batch 105820, train_perplexity=237.92813, train_loss=5.4719687

Batch 105830, train_perplexity=283.92477, train_loss=5.6487093

Batch 105840, train_perplexity=267.02258, train_loss=5.587333

Batch 105850, train_perplexity=219.92157, train_loss=5.393271

Batch 105860, train_perplexity=285.4246, train_loss=5.653978

Batch 105870, train_perplexity=247.6006, train_loss=5.511817

Batch 105880, train_perplexity=259.2717, train_loss=5.5578766

Batch 105890, train_perplexity=229.79129, train_loss=5.4371715

Batch 105900, train_perplexity=222.18993, train_loss=5.4035325

Batch 105910, train_perplexity=246.38234, train_loss=5.5068846

Batch 105920, train_perplexity=251.11125, train_loss=5.525896

Batch 105930, train_perplexity=250.74619, train_loss=5.5244412

Batch 105940, train_perplexity=234.57271, train_loss=5.4577656

Batch 105950, train_perplexity=228.92348, train_loss=5.4333878

Batch 105960, train_perplexity=248.8282, train_loss=5.5167627

Batch 105970, train_perplexity=243.42809, train_loss=5.4948215

Batch 105980, train_perplexity=220.57744, train_loss=5.396249

Batch 105990, train_perplexity=240.36708, train_loss=5.4821672

Batch 106000, train_perplexity=252.64426, train_loss=5.5319824

Batch 106010, train_perplexity=252.40367, train_loss=5.5310297

Batch 106020, train_perplexity=245.94029, train_loss=5.505089

Batch 106030, train_perplexity=263.26776, train_loss=5.5731716

Batch 106040, train_perplexity=230.06177, train_loss=5.438348

Batch 106050, train_perplexity=227.37442, train_loss=5.426598

Batch 106060, train_perplexity=211.76604, train_loss=5.355482

Batch 106070, train_perplexity=258.565, train_loss=5.555147

Batch 106080, train_perplexity=253.80899, train_loss=5.536582

Batch 106090, train_perplexity=222.54704, train_loss=5.4051385

Batch 106100, train_perplexity=249.8121, train_loss=5.520709

Batch 106110, train_perplexity=255.17126, train_loss=5.541935

Batch 106120, train_perplexity=239.74187, train_loss=5.4795628

Batch 106130, train_perplexity=222.50438, train_loss=5.404947

Batch 106140, train_perplexity=263.74625, train_loss=5.5749874

Batch 106150, train_perplexity=269.5494, train_loss=5.5967517

Batch 106160, train_perplexity=264.03766, train_loss=5.576092

Batch 106170, train_perplexity=230.33202, train_loss=5.439522

Batch 106180, train_perplexity=263.33154, train_loss=5.573414

Batch 106190, train_perplexity=225.26154, train_loss=5.417262

Batch 106200, train_perplexity=223.70248, train_loss=5.410317

Batch 106210, train_perplexity=250.47624, train_loss=5.523364

Batch 106220, train_perplexity=258.49942, train_loss=5.5548935

Batch 106230, train_perplexity=257.89713, train_loss=5.552561

Batch 106240, train_perplexity=252.4345, train_loss=5.531152

Batch 106250, train_perplexity=227.28879, train_loss=5.4262214

Batch 106260, train_perplexity=222.75789, train_loss=5.4060855

Batch 106270, train_perplexity=247.2153, train_loss=5.5102596

Batch 106280, train_perplexity=235.70312, train_loss=5.462573

Batch 106290, train_perplexity=241.89609, train_loss=5.488508

Batch 106300, train_perplexity=241.44124, train_loss=5.486626

Batch 106310, train_perplexity=264.52094, train_loss=5.5779204

Batch 106320, train_perplexity=252.92717, train_loss=5.5331016

Batch 106330, train_perplexity=256.89413, train_loss=5.548664

Batch 106340, train_perplexity=217.644, train_loss=5.3828607

Batch 106350, train_perplexity=239.85747, train_loss=5.480045

Batch 106360, train_perplexity=265.10596, train_loss=5.5801296

Batch 106370, train_perplexity=237.37717, train_loss=5.4696503

Batch 106380, train_perplexity=199.64937, train_loss=5.2965627

Batch 106390, train_perplexity=232.67268, train_loss=5.4496326

Batch 106400, train_perplexity=229.13708, train_loss=5.4343204

Batch 106410, train_perplexity=241.33661, train_loss=5.4861927

Batch 106420, train_perplexity=255.34302, train_loss=5.542608

Batch 106430, train_perplexity=241.72475, train_loss=5.4877996

Batch 106440, train_perplexity=225.53249, train_loss=5.418464

Batch 106450, train_perplexity=262.2829, train_loss=5.5694237

Batch 106460, train_perplexity=269.53296, train_loss=5.5966907

Batch 106470, train_perplexity=261.05725, train_loss=5.5647397

Batch 106480, train_perplexity=216.35709, train_loss=5.37693

Batch 106490, train_perplexity=257.3684, train_loss=5.5505085

Batch 106500, train_perplexity=262.2864, train_loss=5.569437

Batch 106510, train_perplexity=265.74295, train_loss=5.5825295

Batch 106520, train_perplexity=257.80444, train_loss=5.5522013

Batch 106530, train_perplexity=245.17686, train_loss=5.50198

Batch 106540, train_perplexity=230.69914, train_loss=5.4411144

Batch 106550, train_perplexity=241.90163, train_loss=5.488531

Batch 106560, train_perplexity=255.4318, train_loss=5.5429554

Batch 106570, train_perplexity=261.7167, train_loss=5.5672626

Batch 106580, train_perplexity=231.5674, train_loss=5.444871

Batch 106590, train_perplexity=245.00763, train_loss=5.5012894

Batch 106600, train_perplexity=239.57341, train_loss=5.47886

Batch 106610, train_perplexity=229.04707, train_loss=5.4339275

Batch 106620, train_perplexity=232.27272, train_loss=5.447912

Batch 106630, train_perplexity=248.20288, train_loss=5.5142465

Batch 106640, train_perplexity=229.75305, train_loss=5.437005

Batch 106650, train_perplexity=237.91258, train_loss=5.4719033

Batch 106660, train_perplexity=239.53229, train_loss=5.4786882

Batch 106670, train_perplexity=242.89484, train_loss=5.4926286

Batch 106680, train_perplexity=254.84903, train_loss=5.5406713

Batch 106690, train_perplexity=237.12308, train_loss=5.4685793

Batch 106700, train_perplexity=238.57205, train_loss=5.4746714

Batch 106710, train_perplexity=223.47336, train_loss=5.409292

Batch 106720, train_perplexity=231.69032, train_loss=5.4454017

Batch 106730, train_perplexity=230.71893, train_loss=5.4412003

Batch 106740, train_perplexity=232.15956, train_loss=5.447425

Batch 106750, train_perplexity=232.32722, train_loss=5.448147

Batch 106760, train_perplexity=223.92563, train_loss=5.411314

Batch 106770, train_perplexity=236.04515, train_loss=5.464023

Batch 106780, train_perplexity=249.07251, train_loss=5.517744

Batch 106790, train_perplexity=238.96758, train_loss=5.476328

Batch 106800, train_perplexity=242.17007, train_loss=5.48964

Batch 106810, train_perplexity=263.038, train_loss=5.5722985

Batch 106820, train_perplexity=230.75337, train_loss=5.4413495

Batch 106830, train_perplexity=269.9193, train_loss=5.598123

Batch 106840, train_perplexity=262.9852, train_loss=5.572098

Batch 106850, train_perplexity=222.3894, train_loss=5.40443

Batch 106860, train_perplexity=233.02576, train_loss=5.451149

Batch 106870, train_perplexity=229.12921, train_loss=5.434286

Batch 106880, train_perplexity=253.70784, train_loss=5.5361834

Batch 106890, train_perplexity=261.90347, train_loss=5.567976

Batch 106900, train_perplexity=250.05688, train_loss=5.5216885

Batch 106910, train_perplexity=251.48488, train_loss=5.527383

Batch 106920, train_perplexity=252.4014, train_loss=5.5310206

Batch 106930, train_perplexity=240.73344, train_loss=5.4836903

Batch 106940, train_perplexity=258.13028, train_loss=5.5534644

Batch 106950, train_perplexity=241.69362, train_loss=5.487671

Batch 106960, train_perplexity=204.44542, train_loss=5.320301

Batch 106970, train_perplexity=246.63083, train_loss=5.5078926

Batch 106980, train_perplexity=267.8452, train_loss=5.5904093

Batch 106990, train_perplexity=260.23187, train_loss=5.561573

Batch 107000, train_perplexity=240.1935, train_loss=5.481445

Batch 107010, train_perplexity=225.83768, train_loss=5.4198165

Batch 107020, train_perplexity=244.23917, train_loss=5.498148

Batch 107030, train_perplexity=221.80449, train_loss=5.4017963

Batch 107040, train_perplexity=260.82544, train_loss=5.5638514

Batch 107050, train_perplexity=228.31952, train_loss=5.430746
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 107060, train_perplexity=229.14627, train_loss=5.4343605

Batch 107070, train_perplexity=242.10738, train_loss=5.4893813

Batch 107080, train_perplexity=249.28947, train_loss=5.518615

Batch 107090, train_perplexity=236.8364, train_loss=5.4673696

Batch 107100, train_perplexity=256.17975, train_loss=5.5458794

Batch 107110, train_perplexity=256.6549, train_loss=5.5477324

Batch 107120, train_perplexity=210.25087, train_loss=5.3483014

Batch 107130, train_perplexity=229.2713, train_loss=5.434906

Batch 107140, train_perplexity=218.74748, train_loss=5.387918

Batch 107150, train_perplexity=258.10443, train_loss=5.5533643

Batch 107160, train_perplexity=246.11497, train_loss=5.505799

Batch 107170, train_perplexity=251.83575, train_loss=5.528777

Batch 107180, train_perplexity=240.9452, train_loss=5.4845695

Batch 107190, train_perplexity=246.83461, train_loss=5.5087185

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00099-of-00100
Loaded 305893 sentences.
Finished loading
Batch 107200, train_perplexity=241.33868, train_loss=5.4862013

Batch 107210, train_perplexity=242.22481, train_loss=5.4898663

Batch 107220, train_perplexity=227.96097, train_loss=5.4291744

Batch 107230, train_perplexity=235.77989, train_loss=5.4628987

Batch 107240, train_perplexity=231.35614, train_loss=5.4439583

Batch 107250, train_perplexity=231.76514, train_loss=5.4457245

Batch 107260, train_perplexity=242.15529, train_loss=5.489579

Batch 107270, train_perplexity=238.29192, train_loss=5.4734964

Batch 107280, train_perplexity=272.44992, train_loss=5.607455

Batch 107290, train_perplexity=248.03098, train_loss=5.5135536

Batch 107300, train_perplexity=260.26364, train_loss=5.561695

Batch 107310, train_perplexity=234.77637, train_loss=5.4586334

Batch 107320, train_perplexity=237.42923, train_loss=5.4698696

Batch 107330, train_perplexity=224.89447, train_loss=5.4156313

Batch 107340, train_perplexity=240.14642, train_loss=5.481249

Batch 107350, train_perplexity=258.87738, train_loss=5.5563545

Batch 107360, train_perplexity=246.48375, train_loss=5.507296

Batch 107370, train_perplexity=232.50687, train_loss=5.44892

Batch 107380, train_perplexity=255.77599, train_loss=5.544302

Batch 107390, train_perplexity=242.70415, train_loss=5.491843

Batch 107400, train_perplexity=241.0946, train_loss=5.4851894

Batch 107410, train_perplexity=234.16882, train_loss=5.4560423

Batch 107420, train_perplexity=228.56604, train_loss=5.431825

Batch 107430, train_perplexity=235.18108, train_loss=5.4603558

Batch 107440, train_perplexity=244.49739, train_loss=5.4992046

Batch 107450, train_perplexity=226.39165, train_loss=5.4222665

Batch 107460, train_perplexity=283.3978, train_loss=5.6468515

Batch 107470, train_perplexity=250.10889, train_loss=5.5218964

Batch 107480, train_perplexity=216.9023, train_loss=5.379447

Batch 107490, train_perplexity=252.04912, train_loss=5.529624

Batch 107500, train_perplexity=255.98608, train_loss=5.545123

Batch 107510, train_perplexity=262.66537, train_loss=5.570881

Batch 107520, train_perplexity=262.33444, train_loss=5.56962

Batch 107530, train_perplexity=253.14423, train_loss=5.5339594

Batch 107540, train_perplexity=214.85966, train_loss=5.369985

Batch 107550, train_perplexity=260.95557, train_loss=5.56435

Batch 107560, train_perplexity=248.45557, train_loss=5.515264

Batch 107570, train_perplexity=214.75182, train_loss=5.369483

Batch 107580, train_perplexity=219.31763, train_loss=5.390521

Batch 107590, train_perplexity=267.47842, train_loss=5.589039

Batch 107600, train_perplexity=222.34859, train_loss=5.4042463

Batch 107610, train_perplexity=257.256, train_loss=5.5500717

Batch 107620, train_perplexity=245.38481, train_loss=5.5028276

Batch 107630, train_perplexity=257.62686, train_loss=5.5515122

Batch 107640, train_perplexity=233.8479, train_loss=5.454671

Batch 107650, train_perplexity=227.35675, train_loss=5.4265203

Batch 107660, train_perplexity=211.61241, train_loss=5.3547564

Batch 107670, train_perplexity=245.64307, train_loss=5.5038795

Batch 107680, train_perplexity=260.22455, train_loss=5.561545

Batch 107690, train_perplexity=250.26945, train_loss=5.522538

Batch 107700, train_perplexity=244.6889, train_loss=5.4999876

Batch 107710, train_perplexity=228.4282, train_loss=5.431222

Batch 107720, train_perplexity=254.79497, train_loss=5.540459

Batch 107730, train_perplexity=229.30585, train_loss=5.4350567

Batch 107740, train_perplexity=225.41055, train_loss=5.4179235

Batch 107750, train_perplexity=235.28136, train_loss=5.460782

Batch 107760, train_perplexity=236.29482, train_loss=5.4650803

Batch 107770, train_perplexity=243.64223, train_loss=5.495701

Batch 107780, train_perplexity=227.04343, train_loss=5.4251413

Batch 107790, train_perplexity=201.90291, train_loss=5.307787

Batch 107800, train_perplexity=249.55672, train_loss=5.519686

Batch 107810, train_perplexity=242.17792, train_loss=5.4896727

Batch 107820, train_perplexity=227.65724, train_loss=5.427841

Batch 107830, train_perplexity=232.8477, train_loss=5.4503846

Batch 107840, train_perplexity=236.97285, train_loss=5.4679456

Batch 107850, train_perplexity=265.93652, train_loss=5.5832577

Batch 107860, train_perplexity=232.68776, train_loss=5.4496975

Batch 107870, train_perplexity=223.31699, train_loss=5.408592

Batch 107880, train_perplexity=234.00372, train_loss=5.455337

Batch 107890, train_perplexity=238.41989, train_loss=5.4740334

Batch 107900, train_perplexity=236.14569, train_loss=5.464449

Batch 107910, train_perplexity=221.60997, train_loss=5.400919

Batch 107920, train_perplexity=251.84093, train_loss=5.5287976

Batch 107930, train_perplexity=221.96362, train_loss=5.4025135

Batch 107940, train_perplexity=231.88054, train_loss=5.4462223

Batch 107950, train_perplexity=222.179, train_loss=5.4034834

Batch 107960, train_perplexity=237.048, train_loss=5.4682627

Batch 107970, train_perplexity=240.4948, train_loss=5.4826984

Batch 107980, train_perplexity=236.08917, train_loss=5.4642096

Batch 107990, train_perplexity=227.18921, train_loss=5.425783

Batch 108000, train_perplexity=253.63852, train_loss=5.53591

Batch 108010, train_perplexity=211.27596, train_loss=5.353165

Batch 108020, train_perplexity=215.45914, train_loss=5.3727713

Batch 108030, train_perplexity=250.70195, train_loss=5.524265

Batch 108040, train_perplexity=213.89825, train_loss=5.3655005

Batch 108050, train_perplexity=220.59395, train_loss=5.3963237

Batch 108060, train_perplexity=250.23999, train_loss=5.5224204

Batch 108070, train_perplexity=229.44301, train_loss=5.4356546

Batch 108080, train_perplexity=242.95148, train_loss=5.4928617

Batch 108090, train_perplexity=265.85312, train_loss=5.582944

Batch 108100, train_perplexity=233.3124, train_loss=5.4523783

Batch 108110, train_perplexity=274.03555, train_loss=5.613258

Batch 108120, train_perplexity=231.3343, train_loss=5.443864

Batch 108130, train_perplexity=254.28749, train_loss=5.5384655

Batch 108140, train_perplexity=267.57867, train_loss=5.5894136

Batch 108150, train_perplexity=220.32515, train_loss=5.3951044

Batch 108160, train_perplexity=228.06578, train_loss=5.429634

Batch 108170, train_perplexity=250.89055, train_loss=5.525017

Batch 108180, train_perplexity=278.5471, train_loss=5.629587

Batch 108190, train_perplexity=240.92131, train_loss=5.4844704

Batch 108200, train_perplexity=220.30939, train_loss=5.395033

Batch 108210, train_perplexity=235.01796, train_loss=5.459662

Batch 108220, train_perplexity=259.83435, train_loss=5.5600443

Batch 108230, train_perplexity=241.67242, train_loss=5.487583

Batch 108240, train_perplexity=281.3249, train_loss=5.63951

Batch 108250, train_perplexity=237.68388, train_loss=5.4709415

Batch 108260, train_perplexity=225.86546, train_loss=5.4199395

Batch 108270, train_perplexity=274.87167, train_loss=5.6163044

Batch 108280, train_perplexity=228.72118, train_loss=5.4325037

Batch 108290, train_perplexity=217.38417, train_loss=5.381666
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 108300, train_perplexity=259.0715, train_loss=5.557104

Batch 108310, train_perplexity=243.95168, train_loss=5.49697

Batch 108320, train_perplexity=229.26944, train_loss=5.434898

Batch 108330, train_perplexity=256.6067, train_loss=5.5475445

Batch 108340, train_perplexity=254.25282, train_loss=5.538329

Batch 108350, train_perplexity=249.06847, train_loss=5.517728

Batch 108360, train_perplexity=236.29662, train_loss=5.465088

Batch 108370, train_perplexity=215.47362, train_loss=5.3728385

Batch 108380, train_perplexity=256.2611, train_loss=5.546197

Batch 108390, train_perplexity=209.15643, train_loss=5.3430824

Batch 108400, train_perplexity=274.62787, train_loss=5.615417

Batch 108410, train_perplexity=258.67365, train_loss=5.5555673

Batch 108420, train_perplexity=214.91695, train_loss=5.3702517

Batch 108430, train_perplexity=266.5106, train_loss=5.585414

Batch 108440, train_perplexity=231.33652, train_loss=5.4438734

Batch 108450, train_perplexity=251.95131, train_loss=5.529236

Batch 108460, train_perplexity=251.02745, train_loss=5.5255623

Batch 108470, train_perplexity=232.51097, train_loss=5.4489374

Batch 108480, train_perplexity=246.57204, train_loss=5.507654

Batch 108490, train_perplexity=215.52562, train_loss=5.37308

Batch 108500, train_perplexity=260.46738, train_loss=5.5624776

Batch 108510, train_perplexity=214.18852, train_loss=5.3668566

Batch 108520, train_perplexity=241.67415, train_loss=5.4875903

Batch 108530, train_perplexity=281.49973, train_loss=5.6401315

Batch 108540, train_perplexity=227.51552, train_loss=5.4272184

Batch 108550, train_perplexity=241.40509, train_loss=5.4864764

Batch 108560, train_perplexity=246.80118, train_loss=5.508583

Batch 108570, train_perplexity=229.06323, train_loss=5.433998

Batch 108580, train_perplexity=260.56824, train_loss=5.562865

Batch 108590, train_perplexity=246.82977, train_loss=5.508699

Batch 108600, train_perplexity=259.13168, train_loss=5.5573363

Batch 108610, train_perplexity=232.37642, train_loss=5.4483585

Batch 108620, train_perplexity=229.93773, train_loss=5.4378085

Batch 108630, train_perplexity=215.39864, train_loss=5.3724904

Batch 108640, train_perplexity=228.71638, train_loss=5.4324827

Batch 108650, train_perplexity=251.26707, train_loss=5.5265164

Batch 108660, train_perplexity=242.26062, train_loss=5.490014

Batch 108670, train_perplexity=227.54459, train_loss=5.427346

Batch 108680, train_perplexity=261.7781, train_loss=5.5674973

Batch 108690, train_perplexity=237.98509, train_loss=5.472208

Batch 108700, train_perplexity=270.94653, train_loss=5.6019216

Batch 108710, train_perplexity=243.00385, train_loss=5.4930773

Batch 108720, train_perplexity=262.28165, train_loss=5.569419

Batch 108730, train_perplexity=248.99141, train_loss=5.5174184

Batch 108740, train_perplexity=232.15977, train_loss=5.447426

Batch 108750, train_perplexity=225.87311, train_loss=5.4199734

Batch 108760, train_perplexity=244.2238, train_loss=5.498085

Batch 108770, train_perplexity=283.86548, train_loss=5.6485004

Batch 108780, train_perplexity=230.4467, train_loss=5.4400196

Batch 108790, train_perplexity=217.94382, train_loss=5.3842373

Batch 108800, train_perplexity=242.88371, train_loss=5.492583

Batch 108810, train_perplexity=233.24431, train_loss=5.4520864

Batch 108820, train_perplexity=256.59177, train_loss=5.5474863

Batch 108830, train_perplexity=213.73656, train_loss=5.364744

Batch 108840, train_perplexity=235.6367, train_loss=5.4622912

Batch 108850, train_perplexity=220.97327, train_loss=5.3980417

Batch 108860, train_perplexity=250.396, train_loss=5.5230436

Batch 108870, train_perplexity=234.88441, train_loss=5.4590936

Batch 108880, train_perplexity=225.48312, train_loss=5.4182453

Batch 108890, train_perplexity=245.31169, train_loss=5.5025296

Batch 108900, train_perplexity=241.04587, train_loss=5.4849873

Batch 108910, train_perplexity=249.74469, train_loss=5.520439

Batch 108920, train_perplexity=232.8598, train_loss=5.4504366

Batch 108930, train_perplexity=248.24194, train_loss=5.514404

Batch 108940, train_perplexity=256.14825, train_loss=5.5457563

Batch 108950, train_perplexity=250.19382, train_loss=5.522236

Batch 108960, train_perplexity=254.58595, train_loss=5.5396385

Batch 108970, train_perplexity=233.92084, train_loss=5.4549828

Batch 108980, train_perplexity=265.64502, train_loss=5.582161

Batch 108990, train_perplexity=220.72894, train_loss=5.3969355

Batch 109000, train_perplexity=232.00153, train_loss=5.446744

Batch 109010, train_perplexity=252.55513, train_loss=5.5316296

Batch 109020, train_perplexity=230.33794, train_loss=5.4395475

Batch 109030, train_perplexity=233.13068, train_loss=5.451599

Batch 109040, train_perplexity=230.51736, train_loss=5.440326

Batch 109050, train_perplexity=244.74118, train_loss=5.500201

Batch 109060, train_perplexity=254.92476, train_loss=5.5409684

Batch 109070, train_perplexity=245.44942, train_loss=5.503091

Batch 109080, train_perplexity=231.0911, train_loss=5.442812

Batch 109090, train_perplexity=284.59763, train_loss=5.6510763

Batch 109100, train_perplexity=238.03276, train_loss=5.4724083

Batch 109110, train_perplexity=228.85788, train_loss=5.433101

Batch 109120, train_perplexity=232.08916, train_loss=5.4471216

Batch 109130, train_perplexity=230.70001, train_loss=5.4411182

Batch 109140, train_perplexity=249.61658, train_loss=5.519926

Batch 109150, train_perplexity=246.42606, train_loss=5.507062

Batch 109160, train_perplexity=219.77994, train_loss=5.392627

Batch 109170, train_perplexity=222.35887, train_loss=5.4042926

Batch 109180, train_perplexity=239.17484, train_loss=5.477195

Batch 109190, train_perplexity=228.70056, train_loss=5.4324136

Batch 109200, train_perplexity=221.12357, train_loss=5.3987217

Batch 109210, train_perplexity=225.02599, train_loss=5.416216

Batch 109220, train_perplexity=234.88274, train_loss=5.4590864

Batch 109230, train_perplexity=218.28943, train_loss=5.385822

Batch 109240, train_perplexity=259.78702, train_loss=5.559862

Batch 109250, train_perplexity=235.64424, train_loss=5.462323

Batch 109260, train_perplexity=240.79555, train_loss=5.483948

Batch 109270, train_perplexity=227.41855, train_loss=5.426792

Batch 109280, train_perplexity=242.06396, train_loss=5.489202

Batch 109290, train_perplexity=255.21945, train_loss=5.542124

Batch 109300, train_perplexity=273.17957, train_loss=5.6101294

Batch 109310, train_perplexity=264.35626, train_loss=5.5772977

Batch 109320, train_perplexity=206.11731, train_loss=5.3284454

Batch 109330, train_perplexity=240.14894, train_loss=5.4812593

Batch 109340, train_perplexity=247.72556, train_loss=5.5123215

Batch 109350, train_perplexity=220.89026, train_loss=5.397666

Batch 109360, train_perplexity=291.0302, train_loss=5.673427

Batch 109370, train_perplexity=248.63524, train_loss=5.515987

Batch 109380, train_perplexity=229.03079, train_loss=5.4338565

Batch 109390, train_perplexity=235.41658, train_loss=5.4613566

Batch 109400, train_perplexity=251.32327, train_loss=5.52674

Batch 109410, train_perplexity=276.28668, train_loss=5.621439

Batch 109420, train_perplexity=236.90991, train_loss=5.46768

Batch 109430, train_perplexity=228.35687, train_loss=5.4309096

Batch 109440, train_perplexity=237.56422, train_loss=5.470438

Batch 109450, train_perplexity=251.66241, train_loss=5.5280886

Batch 109460, train_perplexity=252.34543, train_loss=5.530799

Batch 109470, train_perplexity=228.45937, train_loss=5.4313583

Batch 109480, train_perplexity=232.54889, train_loss=5.4491005

Batch 109490, train_perplexity=241.40015, train_loss=5.486456

Batch 109500, train_perplexity=214.48062, train_loss=5.3682194

Batch 109510, train_perplexity=262.159, train_loss=5.568951

Batch 109520, train_perplexity=255.44981, train_loss=5.543026

Batch 109530, train_perplexity=226.01036, train_loss=5.420581

Batch 109540, train_perplexity=217.21974, train_loss=5.3809094

Batch 109550, train_perplexity=234.45842, train_loss=5.4572783

Batch 109560, train_perplexity=209.77849, train_loss=5.346052

Batch 109570, train_perplexity=216.45038, train_loss=5.3773613

Batch 109580, train_perplexity=272.16827, train_loss=5.6064205

Batch 109590, train_perplexity=218.09143, train_loss=5.3849144
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 109600, train_perplexity=238.301, train_loss=5.4735346

Batch 109610, train_perplexity=242.69513, train_loss=5.491806

Batch 109620, train_perplexity=255.9286, train_loss=5.5448985

Batch 109630, train_perplexity=244.90063, train_loss=5.5008526

Batch 109640, train_perplexity=231.29967, train_loss=5.443714

Batch 109650, train_perplexity=246.71294, train_loss=5.5082254

Batch 109660, train_perplexity=218.19087, train_loss=5.3853703

Batch 109670, train_perplexity=230.32828, train_loss=5.4395056

Batch 109680, train_perplexity=221.75638, train_loss=5.4015794

Batch 109690, train_perplexity=240.17288, train_loss=5.481359

Batch 109700, train_perplexity=230.42659, train_loss=5.4399323

Batch 109710, train_perplexity=249.96307, train_loss=5.521313

Batch 109720, train_perplexity=219.48816, train_loss=5.3912983

Batch 109730, train_perplexity=270.4296, train_loss=5.600012

Batch 109740, train_perplexity=230.46198, train_loss=5.440086

Batch 109750, train_perplexity=262.3662, train_loss=5.5697412

Batch 109760, train_perplexity=232.07002, train_loss=5.447039

Batch 109770, train_perplexity=241.4211, train_loss=5.4865427

Batch 109780, train_perplexity=227.08154, train_loss=5.425309

Batch 109790, train_perplexity=238.76393, train_loss=5.4754753

Batch 109800, train_perplexity=243.52328, train_loss=5.4952126

Batch 109810, train_perplexity=219.27528, train_loss=5.390328

Batch 109820, train_perplexity=253.59995, train_loss=5.535758

Batch 109830, train_perplexity=272.485, train_loss=5.6075835

Batch 109840, train_perplexity=239.13902, train_loss=5.477045

Batch 109850, train_perplexity=225.97739, train_loss=5.420435

Batch 109860, train_perplexity=230.71674, train_loss=5.4411907

Batch 109870, train_perplexity=232.25058, train_loss=5.447817

Batch 109880, train_perplexity=258.99146, train_loss=5.556795

Batch 109890, train_perplexity=247.27118, train_loss=5.5104856

Batch 109900, train_perplexity=217.42294, train_loss=5.3818445

Batch 109910, train_perplexity=248.78906, train_loss=5.5166054

Batch 109920, train_perplexity=236.93082, train_loss=5.467768

Batch 109930, train_perplexity=228.43736, train_loss=5.431262

Batch 109940, train_perplexity=221.26892, train_loss=5.399379

Batch 109950, train_perplexity=255.80318, train_loss=5.5444083

Batch 109960, train_perplexity=229.0165, train_loss=5.433794

Batch 109970, train_perplexity=255.87381, train_loss=5.5446844

Batch 109980, train_perplexity=224.20856, train_loss=5.4125767

Batch 109990, train_perplexity=254.53375, train_loss=5.5394335

Batch 110000, train_perplexity=280.81638, train_loss=5.637701

Batch 110010, train_perplexity=239.90196, train_loss=5.4802303

Batch 110020, train_perplexity=229.18243, train_loss=5.4345183

Batch 110030, train_perplexity=241.73834, train_loss=5.487856

Batch 110040, train_perplexity=250.25346, train_loss=5.5224743

Batch 110050, train_perplexity=211.91554, train_loss=5.356188

Batch 110060, train_perplexity=224.4852, train_loss=5.41381

Batch 110070, train_perplexity=242.11511, train_loss=5.4894133

Batch 110080, train_perplexity=232.26552, train_loss=5.447881

Batch 110090, train_perplexity=212.53275, train_loss=5.359096

Batch 110100, train_perplexity=247.60344, train_loss=5.5118284

Batch 110110, train_perplexity=238.48561, train_loss=5.474309

Batch 110120, train_perplexity=250.60945, train_loss=5.5238957

Batch 110130, train_perplexity=251.87888, train_loss=5.5289483

Batch 110140, train_perplexity=247.0634, train_loss=5.509645

Batch 110150, train_perplexity=214.56041, train_loss=5.3685913

Batch 110160, train_perplexity=235.75606, train_loss=5.4627976

Batch 110170, train_perplexity=225.65157, train_loss=5.418992

Batch 110180, train_perplexity=266.1159, train_loss=5.583932

Batch 110190, train_perplexity=221.31577, train_loss=5.3995905

Batch 110200, train_perplexity=256.51395, train_loss=5.547183

Batch 110210, train_perplexity=243.92842, train_loss=5.496875

Batch 110220, train_perplexity=227.93466, train_loss=5.429059

Batch 110230, train_perplexity=233.56317, train_loss=5.4534526

Batch 110240, train_perplexity=235.88066, train_loss=5.463326

Batch 110250, train_perplexity=236.96301, train_loss=5.467904

Batch 110260, train_perplexity=240.90201, train_loss=5.4843903

Batch 110270, train_perplexity=258.4123, train_loss=5.5545564

Batch 110280, train_perplexity=238.91745, train_loss=5.476118

Batch 110290, train_perplexity=260.38602, train_loss=5.5621653

Batch 110300, train_perplexity=230.4801, train_loss=5.4401646

Batch 110310, train_perplexity=241.74457, train_loss=5.4878817

Batch 110320, train_perplexity=242.69073, train_loss=5.491788

Batch 110330, train_perplexity=264.6914, train_loss=5.5785646

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Batch 110340, train_perplexity=212.72092, train_loss=5.359981

Batch 110350, train_perplexity=239.03732, train_loss=5.4766197

Batch 110360, train_perplexity=275.39383, train_loss=5.618202

Batch 110370, train_perplexity=227.35295, train_loss=5.4265037

Batch 110380, train_perplexity=233.85683, train_loss=5.454709

Batch 110390, train_perplexity=251.39291, train_loss=5.527017

Batch 110400, train_perplexity=255.8931, train_loss=5.5447598

Batch 110410, train_perplexity=232.1941, train_loss=5.4475737

Batch 110420, train_perplexity=249.05518, train_loss=5.5176744

Batch 110430, train_perplexity=234.4117, train_loss=5.457079

Batch 110440, train_perplexity=233.62099, train_loss=5.4537

Batch 110450, train_perplexity=231.78635, train_loss=5.445816

Batch 110460, train_perplexity=214.85567, train_loss=5.3699665

Batch 110470, train_perplexity=244.51709, train_loss=5.499285

Batch 110480, train_perplexity=253.88089, train_loss=5.536865

Batch 110490, train_perplexity=253.8321, train_loss=5.536673

Batch 110500, train_perplexity=230.94733, train_loss=5.4421897

Batch 110510, train_perplexity=241.72658, train_loss=5.4878073

Batch 110520, train_perplexity=254.42334, train_loss=5.5389996

Batch 110530, train_perplexity=222.68706, train_loss=5.4057674

Batch 110540, train_perplexity=247.47961, train_loss=5.511328

Batch 110550, train_perplexity=256.1331, train_loss=5.545697

Batch 110560, train_perplexity=239.99646, train_loss=5.480624

Batch 110570, train_perplexity=231.5643, train_loss=5.4448576

Batch 110580, train_perplexity=219.7638, train_loss=5.3925533

Batch 110590, train_perplexity=251.24756, train_loss=5.5264387

Batch 110600, train_perplexity=233.5996, train_loss=5.4536085

Batch 110610, train_perplexity=190.9758, train_loss=5.2521467

Batch 110620, train_perplexity=236.76503, train_loss=5.467068

Batch 110630, train_perplexity=241.93646, train_loss=5.488675

Batch 110640, train_perplexity=222.72157, train_loss=5.4059224

Batch 110650, train_perplexity=252.67583, train_loss=5.5321074

Batch 110660, train_perplexity=235.5611, train_loss=5.4619703

Batch 110670, train_perplexity=233.19872, train_loss=5.451891

Batch 110680, train_perplexity=241.0983, train_loss=5.4852047

Batch 110690, train_perplexity=249.7304, train_loss=5.520382

Batch 110700, train_perplexity=253.11647, train_loss=5.5338497

Batch 110710, train_perplexity=217.44057, train_loss=5.3819256

Batch 110720, train_perplexity=218.25227, train_loss=5.3856516

Batch 110730, train_perplexity=259.14935, train_loss=5.5574045

Batch 110740, train_perplexity=269.7913, train_loss=5.5976486

Batch 110750, train_perplexity=254.80481, train_loss=5.540498

Batch 110760, train_perplexity=239.92941, train_loss=5.480345

Batch 110770, train_perplexity=245.22504, train_loss=5.5021763

Batch 110780, train_perplexity=227.94531, train_loss=5.4291058

Batch 110790, train_perplexity=220.83623, train_loss=5.3974214

Batch 110800, train_perplexity=254.68248, train_loss=5.5400176

Batch 110810, train_perplexity=244.44003, train_loss=5.49897

Batch 110820, train_perplexity=215.71532, train_loss=5.3739595

Batch 110830, train_perplexity=226.32376, train_loss=5.4219666
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 110840, train_perplexity=279.36514, train_loss=5.6325197

Batch 110850, train_perplexity=245.03146, train_loss=5.5013866

Batch 110860, train_perplexity=234.23225, train_loss=5.456313

Batch 110870, train_perplexity=223.14423, train_loss=5.4078183

Batch 110880, train_perplexity=254.77602, train_loss=5.540385

Batch 110890, train_perplexity=235.93375, train_loss=5.463551

Batch 110900, train_perplexity=263.94125, train_loss=5.5757265

Batch 110910, train_perplexity=228.48267, train_loss=5.4314604

Batch 110920, train_perplexity=234.50952, train_loss=5.457496

Batch 110930, train_perplexity=243.44945, train_loss=5.4949093

Batch 110940, train_perplexity=189.92365, train_loss=5.246622

Batch 110950, train_perplexity=223.84439, train_loss=5.410951

Batch 110960, train_perplexity=258.2593, train_loss=5.553964

Batch 110970, train_perplexity=244.16896, train_loss=5.4978604

Batch 110980, train_perplexity=213.59534, train_loss=5.3640833

Batch 110990, train_perplexity=251.18239, train_loss=5.5261793

Batch 111000, train_perplexity=259.9351, train_loss=5.560432

Batch 111010, train_perplexity=205.64587, train_loss=5.3261557

Batch 111020, train_perplexity=243.33826, train_loss=5.4944525

Batch 111030, train_perplexity=253.22705, train_loss=5.5342865

Batch 111040, train_perplexity=228.10156, train_loss=5.429791

Batch 111050, train_perplexity=205.75888, train_loss=5.326705

Batch 111060, train_perplexity=216.38556, train_loss=5.377062

Batch 111070, train_perplexity=229.34456, train_loss=5.4352255

Batch 111080, train_perplexity=261.6053, train_loss=5.566837

Batch 111090, train_perplexity=232.97798, train_loss=5.450944

Batch 111100, train_perplexity=254.4032, train_loss=5.5389204

Batch 111110, train_perplexity=233.41655, train_loss=5.4528246

Batch 111120, train_perplexity=231.04547, train_loss=5.4426146

Batch 111130, train_perplexity=269.81726, train_loss=5.597745

Batch 111140, train_perplexity=227.9088, train_loss=5.4289455

Batch 111150, train_perplexity=219.78392, train_loss=5.392645

Batch 111160, train_perplexity=230.96947, train_loss=5.4422855

Batch 111170, train_perplexity=266.51376, train_loss=5.585426

Batch 111180, train_perplexity=259.3073, train_loss=5.558014

Batch 111190, train_perplexity=233.45105, train_loss=5.4529724

Batch 111200, train_perplexity=266.3378, train_loss=5.5847654

Batch 111210, train_perplexity=247.57286, train_loss=5.511705

Batch 111220, train_perplexity=229.37978, train_loss=5.435379

Batch 111230, train_perplexity=234.12383, train_loss=5.45585

Batch 111240, train_perplexity=227.67581, train_loss=5.4279227

Batch 111250, train_perplexity=244.5242, train_loss=5.4993143

Batch 111260, train_perplexity=260.17902, train_loss=5.56137

Batch 111270, train_perplexity=228.84991, train_loss=5.4330664

Batch 111280, train_perplexity=237.54407, train_loss=5.470353

Batch 111290, train_perplexity=238.89626, train_loss=5.4760294

Batch 111300, train_perplexity=216.44026, train_loss=5.3773146

Batch 111310, train_perplexity=236.31657, train_loss=5.4651723

Batch 111320, train_perplexity=252.40608, train_loss=5.531039

Batch 111330, train_perplexity=214.28363, train_loss=5.3673005

Batch 111340, train_perplexity=248.07817, train_loss=5.513744

Batch 111350, train_perplexity=237.70949, train_loss=5.4710493

Batch 111360, train_perplexity=251.35768, train_loss=5.526877

Batch 111370, train_perplexity=225.96931, train_loss=5.420399

Batch 111380, train_perplexity=216.79155, train_loss=5.3789363

Batch 111390, train_perplexity=221.13011, train_loss=5.3987513

Batch 111400, train_perplexity=234.94547, train_loss=5.4593534

Batch 111410, train_perplexity=234.72823, train_loss=5.4584284

Batch 111420, train_perplexity=226.33067, train_loss=5.421997

Batch 111430, train_perplexity=238.95642, train_loss=5.476281

Batch 111440, train_perplexity=223.85667, train_loss=5.411006

Batch 111450, train_perplexity=243.75494, train_loss=5.4961634

Batch 111460, train_perplexity=206.47487, train_loss=5.3301787

Batch 111470, train_perplexity=253.46237, train_loss=5.5352154

Batch 111480, train_perplexity=230.08382, train_loss=5.4384437

Batch 111490, train_perplexity=232.78055, train_loss=5.450096

Batch 111500, train_perplexity=228.6479, train_loss=5.4321833

Batch 111510, train_perplexity=224.30373, train_loss=5.413001

Batch 111520, train_perplexity=211.0952, train_loss=5.352309

Batch 111530, train_perplexity=232.49013, train_loss=5.448848

Batch 111540, train_perplexity=208.95964, train_loss=5.342141

Batch 111550, train_perplexity=232.1868, train_loss=5.447542

Batch 111560, train_perplexity=237.75325, train_loss=5.4712334

Batch 111570, train_perplexity=243.58159, train_loss=5.495452

Batch 111580, train_perplexity=246.98636, train_loss=5.509333

Batch 111590, train_perplexity=241.03438, train_loss=5.4849396

Batch 111600, train_perplexity=215.1654, train_loss=5.371407

Batch 111610, train_perplexity=225.19687, train_loss=5.416975

Batch 111620, train_perplexity=253.09135, train_loss=5.5337505

Batch 111630, train_perplexity=224.06181, train_loss=5.411922

Batch 111640, train_perplexity=241.98283, train_loss=5.488867

Batch 111650, train_perplexity=256.0619, train_loss=5.545419

Batch 111660, train_perplexity=238.14401, train_loss=5.4728756

Batch 111670, train_perplexity=239.92735, train_loss=5.480336

Batch 111680, train_perplexity=238.58946, train_loss=5.4747443

Batch 111690, train_perplexity=233.6495, train_loss=5.453822

Batch 111700, train_perplexity=237.4573, train_loss=5.469988

Batch 111710, train_perplexity=218.89804, train_loss=5.388606

Batch 111720, train_perplexity=225.4942, train_loss=5.4182944

Batch 111730, train_perplexity=256.04614, train_loss=5.5453577

Batch 111740, train_perplexity=250.63156, train_loss=5.523984

Batch 111750, train_perplexity=245.85376, train_loss=5.504737

Batch 111760, train_perplexity=239.30602, train_loss=5.477743

Batch 111770, train_perplexity=286.51562, train_loss=5.657793

Batch 111780, train_perplexity=220.97559, train_loss=5.398052

Batch 111790, train_perplexity=245.60887, train_loss=5.5037403

Batch 111800, train_perplexity=220.33272, train_loss=5.3951387

Batch 111810, train_perplexity=275.14838, train_loss=5.6173105

Batch 111820, train_perplexity=266.48782, train_loss=5.5853286

Batch 111830, train_perplexity=236.355, train_loss=5.465335

Batch 111840, train_perplexity=231.425, train_loss=5.444256

Batch 111850, train_perplexity=241.61595, train_loss=5.4873495

Batch 111860, train_perplexity=245.33824, train_loss=5.502638

Batch 111870, train_perplexity=251.60841, train_loss=5.527874

Batch 111880, train_perplexity=226.92914, train_loss=5.424638

Batch 111890, train_perplexity=247.33109, train_loss=5.510728

Batch 111900, train_perplexity=254.4692, train_loss=5.53918

Batch 111910, train_perplexity=253.23344, train_loss=5.534312

Batch 111920, train_perplexity=253.57649, train_loss=5.5356655

Batch 111930, train_perplexity=218.0983, train_loss=5.384946

Batch 111940, train_perplexity=240.59023, train_loss=5.483095

Batch 111950, train_perplexity=239.59158, train_loss=5.4789357

Batch 111960, train_perplexity=245.9478, train_loss=5.5051193

Batch 111970, train_perplexity=235.9845, train_loss=5.463766

Batch 111980, train_perplexity=220.09163, train_loss=5.394044

Batch 111990, train_perplexity=232.40611, train_loss=5.4484863

Batch 112000, train_perplexity=225.10562, train_loss=5.4165697

Batch 112010, train_perplexity=218.23749, train_loss=5.385584

Batch 112020, train_perplexity=254.71443, train_loss=5.540143

Batch 112030, train_perplexity=262.0009, train_loss=5.568348

Batch 112040, train_perplexity=266.064, train_loss=5.583737

Batch 112050, train_perplexity=226.76862, train_loss=5.42393

Batch 112060, train_perplexity=254.64655, train_loss=5.5398765

Batch 112070, train_perplexity=227.61426, train_loss=5.4276524

Batch 112080, train_perplexity=211.08714, train_loss=5.352271

Batch 112090, train_perplexity=228.43932, train_loss=5.4312706

Batch 112100, train_perplexity=253.1312, train_loss=5.533908

Batch 112110, train_perplexity=257.2565, train_loss=5.5500736

Batch 112120, train_perplexity=241.85594, train_loss=5.4883423

Batch 112130, train_perplexity=252.5308, train_loss=5.5315332
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 112140, train_perplexity=239.1892, train_loss=5.477255

Batch 112150, train_perplexity=250.13966, train_loss=5.5220194

Batch 112160, train_perplexity=233.99101, train_loss=5.4552827

Batch 112170, train_perplexity=232.13454, train_loss=5.447317

Batch 112180, train_perplexity=236.84576, train_loss=5.467409

Batch 112190, train_perplexity=205.40744, train_loss=5.3249955

Batch 112200, train_perplexity=244.66382, train_loss=5.499885

Batch 112210, train_perplexity=245.67282, train_loss=5.5040007

Batch 112220, train_perplexity=243.66255, train_loss=5.4957843

Batch 112230, train_perplexity=243.20033, train_loss=5.4938855

Batch 112240, train_perplexity=251.46114, train_loss=5.5272884

Batch 112250, train_perplexity=224.48338, train_loss=5.4138017

Batch 112260, train_perplexity=260.35114, train_loss=5.5620313

Batch 112270, train_perplexity=232.10487, train_loss=5.4471893

Batch 112280, train_perplexity=243.77423, train_loss=5.4962425

Batch 112290, train_perplexity=229.24954, train_loss=5.434811

Batch 112300, train_perplexity=242.7824, train_loss=5.4921656

Batch 112310, train_perplexity=216.63478, train_loss=5.378213

Batch 112320, train_perplexity=248.3684, train_loss=5.514913

Batch 112330, train_perplexity=232.95976, train_loss=5.4508657

Batch 112340, train_perplexity=245.36328, train_loss=5.50274

Batch 112350, train_perplexity=240.83861, train_loss=5.484127

Batch 112360, train_perplexity=251.99913, train_loss=5.5294256

Batch 112370, train_perplexity=208.56743, train_loss=5.3402624

Batch 112380, train_perplexity=245.06477, train_loss=5.5015225

Batch 112390, train_perplexity=248.3922, train_loss=5.515009

Batch 112400, train_perplexity=222.36046, train_loss=5.4042997

Batch 112410, train_perplexity=267.5899, train_loss=5.5894556

Batch 112420, train_perplexity=258.17374, train_loss=5.5536327

Batch 112430, train_perplexity=234.48615, train_loss=5.4573965

Batch 112440, train_perplexity=227.09259, train_loss=5.425358

Batch 112450, train_perplexity=246.52336, train_loss=5.507457

Batch 112460, train_perplexity=254.98639, train_loss=5.54121

Batch 112470, train_perplexity=255.49818, train_loss=5.5432153

Batch 112480, train_perplexity=240.85721, train_loss=5.4842043

Batch 112490, train_perplexity=221.62593, train_loss=5.400991

Batch 112500, train_perplexity=269.81998, train_loss=5.597755

Batch 112510, train_perplexity=278.25797, train_loss=5.6285486

Batch 112520, train_perplexity=235.15001, train_loss=5.4602237

Batch 112530, train_perplexity=237.55199, train_loss=5.4703865

Batch 112540, train_perplexity=246.04587, train_loss=5.505518

Batch 112550, train_perplexity=261.56512, train_loss=5.5666833

Batch 112560, train_perplexity=214.41231, train_loss=5.367901

Batch 112570, train_perplexity=220.98412, train_loss=5.398091

Batch 112580, train_perplexity=231.9866, train_loss=5.4466796

Batch 112590, train_perplexity=239.63705, train_loss=5.4791255

Batch 112600, train_perplexity=257.61642, train_loss=5.5514717

Batch 112610, train_perplexity=227.07721, train_loss=5.42529

Batch 112620, train_perplexity=246.90735, train_loss=5.509013

Batch 112630, train_perplexity=232.96153, train_loss=5.4508734

Batch 112640, train_perplexity=243.19975, train_loss=5.493883

Batch 112650, train_perplexity=232.00816, train_loss=5.4467726

Batch 112660, train_perplexity=229.61418, train_loss=5.4364004

Batch 112670, train_perplexity=242.97395, train_loss=5.4929543

Batch 112680, train_perplexity=236.12497, train_loss=5.464361

Batch 112690, train_perplexity=254.44711, train_loss=5.539093

Batch 112700, train_perplexity=216.55898, train_loss=5.377863

Batch 112710, train_perplexity=239.80383, train_loss=5.479821

Batch 112720, train_perplexity=209.39972, train_loss=5.344245

Batch 112730, train_perplexity=253.86685, train_loss=5.53681

Batch 112740, train_perplexity=239.68791, train_loss=5.4793377

Batch 112750, train_perplexity=256.6299, train_loss=5.547635

Batch 112760, train_perplexity=244.11623, train_loss=5.4976444

Batch 112770, train_perplexity=214.73297, train_loss=5.3693953

Batch 112780, train_perplexity=278.7948, train_loss=5.630476

Batch 112790, train_perplexity=239.12408, train_loss=5.4769826

Batch 112800, train_perplexity=255.8, train_loss=5.544396

Batch 112810, train_perplexity=214.1776, train_loss=5.3668056

Batch 112820, train_perplexity=232.7002, train_loss=5.449751

Batch 112830, train_perplexity=279.37088, train_loss=5.63254

Batch 112840, train_perplexity=214.35016, train_loss=5.367611

Batch 112850, train_perplexity=227.81569, train_loss=5.428537

Batch 112860, train_perplexity=224.08821, train_loss=5.4120398

Batch 112870, train_perplexity=252.0449, train_loss=5.5296073

Batch 112880, train_perplexity=224.72424, train_loss=5.414874

Batch 112890, train_perplexity=235.09026, train_loss=5.4599695

Batch 112900, train_perplexity=217.77449, train_loss=5.38346

Batch 112910, train_perplexity=217.05107, train_loss=5.3801327

Batch 112920, train_perplexity=246.32501, train_loss=5.506652

Batch 112930, train_perplexity=245.34877, train_loss=5.502681

Batch 112940, train_perplexity=238.01721, train_loss=5.472343

Batch 112950, train_perplexity=219.02689, train_loss=5.3891945

Batch 112960, train_perplexity=240.7121, train_loss=5.4836016

Batch 112970, train_perplexity=216.77118, train_loss=5.3788424

Batch 112980, train_perplexity=220.31738, train_loss=5.395069

Batch 112990, train_perplexity=237.5418, train_loss=5.4703436

Batch 113000, train_perplexity=223.89862, train_loss=5.4111934

Batch 113010, train_perplexity=227.5995, train_loss=5.4275875

Batch 113020, train_perplexity=240.49203, train_loss=5.482687

Batch 113030, train_perplexity=232.19089, train_loss=5.44756

Batch 113040, train_perplexity=262.30002, train_loss=5.569489

Batch 113050, train_perplexity=227.40271, train_loss=5.4267225

Batch 113060, train_perplexity=229.92995, train_loss=5.4377747

Batch 113070, train_perplexity=243.66638, train_loss=5.4958

Batch 113080, train_perplexity=255.534, train_loss=5.5433555

Batch 113090, train_perplexity=231.75916, train_loss=5.4456987

Batch 113100, train_perplexity=226.78484, train_loss=5.4240017

Batch 113110, train_perplexity=221.98817, train_loss=5.402624

Batch 113120, train_perplexity=218.58513, train_loss=5.3871756

Batch 113130, train_perplexity=226.21436, train_loss=5.421483

Batch 113140, train_perplexity=241.01254, train_loss=5.484849

Batch 113150, train_perplexity=227.6198, train_loss=5.4276767

Batch 113160, train_perplexity=258.01288, train_loss=5.5530095

Batch 113170, train_perplexity=227.34698, train_loss=5.4264774

Batch 113180, train_perplexity=261.92682, train_loss=5.568065

Batch 113190, train_perplexity=218.82832, train_loss=5.3882875

Batch 113200, train_perplexity=236.54239, train_loss=5.4661274

Batch 113210, train_perplexity=227.77798, train_loss=5.4283714

Batch 113220, train_perplexity=264.09558, train_loss=5.576311

Batch 113230, train_perplexity=233.18648, train_loss=5.4518385

Batch 113240, train_perplexity=241.1751, train_loss=5.485523

Batch 113250, train_perplexity=216.40207, train_loss=5.377138

Batch 113260, train_perplexity=231.95053, train_loss=5.446524

Batch 113270, train_perplexity=233.5135, train_loss=5.45324

Batch 113280, train_perplexity=188.28487, train_loss=5.237956

Batch 113290, train_perplexity=218.0958, train_loss=5.3849344

Batch 113300, train_perplexity=253.71207, train_loss=5.5362

Batch 113310, train_perplexity=227.31209, train_loss=5.426324

Batch 113320, train_perplexity=224.6724, train_loss=5.4146433

Batch 113330, train_perplexity=225.52538, train_loss=5.4184327

Batch 113340, train_perplexity=231.44376, train_loss=5.444337

Batch 113350, train_perplexity=236.8706, train_loss=5.467514

Batch 113360, train_perplexity=220.51056, train_loss=5.3959455

Batch 113370, train_perplexity=221.02417, train_loss=5.398272

Batch 113380, train_perplexity=231.47311, train_loss=5.4444637

Batch 113390, train_perplexity=232.64716, train_loss=5.449523

Batch 113400, train_perplexity=221.67635, train_loss=5.4012184

Batch 113410, train_perplexity=243.10954, train_loss=5.493512

Batch 113420, train_perplexity=234.0699, train_loss=5.45562

Batch 113430, train_perplexity=235.33577, train_loss=5.4610133
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 113440, train_perplexity=214.34137, train_loss=5.36757

Batch 113450, train_perplexity=243.26076, train_loss=5.494134

Batch 113460, train_perplexity=233.132, train_loss=5.451605

Batch 113470, train_perplexity=223.03711, train_loss=5.407338

Batch 113480, train_perplexity=225.09338, train_loss=5.4165154

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Batch 113490, train_perplexity=211.97043, train_loss=5.3564467

Batch 113500, train_perplexity=195.49739, train_loss=5.275547

Batch 113510, train_perplexity=249.82854, train_loss=5.520775

Batch 113520, train_perplexity=205.53864, train_loss=5.325634

Batch 113530, train_perplexity=240.38634, train_loss=5.4822474

Batch 113540, train_perplexity=293.5032, train_loss=5.6818886

Batch 113550, train_perplexity=251.75027, train_loss=5.5284376

Batch 113560, train_perplexity=244.62311, train_loss=5.4997187

Batch 113570, train_perplexity=246.90947, train_loss=5.5090218

Batch 113580, train_perplexity=254.60004, train_loss=5.539694

Batch 113590, train_perplexity=214.19792, train_loss=5.3669004

Batch 113600, train_perplexity=229.76555, train_loss=5.4370594

Batch 113610, train_perplexity=198.16655, train_loss=5.289108

Batch 113620, train_perplexity=227.63576, train_loss=5.427747

Batch 113630, train_perplexity=228.10101, train_loss=5.4297886

Batch 113640, train_perplexity=234.72308, train_loss=5.4584064

Batch 113650, train_perplexity=249.25143, train_loss=5.518462

Batch 113660, train_perplexity=243.15848, train_loss=5.4937134

Batch 113670, train_perplexity=236.26012, train_loss=5.4649334

Batch 113680, train_perplexity=223.85187, train_loss=5.4109845

Batch 113690, train_perplexity=266.36346, train_loss=5.5848618

Batch 113700, train_perplexity=236.46829, train_loss=5.465814

Batch 113710, train_perplexity=214.5289, train_loss=5.3684444

Batch 113720, train_perplexity=234.6246, train_loss=5.457987

Batch 113730, train_perplexity=224.7639, train_loss=5.4150505

Batch 113740, train_perplexity=222.51318, train_loss=5.4049864

Batch 113750, train_perplexity=220.87329, train_loss=5.397589

Batch 113760, train_perplexity=214.18709, train_loss=5.36685

Batch 113770, train_perplexity=233.62923, train_loss=5.4537354

Batch 113780, train_perplexity=234.39783, train_loss=5.45702

Batch 113790, train_perplexity=258.27545, train_loss=5.5540266

Batch 113800, train_perplexity=235.74043, train_loss=5.4627314

Batch 113810, train_perplexity=243.16602, train_loss=5.4937444

Batch 113820, train_perplexity=245.59106, train_loss=5.503668

Batch 113830, train_perplexity=200.45554, train_loss=5.3005924

Batch 113840, train_perplexity=231.48482, train_loss=5.4445143

Batch 113850, train_perplexity=246.0901, train_loss=5.5056977

Batch 113860, train_perplexity=230.20398, train_loss=5.438966

Batch 113870, train_perplexity=248.60039, train_loss=5.5158467

Batch 113880, train_perplexity=238.76154, train_loss=5.4754653

Batch 113890, train_perplexity=247.12833, train_loss=5.5099077

Batch 113900, train_perplexity=223.40262, train_loss=5.4089756

Batch 113910, train_perplexity=239.36342, train_loss=5.477983

Batch 113920, train_perplexity=224.38673, train_loss=5.413371

Batch 113930, train_perplexity=264.3739, train_loss=5.5773644

Batch 113940, train_perplexity=232.67778, train_loss=5.4496546

Batch 113950, train_perplexity=233.56607, train_loss=5.453465

Batch 113960, train_perplexity=228.46992, train_loss=5.4314046

Batch 113970, train_perplexity=244.93333, train_loss=5.500986

Batch 113980, train_perplexity=237.77206, train_loss=5.4713125

Batch 113990, train_perplexity=222.14426, train_loss=5.403327

Batch 114000, train_perplexity=213.33537, train_loss=5.3628654

Batch 114010, train_perplexity=230.3509, train_loss=5.439604

Batch 114020, train_perplexity=209.91878, train_loss=5.3467207

Batch 114030, train_perplexity=244.18712, train_loss=5.497935

Batch 114040, train_perplexity=229.79819, train_loss=5.4372015

Batch 114050, train_perplexity=217.59346, train_loss=5.3826284

Batch 114060, train_perplexity=261.71246, train_loss=5.5672464

Batch 114070, train_perplexity=228.84685, train_loss=5.433053

Batch 114080, train_perplexity=229.31918, train_loss=5.435115

Batch 114090, train_perplexity=225.08716, train_loss=5.4164877

Batch 114100, train_perplexity=227.1801, train_loss=5.425743

Batch 114110, train_perplexity=240.66687, train_loss=5.4834137

Batch 114120, train_perplexity=224.97353, train_loss=5.4159827

Batch 114130, train_perplexity=243.36343, train_loss=5.494556

Batch 114140, train_perplexity=265.13882, train_loss=5.5802536

Batch 114150, train_perplexity=279.49652, train_loss=5.63299

Batch 114160, train_perplexity=221.73914, train_loss=5.4015017

Batch 114170, train_perplexity=243.0451, train_loss=5.493247

Batch 114180, train_perplexity=246.77682, train_loss=5.5084844

Batch 114190, train_perplexity=258.40872, train_loss=5.5545425

Batch 114200, train_perplexity=240.41946, train_loss=5.482385

Batch 114210, train_perplexity=244.72017, train_loss=5.5001154

Batch 114220, train_perplexity=251.78929, train_loss=5.5285926

Batch 114230, train_perplexity=224.47578, train_loss=5.413768

Batch 114240, train_perplexity=258.11685, train_loss=5.5534124

Batch 114250, train_perplexity=242.2054, train_loss=5.489786

Batch 114260, train_perplexity=220.6476, train_loss=5.396567

Batch 114270, train_perplexity=240.40536, train_loss=5.4823265

Batch 114280, train_perplexity=228.3242, train_loss=5.4307666

Batch 114290, train_perplexity=218.63371, train_loss=5.387398

Batch 114300, train_perplexity=240.22935, train_loss=5.481594

Batch 114310, train_perplexity=220.91985, train_loss=5.3978

Batch 114320, train_perplexity=205.82619, train_loss=5.327032

Batch 114330, train_perplexity=244.53214, train_loss=5.4993467

Batch 114340, train_perplexity=235.04744, train_loss=5.4597874

Batch 114350, train_perplexity=234.32117, train_loss=5.4566927

Batch 114360, train_perplexity=263.6792, train_loss=5.5747333

Batch 114370, train_perplexity=228.6623, train_loss=5.432246

Batch 114380, train_perplexity=239.47086, train_loss=5.4784317

Batch 114390, train_perplexity=208.53442, train_loss=5.340104

Batch 114400, train_perplexity=255.71294, train_loss=5.5440555

Batch 114410, train_perplexity=247.0171, train_loss=5.5094576

Batch 114420, train_perplexity=244.17117, train_loss=5.4978695

Batch 114430, train_perplexity=239.58392, train_loss=5.478904

Batch 114440, train_perplexity=214.92863, train_loss=5.370306

Batch 114450, train_perplexity=254.7929, train_loss=5.540451

Batch 114460, train_perplexity=253.58676, train_loss=5.535706

Batch 114470, train_perplexity=226.10931, train_loss=5.4210186

Batch 114480, train_perplexity=233.45662, train_loss=5.4529963

Batch 114490, train_perplexity=222.72018, train_loss=5.405916

Batch 114500, train_perplexity=270.28033, train_loss=5.5994596

Batch 114510, train_perplexity=231.19095, train_loss=5.443244

Batch 114520, train_perplexity=232.29454, train_loss=5.448006

Batch 114530, train_perplexity=245.7811, train_loss=5.5044413

Batch 114540, train_perplexity=271.4047, train_loss=5.603611

Batch 114550, train_perplexity=226.96875, train_loss=5.4248123

Batch 114560, train_perplexity=236.46773, train_loss=5.4658117

Batch 114570, train_perplexity=246.94833, train_loss=5.509179

Batch 114580, train_perplexity=254.06412, train_loss=5.5375867

Batch 114590, train_perplexity=203.88448, train_loss=5.3175535

Batch 114600, train_perplexity=252.66342, train_loss=5.5320582

Batch 114610, train_perplexity=218.94157, train_loss=5.388805

Batch 114620, train_perplexity=249.81044, train_loss=5.5207024

Batch 114630, train_perplexity=254.78099, train_loss=5.5404043

Batch 114640, train_perplexity=220.85318, train_loss=5.397498

Batch 114650, train_perplexity=243.971, train_loss=5.4970493

Batch 114660, train_perplexity=229.00404, train_loss=5.4337397

Batch 114670, train_perplexity=250.08504, train_loss=5.521801
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 114680, train_perplexity=230.07887, train_loss=5.438422

Batch 114690, train_perplexity=229.44891, train_loss=5.4356804

Batch 114700, train_perplexity=234.0775, train_loss=5.455652

Batch 114710, train_perplexity=242.82883, train_loss=5.492357

Batch 114720, train_perplexity=269.2782, train_loss=5.595745

Batch 114730, train_perplexity=265.4556, train_loss=5.5814476

Batch 114740, train_perplexity=229.65863, train_loss=5.436594

Batch 114750, train_perplexity=211.1994, train_loss=5.3528028

Batch 114760, train_perplexity=206.94199, train_loss=5.3324385

Batch 114770, train_perplexity=230.40231, train_loss=5.439827

Batch 114780, train_perplexity=238.04524, train_loss=5.4724607

Batch 114790, train_perplexity=240.29774, train_loss=5.4818788

Batch 114800, train_perplexity=254.44408, train_loss=5.539081

Batch 114810, train_perplexity=241.66136, train_loss=5.4875374

Batch 114820, train_perplexity=234.27782, train_loss=5.4565077

Batch 114830, train_perplexity=243.57938, train_loss=5.495443

Batch 114840, train_perplexity=219.18988, train_loss=5.3899384

Batch 114850, train_perplexity=211.24725, train_loss=5.3530293

Batch 114860, train_perplexity=220.99855, train_loss=5.398156

Batch 114870, train_perplexity=222.53813, train_loss=5.4050984

Batch 114880, train_perplexity=251.58119, train_loss=5.5277658

Batch 114890, train_perplexity=227.30753, train_loss=5.426304

Batch 114900, train_perplexity=236.84587, train_loss=5.4674096

Batch 114910, train_perplexity=212.48886, train_loss=5.3588896

Batch 114920, train_perplexity=229.68668, train_loss=5.436716

Batch 114930, train_perplexity=233.44392, train_loss=5.452942

Batch 114940, train_perplexity=246.85956, train_loss=5.5088196

Batch 114950, train_perplexity=236.99115, train_loss=5.468023

Batch 114960, train_perplexity=245.81953, train_loss=5.5045977

Batch 114970, train_perplexity=230.15317, train_loss=5.438745

Batch 114980, train_perplexity=205.76732, train_loss=5.326746

Batch 114990, train_perplexity=232.84404, train_loss=5.450369

Batch 115000, train_perplexity=236.70508, train_loss=5.466815

Batch 115010, train_perplexity=242.05301, train_loss=5.4891567

Batch 115020, train_perplexity=225.15189, train_loss=5.416775

Batch 115030, train_perplexity=213.22441, train_loss=5.362345

Batch 115040, train_perplexity=239.90848, train_loss=5.4802575

Batch 115050, train_perplexity=234.10205, train_loss=5.455757

Batch 115060, train_perplexity=225.60228, train_loss=5.4187737

Batch 115070, train_perplexity=244.6392, train_loss=5.4997845

Batch 115080, train_perplexity=188.71432, train_loss=5.2402344

Batch 115090, train_perplexity=216.4609, train_loss=5.37741

Batch 115100, train_perplexity=219.89462, train_loss=5.3931484

Batch 115110, train_perplexity=209.6303, train_loss=5.3453455

Batch 115120, train_perplexity=240.7812, train_loss=5.4838886

Batch 115130, train_perplexity=236.5275, train_loss=5.4660645

Batch 115140, train_perplexity=233.22563, train_loss=5.4520063

Batch 115150, train_perplexity=249.74992, train_loss=5.52046

Batch 115160, train_perplexity=254.24312, train_loss=5.538291

Batch 115170, train_perplexity=207.44713, train_loss=5.3348765

Batch 115180, train_perplexity=216.7685, train_loss=5.37883

Batch 115190, train_perplexity=231.69408, train_loss=5.445418

Batch 115200, train_perplexity=234.5179, train_loss=5.457532

Batch 115210, train_perplexity=221.83263, train_loss=5.401923

Batch 115220, train_perplexity=215.25293, train_loss=5.371814

Batch 115230, train_perplexity=219.67181, train_loss=5.3921347

Batch 115240, train_perplexity=234.94412, train_loss=5.4593477

Batch 115250, train_perplexity=247.1328, train_loss=5.509926

Batch 115260, train_perplexity=227.92227, train_loss=5.4290047

Batch 115270, train_perplexity=215.13976, train_loss=5.371288

Batch 115280, train_perplexity=229.7708, train_loss=5.4370823

Batch 115290, train_perplexity=200.0105, train_loss=5.29837

Batch 115300, train_perplexity=253.46793, train_loss=5.5352373

Batch 115310, train_perplexity=227.53027, train_loss=5.4272833

Batch 115320, train_perplexity=253.32185, train_loss=5.534661

Batch 115330, train_perplexity=231.94003, train_loss=5.446479

Batch 115340, train_perplexity=244.51616, train_loss=5.4992814

Batch 115350, train_perplexity=241.03024, train_loss=5.4849224

Batch 115360, train_perplexity=229.44278, train_loss=5.4356537

Batch 115370, train_perplexity=203.80554, train_loss=5.3171663

Batch 115380, train_perplexity=247.0442, train_loss=5.5095673

Batch 115390, train_perplexity=218.22438, train_loss=5.385524

Batch 115400, train_perplexity=237.46614, train_loss=5.470025

Batch 115410, train_perplexity=273.3388, train_loss=5.610712

Batch 115420, train_perplexity=224.01215, train_loss=5.4117002

Batch 115430, train_perplexity=244.77934, train_loss=5.500357

Batch 115440, train_perplexity=235.50708, train_loss=5.461741

Batch 115450, train_perplexity=217.09506, train_loss=5.3803353

Batch 115460, train_perplexity=244.96407, train_loss=5.5011115

Batch 115470, train_perplexity=232.35071, train_loss=5.448248

Batch 115480, train_perplexity=226.83047, train_loss=5.424203

Batch 115490, train_perplexity=216.92256, train_loss=5.3795404

Batch 115500, train_perplexity=246.4881, train_loss=5.5073137

Batch 115510, train_perplexity=266.54208, train_loss=5.585532

Batch 115520, train_perplexity=235.99147, train_loss=5.4637957

Batch 115530, train_perplexity=234.34195, train_loss=5.4567814

Batch 115540, train_perplexity=253.53104, train_loss=5.535486

Batch 115550, train_perplexity=240.27243, train_loss=5.4817734

Batch 115560, train_perplexity=223.11539, train_loss=5.407689

Batch 115570, train_perplexity=229.35866, train_loss=5.435287

Batch 115580, train_perplexity=224.82253, train_loss=5.4153113

Batch 115590, train_perplexity=227.11035, train_loss=5.425436

Batch 115600, train_perplexity=235.23503, train_loss=5.460585

Batch 115610, train_perplexity=232.82028, train_loss=5.450267

Batch 115620, train_perplexity=237.93176, train_loss=5.471984

Batch 115630, train_perplexity=223.27812, train_loss=5.408418

Batch 115640, train_perplexity=238.29486, train_loss=5.473509

Batch 115650, train_perplexity=212.4289, train_loss=5.3586073

Batch 115660, train_perplexity=240.51532, train_loss=5.482784

Batch 115670, train_perplexity=248.95757, train_loss=5.5172825

Batch 115680, train_perplexity=219.61055, train_loss=5.3918557

Batch 115690, train_perplexity=246.69717, train_loss=5.5081615

Batch 115700, train_perplexity=240.16498, train_loss=5.481326

Batch 115710, train_perplexity=225.30579, train_loss=5.4174585

Batch 115720, train_perplexity=228.8606, train_loss=5.433113

Batch 115730, train_perplexity=216.01958, train_loss=5.375369

Batch 115740, train_perplexity=255.73317, train_loss=5.5441346

Batch 115750, train_perplexity=217.78342, train_loss=5.383501

Batch 115760, train_perplexity=229.56097, train_loss=5.4361687

Batch 115770, train_perplexity=228.3008, train_loss=5.430664

Batch 115780, train_perplexity=237.08847, train_loss=5.4684334

Batch 115790, train_perplexity=229.80673, train_loss=5.4372387

Batch 115800, train_perplexity=235.91069, train_loss=5.4634533

Batch 115810, train_perplexity=226.14188, train_loss=5.4211626

Batch 115820, train_perplexity=254.60927, train_loss=5.53973

Batch 115830, train_perplexity=223.82582, train_loss=5.410868

Batch 115840, train_perplexity=223.44064, train_loss=5.409146

Batch 115850, train_perplexity=208.31279, train_loss=5.3390408

Batch 115860, train_perplexity=251.15196, train_loss=5.526058

Batch 115870, train_perplexity=210.75827, train_loss=5.350712

Batch 115880, train_perplexity=200.47206, train_loss=5.300675

Batch 115890, train_perplexity=211.47139, train_loss=5.3540897

Batch 115900, train_perplexity=247.32071, train_loss=5.510686

Batch 115910, train_perplexity=214.48277, train_loss=5.3682294

Batch 115920, train_perplexity=252.26843, train_loss=5.5304937

Batch 115930, train_perplexity=215.20767, train_loss=5.3716035

Batch 115940, train_perplexity=217.9829, train_loss=5.3844166

Batch 115950, train_perplexity=206.84589, train_loss=5.331974

Batch 115960, train_perplexity=226.44553, train_loss=5.4225044

Batch 115970, train_perplexity=241.66608, train_loss=5.487557
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 115980, train_perplexity=223.4366, train_loss=5.4091277

Batch 115990, train_perplexity=230.06177, train_loss=5.438348

Batch 116000, train_perplexity=213.88887, train_loss=5.3654566

Batch 116010, train_perplexity=207.71092, train_loss=5.3361473

Batch 116020, train_perplexity=236.5814, train_loss=5.4662924

Batch 116030, train_perplexity=231.51418, train_loss=5.444641

Batch 116040, train_perplexity=241.00967, train_loss=5.484837

Batch 116050, train_perplexity=252.745, train_loss=5.532381

Batch 116060, train_perplexity=225.94765, train_loss=5.4203033

Batch 116070, train_perplexity=240.17047, train_loss=5.481349

Batch 116080, train_perplexity=213.1054, train_loss=5.361787

Batch 116090, train_perplexity=223.40305, train_loss=5.4089775

Batch 116100, train_perplexity=256.17242, train_loss=5.5458508

Batch 116110, train_perplexity=232.40257, train_loss=5.448471

Batch 116120, train_perplexity=234.73405, train_loss=5.458453

Batch 116130, train_perplexity=239.83768, train_loss=5.4799623

Batch 116140, train_perplexity=208.08287, train_loss=5.3379364

Batch 116150, train_perplexity=228.16782, train_loss=5.4300814

Batch 116160, train_perplexity=245.89268, train_loss=5.504895

Batch 116170, train_perplexity=222.0822, train_loss=5.4030476

Batch 116180, train_perplexity=245.17534, train_loss=5.5019736

Batch 116190, train_perplexity=237.77184, train_loss=5.4713116

Batch 116200, train_perplexity=200.77063, train_loss=5.302163

Batch 116210, train_perplexity=210.36148, train_loss=5.3488274

Batch 116220, train_perplexity=240.31482, train_loss=5.48195

Batch 116230, train_perplexity=232.27129, train_loss=5.447906

Batch 116240, train_perplexity=239.3665, train_loss=5.477996

Batch 116250, train_perplexity=203.72354, train_loss=5.316764

Batch 116260, train_perplexity=233.71725, train_loss=5.454112

Batch 116270, train_perplexity=229.09338, train_loss=5.4341297

Batch 116280, train_perplexity=227.61057, train_loss=5.427636

Batch 116290, train_perplexity=223.66226, train_loss=5.410137

Batch 116300, train_perplexity=210.92857, train_loss=5.3515196

Batch 116310, train_perplexity=221.66768, train_loss=5.4011793

Batch 116320, train_perplexity=224.21925, train_loss=5.4126244

Batch 116330, train_perplexity=216.46957, train_loss=5.37745

Batch 116340, train_perplexity=237.30212, train_loss=5.469334

Batch 116350, train_perplexity=225.46011, train_loss=5.4181433

Batch 116360, train_perplexity=221.15826, train_loss=5.3988786

Batch 116370, train_perplexity=237.30258, train_loss=5.469336

Batch 116380, train_perplexity=218.74821, train_loss=5.3879213

Batch 116390, train_perplexity=229.11075, train_loss=5.4342055

Batch 116400, train_perplexity=245.1239, train_loss=5.501764

Batch 116410, train_perplexity=236.1574, train_loss=5.4644985

Batch 116420, train_perplexity=239.7134, train_loss=5.479444

Batch 116430, train_perplexity=212.97699, train_loss=5.361184

Batch 116440, train_perplexity=221.62666, train_loss=5.4009943

Batch 116450, train_perplexity=226.86693, train_loss=5.4243636

Batch 116460, train_perplexity=254.97557, train_loss=5.5411677

Batch 116470, train_perplexity=230.88203, train_loss=5.441907

Batch 116480, train_perplexity=225.46065, train_loss=5.4181457

Batch 116490, train_perplexity=245.30106, train_loss=5.502486

Batch 116500, train_perplexity=238.971, train_loss=5.476342

Batch 116510, train_perplexity=227.98923, train_loss=5.4292984

Batch 116520, train_perplexity=266.55023, train_loss=5.5855627

Batch 116530, train_perplexity=232.7033, train_loss=5.4497643

Batch 116540, train_perplexity=227.97989, train_loss=5.4292574

Batch 116550, train_perplexity=256.80252, train_loss=5.5483074

Batch 116560, train_perplexity=233.4958, train_loss=5.453164

Batch 116570, train_perplexity=213.72636, train_loss=5.3646965

Batch 116580, train_perplexity=245.22223, train_loss=5.502165

Batch 116590, train_perplexity=246.04352, train_loss=5.5055084

Batch 116600, train_perplexity=251.69218, train_loss=5.528207

Batch 116610, train_perplexity=220.30655, train_loss=5.39502

Batch 116620, train_perplexity=230.06439, train_loss=5.4383593

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Batch 116630, train_perplexity=225.58055, train_loss=5.4186773

Batch 116640, train_perplexity=239.4761, train_loss=5.4784536

Batch 116650, train_perplexity=226.51756, train_loss=5.4228225

Batch 116660, train_perplexity=213.40985, train_loss=5.3632145

Batch 116670, train_perplexity=249.82901, train_loss=5.5207767

Batch 116680, train_perplexity=243.58205, train_loss=5.495454

Batch 116690, train_perplexity=219.17023, train_loss=5.3898487

Batch 116700, train_perplexity=223.86136, train_loss=5.411027

Batch 116710, train_perplexity=238.02481, train_loss=5.472375

Batch 116720, train_perplexity=222.12498, train_loss=5.40324

Batch 116730, train_perplexity=246.1266, train_loss=5.505846

Batch 116740, train_perplexity=236.99986, train_loss=5.4680595

Batch 116750, train_perplexity=229.34608, train_loss=5.435232

Batch 116760, train_perplexity=237.98463, train_loss=5.472206

Batch 116770, train_perplexity=208.54834, train_loss=5.340171

Batch 116780, train_perplexity=248.52014, train_loss=5.515524

Batch 116790, train_perplexity=242.84134, train_loss=5.4924083

Batch 116800, train_perplexity=215.04631, train_loss=5.3708534

Batch 116810, train_perplexity=244.27692, train_loss=5.4983025

Batch 116820, train_perplexity=228.41818, train_loss=5.431178

Batch 116830, train_perplexity=243.46129, train_loss=5.494958

Batch 116840, train_perplexity=245.0144, train_loss=5.501317

Batch 116850, train_perplexity=227.7717, train_loss=5.428344

Batch 116860, train_perplexity=241.20201, train_loss=5.485635

Batch 116870, train_perplexity=228.16476, train_loss=5.430068

Batch 116880, train_perplexity=234.7486, train_loss=5.458515

Batch 116890, train_perplexity=240.7828, train_loss=5.4838953

Batch 116900, train_perplexity=231.87346, train_loss=5.446192

Batch 116910, train_perplexity=241.78699, train_loss=5.488057

Batch 116920, train_perplexity=213.43987, train_loss=5.363355

Batch 116930, train_perplexity=244.85954, train_loss=5.5006847

Batch 116940, train_perplexity=245.10812, train_loss=5.5016994

Batch 116950, train_perplexity=217.30469, train_loss=5.3813004

Batch 116960, train_perplexity=210.63771, train_loss=5.3501396

Batch 116970, train_perplexity=224.81064, train_loss=5.4152584

Batch 116980, train_perplexity=226.69748, train_loss=5.4236164

Batch 116990, train_perplexity=218.56616, train_loss=5.387089

Batch 117000, train_perplexity=251.65582, train_loss=5.5280623

Batch 117010, train_perplexity=255.67624, train_loss=5.543912

Batch 117020, train_perplexity=234.37358, train_loss=5.4569163

Batch 117030, train_perplexity=242.30197, train_loss=5.490185

Batch 117040, train_perplexity=246.44168, train_loss=5.5071254

Batch 117050, train_perplexity=206.33757, train_loss=5.3295135

Batch 117060, train_perplexity=233.05399, train_loss=5.45127

Batch 117070, train_perplexity=272.54477, train_loss=5.607803

Batch 117080, train_perplexity=233.28925, train_loss=5.452279

Batch 117090, train_perplexity=249.55708, train_loss=5.5196877

Batch 117100, train_perplexity=245.11234, train_loss=5.5017166

Batch 117110, train_perplexity=225.50914, train_loss=5.4183607

Batch 117120, train_perplexity=229.33188, train_loss=5.43517

Batch 117130, train_perplexity=229.28912, train_loss=5.4349837

Batch 117140, train_perplexity=231.875, train_loss=5.4461985

Batch 117150, train_perplexity=231.94269, train_loss=5.4464903

Batch 117160, train_perplexity=269.4213, train_loss=5.5962763

Batch 117170, train_perplexity=226.82138, train_loss=5.424163

Batch 117180, train_perplexity=234.23582, train_loss=5.4563284

Batch 117190, train_perplexity=228.6311, train_loss=5.43211

Batch 117200, train_perplexity=235.79912, train_loss=5.4629803

Batch 117210, train_perplexity=249.37389, train_loss=5.5189533
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 117220, train_perplexity=227.61937, train_loss=5.427675

Batch 117230, train_perplexity=237.12975, train_loss=5.4686074

Batch 117240, train_perplexity=245.25673, train_loss=5.5023055

Batch 117250, train_perplexity=220.59406, train_loss=5.396324

Batch 117260, train_perplexity=223.60585, train_loss=5.409885

Batch 117270, train_perplexity=254.22263, train_loss=5.5382104

Batch 117280, train_perplexity=234.22543, train_loss=5.456284

Batch 117290, train_perplexity=206.78229, train_loss=5.3316665

Batch 117300, train_perplexity=256.65808, train_loss=5.5477448

Batch 117310, train_perplexity=248.05759, train_loss=5.513661

Batch 117320, train_perplexity=220.07336, train_loss=5.393961

Batch 117330, train_perplexity=233.2016, train_loss=5.4519033

Batch 117340, train_perplexity=218.46198, train_loss=5.386612

Batch 117350, train_perplexity=240.19339, train_loss=5.4814444

Batch 117360, train_perplexity=215.85854, train_loss=5.3746233

Batch 117370, train_perplexity=238.82747, train_loss=5.4757414

Batch 117380, train_perplexity=264.64874, train_loss=5.5784035

Batch 117390, train_perplexity=225.7016, train_loss=5.419214

Batch 117400, train_perplexity=231.04106, train_loss=5.4425955

Batch 117410, train_perplexity=224.33026, train_loss=5.4131193

Batch 117420, train_perplexity=235.97177, train_loss=5.463712

Batch 117430, train_perplexity=238.88783, train_loss=5.475994

Batch 117440, train_perplexity=251.07175, train_loss=5.5257387

Batch 117450, train_perplexity=235.53201, train_loss=5.461847

Batch 117460, train_perplexity=236.2297, train_loss=5.4648046

Batch 117470, train_perplexity=235.13992, train_loss=5.4601808

Batch 117480, train_perplexity=237.72945, train_loss=5.471133

Batch 117490, train_perplexity=229.73903, train_loss=5.436944

Batch 117500, train_perplexity=227.66473, train_loss=5.427874

Batch 117510, train_perplexity=227.27393, train_loss=5.426156

Batch 117520, train_perplexity=223.4805, train_loss=5.409324

Batch 117530, train_perplexity=246.11087, train_loss=5.505782

Batch 117540, train_perplexity=244.45029, train_loss=5.499012

Batch 117550, train_perplexity=231.08867, train_loss=5.4428015

Batch 117560, train_perplexity=224.40697, train_loss=5.413461

Batch 117570, train_perplexity=266.25095, train_loss=5.5844393

Batch 117580, train_perplexity=219.55096, train_loss=5.3915844

Batch 117590, train_perplexity=252.37202, train_loss=5.5309043

Batch 117600, train_perplexity=220.82011, train_loss=5.3973484

Batch 117610, train_perplexity=205.69609, train_loss=5.3264

Batch 117620, train_perplexity=213.72412, train_loss=5.364686

Batch 117630, train_perplexity=207.91763, train_loss=5.337142

Batch 117640, train_perplexity=237.88377, train_loss=5.471782

Batch 117650, train_perplexity=233.64594, train_loss=5.453807

Batch 117660, train_perplexity=236.04549, train_loss=5.4640245

Batch 117670, train_perplexity=235.2634, train_loss=5.4607058

Batch 117680, train_perplexity=222.60956, train_loss=5.4054193

Batch 117690, train_perplexity=220.49184, train_loss=5.3958607

Batch 117700, train_perplexity=227.8336, train_loss=5.4286156

Batch 117710, train_perplexity=235.52977, train_loss=5.4618373

Batch 117720, train_perplexity=224.19359, train_loss=5.41251

Batch 117730, train_perplexity=215.89046, train_loss=5.374771

Batch 117740, train_perplexity=238.35056, train_loss=5.4737425

Batch 117750, train_perplexity=228.97948, train_loss=5.4336324

Batch 117760, train_perplexity=241.32188, train_loss=5.4861317

Batch 117770, train_perplexity=240.27403, train_loss=5.48178

Batch 117780, train_perplexity=219.6538, train_loss=5.3920527

Batch 117790, train_perplexity=247.19102, train_loss=5.5101614

Batch 117800, train_perplexity=217.5586, train_loss=5.382468

Batch 117810, train_perplexity=246.3088, train_loss=5.506586

Batch 117820, train_perplexity=235.34015, train_loss=5.461032

Batch 117830, train_perplexity=212.57237, train_loss=5.3592825

Batch 117840, train_perplexity=269.89163, train_loss=5.5980206

Batch 117850, train_perplexity=250.18689, train_loss=5.522208

Batch 117860, train_perplexity=229.09142, train_loss=5.434121

Batch 117870, train_perplexity=215.95409, train_loss=5.375066

Batch 117880, train_perplexity=247.06917, train_loss=5.5096684

Batch 117890, train_perplexity=238.54611, train_loss=5.4745626

Batch 117900, train_perplexity=234.17194, train_loss=5.4560556

Batch 117910, train_perplexity=221.95346, train_loss=5.4024677

Batch 117920, train_perplexity=247.92397, train_loss=5.513122

Batch 117930, train_perplexity=216.78029, train_loss=5.3788843

Batch 117940, train_perplexity=210.89308, train_loss=5.3513513

Batch 117950, train_perplexity=233.34665, train_loss=5.452525

Batch 117960, train_perplexity=227.17587, train_loss=5.4257245

Batch 117970, train_perplexity=249.20628, train_loss=5.518281

Batch 117980, train_perplexity=220.8795, train_loss=5.3976173

Batch 117990, train_perplexity=231.61697, train_loss=5.445085

Batch 118000, train_perplexity=204.19931, train_loss=5.3190966

Batch 118010, train_perplexity=253.05782, train_loss=5.533618

Batch 118020, train_perplexity=225.26154, train_loss=5.417262

Batch 118030, train_perplexity=237.17361, train_loss=5.4687924

Batch 118040, train_perplexity=208.57957, train_loss=5.3403206

Batch 118050, train_perplexity=231.42853, train_loss=5.444271

Batch 118060, train_perplexity=246.21065, train_loss=5.5061874

Batch 118070, train_perplexity=238.10756, train_loss=5.4727225

Batch 118080, train_perplexity=232.37663, train_loss=5.4483595

Batch 118090, train_perplexity=238.73524, train_loss=5.475355

Batch 118100, train_perplexity=213.27861, train_loss=5.3625994

Batch 118110, train_perplexity=220.62793, train_loss=5.3964777

Batch 118120, train_perplexity=223.2891, train_loss=5.4084673

Batch 118130, train_perplexity=219.46964, train_loss=5.391214

Batch 118140, train_perplexity=241.28116, train_loss=5.485963

Batch 118150, train_perplexity=216.16797, train_loss=5.3760557

Batch 118160, train_perplexity=221.65097, train_loss=5.401104

Batch 118170, train_perplexity=247.08167, train_loss=5.509719

Batch 118180, train_perplexity=224.08095, train_loss=5.4120073

Batch 118190, train_perplexity=206.62152, train_loss=5.3308887

Batch 118200, train_perplexity=247.31482, train_loss=5.510662

Batch 118210, train_perplexity=268.20792, train_loss=5.5917625

Batch 118220, train_perplexity=242.73772, train_loss=5.4919815

Batch 118230, train_perplexity=247.9359, train_loss=5.5131702

Batch 118240, train_perplexity=247.84346, train_loss=5.5127974

Batch 118250, train_perplexity=249.16898, train_loss=5.5181313

Batch 118260, train_perplexity=232.86803, train_loss=5.450472

Batch 118270, train_perplexity=243.56764, train_loss=5.4953947

Batch 118280, train_perplexity=239.35886, train_loss=5.477964

Batch 118290, train_perplexity=221.32632, train_loss=5.399638

Batch 118300, train_perplexity=232.46863, train_loss=5.4487553

Batch 118310, train_perplexity=249.1755, train_loss=5.5181575

Batch 118320, train_perplexity=266.0838, train_loss=5.5838113

Batch 118330, train_perplexity=235.65907, train_loss=5.462386

Batch 118340, train_perplexity=214.57124, train_loss=5.368642

Batch 118350, train_perplexity=213.2652, train_loss=5.3625364

Batch 118360, train_perplexity=203.10123, train_loss=5.3137045

Batch 118370, train_perplexity=232.80629, train_loss=5.4502068

Batch 118380, train_perplexity=229.57312, train_loss=5.4362216

Batch 118390, train_perplexity=224.68417, train_loss=5.4146957

Batch 118400, train_perplexity=224.57578, train_loss=5.414213

Batch 118410, train_perplexity=220.43802, train_loss=5.3956165

Batch 118420, train_perplexity=251.58058, train_loss=5.5277634

Batch 118430, train_perplexity=248.98607, train_loss=5.517397

Batch 118440, train_perplexity=202.49644, train_loss=5.3107224

Batch 118450, train_perplexity=258.15353, train_loss=5.5535545

Batch 118460, train_perplexity=209.00847, train_loss=5.342375

Batch 118470, train_perplexity=215.31863, train_loss=5.372119

Batch 118480, train_perplexity=258.26498, train_loss=5.553986

Batch 118490, train_perplexity=248.02837, train_loss=5.513543

Batch 118500, train_perplexity=229.7822, train_loss=5.437132

Batch 118510, train_perplexity=245.81743, train_loss=5.504589
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 118520, train_perplexity=210.14592, train_loss=5.347802

Batch 118530, train_perplexity=254.22748, train_loss=5.5382295

Batch 118540, train_perplexity=250.37701, train_loss=5.522968

Batch 118550, train_perplexity=227.00316, train_loss=5.424964

Batch 118560, train_perplexity=232.1663, train_loss=5.447454

Batch 118570, train_perplexity=233.1279, train_loss=5.451587

Batch 118580, train_perplexity=217.28137, train_loss=5.381193

Batch 118590, train_perplexity=248.725, train_loss=5.516348

Batch 118600, train_perplexity=224.1613, train_loss=5.412366

Batch 118610, train_perplexity=189.57095, train_loss=5.2447634

Batch 118620, train_perplexity=206.37517, train_loss=5.3296957

Batch 118630, train_perplexity=198.31665, train_loss=5.289865

Batch 118640, train_perplexity=216.05873, train_loss=5.3755503

Batch 118650, train_perplexity=228.4538, train_loss=5.431334

Batch 118660, train_perplexity=219.75417, train_loss=5.3925095

Batch 118670, train_perplexity=226.23463, train_loss=5.4215727

Batch 118680, train_perplexity=205.83345, train_loss=5.3270674

Batch 118690, train_perplexity=223.46835, train_loss=5.40927

Batch 118700, train_perplexity=222.6826, train_loss=5.4057474

Batch 118710, train_perplexity=225.12483, train_loss=5.416655

Batch 118720, train_perplexity=246.64824, train_loss=5.507963

Batch 118730, train_perplexity=233.07832, train_loss=5.4513745

Batch 118740, train_perplexity=222.17879, train_loss=5.4034824

Batch 118750, train_perplexity=202.77454, train_loss=5.3120947

Batch 118760, train_perplexity=253.28043, train_loss=5.5344973

Batch 118770, train_perplexity=215.41435, train_loss=5.3725634

Batch 118780, train_perplexity=237.14613, train_loss=5.4686766

Batch 118790, train_perplexity=226.58302, train_loss=5.4231114

Batch 118800, train_perplexity=232.59303, train_loss=5.4492903

Batch 118810, train_perplexity=204.80957, train_loss=5.3220806

Batch 118820, train_perplexity=221.12968, train_loss=5.3987494

Batch 118830, train_perplexity=243.40997, train_loss=5.494747

Batch 118840, train_perplexity=232.25456, train_loss=5.447834

Batch 118850, train_perplexity=226.35161, train_loss=5.4220896

Batch 118860, train_perplexity=253.54094, train_loss=5.5355253

Batch 118870, train_perplexity=228.44444, train_loss=5.431293

Batch 118880, train_perplexity=242.94083, train_loss=5.492818

Batch 118890, train_perplexity=216.8634, train_loss=5.3792677

Batch 118900, train_perplexity=198.25586, train_loss=5.2895584

Batch 118910, train_perplexity=211.05052, train_loss=5.3520975

Batch 118920, train_perplexity=248.62291, train_loss=5.5159373

Batch 118930, train_perplexity=245.59201, train_loss=5.5036716

Batch 118940, train_perplexity=211.40233, train_loss=5.353763

Batch 118950, train_perplexity=232.25745, train_loss=5.4478464

Batch 118960, train_perplexity=210.35085, train_loss=5.348777

Batch 118970, train_perplexity=229.74472, train_loss=5.436969

Batch 118980, train_perplexity=215.01114, train_loss=5.37069

Batch 118990, train_perplexity=232.4735, train_loss=5.4487762

Batch 119000, train_perplexity=206.51396, train_loss=5.330368

Batch 119010, train_perplexity=235.66255, train_loss=5.462401

Batch 119020, train_perplexity=255.53119, train_loss=5.5433445

Batch 119030, train_perplexity=215.06723, train_loss=5.3709507

Batch 119040, train_perplexity=241.87717, train_loss=5.48843

Batch 119050, train_perplexity=230.16403, train_loss=5.438792

Batch 119060, train_perplexity=240.7572, train_loss=5.483789

Batch 119070, train_perplexity=222.3401, train_loss=5.404208

Batch 119080, train_perplexity=242.55351, train_loss=5.4912224

Batch 119090, train_perplexity=215.00294, train_loss=5.3706517

Batch 119100, train_perplexity=239.53868, train_loss=5.478715

Batch 119110, train_perplexity=215.0135, train_loss=5.370701

Batch 119120, train_perplexity=238.06136, train_loss=5.4725285

Batch 119130, train_perplexity=233.69775, train_loss=5.4540286

Batch 119140, train_perplexity=235.33342, train_loss=5.4610033

Batch 119150, train_perplexity=238.69403, train_loss=5.4751825

Batch 119160, train_perplexity=228.75783, train_loss=5.432664

Batch 119170, train_perplexity=236.71062, train_loss=5.4668384

Batch 119180, train_perplexity=232.43925, train_loss=5.448629

Batch 119190, train_perplexity=202.44798, train_loss=5.310483

Batch 119200, train_perplexity=229.15894, train_loss=5.434416

Batch 119210, train_perplexity=230.95251, train_loss=5.442212

Batch 119220, train_perplexity=240.74388, train_loss=5.4837337

Batch 119230, train_perplexity=228.59927, train_loss=5.4319706

Batch 119240, train_perplexity=227.67494, train_loss=5.427919

Batch 119250, train_perplexity=235.01886, train_loss=5.459666

Batch 119260, train_perplexity=221.85146, train_loss=5.402008

Batch 119270, train_perplexity=212.42717, train_loss=5.358599

Batch 119280, train_perplexity=231.228, train_loss=5.443404

Batch 119290, train_perplexity=222.14616, train_loss=5.4033356

Batch 119300, train_perplexity=221.60596, train_loss=5.400901

Batch 119310, train_perplexity=251.842, train_loss=5.528802

Batch 119320, train_perplexity=225.14061, train_loss=5.416725

Batch 119330, train_perplexity=228.845, train_loss=5.433045

Batch 119340, train_perplexity=222.11153, train_loss=5.4031796

Batch 119350, train_perplexity=218.64392, train_loss=5.3874445

Batch 119360, train_perplexity=234.92128, train_loss=5.4592505

Batch 119370, train_perplexity=243.56787, train_loss=5.4953957

Batch 119380, train_perplexity=218.56711, train_loss=5.387093

Batch 119390, train_perplexity=230.5432, train_loss=5.4404383

Batch 119400, train_perplexity=243.36217, train_loss=5.4945507

Batch 119410, train_perplexity=214.9274, train_loss=5.3703003

Batch 119420, train_perplexity=230.41429, train_loss=5.439879

Batch 119430, train_perplexity=221.23938, train_loss=5.3992453

Batch 119440, train_perplexity=203.79, train_loss=5.31709

Batch 119450, train_perplexity=249.18561, train_loss=5.518198

Batch 119460, train_perplexity=229.07503, train_loss=5.4340496

Batch 119470, train_perplexity=249.46237, train_loss=5.519308

Batch 119480, train_perplexity=233.06932, train_loss=5.451336

Batch 119490, train_perplexity=256.05615, train_loss=5.545397

Batch 119500, train_perplexity=240.0585, train_loss=5.4808826

Batch 119510, train_perplexity=213.23235, train_loss=5.3623824

Batch 119520, train_perplexity=204.00749, train_loss=5.3181567

Batch 119530, train_perplexity=220.12752, train_loss=5.394207

Batch 119540, train_perplexity=186.90685, train_loss=5.2306104

Batch 119550, train_perplexity=258.6242, train_loss=5.555376

Batch 119560, train_perplexity=221.81444, train_loss=5.401841

Batch 119570, train_perplexity=216.18858, train_loss=5.376151

Batch 119580, train_perplexity=259.83533, train_loss=5.560048

Batch 119590, train_perplexity=230.97949, train_loss=5.442329

Batch 119600, train_perplexity=241.04749, train_loss=5.484994

Batch 119610, train_perplexity=221.87157, train_loss=5.4020987

Batch 119620, train_perplexity=232.97154, train_loss=5.4509163

Batch 119630, train_perplexity=223.14966, train_loss=5.4078426

Batch 119640, train_perplexity=221.47813, train_loss=5.400324

Batch 119650, train_perplexity=213.81575, train_loss=5.3651147

Batch 119660, train_perplexity=258.849, train_loss=5.556245

Batch 119670, train_perplexity=210.80692, train_loss=5.3509426

Batch 119680, train_perplexity=210.86502, train_loss=5.351218

Batch 119690, train_perplexity=220.8616, train_loss=5.3975363

Batch 119700, train_perplexity=221.12378, train_loss=5.3987226

Batch 119710, train_perplexity=236.16731, train_loss=5.4645405

Batch 119720, train_perplexity=222.79092, train_loss=5.406234

Batch 119730, train_perplexity=246.98978, train_loss=5.509347

Batch 119740, train_perplexity=206.64853, train_loss=5.3310194

Batch 119750, train_perplexity=235.39223, train_loss=5.461253

Batch 119760, train_perplexity=204.95747, train_loss=5.3228025

Batch 119770, train_perplexity=230.59906, train_loss=5.4406805

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100
Loaded 304503 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00096-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 304503 sentences.
Finished loading
Batch 119780, train_perplexity=232.80652, train_loss=5.4502077

Batch 119790, train_perplexity=218.4451, train_loss=5.3865347

Batch 119800, train_perplexity=217.81363, train_loss=5.38364

Batch 119810, train_perplexity=231.25456, train_loss=5.443519

Batch 119820, train_perplexity=220.02405, train_loss=5.393737

Batch 119830, train_perplexity=234.56073, train_loss=5.4577146

Batch 119840, train_perplexity=248.28728, train_loss=5.5145864

Batch 119850, train_perplexity=227.23904, train_loss=5.4260025

Batch 119860, train_perplexity=220.80643, train_loss=5.3972864

Batch 119870, train_perplexity=237.56422, train_loss=5.470438

Batch 119880, train_perplexity=225.43045, train_loss=5.4180117

Batch 119890, train_perplexity=184.03757, train_loss=5.21514

Batch 119900, train_perplexity=206.18828, train_loss=5.3287897

Batch 119910, train_perplexity=262.5687, train_loss=5.570513

Batch 119920, train_perplexity=205.46886, train_loss=5.3252945

Batch 119930, train_perplexity=266.4618, train_loss=5.585231

Batch 119940, train_perplexity=222.37128, train_loss=5.4043484

Batch 119950, train_perplexity=223.75198, train_loss=5.410538

Batch 119960, train_perplexity=195.94386, train_loss=5.277828

Batch 119970, train_perplexity=208.21626, train_loss=5.3385773

Batch 119980, train_perplexity=220.11407, train_loss=5.394146

Batch 119990, train_perplexity=232.83604, train_loss=5.4503345

Batch 120000, train_perplexity=238.81995, train_loss=5.47571

Batch 120010, train_perplexity=240.23737, train_loss=5.4816275

Batch 120020, train_perplexity=233.88538, train_loss=5.454831

Batch 120030, train_perplexity=245.20398, train_loss=5.5020905

Batch 120040, train_perplexity=212.29654, train_loss=5.357984

Batch 120050, train_perplexity=223.20935, train_loss=5.40811

Batch 120060, train_perplexity=249.13736, train_loss=5.5180044

Batch 120070, train_perplexity=231.35725, train_loss=5.443963

Batch 120080, train_perplexity=229.95845, train_loss=5.4378986

Batch 120090, train_perplexity=225.29913, train_loss=5.417429

Batch 120100, train_perplexity=215.60487, train_loss=5.3734474

Batch 120110, train_perplexity=231.94977, train_loss=5.446521

Batch 120120, train_perplexity=232.85515, train_loss=5.4504166

Batch 120130, train_perplexity=225.9873, train_loss=5.420479

Batch 120140, train_perplexity=242.65509, train_loss=5.491641

Batch 120150, train_perplexity=227.79993, train_loss=5.4284678

Batch 120160, train_perplexity=245.74805, train_loss=5.504307

Batch 120170, train_perplexity=214.04028, train_loss=5.366164

Batch 120180, train_perplexity=237.75609, train_loss=5.4712453

Batch 120190, train_perplexity=257.27612, train_loss=5.55015

Batch 120200, train_perplexity=228.85384, train_loss=5.4330835

Batch 120210, train_perplexity=210.14772, train_loss=5.3478107

Batch 120220, train_perplexity=237.34773, train_loss=5.4695263

Batch 120230, train_perplexity=201.07933, train_loss=5.3036995

Batch 120240, train_perplexity=246.87062, train_loss=5.5088644

Batch 120250, train_perplexity=235.99608, train_loss=5.463815

Batch 120260, train_perplexity=224.92085, train_loss=5.4157486

Batch 120270, train_perplexity=207.88055, train_loss=5.3369637

Batch 120280, train_perplexity=211.41171, train_loss=5.3538074

Batch 120290, train_perplexity=217.01733, train_loss=5.379977

Batch 120300, train_perplexity=238.10495, train_loss=5.4727116

Batch 120310, train_perplexity=210.15794, train_loss=5.3478594

Batch 120320, train_perplexity=214.39697, train_loss=5.3678293

Batch 120330, train_perplexity=248.87947, train_loss=5.5169687

Batch 120340, train_perplexity=241.90231, train_loss=5.488534

Batch 120350, train_perplexity=223.13603, train_loss=5.4077816

Batch 120360, train_perplexity=225.83316, train_loss=5.4197965

Batch 120370, train_perplexity=229.6812, train_loss=5.436692

Batch 120380, train_perplexity=235.73628, train_loss=5.4627137

Batch 120390, train_perplexity=231.88474, train_loss=5.4462404

Batch 120400, train_perplexity=250.30014, train_loss=5.5226607

Batch 120410, train_perplexity=220.74622, train_loss=5.3970137

Batch 120420, train_perplexity=244.86, train_loss=5.5006866

Batch 120430, train_perplexity=272.2363, train_loss=5.6066704

Batch 120440, train_perplexity=225.90843, train_loss=5.42013

Batch 120450, train_perplexity=204.02704, train_loss=5.3182526

Batch 120460, train_perplexity=246.43828, train_loss=5.5071115

Batch 120470, train_perplexity=208.06639, train_loss=5.3378572

Batch 120480, train_perplexity=227.04712, train_loss=5.4251575

Batch 120490, train_perplexity=196.04312, train_loss=5.2783346

Batch 120500, train_perplexity=230.4489, train_loss=5.440029

Batch 120510, train_perplexity=246.2724, train_loss=5.5064383

Batch 120520, train_perplexity=227.96967, train_loss=5.4292126

Batch 120530, train_perplexity=212.10661, train_loss=5.357089

Batch 120540, train_perplexity=224.41809, train_loss=5.413511

Batch 120550, train_perplexity=213.621, train_loss=5.3642035

Batch 120560, train_perplexity=242.11337, train_loss=5.489406

Batch 120570, train_perplexity=244.58368, train_loss=5.4995575

Batch 120580, train_perplexity=229.52617, train_loss=5.436017

Batch 120590, train_perplexity=246.89476, train_loss=5.508962

Batch 120600, train_perplexity=214.60092, train_loss=5.36878

Batch 120610, train_perplexity=245.47212, train_loss=5.5031834

Batch 120620, train_perplexity=239.74597, train_loss=5.47958

Batch 120630, train_perplexity=225.92287, train_loss=5.4201937

Batch 120640, train_perplexity=208.9475, train_loss=5.342083

Batch 120650, train_perplexity=224.26523, train_loss=5.4128294

Batch 120660, train_perplexity=244.42955, train_loss=5.498927

Batch 120670, train_perplexity=227.19267, train_loss=5.4257984

Batch 120680, train_perplexity=252.76549, train_loss=5.532462

Batch 120690, train_perplexity=193.8742, train_loss=5.2672095

Batch 120700, train_perplexity=246.6186, train_loss=5.507843

Batch 120710, train_perplexity=233.64104, train_loss=5.453786

Batch 120720, train_perplexity=211.31989, train_loss=5.353373

Batch 120730, train_perplexity=224.97577, train_loss=5.4159927

Batch 120740, train_perplexity=217.99828, train_loss=5.384487

Batch 120750, train_perplexity=200.47504, train_loss=5.3006897

Batch 120760, train_perplexity=219.65987, train_loss=5.3920803

Batch 120770, train_perplexity=250.11342, train_loss=5.5219145

Batch 120780, train_perplexity=219.7465, train_loss=5.3924747

Batch 120790, train_perplexity=238.46536, train_loss=5.474224

Batch 120800, train_perplexity=242.64514, train_loss=5.4916

Batch 120810, train_perplexity=211.20273, train_loss=5.3528185

Batch 120820, train_perplexity=210.78862, train_loss=5.350856

Batch 120830, train_perplexity=248.25175, train_loss=5.5144434

Batch 120840, train_perplexity=221.94139, train_loss=5.4024134

Batch 120850, train_perplexity=203.8803, train_loss=5.317533

Batch 120860, train_perplexity=230.21144, train_loss=5.438998

Batch 120870, train_perplexity=236.38362, train_loss=5.465456

Batch 120880, train_perplexity=245.21732, train_loss=5.502145

Batch 120890, train_perplexity=246.39244, train_loss=5.5069256

Batch 120900, train_perplexity=212.33167, train_loss=5.3581495

Batch 120910, train_perplexity=221.83665, train_loss=5.4019413

Batch 120920, train_perplexity=258.03528, train_loss=5.5530963

Batch 120930, train_perplexity=213.55614, train_loss=5.3638997

Batch 120940, train_perplexity=202.13719, train_loss=5.3089466

Batch 120950, train_perplexity=244.38573, train_loss=5.498748

Batch 120960, train_perplexity=210.75696, train_loss=5.3507056

Batch 120970, train_perplexity=238.09497, train_loss=5.4726696

Batch 120980, train_perplexity=211.05646, train_loss=5.3521256

Batch 120990, train_perplexity=207.60992, train_loss=5.335661

Batch 121000, train_perplexity=213.28757, train_loss=5.3626413

Batch 121010, train_perplexity=223.07263, train_loss=5.4074974

Batch 121020, train_perplexity=225.9511, train_loss=5.4203186

Batch 121030, train_perplexity=217.9991, train_loss=5.384491

Batch 121040, train_perplexity=218.95348, train_loss=5.3888593

Batch 121050, train_perplexity=235.8369, train_loss=5.4631405

Batch 121060, train_perplexity=219.7549, train_loss=5.392513
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 121070, train_perplexity=218.06502, train_loss=5.3847933

Batch 121080, train_perplexity=247.57227, train_loss=5.5117025

Batch 121090, train_perplexity=259.8823, train_loss=5.560229

Batch 121100, train_perplexity=233.08487, train_loss=5.4514027

Batch 121110, train_perplexity=241.59822, train_loss=5.487276

Batch 121120, train_perplexity=242.74397, train_loss=5.4920073

Batch 121130, train_perplexity=236.96811, train_loss=5.4679255

Batch 121140, train_perplexity=218.85265, train_loss=5.3883986

Batch 121150, train_perplexity=238.61324, train_loss=5.474844

Batch 121160, train_perplexity=216.87106, train_loss=5.379303

Batch 121170, train_perplexity=243.59552, train_loss=5.495509

Batch 121180, train_perplexity=210.87035, train_loss=5.3512435

Batch 121190, train_perplexity=235.03421, train_loss=5.459731

Batch 121200, train_perplexity=217.26833, train_loss=5.381133

Batch 121210, train_perplexity=237.93857, train_loss=5.4720125

Batch 121220, train_perplexity=259.39117, train_loss=5.558337

Batch 121230, train_perplexity=203.11847, train_loss=5.3137894

Batch 121240, train_perplexity=257.0115, train_loss=5.549121

Batch 121250, train_perplexity=217.0606, train_loss=5.3801765

Batch 121260, train_perplexity=211.27284, train_loss=5.3531504

Batch 121270, train_perplexity=221.68163, train_loss=5.4012423

Batch 121280, train_perplexity=222.90527, train_loss=5.406747

Batch 121290, train_perplexity=212.01541, train_loss=5.356659

Batch 121300, train_perplexity=231.55856, train_loss=5.444833

Batch 121310, train_perplexity=253.80246, train_loss=5.5365562

Batch 121320, train_perplexity=226.74417, train_loss=5.4238224

Batch 121330, train_perplexity=248.53899, train_loss=5.5155997

Batch 121340, train_perplexity=206.79243, train_loss=5.3317156

Batch 121350, train_perplexity=243.27641, train_loss=5.4941983

Batch 121360, train_perplexity=235.88054, train_loss=5.4633255

Batch 121370, train_perplexity=230.65942, train_loss=5.4409423

Batch 121380, train_perplexity=211.1105, train_loss=5.3523817

Batch 121390, train_perplexity=211.6773, train_loss=5.355063

Batch 121400, train_perplexity=210.60236, train_loss=5.349972

Batch 121410, train_perplexity=236.78049, train_loss=5.4671335

Batch 121420, train_perplexity=218.03633, train_loss=5.3846617

Batch 121430, train_perplexity=203.95137, train_loss=5.3178816

Batch 121440, train_perplexity=214.39238, train_loss=5.367808

Batch 121450, train_perplexity=204.63445, train_loss=5.321225

Batch 121460, train_perplexity=245.9987, train_loss=5.5053263

Batch 121470, train_perplexity=244.32094, train_loss=5.4984827

Batch 121480, train_perplexity=204.26884, train_loss=5.319437

Batch 121490, train_perplexity=223.22565, train_loss=5.408183

Batch 121500, train_perplexity=193.26962, train_loss=5.2640862

Batch 121510, train_perplexity=230.06824, train_loss=5.438376

Batch 121520, train_perplexity=216.2558, train_loss=5.376462

Batch 121530, train_perplexity=269.18887, train_loss=5.595413

Batch 121540, train_perplexity=239.72905, train_loss=5.4795094

Batch 121550, train_perplexity=242.64745, train_loss=5.4916096

Batch 121560, train_perplexity=213.33008, train_loss=5.3628407

Batch 121570, train_perplexity=225.1127, train_loss=5.416601

Batch 121580, train_perplexity=218.13928, train_loss=5.3851337

Batch 121590, train_perplexity=227.19083, train_loss=5.4257903

Batch 121600, train_perplexity=207.48047, train_loss=5.335037

Batch 121610, train_perplexity=203.99591, train_loss=5.3181

Batch 121620, train_perplexity=241.90138, train_loss=5.48853

Batch 121630, train_perplexity=211.32997, train_loss=5.3534207

Batch 121640, train_perplexity=228.0611, train_loss=5.4296136

Batch 121650, train_perplexity=233.64783, train_loss=5.453815

Batch 121660, train_perplexity=211.54552, train_loss=5.35444

Batch 121670, train_perplexity=227.73085, train_loss=5.4281645

Batch 121680, train_perplexity=238.61813, train_loss=5.4748645

Batch 121690, train_perplexity=240.08391, train_loss=5.4809885

Batch 121700, train_perplexity=224.2633, train_loss=5.412821

Batch 121710, train_perplexity=212.38129, train_loss=5.358383

Batch 121720, train_perplexity=230.37946, train_loss=5.439728

Batch 121730, train_perplexity=227.79146, train_loss=5.4284306

Batch 121740, train_perplexity=197.35359, train_loss=5.284997

Batch 121750, train_perplexity=261.39218, train_loss=5.566022

Batch 121760, train_perplexity=264.70694, train_loss=5.5786233

Batch 121770, train_perplexity=205.53658, train_loss=5.325624

Batch 121780, train_perplexity=222.41824, train_loss=5.4045596

Batch 121790, train_perplexity=254.39908, train_loss=5.538904

Batch 121800, train_perplexity=241.61296, train_loss=5.487337

Batch 121810, train_perplexity=211.96263, train_loss=5.35641

Batch 121820, train_perplexity=250.07907, train_loss=5.521777

Batch 121830, train_perplexity=211.92857, train_loss=5.3562493

Batch 121840, train_perplexity=220.52338, train_loss=5.3960037

Batch 121850, train_perplexity=218.2431, train_loss=5.3856096

Batch 121860, train_perplexity=225.77878, train_loss=5.4195557

Batch 121870, train_perplexity=213.7916, train_loss=5.3650017

Batch 121880, train_perplexity=211.15259, train_loss=5.352581

Batch 121890, train_perplexity=251.52205, train_loss=5.5275307

Batch 121900, train_perplexity=216.5658, train_loss=5.3778944

Batch 121910, train_perplexity=229.11348, train_loss=5.4342175

Batch 121920, train_perplexity=231.62614, train_loss=5.4451246

Batch 121930, train_perplexity=255.30965, train_loss=5.542477

Batch 121940, train_perplexity=241.43561, train_loss=5.486603

Batch 121950, train_perplexity=217.68561, train_loss=5.383052

Batch 121960, train_perplexity=203.8695, train_loss=5.31748

Batch 121970, train_perplexity=220.22284, train_loss=5.39464

Batch 121980, train_perplexity=221.37044, train_loss=5.3998375

Batch 121990, train_perplexity=234.43047, train_loss=5.457159

Batch 122000, train_perplexity=250.43433, train_loss=5.5231967

Batch 122010, train_perplexity=206.23941, train_loss=5.3290377

Batch 122020, train_perplexity=238.9024, train_loss=5.476055

Batch 122030, train_perplexity=195.29884, train_loss=5.274531

Batch 122040, train_perplexity=219.50093, train_loss=5.3913565

Batch 122050, train_perplexity=218.70482, train_loss=5.387723

Batch 122060, train_perplexity=246.81177, train_loss=5.508626

Batch 122070, train_perplexity=235.46004, train_loss=5.461541

Batch 122080, train_perplexity=225.09488, train_loss=5.416522

Batch 122090, train_perplexity=225.85522, train_loss=5.419894

Batch 122100, train_perplexity=204.0325, train_loss=5.3182793

Batch 122110, train_perplexity=220.35667, train_loss=5.3952475

Batch 122120, train_perplexity=201.11461, train_loss=5.303875

Batch 122130, train_perplexity=237.23843, train_loss=5.4690657

Batch 122140, train_perplexity=212.25383, train_loss=5.357783

Batch 122150, train_perplexity=233.43925, train_loss=5.452922

Batch 122160, train_perplexity=251.14934, train_loss=5.5260477

Batch 122170, train_perplexity=213.27496, train_loss=5.362582

Batch 122180, train_perplexity=250.55652, train_loss=5.5236845

Batch 122190, train_perplexity=213.9366, train_loss=5.3656797

Batch 122200, train_perplexity=201.27032, train_loss=5.304649

Batch 122210, train_perplexity=231.55392, train_loss=5.444813

Batch 122220, train_perplexity=228.16998, train_loss=5.430091

Batch 122230, train_perplexity=225.26228, train_loss=5.4172654

Batch 122240, train_perplexity=246.43991, train_loss=5.507118

Batch 122250, train_perplexity=227.07256, train_loss=5.4252696

Batch 122260, train_perplexity=218.89679, train_loss=5.3886003

Batch 122270, train_perplexity=218.77783, train_loss=5.3880568

Batch 122280, train_perplexity=217.7778, train_loss=5.3834753

Batch 122290, train_perplexity=212.00975, train_loss=5.356632

Batch 122300, train_perplexity=238.6806, train_loss=5.4751263

Batch 122310, train_perplexity=222.76851, train_loss=5.406133

Batch 122320, train_perplexity=275.5588, train_loss=5.618801

Batch 122330, train_perplexity=216.5629, train_loss=5.377881

Batch 122340, train_perplexity=208.269, train_loss=5.3388305

Batch 122350, train_perplexity=251.43596, train_loss=5.5271883

Batch 122360, train_perplexity=228.54292, train_loss=5.431724
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 122370, train_perplexity=223.15508, train_loss=5.407867

Batch 122380, train_perplexity=200.33856, train_loss=5.300009

Batch 122390, train_perplexity=238.59787, train_loss=5.4747796

Batch 122400, train_perplexity=224.47182, train_loss=5.41375

Batch 122410, train_perplexity=234.86012, train_loss=5.45899

Batch 122420, train_perplexity=225.40347, train_loss=5.417892

Batch 122430, train_perplexity=231.9084, train_loss=5.4463425

Batch 122440, train_perplexity=210.64624, train_loss=5.35018

Batch 122450, train_perplexity=242.03822, train_loss=5.4890957

Batch 122460, train_perplexity=217.16391, train_loss=5.3806524

Batch 122470, train_perplexity=218.66165, train_loss=5.3875256

Batch 122480, train_perplexity=214.36652, train_loss=5.367687

Batch 122490, train_perplexity=246.4908, train_loss=5.5073247

Batch 122500, train_perplexity=217.96814, train_loss=5.384349

Batch 122510, train_perplexity=216.8269, train_loss=5.3790994

Batch 122520, train_perplexity=209.82051, train_loss=5.3462524

Batch 122530, train_perplexity=214.69795, train_loss=5.369232

Batch 122540, train_perplexity=261.40454, train_loss=5.566069

Batch 122550, train_perplexity=203.69061, train_loss=5.316602

Batch 122560, train_perplexity=213.07065, train_loss=5.361624

Batch 122570, train_perplexity=201.07616, train_loss=5.3036838

Batch 122580, train_perplexity=197.49706, train_loss=5.2857237

Batch 122590, train_perplexity=229.01923, train_loss=5.433806

Batch 122600, train_perplexity=210.81355, train_loss=5.350974

Batch 122610, train_perplexity=216.75496, train_loss=5.3787675

Batch 122620, train_perplexity=224.65761, train_loss=5.4145775

Batch 122630, train_perplexity=240.30222, train_loss=5.4818974

Batch 122640, train_perplexity=229.06696, train_loss=5.4340143

Batch 122650, train_perplexity=228.43626, train_loss=5.4312572

Batch 122660, train_perplexity=215.75893, train_loss=5.3741617

Batch 122670, train_perplexity=230.2492, train_loss=5.4391623

Batch 122680, train_perplexity=213.06435, train_loss=5.361594

Batch 122690, train_perplexity=219.80519, train_loss=5.3927417

Batch 122700, train_perplexity=218.62161, train_loss=5.3873425

Batch 122710, train_perplexity=225.67223, train_loss=5.4190836

Batch 122720, train_perplexity=222.17859, train_loss=5.4034815

Batch 122730, train_perplexity=238.5518, train_loss=5.4745865

Batch 122740, train_perplexity=244.30814, train_loss=5.4984303

Batch 122750, train_perplexity=208.41354, train_loss=5.3395243

Batch 122760, train_perplexity=212.05403, train_loss=5.356841

Batch 122770, train_perplexity=244.87624, train_loss=5.500753

Batch 122780, train_perplexity=217.8125, train_loss=5.3836346

Batch 122790, train_perplexity=210.99417, train_loss=5.3518305

Batch 122800, train_perplexity=243.50761, train_loss=5.495148

Batch 122810, train_perplexity=221.4134, train_loss=5.4000316

Batch 122820, train_perplexity=234.59932, train_loss=5.457879

Batch 122830, train_perplexity=227.21715, train_loss=5.425906

Batch 122840, train_perplexity=223.37215, train_loss=5.408839

Batch 122850, train_perplexity=232.34949, train_loss=5.4482427

Batch 122860, train_perplexity=229.54346, train_loss=5.4360924

Batch 122870, train_perplexity=215.64539, train_loss=5.3736353

Batch 122880, train_perplexity=204.11267, train_loss=5.318672

Batch 122890, train_perplexity=229.7868, train_loss=5.437152

Batch 122900, train_perplexity=226.12938, train_loss=5.4211073

Batch 122910, train_perplexity=228.73492, train_loss=5.432564

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Batch 122920, train_perplexity=206.71436, train_loss=5.331338

Batch 122930, train_perplexity=226.18092, train_loss=5.421335

Batch 122940, train_perplexity=238.1515, train_loss=5.472907

Batch 122950, train_perplexity=229.43446, train_loss=5.4356174

Batch 122960, train_perplexity=223.83916, train_loss=5.410928

Batch 122970, train_perplexity=205.64862, train_loss=5.326169

Batch 122980, train_perplexity=224.21176, train_loss=5.412591

Batch 122990, train_perplexity=210.65398, train_loss=5.350217

Batch 123000, train_perplexity=232.75568, train_loss=5.4499893

Batch 123010, train_perplexity=188.46864, train_loss=5.2389317

Batch 123020, train_perplexity=203.31711, train_loss=5.314767

Batch 123030, train_perplexity=221.19011, train_loss=5.3990226

Batch 123040, train_perplexity=244.49203, train_loss=5.4991827

Batch 123050, train_perplexity=201.41711, train_loss=5.305378

Batch 123060, train_perplexity=210.35466, train_loss=5.348795

Batch 123070, train_perplexity=207.22855, train_loss=5.3338223

Batch 123080, train_perplexity=241.61952, train_loss=5.4873643

Batch 123090, train_perplexity=214.79236, train_loss=5.369672

Batch 123100, train_perplexity=246.70387, train_loss=5.5081887

Batch 123110, train_perplexity=250.2197, train_loss=5.5223393

Batch 123120, train_perplexity=205.94292, train_loss=5.327599

Batch 123130, train_perplexity=219.10712, train_loss=5.3895607

Batch 123140, train_perplexity=219.41365, train_loss=5.390959

Batch 123150, train_perplexity=228.3204, train_loss=5.43075

Batch 123160, train_perplexity=222.58662, train_loss=5.4053164

Batch 123170, train_perplexity=218.57106, train_loss=5.387111

Batch 123180, train_perplexity=234.1045, train_loss=5.4557676

Batch 123190, train_perplexity=273.58487, train_loss=5.611612

Batch 123200, train_perplexity=241.48994, train_loss=5.486828

Batch 123210, train_perplexity=228.78423, train_loss=5.4327793

Batch 123220, train_perplexity=219.4373, train_loss=5.3910666

Batch 123230, train_perplexity=226.38766, train_loss=5.422249

Batch 123240, train_perplexity=255.67854, train_loss=5.543921

Batch 123250, train_perplexity=223.74739, train_loss=5.4105177

Batch 123260, train_perplexity=196.07584, train_loss=5.2785015

Batch 123270, train_perplexity=223.6017, train_loss=5.4098663

Batch 123280, train_perplexity=216.1525, train_loss=5.375984

Batch 123290, train_perplexity=228.05403, train_loss=5.4295826

Batch 123300, train_perplexity=230.4779, train_loss=5.440155

Batch 123310, train_perplexity=215.37778, train_loss=5.3723936

Batch 123320, train_perplexity=209.1675, train_loss=5.3431354

Batch 123330, train_perplexity=242.5933, train_loss=5.4913864

Batch 123340, train_perplexity=212.41106, train_loss=5.3585234

Batch 123350, train_perplexity=240.6935, train_loss=5.4835243

Batch 123360, train_perplexity=224.74353, train_loss=5.41496

Batch 123370, train_perplexity=207.46405, train_loss=5.334958

Batch 123380, train_perplexity=238.25032, train_loss=5.473322

Batch 123390, train_perplexity=232.18613, train_loss=5.4475393

Batch 123400, train_perplexity=230.51176, train_loss=5.440302

Batch 123410, train_perplexity=242.05162, train_loss=5.489151

Batch 123420, train_perplexity=238.71054, train_loss=5.4752517

Batch 123430, train_perplexity=211.67175, train_loss=5.3550367

Batch 123440, train_perplexity=211.54674, train_loss=5.354446

Batch 123450, train_perplexity=237.95491, train_loss=5.472081

Batch 123460, train_perplexity=214.22928, train_loss=5.367047

Batch 123470, train_perplexity=202.88692, train_loss=5.312649

Batch 123480, train_perplexity=225.76521, train_loss=5.4194956

Batch 123490, train_perplexity=236.16753, train_loss=5.4645414

Batch 123500, train_perplexity=194.84593, train_loss=5.272209

Batch 123510, train_perplexity=218.03372, train_loss=5.3846498

Batch 123520, train_perplexity=240.2321, train_loss=5.4816055

Batch 123530, train_perplexity=221.07993, train_loss=5.3985243

Batch 123540, train_perplexity=219.81064, train_loss=5.3927665

Batch 123550, train_perplexity=248.3086, train_loss=5.5146723

Batch 123560, train_perplexity=259.95456, train_loss=5.560507

Batch 123570, train_perplexity=215.11554, train_loss=5.3711753

Batch 123580, train_perplexity=223.85133, train_loss=5.410982

Batch 123590, train_perplexity=216.19827, train_loss=5.376196

Batch 123600, train_perplexity=229.03484, train_loss=5.433874
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 123610, train_perplexity=214.22887, train_loss=5.367045

Batch 123620, train_perplexity=262.81598, train_loss=5.571454

Batch 123630, train_perplexity=236.12924, train_loss=5.4643793

Batch 123640, train_perplexity=216.25302, train_loss=5.376449

Batch 123650, train_perplexity=226.3909, train_loss=5.422263

Batch 123660, train_perplexity=230.61192, train_loss=5.4407363

Batch 123670, train_perplexity=203.26631, train_loss=5.314517

Batch 123680, train_perplexity=199.84167, train_loss=5.2975254

Batch 123690, train_perplexity=232.6576, train_loss=5.449568

Batch 123700, train_perplexity=214.73021, train_loss=5.3693824

Batch 123710, train_perplexity=226.90977, train_loss=5.4245524

Batch 123720, train_perplexity=246.70752, train_loss=5.5082035

Batch 123730, train_perplexity=221.50296, train_loss=5.400436

Batch 123740, train_perplexity=249.69705, train_loss=5.5202484

Batch 123750, train_perplexity=197.54208, train_loss=5.2859516

Batch 123760, train_perplexity=240.58289, train_loss=5.4830647

Batch 123770, train_perplexity=250.54732, train_loss=5.523648

Batch 123780, train_perplexity=208.3726, train_loss=5.339328

Batch 123790, train_perplexity=261.24765, train_loss=5.565469

Batch 123800, train_perplexity=231.0739, train_loss=5.4427376

Batch 123810, train_perplexity=225.02985, train_loss=5.416233

Batch 123820, train_perplexity=223.29921, train_loss=5.4085126

Batch 123830, train_perplexity=228.96255, train_loss=5.4335585

Batch 123840, train_perplexity=219.34096, train_loss=5.3906274

Batch 123850, train_perplexity=218.48405, train_loss=5.386713

Batch 123860, train_perplexity=216.00145, train_loss=5.375285

Batch 123870, train_perplexity=217.25174, train_loss=5.381057

Batch 123880, train_perplexity=209.91708, train_loss=5.3467126

Batch 123890, train_perplexity=237.3002, train_loss=5.469326

Batch 123900, train_perplexity=220.23882, train_loss=5.3947124

Batch 123910, train_perplexity=195.07463, train_loss=5.273382

Batch 123920, train_perplexity=223.9394, train_loss=5.4113755

Batch 123930, train_perplexity=210.26129, train_loss=5.348351

Batch 123940, train_perplexity=232.32567, train_loss=5.44814

Batch 123950, train_perplexity=225.1957, train_loss=5.41697

Batch 123960, train_perplexity=238.52063, train_loss=5.474456

Batch 123970, train_perplexity=237.27147, train_loss=5.469205

Batch 123980, train_perplexity=207.88115, train_loss=5.3369665

Batch 123990, train_perplexity=205.22308, train_loss=5.3240976

Batch 124000, train_perplexity=221.00308, train_loss=5.3981767

Batch 124010, train_perplexity=220.43338, train_loss=5.3955956

Batch 124020, train_perplexity=226.98346, train_loss=5.424877

Batch 124030, train_perplexity=213.3488, train_loss=5.3629284

Batch 124040, train_perplexity=208.43768, train_loss=5.33964

Batch 124050, train_perplexity=229.61966, train_loss=5.4364243

Batch 124060, train_perplexity=211.18259, train_loss=5.352723

Batch 124070, train_perplexity=226.69974, train_loss=5.4236264

Batch 124080, train_perplexity=234.02437, train_loss=5.4554253

Batch 124090, train_perplexity=222.91185, train_loss=5.4067764

Batch 124100, train_perplexity=217.5171, train_loss=5.3822775

Batch 124110, train_perplexity=233.68025, train_loss=5.4539537

Batch 124120, train_perplexity=227.00294, train_loss=5.424963

Batch 124130, train_perplexity=215.96448, train_loss=5.375114

Batch 124140, train_perplexity=215.29153, train_loss=5.371993

Batch 124150, train_perplexity=211.14674, train_loss=5.3525534

Batch 124160, train_perplexity=212.1932, train_loss=5.357497

Batch 124170, train_perplexity=215.95522, train_loss=5.375071

Batch 124180, train_perplexity=219.77972, train_loss=5.392626

Batch 124190, train_perplexity=231.06255, train_loss=5.4426885

Batch 124200, train_perplexity=228.15062, train_loss=5.430006

Batch 124210, train_perplexity=215.26956, train_loss=5.371891

Batch 124220, train_perplexity=216.97543, train_loss=5.379784

Batch 124230, train_perplexity=227.91183, train_loss=5.428959

Batch 124240, train_perplexity=212.10023, train_loss=5.357059

Batch 124250, train_perplexity=227.50153, train_loss=5.427157

Batch 124260, train_perplexity=202.49934, train_loss=5.3107367

Batch 124270, train_perplexity=216.18301, train_loss=5.3761253

Batch 124280, train_perplexity=214.20874, train_loss=5.366951

Batch 124290, train_perplexity=230.13649, train_loss=5.4386725

Batch 124300, train_perplexity=232.37242, train_loss=5.4483414

Batch 124310, train_perplexity=214.40352, train_loss=5.36786

Batch 124320, train_perplexity=218.13367, train_loss=5.385108

Batch 124330, train_perplexity=232.5744, train_loss=5.44921

Batch 124340, train_perplexity=211.05998, train_loss=5.3521423

Batch 124350, train_perplexity=213.78822, train_loss=5.364986

Batch 124360, train_perplexity=237.21342, train_loss=5.4689603

Batch 124370, train_perplexity=242.8975, train_loss=5.4926395

Batch 124380, train_perplexity=208.28539, train_loss=5.338909

Batch 124390, train_perplexity=241.36124, train_loss=5.4862947

Batch 124400, train_perplexity=232.31425, train_loss=5.448091

Batch 124410, train_perplexity=210.50969, train_loss=5.3495317

Batch 124420, train_perplexity=254.81113, train_loss=5.5405226

Batch 124430, train_perplexity=215.61464, train_loss=5.3734927

Batch 124440, train_perplexity=215.81532, train_loss=5.374423

Batch 124450, train_perplexity=219.28104, train_loss=5.390354

Batch 124460, train_perplexity=234.40118, train_loss=5.457034

Batch 124470, train_perplexity=239.60587, train_loss=5.4789953

Batch 124480, train_perplexity=229.9591, train_loss=5.4379015

Batch 124490, train_perplexity=223.95287, train_loss=5.4114356

Batch 124500, train_perplexity=222.67346, train_loss=5.4057064

Batch 124510, train_perplexity=219.73447, train_loss=5.39242

Batch 124520, train_perplexity=237.27304, train_loss=5.4692116

Batch 124530, train_perplexity=210.58197, train_loss=5.349875

Batch 124540, train_perplexity=223.70792, train_loss=5.4103413

Batch 124550, train_perplexity=223.8904, train_loss=5.4111567

Batch 124560, train_perplexity=224.0927, train_loss=5.41206

Batch 124570, train_perplexity=225.05667, train_loss=5.4163523

Batch 124580, train_perplexity=247.52835, train_loss=5.511525

Batch 124590, train_perplexity=210.9499, train_loss=5.3516207

Batch 124600, train_perplexity=250.79773, train_loss=5.5246468

Batch 124610, train_perplexity=228.30951, train_loss=5.430702

Batch 124620, train_perplexity=218.82457, train_loss=5.3882704

Batch 124630, train_perplexity=219.35089, train_loss=5.3906727

Batch 124640, train_perplexity=224.93608, train_loss=5.4158163

Batch 124650, train_perplexity=256.4094, train_loss=5.5467753

Batch 124660, train_perplexity=242.51224, train_loss=5.491052

Batch 124670, train_perplexity=217.34665, train_loss=5.3814936

Batch 124680, train_perplexity=204.46785, train_loss=5.3204107

Batch 124690, train_perplexity=247.47029, train_loss=5.5112906

Batch 124700, train_perplexity=212.88641, train_loss=5.360759

Batch 124710, train_perplexity=241.91754, train_loss=5.488597

Batch 124720, train_perplexity=211.10135, train_loss=5.3523383

Batch 124730, train_perplexity=261.00336, train_loss=5.564533

Batch 124740, train_perplexity=216.66599, train_loss=5.378357

Batch 124750, train_perplexity=204.46072, train_loss=5.320376

Batch 124760, train_perplexity=230.23515, train_loss=5.439101

Batch 124770, train_perplexity=219.78685, train_loss=5.392658

Batch 124780, train_perplexity=251.06528, train_loss=5.525713

Batch 124790, train_perplexity=214.73184, train_loss=5.36939

Batch 124800, train_perplexity=217.53453, train_loss=5.3823576

Batch 124810, train_perplexity=233.90076, train_loss=5.454897

Batch 124820, train_perplexity=253.70033, train_loss=5.536154

Batch 124830, train_perplexity=227.04657, train_loss=5.425155

Batch 124840, train_perplexity=253.19867, train_loss=5.5341744

Batch 124850, train_perplexity=206.48915, train_loss=5.330248

Batch 124860, train_perplexity=230.01888, train_loss=5.4381614

Batch 124870, train_perplexity=235.44745, train_loss=5.461488

Batch 124880, train_perplexity=234.1945, train_loss=5.456152

Batch 124890, train_perplexity=232.0064, train_loss=5.446765

Batch 124900, train_perplexity=193.13208, train_loss=5.2633743
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 124910, train_perplexity=215.69124, train_loss=5.373848

Batch 124920, train_perplexity=207.32639, train_loss=5.3342943

Batch 124930, train_perplexity=225.10283, train_loss=5.4165573

Batch 124940, train_perplexity=226.87244, train_loss=5.424388

Batch 124950, train_perplexity=224.29367, train_loss=5.412956

Batch 124960, train_perplexity=256.8836, train_loss=5.548623

Batch 124970, train_perplexity=213.2292, train_loss=5.3623676

Batch 124980, train_perplexity=235.33746, train_loss=5.4610205

Batch 124990, train_perplexity=224.57289, train_loss=5.4142003

Batch 125000, train_perplexity=248.08633, train_loss=5.513777

Batch 125010, train_perplexity=220.09299, train_loss=5.39405

Batch 125020, train_perplexity=209.33652, train_loss=5.343943

Batch 125030, train_perplexity=210.26721, train_loss=5.348379

Batch 125040, train_perplexity=239.00906, train_loss=5.4765015

Batch 125050, train_perplexity=221.10587, train_loss=5.3986416

Batch 125060, train_perplexity=225.57658, train_loss=5.4186597

Batch 125070, train_perplexity=218.45406, train_loss=5.3865757

Batch 125080, train_perplexity=206.66292, train_loss=5.331089

Batch 125090, train_perplexity=217.91472, train_loss=5.384104

Batch 125100, train_perplexity=228.21634, train_loss=5.430294

Batch 125110, train_perplexity=229.2503, train_loss=5.4348145

Batch 125120, train_perplexity=223.45717, train_loss=5.4092197

Batch 125130, train_perplexity=231.05539, train_loss=5.4426575

Batch 125140, train_perplexity=221.56412, train_loss=5.400712

Batch 125150, train_perplexity=228.08852, train_loss=5.4297338

Batch 125160, train_perplexity=222.64438, train_loss=5.4055758

Batch 125170, train_perplexity=233.13957, train_loss=5.4516373

Batch 125180, train_perplexity=239.83493, train_loss=5.479951

Batch 125190, train_perplexity=236.80489, train_loss=5.4672365

Batch 125200, train_perplexity=221.79677, train_loss=5.4017615

Batch 125210, train_perplexity=242.76202, train_loss=5.4920816

Batch 125220, train_perplexity=225.35832, train_loss=5.4176917

Batch 125230, train_perplexity=235.06874, train_loss=5.459878

Batch 125240, train_perplexity=224.78426, train_loss=5.415141

Batch 125250, train_perplexity=212.32266, train_loss=5.358107

Batch 125260, train_perplexity=222.85733, train_loss=5.406532

Batch 125270, train_perplexity=248.7148, train_loss=5.516307

Batch 125280, train_perplexity=234.54318, train_loss=5.4576397

Batch 125290, train_perplexity=235.90282, train_loss=5.46342

Batch 125300, train_perplexity=215.10785, train_loss=5.3711395

Batch 125310, train_perplexity=218.48322, train_loss=5.386709

Batch 125320, train_perplexity=225.82356, train_loss=5.419754

Batch 125330, train_perplexity=233.76205, train_loss=5.4543037

Batch 125340, train_perplexity=254.53316, train_loss=5.539431

Batch 125350, train_perplexity=206.08153, train_loss=5.328272

Batch 125360, train_perplexity=218.98616, train_loss=5.3890085

Batch 125370, train_perplexity=267.48633, train_loss=5.5890684

Batch 125380, train_perplexity=227.98076, train_loss=5.429261

Batch 125390, train_perplexity=232.9452, train_loss=5.4508033

Batch 125400, train_perplexity=213.73737, train_loss=5.364748

Batch 125410, train_perplexity=230.21947, train_loss=5.439033

Batch 125420, train_perplexity=224.41199, train_loss=5.4134836

Batch 125430, train_perplexity=212.28064, train_loss=5.357909

Batch 125440, train_perplexity=226.8255, train_loss=5.424181

Batch 125450, train_perplexity=240.8981, train_loss=5.484374

Batch 125460, train_perplexity=202.3341, train_loss=5.3099203

Batch 125470, train_perplexity=247.75981, train_loss=5.5124598

Batch 125480, train_perplexity=226.23831, train_loss=5.421589

Batch 125490, train_perplexity=247.21942, train_loss=5.5102763

Batch 125500, train_perplexity=234.83504, train_loss=5.4588833

Batch 125510, train_perplexity=220.00537, train_loss=5.393652

Batch 125520, train_perplexity=223.77043, train_loss=5.4106207

Batch 125530, train_perplexity=205.00089, train_loss=5.3230143

Batch 125540, train_perplexity=230.40308, train_loss=5.4398303

Batch 125550, train_perplexity=218.58638, train_loss=5.3871813

Batch 125560, train_perplexity=218.32222, train_loss=5.385972

Batch 125570, train_perplexity=235.93071, train_loss=5.463538

Batch 125580, train_perplexity=214.54926, train_loss=5.3685393

Batch 125590, train_perplexity=227.99629, train_loss=5.4293294

Batch 125600, train_perplexity=230.32718, train_loss=5.439501

Batch 125610, train_perplexity=207.56973, train_loss=5.3354673

Batch 125620, train_perplexity=250.44829, train_loss=5.5232525

Batch 125630, train_perplexity=211.11714, train_loss=5.352413

Batch 125640, train_perplexity=254.1767, train_loss=5.5380297

Batch 125650, train_perplexity=207.01846, train_loss=5.332808

Batch 125660, train_perplexity=206.42831, train_loss=5.329953

Batch 125670, train_perplexity=254.70496, train_loss=5.540106

Batch 125680, train_perplexity=221.5732, train_loss=5.400753

Batch 125690, train_perplexity=220.41394, train_loss=5.3955073

Batch 125700, train_perplexity=234.1169, train_loss=5.4558206

Batch 125710, train_perplexity=222.16927, train_loss=5.4034395

Batch 125720, train_perplexity=230.9317, train_loss=5.442122

Batch 125730, train_perplexity=213.67062, train_loss=5.3644357

Batch 125740, train_perplexity=214.45403, train_loss=5.3680954

Batch 125750, train_perplexity=202.06317, train_loss=5.3085804

Batch 125760, train_perplexity=251.98663, train_loss=5.529376

Batch 125770, train_perplexity=228.3315, train_loss=5.4307985

Batch 125780, train_perplexity=235.92554, train_loss=5.463516

Batch 125790, train_perplexity=244.29182, train_loss=5.4983635

Batch 125800, train_perplexity=200.64488, train_loss=5.3015366

Batch 125810, train_perplexity=218.00056, train_loss=5.3844976

Batch 125820, train_perplexity=207.67606, train_loss=5.3359795

Batch 125830, train_perplexity=258.63776, train_loss=5.5554285

Batch 125840, train_perplexity=223.93994, train_loss=5.411378

Batch 125850, train_perplexity=213.32927, train_loss=5.362837

Batch 125860, train_perplexity=224.49312, train_loss=5.413845

Batch 125870, train_perplexity=209.88354, train_loss=5.346553

Batch 125880, train_perplexity=254.78197, train_loss=5.540408

Batch 125890, train_perplexity=223.14294, train_loss=5.4078126

Batch 125900, train_perplexity=239.65202, train_loss=5.479188

Batch 125910, train_perplexity=214.95384, train_loss=5.3704233

Batch 125920, train_perplexity=248.3382, train_loss=5.5147915

Batch 125930, train_perplexity=208.23195, train_loss=5.3386526

Batch 125940, train_perplexity=214.315, train_loss=5.367447

Batch 125950, train_perplexity=255.5095, train_loss=5.5432596

Batch 125960, train_perplexity=214.21304, train_loss=5.366971

Batch 125970, train_perplexity=218.41573, train_loss=5.3864

Batch 125980, train_perplexity=237.2624, train_loss=5.4691668

Batch 125990, train_perplexity=203.76201, train_loss=5.3169527

Batch 126000, train_perplexity=206.72165, train_loss=5.331373

Batch 126010, train_perplexity=216.25436, train_loss=5.3764553

Batch 126020, train_perplexity=241.29312, train_loss=5.4860125

Batch 126030, train_perplexity=213.20723, train_loss=5.3622646

Batch 126040, train_perplexity=209.57193, train_loss=5.345067

Batch 126050, train_perplexity=254.37918, train_loss=5.538826

Batch 126060, train_perplexity=223.98128, train_loss=5.4115624

Batch 126070, train_perplexity=231.43448, train_loss=5.444297

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00051-of-00100
Loaded 305779 sentences.
Finished loading
Batch 126080, train_perplexity=206.09921, train_loss=5.3283577

Batch 126090, train_perplexity=239.62802, train_loss=5.479088

Batch 126100, train_perplexity=225.3984, train_loss=5.4178696

Batch 126110, train_perplexity=220.75357, train_loss=5.397047

Batch 126120, train_perplexity=231.9335, train_loss=5.4464507

Batch 126130, train_perplexity=211.73413, train_loss=5.3553314

Batch 126140, train_perplexity=207.62437, train_loss=5.3357306
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 126150, train_perplexity=203.03131, train_loss=5.31336

Batch 126160, train_perplexity=215.5029, train_loss=5.3729744

Batch 126170, train_perplexity=208.55252, train_loss=5.340191

Batch 126180, train_perplexity=222.91037, train_loss=5.4067698

Batch 126190, train_perplexity=228.77397, train_loss=5.4327345

Batch 126200, train_perplexity=203.80078, train_loss=5.317143

Batch 126210, train_perplexity=206.3661, train_loss=5.329652

Batch 126220, train_perplexity=217.83638, train_loss=5.3837442

Batch 126230, train_perplexity=223.6595, train_loss=5.410125

Batch 126240, train_perplexity=218.39291, train_loss=5.386296

Batch 126250, train_perplexity=232.14915, train_loss=5.44738

Batch 126260, train_perplexity=198.83745, train_loss=5.2924876

Batch 126270, train_perplexity=218.97008, train_loss=5.388935

Batch 126280, train_perplexity=233.30348, train_loss=5.45234

Batch 126290, train_perplexity=229.07219, train_loss=5.434037

Batch 126300, train_perplexity=206.1981, train_loss=5.3288374

Batch 126310, train_perplexity=214.1876, train_loss=5.3668523

Batch 126320, train_perplexity=239.31424, train_loss=5.4777775

Batch 126330, train_perplexity=220.9109, train_loss=5.3977594

Batch 126340, train_perplexity=214.39186, train_loss=5.3678055

Batch 126350, train_perplexity=216.18631, train_loss=5.3761406

Batch 126360, train_perplexity=230.8422, train_loss=5.4417343

Batch 126370, train_perplexity=241.25458, train_loss=5.4858527

Batch 126380, train_perplexity=222.37085, train_loss=5.4043465

Batch 126390, train_perplexity=219.04381, train_loss=5.3892717

Batch 126400, train_perplexity=214.90762, train_loss=5.3702083

Batch 126410, train_perplexity=219.61871, train_loss=5.391893

Batch 126420, train_perplexity=192.2568, train_loss=5.258832

Batch 126430, train_perplexity=207.87103, train_loss=5.336918

Batch 126440, train_perplexity=218.75842, train_loss=5.387968

Batch 126450, train_perplexity=196.88437, train_loss=5.2826166

Batch 126460, train_perplexity=233.36267, train_loss=5.452594

Batch 126470, train_perplexity=221.10818, train_loss=5.398652

Batch 126480, train_perplexity=231.1689, train_loss=5.4431486

Batch 126490, train_perplexity=214.61986, train_loss=5.3688684

Batch 126500, train_perplexity=226.53798, train_loss=5.4229126

Batch 126510, train_perplexity=205.67216, train_loss=5.3262835

Batch 126520, train_perplexity=207.30078, train_loss=5.334171

Batch 126530, train_perplexity=221.08214, train_loss=5.3985343

Batch 126540, train_perplexity=206.60005, train_loss=5.330785

Batch 126550, train_perplexity=220.1037, train_loss=5.3940988

Batch 126560, train_perplexity=255.73354, train_loss=5.544136

Batch 126570, train_perplexity=238.73842, train_loss=5.4753685

Batch 126580, train_perplexity=219.95407, train_loss=5.393419

Batch 126590, train_perplexity=217.6908, train_loss=5.3830757

Batch 126600, train_perplexity=208.09111, train_loss=5.337976

Batch 126610, train_perplexity=229.17555, train_loss=5.4344883

Batch 126620, train_perplexity=201.3063, train_loss=5.3048277

Batch 126630, train_perplexity=250.12224, train_loss=5.52195

Batch 126640, train_perplexity=222.31996, train_loss=5.4041176

Batch 126650, train_perplexity=245.46603, train_loss=5.5031586

Batch 126660, train_perplexity=196.84062, train_loss=5.2823944

Batch 126670, train_perplexity=210.32437, train_loss=5.348651

Batch 126680, train_perplexity=206.1118, train_loss=5.3284187

Batch 126690, train_perplexity=216.20178, train_loss=5.376212

Batch 126700, train_perplexity=220.26706, train_loss=5.3948407

Batch 126710, train_perplexity=217.43819, train_loss=5.3819146

Batch 126720, train_perplexity=235.09138, train_loss=5.4599743

Batch 126730, train_perplexity=232.8034, train_loss=5.4501944

Batch 126740, train_perplexity=221.75331, train_loss=5.4015656

Batch 126750, train_perplexity=215.44125, train_loss=5.3726883

Batch 126760, train_perplexity=211.01288, train_loss=5.351919

Batch 126770, train_perplexity=218.90472, train_loss=5.3886366

Batch 126780, train_perplexity=223.53474, train_loss=5.409567

Batch 126790, train_perplexity=229.02446, train_loss=5.433829

Batch 126800, train_perplexity=230.63138, train_loss=5.4408207

Batch 126810, train_perplexity=206.91731, train_loss=5.3323193

Batch 126820, train_perplexity=194.14064, train_loss=5.268583

Batch 126830, train_perplexity=223.09944, train_loss=5.4076176

Batch 126840, train_perplexity=226.51292, train_loss=5.422802

Batch 126850, train_perplexity=199.12978, train_loss=5.2939568

Batch 126860, train_perplexity=214.53073, train_loss=5.368453

Batch 126870, train_perplexity=210.86784, train_loss=5.3512316

Batch 126880, train_perplexity=219.33415, train_loss=5.3905964

Batch 126890, train_perplexity=223.6467, train_loss=5.4100676

Batch 126900, train_perplexity=222.20782, train_loss=5.403613

Batch 126910, train_perplexity=239.49734, train_loss=5.4785423

Batch 126920, train_perplexity=246.44638, train_loss=5.5071445

Batch 126930, train_perplexity=226.77727, train_loss=5.4239683

Batch 126940, train_perplexity=244.98615, train_loss=5.5012016

Batch 126950, train_perplexity=231.19867, train_loss=5.4432774

Batch 126960, train_perplexity=236.43796, train_loss=5.465686

Batch 126970, train_perplexity=229.97621, train_loss=5.437976

Batch 126980, train_perplexity=223.2118, train_loss=5.408121

Batch 126990, train_perplexity=245.45644, train_loss=5.5031195

Batch 127000, train_perplexity=226.15569, train_loss=5.4212236

Batch 127010, train_perplexity=200.25243, train_loss=5.2995787

Batch 127020, train_perplexity=239.2168, train_loss=5.4773703

Batch 127030, train_perplexity=194.38898, train_loss=5.269861

Batch 127040, train_perplexity=224.87668, train_loss=5.415552

Batch 127050, train_perplexity=213.71576, train_loss=5.364647

Batch 127060, train_perplexity=211.73192, train_loss=5.355321

Batch 127070, train_perplexity=247.08237, train_loss=5.5097218

Batch 127080, train_perplexity=224.7729, train_loss=5.4150906

Batch 127090, train_perplexity=224.9332, train_loss=5.4158034

Batch 127100, train_perplexity=210.14552, train_loss=5.3478003

Batch 127110, train_perplexity=199.20442, train_loss=5.2943316

Batch 127120, train_perplexity=217.30417, train_loss=5.381298

Batch 127130, train_perplexity=223.41956, train_loss=5.4090514

Batch 127140, train_perplexity=222.86095, train_loss=5.406548

Batch 127150, train_perplexity=239.42221, train_loss=5.4782286

Batch 127160, train_perplexity=209.11943, train_loss=5.3429055

Batch 127170, train_perplexity=221.89082, train_loss=5.4021854

Batch 127180, train_perplexity=218.9945, train_loss=5.3890467

Batch 127190, train_perplexity=216.9923, train_loss=5.379862

Batch 127200, train_perplexity=200.57285, train_loss=5.3011775

Batch 127210, train_perplexity=232.72461, train_loss=5.449856

Batch 127220, train_perplexity=244.71667, train_loss=5.500101

Batch 127230, train_perplexity=227.26787, train_loss=5.4261293

Batch 127240, train_perplexity=219.84912, train_loss=5.3929415

Batch 127250, train_perplexity=232.13232, train_loss=5.4473076

Batch 127260, train_perplexity=240.03882, train_loss=5.4808006

Batch 127270, train_perplexity=227.96956, train_loss=5.429212

Batch 127280, train_perplexity=214.13991, train_loss=5.3666296

Batch 127290, train_perplexity=220.21234, train_loss=5.3945923

Batch 127300, train_perplexity=228.60866, train_loss=5.4320116

Batch 127310, train_perplexity=219.26012, train_loss=5.390259

Batch 127320, train_perplexity=216.77388, train_loss=5.3788548

Batch 127330, train_perplexity=201.4687, train_loss=5.305634

Batch 127340, train_perplexity=222.64862, train_loss=5.405595

Batch 127350, train_perplexity=215.0543, train_loss=5.3708906

Batch 127360, train_perplexity=215.3061, train_loss=5.372061

Batch 127370, train_perplexity=218.9114, train_loss=5.388667

Batch 127380, train_perplexity=228.35023, train_loss=5.4308805

Batch 127390, train_perplexity=225.29483, train_loss=5.41741

Batch 127400, train_perplexity=222.52826, train_loss=5.405054

Batch 127410, train_perplexity=197.41919, train_loss=5.2853293

Batch 127420, train_perplexity=228.09993, train_loss=5.429784

Batch 127430, train_perplexity=223.11646, train_loss=5.407694

Batch 127440, train_perplexity=225.19226, train_loss=5.4169545
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 127450, train_perplexity=240.42874, train_loss=5.482424

Batch 127460, train_perplexity=223.0556, train_loss=5.407421

Batch 127470, train_perplexity=190.69298, train_loss=5.2506647

Batch 127480, train_perplexity=206.0827, train_loss=5.3282776

Batch 127490, train_perplexity=197.07016, train_loss=5.28356

Batch 127500, train_perplexity=203.62584, train_loss=5.316284

Batch 127510, train_perplexity=222.2481, train_loss=5.4037943

Batch 127520, train_perplexity=212.9973, train_loss=5.3612795

Batch 127530, train_perplexity=219.76192, train_loss=5.3925447

Batch 127540, train_perplexity=227.16776, train_loss=5.4256887

Batch 127550, train_perplexity=219.36763, train_loss=5.390749

Batch 127560, train_perplexity=212.93971, train_loss=5.361009

Batch 127570, train_perplexity=218.0043, train_loss=5.384515

Batch 127580, train_perplexity=224.6469, train_loss=5.41453

Batch 127590, train_perplexity=221.22292, train_loss=5.399171

Batch 127600, train_perplexity=218.93521, train_loss=5.388776

Batch 127610, train_perplexity=177.2534, train_loss=5.1775804

Batch 127620, train_perplexity=219.07964, train_loss=5.3894353

Batch 127630, train_perplexity=209.47083, train_loss=5.3445845

Batch 127640, train_perplexity=218.64705, train_loss=5.387459

Batch 127650, train_perplexity=220.27158, train_loss=5.394861

Batch 127660, train_perplexity=221.21037, train_loss=5.399114

Batch 127670, train_perplexity=201.46092, train_loss=5.3055954

Batch 127680, train_perplexity=224.37048, train_loss=5.4132986

Batch 127690, train_perplexity=231.98294, train_loss=5.446664

Batch 127700, train_perplexity=199.04396, train_loss=5.2935257

Batch 127710, train_perplexity=215.4805, train_loss=5.3728704

Batch 127720, train_perplexity=237.17395, train_loss=5.468794

Batch 127730, train_perplexity=219.76831, train_loss=5.392574

Batch 127740, train_perplexity=225.54033, train_loss=5.418499

Batch 127750, train_perplexity=219.69905, train_loss=5.3922586

Batch 127760, train_perplexity=231.09859, train_loss=5.4428444

Batch 127770, train_perplexity=249.55684, train_loss=5.5196867

Batch 127780, train_perplexity=225.4485, train_loss=5.418092

Batch 127790, train_perplexity=248.8148, train_loss=5.516709

Batch 127800, train_perplexity=207.25443, train_loss=5.333947

Batch 127810, train_perplexity=225.65985, train_loss=5.4190288

Batch 127820, train_perplexity=210.8043, train_loss=5.35093

Batch 127830, train_perplexity=207.41093, train_loss=5.334702

Batch 127840, train_perplexity=227.48471, train_loss=5.427083

Batch 127850, train_perplexity=233.44037, train_loss=5.4529266

Batch 127860, train_perplexity=249.13844, train_loss=5.5180087

Batch 127870, train_perplexity=263.48453, train_loss=5.5739946

Batch 127880, train_perplexity=205.20381, train_loss=5.3240037

Batch 127890, train_perplexity=214.97578, train_loss=5.3705254

Batch 127900, train_perplexity=221.01868, train_loss=5.3982472

Batch 127910, train_perplexity=227.54047, train_loss=5.427328

Batch 127920, train_perplexity=233.40942, train_loss=5.452794

Batch 127930, train_perplexity=224.86467, train_loss=5.4154987

Batch 127940, train_perplexity=221.01163, train_loss=5.3982153

Batch 127950, train_perplexity=219.97066, train_loss=5.393494

Batch 127960, train_perplexity=220.10043, train_loss=5.394084

Batch 127970, train_perplexity=244.16278, train_loss=5.497835

Batch 127980, train_perplexity=255.65161, train_loss=5.5438156

Batch 127990, train_perplexity=226.16022, train_loss=5.4212437

Batch 128000, train_perplexity=240.48332, train_loss=5.4826508

Batch 128010, train_perplexity=226.75996, train_loss=5.423892

Batch 128020, train_perplexity=201.23817, train_loss=5.304489

Batch 128030, train_perplexity=227.83752, train_loss=5.4286327

Batch 128040, train_perplexity=245.5187, train_loss=5.503373

Batch 128050, train_perplexity=222.15686, train_loss=5.4033837

Batch 128060, train_perplexity=226.52458, train_loss=5.4228535

Batch 128070, train_perplexity=226.01726, train_loss=5.4206114

Batch 128080, train_perplexity=193.18468, train_loss=5.2636466

Batch 128090, train_perplexity=225.17712, train_loss=5.4168873

Batch 128100, train_perplexity=221.96066, train_loss=5.4025

Batch 128110, train_perplexity=210.56741, train_loss=5.349806

Batch 128120, train_perplexity=200.72612, train_loss=5.3019414

Batch 128130, train_perplexity=212.16104, train_loss=5.3573456

Batch 128140, train_perplexity=194.9175, train_loss=5.2725763

Batch 128150, train_perplexity=200.60316, train_loss=5.3013287

Batch 128160, train_perplexity=211.5313, train_loss=5.354373

Batch 128170, train_perplexity=229.10289, train_loss=5.434171

Batch 128180, train_perplexity=244.44739, train_loss=5.499

Batch 128190, train_perplexity=244.17711, train_loss=5.497894

Batch 128200, train_perplexity=223.139, train_loss=5.407795

Batch 128210, train_perplexity=231.6193, train_loss=5.445095

Batch 128220, train_perplexity=224.76132, train_loss=5.415039

Batch 128230, train_perplexity=217.6825, train_loss=5.3830376

Batch 128240, train_perplexity=233.48634, train_loss=5.4531236

Batch 128250, train_perplexity=224.767, train_loss=5.4150643

Batch 128260, train_perplexity=224.99165, train_loss=5.4160633

Batch 128270, train_perplexity=197.60002, train_loss=5.286245

Batch 128280, train_perplexity=211.4094, train_loss=5.3537965

Batch 128290, train_perplexity=228.73035, train_loss=5.4325438

Batch 128300, train_perplexity=211.25047, train_loss=5.3530445

Batch 128310, train_perplexity=209.01685, train_loss=5.342415

Batch 128320, train_perplexity=211.36534, train_loss=5.353588

Batch 128330, train_perplexity=212.2683, train_loss=5.357851

Batch 128340, train_perplexity=216.19704, train_loss=5.37619

Batch 128350, train_perplexity=223.79988, train_loss=5.4107523

Batch 128360, train_perplexity=228.78575, train_loss=5.432786

Batch 128370, train_perplexity=271.2259, train_loss=5.602952

Batch 128380, train_perplexity=206.1284, train_loss=5.3284993

Batch 128390, train_perplexity=183.53444, train_loss=5.2124023

Batch 128400, train_perplexity=223.76926, train_loss=5.4106154

Batch 128410, train_perplexity=251.56895, train_loss=5.527717

Batch 128420, train_perplexity=227.56522, train_loss=5.427437

Batch 128430, train_perplexity=231.0521, train_loss=5.442643

Batch 128440, train_perplexity=232.73892, train_loss=5.4499173

Batch 128450, train_perplexity=230.3832, train_loss=5.439744

Batch 128460, train_perplexity=215.32397, train_loss=5.3721437

Batch 128470, train_perplexity=208.82877, train_loss=5.3415146

Batch 128480, train_perplexity=190.02383, train_loss=5.2471495

Batch 128490, train_perplexity=247.78154, train_loss=5.5125475

Batch 128500, train_perplexity=217.2446, train_loss=5.381024

Batch 128510, train_perplexity=220.03737, train_loss=5.3937974

Batch 128520, train_perplexity=219.78119, train_loss=5.3926325

Batch 128530, train_perplexity=206.6844, train_loss=5.331193

Batch 128540, train_perplexity=245.84813, train_loss=5.504714

Batch 128550, train_perplexity=233.03442, train_loss=5.451186

Batch 128560, train_perplexity=221.54858, train_loss=5.400642

Batch 128570, train_perplexity=212.35233, train_loss=5.358247

Batch 128580, train_perplexity=214.26677, train_loss=5.367222

Batch 128590, train_perplexity=245.83641, train_loss=5.5046663

Batch 128600, train_perplexity=214.27802, train_loss=5.3672743

Batch 128610, train_perplexity=232.45332, train_loss=5.4486895

Batch 128620, train_perplexity=207.87401, train_loss=5.336932

Batch 128630, train_perplexity=231.39366, train_loss=5.4441204

Batch 128640, train_perplexity=211.85806, train_loss=5.3559165

Batch 128650, train_perplexity=214.80733, train_loss=5.3697414

Batch 128660, train_perplexity=229.9409, train_loss=5.4378223

Batch 128670, train_perplexity=246.15993, train_loss=5.5059814

Batch 128680, train_perplexity=254.75159, train_loss=5.540289

Batch 128690, train_perplexity=248.74849, train_loss=5.5164423

Batch 128700, train_perplexity=205.25246, train_loss=5.3242407

Batch 128710, train_perplexity=212.90367, train_loss=5.36084

Batch 128720, train_perplexity=212.57268, train_loss=5.359284

Batch 128730, train_perplexity=226.32733, train_loss=5.4219823

Batch 128740, train_perplexity=206.31583, train_loss=5.329408
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 128750, train_perplexity=231.16626, train_loss=5.443137

Batch 128760, train_perplexity=241.35503, train_loss=5.486269

Batch 128770, train_perplexity=222.73654, train_loss=5.4059896

Batch 128780, train_perplexity=230.98312, train_loss=5.4423447

Batch 128790, train_perplexity=197.92772, train_loss=5.287902

Batch 128800, train_perplexity=213.45259, train_loss=5.363415

Batch 128810, train_perplexity=253.55557, train_loss=5.535583

Batch 128820, train_perplexity=224.56058, train_loss=5.4141455

Batch 128830, train_perplexity=203.5474, train_loss=5.315899

Batch 128840, train_perplexity=221.27176, train_loss=5.3993917

Batch 128850, train_perplexity=213.67184, train_loss=5.3644414

Batch 128860, train_perplexity=209.71628, train_loss=5.3457556

Batch 128870, train_perplexity=233.83496, train_loss=5.4546156

Batch 128880, train_perplexity=243.63641, train_loss=5.495677

Batch 128890, train_perplexity=209.36337, train_loss=5.3440714

Batch 128900, train_perplexity=241.69754, train_loss=5.487687

Batch 128910, train_perplexity=201.54836, train_loss=5.3060293

Batch 128920, train_perplexity=244.4588, train_loss=5.499047

Batch 128930, train_perplexity=245.02014, train_loss=5.5013404

Batch 128940, train_perplexity=192.85178, train_loss=5.261922

Batch 128950, train_perplexity=236.60205, train_loss=5.4663796

Batch 128960, train_perplexity=223.46025, train_loss=5.4092336

Batch 128970, train_perplexity=227.6694, train_loss=5.4278946

Batch 128980, train_perplexity=220.83318, train_loss=5.3974075

Batch 128990, train_perplexity=241.6003, train_loss=5.4872847

Batch 129000, train_perplexity=231.30154, train_loss=5.4437222

Batch 129010, train_perplexity=239.11804, train_loss=5.4769573

Batch 129020, train_perplexity=228.1763, train_loss=5.4301186

Batch 129030, train_perplexity=229.77748, train_loss=5.4371114

Batch 129040, train_perplexity=234.8169, train_loss=5.458806

Batch 129050, train_perplexity=238.82587, train_loss=5.4757347

Batch 129060, train_perplexity=230.2973, train_loss=5.439371

Batch 129070, train_perplexity=202.02156, train_loss=5.3083744

Batch 129080, train_perplexity=221.40317, train_loss=5.3999853

Batch 129090, train_perplexity=224.3341, train_loss=5.4131365

Batch 129100, train_perplexity=207.59923, train_loss=5.3356094

Batch 129110, train_perplexity=196.11174, train_loss=5.2786846

Batch 129120, train_perplexity=208.26254, train_loss=5.3387995

Batch 129130, train_perplexity=222.60446, train_loss=5.4053965

Batch 129140, train_perplexity=209.28572, train_loss=5.3437004

Batch 129150, train_perplexity=219.27737, train_loss=5.3903375

Batch 129160, train_perplexity=203.90022, train_loss=5.317631

Batch 129170, train_perplexity=233.42345, train_loss=5.452854

Batch 129180, train_perplexity=219.98785, train_loss=5.3935723

Batch 129190, train_perplexity=214.1583, train_loss=5.3667154

Batch 129200, train_perplexity=213.31787, train_loss=5.3627834

Batch 129210, train_perplexity=235.83893, train_loss=5.463149

Batch 129220, train_perplexity=215.80843, train_loss=5.374391

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Batch 129230, train_perplexity=226.94493, train_loss=5.4247074

Batch 129240, train_perplexity=210.00087, train_loss=5.3471117

Batch 129250, train_perplexity=227.1006, train_loss=5.425393

Batch 129260, train_perplexity=217.82849, train_loss=5.383708

Batch 129270, train_perplexity=217.37526, train_loss=5.381625

Batch 129280, train_perplexity=202.7202, train_loss=5.3118267

Batch 129290, train_perplexity=216.29417, train_loss=5.3766394

Batch 129300, train_perplexity=194.17572, train_loss=5.2687635

Batch 129310, train_perplexity=226.20293, train_loss=5.4214325

Batch 129320, train_perplexity=223.75699, train_loss=5.4105606

Batch 129330, train_perplexity=228.8593, train_loss=5.4331074

Batch 129340, train_perplexity=243.89748, train_loss=5.496748

Batch 129350, train_perplexity=239.98456, train_loss=5.4805746

Batch 129360, train_perplexity=198.52339, train_loss=5.290907

Batch 129370, train_perplexity=223.59093, train_loss=5.409818

Batch 129380, train_perplexity=198.7075, train_loss=5.291834

Batch 129390, train_perplexity=210.65417, train_loss=5.350218

Batch 129400, train_perplexity=231.97963, train_loss=5.4466496

Batch 129410, train_perplexity=209.44725, train_loss=5.344472

Batch 129420, train_perplexity=214.40639, train_loss=5.367873

Batch 129430, train_perplexity=227.85728, train_loss=5.4287195

Batch 129440, train_perplexity=228.9285, train_loss=5.4334097

Batch 129450, train_perplexity=239.61089, train_loss=5.4790163

Batch 129460, train_perplexity=203.17049, train_loss=5.3140454

Batch 129470, train_perplexity=221.19234, train_loss=5.3990326

Batch 129480, train_perplexity=208.85475, train_loss=5.341639

Batch 129490, train_perplexity=221.22809, train_loss=5.3991942

Batch 129500, train_perplexity=215.11298, train_loss=5.3711634

Batch 129510, train_perplexity=233.58578, train_loss=5.4535494

Batch 129520, train_perplexity=219.90259, train_loss=5.3931847

Batch 129530, train_perplexity=252.99411, train_loss=5.533366

Batch 129540, train_perplexity=212.90825, train_loss=5.3608613

Batch 129550, train_perplexity=219.98239, train_loss=5.3935475

Batch 129560, train_perplexity=225.40895, train_loss=5.4179163

Batch 129570, train_perplexity=233.97952, train_loss=5.4552336

Batch 129580, train_perplexity=234.84914, train_loss=5.4589434

Batch 129590, train_perplexity=224.66168, train_loss=5.4145956

Batch 129600, train_perplexity=198.27798, train_loss=5.28967

Batch 129610, train_perplexity=202.91594, train_loss=5.312792

Batch 129620, train_perplexity=215.44208, train_loss=5.372692

Batch 129630, train_perplexity=219.8248, train_loss=5.392831

Batch 129640, train_perplexity=230.14471, train_loss=5.4387083

Batch 129650, train_perplexity=204.69426, train_loss=5.3215175

Batch 129660, train_perplexity=223.4251, train_loss=5.409076

Batch 129670, train_perplexity=203.17368, train_loss=5.314061

Batch 129680, train_perplexity=214.7469, train_loss=5.36946

Batch 129690, train_perplexity=229.8348, train_loss=5.437361

Batch 129700, train_perplexity=222.44785, train_loss=5.4046926

Batch 129710, train_perplexity=222.7272, train_loss=5.4059477

Batch 129720, train_perplexity=181.65587, train_loss=5.202114

Batch 129730, train_perplexity=218.71838, train_loss=5.387785

Batch 129740, train_perplexity=241.85065, train_loss=5.4883204

Batch 129750, train_perplexity=234.07838, train_loss=5.455656

Batch 129760, train_perplexity=208.66313, train_loss=5.340721

Batch 129770, train_perplexity=213.73044, train_loss=5.3647156

Batch 129780, train_perplexity=209.46542, train_loss=5.3445587

Batch 129790, train_perplexity=214.55038, train_loss=5.3685446

Batch 129800, train_perplexity=199.28708, train_loss=5.2947464

Batch 129810, train_perplexity=209.42618, train_loss=5.3443713

Batch 129820, train_perplexity=281.56375, train_loss=5.640359

Batch 129830, train_perplexity=232.54224, train_loss=5.449072

Batch 129840, train_perplexity=215.89612, train_loss=5.3747973

Batch 129850, train_perplexity=226.7924, train_loss=5.424035

Batch 129860, train_perplexity=215.53682, train_loss=5.3731318

Batch 129870, train_perplexity=213.18549, train_loss=5.3621626

Batch 129880, train_perplexity=235.5127, train_loss=5.461765

Batch 129890, train_perplexity=247.5653, train_loss=5.5116744

Batch 129900, train_perplexity=194.31299, train_loss=5.26947

Batch 129910, train_perplexity=219.6936, train_loss=5.392234

Batch 129920, train_perplexity=213.45483, train_loss=5.3634253

Batch 129930, train_perplexity=232.38084, train_loss=5.4483776

Batch 129940, train_perplexity=229.85255, train_loss=5.437438

Batch 129950, train_perplexity=222.50279, train_loss=5.4049397

Batch 129960, train_perplexity=239.29633, train_loss=5.4777026

Batch 129970, train_perplexity=225.66124, train_loss=5.419035
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 129980, train_perplexity=216.24043, train_loss=5.376391

Batch 129990, train_perplexity=214.8983, train_loss=5.370165

Batch 130000, train_perplexity=248.72192, train_loss=5.5163355

Batch 130010, train_perplexity=197.89252, train_loss=5.287724

Batch 130020, train_perplexity=213.64383, train_loss=5.3643103

Batch 130030, train_perplexity=224.49055, train_loss=5.4138336

Batch 130040, train_perplexity=208.86401, train_loss=5.3416834

Batch 130050, train_perplexity=207.59645, train_loss=5.335596

Batch 130060, train_perplexity=209.8195, train_loss=5.3462477

Batch 130070, train_perplexity=228.77179, train_loss=5.432725

Batch 130080, train_perplexity=212.36194, train_loss=5.358292

Batch 130090, train_perplexity=233.04376, train_loss=5.451226

Batch 130100, train_perplexity=218.25945, train_loss=5.3856845

Batch 130110, train_perplexity=191.23186, train_loss=5.2534866

Batch 130120, train_perplexity=221.35968, train_loss=5.399789

Batch 130130, train_perplexity=242.61737, train_loss=5.4914856

Batch 130140, train_perplexity=193.00937, train_loss=5.2627387

Batch 130150, train_perplexity=199.60301, train_loss=5.2963305

Batch 130160, train_perplexity=229.88664, train_loss=5.4375863

Batch 130170, train_perplexity=229.99815, train_loss=5.4380713

Batch 130180, train_perplexity=240.55696, train_loss=5.482957

Batch 130190, train_perplexity=221.5845, train_loss=5.400804

Batch 130200, train_perplexity=233.38538, train_loss=5.452691

Batch 130210, train_perplexity=238.99333, train_loss=5.4764357

Batch 130220, train_perplexity=213.32225, train_loss=5.362804

Batch 130230, train_perplexity=250.94368, train_loss=5.5252285

Batch 130240, train_perplexity=247.03018, train_loss=5.5095105

Batch 130250, train_perplexity=237.6942, train_loss=5.470985

Batch 130260, train_perplexity=224.08394, train_loss=5.4120207

Batch 130270, train_perplexity=202.52551, train_loss=5.310866

Batch 130280, train_perplexity=225.97006, train_loss=5.4204025

Batch 130290, train_perplexity=214.1001, train_loss=5.3664436

Batch 130300, train_perplexity=243.96227, train_loss=5.4970136

Batch 130310, train_perplexity=200.70631, train_loss=5.3018427

Batch 130320, train_perplexity=230.27315, train_loss=5.439266

Batch 130330, train_perplexity=217.05893, train_loss=5.380169

Batch 130340, train_perplexity=253.09811, train_loss=5.533777

Batch 130350, train_perplexity=221.97908, train_loss=5.402583

Batch 130360, train_perplexity=211.842, train_loss=5.3558407

Batch 130370, train_perplexity=226.26527, train_loss=5.421708

Batch 130380, train_perplexity=217.19104, train_loss=5.3807774

Batch 130390, train_perplexity=253.99956, train_loss=5.5373325

Batch 130400, train_perplexity=228.08188, train_loss=5.4297047

Batch 130410, train_perplexity=206.12103, train_loss=5.3284636

Batch 130420, train_perplexity=207.52995, train_loss=5.3352757

Batch 130430, train_perplexity=231.17905, train_loss=5.4431925

Batch 130440, train_perplexity=238.05228, train_loss=5.4724903

Batch 130450, train_perplexity=203.66623, train_loss=5.3164825

Batch 130460, train_perplexity=247.71492, train_loss=5.5122786

Batch 130470, train_perplexity=212.98958, train_loss=5.3612432

Batch 130480, train_perplexity=224.62943, train_loss=5.414452

Batch 130490, train_perplexity=225.78244, train_loss=5.419572

Batch 130500, train_perplexity=232.91356, train_loss=5.4506674

Batch 130510, train_perplexity=226.4112, train_loss=5.422353

Batch 130520, train_perplexity=202.87192, train_loss=5.312575

Batch 130530, train_perplexity=241.81778, train_loss=5.4881845

Batch 130540, train_perplexity=224.71289, train_loss=5.4148235

Batch 130550, train_perplexity=211.88371, train_loss=5.3560376

Batch 130560, train_perplexity=232.78943, train_loss=5.4501343

Batch 130570, train_perplexity=203.60526, train_loss=5.316183

Batch 130580, train_perplexity=257.33994, train_loss=5.550398

Batch 130590, train_perplexity=214.23224, train_loss=5.3670607

Batch 130600, train_perplexity=197.84958, train_loss=5.287507

Batch 130610, train_perplexity=201.19173, train_loss=5.3042583

Batch 130620, train_perplexity=238.00745, train_loss=5.472302

Batch 130630, train_perplexity=224.41049, train_loss=5.413477

Batch 130640, train_perplexity=202.25992, train_loss=5.3095536

Batch 130650, train_perplexity=198.1126, train_loss=5.2888355

Batch 130660, train_perplexity=244.54402, train_loss=5.4993954

Batch 130670, train_perplexity=232.7033, train_loss=5.4497643

Batch 130680, train_perplexity=216.40074, train_loss=5.377132

Batch 130690, train_perplexity=219.64626, train_loss=5.3920183

Batch 130700, train_perplexity=212.25938, train_loss=5.357809

Batch 130710, train_perplexity=236.03435, train_loss=5.4639773

Batch 130720, train_perplexity=217.39351, train_loss=5.381709

Batch 130730, train_perplexity=221.62498, train_loss=5.4009867

Batch 130740, train_perplexity=222.60489, train_loss=5.4053984

Batch 130750, train_perplexity=219.04036, train_loss=5.389256

Batch 130760, train_perplexity=241.5837, train_loss=5.487216

Batch 130770, train_perplexity=213.9917, train_loss=5.365937

Batch 130780, train_perplexity=205.57059, train_loss=5.3257895

Batch 130790, train_perplexity=217.64348, train_loss=5.3828583

Batch 130800, train_perplexity=236.63275, train_loss=5.4665093

Batch 130810, train_perplexity=216.5252, train_loss=5.377707

Batch 130820, train_perplexity=195.91266, train_loss=5.277669

Batch 130830, train_perplexity=203.31721, train_loss=5.3147674

Batch 130840, train_perplexity=209.91898, train_loss=5.3467216

Batch 130850, train_perplexity=236.71118, train_loss=5.4668407

Batch 130860, train_perplexity=195.01268, train_loss=5.2730646

Batch 130870, train_perplexity=213.22218, train_loss=5.3623347

Batch 130880, train_perplexity=222.13896, train_loss=5.403303

Batch 130890, train_perplexity=245.32468, train_loss=5.5025826

Batch 130900, train_perplexity=231.19778, train_loss=5.4432735

Batch 130910, train_perplexity=232.6922, train_loss=5.4497166

Batch 130920, train_perplexity=237.51823, train_loss=5.4702444

Batch 130930, train_perplexity=209.31805, train_loss=5.343855

Batch 130940, train_perplexity=237.98883, train_loss=5.4722238

Batch 130950, train_perplexity=232.5204, train_loss=5.448978

Batch 130960, train_perplexity=210.77747, train_loss=5.350803

Batch 130970, train_perplexity=207.64111, train_loss=5.335811

Batch 130980, train_perplexity=230.54254, train_loss=5.4404354

Batch 130990, train_perplexity=249.55875, train_loss=5.5196943

Batch 131000, train_perplexity=221.7732, train_loss=5.401655

Batch 131010, train_perplexity=223.45631, train_loss=5.409216

Batch 131020, train_perplexity=218.36073, train_loss=5.3861485

Batch 131030, train_perplexity=205.59097, train_loss=5.3258886

Batch 131040, train_perplexity=215.02591, train_loss=5.3707585

Batch 131050, train_perplexity=195.79611, train_loss=5.277074

Batch 131060, train_perplexity=209.15862, train_loss=5.343093

Batch 131070, train_perplexity=228.28468, train_loss=5.4305935

Batch 131080, train_perplexity=250.29465, train_loss=5.522639

Batch 131090, train_perplexity=227.02936, train_loss=5.4250793

Batch 131100, train_perplexity=229.39597, train_loss=5.4354496

Batch 131110, train_perplexity=238.54588, train_loss=5.4745617

Batch 131120, train_perplexity=227.93706, train_loss=5.4290695

Batch 131130, train_perplexity=220.0251, train_loss=5.3937416

Batch 131140, train_perplexity=235.00485, train_loss=5.459606

Batch 131150, train_perplexity=212.94998, train_loss=5.3610573

Batch 131160, train_perplexity=228.56483, train_loss=5.43182

Batch 131170, train_perplexity=220.60794, train_loss=5.396387

Batch 131180, train_perplexity=206.411, train_loss=5.3298693

Batch 131190, train_perplexity=210.21378, train_loss=5.348125

Batch 131200, train_perplexity=199.87656, train_loss=5.2977

Batch 131210, train_perplexity=239.33113, train_loss=5.477848

Batch 131220, train_perplexity=246.82378, train_loss=5.5086746

Batch 131230, train_perplexity=222.88943, train_loss=5.406676

Batch 131240, train_perplexity=202.97981, train_loss=5.3131065

Batch 131250, train_perplexity=212.89027, train_loss=5.360777

Batch 131260, train_perplexity=224.80377, train_loss=5.415228

Batch 131270, train_perplexity=236.37483, train_loss=5.465419
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 131280, train_perplexity=200.44702, train_loss=5.30055

Batch 131290, train_perplexity=225.62724, train_loss=5.4188843

Batch 131300, train_perplexity=217.9463, train_loss=5.3842487

Batch 131310, train_perplexity=228.38246, train_loss=5.4310217

Batch 131320, train_perplexity=211.543, train_loss=5.3544283

Batch 131330, train_perplexity=211.74353, train_loss=5.355376

Batch 131340, train_perplexity=211.63643, train_loss=5.35487

Batch 131350, train_perplexity=227.00836, train_loss=5.424987

Batch 131360, train_perplexity=203.77814, train_loss=5.317032

Batch 131370, train_perplexity=237.32103, train_loss=5.4694138

Batch 131380, train_perplexity=208.3263, train_loss=5.3391056

Batch 131390, train_perplexity=216.77036, train_loss=5.3788385

Batch 131400, train_perplexity=232.49113, train_loss=5.448852

Batch 131410, train_perplexity=242.24063, train_loss=5.4899316

Batch 131420, train_perplexity=221.31049, train_loss=5.3995667

Batch 131430, train_perplexity=209.803, train_loss=5.346169

Batch 131440, train_perplexity=200.63043, train_loss=5.3014646

Batch 131450, train_perplexity=198.72209, train_loss=5.2919073

Batch 131460, train_perplexity=240.4925, train_loss=5.482689

Batch 131470, train_perplexity=217.68697, train_loss=5.383058

Batch 131480, train_perplexity=221.97252, train_loss=5.4025536

Batch 131490, train_perplexity=210.62807, train_loss=5.350094

Batch 131500, train_perplexity=204.39757, train_loss=5.320067

Batch 131510, train_perplexity=213.31198, train_loss=5.362756

Batch 131520, train_perplexity=199.22218, train_loss=5.2944207

Batch 131530, train_perplexity=216.27066, train_loss=5.3765306

Batch 131540, train_perplexity=204.37067, train_loss=5.3199353

Batch 131550, train_perplexity=202.99927, train_loss=5.3132024

Batch 131560, train_perplexity=216.566, train_loss=5.3778954

Batch 131570, train_perplexity=205.72916, train_loss=5.3265605

Batch 131580, train_perplexity=220.68811, train_loss=5.3967505

Batch 131590, train_perplexity=200.11974, train_loss=5.298916

Batch 131600, train_perplexity=219.05206, train_loss=5.3893094

Batch 131610, train_perplexity=239.06229, train_loss=5.476724

Batch 131620, train_perplexity=233.97417, train_loss=5.4552107

Batch 131630, train_perplexity=234.72487, train_loss=5.458414

Batch 131640, train_perplexity=202.00209, train_loss=5.308278

Batch 131650, train_perplexity=225.082, train_loss=5.416465

Batch 131660, train_perplexity=237.5478, train_loss=5.470369

Batch 131670, train_perplexity=199.99066, train_loss=5.2982707

Batch 131680, train_perplexity=218.74373, train_loss=5.387901

Batch 131690, train_perplexity=245.3331, train_loss=5.502617

Batch 131700, train_perplexity=236.50967, train_loss=5.465989

Batch 131710, train_perplexity=238.34726, train_loss=5.4737287

Batch 131720, train_perplexity=220.00034, train_loss=5.393629

Batch 131730, train_perplexity=206.8303, train_loss=5.3318987

Batch 131740, train_perplexity=219.33415, train_loss=5.3905964

Batch 131750, train_perplexity=231.76978, train_loss=5.4457445

Batch 131760, train_perplexity=209.62491, train_loss=5.3453197

Batch 131770, train_perplexity=215.47661, train_loss=5.3728523

Batch 131780, train_perplexity=209.0274, train_loss=5.3424654

Batch 131790, train_perplexity=211.02847, train_loss=5.351993

Batch 131800, train_perplexity=227.53363, train_loss=5.427298

Batch 131810, train_perplexity=199.94804, train_loss=5.2980576

Batch 131820, train_perplexity=224.62729, train_loss=5.4144425

Batch 131830, train_perplexity=230.34541, train_loss=5.43958

Batch 131840, train_perplexity=227.25356, train_loss=5.4260664

Batch 131850, train_perplexity=201.6093, train_loss=5.3063316

Batch 131860, train_perplexity=212.4684, train_loss=5.3587933

Batch 131870, train_perplexity=204.6682, train_loss=5.32139

Batch 131880, train_perplexity=196.13962, train_loss=5.2788267

Batch 131890, train_perplexity=255.73305, train_loss=5.544134

Batch 131900, train_perplexity=225.75595, train_loss=5.4194546

Batch 131910, train_perplexity=207.57825, train_loss=5.3355083

Batch 131920, train_perplexity=201.69371, train_loss=5.3067503

Batch 131930, train_perplexity=217.5029, train_loss=5.382212

Batch 131940, train_perplexity=224.81406, train_loss=5.4152737

Batch 131950, train_perplexity=233.26811, train_loss=5.4521885

Batch 131960, train_perplexity=195.1057, train_loss=5.2735415

Batch 131970, train_perplexity=201.08488, train_loss=5.303727

Batch 131980, train_perplexity=195.77109, train_loss=5.276946

Batch 131990, train_perplexity=225.60164, train_loss=5.418771

Batch 132000, train_perplexity=233.13646, train_loss=5.451624

Batch 132010, train_perplexity=195.70958, train_loss=5.276632

Batch 132020, train_perplexity=214.88539, train_loss=5.370105

Batch 132030, train_perplexity=227.42723, train_loss=5.4268303

Batch 132040, train_perplexity=204.87247, train_loss=5.3223877

Batch 132050, train_perplexity=223.98672, train_loss=5.4115868

Batch 132060, train_perplexity=213.23245, train_loss=5.362383

Batch 132070, train_perplexity=216.71516, train_loss=5.378584

Batch 132080, train_perplexity=229.0628, train_loss=5.433996

Batch 132090, train_perplexity=243.45804, train_loss=5.4949446

Batch 132100, train_perplexity=217.05315, train_loss=5.380142

Batch 132110, train_perplexity=224.54955, train_loss=5.4140964

Batch 132120, train_perplexity=202.93878, train_loss=5.3129044

Batch 132130, train_perplexity=220.20059, train_loss=5.394539

Batch 132140, train_perplexity=204.55873, train_loss=5.320855

Batch 132150, train_perplexity=203.59933, train_loss=5.316154

Batch 132160, train_perplexity=234.71121, train_loss=5.458356

Batch 132170, train_perplexity=215.1891, train_loss=5.371517

Batch 132180, train_perplexity=191.32124, train_loss=5.253954

Batch 132190, train_perplexity=199.41331, train_loss=5.2953796

Batch 132200, train_perplexity=238.80058, train_loss=5.475629

Batch 132210, train_perplexity=204.3431, train_loss=5.3198004

Batch 132220, train_perplexity=231.60162, train_loss=5.445019

Batch 132230, train_perplexity=209.04236, train_loss=5.342537

Batch 132240, train_perplexity=204.81953, train_loss=5.3221292

Batch 132250, train_perplexity=218.51947, train_loss=5.386875

Batch 132260, train_perplexity=207.08688, train_loss=5.3331385

Batch 132270, train_perplexity=252.48361, train_loss=5.5313463

Batch 132280, train_perplexity=185.54051, train_loss=5.2232733

Batch 132290, train_perplexity=239.9594, train_loss=5.4804697

Batch 132300, train_perplexity=218.60942, train_loss=5.3872867

Batch 132310, train_perplexity=216.2261, train_loss=5.3763247

Batch 132320, train_perplexity=194.69148, train_loss=5.271416

Batch 132330, train_perplexity=221.19264, train_loss=5.399034

Batch 132340, train_perplexity=202.36401, train_loss=5.310068

Batch 132350, train_perplexity=209.8121, train_loss=5.3462124

Batch 132360, train_perplexity=204.85645, train_loss=5.3223095

Batch 132370, train_perplexity=238.46526, train_loss=5.4742236

Batch 132380, train_perplexity=197.82968, train_loss=5.2874064

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Batch 132390, train_perplexity=210.40501, train_loss=5.3490343

Batch 132400, train_perplexity=247.77068, train_loss=5.5125036

Batch 132410, train_perplexity=236.13353, train_loss=5.4643974

Batch 132420, train_perplexity=231.57689, train_loss=5.444912

Batch 132430, train_perplexity=213.61185, train_loss=5.3641605

Batch 132440, train_perplexity=200.30035, train_loss=5.299818

Batch 132450, train_perplexity=229.37387, train_loss=5.4353533

Batch 132460, train_perplexity=207.57904, train_loss=5.335512

Batch 132470, train_perplexity=215.38025, train_loss=5.372405

Batch 132480, train_perplexity=193.65154, train_loss=5.2660604

Batch 132490, train_perplexity=208.05132, train_loss=5.337785

Batch 132500, train_perplexity=216.50179, train_loss=5.377599

Batch 132510, train_perplexity=220.81612, train_loss=5.3973303
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 132520, train_perplexity=224.24277, train_loss=5.4127293

Batch 132530, train_perplexity=223.9739, train_loss=5.4115295

Batch 132540, train_perplexity=211.04529, train_loss=5.3520727

Batch 132550, train_perplexity=229.68668, train_loss=5.436716

Batch 132560, train_perplexity=215.90857, train_loss=5.374855

Batch 132570, train_perplexity=227.64465, train_loss=5.427786

Batch 132580, train_perplexity=225.65813, train_loss=5.419021

Batch 132590, train_perplexity=196.32516, train_loss=5.2797723

Batch 132600, train_perplexity=221.9216, train_loss=5.402324

Batch 132610, train_perplexity=232.55266, train_loss=5.4491167

Batch 132620, train_perplexity=211.56621, train_loss=5.354538

Batch 132630, train_perplexity=228.26237, train_loss=5.4304957

Batch 132640, train_perplexity=217.76617, train_loss=5.383422

Batch 132650, train_perplexity=221.53264, train_loss=5.40057

Batch 132660, train_perplexity=227.15, train_loss=5.4256105

Batch 132670, train_perplexity=225.58658, train_loss=5.418704

Batch 132680, train_perplexity=211.18521, train_loss=5.3527355

Batch 132690, train_perplexity=195.888, train_loss=5.277543

Batch 132700, train_perplexity=208.64304, train_loss=5.340625

Batch 132710, train_perplexity=215.68332, train_loss=5.3738112

Batch 132720, train_perplexity=215.62769, train_loss=5.3735533

Batch 132730, train_perplexity=213.88867, train_loss=5.3654556

Batch 132740, train_perplexity=233.02843, train_loss=5.4511604

Batch 132750, train_perplexity=227.2605, train_loss=5.426097

Batch 132760, train_perplexity=204.31659, train_loss=5.3196707

Batch 132770, train_perplexity=217.97052, train_loss=5.38436

Batch 132780, train_perplexity=216.95898, train_loss=5.3797083

Batch 132790, train_perplexity=238.12062, train_loss=5.4727774

Batch 132800, train_perplexity=210.09642, train_loss=5.3475666

Batch 132810, train_perplexity=222.97447, train_loss=5.4070573

Batch 132820, train_perplexity=204.05418, train_loss=5.3183856

Batch 132830, train_perplexity=226.39435, train_loss=5.4222784

Batch 132840, train_perplexity=218.02562, train_loss=5.3846126

Batch 132850, train_perplexity=237.06937, train_loss=5.468353

Batch 132860, train_perplexity=204.08717, train_loss=5.3185472

Batch 132870, train_perplexity=236.8627, train_loss=5.4674807

Batch 132880, train_perplexity=251.74152, train_loss=5.528403

Batch 132890, train_perplexity=249.11623, train_loss=5.5179195

Batch 132900, train_perplexity=191.12102, train_loss=5.252907

Batch 132910, train_perplexity=211.6214, train_loss=5.354799

Batch 132920, train_perplexity=207.35645, train_loss=5.3344393

Batch 132930, train_perplexity=243.80237, train_loss=5.496358

Batch 132940, train_perplexity=221.03386, train_loss=5.398316

Batch 132950, train_perplexity=204.44035, train_loss=5.3202763

Batch 132960, train_perplexity=253.17126, train_loss=5.534066

Batch 132970, train_perplexity=217.95317, train_loss=5.38428

Batch 132980, train_perplexity=207.39897, train_loss=5.3346443

Batch 132990, train_perplexity=214.72427, train_loss=5.3693547

Batch 133000, train_perplexity=202.2905, train_loss=5.309705

Batch 133010, train_perplexity=227.55067, train_loss=5.427373

Batch 133020, train_perplexity=206.7396, train_loss=5.33146

Batch 133030, train_perplexity=246.28238, train_loss=5.506479

Batch 133040, train_perplexity=204.11813, train_loss=5.318699

Batch 133050, train_perplexity=225.24242, train_loss=5.417177

Batch 133060, train_perplexity=198.3422, train_loss=5.289994

Batch 133070, train_perplexity=214.02518, train_loss=5.3660936

Batch 133080, train_perplexity=208.14023, train_loss=5.338212

Batch 133090, train_perplexity=201.02641, train_loss=5.3034363

Batch 133100, train_perplexity=223.7267, train_loss=5.410425

Batch 133110, train_perplexity=223.58379, train_loss=5.409786

Batch 133120, train_perplexity=230.25493, train_loss=5.439187

Batch 133130, train_perplexity=214.59274, train_loss=5.368742

Batch 133140, train_perplexity=220.8536, train_loss=5.3975

Batch 133150, train_perplexity=197.01472, train_loss=5.2832785

Batch 133160, train_perplexity=240.87973, train_loss=5.4842978

Batch 133170, train_perplexity=243.19836, train_loss=5.4938774

Batch 133180, train_perplexity=220.79716, train_loss=5.3972445

Batch 133190, train_perplexity=201.51117, train_loss=5.305845

Batch 133200, train_perplexity=220.10002, train_loss=5.394082

Batch 133210, train_perplexity=219.75352, train_loss=5.3925066

Batch 133220, train_perplexity=201.93045, train_loss=5.3079233

Batch 133230, train_perplexity=218.93416, train_loss=5.388771

Batch 133240, train_perplexity=234.94838, train_loss=5.459366

Batch 133250, train_perplexity=207.5638, train_loss=5.3354387

Batch 133260, train_perplexity=194.57185, train_loss=5.2708015

Batch 133270, train_perplexity=206.50363, train_loss=5.330318

Batch 133280, train_perplexity=219.34608, train_loss=5.3906507

Batch 133290, train_perplexity=232.52184, train_loss=5.448984

Batch 133300, train_perplexity=199.30675, train_loss=5.294845

Batch 133310, train_perplexity=243.56927, train_loss=5.4954014

Batch 133320, train_perplexity=226.29636, train_loss=5.4218454

Batch 133330, train_perplexity=231.62593, train_loss=5.4451237

Batch 133340, train_perplexity=211.86765, train_loss=5.355962

Batch 133350, train_perplexity=248.31688, train_loss=5.5147057

Batch 133360, train_perplexity=245.35579, train_loss=5.5027094

Batch 133370, train_perplexity=222.19044, train_loss=5.403535

Batch 133380, train_perplexity=250.84282, train_loss=5.5248265

Batch 133390, train_perplexity=232.71584, train_loss=5.449818

Batch 133400, train_perplexity=189.34103, train_loss=5.24355

Batch 133410, train_perplexity=214.37918, train_loss=5.3677464

Batch 133420, train_perplexity=230.69693, train_loss=5.441105

Batch 133430, train_perplexity=190.50485, train_loss=5.2496777

Batch 133440, train_perplexity=202.42143, train_loss=5.310352

Batch 133450, train_perplexity=211.2547, train_loss=5.3530645

Batch 133460, train_perplexity=216.21198, train_loss=5.3762593

Batch 133470, train_perplexity=224.93523, train_loss=5.4158125

Batch 133480, train_perplexity=201.77481, train_loss=5.3071523

Batch 133490, train_perplexity=228.87283, train_loss=5.4331665

Batch 133500, train_perplexity=223.79456, train_loss=5.4107285

Batch 133510, train_perplexity=216.66475, train_loss=5.378351

Batch 133520, train_perplexity=213.47214, train_loss=5.3635063

Batch 133530, train_perplexity=223.0104, train_loss=5.4072185

Batch 133540, train_perplexity=203.43057, train_loss=5.315325

Batch 133550, train_perplexity=207.85696, train_loss=5.33685

Batch 133560, train_perplexity=266.35254, train_loss=5.5848207

Batch 133570, train_perplexity=200.45218, train_loss=5.3005757

Batch 133580, train_perplexity=212.24704, train_loss=5.357751

Batch 133590, train_perplexity=228.23572, train_loss=5.430379

Batch 133600, train_perplexity=225.29279, train_loss=5.417401

Batch 133610, train_perplexity=208.53522, train_loss=5.340108

Batch 133620, train_perplexity=224.7221, train_loss=5.4148645

Batch 133630, train_perplexity=196.98222, train_loss=5.2831135

Batch 133640, train_perplexity=212.2764, train_loss=5.357889

Batch 133650, train_perplexity=201.04692, train_loss=5.3035383

Batch 133660, train_perplexity=224.44025, train_loss=5.4136095

Batch 133670, train_perplexity=247.49673, train_loss=5.5113974

Batch 133680, train_perplexity=201.00858, train_loss=5.3033476

Batch 133690, train_perplexity=227.90727, train_loss=5.428939

Batch 133700, train_perplexity=215.92319, train_loss=5.3749228

Batch 133710, train_perplexity=213.98088, train_loss=5.3658867

Batch 133720, train_perplexity=186.61858, train_loss=5.229067

Batch 133730, train_perplexity=231.86794, train_loss=5.446168

Batch 133740, train_perplexity=194.75499, train_loss=5.2717423

Batch 133750, train_perplexity=247.79454, train_loss=5.5126

Batch 133760, train_perplexity=237.12352, train_loss=5.468581

Batch 133770, train_perplexity=230.23515, train_loss=5.439101

Batch 133780, train_perplexity=217.7829, train_loss=5.3834987

Batch 133790, train_perplexity=221.60765, train_loss=5.4009085

Batch 133800, train_perplexity=191.31998, train_loss=5.2539473

Batch 133810, train_perplexity=229.67462, train_loss=5.4366636
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 133820, train_perplexity=209.1305, train_loss=5.3429585

Batch 133830, train_perplexity=219.24214, train_loss=5.390177

Batch 133840, train_perplexity=235.82745, train_loss=5.4631004

Batch 133850, train_perplexity=223.3108, train_loss=5.4085646

Batch 133860, train_perplexity=224.72768, train_loss=5.4148893

Batch 133870, train_perplexity=234.25838, train_loss=5.4564247

Batch 133880, train_perplexity=221.94585, train_loss=5.4024334

Batch 133890, train_perplexity=212.23096, train_loss=5.357675

Batch 133900, train_perplexity=196.42386, train_loss=5.280275

Batch 133910, train_perplexity=220.19261, train_loss=5.3945026

Batch 133920, train_perplexity=243.98752, train_loss=5.497117

Batch 133930, train_perplexity=245.18376, train_loss=5.502008

Batch 133940, train_perplexity=194.76512, train_loss=5.2717943

Batch 133950, train_perplexity=215.26895, train_loss=5.371888

Batch 133960, train_perplexity=208.22243, train_loss=5.338607

Batch 133970, train_perplexity=218.0359, train_loss=5.38466

Batch 133980, train_perplexity=193.21295, train_loss=5.263793

Batch 133990, train_perplexity=215.04672, train_loss=5.3708553

Batch 134000, train_perplexity=223.9972, train_loss=5.4116335

Batch 134010, train_perplexity=223.22394, train_loss=5.4081755

Batch 134020, train_perplexity=217.77968, train_loss=5.383484

Batch 134030, train_perplexity=226.3896, train_loss=5.4222574

Batch 134040, train_perplexity=253.71921, train_loss=5.536228

Batch 134050, train_perplexity=219.36052, train_loss=5.3907166

Batch 134060, train_perplexity=246.72316, train_loss=5.508267

Batch 134070, train_perplexity=223.78154, train_loss=5.4106703

Batch 134080, train_perplexity=193.53366, train_loss=5.2654514

Batch 134090, train_perplexity=205.42928, train_loss=5.325102

Batch 134100, train_perplexity=216.61237, train_loss=5.3781095

Batch 134110, train_perplexity=224.44398, train_loss=5.413626

Batch 134120, train_perplexity=218.7431, train_loss=5.387898

Batch 134130, train_perplexity=225.1608, train_loss=5.416815

Batch 134140, train_perplexity=238.029, train_loss=5.4723926

Batch 134150, train_perplexity=226.59674, train_loss=5.423172

Batch 134160, train_perplexity=213.59677, train_loss=5.36409

Batch 134170, train_perplexity=205.26517, train_loss=5.3243027

Batch 134180, train_perplexity=206.2636, train_loss=5.329155

Batch 134190, train_perplexity=230.22002, train_loss=5.4390354

Batch 134200, train_perplexity=216.10417, train_loss=5.3757606

Batch 134210, train_perplexity=202.12427, train_loss=5.3088827

Batch 134220, train_perplexity=214.4933, train_loss=5.3682785

Batch 134230, train_perplexity=212.96104, train_loss=5.3611093

Batch 134240, train_perplexity=228.64922, train_loss=5.432189

Batch 134250, train_perplexity=195.57356, train_loss=5.2759366

Batch 134260, train_perplexity=207.02321, train_loss=5.332831

Batch 134270, train_perplexity=182.85919, train_loss=5.2087164

Batch 134280, train_perplexity=243.5651, train_loss=5.495384

Batch 134290, train_perplexity=215.98024, train_loss=5.375187

Batch 134300, train_perplexity=231.39685, train_loss=5.444134

Batch 134310, train_perplexity=199.7283, train_loss=5.296958

Batch 134320, train_perplexity=233.0302, train_loss=5.451168

Batch 134330, train_perplexity=195.03278, train_loss=5.2731676

Batch 134340, train_perplexity=198.22939, train_loss=5.289425

Batch 134350, train_perplexity=204.04553, train_loss=5.318343

Batch 134360, train_perplexity=210.31113, train_loss=5.348588

Batch 134370, train_perplexity=193.71295, train_loss=5.2663774

Batch 134380, train_perplexity=221.03702, train_loss=5.39833

Batch 134390, train_perplexity=209.4923, train_loss=5.344687

Batch 134400, train_perplexity=213.45636, train_loss=5.3634324

Batch 134410, train_perplexity=189.85745, train_loss=5.2462735

Batch 134420, train_perplexity=220.81506, train_loss=5.3973255

Batch 134430, train_perplexity=245.35942, train_loss=5.502724

Batch 134440, train_perplexity=200.98596, train_loss=5.303235

Batch 134450, train_perplexity=201.47437, train_loss=5.305662

Batch 134460, train_perplexity=228.66992, train_loss=5.4322796

Batch 134470, train_perplexity=208.53014, train_loss=5.3400836

Batch 134480, train_perplexity=210.8988, train_loss=5.3513784

Batch 134490, train_perplexity=203.53168, train_loss=5.3158216

Batch 134500, train_perplexity=203.8938, train_loss=5.3175993

Batch 134510, train_perplexity=196.34416, train_loss=5.279869

Batch 134520, train_perplexity=231.74922, train_loss=5.445656

Batch 134530, train_perplexity=206.67494, train_loss=5.331147

Batch 134540, train_perplexity=219.34628, train_loss=5.3906517

Batch 134550, train_perplexity=237.11606, train_loss=5.4685497

Batch 134560, train_perplexity=212.27205, train_loss=5.3578687

Batch 134570, train_perplexity=235.59828, train_loss=5.462128

Batch 134580, train_perplexity=218.21127, train_loss=5.3854637

Batch 134590, train_perplexity=201.61853, train_loss=5.3063774

Batch 134600, train_perplexity=229.13643, train_loss=5.4343176

Batch 134610, train_perplexity=243.83492, train_loss=5.4964914

Batch 134620, train_perplexity=216.8603, train_loss=5.3792534

Batch 134630, train_perplexity=210.81435, train_loss=5.350978

Batch 134640, train_perplexity=237.01048, train_loss=5.4681044

Batch 134650, train_perplexity=193.95474, train_loss=5.267625

Batch 134660, train_perplexity=219.0883, train_loss=5.389475

Batch 134670, train_perplexity=230.0249, train_loss=5.4381876

Batch 134680, train_perplexity=228.56985, train_loss=5.431842

Batch 134690, train_perplexity=203.6709, train_loss=5.3165054

Batch 134700, train_perplexity=229.65053, train_loss=5.4365587

Batch 134710, train_perplexity=214.38174, train_loss=5.3677583

Batch 134720, train_perplexity=244.62987, train_loss=5.4997463

Batch 134730, train_perplexity=239.02661, train_loss=5.476575

Batch 134740, train_perplexity=214.7899, train_loss=5.3696604

Batch 134750, train_perplexity=216.13663, train_loss=5.3759108

Batch 134760, train_perplexity=236.38565, train_loss=5.4654646

Batch 134770, train_perplexity=199.33612, train_loss=5.2949924

Batch 134780, train_perplexity=231.17728, train_loss=5.443185

Batch 134790, train_perplexity=234.79607, train_loss=5.4587173

Batch 134800, train_perplexity=199.19179, train_loss=5.294268

Batch 134810, train_perplexity=233.80353, train_loss=5.454481

Batch 134820, train_perplexity=243.59726, train_loss=5.4955163

Batch 134830, train_perplexity=228.17651, train_loss=5.4301195

Batch 134840, train_perplexity=210.54251, train_loss=5.3496876

Batch 134850, train_perplexity=231.91304, train_loss=5.4463625

Batch 134860, train_perplexity=218.13367, train_loss=5.385108

Batch 134870, train_perplexity=207.37523, train_loss=5.33453

Batch 134880, train_perplexity=237.08734, train_loss=5.4684286

Batch 134890, train_perplexity=225.19151, train_loss=5.416951

Batch 134900, train_perplexity=205.67961, train_loss=5.3263197

Batch 134910, train_perplexity=202.30989, train_loss=5.3098006

Batch 134920, train_perplexity=225.8028, train_loss=5.419662

Batch 134930, train_perplexity=236.16292, train_loss=5.464522

Batch 134940, train_perplexity=208.63467, train_loss=5.3405848

Batch 134950, train_perplexity=214.82626, train_loss=5.3698297

Batch 134960, train_perplexity=194.2815, train_loss=5.269308

Batch 134970, train_perplexity=210.45819, train_loss=5.349287

Batch 134980, train_perplexity=214.89543, train_loss=5.3701515

Batch 134990, train_perplexity=187.3874, train_loss=5.233178

Batch 135000, train_perplexity=220.86519, train_loss=5.3975525

Batch 135010, train_perplexity=223.8586, train_loss=5.4110146

Batch 135020, train_perplexity=255.46346, train_loss=5.5430794

Batch 135030, train_perplexity=243.53304, train_loss=5.4952526

Batch 135040, train_perplexity=226.02115, train_loss=5.4206285

Batch 135050, train_perplexity=194.58893, train_loss=5.2708893

Batch 135060, train_perplexity=207.03772, train_loss=5.332901

Batch 135070, train_perplexity=237.17577, train_loss=5.4688015

Batch 135080, train_perplexity=215.35252, train_loss=5.3722763

Batch 135090, train_perplexity=198.17401, train_loss=5.2891455

Batch 135100, train_perplexity=201.9969, train_loss=5.3082523

Batch 135110, train_perplexity=209.68478, train_loss=5.3456054
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 135120, train_perplexity=239.90207, train_loss=5.480231

Batch 135130, train_perplexity=230.13045, train_loss=5.4386463

Batch 135140, train_perplexity=236.79991, train_loss=5.4672155

Batch 135150, train_perplexity=199.60587, train_loss=5.2963448

Batch 135160, train_perplexity=194.59914, train_loss=5.2709417

Batch 135170, train_perplexity=216.9898, train_loss=5.3798504

Batch 135180, train_perplexity=207.53984, train_loss=5.3353233

Batch 135190, train_perplexity=201.3376, train_loss=5.304983

Batch 135200, train_perplexity=191.3123, train_loss=5.253907

Batch 135210, train_perplexity=219.90112, train_loss=5.393178

Batch 135220, train_perplexity=232.60701, train_loss=5.4493504

Batch 135230, train_perplexity=215.2749, train_loss=5.371916

Batch 135240, train_perplexity=200.19762, train_loss=5.299305

Batch 135250, train_perplexity=207.3771, train_loss=5.334539

Batch 135260, train_perplexity=219.36533, train_loss=5.3907385

Batch 135270, train_perplexity=213.91039, train_loss=5.365557

Batch 135280, train_perplexity=231.53416, train_loss=5.4447274

Batch 135290, train_perplexity=221.80058, train_loss=5.4017787

Batch 135300, train_perplexity=232.3219, train_loss=5.448124

Batch 135310, train_perplexity=240.6251, train_loss=5.48324

Batch 135320, train_perplexity=218.43239, train_loss=5.3864765

Batch 135330, train_perplexity=232.40146, train_loss=5.4484663

Batch 135340, train_perplexity=223.92221, train_loss=5.4112988

Batch 135350, train_perplexity=228.61084, train_loss=5.432021

Batch 135360, train_perplexity=201.947, train_loss=5.3080053

Batch 135370, train_perplexity=212.70985, train_loss=5.359929

Batch 135380, train_perplexity=197.02524, train_loss=5.283332

Batch 135390, train_perplexity=224.71117, train_loss=5.414816

Batch 135400, train_perplexity=233.04688, train_loss=5.4512396

Batch 135410, train_perplexity=228.04327, train_loss=5.4295354

Batch 135420, train_perplexity=192.63928, train_loss=5.2608194

Batch 135430, train_perplexity=234.74323, train_loss=5.4584923

Batch 135440, train_perplexity=212.10884, train_loss=5.3570995

Batch 135450, train_perplexity=221.94055, train_loss=5.4024096

Batch 135460, train_perplexity=218.70139, train_loss=5.387707

Batch 135470, train_perplexity=203.61845, train_loss=5.316248

Batch 135480, train_perplexity=211.6233, train_loss=5.354808

Batch 135490, train_perplexity=199.55847, train_loss=5.2961073

Batch 135500, train_perplexity=244.3933, train_loss=5.498779

Batch 135510, train_perplexity=196.26218, train_loss=5.2794514

Batch 135520, train_perplexity=217.64597, train_loss=5.3828697

Batch 135530, train_perplexity=225.39433, train_loss=5.4178514

Batch 135540, train_perplexity=207.75232, train_loss=5.3363466

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Batch 135550, train_perplexity=229.71777, train_loss=5.4368515

Batch 135560, train_perplexity=231.376, train_loss=5.444044

Batch 135570, train_perplexity=209.56734, train_loss=5.345045

Batch 135580, train_perplexity=206.228, train_loss=5.3289824

Batch 135590, train_perplexity=218.64685, train_loss=5.387458

Batch 135600, train_perplexity=218.58575, train_loss=5.3871784

Batch 135610, train_perplexity=220.20143, train_loss=5.3945427

Batch 135620, train_perplexity=203.60439, train_loss=5.316179

Batch 135630, train_perplexity=215.14662, train_loss=5.37132

Batch 135640, train_perplexity=204.60048, train_loss=5.321059

Batch 135650, train_perplexity=225.36897, train_loss=5.417739

Batch 135660, train_perplexity=198.84607, train_loss=5.292531

Batch 135670, train_perplexity=219.95723, train_loss=5.393433

Batch 135680, train_perplexity=240.20541, train_loss=5.4814944

Batch 135690, train_perplexity=217.96648, train_loss=5.3843412

Batch 135700, train_perplexity=202.01173, train_loss=5.308326

Batch 135710, train_perplexity=217.85321, train_loss=5.3838215

Batch 135720, train_perplexity=216.96208, train_loss=5.3797226

Batch 135730, train_perplexity=219.54364, train_loss=5.391551

Batch 135740, train_perplexity=221.1221, train_loss=5.398715

Batch 135750, train_perplexity=250.86998, train_loss=5.524935

Batch 135760, train_perplexity=204.16933, train_loss=5.3189497

Batch 135770, train_perplexity=237.91747, train_loss=5.471924

Batch 135780, train_perplexity=226.34686, train_loss=5.4220686

Batch 135790, train_perplexity=197.20703, train_loss=5.284254

Batch 135800, train_perplexity=219.02512, train_loss=5.3891864

Batch 135810, train_perplexity=219.4756, train_loss=5.391241

Batch 135820, train_perplexity=224.98715, train_loss=5.4160433

Batch 135830, train_perplexity=236.29301, train_loss=5.4650726

Batch 135840, train_perplexity=225.66899, train_loss=5.4190693

Batch 135850, train_perplexity=234.45126, train_loss=5.4572477

Batch 135860, train_perplexity=196.74031, train_loss=5.2818847

Batch 135870, train_perplexity=220.75842, train_loss=5.397069

Batch 135880, train_perplexity=229.53163, train_loss=5.436041

Batch 135890, train_perplexity=228.71289, train_loss=5.4324675

Batch 135900, train_perplexity=203.11595, train_loss=5.313777

Batch 135910, train_perplexity=211.86725, train_loss=5.35596

Batch 135920, train_perplexity=201.54451, train_loss=5.3060102

Batch 135930, train_perplexity=224.99745, train_loss=5.416089

Batch 135940, train_perplexity=220.32642, train_loss=5.39511

Batch 135950, train_perplexity=219.86946, train_loss=5.393034

Batch 135960, train_perplexity=214.12868, train_loss=5.366577

Batch 135970, train_perplexity=199.29962, train_loss=5.2948093

Batch 135980, train_perplexity=199.67546, train_loss=5.2966933

Batch 135990, train_perplexity=235.94376, train_loss=5.4635935

Batch 136000, train_perplexity=232.922, train_loss=5.4507036

Batch 136010, train_perplexity=214.74904, train_loss=5.36947

Batch 136020, train_perplexity=183.43085, train_loss=5.211838

Batch 136030, train_perplexity=237.41837, train_loss=5.469824

Batch 136040, train_perplexity=230.62929, train_loss=5.4408116

Batch 136050, train_perplexity=213.21619, train_loss=5.3623066

Batch 136060, train_perplexity=216.3674, train_loss=5.376978

Batch 136070, train_perplexity=212.88338, train_loss=5.3607445

Batch 136080, train_perplexity=217.2677, train_loss=5.38113

Batch 136090, train_perplexity=209.39153, train_loss=5.344206

Batch 136100, train_perplexity=212.15274, train_loss=5.3573065

Batch 136110, train_perplexity=219.94643, train_loss=5.393384

Batch 136120, train_perplexity=224.69768, train_loss=5.414756

Batch 136130, train_perplexity=236.23038, train_loss=5.4648075

Batch 136140, train_perplexity=211.99054, train_loss=5.3565416

Batch 136150, train_perplexity=217.96346, train_loss=5.3843274

Batch 136160, train_perplexity=184.33759, train_loss=5.2167687

Batch 136170, train_perplexity=221.46071, train_loss=5.400245

Batch 136180, train_perplexity=219.39745, train_loss=5.390885

Batch 136190, train_perplexity=207.04839, train_loss=5.3329525

Batch 136200, train_perplexity=230.38123, train_loss=5.4397354

Batch 136210, train_perplexity=238.06567, train_loss=5.4725466

Batch 136220, train_perplexity=219.17587, train_loss=5.3898745

Batch 136230, train_perplexity=226.24036, train_loss=5.421598

Batch 136240, train_perplexity=254.02403, train_loss=5.537429

Batch 136250, train_perplexity=230.78407, train_loss=5.4414825

Batch 136260, train_perplexity=215.01761, train_loss=5.37072

Batch 136270, train_perplexity=209.43127, train_loss=5.3443956

Batch 136280, train_perplexity=211.2819, train_loss=5.3531933

Batch 136290, train_perplexity=197.73198, train_loss=5.2869124

Batch 136300, train_perplexity=232.21248, train_loss=5.447653

Batch 136310, train_perplexity=200.30847, train_loss=5.2998586

Batch 136320, train_perplexity=211.87907, train_loss=5.3560157

Batch 136330, train_perplexity=208.95277, train_loss=5.3421082

Batch 136340, train_perplexity=202.9861, train_loss=5.3131375

Batch 136350, train_perplexity=236.73195, train_loss=5.4669285
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 136360, train_perplexity=234.11009, train_loss=5.4557915

Batch 136370, train_perplexity=221.25288, train_loss=5.3993063

Batch 136380, train_perplexity=195.52965, train_loss=5.275712

Batch 136390, train_perplexity=209.39873, train_loss=5.34424

Batch 136400, train_perplexity=195.27193, train_loss=5.274393

Batch 136410, train_perplexity=194.74153, train_loss=5.271673

Batch 136420, train_perplexity=205.87291, train_loss=5.327259

Batch 136430, train_perplexity=225.62187, train_loss=5.4188604

Batch 136440, train_perplexity=212.333, train_loss=5.3581557

Batch 136450, train_perplexity=212.90591, train_loss=5.3608503

Batch 136460, train_perplexity=215.95532, train_loss=5.3750715

Batch 136470, train_perplexity=197.45102, train_loss=5.2854905

Batch 136480, train_perplexity=221.00456, train_loss=5.3981833

Batch 136490, train_perplexity=227.95554, train_loss=5.4291506

Batch 136500, train_perplexity=218.05983, train_loss=5.3847694

Batch 136510, train_perplexity=231.53383, train_loss=5.444726

Batch 136520, train_perplexity=213.42877, train_loss=5.363303

Batch 136530, train_perplexity=227.8751, train_loss=5.4287977

Batch 136540, train_perplexity=201.6343, train_loss=5.3064556

Batch 136550, train_perplexity=219.41867, train_loss=5.3909817

Batch 136560, train_perplexity=218.3194, train_loss=5.385959

Batch 136570, train_perplexity=219.00328, train_loss=5.3890867

Batch 136580, train_perplexity=240.08803, train_loss=5.4810057

Batch 136590, train_perplexity=222.69948, train_loss=5.405823

Batch 136600, train_perplexity=209.95732, train_loss=5.3469043

Batch 136610, train_perplexity=232.3937, train_loss=5.448433

Batch 136620, train_perplexity=206.4037, train_loss=5.329834

Batch 136630, train_perplexity=234.93561, train_loss=5.4593115

Batch 136640, train_perplexity=218.59628, train_loss=5.3872266

Batch 136650, train_perplexity=215.52757, train_loss=5.373089

Batch 136660, train_perplexity=187.28217, train_loss=5.2326164

Batch 136670, train_perplexity=218.82896, train_loss=5.3882904

Batch 136680, train_perplexity=225.79654, train_loss=5.4196343

Batch 136690, train_perplexity=228.63242, train_loss=5.4321156

Batch 136700, train_perplexity=230.19652, train_loss=5.4389334

Batch 136710, train_perplexity=211.6457, train_loss=5.3549137

Batch 136720, train_perplexity=199.31702, train_loss=5.2948966

Batch 136730, train_perplexity=234.09892, train_loss=5.455744

Batch 136740, train_perplexity=213.00441, train_loss=5.361313

Batch 136750, train_perplexity=232.6354, train_loss=5.4494724

Batch 136760, train_perplexity=202.19426, train_loss=5.309229

Batch 136770, train_perplexity=197.11856, train_loss=5.2838054

Batch 136780, train_perplexity=228.80605, train_loss=5.4328747

Batch 136790, train_perplexity=222.40573, train_loss=5.4045033

Batch 136800, train_perplexity=223.2331, train_loss=5.4082165

Batch 136810, train_perplexity=222.06017, train_loss=5.4029484

Batch 136820, train_perplexity=251.18861, train_loss=5.526204

Batch 136830, train_perplexity=215.42647, train_loss=5.3726196

Batch 136840, train_perplexity=221.68004, train_loss=5.401235

Batch 136850, train_perplexity=220.87825, train_loss=5.3976116

Batch 136860, train_perplexity=210.83627, train_loss=5.351082

Batch 136870, train_perplexity=211.19084, train_loss=5.352762

Batch 136880, train_perplexity=227.41345, train_loss=5.4267697

Batch 136890, train_perplexity=197.31859, train_loss=5.2848196

Batch 136900, train_perplexity=234.46199, train_loss=5.4572935

Batch 136910, train_perplexity=217.5447, train_loss=5.3824043

Batch 136920, train_perplexity=235.56717, train_loss=5.461996

Batch 136930, train_perplexity=215.47978, train_loss=5.372867

Batch 136940, train_perplexity=207.72874, train_loss=5.336233

Batch 136950, train_perplexity=229.78822, train_loss=5.437158

Batch 136960, train_perplexity=214.07396, train_loss=5.3663216

Batch 136970, train_perplexity=200.5018, train_loss=5.300823

Batch 136980, train_perplexity=219.44484, train_loss=5.391101

Batch 136990, train_perplexity=203.29831, train_loss=5.3146744

Batch 137000, train_perplexity=207.13133, train_loss=5.333353

Batch 137010, train_perplexity=218.82968, train_loss=5.3882937

Batch 137020, train_perplexity=222.88657, train_loss=5.406663

Batch 137030, train_perplexity=222.02586, train_loss=5.402794

Batch 137040, train_perplexity=241.44344, train_loss=5.486635

Batch 137050, train_perplexity=206.4228, train_loss=5.3299265

Batch 137060, train_perplexity=214.12215, train_loss=5.3665466

Batch 137070, train_perplexity=213.00482, train_loss=5.361315

Batch 137080, train_perplexity=190.0356, train_loss=5.2472115

Batch 137090, train_perplexity=207.23387, train_loss=5.333848

Batch 137100, train_perplexity=222.07129, train_loss=5.4029984

Batch 137110, train_perplexity=215.51112, train_loss=5.3730125

Batch 137120, train_perplexity=198.95818, train_loss=5.2930946

Batch 137130, train_perplexity=214.41844, train_loss=5.3679295

Batch 137140, train_perplexity=200.37181, train_loss=5.3001747

Batch 137150, train_perplexity=213.33456, train_loss=5.3628616

Batch 137160, train_perplexity=212.67741, train_loss=5.3597765

Batch 137170, train_perplexity=231.70612, train_loss=5.44547

Batch 137180, train_perplexity=210.74902, train_loss=5.350668

Batch 137190, train_perplexity=216.9747, train_loss=5.379781

Batch 137200, train_perplexity=222.35112, train_loss=5.404258

Batch 137210, train_perplexity=231.39288, train_loss=5.444117

Batch 137220, train_perplexity=202.0151, train_loss=5.3083425

Batch 137230, train_perplexity=212.39, train_loss=5.358424

Batch 137240, train_perplexity=231.08603, train_loss=5.44279

Batch 137250, train_perplexity=219.85446, train_loss=5.392966

Batch 137260, train_perplexity=189.662, train_loss=5.2452435

Batch 137270, train_perplexity=211.1414, train_loss=5.352528

Batch 137280, train_perplexity=211.33984, train_loss=5.3534675

Batch 137290, train_perplexity=239.52042, train_loss=5.4786386

Batch 137300, train_perplexity=199.47362, train_loss=5.295682

Batch 137310, train_perplexity=211.6223, train_loss=5.354803

Batch 137320, train_perplexity=211.98376, train_loss=5.3565097

Batch 137330, train_perplexity=216.315, train_loss=5.3767357

Batch 137340, train_perplexity=216.42395, train_loss=5.377239

Batch 137350, train_perplexity=207.5161, train_loss=5.335209

Batch 137360, train_perplexity=199.02138, train_loss=5.293412

Batch 137370, train_perplexity=216.13416, train_loss=5.3758993

Batch 137380, train_perplexity=197.6133, train_loss=5.286312

Batch 137390, train_perplexity=205.96698, train_loss=5.327716

Batch 137400, train_perplexity=205.9067, train_loss=5.327423

Batch 137410, train_perplexity=235.55077, train_loss=5.4619265

Batch 137420, train_perplexity=212.07344, train_loss=5.3569326

Batch 137430, train_perplexity=211.44196, train_loss=5.3539505

Batch 137440, train_perplexity=211.39558, train_loss=5.353731

Batch 137450, train_perplexity=234.3643, train_loss=5.4568768

Batch 137460, train_perplexity=221.38628, train_loss=5.399909

Batch 137470, train_perplexity=197.31783, train_loss=5.284816

Batch 137480, train_perplexity=232.4919, train_loss=5.4488554

Batch 137490, train_perplexity=213.54788, train_loss=5.363861

Batch 137500, train_perplexity=201.95471, train_loss=5.3080435

Batch 137510, train_perplexity=187.81357, train_loss=5.23545

Batch 137520, train_perplexity=195.233, train_loss=5.274194

Batch 137530, train_perplexity=196.81023, train_loss=5.28224

Batch 137540, train_perplexity=229.10245, train_loss=5.4341693

Batch 137550, train_perplexity=205.89098, train_loss=5.327347

Batch 137560, train_perplexity=233.12334, train_loss=5.4515676

Batch 137570, train_perplexity=222.16989, train_loss=5.4034424

Batch 137580, train_perplexity=225.0994, train_loss=5.416542

Batch 137590, train_perplexity=207.63448, train_loss=5.335779

Batch 137600, train_perplexity=244.01102, train_loss=5.4972134

Batch 137610, train_perplexity=217.24335, train_loss=5.381018

Batch 137620, train_perplexity=218.007, train_loss=5.384527

Batch 137630, train_perplexity=222.51532, train_loss=5.404996

Batch 137640, train_perplexity=207.57271, train_loss=5.3354816

Batch 137650, train_perplexity=225.11688, train_loss=5.41662
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 137660, train_perplexity=214.65906, train_loss=5.369051

Batch 137670, train_perplexity=214.23746, train_loss=5.367085

Batch 137680, train_perplexity=204.82588, train_loss=5.3221602

Batch 137690, train_perplexity=242.6537, train_loss=5.4916353

Batch 137700, train_perplexity=192.37363, train_loss=5.2594395

Batch 137710, train_perplexity=204.3612, train_loss=5.319889

Batch 137720, train_perplexity=263.1282, train_loss=5.5726414

Batch 137730, train_perplexity=228.52277, train_loss=5.431636

Batch 137740, train_perplexity=194.8425, train_loss=5.2721915

Batch 137750, train_perplexity=214.01721, train_loss=5.3660564

Batch 137760, train_perplexity=243.70915, train_loss=5.4959755

Batch 137770, train_perplexity=211.91858, train_loss=5.356202

Batch 137780, train_perplexity=223.53027, train_loss=5.409547

Batch 137790, train_perplexity=227.56976, train_loss=5.427457

Batch 137800, train_perplexity=217.74251, train_loss=5.383313

Batch 137810, train_perplexity=226.90057, train_loss=5.424512

Batch 137820, train_perplexity=209.00768, train_loss=5.342371

Batch 137830, train_perplexity=233.16936, train_loss=5.451765

Batch 137840, train_perplexity=228.17053, train_loss=5.4300933

Batch 137850, train_perplexity=219.29285, train_loss=5.390408

Batch 137860, train_perplexity=209.85033, train_loss=5.3463945

Batch 137870, train_perplexity=190.99156, train_loss=5.252229

Batch 137880, train_perplexity=241.17993, train_loss=5.4855433

Batch 137890, train_perplexity=212.41046, train_loss=5.3585205

Batch 137900, train_perplexity=226.22202, train_loss=5.421517

Batch 137910, train_perplexity=227.95293, train_loss=5.429139

Batch 137920, train_perplexity=215.96994, train_loss=5.375139

Batch 137930, train_perplexity=234.76718, train_loss=5.4585943

Batch 137940, train_perplexity=227.67418, train_loss=5.4279156

Batch 137950, train_perplexity=205.2456, train_loss=5.3242073

Batch 137960, train_perplexity=199.546, train_loss=5.296045

Batch 137970, train_perplexity=189.57059, train_loss=5.2447615

Batch 137980, train_perplexity=204.11852, train_loss=5.318701

Batch 137990, train_perplexity=221.27545, train_loss=5.3994083

Batch 138000, train_perplexity=208.27187, train_loss=5.3388443

Batch 138010, train_perplexity=208.6102, train_loss=5.3404675

Batch 138020, train_perplexity=196.89479, train_loss=5.2826695

Batch 138030, train_perplexity=193.82956, train_loss=5.266979

Batch 138040, train_perplexity=229.9749, train_loss=5.43797

Batch 138050, train_perplexity=198.02287, train_loss=5.2883825

Batch 138060, train_perplexity=220.65697, train_loss=5.3966093

Batch 138070, train_perplexity=239.39207, train_loss=5.4781027

Batch 138080, train_perplexity=192.66878, train_loss=5.2609725

Batch 138090, train_perplexity=223.18839, train_loss=5.408016

Batch 138100, train_perplexity=209.23184, train_loss=5.343443

Batch 138110, train_perplexity=190.10992, train_loss=5.2476025

Batch 138120, train_perplexity=216.66393, train_loss=5.3783474

Batch 138130, train_perplexity=232.86092, train_loss=5.4504414

Batch 138140, train_perplexity=191.14006, train_loss=5.2530065

Batch 138150, train_perplexity=215.97057, train_loss=5.375142

Batch 138160, train_perplexity=206.43146, train_loss=5.3299685

Batch 138170, train_perplexity=209.2915, train_loss=5.343728

Batch 138180, train_perplexity=222.20718, train_loss=5.40361

Batch 138190, train_perplexity=206.49092, train_loss=5.3302565

Batch 138200, train_perplexity=202.10403, train_loss=5.3087826

Batch 138210, train_perplexity=217.70015, train_loss=5.3831186

Batch 138220, train_perplexity=232.16864, train_loss=5.447464

Batch 138230, train_perplexity=217.36613, train_loss=5.381583

Batch 138240, train_perplexity=228.31909, train_loss=5.430744

Batch 138250, train_perplexity=215.35231, train_loss=5.3722754

Batch 138260, train_perplexity=213.12105, train_loss=5.3618603

Batch 138270, train_perplexity=199.40228, train_loss=5.2953243

Batch 138280, train_perplexity=231.37314, train_loss=5.4440317

Batch 138290, train_perplexity=199.06778, train_loss=5.2936454

Batch 138300, train_perplexity=257.3117, train_loss=5.550288

Batch 138310, train_perplexity=218.0929, train_loss=5.384921

Batch 138320, train_perplexity=217.69972, train_loss=5.3831167

Batch 138330, train_perplexity=201.32675, train_loss=5.3049293

Batch 138340, train_perplexity=235.1147, train_loss=5.4600735

Batch 138350, train_perplexity=210.58559, train_loss=5.349892

Batch 138360, train_perplexity=234.9699, train_loss=5.4594574

Batch 138370, train_perplexity=221.39851, train_loss=5.3999643

Batch 138380, train_perplexity=205.26762, train_loss=5.3243146

Batch 138390, train_perplexity=231.4814, train_loss=5.4444995

Batch 138400, train_perplexity=215.32684, train_loss=5.372157

Batch 138410, train_perplexity=204.56908, train_loss=5.3209057

Batch 138420, train_perplexity=214.47182, train_loss=5.3681784

Batch 138430, train_perplexity=204.73184, train_loss=5.321701

Batch 138440, train_perplexity=239.61923, train_loss=5.479051

Batch 138450, train_perplexity=228.18924, train_loss=5.4301753

Batch 138460, train_perplexity=214.55365, train_loss=5.36856

Batch 138470, train_perplexity=239.80635, train_loss=5.4798317

Batch 138480, train_perplexity=223.09007, train_loss=5.4075756

Batch 138490, train_perplexity=214.09111, train_loss=5.3664017

Batch 138500, train_perplexity=218.19077, train_loss=5.38537

Batch 138510, train_perplexity=208.82338, train_loss=5.341489

Batch 138520, train_perplexity=202.4866, train_loss=5.3106737

Batch 138530, train_perplexity=205.27643, train_loss=5.3243575

Batch 138540, train_perplexity=200.43832, train_loss=5.3005066

Batch 138550, train_perplexity=193.91878, train_loss=5.2674394

Batch 138560, train_perplexity=220.67633, train_loss=5.396697

Batch 138570, train_perplexity=220.10538, train_loss=5.3941064

Batch 138580, train_perplexity=206.70578, train_loss=5.3312964

Batch 138590, train_perplexity=226.20152, train_loss=5.4214263

Batch 138600, train_perplexity=237.65033, train_loss=5.4708004

Batch 138610, train_perplexity=240.89868, train_loss=5.4843764

Batch 138620, train_perplexity=203.34349, train_loss=5.3148966

Batch 138630, train_perplexity=220.90878, train_loss=5.39775

Batch 138640, train_perplexity=208.67955, train_loss=5.3408

Batch 138650, train_perplexity=216.36885, train_loss=5.3769846

Batch 138660, train_perplexity=229.63454, train_loss=5.436489

Batch 138670, train_perplexity=261.7117, train_loss=5.5672436

Batch 138680, train_perplexity=217.30573, train_loss=5.381305

Batch 138690, train_perplexity=231.00395, train_loss=5.442435

Batch 138700, train_perplexity=212.33653, train_loss=5.3581724

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Batch 138710, train_perplexity=177.53914, train_loss=5.179191

Batch 138720, train_perplexity=208.8658, train_loss=5.341692

Batch 138730, train_perplexity=217.96169, train_loss=5.3843193

Batch 138740, train_perplexity=208.5203, train_loss=5.3400364

Batch 138750, train_perplexity=227.30493, train_loss=5.4262924

Batch 138760, train_perplexity=222.8296, train_loss=5.4064074

Batch 138770, train_perplexity=224.13266, train_loss=5.412238

Batch 138780, train_perplexity=226.8955, train_loss=5.4244895

Batch 138790, train_perplexity=214.27188, train_loss=5.3672457

Batch 138800, train_perplexity=229.22452, train_loss=5.434702

Batch 138810, train_perplexity=254.31514, train_loss=5.538574

Batch 138820, train_perplexity=237.53001, train_loss=5.470294

Batch 138830, train_perplexity=211.18823, train_loss=5.35275

Batch 138840, train_perplexity=207.4588, train_loss=5.334933

Batch 138850, train_perplexity=241.74986, train_loss=5.4879036

Batch 138860, train_perplexity=202.98311, train_loss=5.3131227

Batch 138870, train_perplexity=213.09076, train_loss=5.361718

Batch 138880, train_perplexity=175.49191, train_loss=5.167593

Batch 138890, train_perplexity=198.85223, train_loss=5.292562
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 138900, train_perplexity=194.1148, train_loss=5.26845

Batch 138910, train_perplexity=178.82327, train_loss=5.186398

Batch 138920, train_perplexity=215.60056, train_loss=5.3734274

Batch 138930, train_perplexity=217.63268, train_loss=5.3828087

Batch 138940, train_perplexity=223.6659, train_loss=5.4101534

Batch 138950, train_perplexity=199.46124, train_loss=5.29562

Batch 138960, train_perplexity=200.85155, train_loss=5.302566

Batch 138970, train_perplexity=192.61696, train_loss=5.2607036

Batch 138980, train_perplexity=210.80008, train_loss=5.35091

Batch 138990, train_perplexity=210.78218, train_loss=5.3508253

Batch 139000, train_perplexity=201.46352, train_loss=5.3056083

Batch 139010, train_perplexity=213.54494, train_loss=5.3638473

Batch 139020, train_perplexity=222.51532, train_loss=5.404996

Batch 139030, train_perplexity=275.01025, train_loss=5.6168084

Batch 139040, train_perplexity=222.28317, train_loss=5.403952

Batch 139050, train_perplexity=212.16487, train_loss=5.3573637

Batch 139060, train_perplexity=256.62723, train_loss=5.5476246

Batch 139070, train_perplexity=235.13466, train_loss=5.4601583

Batch 139080, train_perplexity=217.19623, train_loss=5.380801

Batch 139090, train_perplexity=244.4088, train_loss=5.4988422

Batch 139100, train_perplexity=231.05836, train_loss=5.4426703

Batch 139110, train_perplexity=240.07773, train_loss=5.4809628

Batch 139120, train_perplexity=215.54771, train_loss=5.3731823

Batch 139130, train_perplexity=234.86505, train_loss=5.459011

Batch 139140, train_perplexity=219.52458, train_loss=5.391464

Batch 139150, train_perplexity=232.01315, train_loss=5.446794

Batch 139160, train_perplexity=217.64908, train_loss=5.382884

Batch 139170, train_perplexity=207.72945, train_loss=5.3362365

Batch 139180, train_perplexity=231.89159, train_loss=5.44627

Batch 139190, train_perplexity=232.77788, train_loss=5.4500847

Batch 139200, train_perplexity=219.9672, train_loss=5.3934784

Batch 139210, train_perplexity=213.41402, train_loss=5.363234

Batch 139220, train_perplexity=212.7691, train_loss=5.3602076

Batch 139230, train_perplexity=216.70009, train_loss=5.3785143

Batch 139240, train_perplexity=240.39848, train_loss=5.482298

Batch 139250, train_perplexity=218.40468, train_loss=5.3863497

Batch 139260, train_perplexity=228.41862, train_loss=5.43118

Batch 139270, train_perplexity=226.80862, train_loss=5.4241066

Batch 139280, train_perplexity=218.14302, train_loss=5.385151

Batch 139290, train_perplexity=216.1727, train_loss=5.3760777

Batch 139300, train_perplexity=238.86232, train_loss=5.4758873

Batch 139310, train_perplexity=208.6906, train_loss=5.3408527

Batch 139320, train_perplexity=218.7309, train_loss=5.387842

Batch 139330, train_perplexity=225.08415, train_loss=5.4164743

Batch 139340, train_perplexity=195.80965, train_loss=5.277143

Batch 139350, train_perplexity=194.8138, train_loss=5.272044

Batch 139360, train_perplexity=199.47342, train_loss=5.295681

Batch 139370, train_perplexity=196.71443, train_loss=5.281753

Batch 139380, train_perplexity=196.98824, train_loss=5.283144

Batch 139390, train_perplexity=194.63506, train_loss=5.2711263

Batch 139400, train_perplexity=188.27867, train_loss=5.237923

Batch 139410, train_perplexity=215.21147, train_loss=5.371621

Batch 139420, train_perplexity=207.45712, train_loss=5.3349247

Batch 139430, train_perplexity=201.78934, train_loss=5.3072243

Batch 139440, train_perplexity=195.4397, train_loss=5.275252

Batch 139450, train_perplexity=231.27066, train_loss=5.4435887

Batch 139460, train_perplexity=209.38893, train_loss=5.3441935

Batch 139470, train_perplexity=216.60628, train_loss=5.3780813

Batch 139480, train_perplexity=216.44872, train_loss=5.3773537

Batch 139490, train_perplexity=205.43105, train_loss=5.3251104

Batch 139500, train_perplexity=209.00967, train_loss=5.3423805

Batch 139510, train_perplexity=199.2485, train_loss=5.294553

Batch 139520, train_perplexity=216.46461, train_loss=5.377427

Batch 139530, train_perplexity=205.64186, train_loss=5.326136

Batch 139540, train_perplexity=216.67445, train_loss=5.378396

Batch 139550, train_perplexity=234.67932, train_loss=5.45822

Batch 139560, train_perplexity=204.65756, train_loss=5.321338

Batch 139570, train_perplexity=200.85834, train_loss=5.3026

Batch 139580, train_perplexity=217.23102, train_loss=5.3809614

Batch 139590, train_perplexity=209.75189, train_loss=5.3459253

Batch 139600, train_perplexity=211.3974, train_loss=5.3537397

Batch 139610, train_perplexity=213.53658, train_loss=5.363808

Batch 139620, train_perplexity=218.73111, train_loss=5.387843

Batch 139630, train_perplexity=201.06505, train_loss=5.3036284

Batch 139640, train_perplexity=230.7482, train_loss=5.441327

Batch 139650, train_perplexity=240.01385, train_loss=5.4806967

Batch 139660, train_perplexity=218.27829, train_loss=5.385771

Batch 139670, train_perplexity=214.73871, train_loss=5.369422

Batch 139680, train_perplexity=209.06967, train_loss=5.3426676

Batch 139690, train_perplexity=208.06738, train_loss=5.337862

Batch 139700, train_perplexity=220.89531, train_loss=5.397689

Batch 139710, train_perplexity=204.68402, train_loss=5.3214674

Batch 139720, train_perplexity=224.80345, train_loss=5.4152265

Batch 139730, train_perplexity=204.6319, train_loss=5.321213

Batch 139740, train_perplexity=207.40224, train_loss=5.33466

Batch 139750, train_perplexity=200.50122, train_loss=5.3008204

Batch 139760, train_perplexity=200.51556, train_loss=5.300892

Batch 139770, train_perplexity=212.8103, train_loss=5.360401

Batch 139780, train_perplexity=217.5003, train_loss=5.3822002

Batch 139790, train_perplexity=194.4053, train_loss=5.269945

Batch 139800, train_perplexity=211.49036, train_loss=5.3541794

Batch 139810, train_perplexity=240.83, train_loss=5.4840913

Batch 139820, train_perplexity=194.45592, train_loss=5.2702055

Batch 139830, train_perplexity=226.33218, train_loss=5.4220037

Batch 139840, train_perplexity=207.55034, train_loss=5.335374

Batch 139850, train_perplexity=221.1282, train_loss=5.3987427

Batch 139860, train_perplexity=217.61307, train_loss=5.3827186

Batch 139870, train_perplexity=224.84515, train_loss=5.415412

Batch 139880, train_perplexity=211.27939, train_loss=5.3531814

Batch 139890, train_perplexity=203.7215, train_loss=5.316754

Batch 139900, train_perplexity=194.64322, train_loss=5.271168

Batch 139910, train_perplexity=192.61081, train_loss=5.2606716

Batch 139920, train_perplexity=188.68697, train_loss=5.2400894

Batch 139930, train_perplexity=222.41177, train_loss=5.4045305

Batch 139940, train_perplexity=207.38057, train_loss=5.3345556

Batch 139950, train_perplexity=218.4553, train_loss=5.3865814

Batch 139960, train_perplexity=198.42279, train_loss=5.2904

Batch 139970, train_perplexity=201.38658, train_loss=5.3052263

Batch 139980, train_perplexity=226.13293, train_loss=5.421123

Batch 139990, train_perplexity=199.69926, train_loss=5.2968125

Batch 140000, train_perplexity=228.84314, train_loss=5.433037

Batch 140010, train_perplexity=237.06203, train_loss=5.468322

Batch 140020, train_perplexity=223.00223, train_loss=5.4071817

Batch 140030, train_perplexity=221.36865, train_loss=5.3998294

Batch 140040, train_perplexity=189.89919, train_loss=5.2464933

Batch 140050, train_perplexity=220.75694, train_loss=5.3970623

Batch 140060, train_perplexity=209.301, train_loss=5.3437734

Batch 140070, train_perplexity=224.01813, train_loss=5.411727

Batch 140080, train_perplexity=210.82059, train_loss=5.3510075

Batch 140090, train_perplexity=221.45195, train_loss=5.4002056

Batch 140100, train_perplexity=218.55293, train_loss=5.387028

Batch 140110, train_perplexity=193.83917, train_loss=5.267029

Batch 140120, train_perplexity=228.47516, train_loss=5.4314275

Batch 140130, train_perplexity=236.57126, train_loss=5.4662495

Batch 140140, train_perplexity=210.70631, train_loss=5.3504653

Batch 140150, train_perplexity=205.59735, train_loss=5.3259196

Batch 140160, train_perplexity=198.39252, train_loss=5.2902474

Batch 140170, train_perplexity=243.61028, train_loss=5.4955697

Batch 140180, train_perplexity=190.80887, train_loss=5.251272

Batch 140190, train_perplexity=211.70688, train_loss=5.3552027
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 140200, train_perplexity=199.27834, train_loss=5.2947025

Batch 140210, train_perplexity=218.30795, train_loss=5.3859067

Batch 140220, train_perplexity=212.28389, train_loss=5.3579245

Batch 140230, train_perplexity=204.36229, train_loss=5.3198943

Batch 140240, train_perplexity=223.2299, train_loss=5.408202

Batch 140250, train_perplexity=201.61064, train_loss=5.3063383

Batch 140260, train_perplexity=224.98157, train_loss=5.4160185

Batch 140270, train_perplexity=185.50548, train_loss=5.2230844

Batch 140280, train_perplexity=220.17696, train_loss=5.3944316

Batch 140290, train_perplexity=228.5958, train_loss=5.4319553

Batch 140300, train_perplexity=227.45303, train_loss=5.426944

Batch 140310, train_perplexity=203.20749, train_loss=5.3142276

Batch 140320, train_perplexity=211.12108, train_loss=5.352432

Batch 140330, train_perplexity=206.28052, train_loss=5.329237

Batch 140340, train_perplexity=204.24138, train_loss=5.3193026

Batch 140350, train_perplexity=209.94711, train_loss=5.3468556

Batch 140360, train_perplexity=256.79028, train_loss=5.5482597

Batch 140370, train_perplexity=224.52321, train_loss=5.413979

Batch 140380, train_perplexity=195.84915, train_loss=5.2773447

Batch 140390, train_perplexity=218.07979, train_loss=5.384861

Batch 140400, train_perplexity=198.6587, train_loss=5.2915883

Batch 140410, train_perplexity=208.94301, train_loss=5.3420615

Batch 140420, train_perplexity=224.3478, train_loss=5.4131975

Batch 140430, train_perplexity=200.61934, train_loss=5.3014092

Batch 140440, train_perplexity=212.93942, train_loss=5.3610077

Batch 140450, train_perplexity=225.87225, train_loss=5.4199696

Batch 140460, train_perplexity=212.28813, train_loss=5.3579445

Batch 140470, train_perplexity=208.14975, train_loss=5.338258

Batch 140480, train_perplexity=194.92084, train_loss=5.2725935

Batch 140490, train_perplexity=226.49757, train_loss=5.4227343

Batch 140500, train_perplexity=207.47731, train_loss=5.335022

Batch 140510, train_perplexity=205.70728, train_loss=5.326454

Batch 140520, train_perplexity=215.36577, train_loss=5.372338

Batch 140530, train_perplexity=226.54964, train_loss=5.422964

Batch 140540, train_perplexity=199.31313, train_loss=5.294877

Batch 140550, train_perplexity=217.5283, train_loss=5.382329

Batch 140560, train_perplexity=207.5248, train_loss=5.335251

Batch 140570, train_perplexity=204.46385, train_loss=5.320391

Batch 140580, train_perplexity=184.66353, train_loss=5.2185354

Batch 140590, train_perplexity=202.38959, train_loss=5.3101945

Batch 140600, train_perplexity=196.25711, train_loss=5.2794256

Batch 140610, train_perplexity=257.51324, train_loss=5.551071

Batch 140620, train_perplexity=220.31717, train_loss=5.395068

Batch 140630, train_perplexity=229.99222, train_loss=5.4380455

Batch 140640, train_perplexity=199.54497, train_loss=5.2960396

Batch 140650, train_perplexity=220.07358, train_loss=5.393962

Batch 140660, train_perplexity=221.71884, train_loss=5.40141

Batch 140670, train_perplexity=210.75063, train_loss=5.3506756

Batch 140680, train_perplexity=240.01672, train_loss=5.4807086

Batch 140690, train_perplexity=208.96085, train_loss=5.342147

Batch 140700, train_perplexity=222.3121, train_loss=5.4040823

Batch 140710, train_perplexity=197.1119, train_loss=5.2837715

Batch 140720, train_perplexity=235.32803, train_loss=5.4609804

Batch 140730, train_perplexity=201.55162, train_loss=5.3060455

Batch 140740, train_perplexity=210.60516, train_loss=5.349985

Batch 140750, train_perplexity=214.96819, train_loss=5.37049

Batch 140760, train_perplexity=217.41827, train_loss=5.381823

Batch 140770, train_perplexity=208.88304, train_loss=5.3417745

Batch 140780, train_perplexity=235.49371, train_loss=5.461684

Batch 140790, train_perplexity=207.58617, train_loss=5.3355465

Batch 140800, train_perplexity=204.15648, train_loss=5.3188868

Batch 140810, train_perplexity=232.01237, train_loss=5.4467907

Batch 140820, train_perplexity=221.04251, train_loss=5.398355

Batch 140830, train_perplexity=186.89893, train_loss=5.230568

Batch 140840, train_perplexity=219.65987, train_loss=5.3920803

Batch 140850, train_perplexity=224.59077, train_loss=5.41428

Batch 140860, train_perplexity=210.57143, train_loss=5.349825

Batch 140870, train_perplexity=216.38506, train_loss=5.3770595

Batch 140880, train_perplexity=224.84023, train_loss=5.41539

Batch 140890, train_perplexity=221.79456, train_loss=5.4017515

Batch 140900, train_perplexity=206.82498, train_loss=5.331873

Batch 140910, train_perplexity=216.35667, train_loss=5.3769283

Batch 140920, train_perplexity=213.86572, train_loss=5.3653483

Batch 140930, train_perplexity=196.24513, train_loss=5.2793646

Batch 140940, train_perplexity=194.13795, train_loss=5.268569

Batch 140950, train_perplexity=202.61032, train_loss=5.3112845

Batch 140960, train_perplexity=224.22523, train_loss=5.412651

Batch 140970, train_perplexity=217.08896, train_loss=5.380307

Batch 140980, train_perplexity=178.52542, train_loss=5.184731

Batch 140990, train_perplexity=201.48454, train_loss=5.3057127

Batch 141000, train_perplexity=216.04266, train_loss=5.375476

Batch 141010, train_perplexity=209.89667, train_loss=5.3466153

Batch 141020, train_perplexity=211.3733, train_loss=5.353626

Batch 141030, train_perplexity=191.09904, train_loss=5.252792

Batch 141040, train_perplexity=206.66528, train_loss=5.3311005

Batch 141050, train_perplexity=221.90732, train_loss=5.40226

Batch 141060, train_perplexity=206.2339, train_loss=5.329011

Batch 141070, train_perplexity=231.87656, train_loss=5.446205

Batch 141080, train_perplexity=210.78943, train_loss=5.3508596

Batch 141090, train_perplexity=220.26695, train_loss=5.3948402

Batch 141100, train_perplexity=226.83318, train_loss=5.424215

Batch 141110, train_perplexity=211.55269, train_loss=5.354474

Batch 141120, train_perplexity=232.8767, train_loss=5.450509

Batch 141130, train_perplexity=203.93814, train_loss=5.3178167

Batch 141140, train_perplexity=209.6467, train_loss=5.3454237

Batch 141150, train_perplexity=207.42557, train_loss=5.3347726

Batch 141160, train_perplexity=225.60745, train_loss=5.4187965

Batch 141170, train_perplexity=205.8522, train_loss=5.3271585

Batch 141180, train_perplexity=223.31816, train_loss=5.4085975

Batch 141190, train_perplexity=247.01051, train_loss=5.509431

Batch 141200, train_perplexity=227.88196, train_loss=5.428828

Batch 141210, train_perplexity=213.88876, train_loss=5.365456

Batch 141220, train_perplexity=193.59918, train_loss=5.26579

Batch 141230, train_perplexity=212.48157, train_loss=5.3588552

Batch 141240, train_perplexity=227.64877, train_loss=5.427804

Batch 141250, train_perplexity=219.94998, train_loss=5.3934

Batch 141260, train_perplexity=193.49028, train_loss=5.2652273

Batch 141270, train_perplexity=226.27153, train_loss=5.421736

Batch 141280, train_perplexity=194.88106, train_loss=5.2723894

Batch 141290, train_perplexity=188.10512, train_loss=5.237001

Batch 141300, train_perplexity=210.74883, train_loss=5.350667

Batch 141310, train_perplexity=235.9935, train_loss=5.4638042

Batch 141320, train_perplexity=215.49736, train_loss=5.3729486

Batch 141330, train_perplexity=234.7448, train_loss=5.458499

Batch 141340, train_perplexity=222.65329, train_loss=5.405616

Batch 141350, train_perplexity=236.68025, train_loss=5.46671

Batch 141360, train_perplexity=223.06529, train_loss=5.4074645

Batch 141370, train_perplexity=207.7034, train_loss=5.336111

Batch 141380, train_perplexity=181.33498, train_loss=5.200346

Batch 141390, train_perplexity=205.34134, train_loss=5.3246737

Batch 141400, train_perplexity=217.40056, train_loss=5.3817415

Batch 141410, train_perplexity=215.31175, train_loss=5.372087

Batch 141420, train_perplexity=207.91951, train_loss=5.337151

Batch 141430, train_perplexity=214.72069, train_loss=5.369338

Batch 141440, train_perplexity=222.19765, train_loss=5.4035673

Batch 141450, train_perplexity=194.19063, train_loss=5.2688403

Batch 141460, train_perplexity=219.73593, train_loss=5.3924265

Batch 141470, train_perplexity=199.69954, train_loss=5.296814

Batch 141480, train_perplexity=211.89554, train_loss=5.3560934

Batch 141490, train_perplexity=204.27168, train_loss=5.319451
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 141500, train_perplexity=203.36482, train_loss=5.3150015

Batch 141510, train_perplexity=194.96155, train_loss=5.2728024

Batch 141520, train_perplexity=189.47426, train_loss=5.244253

Batch 141530, train_perplexity=206.92827, train_loss=5.332372

Batch 141540, train_perplexity=245.89714, train_loss=5.5049133

Batch 141550, train_perplexity=231.5293, train_loss=5.4447064

Batch 141560, train_perplexity=231.85931, train_loss=5.4461308

Batch 141570, train_perplexity=225.64511, train_loss=5.4189634

Batch 141580, train_perplexity=192.35364, train_loss=5.2593355

Batch 141590, train_perplexity=191.68213, train_loss=5.2558384

Batch 141600, train_perplexity=212.93362, train_loss=5.3609805

Batch 141610, train_perplexity=224.7476, train_loss=5.414978

Batch 141620, train_perplexity=211.61856, train_loss=5.3547854

Batch 141630, train_perplexity=214.20915, train_loss=5.366953

Batch 141640, train_perplexity=218.7287, train_loss=5.387832

Batch 141650, train_perplexity=201.65736, train_loss=5.30657

Batch 141660, train_perplexity=221.54214, train_loss=5.400613

Batch 141670, train_perplexity=210.31273, train_loss=5.3485956

Batch 141680, train_perplexity=226.13153, train_loss=5.421117

Batch 141690, train_perplexity=205.82158, train_loss=5.3270097

Batch 141700, train_perplexity=201.53941, train_loss=5.305985

Batch 141710, train_perplexity=202.11406, train_loss=5.308832

Batch 141720, train_perplexity=224.71664, train_loss=5.41484

Batch 141730, train_perplexity=206.34564, train_loss=5.3295527

Batch 141740, train_perplexity=204.05234, train_loss=5.3183765

Batch 141750, train_perplexity=193.6364, train_loss=5.265982

Batch 141760, train_perplexity=207.32541, train_loss=5.3342896

Batch 141770, train_perplexity=226.20195, train_loss=5.421428

Batch 141780, train_perplexity=208.56773, train_loss=5.340264

Batch 141790, train_perplexity=200.97215, train_loss=5.3031664

Batch 141800, train_perplexity=192.19044, train_loss=5.2584867

Batch 141810, train_perplexity=221.74316, train_loss=5.40152

Batch 141820, train_perplexity=221.77182, train_loss=5.401649

Batch 141830, train_perplexity=200.85643, train_loss=5.3025904

Batch 141840, train_perplexity=231.90244, train_loss=5.4463167

Batch 141850, train_perplexity=191.45467, train_loss=5.254651

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Batch 141860, train_perplexity=194.62103, train_loss=5.2710543

Batch 141870, train_perplexity=205.53088, train_loss=5.3255963

Batch 141880, train_perplexity=216.90044, train_loss=5.3794384

Batch 141890, train_perplexity=224.37689, train_loss=5.413327

Batch 141900, train_perplexity=212.56792, train_loss=5.3592615

Batch 141910, train_perplexity=191.04147, train_loss=5.2524905

Batch 141920, train_perplexity=225.2509, train_loss=5.417215

Batch 141930, train_perplexity=215.05554, train_loss=5.3708963

Batch 141940, train_perplexity=211.82462, train_loss=5.3557587

Batch 141950, train_perplexity=210.88835, train_loss=5.351329

Batch 141960, train_perplexity=204.29477, train_loss=5.319564

Batch 141970, train_perplexity=244.33958, train_loss=5.498559

Batch 141980, train_perplexity=201.53682, train_loss=5.305972

Batch 141990, train_perplexity=204.4233, train_loss=5.320193

Batch 142000, train_perplexity=238.70097, train_loss=5.4752116

Batch 142010, train_perplexity=228.5899, train_loss=5.4319296

Batch 142020, train_perplexity=208.43123, train_loss=5.339609

Batch 142030, train_perplexity=216.1894, train_loss=5.376155

Batch 142040, train_perplexity=222.08812, train_loss=5.4030743

Batch 142050, train_perplexity=189.67575, train_loss=5.245316

Batch 142060, train_perplexity=235.66132, train_loss=5.4623957

Batch 142070, train_perplexity=202.54782, train_loss=5.310976

Batch 142080, train_perplexity=225.08748, train_loss=5.416489

Batch 142090, train_perplexity=200.95319, train_loss=5.303072

Batch 142100, train_perplexity=204.47389, train_loss=5.3204403

Batch 142110, train_perplexity=200.73752, train_loss=5.301998

Batch 142120, train_perplexity=207.28072, train_loss=5.334074

Batch 142130, train_perplexity=222.42812, train_loss=5.404604

Batch 142140, train_perplexity=203.68683, train_loss=5.3165836

Batch 142150, train_perplexity=212.67995, train_loss=5.3597884

Batch 142160, train_perplexity=228.32367, train_loss=5.430764

Batch 142170, train_perplexity=234.89238, train_loss=5.4591274

Batch 142180, train_perplexity=215.79906, train_loss=5.3743477

Batch 142190, train_perplexity=207.8748, train_loss=5.336936

Batch 142200, train_perplexity=222.5566, train_loss=5.4051814

Batch 142210, train_perplexity=194.8517, train_loss=5.2722387

Batch 142220, train_perplexity=183.53015, train_loss=5.212379

Batch 142230, train_perplexity=219.20033, train_loss=5.389986

Batch 142240, train_perplexity=195.19615, train_loss=5.274005

Batch 142250, train_perplexity=208.38214, train_loss=5.3393736

Batch 142260, train_perplexity=218.87831, train_loss=5.388516

Batch 142270, train_perplexity=198.14699, train_loss=5.289009

Batch 142280, train_perplexity=194.98238, train_loss=5.272909

Batch 142290, train_perplexity=208.0297, train_loss=5.337681

Batch 142300, train_perplexity=213.96211, train_loss=5.365799

Batch 142310, train_perplexity=213.32388, train_loss=5.3628116

Batch 142320, train_perplexity=227.78093, train_loss=5.4283843

Batch 142330, train_perplexity=211.7909, train_loss=5.3555994

Batch 142340, train_perplexity=188.21, train_loss=5.2375584

Batch 142350, train_perplexity=230.57927, train_loss=5.4405947

Batch 142360, train_perplexity=202.0705, train_loss=5.3086166

Batch 142370, train_perplexity=191.35765, train_loss=5.254144

Batch 142380, train_perplexity=223.25044, train_loss=5.408294

Batch 142390, train_perplexity=213.74573, train_loss=5.364787

Batch 142400, train_perplexity=196.72896, train_loss=5.281827

Batch 142410, train_perplexity=187.0951, train_loss=5.231617

Batch 142420, train_perplexity=200.59207, train_loss=5.3012733

Batch 142430, train_perplexity=207.21313, train_loss=5.333748

Batch 142440, train_perplexity=215.30704, train_loss=5.372065

Batch 142450, train_perplexity=204.94145, train_loss=5.3227243

Batch 142460, train_perplexity=205.0351, train_loss=5.323181

Batch 142470, train_perplexity=214.30254, train_loss=5.3673887

Batch 142480, train_perplexity=228.79874, train_loss=5.4328427

Batch 142490, train_perplexity=213.83707, train_loss=5.3652143

Batch 142500, train_perplexity=213.62305, train_loss=5.364213

Batch 142510, train_perplexity=198.02948, train_loss=5.288416

Batch 142520, train_perplexity=207.19208, train_loss=5.3336463

Batch 142530, train_perplexity=227.89967, train_loss=5.4289055

Batch 142540, train_perplexity=201.63141, train_loss=5.3064413

Batch 142550, train_perplexity=214.92986, train_loss=5.3703117

Batch 142560, train_perplexity=199.462, train_loss=5.295624

Batch 142570, train_perplexity=183.15187, train_loss=5.2103157

Batch 142580, train_perplexity=223.69373, train_loss=5.410278

Batch 142590, train_perplexity=203.32011, train_loss=5.3147817

Batch 142600, train_perplexity=199.91306, train_loss=5.2978826

Batch 142610, train_perplexity=221.42259, train_loss=5.400073

Batch 142620, train_perplexity=235.7718, train_loss=5.4628644

Batch 142630, train_perplexity=215.53497, train_loss=5.373123

Batch 142640, train_perplexity=203.14714, train_loss=5.3139305

Batch 142650, train_perplexity=226.98152, train_loss=5.4248686

Batch 142660, train_perplexity=241.11635, train_loss=5.4852796

Batch 142670, train_perplexity=207.27914, train_loss=5.3340664

Batch 142680, train_perplexity=214.87575, train_loss=5.37006

Batch 142690, train_perplexity=209.77109, train_loss=5.346017

Batch 142700, train_perplexity=224.97449, train_loss=5.415987

Batch 142710, train_perplexity=193.91193, train_loss=5.267404

Batch 142720, train_perplexity=210.32156, train_loss=5.3486376

Batch 142730, train_perplexity=200.69559, train_loss=5.3017893
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 142740, train_perplexity=213.73961, train_loss=5.3647585

Batch 142750, train_perplexity=208.47815, train_loss=5.339834

Batch 142760, train_perplexity=237.74667, train_loss=5.4712057

Batch 142770, train_perplexity=198.40396, train_loss=5.290305

Batch 142780, train_perplexity=194.99539, train_loss=5.272976

Batch 142790, train_perplexity=208.61995, train_loss=5.340514

Batch 142800, train_perplexity=189.86171, train_loss=5.246296

Batch 142810, train_perplexity=210.95976, train_loss=5.3516674

Batch 142820, train_perplexity=225.51915, train_loss=5.418405

Batch 142830, train_perplexity=239.01886, train_loss=5.4765425

Batch 142840, train_perplexity=207.22815, train_loss=5.3338203

Batch 142850, train_perplexity=230.67648, train_loss=5.441016

Batch 142860, train_perplexity=212.45746, train_loss=5.3587418

Batch 142870, train_perplexity=227.77234, train_loss=5.4283466

Batch 142880, train_perplexity=217.72049, train_loss=5.383212

Batch 142890, train_perplexity=236.31859, train_loss=5.465181

Batch 142900, train_perplexity=200.40028, train_loss=5.300317

Batch 142910, train_perplexity=223.00732, train_loss=5.4072046

Batch 142920, train_perplexity=213.06474, train_loss=5.361596

Batch 142930, train_perplexity=193.48439, train_loss=5.265197

Batch 142940, train_perplexity=244.64526, train_loss=5.4998093

Batch 142950, train_perplexity=218.31659, train_loss=5.3859463

Batch 142960, train_perplexity=202.70338, train_loss=5.3117437

Batch 142970, train_perplexity=234.34084, train_loss=5.4567766

Batch 142980, train_perplexity=240.58266, train_loss=5.4830637

Batch 142990, train_perplexity=214.38625, train_loss=5.3677793

Batch 143000, train_perplexity=203.03053, train_loss=5.3133564

Batch 143010, train_perplexity=223.60756, train_loss=5.4098926

Batch 143020, train_perplexity=207.54697, train_loss=5.3353577

Batch 143030, train_perplexity=225.82529, train_loss=5.4197617

Batch 143040, train_perplexity=214.16606, train_loss=5.3667517

Batch 143050, train_perplexity=227.73499, train_loss=5.4281826

Batch 143060, train_perplexity=192.75395, train_loss=5.2614145

Batch 143070, train_perplexity=221.55356, train_loss=5.4006643

Batch 143080, train_perplexity=205.1503, train_loss=5.323743

Batch 143090, train_perplexity=183.15388, train_loss=5.2103267

Batch 143100, train_perplexity=227.28639, train_loss=5.426211

Batch 143110, train_perplexity=214.35373, train_loss=5.3676276

Batch 143120, train_perplexity=206.76552, train_loss=5.3315854

Batch 143130, train_perplexity=217.8767, train_loss=5.3839293

Batch 143140, train_perplexity=208.78813, train_loss=5.34132

Batch 143150, train_perplexity=218.99765, train_loss=5.389061

Batch 143160, train_perplexity=195.40382, train_loss=5.2750683

Batch 143170, train_perplexity=201.3282, train_loss=5.3049364

Batch 143180, train_perplexity=210.28856, train_loss=5.3484807

Batch 143190, train_perplexity=184.80817, train_loss=5.2193184

Batch 143200, train_perplexity=210.23833, train_loss=5.348242

Batch 143210, train_perplexity=212.05565, train_loss=5.3568487

Batch 143220, train_perplexity=205.63136, train_loss=5.326085

Batch 143230, train_perplexity=207.75589, train_loss=5.336364

Batch 143240, train_perplexity=229.47845, train_loss=5.435809

Batch 143250, train_perplexity=217.61203, train_loss=5.382714

Batch 143260, train_perplexity=220.23692, train_loss=5.394704

Batch 143270, train_perplexity=239.70882, train_loss=5.479425

Batch 143280, train_perplexity=217.39319, train_loss=5.3817077

Batch 143290, train_perplexity=201.01894, train_loss=5.303399

Batch 143300, train_perplexity=194.26537, train_loss=5.269225

Batch 143310, train_perplexity=184.88432, train_loss=5.2197304

Batch 143320, train_perplexity=212.86581, train_loss=5.360662

Batch 143330, train_perplexity=209.99077, train_loss=5.3470635

Batch 143340, train_perplexity=227.10017, train_loss=5.425391

Batch 143350, train_perplexity=206.361, train_loss=5.329627

Batch 143360, train_perplexity=212.3655, train_loss=5.358309

Batch 143370, train_perplexity=252.93681, train_loss=5.5331397

Batch 143380, train_perplexity=237.616, train_loss=5.470656

Batch 143390, train_perplexity=236.33832, train_loss=5.4652643

Batch 143400, train_perplexity=216.0676, train_loss=5.3755913

Batch 143410, train_perplexity=195.32855, train_loss=5.274683

Batch 143420, train_perplexity=187.925, train_loss=5.236043

Batch 143430, train_perplexity=198.27846, train_loss=5.2896724

Batch 143440, train_perplexity=228.18892, train_loss=5.430174

Batch 143450, train_perplexity=197.3774, train_loss=5.2851176

Batch 143460, train_perplexity=199.30229, train_loss=5.2948227

Batch 143470, train_perplexity=190.75073, train_loss=5.2509675

Batch 143480, train_perplexity=204.22084, train_loss=5.319202

Batch 143490, train_perplexity=187.82521, train_loss=5.235512

Batch 143500, train_perplexity=209.23703, train_loss=5.3434677

Batch 143510, train_perplexity=229.36632, train_loss=5.4353204

Batch 143520, train_perplexity=219.50658, train_loss=5.391382

Batch 143530, train_perplexity=199.19702, train_loss=5.2942944

Batch 143540, train_perplexity=203.58293, train_loss=5.3160734

Batch 143550, train_perplexity=207.66516, train_loss=5.335927

Batch 143560, train_perplexity=234.86707, train_loss=5.4590197

Batch 143570, train_perplexity=215.30878, train_loss=5.372073

Batch 143580, train_perplexity=209.50838, train_loss=5.3447638

Batch 143590, train_perplexity=219.27612, train_loss=5.3903317

Batch 143600, train_perplexity=224.2417, train_loss=5.4127245

Batch 143610, train_perplexity=224.81085, train_loss=5.4152594

Batch 143620, train_perplexity=173.71423, train_loss=5.1574116

Batch 143630, train_perplexity=206.61482, train_loss=5.3308563

Batch 143640, train_perplexity=213.80801, train_loss=5.3650784

Batch 143650, train_perplexity=197.12439, train_loss=5.283835

Batch 143660, train_perplexity=214.811, train_loss=5.3697586

Batch 143670, train_perplexity=220.37096, train_loss=5.3953123

Batch 143680, train_perplexity=212.96643, train_loss=5.3611345

Batch 143690, train_perplexity=194.92056, train_loss=5.272592

Batch 143700, train_perplexity=198.18233, train_loss=5.2891874

Batch 143710, train_perplexity=210.44977, train_loss=5.349247

Batch 143720, train_perplexity=215.50352, train_loss=5.3729773

Batch 143730, train_perplexity=188.67366, train_loss=5.240019

Batch 143740, train_perplexity=232.73259, train_loss=5.44989

Batch 143750, train_perplexity=233.59825, train_loss=5.453603

Batch 143760, train_perplexity=236.85367, train_loss=5.4674425

Batch 143770, train_perplexity=225.90477, train_loss=5.4201136

Batch 143780, train_perplexity=215.81717, train_loss=5.3744316

Batch 143790, train_perplexity=193.81921, train_loss=5.266926

Batch 143800, train_perplexity=218.57742, train_loss=5.3871403

Batch 143810, train_perplexity=191.59367, train_loss=5.255377

Batch 143820, train_perplexity=217.56566, train_loss=5.3825006

Batch 143830, train_perplexity=182.76653, train_loss=5.2082095

Batch 143840, train_perplexity=183.32295, train_loss=5.2112494

Batch 143850, train_perplexity=210.81636, train_loss=5.3509874

Batch 143860, train_perplexity=224.34074, train_loss=5.413166

Batch 143870, train_perplexity=203.40555, train_loss=5.3152018

Batch 143880, train_perplexity=201.87018, train_loss=5.307625

Batch 143890, train_perplexity=186.93369, train_loss=5.230754

Batch 143900, train_perplexity=228.45642, train_loss=5.4313455

Batch 143910, train_perplexity=195.85373, train_loss=5.277368

Batch 143920, train_perplexity=246.1921, train_loss=5.506112

Batch 143930, train_perplexity=202.48293, train_loss=5.3106556

Batch 143940, train_perplexity=194.91507, train_loss=5.272564

Batch 143950, train_perplexity=224.25613, train_loss=5.412789

Batch 143960, train_perplexity=197.04666, train_loss=5.2834406

Batch 143970, train_perplexity=213.03763, train_loss=5.361469

Batch 143980, train_perplexity=215.8252, train_loss=5.374469

Batch 143990, train_perplexity=206.46445, train_loss=5.330128

Batch 144000, train_perplexity=241.07104, train_loss=5.4850917

Batch 144010, train_perplexity=223.39175, train_loss=5.408927

Batch 144020, train_perplexity=216.2426, train_loss=5.376401

Batch 144030, train_perplexity=196.02377, train_loss=5.278236
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 144040, train_perplexity=201.83456, train_loss=5.3074484

Batch 144050, train_perplexity=193.99979, train_loss=5.267857

Batch 144060, train_perplexity=199.86873, train_loss=5.297661

Batch 144070, train_perplexity=196.32704, train_loss=5.279782

Batch 144080, train_perplexity=205.39667, train_loss=5.324943

Batch 144090, train_perplexity=211.99902, train_loss=5.3565817

Batch 144100, train_perplexity=200.72162, train_loss=5.301919

Batch 144110, train_perplexity=228.62282, train_loss=5.4320736

Batch 144120, train_perplexity=209.60431, train_loss=5.3452215

Batch 144130, train_perplexity=207.20029, train_loss=5.333686

Batch 144140, train_perplexity=214.56818, train_loss=5.3686275

Batch 144150, train_perplexity=222.0625, train_loss=5.402959

Batch 144160, train_perplexity=201.95645, train_loss=5.308052

Batch 144170, train_perplexity=206.9634, train_loss=5.332542

Batch 144180, train_perplexity=227.87196, train_loss=5.428784

Batch 144190, train_perplexity=214.27567, train_loss=5.3672633

Batch 144200, train_perplexity=205.43712, train_loss=5.32514

Batch 144210, train_perplexity=219.49193, train_loss=5.3913155

Batch 144220, train_perplexity=205.0046, train_loss=5.3230324

Batch 144230, train_perplexity=202.15955, train_loss=5.309057

Batch 144240, train_perplexity=202.02011, train_loss=5.3083673

Batch 144250, train_perplexity=211.24706, train_loss=5.3530283

Batch 144260, train_perplexity=196.48184, train_loss=5.28057

Batch 144270, train_perplexity=196.19574, train_loss=5.279113

Batch 144280, train_perplexity=229.86613, train_loss=5.437497

Batch 144290, train_perplexity=201.62064, train_loss=5.306388

Batch 144300, train_perplexity=223.5178, train_loss=5.409491

Batch 144310, train_perplexity=239.31743, train_loss=5.477791

Batch 144320, train_perplexity=206.13853, train_loss=5.3285484

Batch 144330, train_perplexity=211.83159, train_loss=5.3557916

Batch 144340, train_perplexity=203.5672, train_loss=5.315996

Batch 144350, train_perplexity=220.59406, train_loss=5.396324

Batch 144360, train_perplexity=226.7645, train_loss=5.423912

Batch 144370, train_perplexity=205.24149, train_loss=5.3241873

Batch 144380, train_perplexity=202.63863, train_loss=5.3114243

Batch 144390, train_perplexity=195.09082, train_loss=5.273465

Batch 144400, train_perplexity=197.13539, train_loss=5.2838907

Batch 144410, train_perplexity=190.92517, train_loss=5.2518816

Batch 144420, train_perplexity=203.48859, train_loss=5.31561

Batch 144430, train_perplexity=233.75749, train_loss=5.454284

Batch 144440, train_perplexity=180.60011, train_loss=5.1962852

Batch 144450, train_perplexity=199.20775, train_loss=5.2943482

Batch 144460, train_perplexity=225.10863, train_loss=5.416583

Batch 144470, train_perplexity=180.85045, train_loss=5.1976705

Batch 144480, train_perplexity=206.7254, train_loss=5.3313913

Batch 144490, train_perplexity=238.81221, train_loss=5.4756775

Batch 144500, train_perplexity=214.47182, train_loss=5.3681784

Batch 144510, train_perplexity=207.31917, train_loss=5.3342595

Batch 144520, train_perplexity=207.79968, train_loss=5.3365746

Batch 144530, train_perplexity=203.71208, train_loss=5.3167076

Batch 144540, train_perplexity=221.11978, train_loss=5.3987045

Batch 144550, train_perplexity=211.23517, train_loss=5.352972

Batch 144560, train_perplexity=200.86026, train_loss=5.3026094

Batch 144570, train_perplexity=209.2487, train_loss=5.3435235

Batch 144580, train_perplexity=209.6592, train_loss=5.3454833

Batch 144590, train_perplexity=200.12088, train_loss=5.2989216

Batch 144600, train_perplexity=202.03351, train_loss=5.3084335

Batch 144610, train_perplexity=208.73538, train_loss=5.3410673

Batch 144620, train_perplexity=213.6041, train_loss=5.3641243

Batch 144630, train_perplexity=198.99518, train_loss=5.2932806

Batch 144640, train_perplexity=204.86153, train_loss=5.3223343

Batch 144650, train_perplexity=198.35524, train_loss=5.2900596

Batch 144660, train_perplexity=200.10713, train_loss=5.298853

Batch 144670, train_perplexity=213.06882, train_loss=5.361615

Batch 144680, train_perplexity=214.65536, train_loss=5.369034

Batch 144690, train_perplexity=218.7114, train_loss=5.387753

Batch 144700, train_perplexity=238.3385, train_loss=5.473692

Batch 144710, train_perplexity=198.71735, train_loss=5.2918835

Batch 144720, train_perplexity=203.95604, train_loss=5.3179045

Batch 144730, train_perplexity=186.26492, train_loss=5.22717

Batch 144740, train_perplexity=223.7555, train_loss=5.410554

Batch 144750, train_perplexity=189.84016, train_loss=5.2461824

Batch 144760, train_perplexity=216.35925, train_loss=5.3769403

Batch 144770, train_perplexity=225.17905, train_loss=5.416896

Batch 144780, train_perplexity=217.72256, train_loss=5.3832216

Batch 144790, train_perplexity=194.28862, train_loss=5.269345

Batch 144800, train_perplexity=193.36595, train_loss=5.2645845

Batch 144810, train_perplexity=218.90692, train_loss=5.3886466

Batch 144820, train_perplexity=188.47897, train_loss=5.2389865

Batch 144830, train_perplexity=205.20656, train_loss=5.324017

Batch 144840, train_perplexity=205.36131, train_loss=5.324771

Batch 144850, train_perplexity=195.76213, train_loss=5.2769003

Batch 144860, train_perplexity=202.76071, train_loss=5.3120265

Batch 144870, train_perplexity=206.97958, train_loss=5.33262

Batch 144880, train_perplexity=202.75752, train_loss=5.312011

Batch 144890, train_perplexity=221.09015, train_loss=5.3985705

Batch 144900, train_perplexity=253.02512, train_loss=5.5334888

Batch 144910, train_perplexity=203.73618, train_loss=5.316826

Batch 144920, train_perplexity=207.85399, train_loss=5.336836

Batch 144930, train_perplexity=216.86444, train_loss=5.3792725

Batch 144940, train_perplexity=202.7402, train_loss=5.3119254

Batch 144950, train_perplexity=189.16417, train_loss=5.242615

Batch 144960, train_perplexity=217.90515, train_loss=5.38406

Batch 144970, train_perplexity=199.70317, train_loss=5.296832

Batch 144980, train_perplexity=229.66411, train_loss=5.436618

Batch 144990, train_perplexity=227.18628, train_loss=5.4257703

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00022-of-00100
Loaded 306084 sentences.
Finished loading
Batch 145000, train_perplexity=210.60315, train_loss=5.3499756

Batch 145010, train_perplexity=211.65721, train_loss=5.354968

Batch 145020, train_perplexity=213.79955, train_loss=5.365039

Batch 145030, train_perplexity=194.26509, train_loss=5.2692237

Batch 145040, train_perplexity=220.75548, train_loss=5.3970556

Batch 145050, train_perplexity=212.25008, train_loss=5.357765

Batch 145060, train_perplexity=203.64934, train_loss=5.3163996

Batch 145070, train_perplexity=225.30708, train_loss=5.4174643

Batch 145080, train_perplexity=215.27408, train_loss=5.371912

Batch 145090, train_perplexity=214.02303, train_loss=5.3660836

Batch 145100, train_perplexity=231.58594, train_loss=5.444951

Batch 145110, train_perplexity=206.6635, train_loss=5.331092

Batch 145120, train_perplexity=204.18549, train_loss=5.319029

Batch 145130, train_perplexity=234.47932, train_loss=5.4573674

Batch 145140, train_perplexity=198.37898, train_loss=5.2901793

Batch 145150, train_perplexity=183.1028, train_loss=5.2100477

Batch 145160, train_perplexity=221.00993, train_loss=5.3982077

Batch 145170, train_perplexity=209.84132, train_loss=5.3463516

Batch 145180, train_perplexity=227.2633, train_loss=5.4261093

Batch 145190, train_perplexity=212.2598, train_loss=5.357811

Batch 145200, train_perplexity=217.01154, train_loss=5.3799505

Batch 145210, train_perplexity=189.26819, train_loss=5.243165

Batch 145220, train_perplexity=232.90977, train_loss=5.450651

Batch 145230, train_perplexity=195.95415, train_loss=5.2778807

Batch 145240, train_perplexity=195.50858, train_loss=5.2756042

Batch 145250, train_perplexity=201.51692, train_loss=5.3058734

Batch 145260, train_perplexity=211.46898, train_loss=5.3540783

Batch 145270, train_perplexity=212.19685, train_loss=5.3575144
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 145280, train_perplexity=219.59138, train_loss=5.3917685

Batch 145290, train_perplexity=220.84581, train_loss=5.3974648

Batch 145300, train_perplexity=230.79102, train_loss=5.4415126

Batch 145310, train_perplexity=220.18063, train_loss=5.3944483

Batch 145320, train_perplexity=198.58218, train_loss=5.291203

Batch 145330, train_perplexity=232.83849, train_loss=5.450345

Batch 145340, train_perplexity=181.53413, train_loss=5.2014437

Batch 145350, train_perplexity=190.79276, train_loss=5.251188

Batch 145360, train_perplexity=193.996, train_loss=5.2678375

Batch 145370, train_perplexity=205.02629, train_loss=5.323138

Batch 145380, train_perplexity=181.4964, train_loss=5.201236

Batch 145390, train_perplexity=214.94502, train_loss=5.3703823

Batch 145400, train_perplexity=225.74649, train_loss=5.4194126

Batch 145410, train_perplexity=226.45837, train_loss=5.422561

Batch 145420, train_perplexity=219.06448, train_loss=5.389366

Batch 145430, train_perplexity=216.70648, train_loss=5.378544

Batch 145440, train_perplexity=228.24942, train_loss=5.430439

Batch 145450, train_perplexity=189.39684, train_loss=5.2438445

Batch 145460, train_perplexity=229.68896, train_loss=5.436726

Batch 145470, train_perplexity=193.53218, train_loss=5.265444

Batch 145480, train_perplexity=210.61761, train_loss=5.3500443

Batch 145490, train_perplexity=197.11603, train_loss=5.2837925

Batch 145500, train_perplexity=196.30391, train_loss=5.279664

Batch 145510, train_perplexity=210.80067, train_loss=5.350913

Batch 145520, train_perplexity=205.77458, train_loss=5.3267813

Batch 145530, train_perplexity=197.63893, train_loss=5.286442

Batch 145540, train_perplexity=199.17128, train_loss=5.294165

Batch 145550, train_perplexity=213.325, train_loss=5.362817

Batch 145560, train_perplexity=210.51842, train_loss=5.349573

Batch 145570, train_perplexity=189.65088, train_loss=5.245185

Batch 145580, train_perplexity=223.56982, train_loss=5.4097238

Batch 145590, train_perplexity=190.72217, train_loss=5.250818

Batch 145600, train_perplexity=208.49623, train_loss=5.339921

Batch 145610, train_perplexity=199.93965, train_loss=5.2980156

Batch 145620, train_perplexity=231.86351, train_loss=5.446149

Batch 145630, train_perplexity=218.34148, train_loss=5.38606

Batch 145640, train_perplexity=198.48732, train_loss=5.290725

Batch 145650, train_perplexity=220.81317, train_loss=5.397317

Batch 145660, train_perplexity=218.87132, train_loss=5.388484

Batch 145670, train_perplexity=237.49864, train_loss=5.470162

Batch 145680, train_perplexity=196.82326, train_loss=5.282306

Batch 145690, train_perplexity=227.45, train_loss=5.4269304

Batch 145700, train_perplexity=207.46524, train_loss=5.334964

Batch 145710, train_perplexity=192.15433, train_loss=5.258299

Batch 145720, train_perplexity=218.7021, train_loss=5.3877106

Batch 145730, train_perplexity=234.0487, train_loss=5.455529

Batch 145740, train_perplexity=228.37103, train_loss=5.4309716

Batch 145750, train_perplexity=215.81943, train_loss=5.374442

Batch 145760, train_perplexity=194.2274, train_loss=5.2690296

Batch 145770, train_perplexity=213.6534, train_loss=5.364355

Batch 145780, train_perplexity=213.23917, train_loss=5.3624144

Batch 145790, train_perplexity=221.86743, train_loss=5.40208

Batch 145800, train_perplexity=218.4251, train_loss=5.386443

Batch 145810, train_perplexity=229.09753, train_loss=5.434148

Batch 145820, train_perplexity=203.06558, train_loss=5.313529

Batch 145830, train_perplexity=221.41013, train_loss=5.400017

Batch 145840, train_perplexity=215.7865, train_loss=5.3742895

Batch 145850, train_perplexity=174.96068, train_loss=5.1645613

Batch 145860, train_perplexity=193.0228, train_loss=5.2628083

Batch 145870, train_perplexity=191.94737, train_loss=5.257221

Batch 145880, train_perplexity=184.1825, train_loss=5.215927

Batch 145890, train_perplexity=212.63219, train_loss=5.359564

Batch 145900, train_perplexity=171.01631, train_loss=5.141759

Batch 145910, train_perplexity=214.17096, train_loss=5.3667746

Batch 145920, train_perplexity=196.5107, train_loss=5.280717

Batch 145930, train_perplexity=208.48848, train_loss=5.339884

Batch 145940, train_perplexity=216.83528, train_loss=5.379138

Batch 145950, train_perplexity=202.46545, train_loss=5.3105693

Batch 145960, train_perplexity=218.8346, train_loss=5.388316

Batch 145970, train_perplexity=188.99745, train_loss=5.2417336

Batch 145980, train_perplexity=228.42145, train_loss=5.4311924

Batch 145990, train_perplexity=238.1128, train_loss=5.4727445

Batch 146000, train_perplexity=217.75684, train_loss=5.383379

Batch 146010, train_perplexity=192.73328, train_loss=5.2613072

Batch 146020, train_perplexity=223.4595, train_loss=5.40923

Batch 146030, train_perplexity=209.2486, train_loss=5.343523

Batch 146040, train_perplexity=225.92104, train_loss=5.4201856

Batch 146050, train_perplexity=229.60081, train_loss=5.4363422

Batch 146060, train_perplexity=193.46011, train_loss=5.2650714

Batch 146070, train_perplexity=200.767, train_loss=5.302145

Batch 146080, train_perplexity=187.56691, train_loss=5.2341356

Batch 146090, train_perplexity=201.34106, train_loss=5.3050003

Batch 146100, train_perplexity=191.96349, train_loss=5.257305

Batch 146110, train_perplexity=213.33578, train_loss=5.3628674

Batch 146120, train_perplexity=232.90079, train_loss=5.4506125

Batch 146130, train_perplexity=228.09406, train_loss=5.429758

Batch 146140, train_perplexity=213.19514, train_loss=5.362208

Batch 146150, train_perplexity=192.52927, train_loss=5.260248

Batch 146160, train_perplexity=207.63962, train_loss=5.335804

Batch 146170, train_perplexity=210.15935, train_loss=5.347866

Batch 146180, train_perplexity=217.17105, train_loss=5.3806853

Batch 146190, train_perplexity=221.84152, train_loss=5.401963

Batch 146200, train_perplexity=208.0172, train_loss=5.3376207

Batch 146210, train_perplexity=215.51688, train_loss=5.3730392

Batch 146220, train_perplexity=215.33733, train_loss=5.3722057

Batch 146230, train_perplexity=209.1313, train_loss=5.3429623

Batch 146240, train_perplexity=193.74251, train_loss=5.26653

Batch 146250, train_perplexity=190.986, train_loss=5.2522

Batch 146260, train_perplexity=193.29718, train_loss=5.264229

Batch 146270, train_perplexity=196.7817, train_loss=5.282095

Batch 146280, train_perplexity=197.37495, train_loss=5.285105

Batch 146290, train_perplexity=235.13948, train_loss=5.460179

Batch 146300, train_perplexity=241.10956, train_loss=5.4852514

Batch 146310, train_perplexity=194.56017, train_loss=5.2707415

Batch 146320, train_perplexity=191.10406, train_loss=5.252818

Batch 146330, train_perplexity=195.76465, train_loss=5.276913

Batch 146340, train_perplexity=205.4042, train_loss=5.32498

Batch 146350, train_perplexity=215.14395, train_loss=5.3713074

Batch 146360, train_perplexity=184.08398, train_loss=5.215392

Batch 146370, train_perplexity=200.57161, train_loss=5.3011713

Batch 146380, train_perplexity=220.5665, train_loss=5.396199

Batch 146390, train_perplexity=183.2235, train_loss=5.2107067

Batch 146400, train_perplexity=229.88828, train_loss=5.4375935

Batch 146410, train_perplexity=209.65729, train_loss=5.3454742

Batch 146420, train_perplexity=220.81126, train_loss=5.3973083

Batch 146430, train_perplexity=195.41649, train_loss=5.275133

Batch 146440, train_perplexity=182.62592, train_loss=5.20744

Batch 146450, train_perplexity=198.19858, train_loss=5.2892694

Batch 146460, train_perplexity=202.91179, train_loss=5.3127713

Batch 146470, train_perplexity=180.49034, train_loss=5.1956773

Batch 146480, train_perplexity=230.0734, train_loss=5.4383984

Batch 146490, train_perplexity=180.42245, train_loss=5.195301

Batch 146500, train_perplexity=189.52106, train_loss=5.2445

Batch 146510, train_perplexity=217.043, train_loss=5.3800955

Batch 146520, train_perplexity=228.54652, train_loss=5.43174

Batch 146530, train_perplexity=200.54951, train_loss=5.301061

Batch 146540, train_perplexity=220.53894, train_loss=5.3960743

Batch 146550, train_perplexity=222.6671, train_loss=5.405678

Batch 146560, train_perplexity=198.45392, train_loss=5.290557

Batch 146570, train_perplexity=218.15697, train_loss=5.385215
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 146580, train_perplexity=211.77856, train_loss=5.355541

Batch 146590, train_perplexity=228.41307, train_loss=5.4311557

Batch 146600, train_perplexity=207.91664, train_loss=5.337137

Batch 146610, train_perplexity=195.72862, train_loss=5.276729

Batch 146620, train_perplexity=214.40413, train_loss=5.3678627

Batch 146630, train_perplexity=223.8888, train_loss=5.4111495

Batch 146640, train_perplexity=202.7028, train_loss=5.311741

Batch 146650, train_perplexity=203.73985, train_loss=5.316844

Batch 146660, train_perplexity=200.6135, train_loss=5.30138

Batch 146670, train_perplexity=207.21638, train_loss=5.3337636

Batch 146680, train_perplexity=208.59569, train_loss=5.340398

Batch 146690, train_perplexity=214.52092, train_loss=5.3684072

Batch 146700, train_perplexity=204.67894, train_loss=5.3214426

Batch 146710, train_perplexity=208.44803, train_loss=5.3396897

Batch 146720, train_perplexity=229.63367, train_loss=5.4364853

Batch 146730, train_perplexity=196.7678, train_loss=5.2820244

Batch 146740, train_perplexity=212.97029, train_loss=5.3611526

Batch 146750, train_perplexity=216.02432, train_loss=5.375391

Batch 146760, train_perplexity=213.08812, train_loss=5.361706

Batch 146770, train_perplexity=202.00894, train_loss=5.308312

Batch 146780, train_perplexity=229.88039, train_loss=5.437559

Batch 146790, train_perplexity=200.83018, train_loss=5.3024597

Batch 146800, train_perplexity=199.68031, train_loss=5.2967176

Batch 146810, train_perplexity=208.76912, train_loss=5.341229

Batch 146820, train_perplexity=185.40503, train_loss=5.222543

Batch 146830, train_perplexity=217.56316, train_loss=5.382489

Batch 146840, train_perplexity=223.04263, train_loss=5.407363

Batch 146850, train_perplexity=192.72969, train_loss=5.2612886

Batch 146860, train_perplexity=197.46335, train_loss=5.285553

Batch 146870, train_perplexity=189.96783, train_loss=5.246855

Batch 146880, train_perplexity=209.30458, train_loss=5.3437905

Batch 146890, train_perplexity=205.76515, train_loss=5.3267355

Batch 146900, train_perplexity=209.74628, train_loss=5.3458986

Batch 146910, train_perplexity=206.07819, train_loss=5.3282557

Batch 146920, train_perplexity=182.45775, train_loss=5.2065187

Batch 146930, train_perplexity=222.81494, train_loss=5.4063416

Batch 146940, train_perplexity=210.70018, train_loss=5.350436

Batch 146950, train_perplexity=203.34639, train_loss=5.314911

Batch 146960, train_perplexity=182.52258, train_loss=5.206874

Batch 146970, train_perplexity=209.33722, train_loss=5.3439465

Batch 146980, train_perplexity=189.84903, train_loss=5.246229

Batch 146990, train_perplexity=218.2173, train_loss=5.3854914

Batch 147000, train_perplexity=211.94626, train_loss=5.356333

Batch 147010, train_perplexity=199.35399, train_loss=5.295082

Batch 147020, train_perplexity=196.11324, train_loss=5.2786922

Batch 147030, train_perplexity=206.88022, train_loss=5.33214

Batch 147040, train_perplexity=200.18407, train_loss=5.2992373

Batch 147050, train_perplexity=212.3746, train_loss=5.3583517

Batch 147060, train_perplexity=218.76448, train_loss=5.3879957

Batch 147070, train_perplexity=211.73172, train_loss=5.35532

Batch 147080, train_perplexity=217.37454, train_loss=5.381622

Batch 147090, train_perplexity=198.21106, train_loss=5.2893324

Batch 147100, train_perplexity=196.44269, train_loss=5.2803707

Batch 147110, train_perplexity=211.13387, train_loss=5.3524923

Batch 147120, train_perplexity=193.34438, train_loss=5.264473

Batch 147130, train_perplexity=198.67226, train_loss=5.2916565

Batch 147140, train_perplexity=231.23439, train_loss=5.443432

Batch 147150, train_perplexity=189.4309, train_loss=5.2440243

Batch 147160, train_perplexity=197.35199, train_loss=5.284989

Batch 147170, train_perplexity=217.51192, train_loss=5.3822536

Batch 147180, train_perplexity=201.15048, train_loss=5.3040533

Batch 147190, train_perplexity=198.85556, train_loss=5.2925787

Batch 147200, train_perplexity=210.56339, train_loss=5.3497868

Batch 147210, train_perplexity=199.36578, train_loss=5.295141

Batch 147220, train_perplexity=198.79497, train_loss=5.292274

Batch 147230, train_perplexity=220.20877, train_loss=5.394576

Batch 147240, train_perplexity=216.93974, train_loss=5.3796196

Batch 147250, train_perplexity=207.08295, train_loss=5.3331194

Batch 147260, train_perplexity=240.78648, train_loss=5.4839106

Batch 147270, train_perplexity=202.01683, train_loss=5.308351

Batch 147280, train_perplexity=218.04027, train_loss=5.38468

Batch 147290, train_perplexity=210.55235, train_loss=5.3497343

Batch 147300, train_perplexity=200.77206, train_loss=5.3021703

Batch 147310, train_perplexity=226.2683, train_loss=5.4217215

Batch 147320, train_perplexity=190.026, train_loss=5.247161

Batch 147330, train_perplexity=193.22392, train_loss=5.2638497

Batch 147340, train_perplexity=215.50383, train_loss=5.3729787

Batch 147350, train_perplexity=235.12782, train_loss=5.4601293

Batch 147360, train_perplexity=203.6502, train_loss=5.316404

Batch 147370, train_perplexity=196.16234, train_loss=5.2789426

Batch 147380, train_perplexity=188.26332, train_loss=5.2378416

Batch 147390, train_perplexity=192.68329, train_loss=5.261048

Batch 147400, train_perplexity=208.82837, train_loss=5.3415127

Batch 147410, train_perplexity=228.29797, train_loss=5.4306517

Batch 147420, train_perplexity=192.7023, train_loss=5.2611465

Batch 147430, train_perplexity=213.37993, train_loss=5.3630743

Batch 147440, train_perplexity=228.71136, train_loss=5.432461

Batch 147450, train_perplexity=200.23695, train_loss=5.2995014

Batch 147460, train_perplexity=208.39903, train_loss=5.3394547

Batch 147470, train_perplexity=217.38708, train_loss=5.3816795

Batch 147480, train_perplexity=199.50977, train_loss=5.295863

Batch 147490, train_perplexity=227.267, train_loss=5.4261255

Batch 147500, train_perplexity=201.48206, train_loss=5.3057003

Batch 147510, train_perplexity=222.67389, train_loss=5.4057083

Batch 147520, train_perplexity=179.12018, train_loss=5.188057

Batch 147530, train_perplexity=218.33304, train_loss=5.3860216

Batch 147540, train_perplexity=208.16653, train_loss=5.3383384

Batch 147550, train_perplexity=219.22583, train_loss=5.3901024

Batch 147560, train_perplexity=218.11266, train_loss=5.3850117

Batch 147570, train_perplexity=214.54517, train_loss=5.3685203

Batch 147580, train_perplexity=193.16846, train_loss=5.2635627

Batch 147590, train_perplexity=220.0698, train_loss=5.3939447

Batch 147600, train_perplexity=195.94461, train_loss=5.277832

Batch 147610, train_perplexity=227.15909, train_loss=5.4256506

Batch 147620, train_perplexity=200.896, train_loss=5.3027873

Batch 147630, train_perplexity=205.07645, train_loss=5.323383

Batch 147640, train_perplexity=207.87976, train_loss=5.33696

Batch 147650, train_perplexity=188.93222, train_loss=5.2413883

Batch 147660, train_perplexity=217.60518, train_loss=5.3826823

Batch 147670, train_perplexity=193.51059, train_loss=5.265332

Batch 147680, train_perplexity=216.3511, train_loss=5.3769026

Batch 147690, train_perplexity=195.77249, train_loss=5.276953

Batch 147700, train_perplexity=204.21393, train_loss=5.319168

Batch 147710, train_perplexity=250.08098, train_loss=5.521785

Batch 147720, train_perplexity=210.57303, train_loss=5.3498325

Batch 147730, train_perplexity=210.97606, train_loss=5.3517447

Batch 147740, train_perplexity=199.41711, train_loss=5.2953987

Batch 147750, train_perplexity=221.78864, train_loss=5.401725

Batch 147760, train_perplexity=196.83951, train_loss=5.2823887

Batch 147770, train_perplexity=179.44537, train_loss=5.189871

Batch 147780, train_perplexity=214.2488, train_loss=5.367138

Batch 147790, train_perplexity=204.26291, train_loss=5.319408

Batch 147800, train_perplexity=185.26901, train_loss=5.221809

Batch 147810, train_perplexity=206.767, train_loss=5.3315926

Batch 147820, train_perplexity=212.05292, train_loss=5.356836

Batch 147830, train_perplexity=204.74278, train_loss=5.3217545

Batch 147840, train_perplexity=210.49754, train_loss=5.349474

Batch 147850, train_perplexity=198.18062, train_loss=5.289179

Batch 147860, train_perplexity=198.63521, train_loss=5.29147

Batch 147870, train_perplexity=221.08044, train_loss=5.3985267
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 147880, train_perplexity=215.29492, train_loss=5.372009

Batch 147890, train_perplexity=200.5718, train_loss=5.3011723

Batch 147900, train_perplexity=226.34729, train_loss=5.4220705

Batch 147910, train_perplexity=197.73471, train_loss=5.2869263

Batch 147920, train_perplexity=186.07193, train_loss=5.2261333

Batch 147930, train_perplexity=218.0411, train_loss=5.3846836

Batch 147940, train_perplexity=206.46, train_loss=5.3301067

Batch 147950, train_perplexity=181.55959, train_loss=5.201584

Batch 147960, train_perplexity=184.08759, train_loss=5.2154117

Batch 147970, train_perplexity=210.2283, train_loss=5.348194

Batch 147980, train_perplexity=189.36208, train_loss=5.243661

Batch 147990, train_perplexity=190.13513, train_loss=5.247735

Batch 148000, train_perplexity=217.44783, train_loss=5.381959

Batch 148010, train_perplexity=195.85223, train_loss=5.2773604

Batch 148020, train_perplexity=205.5898, train_loss=5.325883

Batch 148030, train_perplexity=195.8796, train_loss=5.2775

Batch 148040, train_perplexity=211.76514, train_loss=5.355478

Batch 148050, train_perplexity=207.52797, train_loss=5.335266

Batch 148060, train_perplexity=185.9866, train_loss=5.2256746

Batch 148070, train_perplexity=238.30122, train_loss=5.4735355

Batch 148080, train_perplexity=216.00598, train_loss=5.375306

Batch 148090, train_perplexity=210.3376, train_loss=5.348714

Batch 148100, train_perplexity=209.74408, train_loss=5.345888

Batch 148110, train_perplexity=201.32455, train_loss=5.3049183

Batch 148120, train_perplexity=193.7852, train_loss=5.2667503

Batch 148130, train_perplexity=199.43642, train_loss=5.2954955

Batch 148140, train_perplexity=206.90271, train_loss=5.3322487

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Batch 148150, train_perplexity=205.39716, train_loss=5.3249454

Batch 148160, train_perplexity=192.74568, train_loss=5.2613716

Batch 148170, train_perplexity=201.44536, train_loss=5.305518

Batch 148180, train_perplexity=209.18994, train_loss=5.3432426

Batch 148190, train_perplexity=218.04007, train_loss=5.384679

Batch 148200, train_perplexity=201.02306, train_loss=5.3034196

Batch 148210, train_perplexity=206.6297, train_loss=5.3309283

Batch 148220, train_perplexity=192.05833, train_loss=5.257799

Batch 148230, train_perplexity=203.07275, train_loss=5.3135643

Batch 148240, train_perplexity=222.64926, train_loss=5.4055977

Batch 148250, train_perplexity=184.525, train_loss=5.217785

Batch 148260, train_perplexity=198.42032, train_loss=5.2903876

Batch 148270, train_perplexity=212.12643, train_loss=5.3571825

Batch 148280, train_perplexity=213.31401, train_loss=5.3627653

Batch 148290, train_perplexity=206.8532, train_loss=5.3320093

Batch 148300, train_perplexity=232.53581, train_loss=5.449044

Batch 148310, train_perplexity=226.02136, train_loss=5.4206295

Batch 148320, train_perplexity=199.84262, train_loss=5.29753

Batch 148330, train_perplexity=200.12164, train_loss=5.2989254

Batch 148340, train_perplexity=207.68051, train_loss=5.336001

Batch 148350, train_perplexity=208.09715, train_loss=5.338005

Batch 148360, train_perplexity=184.1976, train_loss=5.216009

Batch 148370, train_perplexity=189.18663, train_loss=5.242734

Batch 148380, train_perplexity=207.1339, train_loss=5.3333654

Batch 148390, train_perplexity=213.46654, train_loss=5.36348

Batch 148400, train_perplexity=197.10089, train_loss=5.2837157

Batch 148410, train_perplexity=208.83493, train_loss=5.341544

Batch 148420, train_perplexity=234.06343, train_loss=5.455592

Batch 148430, train_perplexity=209.68109, train_loss=5.3455877

Batch 148440, train_perplexity=198.53778, train_loss=5.2909794

Batch 148450, train_perplexity=213.91946, train_loss=5.3655996

Batch 148460, train_perplexity=216.87189, train_loss=5.379307

Batch 148470, train_perplexity=211.44579, train_loss=5.3539686

Batch 148480, train_perplexity=202.88364, train_loss=5.3126326

Batch 148490, train_perplexity=240.24275, train_loss=5.48165

Batch 148500, train_perplexity=217.17178, train_loss=5.3806887

Batch 148510, train_perplexity=221.26259, train_loss=5.39935

Batch 148520, train_perplexity=191.30008, train_loss=5.2538433

Batch 148530, train_perplexity=192.64204, train_loss=5.2608337

Batch 148540, train_perplexity=225.0775, train_loss=5.416445

Batch 148550, train_perplexity=197.31444, train_loss=5.2847986

Batch 148560, train_perplexity=219.93237, train_loss=5.39332

Batch 148570, train_perplexity=193.91628, train_loss=5.2674265

Batch 148580, train_perplexity=217.33276, train_loss=5.3814297

Batch 148590, train_perplexity=241.43951, train_loss=5.486619

Batch 148600, train_perplexity=203.41331, train_loss=5.31524

Batch 148610, train_perplexity=210.54211, train_loss=5.3496857

Batch 148620, train_perplexity=189.59988, train_loss=5.244916

Batch 148630, train_perplexity=216.26663, train_loss=5.376512

Batch 148640, train_perplexity=219.60478, train_loss=5.3918295

Batch 148650, train_perplexity=208.27911, train_loss=5.338879

Batch 148660, train_perplexity=197.0978, train_loss=5.2837

Batch 148670, train_perplexity=191.73369, train_loss=5.2561073

Batch 148680, train_perplexity=195.78575, train_loss=5.277021

Batch 148690, train_perplexity=223.24586, train_loss=5.4082737

Batch 148700, train_perplexity=241.86841, train_loss=5.488394

Batch 148710, train_perplexity=209.67789, train_loss=5.3455725

Batch 148720, train_perplexity=182.52815, train_loss=5.2069044

Batch 148730, train_perplexity=202.69632, train_loss=5.311709

Batch 148740, train_perplexity=203.5738, train_loss=5.3160286

Batch 148750, train_perplexity=203.34154, train_loss=5.314887

Batch 148760, train_perplexity=216.16528, train_loss=5.3760433

Batch 148770, train_perplexity=218.96465, train_loss=5.3889103

Batch 148780, train_perplexity=220.6517, train_loss=5.3965855

Batch 148790, train_perplexity=199.90457, train_loss=5.29784

Batch 148800, train_perplexity=217.71664, train_loss=5.3831944

Batch 148810, train_perplexity=232.452, train_loss=5.4486837

Batch 148820, train_perplexity=213.45686, train_loss=5.363435

Batch 148830, train_perplexity=198.2086, train_loss=5.28932

Batch 148840, train_perplexity=210.98642, train_loss=5.351794

Batch 148850, train_perplexity=205.5204, train_loss=5.3255453

Batch 148860, train_perplexity=215.87892, train_loss=5.3747177

Batch 148870, train_perplexity=206.19319, train_loss=5.3288136

Batch 148880, train_perplexity=203.2447, train_loss=5.3144107

Batch 148890, train_perplexity=186.43065, train_loss=5.2280593

Batch 148900, train_perplexity=210.62305, train_loss=5.35007

Batch 148910, train_perplexity=217.93051, train_loss=5.3841763

Batch 148920, train_perplexity=174.44627, train_loss=5.161617

Batch 148930, train_perplexity=228.30516, train_loss=5.430683

Batch 148940, train_perplexity=226.25536, train_loss=5.421664

Batch 148950, train_perplexity=221.07445, train_loss=5.3984995

Batch 148960, train_perplexity=208.30484, train_loss=5.3390026

Batch 148970, train_perplexity=200.53412, train_loss=5.3009844

Batch 148980, train_perplexity=196.92503, train_loss=5.282823

Batch 148990, train_perplexity=193.05879, train_loss=5.262995

Batch 149000, train_perplexity=215.46222, train_loss=5.3727856

Batch 149010, train_perplexity=249.93243, train_loss=5.5211906

Batch 149020, train_perplexity=175.23445, train_loss=5.166125

Batch 149030, train_perplexity=207.47058, train_loss=5.3349895

Batch 149040, train_perplexity=205.13132, train_loss=5.3236504

Batch 149050, train_perplexity=203.70781, train_loss=5.3166866

Batch 149060, train_perplexity=210.30661, train_loss=5.3485665

Batch 149070, train_perplexity=206.24835, train_loss=5.329081

Batch 149080, train_perplexity=214.33524, train_loss=5.3675413

Batch 149090, train_perplexity=214.43828, train_loss=5.368022

Batch 149100, train_perplexity=204.49397, train_loss=5.3205385

Batch 149110, train_perplexity=228.05208, train_loss=5.429574
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 149120, train_perplexity=230.17621, train_loss=5.438845

Batch 149130, train_perplexity=182.32843, train_loss=5.2058096

Batch 149140, train_perplexity=198.29198, train_loss=5.2897406

Batch 149150, train_perplexity=220.82433, train_loss=5.3973675

Batch 149160, train_perplexity=225.06355, train_loss=5.416383

Batch 149170, train_perplexity=193.60841, train_loss=5.2658377

Batch 149180, train_perplexity=212.16063, train_loss=5.3573437

Batch 149190, train_perplexity=195.33945, train_loss=5.274739

Batch 149200, train_perplexity=228.71147, train_loss=5.4324613

Batch 149210, train_perplexity=186.6645, train_loss=5.229313

Batch 149220, train_perplexity=194.04152, train_loss=5.268072

Batch 149230, train_perplexity=208.31607, train_loss=5.3390565

Batch 149240, train_perplexity=225.87935, train_loss=5.420001

Batch 149250, train_perplexity=198.63683, train_loss=5.291478

Batch 149260, train_perplexity=217.10551, train_loss=5.3803835

Batch 149270, train_perplexity=209.77919, train_loss=5.3460555

Batch 149280, train_perplexity=202.12418, train_loss=5.308882

Batch 149290, train_perplexity=190.06361, train_loss=5.247359

Batch 149300, train_perplexity=234.3329, train_loss=5.456743

Batch 149310, train_perplexity=198.55708, train_loss=5.2910767

Batch 149320, train_perplexity=202.44334, train_loss=5.31046

Batch 149330, train_perplexity=218.7044, train_loss=5.387721

Batch 149340, train_perplexity=208.14023, train_loss=5.338212

Batch 149350, train_perplexity=189.84341, train_loss=5.2461996

Batch 149360, train_perplexity=190.74355, train_loss=5.25093

Batch 149370, train_perplexity=206.26675, train_loss=5.32917

Batch 149380, train_perplexity=217.1519, train_loss=5.380597

Batch 149390, train_perplexity=215.1853, train_loss=5.3714995

Batch 149400, train_perplexity=197.83298, train_loss=5.287423

Batch 149410, train_perplexity=198.82578, train_loss=5.292429

Batch 149420, train_perplexity=228.40512, train_loss=5.431121

Batch 149430, train_perplexity=191.04794, train_loss=5.2525244

Batch 149440, train_perplexity=197.15833, train_loss=5.284007

Batch 149450, train_perplexity=217.34312, train_loss=5.3814774

Batch 149460, train_perplexity=210.77414, train_loss=5.350787

Batch 149470, train_perplexity=211.88756, train_loss=5.3560557

Batch 149480, train_perplexity=225.26143, train_loss=5.4172616

Batch 149490, train_perplexity=204.64302, train_loss=5.321267

Batch 149500, train_perplexity=217.76088, train_loss=5.3833976

Batch 149510, train_perplexity=197.8111, train_loss=5.2873125

Batch 149520, train_perplexity=205.7522, train_loss=5.3266726

Batch 149530, train_perplexity=234.48749, train_loss=5.457402

Batch 149540, train_perplexity=209.29211, train_loss=5.343731

Batch 149550, train_perplexity=220.98254, train_loss=5.3980837

Batch 149560, train_perplexity=222.96075, train_loss=5.406996

Batch 149570, train_perplexity=199.10149, train_loss=5.2938147

Batch 149580, train_perplexity=214.84358, train_loss=5.3699102

Batch 149590, train_perplexity=197.52089, train_loss=5.2858443

Batch 149600, train_perplexity=214.62057, train_loss=5.3688717

Batch 149610, train_perplexity=216.20003, train_loss=5.376204

Batch 149620, train_perplexity=187.98183, train_loss=5.2363453

Batch 149630, train_perplexity=192.80075, train_loss=5.261657

Batch 149640, train_perplexity=218.87842, train_loss=5.3885164

Batch 149650, train_perplexity=221.82628, train_loss=5.4018946

Batch 149660, train_perplexity=201.09142, train_loss=5.3037596

Batch 149670, train_perplexity=210.74782, train_loss=5.350662

Batch 149680, train_perplexity=223.28484, train_loss=5.408448

Batch 149690, train_perplexity=209.26607, train_loss=5.3436065

Batch 149700, train_perplexity=227.20956, train_loss=5.425873

Batch 149710, train_perplexity=180.89107, train_loss=5.197895

Batch 149720, train_perplexity=205.59567, train_loss=5.3259115

Batch 149730, train_perplexity=213.696, train_loss=5.3645544

Batch 149740, train_perplexity=202.83469, train_loss=5.3123913

Batch 149750, train_perplexity=198.10258, train_loss=5.288785

Batch 149760, train_perplexity=193.84361, train_loss=5.2670517

Batch 149770, train_perplexity=181.47354, train_loss=5.20111

Batch 149780, train_perplexity=221.33414, train_loss=5.3996735

Batch 149790, train_perplexity=209.69748, train_loss=5.345666

Batch 149800, train_perplexity=200.41138, train_loss=5.300372

Batch 149810, train_perplexity=197.6394, train_loss=5.286444

Batch 149820, train_perplexity=179.11522, train_loss=5.1880293

Batch 149830, train_perplexity=222.85904, train_loss=5.4065394

Batch 149840, train_perplexity=214.05988, train_loss=5.3662558

Batch 149850, train_perplexity=200.16135, train_loss=5.299124

Batch 149860, train_perplexity=223.61758, train_loss=5.4099374

Batch 149870, train_perplexity=204.62263, train_loss=5.3211675

Batch 149880, train_perplexity=221.74834, train_loss=5.401543

Batch 149890, train_perplexity=191.04703, train_loss=5.2525196

Batch 149900, train_perplexity=212.23976, train_loss=5.3577166

Batch 149910, train_perplexity=226.10835, train_loss=5.4210143

Batch 149920, train_perplexity=201.22704, train_loss=5.304434

Batch 149930, train_perplexity=217.4524, train_loss=5.38198

Batch 149940, train_perplexity=200.88948, train_loss=5.302755

Batch 149950, train_perplexity=212.07405, train_loss=5.3569355

Batch 149960, train_perplexity=233.86975, train_loss=5.4547644

Batch 149970, train_perplexity=183.42009, train_loss=5.211779

Batch 149980, train_perplexity=196.20416, train_loss=5.2791557

Batch 149990, train_perplexity=218.50719, train_loss=5.386819

Batch 150000, train_perplexity=225.37392, train_loss=5.417761

Batch 150010, train_perplexity=205.16527, train_loss=5.323816

Batch 150020, train_perplexity=198.21048, train_loss=5.2893295

Batch 150030, train_perplexity=198.00777, train_loss=5.288306

Batch 150040, train_perplexity=197.38832, train_loss=5.285173

Batch 150050, train_perplexity=231.19713, train_loss=5.4432707

Batch 150060, train_perplexity=236.82475, train_loss=5.4673204

Batch 150070, train_perplexity=218.20503, train_loss=5.385435

Batch 150080, train_perplexity=200.70267, train_loss=5.3018246

Batch 150090, train_perplexity=208.42218, train_loss=5.3395658

Batch 150100, train_perplexity=190.92253, train_loss=5.251868

Batch 150110, train_perplexity=218.07251, train_loss=5.3848276

Batch 150120, train_perplexity=206.92184, train_loss=5.332341

Batch 150130, train_perplexity=210.37492, train_loss=5.3488913

Batch 150140, train_perplexity=210.17117, train_loss=5.3479223

Batch 150150, train_perplexity=201.76933, train_loss=5.307125

Batch 150160, train_perplexity=209.69948, train_loss=5.3456755

Batch 150170, train_perplexity=208.83992, train_loss=5.341568

Batch 150180, train_perplexity=191.30501, train_loss=5.253869

Batch 150190, train_perplexity=208.40201, train_loss=5.339469

Batch 150200, train_perplexity=188.5037, train_loss=5.2391176

Batch 150210, train_perplexity=220.10191, train_loss=5.3940907

Batch 150220, train_perplexity=199.27397, train_loss=5.2946806

Batch 150230, train_perplexity=230.66483, train_loss=5.4409657

Batch 150240, train_perplexity=199.97789, train_loss=5.298207

Batch 150250, train_perplexity=216.40889, train_loss=5.3771696

Batch 150260, train_perplexity=208.94151, train_loss=5.3420544

Batch 150270, train_perplexity=184.49297, train_loss=5.2176113

Batch 150280, train_perplexity=201.90425, train_loss=5.3077936

Batch 150290, train_perplexity=226.15677, train_loss=5.4212284

Batch 150300, train_perplexity=196.99782, train_loss=5.2831926

Batch 150310, train_perplexity=195.31914, train_loss=5.274635

Batch 150320, train_perplexity=210.21437, train_loss=5.348128

Batch 150330, train_perplexity=233.88538, train_loss=5.454831

Batch 150340, train_perplexity=218.00545, train_loss=5.38452

Batch 150350, train_perplexity=229.88094, train_loss=5.4375615

Batch 150360, train_perplexity=209.17877, train_loss=5.3431892

Batch 150370, train_perplexity=212.86003, train_loss=5.360635

Batch 150380, train_perplexity=232.4285, train_loss=5.4485826

Batch 150390, train_perplexity=205.34515, train_loss=5.3246922

Batch 150400, train_perplexity=222.02798, train_loss=5.4028034

Batch 150410, train_perplexity=203.03673, train_loss=5.313387
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 150420, train_perplexity=221.1609, train_loss=5.3988905

Batch 150430, train_perplexity=208.51433, train_loss=5.340008

Batch 150440, train_perplexity=205.77095, train_loss=5.3267636

Batch 150450, train_perplexity=202.55168, train_loss=5.310995

Batch 150460, train_perplexity=200.68584, train_loss=5.3017406

Batch 150470, train_perplexity=198.92554, train_loss=5.2929306

Batch 150480, train_perplexity=193.34383, train_loss=5.26447

Batch 150490, train_perplexity=206.1575, train_loss=5.3286405

Batch 150500, train_perplexity=216.46617, train_loss=5.3774343

Batch 150510, train_perplexity=191.15784, train_loss=5.2530994

Batch 150520, train_perplexity=193.79333, train_loss=5.2667923

Batch 150530, train_perplexity=206.84135, train_loss=5.331952

Batch 150540, train_perplexity=189.83057, train_loss=5.246132

Batch 150550, train_perplexity=200.41893, train_loss=5.30041

Batch 150560, train_perplexity=210.64815, train_loss=5.350189

Batch 150570, train_perplexity=202.61574, train_loss=5.3113112

Batch 150580, train_perplexity=196.69745, train_loss=5.2816668

Batch 150590, train_perplexity=212.06424, train_loss=5.3568892

Batch 150600, train_perplexity=208.57101, train_loss=5.3402796

Batch 150610, train_perplexity=205.81343, train_loss=5.32697

Batch 150620, train_perplexity=218.6409, train_loss=5.3874307

Batch 150630, train_perplexity=212.36609, train_loss=5.3583117

Batch 150640, train_perplexity=213.03722, train_loss=5.361467

Batch 150650, train_perplexity=191.48242, train_loss=5.254796

Batch 150660, train_perplexity=214.40372, train_loss=5.367861

Batch 150670, train_perplexity=199.37167, train_loss=5.295171

Batch 150680, train_perplexity=217.16473, train_loss=5.3806562

Batch 150690, train_perplexity=206.40193, train_loss=5.3298254

Batch 150700, train_perplexity=216.97697, train_loss=5.3797913

Batch 150710, train_perplexity=226.483, train_loss=5.42267

Batch 150720, train_perplexity=197.03014, train_loss=5.2833567

Batch 150730, train_perplexity=191.52087, train_loss=5.254997

Batch 150740, train_perplexity=214.17014, train_loss=5.3667707

Batch 150750, train_perplexity=197.72546, train_loss=5.2868795

Batch 150760, train_perplexity=210.91129, train_loss=5.3514376

Batch 150770, train_perplexity=206.09264, train_loss=5.3283257

Batch 150780, train_perplexity=200.0978, train_loss=5.298806

Batch 150790, train_perplexity=190.12244, train_loss=5.2476683

Batch 150800, train_perplexity=223.02827, train_loss=5.4072986

Batch 150810, train_perplexity=209.95071, train_loss=5.346873

Batch 150820, train_perplexity=205.6645, train_loss=5.3262463

Batch 150830, train_perplexity=212.2007, train_loss=5.3575325

Batch 150840, train_perplexity=210.91188, train_loss=5.3514404

Batch 150850, train_perplexity=174.34074, train_loss=5.1610117

Batch 150860, train_perplexity=214.43828, train_loss=5.368022

Batch 150870, train_perplexity=196.8947, train_loss=5.282669

Batch 150880, train_perplexity=190.29785, train_loss=5.2485905

Batch 150890, train_perplexity=215.21722, train_loss=5.371648

Batch 150900, train_perplexity=206.41177, train_loss=5.329873

Batch 150910, train_perplexity=201.99092, train_loss=5.308223

Batch 150920, train_perplexity=187.2886, train_loss=5.2326508

Batch 150930, train_perplexity=203.92395, train_loss=5.317747

Batch 150940, train_perplexity=180.68849, train_loss=5.1967745

Batch 150950, train_perplexity=200.18979, train_loss=5.299266

Batch 150960, train_perplexity=208.94052, train_loss=5.3420496

Batch 150970, train_perplexity=230.01657, train_loss=5.4381514

Batch 150980, train_perplexity=189.5273, train_loss=5.244533

Batch 150990, train_perplexity=218.94951, train_loss=5.388841

Batch 151000, train_perplexity=231.8832, train_loss=5.4462337

Batch 151010, train_perplexity=203.17581, train_loss=5.3140717

Batch 151020, train_perplexity=213.47377, train_loss=5.363514

Batch 151030, train_perplexity=210.13611, train_loss=5.3477554

Batch 151040, train_perplexity=211.04347, train_loss=5.352064

Batch 151050, train_perplexity=187.85352, train_loss=5.2356625

Batch 151060, train_perplexity=194.07909, train_loss=5.2682657

Batch 151070, train_perplexity=210.83426, train_loss=5.3510723

Batch 151080, train_perplexity=183.98729, train_loss=5.2148666

Batch 151090, train_perplexity=229.37933, train_loss=5.435377

Batch 151100, train_perplexity=202.70724, train_loss=5.311763

Batch 151110, train_perplexity=205.72012, train_loss=5.3265166

Batch 151120, train_perplexity=217.92157, train_loss=5.3841352

Batch 151130, train_perplexity=209.97725, train_loss=5.346999

Batch 151140, train_perplexity=188.45984, train_loss=5.238885

Batch 151150, train_perplexity=193.39482, train_loss=5.264734

Batch 151160, train_perplexity=217.83794, train_loss=5.3837514

Batch 151170, train_perplexity=188.1122, train_loss=5.2370386

Batch 151180, train_perplexity=233.4044, train_loss=5.4527726

Batch 151190, train_perplexity=216.53987, train_loss=5.3777747

Batch 151200, train_perplexity=183.8198, train_loss=5.213956

Batch 151210, train_perplexity=186.2207, train_loss=5.2269325

Batch 151220, train_perplexity=201.1311, train_loss=5.303957

Batch 151230, train_perplexity=204.30879, train_loss=5.3196325

Batch 151240, train_perplexity=202.67216, train_loss=5.3115897

Batch 151250, train_perplexity=185.20268, train_loss=5.221451

Batch 151260, train_perplexity=199.4701, train_loss=5.2956643

Batch 151270, train_perplexity=210.8797, train_loss=5.351288

Batch 151280, train_perplexity=201.45976, train_loss=5.3055897

Batch 151290, train_perplexity=219.57233, train_loss=5.3916817

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00075-of-00100
Loaded 305395 sentences.
Finished loading
Batch 151300, train_perplexity=198.39743, train_loss=5.290272

Batch 151310, train_perplexity=180.44998, train_loss=5.1954536

Batch 151320, train_perplexity=220.1567, train_loss=5.3943396

Batch 151330, train_perplexity=199.04243, train_loss=5.293518

Batch 151340, train_perplexity=220.97243, train_loss=5.398038

Batch 151350, train_perplexity=207.64943, train_loss=5.335851

Batch 151360, train_perplexity=198.98607, train_loss=5.293235

Batch 151370, train_perplexity=229.61539, train_loss=5.4364057

Batch 151380, train_perplexity=221.39725, train_loss=5.3999586

Batch 151390, train_perplexity=217.8956, train_loss=5.384016

Batch 151400, train_perplexity=211.99944, train_loss=5.3565836

Batch 151410, train_perplexity=217.294, train_loss=5.3812513

Batch 151420, train_perplexity=209.4127, train_loss=5.344307

Batch 151430, train_perplexity=221.54573, train_loss=5.400629

Batch 151440, train_perplexity=199.28737, train_loss=5.294748

Batch 151450, train_perplexity=227.2721, train_loss=5.426148

Batch 151460, train_perplexity=207.15218, train_loss=5.3334537

Batch 151470, train_perplexity=206.37221, train_loss=5.3296814

Batch 151480, train_perplexity=197.26158, train_loss=5.2845306

Batch 151490, train_perplexity=197.0712, train_loss=5.283565

Batch 151500, train_perplexity=211.5416, train_loss=5.3544216

Batch 151510, train_perplexity=207.23921, train_loss=5.3338737

Batch 151520, train_perplexity=207.47751, train_loss=5.335023

Batch 151530, train_perplexity=214.27863, train_loss=5.367277

Batch 151540, train_perplexity=208.54744, train_loss=5.3401666

Batch 151550, train_perplexity=200.97829, train_loss=5.303197

Batch 151560, train_perplexity=200.38748, train_loss=5.300253

Batch 151570, train_perplexity=208.07474, train_loss=5.3378973

Batch 151580, train_perplexity=190.01947, train_loss=5.2471266

Batch 151590, train_perplexity=197.40273, train_loss=5.285246

Batch 151600, train_perplexity=210.52765, train_loss=5.349617

Batch 151610, train_perplexity=224.73132, train_loss=5.4149055

Batch 151620, train_perplexity=225.34543, train_loss=5.4176345

Batch 151630, train_perplexity=187.23413, train_loss=5.23236

Batch 151640, train_perplexity=207.44565, train_loss=5.3348694

Batch 151650, train_perplexity=178.50789, train_loss=5.184633
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 151660, train_perplexity=207.89482, train_loss=5.3370323

Batch 151670, train_perplexity=216.87819, train_loss=5.379336

Batch 151680, train_perplexity=193.56578, train_loss=5.2656174

Batch 151690, train_perplexity=202.60925, train_loss=5.3112793

Batch 151700, train_perplexity=207.91415, train_loss=5.3371253

Batch 151710, train_perplexity=200.4691, train_loss=5.30066

Batch 151720, train_perplexity=206.14581, train_loss=5.3285837

Batch 151730, train_perplexity=208.80785, train_loss=5.3414145

Batch 151740, train_perplexity=186.6079, train_loss=5.2290096

Batch 151750, train_perplexity=211.6982, train_loss=5.3551617

Batch 151760, train_perplexity=215.64085, train_loss=5.3736143

Batch 151770, train_perplexity=196.83612, train_loss=5.2823715

Batch 151780, train_perplexity=193.48439, train_loss=5.265197

Batch 151790, train_perplexity=215.8219, train_loss=5.3744535

Batch 151800, train_perplexity=201.43968, train_loss=5.30549

Batch 151810, train_perplexity=197.2928, train_loss=5.284689

Batch 151820, train_perplexity=190.13994, train_loss=5.2477603

Batch 151830, train_perplexity=169.25183, train_loss=5.1313877

Batch 151840, train_perplexity=210.58348, train_loss=5.349882

Batch 151850, train_perplexity=216.59016, train_loss=5.378007

Batch 151860, train_perplexity=204.74551, train_loss=5.321768

Batch 151870, train_perplexity=209.3426, train_loss=5.343972

Batch 151880, train_perplexity=217.1312, train_loss=5.3805017

Batch 151890, train_perplexity=201.69228, train_loss=5.306743

Batch 151900, train_perplexity=205.48355, train_loss=5.325366

Batch 151910, train_perplexity=184.50862, train_loss=5.217696

Batch 151920, train_perplexity=213.7708, train_loss=5.3649044

Batch 151930, train_perplexity=222.95778, train_loss=5.4069824

Batch 151940, train_perplexity=206.07318, train_loss=5.3282313

Batch 151950, train_perplexity=185.61723, train_loss=5.2236867

Batch 151960, train_perplexity=203.96469, train_loss=5.317947

Batch 151970, train_perplexity=227.9538, train_loss=5.429143

Batch 151980, train_perplexity=195.1833, train_loss=5.273939

Batch 151990, train_perplexity=228.02663, train_loss=5.4294624

Batch 152000, train_perplexity=212.18471, train_loss=5.357457

Batch 152010, train_perplexity=206.41995, train_loss=5.3299127

Batch 152020, train_perplexity=193.13374, train_loss=5.263383

Batch 152030, train_perplexity=184.45699, train_loss=5.2174163

Batch 152040, train_perplexity=205.88165, train_loss=5.3273015

Batch 152050, train_perplexity=216.40785, train_loss=5.377165

Batch 152060, train_perplexity=202.65294, train_loss=5.311495

Batch 152070, train_perplexity=241.24124, train_loss=5.4857974

Batch 152080, train_perplexity=202.49934, train_loss=5.3107367

Batch 152090, train_perplexity=184.95699, train_loss=5.2201233

Batch 152100, train_perplexity=191.71649, train_loss=5.2560177

Batch 152110, train_perplexity=202.01424, train_loss=5.308338

Batch 152120, train_perplexity=224.25111, train_loss=5.4127665

Batch 152130, train_perplexity=222.35324, train_loss=5.4042673

Batch 152140, train_perplexity=210.35957, train_loss=5.3488183

Batch 152150, train_perplexity=221.78821, train_loss=5.401723

Batch 152160, train_perplexity=204.49057, train_loss=5.320522

Batch 152170, train_perplexity=219.70114, train_loss=5.392268

Batch 152180, train_perplexity=214.28648, train_loss=5.367314

Batch 152190, train_perplexity=180.31116, train_loss=5.194684

Batch 152200, train_perplexity=212.4755, train_loss=5.3588266

Batch 152210, train_perplexity=213.26093, train_loss=5.3625164

Batch 152220, train_perplexity=197.22339, train_loss=5.284337

Batch 152230, train_perplexity=214.88548, train_loss=5.3701053

Batch 152240, train_perplexity=194.82503, train_loss=5.272102

Batch 152250, train_perplexity=200.4932, train_loss=5.3007803

Batch 152260, train_perplexity=237.04144, train_loss=5.468235

Batch 152270, train_perplexity=193.72699, train_loss=5.26645

Batch 152280, train_perplexity=224.18462, train_loss=5.41247

Batch 152290, train_perplexity=194.01598, train_loss=5.2679405

Batch 152300, train_perplexity=224.42313, train_loss=5.413533

Batch 152310, train_perplexity=212.86957, train_loss=5.3606796

Batch 152320, train_perplexity=182.9724, train_loss=5.2093353

Batch 152330, train_perplexity=210.82059, train_loss=5.3510075

Batch 152340, train_perplexity=210.28075, train_loss=5.3484435

Batch 152350, train_perplexity=227.24164, train_loss=5.426014

Batch 152360, train_perplexity=196.04181, train_loss=5.278328

Batch 152370, train_perplexity=205.94183, train_loss=5.327594

Batch 152380, train_perplexity=202.8198, train_loss=5.312318

Batch 152390, train_perplexity=205.33194, train_loss=5.324628

Batch 152400, train_perplexity=220.26485, train_loss=5.3948307

Batch 152410, train_perplexity=212.01157, train_loss=5.356641

Batch 152420, train_perplexity=192.70892, train_loss=5.261181

Batch 152430, train_perplexity=221.86754, train_loss=5.4020805

Batch 152440, train_perplexity=194.59413, train_loss=5.270916

Batch 152450, train_perplexity=198.22267, train_loss=5.289391

Batch 152460, train_perplexity=208.01085, train_loss=5.33759

Batch 152470, train_perplexity=197.9329, train_loss=5.287928

Batch 152480, train_perplexity=232.88669, train_loss=5.450552

Batch 152490, train_perplexity=182.99492, train_loss=5.2094584

Batch 152500, train_perplexity=202.10365, train_loss=5.3087807

Batch 152510, train_perplexity=201.96396, train_loss=5.3080893

Batch 152520, train_perplexity=190.88512, train_loss=5.251672

Batch 152530, train_perplexity=213.18802, train_loss=5.3621745

Batch 152540, train_perplexity=218.78232, train_loss=5.3880773

Batch 152550, train_perplexity=226.43365, train_loss=5.422452

Batch 152560, train_perplexity=203.56195, train_loss=5.3159704

Batch 152570, train_perplexity=214.09479, train_loss=5.366419

Batch 152580, train_perplexity=197.0057, train_loss=5.2832327

Batch 152590, train_perplexity=233.97728, train_loss=5.455224

Batch 152600, train_perplexity=199.38622, train_loss=5.2952437

Batch 152610, train_perplexity=190.96632, train_loss=5.252097

Batch 152620, train_perplexity=196.00105, train_loss=5.27812

Batch 152630, train_perplexity=202.8492, train_loss=5.312463

Batch 152640, train_perplexity=225.86977, train_loss=5.4199586

Batch 152650, train_perplexity=211.42401, train_loss=5.3538656

Batch 152660, train_perplexity=200.04436, train_loss=5.298539

Batch 152670, train_perplexity=198.31316, train_loss=5.2898474

Batch 152680, train_perplexity=207.10614, train_loss=5.3332314

Batch 152690, train_perplexity=204.34952, train_loss=5.319832

Batch 152700, train_perplexity=189.34583, train_loss=5.243575

Batch 152710, train_perplexity=214.08713, train_loss=5.366383

Batch 152720, train_perplexity=186.3839, train_loss=5.2278085

Batch 152730, train_perplexity=199.31996, train_loss=5.2949114

Batch 152740, train_perplexity=186.69708, train_loss=5.2294874

Batch 152750, train_perplexity=221.8121, train_loss=5.4018307

Batch 152760, train_perplexity=218.40239, train_loss=5.386339

Batch 152770, train_perplexity=199.62376, train_loss=5.2964344

Batch 152780, train_perplexity=168.31445, train_loss=5.125834

Batch 152790, train_perplexity=215.42996, train_loss=5.372636

Batch 152800, train_perplexity=177.13435, train_loss=5.1769085

Batch 152810, train_perplexity=221.128, train_loss=5.3987417

Batch 152820, train_perplexity=181.39706, train_loss=5.2006884

Batch 152830, train_perplexity=201.01836, train_loss=5.303396

Batch 152840, train_perplexity=209.7917, train_loss=5.346115

Batch 152850, train_perplexity=201.9185, train_loss=5.307864

Batch 152860, train_perplexity=204.71681, train_loss=5.3216276

Batch 152870, train_perplexity=199.23643, train_loss=5.2944922

Batch 152880, train_perplexity=162.8283, train_loss=5.092696

Batch 152890, train_perplexity=221.0854, train_loss=5.398549

Batch 152900, train_perplexity=196.09155, train_loss=5.2785816

Batch 152910, train_perplexity=198.54147, train_loss=5.290998

Batch 152920, train_perplexity=196.76509, train_loss=5.2820106

Batch 152930, train_perplexity=189.04433, train_loss=5.2419815

Batch 152940, train_perplexity=182.5259, train_loss=5.206892

Batch 152950, train_perplexity=222.21693, train_loss=5.403654
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 152960, train_perplexity=210.76811, train_loss=5.3507586

Batch 152970, train_perplexity=201.94334, train_loss=5.307987

Batch 152980, train_perplexity=202.52629, train_loss=5.3108697

Batch 152990, train_perplexity=201.0287, train_loss=5.3034477

Batch 153000, train_perplexity=189.2949, train_loss=5.243306

Batch 153010, train_perplexity=191.88916, train_loss=5.256918

Batch 153020, train_perplexity=228.52582, train_loss=5.431649

Batch 153030, train_perplexity=202.68443, train_loss=5.3116503

Batch 153040, train_perplexity=194.67134, train_loss=5.2713127

Batch 153050, train_perplexity=201.59978, train_loss=5.3062844

Batch 153060, train_perplexity=200.28279, train_loss=5.2997303

Batch 153070, train_perplexity=207.15681, train_loss=5.333476

Batch 153080, train_perplexity=222.20444, train_loss=5.403598

Batch 153090, train_perplexity=216.42891, train_loss=5.377262

Batch 153100, train_perplexity=197.1119, train_loss=5.2837715

Batch 153110, train_perplexity=214.02579, train_loss=5.3660965

Batch 153120, train_perplexity=196.01302, train_loss=5.278181

Batch 153130, train_perplexity=203.38043, train_loss=5.3150783

Batch 153140, train_perplexity=208.58434, train_loss=5.3403435

Batch 153150, train_perplexity=205.58255, train_loss=5.3258476

Batch 153160, train_perplexity=223.27397, train_loss=5.4083996

Batch 153170, train_perplexity=211.70062, train_loss=5.355173

Batch 153180, train_perplexity=207.02173, train_loss=5.3328238

Batch 153190, train_perplexity=200.8189, train_loss=5.3024035

Batch 153200, train_perplexity=197.5176, train_loss=5.2858276

Batch 153210, train_perplexity=195.6326, train_loss=5.2762384

Batch 153220, train_perplexity=208.58713, train_loss=5.340357

Batch 153230, train_perplexity=239.58736, train_loss=5.478918

Batch 153240, train_perplexity=208.23235, train_loss=5.3386545

Batch 153250, train_perplexity=203.6338, train_loss=5.3163233

Batch 153260, train_perplexity=183.10507, train_loss=5.21006

Batch 153270, train_perplexity=196.18929, train_loss=5.27908

Batch 153280, train_perplexity=167.57162, train_loss=5.121411

Batch 153290, train_perplexity=207.9097, train_loss=5.337104

Batch 153300, train_perplexity=214.64104, train_loss=5.368967

Batch 153310, train_perplexity=185.90831, train_loss=5.2252536

Batch 153320, train_perplexity=225.45775, train_loss=5.418133

Batch 153330, train_perplexity=218.79358, train_loss=5.3881288

Batch 153340, train_perplexity=208.54059, train_loss=5.3401337

Batch 153350, train_perplexity=222.16841, train_loss=5.4034357

Batch 153360, train_perplexity=210.60104, train_loss=5.3499656

Batch 153370, train_perplexity=204.28249, train_loss=5.319504

Batch 153380, train_perplexity=196.97603, train_loss=5.283082

Batch 153390, train_perplexity=190.92207, train_loss=5.2518654

Batch 153400, train_perplexity=238.1355, train_loss=5.47284

Batch 153410, train_perplexity=185.4305, train_loss=5.22268

Batch 153420, train_perplexity=223.0103, train_loss=5.407218

Batch 153430, train_perplexity=183.90202, train_loss=5.214403

Batch 153440, train_perplexity=202.1634, train_loss=5.3090763

Batch 153450, train_perplexity=208.8214, train_loss=5.3414793

Batch 153460, train_perplexity=197.85298, train_loss=5.287524

Batch 153470, train_perplexity=202.7666, train_loss=5.3120556

Batch 153480, train_perplexity=217.82765, train_loss=5.383704

Batch 153490, train_perplexity=204.36852, train_loss=5.319925

Batch 153500, train_perplexity=188.64847, train_loss=5.2398853

Batch 153510, train_perplexity=200.43947, train_loss=5.3005123

Batch 153520, train_perplexity=219.514, train_loss=5.391416

Batch 153530, train_perplexity=208.52686, train_loss=5.340068

Batch 153540, train_perplexity=217.62582, train_loss=5.382777

Batch 153550, train_perplexity=198.48032, train_loss=5.29069

Batch 153560, train_perplexity=213.94426, train_loss=5.3657155

Batch 153570, train_perplexity=200.11534, train_loss=5.298894

Batch 153580, train_perplexity=228.8187, train_loss=5.43293

Batch 153590, train_perplexity=186.75513, train_loss=5.2297983

Batch 153600, train_perplexity=213.34372, train_loss=5.3629045

Batch 153610, train_perplexity=201.84996, train_loss=5.3075247

Batch 153620, train_perplexity=209.75218, train_loss=5.345927

Batch 153630, train_perplexity=220.98285, train_loss=5.398085

Batch 153640, train_perplexity=214.96399, train_loss=5.3704705

Batch 153650, train_perplexity=200.75609, train_loss=5.3020906

Batch 153660, train_perplexity=216.99043, train_loss=5.3798532

Batch 153670, train_perplexity=203.79855, train_loss=5.317132

Batch 153680, train_perplexity=203.49927, train_loss=5.3156624

Batch 153690, train_perplexity=178.96121, train_loss=5.187169

Batch 153700, train_perplexity=239.44414, train_loss=5.47832

Batch 153710, train_perplexity=182.95477, train_loss=5.209239

Batch 153720, train_perplexity=222.948, train_loss=5.4069386

Batch 153730, train_perplexity=208.41165, train_loss=5.339515

Batch 153740, train_perplexity=209.38414, train_loss=5.3441706

Batch 153750, train_perplexity=197.84335, train_loss=5.2874756

Batch 153760, train_perplexity=190.12189, train_loss=5.2476654

Batch 153770, train_perplexity=212.07486, train_loss=5.3569393

Batch 153780, train_perplexity=217.36241, train_loss=5.381566

Batch 153790, train_perplexity=207.92259, train_loss=5.337166

Batch 153800, train_perplexity=203.12233, train_loss=5.3138084

Batch 153810, train_perplexity=218.72755, train_loss=5.387827

Batch 153820, train_perplexity=227.39687, train_loss=5.426697

Batch 153830, train_perplexity=207.76085, train_loss=5.3363876

Batch 153840, train_perplexity=193.46353, train_loss=5.265089

Batch 153850, train_perplexity=216.58624, train_loss=5.377989

Batch 153860, train_perplexity=213.43265, train_loss=5.3633213

Batch 153870, train_perplexity=212.33096, train_loss=5.358146

Batch 153880, train_perplexity=191.2594, train_loss=5.2536306

Batch 153890, train_perplexity=222.96863, train_loss=5.407031

Batch 153900, train_perplexity=190.4747, train_loss=5.2495193

Batch 153910, train_perplexity=201.88086, train_loss=5.3076777

Batch 153920, train_perplexity=194.23341, train_loss=5.2690606

Batch 153930, train_perplexity=190.96487, train_loss=5.2520895

Batch 153940, train_perplexity=196.55371, train_loss=5.280936

Batch 153950, train_perplexity=194.29149, train_loss=5.2693596

Batch 153960, train_perplexity=184.59064, train_loss=5.2181406

Batch 153970, train_perplexity=190.2054, train_loss=5.2481046

Batch 153980, train_perplexity=214.95374, train_loss=5.370423

Batch 153990, train_perplexity=189.04765, train_loss=5.241999

Batch 154000, train_perplexity=213.76305, train_loss=5.364868

Batch 154010, train_perplexity=175.66841, train_loss=5.168598

Batch 154020, train_perplexity=189.04216, train_loss=5.24197

Batch 154030, train_perplexity=201.27934, train_loss=5.3046937

Batch 154040, train_perplexity=214.11092, train_loss=5.366494

Batch 154050, train_perplexity=227.44263, train_loss=5.426898

Batch 154060, train_perplexity=209.2438, train_loss=5.3435

Batch 154070, train_perplexity=194.7679, train_loss=5.2718086

Batch 154080, train_perplexity=198.5819, train_loss=5.2912016

Batch 154090, train_perplexity=200.17175, train_loss=5.2991757

Batch 154100, train_perplexity=219.3987, train_loss=5.3908906

Batch 154110, train_perplexity=219.61945, train_loss=5.3918962

Batch 154120, train_perplexity=208.7105, train_loss=5.340948

Batch 154130, train_perplexity=224.99884, train_loss=5.4160953

Batch 154140, train_perplexity=223.32274, train_loss=5.408618

Batch 154150, train_perplexity=195.89641, train_loss=5.277586

Batch 154160, train_perplexity=196.87349, train_loss=5.2825613

Batch 154170, train_perplexity=186.94865, train_loss=5.230834

Batch 154180, train_perplexity=181.26236, train_loss=5.1999454

Batch 154190, train_perplexity=205.75574, train_loss=5.3266897

Batch 154200, train_perplexity=212.95496, train_loss=5.3610806

Batch 154210, train_perplexity=202.60095, train_loss=5.3112383

Batch 154220, train_perplexity=225.40627, train_loss=5.4179044

Batch 154230, train_perplexity=213.15387, train_loss=5.3620143

Batch 154240, train_perplexity=211.98993, train_loss=5.356539

Batch 154250, train_perplexity=197.94754, train_loss=5.288002
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 154260, train_perplexity=216.28304, train_loss=5.376588

Batch 154270, train_perplexity=194.14795, train_loss=5.2686205

Batch 154280, train_perplexity=217.48288, train_loss=5.38212

Batch 154290, train_perplexity=202.09767, train_loss=5.308751

Batch 154300, train_perplexity=187.70398, train_loss=5.234866

Batch 154310, train_perplexity=171.84315, train_loss=5.146582

Batch 154320, train_perplexity=178.28008, train_loss=5.183356

Batch 154330, train_perplexity=204.96149, train_loss=5.322822

Batch 154340, train_perplexity=208.6695, train_loss=5.3407516

Batch 154350, train_perplexity=218.37573, train_loss=5.386217

Batch 154360, train_perplexity=210.02832, train_loss=5.3472424

Batch 154370, train_perplexity=232.11661, train_loss=5.44724

Batch 154380, train_perplexity=216.77594, train_loss=5.3788643

Batch 154390, train_perplexity=208.65488, train_loss=5.3406816

Batch 154400, train_perplexity=210.3344, train_loss=5.3486986

Batch 154410, train_perplexity=201.74469, train_loss=5.307003

Batch 154420, train_perplexity=211.79321, train_loss=5.3556104

Batch 154430, train_perplexity=177.14922, train_loss=5.1769924

Batch 154440, train_perplexity=198.87755, train_loss=5.2926893

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00095-of-00100
Loaded 305703 sentences.
Finished loading
Batch 154450, train_perplexity=203.47403, train_loss=5.3155384

Batch 154460, train_perplexity=183.56384, train_loss=5.2125626

Batch 154470, train_perplexity=189.19519, train_loss=5.2427793

Batch 154480, train_perplexity=204.2821, train_loss=5.319502

Batch 154490, train_perplexity=221.97908, train_loss=5.402583

Batch 154500, train_perplexity=193.18034, train_loss=5.263624

Batch 154510, train_perplexity=202.73567, train_loss=5.311903

Batch 154520, train_perplexity=212.71858, train_loss=5.35997

Batch 154530, train_perplexity=198.5302, train_loss=5.290941

Batch 154540, train_perplexity=219.34671, train_loss=5.3906536

Batch 154550, train_perplexity=192.40189, train_loss=5.2595863

Batch 154560, train_perplexity=198.69252, train_loss=5.2917585

Batch 154570, train_perplexity=193.79778, train_loss=5.266815

Batch 154580, train_perplexity=195.08188, train_loss=5.2734194

Batch 154590, train_perplexity=203.74481, train_loss=5.3168683

Batch 154600, train_perplexity=199.89828, train_loss=5.2978086

Batch 154610, train_perplexity=201.93872, train_loss=5.3079643

Batch 154620, train_perplexity=192.58261, train_loss=5.260525

Batch 154630, train_perplexity=201.54163, train_loss=5.305996

Batch 154640, train_perplexity=226.23485, train_loss=5.4215736

Batch 154650, train_perplexity=207.3513, train_loss=5.3344145

Batch 154660, train_perplexity=195.79181, train_loss=5.277052

Batch 154670, train_perplexity=204.93344, train_loss=5.3226852

Batch 154680, train_perplexity=211.69699, train_loss=5.355156

Batch 154690, train_perplexity=220.55661, train_loss=5.3961544

Batch 154700, train_perplexity=224.68782, train_loss=5.414712

Batch 154710, train_perplexity=201.72873, train_loss=5.306924

Batch 154720, train_perplexity=203.5077, train_loss=5.315704

Batch 154730, train_perplexity=202.38535, train_loss=5.3101735

Batch 154740, train_perplexity=194.18257, train_loss=5.268799

Batch 154750, train_perplexity=206.89087, train_loss=5.3321915

Batch 154760, train_perplexity=227.80113, train_loss=5.428473

Batch 154770, train_perplexity=230.26271, train_loss=5.439221

Batch 154780, train_perplexity=203.65594, train_loss=5.316432

Batch 154790, train_perplexity=218.91872, train_loss=5.3887005

Batch 154800, train_perplexity=209.47691, train_loss=5.3446136

Batch 154810, train_perplexity=217.51326, train_loss=5.38226

Batch 154820, train_perplexity=193.41318, train_loss=5.2648287

Batch 154830, train_perplexity=206.9108, train_loss=5.332288

Batch 154840, train_perplexity=181.23228, train_loss=5.1997795

Batch 154850, train_perplexity=222.9648, train_loss=5.407014

Batch 154860, train_perplexity=198.41153, train_loss=5.2903433

Batch 154870, train_perplexity=196.91084, train_loss=5.282751

Batch 154880, train_perplexity=182.4267, train_loss=5.2063484

Batch 154890, train_perplexity=210.44865, train_loss=5.3492417

Batch 154900, train_perplexity=210.5155, train_loss=5.3495593

Batch 154910, train_perplexity=187.72331, train_loss=5.234969

Batch 154920, train_perplexity=205.63116, train_loss=5.326084

Batch 154930, train_perplexity=190.8418, train_loss=5.251445

Batch 154940, train_perplexity=220.4027, train_loss=5.3954563

Batch 154950, train_perplexity=211.77979, train_loss=5.355547

Batch 154960, train_perplexity=200.20258, train_loss=5.2993298

Batch 154970, train_perplexity=209.97234, train_loss=5.346976

Batch 154980, train_perplexity=203.90683, train_loss=5.317663

Batch 154990, train_perplexity=182.99823, train_loss=5.2094765

Batch 155000, train_perplexity=212.64293, train_loss=5.3596144

Batch 155010, train_perplexity=213.23367, train_loss=5.3623886

Batch 155020, train_perplexity=210.20204, train_loss=5.348069

Batch 155030, train_perplexity=205.7519, train_loss=5.326671

Batch 155040, train_perplexity=193.95677, train_loss=5.2676353

Batch 155050, train_perplexity=232.13232, train_loss=5.4473076

Batch 155060, train_perplexity=222.17604, train_loss=5.40347

Batch 155070, train_perplexity=214.22049, train_loss=5.367006

Batch 155080, train_perplexity=202.66289, train_loss=5.311544

Batch 155090, train_perplexity=216.68892, train_loss=5.378463

Batch 155100, train_perplexity=198.54213, train_loss=5.2910013

Batch 155110, train_perplexity=212.77458, train_loss=5.3602333

Batch 155120, train_perplexity=196.9996, train_loss=5.2832017

Batch 155130, train_perplexity=206.47566, train_loss=5.3301826

Batch 155140, train_perplexity=209.33463, train_loss=5.343934

Batch 155150, train_perplexity=207.55775, train_loss=5.3354096

Batch 155160, train_perplexity=188.78453, train_loss=5.2406063

Batch 155170, train_perplexity=206.21748, train_loss=5.3289313

Batch 155180, train_perplexity=193.85619, train_loss=5.2671165

Batch 155190, train_perplexity=189.29887, train_loss=5.243327

Batch 155200, train_perplexity=205.77007, train_loss=5.3267593

Batch 155210, train_perplexity=194.50081, train_loss=5.2704363

Batch 155220, train_perplexity=215.02089, train_loss=5.370735

Batch 155230, train_perplexity=232.69154, train_loss=5.4497137

Batch 155240, train_perplexity=189.28064, train_loss=5.243231

Batch 155250, train_perplexity=202.99762, train_loss=5.3131943

Batch 155260, train_perplexity=193.32594, train_loss=5.2643776

Batch 155270, train_perplexity=195.26485, train_loss=5.274357

Batch 155280, train_perplexity=208.96582, train_loss=5.3421707

Batch 155290, train_perplexity=200.53679, train_loss=5.3009977

Batch 155300, train_perplexity=196.22305, train_loss=5.279252

Batch 155310, train_perplexity=220.22873, train_loss=5.3946667

Batch 155320, train_perplexity=217.5002, train_loss=5.3822

Batch 155330, train_perplexity=236.51735, train_loss=5.4660215

Batch 155340, train_perplexity=201.36449, train_loss=5.3051167

Batch 155350, train_perplexity=213.96712, train_loss=5.3658223

Batch 155360, train_perplexity=210.0808, train_loss=5.347492

Batch 155370, train_perplexity=215.0861, train_loss=5.3710384

Batch 155380, train_perplexity=203.12892, train_loss=5.313841

Batch 155390, train_perplexity=204.47984, train_loss=5.3204694

Batch 155400, train_perplexity=206.5435, train_loss=5.330511

Batch 155410, train_perplexity=187.07082, train_loss=5.2314873

Batch 155420, train_perplexity=186.7311, train_loss=5.2296696

Batch 155430, train_perplexity=197.90619, train_loss=5.287793

Batch 155440, train_perplexity=199.16472, train_loss=5.294132

Batch 155450, train_perplexity=209.55695, train_loss=5.3449955

Batch 155460, train_perplexity=220.5789, train_loss=5.3962555

Batch 155470, train_perplexity=203.76823, train_loss=5.316983

Batch 155480, train_perplexity=183.0821, train_loss=5.2099347

Batch 155490, train_perplexity=206.67897, train_loss=5.3311667
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 155500, train_perplexity=205.31764, train_loss=5.3245583

Batch 155510, train_perplexity=209.50958, train_loss=5.3447695

Batch 155520, train_perplexity=205.50551, train_loss=5.325473

Batch 155530, train_perplexity=195.76231, train_loss=5.2769012

Batch 155540, train_perplexity=214.72273, train_loss=5.3693476

Batch 155550, train_perplexity=176.33653, train_loss=5.1723943

Batch 155560, train_perplexity=226.34212, train_loss=5.4220476

Batch 155570, train_perplexity=200.07202, train_loss=5.2986774

Batch 155580, train_perplexity=180.73933, train_loss=5.197056

Batch 155590, train_perplexity=214.11806, train_loss=5.3665276

Batch 155600, train_perplexity=202.15414, train_loss=5.3090305

Batch 155610, train_perplexity=221.70393, train_loss=5.401343

Batch 155620, train_perplexity=205.0124, train_loss=5.3230705

Batch 155630, train_perplexity=213.52437, train_loss=5.363751

Batch 155640, train_perplexity=208.2255, train_loss=5.3386216

Batch 155650, train_perplexity=188.67726, train_loss=5.240038

Batch 155660, train_perplexity=175.10257, train_loss=5.165372

Batch 155670, train_perplexity=222.37679, train_loss=5.404373

Batch 155680, train_perplexity=204.01245, train_loss=5.318181

Batch 155690, train_perplexity=199.64308, train_loss=5.296531

Batch 155700, train_perplexity=216.59016, train_loss=5.378007

Batch 155710, train_perplexity=203.8277, train_loss=5.317275

Batch 155720, train_perplexity=202.15414, train_loss=5.3090305

Batch 155730, train_perplexity=191.42583, train_loss=5.2545004

Batch 155740, train_perplexity=235.32669, train_loss=5.4609747

Batch 155750, train_perplexity=193.02713, train_loss=5.2628307

Batch 155760, train_perplexity=237.60094, train_loss=5.4705925

Batch 155770, train_perplexity=199.26332, train_loss=5.294627

Batch 155780, train_perplexity=216.45688, train_loss=5.3773913

Batch 155790, train_perplexity=202.62424, train_loss=5.311353

Batch 155800, train_perplexity=225.54097, train_loss=5.418502

Batch 155810, train_perplexity=203.06752, train_loss=5.3135386

Batch 155820, train_perplexity=215.53137, train_loss=5.3731065

Batch 155830, train_perplexity=214.38838, train_loss=5.3677893

Batch 155840, train_perplexity=221.11113, train_loss=5.3986654

Batch 155850, train_perplexity=206.04665, train_loss=5.3281026

Batch 155860, train_perplexity=180.31839, train_loss=5.194724

Batch 155870, train_perplexity=196.21117, train_loss=5.2791915

Batch 155880, train_perplexity=235.79103, train_loss=5.462946

Batch 155890, train_perplexity=205.04936, train_loss=5.323251

Batch 155900, train_perplexity=197.66278, train_loss=5.2865624

Batch 155910, train_perplexity=215.84845, train_loss=5.3745766

Batch 155920, train_perplexity=193.78539, train_loss=5.2667513

Batch 155930, train_perplexity=224.21904, train_loss=5.4126234

Batch 155940, train_perplexity=217.83575, train_loss=5.3837414

Batch 155950, train_perplexity=201.85805, train_loss=5.3075647

Batch 155960, train_perplexity=222.18982, train_loss=5.403532

Batch 155970, train_perplexity=196.48587, train_loss=5.2805905

Batch 155980, train_perplexity=234.8609, train_loss=5.4589934

Batch 155990, train_perplexity=185.8149, train_loss=5.224751

Batch 156000, train_perplexity=206.28229, train_loss=5.3292456

Batch 156010, train_perplexity=209.92189, train_loss=5.3467355

Batch 156020, train_perplexity=192.45712, train_loss=5.2598734

Batch 156030, train_perplexity=196.69313, train_loss=5.281645

Batch 156040, train_perplexity=228.96452, train_loss=5.433567

Batch 156050, train_perplexity=182.13066, train_loss=5.2047243

Batch 156060, train_perplexity=211.52747, train_loss=5.354355

Batch 156070, train_perplexity=177.74829, train_loss=5.1803684

Batch 156080, train_perplexity=231.32791, train_loss=5.443836

Batch 156090, train_perplexity=209.8173, train_loss=5.346237

Batch 156100, train_perplexity=204.59464, train_loss=5.3210306

Batch 156110, train_perplexity=223.1654, train_loss=5.407913

Batch 156120, train_perplexity=203.00807, train_loss=5.313246

Batch 156130, train_perplexity=205.31706, train_loss=5.3245554

Batch 156140, train_perplexity=199.41199, train_loss=5.295373

Batch 156150, train_perplexity=207.25404, train_loss=5.3339453

Batch 156160, train_perplexity=209.50179, train_loss=5.3447323

Batch 156170, train_perplexity=206.55917, train_loss=5.330587

Batch 156180, train_perplexity=223.75102, train_loss=5.410534

Batch 156190, train_perplexity=220.31758, train_loss=5.39507

Batch 156200, train_perplexity=206.35765, train_loss=5.329611

Batch 156210, train_perplexity=205.59283, train_loss=5.3258977

Batch 156220, train_perplexity=202.29272, train_loss=5.3097157

Batch 156230, train_perplexity=222.4315, train_loss=5.404619

Batch 156240, train_perplexity=202.5772, train_loss=5.311121

Batch 156250, train_perplexity=180.0347, train_loss=5.1931496

Batch 156260, train_perplexity=206.41315, train_loss=5.3298798

Batch 156270, train_perplexity=188.20175, train_loss=5.2375145

Batch 156280, train_perplexity=209.60422, train_loss=5.345221

Batch 156290, train_perplexity=209.02043, train_loss=5.342432

Batch 156300, train_perplexity=192.52744, train_loss=5.2602386

Batch 156310, train_perplexity=228.38214, train_loss=5.4310203

Batch 156320, train_perplexity=198.92868, train_loss=5.2929463

Batch 156330, train_perplexity=222.89102, train_loss=5.406683

Batch 156340, train_perplexity=216.08553, train_loss=5.3756742

Batch 156350, train_perplexity=192.78447, train_loss=5.261573

Batch 156360, train_perplexity=207.8081, train_loss=5.336615

Batch 156370, train_perplexity=227.66072, train_loss=5.4278564

Batch 156380, train_perplexity=232.31815, train_loss=5.4481077

Batch 156390, train_perplexity=203.15527, train_loss=5.3139706

Batch 156400, train_perplexity=219.05843, train_loss=5.3893385

Batch 156410, train_perplexity=176.86882, train_loss=5.1754084

Batch 156420, train_perplexity=206.98235, train_loss=5.3326335

Batch 156430, train_perplexity=186.94295, train_loss=5.2308035

Batch 156440, train_perplexity=209.88374, train_loss=5.346554

Batch 156450, train_perplexity=165.64494, train_loss=5.1098466

Batch 156460, train_perplexity=202.02927, train_loss=5.3084126

Batch 156470, train_perplexity=221.90584, train_loss=5.402253

Batch 156480, train_perplexity=174.8165, train_loss=5.163737

Batch 156490, train_perplexity=199.46677, train_loss=5.2956476

Batch 156500, train_perplexity=206.38245, train_loss=5.329731

Batch 156510, train_perplexity=207.62071, train_loss=5.335713

Batch 156520, train_perplexity=215.52264, train_loss=5.373066

Batch 156530, train_perplexity=198.97867, train_loss=5.2931976

Batch 156540, train_perplexity=222.17615, train_loss=5.4034705

Batch 156550, train_perplexity=204.44328, train_loss=5.3202906

Batch 156560, train_perplexity=198.8438, train_loss=5.2925196

Batch 156570, train_perplexity=221.89177, train_loss=5.4021897

Batch 156580, train_perplexity=183.14174, train_loss=5.2102604

Batch 156590, train_perplexity=209.67068, train_loss=5.345538

Batch 156600, train_perplexity=213.39581, train_loss=5.3631487

Batch 156610, train_perplexity=179.90588, train_loss=5.192434

Batch 156620, train_perplexity=227.84914, train_loss=5.4286838

Batch 156630, train_perplexity=196.26929, train_loss=5.2794876

Batch 156640, train_perplexity=204.96628, train_loss=5.3228455

Batch 156650, train_perplexity=230.28215, train_loss=5.4393053

Batch 156660, train_perplexity=229.27435, train_loss=5.4349194

Batch 156670, train_perplexity=172.2131, train_loss=5.1487327

Batch 156680, train_perplexity=207.14655, train_loss=5.3334265

Batch 156690, train_perplexity=217.01143, train_loss=5.37995

Batch 156700, train_perplexity=184.52983, train_loss=5.217811

Batch 156710, train_perplexity=199.28717, train_loss=5.294747

Batch 156720, train_perplexity=202.35524, train_loss=5.3100247

Batch 156730, train_perplexity=198.0123, train_loss=5.288329

Batch 156740, train_perplexity=215.51472, train_loss=5.373029

Batch 156750, train_perplexity=210.51399, train_loss=5.349552

Batch 156760, train_perplexity=207.2319, train_loss=5.3338385

Batch 156770, train_perplexity=233.30392, train_loss=5.452342

Batch 156780, train_perplexity=229.81923, train_loss=5.437293

Batch 156790, train_perplexity=209.54794, train_loss=5.3449526
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 156800, train_perplexity=223.30719, train_loss=5.4085484

Batch 156810, train_perplexity=220.48984, train_loss=5.3958516

Batch 156820, train_perplexity=212.43031, train_loss=5.358614

Batch 156830, train_perplexity=196.60349, train_loss=5.281189

Batch 156840, train_perplexity=205.5549, train_loss=5.325713

Batch 156850, train_perplexity=198.02608, train_loss=5.2883987

Batch 156860, train_perplexity=226.92049, train_loss=5.4245996

Batch 156870, train_perplexity=220.91911, train_loss=5.3977966

Batch 156880, train_perplexity=207.27321, train_loss=5.334038

Batch 156890, train_perplexity=199.39487, train_loss=5.295287

Batch 156900, train_perplexity=176.80339, train_loss=5.1750383

Batch 156910, train_perplexity=197.40009, train_loss=5.2852325

Batch 156920, train_perplexity=206.66321, train_loss=5.3310905

Batch 156930, train_perplexity=189.33119, train_loss=5.243498

Batch 156940, train_perplexity=201.43134, train_loss=5.3054485

Batch 156950, train_perplexity=194.7431, train_loss=5.2716813

Batch 156960, train_perplexity=201.49684, train_loss=5.3057737

Batch 156970, train_perplexity=184.2149, train_loss=5.216103

Batch 156980, train_perplexity=207.70122, train_loss=5.3361006

Batch 156990, train_perplexity=201.7395, train_loss=5.3069773

Batch 157000, train_perplexity=209.95462, train_loss=5.3468914

Batch 157010, train_perplexity=224.46016, train_loss=5.413698

Batch 157020, train_perplexity=199.69583, train_loss=5.2967954

Batch 157030, train_perplexity=206.68735, train_loss=5.3312073

Batch 157040, train_perplexity=208.78873, train_loss=5.341323

Batch 157050, train_perplexity=196.01433, train_loss=5.2781878

Batch 157060, train_perplexity=208.3572, train_loss=5.339254

Batch 157070, train_perplexity=214.47427, train_loss=5.36819

Batch 157080, train_perplexity=191.01277, train_loss=5.2523403

Batch 157090, train_perplexity=197.35265, train_loss=5.284992

Batch 157100, train_perplexity=202.15088, train_loss=5.3090143

Batch 157110, train_perplexity=222.0875, train_loss=5.4030714

Batch 157120, train_perplexity=185.996, train_loss=5.225725

Batch 157130, train_perplexity=195.03296, train_loss=5.2731686

Batch 157140, train_perplexity=184.28256, train_loss=5.2164702

Batch 157150, train_perplexity=195.9135, train_loss=5.2776732

Batch 157160, train_perplexity=228.42984, train_loss=5.431229

Batch 157170, train_perplexity=198.59875, train_loss=5.2912865

Batch 157180, train_perplexity=186.24432, train_loss=5.2270594

Batch 157190, train_perplexity=188.07597, train_loss=5.236846

Batch 157200, train_perplexity=195.50243, train_loss=5.275573

Batch 157210, train_perplexity=192.28064, train_loss=5.258956

Batch 157220, train_perplexity=194.74878, train_loss=5.2717104

Batch 157230, train_perplexity=194.1324, train_loss=5.2685404

Batch 157240, train_perplexity=211.48561, train_loss=5.354157

Batch 157250, train_perplexity=220.1122, train_loss=5.3941374

Batch 157260, train_perplexity=194.57658, train_loss=5.270826

Batch 157270, train_perplexity=217.51295, train_loss=5.3822584

Batch 157280, train_perplexity=201.00916, train_loss=5.3033504

Batch 157290, train_perplexity=191.7655, train_loss=5.2562733

Batch 157300, train_perplexity=203.15681, train_loss=5.313978

Batch 157310, train_perplexity=205.38011, train_loss=5.3248625

Batch 157320, train_perplexity=199.43605, train_loss=5.2954936

Batch 157330, train_perplexity=196.60217, train_loss=5.2811823

Batch 157340, train_perplexity=198.47133, train_loss=5.2906446

Batch 157350, train_perplexity=194.85532, train_loss=5.2722573

Batch 157360, train_perplexity=214.08018, train_loss=5.3663507

Batch 157370, train_perplexity=194.4231, train_loss=5.2700367

Batch 157380, train_perplexity=206.72974, train_loss=5.3314123

Batch 157390, train_perplexity=191.36723, train_loss=5.2541943

Batch 157400, train_perplexity=213.41576, train_loss=5.363242

Batch 157410, train_perplexity=217.67337, train_loss=5.3829956

Batch 157420, train_perplexity=212.37805, train_loss=5.358368

Batch 157430, train_perplexity=203.73442, train_loss=5.3168173

Batch 157440, train_perplexity=197.2342, train_loss=5.284392

Batch 157450, train_perplexity=202.46458, train_loss=5.310565

Batch 157460, train_perplexity=200.78557, train_loss=5.3022375

Batch 157470, train_perplexity=219.1255, train_loss=5.3896446

Batch 157480, train_perplexity=204.08815, train_loss=5.318552

Batch 157490, train_perplexity=188.61572, train_loss=5.2397118

Batch 157500, train_perplexity=189.082, train_loss=5.242181

Batch 157510, train_perplexity=204.83086, train_loss=5.3221846

Batch 157520, train_perplexity=194.0578, train_loss=5.268156

Batch 157530, train_perplexity=215.68282, train_loss=5.373809

Batch 157540, train_perplexity=218.49405, train_loss=5.386759

Batch 157550, train_perplexity=181.15038, train_loss=5.1993275

Batch 157560, train_perplexity=209.86694, train_loss=5.3464737

Batch 157570, train_perplexity=191.4879, train_loss=5.2548246

Batch 157580, train_perplexity=197.18578, train_loss=5.2841463

Batch 157590, train_perplexity=181.55377, train_loss=5.201552

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Batch 157600, train_perplexity=209.24182, train_loss=5.3434906

Batch 157610, train_perplexity=207.20236, train_loss=5.333696

Batch 157620, train_perplexity=206.21246, train_loss=5.328907

Batch 157630, train_perplexity=193.81236, train_loss=5.2668905

Batch 157640, train_perplexity=210.5684, train_loss=5.3498106

Batch 157650, train_perplexity=206.5575, train_loss=5.330579

Batch 157660, train_perplexity=199.07832, train_loss=5.2936983

Batch 157670, train_perplexity=212.41208, train_loss=5.358528

Batch 157680, train_perplexity=207.44426, train_loss=5.3348627

Batch 157690, train_perplexity=216.27498, train_loss=5.3765507

Batch 157700, train_perplexity=200.59436, train_loss=5.301285

Batch 157710, train_perplexity=199.88933, train_loss=5.297764

Batch 157720, train_perplexity=199.93985, train_loss=5.2980165

Batch 157730, train_perplexity=206.35825, train_loss=5.3296137

Batch 157740, train_perplexity=219.66867, train_loss=5.3921204

Batch 157750, train_perplexity=208.54715, train_loss=5.340165

Batch 157760, train_perplexity=191.37134, train_loss=5.2542157

Batch 157770, train_perplexity=185.3299, train_loss=5.2221375

Batch 157780, train_perplexity=203.30664, train_loss=5.3147154

Batch 157790, train_perplexity=218.10901, train_loss=5.384995

Batch 157800, train_perplexity=216.14766, train_loss=5.375962

Batch 157810, train_perplexity=204.93344, train_loss=5.3226852

Batch 157820, train_perplexity=224.91678, train_loss=5.4157305

Batch 157830, train_perplexity=194.99287, train_loss=5.272963

Batch 157840, train_perplexity=206.8958, train_loss=5.3322153

Batch 157850, train_perplexity=220.27536, train_loss=5.3948784

Batch 157860, train_perplexity=182.05286, train_loss=5.204297

Batch 157870, train_perplexity=184.56653, train_loss=5.21801

Batch 157880, train_perplexity=209.52736, train_loss=5.3448544

Batch 157890, train_perplexity=204.95357, train_loss=5.3227835

Batch 157900, train_perplexity=194.61212, train_loss=5.2710085

Batch 157910, train_perplexity=193.08844, train_loss=5.2631483

Batch 157920, train_perplexity=204.05449, train_loss=5.318387

Batch 157930, train_perplexity=205.41508, train_loss=5.3250327

Batch 157940, train_perplexity=192.3308, train_loss=5.259217

Batch 157950, train_perplexity=184.5118, train_loss=5.2177134

Batch 157960, train_perplexity=220.95557, train_loss=5.3979616

Batch 157970, train_perplexity=190.73445, train_loss=5.250882

Batch 157980, train_perplexity=203.0408, train_loss=5.313407

Batch 157990, train_perplexity=201.60852, train_loss=5.306328

Batch 158000, train_perplexity=214.77844, train_loss=5.369607

Batch 158010, train_perplexity=235.80452, train_loss=5.463003

Batch 158020, train_perplexity=199.96826, train_loss=5.2981586
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 158030, train_perplexity=197.45168, train_loss=5.285494

Batch 158040, train_perplexity=198.00436, train_loss=5.288289

Batch 158050, train_perplexity=179.65059, train_loss=5.191014

Batch 158060, train_perplexity=212.7624, train_loss=5.360176

Batch 158070, train_perplexity=196.57744, train_loss=5.2810564

Batch 158080, train_perplexity=195.3319, train_loss=5.2747

Batch 158090, train_perplexity=209.57393, train_loss=5.3450766

Batch 158100, train_perplexity=186.57658, train_loss=5.228842

Batch 158110, train_perplexity=218.36157, train_loss=5.3861523

Batch 158120, train_perplexity=201.78876, train_loss=5.3072214

Batch 158130, train_perplexity=201.30017, train_loss=5.304797

Batch 158140, train_perplexity=221.39471, train_loss=5.399947

Batch 158150, train_perplexity=225.82013, train_loss=5.419739

Batch 158160, train_perplexity=196.99434, train_loss=5.283175

Batch 158170, train_perplexity=195.73282, train_loss=5.2767506

Batch 158180, train_perplexity=182.87086, train_loss=5.2087803

Batch 158190, train_perplexity=226.61145, train_loss=5.423237

Batch 158200, train_perplexity=197.56468, train_loss=5.286066

Batch 158210, train_perplexity=212.03542, train_loss=5.3567533

Batch 158220, train_perplexity=179.51607, train_loss=5.1902647

Batch 158230, train_perplexity=196.44962, train_loss=5.280406

Batch 158240, train_perplexity=191.36797, train_loss=5.254198

Batch 158250, train_perplexity=183.43925, train_loss=5.2118835

Batch 158260, train_perplexity=184.3084, train_loss=5.2166104

Batch 158270, train_perplexity=202.55246, train_loss=5.310999

Batch 158280, train_perplexity=214.45955, train_loss=5.368121

Batch 158290, train_perplexity=198.5819, train_loss=5.2912016

Batch 158300, train_perplexity=186.23065, train_loss=5.226986

Batch 158310, train_perplexity=201.53201, train_loss=5.3059483

Batch 158320, train_perplexity=184.60068, train_loss=5.218195

Batch 158330, train_perplexity=218.43301, train_loss=5.3864794

Batch 158340, train_perplexity=204.0611, train_loss=5.3184195

Batch 158350, train_perplexity=191.28767, train_loss=5.2537785

Batch 158360, train_perplexity=185.26309, train_loss=5.221777

Batch 158370, train_perplexity=215.0299, train_loss=5.370777

Batch 158380, train_perplexity=186.24619, train_loss=5.2270694

Batch 158390, train_perplexity=236.7499, train_loss=5.4670043

Batch 158400, train_perplexity=196.58015, train_loss=5.28107

Batch 158410, train_perplexity=213.16678, train_loss=5.362075

Batch 158420, train_perplexity=201.74239, train_loss=5.3069916

Batch 158430, train_perplexity=213.18488, train_loss=5.3621597

Batch 158440, train_perplexity=203.5311, train_loss=5.315819

Batch 158450, train_perplexity=204.58331, train_loss=5.3209753

Batch 158460, train_perplexity=190.15273, train_loss=5.2478275

Batch 158470, train_perplexity=216.81573, train_loss=5.379048

Batch 158480, train_perplexity=206.24512, train_loss=5.3290653

Batch 158490, train_perplexity=175.8714, train_loss=5.169753

Batch 158500, train_perplexity=203.3683, train_loss=5.3150187

Batch 158510, train_perplexity=193.16505, train_loss=5.263545

Batch 158520, train_perplexity=198.42978, train_loss=5.2904353

Batch 158530, train_perplexity=239.41194, train_loss=5.4781857

Batch 158540, train_perplexity=215.46602, train_loss=5.372803

Batch 158550, train_perplexity=190.84198, train_loss=5.251446

Batch 158560, train_perplexity=194.56499, train_loss=5.2707663

Batch 158570, train_perplexity=202.25636, train_loss=5.309536

Batch 158580, train_perplexity=183.70596, train_loss=5.2133365

Batch 158590, train_perplexity=207.93141, train_loss=5.3372083

Batch 158600, train_perplexity=216.12973, train_loss=5.375879

Batch 158610, train_perplexity=232.09691, train_loss=5.447155

Batch 158620, train_perplexity=202.4924, train_loss=5.3107023

Batch 158630, train_perplexity=197.64459, train_loss=5.2864704

Batch 158640, train_perplexity=181.17387, train_loss=5.199457

Batch 158650, train_perplexity=200.56319, train_loss=5.3011293

Batch 158660, train_perplexity=223.7188, train_loss=5.41039

Batch 158670, train_perplexity=190.90068, train_loss=5.2517533

Batch 158680, train_perplexity=186.6962, train_loss=5.2294827

Batch 158690, train_perplexity=219.40193, train_loss=5.3909054

Batch 158700, train_perplexity=192.53212, train_loss=5.260263

Batch 158710, train_perplexity=198.66391, train_loss=5.2916145

Batch 158720, train_perplexity=189.9625, train_loss=5.2468266

Batch 158730, train_perplexity=196.62918, train_loss=5.2813196

Batch 158740, train_perplexity=202.40378, train_loss=5.3102646

Batch 158750, train_perplexity=179.86969, train_loss=5.1922326

Batch 158760, train_perplexity=215.69597, train_loss=5.37387

Batch 158770, train_perplexity=226.95781, train_loss=5.424764

Batch 158780, train_perplexity=204.74141, train_loss=5.321748

Batch 158790, train_perplexity=193.70917, train_loss=5.266358

Batch 158800, train_perplexity=177.82594, train_loss=5.180805

Batch 158810, train_perplexity=191.37654, train_loss=5.254243

Batch 158820, train_perplexity=206.93498, train_loss=5.3324046

Batch 158830, train_perplexity=211.95415, train_loss=5.35637

Batch 158840, train_perplexity=212.20212, train_loss=5.357539

Batch 158850, train_perplexity=188.45319, train_loss=5.2388496

Batch 158860, train_perplexity=185.58423, train_loss=5.223509

Batch 158870, train_perplexity=216.11932, train_loss=5.3758307

Batch 158880, train_perplexity=201.45477, train_loss=5.305565

Batch 158890, train_perplexity=184.76166, train_loss=5.2190666

Batch 158900, train_perplexity=195.23189, train_loss=5.274188

Batch 158910, train_perplexity=195.39944, train_loss=5.275046

Batch 158920, train_perplexity=190.04358, train_loss=5.2472534

Batch 158930, train_perplexity=208.00221, train_loss=5.3375487

Batch 158940, train_perplexity=211.89615, train_loss=5.3560963

Batch 158950, train_perplexity=213.80647, train_loss=5.3650713

Batch 158960, train_perplexity=199.84511, train_loss=5.2975426

Batch 158970, train_perplexity=202.22868, train_loss=5.309399

Batch 158980, train_perplexity=197.1177, train_loss=5.283801

Batch 158990, train_perplexity=188.77966, train_loss=5.2405806

Batch 159000, train_perplexity=190.82378, train_loss=5.2513504

Batch 159010, train_perplexity=208.20415, train_loss=5.338519

Batch 159020, train_perplexity=186.0479, train_loss=5.226004

Batch 159030, train_perplexity=217.12953, train_loss=5.380494

Batch 159040, train_perplexity=192.77362, train_loss=5.2615166

Batch 159050, train_perplexity=200.62344, train_loss=5.3014297

Batch 159060, train_perplexity=202.20987, train_loss=5.309306

Batch 159070, train_perplexity=195.616, train_loss=5.2761536

Batch 159080, train_perplexity=194.22183, train_loss=5.269001

Batch 159090, train_perplexity=199.65727, train_loss=5.2966022

Batch 159100, train_perplexity=194.18091, train_loss=5.2687902

Batch 159110, train_perplexity=201.09726, train_loss=5.3037887

Batch 159120, train_perplexity=198.67433, train_loss=5.291667

Batch 159130, train_perplexity=188.72333, train_loss=5.240282

Batch 159140, train_perplexity=184.22308, train_loss=5.2161474

Batch 159150, train_perplexity=205.20547, train_loss=5.324012

Batch 159160, train_perplexity=191.45267, train_loss=5.2546406

Batch 159170, train_perplexity=174.82866, train_loss=5.1638064

Batch 159180, train_perplexity=209.68378, train_loss=5.3456006

Batch 159190, train_perplexity=210.43712, train_loss=5.349187

Batch 159200, train_perplexity=209.08383, train_loss=5.3427353

Batch 159210, train_perplexity=197.41458, train_loss=5.285306

Batch 159220, train_perplexity=192.55827, train_loss=5.260399

Batch 159230, train_perplexity=184.96765, train_loss=5.220181

Batch 159240, train_perplexity=219.03409, train_loss=5.3892274

Batch 159250, train_perplexity=200.60585, train_loss=5.301342

Batch 159260, train_perplexity=235.03937, train_loss=5.459753

Batch 159270, train_perplexity=181.27618, train_loss=5.2000217

Batch 159280, train_perplexity=229.28212, train_loss=5.434953

Batch 159290, train_perplexity=200.15677, train_loss=5.299101

Batch 159300, train_perplexity=193.68173, train_loss=5.2662163

Batch 159310, train_perplexity=191.26341, train_loss=5.2536516

Batch 159320, train_perplexity=207.70201, train_loss=5.3361044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 159330, train_perplexity=208.36494, train_loss=5.339291

Batch 159340, train_perplexity=219.22333, train_loss=5.390091

Batch 159350, train_perplexity=187.60904, train_loss=5.23436

Batch 159360, train_perplexity=204.79883, train_loss=5.322028

Batch 159370, train_perplexity=221.29773, train_loss=5.399509

Batch 159380, train_perplexity=167.86256, train_loss=5.1231456

Batch 159390, train_perplexity=185.92001, train_loss=5.2253165

Batch 159400, train_perplexity=192.05412, train_loss=5.257777

Batch 159410, train_perplexity=211.77533, train_loss=5.355526

Batch 159420, train_perplexity=222.88976, train_loss=5.4066772

Batch 159430, train_perplexity=172.65257, train_loss=5.1512814

Batch 159440, train_perplexity=186.93636, train_loss=5.230768

Batch 159450, train_perplexity=188.80092, train_loss=5.240693

Batch 159460, train_perplexity=195.64987, train_loss=5.2763267

Batch 159470, train_perplexity=200.51901, train_loss=5.300909

Batch 159480, train_perplexity=198.92308, train_loss=5.292918

Batch 159490, train_perplexity=174.13637, train_loss=5.1598387

Batch 159500, train_perplexity=211.06198, train_loss=5.352152

Batch 159510, train_perplexity=208.81702, train_loss=5.3414583

Batch 159520, train_perplexity=222.10687, train_loss=5.4031587

Batch 159530, train_perplexity=219.60028, train_loss=5.391809

Batch 159540, train_perplexity=202.8755, train_loss=5.3125925

Batch 159550, train_perplexity=184.43983, train_loss=5.2173233

Batch 159560, train_perplexity=213.55287, train_loss=5.3638844

Batch 159570, train_perplexity=229.77496, train_loss=5.4371004

Batch 159580, train_perplexity=217.6906, train_loss=5.3830748

Batch 159590, train_perplexity=186.85739, train_loss=5.2303457

Batch 159600, train_perplexity=205.47063, train_loss=5.325303

Batch 159610, train_perplexity=196.44568, train_loss=5.280386

Batch 159620, train_perplexity=211.3593, train_loss=5.3535595

Batch 159630, train_perplexity=208.53462, train_loss=5.340105

Batch 159640, train_perplexity=199.0246, train_loss=5.2934284

Batch 159650, train_perplexity=204.90959, train_loss=5.322569

Batch 159660, train_perplexity=218.74257, train_loss=5.3878956

Batch 159670, train_perplexity=180.8633, train_loss=5.1977415

Batch 159680, train_perplexity=186.92378, train_loss=5.230701

Batch 159690, train_perplexity=177.29169, train_loss=5.1777964

Batch 159700, train_perplexity=199.2752, train_loss=5.294687

Batch 159710, train_perplexity=204.79414, train_loss=5.3220053

Batch 159720, train_perplexity=210.64433, train_loss=5.350171

Batch 159730, train_perplexity=183.66498, train_loss=5.2131133

Batch 159740, train_perplexity=186.42203, train_loss=5.228013

Batch 159750, train_perplexity=192.45052, train_loss=5.259839

Batch 159760, train_perplexity=204.09477, train_loss=5.3185844

Batch 159770, train_perplexity=180.93593, train_loss=5.198143

Batch 159780, train_perplexity=190.51685, train_loss=5.2497406

Batch 159790, train_perplexity=189.92365, train_loss=5.246622

Batch 159800, train_perplexity=213.3076, train_loss=5.3627353

Batch 159810, train_perplexity=212.12138, train_loss=5.3571587

Batch 159820, train_perplexity=196.94963, train_loss=5.282948

Batch 159830, train_perplexity=190.43745, train_loss=5.249324

Batch 159840, train_perplexity=206.41257, train_loss=5.329877

Batch 159850, train_perplexity=218.69377, train_loss=5.3876724

Batch 159860, train_perplexity=205.11371, train_loss=5.3235645

Batch 159870, train_perplexity=214.67769, train_loss=5.369138

Batch 159880, train_perplexity=208.42924, train_loss=5.3395996

Batch 159890, train_perplexity=215.54073, train_loss=5.37315

Batch 159900, train_perplexity=212.84642, train_loss=5.360571

Batch 159910, train_perplexity=214.7168, train_loss=5.36932

Batch 159920, train_perplexity=188.50838, train_loss=5.2391424

Batch 159930, train_perplexity=236.8191, train_loss=5.4672966

Batch 159940, train_perplexity=203.72743, train_loss=5.316783

Batch 159950, train_perplexity=188.60133, train_loss=5.2396355

Batch 159960, train_perplexity=191.7516, train_loss=5.256201

Batch 159970, train_perplexity=203.92706, train_loss=5.3177624

Batch 159980, train_perplexity=204.26086, train_loss=5.319398

Batch 159990, train_perplexity=200.01918, train_loss=5.2984133

Batch 160000, train_perplexity=222.13622, train_loss=5.4032907

Batch 160010, train_perplexity=182.41556, train_loss=5.2062874

Batch 160020, train_perplexity=206.43028, train_loss=5.3299627

Batch 160030, train_perplexity=199.5541, train_loss=5.2960854

Batch 160040, train_perplexity=209.04785, train_loss=5.342563

Batch 160050, train_perplexity=202.07657, train_loss=5.3086467

Batch 160060, train_perplexity=195.05156, train_loss=5.273264

Batch 160070, train_perplexity=209.75409, train_loss=5.345936

Batch 160080, train_perplexity=206.69424, train_loss=5.3312407

Batch 160090, train_perplexity=192.21124, train_loss=5.258595

Batch 160100, train_perplexity=228.67407, train_loss=5.4322977

Batch 160110, train_perplexity=188.74727, train_loss=5.240409

Batch 160120, train_perplexity=184.90443, train_loss=5.219839

Batch 160130, train_perplexity=190.40405, train_loss=5.2491484

Batch 160140, train_perplexity=194.61964, train_loss=5.271047

Batch 160150, train_perplexity=193.959, train_loss=5.267647

Batch 160160, train_perplexity=188.4433, train_loss=5.238797

Batch 160170, train_perplexity=211.0304, train_loss=5.352002

Batch 160180, train_perplexity=199.4486, train_loss=5.2955565

Batch 160190, train_perplexity=187.38766, train_loss=5.2331796

Batch 160200, train_perplexity=207.66496, train_loss=5.335926

Batch 160210, train_perplexity=217.09268, train_loss=5.3803244

Batch 160220, train_perplexity=221.49915, train_loss=5.4004188

Batch 160230, train_perplexity=185.44623, train_loss=5.222765

Batch 160240, train_perplexity=203.73558, train_loss=5.316823

Batch 160250, train_perplexity=182.982, train_loss=5.209388

Batch 160260, train_perplexity=210.32999, train_loss=5.3486776

Batch 160270, train_perplexity=191.37636, train_loss=5.254242

Batch 160280, train_perplexity=210.47345, train_loss=5.3493595

Batch 160290, train_perplexity=206.20695, train_loss=5.3288803

Batch 160300, train_perplexity=189.00034, train_loss=5.241749

Batch 160310, train_perplexity=196.12222, train_loss=5.278738

Batch 160320, train_perplexity=187.41957, train_loss=5.23335

Batch 160330, train_perplexity=185.92471, train_loss=5.225342

Batch 160340, train_perplexity=213.32907, train_loss=5.362836

Batch 160350, train_perplexity=191.59258, train_loss=5.255371

Batch 160360, train_perplexity=208.67885, train_loss=5.3407965

Batch 160370, train_perplexity=210.23833, train_loss=5.348242

Batch 160380, train_perplexity=207.5822, train_loss=5.3355274

Batch 160390, train_perplexity=212.36711, train_loss=5.3583164

Batch 160400, train_perplexity=187.1297, train_loss=5.231802

Batch 160410, train_perplexity=201.79868, train_loss=5.3072705

Batch 160420, train_perplexity=192.24176, train_loss=5.258754

Batch 160430, train_perplexity=189.90425, train_loss=5.24652

Batch 160440, train_perplexity=204.2898, train_loss=5.3195395

Batch 160450, train_perplexity=212.5229, train_loss=5.35905

Batch 160460, train_perplexity=183.00992, train_loss=5.2095404

Batch 160470, train_perplexity=219.58551, train_loss=5.3917418

Batch 160480, train_perplexity=199.5463, train_loss=5.2960463

Batch 160490, train_perplexity=180.9104, train_loss=5.198002

Batch 160500, train_perplexity=193.5212, train_loss=5.265387

Batch 160510, train_perplexity=214.95845, train_loss=5.370445

Batch 160520, train_perplexity=208.84908, train_loss=5.341612

Batch 160530, train_perplexity=196.68958, train_loss=5.2816267

Batch 160540, train_perplexity=190.72063, train_loss=5.2508097

Batch 160550, train_perplexity=177.7693, train_loss=5.1804867

Batch 160560, train_perplexity=192.33804, train_loss=5.2592545

Batch 160570, train_perplexity=195.63597, train_loss=5.2762556

Batch 160580, train_perplexity=189.85591, train_loss=5.2462654

Batch 160590, train_perplexity=217.4495, train_loss=5.3819666

Batch 160600, train_perplexity=174.56659, train_loss=5.1623063

Batch 160610, train_perplexity=211.61967, train_loss=5.3547907

Batch 160620, train_perplexity=186.81801, train_loss=5.230135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 160630, train_perplexity=205.50385, train_loss=5.3254647

Batch 160640, train_perplexity=199.08534, train_loss=5.2937336

Batch 160650, train_perplexity=197.81741, train_loss=5.2873445

Batch 160660, train_perplexity=185.7497, train_loss=5.2244

Batch 160670, train_perplexity=204.02812, train_loss=5.318258

Batch 160680, train_perplexity=179.6267, train_loss=5.190881

Batch 160690, train_perplexity=178.0771, train_loss=5.1822166

Batch 160700, train_perplexity=219.73048, train_loss=5.3924017

Batch 160710, train_perplexity=197.64459, train_loss=5.2864704

Batch 160720, train_perplexity=198.26485, train_loss=5.2896037

Batch 160730, train_perplexity=186.10458, train_loss=5.226309

Batch 160740, train_perplexity=214.85997, train_loss=5.3699865

Batch 160750, train_perplexity=202.83952, train_loss=5.312415

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00084-of-00100
Loaded 307251 sentences.
Finished loading
Batch 160760, train_perplexity=201.26706, train_loss=5.3046327

Batch 160770, train_perplexity=210.44736, train_loss=5.3492355

Batch 160780, train_perplexity=192.85011, train_loss=5.2619133

Batch 160790, train_perplexity=198.0716, train_loss=5.2886286

Batch 160800, train_perplexity=192.86833, train_loss=5.2620077

Batch 160810, train_perplexity=201.3881, train_loss=5.305234

Batch 160820, train_perplexity=219.83401, train_loss=5.392873

Batch 160830, train_perplexity=203.29054, train_loss=5.314636

Batch 160840, train_perplexity=223.99004, train_loss=5.4116015

Batch 160850, train_perplexity=196.45917, train_loss=5.2804546

Batch 160860, train_perplexity=192.41537, train_loss=5.2596564

Batch 160870, train_perplexity=188.84161, train_loss=5.2409086

Batch 160880, train_perplexity=197.19443, train_loss=5.28419

Batch 160890, train_perplexity=197.57278, train_loss=5.286107

Batch 160900, train_perplexity=202.22395, train_loss=5.309376

Batch 160910, train_perplexity=185.67398, train_loss=5.2239923

Batch 160920, train_perplexity=205.26224, train_loss=5.3242884

Batch 160930, train_perplexity=205.49904, train_loss=5.3254414

Batch 160940, train_perplexity=202.35823, train_loss=5.3100395

Batch 160950, train_perplexity=191.36522, train_loss=5.254184

Batch 160960, train_perplexity=220.609, train_loss=5.396392

Batch 160970, train_perplexity=204.59912, train_loss=5.3210526

Batch 160980, train_perplexity=197.997, train_loss=5.288252

Batch 160990, train_perplexity=209.2516, train_loss=5.3435373

Batch 161000, train_perplexity=195.55818, train_loss=5.275858

Batch 161010, train_perplexity=207.94281, train_loss=5.337263

Batch 161020, train_perplexity=205.37257, train_loss=5.324826

Batch 161030, train_perplexity=204.36569, train_loss=5.319911

Batch 161040, train_perplexity=175.75337, train_loss=5.1690817

Batch 161050, train_perplexity=173.31676, train_loss=5.155121

Batch 161060, train_perplexity=213.92497, train_loss=5.3656254

Batch 161070, train_perplexity=201.8212, train_loss=5.307382

Batch 161080, train_perplexity=192.72601, train_loss=5.2612696

Batch 161090, train_perplexity=193.2983, train_loss=5.2642345

Batch 161100, train_perplexity=206.36916, train_loss=5.3296666

Batch 161110, train_perplexity=209.81471, train_loss=5.346225

Batch 161120, train_perplexity=200.58672, train_loss=5.3012466

Batch 161130, train_perplexity=234.46333, train_loss=5.457299

Batch 161140, train_perplexity=224.13971, train_loss=5.4122696

Batch 161150, train_perplexity=217.31639, train_loss=5.3813543

Batch 161160, train_perplexity=194.65166, train_loss=5.2712116

Batch 161170, train_perplexity=216.54669, train_loss=5.377806

Batch 161180, train_perplexity=205.71709, train_loss=5.326502

Batch 161190, train_perplexity=207.01956, train_loss=5.3328133

Batch 161200, train_perplexity=203.45618, train_loss=5.3154507

Batch 161210, train_perplexity=197.7595, train_loss=5.2870517

Batch 161220, train_perplexity=213.26071, train_loss=5.3625154

Batch 161230, train_perplexity=207.88283, train_loss=5.3369746

Batch 161240, train_perplexity=205.96935, train_loss=5.3277273

Batch 161250, train_perplexity=209.85292, train_loss=5.346407

Batch 161260, train_perplexity=196.81378, train_loss=5.282258

Batch 161270, train_perplexity=188.66089, train_loss=5.239951

Batch 161280, train_perplexity=181.38322, train_loss=5.200612

Batch 161290, train_perplexity=189.34654, train_loss=5.243579

Batch 161300, train_perplexity=182.65466, train_loss=5.2075973

Batch 161310, train_perplexity=200.31813, train_loss=5.2999067

Batch 161320, train_perplexity=199.2447, train_loss=5.2945337

Batch 161330, train_perplexity=192.45363, train_loss=5.2598553

Batch 161340, train_perplexity=177.88022, train_loss=5.1811104

Batch 161350, train_perplexity=224.50575, train_loss=5.4139013

Batch 161360, train_perplexity=195.96611, train_loss=5.2779417

Batch 161370, train_perplexity=201.05019, train_loss=5.3035545

Batch 161380, train_perplexity=172.19208, train_loss=5.1486106

Batch 161390, train_perplexity=212.21971, train_loss=5.357622

Batch 161400, train_perplexity=183.04012, train_loss=5.2097054

Batch 161410, train_perplexity=202.81747, train_loss=5.3123064

Batch 161420, train_perplexity=198.57602, train_loss=5.291172

Batch 161430, train_perplexity=203.06926, train_loss=5.313547

Batch 161440, train_perplexity=197.13641, train_loss=5.283896

Batch 161450, train_perplexity=199.91725, train_loss=5.2979035

Batch 161460, train_perplexity=190.64825, train_loss=5.25043

Batch 161470, train_perplexity=182.55948, train_loss=5.207076

Batch 161480, train_perplexity=198.53209, train_loss=5.290951

Batch 161490, train_perplexity=199.80003, train_loss=5.297317

Batch 161500, train_perplexity=237.13744, train_loss=5.46864

Batch 161510, train_perplexity=199.9778, train_loss=5.2982063

Batch 161520, train_perplexity=206.61955, train_loss=5.330879

Batch 161530, train_perplexity=194.28789, train_loss=5.269341

Batch 161540, train_perplexity=190.27806, train_loss=5.2484865

Batch 161550, train_perplexity=176.80803, train_loss=5.1750646

Batch 161560, train_perplexity=196.45, train_loss=5.280408

Batch 161570, train_perplexity=204.4658, train_loss=5.3204007

Batch 161580, train_perplexity=207.33173, train_loss=5.33432

Batch 161590, train_perplexity=226.76656, train_loss=5.423921

Batch 161600, train_perplexity=204.60995, train_loss=5.3211055

Batch 161610, train_perplexity=189.65892, train_loss=5.2452273

Batch 161620, train_perplexity=214.19997, train_loss=5.36691

Batch 161630, train_perplexity=217.02872, train_loss=5.3800297

Batch 161640, train_perplexity=202.43843, train_loss=5.310436

Batch 161650, train_perplexity=196.42583, train_loss=5.280285

Batch 161660, train_perplexity=202.99075, train_loss=5.3131604

Batch 161670, train_perplexity=200.4995, train_loss=5.300812

Batch 161680, train_perplexity=190.93828, train_loss=5.2519503

Batch 161690, train_perplexity=198.65387, train_loss=5.291564

Batch 161700, train_perplexity=197.27625, train_loss=5.284605

Batch 161710, train_perplexity=216.74565, train_loss=5.3787246

Batch 161720, train_perplexity=192.57205, train_loss=5.2604704

Batch 161730, train_perplexity=218.21054, train_loss=5.3854604

Batch 161740, train_perplexity=169.9175, train_loss=5.135313

Batch 161750, train_perplexity=177.02888, train_loss=5.176313

Batch 161760, train_perplexity=195.32501, train_loss=5.274665

Batch 161770, train_perplexity=184.62936, train_loss=5.2183504

Batch 161780, train_perplexity=199.38898, train_loss=5.2952576

Batch 161790, train_perplexity=203.1668, train_loss=5.3140273

Batch 161800, train_perplexity=209.42778, train_loss=5.344379

Batch 161810, train_perplexity=213.72493, train_loss=5.36469

Batch 161820, train_perplexity=205.5851, train_loss=5.32586

Batch 161830, train_perplexity=208.94012, train_loss=5.3420477

Batch 161840, train_perplexity=191.00148, train_loss=5.252281

Batch 161850, train_perplexity=188.17896, train_loss=5.2373934

Batch 161860, train_perplexity=193.61488, train_loss=5.265871
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 161870, train_perplexity=211.56853, train_loss=5.354549

Batch 161880, train_perplexity=170.45505, train_loss=5.1384716

Batch 161890, train_perplexity=238.7259, train_loss=5.475316

Batch 161900, train_perplexity=190.7712, train_loss=5.251075

Batch 161910, train_perplexity=190.53647, train_loss=5.2498436

Batch 161920, train_perplexity=188.06252, train_loss=5.2367744

Batch 161930, train_perplexity=178.72404, train_loss=5.185843

Batch 161940, train_perplexity=188.67006, train_loss=5.24

Batch 161950, train_perplexity=212.29381, train_loss=5.357971

Batch 161960, train_perplexity=196.78601, train_loss=5.282117

Batch 161970, train_perplexity=214.2679, train_loss=5.367227

Batch 161980, train_perplexity=181.93396, train_loss=5.203644

Batch 161990, train_perplexity=180.48665, train_loss=5.195657

Batch 162000, train_perplexity=201.36603, train_loss=5.3051243

Batch 162010, train_perplexity=201.86171, train_loss=5.307583

Batch 162020, train_perplexity=201.08987, train_loss=5.303752

Batch 162030, train_perplexity=181.17853, train_loss=5.199483

Batch 162040, train_perplexity=201.15433, train_loss=5.3040724

Batch 162050, train_perplexity=202.03023, train_loss=5.3084173

Batch 162060, train_perplexity=184.22203, train_loss=5.2161417

Batch 162070, train_perplexity=217.2563, train_loss=5.381078

Batch 162080, train_perplexity=235.7591, train_loss=5.4628105

Batch 162090, train_perplexity=206.4357, train_loss=5.329989

Batch 162100, train_perplexity=204.25989, train_loss=5.319393

Batch 162110, train_perplexity=188.51422, train_loss=5.2391734

Batch 162120, train_perplexity=187.55081, train_loss=5.23405

Batch 162130, train_perplexity=200.83134, train_loss=5.3024654

Batch 162140, train_perplexity=197.89658, train_loss=5.2877445

Batch 162150, train_perplexity=179.80717, train_loss=5.191885

Batch 162160, train_perplexity=201.31447, train_loss=5.304868

Batch 162170, train_perplexity=184.08574, train_loss=5.2154016

Batch 162180, train_perplexity=189.62817, train_loss=5.245065

Batch 162190, train_perplexity=183.17021, train_loss=5.210416

Batch 162200, train_perplexity=207.85725, train_loss=5.3368516

Batch 162210, train_perplexity=212.12097, train_loss=5.3571568

Batch 162220, train_perplexity=185.94368, train_loss=5.225444

Batch 162230, train_perplexity=211.37462, train_loss=5.353632

Batch 162240, train_perplexity=183.06194, train_loss=5.2098246

Batch 162250, train_perplexity=185.11563, train_loss=5.2209806

Batch 162260, train_perplexity=200.45305, train_loss=5.30058

Batch 162270, train_perplexity=189.82921, train_loss=5.2461247

Batch 162280, train_perplexity=205.52922, train_loss=5.325588

Batch 162290, train_perplexity=186.58815, train_loss=5.228904

Batch 162300, train_perplexity=201.33952, train_loss=5.3049927

Batch 162310, train_perplexity=191.13013, train_loss=5.2529545

Batch 162320, train_perplexity=193.29314, train_loss=5.264208

Batch 162330, train_perplexity=216.25859, train_loss=5.376475

Batch 162340, train_perplexity=184.75426, train_loss=5.2190266

Batch 162350, train_perplexity=184.18303, train_loss=5.21593

Batch 162360, train_perplexity=200.41586, train_loss=5.3003945

Batch 162370, train_perplexity=181.4007, train_loss=5.2007084

Batch 162380, train_perplexity=199.27274, train_loss=5.2946744

Batch 162390, train_perplexity=206.51909, train_loss=5.330393

Batch 162400, train_perplexity=218.21231, train_loss=5.3854685

Batch 162410, train_perplexity=199.79774, train_loss=5.2973056

Batch 162420, train_perplexity=202.09691, train_loss=5.3087473

Batch 162430, train_perplexity=175.73242, train_loss=5.1689625

Batch 162440, train_perplexity=186.2699, train_loss=5.2271967

Batch 162450, train_perplexity=193.00633, train_loss=5.262723

Batch 162460, train_perplexity=225.16208, train_loss=5.4168205

Batch 162470, train_perplexity=203.04584, train_loss=5.3134317

Batch 162480, train_perplexity=188.80254, train_loss=5.2407017

Batch 162490, train_perplexity=195.36693, train_loss=5.2748795

Batch 162500, train_perplexity=222.16927, train_loss=5.4034395

Batch 162510, train_perplexity=204.25862, train_loss=5.319387

Batch 162520, train_perplexity=204.40419, train_loss=5.3200994

Batch 162530, train_perplexity=207.30396, train_loss=5.334186

Batch 162540, train_perplexity=223.98715, train_loss=5.4115887

Batch 162550, train_perplexity=221.41139, train_loss=5.4000225

Batch 162560, train_perplexity=213.12408, train_loss=5.3618746

Batch 162570, train_perplexity=201.17313, train_loss=5.304166

Batch 162580, train_perplexity=212.8513, train_loss=5.360594

Batch 162590, train_perplexity=232.9662, train_loss=5.4508934

Batch 162600, train_perplexity=187.63354, train_loss=5.234491

Batch 162610, train_perplexity=182.422, train_loss=5.2063227

Batch 162620, train_perplexity=190.17603, train_loss=5.24795

Batch 162630, train_perplexity=185.46587, train_loss=5.222871

Batch 162640, train_perplexity=213.02513, train_loss=5.36141

Batch 162650, train_perplexity=229.61111, train_loss=5.436387

Batch 162660, train_perplexity=187.846, train_loss=5.2356224

Batch 162670, train_perplexity=205.84131, train_loss=5.3271055

Batch 162680, train_perplexity=193.29811, train_loss=5.2642336

Batch 162690, train_perplexity=201.61044, train_loss=5.3063374

Batch 162700, train_perplexity=194.63292, train_loss=5.2711153

Batch 162710, train_perplexity=173.4832, train_loss=5.1560807

Batch 162720, train_perplexity=179.69498, train_loss=5.191261

Batch 162730, train_perplexity=208.97539, train_loss=5.3422165

Batch 162740, train_perplexity=221.04019, train_loss=5.3983445

Batch 162750, train_perplexity=183.6746, train_loss=5.2131658

Batch 162760, train_perplexity=196.95724, train_loss=5.2829866

Batch 162770, train_perplexity=203.60312, train_loss=5.3161726

Batch 162780, train_perplexity=202.14384, train_loss=5.3089795

Batch 162790, train_perplexity=200.59265, train_loss=5.301276

Batch 162800, train_perplexity=199.13358, train_loss=5.293976

Batch 162810, train_perplexity=197.61368, train_loss=5.286314

Batch 162820, train_perplexity=189.38213, train_loss=5.243767

Batch 162830, train_perplexity=175.92023, train_loss=5.1700306

Batch 162840, train_perplexity=181.82799, train_loss=5.203061

Batch 162850, train_perplexity=235.44499, train_loss=5.4614773

Batch 162860, train_perplexity=196.23372, train_loss=5.2793064

Batch 162870, train_perplexity=201.14397, train_loss=5.304021

Batch 162880, train_perplexity=201.80261, train_loss=5.30729

Batch 162890, train_perplexity=195.92612, train_loss=5.2777376

Batch 162900, train_perplexity=190.40967, train_loss=5.249178

Batch 162910, train_perplexity=190.09851, train_loss=5.2475424

Batch 162920, train_perplexity=211.20474, train_loss=5.352828

Batch 162930, train_perplexity=196.24458, train_loss=5.2793617

Batch 162940, train_perplexity=208.35532, train_loss=5.339245

Batch 162950, train_perplexity=191.13104, train_loss=5.2529593

Batch 162960, train_perplexity=222.70085, train_loss=5.4058294

Batch 162970, train_perplexity=191.39314, train_loss=5.2543297

Batch 162980, train_perplexity=204.66489, train_loss=5.321374

Batch 162990, train_perplexity=205.46808, train_loss=5.3252907

Batch 163000, train_perplexity=193.834, train_loss=5.267002

Batch 163010, train_perplexity=179.05595, train_loss=5.1876984

Batch 163020, train_perplexity=188.33981, train_loss=5.238248

Batch 163030, train_perplexity=219.73363, train_loss=5.392416

Batch 163040, train_perplexity=201.77798, train_loss=5.307168

Batch 163050, train_perplexity=209.86333, train_loss=5.3464565

Batch 163060, train_perplexity=191.01605, train_loss=5.2523575

Batch 163070, train_perplexity=213.32886, train_loss=5.362835

Batch 163080, train_perplexity=193.96066, train_loss=5.2676554

Batch 163090, train_perplexity=195.72665, train_loss=5.276719

Batch 163100, train_perplexity=190.27997, train_loss=5.2484965

Batch 163110, train_perplexity=189.29346, train_loss=5.2432985

Batch 163120, train_perplexity=205.65706, train_loss=5.32621

Batch 163130, train_perplexity=191.76569, train_loss=5.256274

Batch 163140, train_perplexity=196.08173, train_loss=5.2785316

Batch 163150, train_perplexity=205.52255, train_loss=5.325556

Batch 163160, train_perplexity=216.1355, train_loss=5.3759055
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 163170, train_perplexity=198.78085, train_loss=5.292203

Batch 163180, train_perplexity=177.26532, train_loss=5.1776476

Batch 163190, train_perplexity=209.6223, train_loss=5.3453074

Batch 163200, train_perplexity=205.4669, train_loss=5.325285

Batch 163210, train_perplexity=195.13156, train_loss=5.273674

Batch 163220, train_perplexity=212.23874, train_loss=5.357712

Batch 163230, train_perplexity=187.62085, train_loss=5.234423

Batch 163240, train_perplexity=169.20406, train_loss=5.1311054

Batch 163250, train_perplexity=185.32635, train_loss=5.2221184

Batch 163260, train_perplexity=195.27779, train_loss=5.274423

Batch 163270, train_perplexity=189.61272, train_loss=5.2449837

Batch 163280, train_perplexity=198.56541, train_loss=5.2911186

Batch 163290, train_perplexity=226.87979, train_loss=5.4244204

Batch 163300, train_perplexity=208.5191, train_loss=5.3400307

Batch 163310, train_perplexity=173.7956, train_loss=5.15788

Batch 163320, train_perplexity=179.27074, train_loss=5.188897

Batch 163330, train_perplexity=196.88249, train_loss=5.282607

Batch 163340, train_perplexity=232.98431, train_loss=5.450971

Batch 163350, train_perplexity=203.66342, train_loss=5.3164687

Batch 163360, train_perplexity=203.8732, train_loss=5.317498

Batch 163370, train_perplexity=178.98254, train_loss=5.1872883

Batch 163380, train_perplexity=202.17517, train_loss=5.3091345

Batch 163390, train_perplexity=219.55222, train_loss=5.39159

Batch 163400, train_perplexity=186.71684, train_loss=5.2295933

Batch 163410, train_perplexity=206.92184, train_loss=5.332341

Batch 163420, train_perplexity=190.52592, train_loss=5.2497883

Batch 163430, train_perplexity=203.56195, train_loss=5.3159704

Batch 163440, train_perplexity=178.80315, train_loss=5.1862855

Batch 163450, train_perplexity=213.14441, train_loss=5.36197

Batch 163460, train_perplexity=232.55588, train_loss=5.4491305

Batch 163470, train_perplexity=182.60442, train_loss=5.207322

Batch 163480, train_perplexity=204.82735, train_loss=5.3221674

Batch 163490, train_perplexity=189.0765, train_loss=5.2421517

Batch 163500, train_perplexity=179.51324, train_loss=5.190249

Batch 163510, train_perplexity=192.76186, train_loss=5.2614555

Batch 163520, train_perplexity=191.14635, train_loss=5.2530394

Batch 163530, train_perplexity=206.48827, train_loss=5.3302436

Batch 163540, train_perplexity=233.25255, train_loss=5.4521217

Batch 163550, train_perplexity=195.2589, train_loss=5.2743263

Batch 163560, train_perplexity=207.49017, train_loss=5.335084

Batch 163570, train_perplexity=182.70709, train_loss=5.2078843

Batch 163580, train_perplexity=215.52573, train_loss=5.3730803

Batch 163590, train_perplexity=207.18596, train_loss=5.3336167

Batch 163600, train_perplexity=177.70184, train_loss=5.180107

Batch 163610, train_perplexity=187.11409, train_loss=5.2317185

Batch 163620, train_perplexity=212.16528, train_loss=5.3573656

Batch 163630, train_perplexity=193.28778, train_loss=5.26418

Batch 163640, train_perplexity=213.81851, train_loss=5.3651276

Batch 163650, train_perplexity=184.38945, train_loss=5.21705

Batch 163660, train_perplexity=183.83347, train_loss=5.2140303

Batch 163670, train_perplexity=204.28482, train_loss=5.319515

Batch 163680, train_perplexity=181.64566, train_loss=5.202058

Batch 163690, train_perplexity=212.91362, train_loss=5.3608866

Batch 163700, train_perplexity=194.17369, train_loss=5.268753

Batch 163710, train_perplexity=179.70114, train_loss=5.191295

Batch 163720, train_perplexity=188.50298, train_loss=5.239114

Batch 163730, train_perplexity=186.75763, train_loss=5.2298117

Batch 163740, train_perplexity=189.31242, train_loss=5.2433987

Batch 163750, train_perplexity=173.47864, train_loss=5.1560545

Batch 163760, train_perplexity=201.40175, train_loss=5.3053017

Batch 163770, train_perplexity=192.25827, train_loss=5.2588396

Batch 163780, train_perplexity=196.01843, train_loss=5.2782087

Batch 163790, train_perplexity=208.62413, train_loss=5.340534

Batch 163800, train_perplexity=200.43642, train_loss=5.300497

Batch 163810, train_perplexity=217.33224, train_loss=5.3814273

Batch 163820, train_perplexity=209.72108, train_loss=5.3457785

Batch 163830, train_perplexity=191.70415, train_loss=5.2559533

Batch 163840, train_perplexity=208.96263, train_loss=5.3421555

Batch 163850, train_perplexity=216.36256, train_loss=5.3769555

Batch 163860, train_perplexity=204.09604, train_loss=5.3185906

Batch 163870, train_perplexity=193.78308, train_loss=5.2667394

Batch 163880, train_perplexity=182.57219, train_loss=5.2071457

Batch 163890, train_perplexity=196.70609, train_loss=5.2817106

Batch 163900, train_perplexity=183.00626, train_loss=5.2095203

Batch 163910, train_perplexity=215.26135, train_loss=5.371853

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00030-of-00100
Loaded 305807 sentences.
Finished loading
Batch 163920, train_perplexity=204.28249, train_loss=5.319504

Batch 163930, train_perplexity=209.6085, train_loss=5.3452415

Batch 163940, train_perplexity=194.85486, train_loss=5.272255

Batch 163950, train_perplexity=200.10944, train_loss=5.2988644

Batch 163960, train_perplexity=192.53477, train_loss=5.260277

Batch 163970, train_perplexity=195.36264, train_loss=5.2748575

Batch 163980, train_perplexity=183.1358, train_loss=5.210228

Batch 163990, train_perplexity=203.32022, train_loss=5.314782

Batch 164000, train_perplexity=196.9856, train_loss=5.2831306

Batch 164010, train_perplexity=216.22012, train_loss=5.376297

Batch 164020, train_perplexity=193.15843, train_loss=5.2635107

Batch 164030, train_perplexity=188.92447, train_loss=5.2413473

Batch 164040, train_perplexity=207.26047, train_loss=5.3339763

Batch 164050, train_perplexity=174.56702, train_loss=5.1623087

Batch 164060, train_perplexity=197.20421, train_loss=5.28424

Batch 164070, train_perplexity=204.31308, train_loss=5.3196535

Batch 164080, train_perplexity=188.88106, train_loss=5.2411175

Batch 164090, train_perplexity=187.96246, train_loss=5.2362423

Batch 164100, train_perplexity=179.98834, train_loss=5.192892

Batch 164110, train_perplexity=171.32892, train_loss=5.143585

Batch 164120, train_perplexity=230.17194, train_loss=5.4388266

Batch 164130, train_perplexity=188.81918, train_loss=5.24079

Batch 164140, train_perplexity=190.54828, train_loss=5.2499056

Batch 164150, train_perplexity=189.17102, train_loss=5.2426515

Batch 164160, train_perplexity=199.13718, train_loss=5.293994

Batch 164170, train_perplexity=177.28781, train_loss=5.1777744

Batch 164180, train_perplexity=199.02156, train_loss=5.293413

Batch 164190, train_perplexity=184.215, train_loss=5.2161036

Batch 164200, train_perplexity=193.37204, train_loss=5.264616

Batch 164210, train_perplexity=209.9332, train_loss=5.3467894

Batch 164220, train_perplexity=181.63612, train_loss=5.2020054

Batch 164230, train_perplexity=178.34717, train_loss=5.183732

Batch 164240, train_perplexity=202.16687, train_loss=5.3090935

Batch 164250, train_perplexity=190.80904, train_loss=5.251273

Batch 164260, train_perplexity=185.05905, train_loss=5.220675

Batch 164270, train_perplexity=210.44856, train_loss=5.3492413

Batch 164280, train_perplexity=209.2977, train_loss=5.3437576

Batch 164290, train_perplexity=182.39835, train_loss=5.206193

Batch 164300, train_perplexity=186.85811, train_loss=5.2303495

Batch 164310, train_perplexity=191.54881, train_loss=5.2551427

Batch 164320, train_perplexity=177.02095, train_loss=5.176268

Batch 164330, train_perplexity=197.464, train_loss=5.2855563

Batch 164340, train_perplexity=215.76912, train_loss=5.374209

Batch 164350, train_perplexity=198.57034, train_loss=5.2911434

Batch 164360, train_perplexity=185.60307, train_loss=5.2236104

Batch 164370, train_perplexity=208.5365, train_loss=5.340114

Batch 164380, train_perplexity=206.71959, train_loss=5.331363

Batch 164390, train_perplexity=200.14589, train_loss=5.2990465
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 164400, train_perplexity=186.37386, train_loss=5.2277546

Batch 164410, train_perplexity=202.92824, train_loss=5.3128524

Batch 164420, train_perplexity=187.19164, train_loss=5.232133

Batch 164430, train_perplexity=197.94159, train_loss=5.287972

Batch 164440, train_perplexity=195.98424, train_loss=5.278034

Batch 164450, train_perplexity=193.53697, train_loss=5.2654686

Batch 164460, train_perplexity=182.89748, train_loss=5.2089257

Batch 164470, train_perplexity=188.4789, train_loss=5.238986

Batch 164480, train_perplexity=209.68929, train_loss=5.345627

Batch 164490, train_perplexity=194.81834, train_loss=5.2720675

Batch 164500, train_perplexity=221.39789, train_loss=5.3999615

Batch 164510, train_perplexity=180.84312, train_loss=5.19763

Batch 164520, train_perplexity=190.52592, train_loss=5.2497883

Batch 164530, train_perplexity=188.14827, train_loss=5.2372303

Batch 164540, train_perplexity=175.94724, train_loss=5.170184

Batch 164550, train_perplexity=194.66168, train_loss=5.271263

Batch 164560, train_perplexity=196.39455, train_loss=5.2801256

Batch 164570, train_perplexity=202.1827, train_loss=5.3091717

Batch 164580, train_perplexity=229.92084, train_loss=5.437735

Batch 164590, train_perplexity=214.41742, train_loss=5.3679247

Batch 164600, train_perplexity=200.62698, train_loss=5.3014474

Batch 164610, train_perplexity=213.17877, train_loss=5.362131

Batch 164620, train_perplexity=185.08368, train_loss=5.220808

Batch 164630, train_perplexity=202.06375, train_loss=5.3085833

Batch 164640, train_perplexity=205.98604, train_loss=5.3278084

Batch 164650, train_perplexity=203.23297, train_loss=5.314353

Batch 164660, train_perplexity=197.80241, train_loss=5.2872686

Batch 164670, train_perplexity=179.28458, train_loss=5.1889744

Batch 164680, train_perplexity=204.99306, train_loss=5.322976

Batch 164690, train_perplexity=198.18903, train_loss=5.2892213

Batch 164700, train_perplexity=216.8571, train_loss=5.3792386

Batch 164710, train_perplexity=198.71507, train_loss=5.291872

Batch 164720, train_perplexity=185.67947, train_loss=5.224022

Batch 164730, train_perplexity=222.12572, train_loss=5.4032435

Batch 164740, train_perplexity=206.01541, train_loss=5.327951

Batch 164750, train_perplexity=196.95282, train_loss=5.282964

Batch 164760, train_perplexity=198.13187, train_loss=5.288933

Batch 164770, train_perplexity=198.88495, train_loss=5.2927265

Batch 164780, train_perplexity=178.62437, train_loss=5.185285

Batch 164790, train_perplexity=208.48839, train_loss=5.3398833

Batch 164800, train_perplexity=205.22809, train_loss=5.324122

Batch 164810, train_perplexity=214.35036, train_loss=5.367612

Batch 164820, train_perplexity=202.34761, train_loss=5.309987

Batch 164830, train_perplexity=210.31474, train_loss=5.348605

Batch 164840, train_perplexity=227.38277, train_loss=5.426635

Batch 164850, train_perplexity=187.86856, train_loss=5.2357426

Batch 164860, train_perplexity=211.70395, train_loss=5.355189

Batch 164870, train_perplexity=202.5007, train_loss=5.3107433

Batch 164880, train_perplexity=201.56123, train_loss=5.306093

Batch 164890, train_perplexity=169.1897, train_loss=5.1310205

Batch 164900, train_perplexity=185.04097, train_loss=5.2205772

Batch 164910, train_perplexity=203.4527, train_loss=5.3154335

Batch 164920, train_perplexity=195.9307, train_loss=5.277761

Batch 164930, train_perplexity=198.79033, train_loss=5.2922506

Batch 164940, train_perplexity=200.17882, train_loss=5.299211

Batch 164950, train_perplexity=194.00154, train_loss=5.267866

Batch 164960, train_perplexity=202.79126, train_loss=5.312177

Batch 164970, train_perplexity=195.49683, train_loss=5.275544

Batch 164980, train_perplexity=200.52321, train_loss=5.30093

Batch 164990, train_perplexity=199.15588, train_loss=5.294088

Batch 165000, train_perplexity=179.73671, train_loss=5.191493

Batch 165010, train_perplexity=204.43294, train_loss=5.32024

Batch 165020, train_perplexity=199.2067, train_loss=5.294343

Batch 165030, train_perplexity=214.0658, train_loss=5.3662834

Batch 165040, train_perplexity=191.35263, train_loss=5.254118

Batch 165050, train_perplexity=199.83043, train_loss=5.297469

Batch 165060, train_perplexity=227.41248, train_loss=5.4267654

Batch 165070, train_perplexity=219.71098, train_loss=5.392313

Batch 165080, train_perplexity=197.60219, train_loss=5.286256

Batch 165090, train_perplexity=235.22482, train_loss=5.4605417

Batch 165100, train_perplexity=191.84818, train_loss=5.2567043

Batch 165110, train_perplexity=185.9256, train_loss=5.2253466

Batch 165120, train_perplexity=186.67001, train_loss=5.2293425

Batch 165130, train_perplexity=190.47433, train_loss=5.2495174

Batch 165140, train_perplexity=194.44943, train_loss=5.270172

Batch 165150, train_perplexity=186.06802, train_loss=5.2261124

Batch 165160, train_perplexity=211.70648, train_loss=5.355201

Batch 165170, train_perplexity=221.16597, train_loss=5.3989134

Batch 165180, train_perplexity=224.7997, train_loss=5.41521

Batch 165190, train_perplexity=195.16599, train_loss=5.2738504

Batch 165200, train_perplexity=199.14857, train_loss=5.294051

Batch 165210, train_perplexity=176.17776, train_loss=5.1714935

Batch 165220, train_perplexity=176.09445, train_loss=5.1710205

Batch 165230, train_perplexity=212.35962, train_loss=5.358281

Batch 165240, train_perplexity=191.04448, train_loss=5.2525063

Batch 165250, train_perplexity=198.56201, train_loss=5.2911015

Batch 165260, train_perplexity=182.19737, train_loss=5.2050905

Batch 165270, train_perplexity=223.55373, train_loss=5.4096518

Batch 165280, train_perplexity=197.92432, train_loss=5.2878847

Batch 165290, train_perplexity=215.53148, train_loss=5.373107

Batch 165300, train_perplexity=206.37103, train_loss=5.3296757

Batch 165310, train_perplexity=197.39839, train_loss=5.285224

Batch 165320, train_perplexity=200.91649, train_loss=5.3028893

Batch 165330, train_perplexity=172.3645, train_loss=5.1496115

Batch 165340, train_perplexity=189.4383, train_loss=5.2440634

Batch 165350, train_perplexity=198.0022, train_loss=5.288278

Batch 165360, train_perplexity=189.49161, train_loss=5.2443447

Batch 165370, train_perplexity=212.42949, train_loss=5.35861

Batch 165380, train_perplexity=209.77289, train_loss=5.3460255

Batch 165390, train_perplexity=214.44237, train_loss=5.368041

Batch 165400, train_perplexity=216.26724, train_loss=5.376515

Batch 165410, train_perplexity=201.2829, train_loss=5.3047113

Batch 165420, train_perplexity=195.1807, train_loss=5.273926

Batch 165430, train_perplexity=183.02092, train_loss=5.2096004

Batch 165440, train_perplexity=201.24019, train_loss=5.304499

Batch 165450, train_perplexity=201.74026, train_loss=5.306981

Batch 165460, train_perplexity=189.90027, train_loss=5.246499

Batch 165470, train_perplexity=197.76828, train_loss=5.287096

Batch 165480, train_perplexity=159.71043, train_loss=5.0733624

Batch 165490, train_perplexity=209.00548, train_loss=5.3423605

Batch 165500, train_perplexity=206.31987, train_loss=5.3294277

Batch 165510, train_perplexity=208.22401, train_loss=5.3386145

Batch 165520, train_perplexity=184.6174, train_loss=5.2182856

Batch 165530, train_perplexity=197.46146, train_loss=5.2855434

Batch 165540, train_perplexity=209.76389, train_loss=5.3459826

Batch 165550, train_perplexity=192.26936, train_loss=5.2588973

Batch 165560, train_perplexity=197.70906, train_loss=5.2867966

Batch 165570, train_perplexity=186.90445, train_loss=5.2305975

Batch 165580, train_perplexity=225.38445, train_loss=5.4178076

Batch 165590, train_perplexity=194.96118, train_loss=5.2728004

Batch 165600, train_perplexity=198.41739, train_loss=5.290373

Batch 165610, train_perplexity=197.20816, train_loss=5.28426

Batch 165620, train_perplexity=178.17181, train_loss=5.1827483

Batch 165630, train_perplexity=201.72171, train_loss=5.306889

Batch 165640, train_perplexity=181.46255, train_loss=5.2010493

Batch 165650, train_perplexity=198.15143, train_loss=5.2890315

Batch 165660, train_perplexity=188.85278, train_loss=5.2409678

Batch 165670, train_perplexity=199.95015, train_loss=5.298068

Batch 165680, train_perplexity=185.41307, train_loss=5.222586

Batch 165690, train_perplexity=204.33861, train_loss=5.3197784
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 165700, train_perplexity=193.16386, train_loss=5.263539

Batch 165710, train_perplexity=185.44446, train_loss=5.2227554

Batch 165720, train_perplexity=196.42227, train_loss=5.280267

Batch 165730, train_perplexity=194.56935, train_loss=5.2707887

Batch 165740, train_perplexity=196.11595, train_loss=5.278706

Batch 165750, train_perplexity=210.70682, train_loss=5.3504677

Batch 165760, train_perplexity=211.12682, train_loss=5.352459

Batch 165770, train_perplexity=190.62589, train_loss=5.250313

Batch 165780, train_perplexity=198.06007, train_loss=5.2885704

Batch 165790, train_perplexity=191.02133, train_loss=5.252385

Batch 165800, train_perplexity=188.02666, train_loss=5.2365837

Batch 165810, train_perplexity=230.39912, train_loss=5.439813

Batch 165820, train_perplexity=204.3083, train_loss=5.31963

Batch 165830, train_perplexity=183.22324, train_loss=5.2107053

Batch 165840, train_perplexity=187.64294, train_loss=5.234541

Batch 165850, train_perplexity=180.474, train_loss=5.1955867

Batch 165860, train_perplexity=204.19406, train_loss=5.319071

Batch 165870, train_perplexity=193.50209, train_loss=5.2652884

Batch 165880, train_perplexity=191.98828, train_loss=5.2574344

Batch 165890, train_perplexity=192.63083, train_loss=5.2607756

Batch 165900, train_perplexity=178.10683, train_loss=5.1823835

Batch 165910, train_perplexity=224.01535, train_loss=5.4117146

Batch 165920, train_perplexity=195.60686, train_loss=5.276107

Batch 165930, train_perplexity=223.6612, train_loss=5.4101324

Batch 165940, train_perplexity=219.06657, train_loss=5.3893757

Batch 165950, train_perplexity=188.21628, train_loss=5.2375917

Batch 165960, train_perplexity=201.2138, train_loss=5.304368

Batch 165970, train_perplexity=189.97626, train_loss=5.246899

Batch 165980, train_perplexity=181.36143, train_loss=5.200492

Batch 165990, train_perplexity=181.95001, train_loss=5.203732

Batch 166000, train_perplexity=198.50237, train_loss=5.290801

Batch 166010, train_perplexity=194.97234, train_loss=5.2728577

Batch 166020, train_perplexity=190.96906, train_loss=5.2521114

Batch 166030, train_perplexity=208.31995, train_loss=5.339075

Batch 166040, train_perplexity=192.85655, train_loss=5.2619467

Batch 166050, train_perplexity=205.60078, train_loss=5.3259363

Batch 166060, train_perplexity=198.69745, train_loss=5.2917833

Batch 166070, train_perplexity=206.35568, train_loss=5.3296013

Batch 166080, train_perplexity=218.74884, train_loss=5.387924

Batch 166090, train_perplexity=198.83167, train_loss=5.2924585

Batch 166100, train_perplexity=182.65823, train_loss=5.207617

Batch 166110, train_perplexity=188.35374, train_loss=5.238322

Batch 166120, train_perplexity=187.97852, train_loss=5.2363276

Batch 166130, train_perplexity=199.22571, train_loss=5.2944384

Batch 166140, train_perplexity=203.3557, train_loss=5.3149567

Batch 166150, train_perplexity=204.93637, train_loss=5.3226995

Batch 166160, train_perplexity=204.0326, train_loss=5.3182797

Batch 166170, train_perplexity=209.55675, train_loss=5.3449945

Batch 166180, train_perplexity=197.26233, train_loss=5.2845345

Batch 166190, train_perplexity=199.18828, train_loss=5.2942505

Batch 166200, train_perplexity=189.93994, train_loss=5.246708

Batch 166210, train_perplexity=219.13824, train_loss=5.389703

Batch 166220, train_perplexity=187.27449, train_loss=5.2325754

Batch 166230, train_perplexity=183.91518, train_loss=5.2144747

Batch 166240, train_perplexity=194.7901, train_loss=5.2719226

Batch 166250, train_perplexity=181.4334, train_loss=5.2008886

Batch 166260, train_perplexity=217.6825, train_loss=5.3830376

Batch 166270, train_perplexity=210.46492, train_loss=5.349319

Batch 166280, train_perplexity=180.04482, train_loss=5.193206

Batch 166290, train_perplexity=197.70642, train_loss=5.286783

Batch 166300, train_perplexity=194.79094, train_loss=5.271927

Batch 166310, train_perplexity=185.99077, train_loss=5.225697

Batch 166320, train_perplexity=196.06068, train_loss=5.2784243

Batch 166330, train_perplexity=183.96184, train_loss=5.2147284

Batch 166340, train_perplexity=195.85904, train_loss=5.2773952

Batch 166350, train_perplexity=195.97144, train_loss=5.277969

Batch 166360, train_perplexity=193.01736, train_loss=5.26278

Batch 166370, train_perplexity=215.34718, train_loss=5.3722515

Batch 166380, train_perplexity=199.85149, train_loss=5.2975745

Batch 166390, train_perplexity=188.70811, train_loss=5.2402015

Batch 166400, train_perplexity=205.71326, train_loss=5.3264832

Batch 166410, train_perplexity=194.60489, train_loss=5.2709713

Batch 166420, train_perplexity=192.63432, train_loss=5.2607937

Batch 166430, train_perplexity=189.26639, train_loss=5.2431555

Batch 166440, train_perplexity=177.02222, train_loss=5.1762753

Batch 166450, train_perplexity=169.99513, train_loss=5.13577

Batch 166460, train_perplexity=193.65292, train_loss=5.2660675

Batch 166470, train_perplexity=191.68358, train_loss=5.255846

Batch 166480, train_perplexity=189.83038, train_loss=5.246131

Batch 166490, train_perplexity=204.49495, train_loss=5.3205433

Batch 166500, train_perplexity=177.31114, train_loss=5.177906

Batch 166510, train_perplexity=185.96655, train_loss=5.225567

Batch 166520, train_perplexity=204.56497, train_loss=5.3208857

Batch 166530, train_perplexity=227.98772, train_loss=5.4292917

Batch 166540, train_perplexity=197.30014, train_loss=5.284726

Batch 166550, train_perplexity=195.8994, train_loss=5.2776012

Batch 166560, train_perplexity=212.1487, train_loss=5.3572874

Batch 166570, train_perplexity=199.8571, train_loss=5.2976027

Batch 166580, train_perplexity=194.65045, train_loss=5.2712054

Batch 166590, train_perplexity=187.47284, train_loss=5.233634

Batch 166600, train_perplexity=189.87384, train_loss=5.24636

Batch 166610, train_perplexity=184.28775, train_loss=5.2164984

Batch 166620, train_perplexity=212.26445, train_loss=5.357833

Batch 166630, train_perplexity=202.93694, train_loss=5.3128953

Batch 166640, train_perplexity=170.37524, train_loss=5.1380033

Batch 166650, train_perplexity=176.42947, train_loss=5.172921

Batch 166660, train_perplexity=211.72202, train_loss=5.355274

Batch 166670, train_perplexity=203.32875, train_loss=5.314824

Batch 166680, train_perplexity=181.56859, train_loss=5.2016335

Batch 166690, train_perplexity=186.5571, train_loss=5.2287374

Batch 166700, train_perplexity=192.06218, train_loss=5.257819

Batch 166710, train_perplexity=186.09439, train_loss=5.226254

Batch 166720, train_perplexity=187.41992, train_loss=5.2333517

Batch 166730, train_perplexity=200.93785, train_loss=5.3029957

Batch 166740, train_perplexity=183.77843, train_loss=5.213731

Batch 166750, train_perplexity=193.2876, train_loss=5.264179

Batch 166760, train_perplexity=194.06363, train_loss=5.268186

Batch 166770, train_perplexity=197.08182, train_loss=5.283619

Batch 166780, train_perplexity=175.31393, train_loss=5.1665783

Batch 166790, train_perplexity=209.06947, train_loss=5.3426666

Batch 166800, train_perplexity=216.64738, train_loss=5.378271

Batch 166810, train_perplexity=176.98424, train_loss=5.1760607

Batch 166820, train_perplexity=221.61589, train_loss=5.4009457

Batch 166830, train_perplexity=215.94481, train_loss=5.375023

Batch 166840, train_perplexity=187.98935, train_loss=5.2363853

Batch 166850, train_perplexity=187.55806, train_loss=5.2340884

Batch 166860, train_perplexity=192.904, train_loss=5.2621927

Batch 166870, train_perplexity=203.8347, train_loss=5.3173094

Batch 166880, train_perplexity=203.6437, train_loss=5.316372

Batch 166890, train_perplexity=185.51274, train_loss=5.2231236

Batch 166900, train_perplexity=193.04987, train_loss=5.2629485

Batch 166910, train_perplexity=203.34601, train_loss=5.314909

Batch 166920, train_perplexity=191.19667, train_loss=5.2533026

Batch 166930, train_perplexity=189.82169, train_loss=5.246085

Batch 166940, train_perplexity=189.84288, train_loss=5.2461967

Batch 166950, train_perplexity=187.70201, train_loss=5.2348557

Batch 166960, train_perplexity=215.3131, train_loss=5.372093

Batch 166970, train_perplexity=190.66406, train_loss=5.250513

Batch 166980, train_perplexity=175.60828, train_loss=5.168256

Batch 166990, train_perplexity=200.6378, train_loss=5.3015013
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 167000, train_perplexity=180.2075, train_loss=5.194109

Batch 167010, train_perplexity=186.14702, train_loss=5.2265368

Batch 167020, train_perplexity=213.33568, train_loss=5.362867

Batch 167030, train_perplexity=195.3129, train_loss=5.274603

Batch 167040, train_perplexity=194.6381, train_loss=5.271142

Batch 167050, train_perplexity=194.84009, train_loss=5.272179

Batch 167060, train_perplexity=199.47874, train_loss=5.2957077

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00079-of-00100
Loaded 305931 sentences.
Finished loading
Batch 167070, train_perplexity=205.4769, train_loss=5.3253336

Batch 167080, train_perplexity=186.9606, train_loss=5.230898

Batch 167090, train_perplexity=200.29941, train_loss=5.2998133

Batch 167100, train_perplexity=200.72296, train_loss=5.3019257

Batch 167110, train_perplexity=203.97559, train_loss=5.3180003

Batch 167120, train_perplexity=193.43024, train_loss=5.264917

Batch 167130, train_perplexity=209.54755, train_loss=5.3449507

Batch 167140, train_perplexity=183.6591, train_loss=5.2130814

Batch 167150, train_perplexity=206.2214, train_loss=5.3289504

Batch 167160, train_perplexity=194.17249, train_loss=5.268747

Batch 167170, train_perplexity=193.43106, train_loss=5.264921

Batch 167180, train_perplexity=200.1331, train_loss=5.2989826

Batch 167190, train_perplexity=190.75418, train_loss=5.2509856

Batch 167200, train_perplexity=223.60448, train_loss=5.4098787

Batch 167210, train_perplexity=184.51407, train_loss=5.2177258

Batch 167220, train_perplexity=212.15659, train_loss=5.3573246

Batch 167230, train_perplexity=182.18773, train_loss=5.2050376

Batch 167240, train_perplexity=184.08662, train_loss=5.2154064

Batch 167250, train_perplexity=195.04765, train_loss=5.273244

Batch 167260, train_perplexity=200.26389, train_loss=5.299636

Batch 167270, train_perplexity=194.84036, train_loss=5.2721806

Batch 167280, train_perplexity=186.67625, train_loss=5.229376

Batch 167290, train_perplexity=183.21188, train_loss=5.2106433

Batch 167300, train_perplexity=186.77304, train_loss=5.229894

Batch 167310, train_perplexity=209.40941, train_loss=5.344291

Batch 167320, train_perplexity=213.4817, train_loss=5.363551

Batch 167330, train_perplexity=182.87698, train_loss=5.2088137

Batch 167340, train_perplexity=193.4663, train_loss=5.2651033

Batch 167350, train_perplexity=209.91318, train_loss=5.346694

Batch 167360, train_perplexity=209.30768, train_loss=5.3438053

Batch 167370, train_perplexity=197.71452, train_loss=5.286824

Batch 167380, train_perplexity=202.39238, train_loss=5.3102083

Batch 167390, train_perplexity=202.17671, train_loss=5.309142

Batch 167400, train_perplexity=197.18729, train_loss=5.284154

Batch 167410, train_perplexity=188.54756, train_loss=5.2393503

Batch 167420, train_perplexity=195.31848, train_loss=5.2746315

Batch 167430, train_perplexity=187.89812, train_loss=5.2359

Batch 167440, train_perplexity=213.54625, train_loss=5.3638535

Batch 167450, train_perplexity=199.27682, train_loss=5.294695

Batch 167460, train_perplexity=210.49472, train_loss=5.3494606

Batch 167470, train_perplexity=185.33395, train_loss=5.2221594

Batch 167480, train_perplexity=185.8741, train_loss=5.2250695

Batch 167490, train_perplexity=191.08711, train_loss=5.2527294

Batch 167500, train_perplexity=178.51988, train_loss=5.1847

Batch 167510, train_perplexity=181.025, train_loss=5.198635

Batch 167520, train_perplexity=231.7848, train_loss=5.4458094

Batch 167530, train_perplexity=192.43033, train_loss=5.259734

Batch 167540, train_perplexity=193.21149, train_loss=5.2637854

Batch 167550, train_perplexity=192.59447, train_loss=5.2605867

Batch 167560, train_perplexity=189.99048, train_loss=5.246974

Batch 167570, train_perplexity=181.99974, train_loss=5.2040052

Batch 167580, train_perplexity=186.93315, train_loss=5.230751

Batch 167590, train_perplexity=190.61679, train_loss=5.250265

Batch 167600, train_perplexity=184.64081, train_loss=5.2184124

Batch 167610, train_perplexity=188.72873, train_loss=5.2403107

Batch 167620, train_perplexity=159.40686, train_loss=5.07146

Batch 167630, train_perplexity=199.31313, train_loss=5.294877

Batch 167640, train_perplexity=213.4192, train_loss=5.3632584

Batch 167650, train_perplexity=201.21466, train_loss=5.3043723

Batch 167660, train_perplexity=198.17079, train_loss=5.2891293

Batch 167670, train_perplexity=183.5747, train_loss=5.2126217

Batch 167680, train_perplexity=212.48451, train_loss=5.358869

Batch 167690, train_perplexity=193.96594, train_loss=5.2676826

Batch 167700, train_perplexity=180.34091, train_loss=5.194849

Batch 167710, train_perplexity=226.13089, train_loss=5.421114

Batch 167720, train_perplexity=201.6714, train_loss=5.3066397

Batch 167730, train_perplexity=192.79349, train_loss=5.2616196

Batch 167740, train_perplexity=199.11914, train_loss=5.2939034

Batch 167750, train_perplexity=200.86324, train_loss=5.302624

Batch 167760, train_perplexity=185.93233, train_loss=5.225383

Batch 167770, train_perplexity=203.40283, train_loss=5.3151884

Batch 167780, train_perplexity=216.15395, train_loss=5.375991

Batch 167790, train_perplexity=189.59735, train_loss=5.2449026

Batch 167800, train_perplexity=174.96494, train_loss=5.1645856

Batch 167810, train_perplexity=198.12808, train_loss=5.2889137

Batch 167820, train_perplexity=215.86905, train_loss=5.374672

Batch 167830, train_perplexity=202.10037, train_loss=5.3087645

Batch 167840, train_perplexity=205.64098, train_loss=5.326132

Batch 167850, train_perplexity=187.11017, train_loss=5.2316976

Batch 167860, train_perplexity=192.21097, train_loss=5.2585936

Batch 167870, train_perplexity=168.30539, train_loss=5.12578

Batch 167880, train_perplexity=214.49554, train_loss=5.368289

Batch 167890, train_perplexity=177.29068, train_loss=5.1777906

Batch 167900, train_perplexity=201.04768, train_loss=5.303542

Batch 167910, train_perplexity=198.56987, train_loss=5.291141

Batch 167920, train_perplexity=217.49045, train_loss=5.382155

Batch 167930, train_perplexity=203.66216, train_loss=5.3164625

Batch 167940, train_perplexity=194.18036, train_loss=5.2687874

Batch 167950, train_perplexity=195.73506, train_loss=5.276762

Batch 167960, train_perplexity=195.21458, train_loss=5.2740993

Batch 167970, train_perplexity=187.00981, train_loss=5.231161

Batch 167980, train_perplexity=221.48595, train_loss=5.400359

Batch 167990, train_perplexity=196.61737, train_loss=5.2812595

Batch 168000, train_perplexity=206.51279, train_loss=5.3303623

Batch 168010, train_perplexity=187.10632, train_loss=5.231677

Batch 168020, train_perplexity=213.81342, train_loss=5.3651037

Batch 168030, train_perplexity=185.94687, train_loss=5.225461

Batch 168040, train_perplexity=178.58714, train_loss=5.1850767

Batch 168050, train_perplexity=166.56712, train_loss=5.1153984

Batch 168060, train_perplexity=175.57814, train_loss=5.168084

Batch 168070, train_perplexity=195.7444, train_loss=5.2768097

Batch 168080, train_perplexity=209.8083, train_loss=5.3461943

Batch 168090, train_perplexity=217.80367, train_loss=5.383594

Batch 168100, train_perplexity=194.79865, train_loss=5.2719665

Batch 168110, train_perplexity=184.28677, train_loss=5.216493

Batch 168120, train_perplexity=193.9221, train_loss=5.2674565

Batch 168130, train_perplexity=190.2317, train_loss=5.248243

Batch 168140, train_perplexity=189.02792, train_loss=5.2418947

Batch 168150, train_perplexity=211.43288, train_loss=5.3539076

Batch 168160, train_perplexity=194.43181, train_loss=5.2700815

Batch 168170, train_perplexity=206.33502, train_loss=5.329501

Batch 168180, train_perplexity=179.22185, train_loss=5.1886244

Batch 168190, train_perplexity=215.91043, train_loss=5.3748636

Batch 168200, train_perplexity=184.8807, train_loss=5.219711

Batch 168210, train_perplexity=178.21387, train_loss=5.1829844

Batch 168220, train_perplexity=197.29102, train_loss=5.28468

Batch 168230, train_perplexity=201.7119, train_loss=5.3068404
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 168240, train_perplexity=180.75708, train_loss=5.197154

Batch 168250, train_perplexity=213.73105, train_loss=5.3647184

Batch 168260, train_perplexity=188.05354, train_loss=5.2367268

Batch 168270, train_perplexity=198.3733, train_loss=5.2901506

Batch 168280, train_perplexity=212.81598, train_loss=5.360428

Batch 168290, train_perplexity=200.8775, train_loss=5.3026953

Batch 168300, train_perplexity=210.61943, train_loss=5.350053

Batch 168310, train_perplexity=194.66948, train_loss=5.271303

Batch 168320, train_perplexity=203.00508, train_loss=5.313231

Batch 168330, train_perplexity=209.00539, train_loss=5.34236

Batch 168340, train_perplexity=189.61905, train_loss=5.245017

Batch 168350, train_perplexity=184.24364, train_loss=5.216259

Batch 168360, train_perplexity=193.7427, train_loss=5.266531

Batch 168370, train_perplexity=218.1891, train_loss=5.385362

Batch 168380, train_perplexity=187.47446, train_loss=5.2336426

Batch 168390, train_perplexity=190.8802, train_loss=5.251646

Batch 168400, train_perplexity=202.93878, train_loss=5.3129044

Batch 168410, train_perplexity=198.19923, train_loss=5.289273

Batch 168420, train_perplexity=207.32777, train_loss=5.334301

Batch 168430, train_perplexity=197.97057, train_loss=5.2881184

Batch 168440, train_perplexity=192.52037, train_loss=5.260202

Batch 168450, train_perplexity=195.2779, train_loss=5.2744236

Batch 168460, train_perplexity=177.97914, train_loss=5.1816664

Batch 168470, train_perplexity=181.70828, train_loss=5.2024026

Batch 168480, train_perplexity=191.21555, train_loss=5.2534013

Batch 168490, train_perplexity=202.3974, train_loss=5.310233

Batch 168500, train_perplexity=199.09103, train_loss=5.293762

Batch 168510, train_perplexity=196.23297, train_loss=5.2793026

Batch 168520, train_perplexity=199.23416, train_loss=5.294481

Batch 168530, train_perplexity=189.95715, train_loss=5.2467985

Batch 168540, train_perplexity=207.03021, train_loss=5.3328648

Batch 168550, train_perplexity=184.19972, train_loss=5.2160206

Batch 168560, train_perplexity=207.43675, train_loss=5.3348265

Batch 168570, train_perplexity=217.6303, train_loss=5.3827977

Batch 168580, train_perplexity=179.83804, train_loss=5.1920567

Batch 168590, train_perplexity=200.99765, train_loss=5.303293

Batch 168600, train_perplexity=183.61847, train_loss=5.21286

Batch 168610, train_perplexity=211.20555, train_loss=5.352832

Batch 168620, train_perplexity=211.0931, train_loss=5.352299

Batch 168630, train_perplexity=223.21329, train_loss=5.408128

Batch 168640, train_perplexity=185.0093, train_loss=5.220406

Batch 168650, train_perplexity=171.53966, train_loss=5.1448145

Batch 168660, train_perplexity=201.12996, train_loss=5.3039513

Batch 168670, train_perplexity=190.24713, train_loss=5.248324

Batch 168680, train_perplexity=212.26445, train_loss=5.357833

Batch 168690, train_perplexity=183.65744, train_loss=5.2130723

Batch 168700, train_perplexity=186.04585, train_loss=5.225993

Batch 168710, train_perplexity=206.21217, train_loss=5.3289056

Batch 168720, train_perplexity=231.06024, train_loss=5.4426785

Batch 168730, train_perplexity=193.97453, train_loss=5.267727

Batch 168740, train_perplexity=170.55782, train_loss=5.1390743

Batch 168750, train_perplexity=194.7888, train_loss=5.271916

Batch 168760, train_perplexity=192.2733, train_loss=5.258918

Batch 168770, train_perplexity=196.58615, train_loss=5.2811007

Batch 168780, train_perplexity=199.27492, train_loss=5.2946854

Batch 168790, train_perplexity=190.33942, train_loss=5.248809

Batch 168800, train_perplexity=193.7524, train_loss=5.266581

Batch 168810, train_perplexity=218.34303, train_loss=5.3860674

Batch 168820, train_perplexity=217.57779, train_loss=5.3825564

Batch 168830, train_perplexity=170.0827, train_loss=5.136285

Batch 168840, train_perplexity=192.86722, train_loss=5.262002

Batch 168850, train_perplexity=195.00897, train_loss=5.2730455

Batch 168860, train_perplexity=209.07825, train_loss=5.3427086

Batch 168870, train_perplexity=196.6815, train_loss=5.2815857

Batch 168880, train_perplexity=175.66095, train_loss=5.1685557

Batch 168890, train_perplexity=200.7961, train_loss=5.30229

Batch 168900, train_perplexity=209.07825, train_loss=5.3427086

Batch 168910, train_perplexity=185.54689, train_loss=5.2233076

Batch 168920, train_perplexity=186.29477, train_loss=5.22733

Batch 168930, train_perplexity=211.21019, train_loss=5.352854

Batch 168940, train_perplexity=172.91235, train_loss=5.152785

Batch 168950, train_perplexity=222.14172, train_loss=5.4033155

Batch 168960, train_perplexity=190.8864, train_loss=5.2516785

Batch 168970, train_perplexity=178.51529, train_loss=5.1846743

Batch 168980, train_perplexity=189.39838, train_loss=5.2438526

Batch 168990, train_perplexity=188.8643, train_loss=5.241029

Batch 169000, train_perplexity=211.52565, train_loss=5.3543463

Batch 169010, train_perplexity=191.36595, train_loss=5.2541876

Batch 169020, train_perplexity=221.39388, train_loss=5.3999434

Batch 169030, train_perplexity=207.85141, train_loss=5.3368235

Batch 169040, train_perplexity=193.41907, train_loss=5.264859

Batch 169050, train_perplexity=200.38586, train_loss=5.300245

Batch 169060, train_perplexity=191.77968, train_loss=5.256347

Batch 169070, train_perplexity=192.09598, train_loss=5.257995

Batch 169080, train_perplexity=177.18343, train_loss=5.1771855

Batch 169090, train_perplexity=197.76488, train_loss=5.287079

Batch 169100, train_perplexity=200.30542, train_loss=5.2998433

Batch 169110, train_perplexity=192.13528, train_loss=5.2581997

Batch 169120, train_perplexity=172.62534, train_loss=5.1511235

Batch 169130, train_perplexity=212.73471, train_loss=5.360046

Batch 169140, train_perplexity=225.82895, train_loss=5.419778

Batch 169150, train_perplexity=193.41345, train_loss=5.26483

Batch 169160, train_perplexity=184.66187, train_loss=5.2185264

Batch 169170, train_perplexity=187.44441, train_loss=5.2334824

Batch 169180, train_perplexity=209.75468, train_loss=5.3459387

Batch 169190, train_perplexity=198.68002, train_loss=5.2916956

Batch 169200, train_perplexity=216.27829, train_loss=5.376566

Batch 169210, train_perplexity=183.96526, train_loss=5.214747

Batch 169220, train_perplexity=190.13513, train_loss=5.247735

Batch 169230, train_perplexity=188.66988, train_loss=5.239999

Batch 169240, train_perplexity=188.73196, train_loss=5.240328

Batch 169250, train_perplexity=203.49849, train_loss=5.3156586

Batch 169260, train_perplexity=191.10489, train_loss=5.2528224

Batch 169270, train_perplexity=191.76952, train_loss=5.2562943

Batch 169280, train_perplexity=197.36497, train_loss=5.2850547

Batch 169290, train_perplexity=195.68037, train_loss=5.2764826

Batch 169300, train_perplexity=207.67625, train_loss=5.3359804

Batch 169310, train_perplexity=189.7341, train_loss=5.2456236

Batch 169320, train_perplexity=194.87418, train_loss=5.272354

Batch 169330, train_perplexity=196.41066, train_loss=5.2802076

Batch 169340, train_perplexity=184.63914, train_loss=5.2184033

Batch 169350, train_perplexity=195.16878, train_loss=5.2738647

Batch 169360, train_perplexity=226.34082, train_loss=5.422042

Batch 169370, train_perplexity=208.96214, train_loss=5.342153

Batch 169380, train_perplexity=196.95386, train_loss=5.2829695

Batch 169390, train_perplexity=185.60794, train_loss=5.2236366

Batch 169400, train_perplexity=210.08871, train_loss=5.34753

Batch 169410, train_perplexity=197.48839, train_loss=5.28568

Batch 169420, train_perplexity=200.08386, train_loss=5.2987366

Batch 169430, train_perplexity=183.03366, train_loss=5.20967

Batch 169440, train_perplexity=205.24834, train_loss=5.3242207

Batch 169450, train_perplexity=185.47542, train_loss=5.2229223

Batch 169460, train_perplexity=192.93492, train_loss=5.262353

Batch 169470, train_perplexity=181.14804, train_loss=5.1993146

Batch 169480, train_perplexity=198.25604, train_loss=5.2895594

Batch 169490, train_perplexity=186.36229, train_loss=5.2276926

Batch 169500, train_perplexity=228.95972, train_loss=5.433546

Batch 169510, train_perplexity=205.77242, train_loss=5.326771

Batch 169520, train_perplexity=179.59158, train_loss=5.1906853

Batch 169530, train_perplexity=186.19939, train_loss=5.226818
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 169540, train_perplexity=207.99835, train_loss=5.33753

Batch 169550, train_perplexity=181.18459, train_loss=5.1995163

Batch 169560, train_perplexity=188.49075, train_loss=5.239049

Batch 169570, train_perplexity=185.91876, train_loss=5.22531

Batch 169580, train_perplexity=188.70352, train_loss=5.240177

Batch 169590, train_perplexity=180.78285, train_loss=5.1972966

Batch 169600, train_perplexity=197.24426, train_loss=5.284443

Batch 169610, train_perplexity=197.98096, train_loss=5.288171

Batch 169620, train_perplexity=185.82455, train_loss=5.224803

Batch 169630, train_perplexity=208.8192, train_loss=5.341469

Batch 169640, train_perplexity=186.59402, train_loss=5.2289352

Batch 169650, train_perplexity=188.28711, train_loss=5.237968

Batch 169660, train_perplexity=198.37123, train_loss=5.29014

Batch 169670, train_perplexity=218.32825, train_loss=5.3859997

Batch 169680, train_perplexity=169.18857, train_loss=5.131014

Batch 169690, train_perplexity=182.64186, train_loss=5.207527

Batch 169700, train_perplexity=193.15686, train_loss=5.2635026

Batch 169710, train_perplexity=194.94212, train_loss=5.2727027

Batch 169720, train_perplexity=201.98457, train_loss=5.3081913

Batch 169730, train_perplexity=188.36667, train_loss=5.2383904

Batch 169740, train_perplexity=187.06181, train_loss=5.231439

Batch 169750, train_perplexity=196.2706, train_loss=5.2794943

Batch 169760, train_perplexity=204.18588, train_loss=5.319031

Batch 169770, train_perplexity=208.1978, train_loss=5.3384886

Batch 169780, train_perplexity=178.8162, train_loss=5.1863585

Batch 169790, train_perplexity=196.58624, train_loss=5.281101

Batch 169800, train_perplexity=201.14616, train_loss=5.304032

Batch 169810, train_perplexity=184.00807, train_loss=5.2149796

Batch 169820, train_perplexity=194.79335, train_loss=5.2719393

Batch 169830, train_perplexity=194.9477, train_loss=5.2727313

Batch 169840, train_perplexity=206.82715, train_loss=5.3318834

Batch 169850, train_perplexity=214.26718, train_loss=5.3672237

Batch 169860, train_perplexity=193.84029, train_loss=5.2670345

Batch 169870, train_perplexity=199.21202, train_loss=5.2943697

Batch 169880, train_perplexity=187.0125, train_loss=5.2311754

Batch 169890, train_perplexity=190.05011, train_loss=5.2472878

Batch 169900, train_perplexity=185.66566, train_loss=5.2239475

Batch 169910, train_perplexity=183.59816, train_loss=5.2127495

Batch 169920, train_perplexity=214.69293, train_loss=5.369209

Batch 169930, train_perplexity=192.98424, train_loss=5.2626085

Batch 169940, train_perplexity=195.91948, train_loss=5.277704

Batch 169950, train_perplexity=190.93547, train_loss=5.2519355

Batch 169960, train_perplexity=202.5287, train_loss=5.3108816

Batch 169970, train_perplexity=197.11996, train_loss=5.2838125

Batch 169980, train_perplexity=198.92517, train_loss=5.2929287

Batch 169990, train_perplexity=190.57372, train_loss=5.250039

Batch 170000, train_perplexity=198.79573, train_loss=5.292278

Batch 170010, train_perplexity=190.64752, train_loss=5.2504263

Batch 170020, train_perplexity=177.40349, train_loss=5.1784267

Batch 170030, train_perplexity=206.03516, train_loss=5.328047

Batch 170040, train_perplexity=182.31087, train_loss=5.2057133

Batch 170050, train_perplexity=206.99783, train_loss=5.3327084

Batch 170060, train_perplexity=193.19518, train_loss=5.263701

Batch 170070, train_perplexity=190.80412, train_loss=5.2512474

Batch 170080, train_perplexity=194.6008, train_loss=5.2709503

Batch 170090, train_perplexity=186.64003, train_loss=5.229182

Batch 170100, train_perplexity=185.56229, train_loss=5.2233906

Batch 170110, train_perplexity=208.96872, train_loss=5.3421845

Batch 170120, train_perplexity=206.83228, train_loss=5.331908

Batch 170130, train_perplexity=179.21979, train_loss=5.188613

Batch 170140, train_perplexity=220.62877, train_loss=5.3964815

Batch 170150, train_perplexity=203.63963, train_loss=5.316352

Batch 170160, train_perplexity=176.34418, train_loss=5.1724377

Batch 170170, train_perplexity=209.4031, train_loss=5.344261

Batch 170180, train_perplexity=203.75122, train_loss=5.3169

Batch 170190, train_perplexity=194.72816, train_loss=5.2716045

Batch 170200, train_perplexity=167.6868, train_loss=5.122098

Batch 170210, train_perplexity=201.19145, train_loss=5.304257

Batch 170220, train_perplexity=188.69472, train_loss=5.2401304

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00048-of-00100
Loaded 305065 sentences.
Finished loading
Batch 170230, train_perplexity=213.97232, train_loss=5.3658466

Batch 170240, train_perplexity=191.78937, train_loss=5.2563977

Batch 170250, train_perplexity=214.28291, train_loss=5.367297

Batch 170260, train_perplexity=185.62636, train_loss=5.223736

Batch 170270, train_perplexity=197.87694, train_loss=5.2876453

Batch 170280, train_perplexity=210.19041, train_loss=5.348014

Batch 170290, train_perplexity=208.90764, train_loss=5.3418922

Batch 170300, train_perplexity=215.68991, train_loss=5.373842

Batch 170310, train_perplexity=200.36589, train_loss=5.300145

Batch 170320, train_perplexity=212.56528, train_loss=5.359249

Batch 170330, train_perplexity=218.43092, train_loss=5.38647

Batch 170340, train_perplexity=205.89, train_loss=5.327342

Batch 170350, train_perplexity=193.09424, train_loss=5.2631783

Batch 170360, train_perplexity=206.46375, train_loss=5.330125

Batch 170370, train_perplexity=194.7486, train_loss=5.2717094

Batch 170380, train_perplexity=187.96211, train_loss=5.2362404

Batch 170390, train_perplexity=203.63185, train_loss=5.3163137

Batch 170400, train_perplexity=206.67207, train_loss=5.3311334

Batch 170410, train_perplexity=201.86739, train_loss=5.307611

Batch 170420, train_perplexity=199.1501, train_loss=5.294059

Batch 170430, train_perplexity=209.74538, train_loss=5.3458943

Batch 170440, train_perplexity=202.05566, train_loss=5.308543

Batch 170450, train_perplexity=183.19275, train_loss=5.210539

Batch 170460, train_perplexity=195.23729, train_loss=5.2742157

Batch 170470, train_perplexity=181.3329, train_loss=5.2003345

Batch 170480, train_perplexity=216.02762, train_loss=5.3754063

Batch 170490, train_perplexity=210.25658, train_loss=5.3483286

Batch 170500, train_perplexity=189.6413, train_loss=5.2451344

Batch 170510, train_perplexity=187.07162, train_loss=5.2314916

Batch 170520, train_perplexity=188.3153, train_loss=5.2381177

Batch 170530, train_perplexity=201.82196, train_loss=5.307386

Batch 170540, train_perplexity=192.24075, train_loss=5.2587485

Batch 170550, train_perplexity=205.93408, train_loss=5.327556

Batch 170560, train_perplexity=200.31544, train_loss=5.2998934

Batch 170570, train_perplexity=208.49007, train_loss=5.3398914

Batch 170580, train_perplexity=209.98805, train_loss=5.3470507

Batch 170590, train_perplexity=194.28075, train_loss=5.2693043

Batch 170600, train_perplexity=197.51617, train_loss=5.2858205

Batch 170610, train_perplexity=211.30629, train_loss=5.3533087

Batch 170620, train_perplexity=178.14319, train_loss=5.1825876

Batch 170630, train_perplexity=186.51645, train_loss=5.2285194

Batch 170640, train_perplexity=196.15636, train_loss=5.278912

Batch 170650, train_perplexity=197.58061, train_loss=5.2861466

Batch 170660, train_perplexity=201.27252, train_loss=5.30466

Batch 170670, train_perplexity=203.42107, train_loss=5.315278

Batch 170680, train_perplexity=194.53716, train_loss=5.270623

Batch 170690, train_perplexity=170.6408, train_loss=5.1395607

Batch 170700, train_perplexity=184.46191, train_loss=5.217443

Batch 170710, train_perplexity=184.98627, train_loss=5.2202816

Batch 170720, train_perplexity=223.49489, train_loss=5.4093885

Batch 170730, train_perplexity=179.27295, train_loss=5.1889095

Batch 170740, train_perplexity=209.53116, train_loss=5.3448725

Batch 170750, train_perplexity=225.93796, train_loss=5.4202604

Batch 170760, train_perplexity=211.04901, train_loss=5.3520904
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 170770, train_perplexity=205.95264, train_loss=5.3276463

Batch 170780, train_perplexity=176.66383, train_loss=5.1742487

Batch 170790, train_perplexity=207.2319, train_loss=5.3338385

Batch 170800, train_perplexity=199.81613, train_loss=5.2973976

Batch 170810, train_perplexity=196.01517, train_loss=5.278192

Batch 170820, train_perplexity=205.472, train_loss=5.3253098

Batch 170830, train_perplexity=182.08455, train_loss=5.204471

Batch 170840, train_perplexity=180.96837, train_loss=5.1983223

Batch 170850, train_perplexity=181.36731, train_loss=5.2005243

Batch 170860, train_perplexity=181.93033, train_loss=5.203624

Batch 170870, train_perplexity=192.91266, train_loss=5.2622375

Batch 170880, train_perplexity=180.02731, train_loss=5.1931086

Batch 170890, train_perplexity=207.83298, train_loss=5.336735

Batch 170900, train_perplexity=188.4115, train_loss=5.2386284

Batch 170910, train_perplexity=198.5678, train_loss=5.2911305

Batch 170920, train_perplexity=179.9233, train_loss=5.1925306

Batch 170930, train_perplexity=192.60732, train_loss=5.2606535

Batch 170940, train_perplexity=188.32086, train_loss=5.2381473

Batch 170950, train_perplexity=223.75049, train_loss=5.4105315

Batch 170960, train_perplexity=191.33603, train_loss=5.254031

Batch 170970, train_perplexity=172.5943, train_loss=5.1509438

Batch 170980, train_perplexity=225.4384, train_loss=5.418047

Batch 170990, train_perplexity=211.78554, train_loss=5.355574

Batch 171000, train_perplexity=174.4843, train_loss=5.1618347

Batch 171010, train_perplexity=210.3353, train_loss=5.348703

Batch 171020, train_perplexity=188.58228, train_loss=5.2395344

Batch 171030, train_perplexity=190.07367, train_loss=5.2474117

Batch 171040, train_perplexity=221.2763, train_loss=5.399412

Batch 171050, train_perplexity=184.44406, train_loss=5.217346

Batch 171060, train_perplexity=200.33932, train_loss=5.3000126

Batch 171070, train_perplexity=199.71983, train_loss=5.2969155

Batch 171080, train_perplexity=202.95445, train_loss=5.3129816

Batch 171090, train_perplexity=182.58351, train_loss=5.2072077

Batch 171100, train_perplexity=204.66391, train_loss=5.321369

Batch 171110, train_perplexity=195.13129, train_loss=5.2736726

Batch 171120, train_perplexity=181.8084, train_loss=5.2029533

Batch 171130, train_perplexity=207.84338, train_loss=5.336785

Batch 171140, train_perplexity=212.37248, train_loss=5.3583417

Batch 171150, train_perplexity=181.43184, train_loss=5.20088

Batch 171160, train_perplexity=205.20372, train_loss=5.324003

Batch 171170, train_perplexity=207.51105, train_loss=5.3351846

Batch 171180, train_perplexity=204.34581, train_loss=5.3198137

Batch 171190, train_perplexity=212.01056, train_loss=5.356636

Batch 171200, train_perplexity=202.45822, train_loss=5.3105335

Batch 171210, train_perplexity=198.66136, train_loss=5.2916017

Batch 171220, train_perplexity=191.89584, train_loss=5.256953

Batch 171230, train_perplexity=170.36429, train_loss=5.137939

Batch 171240, train_perplexity=213.00664, train_loss=5.3613234

Batch 171250, train_perplexity=190.30493, train_loss=5.2486277

Batch 171260, train_perplexity=179.22603, train_loss=5.1886477

Batch 171270, train_perplexity=201.29019, train_loss=5.3047476

Batch 171280, train_perplexity=192.42116, train_loss=5.2596865

Batch 171290, train_perplexity=190.92563, train_loss=5.251884

Batch 171300, train_perplexity=196.2676, train_loss=5.279479

Batch 171310, train_perplexity=193.12968, train_loss=5.263362

Batch 171320, train_perplexity=203.86221, train_loss=5.3174443

Batch 171330, train_perplexity=203.54236, train_loss=5.315874

Batch 171340, train_perplexity=201.99228, train_loss=5.3082294

Batch 171350, train_perplexity=206.7315, train_loss=5.331421

Batch 171360, train_perplexity=201.3881, train_loss=5.305234

Batch 171370, train_perplexity=186.36282, train_loss=5.2276955

Batch 171380, train_perplexity=192.62532, train_loss=5.260747

Batch 171390, train_perplexity=194.01968, train_loss=5.2679596

Batch 171400, train_perplexity=193.8756, train_loss=5.2672167

Batch 171410, train_perplexity=190.4361, train_loss=5.2493167

Batch 171420, train_perplexity=194.6279, train_loss=5.2710896

Batch 171430, train_perplexity=194.09944, train_loss=5.2683706

Batch 171440, train_perplexity=174.72208, train_loss=5.1631966

Batch 171450, train_perplexity=198.02438, train_loss=5.28839

Batch 171460, train_perplexity=177.30302, train_loss=5.1778603

Batch 171470, train_perplexity=198.66913, train_loss=5.2916408

Batch 171480, train_perplexity=199.201, train_loss=5.2943144

Batch 171490, train_perplexity=212.65388, train_loss=5.359666

Batch 171500, train_perplexity=173.07559, train_loss=5.1537285

Batch 171510, train_perplexity=182.0769, train_loss=5.204429

Batch 171520, train_perplexity=178.38084, train_loss=5.183921

Batch 171530, train_perplexity=196.62355, train_loss=5.281291

Batch 171540, train_perplexity=196.7693, train_loss=5.282032

Batch 171550, train_perplexity=173.4368, train_loss=5.155813

Batch 171560, train_perplexity=187.49207, train_loss=5.2337365

Batch 171570, train_perplexity=197.1335, train_loss=5.283881

Batch 171580, train_perplexity=204.13701, train_loss=5.3187914

Batch 171590, train_perplexity=178.78142, train_loss=5.186164

Batch 171600, train_perplexity=193.8976, train_loss=5.26733

Batch 171610, train_perplexity=188.5589, train_loss=5.2394104

Batch 171620, train_perplexity=207.04167, train_loss=5.33292

Batch 171630, train_perplexity=187.81947, train_loss=5.2354813

Batch 171640, train_perplexity=191.89365, train_loss=5.2569413

Batch 171650, train_perplexity=178.8156, train_loss=5.186355

Batch 171660, train_perplexity=210.05266, train_loss=5.347358

Batch 171670, train_perplexity=181.23625, train_loss=5.1998014

Batch 171680, train_perplexity=188.60944, train_loss=5.2396784

Batch 171690, train_perplexity=195.75336, train_loss=5.2768555

Batch 171700, train_perplexity=210.47777, train_loss=5.34938

Batch 171710, train_perplexity=185.97871, train_loss=5.225632

Batch 171720, train_perplexity=218.62537, train_loss=5.3873596

Batch 171730, train_perplexity=221.01921, train_loss=5.3982496

Batch 171740, train_perplexity=181.03139, train_loss=5.1986704

Batch 171750, train_perplexity=182.72182, train_loss=5.207965

Batch 171760, train_perplexity=166.26163, train_loss=5.1135626

Batch 171770, train_perplexity=181.9619, train_loss=5.2037973

Batch 171780, train_perplexity=190.22853, train_loss=5.248226

Batch 171790, train_perplexity=187.6171, train_loss=5.234403

Batch 171800, train_perplexity=210.34041, train_loss=5.348727

Batch 171810, train_perplexity=165.7852, train_loss=5.110693

Batch 171820, train_perplexity=224.85309, train_loss=5.415447

Batch 171830, train_perplexity=237.03603, train_loss=5.468212

Batch 171840, train_perplexity=175.86655, train_loss=5.1697254

Batch 171850, train_perplexity=173.38089, train_loss=5.155491

Batch 171860, train_perplexity=192.75562, train_loss=5.261423

Batch 171870, train_perplexity=183.62767, train_loss=5.21291

Batch 171880, train_perplexity=186.84117, train_loss=5.230259

Batch 171890, train_perplexity=180.20337, train_loss=5.194086

Batch 171900, train_perplexity=181.04778, train_loss=5.198761

Batch 171910, train_perplexity=197.01088, train_loss=5.283259

Batch 171920, train_perplexity=184.0875, train_loss=5.215411

Batch 171930, train_perplexity=186.57071, train_loss=5.2288103

Batch 171940, train_perplexity=183.69572, train_loss=5.2132807

Batch 171950, train_perplexity=211.88614, train_loss=5.356049

Batch 171960, train_perplexity=179.61899, train_loss=5.190838

Batch 171970, train_perplexity=183.57033, train_loss=5.212598

Batch 171980, train_perplexity=197.1085, train_loss=5.2837543

Batch 171990, train_perplexity=204.56126, train_loss=5.3208675

Batch 172000, train_perplexity=200.13042, train_loss=5.2989693

Batch 172010, train_perplexity=187.8038, train_loss=5.235398

Batch 172020, train_perplexity=211.65782, train_loss=5.354971

Batch 172030, train_perplexity=204.11598, train_loss=5.3186884

Batch 172040, train_perplexity=175.3462, train_loss=5.1667624

Batch 172050, train_perplexity=199.92307, train_loss=5.2979326

Batch 172060, train_perplexity=180.75949, train_loss=5.1971674
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 172070, train_perplexity=185.98944, train_loss=5.22569

Batch 172080, train_perplexity=190.0802, train_loss=5.247446

Batch 172090, train_perplexity=180.62172, train_loss=5.196405

Batch 172100, train_perplexity=206.36256, train_loss=5.3296347

Batch 172110, train_perplexity=189.64708, train_loss=5.245165

Batch 172120, train_perplexity=188.27948, train_loss=5.2379274

Batch 172130, train_perplexity=183.30669, train_loss=5.2111607

Batch 172140, train_perplexity=202.53352, train_loss=5.3109055

Batch 172150, train_perplexity=190.8529, train_loss=5.251503

Batch 172160, train_perplexity=201.55527, train_loss=5.3060637

Batch 172170, train_perplexity=195.91454, train_loss=5.2776785

Batch 172180, train_perplexity=202.44818, train_loss=5.310484

Batch 172190, train_perplexity=188.5357, train_loss=5.2392874

Batch 172200, train_perplexity=196.43033, train_loss=5.280308

Batch 172210, train_perplexity=188.81117, train_loss=5.2407475

Batch 172220, train_perplexity=201.24115, train_loss=5.304504

Batch 172230, train_perplexity=186.57516, train_loss=5.228834

Batch 172240, train_perplexity=191.06015, train_loss=5.2525883

Batch 172250, train_perplexity=197.58353, train_loss=5.2861614

Batch 172260, train_perplexity=202.04005, train_loss=5.308466

Batch 172270, train_perplexity=198.94508, train_loss=5.293029

Batch 172280, train_perplexity=200.5389, train_loss=5.301008

Batch 172290, train_perplexity=195.74066, train_loss=5.2767906

Batch 172300, train_perplexity=177.06932, train_loss=5.1765413

Batch 172310, train_perplexity=202.39142, train_loss=5.3102036

Batch 172320, train_perplexity=185.4319, train_loss=5.2226877

Batch 172330, train_perplexity=226.03386, train_loss=5.420685

Batch 172340, train_perplexity=195.11984, train_loss=5.273614

Batch 172350, train_perplexity=196.7618, train_loss=5.281994

Batch 172360, train_perplexity=184.93008, train_loss=5.219978

Batch 172370, train_perplexity=200.57246, train_loss=5.3011756

Batch 172380, train_perplexity=199.55353, train_loss=5.2960825

Batch 172390, train_perplexity=179.30501, train_loss=5.1890883

Batch 172400, train_perplexity=167.51706, train_loss=5.121085

Batch 172410, train_perplexity=184.11156, train_loss=5.215542

Batch 172420, train_perplexity=180.16179, train_loss=5.1938553

Batch 172430, train_perplexity=190.72508, train_loss=5.250833

Batch 172440, train_perplexity=192.35922, train_loss=5.2593646

Batch 172450, train_perplexity=205.45476, train_loss=5.325226

Batch 172460, train_perplexity=196.24345, train_loss=5.279356

Batch 172470, train_perplexity=177.23244, train_loss=5.177462

Batch 172480, train_perplexity=188.3109, train_loss=5.2380943

Batch 172490, train_perplexity=193.32097, train_loss=5.264352

Batch 172500, train_perplexity=190.37553, train_loss=5.2489986

Batch 172510, train_perplexity=205.95992, train_loss=5.3276815

Batch 172520, train_perplexity=205.092, train_loss=5.3234587

Batch 172530, train_perplexity=187.03871, train_loss=5.2313156

Batch 172540, train_perplexity=204.82011, train_loss=5.322132

Batch 172550, train_perplexity=204.27693, train_loss=5.3194766

Batch 172560, train_perplexity=201.5031, train_loss=5.3058047

Batch 172570, train_perplexity=192.69992, train_loss=5.261134

Batch 172580, train_perplexity=207.29575, train_loss=5.3341465

Batch 172590, train_perplexity=187.81876, train_loss=5.2354774

Batch 172600, train_perplexity=208.65378, train_loss=5.3406763

Batch 172610, train_perplexity=189.44426, train_loss=5.244095

Batch 172620, train_perplexity=179.61282, train_loss=5.1908035

Batch 172630, train_perplexity=207.80127, train_loss=5.336582

Batch 172640, train_perplexity=216.11272, train_loss=5.3758

Batch 172650, train_perplexity=174.92714, train_loss=5.1643696

Batch 172660, train_perplexity=203.5175, train_loss=5.315752

Batch 172670, train_perplexity=205.2131, train_loss=5.324049

Batch 172680, train_perplexity=183.07181, train_loss=5.2098784

Batch 172690, train_perplexity=180.75226, train_loss=5.1971273

Batch 172700, train_perplexity=173.0114, train_loss=5.1533575

Batch 172710, train_perplexity=199.68564, train_loss=5.2967443

Batch 172720, train_perplexity=195.29585, train_loss=5.2745156

Batch 172730, train_perplexity=190.63898, train_loss=5.2503815

Batch 172740, train_perplexity=188.26395, train_loss=5.237845

Batch 172750, train_perplexity=210.04555, train_loss=5.3473244

Batch 172760, train_perplexity=220.30666, train_loss=5.3950205

Batch 172770, train_perplexity=194.88338, train_loss=5.2724013

Batch 172780, train_perplexity=197.00476, train_loss=5.283228

Batch 172790, train_perplexity=171.0924, train_loss=5.142204

Batch 172800, train_perplexity=186.43544, train_loss=5.228085

Batch 172810, train_perplexity=174.64711, train_loss=5.1627674

Batch 172820, train_perplexity=185.83936, train_loss=5.2248826

Batch 172830, train_perplexity=213.8389, train_loss=5.365223

Batch 172840, train_perplexity=179.93343, train_loss=5.192587

Batch 172850, train_perplexity=182.13152, train_loss=5.204729

Batch 172860, train_perplexity=199.22865, train_loss=5.294453

Batch 172870, train_perplexity=199.83786, train_loss=5.2975063

Batch 172880, train_perplexity=189.35168, train_loss=5.243606

Batch 172890, train_perplexity=199.17726, train_loss=5.294195

Batch 172900, train_perplexity=182.42809, train_loss=5.206356

Batch 172910, train_perplexity=170.47862, train_loss=5.13861

Batch 172920, train_perplexity=182.62096, train_loss=5.2074127

Batch 172930, train_perplexity=189.45311, train_loss=5.2441416

Batch 172940, train_perplexity=198.5764, train_loss=5.291174

Batch 172950, train_perplexity=208.62831, train_loss=5.340554

Batch 172960, train_perplexity=191.4333, train_loss=5.2545395

Batch 172970, train_perplexity=181.09924, train_loss=5.199045

Batch 172980, train_perplexity=184.67366, train_loss=5.2185903

Batch 172990, train_perplexity=222.53293, train_loss=5.405075

Batch 173000, train_perplexity=201.38148, train_loss=5.305201

Batch 173010, train_perplexity=188.95068, train_loss=5.241486

Batch 173020, train_perplexity=183.20227, train_loss=5.210591

Batch 173030, train_perplexity=194.00432, train_loss=5.2678804

Batch 173040, train_perplexity=203.84248, train_loss=5.3173475

Batch 173050, train_perplexity=206.36758, train_loss=5.329659

Batch 173060, train_perplexity=173.23082, train_loss=5.154625

Batch 173070, train_perplexity=217.4412, train_loss=5.3819284

Batch 173080, train_perplexity=164.50446, train_loss=5.1029377

Batch 173090, train_perplexity=190.78894, train_loss=5.251168

Batch 173100, train_perplexity=197.85619, train_loss=5.2875404

Batch 173110, train_perplexity=195.08783, train_loss=5.27345

Batch 173120, train_perplexity=190.78676, train_loss=5.2511563

Batch 173130, train_perplexity=192.84148, train_loss=5.2618685

Batch 173140, train_perplexity=174.46242, train_loss=5.1617093

Batch 173150, train_perplexity=187.50986, train_loss=5.2338314

Batch 173160, train_perplexity=196.79399, train_loss=5.2821574

Batch 173170, train_perplexity=184.85664, train_loss=5.2195807

Batch 173180, train_perplexity=233.73264, train_loss=5.454178

Batch 173190, train_perplexity=181.18666, train_loss=5.1995277

Batch 173200, train_perplexity=186.44283, train_loss=5.2281246

Batch 173210, train_perplexity=204.94438, train_loss=5.3227386

Batch 173220, train_perplexity=176.35654, train_loss=5.172508

Batch 173230, train_perplexity=193.05382, train_loss=5.262969

Batch 173240, train_perplexity=188.51538, train_loss=5.2391796

Batch 173250, train_perplexity=190.47379, train_loss=5.2495146

Batch 173260, train_perplexity=201.81186, train_loss=5.307336

Batch 173270, train_perplexity=192.99068, train_loss=5.262642

Batch 173280, train_perplexity=175.84877, train_loss=5.1696243

Batch 173290, train_perplexity=208.51791, train_loss=5.340025

Batch 173300, train_perplexity=228.9929, train_loss=5.433691

Batch 173310, train_perplexity=201.83302, train_loss=5.3074408

Batch 173320, train_perplexity=199.2523, train_loss=5.294572

Batch 173330, train_perplexity=191.86484, train_loss=5.256791

Batch 173340, train_perplexity=180.20647, train_loss=5.1941032

Batch 173350, train_perplexity=208.2968, train_loss=5.338964

Batch 173360, train_perplexity=187.33612, train_loss=5.2329044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Batch 173370, train_perplexity=188.65953, train_loss=5.239944

Batch 173380, train_perplexity=213.58485, train_loss=5.364034

Batch 173390, train_perplexity=190.95003, train_loss=5.252012

Batch 173400, train_perplexity=197.50261, train_loss=5.285752

Batch 173410, train_perplexity=179.2109, train_loss=5.1885633

Batch 173420, train_perplexity=190.32335, train_loss=5.2487245

Batch 173430, train_perplexity=209.37596, train_loss=5.3441315

Batch 173440, train_perplexity=192.76562, train_loss=5.261475

Batch 173450, train_perplexity=200.31802, train_loss=5.2999063

Batch 173460, train_perplexity=189.4533, train_loss=5.2441425

Batch 173470, train_perplexity=201.21716, train_loss=5.3043847

Batch 173480, train_perplexity=198.89073, train_loss=5.2927556

Batch 173490, train_perplexity=193.4721, train_loss=5.2651334

Batch 173500, train_perplexity=197.14929, train_loss=5.2839613

Batch 173510, train_perplexity=195.31737, train_loss=5.274626

Batch 173520, train_perplexity=213.46115, train_loss=5.363455

Batch 173530, train_perplexity=191.6485, train_loss=5.255663

Batch 173540, train_perplexity=199.88837, train_loss=5.297759

Batch 173550, train_perplexity=190.29204, train_loss=5.24856

Batch 173560, train_perplexity=187.5966, train_loss=5.234294

Batch 173570, train_perplexity=219.06573, train_loss=5.389372

Batch 173580, train_perplexity=196.22006, train_loss=5.279237

Batch 173590, train_perplexity=193.14388, train_loss=5.2634354

Batch 173600, train_perplexity=198.53569, train_loss=5.290969

Batch 173610, train_perplexity=182.86739, train_loss=5.208761

Batch 173620, train_perplexity=204.73926, train_loss=5.3217373

Batch 173630, train_perplexity=179.19304, train_loss=5.1884637

Batch 173640, train_perplexity=188.46883, train_loss=5.2389326

Batch 173650, train_perplexity=206.32242, train_loss=5.32944

Batch 173660, train_perplexity=185.70294, train_loss=5.2241483

Batch 173670, train_perplexity=183.13545, train_loss=5.210226

Batch 173680, train_perplexity=207.28506, train_loss=5.334095

Batch 173690, train_perplexity=212.26476, train_loss=5.3578343

Batch 173700, train_perplexity=197.44527, train_loss=5.2854614

Batch 173710, train_perplexity=224.35251, train_loss=5.4132185

Batch 173720, train_perplexity=208.42348, train_loss=5.339572

Batch 173730, train_perplexity=199.38347, train_loss=5.29523

Batch 173740, train_perplexity=180.31572, train_loss=5.1947093

Batch 173750, train_perplexity=166.24315, train_loss=5.1134515

Batch 173760, train_perplexity=190.93901, train_loss=5.251954

Batch 173770, train_perplexity=193.09451, train_loss=5.26318

Batch 173780, train_perplexity=192.78355, train_loss=5.261568

Batch 173790, train_perplexity=198.62868, train_loss=5.291437

Batch 173800, train_perplexity=191.27254, train_loss=5.2536993

Batch 173810, train_perplexity=190.43864, train_loss=5.24933

Batch 173820, train_perplexity=172.82993, train_loss=5.152308

Batch 173830, train_perplexity=177.85681, train_loss=5.180979

Batch 173840, train_perplexity=188.04135, train_loss=5.236662

Batch 173850, train_perplexity=195.8131, train_loss=5.2771606

Batch 173860, train_perplexity=188.71432, train_loss=5.2402344

Batch 173870, train_perplexity=206.76118, train_loss=5.3315644

Batch 173880, train_perplexity=197.14836, train_loss=5.2839565

Batch 173890, train_perplexity=179.41577, train_loss=5.189706

Batch 173900, train_perplexity=193.65422, train_loss=5.266074

Batch 173910, train_perplexity=209.7844, train_loss=5.3460803

Batch 173920, train_perplexity=194.78416, train_loss=5.271892

Batch 173930, train_perplexity=200.39551, train_loss=5.300293

Batch 173940, train_perplexity=198.22958, train_loss=5.289426

Batch 173950, train_perplexity=198.16096, train_loss=5.2890797

Batch 173960, train_perplexity=199.24147, train_loss=5.2945175

Batch 173970, train_perplexity=200.71014, train_loss=5.301862

Batch 173980, train_perplexity=200.33723, train_loss=5.300002

Batch 173990, train_perplexity=215.52686, train_loss=5.3730855

Batch 174000, train_perplexity=206.8671, train_loss=5.3320765

Batch 174010, train_perplexity=208.48291, train_loss=5.339857

Batch 174020, train_perplexity=211.83675, train_loss=5.355816

Batch 174030, train_perplexity=217.64471, train_loss=5.382864

Batch 174040, train_perplexity=197.1429, train_loss=5.283929

Batch 174050, train_perplexity=216.37607, train_loss=5.377018

Batch 174060, train_perplexity=197.66985, train_loss=5.286598

Batch 174070, train_perplexity=190.85654, train_loss=5.251522

Batch 174080, train_perplexity=192.26991, train_loss=5.2589

Batch 174090, train_perplexity=183.44572, train_loss=5.211919

Batch 174100, train_perplexity=179.46625, train_loss=5.189987

Batch 174110, train_perplexity=209.17897, train_loss=5.34319

Batch 174120, train_perplexity=201.44958, train_loss=5.305539

Batch 174130, train_perplexity=192.9674, train_loss=5.2625213

Batch 174140, train_perplexity=202.42732, train_loss=5.310381

Batch 174150, train_perplexity=194.03726, train_loss=5.26805

Batch 174160, train_perplexity=186.75424, train_loss=5.2297935

Batch 174170, train_perplexity=185.74243, train_loss=5.224361

Batch 174180, train_perplexity=176.59882, train_loss=5.1738806

Batch 174190, train_perplexity=189.11708, train_loss=5.2423663

Batch 174200, train_perplexity=184.82184, train_loss=5.2193923

Batch 174210, train_perplexity=204.03784, train_loss=5.3183055

Batch 174220, train_perplexity=186.42087, train_loss=5.228007

Batch 174230, train_perplexity=192.17972, train_loss=5.258431

Batch 174240, train_perplexity=209.20291, train_loss=5.3433046

Batch 174250, train_perplexity=193.53882, train_loss=5.265478

Batch 174260, train_perplexity=204.51036, train_loss=5.3206186

Batch 174270, train_perplexity=187.95673, train_loss=5.236212

Batch 174280, train_perplexity=184.39569, train_loss=5.217084

Batch 174290, train_perplexity=200.6445, train_loss=5.3015347

Batch 174300, train_perplexity=186.31148, train_loss=5.22742

Batch 174310, train_perplexity=207.52896, train_loss=5.335271

Batch 174320, train_perplexity=183.99026, train_loss=5.214883

Batch 174330, train_perplexity=198.75052, train_loss=5.2920504

Batch 174340, train_perplexity=188.15652, train_loss=5.237274

Batch 174350, train_perplexity=182.48952, train_loss=5.2066927

Batch 174360, train_perplexity=177.9306, train_loss=5.1813936

Batch 174370, train_perplexity=215.03893, train_loss=5.370819

Batch 174380, train_perplexity=198.63379, train_loss=5.291463

Batch 174390, train_perplexity=186.07016, train_loss=5.226124

Batch 174400, train_perplexity=201.33818, train_loss=5.304986

Batch 174410, train_perplexity=198.12355, train_loss=5.288891

Batch 174420, train_perplexity=184.85709, train_loss=5.219583

Batch 174430, train_perplexity=193.4532, train_loss=5.2650356

Batch 174440, train_perplexity=196.89339, train_loss=5.2826624

Batch 174450, train_perplexity=198.36441, train_loss=5.290106

Batch 174460, train_perplexity=194.61176, train_loss=5.2710066

Batch 174470, train_perplexity=194.10536, train_loss=5.268401

Batch 174480, train_perplexity=198.63986, train_loss=5.2914934

Batch 174490, train_perplexity=208.56624, train_loss=5.3402567

Batch 174500, train_perplexity=177.99103, train_loss=5.181733

Batch 174510, train_perplexity=211.59566, train_loss=5.354677

Batch 174520, train_perplexity=198.60869, train_loss=5.2913365

Batch 174530, train_perplexity=194.67375, train_loss=5.271325

Batch 174540, train_perplexity=173.92857, train_loss=5.1586447

Batch 174550, train_perplexity=183.23206, train_loss=5.2107534

Batch 174560, train_perplexity=202.00075, train_loss=5.3082714

Batch 174570, train_perplexity=191.81644, train_loss=5.256539

Batch 174580, train_perplexity=213.72961, train_loss=5.3647118

Batch 174590, train_perplexity=185.2004, train_loss=5.2214384

Batch 174600, train_perplexity=184.25874, train_loss=5.216341
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 174610, train_perplexity=211.61786, train_loss=5.354782

Batch 174620, train_perplexity=201.614, train_loss=5.306355

Batch 174630, train_perplexity=204.07317, train_loss=5.3184786

Batch 174640, train_perplexity=193.49878, train_loss=5.265271

Batch 174650, train_perplexity=205.04663, train_loss=5.3232374

Batch 174660, train_perplexity=201.23068, train_loss=5.304452

Batch 174670, train_perplexity=200.22826, train_loss=5.299458

Batch 174680, train_perplexity=167.9818, train_loss=5.1238556

Batch 174690, train_perplexity=181.30557, train_loss=5.200184

Batch 174700, train_perplexity=185.42041, train_loss=5.2226257

Batch 174710, train_perplexity=185.51521, train_loss=5.223137

Batch 174720, train_perplexity=200.40172, train_loss=5.300324

Batch 174730, train_perplexity=195.22649, train_loss=5.2741604

Batch 174740, train_perplexity=194.27223, train_loss=5.2692604

Batch 174750, train_perplexity=210.06287, train_loss=5.347407

Batch 174760, train_perplexity=192.1187, train_loss=5.2581134

Batch 174770, train_perplexity=169.23053, train_loss=5.131262

Batch 174780, train_perplexity=183.85634, train_loss=5.2141547

Batch 174790, train_perplexity=178.8962, train_loss=5.1868057

Batch 174800, train_perplexity=189.07335, train_loss=5.242135

Batch 174810, train_perplexity=193.04407, train_loss=5.2629185

Batch 174820, train_perplexity=180.1513, train_loss=5.193797

Batch 174830, train_perplexity=201.2925, train_loss=5.304759

Batch 174840, train_perplexity=192.85803, train_loss=5.2619543

Batch 174850, train_perplexity=181.32477, train_loss=5.2002897

Batch 174860, train_perplexity=215.9512, train_loss=5.3750525

Batch 174870, train_perplexity=188.61311, train_loss=5.239698

Batch 174880, train_perplexity=188.20543, train_loss=5.237534

Batch 174890, train_perplexity=189.88705, train_loss=5.2464294

Batch 174900, train_perplexity=167.36703, train_loss=5.120189

Batch 174910, train_perplexity=176.26869, train_loss=5.1720095

Batch 174920, train_perplexity=200.71712, train_loss=5.3018966

Batch 174930, train_perplexity=201.32321, train_loss=5.3049116

Batch 174940, train_perplexity=188.82504, train_loss=5.240821

Batch 174950, train_perplexity=193.18385, train_loss=5.2636423

Batch 174960, train_perplexity=206.07819, train_loss=5.3282557

Batch 174970, train_perplexity=195.5151, train_loss=5.2756376

Batch 174980, train_perplexity=192.21555, train_loss=5.2586174

Batch 174990, train_perplexity=199.21411, train_loss=5.29438

Batch 175000, train_perplexity=194.33115, train_loss=5.2695637

Batch 175010, train_perplexity=185.7133, train_loss=5.224204

Batch 175020, train_perplexity=187.94437, train_loss=5.236146

Batch 175030, train_perplexity=199.6986, train_loss=5.296809

Batch 175040, train_perplexity=180.56549, train_loss=5.1960936

Batch 175050, train_perplexity=187.79924, train_loss=5.2353735

Batch 175060, train_perplexity=195.32762, train_loss=5.274678

Batch 175070, train_perplexity=192.02666, train_loss=5.257634

Batch 175080, train_perplexity=178.11668, train_loss=5.182439

Batch 175090, train_perplexity=186.87772, train_loss=5.2304544

Batch 175100, train_perplexity=214.41261, train_loss=5.3679023

Batch 175110, train_perplexity=185.20145, train_loss=5.221444

Batch 175120, train_perplexity=169.72995, train_loss=5.1342087

Batch 175130, train_perplexity=217.95015, train_loss=5.3842664

Batch 175140, train_perplexity=188.09732, train_loss=5.2369595

Batch 175150, train_perplexity=188.34665, train_loss=5.238284

Batch 175160, train_perplexity=183.66568, train_loss=5.213117

Batch 175170, train_perplexity=202.44316, train_loss=5.310459

Batch 175180, train_perplexity=221.44751, train_loss=5.4001856

Batch 175190, train_perplexity=189.74097, train_loss=5.24566

Batch 175200, train_perplexity=194.19295, train_loss=5.268852

Batch 175210, train_perplexity=193.97583, train_loss=5.2677336

Batch 175220, train_perplexity=207.72766, train_loss=5.336228

Batch 175230, train_perplexity=184.16846, train_loss=5.215851

Batch 175240, train_perplexity=204.13623, train_loss=5.3187876

Batch 175250, train_perplexity=209.53456, train_loss=5.3448887

Batch 175260, train_perplexity=209.04495, train_loss=5.3425493

Batch 175270, train_perplexity=182.93977, train_loss=5.209157

Batch 175280, train_perplexity=200.75455, train_loss=5.302083

Batch 175290, train_perplexity=208.26482, train_loss=5.3388104

Batch 175300, train_perplexity=177.10664, train_loss=5.176752

Batch 175310, train_perplexity=189.91875, train_loss=5.2465963

Batch 175320, train_perplexity=185.28183, train_loss=5.221878

Batch 175330, train_perplexity=199.19397, train_loss=5.294279

Batch 175340, train_perplexity=204.70032, train_loss=5.321547

Batch 175350, train_perplexity=191.97995, train_loss=5.257391

Batch 175360, train_perplexity=224.05786, train_loss=5.4119043

Batch 175370, train_perplexity=173.99028, train_loss=5.1589994

Batch 175380, train_perplexity=203.41214, train_loss=5.315234

Batch 175390, train_perplexity=172.83478, train_loss=5.152336

Batch 175400, train_perplexity=175.72437, train_loss=5.1689167

Batch 175410, train_perplexity=194.52176, train_loss=5.270544

Batch 175420, train_perplexity=203.40729, train_loss=5.3152103

Batch 175430, train_perplexity=199.18466, train_loss=5.2942324

Batch 175440, train_perplexity=176.86343, train_loss=5.175378

Batch 175450, train_perplexity=191.8834, train_loss=5.256888

Batch 175460, train_perplexity=203.32875, train_loss=5.314824

Batch 175470, train_perplexity=205.64627, train_loss=5.3261576

Batch 175480, train_perplexity=175.90797, train_loss=5.169961

Batch 175490, train_perplexity=194.09962, train_loss=5.2683716

Batch 175500, train_perplexity=173.39809, train_loss=5.15559

Batch 175510, train_perplexity=198.0935, train_loss=5.288739

Batch 175520, train_perplexity=195.56229, train_loss=5.275879

Batch 175530, train_perplexity=192.90971, train_loss=5.2622223

Batch 175540, train_perplexity=185.87454, train_loss=5.225072

Batch 175550, train_perplexity=200.50792, train_loss=5.3008537

Batch 175560, train_perplexity=177.66068, train_loss=5.1798754

Batch 175570, train_perplexity=188.60376, train_loss=5.2396483

Batch 175580, train_perplexity=200.09398, train_loss=5.298787

Batch 175590, train_perplexity=196.16365, train_loss=5.2789493

Batch 175600, train_perplexity=184.98486, train_loss=5.220274

Batch 175610, train_perplexity=182.71815, train_loss=5.207945

Batch 175620, train_perplexity=181.29659, train_loss=5.2001343

Batch 175630, train_perplexity=199.11287, train_loss=5.293872

Batch 175640, train_perplexity=169.36615, train_loss=5.132063

Batch 175650, train_perplexity=213.02716, train_loss=5.3614197

Batch 175660, train_perplexity=185.27917, train_loss=5.2218637

Batch 175670, train_perplexity=220.51854, train_loss=5.395982

Batch 175680, train_perplexity=177.37659, train_loss=5.178275

Batch 175690, train_perplexity=187.93361, train_loss=5.2360888

Batch 175700, train_perplexity=208.21269, train_loss=5.33856

Batch 175710, train_perplexity=204.0792, train_loss=5.318508

Batch 175720, train_perplexity=175.53293, train_loss=5.1678267

Batch 175730, train_perplexity=181.71089, train_loss=5.202417

Batch 175740, train_perplexity=192.42088, train_loss=5.259685

Batch 175750, train_perplexity=189.16678, train_loss=5.242629

Batch 175760, train_perplexity=181.19426, train_loss=5.1995697

Batch 175770, train_perplexity=204.77344, train_loss=5.321904

Batch 175780, train_perplexity=193.06155, train_loss=5.263009

Batch 175790, train_perplexity=195.71788, train_loss=5.2766743

Batch 175800, train_perplexity=187.54903, train_loss=5.2340403

Batch 175810, train_perplexity=201.54163, train_loss=5.305996

Batch 175820, train_perplexity=205.5159, train_loss=5.3255234

Batch 175830, train_perplexity=174.5358, train_loss=5.16213

Batch 175840, train_perplexity=197.54698, train_loss=5.2859764

Batch 175850, train_perplexity=193.96066, train_loss=5.2676554

Batch 175860, train_perplexity=172.23651, train_loss=5.1488686

Batch 175870, train_perplexity=185.96497, train_loss=5.2255583

Batch 175880, train_perplexity=204.5166, train_loss=5.320649

Batch 175890, train_perplexity=200.66115, train_loss=5.3016176

Batch 175900, train_perplexity=189.03874, train_loss=5.241952
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 175910, train_perplexity=174.19574, train_loss=5.1601796

Batch 175920, train_perplexity=190.17085, train_loss=5.247923

Batch 175930, train_perplexity=169.53195, train_loss=5.1330414

Batch 175940, train_perplexity=189.24356, train_loss=5.243035

Batch 175950, train_perplexity=182.2376, train_loss=5.2053113

Batch 175960, train_perplexity=220.22137, train_loss=5.3946333

Batch 175970, train_perplexity=216.84479, train_loss=5.379182

Batch 175980, train_perplexity=177.57495, train_loss=5.179393

Batch 175990, train_perplexity=206.85901, train_loss=5.3320374

Batch 176000, train_perplexity=186.2477, train_loss=5.2270775

Batch 176010, train_perplexity=195.92574, train_loss=5.2777357

Batch 176020, train_perplexity=204.59053, train_loss=5.3210106

Batch 176030, train_perplexity=219.99425, train_loss=5.3936014

Batch 176040, train_perplexity=200.03493, train_loss=5.298492

Batch 176050, train_perplexity=204.67719, train_loss=5.321434

Batch 176060, train_perplexity=195.05565, train_loss=5.273285

Batch 176070, train_perplexity=207.91624, train_loss=5.3371353

Batch 176080, train_perplexity=171.39789, train_loss=5.1439877

Batch 176090, train_perplexity=161.75972, train_loss=5.086112

Batch 176100, train_perplexity=185.30179, train_loss=5.221986

Batch 176110, train_perplexity=196.22362, train_loss=5.279255

Batch 176120, train_perplexity=167.90051, train_loss=5.1233716

Batch 176130, train_perplexity=182.59535, train_loss=5.2072725

Batch 176140, train_perplexity=177.83595, train_loss=5.1808615

Batch 176150, train_perplexity=180.68436, train_loss=5.1967516

Batch 176160, train_perplexity=199.62624, train_loss=5.296447

Batch 176170, train_perplexity=205.40636, train_loss=5.3249903

Batch 176180, train_perplexity=186.12819, train_loss=5.2264357

Batch 176190, train_perplexity=187.45514, train_loss=5.2335396

Batch 176200, train_perplexity=180.08441, train_loss=5.1934257

Batch 176210, train_perplexity=198.93987, train_loss=5.2930026

Batch 176220, train_perplexity=183.67049, train_loss=5.2131433

Batch 176230, train_perplexity=204.23749, train_loss=5.3192835

Batch 176240, train_perplexity=203.13861, train_loss=5.3138885

Batch 176250, train_perplexity=162.66586, train_loss=5.091698

Batch 176260, train_perplexity=195.56787, train_loss=5.2759075

Batch 176270, train_perplexity=202.00615, train_loss=5.308298

Batch 176280, train_perplexity=201.33818, train_loss=5.304986

Batch 176290, train_perplexity=186.65364, train_loss=5.2292547

Batch 176300, train_perplexity=183.45079, train_loss=5.2119465

Batch 176310, train_perplexity=218.74132, train_loss=5.38789

Batch 176320, train_perplexity=178.0377, train_loss=5.1819954

Batch 176330, train_perplexity=195.6757, train_loss=5.2764587

Batch 176340, train_perplexity=164.51569, train_loss=5.103006

Batch 176350, train_perplexity=180.2601, train_loss=5.194401

Batch 176360, train_perplexity=194.39445, train_loss=5.2698894

Batch 176370, train_perplexity=211.33742, train_loss=5.353456

Batch 176380, train_perplexity=180.63533, train_loss=5.1964803

Batch 176390, train_perplexity=174.88127, train_loss=5.1641073

Batch 176400, train_perplexity=191.10051, train_loss=5.2527995

Batch 176410, train_perplexity=190.24658, train_loss=5.248321

Batch 176420, train_perplexity=215.56169, train_loss=5.373247

Batch 176430, train_perplexity=200.77197, train_loss=5.30217

Batch 176440, train_perplexity=180.99945, train_loss=5.198494

Batch 176450, train_perplexity=193.90425, train_loss=5.2673645

Batch 176460, train_perplexity=167.90083, train_loss=5.1233735

Batch 176470, train_perplexity=187.21074, train_loss=5.232235

Batch 176480, train_perplexity=184.48795, train_loss=5.217584

Batch 176490, train_perplexity=218.46677, train_loss=5.386634

Batch 176500, train_perplexity=182.75493, train_loss=5.208146

Batch 176510, train_perplexity=176.90071, train_loss=5.1755886

Batch 176520, train_perplexity=195.83401, train_loss=5.2772675

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00010-of-00100
Loaded 306380 sentences.
Finished loading
Batch 176530, train_perplexity=195.73393, train_loss=5.2767563

Batch 176540, train_perplexity=205.72572, train_loss=5.326544

Batch 176550, train_perplexity=174.74965, train_loss=5.1633544

Batch 176560, train_perplexity=178.51317, train_loss=5.1846623

Batch 176570, train_perplexity=187.07234, train_loss=5.2314954

Batch 176580, train_perplexity=180.28691, train_loss=5.1945496

Batch 176590, train_perplexity=193.37001, train_loss=5.2646055

Batch 176600, train_perplexity=208.87099, train_loss=5.341717

Batch 176610, train_perplexity=157.12964, train_loss=5.057071

Batch 176620, train_perplexity=186.07637, train_loss=5.226157

Batch 176630, train_perplexity=186.75015, train_loss=5.2297716

Batch 176640, train_perplexity=176.61885, train_loss=5.173994

Batch 176650, train_perplexity=213.61368, train_loss=5.364169

Batch 176660, train_perplexity=184.32088, train_loss=5.216678

Batch 176670, train_perplexity=211.63844, train_loss=5.3548794

Batch 176680, train_perplexity=171.48143, train_loss=5.144475

Batch 176690, train_perplexity=193.04037, train_loss=5.2628994

Batch 176700, train_perplexity=188.26816, train_loss=5.2378674

Batch 176710, train_perplexity=193.63058, train_loss=5.265952

Batch 176720, train_perplexity=221.0658, train_loss=5.3984604

Batch 176730, train_perplexity=195.67421, train_loss=5.276451

Batch 176740, train_perplexity=188.93619, train_loss=5.2414093

Batch 176750, train_perplexity=205.9222, train_loss=5.3274984

Batch 176760, train_perplexity=183.35214, train_loss=5.2114086

Batch 176770, train_perplexity=185.67735, train_loss=5.2240105

Batch 176780, train_perplexity=182.92041, train_loss=5.209051

Batch 176790, train_perplexity=182.13586, train_loss=5.204753

Batch 176800, train_perplexity=179.36769, train_loss=5.189438

Batch 176810, train_perplexity=192.09021, train_loss=5.257965

Batch 176820, train_perplexity=186.74284, train_loss=5.2297325

Batch 176830, train_perplexity=183.05374, train_loss=5.2097797

Batch 176840, train_perplexity=205.41988, train_loss=5.325056

Batch 176850, train_perplexity=190.32634, train_loss=5.24874

Batch 176860, train_perplexity=181.00394, train_loss=5.1985188

Batch 176870, train_perplexity=201.45679, train_loss=5.305575

Batch 176880, train_perplexity=207.01897, train_loss=5.3328104

Batch 176890, train_perplexity=181.89684, train_loss=5.2034397

Batch 176900, train_perplexity=192.66748, train_loss=5.260966

Batch 176910, train_perplexity=202.06337, train_loss=5.3085814

Batch 176920, train_perplexity=194.0344, train_loss=5.2680354

Batch 176930, train_perplexity=172.73526, train_loss=5.15176

Batch 176940, train_perplexity=202.06058, train_loss=5.3085675

Batch 176950, train_perplexity=177.37878, train_loss=5.1782875

Batch 176960, train_perplexity=195.70006, train_loss=5.276583

Batch 176970, train_perplexity=201.03848, train_loss=5.3034964

Batch 176980, train_perplexity=199.92688, train_loss=5.2979517

Batch 176990, train_perplexity=179.74673, train_loss=5.191549

Batch 177000, train_perplexity=188.36523, train_loss=5.238383

Batch 177010, train_perplexity=204.0865, train_loss=5.318544

Batch 177020, train_perplexity=221.89949, train_loss=5.4022245

Batch 177030, train_perplexity=180.36378, train_loss=5.194976

Batch 177040, train_perplexity=190.61008, train_loss=5.25023

Batch 177050, train_perplexity=183.0533, train_loss=5.2097774

Batch 177060, train_perplexity=196.29755, train_loss=5.2796316

Batch 177070, train_perplexity=196.2968, train_loss=5.279628

Batch 177080, train_perplexity=203.46724, train_loss=5.315505

Batch 177090, train_perplexity=198.3089, train_loss=5.289826

Batch 177100, train_perplexity=215.96819, train_loss=5.375131

Batch 177110, train_perplexity=194.62215, train_loss=5.27106

Batch 177120, train_perplexity=192.15755, train_loss=5.2583156

Batch 177130, train_perplexity=183.55089, train_loss=5.212492
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 177140, train_perplexity=203.0865, train_loss=5.313632

Batch 177150, train_perplexity=188.39165, train_loss=5.238523

Batch 177160, train_perplexity=190.87675, train_loss=5.251628

Batch 177170, train_perplexity=191.67966, train_loss=5.2558255

Batch 177180, train_perplexity=194.9859, train_loss=5.2729273

Batch 177190, train_perplexity=196.19846, train_loss=5.2791266

Batch 177200, train_perplexity=198.88286, train_loss=5.292716

Batch 177210, train_perplexity=194.68082, train_loss=5.2713614

Batch 177220, train_perplexity=215.0734, train_loss=5.3709793

Batch 177230, train_perplexity=187.81975, train_loss=5.2354827

Batch 177240, train_perplexity=189.81427, train_loss=5.246046

Batch 177250, train_perplexity=196.52945, train_loss=5.2808123

Batch 177260, train_perplexity=170.62875, train_loss=5.13949

Batch 177270, train_perplexity=181.00221, train_loss=5.198509

Batch 177280, train_perplexity=201.49896, train_loss=5.305784

Batch 177290, train_perplexity=167.2551, train_loss=5.11952

Batch 177300, train_perplexity=205.08868, train_loss=5.3234425

Batch 177310, train_perplexity=226.54489, train_loss=5.422943

Batch 177320, train_perplexity=198.61845, train_loss=5.2913857

Batch 177330, train_perplexity=214.09866, train_loss=5.366437

Batch 177340, train_perplexity=200.24994, train_loss=5.2995663

Batch 177350, train_perplexity=178.05775, train_loss=5.182108

Batch 177360, train_perplexity=190.0327, train_loss=5.247196

Batch 177370, train_perplexity=168.24457, train_loss=5.1254187

Batch 177380, train_perplexity=208.88344, train_loss=5.3417764

Batch 177390, train_perplexity=189.87274, train_loss=5.246354

Batch 177400, train_perplexity=199.98723, train_loss=5.2982535

Batch 177410, train_perplexity=180.31013, train_loss=5.1946783

Batch 177420, train_perplexity=175.60191, train_loss=5.1682196

Batch 177430, train_perplexity=185.72853, train_loss=5.224286

Batch 177440, train_perplexity=178.9619, train_loss=5.187173

Batch 177450, train_perplexity=178.12195, train_loss=5.1824684

Batch 177460, train_perplexity=209.4906, train_loss=5.344679

Batch 177470, train_perplexity=185.38823, train_loss=5.222452

Batch 177480, train_perplexity=194.67116, train_loss=5.2713118

Batch 177490, train_perplexity=201.70584, train_loss=5.3068104

Batch 177500, train_perplexity=182.03688, train_loss=5.2042093

Batch 177510, train_perplexity=202.80702, train_loss=5.312255

Batch 177520, train_perplexity=194.1711, train_loss=5.2687397

Batch 177530, train_perplexity=171.65683, train_loss=5.1454973

Batch 177540, train_perplexity=175.27882, train_loss=5.166378

Batch 177550, train_perplexity=193.22356, train_loss=5.263848

Batch 177560, train_perplexity=192.24956, train_loss=5.2587943

Batch 177570, train_perplexity=188.6877, train_loss=5.240093

Batch 177580, train_perplexity=200.10075, train_loss=5.298821

Batch 177590, train_perplexity=185.99193, train_loss=5.2257032

Batch 177600, train_perplexity=179.22885, train_loss=5.1886635

Batch 177610, train_perplexity=187.20914, train_loss=5.2322264

Batch 177620, train_perplexity=201.76404, train_loss=5.307099

Batch 177630, train_perplexity=183.51117, train_loss=5.2122755

Batch 177640, train_perplexity=185.31451, train_loss=5.2220545

Batch 177650, train_perplexity=185.74695, train_loss=5.2243853

Batch 177660, train_perplexity=177.70404, train_loss=5.1801195

Batch 177670, train_perplexity=203.70139, train_loss=5.316655

Batch 177680, train_perplexity=205.35583, train_loss=5.324744

Batch 177690, train_perplexity=200.17213, train_loss=5.2991776

Batch 177700, train_perplexity=201.16487, train_loss=5.304125

Batch 177710, train_perplexity=187.8793, train_loss=5.2358

Batch 177720, train_perplexity=188.89636, train_loss=5.2411985

Batch 177730, train_perplexity=193.8109, train_loss=5.266883

Batch 177740, train_perplexity=183.97052, train_loss=5.2147756

Batch 177750, train_perplexity=198.03363, train_loss=5.288437

Batch 177760, train_perplexity=179.99452, train_loss=5.1929264

Batch 177770, train_perplexity=208.17387, train_loss=5.3383737

Batch 177780, train_perplexity=198.86304, train_loss=5.2926164

Batch 177790, train_perplexity=232.8436, train_loss=5.450367

Batch 177800, train_perplexity=170.40352, train_loss=5.1381693

Batch 177810, train_perplexity=205.39589, train_loss=5.3249393

Batch 177820, train_perplexity=193.10501, train_loss=5.263234

Batch 177830, train_perplexity=174.97052, train_loss=5.1646175

Batch 177840, train_perplexity=192.21977, train_loss=5.2586393

Batch 177850, train_perplexity=199.38889, train_loss=5.295257

Batch 177860, train_perplexity=189.62502, train_loss=5.2450485

Batch 177870, train_perplexity=177.83035, train_loss=5.18083

Batch 177880, train_perplexity=189.68109, train_loss=5.245344

Batch 177890, train_perplexity=187.84868, train_loss=5.2356367

Batch 177900, train_perplexity=183.71638, train_loss=5.213393

Batch 177910, train_perplexity=197.01698, train_loss=5.28329

Batch 177920, train_perplexity=193.82901, train_loss=5.2669764

Batch 177930, train_perplexity=194.75378, train_loss=5.271736

Batch 177940, train_perplexity=188.81946, train_loss=5.2407913

Batch 177950, train_perplexity=189.8168, train_loss=5.2460594

Batch 177960, train_perplexity=206.63257, train_loss=5.330942

Batch 177970, train_perplexity=168.10448, train_loss=5.1245856

Batch 177980, train_perplexity=175.36511, train_loss=5.16687

Batch 177990, train_perplexity=193.44287, train_loss=5.264982

Batch 178000, train_perplexity=209.74008, train_loss=5.345869

Batch 178010, train_perplexity=199.02527, train_loss=5.2934318

Batch 178020, train_perplexity=218.37407, train_loss=5.3862095

Batch 178030, train_perplexity=207.43161, train_loss=5.3348017

Batch 178040, train_perplexity=192.60345, train_loss=5.2606335

Batch 178050, train_perplexity=181.02344, train_loss=5.1986265

Batch 178060, train_perplexity=195.1431, train_loss=5.273733

Batch 178070, train_perplexity=181.94316, train_loss=5.2036943

Batch 178080, train_perplexity=190.03996, train_loss=5.2472343

Batch 178090, train_perplexity=193.53827, train_loss=5.2654753

Batch 178100, train_perplexity=193.30345, train_loss=5.2642612

Batch 178110, train_perplexity=177.3503, train_loss=5.178127

Batch 178120, train_perplexity=192.87697, train_loss=5.2620525

Batch 178130, train_perplexity=184.52939, train_loss=5.2178087

Batch 178140, train_perplexity=204.84688, train_loss=5.322263

Batch 178150, train_perplexity=204.18608, train_loss=5.3190317

Batch 178160, train_perplexity=173.73999, train_loss=5.15756

Batch 178170, train_perplexity=190.60098, train_loss=5.250182

Batch 178180, train_perplexity=198.77307, train_loss=5.292164

Batch 178190, train_perplexity=192.00293, train_loss=5.2575107

Batch 178200, train_perplexity=189.53119, train_loss=5.2445536

Batch 178210, train_perplexity=200.09225, train_loss=5.2987785

Batch 178220, train_perplexity=201.97533, train_loss=5.3081455

Batch 178230, train_perplexity=196.00311, train_loss=5.2781305

Batch 178240, train_perplexity=198.8565, train_loss=5.2925835

Batch 178250, train_perplexity=200.28957, train_loss=5.299764

Batch 178260, train_perplexity=180.53674, train_loss=5.1959343

Batch 178270, train_perplexity=204.62508, train_loss=5.3211794

Batch 178280, train_perplexity=184.72202, train_loss=5.218852

Batch 178290, train_perplexity=191.01752, train_loss=5.252365

Batch 178300, train_perplexity=188.38922, train_loss=5.23851

Batch 178310, train_perplexity=188.78525, train_loss=5.24061

Batch 178320, train_perplexity=190.45763, train_loss=5.2494297

Batch 178330, train_perplexity=186.11417, train_loss=5.2263603

Batch 178340, train_perplexity=184.3637, train_loss=5.2169104

Batch 178350, train_perplexity=181.10823, train_loss=5.199095

Batch 178360, train_perplexity=174.91864, train_loss=5.164321

Batch 178370, train_perplexity=181.23816, train_loss=5.199812

Batch 178380, train_perplexity=191.64319, train_loss=5.2556353

Batch 178390, train_perplexity=201.24968, train_loss=5.3045464

Batch 178400, train_perplexity=181.5421, train_loss=5.2014875

Batch 178410, train_perplexity=201.08047, train_loss=5.303705

Batch 178420, train_perplexity=197.94186, train_loss=5.2879734

Batch 178430, train_perplexity=191.18172, train_loss=5.2532244
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 178440, train_perplexity=184.84554, train_loss=5.2195206

Batch 178450, train_perplexity=189.51762, train_loss=5.244482

Batch 178460, train_perplexity=203.81613, train_loss=5.3172183

Batch 178470, train_perplexity=193.07224, train_loss=5.2630644

Batch 178480, train_perplexity=202.18433, train_loss=5.30918

Batch 178490, train_perplexity=187.85727, train_loss=5.2356825

Batch 178500, train_perplexity=191.17616, train_loss=5.2531953

Batch 178510, train_perplexity=198.15794, train_loss=5.2890644

Batch 178520, train_perplexity=222.39832, train_loss=5.40447

Batch 178530, train_perplexity=200.4197, train_loss=5.3004136

Batch 178540, train_perplexity=169.03214, train_loss=5.130089

Batch 178550, train_perplexity=189.29166, train_loss=5.243289

Batch 178560, train_perplexity=152.74844, train_loss=5.0287924

Batch 178570, train_perplexity=205.28474, train_loss=5.324398

Batch 178580, train_perplexity=201.95058, train_loss=5.308023

Batch 178590, train_perplexity=227.07245, train_loss=5.425269

Batch 178600, train_perplexity=213.78334, train_loss=5.364963

Batch 178610, train_perplexity=206.71811, train_loss=5.331356

Batch 178620, train_perplexity=184.57162, train_loss=5.2180376

Batch 178630, train_perplexity=192.02325, train_loss=5.2576165

Batch 178640, train_perplexity=200.73311, train_loss=5.301976

Batch 178650, train_perplexity=189.59229, train_loss=5.244876

Batch 178660, train_perplexity=202.61835, train_loss=5.311324

Batch 178670, train_perplexity=208.6897, train_loss=5.3408484

Batch 178680, train_perplexity=181.90724, train_loss=5.203497

Batch 178690, train_perplexity=193.96973, train_loss=5.267702

Batch 178700, train_perplexity=186.12695, train_loss=5.226429

Batch 178710, train_perplexity=172.96843, train_loss=5.153109

Batch 178720, train_perplexity=180.22949, train_loss=5.194231

Batch 178730, train_perplexity=176.37909, train_loss=5.1726356

Batch 178740, train_perplexity=179.57463, train_loss=5.190591

Batch 178750, train_perplexity=197.32198, train_loss=5.284837

Batch 178760, train_perplexity=193.7669, train_loss=5.266656

Batch 178770, train_perplexity=171.74493, train_loss=5.1460104

Batch 178780, train_perplexity=184.60815, train_loss=5.2182355

Batch 178790, train_perplexity=203.09406, train_loss=5.313669

Batch 178800, train_perplexity=195.34056, train_loss=5.2747445

Batch 178810, train_perplexity=176.246, train_loss=5.1718807

Batch 178820, train_perplexity=191.80913, train_loss=5.2565007

Batch 178830, train_perplexity=182.37564, train_loss=5.2060685

Batch 178840, train_perplexity=195.77052, train_loss=5.276943

Batch 178850, train_perplexity=181.78152, train_loss=5.2028055

Batch 178860, train_perplexity=177.19121, train_loss=5.1772294

Batch 178870, train_perplexity=224.00809, train_loss=5.411682

Batch 178880, train_perplexity=179.71597, train_loss=5.1913776

Batch 178890, train_perplexity=197.35876, train_loss=5.285023

Batch 178900, train_perplexity=216.12364, train_loss=5.3758507

Batch 178910, train_perplexity=201.7978, train_loss=5.307266

Batch 178920, train_perplexity=195.41844, train_loss=5.275143

Batch 178930, train_perplexity=191.99414, train_loss=5.257465

Batch 178940, train_perplexity=186.92958, train_loss=5.230732

Batch 178950, train_perplexity=180.47202, train_loss=5.1955757

Batch 178960, train_perplexity=182.66249, train_loss=5.20764

Batch 178970, train_perplexity=185.87897, train_loss=5.2250957

Batch 178980, train_perplexity=165.89227, train_loss=5.1113386

Batch 178990, train_perplexity=192.1602, train_loss=5.2583294

Batch 179000, train_perplexity=202.16803, train_loss=5.309099

Batch 179010, train_perplexity=169.53235, train_loss=5.133044

Batch 179020, train_perplexity=203.54439, train_loss=5.315884

Batch 179030, train_perplexity=184.42313, train_loss=5.2172327

Batch 179040, train_perplexity=163.61592, train_loss=5.097522

Batch 179050, train_perplexity=205.23209, train_loss=5.3241415

Batch 179060, train_perplexity=211.44458, train_loss=5.353963

Batch 179070, train_perplexity=195.8795, train_loss=5.2774997

Batch 179080, train_perplexity=190.04756, train_loss=5.2472744

Batch 179090, train_perplexity=217.5557, train_loss=5.382455

Batch 179100, train_perplexity=179.29663, train_loss=5.1890416

Batch 179110, train_perplexity=188.24446, train_loss=5.2377415

Batch 179120, train_perplexity=179.85365, train_loss=5.1921434

Batch 179130, train_perplexity=167.64555, train_loss=5.121852

Batch 179140, train_perplexity=192.57646, train_loss=5.2604933

Batch 179150, train_perplexity=188.18587, train_loss=5.23743

Batch 179160, train_perplexity=185.49637, train_loss=5.2230353

Batch 179170, train_perplexity=182.56227, train_loss=5.2070913

Batch 179180, train_perplexity=207.60042, train_loss=5.335615

Batch 179190, train_perplexity=171.45003, train_loss=5.144292

Batch 179200, train_perplexity=193.44435, train_loss=5.26499

Batch 179210, train_perplexity=204.23447, train_loss=5.3192687

Batch 179220, train_perplexity=182.8775, train_loss=5.2088165

Batch 179230, train_perplexity=186.04088, train_loss=5.2259665

Batch 179240, train_perplexity=192.96693, train_loss=5.262519

Batch 179250, train_perplexity=208.68442, train_loss=5.340823

Batch 179260, train_perplexity=187.08366, train_loss=5.231556

Batch 179270, train_perplexity=202.19484, train_loss=5.3092318

Batch 179280, train_perplexity=219.78539, train_loss=5.3926516

Batch 179290, train_perplexity=218.80037, train_loss=5.3881598

Batch 179300, train_perplexity=205.36797, train_loss=5.3248034

Batch 179310, train_perplexity=169.41154, train_loss=5.132331

Batch 179320, train_perplexity=185.81818, train_loss=5.2247686

Batch 179330, train_perplexity=198.4295, train_loss=5.290434

Batch 179340, train_perplexity=216.66763, train_loss=5.3783646

Batch 179350, train_perplexity=211.66832, train_loss=5.3550205

Batch 179360, train_perplexity=180.14685, train_loss=5.1937723

Batch 179370, train_perplexity=178.59447, train_loss=5.1851177

Batch 179380, train_perplexity=219.59996, train_loss=5.3918076

Batch 179390, train_perplexity=195.17865, train_loss=5.2739153

Batch 179400, train_perplexity=210.55948, train_loss=5.349768

Batch 179410, train_perplexity=192.99648, train_loss=5.262672

Batch 179420, train_perplexity=167.05544, train_loss=5.1183257

Batch 179430, train_perplexity=181.22311, train_loss=5.199729

Batch 179440, train_perplexity=201.59584, train_loss=5.306265

Batch 179450, train_perplexity=179.89189, train_loss=5.192356

Batch 179460, train_perplexity=185.78964, train_loss=5.224615

Batch 179470, train_perplexity=225.0159, train_loss=5.416171

Batch 179480, train_perplexity=175.00458, train_loss=5.164812

Batch 179490, train_perplexity=175.00858, train_loss=5.164835

Batch 179500, train_perplexity=178.05385, train_loss=5.182086

Batch 179510, train_perplexity=196.46143, train_loss=5.280466

Batch 179520, train_perplexity=211.24785, train_loss=5.353032

Batch 179530, train_perplexity=183.63301, train_loss=5.2129393

Batch 179540, train_perplexity=169.0526, train_loss=5.13021

Batch 179550, train_perplexity=159.045, train_loss=5.069187

Batch 179560, train_perplexity=179.57703, train_loss=5.190604

Batch 179570, train_perplexity=198.73677, train_loss=5.291981

Batch 179580, train_perplexity=219.128, train_loss=5.389656

Batch 179590, train_perplexity=212.88885, train_loss=5.36077

Batch 179600, train_perplexity=206.89886, train_loss=5.33223

Batch 179610, train_perplexity=182.97354, train_loss=5.2093415

Batch 179620, train_perplexity=172.82053, train_loss=5.1522536

Batch 179630, train_perplexity=180.12589, train_loss=5.193656

Batch 179640, train_perplexity=182.66241, train_loss=5.2076397

Batch 179650, train_perplexity=167.67384, train_loss=5.1220207

Batch 179660, train_perplexity=207.2829, train_loss=5.3340845

Batch 179670, train_perplexity=202.60037, train_loss=5.3112354

Batch 179680, train_perplexity=177.5482, train_loss=5.179242

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305749 sentences.
Finished loading
Batch 179690, train_perplexity=197.14177, train_loss=5.283923

Batch 179700, train_perplexity=187.91496, train_loss=5.2359896

Batch 179710, train_perplexity=199.02232, train_loss=5.293417

Batch 179720, train_perplexity=178.0963, train_loss=5.1823244

Batch 179730, train_perplexity=193.593, train_loss=5.265758

Batch 179740, train_perplexity=182.55304, train_loss=5.207041

Batch 179750, train_perplexity=204.33548, train_loss=5.319763

Batch 179760, train_perplexity=201.54047, train_loss=5.30599

Batch 179770, train_perplexity=187.22467, train_loss=5.2323093

Batch 179780, train_perplexity=166.22762, train_loss=5.113358

Batch 179790, train_perplexity=177.99391, train_loss=5.1817493

Batch 179800, train_perplexity=176.87936, train_loss=5.175468

Batch 179810, train_perplexity=180.05307, train_loss=5.1932516

Batch 179820, train_perplexity=181.4816, train_loss=5.201154

Batch 179830, train_perplexity=182.19258, train_loss=5.2050643

Batch 179840, train_perplexity=193.15198, train_loss=5.2634773

Batch 179850, train_perplexity=167.0301, train_loss=5.118174

Batch 179860, train_perplexity=187.79306, train_loss=5.2353406

Batch 179870, train_perplexity=186.98164, train_loss=5.2310104

Batch 179880, train_perplexity=186.17188, train_loss=5.2266703

Batch 179890, train_perplexity=183.4522, train_loss=5.211954

Batch 179900, train_perplexity=183.25269, train_loss=5.210866

Batch 179910, train_perplexity=187.0373, train_loss=5.231308

Batch 179920, train_perplexity=190.71117, train_loss=5.25076

Batch 179930, train_perplexity=206.01216, train_loss=5.327935

Batch 179940, train_perplexity=214.5775, train_loss=5.368671

Batch 179950, train_perplexity=185.8304, train_loss=5.2248344

Batch 179960, train_perplexity=178.31561, train_loss=5.183555

Batch 179970, train_perplexity=195.13435, train_loss=5.2736883

Batch 179980, train_perplexity=180.60045, train_loss=5.196287

Batch 179990, train_perplexity=196.90202, train_loss=5.2827063

Batch 180000, train_perplexity=178.9991, train_loss=5.187381

Batch 180010, train_perplexity=200.79955, train_loss=5.302307

Batch 180020, train_perplexity=213.57538, train_loss=5.36399

Batch 180030, train_perplexity=180.37222, train_loss=5.1950226

Batch 180040, train_perplexity=181.64618, train_loss=5.2020607

Batch 180050, train_perplexity=184.03905, train_loss=5.215148

Batch 180060, train_perplexity=181.4379, train_loss=5.2009134

Batch 180070, train_perplexity=181.89163, train_loss=5.203411

Batch 180080, train_perplexity=183.79, train_loss=5.2137938

Batch 180090, train_perplexity=179.47447, train_loss=5.190033

Batch 180100, train_perplexity=173.53027, train_loss=5.156352

Batch 180110, train_perplexity=197.22357, train_loss=5.284338

Batch 180120, train_perplexity=191.73149, train_loss=5.256096

Batch 180130, train_perplexity=188.31593, train_loss=5.238121

Batch 180140, train_perplexity=187.14969, train_loss=5.231909

Batch 180150, train_perplexity=209.67589, train_loss=5.345563

Batch 180160, train_perplexity=184.46306, train_loss=5.217449

Batch 180170, train_perplexity=204.18442, train_loss=5.3190236

Batch 180180, train_perplexity=200.76183, train_loss=5.3021193

Batch 180190, train_perplexity=191.29462, train_loss=5.2538147

Batch 180200, train_perplexity=215.27275, train_loss=5.371906

Batch 180210, train_perplexity=195.84401, train_loss=5.2773185

Batch 180220, train_perplexity=190.2064, train_loss=5.24811

Batch 180230, train_perplexity=206.74039, train_loss=5.331464

Batch 180240, train_perplexity=196.04517, train_loss=5.278345

Batch 180250, train_perplexity=191.0822, train_loss=5.2527037

Batch 180260, train_perplexity=183.61462, train_loss=5.212839

Batch 180270, train_perplexity=192.31824, train_loss=5.2591515

Batch 180280, train_perplexity=166.47296, train_loss=5.114833

Batch 180290, train_perplexity=177.15082, train_loss=5.1770015

Batch 180300, train_perplexity=195.60779, train_loss=5.2761116

Batch 180310, train_perplexity=191.99323, train_loss=5.25746

Batch 180320, train_perplexity=203.63554, train_loss=5.316332

Batch 180330, train_perplexity=181.25043, train_loss=5.1998796

Batch 180340, train_perplexity=190.95905, train_loss=5.252059

Batch 180350, train_perplexity=211.29924, train_loss=5.3532753

Batch 180360, train_perplexity=184.72783, train_loss=5.2188835

Batch 180370, train_perplexity=172.4783, train_loss=5.1502714

Batch 180380, train_perplexity=188.92068, train_loss=5.2413273

Batch 180390, train_perplexity=207.27202, train_loss=5.334032

Batch 180400, train_perplexity=187.92519, train_loss=5.236044

Batch 180410, train_perplexity=179.24287, train_loss=5.1887417

Batch 180420, train_perplexity=174.50818, train_loss=5.1619716

Batch 180430, train_perplexity=174.53471, train_loss=5.1621237

Batch 180440, train_perplexity=190.71062, train_loss=5.250757

Batch 180450, train_perplexity=200.08014, train_loss=5.298718

Batch 180460, train_perplexity=161.15382, train_loss=5.0823593

Batch 180470, train_perplexity=173.13057, train_loss=5.154046

Batch 180480, train_perplexity=199.0579, train_loss=5.293596

Batch 180490, train_perplexity=170.66707, train_loss=5.1397147

Batch 180500, train_perplexity=178.85934, train_loss=5.1865997

Batch 180510, train_perplexity=180.81656, train_loss=5.197483

Batch 180520, train_perplexity=208.20793, train_loss=5.338537

Batch 180530, train_perplexity=178.6006, train_loss=5.185152

Batch 180540, train_perplexity=190.09651, train_loss=5.247532

Batch 180550, train_perplexity=190.719, train_loss=5.250801

Batch 180560, train_perplexity=182.55304, train_loss=5.207041

Batch 180570, train_perplexity=201.25209, train_loss=5.3045583

Batch 180580, train_perplexity=193.49693, train_loss=5.2652617

Batch 180590, train_perplexity=207.08847, train_loss=5.333146

Batch 180600, train_perplexity=170.0373, train_loss=5.136018

Batch 180610, train_perplexity=200.46126, train_loss=5.300621

Batch 180620, train_perplexity=163.8089, train_loss=5.0987005

Batch 180630, train_perplexity=186.94777, train_loss=5.2308292

Batch 180640, train_perplexity=197.34926, train_loss=5.284975

Batch 180650, train_perplexity=191.20105, train_loss=5.2533255

Batch 180660, train_perplexity=186.11098, train_loss=5.226343

Batch 180670, train_perplexity=198.55823, train_loss=5.2910824

Batch 180680, train_perplexity=204.8586, train_loss=5.32232

Batch 180690, train_perplexity=187.5882, train_loss=5.234249

Batch 180700, train_perplexity=189.27586, train_loss=5.2432055

Batch 180710, train_perplexity=195.0817, train_loss=5.2734184

Batch 180720, train_perplexity=175.6354, train_loss=5.1684103

Batch 180730, train_perplexity=164.80423, train_loss=5.1047583

Batch 180740, train_perplexity=188.41347, train_loss=5.238639

Batch 180750, train_perplexity=200.37086, train_loss=5.30017

Batch 180760, train_perplexity=183.90694, train_loss=5.21443

Batch 180770, train_perplexity=177.51425, train_loss=5.179051

Batch 180780, train_perplexity=164.68396, train_loss=5.104028

Batch 180790, train_perplexity=193.93108, train_loss=5.267503

Batch 180800, train_perplexity=178.72984, train_loss=5.1858754

Batch 180810, train_perplexity=186.7895, train_loss=5.2299824

Batch 180820, train_perplexity=193.98601, train_loss=5.267786

Batch 180830, train_perplexity=203.61418, train_loss=5.316227

Batch 180840, train_perplexity=207.08966, train_loss=5.333152

Batch 180850, train_perplexity=180.51883, train_loss=5.195835

Batch 180860, train_perplexity=200.3293, train_loss=5.2999625

Batch 180870, train_perplexity=201.89896, train_loss=5.3077674

Batch 180880, train_perplexity=207.28526, train_loss=5.334096

Batch 180890, train_perplexity=210.77013, train_loss=5.350768

Batch 180900, train_perplexity=187.39589, train_loss=5.2332234

Batch 180910, train_perplexity=171.48029, train_loss=5.1444683

Batch 180920, train_perplexity=169.38948, train_loss=5.1322007

Batch 180930, train_perplexity=221.15163, train_loss=5.3988485

Batch 180940, train_perplexity=205.65059, train_loss=5.3261786

Batch 180950, train_perplexity=186.98503, train_loss=5.2310286

Batch 180960, train_perplexity=191.59221, train_loss=5.255369

Batch 180970, train_perplexity=180.96596, train_loss=5.198309

Batch 180980, train_perplexity=182.5736, train_loss=5.2071533
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 180990, train_perplexity=196.32442, train_loss=5.2797685

Batch 181000, train_perplexity=173.82675, train_loss=5.158059

Batch 181010, train_perplexity=188.60953, train_loss=5.239679

Batch 181020, train_perplexity=195.29837, train_loss=5.2745285

Batch 181030, train_perplexity=196.9686, train_loss=5.2830443

Batch 181040, train_perplexity=182.1699, train_loss=5.20494

Batch 181050, train_perplexity=169.37778, train_loss=5.1321316

Batch 181060, train_perplexity=206.52913, train_loss=5.3304415

Batch 181070, train_perplexity=196.75842, train_loss=5.2819767

Batch 181080, train_perplexity=198.20473, train_loss=5.2893004

Batch 181090, train_perplexity=188.95943, train_loss=5.2415323

Batch 181100, train_perplexity=214.42192, train_loss=5.3679457

Batch 181110, train_perplexity=194.16869, train_loss=5.2687273

Batch 181120, train_perplexity=175.80818, train_loss=5.1693935

Batch 181130, train_perplexity=204.64752, train_loss=5.321289

Batch 181140, train_perplexity=204.256, train_loss=5.319374

Batch 181150, train_perplexity=206.98984, train_loss=5.3326697

Batch 181160, train_perplexity=197.64967, train_loss=5.286496

Batch 181170, train_perplexity=192.43694, train_loss=5.2597685

Batch 181180, train_perplexity=188.17868, train_loss=5.237392

Batch 181190, train_perplexity=197.0119, train_loss=5.283264

Batch 181200, train_perplexity=177.30598, train_loss=5.177877

Batch 181210, train_perplexity=171.53705, train_loss=5.144799

Batch 181220, train_perplexity=185.16983, train_loss=5.2212734

Batch 181230, train_perplexity=192.64075, train_loss=5.260827

Batch 181240, train_perplexity=193.73004, train_loss=5.2664657

Batch 181250, train_perplexity=172.55678, train_loss=5.1507263

Batch 181260, train_perplexity=220.5317, train_loss=5.3960414

Batch 181270, train_perplexity=166.94499, train_loss=5.1176643

Batch 181280, train_perplexity=199.91402, train_loss=5.2978873

Batch 181290, train_perplexity=185.20198, train_loss=5.221447

Batch 181300, train_perplexity=196.85246, train_loss=5.2824545

Batch 181310, train_perplexity=201.2397, train_loss=5.304497

Batch 181320, train_perplexity=190.2553, train_loss=5.248367

Batch 181330, train_perplexity=208.97101, train_loss=5.3421955

Batch 181340, train_perplexity=200.04779, train_loss=5.2985563

Batch 181350, train_perplexity=180.02396, train_loss=5.19309

Batch 181360, train_perplexity=201.93121, train_loss=5.307927

Batch 181370, train_perplexity=187.18843, train_loss=5.2321157

Batch 181380, train_perplexity=187.26788, train_loss=5.23254

Batch 181390, train_perplexity=179.15024, train_loss=5.188225

Batch 181400, train_perplexity=208.34477, train_loss=5.3391943

Batch 181410, train_perplexity=195.70482, train_loss=5.2766075

Batch 181420, train_perplexity=196.62346, train_loss=5.2812905

Batch 181430, train_perplexity=176.23817, train_loss=5.1718364

Batch 181440, train_perplexity=188.46613, train_loss=5.2389183

Batch 181450, train_perplexity=184.92145, train_loss=5.219931

Batch 181460, train_perplexity=185.31337, train_loss=5.2220483

Batch 181470, train_perplexity=184.82378, train_loss=5.219403

Batch 181480, train_perplexity=207.56389, train_loss=5.335439

Batch 181490, train_perplexity=174.19865, train_loss=5.1601963

Batch 181500, train_perplexity=192.82649, train_loss=5.2617908

Batch 181510, train_perplexity=179.15828, train_loss=5.1882696

Batch 181520, train_perplexity=193.096, train_loss=5.2631874

Batch 181530, train_perplexity=201.19788, train_loss=5.304289

Batch 181540, train_perplexity=155.9534, train_loss=5.049557

Batch 181550, train_perplexity=216.53688, train_loss=5.377761

Batch 181560, train_perplexity=176.11746, train_loss=5.171151

Batch 181570, train_perplexity=203.03712, train_loss=5.313389

Batch 181580, train_perplexity=190.68898, train_loss=5.2506437

Batch 181590, train_perplexity=170.02295, train_loss=5.1359334

Batch 181600, train_perplexity=175.99992, train_loss=5.1704836

Batch 181610, train_perplexity=187.02783, train_loss=5.2312574

Batch 181620, train_perplexity=184.75073, train_loss=5.2190075

Batch 181630, train_perplexity=184.72148, train_loss=5.218849

Batch 181640, train_perplexity=164.8635, train_loss=5.105118

Batch 181650, train_perplexity=170.20676, train_loss=5.137014

Batch 181660, train_perplexity=190.68335, train_loss=5.250614

Batch 181670, train_perplexity=193.21664, train_loss=5.263812

Batch 181680, train_perplexity=173.14287, train_loss=5.154117

Batch 181690, train_perplexity=201.66641, train_loss=5.306615

Batch 181700, train_perplexity=193.53181, train_loss=5.265442

Batch 181710, train_perplexity=189.16173, train_loss=5.2426023

Batch 181720, train_perplexity=174.17398, train_loss=5.1600547

Batch 181730, train_perplexity=186.71756, train_loss=5.229597

Batch 181740, train_perplexity=168.65398, train_loss=5.127849

Batch 181750, train_perplexity=191.50873, train_loss=5.2549334

Batch 181760, train_perplexity=176.91673, train_loss=5.175679

Batch 181770, train_perplexity=179.22629, train_loss=5.188649

Batch 181780, train_perplexity=193.23332, train_loss=5.2638984

Batch 181790, train_perplexity=175.47543, train_loss=5.167499

Batch 181800, train_perplexity=207.3869, train_loss=5.334586

Batch 181810, train_perplexity=188.50064, train_loss=5.2391014

Batch 181820, train_perplexity=187.65163, train_loss=5.234587

Batch 181830, train_perplexity=174.86069, train_loss=5.1639895

Batch 181840, train_perplexity=180.91833, train_loss=5.1980457

Batch 181850, train_perplexity=176.56833, train_loss=5.173708

Batch 181860, train_perplexity=173.08665, train_loss=5.1537924

Batch 181870, train_perplexity=188.44186, train_loss=5.2387896

Batch 181880, train_perplexity=205.40988, train_loss=5.3250074

Batch 181890, train_perplexity=198.28328, train_loss=5.2896967

Batch 181900, train_perplexity=177.41026, train_loss=5.178465

Batch 181910, train_perplexity=184.52199, train_loss=5.2177687

Batch 181920, train_perplexity=201.06917, train_loss=5.303649

Batch 181930, train_perplexity=191.44481, train_loss=5.2545996

Batch 181940, train_perplexity=173.70975, train_loss=5.157386

Batch 181950, train_perplexity=190.76901, train_loss=5.2510633

Batch 181960, train_perplexity=206.19171, train_loss=5.3288064

Batch 181970, train_perplexity=193.71352, train_loss=5.2663803

Batch 181980, train_perplexity=192.63469, train_loss=5.2607956

Batch 181990, train_perplexity=203.02202, train_loss=5.3133144

Batch 182000, train_perplexity=201.96896, train_loss=5.308114

Batch 182010, train_perplexity=189.86224, train_loss=5.246299

Batch 182020, train_perplexity=175.8335, train_loss=5.1695375

Batch 182030, train_perplexity=183.57909, train_loss=5.2126455

Batch 182040, train_perplexity=180.14934, train_loss=5.193786

Batch 182050, train_perplexity=206.05667, train_loss=5.328151

Batch 182060, train_perplexity=181.9461, train_loss=5.2037106

Batch 182070, train_perplexity=169.7162, train_loss=5.1341276

Batch 182080, train_perplexity=188.80586, train_loss=5.2407193

Batch 182090, train_perplexity=186.59544, train_loss=5.228943

Batch 182100, train_perplexity=191.5743, train_loss=5.2552757

Batch 182110, train_perplexity=191.51804, train_loss=5.254982

Batch 182120, train_perplexity=190.14183, train_loss=5.2477703

Batch 182130, train_perplexity=216.54773, train_loss=5.377811

Batch 182140, train_perplexity=166.58952, train_loss=5.115533

Batch 182150, train_perplexity=185.81145, train_loss=5.2247324

Batch 182160, train_perplexity=158.23114, train_loss=5.064057

Batch 182170, train_perplexity=171.76147, train_loss=5.1461067

Batch 182180, train_perplexity=165.15257, train_loss=5.1068697

Batch 182190, train_perplexity=178.29224, train_loss=5.183424

Batch 182200, train_perplexity=210.61722, train_loss=5.3500423

Batch 182210, train_perplexity=201.02823, train_loss=5.3034453

Batch 182220, train_perplexity=159.37813, train_loss=5.0712795

Batch 182230, train_perplexity=186.65738, train_loss=5.2292747

Batch 182240, train_perplexity=190.49059, train_loss=5.249603

Batch 182250, train_perplexity=206.6171, train_loss=5.3308673

Batch 182260, train_perplexity=184.20982, train_loss=5.2160754

Batch 182270, train_perplexity=201.05997, train_loss=5.303603

Batch 182280, train_perplexity=172.72792, train_loss=5.1517177
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 182290, train_perplexity=197.51053, train_loss=5.285792

Batch 182300, train_perplexity=181.19261, train_loss=5.1995606

Batch 182310, train_perplexity=181.00255, train_loss=5.198511

Batch 182320, train_perplexity=185.15889, train_loss=5.2212143

Batch 182330, train_perplexity=182.70248, train_loss=5.207859

Batch 182340, train_perplexity=195.16618, train_loss=5.2738514

Batch 182350, train_perplexity=204.34532, train_loss=5.3198113

Batch 182360, train_perplexity=167.1383, train_loss=5.1188216

Batch 182370, train_perplexity=204.20691, train_loss=5.3191338

Batch 182380, train_perplexity=177.42566, train_loss=5.1785517

Batch 182390, train_perplexity=198.5622, train_loss=5.2911024

Batch 182400, train_perplexity=177.3184, train_loss=5.177947

Batch 182410, train_perplexity=186.16974, train_loss=5.226659

Batch 182420, train_perplexity=196.38368, train_loss=5.2800703

Batch 182430, train_perplexity=185.02553, train_loss=5.220494

Batch 182440, train_perplexity=164.77971, train_loss=5.1046095

Batch 182450, train_perplexity=201.0893, train_loss=5.303749

Batch 182460, train_perplexity=190.8367, train_loss=5.251418

Batch 182470, train_perplexity=197.62498, train_loss=5.286371

Batch 182480, train_perplexity=168.53154, train_loss=5.127123

Batch 182490, train_perplexity=189.8503, train_loss=5.246236

Batch 182500, train_perplexity=193.42128, train_loss=5.2648706

Batch 182510, train_perplexity=173.67978, train_loss=5.157213

Batch 182520, train_perplexity=193.80794, train_loss=5.2668676

Batch 182530, train_perplexity=181.78741, train_loss=5.202838

Batch 182540, train_perplexity=187.37543, train_loss=5.2331142

Batch 182550, train_perplexity=196.28108, train_loss=5.2795477

Batch 182560, train_perplexity=195.60359, train_loss=5.27609

Batch 182570, train_perplexity=184.25154, train_loss=5.216302

Batch 182580, train_perplexity=182.30981, train_loss=5.2057076

Batch 182590, train_perplexity=188.16531, train_loss=5.237321

Batch 182600, train_perplexity=193.15198, train_loss=5.2634773

Batch 182610, train_perplexity=196.91724, train_loss=5.2827835

Batch 182620, train_perplexity=190.84726, train_loss=5.2514734

Batch 182630, train_perplexity=161.31735, train_loss=5.0833735

Batch 182640, train_perplexity=193.46768, train_loss=5.2651105

Batch 182650, train_perplexity=198.0086, train_loss=5.2883105

Batch 182660, train_perplexity=190.5799, train_loss=5.2500715

Batch 182670, train_perplexity=173.3303, train_loss=5.155199

Batch 182680, train_perplexity=176.27306, train_loss=5.1720343

Batch 182690, train_perplexity=178.63382, train_loss=5.185338

Batch 182700, train_perplexity=177.38522, train_loss=5.1783237

Batch 182710, train_perplexity=223.72029, train_loss=5.4103966

Batch 182720, train_perplexity=201.14243, train_loss=5.3040133

Batch 182730, train_perplexity=184.68889, train_loss=5.2186728

Batch 182740, train_perplexity=202.43321, train_loss=5.31041

Batch 182750, train_perplexity=187.44916, train_loss=5.2335076

Batch 182760, train_perplexity=180.74208, train_loss=5.197071

Batch 182770, train_perplexity=218.35574, train_loss=5.3861256

Batch 182780, train_perplexity=187.16522, train_loss=5.231992

Batch 182790, train_perplexity=224.41692, train_loss=5.4135056

Batch 182800, train_perplexity=174.89561, train_loss=5.1641893

Batch 182810, train_perplexity=190.86964, train_loss=5.2515907

Batch 182820, train_perplexity=207.6717, train_loss=5.3359585

Batch 182830, train_perplexity=189.99448, train_loss=5.246995

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Batch 182840, train_perplexity=182.48865, train_loss=5.206688

Batch 182850, train_perplexity=196.6483, train_loss=5.281417

Batch 182860, train_perplexity=186.77704, train_loss=5.2299156

Batch 182870, train_perplexity=182.7328, train_loss=5.208025

Batch 182880, train_perplexity=185.86018, train_loss=5.2249947

Batch 182890, train_perplexity=204.83585, train_loss=5.322209

Batch 182900, train_perplexity=174.70708, train_loss=5.1631107

Batch 182910, train_perplexity=189.31026, train_loss=5.243387

Batch 182920, train_perplexity=201.2257, train_loss=5.304427

Batch 182930, train_perplexity=189.48709, train_loss=5.244321

Batch 182940, train_perplexity=195.6549, train_loss=5.2763524

Batch 182950, train_perplexity=165.45303, train_loss=5.1086874

Batch 182960, train_perplexity=198.49025, train_loss=5.29074

Batch 182970, train_perplexity=179.37044, train_loss=5.189453

Batch 182980, train_perplexity=183.17545, train_loss=5.2104445

Batch 182990, train_perplexity=176.39565, train_loss=5.1727295

Batch 183000, train_perplexity=186.15625, train_loss=5.2265863

Batch 183010, train_perplexity=186.14497, train_loss=5.226526

Batch 183020, train_perplexity=156.95763, train_loss=5.055976

Batch 183030, train_perplexity=177.28206, train_loss=5.177742

Batch 183040, train_perplexity=194.91898, train_loss=5.272584

Batch 183050, train_perplexity=200.77542, train_loss=5.302187

Batch 183060, train_perplexity=182.15845, train_loss=5.204877

Batch 183070, train_perplexity=201.08182, train_loss=5.303712

Batch 183080, train_perplexity=194.86656, train_loss=5.272315

Batch 183090, train_perplexity=188.84953, train_loss=5.2409506

Batch 183100, train_perplexity=192.90382, train_loss=5.262192

Batch 183110, train_perplexity=192.38766, train_loss=5.2595124

Batch 183120, train_perplexity=169.3613, train_loss=5.1320343

Batch 183130, train_perplexity=184.94711, train_loss=5.22007

Batch 183140, train_perplexity=175.3218, train_loss=5.166623

Batch 183150, train_perplexity=175.80165, train_loss=5.1693563

Batch 183160, train_perplexity=185.57166, train_loss=5.223441

Batch 183170, train_perplexity=213.62427, train_loss=5.3642187

Batch 183180, train_perplexity=204.38362, train_loss=5.3199987

Batch 183190, train_perplexity=200.91026, train_loss=5.3028584

Batch 183200, train_perplexity=177.2753, train_loss=5.177704

Batch 183210, train_perplexity=187.25574, train_loss=5.2324753

Batch 183220, train_perplexity=193.5128, train_loss=5.2653437

Batch 183230, train_perplexity=199.85835, train_loss=5.297609

Batch 183240, train_perplexity=181.82036, train_loss=5.203019

Batch 183250, train_perplexity=178.96385, train_loss=5.187184

Batch 183260, train_perplexity=185.3474, train_loss=5.222232

Batch 183270, train_perplexity=204.88849, train_loss=5.322466

Batch 183280, train_perplexity=176.62172, train_loss=5.1740103

Batch 183290, train_perplexity=192.09204, train_loss=5.2579746

Batch 183300, train_perplexity=202.55643, train_loss=5.3110185

Batch 183310, train_perplexity=191.1933, train_loss=5.253285

Batch 183320, train_perplexity=205.16751, train_loss=5.323827

Batch 183330, train_perplexity=183.07312, train_loss=5.2098856

Batch 183340, train_perplexity=184.79054, train_loss=5.219223

Batch 183350, train_perplexity=174.85092, train_loss=5.1639338

Batch 183360, train_perplexity=187.00125, train_loss=5.2311153

Batch 183370, train_perplexity=189.06253, train_loss=5.242078

Batch 183380, train_perplexity=202.91826, train_loss=5.3128033

Batch 183390, train_perplexity=181.08353, train_loss=5.1989584

Batch 183400, train_perplexity=199.52916, train_loss=5.2959604

Batch 183410, train_perplexity=192.22078, train_loss=5.2586446

Batch 183420, train_perplexity=187.79378, train_loss=5.2353444

Batch 183430, train_perplexity=198.78833, train_loss=5.2922406

Batch 183440, train_perplexity=192.58528, train_loss=5.260539

Batch 183450, train_perplexity=198.12071, train_loss=5.2888765

Batch 183460, train_perplexity=172.76277, train_loss=5.1519194

Batch 183470, train_perplexity=202.26224, train_loss=5.309565

Batch 183480, train_perplexity=182.09279, train_loss=5.2045164

Batch 183490, train_perplexity=196.31525, train_loss=5.2797217

Batch 183500, train_perplexity=185.92046, train_loss=5.225319

Batch 183510, train_perplexity=213.3832, train_loss=5.3630896

Batch 183520, train_perplexity=179.4458, train_loss=5.189873
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 183530, train_perplexity=157.38242, train_loss=5.0586786

Batch 183540, train_perplexity=177.55336, train_loss=5.179271

Batch 183550, train_perplexity=189.15398, train_loss=5.2425613

Batch 183560, train_perplexity=158.31529, train_loss=5.0645885

Batch 183570, train_perplexity=199.298, train_loss=5.294801

Batch 183580, train_perplexity=181.6299, train_loss=5.201971

Batch 183590, train_perplexity=185.48479, train_loss=5.222973

Batch 183600, train_perplexity=186.7425, train_loss=5.2297306

Batch 183610, train_perplexity=195.94144, train_loss=5.277816

Batch 183620, train_perplexity=175.16394, train_loss=5.1657224

Batch 183630, train_perplexity=194.22221, train_loss=5.269003

Batch 183640, train_perplexity=206.9258, train_loss=5.3323603

Batch 183650, train_perplexity=193.71794, train_loss=5.266403

Batch 183660, train_perplexity=177.86826, train_loss=5.181043

Batch 183670, train_perplexity=205.71277, train_loss=5.326481

Batch 183680, train_perplexity=187.87044, train_loss=5.2357526

Batch 183690, train_perplexity=197.796, train_loss=5.287236

Batch 183700, train_perplexity=214.97403, train_loss=5.3705173

Batch 183710, train_perplexity=186.3463, train_loss=5.227607

Batch 183720, train_perplexity=193.4211, train_loss=5.2648697

Batch 183730, train_perplexity=172.02359, train_loss=5.1476316

Batch 183740, train_perplexity=162.11647, train_loss=5.088315

Batch 183750, train_perplexity=183.13676, train_loss=5.210233

Batch 183760, train_perplexity=180.8445, train_loss=5.1976376

Batch 183770, train_perplexity=192.40685, train_loss=5.259612

Batch 183780, train_perplexity=195.82132, train_loss=5.2772026

Batch 183790, train_perplexity=176.9882, train_loss=5.176083

Batch 183800, train_perplexity=186.24077, train_loss=5.2270403

Batch 183810, train_perplexity=194.78314, train_loss=5.271887

Batch 183820, train_perplexity=159.19759, train_loss=5.070146

Batch 183830, train_perplexity=207.83803, train_loss=5.336759

Batch 183840, train_perplexity=190.00897, train_loss=5.2470713

Batch 183850, train_perplexity=203.55527, train_loss=5.3159375

Batch 183860, train_perplexity=175.80818, train_loss=5.1693935

Batch 183870, train_perplexity=190.70235, train_loss=5.250714

Batch 183880, train_perplexity=179.82826, train_loss=5.1920023

Batch 183890, train_perplexity=176.9861, train_loss=5.176071

Batch 183900, train_perplexity=205.0526, train_loss=5.3232665

Batch 183910, train_perplexity=180.04225, train_loss=5.1931915

Batch 183920, train_perplexity=176.68524, train_loss=5.17437

Batch 183930, train_perplexity=175.12828, train_loss=5.1655188

Batch 183940, train_perplexity=202.40648, train_loss=5.310278

Batch 183950, train_perplexity=202.70464, train_loss=5.31175

Batch 183960, train_perplexity=165.82979, train_loss=5.110962

Batch 183970, train_perplexity=173.69402, train_loss=5.157295

Batch 183980, train_perplexity=215.93494, train_loss=5.374977

Batch 183990, train_perplexity=179.37762, train_loss=5.189493

Batch 184000, train_perplexity=165.1606, train_loss=5.1069183

Batch 184010, train_perplexity=189.55522, train_loss=5.2446804

Batch 184020, train_perplexity=194.24536, train_loss=5.269122

Batch 184030, train_perplexity=146.30255, train_loss=4.985677

Batch 184040, train_perplexity=206.21246, train_loss=5.328907

Batch 184050, train_perplexity=188.91888, train_loss=5.2413177

Batch 184060, train_perplexity=202.35919, train_loss=5.3100443

Batch 184070, train_perplexity=190.48668, train_loss=5.2495823

Batch 184080, train_perplexity=189.38683, train_loss=5.2437916

Batch 184090, train_perplexity=197.15363, train_loss=5.283983

Batch 184100, train_perplexity=194.81639, train_loss=5.2720575

Batch 184110, train_perplexity=183.16374, train_loss=5.2103806

Batch 184120, train_perplexity=210.35004, train_loss=5.348773

Batch 184130, train_perplexity=182.40051, train_loss=5.206205

Batch 184140, train_perplexity=184.29514, train_loss=5.2165384

Batch 184150, train_perplexity=181.47198, train_loss=5.2011013

Batch 184160, train_perplexity=181.3348, train_loss=5.200345

Batch 184170, train_perplexity=194.11378, train_loss=5.2684445

Batch 184180, train_perplexity=198.64952, train_loss=5.291542

Batch 184190, train_perplexity=181.17914, train_loss=5.1994863

Batch 184200, train_perplexity=185.87843, train_loss=5.225093

Batch 184210, train_perplexity=178.36656, train_loss=5.1838408

Batch 184220, train_perplexity=175.84256, train_loss=5.169589

Batch 184230, train_perplexity=195.9093, train_loss=5.277652

Batch 184240, train_perplexity=205.0437, train_loss=5.323223

Batch 184250, train_perplexity=174.94826, train_loss=5.16449

Batch 184260, train_perplexity=168.27345, train_loss=5.1255903

Batch 184270, train_perplexity=201.3764, train_loss=5.305176

Batch 184280, train_perplexity=204.62157, train_loss=5.321162

Batch 184290, train_perplexity=188.97276, train_loss=5.241603

Batch 184300, train_perplexity=192.18896, train_loss=5.258479

Batch 184310, train_perplexity=183.97623, train_loss=5.2148066

Batch 184320, train_perplexity=180.49164, train_loss=5.1956844

Batch 184330, train_perplexity=187.81929, train_loss=5.2354803

Batch 184340, train_perplexity=192.03654, train_loss=5.2576857

Batch 184350, train_perplexity=179.90005, train_loss=5.1924014

Batch 184360, train_perplexity=185.9873, train_loss=5.2256784

Batch 184370, train_perplexity=153.42, train_loss=5.0331793

Batch 184380, train_perplexity=206.25258, train_loss=5.3291016

Batch 184390, train_perplexity=192.95241, train_loss=5.2624435

Batch 184400, train_perplexity=200.44846, train_loss=5.300557

Batch 184410, train_perplexity=196.60977, train_loss=5.281221

Batch 184420, train_perplexity=176.01721, train_loss=5.170582

Batch 184430, train_perplexity=180.01186, train_loss=5.1930227

Batch 184440, train_perplexity=174.29918, train_loss=5.1607733

Batch 184450, train_perplexity=191.49065, train_loss=5.254839

Batch 184460, train_perplexity=205.12701, train_loss=5.3236294

Batch 184470, train_perplexity=210.34512, train_loss=5.3487496

Batch 184480, train_perplexity=184.15756, train_loss=5.2157917

Batch 184490, train_perplexity=196.18134, train_loss=5.2790394

Batch 184500, train_perplexity=200.06688, train_loss=5.2986517

Batch 184510, train_perplexity=189.6735, train_loss=5.245304

Batch 184520, train_perplexity=181.55144, train_loss=5.201539

Batch 184530, train_perplexity=190.7088, train_loss=5.2507477

Batch 184540, train_perplexity=184.94835, train_loss=5.2200766

Batch 184550, train_perplexity=200.3124, train_loss=5.299878

Batch 184560, train_perplexity=175.57495, train_loss=5.168066

Batch 184570, train_perplexity=182.68732, train_loss=5.207776

Batch 184580, train_perplexity=169.65341, train_loss=5.1337576

Batch 184590, train_perplexity=196.6048, train_loss=5.2811956

Batch 184600, train_perplexity=209.60551, train_loss=5.3452272

Batch 184610, train_perplexity=193.69135, train_loss=5.266266

Batch 184620, train_perplexity=189.15073, train_loss=5.242544

Batch 184630, train_perplexity=190.26355, train_loss=5.24841

Batch 184640, train_perplexity=210.36348, train_loss=5.348837

Batch 184650, train_perplexity=177.86664, train_loss=5.181034

Batch 184660, train_perplexity=193.37794, train_loss=5.2646465

Batch 184670, train_perplexity=191.04156, train_loss=5.252491

Batch 184680, train_perplexity=172.40775, train_loss=5.1498623

Batch 184690, train_perplexity=180.84225, train_loss=5.197625

Batch 184700, train_perplexity=162.52747, train_loss=5.090847

Batch 184710, train_perplexity=180.52959, train_loss=5.1958947

Batch 184720, train_perplexity=187.55501, train_loss=5.234072

Batch 184730, train_perplexity=181.17827, train_loss=5.1994815

Batch 184740, train_perplexity=180.46048, train_loss=5.195512

Batch 184750, train_perplexity=187.75598, train_loss=5.235143

Batch 184760, train_perplexity=171.7166, train_loss=5.1458454

Batch 184770, train_perplexity=179.07388, train_loss=5.1877985

Batch 184780, train_perplexity=166.74704, train_loss=5.116478

Batch 184790, train_perplexity=179.58627, train_loss=5.1906557

Batch 184800, train_perplexity=190.65788, train_loss=5.2504807

Batch 184810, train_perplexity=206.66528, train_loss=5.3311005

Batch 184820, train_perplexity=173.87193, train_loss=5.158319
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 184830, train_perplexity=169.37996, train_loss=5.1321445

Batch 184840, train_perplexity=179.72179, train_loss=5.19141

Batch 184850, train_perplexity=175.60794, train_loss=5.168254

Batch 184860, train_perplexity=195.6565, train_loss=5.2763605

Batch 184870, train_perplexity=201.69478, train_loss=5.3067555

Batch 184880, train_perplexity=187.06601, train_loss=5.2314615

Batch 184890, train_perplexity=200.62497, train_loss=5.3014374

Batch 184900, train_perplexity=202.48795, train_loss=5.3106804

Batch 184910, train_perplexity=224.73068, train_loss=5.4149027

Batch 184920, train_perplexity=187.06128, train_loss=5.2314363

Batch 184930, train_perplexity=187.30075, train_loss=5.2327156

Batch 184940, train_perplexity=198.56561, train_loss=5.2911196

Batch 184950, train_perplexity=190.24596, train_loss=5.2483177

Batch 184960, train_perplexity=196.74716, train_loss=5.2819195

Batch 184970, train_perplexity=179.25928, train_loss=5.188833

Batch 184980, train_perplexity=188.12799, train_loss=5.2371225

Batch 184990, train_perplexity=178.64507, train_loss=5.185401

Batch 185000, train_perplexity=195.18814, train_loss=5.273964

Batch 185010, train_perplexity=180.96336, train_loss=5.1982946

Batch 185020, train_perplexity=198.4596, train_loss=5.2905855

Batch 185030, train_perplexity=205.5594, train_loss=5.325735

Batch 185040, train_perplexity=194.22832, train_loss=5.2690344

Batch 185050, train_perplexity=189.96712, train_loss=5.246851

Batch 185060, train_perplexity=197.30052, train_loss=5.284728

Batch 185070, train_perplexity=193.49342, train_loss=5.2652435

Batch 185080, train_perplexity=184.95363, train_loss=5.220105

Batch 185090, train_perplexity=210.16937, train_loss=5.3479137

Batch 185100, train_perplexity=201.62285, train_loss=5.306399

Batch 185110, train_perplexity=191.56224, train_loss=5.255213

Batch 185120, train_perplexity=182.39703, train_loss=5.206186

Batch 185130, train_perplexity=176.85533, train_loss=5.175332

Batch 185140, train_perplexity=181.0155, train_loss=5.1985826

Batch 185150, train_perplexity=192.65894, train_loss=5.2609215

Batch 185160, train_perplexity=184.69594, train_loss=5.218711

Batch 185170, train_perplexity=176.78249, train_loss=5.17492

Batch 185180, train_perplexity=176.93681, train_loss=5.1757927

Batch 185190, train_perplexity=189.78043, train_loss=5.2458677

Batch 185200, train_perplexity=180.6318, train_loss=5.1964607

Batch 185210, train_perplexity=180.60252, train_loss=5.1962986

Batch 185220, train_perplexity=167.69208, train_loss=5.1221294

Batch 185230, train_perplexity=181.16385, train_loss=5.199402

Batch 185240, train_perplexity=176.73833, train_loss=5.17467

Batch 185250, train_perplexity=197.73338, train_loss=5.2869196

Batch 185260, train_perplexity=197.82053, train_loss=5.28736

Batch 185270, train_perplexity=191.72838, train_loss=5.2560797

Batch 185280, train_perplexity=175.98918, train_loss=5.1704226

Batch 185290, train_perplexity=193.43161, train_loss=5.264924

Batch 185300, train_perplexity=184.68097, train_loss=5.21863

Batch 185310, train_perplexity=194.37628, train_loss=5.269796

Batch 185320, train_perplexity=177.12, train_loss=5.1768274

Batch 185330, train_perplexity=194.47687, train_loss=5.2703133

Batch 185340, train_perplexity=188.70181, train_loss=5.240168

Batch 185350, train_perplexity=203.68178, train_loss=5.316559

Batch 185360, train_perplexity=187.7644, train_loss=5.235188

Batch 185370, train_perplexity=203.92862, train_loss=5.31777

Batch 185380, train_perplexity=188.92555, train_loss=5.241353

Batch 185390, train_perplexity=198.9968, train_loss=5.2932887

Batch 185400, train_perplexity=174.96277, train_loss=5.164573

Batch 185410, train_perplexity=175.54347, train_loss=5.1678867

Batch 185420, train_perplexity=172.99498, train_loss=5.1532626

Batch 185430, train_perplexity=196.99875, train_loss=5.2831974

Batch 185440, train_perplexity=193.59033, train_loss=5.265744

Batch 185450, train_perplexity=174.9254, train_loss=5.1643596

Batch 185460, train_perplexity=206.95284, train_loss=5.332491

Batch 185470, train_perplexity=193.79343, train_loss=5.266793

Batch 185480, train_perplexity=195.12114, train_loss=5.2736206

Batch 185490, train_perplexity=209.61931, train_loss=5.345293

Batch 185500, train_perplexity=178.90524, train_loss=5.1868563

Batch 185510, train_perplexity=188.47215, train_loss=5.2389503

Batch 185520, train_perplexity=194.8451, train_loss=5.272205

Batch 185530, train_perplexity=198.99367, train_loss=5.293273

Batch 185540, train_perplexity=191.276, train_loss=5.2537174

Batch 185550, train_perplexity=179.60905, train_loss=5.1907825

Batch 185560, train_perplexity=187.17575, train_loss=5.232048

Batch 185570, train_perplexity=203.96362, train_loss=5.3179417

Batch 185580, train_perplexity=188.49057, train_loss=5.239048

Batch 185590, train_perplexity=188.00665, train_loss=5.2364774

Batch 185600, train_perplexity=197.0072, train_loss=5.2832403

Batch 185610, train_perplexity=196.78995, train_loss=5.282137

Batch 185620, train_perplexity=203.22125, train_loss=5.3142953

Batch 185630, train_perplexity=208.17416, train_loss=5.338375

Batch 185640, train_perplexity=187.34094, train_loss=5.23293

Batch 185650, train_perplexity=214.07478, train_loss=5.3663254

Batch 185660, train_perplexity=160.79674, train_loss=5.080141

Batch 185670, train_perplexity=184.14194, train_loss=5.215707

Batch 185680, train_perplexity=178.0816, train_loss=5.182242

Batch 185690, train_perplexity=168.44589, train_loss=5.1266146

Batch 185700, train_perplexity=186.68123, train_loss=5.2294025

Batch 185710, train_perplexity=180.12572, train_loss=5.193655

Batch 185720, train_perplexity=173.1692, train_loss=5.154269

Batch 185730, train_perplexity=202.03976, train_loss=5.3084645

Batch 185740, train_perplexity=190.50175, train_loss=5.2496614

Batch 185750, train_perplexity=188.47997, train_loss=5.2389917

Batch 185760, train_perplexity=170.6634, train_loss=5.1396933

Batch 185770, train_perplexity=173.93271, train_loss=5.1586685

Batch 185780, train_perplexity=190.76775, train_loss=5.2510567

Batch 185790, train_perplexity=179.74399, train_loss=5.1915336

Batch 185800, train_perplexity=206.38068, train_loss=5.3297224

Batch 185810, train_perplexity=192.71921, train_loss=5.2612343

Batch 185820, train_perplexity=206.14336, train_loss=5.328572

Batch 185830, train_perplexity=197.93016, train_loss=5.2879143

Batch 185840, train_perplexity=217.729, train_loss=5.383251

Batch 185850, train_perplexity=176.36151, train_loss=5.172536

Batch 185860, train_perplexity=184.56186, train_loss=5.2179847

Batch 185870, train_perplexity=170.66553, train_loss=5.1397057

Batch 185880, train_perplexity=201.57257, train_loss=5.3061495

Batch 185890, train_perplexity=198.81175, train_loss=5.2923584

Batch 185900, train_perplexity=202.29088, train_loss=5.3097067

Batch 185910, train_perplexity=206.52342, train_loss=5.330414

Batch 185920, train_perplexity=173.39114, train_loss=5.15555

Batch 185930, train_perplexity=164.26413, train_loss=5.1014757

Batch 185940, train_perplexity=194.44368, train_loss=5.2701426

Batch 185950, train_perplexity=164.87025, train_loss=5.105159

Batch 185960, train_perplexity=206.00372, train_loss=5.327894

Batch 185970, train_perplexity=201.08412, train_loss=5.3037233

Batch 185980, train_perplexity=182.3413, train_loss=5.20588

Batch 185990, train_perplexity=185.55812, train_loss=5.223368

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Batch 186000, train_perplexity=182.919, train_loss=5.2090435

Batch 186010, train_perplexity=180.89255, train_loss=5.197903

Batch 186020, train_perplexity=203.58759, train_loss=5.3160963

Batch 186030, train_perplexity=175.14716, train_loss=5.1656265

Batch 186040, train_perplexity=189.42458, train_loss=5.243991

Batch 186050, train_perplexity=166.8705, train_loss=5.117218

Batch 186060, train_perplexity=183.6514, train_loss=5.2130394
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 186070, train_perplexity=193.88373, train_loss=5.2672586

Batch 186080, train_perplexity=175.08086, train_loss=5.165248

Batch 186090, train_perplexity=193.38661, train_loss=5.2646914

Batch 186100, train_perplexity=189.79237, train_loss=5.2459307

Batch 186110, train_perplexity=178.45827, train_loss=5.184355

Batch 186120, train_perplexity=187.53937, train_loss=5.233989

Batch 186130, train_perplexity=207.08028, train_loss=5.3331065

Batch 186140, train_perplexity=193.39749, train_loss=5.2647476

Batch 186150, train_perplexity=187.94669, train_loss=5.2361584

Batch 186160, train_perplexity=185.828, train_loss=5.2248216

Batch 186170, train_perplexity=180.615, train_loss=5.1963677

Batch 186180, train_perplexity=200.48766, train_loss=5.3007526

Batch 186190, train_perplexity=157.65762, train_loss=5.0604258

Batch 186200, train_perplexity=192.86244, train_loss=5.261977

Batch 186210, train_perplexity=174.40735, train_loss=5.1613936

Batch 186220, train_perplexity=207.92418, train_loss=5.3371735

Batch 186230, train_perplexity=164.18176, train_loss=5.100974

Batch 186240, train_perplexity=183.92615, train_loss=5.2145343

Batch 186250, train_perplexity=200.7668, train_loss=5.302144

Batch 186260, train_perplexity=184.28168, train_loss=5.2164655

Batch 186270, train_perplexity=183.78079, train_loss=5.2137437

Batch 186280, train_perplexity=199.36111, train_loss=5.295118

Batch 186290, train_perplexity=179.87971, train_loss=5.1922884

Batch 186300, train_perplexity=195.74178, train_loss=5.2767963

Batch 186310, train_perplexity=190.5659, train_loss=5.249998

Batch 186320, train_perplexity=189.47606, train_loss=5.2442627

Batch 186330, train_perplexity=217.2072, train_loss=5.3808517

Batch 186340, train_perplexity=190.38889, train_loss=5.2490687

Batch 186350, train_perplexity=185.47293, train_loss=5.222909

Batch 186360, train_perplexity=193.29553, train_loss=5.26422

Batch 186370, train_perplexity=214.17505, train_loss=5.3667936

Batch 186380, train_perplexity=175.32463, train_loss=5.1666393

Batch 186390, train_perplexity=168.66266, train_loss=5.1279006

Batch 186400, train_perplexity=200.09474, train_loss=5.298791

Batch 186410, train_perplexity=202.93123, train_loss=5.312867

Batch 186420, train_perplexity=191.33876, train_loss=5.2540455

Batch 186430, train_perplexity=189.10536, train_loss=5.2423043

Batch 186440, train_perplexity=212.34361, train_loss=5.358206

Batch 186450, train_perplexity=186.33226, train_loss=5.2275314

Batch 186460, train_perplexity=191.42874, train_loss=5.2545156

Batch 186470, train_perplexity=188.67348, train_loss=5.240018

Batch 186480, train_perplexity=204.52675, train_loss=5.3206987

Batch 186490, train_perplexity=205.62, train_loss=5.32603

Batch 186500, train_perplexity=170.91327, train_loss=5.141156

Batch 186510, train_perplexity=185.34386, train_loss=5.222213

Batch 186520, train_perplexity=183.6204, train_loss=5.2128706

Batch 186530, train_perplexity=166.15685, train_loss=5.112932

Batch 186540, train_perplexity=197.54585, train_loss=5.2859707

Batch 186550, train_perplexity=156.53578, train_loss=5.0532846

Batch 186560, train_perplexity=176.84445, train_loss=5.1752706

Batch 186570, train_perplexity=185.14, train_loss=5.2211123

Batch 186580, train_perplexity=178.92178, train_loss=5.186949

Batch 186590, train_perplexity=197.22226, train_loss=5.2843313

Batch 186600, train_perplexity=198.80284, train_loss=5.2923136

Batch 186610, train_perplexity=199.11154, train_loss=5.293865

Batch 186620, train_perplexity=182.70657, train_loss=5.2078815

Batch 186630, train_perplexity=177.31647, train_loss=5.177936

Batch 186640, train_perplexity=172.53621, train_loss=5.150607

Batch 186650, train_perplexity=179.3931, train_loss=5.1895795

Batch 186660, train_perplexity=199.17877, train_loss=5.294203

Batch 186670, train_perplexity=209.19414, train_loss=5.3432627

Batch 186680, train_perplexity=166.50534, train_loss=5.1150274

Batch 186690, train_perplexity=195.89015, train_loss=5.277554

Batch 186700, train_perplexity=188.77751, train_loss=5.240569

Batch 186710, train_perplexity=179.88419, train_loss=5.192313

Batch 186720, train_perplexity=204.15395, train_loss=5.3188744

Batch 186730, train_perplexity=163.10898, train_loss=5.0944185

Batch 186740, train_perplexity=185.35677, train_loss=5.2222824

Batch 186750, train_perplexity=190.83006, train_loss=5.2513833

Batch 186760, train_perplexity=198.16682, train_loss=5.289109

Batch 186770, train_perplexity=204.0866, train_loss=5.3185444

Batch 186780, train_perplexity=198.243, train_loss=5.2894936

Batch 186790, train_perplexity=206.7534, train_loss=5.3315268

Batch 186800, train_perplexity=195.32295, train_loss=5.2746544

Batch 186810, train_perplexity=206.76364, train_loss=5.3315763

Batch 186820, train_perplexity=191.1788, train_loss=5.253209

Batch 186830, train_perplexity=183.67838, train_loss=5.2131863

Batch 186840, train_perplexity=182.06258, train_loss=5.2043505

Batch 186850, train_perplexity=182.44305, train_loss=5.206438

Batch 186860, train_perplexity=169.02261, train_loss=5.1300325

Batch 186870, train_perplexity=166.71843, train_loss=5.1163063

Batch 186880, train_perplexity=194.32207, train_loss=5.269517

Batch 186890, train_perplexity=179.27364, train_loss=5.1889133

Batch 186900, train_perplexity=179.84283, train_loss=5.1920834

Batch 186910, train_perplexity=197.81958, train_loss=5.2873554

Batch 186920, train_perplexity=196.40016, train_loss=5.280154

Batch 186930, train_perplexity=191.31631, train_loss=5.253928

Batch 186940, train_perplexity=187.18584, train_loss=5.232102

Batch 186950, train_perplexity=184.88556, train_loss=5.219737

Batch 186960, train_perplexity=186.46309, train_loss=5.2282333

Batch 186970, train_perplexity=171.71152, train_loss=5.145816

Batch 186980, train_perplexity=206.76976, train_loss=5.331606

Batch 186990, train_perplexity=169.14282, train_loss=5.1307435

Batch 187000, train_perplexity=168.0579, train_loss=5.1243086

Batch 187010, train_perplexity=182.77437, train_loss=5.2082524

Batch 187020, train_perplexity=206.0776, train_loss=5.328253

Batch 187030, train_perplexity=171.63817, train_loss=5.1453886

Batch 187040, train_perplexity=177.95293, train_loss=5.181519

Batch 187050, train_perplexity=186.60257, train_loss=5.228981

Batch 187060, train_perplexity=179.9868, train_loss=5.1928835

Batch 187070, train_perplexity=166.26154, train_loss=5.113562

Batch 187080, train_perplexity=189.49486, train_loss=5.244362

Batch 187090, train_perplexity=176.34822, train_loss=5.1724606

Batch 187100, train_perplexity=192.93915, train_loss=5.262375

Batch 187110, train_perplexity=201.449, train_loss=5.3055363

Batch 187120, train_perplexity=183.10909, train_loss=5.210082

Batch 187130, train_perplexity=171.68335, train_loss=5.145652

Batch 187140, train_perplexity=180.7345, train_loss=5.197029

Batch 187150, train_perplexity=210.9316, train_loss=5.351534

Batch 187160, train_perplexity=167.23485, train_loss=5.119399

Batch 187170, train_perplexity=188.07121, train_loss=5.2368207

Batch 187180, train_perplexity=178.3856, train_loss=5.1839476

Batch 187190, train_perplexity=203.60895, train_loss=5.316201

Batch 187200, train_perplexity=187.25217, train_loss=5.232456

Batch 187210, train_perplexity=191.74457, train_loss=5.256164

Batch 187220, train_perplexity=194.43811, train_loss=5.270114

Batch 187230, train_perplexity=192.75395, train_loss=5.2614145

Batch 187240, train_perplexity=216.01897, train_loss=5.375366

Batch 187250, train_perplexity=213.49901, train_loss=5.363632

Batch 187260, train_perplexity=166.39995, train_loss=5.114394

Batch 187270, train_perplexity=196.04498, train_loss=5.278344

Batch 187280, train_perplexity=189.96204, train_loss=5.2468243

Batch 187290, train_perplexity=186.29832, train_loss=5.2273493

Batch 187300, train_perplexity=171.15344, train_loss=5.1425605

Batch 187310, train_perplexity=196.58249, train_loss=5.281082

Batch 187320, train_perplexity=209.31366, train_loss=5.343834

Batch 187330, train_perplexity=199.84605, train_loss=5.2975473

Batch 187340, train_perplexity=180.01057, train_loss=5.1930156

Batch 187350, train_perplexity=194.87808, train_loss=5.272374

Batch 187360, train_perplexity=203.2198, train_loss=5.314288
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 187370, train_perplexity=182.64238, train_loss=5.20753

Batch 187380, train_perplexity=176.81013, train_loss=5.1750765

Batch 187390, train_perplexity=191.26323, train_loss=5.2536507

Batch 187400, train_perplexity=211.471, train_loss=5.354088

Batch 187410, train_perplexity=202.8109, train_loss=5.312274

Batch 187420, train_perplexity=189.12384, train_loss=5.242402

Batch 187430, train_perplexity=187.24414, train_loss=5.2324133

Batch 187440, train_perplexity=174.6223, train_loss=5.1626253

Batch 187450, train_perplexity=197.51863, train_loss=5.285833

Batch 187460, train_perplexity=190.21738, train_loss=5.2481675

Batch 187470, train_perplexity=184.34496, train_loss=5.216809

Batch 187480, train_perplexity=189.63876, train_loss=5.245121

Batch 187490, train_perplexity=173.70859, train_loss=5.157379

Batch 187500, train_perplexity=196.10014, train_loss=5.2786255

Batch 187510, train_perplexity=207.3788, train_loss=5.334547

Batch 187520, train_perplexity=169.93387, train_loss=5.1354094

Batch 187530, train_perplexity=197.75177, train_loss=5.2870126

Batch 187540, train_perplexity=196.83669, train_loss=5.2823744

Batch 187550, train_perplexity=185.88872, train_loss=5.225148

Batch 187560, train_perplexity=181.10132, train_loss=5.1990566

Batch 187570, train_perplexity=192.38748, train_loss=5.2595115

Batch 187580, train_perplexity=205.28651, train_loss=5.3244066

Batch 187590, train_perplexity=205.30315, train_loss=5.3244877

Batch 187600, train_perplexity=188.7379, train_loss=5.2403593

Batch 187610, train_perplexity=188.82684, train_loss=5.2408304

Batch 187620, train_perplexity=182.20622, train_loss=5.205139

Batch 187630, train_perplexity=180.0869, train_loss=5.1934395

Batch 187640, train_perplexity=175.18124, train_loss=5.165821

Batch 187650, train_perplexity=174.66144, train_loss=5.1628494

Batch 187660, train_perplexity=194.97624, train_loss=5.2728777

Batch 187670, train_perplexity=186.43953, train_loss=5.228107

Batch 187680, train_perplexity=210.9652, train_loss=5.351693

Batch 187690, train_perplexity=209.28911, train_loss=5.3437166

Batch 187700, train_perplexity=180.3459, train_loss=5.1948767

Batch 187710, train_perplexity=171.34314, train_loss=5.143668

Batch 187720, train_perplexity=182.67679, train_loss=5.2077184

Batch 187730, train_perplexity=186.6604, train_loss=5.229291

Batch 187740, train_perplexity=177.96455, train_loss=5.1815844

Batch 187750, train_perplexity=213.7124, train_loss=5.364631

Batch 187760, train_perplexity=174.04338, train_loss=5.1593046

Batch 187770, train_perplexity=197.22404, train_loss=5.2843404

Batch 187780, train_perplexity=175.60393, train_loss=5.168231

Batch 187790, train_perplexity=171.58334, train_loss=5.145069

Batch 187800, train_perplexity=188.95132, train_loss=5.2414894

Batch 187810, train_perplexity=180.96423, train_loss=5.1982994

Batch 187820, train_perplexity=197.08406, train_loss=5.2836304

Batch 187830, train_perplexity=182.7931, train_loss=5.208355

Batch 187840, train_perplexity=185.81224, train_loss=5.2247367

Batch 187850, train_perplexity=186.06023, train_loss=5.2260704

Batch 187860, train_perplexity=201.88336, train_loss=5.30769

Batch 187870, train_perplexity=168.3322, train_loss=5.1259394

Batch 187880, train_perplexity=170.41205, train_loss=5.1382194

Batch 187890, train_perplexity=189.08922, train_loss=5.242219

Batch 187900, train_perplexity=168.8425, train_loss=5.1289663

Batch 187910, train_perplexity=210.16656, train_loss=5.3479004

Batch 187920, train_perplexity=198.63171, train_loss=5.2914524

Batch 187930, train_perplexity=173.68358, train_loss=5.157235

Batch 187940, train_perplexity=169.24754, train_loss=5.1313624

Batch 187950, train_perplexity=161.80963, train_loss=5.0864205

Batch 187960, train_perplexity=169.26538, train_loss=5.131468

Batch 187970, train_perplexity=188.58353, train_loss=5.239541

Batch 187980, train_perplexity=188.99583, train_loss=5.241725

Batch 187990, train_perplexity=170.95076, train_loss=5.1413755

Batch 188000, train_perplexity=183.04535, train_loss=5.209734

Batch 188010, train_perplexity=186.61974, train_loss=5.229073

Batch 188020, train_perplexity=171.4883, train_loss=5.144515

Batch 188030, train_perplexity=183.51344, train_loss=5.212288

Batch 188040, train_perplexity=187.90314, train_loss=5.2359266

Batch 188050, train_perplexity=189.0244, train_loss=5.241876

Batch 188060, train_perplexity=206.74956, train_loss=5.331508

Batch 188070, train_perplexity=195.80096, train_loss=5.2770987

Batch 188080, train_perplexity=198.69186, train_loss=5.291755

Batch 188090, train_perplexity=204.65903, train_loss=5.3213453

Batch 188100, train_perplexity=174.53023, train_loss=5.162098

Batch 188110, train_perplexity=211.58517, train_loss=5.3546276

Batch 188120, train_perplexity=198.83128, train_loss=5.2924566

Batch 188130, train_perplexity=177.03362, train_loss=5.1763396

Batch 188140, train_perplexity=164.6352, train_loss=5.103732

Batch 188150, train_perplexity=194.98405, train_loss=5.2729177

Batch 188160, train_perplexity=192.56764, train_loss=5.2604475

Batch 188170, train_perplexity=199.84206, train_loss=5.2975273

Batch 188180, train_perplexity=187.00928, train_loss=5.2311583

Batch 188190, train_perplexity=199.02213, train_loss=5.293416

Batch 188200, train_perplexity=171.3482, train_loss=5.1436977

Batch 188210, train_perplexity=200.84322, train_loss=5.3025246

Batch 188220, train_perplexity=186.57677, train_loss=5.2288427

Batch 188230, train_perplexity=206.33934, train_loss=5.329522

Batch 188240, train_perplexity=184.03844, train_loss=5.2151446

Batch 188250, train_perplexity=186.13965, train_loss=5.226497

Batch 188260, train_perplexity=203.0834, train_loss=5.3136168

Batch 188270, train_perplexity=185.08174, train_loss=5.2207975

Batch 188280, train_perplexity=188.96448, train_loss=5.241559

Batch 188290, train_perplexity=200.32596, train_loss=5.299946

Batch 188300, train_perplexity=159.23083, train_loss=5.070355

Batch 188310, train_perplexity=176.43637, train_loss=5.1729603

Batch 188320, train_perplexity=181.37856, train_loss=5.2005863

Batch 188330, train_perplexity=185.0602, train_loss=5.220681

Batch 188340, train_perplexity=191.96568, train_loss=5.2573166

Batch 188350, train_perplexity=186.99323, train_loss=5.2310724

Batch 188360, train_perplexity=164.84666, train_loss=5.1050158

Batch 188370, train_perplexity=182.19658, train_loss=5.205086

Batch 188380, train_perplexity=159.31529, train_loss=5.070885

Batch 188390, train_perplexity=170.38824, train_loss=5.1380796

Batch 188400, train_perplexity=193.82808, train_loss=5.2669716

Batch 188410, train_perplexity=177.81644, train_loss=5.180752

Batch 188420, train_perplexity=187.53043, train_loss=5.233941

Batch 188430, train_perplexity=226.07709, train_loss=5.420876

Batch 188440, train_perplexity=183.73952, train_loss=5.213519

Batch 188450, train_perplexity=174.80807, train_loss=5.1636887

Batch 188460, train_perplexity=184.9861, train_loss=5.2202806

Batch 188470, train_perplexity=193.6568, train_loss=5.2660875

Batch 188480, train_perplexity=175.1383, train_loss=5.165576

Batch 188490, train_perplexity=190.82852, train_loss=5.251375

Batch 188500, train_perplexity=185.26892, train_loss=5.2218084

Batch 188510, train_perplexity=186.40709, train_loss=5.227933

Batch 188520, train_perplexity=184.01291, train_loss=5.215006

Batch 188530, train_perplexity=178.3124, train_loss=5.183537

Batch 188540, train_perplexity=180.40997, train_loss=5.195232

Batch 188550, train_perplexity=204.56908, train_loss=5.3209057

Batch 188560, train_perplexity=177.17946, train_loss=5.177163

Batch 188570, train_perplexity=183.60378, train_loss=5.21278

Batch 188580, train_perplexity=173.17384, train_loss=5.154296

Batch 188590, train_perplexity=201.88905, train_loss=5.3077183

Batch 188600, train_perplexity=172.3927, train_loss=5.149775

Batch 188610, train_perplexity=187.01222, train_loss=5.231174

Batch 188620, train_perplexity=177.13875, train_loss=5.1769333

Batch 188630, train_perplexity=170.57701, train_loss=5.139187

Batch 188640, train_perplexity=181.80579, train_loss=5.202939

Batch 188650, train_perplexity=200.1331, train_loss=5.2989826

Batch 188660, train_perplexity=193.40819, train_loss=5.264803
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 188670, train_perplexity=178.76564, train_loss=5.1860757

Batch 188680, train_perplexity=163.83952, train_loss=5.0988874

Batch 188690, train_perplexity=175.09105, train_loss=5.165306

Batch 188700, train_perplexity=170.78976, train_loss=5.1404333

Batch 188710, train_perplexity=190.85582, train_loss=5.2515182

Batch 188720, train_perplexity=188.17993, train_loss=5.2373986

Batch 188730, train_perplexity=204.49777, train_loss=5.320557

Batch 188740, train_perplexity=196.23204, train_loss=5.279298

Batch 188750, train_perplexity=193.78705, train_loss=5.26676

Batch 188760, train_perplexity=168.55403, train_loss=5.1272564

Batch 188770, train_perplexity=201.42519, train_loss=5.305418

Batch 188780, train_perplexity=188.40916, train_loss=5.238616

Batch 188790, train_perplexity=166.81894, train_loss=5.116909

Batch 188800, train_perplexity=177.6317, train_loss=5.1797123

Batch 188810, train_perplexity=184.12428, train_loss=5.215611

Batch 188820, train_perplexity=193.72505, train_loss=5.26644

Batch 188830, train_perplexity=186.47075, train_loss=5.2282743

Batch 188840, train_perplexity=199.46657, train_loss=5.2956467

Batch 188850, train_perplexity=180.00482, train_loss=5.1929836

Batch 188860, train_perplexity=193.03964, train_loss=5.2628956

Batch 188870, train_perplexity=184.60225, train_loss=5.2182035

Batch 188880, train_perplexity=176.14896, train_loss=5.17133

Batch 188890, train_perplexity=196.58794, train_loss=5.28111

Batch 188900, train_perplexity=197.53172, train_loss=5.285899

Batch 188910, train_perplexity=169.72551, train_loss=5.1341825

Batch 188920, train_perplexity=183.74942, train_loss=5.213573

Batch 188930, train_perplexity=185.73322, train_loss=5.2243114

Batch 188940, train_perplexity=167.56627, train_loss=5.121379

Batch 188950, train_perplexity=190.05246, train_loss=5.2473

Batch 188960, train_perplexity=189.14279, train_loss=5.242502

Batch 188970, train_perplexity=209.00847, train_loss=5.342375

Batch 188980, train_perplexity=171.65846, train_loss=5.145507

Batch 188990, train_perplexity=184.5769, train_loss=5.218066

Batch 189000, train_perplexity=196.95132, train_loss=5.2829566

Batch 189010, train_perplexity=170.78374, train_loss=5.140398

Batch 189020, train_perplexity=176.90552, train_loss=5.175616

Batch 189030, train_perplexity=176.07077, train_loss=5.170886

Batch 189040, train_perplexity=196.11998, train_loss=5.2787266

Batch 189050, train_perplexity=194.75601, train_loss=5.2717476

Batch 189060, train_perplexity=170.4266, train_loss=5.1383047

Batch 189070, train_perplexity=185.24959, train_loss=5.221704

Batch 189080, train_perplexity=186.65044, train_loss=5.2292376

Batch 189090, train_perplexity=175.69957, train_loss=5.1687756

Batch 189100, train_perplexity=171.98799, train_loss=5.1474247

Batch 189110, train_perplexity=186.5023, train_loss=5.2284436

Batch 189120, train_perplexity=190.81133, train_loss=5.251285

Batch 189130, train_perplexity=191.45148, train_loss=5.2546344

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Batch 189140, train_perplexity=189.83853, train_loss=5.246174

Batch 189150, train_perplexity=197.94301, train_loss=5.287979

Batch 189160, train_perplexity=204.28542, train_loss=5.319518

Batch 189170, train_perplexity=181.92209, train_loss=5.2035785

Batch 189180, train_perplexity=193.04968, train_loss=5.2629476

Batch 189190, train_perplexity=194.32004, train_loss=5.2695065

Batch 189200, train_perplexity=167.80519, train_loss=5.1228037

Batch 189210, train_perplexity=186.22673, train_loss=5.226965

Batch 189220, train_perplexity=189.4047, train_loss=5.243886

Batch 189230, train_perplexity=190.8216, train_loss=5.251339

Batch 189240, train_perplexity=185.22008, train_loss=5.2215447

Batch 189250, train_perplexity=168.95468, train_loss=5.1296306

Batch 189260, train_perplexity=173.27386, train_loss=5.1548734

Batch 189270, train_perplexity=165.9923, train_loss=5.1119413

Batch 189280, train_perplexity=179.4203, train_loss=5.189731

Batch 189290, train_perplexity=188.07031, train_loss=5.236816

Batch 189300, train_perplexity=184.24733, train_loss=5.216279

Batch 189310, train_perplexity=202.20448, train_loss=5.3092794

Batch 189320, train_perplexity=198.96501, train_loss=5.293129

Batch 189330, train_perplexity=171.13655, train_loss=5.142462

Batch 189340, train_perplexity=197.42908, train_loss=5.2853794

Batch 189350, train_perplexity=175.40909, train_loss=5.167121

Batch 189360, train_perplexity=214.35915, train_loss=5.367653

Batch 189370, train_perplexity=195.56079, train_loss=5.2758713

Batch 189380, train_perplexity=196.30054, train_loss=5.279647

Batch 189390, train_perplexity=189.67683, train_loss=5.2453218

Batch 189400, train_perplexity=203.22813, train_loss=5.314329

Batch 189410, train_perplexity=189.4141, train_loss=5.2439356

Batch 189420, train_perplexity=211.45204, train_loss=5.353998

Batch 189430, train_perplexity=176.48416, train_loss=5.173231

Batch 189440, train_perplexity=184.88028, train_loss=5.2197084

Batch 189450, train_perplexity=185.79585, train_loss=5.2246485

Batch 189460, train_perplexity=191.41322, train_loss=5.2544346

Batch 189470, train_perplexity=176.2739, train_loss=5.172039

Batch 189480, train_perplexity=161.38867, train_loss=5.0838156

Batch 189490, train_perplexity=181.16168, train_loss=5.19939

Batch 189500, train_perplexity=191.2937, train_loss=5.25381

Batch 189510, train_perplexity=189.77237, train_loss=5.2458253

Batch 189520, train_perplexity=180.36671, train_loss=5.194992

Batch 189530, train_perplexity=185.32423, train_loss=5.222107

Batch 189540, train_perplexity=193.89288, train_loss=5.267306

Batch 189550, train_perplexity=184.58694, train_loss=5.2181206

Batch 189560, train_perplexity=178.14505, train_loss=5.182598

Batch 189570, train_perplexity=173.02345, train_loss=5.153427

Batch 189580, train_perplexity=213.05418, train_loss=5.3615465

Batch 189590, train_perplexity=162.43666, train_loss=5.090288

Batch 189600, train_perplexity=199.51965, train_loss=5.2959127

Batch 189610, train_perplexity=179.99786, train_loss=5.192945

Batch 189620, train_perplexity=189.64436, train_loss=5.2451506

Batch 189630, train_perplexity=187.03978, train_loss=5.2313213

Batch 189640, train_perplexity=189.6044, train_loss=5.24494

Batch 189650, train_perplexity=212.78726, train_loss=5.360293

Batch 189660, train_perplexity=192.61337, train_loss=5.260685

Batch 189670, train_perplexity=206.45892, train_loss=5.3301015

Batch 189680, train_perplexity=193.87892, train_loss=5.267234

Batch 189690, train_perplexity=172.96216, train_loss=5.153073

Batch 189700, train_perplexity=186.20703, train_loss=5.226859

Batch 189710, train_perplexity=160.24739, train_loss=5.076719

Batch 189720, train_perplexity=191.02525, train_loss=5.2524056

Batch 189730, train_perplexity=175.58751, train_loss=5.1681376

Batch 189740, train_perplexity=181.35797, train_loss=5.200473

Batch 189750, train_perplexity=193.05989, train_loss=5.2630005

Batch 189760, train_perplexity=182.01328, train_loss=5.2040796

Batch 189770, train_perplexity=180.17442, train_loss=5.1939254

Batch 189780, train_perplexity=191.68195, train_loss=5.2558374

Batch 189790, train_perplexity=172.89586, train_loss=5.1526895

Batch 189800, train_perplexity=208.07791, train_loss=5.3379126

Batch 189810, train_perplexity=186.9064, train_loss=5.230608

Batch 189820, train_perplexity=182.93593, train_loss=5.209136

Batch 189830, train_perplexity=164.93033, train_loss=5.105523

Batch 189840, train_perplexity=184.73839, train_loss=5.2189407

Batch 189850, train_perplexity=196.05377, train_loss=5.278389

Batch 189860, train_perplexity=176.55638, train_loss=5.1736403

Batch 189870, train_perplexity=188.30856, train_loss=5.238082

Batch 189880, train_perplexity=188.59946, train_loss=5.2396255

Batch 189890, train_perplexity=179.86694, train_loss=5.1922174

Batch 189900, train_perplexity=192.96326, train_loss=5.2625
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 189910, train_perplexity=195.74608, train_loss=5.2768183

Batch 189920, train_perplexity=178.63382, train_loss=5.185338

Batch 189930, train_perplexity=165.78465, train_loss=5.1106896

Batch 189940, train_perplexity=177.3338, train_loss=5.178034

Batch 189950, train_perplexity=185.15985, train_loss=5.2212195

Batch 189960, train_perplexity=165.45998, train_loss=5.1087294

Batch 189970, train_perplexity=179.55133, train_loss=5.190461

Batch 189980, train_perplexity=174.45334, train_loss=5.1616573

Batch 189990, train_perplexity=188.0496, train_loss=5.236706

Batch 190000, train_perplexity=195.27454, train_loss=5.2744064

Batch 190010, train_perplexity=175.12662, train_loss=5.165509

Batch 190020, train_perplexity=190.97762, train_loss=5.2521563

Batch 190030, train_perplexity=185.89333, train_loss=5.225173

Batch 190040, train_perplexity=174.21352, train_loss=5.1602817

Batch 190050, train_perplexity=177.28003, train_loss=5.1777306

Batch 190060, train_perplexity=171.28587, train_loss=5.143334

Batch 190070, train_perplexity=217.79204, train_loss=5.3835406

Batch 190080, train_perplexity=181.553, train_loss=5.2015476

Batch 190090, train_perplexity=199.3652, train_loss=5.2951384

Batch 190100, train_perplexity=179.17245, train_loss=5.188349

Batch 190110, train_perplexity=179.31725, train_loss=5.1891565

Batch 190120, train_perplexity=189.03134, train_loss=5.241913

Batch 190130, train_perplexity=191.52863, train_loss=5.2550373

Batch 190140, train_perplexity=180.84097, train_loss=5.197618

Batch 190150, train_perplexity=168.85103, train_loss=5.129017

Batch 190160, train_perplexity=215.49078, train_loss=5.372918

Batch 190170, train_perplexity=181.63007, train_loss=5.201972

Batch 190180, train_perplexity=178.5415, train_loss=5.184821

Batch 190190, train_perplexity=185.53813, train_loss=5.2232604

Batch 190200, train_perplexity=210.12187, train_loss=5.3476877

Batch 190210, train_perplexity=177.75456, train_loss=5.1804037

Batch 190220, train_perplexity=189.42891, train_loss=5.244014

Batch 190230, train_perplexity=179.75488, train_loss=5.191594

Batch 190240, train_perplexity=182.80147, train_loss=5.2084007

Batch 190250, train_perplexity=187.071, train_loss=5.231488

Batch 190260, train_perplexity=194.95485, train_loss=5.272768

Batch 190270, train_perplexity=189.59645, train_loss=5.244898

Batch 190280, train_perplexity=191.88551, train_loss=5.256899

Batch 190290, train_perplexity=210.34933, train_loss=5.3487697

Batch 190300, train_perplexity=201.0845, train_loss=5.3037252

Batch 190310, train_perplexity=192.97118, train_loss=5.262541

Batch 190320, train_perplexity=167.68048, train_loss=5.1220603

Batch 190330, train_perplexity=202.79233, train_loss=5.3121824

Batch 190340, train_perplexity=183.61357, train_loss=5.2128334

Batch 190350, train_perplexity=174.02048, train_loss=5.159173

Batch 190360, train_perplexity=178.78311, train_loss=5.1861734

Batch 190370, train_perplexity=165.93222, train_loss=5.1115794

Batch 190380, train_perplexity=187.20413, train_loss=5.2321997

Batch 190390, train_perplexity=191.8974, train_loss=5.256961

Batch 190400, train_perplexity=160.05212, train_loss=5.0754995

Batch 190410, train_perplexity=176.13745, train_loss=5.1712646

Batch 190420, train_perplexity=187.4176, train_loss=5.2333393

Batch 190430, train_perplexity=174.27492, train_loss=5.160634

Batch 190440, train_perplexity=194.81741, train_loss=5.272063

Batch 190450, train_perplexity=185.2851, train_loss=5.2218957

Batch 190460, train_perplexity=199.60796, train_loss=5.2963552

Batch 190470, train_perplexity=198.3576, train_loss=5.2900715

Batch 190480, train_perplexity=183.9443, train_loss=5.214633

Batch 190490, train_perplexity=191.88478, train_loss=5.256895

Batch 190500, train_perplexity=174.64311, train_loss=5.1627445

Batch 190510, train_perplexity=162.4571, train_loss=5.090414

Batch 190520, train_perplexity=193.20181, train_loss=5.2637353

Batch 190530, train_perplexity=214.9026, train_loss=5.370185

Batch 190540, train_perplexity=186.0557, train_loss=5.226046

Batch 190550, train_perplexity=215.49016, train_loss=5.3729153

Batch 190560, train_perplexity=177.59833, train_loss=5.1795244

Batch 190570, train_perplexity=187.74336, train_loss=5.235076

Batch 190580, train_perplexity=181.53941, train_loss=5.2014728

Batch 190590, train_perplexity=213.87907, train_loss=5.365411

Batch 190600, train_perplexity=180.15698, train_loss=5.1938286

Batch 190610, train_perplexity=181.84499, train_loss=5.2031546

Batch 190620, train_perplexity=189.3684, train_loss=5.2436943

Batch 190630, train_perplexity=176.49544, train_loss=5.173295

Batch 190640, train_perplexity=183.89378, train_loss=5.2143583

Batch 190650, train_perplexity=194.4409, train_loss=5.2701283

Batch 190660, train_perplexity=170.79953, train_loss=5.1404905

Batch 190670, train_perplexity=173.48708, train_loss=5.156103

Batch 190680, train_perplexity=181.76886, train_loss=5.202736

Batch 190690, train_perplexity=189.75436, train_loss=5.2457304

Batch 190700, train_perplexity=194.18405, train_loss=5.2688065

Batch 190710, train_perplexity=204.65883, train_loss=5.3213444

Batch 190720, train_perplexity=178.18362, train_loss=5.1828146

Batch 190730, train_perplexity=196.97226, train_loss=5.283063

Batch 190740, train_perplexity=157.52168, train_loss=5.059563

Batch 190750, train_perplexity=165.41864, train_loss=5.1084795

Batch 190760, train_perplexity=176.61162, train_loss=5.173953

Batch 190770, train_perplexity=180.14908, train_loss=5.1937847

Batch 190780, train_perplexity=189.15866, train_loss=5.242586

Batch 190790, train_perplexity=183.63109, train_loss=5.212929

Batch 190800, train_perplexity=194.27112, train_loss=5.2692547

Batch 190810, train_perplexity=179.01413, train_loss=5.1874647

Batch 190820, train_perplexity=176.84758, train_loss=5.175288

Batch 190830, train_perplexity=189.1538, train_loss=5.2425604

Batch 190840, train_perplexity=197.44508, train_loss=5.2854605

Batch 190850, train_perplexity=166.84456, train_loss=5.1170626

Batch 190860, train_perplexity=180.37979, train_loss=5.1950645

Batch 190870, train_perplexity=186.22665, train_loss=5.2269645

Batch 190880, train_perplexity=183.74617, train_loss=5.2135553

Batch 190890, train_perplexity=205.98347, train_loss=5.327796

Batch 190900, train_perplexity=200.28203, train_loss=5.2997265

Batch 190910, train_perplexity=182.511, train_loss=5.2068105

Batch 190920, train_perplexity=183.75792, train_loss=5.213619

Batch 190930, train_perplexity=191.21709, train_loss=5.2534094

Batch 190940, train_perplexity=176.6763, train_loss=5.1743193

Batch 190950, train_perplexity=166.83325, train_loss=5.116995

Batch 190960, train_perplexity=177.06241, train_loss=5.176502

Batch 190970, train_perplexity=176.3013, train_loss=5.1721945

Batch 190980, train_perplexity=183.84978, train_loss=5.214119

Batch 190990, train_perplexity=190.87201, train_loss=5.251603

Batch 191000, train_perplexity=166.23563, train_loss=5.113406

Batch 191010, train_perplexity=179.95813, train_loss=5.192724

Batch 191020, train_perplexity=206.28731, train_loss=5.32927

Batch 191030, train_perplexity=169.61038, train_loss=5.133504

Batch 191040, train_perplexity=215.09729, train_loss=5.3710904

Batch 191050, train_perplexity=173.1892, train_loss=5.1543846

Batch 191060, train_perplexity=196.30223, train_loss=5.2796555

Batch 191070, train_perplexity=182.36739, train_loss=5.206023

Batch 191080, train_perplexity=184.34593, train_loss=5.216814

Batch 191090, train_perplexity=177.8217, train_loss=5.1807814

Batch 191100, train_perplexity=167.39888, train_loss=5.1203794

Batch 191110, train_perplexity=199.97607, train_loss=5.2981977

Batch 191120, train_perplexity=187.46945, train_loss=5.233616

Batch 191130, train_perplexity=181.64722, train_loss=5.2020664

Batch 191140, train_perplexity=184.21025, train_loss=5.216078

Batch 191150, train_perplexity=171.4408, train_loss=5.144238

Batch 191160, train_perplexity=187.39624, train_loss=5.2332253

Batch 191170, train_perplexity=191.93968, train_loss=5.257181

Batch 191180, train_perplexity=197.05023, train_loss=5.2834587

Batch 191190, train_perplexity=168.20206, train_loss=5.125166

Batch 191200, train_perplexity=190.84453, train_loss=5.251459
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 191210, train_perplexity=182.08794, train_loss=5.2044897

Batch 191220, train_perplexity=174.59698, train_loss=5.1624804

Batch 191230, train_perplexity=182.09418, train_loss=5.204524

Batch 191240, train_perplexity=216.4452, train_loss=5.3773375

Batch 191250, train_perplexity=181.99835, train_loss=5.2039976

Batch 191260, train_perplexity=181.95695, train_loss=5.20377

Batch 191270, train_perplexity=208.0785, train_loss=5.3379154

Batch 191280, train_perplexity=206.04105, train_loss=5.3280754

Batch 191290, train_perplexity=167.23253, train_loss=5.1193852

Batch 191300, train_perplexity=161.87175, train_loss=5.0868044

Batch 191310, train_perplexity=204.16261, train_loss=5.318917

Batch 191320, train_perplexity=185.35667, train_loss=5.222282

Batch 191330, train_perplexity=202.15146, train_loss=5.309017

Batch 191340, train_perplexity=155.36932, train_loss=5.045805

Batch 191350, train_perplexity=174.47913, train_loss=5.161805

Batch 191360, train_perplexity=193.83492, train_loss=5.267007

Batch 191370, train_perplexity=182.40321, train_loss=5.2062197

Batch 191380, train_perplexity=191.90015, train_loss=5.256975

Batch 191390, train_perplexity=187.38097, train_loss=5.233144

Batch 191400, train_perplexity=186.68925, train_loss=5.2294455

Batch 191410, train_perplexity=179.7355, train_loss=5.1914864

Batch 191420, train_perplexity=153.26155, train_loss=5.032146

Batch 191430, train_perplexity=183.82277, train_loss=5.213972

Batch 191440, train_perplexity=177.56462, train_loss=5.1793346

Batch 191450, train_perplexity=191.37727, train_loss=5.2542467

Batch 191460, train_perplexity=171.66534, train_loss=5.145547

Batch 191470, train_perplexity=179.10036, train_loss=5.1879463

Batch 191480, train_perplexity=195.85083, train_loss=5.2773533

Batch 191490, train_perplexity=186.40816, train_loss=5.2279387

Batch 191500, train_perplexity=193.79315, train_loss=5.2667913

Batch 191510, train_perplexity=172.89528, train_loss=5.152686

Batch 191520, train_perplexity=199.11592, train_loss=5.293887

Batch 191530, train_perplexity=169.49791, train_loss=5.1328406

Batch 191540, train_perplexity=199.01909, train_loss=5.293401

Batch 191550, train_perplexity=177.42607, train_loss=5.178554

Batch 191560, train_perplexity=178.36078, train_loss=5.1838083

Batch 191570, train_perplexity=214.19914, train_loss=5.366906

Batch 191580, train_perplexity=194.2687, train_loss=5.2692423

Batch 191590, train_perplexity=194.32318, train_loss=5.2695227

Batch 191600, train_perplexity=177.514, train_loss=5.1790495

Batch 191610, train_perplexity=199.14421, train_loss=5.294029

Batch 191620, train_perplexity=166.35606, train_loss=5.1141305

Batch 191630, train_perplexity=193.64258, train_loss=5.266014

Batch 191640, train_perplexity=163.7558, train_loss=5.0983763

Batch 191650, train_perplexity=173.192, train_loss=5.154401

Batch 191660, train_perplexity=171.03848, train_loss=5.1418886

Batch 191670, train_perplexity=186.23651, train_loss=5.2270174

Batch 191680, train_perplexity=179.38959, train_loss=5.18956

Batch 191690, train_perplexity=183.25688, train_loss=5.210889

Batch 191700, train_perplexity=182.84734, train_loss=5.2086515

Batch 191710, train_perplexity=198.72417, train_loss=5.291918

Batch 191720, train_perplexity=182.75485, train_loss=5.2081456

Batch 191730, train_perplexity=167.55812, train_loss=5.1213303

Batch 191740, train_perplexity=186.61458, train_loss=5.2290454

Batch 191750, train_perplexity=188.24213, train_loss=5.237729

Batch 191760, train_perplexity=175.03561, train_loss=5.1649895

Batch 191770, train_perplexity=179.66542, train_loss=5.1910963

Batch 191780, train_perplexity=179.63945, train_loss=5.190952

Batch 191790, train_perplexity=181.43962, train_loss=5.200923

Batch 191800, train_perplexity=177.11517, train_loss=5.1768003

Batch 191810, train_perplexity=187.38428, train_loss=5.2331614

Batch 191820, train_perplexity=192.8801, train_loss=5.2620687

Batch 191830, train_perplexity=194.8425, train_loss=5.2721915

Batch 191840, train_perplexity=180.6008, train_loss=5.196289

Batch 191850, train_perplexity=186.14844, train_loss=5.2265444

Batch 191860, train_perplexity=174.83667, train_loss=5.163852

Batch 191870, train_perplexity=199.60654, train_loss=5.296348

Batch 191880, train_perplexity=150.37212, train_loss=5.013113

Batch 191890, train_perplexity=172.88004, train_loss=5.152598

Batch 191900, train_perplexity=172.16869, train_loss=5.1484747

Batch 191910, train_perplexity=164.67635, train_loss=5.103982

Batch 191920, train_perplexity=197.65459, train_loss=5.286521

Batch 191930, train_perplexity=187.01375, train_loss=5.231182

Batch 191940, train_perplexity=191.88066, train_loss=5.2568736

Batch 191950, train_perplexity=174.67334, train_loss=5.1629176

Batch 191960, train_perplexity=196.33687, train_loss=5.279832

Batch 191970, train_perplexity=181.57394, train_loss=5.201663

Batch 191980, train_perplexity=179.3972, train_loss=5.1896024

Batch 191990, train_perplexity=185.2882, train_loss=5.2219124

Batch 192000, train_perplexity=202.84164, train_loss=5.3124256

Batch 192010, train_perplexity=182.9546, train_loss=5.209238

Batch 192020, train_perplexity=184.01256, train_loss=5.215004

Batch 192030, train_perplexity=191.45102, train_loss=5.254632

Batch 192040, train_perplexity=211.46918, train_loss=5.3540792

Batch 192050, train_perplexity=182.98488, train_loss=5.2094035

Batch 192060, train_perplexity=174.33493, train_loss=5.1609783

Batch 192070, train_perplexity=199.33517, train_loss=5.2949877

Batch 192080, train_perplexity=217.43684, train_loss=5.3819084

Batch 192090, train_perplexity=179.04144, train_loss=5.1876173

Batch 192100, train_perplexity=183.81172, train_loss=5.213912

Batch 192110, train_perplexity=181.83727, train_loss=5.203112

Batch 192120, train_perplexity=198.77478, train_loss=5.2921724

Batch 192130, train_perplexity=164.52126, train_loss=5.1030397

Batch 192140, train_perplexity=199.88933, train_loss=5.297764

Batch 192150, train_perplexity=183.63371, train_loss=5.212943

Batch 192160, train_perplexity=181.66628, train_loss=5.2021713

Batch 192170, train_perplexity=175.92978, train_loss=5.170085

Batch 192180, train_perplexity=181.41524, train_loss=5.2007885

Batch 192190, train_perplexity=178.55156, train_loss=5.1848774

Batch 192200, train_perplexity=173.06157, train_loss=5.1536474

Batch 192210, train_perplexity=178.2919, train_loss=5.183422

Batch 192220, train_perplexity=158.32375, train_loss=5.064642

Batch 192230, train_perplexity=199.2867, train_loss=5.2947445

Batch 192240, train_perplexity=186.6476, train_loss=5.2292223

Batch 192250, train_perplexity=173.9197, train_loss=5.1585937

Batch 192260, train_perplexity=192.59502, train_loss=5.2605896

Batch 192270, train_perplexity=178.80861, train_loss=5.186316

Batch 192280, train_perplexity=180.87909, train_loss=5.197829

Batch 192290, train_perplexity=169.53357, train_loss=5.133051

Batch 192300, train_perplexity=187.0892, train_loss=5.2315855

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Batch 192310, train_perplexity=196.93546, train_loss=5.282876

Batch 192320, train_perplexity=218.4401, train_loss=5.386512

Batch 192330, train_perplexity=183.38467, train_loss=5.211586

Batch 192340, train_perplexity=165.61201, train_loss=5.1096478

Batch 192350, train_perplexity=206.28632, train_loss=5.329265

Batch 192360, train_perplexity=195.97395, train_loss=5.2779818

Batch 192370, train_perplexity=191.6527, train_loss=5.255685

Batch 192380, train_perplexity=174.10829, train_loss=5.1596775

Batch 192390, train_perplexity=175.50706, train_loss=5.1676793

Batch 192400, train_perplexity=173.44522, train_loss=5.155862

Batch 192410, train_perplexity=182.89948, train_loss=5.2089367

Batch 192420, train_perplexity=182.58926, train_loss=5.207239

Batch 192430, train_perplexity=165.12871, train_loss=5.106725

Batch 192440, train_perplexity=185.8561, train_loss=5.2249727
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 192450, train_perplexity=187.29271, train_loss=5.2326727

Batch 192460, train_perplexity=185.61378, train_loss=5.223668

Batch 192470, train_perplexity=176.41281, train_loss=5.172827

Batch 192480, train_perplexity=187.0241, train_loss=5.2312374

Batch 192490, train_perplexity=170.67448, train_loss=5.139758

Batch 192500, train_perplexity=203.26894, train_loss=5.31453

Batch 192510, train_perplexity=190.95714, train_loss=5.252049

Batch 192520, train_perplexity=187.31209, train_loss=5.232776

Batch 192530, train_perplexity=157.53499, train_loss=5.0596476

Batch 192540, train_perplexity=182.06267, train_loss=5.204351

Batch 192550, train_perplexity=175.19678, train_loss=5.16591

Batch 192560, train_perplexity=192.87071, train_loss=5.26202

Batch 192570, train_perplexity=188.27534, train_loss=5.2379055

Batch 192580, train_perplexity=205.9823, train_loss=5.3277903

Batch 192590, train_perplexity=191.90729, train_loss=5.2570124

Batch 192600, train_perplexity=192.0249, train_loss=5.257625

Batch 192610, train_perplexity=168.81377, train_loss=5.128796

Batch 192620, train_perplexity=174.05634, train_loss=5.159379

Batch 192630, train_perplexity=196.1153, train_loss=5.2787027

Batch 192640, train_perplexity=182.49142, train_loss=5.206703

Batch 192650, train_perplexity=188.70479, train_loss=5.240184

Batch 192660, train_perplexity=172.59859, train_loss=5.1509686

Batch 192670, train_perplexity=197.38446, train_loss=5.2851534

Batch 192680, train_perplexity=194.0441, train_loss=5.2680855

Batch 192690, train_perplexity=189.973, train_loss=5.246882

Batch 192700, train_perplexity=186.33919, train_loss=5.2275686

Batch 192710, train_perplexity=192.10147, train_loss=5.2580237

Batch 192720, train_perplexity=177.45383, train_loss=5.1787105

Batch 192730, train_perplexity=170.7948, train_loss=5.140463

Batch 192740, train_perplexity=172.73228, train_loss=5.151743

Batch 192750, train_perplexity=194.36034, train_loss=5.269714

Batch 192760, train_perplexity=170.37566, train_loss=5.1380057

Batch 192770, train_perplexity=183.06866, train_loss=5.2098613

Batch 192780, train_perplexity=164.60286, train_loss=5.1035357

Batch 192790, train_perplexity=172.60007, train_loss=5.150977

Batch 192800, train_perplexity=174.57617, train_loss=5.162361

Batch 192810, train_perplexity=162.97253, train_loss=5.0935817

Batch 192820, train_perplexity=195.65584, train_loss=5.276357

Batch 192830, train_perplexity=186.96854, train_loss=5.2309403

Batch 192840, train_perplexity=201.78442, train_loss=5.3072

Batch 192850, train_perplexity=207.78958, train_loss=5.336526

Batch 192860, train_perplexity=176.14316, train_loss=5.171297

Batch 192870, train_perplexity=200.66286, train_loss=5.301626

Batch 192880, train_perplexity=192.19585, train_loss=5.258515

Batch 192890, train_perplexity=186.21129, train_loss=5.226882

Batch 192900, train_perplexity=179.16254, train_loss=5.1882935

Batch 192910, train_perplexity=188.60782, train_loss=5.23967

Batch 192920, train_perplexity=195.50569, train_loss=5.2755895

Batch 192930, train_perplexity=189.00584, train_loss=5.241778

Batch 192940, train_perplexity=173.45267, train_loss=5.155905

Batch 192950, train_perplexity=185.75298, train_loss=5.2244177

Batch 192960, train_perplexity=198.47624, train_loss=5.2906694

Batch 192970, train_perplexity=192.0348, train_loss=5.2576766

Batch 192980, train_perplexity=198.87831, train_loss=5.292693

Batch 192990, train_perplexity=194.7016, train_loss=5.271468

Batch 193000, train_perplexity=193.49657, train_loss=5.2652597

Batch 193010, train_perplexity=189.08066, train_loss=5.2421737

Batch 193020, train_perplexity=211.27444, train_loss=5.353158

Batch 193030, train_perplexity=198.57527, train_loss=5.291168

Batch 193040, train_perplexity=188.0644, train_loss=5.2367845

Batch 193050, train_perplexity=184.03151, train_loss=5.215107

Batch 193060, train_perplexity=190.39442, train_loss=5.249098

Batch 193070, train_perplexity=165.00223, train_loss=5.105959

Batch 193080, train_perplexity=192.52597, train_loss=5.260231

Batch 193090, train_perplexity=198.31505, train_loss=5.289857

Batch 193100, train_perplexity=204.00233, train_loss=5.3181314

Batch 193110, train_perplexity=180.76208, train_loss=5.1971817

Batch 193120, train_perplexity=195.3429, train_loss=5.2747564

Batch 193130, train_perplexity=175.16052, train_loss=5.165703

Batch 193140, train_perplexity=179.47678, train_loss=5.190046

Batch 193150, train_perplexity=173.27957, train_loss=5.1549063

Batch 193160, train_perplexity=192.13472, train_loss=5.258197

Batch 193170, train_perplexity=172.28398, train_loss=5.149144

Batch 193180, train_perplexity=185.02438, train_loss=5.2204876

Batch 193190, train_perplexity=180.15097, train_loss=5.193795

Batch 193200, train_perplexity=174.23703, train_loss=5.1604166

Batch 193210, train_perplexity=187.37758, train_loss=5.2331257

Batch 193220, train_perplexity=178.28595, train_loss=5.1833887

Batch 193230, train_perplexity=188.16612, train_loss=5.237325

Batch 193240, train_perplexity=166.88147, train_loss=5.117284

Batch 193250, train_perplexity=163.72932, train_loss=5.0982146

Batch 193260, train_perplexity=201.53038, train_loss=5.30594

Batch 193270, train_perplexity=165.28381, train_loss=5.107664

Batch 193280, train_perplexity=168.1589, train_loss=5.1249094

Batch 193290, train_perplexity=198.34067, train_loss=5.289986

Batch 193300, train_perplexity=164.31317, train_loss=5.101774

Batch 193310, train_perplexity=181.08664, train_loss=5.1989756

Batch 193320, train_perplexity=169.09436, train_loss=5.130457

Batch 193330, train_perplexity=207.29634, train_loss=5.3341494

Batch 193340, train_perplexity=171.97652, train_loss=5.147358

Batch 193350, train_perplexity=192.12923, train_loss=5.258168

Batch 193360, train_perplexity=158.10948, train_loss=5.0632877

Batch 193370, train_perplexity=179.17212, train_loss=5.188347

Batch 193380, train_perplexity=189.95642, train_loss=5.2467947

Batch 193390, train_perplexity=189.25583, train_loss=5.2430997

Batch 193400, train_perplexity=186.60826, train_loss=5.2290115

Batch 193410, train_perplexity=188.34727, train_loss=5.2382874

Batch 193420, train_perplexity=176.35208, train_loss=5.1724825

Batch 193430, train_perplexity=167.21283, train_loss=5.1192675

Batch 193440, train_perplexity=188.8191, train_loss=5.2407894

Batch 193450, train_perplexity=182.284, train_loss=5.205566

Batch 193460, train_perplexity=186.35448, train_loss=5.2276506

Batch 193470, train_perplexity=214.08928, train_loss=5.366393

Batch 193480, train_perplexity=176.22298, train_loss=5.17175

Batch 193490, train_perplexity=173.40901, train_loss=5.155653

Batch 193500, train_perplexity=208.8441, train_loss=5.341588

Batch 193510, train_perplexity=181.46074, train_loss=5.2010393

Batch 193520, train_perplexity=178.98767, train_loss=5.187317

Batch 193530, train_perplexity=181.22269, train_loss=5.1997266

Batch 193540, train_perplexity=185.73827, train_loss=5.2243385

Batch 193550, train_perplexity=191.11719, train_loss=5.252887

Batch 193560, train_perplexity=173.22157, train_loss=5.1545715

Batch 193570, train_perplexity=211.99498, train_loss=5.3565626

Batch 193580, train_perplexity=173.49196, train_loss=5.1561313

Batch 193590, train_perplexity=187.34416, train_loss=5.2329473

Batch 193600, train_perplexity=185.16719, train_loss=5.221259

Batch 193610, train_perplexity=179.10822, train_loss=5.18799

Batch 193620, train_perplexity=177.09601, train_loss=5.176692

Batch 193630, train_perplexity=195.96806, train_loss=5.2779517

Batch 193640, train_perplexity=171.60985, train_loss=5.1452236

Batch 193650, train_perplexity=183.98378, train_loss=5.2148476

Batch 193660, train_perplexity=176.03123, train_loss=5.1706614

Batch 193670, train_perplexity=184.39604, train_loss=5.217086

Batch 193680, train_perplexity=187.24574, train_loss=5.232422

Batch 193690, train_perplexity=181.60962, train_loss=5.2018595

Batch 193700, train_perplexity=191.72948, train_loss=5.2560854

Batch 193710, train_perplexity=191.5173, train_loss=5.254978

Batch 193720, train_perplexity=182.78831, train_loss=5.2083287

Batch 193730, train_perplexity=188.7578, train_loss=5.2404647

Batch 193740, train_perplexity=189.29726, train_loss=5.2433186
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 193750, train_perplexity=211.70163, train_loss=5.355178

Batch 193760, train_perplexity=181.10529, train_loss=5.1990786

Batch 193770, train_perplexity=183.1814, train_loss=5.210477

Batch 193780, train_perplexity=190.25566, train_loss=5.2483687

Batch 193790, train_perplexity=203.99164, train_loss=5.318079

Batch 193800, train_perplexity=173.24123, train_loss=5.154685

Batch 193810, train_perplexity=194.34024, train_loss=5.2696104

Batch 193820, train_perplexity=173.67522, train_loss=5.157187

Batch 193830, train_perplexity=200.47847, train_loss=5.300707

Batch 193840, train_perplexity=175.19344, train_loss=5.1658907

Batch 193850, train_perplexity=224.36876, train_loss=5.413291

Batch 193860, train_perplexity=190.84962, train_loss=5.251486

Batch 193870, train_perplexity=185.5768, train_loss=5.223469

Batch 193880, train_perplexity=180.94353, train_loss=5.198185

Batch 193890, train_perplexity=191.51657, train_loss=5.2549744

Batch 193900, train_perplexity=193.61433, train_loss=5.265868

Batch 193910, train_perplexity=198.62167, train_loss=5.291402

Batch 193920, train_perplexity=172.60689, train_loss=5.1510167

Batch 193930, train_perplexity=173.92351, train_loss=5.1586156

Batch 193940, train_perplexity=187.57066, train_loss=5.2341557

Batch 193950, train_perplexity=161.35928, train_loss=5.0836334

Batch 193960, train_perplexity=174.14351, train_loss=5.1598797

Batch 193970, train_perplexity=174.85643, train_loss=5.163965

Batch 193980, train_perplexity=164.96202, train_loss=5.1057153

Batch 193990, train_perplexity=203.21631, train_loss=5.314271

Batch 194000, train_perplexity=185.27193, train_loss=5.2218246

Batch 194010, train_perplexity=193.5045, train_loss=5.2653008

Batch 194020, train_perplexity=176.31358, train_loss=5.172264

Batch 194030, train_perplexity=181.04295, train_loss=5.1987343

Batch 194040, train_perplexity=174.845, train_loss=5.1639

Batch 194050, train_perplexity=198.3002, train_loss=5.289782

Batch 194060, train_perplexity=174.26768, train_loss=5.1605926

Batch 194070, train_perplexity=176.476, train_loss=5.173185

Batch 194080, train_perplexity=171.04796, train_loss=5.141944

Batch 194090, train_perplexity=169.76913, train_loss=5.1344395

Batch 194100, train_perplexity=167.9315, train_loss=5.123556

Batch 194110, train_perplexity=211.51436, train_loss=5.354293

Batch 194120, train_perplexity=176.31139, train_loss=5.1722517

Batch 194130, train_perplexity=184.61449, train_loss=5.21827

Batch 194140, train_perplexity=173.11769, train_loss=5.1539717

Batch 194150, train_perplexity=159.6374, train_loss=5.072905

Batch 194160, train_perplexity=187.84715, train_loss=5.2356286

Batch 194170, train_perplexity=174.35771, train_loss=5.161109

Batch 194180, train_perplexity=211.98447, train_loss=5.356513

Batch 194190, train_perplexity=188.1594, train_loss=5.2372894

Batch 194200, train_perplexity=191.29352, train_loss=5.253809

Batch 194210, train_perplexity=190.64297, train_loss=5.2504025

Batch 194220, train_perplexity=195.68896, train_loss=5.2765265

Batch 194230, train_perplexity=174.13321, train_loss=5.1598206

Batch 194240, train_perplexity=204.53142, train_loss=5.3207216

Batch 194250, train_perplexity=180.95189, train_loss=5.198231

Batch 194260, train_perplexity=180.9085, train_loss=5.1979914

Batch 194270, train_perplexity=187.93127, train_loss=5.2360764

Batch 194280, train_perplexity=173.88097, train_loss=5.158371

Batch 194290, train_perplexity=168.71599, train_loss=5.1282167

Batch 194300, train_perplexity=204.38333, train_loss=5.3199973

Batch 194310, train_perplexity=215.23097, train_loss=5.3717117

Batch 194320, train_perplexity=195.94078, train_loss=5.2778125

Batch 194330, train_perplexity=178.73778, train_loss=5.18592

Batch 194340, train_perplexity=199.92555, train_loss=5.297945

Batch 194350, train_perplexity=176.79193, train_loss=5.1749735

Batch 194360, train_perplexity=198.30331, train_loss=5.289798

Batch 194370, train_perplexity=196.93263, train_loss=5.2828617

Batch 194380, train_perplexity=185.26416, train_loss=5.2217827

Batch 194390, train_perplexity=164.25943, train_loss=5.101447

Batch 194400, train_perplexity=153.83492, train_loss=5.03588

Batch 194410, train_perplexity=174.81758, train_loss=5.163743

Batch 194420, train_perplexity=162.51057, train_loss=5.090743

Batch 194430, train_perplexity=163.30812, train_loss=5.0956388

Batch 194440, train_perplexity=184.76976, train_loss=5.2191105

Batch 194450, train_perplexity=199.8109, train_loss=5.2973714

Batch 194460, train_perplexity=185.09322, train_loss=5.2208595

Batch 194470, train_perplexity=192.89702, train_loss=5.2621565

Batch 194480, train_perplexity=168.80249, train_loss=5.1287293

Batch 194490, train_perplexity=171.7654, train_loss=5.1461296

Batch 194500, train_perplexity=179.85785, train_loss=5.192167

Batch 194510, train_perplexity=188.19151, train_loss=5.23746

Batch 194520, train_perplexity=198.84872, train_loss=5.2925444

Batch 194530, train_perplexity=171.10081, train_loss=5.142253

Batch 194540, train_perplexity=164.70091, train_loss=5.104131

Batch 194550, train_perplexity=192.69128, train_loss=5.2610893

Batch 194560, train_perplexity=193.62929, train_loss=5.2659454

Batch 194570, train_perplexity=196.95987, train_loss=5.283

Batch 194580, train_perplexity=179.5945, train_loss=5.1907015

Batch 194590, train_perplexity=176.51025, train_loss=5.173379

Batch 194600, train_perplexity=197.26703, train_loss=5.2845583

Batch 194610, train_perplexity=174.30334, train_loss=5.160797

Batch 194620, train_perplexity=180.42151, train_loss=5.195296

Batch 194630, train_perplexity=173.38115, train_loss=5.1554923

Batch 194640, train_perplexity=174.62613, train_loss=5.1626472

Batch 194650, train_perplexity=177.55092, train_loss=5.1792574

Batch 194660, train_perplexity=187.03104, train_loss=5.2312746

Batch 194670, train_perplexity=197.53435, train_loss=5.2859125

Batch 194680, train_perplexity=187.10481, train_loss=5.231669

Batch 194690, train_perplexity=210.23813, train_loss=5.348241

Batch 194700, train_perplexity=161.27013, train_loss=5.083081

Batch 194710, train_perplexity=177.96158, train_loss=5.1815677

Batch 194720, train_perplexity=152.13857, train_loss=5.0247917

Batch 194730, train_perplexity=196.78151, train_loss=5.282094

Batch 194740, train_perplexity=194.34839, train_loss=5.2696524

Batch 194750, train_perplexity=179.94286, train_loss=5.1926394

Batch 194760, train_perplexity=189.0052, train_loss=5.2417746

Batch 194770, train_perplexity=192.72748, train_loss=5.261277

Batch 194780, train_perplexity=179.18468, train_loss=5.188417

Batch 194790, train_perplexity=181.0597, train_loss=5.198827

Batch 194800, train_perplexity=171.37173, train_loss=5.143835

Batch 194810, train_perplexity=160.28613, train_loss=5.0769606

Batch 194820, train_perplexity=185.687, train_loss=5.2240624

Batch 194830, train_perplexity=190.13875, train_loss=5.247754

Batch 194840, train_perplexity=179.40585, train_loss=5.1896505

Batch 194850, train_perplexity=193.6158, train_loss=5.265876

Batch 194860, train_perplexity=189.48889, train_loss=5.2443304

Batch 194870, train_perplexity=172.11664, train_loss=5.1481724

Batch 194880, train_perplexity=182.4286, train_loss=5.206359

Batch 194890, train_perplexity=184.38452, train_loss=5.2170234

Batch 194900, train_perplexity=156.48846, train_loss=5.0529823

Batch 194910, train_perplexity=181.30583, train_loss=5.2001853

Batch 194920, train_perplexity=184.0222, train_loss=5.2150564

Batch 194930, train_perplexity=181.54556, train_loss=5.2015066

Batch 194940, train_perplexity=165.1806, train_loss=5.1070395

Batch 194950, train_perplexity=166.24863, train_loss=5.1134844

Batch 194960, train_perplexity=199.05772, train_loss=5.293595

Batch 194970, train_perplexity=188.22652, train_loss=5.237646

Batch 194980, train_perplexity=170.51512, train_loss=5.138824

Batch 194990, train_perplexity=195.65294, train_loss=5.2763424

Batch 195000, train_perplexity=188.37834, train_loss=5.2384524

Batch 195010, train_perplexity=172.019, train_loss=5.147605

Batch 195020, train_perplexity=177.74794, train_loss=5.1803665

Batch 195030, train_perplexity=177.52332, train_loss=5.179102

Batch 195040, train_perplexity=180.39261, train_loss=5.1951356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 195050, train_perplexity=178.91035, train_loss=5.186885

Batch 195060, train_perplexity=195.92238, train_loss=5.2777185

Batch 195070, train_perplexity=170.14046, train_loss=5.1366243

Batch 195080, train_perplexity=193.04628, train_loss=5.26293

Batch 195090, train_perplexity=184.33046, train_loss=5.21673

Batch 195100, train_perplexity=212.40579, train_loss=5.3584986

Batch 195110, train_perplexity=169.3495, train_loss=5.1319647

Batch 195120, train_perplexity=179.94157, train_loss=5.192632

Batch 195130, train_perplexity=171.44627, train_loss=5.14427

Batch 195140, train_perplexity=177.25899, train_loss=5.177612

Batch 195150, train_perplexity=189.69176, train_loss=5.2454004

Batch 195160, train_perplexity=187.11945, train_loss=5.231747

Batch 195170, train_perplexity=195.0033, train_loss=5.2730165

Batch 195180, train_perplexity=174.88911, train_loss=5.164152

Batch 195190, train_perplexity=186.64955, train_loss=5.229233

Batch 195200, train_perplexity=182.41669, train_loss=5.2062936

Batch 195210, train_perplexity=171.19034, train_loss=5.142776

Batch 195220, train_perplexity=196.4233, train_loss=5.280272

Batch 195230, train_perplexity=200.30208, train_loss=5.2998266

Batch 195240, train_perplexity=200.69617, train_loss=5.301792

Batch 195250, train_perplexity=205.37756, train_loss=5.32485

Batch 195260, train_perplexity=172.28143, train_loss=5.1491294

Batch 195270, train_perplexity=193.61285, train_loss=5.2658606

Batch 195280, train_perplexity=203.67886, train_loss=5.3165445

Batch 195290, train_perplexity=182.16626, train_loss=5.20492

Batch 195300, train_perplexity=166.86429, train_loss=5.117181

Batch 195310, train_perplexity=172.36353, train_loss=5.1496058

Batch 195320, train_perplexity=181.79027, train_loss=5.2028537

Batch 195330, train_perplexity=184.91977, train_loss=5.219922

Batch 195340, train_perplexity=204.76132, train_loss=5.321845

Batch 195350, train_perplexity=181.52808, train_loss=5.2014103

Batch 195360, train_perplexity=162.93329, train_loss=5.093341

Batch 195370, train_perplexity=182.5689, train_loss=5.2071276

Batch 195380, train_perplexity=199.52498, train_loss=5.2959394

Batch 195390, train_perplexity=191.85532, train_loss=5.2567415

Batch 195400, train_perplexity=195.04459, train_loss=5.273228

Batch 195410, train_perplexity=189.94926, train_loss=5.246757

Batch 195420, train_perplexity=175.84348, train_loss=5.1695943

Batch 195430, train_perplexity=188.21611, train_loss=5.237591

Batch 195440, train_perplexity=166.24323, train_loss=5.113452

Batch 195450, train_perplexity=190.7601, train_loss=5.2510166

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Batch 195460, train_perplexity=198.36745, train_loss=5.290121

Batch 195470, train_perplexity=198.99698, train_loss=5.2932897

Batch 195480, train_perplexity=169.49178, train_loss=5.1328044

Batch 195490, train_perplexity=197.95631, train_loss=5.2880464

Batch 195500, train_perplexity=174.45093, train_loss=5.1616435

Batch 195510, train_perplexity=184.97745, train_loss=5.220234

Batch 195520, train_perplexity=185.0715, train_loss=5.220742

Batch 195530, train_perplexity=191.38612, train_loss=5.254293

Batch 195540, train_perplexity=156.78604, train_loss=5.054882

Batch 195550, train_perplexity=180.76657, train_loss=5.1972065

Batch 195560, train_perplexity=186.52713, train_loss=5.2285767

Batch 195570, train_perplexity=166.21953, train_loss=5.1133094

Batch 195580, train_perplexity=183.0643, train_loss=5.2098374

Batch 195590, train_perplexity=173.36485, train_loss=5.1553984

Batch 195600, train_perplexity=179.92038, train_loss=5.1925144

Batch 195610, train_perplexity=162.78186, train_loss=5.092411

Batch 195620, train_perplexity=189.60928, train_loss=5.2449656

Batch 195630, train_perplexity=183.84407, train_loss=5.214088

Batch 195640, train_perplexity=194.694, train_loss=5.271429

Batch 195650, train_perplexity=176.11008, train_loss=5.171109

Batch 195660, train_perplexity=197.40216, train_loss=5.285243

Batch 195670, train_perplexity=170.73993, train_loss=5.1401415

Batch 195680, train_perplexity=185.25974, train_loss=5.221759

Batch 195690, train_perplexity=180.6591, train_loss=5.196612

Batch 195700, train_perplexity=180.24986, train_loss=5.194344

Batch 195710, train_perplexity=181.75249, train_loss=5.202646

Batch 195720, train_perplexity=183.2193, train_loss=5.210684

Batch 195730, train_perplexity=188.87979, train_loss=5.241111

Batch 195740, train_perplexity=176.3314, train_loss=5.172365

Batch 195750, train_perplexity=181.1596, train_loss=5.1993785

Batch 195760, train_perplexity=187.51004, train_loss=5.2338324

Batch 195770, train_perplexity=185.26071, train_loss=5.221764

Batch 195780, train_perplexity=192.83163, train_loss=5.2618175

Batch 195790, train_perplexity=183.99202, train_loss=5.2148924

Batch 195800, train_perplexity=165.65434, train_loss=5.1099033

Batch 195810, train_perplexity=160.2506, train_loss=5.076739

Batch 195820, train_perplexity=162.88124, train_loss=5.0930214

Batch 195830, train_perplexity=163.33795, train_loss=5.0958214

Batch 195840, train_perplexity=182.60825, train_loss=5.207343

Batch 195850, train_perplexity=179.34598, train_loss=5.1893167

Batch 195860, train_perplexity=189.07118, train_loss=5.2421236

Batch 195870, train_perplexity=188.51053, train_loss=5.239154

Batch 195880, train_perplexity=189.74821, train_loss=5.245698

Batch 195890, train_perplexity=191.80858, train_loss=5.256498

Batch 195900, train_perplexity=202.2797, train_loss=5.3096514

Batch 195910, train_perplexity=163.86288, train_loss=5.09903

Batch 195920, train_perplexity=188.59486, train_loss=5.239601

Batch 195930, train_perplexity=192.95222, train_loss=5.2624426

Batch 195940, train_perplexity=168.8578, train_loss=5.129057

Batch 195950, train_perplexity=153.05699, train_loss=5.0308104

Batch 195960, train_perplexity=190.00226, train_loss=5.247036

Batch 195970, train_perplexity=178.17538, train_loss=5.1827683

Batch 195980, train_perplexity=192.49402, train_loss=5.260065

Batch 195990, train_perplexity=193.54547, train_loss=5.2655125

Batch 196000, train_perplexity=180.1922, train_loss=5.194024

Batch 196010, train_perplexity=196.41347, train_loss=5.280222

Batch 196020, train_perplexity=222.53123, train_loss=5.4050674

Batch 196030, train_perplexity=176.36874, train_loss=5.172577

Batch 196040, train_perplexity=197.43048, train_loss=5.2853866

Batch 196050, train_perplexity=171.64447, train_loss=5.1454253

Batch 196060, train_perplexity=187.62201, train_loss=5.2344294

Batch 196070, train_perplexity=202.24188, train_loss=5.3094645

Batch 196080, train_perplexity=167.87273, train_loss=5.123206

Batch 196090, train_perplexity=172.44507, train_loss=5.150079

Batch 196100, train_perplexity=183.22542, train_loss=5.210717

Batch 196110, train_perplexity=203.36298, train_loss=5.3149924

Batch 196120, train_perplexity=193.29018, train_loss=5.2641926

Batch 196130, train_perplexity=185.97223, train_loss=5.2255974

Batch 196140, train_perplexity=192.78098, train_loss=5.2615547

Batch 196150, train_perplexity=186.44531, train_loss=5.228138

Batch 196160, train_perplexity=179.3735, train_loss=5.1894703

Batch 196170, train_perplexity=208.33206, train_loss=5.3391333

Batch 196180, train_perplexity=188.10547, train_loss=5.237003

Batch 196190, train_perplexity=164.6381, train_loss=5.1037498

Batch 196200, train_perplexity=196.18872, train_loss=5.279077

Batch 196210, train_perplexity=175.11308, train_loss=5.165432

Batch 196220, train_perplexity=181.32979, train_loss=5.2003174

Batch 196230, train_perplexity=178.68988, train_loss=5.185652

Batch 196240, train_perplexity=196.65965, train_loss=5.2814746

Batch 196250, train_perplexity=186.9163, train_loss=5.230661

Batch 196260, train_perplexity=185.75935, train_loss=5.224452

Batch 196270, train_perplexity=179.27107, train_loss=5.188899

Batch 196280, train_perplexity=192.16423, train_loss=5.2583504
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 196290, train_perplexity=173.76286, train_loss=5.1576915

Batch 196300, train_perplexity=195.23822, train_loss=5.2742205

Batch 196310, train_perplexity=198.97961, train_loss=5.2932024

Batch 196320, train_perplexity=183.12811, train_loss=5.210186

Batch 196330, train_perplexity=170.35104, train_loss=5.1378613

Batch 196340, train_perplexity=170.54172, train_loss=5.13898

Batch 196350, train_perplexity=185.20798, train_loss=5.2214794

Batch 196360, train_perplexity=186.74962, train_loss=5.2297688

Batch 196370, train_perplexity=186.09119, train_loss=5.226237

Batch 196380, train_perplexity=164.27682, train_loss=5.101553

Batch 196390, train_perplexity=183.75003, train_loss=5.2135763

Batch 196400, train_perplexity=195.82767, train_loss=5.277235

Batch 196410, train_perplexity=165.98169, train_loss=5.1118774

Batch 196420, train_perplexity=194.09981, train_loss=5.2683725

Batch 196430, train_perplexity=164.66597, train_loss=5.103919

Batch 196440, train_perplexity=182.58003, train_loss=5.2071886

Batch 196450, train_perplexity=177.91092, train_loss=5.181283

Batch 196460, train_perplexity=171.71857, train_loss=5.145857

Batch 196470, train_perplexity=172.21367, train_loss=5.148736

Batch 196480, train_perplexity=176.41046, train_loss=5.1728134

Batch 196490, train_perplexity=172.86923, train_loss=5.1525354

Batch 196500, train_perplexity=179.1933, train_loss=5.188465

Batch 196510, train_perplexity=169.05841, train_loss=5.1302443

Batch 196520, train_perplexity=176.69374, train_loss=5.174418

Batch 196530, train_perplexity=191.24883, train_loss=5.2535753

Batch 196540, train_perplexity=182.14786, train_loss=5.2048187

Batch 196550, train_perplexity=197.48218, train_loss=5.2856483

Batch 196560, train_perplexity=176.19322, train_loss=5.1715813

Batch 196570, train_perplexity=174.21892, train_loss=5.1603127

Batch 196580, train_perplexity=184.4949, train_loss=5.217622

Batch 196590, train_perplexity=173.02362, train_loss=5.153428

Batch 196600, train_perplexity=181.50133, train_loss=5.201263

Batch 196610, train_perplexity=185.78717, train_loss=5.2246017

Batch 196620, train_perplexity=177.84663, train_loss=5.1809216

Batch 196630, train_perplexity=172.19708, train_loss=5.1486397

Batch 196640, train_perplexity=168.58812, train_loss=5.1274586

Batch 196650, train_perplexity=187.4201, train_loss=5.2333527

Batch 196660, train_perplexity=196.46179, train_loss=5.280468

Batch 196670, train_perplexity=200.00163, train_loss=5.2983255

Batch 196680, train_perplexity=182.26784, train_loss=5.205477

Batch 196690, train_perplexity=171.36201, train_loss=5.1437783

Batch 196700, train_perplexity=185.38557, train_loss=5.222438

Batch 196710, train_perplexity=169.6403, train_loss=5.1336803

Batch 196720, train_perplexity=174.46982, train_loss=5.1617517

Batch 196730, train_perplexity=190.10576, train_loss=5.2475805

Batch 196740, train_perplexity=185.96648, train_loss=5.2255664

Batch 196750, train_perplexity=165.60205, train_loss=5.1095877

Batch 196760, train_perplexity=196.75214, train_loss=5.2819448

Batch 196770, train_perplexity=176.14903, train_loss=5.1713305

Batch 196780, train_perplexity=189.30339, train_loss=5.243351

Batch 196790, train_perplexity=187.3799, train_loss=5.233138

Batch 196800, train_perplexity=184.16556, train_loss=5.215835

Batch 196810, train_perplexity=188.04683, train_loss=5.236691

Batch 196820, train_perplexity=152.54434, train_loss=5.0274553

Batch 196830, train_perplexity=187.60072, train_loss=5.234316

Batch 196840, train_perplexity=186.38167, train_loss=5.2277966

Batch 196850, train_perplexity=190.31418, train_loss=5.2486763

Batch 196860, train_perplexity=188.7506, train_loss=5.2404265

Batch 196870, train_perplexity=187.8381, train_loss=5.2355804

Batch 196880, train_perplexity=212.94449, train_loss=5.3610315

Batch 196890, train_perplexity=165.15604, train_loss=5.1068907

Batch 196900, train_perplexity=159.79971, train_loss=5.073921

Batch 196910, train_perplexity=166.63783, train_loss=5.115823

Batch 196920, train_perplexity=191.46745, train_loss=5.254718

Batch 196930, train_perplexity=179.24295, train_loss=5.188742

Batch 196940, train_perplexity=188.48068, train_loss=5.2389956

Batch 196950, train_perplexity=204.66489, train_loss=5.321374

Batch 196960, train_perplexity=212.83446, train_loss=5.3605146

Batch 196970, train_perplexity=169.85205, train_loss=5.1349277

Batch 196980, train_perplexity=198.34645, train_loss=5.290015

Batch 196990, train_perplexity=176.59966, train_loss=5.1738853

Batch 197000, train_perplexity=187.38794, train_loss=5.233181

Batch 197010, train_perplexity=167.11049, train_loss=5.118655

Batch 197020, train_perplexity=177.52661, train_loss=5.1791205

Batch 197030, train_perplexity=175.59805, train_loss=5.1681976

Batch 197040, train_perplexity=188.72783, train_loss=5.240306

Batch 197050, train_perplexity=171.56714, train_loss=5.1449747

Batch 197060, train_perplexity=184.53818, train_loss=5.2178564

Batch 197070, train_perplexity=172.46242, train_loss=5.1501794

Batch 197080, train_perplexity=193.16174, train_loss=5.263528

Batch 197090, train_perplexity=177.50757, train_loss=5.1790133

Batch 197100, train_perplexity=198.44104, train_loss=5.290492

Batch 197110, train_perplexity=158.31258, train_loss=5.0645714

Batch 197120, train_perplexity=197.27512, train_loss=5.2845993

Batch 197130, train_perplexity=173.7312, train_loss=5.1575093

Batch 197140, train_perplexity=185.30197, train_loss=5.221987

Batch 197150, train_perplexity=175.4203, train_loss=5.167185

Batch 197160, train_perplexity=167.81927, train_loss=5.1228876

Batch 197170, train_perplexity=178.79727, train_loss=5.1862526

Batch 197180, train_perplexity=187.73227, train_loss=5.235017

Batch 197190, train_perplexity=182.12787, train_loss=5.204709

Batch 197200, train_perplexity=174.69675, train_loss=5.1630516

Batch 197210, train_perplexity=188.41492, train_loss=5.2386465

Batch 197220, train_perplexity=200.01498, train_loss=5.2983923

Batch 197230, train_perplexity=171.71512, train_loss=5.145837

Batch 197240, train_perplexity=184.06047, train_loss=5.2152643

Batch 197250, train_perplexity=175.8559, train_loss=5.169665

Batch 197260, train_perplexity=183.85406, train_loss=5.2141423

Batch 197270, train_perplexity=177.8724, train_loss=5.1810665

Batch 197280, train_perplexity=183.66997, train_loss=5.2131405

Batch 197290, train_perplexity=191.4648, train_loss=5.254704

Batch 197300, train_perplexity=182.9356, train_loss=5.209134

Batch 197310, train_perplexity=195.5373, train_loss=5.275751

Batch 197320, train_perplexity=184.25822, train_loss=5.216338

Batch 197330, train_perplexity=189.7102, train_loss=5.2454977

Batch 197340, train_perplexity=182.98811, train_loss=5.209421

Batch 197350, train_perplexity=185.6214, train_loss=5.223709

Batch 197360, train_perplexity=171.82405, train_loss=5.146471

Batch 197370, train_perplexity=171.2964, train_loss=5.1433954

Batch 197380, train_perplexity=178.94022, train_loss=5.187052

Batch 197390, train_perplexity=177.59274, train_loss=5.179493

Batch 197400, train_perplexity=186.0683, train_loss=5.226114

Batch 197410, train_perplexity=189.05658, train_loss=5.2420464

Batch 197420, train_perplexity=175.84969, train_loss=5.1696296

Batch 197430, train_perplexity=181.16583, train_loss=5.199413

Batch 197440, train_perplexity=179.26372, train_loss=5.188858

Batch 197450, train_perplexity=169.73943, train_loss=5.1342645

Batch 197460, train_perplexity=181.64651, train_loss=5.2020626

Batch 197470, train_perplexity=160.16855, train_loss=5.0762267

Batch 197480, train_perplexity=179.03769, train_loss=5.1875963

Batch 197490, train_perplexity=185.15738, train_loss=5.221206

Batch 197500, train_perplexity=175.03537, train_loss=5.164988

Batch 197510, train_perplexity=179.59158, train_loss=5.1906853

Batch 197520, train_perplexity=179.20457, train_loss=5.188528

Batch 197530, train_perplexity=182.0413, train_loss=5.2042336

Batch 197540, train_perplexity=170.32724, train_loss=5.1377215

Batch 197550, train_perplexity=191.1396, train_loss=5.253004

Batch 197560, train_perplexity=178.72021, train_loss=5.1858215

Batch 197570, train_perplexity=178.3817, train_loss=5.1839256

Batch 197580, train_perplexity=186.7554, train_loss=5.2297997
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 197590, train_perplexity=192.08975, train_loss=5.2579627

Batch 197600, train_perplexity=177.87173, train_loss=5.1810627

Batch 197610, train_perplexity=188.14503, train_loss=5.237213

Batch 197620, train_perplexity=206.06345, train_loss=5.328184

Batch 197630, train_perplexity=189.74803, train_loss=5.245697

Batch 197640, train_perplexity=189.27632, train_loss=5.243208

Batch 197650, train_perplexity=194.84445, train_loss=5.2722015

Batch 197660, train_perplexity=196.41609, train_loss=5.2802353

Batch 197670, train_perplexity=165.42432, train_loss=5.108514

Batch 197680, train_perplexity=173.68573, train_loss=5.1572475

Batch 197690, train_perplexity=189.96974, train_loss=5.246865

Batch 197700, train_perplexity=173.7733, train_loss=5.1577516

Batch 197710, train_perplexity=170.91898, train_loss=5.1411896

Batch 197720, train_perplexity=194.64201, train_loss=5.271162

Batch 197730, train_perplexity=181.75153, train_loss=5.2026405

Batch 197740, train_perplexity=163.76875, train_loss=5.0984554

Batch 197750, train_perplexity=186.12137, train_loss=5.226399

Batch 197760, train_perplexity=158.12827, train_loss=5.0634065

Batch 197770, train_perplexity=190.9046, train_loss=5.251774

Batch 197780, train_perplexity=166.66064, train_loss=5.1159596

Batch 197790, train_perplexity=183.32715, train_loss=5.2112722

Batch 197800, train_perplexity=179.61197, train_loss=5.1907988

Batch 197810, train_perplexity=206.12831, train_loss=5.328499

Batch 197820, train_perplexity=170.3975, train_loss=5.138134

Batch 197830, train_perplexity=200.3591, train_loss=5.3001113

Batch 197840, train_perplexity=194.58243, train_loss=5.270856

Batch 197850, train_perplexity=183.20288, train_loss=5.210594

Batch 197860, train_perplexity=169.63448, train_loss=5.133646

Batch 197870, train_perplexity=173.61884, train_loss=5.1568623

Batch 197880, train_perplexity=173.04176, train_loss=5.153533

Batch 197890, train_perplexity=184.41434, train_loss=5.217185

Batch 197900, train_perplexity=190.18555, train_loss=5.248

Batch 197910, train_perplexity=200.44855, train_loss=5.3005576

Batch 197920, train_perplexity=158.70242, train_loss=5.067031

Batch 197930, train_perplexity=174.60306, train_loss=5.162515

Batch 197940, train_perplexity=169.988, train_loss=5.135728

Batch 197950, train_perplexity=174.54454, train_loss=5.16218

Batch 197960, train_perplexity=173.64209, train_loss=5.1569963

Batch 197970, train_perplexity=181.61916, train_loss=5.201912

Batch 197980, train_perplexity=186.31583, train_loss=5.227443

Batch 197990, train_perplexity=168.64014, train_loss=5.127767

Batch 198000, train_perplexity=179.23996, train_loss=5.1887255

Batch 198010, train_perplexity=180.83347, train_loss=5.1975765

Batch 198020, train_perplexity=186.30525, train_loss=5.2273865

Batch 198030, train_perplexity=177.60384, train_loss=5.1795554

Batch 198040, train_perplexity=188.25308, train_loss=5.2377872

Batch 198050, train_perplexity=188.29366, train_loss=5.238003

Batch 198060, train_perplexity=188.82883, train_loss=5.240841

Batch 198070, train_perplexity=170.38809, train_loss=5.1380787

Batch 198080, train_perplexity=192.8082, train_loss=5.261696

Batch 198090, train_perplexity=174.00829, train_loss=5.159103

Batch 198100, train_perplexity=187.28279, train_loss=5.23262

Batch 198110, train_perplexity=178.07336, train_loss=5.1821957

Batch 198120, train_perplexity=196.28894, train_loss=5.2795877

Batch 198130, train_perplexity=194.2549, train_loss=5.269171

Batch 198140, train_perplexity=187.2594, train_loss=5.232495

Batch 198150, train_perplexity=176.39447, train_loss=5.172723

Batch 198160, train_perplexity=190.91098, train_loss=5.251807

Batch 198170, train_perplexity=174.63228, train_loss=5.1626825

Batch 198180, train_perplexity=178.9601, train_loss=5.187163

Batch 198190, train_perplexity=175.68047, train_loss=5.168667

Batch 198200, train_perplexity=187.55403, train_loss=5.234067

Batch 198210, train_perplexity=174.94232, train_loss=5.1644564

Batch 198220, train_perplexity=178.17622, train_loss=5.182773

Batch 198230, train_perplexity=180.30016, train_loss=5.194623

Batch 198240, train_perplexity=181.77675, train_loss=5.2027793

Batch 198250, train_perplexity=179.17485, train_loss=5.188362

Batch 198260, train_perplexity=172.39952, train_loss=5.1498146

Batch 198270, train_perplexity=189.98415, train_loss=5.2469406

Batch 198280, train_perplexity=177.53288, train_loss=5.179156

Batch 198290, train_perplexity=178.70923, train_loss=5.18576

Batch 198300, train_perplexity=172.99779, train_loss=5.153279

Batch 198310, train_perplexity=177.41313, train_loss=5.178481

Batch 198320, train_perplexity=167.39313, train_loss=5.120345

Batch 198330, train_perplexity=190.15489, train_loss=5.247839

Batch 198340, train_perplexity=188.77426, train_loss=5.240552

Batch 198350, train_perplexity=173.16475, train_loss=5.1542435

Batch 198360, train_perplexity=169.095, train_loss=5.1304607

Batch 198370, train_perplexity=176.69113, train_loss=5.174403

Batch 198380, train_perplexity=207.44229, train_loss=5.334853

Batch 198390, train_perplexity=173.57198, train_loss=5.1565924

Batch 198400, train_perplexity=168.88292, train_loss=5.1292057

Batch 198410, train_perplexity=168.97668, train_loss=5.1297607

Batch 198420, train_perplexity=180.71864, train_loss=5.1969414

Batch 198430, train_perplexity=183.13379, train_loss=5.210217

Batch 198440, train_perplexity=190.59207, train_loss=5.2501354

Batch 198450, train_perplexity=198.37964, train_loss=5.2901826

Batch 198460, train_perplexity=186.99635, train_loss=5.231089

Batch 198470, train_perplexity=165.39096, train_loss=5.108312

Batch 198480, train_perplexity=175.18324, train_loss=5.1658325

Batch 198490, train_perplexity=183.19223, train_loss=5.210536

Batch 198500, train_perplexity=192.3087, train_loss=5.259102

Batch 198510, train_perplexity=206.12517, train_loss=5.3284836

Batch 198520, train_perplexity=175.08237, train_loss=5.1652565

Batch 198530, train_perplexity=167.98123, train_loss=5.1238523

Batch 198540, train_perplexity=213.24994, train_loss=5.362465

Batch 198550, train_perplexity=192.30713, train_loss=5.2590938

Batch 198560, train_perplexity=189.28218, train_loss=5.243239

Batch 198570, train_perplexity=179.70894, train_loss=5.1913385

Batch 198580, train_perplexity=190.06642, train_loss=5.2473736

Batch 198590, train_perplexity=186.90053, train_loss=5.2305765

Batch 198600, train_perplexity=186.07832, train_loss=5.2261677

Batch 198610, train_perplexity=187.65503, train_loss=5.2346053

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00061-of-00100
Loaded 306420 sentences.
Finished loading
Batch 198620, train_perplexity=199.94176, train_loss=5.298026

Batch 198630, train_perplexity=199.1577, train_loss=5.294097

Batch 198640, train_perplexity=168.5161, train_loss=5.1270313

Batch 198650, train_perplexity=178.12569, train_loss=5.1824894

Batch 198660, train_perplexity=184.07082, train_loss=5.2153206

Batch 198670, train_perplexity=180.5376, train_loss=5.195939

Batch 198680, train_perplexity=181.40244, train_loss=5.200718

Batch 198690, train_perplexity=184.19498, train_loss=5.215995

Batch 198700, train_perplexity=191.61496, train_loss=5.255488

Batch 198710, train_perplexity=176.31165, train_loss=5.172253

Batch 198720, train_perplexity=193.18367, train_loss=5.2636414

Batch 198730, train_perplexity=200.15294, train_loss=5.299082

Batch 198740, train_perplexity=183.65088, train_loss=5.2130365

Batch 198750, train_perplexity=171.05896, train_loss=5.1420083

Batch 198760, train_perplexity=179.70996, train_loss=5.1913443

Batch 198770, train_perplexity=175.86394, train_loss=5.1697106

Batch 198780, train_perplexity=222.40404, train_loss=5.4044957

Batch 198790, train_perplexity=191.51904, train_loss=5.2549872

Batch 198800, train_perplexity=184.36122, train_loss=5.216897

Batch 198810, train_perplexity=177.99544, train_loss=5.181758

Batch 198820, train_perplexity=190.98964, train_loss=5.252219
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 198830, train_perplexity=174.3715, train_loss=5.161188

Batch 198840, train_perplexity=198.88097, train_loss=5.2927065

Batch 198850, train_perplexity=192.83044, train_loss=5.2618113

Batch 198860, train_perplexity=173.71397, train_loss=5.15741

Batch 198870, train_perplexity=169.46243, train_loss=5.1326313

Batch 198880, train_perplexity=195.10207, train_loss=5.273523

Batch 198890, train_perplexity=167.40128, train_loss=5.1203938

Batch 198900, train_perplexity=165.1944, train_loss=5.107123

Batch 198910, train_perplexity=186.8802, train_loss=5.230468

Batch 198920, train_perplexity=188.42857, train_loss=5.238719

Batch 198930, train_perplexity=196.21146, train_loss=5.279193

Batch 198940, train_perplexity=172.34758, train_loss=5.1495132

Batch 198950, train_perplexity=204.69066, train_loss=5.3215

Batch 198960, train_perplexity=165.65971, train_loss=5.1099358

Batch 198970, train_perplexity=172.093, train_loss=5.148035

Batch 198980, train_perplexity=200.6267, train_loss=5.301446

Batch 198990, train_perplexity=190.68806, train_loss=5.250639

Batch 199000, train_perplexity=178.91565, train_loss=5.1869144

Batch 199010, train_perplexity=179.17946, train_loss=5.188388

Batch 199020, train_perplexity=186.42487, train_loss=5.2280283

Batch 199030, train_perplexity=200.63377, train_loss=5.3014812

Batch 199040, train_perplexity=186.20045, train_loss=5.226824

Batch 199050, train_perplexity=178.23741, train_loss=5.1831164

Batch 199060, train_perplexity=182.8979, train_loss=5.208928

Batch 199070, train_perplexity=199.5288, train_loss=5.2959585

Batch 199080, train_perplexity=193.10455, train_loss=5.2632318

Batch 199090, train_perplexity=193.93948, train_loss=5.267546

Batch 199100, train_perplexity=177.92188, train_loss=5.1813445

Batch 199110, train_perplexity=188.45148, train_loss=5.2388406

Batch 199120, train_perplexity=171.79424, train_loss=5.1462975

Batch 199130, train_perplexity=181.43877, train_loss=5.200918

Batch 199140, train_perplexity=172.13165, train_loss=5.1482596

Batch 199150, train_perplexity=203.97481, train_loss=5.3179965

Batch 199160, train_perplexity=174.2549, train_loss=5.160519

Batch 199170, train_perplexity=155.51666, train_loss=5.046753

Batch 199180, train_perplexity=192.02556, train_loss=5.2576284

Batch 199190, train_perplexity=167.44167, train_loss=5.120635

Batch 199200, train_perplexity=166.73973, train_loss=5.116434

Batch 199210, train_perplexity=187.7644, train_loss=5.235188

Batch 199220, train_perplexity=177.92552, train_loss=5.181365

Batch 199230, train_perplexity=185.9685, train_loss=5.2255774

Batch 199240, train_perplexity=186.53397, train_loss=5.2286134

Batch 199250, train_perplexity=165.0149, train_loss=5.1060357

Batch 199260, train_perplexity=176.35529, train_loss=5.1725006

Batch 199270, train_perplexity=177.07558, train_loss=5.1765766

Batch 199280, train_perplexity=180.10837, train_loss=5.1935587

Batch 199290, train_perplexity=167.84247, train_loss=5.123026

Batch 199300, train_perplexity=169.68529, train_loss=5.1339455

Batch 199310, train_perplexity=181.05261, train_loss=5.1987877

Batch 199320, train_perplexity=184.4854, train_loss=5.2175703

Batch 199330, train_perplexity=201.47052, train_loss=5.305643

Batch 199340, train_perplexity=194.16026, train_loss=5.268684

Batch 199350, train_perplexity=193.96547, train_loss=5.26768

Batch 199360, train_perplexity=166.94754, train_loss=5.1176796

Batch 199370, train_perplexity=181.57024, train_loss=5.2016425

Batch 199380, train_perplexity=181.50764, train_loss=5.2012978

Batch 199390, train_perplexity=208.58266, train_loss=5.3403354

Batch 199400, train_perplexity=179.28287, train_loss=5.188965

Batch 199410, train_perplexity=175.68063, train_loss=5.168668

Batch 199420, train_perplexity=182.21179, train_loss=5.2051697

Batch 199430, train_perplexity=169.9183, train_loss=5.135318

Batch 199440, train_perplexity=174.83092, train_loss=5.1638193

Batch 199450, train_perplexity=182.79808, train_loss=5.208382

Batch 199460, train_perplexity=182.67033, train_loss=5.207683

Batch 199470, train_perplexity=180.01126, train_loss=5.1930194

Batch 199480, train_perplexity=194.7732, train_loss=5.271836

Batch 199490, train_perplexity=176.8534, train_loss=5.175321

Batch 199500, train_perplexity=191.69528, train_loss=5.255907

Batch 199510, train_perplexity=179.14665, train_loss=5.188205

Batch 199520, train_perplexity=203.2764, train_loss=5.3145666

Batch 199530, train_perplexity=185.43951, train_loss=5.2227287

Batch 199540, train_perplexity=181.0041, train_loss=5.1985197

Batch 199550, train_perplexity=185.5467, train_loss=5.2233067

Batch 199560, train_perplexity=193.92053, train_loss=5.2674484

Batch 199570, train_perplexity=193.16708, train_loss=5.2635555

Batch 199580, train_perplexity=171.27582, train_loss=5.1432753

Batch 199590, train_perplexity=198.75128, train_loss=5.292054

Batch 199600, train_perplexity=171.28171, train_loss=5.1433096

Batch 199610, train_perplexity=212.89839, train_loss=5.360815

Batch 199620, train_perplexity=189.9885, train_loss=5.2469635

Batch 199630, train_perplexity=181.81412, train_loss=5.202985

Batch 199640, train_perplexity=187.5848, train_loss=5.234231

Batch 199650, train_perplexity=174.63702, train_loss=5.1627097

Batch 199660, train_perplexity=172.63751, train_loss=5.151194

Batch 199670, train_perplexity=203.08708, train_loss=5.313635

Batch 199680, train_perplexity=178.56723, train_loss=5.184965

Batch 199690, train_perplexity=211.85835, train_loss=5.355918

Batch 199700, train_perplexity=194.91972, train_loss=5.272588

Batch 199710, train_perplexity=173.3685, train_loss=5.1554193

Batch 199720, train_perplexity=176.46977, train_loss=5.1731496

Batch 199730, train_perplexity=175.11977, train_loss=5.16547

Batch 199740, train_perplexity=177.08487, train_loss=5.176629

Batch 199750, train_perplexity=161.83131, train_loss=5.0865545

Batch 199760, train_perplexity=186.0746, train_loss=5.2261477

Batch 199770, train_perplexity=177.03775, train_loss=5.176363

Batch 199780, train_perplexity=182.4468, train_loss=5.2064586

Batch 199790, train_perplexity=181.65431, train_loss=5.2021055

Batch 199800, train_perplexity=200.62364, train_loss=5.3014307

Batch 199810, train_perplexity=186.84555, train_loss=5.2302823

Batch 199820, train_perplexity=187.3547, train_loss=5.2330036

Batch 199830, train_perplexity=184.63765, train_loss=5.218395

Batch 199840, train_perplexity=173.06651, train_loss=5.153676

Batch 199850, train_perplexity=174.48346, train_loss=5.16183

Batch 199860, train_perplexity=181.64998, train_loss=5.2020817

Batch 199870, train_perplexity=212.2019, train_loss=5.357538

Batch 199880, train_perplexity=180.28786, train_loss=5.194555

Batch 199890, train_perplexity=173.63124, train_loss=5.156934

Batch 199900, train_perplexity=182.8449, train_loss=5.208638

Batch 199910, train_perplexity=180.76466, train_loss=5.197196

Batch 199920, train_perplexity=177.35782, train_loss=5.1781693

Batch 199930, train_perplexity=178.81824, train_loss=5.18637

Batch 199940, train_perplexity=183.81654, train_loss=5.213938

Batch 199950, train_perplexity=186.8556, train_loss=5.230336

Batch 199960, train_perplexity=172.36237, train_loss=5.149599

Batch 199970, train_perplexity=205.23709, train_loss=5.324166

Batch 199980, train_perplexity=184.51407, train_loss=5.2177258

Batch 199990, train_perplexity=173.50627, train_loss=5.1562138

Batch 200000, train_perplexity=171.81882, train_loss=5.1464405

Batch 200010, train_perplexity=198.99138, train_loss=5.2932615

Batch 200020, train_perplexity=174.70291, train_loss=5.163087

Batch 200030, train_perplexity=181.70665, train_loss=5.2023935

Batch 200040, train_perplexity=180.91937, train_loss=5.1980515

Batch 200050, train_perplexity=168.77988, train_loss=5.1285954

Batch 200060, train_perplexity=189.34094, train_loss=5.2435493

Batch 200070, train_perplexity=181.50246, train_loss=5.201269

Batch 200080, train_perplexity=190.39043, train_loss=5.249077

Batch 200090, train_perplexity=196.17497, train_loss=5.279007

Batch 200100, train_perplexity=171.33562, train_loss=5.1436243

Batch 200110, train_perplexity=209.56844, train_loss=5.3450503

Batch 200120, train_perplexity=172.16432, train_loss=5.1484494
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 200130, train_perplexity=181.85495, train_loss=5.2032094

Batch 200140, train_perplexity=176.0303, train_loss=5.170656

Batch 200150, train_perplexity=184.23415, train_loss=5.2162075

Batch 200160, train_perplexity=176.94458, train_loss=5.1758366

Batch 200170, train_perplexity=177.28197, train_loss=5.1777415

Batch 200180, train_perplexity=162.85306, train_loss=5.0928483

Batch 200190, train_perplexity=175.6386, train_loss=5.1684284

Batch 200200, train_perplexity=158.9037, train_loss=5.0682983

Batch 200210, train_perplexity=157.21605, train_loss=5.057621

Batch 200220, train_perplexity=188.06952, train_loss=5.2368116

Batch 200230, train_perplexity=165.07202, train_loss=5.106382

Batch 200240, train_perplexity=176.23254, train_loss=5.1718044

Batch 200250, train_perplexity=182.3126, train_loss=5.205723

Batch 200260, train_perplexity=176.78055, train_loss=5.174909

Batch 200270, train_perplexity=171.70784, train_loss=5.1457944

Batch 200280, train_perplexity=191.21837, train_loss=5.253416

Batch 200290, train_perplexity=167.29466, train_loss=5.1197567

Batch 200300, train_perplexity=192.83485, train_loss=5.261834

Batch 200310, train_perplexity=202.0916, train_loss=5.308721

Batch 200320, train_perplexity=186.22638, train_loss=5.226963

Batch 200330, train_perplexity=174.71425, train_loss=5.1631517

Batch 200340, train_perplexity=184.56581, train_loss=5.218006

Batch 200350, train_perplexity=175.40224, train_loss=5.167082

Batch 200360, train_perplexity=170.11685, train_loss=5.1364856

Batch 200370, train_perplexity=176.75787, train_loss=5.174781

Batch 200380, train_perplexity=183.63275, train_loss=5.212938

Batch 200390, train_perplexity=178.13281, train_loss=5.1825294

Batch 200400, train_perplexity=203.24625, train_loss=5.3144183

Batch 200410, train_perplexity=175.41528, train_loss=5.167156

Batch 200420, train_perplexity=179.73662, train_loss=5.1914926

Batch 200430, train_perplexity=196.57443, train_loss=5.281041

Batch 200440, train_perplexity=185.84033, train_loss=5.224888

Batch 200450, train_perplexity=182.30981, train_loss=5.2057076

Batch 200460, train_perplexity=171.17947, train_loss=5.1427126

Batch 200470, train_perplexity=175.5835, train_loss=5.1681147

Batch 200480, train_perplexity=169.42366, train_loss=5.1324024

Batch 200490, train_perplexity=173.19696, train_loss=5.1544294

Batch 200500, train_perplexity=186.62463, train_loss=5.2290993

Batch 200510, train_perplexity=165.96657, train_loss=5.1117864

Batch 200520, train_perplexity=191.9232, train_loss=5.2570953

Batch 200530, train_perplexity=199.53145, train_loss=5.295972

Batch 200540, train_perplexity=173.87947, train_loss=5.1583624

Batch 200550, train_perplexity=192.53166, train_loss=5.2602606

Batch 200560, train_perplexity=174.37616, train_loss=5.161215

Batch 200570, train_perplexity=167.72894, train_loss=5.1223493

Batch 200580, train_perplexity=179.32169, train_loss=5.1891813

Batch 200590, train_perplexity=181.48099, train_loss=5.201151

Batch 200600, train_perplexity=188.50208, train_loss=5.239109

Batch 200610, train_perplexity=181.32208, train_loss=5.200275

Batch 200620, train_perplexity=193.33092, train_loss=5.2644033

Batch 200630, train_perplexity=197.59145, train_loss=5.2862015

Batch 200640, train_perplexity=204.01363, train_loss=5.3181868

Batch 200650, train_perplexity=169.37358, train_loss=5.132107

Batch 200660, train_perplexity=179.86427, train_loss=5.1922026

Batch 200670, train_perplexity=172.67497, train_loss=5.151411

Batch 200680, train_perplexity=193.39288, train_loss=5.264724

Batch 200690, train_perplexity=179.56349, train_loss=5.190529

Batch 200700, train_perplexity=176.00496, train_loss=5.170512

Batch 200710, train_perplexity=193.40846, train_loss=5.2648044

Batch 200720, train_perplexity=156.01863, train_loss=5.0499754

Batch 200730, train_perplexity=173.8275, train_loss=5.1580634

Batch 200740, train_perplexity=170.8168, train_loss=5.1405916

Batch 200750, train_perplexity=193.07812, train_loss=5.263095

Batch 200760, train_perplexity=175.10023, train_loss=5.1653585

Batch 200770, train_perplexity=186.68176, train_loss=5.2294054

Batch 200780, train_perplexity=156.5396, train_loss=5.053309

Batch 200790, train_perplexity=166.89293, train_loss=5.1173525

Batch 200800, train_perplexity=177.78694, train_loss=5.180586

Batch 200810, train_perplexity=189.43117, train_loss=5.2440257

Batch 200820, train_perplexity=159.15903, train_loss=5.069904

Batch 200830, train_perplexity=187.58981, train_loss=5.2342577

Batch 200840, train_perplexity=199.79994, train_loss=5.2973166

Batch 200850, train_perplexity=183.05609, train_loss=5.2097926

Batch 200860, train_perplexity=194.5162, train_loss=5.2705154

Batch 200870, train_perplexity=161.44225, train_loss=5.0841475

Batch 200880, train_perplexity=177.36467, train_loss=5.178208

Batch 200890, train_perplexity=198.71194, train_loss=5.2918563

Batch 200900, train_perplexity=177.19128, train_loss=5.17723

Batch 200910, train_perplexity=176.58315, train_loss=5.173792

Batch 200920, train_perplexity=180.91661, train_loss=5.198036

Batch 200930, train_perplexity=184.4539, train_loss=5.2173996

Batch 200940, train_perplexity=186.01889, train_loss=5.225848

Batch 200950, train_perplexity=194.8071, train_loss=5.27201

Batch 200960, train_perplexity=195.84, train_loss=5.277298

Batch 200970, train_perplexity=171.87839, train_loss=5.146787

Batch 200980, train_perplexity=186.82745, train_loss=5.2301855

Batch 200990, train_perplexity=178.24413, train_loss=5.183154

Batch 201000, train_perplexity=169.865, train_loss=5.135004

Batch 201010, train_perplexity=208.8192, train_loss=5.341469

Batch 201020, train_perplexity=197.32103, train_loss=5.284832

Batch 201030, train_perplexity=187.09616, train_loss=5.2316227

Batch 201040, train_perplexity=199.94824, train_loss=5.2980585

Batch 201050, train_perplexity=189.46115, train_loss=5.244184

Batch 201060, train_perplexity=208.26105, train_loss=5.3387923

Batch 201070, train_perplexity=192.66739, train_loss=5.2609653

Batch 201080, train_perplexity=207.62576, train_loss=5.335737

Batch 201090, train_perplexity=190.32834, train_loss=5.2487507

Batch 201100, train_perplexity=183.669, train_loss=5.2131352

Batch 201110, train_perplexity=165.16391, train_loss=5.1069384

Batch 201120, train_perplexity=166.57793, train_loss=5.1154633

Batch 201130, train_perplexity=187.00589, train_loss=5.23114

Batch 201140, train_perplexity=196.89423, train_loss=5.2826667

Batch 201150, train_perplexity=199.83424, train_loss=5.297488

Batch 201160, train_perplexity=166.39304, train_loss=5.1143527

Batch 201170, train_perplexity=196.05826, train_loss=5.278412

Batch 201180, train_perplexity=192.88074, train_loss=5.262072

Batch 201190, train_perplexity=191.14471, train_loss=5.253031

Batch 201200, train_perplexity=183.10646, train_loss=5.2100677

Batch 201210, train_perplexity=158.49393, train_loss=5.0657163

Batch 201220, train_perplexity=195.6298, train_loss=5.276224

Batch 201230, train_perplexity=207.68636, train_loss=5.336029

Batch 201240, train_perplexity=177.34328, train_loss=5.178087

Batch 201250, train_perplexity=178.6822, train_loss=5.185609

Batch 201260, train_perplexity=159.96317, train_loss=5.0749435

Batch 201270, train_perplexity=189.72324, train_loss=5.2455664

Batch 201280, train_perplexity=177.07422, train_loss=5.176569

Batch 201290, train_perplexity=204.76523, train_loss=5.321864

Batch 201300, train_perplexity=182.22273, train_loss=5.2052298

Batch 201310, train_perplexity=172.46045, train_loss=5.150168

Batch 201320, train_perplexity=209.77798, train_loss=5.34605

Batch 201330, train_perplexity=165.78456, train_loss=5.110689

Batch 201340, train_perplexity=192.05467, train_loss=5.25778

Batch 201350, train_perplexity=171.34085, train_loss=5.143655

Batch 201360, train_perplexity=165.30226, train_loss=5.1077757

Batch 201370, train_perplexity=182.83826, train_loss=5.208602

Batch 201380, train_perplexity=179.15503, train_loss=5.1882515

Batch 201390, train_perplexity=192.77896, train_loss=5.261544

Batch 201400, train_perplexity=195.78528, train_loss=5.2770185

Batch 201410, train_perplexity=174.30484, train_loss=5.1608057

Batch 201420, train_perplexity=194.40251, train_loss=5.269931
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 201430, train_perplexity=182.24889, train_loss=5.2053733

Batch 201440, train_perplexity=168.5218, train_loss=5.127065

Batch 201450, train_perplexity=181.11444, train_loss=5.199129

Batch 201460, train_perplexity=169.5195, train_loss=5.132968

Batch 201470, train_perplexity=174.09535, train_loss=5.159603

Batch 201480, train_perplexity=172.47253, train_loss=5.150238

Batch 201490, train_perplexity=176.92711, train_loss=5.175738

Batch 201500, train_perplexity=189.44064, train_loss=5.244076

Batch 201510, train_perplexity=179.5736, train_loss=5.190585

Batch 201520, train_perplexity=173.88976, train_loss=5.1584215

Batch 201530, train_perplexity=187.24055, train_loss=5.232394

Batch 201540, train_perplexity=170.86763, train_loss=5.140889

Batch 201550, train_perplexity=174.9229, train_loss=5.1643453

Batch 201560, train_perplexity=178.32446, train_loss=5.1836047

Batch 201570, train_perplexity=194.17165, train_loss=5.2687426

Batch 201580, train_perplexity=182.22395, train_loss=5.2052364

Batch 201590, train_perplexity=191.97447, train_loss=5.2573624

Batch 201600, train_perplexity=155.09485, train_loss=5.044037

Batch 201610, train_perplexity=190.90369, train_loss=5.251769

Batch 201620, train_perplexity=171.95676, train_loss=5.147243

Batch 201630, train_perplexity=179.26749, train_loss=5.188879

Batch 201640, train_perplexity=180.60622, train_loss=5.196319

Batch 201650, train_perplexity=184.28291, train_loss=5.216472

Batch 201660, train_perplexity=172.72685, train_loss=5.1517115

Batch 201670, train_perplexity=179.34273, train_loss=5.1892986

Batch 201680, train_perplexity=200.29196, train_loss=5.299776

Batch 201690, train_perplexity=170.80556, train_loss=5.140526

Batch 201700, train_perplexity=187.9397, train_loss=5.236121

Batch 201710, train_perplexity=179.29228, train_loss=5.1890173

Batch 201720, train_perplexity=197.59729, train_loss=5.286231

Batch 201730, train_perplexity=171.4816, train_loss=5.144476

Batch 201740, train_perplexity=185.01009, train_loss=5.2204103

Batch 201750, train_perplexity=194.16554, train_loss=5.268711

Batch 201760, train_perplexity=180.17227, train_loss=5.1939135

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00003-of-00100
Loaded 305915 sentences.
Finished loading
Batch 201770, train_perplexity=188.36514, train_loss=5.2383823

Batch 201780, train_perplexity=190.47487, train_loss=5.2495203

Batch 201790, train_perplexity=174.33792, train_loss=5.1609955

Batch 201800, train_perplexity=176.65288, train_loss=5.1741867

Batch 201810, train_perplexity=194.33264, train_loss=5.2695713

Batch 201820, train_perplexity=172.09506, train_loss=5.148047

Batch 201830, train_perplexity=208.38461, train_loss=5.3393855

Batch 201840, train_perplexity=183.82996, train_loss=5.214011

Batch 201850, train_perplexity=177.29913, train_loss=5.1778383

Batch 201860, train_perplexity=184.4678, train_loss=5.217475

Batch 201870, train_perplexity=187.74579, train_loss=5.235089

Batch 201880, train_perplexity=173.64375, train_loss=5.157006

Batch 201890, train_perplexity=185.92506, train_loss=5.2253437

Batch 201900, train_perplexity=173.89796, train_loss=5.1584687

Batch 201910, train_perplexity=182.20415, train_loss=5.2051277

Batch 201920, train_perplexity=181.5556, train_loss=5.201562

Batch 201930, train_perplexity=172.91531, train_loss=5.152802

Batch 201940, train_perplexity=197.0043, train_loss=5.2832255

Batch 201950, train_perplexity=185.14458, train_loss=5.221137

Batch 201960, train_perplexity=153.41898, train_loss=5.0331726

Batch 201970, train_perplexity=158.64189, train_loss=5.0666494

Batch 201980, train_perplexity=180.36482, train_loss=5.1949816

Batch 201990, train_perplexity=161.44579, train_loss=5.0841694

Batch 202000, train_perplexity=174.2888, train_loss=5.1607137

Batch 202010, train_perplexity=175.218, train_loss=5.166031

Batch 202020, train_perplexity=178.0114, train_loss=5.1818476

Batch 202030, train_perplexity=163.73378, train_loss=5.098242

Batch 202040, train_perplexity=189.77481, train_loss=5.245838

Batch 202050, train_perplexity=191.98618, train_loss=5.2574234

Batch 202060, train_perplexity=178.05962, train_loss=5.1821184

Batch 202070, train_perplexity=175.15567, train_loss=5.165675

Batch 202080, train_perplexity=178.30882, train_loss=5.183517

Batch 202090, train_perplexity=213.89275, train_loss=5.3654747

Batch 202100, train_perplexity=185.097, train_loss=5.22088

Batch 202110, train_perplexity=181.67589, train_loss=5.2022243

Batch 202120, train_perplexity=196.46011, train_loss=5.2804594

Batch 202130, train_perplexity=182.90227, train_loss=5.208952

Batch 202140, train_perplexity=178.9304, train_loss=5.186997

Batch 202150, train_perplexity=177.22897, train_loss=5.1774426

Batch 202160, train_perplexity=200.51604, train_loss=5.3008943

Batch 202170, train_perplexity=176.02167, train_loss=5.170607

Batch 202180, train_perplexity=192.29659, train_loss=5.259039

Batch 202190, train_perplexity=181.95782, train_loss=5.203775

Batch 202200, train_perplexity=197.53784, train_loss=5.28593

Batch 202210, train_perplexity=174.60364, train_loss=5.1625185

Batch 202220, train_perplexity=209.52837, train_loss=5.344859

Batch 202230, train_perplexity=168.09943, train_loss=5.1245556

Batch 202240, train_perplexity=209.52577, train_loss=5.3448467

Batch 202250, train_perplexity=159.11281, train_loss=5.0696135

Batch 202260, train_perplexity=167.17169, train_loss=5.1190214

Batch 202270, train_perplexity=172.14816, train_loss=5.1483555

Batch 202280, train_perplexity=195.23189, train_loss=5.274188

Batch 202290, train_perplexity=163.27643, train_loss=5.0954447

Batch 202300, train_perplexity=175.63675, train_loss=5.168418

Batch 202310, train_perplexity=186.85294, train_loss=5.230322

Batch 202320, train_perplexity=188.53587, train_loss=5.2392883

Batch 202330, train_perplexity=165.75468, train_loss=5.110509

Batch 202340, train_perplexity=202.35832, train_loss=5.31004

Batch 202350, train_perplexity=161.7119, train_loss=5.0858164

Batch 202360, train_perplexity=161.06271, train_loss=5.081794

Batch 202370, train_perplexity=192.69469, train_loss=5.261107

Batch 202380, train_perplexity=179.12282, train_loss=5.1880717

Batch 202390, train_perplexity=179.90733, train_loss=5.192442

Batch 202400, train_perplexity=166.4374, train_loss=5.1146193

Batch 202410, train_perplexity=192.12538, train_loss=5.258148

Batch 202420, train_perplexity=192.3594, train_loss=5.2593656

Batch 202430, train_perplexity=181.20143, train_loss=5.1996093

Batch 202440, train_perplexity=182.58403, train_loss=5.2072105

Batch 202450, train_perplexity=172.28711, train_loss=5.1491623

Batch 202460, train_perplexity=159.62912, train_loss=5.072853

Batch 202470, train_perplexity=161.4608, train_loss=5.0842624

Batch 202480, train_perplexity=178.48141, train_loss=5.1844845

Batch 202490, train_perplexity=177.16257, train_loss=5.1770678

Batch 202500, train_perplexity=180.80019, train_loss=5.1973925

Batch 202510, train_perplexity=195.27612, train_loss=5.2744145

Batch 202520, train_perplexity=189.77426, train_loss=5.2458353

Batch 202530, train_perplexity=177.97804, train_loss=5.18166

Batch 202540, train_perplexity=186.40735, train_loss=5.2279344

Batch 202550, train_perplexity=173.21802, train_loss=5.154551

Batch 202560, train_perplexity=196.62617, train_loss=5.2813044

Batch 202570, train_perplexity=164.49686, train_loss=5.1028914

Batch 202580, train_perplexity=213.02107, train_loss=5.361391

Batch 202590, train_perplexity=169.46841, train_loss=5.1326666

Batch 202600, train_perplexity=194.90076, train_loss=5.2724905

Batch 202610, train_perplexity=163.36038, train_loss=5.0959587

Batch 202620, train_perplexity=194.38593, train_loss=5.2698455

Batch 202630, train_perplexity=177.04568, train_loss=5.176408

Batch 202640, train_perplexity=184.51988, train_loss=5.217757

Batch 202650, train_perplexity=168.52985, train_loss=5.127113

Batch 202660, train_perplexity=188.32428, train_loss=5.2381654
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 202670, train_perplexity=169.46793, train_loss=5.1326637

Batch 202680, train_perplexity=199.17365, train_loss=5.294177

Batch 202690, train_perplexity=176.67293, train_loss=5.1743

Batch 202700, train_perplexity=202.20583, train_loss=5.309286

Batch 202710, train_perplexity=179.2827, train_loss=5.188964

Batch 202720, train_perplexity=193.51778, train_loss=5.2653694

Batch 202730, train_perplexity=173.40248, train_loss=5.1556153

Batch 202740, train_perplexity=179.91867, train_loss=5.192505

Batch 202750, train_perplexity=186.91461, train_loss=5.230652

Batch 202760, train_perplexity=176.18886, train_loss=5.1715565

Batch 202770, train_perplexity=181.67207, train_loss=5.2022033

Batch 202780, train_perplexity=200.47675, train_loss=5.3006983

Batch 202790, train_perplexity=179.50742, train_loss=5.1902165

Batch 202800, train_perplexity=175.77164, train_loss=5.1691856

Batch 202810, train_perplexity=177.98186, train_loss=5.1816816

Batch 202820, train_perplexity=160.5107, train_loss=5.0783606

Batch 202830, train_perplexity=160.56612, train_loss=5.078706

Batch 202840, train_perplexity=183.14995, train_loss=5.210305

Batch 202850, train_perplexity=188.91096, train_loss=5.241276

Batch 202860, train_perplexity=182.08481, train_loss=5.2044725

Batch 202870, train_perplexity=194.79298, train_loss=5.2719374

Batch 202880, train_perplexity=173.38652, train_loss=5.1555233

Batch 202890, train_perplexity=161.08913, train_loss=5.081958

Batch 202900, train_perplexity=204.83789, train_loss=5.322219

Batch 202910, train_perplexity=156.6194, train_loss=5.0538187

Batch 202920, train_perplexity=190.37209, train_loss=5.2489805

Batch 202930, train_perplexity=160.33429, train_loss=5.077261

Batch 202940, train_perplexity=198.39459, train_loss=5.290258

Batch 202950, train_perplexity=172.47156, train_loss=5.1502323

Batch 202960, train_perplexity=174.07285, train_loss=5.159474

Batch 202970, train_perplexity=182.43452, train_loss=5.2063913

Batch 202980, train_perplexity=189.6999, train_loss=5.2454433

Batch 202990, train_perplexity=197.13454, train_loss=5.2838864

Batch 203000, train_perplexity=196.22063, train_loss=5.2792397

Batch 203010, train_perplexity=187.06332, train_loss=5.231447

Batch 203020, train_perplexity=151.29128, train_loss=5.019207

Batch 203030, train_perplexity=196.41318, train_loss=5.2802205

Batch 203040, train_perplexity=186.4583, train_loss=5.2282076

Batch 203050, train_perplexity=188.73125, train_loss=5.240324

Batch 203060, train_perplexity=194.49023, train_loss=5.270382

Batch 203070, train_perplexity=181.81273, train_loss=5.202977

Batch 203080, train_perplexity=159.71141, train_loss=5.0733685

Batch 203090, train_perplexity=202.86835, train_loss=5.312557

Batch 203100, train_perplexity=185.40715, train_loss=5.222554

Batch 203110, train_perplexity=168.03354, train_loss=5.1241636

Batch 203120, train_perplexity=181.99019, train_loss=5.203953

Batch 203130, train_perplexity=177.62848, train_loss=5.179694

Batch 203140, train_perplexity=174.05467, train_loss=5.1593695

Batch 203150, train_perplexity=185.1921, train_loss=5.2213936

Batch 203160, train_perplexity=188.54388, train_loss=5.239331

Batch 203170, train_perplexity=162.46742, train_loss=5.0904775

Batch 203180, train_perplexity=172.25803, train_loss=5.1489935

Batch 203190, train_perplexity=196.26639, train_loss=5.279473

Batch 203200, train_perplexity=177.4117, train_loss=5.178473

Batch 203210, train_perplexity=194.61566, train_loss=5.2710266

Batch 203220, train_perplexity=166.16977, train_loss=5.11301

Batch 203230, train_perplexity=184.11357, train_loss=5.215553

Batch 203240, train_perplexity=176.88594, train_loss=5.175505

Batch 203250, train_perplexity=191.0627, train_loss=5.2526016

Batch 203260, train_perplexity=187.32683, train_loss=5.232855

Batch 203270, train_perplexity=190.11827, train_loss=5.2476463

Batch 203280, train_perplexity=172.9997, train_loss=5.15329

Batch 203290, train_perplexity=179.21278, train_loss=5.188574

Batch 203300, train_perplexity=178.35924, train_loss=5.1837997

Batch 203310, train_perplexity=201.48608, train_loss=5.3057203

Batch 203320, train_perplexity=191.64062, train_loss=5.255622

Batch 203330, train_perplexity=186.91255, train_loss=5.230641

Batch 203340, train_perplexity=172.35382, train_loss=5.1495495

Batch 203350, train_perplexity=199.26105, train_loss=5.2946157

Batch 203360, train_perplexity=173.94449, train_loss=5.158736

Batch 203370, train_perplexity=195.00153, train_loss=5.2730074

Batch 203380, train_perplexity=175.09789, train_loss=5.165345

Batch 203390, train_perplexity=171.4852, train_loss=5.144497

Batch 203400, train_perplexity=206.81866, train_loss=5.3318424

Batch 203410, train_perplexity=197.50798, train_loss=5.285779

Batch 203420, train_perplexity=195.5123, train_loss=5.2756233

Batch 203430, train_perplexity=182.47375, train_loss=5.2066064

Batch 203440, train_perplexity=181.6401, train_loss=5.2020273

Batch 203450, train_perplexity=195.2062, train_loss=5.2740564

Batch 203460, train_perplexity=157.58931, train_loss=5.0599923

Batch 203470, train_perplexity=180.26886, train_loss=5.1944494

Batch 203480, train_perplexity=191.10826, train_loss=5.25284

Batch 203490, train_perplexity=157.31969, train_loss=5.05828

Batch 203500, train_perplexity=176.8894, train_loss=5.1755247

Batch 203510, train_perplexity=173.60881, train_loss=5.1568046

Batch 203520, train_perplexity=186.24744, train_loss=5.227076

Batch 203530, train_perplexity=178.06964, train_loss=5.1821747

Batch 203540, train_perplexity=166.10457, train_loss=5.1126175

Batch 203550, train_perplexity=165.27248, train_loss=5.1075954

Batch 203560, train_perplexity=190.94147, train_loss=5.251967

Batch 203570, train_perplexity=177.42574, train_loss=5.178552

Batch 203580, train_perplexity=175.84407, train_loss=5.1695976

Batch 203590, train_perplexity=174.48946, train_loss=5.1618643

Batch 203600, train_perplexity=166.92747, train_loss=5.1175594

Batch 203610, train_perplexity=190.90224, train_loss=5.2517614

Batch 203620, train_perplexity=188.02737, train_loss=5.2365875

Batch 203630, train_perplexity=195.50018, train_loss=5.2755613

Batch 203640, train_perplexity=183.73943, train_loss=5.2135186

Batch 203650, train_perplexity=196.25711, train_loss=5.2794256

Batch 203660, train_perplexity=174.0004, train_loss=5.1590576

Batch 203670, train_perplexity=175.96805, train_loss=5.1703024

Batch 203680, train_perplexity=179.72102, train_loss=5.191406

Batch 203690, train_perplexity=179.00055, train_loss=5.187389

Batch 203700, train_perplexity=181.3769, train_loss=5.2005773

Batch 203710, train_perplexity=192.14444, train_loss=5.2582474

Batch 203720, train_perplexity=172.85555, train_loss=5.1524563

Batch 203730, train_perplexity=163.23145, train_loss=5.095169

Batch 203740, train_perplexity=186.08116, train_loss=5.226183

Batch 203750, train_perplexity=190.1412, train_loss=5.247767

Batch 203760, train_perplexity=169.68901, train_loss=5.1339674

Batch 203770, train_perplexity=182.31277, train_loss=5.205724

Batch 203780, train_perplexity=190.80722, train_loss=5.2512636

Batch 203790, train_perplexity=176.39178, train_loss=5.1727076

Batch 203800, train_perplexity=192.87201, train_loss=5.262027

Batch 203810, train_perplexity=181.33342, train_loss=5.2003374

Batch 203820, train_perplexity=179.21568, train_loss=5.18859

Batch 203830, train_perplexity=197.86223, train_loss=5.287571

Batch 203840, train_perplexity=184.49596, train_loss=5.2176275

Batch 203850, train_perplexity=180.13156, train_loss=5.1936874

Batch 203860, train_perplexity=179.29791, train_loss=5.189049

Batch 203870, train_perplexity=177.44113, train_loss=5.178639

Batch 203880, train_perplexity=179.68263, train_loss=5.191192

Batch 203890, train_perplexity=188.18002, train_loss=5.237399

Batch 203900, train_perplexity=199.47218, train_loss=5.295675

Batch 203910, train_perplexity=164.90138, train_loss=5.1053476

Batch 203920, train_perplexity=181.91713, train_loss=5.2035513

Batch 203930, train_perplexity=166.00273, train_loss=5.1120043

Batch 203940, train_perplexity=180.09067, train_loss=5.1934605

Batch 203950, train_perplexity=176.9818, train_loss=5.176047

Batch 203960, train_perplexity=209.19753, train_loss=5.343279
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 203970, train_perplexity=193.47395, train_loss=5.265143

Batch 203980, train_perplexity=182.76784, train_loss=5.2082167

Batch 203990, train_perplexity=208.07553, train_loss=5.337901

Batch 204000, train_perplexity=175.16002, train_loss=5.1657

Batch 204010, train_perplexity=179.13402, train_loss=5.188134

Batch 204020, train_perplexity=175.91016, train_loss=5.1699734

Batch 204030, train_perplexity=181.26495, train_loss=5.1999598

Batch 204040, train_perplexity=188.73718, train_loss=5.2403555

Batch 204050, train_perplexity=184.6373, train_loss=5.2183933

Batch 204060, train_perplexity=173.30815, train_loss=5.1550713

Batch 204070, train_perplexity=194.60591, train_loss=5.2709765

Batch 204080, train_perplexity=187.40956, train_loss=5.2332964

Batch 204090, train_perplexity=174.74324, train_loss=5.1633177

Batch 204100, train_perplexity=186.62285, train_loss=5.2290897

Batch 204110, train_perplexity=166.06418, train_loss=5.1123743

Batch 204120, train_perplexity=173.94673, train_loss=5.158749

Batch 204130, train_perplexity=165.38757, train_loss=5.1082916

Batch 204140, train_perplexity=186.38309, train_loss=5.227804

Batch 204150, train_perplexity=154.06668, train_loss=5.0373855

Batch 204160, train_perplexity=188.76634, train_loss=5.24051

Batch 204170, train_perplexity=192.21097, train_loss=5.2585936

Batch 204180, train_perplexity=215.16058, train_loss=5.3713846

Batch 204190, train_perplexity=193.02565, train_loss=5.262823

Batch 204200, train_perplexity=183.30826, train_loss=5.2111692

Batch 204210, train_perplexity=154.03964, train_loss=5.03721

Batch 204220, train_perplexity=168.56473, train_loss=5.12732

Batch 204230, train_perplexity=171.23769, train_loss=5.1430526

Batch 204240, train_perplexity=189.55396, train_loss=5.2446737

Batch 204250, train_perplexity=172.92265, train_loss=5.1528444

Batch 204260, train_perplexity=182.43053, train_loss=5.2063694

Batch 204270, train_perplexity=210.16066, train_loss=5.3478723

Batch 204280, train_perplexity=191.89209, train_loss=5.256933

Batch 204290, train_perplexity=193.0344, train_loss=5.2628684

Batch 204300, train_perplexity=185.37921, train_loss=5.2224035

Batch 204310, train_perplexity=191.8286, train_loss=5.2566023

Batch 204320, train_perplexity=175.23988, train_loss=5.166156

Batch 204330, train_perplexity=195.85661, train_loss=5.277383

Batch 204340, train_perplexity=154.56918, train_loss=5.040642

Batch 204350, train_perplexity=183.83066, train_loss=5.214015

Batch 204360, train_perplexity=169.97755, train_loss=5.1356664

Batch 204370, train_perplexity=175.88835, train_loss=5.1698494

Batch 204380, train_perplexity=157.30978, train_loss=5.058217

Batch 204390, train_perplexity=168.18184, train_loss=5.125046

Batch 204400, train_perplexity=164.03949, train_loss=5.100107

Batch 204410, train_perplexity=183.77773, train_loss=5.213727

Batch 204420, train_perplexity=192.77583, train_loss=5.261528

Batch 204430, train_perplexity=170.7225, train_loss=5.1400394

Batch 204440, train_perplexity=199.32205, train_loss=5.294922

Batch 204450, train_perplexity=165.05495, train_loss=5.1062784

Batch 204460, train_perplexity=172.52733, train_loss=5.1505556

Batch 204470, train_perplexity=155.85199, train_loss=5.048907

Batch 204480, train_perplexity=172.93321, train_loss=5.1529055

Batch 204490, train_perplexity=175.57243, train_loss=5.1680517

Batch 204500, train_perplexity=166.19995, train_loss=5.1131916

Batch 204510, train_perplexity=171.94397, train_loss=5.1471686

Batch 204520, train_perplexity=192.6808, train_loss=5.261035

Batch 204530, train_perplexity=164.90594, train_loss=5.1053753

Batch 204540, train_perplexity=181.39749, train_loss=5.2006907

Batch 204550, train_perplexity=163.47595, train_loss=5.096666

Batch 204560, train_perplexity=191.8449, train_loss=5.256687

Batch 204570, train_perplexity=171.09877, train_loss=5.142241

Batch 204580, train_perplexity=167.85344, train_loss=5.123091

Batch 204590, train_perplexity=187.95082, train_loss=5.2361803

Batch 204600, train_perplexity=194.13777, train_loss=5.268568

Batch 204610, train_perplexity=173.53821, train_loss=5.156398

Batch 204620, train_perplexity=179.33469, train_loss=5.189254

Batch 204630, train_perplexity=177.78983, train_loss=5.180602

Batch 204640, train_perplexity=188.93213, train_loss=5.241388

Batch 204650, train_perplexity=172.82489, train_loss=5.152279

Batch 204660, train_perplexity=186.19139, train_loss=5.226775

Batch 204670, train_perplexity=196.69107, train_loss=5.2816343

Batch 204680, train_perplexity=187.66388, train_loss=5.2346525

Batch 204690, train_perplexity=165.70332, train_loss=5.110199

Batch 204700, train_perplexity=182.7375, train_loss=5.2080507

Batch 204710, train_perplexity=176.72989, train_loss=5.1746225

Batch 204720, train_perplexity=194.34283, train_loss=5.2696238

Batch 204730, train_perplexity=181.22847, train_loss=5.1997585

Batch 204740, train_perplexity=196.99059, train_loss=5.283156

Batch 204750, train_perplexity=183.93701, train_loss=5.2145934

Batch 204760, train_perplexity=179.29253, train_loss=5.1890187

Batch 204770, train_perplexity=185.81268, train_loss=5.224739

Batch 204780, train_perplexity=185.56989, train_loss=5.2234316

Batch 204790, train_perplexity=197.42249, train_loss=5.285346

Batch 204800, train_perplexity=191.18199, train_loss=5.253226

Batch 204810, train_perplexity=178.52551, train_loss=5.1847315

Batch 204820, train_perplexity=180.55965, train_loss=5.196061

Batch 204830, train_perplexity=185.48232, train_loss=5.2229595

Batch 204840, train_perplexity=183.35915, train_loss=5.211447

Batch 204850, train_perplexity=179.24373, train_loss=5.1887465

Batch 204860, train_perplexity=162.9075, train_loss=5.0931826

Batch 204870, train_perplexity=174.1925, train_loss=5.160161

Batch 204880, train_perplexity=182.16905, train_loss=5.204935

Batch 204890, train_perplexity=178.83026, train_loss=5.186437

Batch 204900, train_perplexity=170.5257, train_loss=5.138886

Batch 204910, train_perplexity=174.2033, train_loss=5.160223

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Batch 204920, train_perplexity=183.62022, train_loss=5.2128696

Batch 204930, train_perplexity=173.62181, train_loss=5.1568794

Batch 204940, train_perplexity=173.78929, train_loss=5.1578436

Batch 204950, train_perplexity=195.69493, train_loss=5.276557

Batch 204960, train_perplexity=190.115, train_loss=5.247629

Batch 204970, train_perplexity=192.66924, train_loss=5.260975

Batch 204980, train_perplexity=161.3042, train_loss=5.083292

Batch 204990, train_perplexity=170.5243, train_loss=5.138878

Batch 205000, train_perplexity=155.26274, train_loss=5.045119

Batch 205010, train_perplexity=185.04732, train_loss=5.2206116

Batch 205020, train_perplexity=178.88707, train_loss=5.1867547

Batch 205030, train_perplexity=188.9989, train_loss=5.241741

Batch 205040, train_perplexity=188.52293, train_loss=5.2392197

Batch 205050, train_perplexity=172.68913, train_loss=5.151493

Batch 205060, train_perplexity=197.59427, train_loss=5.286216

Batch 205070, train_perplexity=195.93564, train_loss=5.2777863

Batch 205080, train_perplexity=163.04008, train_loss=5.093996

Batch 205090, train_perplexity=186.40744, train_loss=5.227935

Batch 205100, train_perplexity=196.98654, train_loss=5.2831354

Batch 205110, train_perplexity=192.49475, train_loss=5.260069

Batch 205120, train_perplexity=194.01265, train_loss=5.2679234

Batch 205130, train_perplexity=191.78937, train_loss=5.2563977

Batch 205140, train_perplexity=192.22792, train_loss=5.258682

Batch 205150, train_perplexity=189.86894, train_loss=5.246334

Batch 205160, train_perplexity=176.89987, train_loss=5.175584

Batch 205170, train_perplexity=184.35023, train_loss=5.2168374

Batch 205180, train_perplexity=192.51843, train_loss=5.260192

Batch 205190, train_perplexity=169.83261, train_loss=5.1348133

Batch 205200, train_perplexity=176.73546, train_loss=5.174654
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 205210, train_perplexity=188.28755, train_loss=5.2379704

Batch 205220, train_perplexity=179.99315, train_loss=5.192919

Batch 205230, train_perplexity=176.62408, train_loss=5.1740236

Batch 205240, train_perplexity=172.5479, train_loss=5.150675

Batch 205250, train_perplexity=183.4165, train_loss=5.2117596

Batch 205260, train_perplexity=169.97244, train_loss=5.1356363

Batch 205270, train_perplexity=182.22456, train_loss=5.20524

Batch 205280, train_perplexity=189.45221, train_loss=5.244137

Batch 205290, train_perplexity=169.86841, train_loss=5.135024

Batch 205300, train_perplexity=162.9566, train_loss=5.093484

Batch 205310, train_perplexity=175.7604, train_loss=5.1691217

Batch 205320, train_perplexity=190.29958, train_loss=5.2485995

Batch 205330, train_perplexity=168.25195, train_loss=5.1254625

Batch 205340, train_perplexity=191.16357, train_loss=5.2531295

Batch 205350, train_perplexity=180.38718, train_loss=5.1951056

Batch 205360, train_perplexity=181.99713, train_loss=5.203991

Batch 205370, train_perplexity=165.22803, train_loss=5.1073265

Batch 205380, train_perplexity=200.70737, train_loss=5.301848

Batch 205390, train_perplexity=167.56084, train_loss=5.1213465

Batch 205400, train_perplexity=201.40636, train_loss=5.3053246

Batch 205410, train_perplexity=178.44482, train_loss=5.1842794

Batch 205420, train_perplexity=173.93338, train_loss=5.1586723

Batch 205430, train_perplexity=206.97287, train_loss=5.3325877

Batch 205440, train_perplexity=168.3884, train_loss=5.126273

Batch 205450, train_perplexity=183.15851, train_loss=5.210352

Batch 205460, train_perplexity=177.02805, train_loss=5.176308

Batch 205470, train_perplexity=181.40121, train_loss=5.2007113

Batch 205480, train_perplexity=167.06229, train_loss=5.1183667

Batch 205490, train_perplexity=178.26945, train_loss=5.183296

Batch 205500, train_perplexity=178.13681, train_loss=5.182552

Batch 205510, train_perplexity=193.91563, train_loss=5.267423

Batch 205520, train_perplexity=181.68793, train_loss=5.2022905

Batch 205530, train_perplexity=185.38168, train_loss=5.222417

Batch 205540, train_perplexity=178.42653, train_loss=5.184177

Batch 205550, train_perplexity=180.42967, train_loss=5.195341

Batch 205560, train_perplexity=181.73438, train_loss=5.202546

Batch 205570, train_perplexity=185.24966, train_loss=5.2217045

Batch 205580, train_perplexity=185.29967, train_loss=5.2219744

Batch 205590, train_perplexity=179.30356, train_loss=5.18908

Batch 205600, train_perplexity=176.13712, train_loss=5.1712627

Batch 205610, train_perplexity=184.0947, train_loss=5.2154503

Batch 205620, train_perplexity=177.3212, train_loss=5.177963

Batch 205630, train_perplexity=176.5111, train_loss=5.1733837

Batch 205640, train_perplexity=182.84558, train_loss=5.208642

Batch 205650, train_perplexity=163.9477, train_loss=5.0995474

Batch 205660, train_perplexity=171.09673, train_loss=5.142229

Batch 205670, train_perplexity=176.96313, train_loss=5.1759415

Batch 205680, train_perplexity=170.84604, train_loss=5.140763

Batch 205690, train_perplexity=185.0737, train_loss=5.220754

Batch 205700, train_perplexity=175.29204, train_loss=5.1664534

Batch 205710, train_perplexity=194.4422, train_loss=5.270135

Batch 205720, train_perplexity=175.47552, train_loss=5.1674995

Batch 205730, train_perplexity=161.71329, train_loss=5.085825

Batch 205740, train_perplexity=176.55452, train_loss=5.1736298

Batch 205750, train_perplexity=189.68959, train_loss=5.245389

Batch 205760, train_perplexity=174.83542, train_loss=5.163845

Batch 205770, train_perplexity=177.30083, train_loss=5.177848

Batch 205780, train_perplexity=168.21039, train_loss=5.1252155

Batch 205790, train_perplexity=180.76605, train_loss=5.1972036

Batch 205800, train_perplexity=178.8631, train_loss=5.1866207

Batch 205810, train_perplexity=180.30377, train_loss=5.194643

Batch 205820, train_perplexity=177.71269, train_loss=5.180168

Batch 205830, train_perplexity=184.44449, train_loss=5.2173486

Batch 205840, train_perplexity=175.14348, train_loss=5.1656055

Batch 205850, train_perplexity=184.97154, train_loss=5.220202

Batch 205860, train_perplexity=180.62148, train_loss=5.1964035

Batch 205870, train_perplexity=185.97916, train_loss=5.2256346

Batch 205880, train_perplexity=184.1112, train_loss=5.21554

Batch 205890, train_perplexity=194.43181, train_loss=5.2700815

Batch 205900, train_perplexity=166.12761, train_loss=5.1127563

Batch 205910, train_perplexity=184.23943, train_loss=5.216236

Batch 205920, train_perplexity=186.21999, train_loss=5.2269287

Batch 205930, train_perplexity=164.00108, train_loss=5.099873

Batch 205940, train_perplexity=196.63312, train_loss=5.2813396

Batch 205950, train_perplexity=200.58002, train_loss=5.3012133

Batch 205960, train_perplexity=179.82227, train_loss=5.191969

Batch 205970, train_perplexity=205.5892, train_loss=5.32588

Batch 205980, train_perplexity=194.74394, train_loss=5.2716856

Batch 205990, train_perplexity=185.02844, train_loss=5.2205095

Batch 206000, train_perplexity=187.34273, train_loss=5.2329397

Batch 206010, train_perplexity=195.87717, train_loss=5.2774878

Batch 206020, train_perplexity=176.52792, train_loss=5.173479

Batch 206030, train_perplexity=187.60555, train_loss=5.2343416

Batch 206040, train_perplexity=183.48807, train_loss=5.2121496

Batch 206050, train_perplexity=172.16605, train_loss=5.1484594

Batch 206060, train_perplexity=170.26634, train_loss=5.137364

Batch 206070, train_perplexity=170.99527, train_loss=5.141636

Batch 206080, train_perplexity=188.2635, train_loss=5.2378426

Batch 206090, train_perplexity=168.0434, train_loss=5.1242223

Batch 206100, train_perplexity=182.27905, train_loss=5.2055387

Batch 206110, train_perplexity=172.4112, train_loss=5.1498823

Batch 206120, train_perplexity=167.17775, train_loss=5.1190577

Batch 206130, train_perplexity=197.26872, train_loss=5.284567

Batch 206140, train_perplexity=162.33305, train_loss=5.08965

Batch 206150, train_perplexity=174.0009, train_loss=5.1590605

Batch 206160, train_perplexity=177.96913, train_loss=5.18161

Batch 206170, train_perplexity=169.78613, train_loss=5.1345396

Batch 206180, train_perplexity=192.95195, train_loss=5.262441

Batch 206190, train_perplexity=196.42696, train_loss=5.2802906

Batch 206200, train_perplexity=211.21542, train_loss=5.3528786

Batch 206210, train_perplexity=197.27173, train_loss=5.284582

Batch 206220, train_perplexity=155.61577, train_loss=5.04739

Batch 206230, train_perplexity=202.68907, train_loss=5.311673

Batch 206240, train_perplexity=181.85686, train_loss=5.20322

Batch 206250, train_perplexity=173.33229, train_loss=5.1552105

Batch 206260, train_perplexity=196.96889, train_loss=5.283046

Batch 206270, train_perplexity=172.51086, train_loss=5.1504602

Batch 206280, train_perplexity=193.64148, train_loss=5.2660084

Batch 206290, train_perplexity=188.9688, train_loss=5.241582

Batch 206300, train_perplexity=185.425, train_loss=5.2226505

Batch 206310, train_perplexity=196.4558, train_loss=5.2804375

Batch 206320, train_perplexity=177.97559, train_loss=5.1816463

Batch 206330, train_perplexity=187.55116, train_loss=5.2340517

Batch 206340, train_perplexity=180.79605, train_loss=5.1973696

Batch 206350, train_perplexity=177.9194, train_loss=5.1813307

Batch 206360, train_perplexity=186.48932, train_loss=5.228374

Batch 206370, train_perplexity=167.88954, train_loss=5.1233063

Batch 206380, train_perplexity=177.92636, train_loss=5.18137

Batch 206390, train_perplexity=169.7659, train_loss=5.1344204

Batch 206400, train_perplexity=180.32802, train_loss=5.1947775

Batch 206410, train_perplexity=195.78117, train_loss=5.2769976

Batch 206420, train_perplexity=168.55885, train_loss=5.127285

Batch 206430, train_perplexity=198.33188, train_loss=5.289942

Batch 206440, train_perplexity=179.31108, train_loss=5.189122

Batch 206450, train_perplexity=195.41211, train_loss=5.2751107

Batch 206460, train_perplexity=181.82799, train_loss=5.203061

Batch 206470, train_perplexity=179.89653, train_loss=5.192382

Batch 206480, train_perplexity=209.74129, train_loss=5.345875

Batch 206490, train_perplexity=164.0553, train_loss=5.1002035

Batch 206500, train_perplexity=180.35046, train_loss=5.194902
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 206510, train_perplexity=183.53732, train_loss=5.212418

Batch 206520, train_perplexity=173.02428, train_loss=5.153432

Batch 206530, train_perplexity=178.1369, train_loss=5.1825523

Batch 206540, train_perplexity=166.62115, train_loss=5.1157227

Batch 206550, train_perplexity=189.33372, train_loss=5.243511

Batch 206560, train_perplexity=192.30026, train_loss=5.259058

Batch 206570, train_perplexity=179.70251, train_loss=5.191303

Batch 206580, train_perplexity=178.1099, train_loss=5.1824007

Batch 206590, train_perplexity=186.69049, train_loss=5.229452

Batch 206600, train_perplexity=174.39038, train_loss=5.1612964

Batch 206610, train_perplexity=173.66934, train_loss=5.157153

Batch 206620, train_perplexity=202.02753, train_loss=5.308404

Batch 206630, train_perplexity=181.88972, train_loss=5.2034006

Batch 206640, train_perplexity=172.20407, train_loss=5.14868

Batch 206650, train_perplexity=186.46727, train_loss=5.2282557

Batch 206660, train_perplexity=182.02386, train_loss=5.204138

Batch 206670, train_perplexity=173.5968, train_loss=5.1567354

Batch 206680, train_perplexity=164.36043, train_loss=5.1020617

Batch 206690, train_perplexity=195.67273, train_loss=5.2764435

Batch 206700, train_perplexity=193.04793, train_loss=5.2629385

Batch 206710, train_perplexity=172.17918, train_loss=5.1485357

Batch 206720, train_perplexity=182.42844, train_loss=5.206358

Batch 206730, train_perplexity=185.84308, train_loss=5.2249026

Batch 206740, train_perplexity=175.05331, train_loss=5.1650906

Batch 206750, train_perplexity=188.33856, train_loss=5.238241

Batch 206760, train_perplexity=181.87949, train_loss=5.2033443

Batch 206770, train_perplexity=170.86209, train_loss=5.1408567

Batch 206780, train_perplexity=165.83675, train_loss=5.111004

Batch 206790, train_perplexity=181.34535, train_loss=5.200403

Batch 206800, train_perplexity=177.35553, train_loss=5.1781564

Batch 206810, train_perplexity=181.73195, train_loss=5.202533

Batch 206820, train_perplexity=203.25507, train_loss=5.3144617

Batch 206830, train_perplexity=192.03984, train_loss=5.257703

Batch 206840, train_perplexity=188.41815, train_loss=5.2386637

Batch 206850, train_perplexity=187.3304, train_loss=5.232874

Batch 206860, train_perplexity=166.93433, train_loss=5.1176004

Batch 206870, train_perplexity=206.86572, train_loss=5.33207

Batch 206880, train_perplexity=180.09618, train_loss=5.193491

Batch 206890, train_perplexity=184.72993, train_loss=5.218895

Batch 206900, train_perplexity=183.40077, train_loss=5.2116737

Batch 206910, train_perplexity=184.33907, train_loss=5.216777

Batch 206920, train_perplexity=164.81256, train_loss=5.104809

Batch 206930, train_perplexity=180.68729, train_loss=5.196768

Batch 206940, train_perplexity=191.23697, train_loss=5.2535133

Batch 206950, train_perplexity=186.80055, train_loss=5.2300415

Batch 206960, train_perplexity=166.74847, train_loss=5.1164865

Batch 206970, train_perplexity=184.87604, train_loss=5.2196856

Batch 206980, train_perplexity=190.54265, train_loss=5.249876

Batch 206990, train_perplexity=173.34634, train_loss=5.1552916

Batch 207000, train_perplexity=174.37175, train_loss=5.1611896

Batch 207010, train_perplexity=163.78969, train_loss=5.098583

Batch 207020, train_perplexity=162.673, train_loss=5.091742

Batch 207030, train_perplexity=198.25491, train_loss=5.2895536

Batch 207040, train_perplexity=171.96758, train_loss=5.147306

Batch 207050, train_perplexity=192.3784, train_loss=5.2594643

Batch 207060, train_perplexity=171.54465, train_loss=5.1448436

Batch 207070, train_perplexity=178.0321, train_loss=5.181964

Batch 207080, train_perplexity=174.10016, train_loss=5.159631

Batch 207090, train_perplexity=172.31314, train_loss=5.1493134

Batch 207100, train_perplexity=187.43102, train_loss=5.233411

Batch 207110, train_perplexity=183.04378, train_loss=5.2097254

Batch 207120, train_perplexity=174.09618, train_loss=5.159608

Batch 207130, train_perplexity=176.91707, train_loss=5.175681

Batch 207140, train_perplexity=156.0265, train_loss=5.050026

Batch 207150, train_perplexity=175.98633, train_loss=5.1704063

Batch 207160, train_perplexity=184.82474, train_loss=5.219408

Batch 207170, train_perplexity=193.41206, train_loss=5.264823

Batch 207180, train_perplexity=158.49295, train_loss=5.06571

Batch 207190, train_perplexity=202.381, train_loss=5.310152

Batch 207200, train_perplexity=176.44832, train_loss=5.173028

Batch 207210, train_perplexity=175.9615, train_loss=5.170265

Batch 207220, train_perplexity=182.28131, train_loss=5.205551

Batch 207230, train_perplexity=177.84799, train_loss=5.180929

Batch 207240, train_perplexity=187.9959, train_loss=5.23642

Batch 207250, train_perplexity=172.15752, train_loss=5.14841

Batch 207260, train_perplexity=176.23036, train_loss=5.171792

Batch 207270, train_perplexity=181.08594, train_loss=5.1989717

Batch 207280, train_perplexity=178.99646, train_loss=5.187366

Batch 207290, train_perplexity=181.9082, train_loss=5.203502

Batch 207300, train_perplexity=157.05211, train_loss=5.0565777

Batch 207310, train_perplexity=174.74957, train_loss=5.163354

Batch 207320, train_perplexity=182.65178, train_loss=5.2075815

Batch 207330, train_perplexity=182.78735, train_loss=5.2083235

Batch 207340, train_perplexity=186.5064, train_loss=5.2284656

Batch 207350, train_perplexity=193.24649, train_loss=5.2639666

Batch 207360, train_perplexity=169.91402, train_loss=5.1352925

Batch 207370, train_perplexity=183.73444, train_loss=5.2134914

Batch 207380, train_perplexity=202.97981, train_loss=5.3131065

Batch 207390, train_perplexity=188.92807, train_loss=5.2413664

Batch 207400, train_perplexity=154.1195, train_loss=5.0377283

Batch 207410, train_perplexity=161.60953, train_loss=5.085183

Batch 207420, train_perplexity=162.28516, train_loss=5.089355

Batch 207430, train_perplexity=185.46922, train_loss=5.222889

Batch 207440, train_perplexity=176.25877, train_loss=5.171953

Batch 207450, train_perplexity=178.46422, train_loss=5.184388

Batch 207460, train_perplexity=189.70189, train_loss=5.245454

Batch 207470, train_perplexity=172.26024, train_loss=5.1490064

Batch 207480, train_perplexity=174.52922, train_loss=5.162092

Batch 207490, train_perplexity=200.02205, train_loss=5.2984276

Batch 207500, train_perplexity=181.43227, train_loss=5.2008824

Batch 207510, train_perplexity=188.79353, train_loss=5.240654

Batch 207520, train_perplexity=174.50592, train_loss=5.1619587

Batch 207530, train_perplexity=173.81151, train_loss=5.1579714

Batch 207540, train_perplexity=181.63525, train_loss=5.2020006

Batch 207550, train_perplexity=184.98256, train_loss=5.2202616

Batch 207560, train_perplexity=179.32014, train_loss=5.1891727

Batch 207570, train_perplexity=203.85628, train_loss=5.317415

Batch 207580, train_perplexity=179.68974, train_loss=5.1912317

Batch 207590, train_perplexity=194.9504, train_loss=5.272745

Batch 207600, train_perplexity=186.46327, train_loss=5.2282343

Batch 207610, train_perplexity=168.1456, train_loss=5.1248302

Batch 207620, train_perplexity=180.06354, train_loss=5.19331

Batch 207630, train_perplexity=163.13432, train_loss=5.094574

Batch 207640, train_perplexity=192.29585, train_loss=5.259035

Batch 207650, train_perplexity=202.32677, train_loss=5.309884

Batch 207660, train_perplexity=187.45773, train_loss=5.2335534

Batch 207670, train_perplexity=198.29425, train_loss=5.289752

Batch 207680, train_perplexity=169.98492, train_loss=5.13571

Batch 207690, train_perplexity=178.07388, train_loss=5.1821985

Batch 207700, train_perplexity=182.65352, train_loss=5.207591

Batch 207710, train_perplexity=165.79105, train_loss=5.1107283

Batch 207720, train_perplexity=167.93343, train_loss=5.1235676

Batch 207730, train_perplexity=171.98062, train_loss=5.147382

Batch 207740, train_perplexity=190.63425, train_loss=5.2503567

Batch 207750, train_perplexity=178.77203, train_loss=5.1861115

Batch 207760, train_perplexity=173.1007, train_loss=5.1538734

Batch 207770, train_perplexity=165.09651, train_loss=5.10653

Batch 207780, train_perplexity=160.35188, train_loss=5.0773706

Batch 207790, train_perplexity=189.88072, train_loss=5.246396

Batch 207800, train_perplexity=157.2138, train_loss=5.0576067
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 207810, train_perplexity=185.76732, train_loss=5.224495

Batch 207820, train_perplexity=186.7489, train_loss=5.229765

Batch 207830, train_perplexity=186.04514, train_loss=5.2259893

Batch 207840, train_perplexity=169.41785, train_loss=5.132368

Batch 207850, train_perplexity=179.95685, train_loss=5.192717

Batch 207860, train_perplexity=193.8218, train_loss=5.266939

Batch 207870, train_perplexity=157.02785, train_loss=5.056423

Batch 207880, train_perplexity=184.99474, train_loss=5.2203274

Batch 207890, train_perplexity=186.77374, train_loss=5.229898

Batch 207900, train_perplexity=168.76973, train_loss=5.1285353

Batch 207910, train_perplexity=171.86159, train_loss=5.1466894

Batch 207920, train_perplexity=183.62811, train_loss=5.2129126

Batch 207930, train_perplexity=185.57095, train_loss=5.2234373

Batch 207940, train_perplexity=180.24205, train_loss=5.1943007

Batch 207950, train_perplexity=170.73683, train_loss=5.1401234

Batch 207960, train_perplexity=181.07118, train_loss=5.19889

Batch 207970, train_perplexity=174.53447, train_loss=5.1621222

Batch 207980, train_perplexity=173.6397, train_loss=5.1569824

Batch 207990, train_perplexity=171.8216, train_loss=5.1464567

Batch 208000, train_perplexity=203.33844, train_loss=5.314872

Batch 208010, train_perplexity=190.12697, train_loss=5.247692

Batch 208020, train_perplexity=181.33083, train_loss=5.200323

Batch 208030, train_perplexity=169.61555, train_loss=5.1335344

Batch 208040, train_perplexity=169.2085, train_loss=5.1311316

Batch 208050, train_perplexity=194.43979, train_loss=5.2701225

Batch 208060, train_perplexity=182.15706, train_loss=5.2048693

Batch 208070, train_perplexity=165.43648, train_loss=5.1085873

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00086-of-00100
Loaded 305744 sentences.
Finished loading
Batch 208080, train_perplexity=185.27528, train_loss=5.221843

Batch 208090, train_perplexity=184.72482, train_loss=5.2188673

Batch 208100, train_perplexity=174.32845, train_loss=5.160941

Batch 208110, train_perplexity=179.36394, train_loss=5.189417

Batch 208120, train_perplexity=186.56555, train_loss=5.2287827

Batch 208130, train_perplexity=196.69202, train_loss=5.281639

Batch 208140, train_perplexity=171.53574, train_loss=5.1447916

Batch 208150, train_perplexity=187.39436, train_loss=5.2332153

Batch 208160, train_perplexity=197.56026, train_loss=5.2860436

Batch 208170, train_perplexity=189.25212, train_loss=5.24308

Batch 208180, train_perplexity=185.93047, train_loss=5.225373

Batch 208190, train_perplexity=177.2038, train_loss=5.1773005

Batch 208200, train_perplexity=165.43678, train_loss=5.108589

Batch 208210, train_perplexity=185.7473, train_loss=5.224387

Batch 208220, train_perplexity=159.67853, train_loss=5.0731626

Batch 208230, train_perplexity=184.97409, train_loss=5.220216

Batch 208240, train_perplexity=189.80983, train_loss=5.2460227

Batch 208250, train_perplexity=196.07004, train_loss=5.278472

Batch 208260, train_perplexity=184.16573, train_loss=5.215836

Batch 208270, train_perplexity=186.641, train_loss=5.229187

Batch 208280, train_perplexity=179.39233, train_loss=5.189575

Batch 208290, train_perplexity=184.13385, train_loss=5.215663

Batch 208300, train_perplexity=206.42398, train_loss=5.329932

Batch 208310, train_perplexity=159.55606, train_loss=5.0723953

Batch 208320, train_perplexity=185.38063, train_loss=5.222411

Batch 208330, train_perplexity=153.52773, train_loss=5.033881

Batch 208340, train_perplexity=177.86775, train_loss=5.1810403

Batch 208350, train_perplexity=169.9791, train_loss=5.1356754

Batch 208360, train_perplexity=168.81529, train_loss=5.128805

Batch 208370, train_perplexity=184.46921, train_loss=5.2174826

Batch 208380, train_perplexity=177.56903, train_loss=5.1793594

Batch 208390, train_perplexity=169.47707, train_loss=5.1327176

Batch 208400, train_perplexity=149.88232, train_loss=5.0098505

Batch 208410, train_perplexity=168.74905, train_loss=5.1284127

Batch 208420, train_perplexity=178.23715, train_loss=5.183115

Batch 208430, train_perplexity=175.05247, train_loss=5.165086

Batch 208440, train_perplexity=186.6394, train_loss=5.2291784

Batch 208450, train_perplexity=179.65874, train_loss=5.191059

Batch 208460, train_perplexity=183.9186, train_loss=5.2144933

Batch 208470, train_perplexity=159.97125, train_loss=5.074994

Batch 208480, train_perplexity=184.03264, train_loss=5.215113

Batch 208490, train_perplexity=185.71082, train_loss=5.2241907

Batch 208500, train_perplexity=179.63525, train_loss=5.1909285

Batch 208510, train_perplexity=176.26826, train_loss=5.172007

Batch 208520, train_perplexity=176.2571, train_loss=5.1719437

Batch 208530, train_perplexity=174.6238, train_loss=5.162634

Batch 208540, train_perplexity=182.02264, train_loss=5.204131

Batch 208550, train_perplexity=177.18048, train_loss=5.177169

Batch 208560, train_perplexity=174.34723, train_loss=5.161049

Batch 208570, train_perplexity=179.81935, train_loss=5.1919527

Batch 208580, train_perplexity=167.52264, train_loss=5.1211185

Batch 208590, train_perplexity=186.26385, train_loss=5.2271643

Batch 208600, train_perplexity=169.88266, train_loss=5.135108

Batch 208610, train_perplexity=171.46149, train_loss=5.1443586

Batch 208620, train_perplexity=190.57227, train_loss=5.2500315

Batch 208630, train_perplexity=164.42055, train_loss=5.1024275

Batch 208640, train_perplexity=176.10008, train_loss=5.1710525

Batch 208650, train_perplexity=170.07962, train_loss=5.1362667

Batch 208660, train_perplexity=170.45813, train_loss=5.1384897

Batch 208670, train_perplexity=181.78516, train_loss=5.2028255

Batch 208680, train_perplexity=204.53981, train_loss=5.3207626

Batch 208690, train_perplexity=189.30736, train_loss=5.243372

Batch 208700, train_perplexity=187.2577, train_loss=5.232486

Batch 208710, train_perplexity=177.82137, train_loss=5.1807795

Batch 208720, train_perplexity=187.48116, train_loss=5.2336783

Batch 208730, train_perplexity=174.35388, train_loss=5.161087

Batch 208740, train_perplexity=184.41292, train_loss=5.2171774

Batch 208750, train_perplexity=164.51724, train_loss=5.1030154

Batch 208760, train_perplexity=191.76605, train_loss=5.256276

Batch 208770, train_perplexity=201.4465, train_loss=5.305524

Batch 208780, train_perplexity=174.35089, train_loss=5.16107

Batch 208790, train_perplexity=183.32234, train_loss=5.211246

Batch 208800, train_perplexity=191.56151, train_loss=5.255209

Batch 208810, train_perplexity=175.7905, train_loss=5.169293

Batch 208820, train_perplexity=173.05159, train_loss=5.1535897

Batch 208830, train_perplexity=175.00473, train_loss=5.164813

Batch 208840, train_perplexity=182.97554, train_loss=5.2093525

Batch 208850, train_perplexity=188.47287, train_loss=5.238954

Batch 208860, train_perplexity=172.09021, train_loss=5.148019

Batch 208870, train_perplexity=190.20567, train_loss=5.248106

Batch 208880, train_perplexity=164.06288, train_loss=5.10025

Batch 208890, train_perplexity=193.11311, train_loss=5.263276

Batch 208900, train_perplexity=182.62427, train_loss=5.207431

Batch 208910, train_perplexity=181.40848, train_loss=5.2007513

Batch 208920, train_perplexity=169.72478, train_loss=5.134178

Batch 208930, train_perplexity=185.77477, train_loss=5.224535

Batch 208940, train_perplexity=198.19272, train_loss=5.28924

Batch 208950, train_perplexity=184.57585, train_loss=5.2180605

Batch 208960, train_perplexity=188.21692, train_loss=5.237595

Batch 208970, train_perplexity=184.47598, train_loss=5.2175193

Batch 208980, train_perplexity=179.52454, train_loss=5.190312

Batch 208990, train_perplexity=207.64001, train_loss=5.335806

Batch 209000, train_perplexity=169.15831, train_loss=5.130835

Batch 209010, train_perplexity=179.60425, train_loss=5.190756

Batch 209020, train_perplexity=200.09912, train_loss=5.298813

Batch 209030, train_perplexity=184.86952, train_loss=5.2196503

Batch 209040, train_perplexity=165.28966, train_loss=5.1076994
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 209050, train_perplexity=174.57742, train_loss=5.1623683

Batch 209060, train_perplexity=178.11566, train_loss=5.182433

Batch 209070, train_perplexity=186.96149, train_loss=5.2309027

Batch 209080, train_perplexity=157.98927, train_loss=5.062527

Batch 209090, train_perplexity=178.28407, train_loss=5.183378

Batch 209100, train_perplexity=173.12413, train_loss=5.154009

Batch 209110, train_perplexity=181.88226, train_loss=5.2033596

Batch 209120, train_perplexity=167.75847, train_loss=5.122525

Batch 209130, train_perplexity=164.07642, train_loss=5.1003323

Batch 209140, train_perplexity=172.7738, train_loss=5.1519833

Batch 209150, train_perplexity=195.25238, train_loss=5.274293

Batch 209160, train_perplexity=169.91928, train_loss=5.1353235

Batch 209170, train_perplexity=179.78384, train_loss=5.1917553

Batch 209180, train_perplexity=191.18172, train_loss=5.2532244

Batch 209190, train_perplexity=193.59476, train_loss=5.265767

Batch 209200, train_perplexity=180.43466, train_loss=5.195369

Batch 209210, train_perplexity=160.1978, train_loss=5.0764093

Batch 209220, train_perplexity=170.55228, train_loss=5.139042

Batch 209230, train_perplexity=167.04851, train_loss=5.118284

Batch 209240, train_perplexity=184.87666, train_loss=5.219689

Batch 209250, train_perplexity=168.16629, train_loss=5.1249533

Batch 209260, train_perplexity=197.60878, train_loss=5.286289

Batch 209270, train_perplexity=170.42676, train_loss=5.1383057

Batch 209280, train_perplexity=160.81407, train_loss=5.080249

Batch 209290, train_perplexity=183.81856, train_loss=5.213949

Batch 209300, train_perplexity=171.59021, train_loss=5.145109

Batch 209310, train_perplexity=182.06415, train_loss=5.204359

Batch 209320, train_perplexity=175.15501, train_loss=5.1656713

Batch 209330, train_perplexity=171.447, train_loss=5.144274

Batch 209340, train_perplexity=176.07664, train_loss=5.1709194

Batch 209350, train_perplexity=183.56902, train_loss=5.2125907

Batch 209360, train_perplexity=199.71898, train_loss=5.2969112

Batch 209370, train_perplexity=194.32744, train_loss=5.2695446

Batch 209380, train_perplexity=187.09598, train_loss=5.2316217

Batch 209390, train_perplexity=174.0311, train_loss=5.159234

Batch 209400, train_perplexity=156.71204, train_loss=5.05441

Batch 209410, train_perplexity=194.38203, train_loss=5.2698255

Batch 209420, train_perplexity=168.89919, train_loss=5.129302

Batch 209430, train_perplexity=161.69348, train_loss=5.0857024

Batch 209440, train_perplexity=166.33458, train_loss=5.1140013

Batch 209450, train_perplexity=176.98863, train_loss=5.1760855

Batch 209460, train_perplexity=168.96153, train_loss=5.129671

Batch 209470, train_perplexity=177.88768, train_loss=5.1811523

Batch 209480, train_perplexity=190.03134, train_loss=5.247189

Batch 209490, train_perplexity=152.01804, train_loss=5.023999

Batch 209500, train_perplexity=170.03487, train_loss=5.1360035

Batch 209510, train_perplexity=164.75378, train_loss=5.104452

Batch 209520, train_perplexity=192.8069, train_loss=5.261689

Batch 209530, train_perplexity=183.4697, train_loss=5.2120495

Batch 209540, train_perplexity=179.73517, train_loss=5.1914845

Batch 209550, train_perplexity=172.81013, train_loss=5.1521935

Batch 209560, train_perplexity=184.01448, train_loss=5.2150145

Batch 209570, train_perplexity=167.01219, train_loss=5.118067

Batch 209580, train_perplexity=187.33763, train_loss=5.2329125

Batch 209590, train_perplexity=173.17879, train_loss=5.1543245

Batch 209600, train_perplexity=171.67271, train_loss=5.14559

Batch 209610, train_perplexity=174.73048, train_loss=5.1632447

Batch 209620, train_perplexity=156.09818, train_loss=5.050485

Batch 209630, train_perplexity=181.11168, train_loss=5.199114

Batch 209640, train_perplexity=168.08427, train_loss=5.1244655

Batch 209650, train_perplexity=175.82118, train_loss=5.1694674

Batch 209660, train_perplexity=194.28047, train_loss=5.269303

Batch 209670, train_perplexity=191.89905, train_loss=5.2569695

Batch 209680, train_perplexity=190.2329, train_loss=5.248249

Batch 209690, train_perplexity=162.04274, train_loss=5.08786

Batch 209700, train_perplexity=196.75458, train_loss=5.281957

Batch 209710, train_perplexity=191.23167, train_loss=5.2534857

Batch 209720, train_perplexity=181.05608, train_loss=5.198807

Batch 209730, train_perplexity=168.0434, train_loss=5.1242223

Batch 209740, train_perplexity=181.94975, train_loss=5.2037306

Batch 209750, train_perplexity=179.869, train_loss=5.192229

Batch 209760, train_perplexity=164.71451, train_loss=5.1042137

Batch 209770, train_perplexity=185.09444, train_loss=5.220866

Batch 209780, train_perplexity=172.50058, train_loss=5.1504006

Batch 209790, train_perplexity=168.51772, train_loss=5.127041

Batch 209800, train_perplexity=152.38876, train_loss=5.026435

Batch 209810, train_perplexity=184.52516, train_loss=5.217786

Batch 209820, train_perplexity=178.41794, train_loss=5.1841288

Batch 209830, train_perplexity=185.158, train_loss=5.2212095

Batch 209840, train_perplexity=170.03, train_loss=5.135975

Batch 209850, train_perplexity=185.35129, train_loss=5.222253

Batch 209860, train_perplexity=176.67697, train_loss=5.174323

Batch 209870, train_perplexity=189.33932, train_loss=5.243541

Batch 209880, train_perplexity=188.79515, train_loss=5.2406626

Batch 209890, train_perplexity=183.0196, train_loss=5.2095933

Batch 209900, train_perplexity=155.8818, train_loss=5.049098

Batch 209910, train_perplexity=182.77672, train_loss=5.2082653

Batch 209920, train_perplexity=163.14873, train_loss=5.094662

Batch 209930, train_perplexity=192.62009, train_loss=5.26072

Batch 209940, train_perplexity=190.28261, train_loss=5.2485104

Batch 209950, train_perplexity=178.89296, train_loss=5.1867876

Batch 209960, train_perplexity=187.9855, train_loss=5.236365

Batch 209970, train_perplexity=170.12788, train_loss=5.1365504

Batch 209980, train_perplexity=191.55402, train_loss=5.25517

Batch 209990, train_perplexity=177.48158, train_loss=5.178867

Batch 210000, train_perplexity=201.7041, train_loss=5.306802

Batch 210010, train_perplexity=165.125, train_loss=5.106703

Batch 210020, train_perplexity=188.3682, train_loss=5.2383986

Batch 210030, train_perplexity=188.43997, train_loss=5.2387795

Batch 210040, train_perplexity=185.01839, train_loss=5.220455

Batch 210050, train_perplexity=175.09338, train_loss=5.1653194

Batch 210060, train_perplexity=190.48433, train_loss=5.24957

Batch 210070, train_perplexity=165.25418, train_loss=5.107485

Batch 210080, train_perplexity=178.5318, train_loss=5.184767

Batch 210090, train_perplexity=198.26588, train_loss=5.289609

Batch 210100, train_perplexity=185.03691, train_loss=5.2205553

Batch 210110, train_perplexity=178.30942, train_loss=5.1835203

Batch 210120, train_perplexity=189.0053, train_loss=5.241775

Batch 210130, train_perplexity=187.13292, train_loss=5.231819

Batch 210140, train_perplexity=185.29507, train_loss=5.2219496

Batch 210150, train_perplexity=170.86478, train_loss=5.1408725

Batch 210160, train_perplexity=182.44714, train_loss=5.2064605

Batch 210170, train_perplexity=192.84497, train_loss=5.2618866

Batch 210180, train_perplexity=178.65248, train_loss=5.1854424

Batch 210190, train_perplexity=197.97783, train_loss=5.288155

Batch 210200, train_perplexity=165.63602, train_loss=5.1097927

Batch 210210, train_perplexity=168.65887, train_loss=5.127878

Batch 210220, train_perplexity=179.51135, train_loss=5.1902385

Batch 210230, train_perplexity=179.96722, train_loss=5.192775

Batch 210240, train_perplexity=174.6397, train_loss=5.162725

Batch 210250, train_perplexity=186.31982, train_loss=5.2274647

Batch 210260, train_perplexity=170.66179, train_loss=5.1396837

Batch 210270, train_perplexity=188.91222, train_loss=5.2412825

Batch 210280, train_perplexity=175.95898, train_loss=5.170251

Batch 210290, train_perplexity=183.59177, train_loss=5.2127147

Batch 210300, train_perplexity=169.23988, train_loss=5.131317

Batch 210310, train_perplexity=193.8964, train_loss=5.267324

Batch 210320, train_perplexity=186.79076, train_loss=5.229989

Batch 210330, train_perplexity=173.99808, train_loss=5.1590443

Batch 210340, train_perplexity=170.61581, train_loss=5.1394143
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 210350, train_perplexity=166.99077, train_loss=5.1179385

Batch 210360, train_perplexity=170.54187, train_loss=5.138981

Batch 210370, train_perplexity=163.86523, train_loss=5.0990443

Batch 210380, train_perplexity=174.51958, train_loss=5.162037

Batch 210390, train_perplexity=186.8852, train_loss=5.2304945

Batch 210400, train_perplexity=190.29712, train_loss=5.2485867

Batch 210410, train_perplexity=161.27359, train_loss=5.083102

Batch 210420, train_perplexity=162.49748, train_loss=5.0906625

Batch 210430, train_perplexity=173.27882, train_loss=5.154902

Batch 210440, train_perplexity=199.3363, train_loss=5.2949934

Batch 210450, train_perplexity=186.55078, train_loss=5.2287035

Batch 210460, train_perplexity=163.15471, train_loss=5.094699

Batch 210470, train_perplexity=192.71002, train_loss=5.2611866

Batch 210480, train_perplexity=154.95262, train_loss=5.0431194

Batch 210490, train_perplexity=184.13974, train_loss=5.215695

Batch 210500, train_perplexity=171.26929, train_loss=5.143237

Batch 210510, train_perplexity=205.20705, train_loss=5.3240194

Batch 210520, train_perplexity=175.76627, train_loss=5.169155

Batch 210530, train_perplexity=180.89694, train_loss=5.1979275

Batch 210540, train_perplexity=174.93999, train_loss=5.164443

Batch 210550, train_perplexity=168.9633, train_loss=5.1296816

Batch 210560, train_perplexity=163.60625, train_loss=5.0974627

Batch 210570, train_perplexity=164.00679, train_loss=5.099908

Batch 210580, train_perplexity=170.9374, train_loss=5.1412973

Batch 210590, train_perplexity=174.82516, train_loss=5.1637864

Batch 210600, train_perplexity=176.89194, train_loss=5.175539

Batch 210610, train_perplexity=167.31828, train_loss=5.119898

Batch 210620, train_perplexity=164.90146, train_loss=5.105348

Batch 210630, train_perplexity=164.43568, train_loss=5.1025195

Batch 210640, train_perplexity=198.66467, train_loss=5.2916183

Batch 210650, train_perplexity=172.88614, train_loss=5.152633

Batch 210660, train_perplexity=196.05096, train_loss=5.2783747

Batch 210670, train_perplexity=158.57896, train_loss=5.0662527

Batch 210680, train_perplexity=186.71542, train_loss=5.2295856

Batch 210690, train_perplexity=189.96059, train_loss=5.2468166

Batch 210700, train_perplexity=189.52449, train_loss=5.2445183

Batch 210710, train_perplexity=192.58325, train_loss=5.2605286

Batch 210720, train_perplexity=176.6645, train_loss=5.1742525

Batch 210730, train_perplexity=176.85854, train_loss=5.17535

Batch 210740, train_perplexity=166.47455, train_loss=5.1148424

Batch 210750, train_perplexity=175.50063, train_loss=5.1676426

Batch 210760, train_perplexity=192.68338, train_loss=5.2610483

Batch 210770, train_perplexity=171.60536, train_loss=5.1451974

Batch 210780, train_perplexity=178.31299, train_loss=5.1835403

Batch 210790, train_perplexity=178.70709, train_loss=5.185748

Batch 210800, train_perplexity=175.66238, train_loss=5.168564

Batch 210810, train_perplexity=189.62628, train_loss=5.245055

Batch 210820, train_perplexity=179.74408, train_loss=5.191534

Batch 210830, train_perplexity=183.95255, train_loss=5.214678

Batch 210840, train_perplexity=184.1385, train_loss=5.215688

Batch 210850, train_perplexity=191.69638, train_loss=5.255913

Batch 210860, train_perplexity=175.90915, train_loss=5.1699677

Batch 210870, train_perplexity=162.04166, train_loss=5.0878534

Batch 210880, train_perplexity=174.41292, train_loss=5.1614256

Batch 210890, train_perplexity=167.38275, train_loss=5.120283

Batch 210900, train_perplexity=183.0822, train_loss=5.209935

Batch 210910, train_perplexity=178.22746, train_loss=5.1830606

Batch 210920, train_perplexity=193.38864, train_loss=5.264702

Batch 210930, train_perplexity=156.34735, train_loss=5.05208

Batch 210940, train_perplexity=166.78314, train_loss=5.1166945

Batch 210950, train_perplexity=174.31581, train_loss=5.1608686

Batch 210960, train_perplexity=185.28535, train_loss=5.221897

Batch 210970, train_perplexity=201.39734, train_loss=5.3052797

Batch 210980, train_perplexity=177.57208, train_loss=5.1793766

Batch 210990, train_perplexity=172.09973, train_loss=5.148074

Batch 211000, train_perplexity=185.2494, train_loss=5.221703

Batch 211010, train_perplexity=176.50882, train_loss=5.173371

Batch 211020, train_perplexity=160.64024, train_loss=5.0791674

Batch 211030, train_perplexity=166.89278, train_loss=5.1173515

Batch 211040, train_perplexity=175.01709, train_loss=5.1648836

Batch 211050, train_perplexity=180.21901, train_loss=5.194173

Batch 211060, train_perplexity=160.64537, train_loss=5.0791993

Batch 211070, train_perplexity=157.9434, train_loss=5.062237

Batch 211080, train_perplexity=149.7179, train_loss=5.008753

Batch 211090, train_perplexity=179.81917, train_loss=5.1919518

Batch 211100, train_perplexity=187.16112, train_loss=5.23197

Batch 211110, train_perplexity=174.16443, train_loss=5.16

Batch 211120, train_perplexity=181.30057, train_loss=5.200156

Batch 211130, train_perplexity=186.82265, train_loss=5.2301598

Batch 211140, train_perplexity=171.62933, train_loss=5.145337

Batch 211150, train_perplexity=195.29167, train_loss=5.274494

Batch 211160, train_perplexity=162.39577, train_loss=5.0900364

Batch 211170, train_perplexity=182.64447, train_loss=5.2075415

Batch 211180, train_perplexity=180.0334, train_loss=5.1931424

Batch 211190, train_perplexity=179.88829, train_loss=5.192336

Batch 211200, train_perplexity=195.94704, train_loss=5.2778444

Batch 211210, train_perplexity=172.64418, train_loss=5.1512327

Batch 211220, train_perplexity=184.9069, train_loss=5.2198524

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Batch 211230, train_perplexity=175.04355, train_loss=5.165035

Batch 211240, train_perplexity=185.00426, train_loss=5.220379

Batch 211250, train_perplexity=182.97415, train_loss=5.209345

Batch 211260, train_perplexity=190.94594, train_loss=5.2519903

Batch 211270, train_perplexity=179.35573, train_loss=5.189371

Batch 211280, train_perplexity=182.70857, train_loss=5.2078924

Batch 211290, train_perplexity=190.06026, train_loss=5.247341

Batch 211300, train_perplexity=213.23662, train_loss=5.3624024

Batch 211310, train_perplexity=171.00171, train_loss=5.1416736

Batch 211320, train_perplexity=166.42462, train_loss=5.1145425

Batch 211330, train_perplexity=198.50653, train_loss=5.290822

Batch 211340, train_perplexity=179.53404, train_loss=5.190365

Batch 211350, train_perplexity=185.56476, train_loss=5.223404

Batch 211360, train_perplexity=188.27176, train_loss=5.2378864

Batch 211370, train_perplexity=177.00938, train_loss=5.176203

Batch 211380, train_perplexity=176.00815, train_loss=5.1705303

Batch 211390, train_perplexity=174.73474, train_loss=5.163269

Batch 211400, train_perplexity=181.13525, train_loss=5.199244

Batch 211410, train_perplexity=169.87343, train_loss=5.1350536

Batch 211420, train_perplexity=181.19615, train_loss=5.19958

Batch 211430, train_perplexity=184.4168, train_loss=5.2171984

Batch 211440, train_perplexity=191.6516, train_loss=5.255679

Batch 211450, train_perplexity=173.28188, train_loss=5.1549196

Batch 211460, train_perplexity=172.69902, train_loss=5.1515503

Batch 211470, train_perplexity=152.97754, train_loss=5.030291

Batch 211480, train_perplexity=177.77702, train_loss=5.18053

Batch 211490, train_perplexity=152.23051, train_loss=5.025396

Batch 211500, train_perplexity=177.86241, train_loss=5.1810102

Batch 211510, train_perplexity=178.33093, train_loss=5.183641

Batch 211520, train_perplexity=180.89883, train_loss=5.197938

Batch 211530, train_perplexity=188.10843, train_loss=5.2370186

Batch 211540, train_perplexity=207.6824, train_loss=5.33601

Batch 211550, train_perplexity=181.80388, train_loss=5.2029285

Batch 211560, train_perplexity=163.55228, train_loss=5.0971327

Batch 211570, train_perplexity=183.86432, train_loss=5.214198

Batch 211580, train_perplexity=179.43767, train_loss=5.189828
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 211590, train_perplexity=183.54355, train_loss=5.212452

Batch 211600, train_perplexity=179.21791, train_loss=5.1886024

Batch 211610, train_perplexity=157.37357, train_loss=5.0586224

Batch 211620, train_perplexity=192.74367, train_loss=5.261361

Batch 211630, train_perplexity=173.51338, train_loss=5.156255

Batch 211640, train_perplexity=196.43669, train_loss=5.28034

Batch 211650, train_perplexity=164.36725, train_loss=5.102103

Batch 211660, train_perplexity=196.70308, train_loss=5.2816954

Batch 211670, train_perplexity=187.52058, train_loss=5.2338886

Batch 211680, train_perplexity=155.1373, train_loss=5.0443106

Batch 211690, train_perplexity=172.09415, train_loss=5.1480417

Batch 211700, train_perplexity=163.59534, train_loss=5.097396

Batch 211710, train_perplexity=155.5054, train_loss=5.0466805

Batch 211720, train_perplexity=182.39243, train_loss=5.2061605

Batch 211730, train_perplexity=168.18553, train_loss=5.1250677

Batch 211740, train_perplexity=168.62784, train_loss=5.127694

Batch 211750, train_perplexity=182.46567, train_loss=5.206562

Batch 211760, train_perplexity=182.37677, train_loss=5.2060747

Batch 211770, train_perplexity=181.92842, train_loss=5.2036133

Batch 211780, train_perplexity=181.0382, train_loss=5.198708

Batch 211790, train_perplexity=171.72363, train_loss=5.1458864

Batch 211800, train_perplexity=175.3202, train_loss=5.166614

Batch 211810, train_perplexity=181.53577, train_loss=5.2014527

Batch 211820, train_perplexity=172.26837, train_loss=5.1490536

Batch 211830, train_perplexity=180.70314, train_loss=5.1968555

Batch 211840, train_perplexity=194.21416, train_loss=5.2689614

Batch 211850, train_perplexity=179.7247, train_loss=5.1914263

Batch 211860, train_perplexity=162.46254, train_loss=5.0904474

Batch 211870, train_perplexity=199.08525, train_loss=5.293733

Batch 211880, train_perplexity=168.69008, train_loss=5.128063

Batch 211890, train_perplexity=177.52179, train_loss=5.1790934

Batch 211900, train_perplexity=148.72104, train_loss=5.0020723

Batch 211910, train_perplexity=175.53996, train_loss=5.1678667

Batch 211920, train_perplexity=177.17084, train_loss=5.1771145

Batch 211930, train_perplexity=160.4432, train_loss=5.07794

Batch 211940, train_perplexity=196.53357, train_loss=5.2808332

Batch 211950, train_perplexity=189.68326, train_loss=5.2453556

Batch 211960, train_perplexity=179.16101, train_loss=5.188285

Batch 211970, train_perplexity=191.22986, train_loss=5.253476

Batch 211980, train_perplexity=185.93463, train_loss=5.225395

Batch 211990, train_perplexity=195.89575, train_loss=5.2775826

Batch 212000, train_perplexity=174.21277, train_loss=5.1602774

Batch 212010, train_perplexity=177.55328, train_loss=5.1792707

Batch 212020, train_perplexity=172.57866, train_loss=5.150853

Batch 212030, train_perplexity=179.78967, train_loss=5.1917877

Batch 212040, train_perplexity=170.54773, train_loss=5.139015

Batch 212050, train_perplexity=187.1248, train_loss=5.2317758

Batch 212060, train_perplexity=173.42041, train_loss=5.155719

Batch 212070, train_perplexity=171.6462, train_loss=5.1454353

Batch 212080, train_perplexity=185.0678, train_loss=5.220722

Batch 212090, train_perplexity=179.52138, train_loss=5.1902943

Batch 212100, train_perplexity=188.20534, train_loss=5.2375336

Batch 212110, train_perplexity=182.27861, train_loss=5.2055364

Batch 212120, train_perplexity=164.80305, train_loss=5.104751

Batch 212130, train_perplexity=181.47406, train_loss=5.2011127

Batch 212140, train_perplexity=193.77356, train_loss=5.2666903

Batch 212150, train_perplexity=177.80458, train_loss=5.180685

Batch 212160, train_perplexity=162.25381, train_loss=5.089162

Batch 212170, train_perplexity=194.75203, train_loss=5.271727

Batch 212180, train_perplexity=191.23032, train_loss=5.2534785

Batch 212190, train_perplexity=193.81921, train_loss=5.266926

Batch 212200, train_perplexity=173.43158, train_loss=5.155783

Batch 212210, train_perplexity=181.18527, train_loss=5.19952

Batch 212220, train_perplexity=182.23343, train_loss=5.2052884

Batch 212230, train_perplexity=176.55705, train_loss=5.173644

Batch 212240, train_perplexity=170.08968, train_loss=5.136326

Batch 212250, train_perplexity=175.96721, train_loss=5.1702976

Batch 212260, train_perplexity=193.51344, train_loss=5.265347

Batch 212270, train_perplexity=161.60861, train_loss=5.0851774

Batch 212280, train_perplexity=157.58224, train_loss=5.0599475

Batch 212290, train_perplexity=165.15681, train_loss=5.1068954

Batch 212300, train_perplexity=161.48982, train_loss=5.084442

Batch 212310, train_perplexity=181.87837, train_loss=5.203338

Batch 212320, train_perplexity=188.2136, train_loss=5.2375774

Batch 212330, train_perplexity=176.84715, train_loss=5.175286

Batch 212340, train_perplexity=171.52673, train_loss=5.144739

Batch 212350, train_perplexity=184.51971, train_loss=5.2177563

Batch 212360, train_perplexity=183.21074, train_loss=5.210637

Batch 212370, train_perplexity=161.88603, train_loss=5.0868926

Batch 212380, train_perplexity=168.47794, train_loss=5.126805

Batch 212390, train_perplexity=177.10504, train_loss=5.176743

Batch 212400, train_perplexity=174.33327, train_loss=5.160969

Batch 212410, train_perplexity=177.72371, train_loss=5.18023

Batch 212420, train_perplexity=169.1267, train_loss=5.130648

Batch 212430, train_perplexity=185.78584, train_loss=5.2245946

Batch 212440, train_perplexity=173.32501, train_loss=5.1551685

Batch 212450, train_perplexity=164.79385, train_loss=5.1046953

Batch 212460, train_perplexity=176.35587, train_loss=5.172504

Batch 212470, train_perplexity=166.2158, train_loss=5.113287

Batch 212480, train_perplexity=173.9201, train_loss=5.158596

Batch 212490, train_perplexity=178.17393, train_loss=5.1827602

Batch 212500, train_perplexity=178.56816, train_loss=5.1849704

Batch 212510, train_perplexity=183.22638, train_loss=5.2107224

Batch 212520, train_perplexity=177.8334, train_loss=5.180847

Batch 212530, train_perplexity=175.38525, train_loss=5.166985

Batch 212540, train_perplexity=173.01404, train_loss=5.153373

Batch 212550, train_perplexity=176.88773, train_loss=5.175515

Batch 212560, train_perplexity=177.27893, train_loss=5.1777244

Batch 212570, train_perplexity=176.56277, train_loss=5.1736765

Batch 212580, train_perplexity=179.33931, train_loss=5.1892796

Batch 212590, train_perplexity=172.06412, train_loss=5.147867

Batch 212600, train_perplexity=163.87305, train_loss=5.099092

Batch 212610, train_perplexity=199.008, train_loss=5.293345

Batch 212620, train_perplexity=167.34013, train_loss=5.1200285

Batch 212630, train_perplexity=203.17194, train_loss=5.3140526

Batch 212640, train_perplexity=165.85953, train_loss=5.111141

Batch 212650, train_perplexity=175.834, train_loss=5.1695404

Batch 212660, train_perplexity=171.0402, train_loss=5.1418986

Batch 212670, train_perplexity=166.4601, train_loss=5.1147556

Batch 212680, train_perplexity=186.56876, train_loss=5.2288

Batch 212690, train_perplexity=156.41104, train_loss=5.0524874

Batch 212700, train_perplexity=182.4554, train_loss=5.206506

Batch 212710, train_perplexity=165.08415, train_loss=5.1064553

Batch 212720, train_perplexity=189.49007, train_loss=5.2443366

Batch 212730, train_perplexity=183.00381, train_loss=5.209507

Batch 212740, train_perplexity=168.29623, train_loss=5.1257257

Batch 212750, train_perplexity=157.75064, train_loss=5.0610156

Batch 212760, train_perplexity=182.58273, train_loss=5.2072034

Batch 212770, train_perplexity=195.09035, train_loss=5.273463

Batch 212780, train_perplexity=181.0376, train_loss=5.1987047

Batch 212790, train_perplexity=173.06917, train_loss=5.1536913

Batch 212800, train_perplexity=176.8841, train_loss=5.1754947

Batch 212810, train_perplexity=178.62445, train_loss=5.1852856

Batch 212820, train_perplexity=187.2577, train_loss=5.232486

Batch 212830, train_perplexity=195.7683, train_loss=5.276932

Batch 212840, train_perplexity=186.11967, train_loss=5.22639

Batch 212850, train_perplexity=200.74554, train_loss=5.302038

Batch 212860, train_perplexity=162.3134, train_loss=5.089529

Batch 212870, train_perplexity=172.42863, train_loss=5.1499834

Batch 212880, train_perplexity=177.34412, train_loss=5.178092
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 212890, train_perplexity=186.97104, train_loss=5.2309537

Batch 212900, train_perplexity=171.96791, train_loss=5.147308

Batch 212910, train_perplexity=191.41414, train_loss=5.2544394

Batch 212920, train_perplexity=161.27344, train_loss=5.0831013

Batch 212930, train_perplexity=173.9357, train_loss=5.1586857

Batch 212940, train_perplexity=175.2663, train_loss=5.1663065

Batch 212950, train_perplexity=172.08955, train_loss=5.148015

Batch 212960, train_perplexity=158.1386, train_loss=5.063472

Batch 212970, train_perplexity=165.40176, train_loss=5.1083775

Batch 212980, train_perplexity=184.38286, train_loss=5.2170143

Batch 212990, train_perplexity=174.85092, train_loss=5.1639338

Batch 213000, train_perplexity=174.98404, train_loss=5.164695

Batch 213010, train_perplexity=181.98376, train_loss=5.2039175

Batch 213020, train_perplexity=153.11043, train_loss=5.0311594

Batch 213030, train_perplexity=175.22493, train_loss=5.1660705

Batch 213040, train_perplexity=177.95157, train_loss=5.1815114

Batch 213050, train_perplexity=164.15318, train_loss=5.1008

Batch 213060, train_perplexity=167.01395, train_loss=5.1180773

Batch 213070, train_perplexity=193.01627, train_loss=5.2627745

Batch 213080, train_perplexity=167.99413, train_loss=5.123929

Batch 213090, train_perplexity=186.54028, train_loss=5.228647

Batch 213100, train_perplexity=219.40382, train_loss=5.390914

Batch 213110, train_perplexity=167.89723, train_loss=5.123352

Batch 213120, train_perplexity=158.38657, train_loss=5.0650387

Batch 213130, train_perplexity=184.81206, train_loss=5.2193394

Batch 213140, train_perplexity=185.82986, train_loss=5.2248316

Batch 213150, train_perplexity=168.97845, train_loss=5.129771

Batch 213160, train_perplexity=147.4925, train_loss=4.9937773

Batch 213170, train_perplexity=169.74226, train_loss=5.134281

Batch 213180, train_perplexity=182.40025, train_loss=5.2062035

Batch 213190, train_perplexity=164.56126, train_loss=5.103283

Batch 213200, train_perplexity=190.88959, train_loss=5.251695

Batch 213210, train_perplexity=171.31216, train_loss=5.1434875

Batch 213220, train_perplexity=188.82945, train_loss=5.2408442

Batch 213230, train_perplexity=179.8232, train_loss=5.191974

Batch 213240, train_perplexity=160.6607, train_loss=5.0792947

Batch 213250, train_perplexity=182.86903, train_loss=5.2087703

Batch 213260, train_perplexity=181.5491, train_loss=5.201526

Batch 213270, train_perplexity=174.82841, train_loss=5.163805

Batch 213280, train_perplexity=161.48366, train_loss=5.084404

Batch 213290, train_perplexity=149.99786, train_loss=5.010621

Batch 213300, train_perplexity=200.38977, train_loss=5.3002644

Batch 213310, train_perplexity=186.88564, train_loss=5.230497

Batch 213320, train_perplexity=166.52179, train_loss=5.115126

Batch 213330, train_perplexity=181.30402, train_loss=5.2001753

Batch 213340, train_perplexity=182.80566, train_loss=5.2084236

Batch 213350, train_perplexity=182.89877, train_loss=5.208933

Batch 213360, train_perplexity=162.13641, train_loss=5.088438

Batch 213370, train_perplexity=169.20729, train_loss=5.1311245

Batch 213380, train_perplexity=183.2208, train_loss=5.210692

Batch 213390, train_perplexity=177.88506, train_loss=5.1811376

Batch 213400, train_perplexity=173.55112, train_loss=5.156472

Batch 213410, train_perplexity=190.46089, train_loss=5.249447

Batch 213420, train_perplexity=186.71533, train_loss=5.229585

Batch 213430, train_perplexity=184.9651, train_loss=5.220167

Batch 213440, train_perplexity=196.3584, train_loss=5.2799416

Batch 213450, train_perplexity=190.64389, train_loss=5.250407

Batch 213460, train_perplexity=181.0996, train_loss=5.199047

Batch 213470, train_perplexity=185.70737, train_loss=5.224172

Batch 213480, train_perplexity=187.61772, train_loss=5.2344065

Batch 213490, train_perplexity=185.92825, train_loss=5.225361

Batch 213500, train_perplexity=194.68657, train_loss=5.271391

Batch 213510, train_perplexity=174.93231, train_loss=5.164399

Batch 213520, train_perplexity=173.35179, train_loss=5.155323

Batch 213530, train_perplexity=176.99908, train_loss=5.1761446

Batch 213540, train_perplexity=158.65385, train_loss=5.066725

Batch 213550, train_perplexity=170.22388, train_loss=5.1371145

Batch 213560, train_perplexity=174.07774, train_loss=5.159502

Batch 213570, train_perplexity=166.58508, train_loss=5.115506

Batch 213580, train_perplexity=175.51987, train_loss=5.1677523

Batch 213590, train_perplexity=172.71582, train_loss=5.1516476

Batch 213600, train_perplexity=205.1505, train_loss=5.323744

Batch 213610, train_perplexity=178.71152, train_loss=5.185773

Batch 213620, train_perplexity=187.89436, train_loss=5.23588

Batch 213630, train_perplexity=173.20374, train_loss=5.1544685

Batch 213640, train_perplexity=169.70486, train_loss=5.134061

Batch 213650, train_perplexity=175.71071, train_loss=5.168839

Batch 213660, train_perplexity=163.9122, train_loss=5.099331

Batch 213670, train_perplexity=166.06639, train_loss=5.1123877

Batch 213680, train_perplexity=176.91724, train_loss=5.175682

Batch 213690, train_perplexity=166.61813, train_loss=5.1157045

Batch 213700, train_perplexity=156.86179, train_loss=5.055365

Batch 213710, train_perplexity=167.84152, train_loss=5.12302

Batch 213720, train_perplexity=161.01503, train_loss=5.0814977

Batch 213730, train_perplexity=201.53758, train_loss=5.305976

Batch 213740, train_perplexity=188.40143, train_loss=5.238575

Batch 213750, train_perplexity=178.28883, train_loss=5.183405

Batch 213760, train_perplexity=175.35332, train_loss=5.166803

Batch 213770, train_perplexity=166.63672, train_loss=5.115816

Batch 213780, train_perplexity=174.53572, train_loss=5.1621294

Batch 213790, train_perplexity=167.93542, train_loss=5.1235795

Batch 213800, train_perplexity=175.88919, train_loss=5.169854

Batch 213810, train_perplexity=172.44334, train_loss=5.1500688

Batch 213820, train_perplexity=188.37646, train_loss=5.2384424

Batch 213830, train_perplexity=184.2085, train_loss=5.2160683

Batch 213840, train_perplexity=182.08463, train_loss=5.2044716

Batch 213850, train_perplexity=186.87201, train_loss=5.230424

Batch 213860, train_perplexity=181.86891, train_loss=5.203286

Batch 213870, train_perplexity=176.1939, train_loss=5.171585

Batch 213880, train_perplexity=168.12491, train_loss=5.124707

Batch 213890, train_perplexity=183.75967, train_loss=5.213629

Batch 213900, train_perplexity=169.30446, train_loss=5.1316986

Batch 213910, train_perplexity=173.42621, train_loss=5.155752

Batch 213920, train_perplexity=175.8937, train_loss=5.16988

Batch 213930, train_perplexity=191.39917, train_loss=5.254361

Batch 213940, train_perplexity=169.05164, train_loss=5.130204

Batch 213950, train_perplexity=201.75778, train_loss=5.307068

Batch 213960, train_perplexity=210.9324, train_loss=5.3515377

Batch 213970, train_perplexity=165.66406, train_loss=5.109962

Batch 213980, train_perplexity=183.03471, train_loss=5.209676

Batch 213990, train_perplexity=164.99403, train_loss=5.1059093

Batch 214000, train_perplexity=168.3725, train_loss=5.1261787

Batch 214010, train_perplexity=173.46565, train_loss=5.1559796

Batch 214020, train_perplexity=179.4115, train_loss=5.189682

Batch 214030, train_perplexity=181.48358, train_loss=5.201165

Batch 214040, train_perplexity=172.12288, train_loss=5.1482086

Batch 214050, train_perplexity=186.16992, train_loss=5.22666

Batch 214060, train_perplexity=175.10875, train_loss=5.165407

Batch 214070, train_perplexity=176.69846, train_loss=5.1744447

Batch 214080, train_perplexity=168.64825, train_loss=5.1278152

Batch 214090, train_perplexity=191.04083, train_loss=5.252487

Batch 214100, train_perplexity=194.22462, train_loss=5.2690153

Batch 214110, train_perplexity=182.33868, train_loss=5.205866

Batch 214120, train_perplexity=179.37419, train_loss=5.189474

Batch 214130, train_perplexity=193.17998, train_loss=5.2636223

Batch 214140, train_perplexity=168.04445, train_loss=5.1242285

Batch 214150, train_perplexity=185.37515, train_loss=5.2223816

Batch 214160, train_perplexity=157.24214, train_loss=5.057787

Batch 214170, train_perplexity=177.3969, train_loss=5.1783895

Batch 214180, train_perplexity=167.64203, train_loss=5.121831
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 214190, train_perplexity=165.48523, train_loss=5.108882

Batch 214200, train_perplexity=155.36517, train_loss=5.0457783

Batch 214210, train_perplexity=169.82678, train_loss=5.134779

Batch 214220, train_perplexity=160.48085, train_loss=5.0781746

Batch 214230, train_perplexity=177.65694, train_loss=5.1798544

Batch 214240, train_perplexity=175.57562, train_loss=5.16807

Batch 214250, train_perplexity=181.63266, train_loss=5.2019863

Batch 214260, train_perplexity=167.1371, train_loss=5.1188145

Batch 214270, train_perplexity=180.37135, train_loss=5.195018

Batch 214280, train_perplexity=168.80089, train_loss=5.12872

Batch 214290, train_perplexity=159.88202, train_loss=5.074436

Batch 214300, train_perplexity=159.0773, train_loss=5.0693903

Batch 214310, train_perplexity=179.0399, train_loss=5.1876087

Batch 214320, train_perplexity=185.26372, train_loss=5.2217803

Batch 214330, train_perplexity=162.82176, train_loss=5.092656

Batch 214340, train_perplexity=162.31494, train_loss=5.0895386

Batch 214350, train_perplexity=186.42415, train_loss=5.2280245

Batch 214360, train_perplexity=181.01671, train_loss=5.1985893

Batch 214370, train_perplexity=166.08136, train_loss=5.112478

Batch 214380, train_perplexity=194.63663, train_loss=5.2711344

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00082-of-00100
Loaded 304654 sentences.
Finished loading
Batch 214390, train_perplexity=170.91986, train_loss=5.141195

Batch 214400, train_perplexity=186.46843, train_loss=5.228262

Batch 214410, train_perplexity=186.8181, train_loss=5.2301354

Batch 214420, train_perplexity=184.58685, train_loss=5.21812

Batch 214430, train_perplexity=186.2271, train_loss=5.226967

Batch 214440, train_perplexity=191.29643, train_loss=5.253824

Batch 214450, train_perplexity=160.50504, train_loss=5.0783253

Batch 214460, train_perplexity=166.76875, train_loss=5.116608

Batch 214470, train_perplexity=175.35892, train_loss=5.166835

Batch 214480, train_perplexity=182.87, train_loss=5.2087755

Batch 214490, train_perplexity=187.77989, train_loss=5.2352705

Batch 214500, train_perplexity=185.60547, train_loss=5.2236233

Batch 214510, train_perplexity=201.53432, train_loss=5.3059597

Batch 214520, train_perplexity=171.44496, train_loss=5.1442623

Batch 214530, train_perplexity=165.7852, train_loss=5.110693

Batch 214540, train_perplexity=167.0117, train_loss=5.118064

Batch 214550, train_perplexity=184.6402, train_loss=5.218409

Batch 214560, train_perplexity=188.19385, train_loss=5.2374725

Batch 214570, train_perplexity=191.4565, train_loss=5.2546606

Batch 214580, train_perplexity=178.66568, train_loss=5.1855164

Batch 214590, train_perplexity=164.4887, train_loss=5.102842

Batch 214600, train_perplexity=154.67033, train_loss=5.041296

Batch 214610, train_perplexity=198.9504, train_loss=5.2930555

Batch 214620, train_perplexity=166.62067, train_loss=5.11572

Batch 214630, train_perplexity=181.05858, train_loss=5.1988206

Batch 214640, train_perplexity=183.86319, train_loss=5.214192

Batch 214650, train_perplexity=168.08379, train_loss=5.1244626

Batch 214660, train_perplexity=167.35802, train_loss=5.1201353

Batch 214670, train_perplexity=180.7021, train_loss=5.19685

Batch 214680, train_perplexity=164.70303, train_loss=5.104144

Batch 214690, train_perplexity=172.4737, train_loss=5.1502447

Batch 214700, train_perplexity=173.22487, train_loss=5.1545906

Batch 214710, train_perplexity=165.5269, train_loss=5.1091337

Batch 214720, train_perplexity=165.29446, train_loss=5.1077285

Batch 214730, train_perplexity=188.29527, train_loss=5.2380114

Batch 214740, train_perplexity=165.63365, train_loss=5.1097784

Batch 214750, train_perplexity=184.4736, train_loss=5.2175064

Batch 214760, train_perplexity=163.78844, train_loss=5.0985756

Batch 214770, train_perplexity=163.03137, train_loss=5.0939426

Batch 214780, train_perplexity=181.75586, train_loss=5.2026644

Batch 214790, train_perplexity=181.84723, train_loss=5.203167

Batch 214800, train_perplexity=182.70004, train_loss=5.2078457

Batch 214810, train_perplexity=187.21922, train_loss=5.2322803

Batch 214820, train_perplexity=175.18776, train_loss=5.1658583

Batch 214830, train_perplexity=174.60098, train_loss=5.1625032

Batch 214840, train_perplexity=171.21736, train_loss=5.142934

Batch 214850, train_perplexity=163.78796, train_loss=5.0985727

Batch 214860, train_perplexity=170.59457, train_loss=5.13929

Batch 214870, train_perplexity=155.64352, train_loss=5.0475683

Batch 214880, train_perplexity=162.31146, train_loss=5.089517

Batch 214890, train_perplexity=191.49812, train_loss=5.254878

Batch 214900, train_perplexity=185.73093, train_loss=5.224299

Batch 214910, train_perplexity=179.4583, train_loss=5.189943

Batch 214920, train_perplexity=195.88557, train_loss=5.2775307

Batch 214930, train_perplexity=181.377, train_loss=5.2005777

Batch 214940, train_perplexity=155.68138, train_loss=5.0478115

Batch 214950, train_perplexity=191.98718, train_loss=5.2574286

Batch 214960, train_perplexity=186.77544, train_loss=5.229907

Batch 214970, train_perplexity=162.88824, train_loss=5.0930643

Batch 214980, train_perplexity=147.89702, train_loss=4.996516

Batch 214990, train_perplexity=160.71779, train_loss=5.07965

Batch 215000, train_perplexity=173.42654, train_loss=5.155754

Batch 215010, train_perplexity=170.30734, train_loss=5.1376047

Batch 215020, train_perplexity=189.91603, train_loss=5.246582

Batch 215030, train_perplexity=187.73567, train_loss=5.235035

Batch 215040, train_perplexity=189.56616, train_loss=5.244738

Batch 215050, train_perplexity=171.20935, train_loss=5.142887

Batch 215060, train_perplexity=195.93901, train_loss=5.2778034

Batch 215070, train_perplexity=178.44925, train_loss=5.184304

Batch 215080, train_perplexity=194.20961, train_loss=5.268938

Batch 215090, train_perplexity=206.42752, train_loss=5.3299494

Batch 215100, train_perplexity=173.92484, train_loss=5.158623

Batch 215110, train_perplexity=164.85043, train_loss=5.1050386

Batch 215120, train_perplexity=176.18701, train_loss=5.171546

Batch 215130, train_perplexity=189.71437, train_loss=5.2455196

Batch 215140, train_perplexity=184.8808, train_loss=5.2197113

Batch 215150, train_perplexity=179.48439, train_loss=5.1900883

Batch 215160, train_perplexity=186.66388, train_loss=5.2293096

Batch 215170, train_perplexity=151.59227, train_loss=5.0211945

Batch 215180, train_perplexity=182.22047, train_loss=5.2052174

Batch 215190, train_perplexity=172.63446, train_loss=5.1511765

Batch 215200, train_perplexity=164.82034, train_loss=5.104856

Batch 215210, train_perplexity=182.76399, train_loss=5.2081957

Batch 215220, train_perplexity=174.23312, train_loss=5.160394

Batch 215230, train_perplexity=155.75638, train_loss=5.048293

Batch 215240, train_perplexity=155.04044, train_loss=5.043686

Batch 215250, train_perplexity=180.73691, train_loss=5.1970425

Batch 215260, train_perplexity=183.64589, train_loss=5.2130094

Batch 215270, train_perplexity=182.78996, train_loss=5.208338

Batch 215280, train_perplexity=158.35213, train_loss=5.0648212

Batch 215290, train_perplexity=195.01324, train_loss=5.2730675

Batch 215300, train_perplexity=167.33694, train_loss=5.1200094

Batch 215310, train_perplexity=170.00519, train_loss=5.135829

Batch 215320, train_perplexity=164.88495, train_loss=5.105248

Batch 215330, train_perplexity=180.50197, train_loss=5.1957417

Batch 215340, train_perplexity=170.71826, train_loss=5.1400146

Batch 215350, train_perplexity=178.78772, train_loss=5.186199

Batch 215360, train_perplexity=181.91046, train_loss=5.2035146

Batch 215370, train_perplexity=192.18695, train_loss=5.2584686

Batch 215380, train_perplexity=198.11316, train_loss=5.2888384

Batch 215390, train_perplexity=177.24123, train_loss=5.1775117

Batch 215400, train_perplexity=173.8299, train_loss=5.1580772

Batch 215410, train_perplexity=177.877, train_loss=5.1810923

Batch 215420, train_perplexity=195.38052, train_loss=5.274949
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 215430, train_perplexity=180.4771, train_loss=5.195604

Batch 215440, train_perplexity=170.24329, train_loss=5.1372285

Batch 215450, train_perplexity=175.54364, train_loss=5.1678877

Batch 215460, train_perplexity=178.76726, train_loss=5.1860847

Batch 215470, train_perplexity=176.18936, train_loss=5.1715593

Batch 215480, train_perplexity=194.2574, train_loss=5.269184

Batch 215490, train_perplexity=184.07645, train_loss=5.215351

Batch 215500, train_perplexity=183.87871, train_loss=5.2142763

Batch 215510, train_perplexity=182.79903, train_loss=5.2083874

Batch 215520, train_perplexity=186.61137, train_loss=5.229028

Batch 215530, train_perplexity=172.37027, train_loss=5.149645

Batch 215540, train_perplexity=163.9161, train_loss=5.0993547

Batch 215550, train_perplexity=181.0837, train_loss=5.1989594

Batch 215560, train_perplexity=158.16951, train_loss=5.0636673

Batch 215570, train_perplexity=166.98718, train_loss=5.117917

Batch 215580, train_perplexity=176.27188, train_loss=5.1720276

Batch 215590, train_perplexity=174.60164, train_loss=5.162507

Batch 215600, train_perplexity=183.90817, train_loss=5.2144365

Batch 215610, train_perplexity=184.28055, train_loss=5.2164593

Batch 215620, train_perplexity=189.17725, train_loss=5.2426844

Batch 215630, train_perplexity=164.45207, train_loss=5.102619

Batch 215640, train_perplexity=166.41058, train_loss=5.114458

Batch 215650, train_perplexity=181.42215, train_loss=5.2008266

Batch 215660, train_perplexity=196.8502, train_loss=5.282443

Batch 215670, train_perplexity=181.83025, train_loss=5.2030735

Batch 215680, train_perplexity=163.01784, train_loss=5.0938597

Batch 215690, train_perplexity=196.55925, train_loss=5.280964

Batch 215700, train_perplexity=167.74998, train_loss=5.1224747

Batch 215710, train_perplexity=160.78247, train_loss=5.0800524

Batch 215720, train_perplexity=175.06282, train_loss=5.165145

Batch 215730, train_perplexity=172.75494, train_loss=5.151874

Batch 215740, train_perplexity=165.06644, train_loss=5.106348

Batch 215750, train_perplexity=183.59624, train_loss=5.212739

Batch 215760, train_perplexity=177.15277, train_loss=5.1770124

Batch 215770, train_perplexity=185.81268, train_loss=5.224739

Batch 215780, train_perplexity=174.03143, train_loss=5.159236

Batch 215790, train_perplexity=183.53006, train_loss=5.2123785

Batch 215800, train_perplexity=185.4733, train_loss=5.222911

Batch 215810, train_perplexity=177.36424, train_loss=5.1782055

Batch 215820, train_perplexity=179.63722, train_loss=5.1909394

Batch 215830, train_perplexity=204.51192, train_loss=5.3206263

Batch 215840, train_perplexity=172.17229, train_loss=5.1484957

Batch 215850, train_perplexity=199.23558, train_loss=5.294488

Batch 215860, train_perplexity=198.20274, train_loss=5.2892904

Batch 215870, train_perplexity=186.32008, train_loss=5.227466

Batch 215880, train_perplexity=168.49835, train_loss=5.126926

Batch 215890, train_perplexity=186.88858, train_loss=5.2305126

Batch 215900, train_perplexity=168.47794, train_loss=5.126805

Batch 215910, train_perplexity=172.65405, train_loss=5.15129

Batch 215920, train_perplexity=165.2348, train_loss=5.1073675

Batch 215930, train_perplexity=191.60172, train_loss=5.255419

Batch 215940, train_perplexity=194.69966, train_loss=5.271458

Batch 215950, train_perplexity=176.57448, train_loss=5.173743

Batch 215960, train_perplexity=163.25969, train_loss=5.095342

Batch 215970, train_perplexity=188.41203, train_loss=5.2386312

Batch 215980, train_perplexity=179.4772, train_loss=5.190048

Batch 215990, train_perplexity=185.72818, train_loss=5.224284

Batch 216000, train_perplexity=165.9562, train_loss=5.111724

Batch 216010, train_perplexity=170.69955, train_loss=5.139905

Batch 216020, train_perplexity=171.84094, train_loss=5.1465693

Batch 216030, train_perplexity=191.87672, train_loss=5.256853

Batch 216040, train_perplexity=157.25752, train_loss=5.0578847

Batch 216050, train_perplexity=185.05756, train_loss=5.220667

Batch 216060, train_perplexity=168.64021, train_loss=5.1277676

Batch 216070, train_perplexity=175.85892, train_loss=5.169682

Batch 216080, train_perplexity=171.32066, train_loss=5.143537

Batch 216090, train_perplexity=171.15067, train_loss=5.1425443

Batch 216100, train_perplexity=155.86969, train_loss=5.0490203

Batch 216110, train_perplexity=196.48325, train_loss=5.280577

Batch 216120, train_perplexity=178.18889, train_loss=5.182844

Batch 216130, train_perplexity=165.81714, train_loss=5.1108856

Batch 216140, train_perplexity=157.19649, train_loss=5.0574965

Batch 216150, train_perplexity=184.08601, train_loss=5.215403

Batch 216160, train_perplexity=168.50165, train_loss=5.1269455

Batch 216170, train_perplexity=159.76299, train_loss=5.0736914

Batch 216180, train_perplexity=188.99854, train_loss=5.2417393

Batch 216190, train_perplexity=175.11, train_loss=5.1654143

Batch 216200, train_perplexity=181.41394, train_loss=5.2007813

Batch 216210, train_perplexity=165.33931, train_loss=5.108

Batch 216220, train_perplexity=184.0415, train_loss=5.2151613

Batch 216230, train_perplexity=158.16438, train_loss=5.063635

Batch 216240, train_perplexity=187.12292, train_loss=5.2317657

Batch 216250, train_perplexity=175.79678, train_loss=5.1693287

Batch 216260, train_perplexity=167.75638, train_loss=5.122513

Batch 216270, train_perplexity=185.64883, train_loss=5.223857

Batch 216280, train_perplexity=165.31715, train_loss=5.107866

Batch 216290, train_perplexity=178.43564, train_loss=5.184228

Batch 216300, train_perplexity=169.09517, train_loss=5.1304617

Batch 216310, train_perplexity=162.81773, train_loss=5.0926313

Batch 216320, train_perplexity=173.60542, train_loss=5.156785

Batch 216330, train_perplexity=188.68428, train_loss=5.240075

Batch 216340, train_perplexity=155.04738, train_loss=5.0437307

Batch 216350, train_perplexity=160.22057, train_loss=5.0765514

Batch 216360, train_perplexity=164.19037, train_loss=5.1010265

Batch 216370, train_perplexity=157.00659, train_loss=5.056288

Batch 216380, train_perplexity=176.63873, train_loss=5.1741066

Batch 216390, train_perplexity=176.62921, train_loss=5.1740527

Batch 216400, train_perplexity=175.71147, train_loss=5.1688433

Batch 216410, train_perplexity=158.38052, train_loss=5.0650005

Batch 216420, train_perplexity=170.74742, train_loss=5.1401854

Batch 216430, train_perplexity=191.89539, train_loss=5.2569504

Batch 216440, train_perplexity=174.59848, train_loss=5.162489

Batch 216450, train_perplexity=170.27356, train_loss=5.1374063

Batch 216460, train_perplexity=176.0659, train_loss=5.1708584

Batch 216470, train_perplexity=148.5246, train_loss=5.0007505

Batch 216480, train_perplexity=178.64268, train_loss=5.1853876

Batch 216490, train_perplexity=181.00703, train_loss=5.198536

Batch 216500, train_perplexity=184.76888, train_loss=5.2191057

Batch 216510, train_perplexity=173.6026, train_loss=5.156769

Batch 216520, train_perplexity=184.86493, train_loss=5.2196255

Batch 216530, train_perplexity=196.80403, train_loss=5.2822084

Batch 216540, train_perplexity=164.74248, train_loss=5.1043835

Batch 216550, train_perplexity=181.08525, train_loss=5.198968

Batch 216560, train_perplexity=154.95972, train_loss=5.043165

Batch 216570, train_perplexity=170.43123, train_loss=5.138332

Batch 216580, train_perplexity=184.0193, train_loss=5.2150407

Batch 216590, train_perplexity=159.51453, train_loss=5.072135

Batch 216600, train_perplexity=171.6696, train_loss=5.1455717

Batch 216610, train_perplexity=172.24406, train_loss=5.1489124

Batch 216620, train_perplexity=198.91075, train_loss=5.292856

Batch 216630, train_perplexity=177.95216, train_loss=5.1815147

Batch 216640, train_perplexity=182.53111, train_loss=5.2069206

Batch 216650, train_perplexity=154.94258, train_loss=5.0430546

Batch 216660, train_perplexity=178.4044, train_loss=5.184053

Batch 216670, train_perplexity=173.04268, train_loss=5.153538

Batch 216680, train_perplexity=169.85416, train_loss=5.13494

Batch 216690, train_perplexity=170.24417, train_loss=5.1372337

Batch 216700, train_perplexity=170.24678, train_loss=5.137249

Batch 216710, train_perplexity=151.41, train_loss=5.0199914

Batch 216720, train_perplexity=174.81792, train_loss=5.163745
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 216730, train_perplexity=179.10667, train_loss=5.1879816

Batch 216740, train_perplexity=170.70102, train_loss=5.1399136

Batch 216750, train_perplexity=188.74295, train_loss=5.240386

Batch 216760, train_perplexity=183.76642, train_loss=5.2136655

Batch 216770, train_perplexity=174.36452, train_loss=5.161148

Batch 216780, train_perplexity=159.08398, train_loss=5.0694323

Batch 216790, train_perplexity=184.63104, train_loss=5.2183595

Batch 216800, train_perplexity=169.52742, train_loss=5.1330147

Batch 216810, train_perplexity=188.6177, train_loss=5.2397223

Batch 216820, train_perplexity=188.54973, train_loss=5.239362

Batch 216830, train_perplexity=174.21045, train_loss=5.160264

Batch 216840, train_perplexity=155.65422, train_loss=5.047637

Batch 216850, train_perplexity=169.76654, train_loss=5.134424

Batch 216860, train_perplexity=169.3626, train_loss=5.132042

Batch 216870, train_perplexity=168.00775, train_loss=5.12401

Batch 216880, train_perplexity=159.11191, train_loss=5.0696077

Batch 216890, train_perplexity=189.60838, train_loss=5.244961

Batch 216900, train_perplexity=171.7985, train_loss=5.1463223

Batch 216910, train_perplexity=169.0618, train_loss=5.1302643

Batch 216920, train_perplexity=190.03017, train_loss=5.247183

Batch 216930, train_perplexity=183.04875, train_loss=5.2097526

Batch 216940, train_perplexity=166.9629, train_loss=5.1177716

Batch 216950, train_perplexity=154.48607, train_loss=5.040104

Batch 216960, train_perplexity=190.084, train_loss=5.247466

Batch 216970, train_perplexity=183.78166, train_loss=5.2137485

Batch 216980, train_perplexity=178.42184, train_loss=5.1841507

Batch 216990, train_perplexity=165.41833, train_loss=5.1084776

Batch 217000, train_perplexity=155.65979, train_loss=5.0476727

Batch 217010, train_perplexity=179.0551, train_loss=5.1876936

Batch 217020, train_perplexity=172.11992, train_loss=5.1481915

Batch 217030, train_perplexity=178.36877, train_loss=5.183853

Batch 217040, train_perplexity=185.10204, train_loss=5.220907

Batch 217050, train_perplexity=172.50034, train_loss=5.150399

Batch 217060, train_perplexity=186.98993, train_loss=5.231055

Batch 217070, train_perplexity=162.46579, train_loss=5.0904675

Batch 217080, train_perplexity=193.27977, train_loss=5.2641387

Batch 217090, train_perplexity=168.90854, train_loss=5.1293573

Batch 217100, train_perplexity=174.2902, train_loss=5.160722

Batch 217110, train_perplexity=185.7133, train_loss=5.224204

Batch 217120, train_perplexity=156.82268, train_loss=5.0551157

Batch 217130, train_perplexity=205.40852, train_loss=5.325001

Batch 217140, train_perplexity=179.93994, train_loss=5.192623

Batch 217150, train_perplexity=174.47997, train_loss=5.16181

Batch 217160, train_perplexity=153.63751, train_loss=5.034596

Batch 217170, train_perplexity=176.68414, train_loss=5.1743636

Batch 217180, train_perplexity=176.54207, train_loss=5.173559

Batch 217190, train_perplexity=187.9743, train_loss=5.236305

Batch 217200, train_perplexity=161.80554, train_loss=5.0863953

Batch 217210, train_perplexity=166.12761, train_loss=5.1127563

Batch 217220, train_perplexity=167.78622, train_loss=5.1226907

Batch 217230, train_perplexity=185.12587, train_loss=5.221036

Batch 217240, train_perplexity=189.41824, train_loss=5.2439575

Batch 217250, train_perplexity=190.00108, train_loss=5.24703

Batch 217260, train_perplexity=163.69249, train_loss=5.0979896

Batch 217270, train_perplexity=160.83093, train_loss=5.0803537

Batch 217280, train_perplexity=163.52686, train_loss=5.096977

Batch 217290, train_perplexity=164.84447, train_loss=5.1050024

Batch 217300, train_perplexity=168.58795, train_loss=5.1274576

Batch 217310, train_perplexity=157.69823, train_loss=5.0606833

Batch 217320, train_perplexity=174.81808, train_loss=5.163746

Batch 217330, train_perplexity=179.63518, train_loss=5.190928

Batch 217340, train_perplexity=187.79091, train_loss=5.235329

Batch 217350, train_perplexity=180.71468, train_loss=5.1969194

Batch 217360, train_perplexity=187.7507, train_loss=5.235115

Batch 217370, train_perplexity=186.96541, train_loss=5.2309237

Batch 217380, train_perplexity=175.66656, train_loss=5.1685877

Batch 217390, train_perplexity=172.98088, train_loss=5.153181

Batch 217400, train_perplexity=181.46715, train_loss=5.2010746

Batch 217410, train_perplexity=164.31772, train_loss=5.101802

Batch 217420, train_perplexity=185.5223, train_loss=5.223175

Batch 217430, train_perplexity=178.5868, train_loss=5.185075

Batch 217440, train_perplexity=198.6552, train_loss=5.2915707

Batch 217450, train_perplexity=159.99336, train_loss=5.0751324

Batch 217460, train_perplexity=175.80525, train_loss=5.169377

Batch 217470, train_perplexity=189.84703, train_loss=5.2462187

Batch 217480, train_perplexity=165.16643, train_loss=5.1069536

Batch 217490, train_perplexity=191.15392, train_loss=5.253079

Batch 217500, train_perplexity=185.52034, train_loss=5.2231646

Batch 217510, train_perplexity=159.77716, train_loss=5.07378

Batch 217520, train_perplexity=159.51315, train_loss=5.0721264

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00044-of-00100
Loaded 305912 sentences.
Finished loading
Batch 217530, train_perplexity=173.87201, train_loss=5.1583195

Batch 217540, train_perplexity=178.36707, train_loss=5.1838436

Batch 217550, train_perplexity=159.66824, train_loss=5.073098

Batch 217560, train_perplexity=183.48334, train_loss=5.212124

Batch 217570, train_perplexity=198.8072, train_loss=5.2923355

Batch 217580, train_perplexity=167.30998, train_loss=5.1198483

Batch 217590, train_perplexity=193.70853, train_loss=5.2663546

Batch 217600, train_perplexity=169.50494, train_loss=5.132882

Batch 217610, train_perplexity=165.5768, train_loss=5.109435

Batch 217620, train_perplexity=179.50255, train_loss=5.1901894

Batch 217630, train_perplexity=169.45581, train_loss=5.132592

Batch 217640, train_perplexity=172.75008, train_loss=5.151846

Batch 217650, train_perplexity=163.42006, train_loss=5.096324

Batch 217660, train_perplexity=184.19708, train_loss=5.2160063

Batch 217670, train_perplexity=178.05621, train_loss=5.1820993

Batch 217680, train_perplexity=178.92299, train_loss=5.1869555

Batch 217690, train_perplexity=163.40932, train_loss=5.096258

Batch 217700, train_perplexity=173.74545, train_loss=5.1575913

Batch 217710, train_perplexity=147.82271, train_loss=4.9960136

Batch 217720, train_perplexity=161.27913, train_loss=5.0831366

Batch 217730, train_perplexity=196.76631, train_loss=5.2820168

Batch 217740, train_perplexity=169.34692, train_loss=5.1319494

Batch 217750, train_perplexity=162.56172, train_loss=5.091058

Batch 217760, train_perplexity=175.52489, train_loss=5.167781

Batch 217770, train_perplexity=170.6294, train_loss=5.139494

Batch 217780, train_perplexity=187.17575, train_loss=5.232048

Batch 217790, train_perplexity=185.8444, train_loss=5.22491

Batch 217800, train_perplexity=204.8924, train_loss=5.322485

Batch 217810, train_perplexity=189.48048, train_loss=5.244286

Batch 217820, train_perplexity=179.41542, train_loss=5.189704

Batch 217830, train_perplexity=196.43463, train_loss=5.2803297

Batch 217840, train_perplexity=157.15265, train_loss=5.0572176

Batch 217850, train_perplexity=168.82874, train_loss=5.128885

Batch 217860, train_perplexity=178.4113, train_loss=5.1840916

Batch 217870, train_perplexity=153.47897, train_loss=5.0335636

Batch 217880, train_perplexity=175.11676, train_loss=5.165453

Batch 217890, train_perplexity=151.89674, train_loss=5.023201

Batch 217900, train_perplexity=157.99878, train_loss=5.0625873

Batch 217910, train_perplexity=186.06076, train_loss=5.2260733

Batch 217920, train_perplexity=183.80655, train_loss=5.213884

Batch 217930, train_perplexity=171.62656, train_loss=5.145321

Batch 217940, train_perplexity=167.93758, train_loss=5.1235924

Batch 217950, train_perplexity=184.65904, train_loss=5.218511

Batch 217960, train_perplexity=192.48254, train_loss=5.2600055
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 217970, train_perplexity=170.47115, train_loss=5.138566

Batch 217980, train_perplexity=154.13406, train_loss=5.0378227

Batch 217990, train_perplexity=190.84216, train_loss=5.2514467

Batch 218000, train_perplexity=156.00166, train_loss=5.0498667

Batch 218010, train_perplexity=184.68907, train_loss=5.2186737

Batch 218020, train_perplexity=170.7019, train_loss=5.139919

Batch 218030, train_perplexity=172.49968, train_loss=5.1503954

Batch 218040, train_perplexity=197.30183, train_loss=5.2847347

Batch 218050, train_perplexity=170.93364, train_loss=5.1412754

Batch 218060, train_perplexity=176.68633, train_loss=5.174376

Batch 218070, train_perplexity=201.10751, train_loss=5.3038397

Batch 218080, train_perplexity=196.74182, train_loss=5.2818923

Batch 218090, train_perplexity=178.55223, train_loss=5.184881

Batch 218100, train_perplexity=189.91168, train_loss=5.246559

Batch 218110, train_perplexity=196.54819, train_loss=5.2809076

Batch 218120, train_perplexity=178.6793, train_loss=5.1855927

Batch 218130, train_perplexity=187.73477, train_loss=5.23503

Batch 218140, train_perplexity=177.3825, train_loss=5.1783085

Batch 218150, train_perplexity=178.66943, train_loss=5.1855373

Batch 218160, train_perplexity=192.5076, train_loss=5.2601357

Batch 218170, train_perplexity=166.2261, train_loss=5.113349

Batch 218180, train_perplexity=186.45634, train_loss=5.228197

Batch 218190, train_perplexity=170.61247, train_loss=5.1393948

Batch 218200, train_perplexity=172.23676, train_loss=5.14887

Batch 218210, train_perplexity=172.36665, train_loss=5.149624

Batch 218220, train_perplexity=172.45108, train_loss=5.1501136

Batch 218230, train_perplexity=165.91743, train_loss=5.1114902

Batch 218240, train_perplexity=170.93086, train_loss=5.141259

Batch 218250, train_perplexity=158.39035, train_loss=5.0650625

Batch 218260, train_perplexity=162.63011, train_loss=5.0914783

Batch 218270, train_perplexity=178.39769, train_loss=5.1840153

Batch 218280, train_perplexity=178.37813, train_loss=5.1839056

Batch 218290, train_perplexity=184.10681, train_loss=5.215516

Batch 218300, train_perplexity=199.38916, train_loss=5.2952585

Batch 218310, train_perplexity=201.34952, train_loss=5.3050423

Batch 218320, train_perplexity=156.02554, train_loss=5.0500197

Batch 218330, train_perplexity=180.40025, train_loss=5.195178

Batch 218340, train_perplexity=167.90459, train_loss=5.123396

Batch 218350, train_perplexity=168.23671, train_loss=5.125372

Batch 218360, train_perplexity=164.30707, train_loss=5.101737

Batch 218370, train_perplexity=173.65219, train_loss=5.1570544

Batch 218380, train_perplexity=166.64514, train_loss=5.1158667

Batch 218390, train_perplexity=188.99142, train_loss=5.2417016

Batch 218400, train_perplexity=180.49164, train_loss=5.1956844

Batch 218410, train_perplexity=162.98427, train_loss=5.0936537

Batch 218420, train_perplexity=159.22249, train_loss=5.0703025

Batch 218430, train_perplexity=163.40182, train_loss=5.0962124

Batch 218440, train_perplexity=185.59273, train_loss=5.2235546

Batch 218450, train_perplexity=184.8207, train_loss=5.219386

Batch 218460, train_perplexity=178.07048, train_loss=5.1821795

Batch 218470, train_perplexity=185.18555, train_loss=5.2213583

Batch 218480, train_perplexity=167.43951, train_loss=5.120622

Batch 218490, train_perplexity=176.06271, train_loss=5.1708403

Batch 218500, train_perplexity=174.48146, train_loss=5.1618185

Batch 218510, train_perplexity=165.30873, train_loss=5.107815

Batch 218520, train_perplexity=186.07034, train_loss=5.226125

Batch 218530, train_perplexity=171.94159, train_loss=5.147155

Batch 218540, train_perplexity=184.84581, train_loss=5.219522

Batch 218550, train_perplexity=183.49121, train_loss=5.212167

Batch 218560, train_perplexity=156.84428, train_loss=5.0552535

Batch 218570, train_perplexity=194.50998, train_loss=5.2704835

Batch 218580, train_perplexity=188.9442, train_loss=5.2414517

Batch 218590, train_perplexity=163.71722, train_loss=5.0981407

Batch 218600, train_perplexity=156.74791, train_loss=5.054639

Batch 218610, train_perplexity=168.19386, train_loss=5.1251173

Batch 218620, train_perplexity=184.77135, train_loss=5.219119

Batch 218630, train_perplexity=156.56453, train_loss=5.053468

Batch 218640, train_perplexity=172.61076, train_loss=5.151039

Batch 218650, train_perplexity=181.3175, train_loss=5.2002497

Batch 218660, train_perplexity=184.57639, train_loss=5.2180634

Batch 218670, train_perplexity=162.49925, train_loss=5.0906734

Batch 218680, train_perplexity=171.42117, train_loss=5.1441236

Batch 218690, train_perplexity=175.47937, train_loss=5.1675215

Batch 218700, train_perplexity=182.71782, train_loss=5.207943

Batch 218710, train_perplexity=181.07178, train_loss=5.1988935

Batch 218720, train_perplexity=186.5862, train_loss=5.2288933

Batch 218730, train_perplexity=161.3388, train_loss=5.0835066

Batch 218740, train_perplexity=178.66806, train_loss=5.1855297

Batch 218750, train_perplexity=162.51382, train_loss=5.090763

Batch 218760, train_perplexity=165.26538, train_loss=5.1075525

Batch 218770, train_perplexity=164.93063, train_loss=5.105525

Batch 218780, train_perplexity=172.78568, train_loss=5.152052

Batch 218790, train_perplexity=198.44218, train_loss=5.290498

Batch 218800, train_perplexity=183.2422, train_loss=5.2108088

Batch 218810, train_perplexity=157.37552, train_loss=5.0586348

Batch 218820, train_perplexity=164.088, train_loss=5.100403

Batch 218830, train_perplexity=174.06348, train_loss=5.15942

Batch 218840, train_perplexity=175.59363, train_loss=5.1681724

Batch 218850, train_perplexity=162.47926, train_loss=5.0905504

Batch 218860, train_perplexity=175.54114, train_loss=5.1678734

Batch 218870, train_perplexity=186.93315, train_loss=5.230751

Batch 218880, train_perplexity=188.02522, train_loss=5.236576

Batch 218890, train_perplexity=164.05936, train_loss=5.1002283

Batch 218900, train_perplexity=158.43355, train_loss=5.0653353

Batch 218910, train_perplexity=160.58725, train_loss=5.0788374

Batch 218920, train_perplexity=166.89453, train_loss=5.117362

Batch 218930, train_perplexity=182.3966, train_loss=5.2061834

Batch 218940, train_perplexity=170.0716, train_loss=5.1362195

Batch 218950, train_perplexity=172.18404, train_loss=5.148564

Batch 218960, train_perplexity=191.11563, train_loss=5.2528787

Batch 218970, train_perplexity=156.12057, train_loss=5.0506287

Batch 218980, train_perplexity=178.94696, train_loss=5.1870894

Batch 218990, train_perplexity=165.69905, train_loss=5.110173

Batch 219000, train_perplexity=184.77187, train_loss=5.219122

Batch 219010, train_perplexity=161.30696, train_loss=5.083309

Batch 219020, train_perplexity=164.78969, train_loss=5.10467

Batch 219030, train_perplexity=198.5073, train_loss=5.290826

Batch 219040, train_perplexity=190.8387, train_loss=5.2514286

Batch 219050, train_perplexity=173.77603, train_loss=5.1577673

Batch 219060, train_perplexity=190.30746, train_loss=5.248641

Batch 219070, train_perplexity=188.41437, train_loss=5.2386436

Batch 219080, train_perplexity=170.375, train_loss=5.138002

Batch 219090, train_perplexity=195.68373, train_loss=5.2764997

Batch 219100, train_perplexity=181.60721, train_loss=5.201846

Batch 219110, train_perplexity=178.39548, train_loss=5.184003

Batch 219120, train_perplexity=170.97359, train_loss=5.141509

Batch 219130, train_perplexity=176.57979, train_loss=5.173773

Batch 219140, train_perplexity=156.73222, train_loss=5.0545387

Batch 219150, train_perplexity=148.77402, train_loss=5.0024285

Batch 219160, train_perplexity=172.93123, train_loss=5.152894

Batch 219170, train_perplexity=178.02701, train_loss=5.1819353

Batch 219180, train_perplexity=155.57207, train_loss=5.047109

Batch 219190, train_perplexity=176.5217, train_loss=5.173444

Batch 219200, train_perplexity=190.1646, train_loss=5.24789

Batch 219210, train_perplexity=172.98979, train_loss=5.1532326

Batch 219220, train_perplexity=164.20212, train_loss=5.101098

Batch 219230, train_perplexity=172.82028, train_loss=5.152252

Batch 219240, train_perplexity=163.82109, train_loss=5.098775

Batch 219250, train_perplexity=185.58397, train_loss=5.2235074

Batch 219260, train_perplexity=178.62267, train_loss=5.1852756
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 219270, train_perplexity=180.73494, train_loss=5.1970315

Batch 219280, train_perplexity=176.20331, train_loss=5.1716385

Batch 219290, train_perplexity=167.18573, train_loss=5.1191053

Batch 219300, train_perplexity=163.32666, train_loss=5.0957522

Batch 219310, train_perplexity=168.92802, train_loss=5.1294727

Batch 219320, train_perplexity=163.84929, train_loss=5.098947

Batch 219330, train_perplexity=201.05019, train_loss=5.3035545

Batch 219340, train_perplexity=163.0281, train_loss=5.0939226

Batch 219350, train_perplexity=203.06578, train_loss=5.31353

Batch 219360, train_perplexity=214.17668, train_loss=5.3668013

Batch 219370, train_perplexity=167.23245, train_loss=5.119385

Batch 219380, train_perplexity=180.0225, train_loss=5.193082

Batch 219390, train_perplexity=183.70273, train_loss=5.213319

Batch 219400, train_perplexity=194.9582, train_loss=5.272785

Batch 219410, train_perplexity=157.16449, train_loss=5.057293

Batch 219420, train_perplexity=182.8048, train_loss=5.208419

Batch 219430, train_perplexity=166.2862, train_loss=5.1137104

Batch 219440, train_perplexity=178.90677, train_loss=5.186865

Batch 219450, train_perplexity=185.53362, train_loss=5.223236

Batch 219460, train_perplexity=167.50652, train_loss=5.121022

Batch 219470, train_perplexity=152.59541, train_loss=5.02779

Batch 219480, train_perplexity=165.48665, train_loss=5.1088905

Batch 219490, train_perplexity=169.4508, train_loss=5.1325626

Batch 219500, train_perplexity=170.56514, train_loss=5.1391172

Batch 219510, train_perplexity=175.06566, train_loss=5.165161

Batch 219520, train_perplexity=170.11159, train_loss=5.1364546

Batch 219530, train_perplexity=169.31268, train_loss=5.1317472

Batch 219540, train_perplexity=169.42244, train_loss=5.1323953

Batch 219550, train_perplexity=172.6748, train_loss=5.15141

Batch 219560, train_perplexity=177.77779, train_loss=5.1805344

Batch 219570, train_perplexity=181.62227, train_loss=5.201929

Batch 219580, train_perplexity=192.31639, train_loss=5.259142

Batch 219590, train_perplexity=174.28539, train_loss=5.160694

Batch 219600, train_perplexity=161.07861, train_loss=5.0818925

Batch 219610, train_perplexity=184.80077, train_loss=5.2192783

Batch 219620, train_perplexity=180.84277, train_loss=5.197628

Batch 219630, train_perplexity=184.52446, train_loss=5.217782

Batch 219640, train_perplexity=180.68788, train_loss=5.196771

Batch 219650, train_perplexity=191.20232, train_loss=5.253332

Batch 219660, train_perplexity=189.34709, train_loss=5.243582

Batch 219670, train_perplexity=163.99413, train_loss=5.0998306

Batch 219680, train_perplexity=163.16997, train_loss=5.0947924

Batch 219690, train_perplexity=154.14375, train_loss=5.0378857

Batch 219700, train_perplexity=176.50755, train_loss=5.1733637

Batch 219710, train_perplexity=166.37773, train_loss=5.1142607

Batch 219720, train_perplexity=167.48766, train_loss=5.1209097

Batch 219730, train_perplexity=173.0969, train_loss=5.1538515

Batch 219740, train_perplexity=178.36852, train_loss=5.1838517

Batch 219750, train_perplexity=168.36703, train_loss=5.1261463

Batch 219760, train_perplexity=157.1477, train_loss=5.057186

Batch 219770, train_perplexity=167.32864, train_loss=5.11996

Batch 219780, train_perplexity=193.53697, train_loss=5.2654686

Batch 219790, train_perplexity=183.02911, train_loss=5.2096453

Batch 219800, train_perplexity=190.08563, train_loss=5.2474747

Batch 219810, train_perplexity=178.15753, train_loss=5.182668

Batch 219820, train_perplexity=169.63715, train_loss=5.1336617

Batch 219830, train_perplexity=173.89772, train_loss=5.1584673

Batch 219840, train_perplexity=177.80687, train_loss=5.180698

Batch 219850, train_perplexity=164.82686, train_loss=5.1048956

Batch 219860, train_perplexity=171.62909, train_loss=5.1453357

Batch 219870, train_perplexity=169.27492, train_loss=5.131524

Batch 219880, train_perplexity=183.12349, train_loss=5.2101607

Batch 219890, train_perplexity=189.98468, train_loss=5.2469435

Batch 219900, train_perplexity=174.29286, train_loss=5.160737

Batch 219910, train_perplexity=168.3998, train_loss=5.126341

Batch 219920, train_perplexity=169.54779, train_loss=5.133135

Batch 219930, train_perplexity=177.52467, train_loss=5.1791096

Batch 219940, train_perplexity=167.2744, train_loss=5.1196356

Batch 219950, train_perplexity=178.16953, train_loss=5.1827354

Batch 219960, train_perplexity=165.1606, train_loss=5.1069183

Batch 219970, train_perplexity=167.40015, train_loss=5.120387

Batch 219980, train_perplexity=200.07852, train_loss=5.29871

Batch 219990, train_perplexity=177.81415, train_loss=5.180739

Batch 220000, train_perplexity=188.94681, train_loss=5.2414656

Batch 220010, train_perplexity=175.253, train_loss=5.1662307

Batch 220020, train_perplexity=166.34576, train_loss=5.1140685

Batch 220030, train_perplexity=181.7803, train_loss=5.202799

Batch 220040, train_perplexity=183.45264, train_loss=5.2119565

Batch 220050, train_perplexity=192.38042, train_loss=5.2594748

Batch 220060, train_perplexity=168.83589, train_loss=5.128927

Batch 220070, train_perplexity=176.44023, train_loss=5.172982

Batch 220080, train_perplexity=176.13443, train_loss=5.1712475

Batch 220090, train_perplexity=172.50626, train_loss=5.1504335

Batch 220100, train_perplexity=183.46759, train_loss=5.212038

Batch 220110, train_perplexity=154.5882, train_loss=5.040765

Batch 220120, train_perplexity=195.20322, train_loss=5.274041

Batch 220130, train_perplexity=155.53847, train_loss=5.046893

Batch 220140, train_perplexity=169.24327, train_loss=5.131337

Batch 220150, train_perplexity=153.6616, train_loss=5.034753

Batch 220160, train_perplexity=174.05875, train_loss=5.159393

Batch 220170, train_perplexity=175.68951, train_loss=5.1687183

Batch 220180, train_perplexity=162.12358, train_loss=5.088359

Batch 220190, train_perplexity=175.15802, train_loss=5.1656885

Batch 220200, train_perplexity=160.34041, train_loss=5.077299

Batch 220210, train_perplexity=178.65947, train_loss=5.1854815

Batch 220220, train_perplexity=193.9357, train_loss=5.2675266

Batch 220230, train_perplexity=153.5371, train_loss=5.033942

Batch 220240, train_perplexity=155.53053, train_loss=5.046842

Batch 220250, train_perplexity=173.39264, train_loss=5.1555586

Batch 220260, train_perplexity=171.18552, train_loss=5.142748

Batch 220270, train_perplexity=176.0758, train_loss=5.1709146

Batch 220280, train_perplexity=167.13678, train_loss=5.1188126

Batch 220290, train_perplexity=211.33652, train_loss=5.3534517

Batch 220300, train_perplexity=167.5602, train_loss=5.1213427

Batch 220310, train_perplexity=171.99834, train_loss=5.147485

Batch 220320, train_perplexity=163.10905, train_loss=5.094419

Batch 220330, train_perplexity=175.61029, train_loss=5.1682673

Batch 220340, train_perplexity=181.30557, train_loss=5.200184

Batch 220350, train_perplexity=139.75313, train_loss=4.9398775

Batch 220360, train_perplexity=193.40561, train_loss=5.2647896

Batch 220370, train_perplexity=159.21185, train_loss=5.0702357

Batch 220380, train_perplexity=197.05513, train_loss=5.2834835

Batch 220390, train_perplexity=158.26993, train_loss=5.064302

Batch 220400, train_perplexity=174.77657, train_loss=5.1635084

Batch 220410, train_perplexity=169.91766, train_loss=5.135314

Batch 220420, train_perplexity=154.21367, train_loss=5.038339

Batch 220430, train_perplexity=180.18791, train_loss=5.1940002

Batch 220440, train_perplexity=167.66602, train_loss=5.121974

Batch 220450, train_perplexity=181.32831, train_loss=5.2003093

Batch 220460, train_perplexity=187.46802, train_loss=5.2336082

Batch 220470, train_perplexity=181.6337, train_loss=5.201992

Batch 220480, train_perplexity=169.503, train_loss=5.1328707

Batch 220490, train_perplexity=164.19624, train_loss=5.1010623

Batch 220500, train_perplexity=164.96957, train_loss=5.105761

Batch 220510, train_perplexity=166.15709, train_loss=5.1129336

Batch 220520, train_perplexity=185.38911, train_loss=5.222457

Batch 220530, train_perplexity=178.21744, train_loss=5.1830044

Batch 220540, train_perplexity=162.52824, train_loss=5.090852

Batch 220550, train_perplexity=186.70474, train_loss=5.2295284

Batch 220560, train_perplexity=179.09608, train_loss=5.1879225
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 220570, train_perplexity=181.96936, train_loss=5.2038383

Batch 220580, train_perplexity=155.22543, train_loss=5.0448785

Batch 220590, train_perplexity=180.21454, train_loss=5.194148

Batch 220600, train_perplexity=193.77892, train_loss=5.266718

Batch 220610, train_perplexity=170.31952, train_loss=5.1376762

Batch 220620, train_perplexity=165.8037, train_loss=5.1108046

Batch 220630, train_perplexity=173.97585, train_loss=5.1589165

Batch 220640, train_perplexity=182.13135, train_loss=5.204728

Batch 220650, train_perplexity=172.73839, train_loss=5.151778

Batch 220660, train_perplexity=163.13402, train_loss=5.094572

Batch 220670, train_perplexity=176.12971, train_loss=5.171221

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Batch 220680, train_perplexity=177.79712, train_loss=5.180643

Batch 220690, train_perplexity=178.08508, train_loss=5.1822615

Batch 220700, train_perplexity=161.47043, train_loss=5.084322

Batch 220710, train_perplexity=174.69308, train_loss=5.1630306

Batch 220720, train_perplexity=177.83884, train_loss=5.1808777

Batch 220730, train_perplexity=167.27264, train_loss=5.119625

Batch 220740, train_perplexity=169.31721, train_loss=5.131774

Batch 220750, train_perplexity=178.40459, train_loss=5.184054

Batch 220760, train_perplexity=202.70628, train_loss=5.311758

Batch 220770, train_perplexity=179.52565, train_loss=5.190318

Batch 220780, train_perplexity=164.73257, train_loss=5.1043234

Batch 220790, train_perplexity=202.52899, train_loss=5.310883

Batch 220800, train_perplexity=165.74426, train_loss=5.110446

Batch 220810, train_perplexity=164.2657, train_loss=5.1014853

Batch 220820, train_perplexity=167.45253, train_loss=5.1207

Batch 220830, train_perplexity=183.40794, train_loss=5.211713

Batch 220840, train_perplexity=187.70757, train_loss=5.234885

Batch 220850, train_perplexity=168.57944, train_loss=5.127407

Batch 220860, train_perplexity=175.68423, train_loss=5.1686883

Batch 220870, train_perplexity=159.1025, train_loss=5.0695486

Batch 220880, train_perplexity=179.33469, train_loss=5.189254

Batch 220890, train_perplexity=160.32106, train_loss=5.0771785

Batch 220900, train_perplexity=183.1765, train_loss=5.21045

Batch 220910, train_perplexity=169.77197, train_loss=5.134456

Batch 220920, train_perplexity=184.1602, train_loss=5.215806

Batch 220930, train_perplexity=181.09648, train_loss=5.19903

Batch 220940, train_perplexity=194.89342, train_loss=5.272453

Batch 220950, train_perplexity=181.89719, train_loss=5.2034416

Batch 220960, train_perplexity=168.05855, train_loss=5.1243124

Batch 220970, train_perplexity=175.26863, train_loss=5.16632

Batch 220980, train_perplexity=174.49693, train_loss=5.161907

Batch 220990, train_perplexity=183.05093, train_loss=5.2097645

Batch 221000, train_perplexity=178.24779, train_loss=5.1831746

Batch 221010, train_perplexity=160.97565, train_loss=5.081253

Batch 221020, train_perplexity=166.28334, train_loss=5.113693

Batch 221030, train_perplexity=193.22198, train_loss=5.2638397

Batch 221040, train_perplexity=172.76622, train_loss=5.1519394

Batch 221050, train_perplexity=171.83922, train_loss=5.1465592

Batch 221060, train_perplexity=171.7912, train_loss=5.14628

Batch 221070, train_perplexity=162.54002, train_loss=5.0909243

Batch 221080, train_perplexity=160.82234, train_loss=5.0803003

Batch 221090, train_perplexity=181.07161, train_loss=5.1988926

Batch 221100, train_perplexity=166.88847, train_loss=5.117326

Batch 221110, train_perplexity=179.52257, train_loss=5.190301

Batch 221120, train_perplexity=167.6047, train_loss=5.1216083

Batch 221130, train_perplexity=176.02846, train_loss=5.1706457

Batch 221140, train_perplexity=179.32323, train_loss=5.18919

Batch 221150, train_perplexity=191.01952, train_loss=5.2523756

Batch 221160, train_perplexity=173.9829, train_loss=5.158957

Batch 221170, train_perplexity=177.69524, train_loss=5.18007

Batch 221180, train_perplexity=154.40608, train_loss=5.039586

Batch 221190, train_perplexity=165.9706, train_loss=5.1118107

Batch 221200, train_perplexity=183.532, train_loss=5.212389

Batch 221210, train_perplexity=178.48175, train_loss=5.1844864

Batch 221220, train_perplexity=171.6407, train_loss=5.1454034

Batch 221230, train_perplexity=174.50194, train_loss=5.161936

Batch 221240, train_perplexity=168.01823, train_loss=5.1240726

Batch 221250, train_perplexity=166.12785, train_loss=5.1127577

Batch 221260, train_perplexity=168.34247, train_loss=5.1260004

Batch 221270, train_perplexity=165.2545, train_loss=5.1074867

Batch 221280, train_perplexity=167.19147, train_loss=5.1191397

Batch 221290, train_perplexity=181.85062, train_loss=5.2031856

Batch 221300, train_perplexity=205.94263, train_loss=5.3275976

Batch 221310, train_perplexity=179.64717, train_loss=5.1909947

Batch 221320, train_perplexity=171.7256, train_loss=5.145898

Batch 221330, train_perplexity=196.07697, train_loss=5.278507

Batch 221340, train_perplexity=165.23378, train_loss=5.1073613

Batch 221350, train_perplexity=181.17975, train_loss=5.1994896

Batch 221360, train_perplexity=170.40247, train_loss=5.138163

Batch 221370, train_perplexity=169.57617, train_loss=5.133302

Batch 221380, train_perplexity=166.6712, train_loss=5.116023

Batch 221390, train_perplexity=178.16917, train_loss=5.1827335

Batch 221400, train_perplexity=177.83493, train_loss=5.1808558

Batch 221410, train_perplexity=171.731, train_loss=5.1459293

Batch 221420, train_perplexity=183.46147, train_loss=5.2120047

Batch 221430, train_perplexity=185.86859, train_loss=5.22504

Batch 221440, train_perplexity=162.92941, train_loss=5.093317

Batch 221450, train_perplexity=174.95685, train_loss=5.1645393

Batch 221460, train_perplexity=167.88098, train_loss=5.1232553

Batch 221470, train_perplexity=165.56479, train_loss=5.1093626

Batch 221480, train_perplexity=157.98393, train_loss=5.0624933

Batch 221490, train_perplexity=167.89835, train_loss=5.1233587

Batch 221500, train_perplexity=175.06416, train_loss=5.1651525

Batch 221510, train_perplexity=176.55183, train_loss=5.1736145

Batch 221520, train_perplexity=165.59938, train_loss=5.1095715

Batch 221530, train_perplexity=183.01184, train_loss=5.209551

Batch 221540, train_perplexity=168.32994, train_loss=5.125926

Batch 221550, train_perplexity=174.97662, train_loss=5.1646523

Batch 221560, train_perplexity=160.20163, train_loss=5.076433

Batch 221570, train_perplexity=191.9297, train_loss=5.257129

Batch 221580, train_perplexity=175.36469, train_loss=5.1668677

Batch 221590, train_perplexity=160.62332, train_loss=5.079062

Batch 221600, train_perplexity=163.90907, train_loss=5.099312

Batch 221610, train_perplexity=197.654, train_loss=5.286518

Batch 221620, train_perplexity=186.13405, train_loss=5.226467

Batch 221630, train_perplexity=156.9232, train_loss=5.0557566

Batch 221640, train_perplexity=163.41461, train_loss=5.0962906

Batch 221650, train_perplexity=164.44783, train_loss=5.1025934

Batch 221660, train_perplexity=188.6374, train_loss=5.2398267

Batch 221670, train_perplexity=157.65446, train_loss=5.0604057

Batch 221680, train_perplexity=179.5117, train_loss=5.1902404

Batch 221690, train_perplexity=172.49985, train_loss=5.1503963

Batch 221700, train_perplexity=166.83644, train_loss=5.117014

Batch 221710, train_perplexity=178.34367, train_loss=5.1837125

Batch 221720, train_perplexity=154.78181, train_loss=5.0420165

Batch 221730, train_perplexity=181.91576, train_loss=5.2035437

Batch 221740, train_perplexity=175.68634, train_loss=5.1687

Batch 221750, train_perplexity=163.88477, train_loss=5.0991635

Batch 221760, train_perplexity=181.34439, train_loss=5.200398

Batch 221770, train_perplexity=175.05156, train_loss=5.1650805

Batch 221780, train_perplexity=185.93819, train_loss=5.2254143

Batch 221790, train_perplexity=182.02838, train_loss=5.2041626

Batch 221800, train_perplexity=186.01515, train_loss=5.225828
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 221810, train_perplexity=186.20259, train_loss=5.2268353

Batch 221820, train_perplexity=161.23868, train_loss=5.0828857

Batch 221830, train_perplexity=161.94147, train_loss=5.087235

Batch 221840, train_perplexity=179.49954, train_loss=5.1901727

Batch 221850, train_perplexity=190.77083, train_loss=5.251073

Batch 221860, train_perplexity=169.0905, train_loss=5.130434

Batch 221870, train_perplexity=187.62935, train_loss=5.2344685

Batch 221880, train_perplexity=177.19315, train_loss=5.1772404

Batch 221890, train_perplexity=188.60889, train_loss=5.2396755

Batch 221900, train_perplexity=183.17162, train_loss=5.2104235

Batch 221910, train_perplexity=171.98193, train_loss=5.1473894

Batch 221920, train_perplexity=152.10245, train_loss=5.0245543

Batch 221930, train_perplexity=171.65486, train_loss=5.145486

Batch 221940, train_perplexity=177.37009, train_loss=5.1782384

Batch 221950, train_perplexity=181.7628, train_loss=5.2027025

Batch 221960, train_perplexity=191.16621, train_loss=5.2531433

Batch 221970, train_perplexity=158.91962, train_loss=5.0683985

Batch 221980, train_perplexity=170.46416, train_loss=5.138525

Batch 221990, train_perplexity=172.13666, train_loss=5.1482887

Batch 222000, train_perplexity=176.65323, train_loss=5.1741886

Batch 222010, train_perplexity=158.08023, train_loss=5.0631027

Batch 222020, train_perplexity=174.00986, train_loss=5.159112

Batch 222030, train_perplexity=176.314, train_loss=5.1722665

Batch 222040, train_perplexity=197.47446, train_loss=5.2856092

Batch 222050, train_perplexity=187.41403, train_loss=5.23332

Batch 222060, train_perplexity=185.86531, train_loss=5.2250223

Batch 222070, train_perplexity=168.11209, train_loss=5.124631

Batch 222080, train_perplexity=178.28578, train_loss=5.1833878

Batch 222090, train_perplexity=167.43752, train_loss=5.12061

Batch 222100, train_perplexity=153.23459, train_loss=5.03197

Batch 222110, train_perplexity=162.31642, train_loss=5.0895476

Batch 222120, train_perplexity=183.32208, train_loss=5.2112446

Batch 222130, train_perplexity=175.0872, train_loss=5.165284

Batch 222140, train_perplexity=190.32153, train_loss=5.248715

Batch 222150, train_perplexity=167.09384, train_loss=5.1185555

Batch 222160, train_perplexity=162.59552, train_loss=5.0912657

Batch 222170, train_perplexity=180.79665, train_loss=5.197373

Batch 222180, train_perplexity=166.41019, train_loss=5.1144557

Batch 222190, train_perplexity=178.46985, train_loss=5.1844196

Batch 222200, train_perplexity=168.29318, train_loss=5.1257076

Batch 222210, train_perplexity=167.2996, train_loss=5.1197863

Batch 222220, train_perplexity=168.05838, train_loss=5.1243114

Batch 222230, train_perplexity=168.25772, train_loss=5.125497

Batch 222240, train_perplexity=201.24124, train_loss=5.3045044

Batch 222250, train_perplexity=194.65742, train_loss=5.271241

Batch 222260, train_perplexity=173.3642, train_loss=5.1553946

Batch 222270, train_perplexity=183.50504, train_loss=5.212242

Batch 222280, train_perplexity=168.26479, train_loss=5.125539

Batch 222290, train_perplexity=179.1453, train_loss=5.188197

Batch 222300, train_perplexity=146.93687, train_loss=4.990003

Batch 222310, train_perplexity=155.1928, train_loss=5.044668

Batch 222320, train_perplexity=173.16978, train_loss=5.1542726

Batch 222330, train_perplexity=174.90588, train_loss=5.164248

Batch 222340, train_perplexity=161.87036, train_loss=5.086796

Batch 222350, train_perplexity=181.2952, train_loss=5.2001266

Batch 222360, train_perplexity=178.545, train_loss=5.1848407

Batch 222370, train_perplexity=190.51839, train_loss=5.2497487

Batch 222380, train_perplexity=177.48032, train_loss=5.1788597

Batch 222390, train_perplexity=171.0075, train_loss=5.1417074

Batch 222400, train_perplexity=169.97633, train_loss=5.135659

Batch 222410, train_perplexity=154.05669, train_loss=5.0373206

Batch 222420, train_perplexity=179.9681, train_loss=5.1927795

Batch 222430, train_perplexity=167.50444, train_loss=5.12101

Batch 222440, train_perplexity=167.26619, train_loss=5.1195865

Batch 222450, train_perplexity=170.31157, train_loss=5.1376295

Batch 222460, train_perplexity=188.9824, train_loss=5.241654

Batch 222470, train_perplexity=172.33032, train_loss=5.149413

Batch 222480, train_perplexity=168.61786, train_loss=5.127635

Batch 222490, train_perplexity=163.1264, train_loss=5.0945253

Batch 222500, train_perplexity=176.89632, train_loss=5.175564

Batch 222510, train_perplexity=158.48508, train_loss=5.0656605

Batch 222520, train_perplexity=174.78516, train_loss=5.1635575

Batch 222530, train_perplexity=191.49557, train_loss=5.2548647

Batch 222540, train_perplexity=186.47333, train_loss=5.228288

Batch 222550, train_perplexity=195.33507, train_loss=5.2747164

Batch 222560, train_perplexity=176.56952, train_loss=5.1737146

Batch 222570, train_perplexity=179.95135, train_loss=5.1926866

Batch 222580, train_perplexity=163.24638, train_loss=5.0952606

Batch 222590, train_perplexity=198.08936, train_loss=5.288718

Batch 222600, train_perplexity=165.09863, train_loss=5.106543

Batch 222610, train_perplexity=142.41522, train_loss=4.958747

Batch 222620, train_perplexity=183.35617, train_loss=5.2114305

Batch 222630, train_perplexity=182.79459, train_loss=5.208363

Batch 222640, train_perplexity=179.09473, train_loss=5.187915

Batch 222650, train_perplexity=180.47717, train_loss=5.1956043

Batch 222660, train_perplexity=171.51765, train_loss=5.144686

Batch 222670, train_perplexity=174.9862, train_loss=5.164707

Batch 222680, train_perplexity=161.48767, train_loss=5.084429

Batch 222690, train_perplexity=165.16391, train_loss=5.1069384

Batch 222700, train_perplexity=166.24593, train_loss=5.113468

Batch 222710, train_perplexity=179.81609, train_loss=5.1919346

Batch 222720, train_perplexity=187.68161, train_loss=5.234747

Batch 222730, train_perplexity=177.30235, train_loss=5.1778564

Batch 222740, train_perplexity=176.21893, train_loss=5.171727

Batch 222750, train_perplexity=179.47516, train_loss=5.190037

Batch 222760, train_perplexity=153.0951, train_loss=5.0310593

Batch 222770, train_perplexity=170.1171, train_loss=5.136487

Batch 222780, train_perplexity=176.09428, train_loss=5.1710196

Batch 222790, train_perplexity=166.00536, train_loss=5.11202

Batch 222800, train_perplexity=174.23886, train_loss=5.160427

Batch 222810, train_perplexity=163.36086, train_loss=5.0959616

Batch 222820, train_perplexity=175.44456, train_loss=5.167323

Batch 222830, train_perplexity=177.73575, train_loss=5.180298

Batch 222840, train_perplexity=165.33443, train_loss=5.10797

Batch 222850, train_perplexity=166.13632, train_loss=5.1128087

Batch 222860, train_perplexity=180.51074, train_loss=5.1957903

Batch 222870, train_perplexity=162.8651, train_loss=5.092922

Batch 222880, train_perplexity=154.69402, train_loss=5.041449

Batch 222890, train_perplexity=168.77472, train_loss=5.128565

Batch 222900, train_perplexity=180.10063, train_loss=5.193516

Batch 222910, train_perplexity=189.63731, train_loss=5.2451134

Batch 222920, train_perplexity=179.3854, train_loss=5.1895366

Batch 222930, train_perplexity=154.22581, train_loss=5.038418

Batch 222940, train_perplexity=182.34181, train_loss=5.205883

Batch 222950, train_perplexity=201.63861, train_loss=5.306477

Batch 222960, train_perplexity=151.95253, train_loss=5.023568

Batch 222970, train_perplexity=161.1764, train_loss=5.0824995

Batch 222980, train_perplexity=162.66609, train_loss=5.0916996

Batch 222990, train_perplexity=172.87418, train_loss=5.152564

Batch 223000, train_perplexity=191.73404, train_loss=5.256109

Batch 223010, train_perplexity=180.46823, train_loss=5.1955547

Batch 223020, train_perplexity=172.52666, train_loss=5.150552

Batch 223030, train_perplexity=180.30875, train_loss=5.1946707

Batch 223040, train_perplexity=183.4977, train_loss=5.212202

Batch 223050, train_perplexity=178.64626, train_loss=5.1854076

Batch 223060, train_perplexity=170.52138, train_loss=5.1388607

Batch 223070, train_perplexity=181.56477, train_loss=5.2016125

Batch 223080, train_perplexity=165.82253, train_loss=5.110918

Batch 223090, train_perplexity=177.08647, train_loss=5.176638

Batch 223100, train_perplexity=176.06506, train_loss=5.1708536
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 223110, train_perplexity=165.5572, train_loss=5.109317

Batch 223120, train_perplexity=175.16211, train_loss=5.165712

Batch 223130, train_perplexity=180.63077, train_loss=5.196455

Batch 223140, train_perplexity=178.40994, train_loss=5.184084

Batch 223150, train_perplexity=181.35625, train_loss=5.2004633

Batch 223160, train_perplexity=168.0769, train_loss=5.1244216

Batch 223170, train_perplexity=190.83998, train_loss=5.2514353

Batch 223180, train_perplexity=169.61734, train_loss=5.133545

Batch 223190, train_perplexity=171.22365, train_loss=5.1429706

Batch 223200, train_perplexity=171.91667, train_loss=5.14701

Batch 223210, train_perplexity=148.53514, train_loss=5.0008216

Batch 223220, train_perplexity=155.0218, train_loss=5.0435658

Batch 223230, train_perplexity=159.0654, train_loss=5.0693154

Batch 223240, train_perplexity=165.98738, train_loss=5.111912

Batch 223250, train_perplexity=174.50143, train_loss=5.161933

Batch 223260, train_perplexity=183.02939, train_loss=5.2096467

Batch 223270, train_perplexity=164.65514, train_loss=5.103853

Batch 223280, train_perplexity=175.92995, train_loss=5.170086

Batch 223290, train_perplexity=167.19698, train_loss=5.1191726

Batch 223300, train_perplexity=163.02585, train_loss=5.093909

Batch 223310, train_perplexity=155.97801, train_loss=5.049715

Batch 223320, train_perplexity=178.12993, train_loss=5.182513

Batch 223330, train_perplexity=186.31592, train_loss=5.2274437

Batch 223340, train_perplexity=187.62022, train_loss=5.23442

Batch 223350, train_perplexity=169.7005, train_loss=5.134035

Batch 223360, train_perplexity=190.88011, train_loss=5.2516456

Batch 223370, train_perplexity=158.49725, train_loss=5.0657372

Batch 223380, train_perplexity=176.59714, train_loss=5.173871

Batch 223390, train_perplexity=172.19135, train_loss=5.1486063

Batch 223400, train_perplexity=165.08989, train_loss=5.10649

Batch 223410, train_perplexity=173.53342, train_loss=5.15637

Batch 223420, train_perplexity=160.82771, train_loss=5.0803337

Batch 223430, train_perplexity=185.29295, train_loss=5.221938

Batch 223440, train_perplexity=164.2335, train_loss=5.1012893

Batch 223450, train_perplexity=166.75182, train_loss=5.1165066

Batch 223460, train_perplexity=180.1679, train_loss=5.193889

Batch 223470, train_perplexity=168.34904, train_loss=5.1260395

Batch 223480, train_perplexity=169.99513, train_loss=5.13577

Batch 223490, train_perplexity=165.74536, train_loss=5.1104527

Batch 223500, train_perplexity=173.2305, train_loss=5.154623

Batch 223510, train_perplexity=181.02827, train_loss=5.198653

Batch 223520, train_perplexity=162.50919, train_loss=5.0907345

Batch 223530, train_perplexity=189.90715, train_loss=5.2465353

Batch 223540, train_perplexity=175.43268, train_loss=5.1672554

Batch 223550, train_perplexity=174.92014, train_loss=5.1643295

Batch 223560, train_perplexity=196.50415, train_loss=5.2806835

Batch 223570, train_perplexity=170.20221, train_loss=5.136987

Batch 223580, train_perplexity=167.09535, train_loss=5.1185646

Batch 223590, train_perplexity=168.25034, train_loss=5.125453

Batch 223600, train_perplexity=192.40353, train_loss=5.259595

Batch 223610, train_perplexity=174.90546, train_loss=5.1642456

Batch 223620, train_perplexity=179.17194, train_loss=5.188346

Batch 223630, train_perplexity=179.00687, train_loss=5.187424

Batch 223640, train_perplexity=165.51016, train_loss=5.1090326

Batch 223650, train_perplexity=167.04436, train_loss=5.1182594

Batch 223660, train_perplexity=175.29187, train_loss=5.1664524

Batch 223670, train_perplexity=165.07022, train_loss=5.106371

Batch 223680, train_perplexity=176.46278, train_loss=5.17311

Batch 223690, train_perplexity=169.20293, train_loss=5.1310987

Batch 223700, train_perplexity=160.51811, train_loss=5.078407

Batch 223710, train_perplexity=173.8561, train_loss=5.158228

Batch 223720, train_perplexity=183.56998, train_loss=5.212596

Batch 223730, train_perplexity=180.13448, train_loss=5.1937037

Batch 223740, train_perplexity=174.95534, train_loss=5.1645308

Batch 223750, train_perplexity=176.03317, train_loss=5.1706724

Batch 223760, train_perplexity=170.32253, train_loss=5.137694

Batch 223770, train_perplexity=164.2574, train_loss=5.1014347

Batch 223780, train_perplexity=177.26785, train_loss=5.177662

Batch 223790, train_perplexity=153.52861, train_loss=5.033887

Batch 223800, train_perplexity=175.81473, train_loss=5.1694307

Batch 223810, train_perplexity=174.96703, train_loss=5.1645975

Batch 223820, train_perplexity=173.28998, train_loss=5.1549664

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Batch 223830, train_perplexity=183.0232, train_loss=5.209613

Batch 223840, train_perplexity=167.84184, train_loss=5.123022

Batch 223850, train_perplexity=186.97139, train_loss=5.2309556

Batch 223860, train_perplexity=158.9989, train_loss=5.0688972

Batch 223870, train_perplexity=163.3875, train_loss=5.0961246

Batch 223880, train_perplexity=181.0958, train_loss=5.199026

Batch 223890, train_perplexity=176.01704, train_loss=5.170581

Batch 223900, train_perplexity=175.64043, train_loss=5.168439

Batch 223910, train_perplexity=161.94154, train_loss=5.0872355

Batch 223920, train_perplexity=167.55109, train_loss=5.1212883

Batch 223930, train_perplexity=184.47942, train_loss=5.217538

Batch 223940, train_perplexity=151.27823, train_loss=5.0191207

Batch 223950, train_perplexity=169.28493, train_loss=5.131583

Batch 223960, train_perplexity=165.51111, train_loss=5.1090384

Batch 223970, train_perplexity=181.703, train_loss=5.2023735

Batch 223980, train_perplexity=164.0797, train_loss=5.1003523

Batch 223990, train_perplexity=191.71988, train_loss=5.2560353

Batch 224000, train_perplexity=184.9316, train_loss=5.219986

Batch 224010, train_perplexity=194.71887, train_loss=5.271557

Batch 224020, train_perplexity=168.57509, train_loss=5.1273813

Batch 224030, train_perplexity=153.17133, train_loss=5.031557

Batch 224040, train_perplexity=158.4038, train_loss=5.0651474

Batch 224050, train_perplexity=166.09213, train_loss=5.1125426

Batch 224060, train_perplexity=171.77687, train_loss=5.1461964

Batch 224070, train_perplexity=191.75708, train_loss=5.2562294

Batch 224080, train_perplexity=164.54291, train_loss=5.1031713

Batch 224090, train_perplexity=171.49043, train_loss=5.1445274

Batch 224100, train_perplexity=173.56833, train_loss=5.1565714

Batch 224110, train_perplexity=191.92448, train_loss=5.257102

Batch 224120, train_perplexity=185.65796, train_loss=5.223906

Batch 224130, train_perplexity=164.8311, train_loss=5.1049213

Batch 224140, train_perplexity=178.17657, train_loss=5.182775

Batch 224150, train_perplexity=171.05325, train_loss=5.141975

Batch 224160, train_perplexity=188.97745, train_loss=5.2416277

Batch 224170, train_perplexity=166.07019, train_loss=5.1124105

Batch 224180, train_perplexity=179.10558, train_loss=5.1879754

Batch 224190, train_perplexity=202.89418, train_loss=5.3126845

Batch 224200, train_perplexity=176.34477, train_loss=5.172441

Batch 224210, train_perplexity=165.27452, train_loss=5.107608

Batch 224220, train_perplexity=156.55788, train_loss=5.053426

Batch 224230, train_perplexity=177.72168, train_loss=5.1802187

Batch 224240, train_perplexity=143.26184, train_loss=4.964674

Batch 224250, train_perplexity=191.34497, train_loss=5.254078

Batch 224260, train_perplexity=172.12633, train_loss=5.1482286

Batch 224270, train_perplexity=175.78656, train_loss=5.1692705

Batch 224280, train_perplexity=178.86037, train_loss=5.1866055

Batch 224290, train_perplexity=170.66797, train_loss=5.13972

Batch 224300, train_perplexity=171.98267, train_loss=5.1473937

Batch 224310, train_perplexity=169.5389, train_loss=5.1330824

Batch 224320, train_perplexity=172.41367, train_loss=5.1498966

Batch 224330, train_perplexity=156.87369, train_loss=5.055441

Batch 224340, train_perplexity=151.15428, train_loss=5.018301
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 224350, train_perplexity=179.29877, train_loss=5.1890535

Batch 224360, train_perplexity=172.93263, train_loss=5.152902

Batch 224370, train_perplexity=166.04494, train_loss=5.1122584

Batch 224380, train_perplexity=165.70459, train_loss=5.1102066

Batch 224390, train_perplexity=181.29883, train_loss=5.2001467

Batch 224400, train_perplexity=156.32753, train_loss=5.0519533

Batch 224410, train_perplexity=189.99892, train_loss=5.2470183

Batch 224420, train_perplexity=176.19768, train_loss=5.1716065

Batch 224430, train_perplexity=169.54747, train_loss=5.133133

Batch 224440, train_perplexity=181.51353, train_loss=5.20133

Batch 224450, train_perplexity=161.39668, train_loss=5.083865

Batch 224460, train_perplexity=174.25116, train_loss=5.1604977

Batch 224470, train_perplexity=161.25275, train_loss=5.082973

Batch 224480, train_perplexity=174.6646, train_loss=5.1628675

Batch 224490, train_perplexity=169.25974, train_loss=5.1314344

Batch 224500, train_perplexity=175.21532, train_loss=5.1660156

Batch 224510, train_perplexity=172.98749, train_loss=5.153219

Batch 224520, train_perplexity=164.70932, train_loss=5.1041822

Batch 224530, train_perplexity=162.58095, train_loss=5.091176

Batch 224540, train_perplexity=180.3183, train_loss=5.1947236

Batch 224550, train_perplexity=160.31082, train_loss=5.0771146

Batch 224560, train_perplexity=166.96664, train_loss=5.117794

Batch 224570, train_perplexity=191.54214, train_loss=5.255108

Batch 224580, train_perplexity=164.37862, train_loss=5.1021724

Batch 224590, train_perplexity=144.13727, train_loss=4.970766

Batch 224600, train_perplexity=179.72737, train_loss=5.191441

Batch 224610, train_perplexity=168.01086, train_loss=5.1240287

Batch 224620, train_perplexity=175.96074, train_loss=5.170261

Batch 224630, train_perplexity=190.19553, train_loss=5.2480526

Batch 224640, train_perplexity=170.2562, train_loss=5.1373043

Batch 224650, train_perplexity=176.31694, train_loss=5.172283

Batch 224660, train_perplexity=181.0067, train_loss=5.198534

Batch 224670, train_perplexity=179.33469, train_loss=5.189254

Batch 224680, train_perplexity=190.94257, train_loss=5.2519727

Batch 224690, train_perplexity=168.22435, train_loss=5.1252985

Batch 224700, train_perplexity=182.0308, train_loss=5.204176

Batch 224710, train_perplexity=173.15005, train_loss=5.1541586

Batch 224720, train_perplexity=164.65546, train_loss=5.103855

Batch 224730, train_perplexity=164.93567, train_loss=5.1055555

Batch 224740, train_perplexity=187.18307, train_loss=5.232087

Batch 224750, train_perplexity=186.11151, train_loss=5.226346

Batch 224760, train_perplexity=160.92499, train_loss=5.0809383

Batch 224770, train_perplexity=194.86461, train_loss=5.272305

Batch 224780, train_perplexity=196.8534, train_loss=5.2824593

Batch 224790, train_perplexity=163.13184, train_loss=5.0945587

Batch 224800, train_perplexity=179.37514, train_loss=5.1894794

Batch 224810, train_perplexity=180.47752, train_loss=5.195606

Batch 224820, train_perplexity=185.82349, train_loss=5.2247972

Batch 224830, train_perplexity=188.43073, train_loss=5.2387304

Batch 224840, train_perplexity=158.93794, train_loss=5.068514

Batch 224850, train_perplexity=168.66724, train_loss=5.127928

Batch 224860, train_perplexity=184.63852, train_loss=5.2184

Batch 224870, train_perplexity=161.06808, train_loss=5.081827

Batch 224880, train_perplexity=171.05986, train_loss=5.1420135

Batch 224890, train_perplexity=162.04027, train_loss=5.087845

Batch 224900, train_perplexity=164.13965, train_loss=5.1007175

Batch 224910, train_perplexity=171.87044, train_loss=5.146741

Batch 224920, train_perplexity=185.29675, train_loss=5.2219586

Batch 224930, train_perplexity=166.47153, train_loss=5.1148243

Batch 224940, train_perplexity=181.01222, train_loss=5.1985645

Batch 224950, train_perplexity=163.61687, train_loss=5.0975275

Batch 224960, train_perplexity=180.19186, train_loss=5.194022

Batch 224970, train_perplexity=166.22191, train_loss=5.1133237

Batch 224980, train_perplexity=185.59015, train_loss=5.223541

Batch 224990, train_perplexity=172.20563, train_loss=5.1486893

Batch 225000, train_perplexity=170.77559, train_loss=5.1403503

Batch 225010, train_perplexity=164.18192, train_loss=5.100975

Batch 225020, train_perplexity=160.64836, train_loss=5.079218

Batch 225030, train_perplexity=165.70308, train_loss=5.1101975

Batch 225040, train_perplexity=169.90633, train_loss=5.135247

Batch 225050, train_perplexity=179.87277, train_loss=5.19225

Batch 225060, train_perplexity=177.21638, train_loss=5.1773715

Batch 225070, train_perplexity=182.91832, train_loss=5.2090397

Batch 225080, train_perplexity=165.74205, train_loss=5.1104326

Batch 225090, train_perplexity=186.17195, train_loss=5.2266707

Batch 225100, train_perplexity=178.45367, train_loss=5.184329

Batch 225110, train_perplexity=191.56499, train_loss=5.255227

Batch 225120, train_perplexity=182.50465, train_loss=5.2067757

Batch 225130, train_perplexity=177.75693, train_loss=5.180417

Batch 225140, train_perplexity=190.47487, train_loss=5.2495203

Batch 225150, train_perplexity=167.3213, train_loss=5.119916

Batch 225160, train_perplexity=167.32362, train_loss=5.11993

Batch 225170, train_perplexity=161.72124, train_loss=5.085874

Batch 225180, train_perplexity=163.25674, train_loss=5.095324

Batch 225190, train_perplexity=177.89192, train_loss=5.181176

Batch 225200, train_perplexity=159.28453, train_loss=5.070692

Batch 225210, train_perplexity=167.34668, train_loss=5.1200676

Batch 225220, train_perplexity=166.762, train_loss=5.1165676

Batch 225230, train_perplexity=167.61789, train_loss=5.121687

Batch 225240, train_perplexity=166.49463, train_loss=5.114963

Batch 225250, train_perplexity=170.89256, train_loss=5.141035

Batch 225260, train_perplexity=160.06418, train_loss=5.075575

Batch 225270, train_perplexity=178.67761, train_loss=5.185583

Batch 225280, train_perplexity=170.85443, train_loss=5.140812

Batch 225290, train_perplexity=170.21463, train_loss=5.13706

Batch 225300, train_perplexity=164.63661, train_loss=5.1037407

Batch 225310, train_perplexity=165.85905, train_loss=5.1111383

Batch 225320, train_perplexity=179.25415, train_loss=5.1888046

Batch 225330, train_perplexity=164.64847, train_loss=5.1038127

Batch 225340, train_perplexity=163.23315, train_loss=5.0951796

Batch 225350, train_perplexity=181.72284, train_loss=5.2024827

Batch 225360, train_perplexity=167.33336, train_loss=5.119988

Batch 225370, train_perplexity=168.13853, train_loss=5.1247883

Batch 225380, train_perplexity=144.11871, train_loss=4.9706373

Batch 225390, train_perplexity=182.42061, train_loss=5.206315

Batch 225400, train_perplexity=175.67796, train_loss=5.1686525

Batch 225410, train_perplexity=187.21547, train_loss=5.23226

Batch 225420, train_perplexity=163.40932, train_loss=5.096258

Batch 225430, train_perplexity=178.9177, train_loss=5.186926

Batch 225440, train_perplexity=172.99251, train_loss=5.1532483

Batch 225450, train_perplexity=167.3083, train_loss=5.119838

Batch 225460, train_perplexity=153.9428, train_loss=5.036581

Batch 225470, train_perplexity=152.88557, train_loss=5.02969

Batch 225480, train_perplexity=181.96677, train_loss=5.203824

Batch 225490, train_perplexity=156.83284, train_loss=5.0551805

Batch 225500, train_perplexity=174.8691, train_loss=5.1640377

Batch 225510, train_perplexity=150.84453, train_loss=5.0162497

Batch 225520, train_perplexity=185.22485, train_loss=5.2215705

Batch 225530, train_perplexity=169.10695, train_loss=5.1305313

Batch 225540, train_perplexity=147.94386, train_loss=4.996833

Batch 225550, train_perplexity=165.56053, train_loss=5.109337

Batch 225560, train_perplexity=149.2202, train_loss=5.005423

Batch 225570, train_perplexity=154.93991, train_loss=5.0430374

Batch 225580, train_perplexity=174.459, train_loss=5.1616898

Batch 225590, train_perplexity=173.75117, train_loss=5.1576242

Batch 225600, train_perplexity=164.19952, train_loss=5.1010823

Batch 225610, train_perplexity=173.34782, train_loss=5.1553

Batch 225620, train_perplexity=182.59535, train_loss=5.2072725

Batch 225630, train_perplexity=180.79121, train_loss=5.197343

Batch 225640, train_perplexity=177.00484, train_loss=5.176177
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 225650, train_perplexity=184.40985, train_loss=5.2171607

Batch 225660, train_perplexity=172.32704, train_loss=5.149394

Batch 225670, train_perplexity=171.76189, train_loss=5.146109

Batch 225680, train_perplexity=166.96393, train_loss=5.117778

Batch 225690, train_perplexity=183.22176, train_loss=5.210697

Batch 225700, train_perplexity=185.62529, train_loss=5.22373

Batch 225710, train_perplexity=165.27814, train_loss=5.10763

Batch 225720, train_perplexity=182.54416, train_loss=5.206992

Batch 225730, train_perplexity=193.76987, train_loss=5.266671

Batch 225740, train_perplexity=192.72896, train_loss=5.261285

Batch 225750, train_perplexity=182.58856, train_loss=5.2072353

Batch 225760, train_perplexity=184.79602, train_loss=5.2192526

Batch 225770, train_perplexity=173.17813, train_loss=5.1543207

Batch 225780, train_perplexity=206.48482, train_loss=5.330227

Batch 225790, train_perplexity=187.65575, train_loss=5.234609

Batch 225800, train_perplexity=163.73932, train_loss=5.0982757

Batch 225810, train_perplexity=186.22798, train_loss=5.2269716

Batch 225820, train_perplexity=181.26701, train_loss=5.199971

Batch 225830, train_perplexity=167.76422, train_loss=5.1225595

Batch 225840, train_perplexity=154.75015, train_loss=5.041812

Batch 225850, train_perplexity=169.38101, train_loss=5.1321507

Batch 225860, train_perplexity=175.8184, train_loss=5.1694517

Batch 225870, train_perplexity=190.7359, train_loss=5.25089

Batch 225880, train_perplexity=187.01855, train_loss=5.231208

Batch 225890, train_perplexity=157.24387, train_loss=5.057798

Batch 225900, train_perplexity=164.91711, train_loss=5.105443

Batch 225910, train_perplexity=176.67714, train_loss=5.174324

Batch 225920, train_perplexity=174.75307, train_loss=5.163374

Batch 225930, train_perplexity=160.66307, train_loss=5.0793095

Batch 225940, train_perplexity=169.62445, train_loss=5.133587

Batch 225950, train_perplexity=179.14307, train_loss=5.1881847

Batch 225960, train_perplexity=179.638, train_loss=5.1909437

Batch 225970, train_perplexity=177.35393, train_loss=5.1781473

Batch 225980, train_perplexity=184.15211, train_loss=5.215762

Batch 225990, train_perplexity=184.36826, train_loss=5.216935

Batch 226000, train_perplexity=166.0371, train_loss=5.112211

Batch 226010, train_perplexity=167.36249, train_loss=5.120162

Batch 226020, train_perplexity=162.43271, train_loss=5.090264

Batch 226030, train_perplexity=166.89006, train_loss=5.1173353

Batch 226040, train_perplexity=186.48355, train_loss=5.228343

Batch 226050, train_perplexity=176.24953, train_loss=5.1719007

Batch 226060, train_perplexity=154.9677, train_loss=5.0432167

Batch 226070, train_perplexity=169.99579, train_loss=5.1357737

Batch 226080, train_perplexity=157.92503, train_loss=5.0621204

Batch 226090, train_perplexity=187.16237, train_loss=5.2319765

Batch 226100, train_perplexity=175.84726, train_loss=5.1696157

Batch 226110, train_perplexity=169.93687, train_loss=5.135427

Batch 226120, train_perplexity=178.21863, train_loss=5.183011

Batch 226130, train_perplexity=179.50322, train_loss=5.190193

Batch 226140, train_perplexity=166.62926, train_loss=5.1157713

Batch 226150, train_perplexity=159.61191, train_loss=5.0727453

Batch 226160, train_perplexity=150.67027, train_loss=5.015094

Batch 226170, train_perplexity=166.57039, train_loss=5.115418

Batch 226180, train_perplexity=185.71454, train_loss=5.2242107

Batch 226190, train_perplexity=190.28342, train_loss=5.2485147

Batch 226200, train_perplexity=161.35466, train_loss=5.083605

Batch 226210, train_perplexity=153.38342, train_loss=5.032941

Batch 226220, train_perplexity=164.63354, train_loss=5.103722

Batch 226230, train_perplexity=165.32394, train_loss=5.107907

Batch 226240, train_perplexity=174.82408, train_loss=5.16378

Batch 226250, train_perplexity=179.36154, train_loss=5.1894035

Batch 226260, train_perplexity=147.66328, train_loss=4.9949346

Batch 226270, train_perplexity=178.27745, train_loss=5.183341

Batch 226280, train_perplexity=156.14746, train_loss=5.050801

Batch 226290, train_perplexity=194.66624, train_loss=5.2712865

Batch 226300, train_perplexity=189.5546, train_loss=5.244677

Batch 226310, train_perplexity=165.62527, train_loss=5.109728

Batch 226320, train_perplexity=184.98204, train_loss=5.2202587

Batch 226330, train_perplexity=187.1645, train_loss=5.231988

Batch 226340, train_perplexity=184.8332, train_loss=5.219454

Batch 226350, train_perplexity=177.13469, train_loss=5.1769104

Batch 226360, train_perplexity=185.18193, train_loss=5.2213387

Batch 226370, train_perplexity=164.4549, train_loss=5.1026363

Batch 226380, train_perplexity=171.44455, train_loss=5.14426

Batch 226390, train_perplexity=165.41376, train_loss=5.10845

Batch 226400, train_perplexity=155.79367, train_loss=5.0485325

Batch 226410, train_perplexity=191.183, train_loss=5.253231

Batch 226420, train_perplexity=173.46962, train_loss=5.1560025

Batch 226430, train_perplexity=159.98604, train_loss=5.0750866

Batch 226440, train_perplexity=171.93782, train_loss=5.147133

Batch 226450, train_perplexity=166.07826, train_loss=5.112459

Batch 226460, train_perplexity=178.28969, train_loss=5.1834097

Batch 226470, train_perplexity=181.60754, train_loss=5.201848

Batch 226480, train_perplexity=170.05757, train_loss=5.136137

Batch 226490, train_perplexity=157.31685, train_loss=5.058262

Batch 226500, train_perplexity=182.2416, train_loss=5.205333

Batch 226510, train_perplexity=165.18668, train_loss=5.107076

Batch 226520, train_perplexity=156.10495, train_loss=5.0505285

Batch 226530, train_perplexity=170.22064, train_loss=5.1370955

Batch 226540, train_perplexity=178.13622, train_loss=5.1825485

Batch 226550, train_perplexity=167.20613, train_loss=5.1192274

Batch 226560, train_perplexity=168.78728, train_loss=5.128639

Batch 226570, train_perplexity=164.59407, train_loss=5.1034822

Batch 226580, train_perplexity=170.54643, train_loss=5.1390076

Batch 226590, train_perplexity=181.83015, train_loss=5.203073

Batch 226600, train_perplexity=158.68631, train_loss=5.0669293

Batch 226610, train_perplexity=196.31215, train_loss=5.279706

Batch 226620, train_perplexity=181.87143, train_loss=5.2033

Batch 226630, train_perplexity=190.42293, train_loss=5.2492476

Batch 226640, train_perplexity=184.12718, train_loss=5.2156267

Batch 226650, train_perplexity=197.85052, train_loss=5.287512

Batch 226660, train_perplexity=176.03418, train_loss=5.170678

Batch 226670, train_perplexity=167.07144, train_loss=5.1184216

Batch 226680, train_perplexity=186.47758, train_loss=5.228311

Batch 226690, train_perplexity=175.58215, train_loss=5.168107

Batch 226700, train_perplexity=185.21725, train_loss=5.2215295

Batch 226710, train_perplexity=174.13055, train_loss=5.1598053

Batch 226720, train_perplexity=171.40768, train_loss=5.144045

Batch 226730, train_perplexity=192.37051, train_loss=5.2594233

Batch 226740, train_perplexity=158.89323, train_loss=5.0682325

Batch 226750, train_perplexity=174.06049, train_loss=5.159403

Batch 226760, train_perplexity=167.01608, train_loss=5.11809

Batch 226770, train_perplexity=172.72076, train_loss=5.151676

Batch 226780, train_perplexity=161.84505, train_loss=5.0866394

Batch 226790, train_perplexity=172.20769, train_loss=5.148701

Batch 226800, train_perplexity=163.16412, train_loss=5.0947566

Batch 226810, train_perplexity=147.58269, train_loss=4.9943886

Batch 226820, train_perplexity=173.64755, train_loss=5.1570277

Batch 226830, train_perplexity=164.98994, train_loss=5.1058846

Batch 226840, train_perplexity=178.65631, train_loss=5.185464

Batch 226850, train_perplexity=155.44008, train_loss=5.0462604

Batch 226860, train_perplexity=174.8305, train_loss=5.163817

Batch 226870, train_perplexity=178.10037, train_loss=5.1823473

Batch 226880, train_perplexity=158.6071, train_loss=5.06643

Batch 226890, train_perplexity=160.06525, train_loss=5.0755816

Batch 226900, train_perplexity=153.13423, train_loss=5.031315

Batch 226910, train_perplexity=172.85324, train_loss=5.152443

Batch 226920, train_perplexity=165.1099, train_loss=5.1066113

Batch 226930, train_perplexity=159.11539, train_loss=5.0696297

Batch 226940, train_perplexity=178.58417, train_loss=5.18506
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 226950, train_perplexity=163.72238, train_loss=5.098172

Batch 226960, train_perplexity=172.91655, train_loss=5.152809

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Batch 226970, train_perplexity=159.19614, train_loss=5.070137

Batch 226980, train_perplexity=165.67361, train_loss=5.1100197

Batch 226990, train_perplexity=161.40498, train_loss=5.0839167

Batch 227000, train_perplexity=171.77097, train_loss=5.146162

Batch 227010, train_perplexity=184.00887, train_loss=5.214984

Batch 227020, train_perplexity=169.76735, train_loss=5.134429

Batch 227030, train_perplexity=166.36536, train_loss=5.1141863

Batch 227040, train_perplexity=176.71338, train_loss=5.174529

Batch 227050, train_perplexity=190.56454, train_loss=5.249991

Batch 227060, train_perplexity=173.45134, train_loss=5.155897

Batch 227070, train_perplexity=185.02261, train_loss=5.220478

Batch 227080, train_perplexity=163.83328, train_loss=5.0988493

Batch 227090, train_perplexity=168.74905, train_loss=5.1284127

Batch 227100, train_perplexity=186.65791, train_loss=5.2292776

Batch 227110, train_perplexity=201.40617, train_loss=5.3053236

Batch 227120, train_perplexity=178.72235, train_loss=5.1858335

Batch 227130, train_perplexity=197.13293, train_loss=5.2838783

Batch 227140, train_perplexity=155.67796, train_loss=5.0477896

Batch 227150, train_perplexity=174.03493, train_loss=5.159256

Batch 227160, train_perplexity=164.43686, train_loss=5.1025267

Batch 227170, train_perplexity=167.01259, train_loss=5.118069

Batch 227180, train_perplexity=164.92654, train_loss=5.1055

Batch 227190, train_perplexity=169.34483, train_loss=5.131937

Batch 227200, train_perplexity=156.61111, train_loss=5.053766

Batch 227210, train_perplexity=157.95741, train_loss=5.0623255

Batch 227220, train_perplexity=158.33159, train_loss=5.0646915

Batch 227230, train_perplexity=172.71747, train_loss=5.151657

Batch 227240, train_perplexity=155.39081, train_loss=5.0459433

Batch 227250, train_perplexity=166.66835, train_loss=5.116006

Batch 227260, train_perplexity=181.10684, train_loss=5.199087

Batch 227270, train_perplexity=181.14735, train_loss=5.199311

Batch 227280, train_perplexity=164.6611, train_loss=5.1038895

Batch 227290, train_perplexity=156.41417, train_loss=5.0525074

Batch 227300, train_perplexity=153.78944, train_loss=5.0355844

Batch 227310, train_perplexity=170.24564, train_loss=5.1372423

Batch 227320, train_perplexity=178.18303, train_loss=5.1828113

Batch 227330, train_perplexity=172.93263, train_loss=5.152902

Batch 227340, train_perplexity=161.81773, train_loss=5.0864706

Batch 227350, train_perplexity=166.50908, train_loss=5.11505

Batch 227360, train_perplexity=180.376, train_loss=5.1950436

Batch 227370, train_perplexity=160.82297, train_loss=5.080304

Batch 227380, train_perplexity=172.04115, train_loss=5.1477337

Batch 227390, train_perplexity=167.79662, train_loss=5.1227527

Batch 227400, train_perplexity=156.47026, train_loss=5.052866

Batch 227410, train_perplexity=170.63022, train_loss=5.1394987

Batch 227420, train_perplexity=201.6716, train_loss=5.3066406

Batch 227430, train_perplexity=187.05031, train_loss=5.2313776

Batch 227440, train_perplexity=167.5022, train_loss=5.1209965

Batch 227450, train_perplexity=165.6797, train_loss=5.1100564

Batch 227460, train_perplexity=146.8599, train_loss=4.989479

Batch 227470, train_perplexity=169.92464, train_loss=5.135355

Batch 227480, train_perplexity=165.11949, train_loss=5.1066694

Batch 227490, train_perplexity=165.8967, train_loss=5.1113653

Batch 227500, train_perplexity=188.02164, train_loss=5.236557

Batch 227510, train_perplexity=147.43526, train_loss=4.993389

Batch 227520, train_perplexity=148.64745, train_loss=5.0015774

Batch 227530, train_perplexity=183.21494, train_loss=5.21066

Batch 227540, train_perplexity=158.15457, train_loss=5.063573

Batch 227550, train_perplexity=190.05844, train_loss=5.2473316

Batch 227560, train_perplexity=162.86284, train_loss=5.0929084

Batch 227570, train_perplexity=172.29253, train_loss=5.149194

Batch 227580, train_perplexity=177.9301, train_loss=5.181391

Batch 227590, train_perplexity=173.40967, train_loss=5.155657

Batch 227600, train_perplexity=191.44016, train_loss=5.2545753

Batch 227610, train_perplexity=168.43529, train_loss=5.1265516

Batch 227620, train_perplexity=185.91566, train_loss=5.225293

Batch 227630, train_perplexity=169.52791, train_loss=5.1330175

Batch 227640, train_perplexity=173.55609, train_loss=5.156501

Batch 227650, train_perplexity=177.40764, train_loss=5.17845

Batch 227660, train_perplexity=180.66928, train_loss=5.196668

Batch 227670, train_perplexity=160.16168, train_loss=5.076184

Batch 227680, train_perplexity=167.49565, train_loss=5.1209574

Batch 227690, train_perplexity=181.764, train_loss=5.202709

Batch 227700, train_perplexity=198.30861, train_loss=5.2898245

Batch 227710, train_perplexity=182.31668, train_loss=5.205745

Batch 227720, train_perplexity=176.31088, train_loss=5.172249

Batch 227730, train_perplexity=173.4301, train_loss=5.1557746

Batch 227740, train_perplexity=188.0609, train_loss=5.236766

Batch 227750, train_perplexity=163.44975, train_loss=5.0965056

Batch 227760, train_perplexity=164.39656, train_loss=5.1022816

Batch 227770, train_perplexity=175.26219, train_loss=5.166283

Batch 227780, train_perplexity=163.2182, train_loss=5.095088

Batch 227790, train_perplexity=164.89934, train_loss=5.105335

Batch 227800, train_perplexity=169.08453, train_loss=5.1303988

Batch 227810, train_perplexity=158.53209, train_loss=5.065957

Batch 227820, train_perplexity=181.0299, train_loss=5.1986623

Batch 227830, train_perplexity=169.62138, train_loss=5.133569

Batch 227840, train_perplexity=174.71008, train_loss=5.163128

Batch 227850, train_perplexity=182.5238, train_loss=5.2068806

Batch 227860, train_perplexity=155.77762, train_loss=5.0484295

Batch 227870, train_perplexity=157.86089, train_loss=5.061714

Batch 227880, train_perplexity=190.78922, train_loss=5.251169

Batch 227890, train_perplexity=164.48227, train_loss=5.1028028

Batch 227900, train_perplexity=172.99193, train_loss=5.153245

Batch 227910, train_perplexity=159.44653, train_loss=5.0717087

Batch 227920, train_perplexity=169.802, train_loss=5.134633

Batch 227930, train_perplexity=166.33894, train_loss=5.1140275

Batch 227940, train_perplexity=188.21269, train_loss=5.2375727

Batch 227950, train_perplexity=188.88466, train_loss=5.2411366

Batch 227960, train_perplexity=178.71297, train_loss=5.185781

Batch 227970, train_perplexity=158.79985, train_loss=5.0676446

Batch 227980, train_perplexity=154.59322, train_loss=5.040797

Batch 227990, train_perplexity=155.88158, train_loss=5.0490966

Batch 228000, train_perplexity=184.11673, train_loss=5.21557

Batch 228010, train_perplexity=155.1888, train_loss=5.0446424

Batch 228020, train_perplexity=180.61096, train_loss=5.1963453

Batch 228030, train_perplexity=167.07767, train_loss=5.1184587

Batch 228040, train_perplexity=164.7156, train_loss=5.1042204

Batch 228050, train_perplexity=174.50526, train_loss=5.161955

Batch 228060, train_perplexity=189.99013, train_loss=5.246972

Batch 228070, train_perplexity=172.32251, train_loss=5.149368

Batch 228080, train_perplexity=156.7228, train_loss=5.0544786

Batch 228090, train_perplexity=193.62817, train_loss=5.2659397

Batch 228100, train_perplexity=166.35147, train_loss=5.114103

Batch 228110, train_perplexity=180.42366, train_loss=5.1953077

Batch 228120, train_perplexity=156.52496, train_loss=5.0532155

Batch 228130, train_perplexity=168.58475, train_loss=5.1274385

Batch 228140, train_perplexity=173.93338, train_loss=5.1586723

Batch 228150, train_perplexity=168.54391, train_loss=5.1271963

Batch 228160, train_perplexity=163.96184, train_loss=5.0996337

Batch 228170, train_perplexity=175.27899, train_loss=5.166379

Batch 228180, train_perplexity=174.62854, train_loss=5.162661
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 228190, train_perplexity=177.66092, train_loss=5.179877

Batch 228200, train_perplexity=181.93951, train_loss=5.2036743

Batch 228210, train_perplexity=174.17714, train_loss=5.160073

Batch 228220, train_perplexity=165.92012, train_loss=5.1115065

Batch 228230, train_perplexity=157.97324, train_loss=5.0624256

Batch 228240, train_perplexity=185.06497, train_loss=5.220707

Batch 228250, train_perplexity=177.80711, train_loss=5.1806993

Batch 228260, train_perplexity=148.28964, train_loss=4.9991674

Batch 228270, train_perplexity=173.18169, train_loss=5.154341

Batch 228280, train_perplexity=177.609, train_loss=5.1795845

Batch 228290, train_perplexity=163.62506, train_loss=5.0975776

Batch 228300, train_perplexity=165.16911, train_loss=5.10697

Batch 228310, train_perplexity=181.86267, train_loss=5.203252

Batch 228320, train_perplexity=177.20549, train_loss=5.17731

Batch 228330, train_perplexity=166.65604, train_loss=5.115932

Batch 228340, train_perplexity=167.91356, train_loss=5.1234493

Batch 228350, train_perplexity=173.68259, train_loss=5.1572294

Batch 228360, train_perplexity=155.66417, train_loss=5.047701

Batch 228370, train_perplexity=174.0497, train_loss=5.159341

Batch 228380, train_perplexity=164.14262, train_loss=5.1007357

Batch 228390, train_perplexity=163.15028, train_loss=5.0946717

Batch 228400, train_perplexity=191.22238, train_loss=5.253437

Batch 228410, train_perplexity=164.63152, train_loss=5.1037097

Batch 228420, train_perplexity=170.40385, train_loss=5.138171

Batch 228430, train_perplexity=168.70906, train_loss=5.1281757

Batch 228440, train_perplexity=179.59732, train_loss=5.190717

Batch 228450, train_perplexity=182.32843, train_loss=5.2058096

Batch 228460, train_perplexity=174.97754, train_loss=5.1646576

Batch 228470, train_perplexity=190.363, train_loss=5.248933

Batch 228480, train_perplexity=167.48997, train_loss=5.1209235

Batch 228490, train_perplexity=171.45969, train_loss=5.144348

Batch 228500, train_perplexity=164.0908, train_loss=5.10042

Batch 228510, train_perplexity=163.02173, train_loss=5.0938835

Batch 228520, train_perplexity=176.93445, train_loss=5.1757793

Batch 228530, train_perplexity=182.58403, train_loss=5.2072105

Batch 228540, train_perplexity=190.17393, train_loss=5.247939

Batch 228550, train_perplexity=164.12962, train_loss=5.1006565

Batch 228560, train_perplexity=141.65208, train_loss=4.953374

Batch 228570, train_perplexity=159.97095, train_loss=5.074992

Batch 228580, train_perplexity=160.23141, train_loss=5.076619

Batch 228590, train_perplexity=160.84337, train_loss=5.080431

Batch 228600, train_perplexity=165.80908, train_loss=5.110837

Batch 228610, train_perplexity=147.96262, train_loss=4.9969597

Batch 228620, train_perplexity=183.54004, train_loss=5.212433

Batch 228630, train_perplexity=156.88206, train_loss=5.0554943

Batch 228640, train_perplexity=172.33574, train_loss=5.1494446

Batch 228650, train_perplexity=172.70084, train_loss=5.151561

Batch 228660, train_perplexity=179.22885, train_loss=5.1886635

Batch 228670, train_perplexity=170.83342, train_loss=5.140689

Batch 228680, train_perplexity=171.50792, train_loss=5.1446295

Batch 228690, train_perplexity=166.07637, train_loss=5.1124477

Batch 228700, train_perplexity=194.68416, train_loss=5.2713785

Batch 228710, train_perplexity=194.82848, train_loss=5.2721195

Batch 228720, train_perplexity=155.56628, train_loss=5.047072

Batch 228730, train_perplexity=174.33168, train_loss=5.1609597

Batch 228740, train_perplexity=155.21011, train_loss=5.04478

Batch 228750, train_perplexity=193.03035, train_loss=5.2628474

Batch 228760, train_perplexity=185.65398, train_loss=5.2238846

Batch 228770, train_perplexity=194.44627, train_loss=5.270156

Batch 228780, train_perplexity=166.3536, train_loss=5.1141157

Batch 228790, train_perplexity=175.42825, train_loss=5.16723

Batch 228800, train_perplexity=179.6649, train_loss=5.1910934

Batch 228810, train_perplexity=189.10446, train_loss=5.2422996

Batch 228820, train_perplexity=157.07863, train_loss=5.0567465

Batch 228830, train_perplexity=186.10548, train_loss=5.2263136

Batch 228840, train_perplexity=185.26743, train_loss=5.2218003

Batch 228850, train_perplexity=181.07886, train_loss=5.1989326

Batch 228860, train_perplexity=158.63281, train_loss=5.066592

Batch 228870, train_perplexity=171.00302, train_loss=5.141681

Batch 228880, train_perplexity=164.2921, train_loss=5.101646

Batch 228890, train_perplexity=186.73555, train_loss=5.2296934

Batch 228900, train_perplexity=157.76021, train_loss=5.061076

Batch 228910, train_perplexity=183.18349, train_loss=5.2104883

Batch 228920, train_perplexity=177.56953, train_loss=5.1793623

Batch 228930, train_perplexity=177.87614, train_loss=5.1810875

Batch 228940, train_perplexity=180.273, train_loss=5.1944723

Batch 228950, train_perplexity=171.77597, train_loss=5.146191

Batch 228960, train_perplexity=153.29613, train_loss=5.0323715

Batch 228970, train_perplexity=157.70093, train_loss=5.0607004

Batch 228980, train_perplexity=180.64507, train_loss=5.196534

Batch 228990, train_perplexity=194.55553, train_loss=5.2707176

Batch 229000, train_perplexity=180.44293, train_loss=5.1954145

Batch 229010, train_perplexity=163.46043, train_loss=5.096571

Batch 229020, train_perplexity=180.11386, train_loss=5.193589

Batch 229030, train_perplexity=195.97862, train_loss=5.2780056

Batch 229040, train_perplexity=182.29904, train_loss=5.2056484

Batch 229050, train_perplexity=160.54651, train_loss=5.0785837

Batch 229060, train_perplexity=170.75075, train_loss=5.140205

Batch 229070, train_perplexity=183.18672, train_loss=5.210506

Batch 229080, train_perplexity=173.9546, train_loss=5.1587944

Batch 229090, train_perplexity=189.49739, train_loss=5.244375

Batch 229100, train_perplexity=164.73132, train_loss=5.1043158

Batch 229110, train_perplexity=183.01794, train_loss=5.209584

Batch 229120, train_perplexity=182.8523, train_loss=5.2086787

Batch 229130, train_perplexity=182.20467, train_loss=5.2051306

Batch 229140, train_perplexity=167.37302, train_loss=5.120225

Batch 229150, train_perplexity=178.33272, train_loss=5.183651

Batch 229160, train_perplexity=176.92079, train_loss=5.175702

Batch 229170, train_perplexity=171.12349, train_loss=5.1423855

Batch 229180, train_perplexity=179.14589, train_loss=5.1882005

Batch 229190, train_perplexity=169.4972, train_loss=5.1328363

Batch 229200, train_perplexity=176.51917, train_loss=5.1734295

Batch 229210, train_perplexity=175.71104, train_loss=5.168841

Batch 229220, train_perplexity=168.72604, train_loss=5.1282763

Batch 229230, train_perplexity=154.17427, train_loss=5.0380836

Batch 229240, train_perplexity=178.8469, train_loss=5.18653

Batch 229250, train_perplexity=166.21248, train_loss=5.113267

Batch 229260, train_perplexity=174.58824, train_loss=5.1624303

Batch 229270, train_perplexity=155.75497, train_loss=5.048284

Batch 229280, train_perplexity=169.37115, train_loss=5.1320925

Batch 229290, train_perplexity=153.58578, train_loss=5.0342593

Batch 229300, train_perplexity=198.11429, train_loss=5.288844

Batch 229310, train_perplexity=163.07701, train_loss=5.0942225

Batch 229320, train_perplexity=181.38496, train_loss=5.2006216

Batch 229330, train_perplexity=157.41664, train_loss=5.058896

Batch 229340, train_perplexity=185.30391, train_loss=5.2219973

Batch 229350, train_perplexity=185.11209, train_loss=5.2209616

Batch 229360, train_perplexity=172.46661, train_loss=5.1502037

Batch 229370, train_perplexity=178.18584, train_loss=5.182827

Batch 229380, train_perplexity=159.85199, train_loss=5.0742483

Batch 229390, train_perplexity=160.51192, train_loss=5.078368

Batch 229400, train_perplexity=161.02478, train_loss=5.081558

Batch 229410, train_perplexity=184.52199, train_loss=5.2177687

Batch 229420, train_perplexity=165.90746, train_loss=5.11143

Batch 229430, train_perplexity=171.4417, train_loss=5.1442432

Batch 229440, train_perplexity=176.2876, train_loss=5.1721168

Batch 229450, train_perplexity=179.76071, train_loss=5.1916265

Batch 229460, train_perplexity=178.41573, train_loss=5.1841164

Batch 229470, train_perplexity=173.476, train_loss=5.156039

Batch 229480, train_perplexity=176.3657, train_loss=5.1725597
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 229490, train_perplexity=182.41243, train_loss=5.20627

Batch 229500, train_perplexity=169.92155, train_loss=5.135337

Batch 229510, train_perplexity=163.7722, train_loss=5.0984764

Batch 229520, train_perplexity=179.80014, train_loss=5.191846

Batch 229530, train_perplexity=155.89622, train_loss=5.0491905

Batch 229540, train_perplexity=177.94673, train_loss=5.181484

Batch 229550, train_perplexity=168.6867, train_loss=5.128043

Batch 229560, train_perplexity=153.34424, train_loss=5.0326853

Batch 229570, train_perplexity=159.5028, train_loss=5.0720615

Batch 229580, train_perplexity=155.23898, train_loss=5.0449657

Batch 229590, train_perplexity=159.80351, train_loss=5.073945

Batch 229600, train_perplexity=176.62796, train_loss=5.1740456

Batch 229610, train_perplexity=166.96967, train_loss=5.117812

Batch 229620, train_perplexity=178.7203, train_loss=5.185822

Batch 229630, train_perplexity=188.36963, train_loss=5.238406

Batch 229640, train_perplexity=160.72414, train_loss=5.0796895

Batch 229650, train_perplexity=170.36948, train_loss=5.1379695

Batch 229660, train_perplexity=197.01604, train_loss=5.283285

Batch 229670, train_perplexity=182.32599, train_loss=5.2057962

Batch 229680, train_perplexity=162.41095, train_loss=5.09013

Batch 229690, train_perplexity=179.54834, train_loss=5.1904445

Batch 229700, train_perplexity=163.34099, train_loss=5.09584

Batch 229710, train_perplexity=181.8488, train_loss=5.2031755

Batch 229720, train_perplexity=175.36862, train_loss=5.16689

Batch 229730, train_perplexity=175.95345, train_loss=5.1702194

Batch 229740, train_perplexity=174.82858, train_loss=5.163806

Batch 229750, train_perplexity=174.11362, train_loss=5.159708

Batch 229760, train_perplexity=155.45306, train_loss=5.046344

Batch 229770, train_perplexity=176.94719, train_loss=5.1758513

Batch 229780, train_perplexity=187.96748, train_loss=5.236269

Batch 229790, train_perplexity=165.76624, train_loss=5.1105785

Batch 229800, train_perplexity=168.95566, train_loss=5.1296363

Batch 229810, train_perplexity=154.74971, train_loss=5.041809

Batch 229820, train_perplexity=179.54192, train_loss=5.1904087

Batch 229830, train_perplexity=159.29813, train_loss=5.0707774

Batch 229840, train_perplexity=166.07059, train_loss=5.112413

Batch 229850, train_perplexity=167.87953, train_loss=5.1232467

Batch 229860, train_perplexity=155.95956, train_loss=5.049597

Batch 229870, train_perplexity=170.55855, train_loss=5.1390786

Batch 229880, train_perplexity=181.76106, train_loss=5.202693

Batch 229890, train_perplexity=155.09041, train_loss=5.0440083

Batch 229900, train_perplexity=175.44473, train_loss=5.167324

Batch 229910, train_perplexity=172.2958, train_loss=5.149213

Batch 229920, train_perplexity=180.90868, train_loss=5.1979923

Batch 229930, train_perplexity=168.58008, train_loss=5.127411

Batch 229940, train_perplexity=172.5474, train_loss=5.150672

Batch 229950, train_perplexity=172.9624, train_loss=5.1530743

Batch 229960, train_perplexity=160.98769, train_loss=5.081328

Batch 229970, train_perplexity=157.80339, train_loss=5.06135

Batch 229980, train_perplexity=178.36613, train_loss=5.1838384

Batch 229990, train_perplexity=156.11284, train_loss=5.050579

Batch 230000, train_perplexity=178.04297, train_loss=5.182025

Batch 230010, train_perplexity=179.73207, train_loss=5.1914673

Batch 230020, train_perplexity=176.40996, train_loss=5.1728106

Batch 230030, train_perplexity=174.8565, train_loss=5.1639657

Batch 230040, train_perplexity=182.82301, train_loss=5.2085185

Batch 230050, train_perplexity=156.4378, train_loss=5.0526586

Batch 230060, train_perplexity=163.96294, train_loss=5.0996404

Batch 230070, train_perplexity=172.65703, train_loss=5.151307

Batch 230080, train_perplexity=167.82295, train_loss=5.1229095

Batch 230090, train_perplexity=156.79613, train_loss=5.0549464

Batch 230100, train_perplexity=161.67265, train_loss=5.0855737

Batch 230110, train_perplexity=182.3053, train_loss=5.2056828

Batch 230120, train_perplexity=190.363, train_loss=5.248933

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Batch 230130, train_perplexity=174.52657, train_loss=5.162077

Batch 230140, train_perplexity=162.23022, train_loss=5.0890164

Batch 230150, train_perplexity=189.23976, train_loss=5.243015

Batch 230160, train_perplexity=181.98169, train_loss=5.203906

Batch 230170, train_perplexity=191.64136, train_loss=5.2556257

Batch 230180, train_perplexity=170.1098, train_loss=5.136444

Batch 230190, train_perplexity=173.11769, train_loss=5.1539717

Batch 230200, train_perplexity=191.18135, train_loss=5.2532225

Batch 230210, train_perplexity=143.14958, train_loss=4.96389

Batch 230220, train_perplexity=167.8921, train_loss=5.1233215

Batch 230230, train_perplexity=181.95522, train_loss=5.2037606

Batch 230240, train_perplexity=180.1465, train_loss=5.1937704

Batch 230250, train_perplexity=181.56044, train_loss=5.2015886

Batch 230260, train_perplexity=143.8478, train_loss=4.9687557

Batch 230270, train_perplexity=177.61214, train_loss=5.179602

Batch 230280, train_perplexity=173.43109, train_loss=5.1557803

Batch 230290, train_perplexity=184.14447, train_loss=5.2157207

Batch 230300, train_perplexity=188.04333, train_loss=5.2366724

Batch 230310, train_perplexity=195.20312, train_loss=5.2740407

Batch 230320, train_perplexity=175.61598, train_loss=5.1682997

Batch 230330, train_perplexity=183.87607, train_loss=5.214262

Batch 230340, train_perplexity=163.48787, train_loss=5.096739

Batch 230350, train_perplexity=166.37733, train_loss=5.1142583

Batch 230360, train_perplexity=168.9546, train_loss=5.12963

Batch 230370, train_perplexity=178.95499, train_loss=5.1871343

Batch 230380, train_perplexity=151.57376, train_loss=5.0210724

Batch 230390, train_perplexity=164.31122, train_loss=5.1017623

Batch 230400, train_perplexity=189.29237, train_loss=5.243293

Batch 230410, train_perplexity=155.41362, train_loss=5.04609

Batch 230420, train_perplexity=169.9504, train_loss=5.1355066

Batch 230430, train_perplexity=154.05338, train_loss=5.037299

Batch 230440, train_perplexity=165.04251, train_loss=5.106203

Batch 230450, train_perplexity=152.54768, train_loss=5.0274773

Batch 230460, train_perplexity=181.68889, train_loss=5.202296

Batch 230470, train_perplexity=187.24762, train_loss=5.232432

Batch 230480, train_perplexity=184.89561, train_loss=5.2197914

Batch 230490, train_perplexity=195.32565, train_loss=5.274668

Batch 230500, train_perplexity=169.7141, train_loss=5.134115

Batch 230510, train_perplexity=162.6236, train_loss=5.0914383

Batch 230520, train_perplexity=168.30411, train_loss=5.1257725

Batch 230530, train_perplexity=163.18576, train_loss=5.094889

Batch 230540, train_perplexity=177.54532, train_loss=5.179226

Batch 230550, train_perplexity=161.92332, train_loss=5.087123

Batch 230560, train_perplexity=170.12407, train_loss=5.136528

Batch 230570, train_perplexity=162.68169, train_loss=5.0917954

Batch 230580, train_perplexity=163.44243, train_loss=5.096461

Batch 230590, train_perplexity=189.04541, train_loss=5.241987

Batch 230600, train_perplexity=160.68008, train_loss=5.0794153

Batch 230610, train_perplexity=178.405, train_loss=5.1840563

Batch 230620, train_perplexity=171.12186, train_loss=5.142376

Batch 230630, train_perplexity=175.06032, train_loss=5.1651306

Batch 230640, train_perplexity=176.78679, train_loss=5.1749444

Batch 230650, train_perplexity=173.56519, train_loss=5.1565533

Batch 230660, train_perplexity=161.32066, train_loss=5.083394

Batch 230670, train_perplexity=174.71425, train_loss=5.1631517

Batch 230680, train_perplexity=189.57584, train_loss=5.244789

Batch 230690, train_perplexity=170.92696, train_loss=5.1412363

Batch 230700, train_perplexity=174.13121, train_loss=5.159809

Batch 230710, train_perplexity=168.5603, train_loss=5.1272936

Batch 230720, train_perplexity=182.1892, train_loss=5.2050457
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 230730, train_perplexity=174.29105, train_loss=5.1607265

Batch 230740, train_perplexity=174.55818, train_loss=5.162258

Batch 230750, train_perplexity=152.20859, train_loss=5.025252

Batch 230760, train_perplexity=155.00938, train_loss=5.0434856

Batch 230770, train_perplexity=161.19548, train_loss=5.0826178

Batch 230780, train_perplexity=189.86342, train_loss=5.246305

Batch 230790, train_perplexity=160.98079, train_loss=5.081285

Batch 230800, train_perplexity=152.11491, train_loss=5.0246363

Batch 230810, train_perplexity=172.22993, train_loss=5.1488304

Batch 230820, train_perplexity=167.4693, train_loss=5.1208

Batch 230830, train_perplexity=146.35684, train_loss=4.9860477

Batch 230840, train_perplexity=166.11517, train_loss=5.1126814

Batch 230850, train_perplexity=162.59879, train_loss=5.0912857

Batch 230860, train_perplexity=151.82759, train_loss=5.0227456

Batch 230870, train_perplexity=164.22983, train_loss=5.101267

Batch 230880, train_perplexity=179.82928, train_loss=5.192008

Batch 230890, train_perplexity=178.50423, train_loss=5.1846123

Batch 230900, train_perplexity=170.09593, train_loss=5.1363626

Batch 230910, train_perplexity=164.84274, train_loss=5.104992

Batch 230920, train_perplexity=148.26279, train_loss=4.9989862

Batch 230930, train_perplexity=175.18584, train_loss=5.1658473

Batch 230940, train_perplexity=173.41429, train_loss=5.1556835

Batch 230950, train_perplexity=178.94824, train_loss=5.1870966

Batch 230960, train_perplexity=177.85529, train_loss=5.18097

Batch 230970, train_perplexity=166.01897, train_loss=5.112102

Batch 230980, train_perplexity=182.44235, train_loss=5.2064342

Batch 230990, train_perplexity=164.53944, train_loss=5.1031504

Batch 231000, train_perplexity=166.125, train_loss=5.1127405

Batch 231010, train_perplexity=150.80612, train_loss=5.015995

Batch 231020, train_perplexity=171.5592, train_loss=5.1449285

Batch 231030, train_perplexity=172.40495, train_loss=5.149846

Batch 231040, train_perplexity=186.51414, train_loss=5.228507

Batch 231050, train_perplexity=151.05052, train_loss=5.0176144

Batch 231060, train_perplexity=146.62549, train_loss=4.9878817

Batch 231070, train_perplexity=170.34868, train_loss=5.1378474

Batch 231080, train_perplexity=173.22702, train_loss=5.154603

Batch 231090, train_perplexity=156.06491, train_loss=5.050272

Batch 231100, train_perplexity=190.16179, train_loss=5.247875

Batch 231110, train_perplexity=169.55377, train_loss=5.13317

Batch 231120, train_perplexity=161.23238, train_loss=5.0828466

Batch 231130, train_perplexity=181.28258, train_loss=5.200057

Batch 231140, train_perplexity=160.43991, train_loss=5.0779195

Batch 231150, train_perplexity=173.61601, train_loss=5.156846

Batch 231160, train_perplexity=164.49191, train_loss=5.1028614

Batch 231170, train_perplexity=185.12746, train_loss=5.2210445

Batch 231180, train_perplexity=171.58546, train_loss=5.1450815

Batch 231190, train_perplexity=152.7065, train_loss=5.0285177

Batch 231200, train_perplexity=158.76419, train_loss=5.06742

Batch 231210, train_perplexity=185.4647, train_loss=5.2228646

Batch 231220, train_perplexity=180.22348, train_loss=5.1941977

Batch 231230, train_perplexity=160.32771, train_loss=5.07722

Batch 231240, train_perplexity=162.20538, train_loss=5.0888634

Batch 231250, train_perplexity=180.56445, train_loss=5.196088

Batch 231260, train_perplexity=170.09406, train_loss=5.1363516

Batch 231270, train_perplexity=163.90813, train_loss=5.099306

Batch 231280, train_perplexity=153.49515, train_loss=5.033669

Batch 231290, train_perplexity=175.92558, train_loss=5.170061

Batch 231300, train_perplexity=173.0312, train_loss=5.153472

Batch 231310, train_perplexity=180.1648, train_loss=5.193872

Batch 231320, train_perplexity=171.4479, train_loss=5.1442795

Batch 231330, train_perplexity=155.60464, train_loss=5.0473185

Batch 231340, train_perplexity=185.86133, train_loss=5.225001

Batch 231350, train_perplexity=166.54774, train_loss=5.115282

Batch 231360, train_perplexity=185.89474, train_loss=5.2251806

Batch 231370, train_perplexity=180.54036, train_loss=5.1959543

Batch 231380, train_perplexity=178.8, train_loss=5.186268

Batch 231390, train_perplexity=178.73505, train_loss=5.1859045

Batch 231400, train_perplexity=172.51598, train_loss=5.15049

Batch 231410, train_perplexity=188.07974, train_loss=5.236866

Batch 231420, train_perplexity=157.38902, train_loss=5.0587206

Batch 231430, train_perplexity=156.04094, train_loss=5.0501184

Batch 231440, train_perplexity=181.63023, train_loss=5.201973

Batch 231450, train_perplexity=156.79352, train_loss=5.0549297

Batch 231460, train_perplexity=173.6763, train_loss=5.157193

Batch 231470, train_perplexity=166.52043, train_loss=5.115118

Batch 231480, train_perplexity=175.3753, train_loss=5.1669283

Batch 231490, train_perplexity=186.82425, train_loss=5.2301683

Batch 231500, train_perplexity=188.17662, train_loss=5.237381

Batch 231510, train_perplexity=170.2609, train_loss=5.137332

Batch 231520, train_perplexity=174.69966, train_loss=5.1630683

Batch 231530, train_perplexity=182.88483, train_loss=5.2088566

Batch 231540, train_perplexity=141.67552, train_loss=4.9535394

Batch 231550, train_perplexity=187.84616, train_loss=5.2356234

Batch 231560, train_perplexity=180.13086, train_loss=5.1936836

Batch 231570, train_perplexity=146.61885, train_loss=4.9878364

Batch 231580, train_perplexity=180.89789, train_loss=5.1979327

Batch 231590, train_perplexity=166.71031, train_loss=5.1162577

Batch 231600, train_perplexity=148.30202, train_loss=4.999251

Batch 231610, train_perplexity=165.21605, train_loss=5.107254

Batch 231620, train_perplexity=182.92137, train_loss=5.2090564

Batch 231630, train_perplexity=167.38403, train_loss=5.1202908

Batch 231640, train_perplexity=157.45132, train_loss=5.0591164

Batch 231650, train_perplexity=171.06157, train_loss=5.1420236

Batch 231660, train_perplexity=178.74365, train_loss=5.1859527

Batch 231670, train_perplexity=196.77756, train_loss=5.282074

Batch 231680, train_perplexity=155.03954, train_loss=5.04368

Batch 231690, train_perplexity=170.81973, train_loss=5.140609

Batch 231700, train_perplexity=165.17407, train_loss=5.107

Batch 231710, train_perplexity=193.26529, train_loss=5.264064

Batch 231720, train_perplexity=194.42523, train_loss=5.2700477

Batch 231730, train_perplexity=176.24927, train_loss=5.1718993

Batch 231740, train_perplexity=160.16344, train_loss=5.076195

Batch 231750, train_perplexity=158.0106, train_loss=5.062662

Batch 231760, train_perplexity=166.8377, train_loss=5.1170216

Batch 231770, train_perplexity=179.25064, train_loss=5.188785

Batch 231780, train_perplexity=185.94554, train_loss=5.225454

Batch 231790, train_perplexity=159.90321, train_loss=5.0745687

Batch 231800, train_perplexity=163.73425, train_loss=5.0982447

Batch 231810, train_perplexity=170.17032, train_loss=5.1368

Batch 231820, train_perplexity=157.7347, train_loss=5.0609145

Batch 231830, train_perplexity=169.482, train_loss=5.1327467

Batch 231840, train_perplexity=179.31023, train_loss=5.1891174

Batch 231850, train_perplexity=175.85933, train_loss=5.1696844

Batch 231860, train_perplexity=184.48592, train_loss=5.217573

Batch 231870, train_perplexity=155.33295, train_loss=5.045571

Batch 231880, train_perplexity=180.48285, train_loss=5.195636

Batch 231890, train_perplexity=157.30312, train_loss=5.0581746

Batch 231900, train_perplexity=177.34479, train_loss=5.178096

Batch 231910, train_perplexity=174.03807, train_loss=5.159274

Batch 231920, train_perplexity=169.27733, train_loss=5.1315384

Batch 231930, train_perplexity=169.0963, train_loss=5.1304684

Batch 231940, train_perplexity=168.91135, train_loss=5.129374

Batch 231950, train_perplexity=178.41454, train_loss=5.1841097

Batch 231960, train_perplexity=177.83832, train_loss=5.180875

Batch 231970, train_perplexity=172.1649, train_loss=5.1484528

Batch 231980, train_perplexity=168.67392, train_loss=5.1279674

Batch 231990, train_perplexity=185.10945, train_loss=5.2209473

Batch 232000, train_perplexity=173.73576, train_loss=5.1575356

Batch 232010, train_perplexity=166.95947, train_loss=5.117751

Batch 232020, train_perplexity=184.81998, train_loss=5.2193823
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 232030, train_perplexity=181.20618, train_loss=5.1996355

Batch 232040, train_perplexity=157.49698, train_loss=5.0594063

Batch 232050, train_perplexity=185.10027, train_loss=5.2208977

Batch 232060, train_perplexity=172.06511, train_loss=5.147873

Batch 232070, train_perplexity=184.86855, train_loss=5.219645

Batch 232080, train_perplexity=162.79785, train_loss=5.0925093

Batch 232090, train_perplexity=170.40515, train_loss=5.138179

Batch 232100, train_perplexity=166.4082, train_loss=5.114444

Batch 232110, train_perplexity=157.27919, train_loss=5.0580225

Batch 232120, train_perplexity=164.57664, train_loss=5.1033764

Batch 232130, train_perplexity=177.76355, train_loss=5.1804543

Batch 232140, train_perplexity=170.91864, train_loss=5.1411877

Batch 232150, train_perplexity=169.67397, train_loss=5.1338787

Batch 232160, train_perplexity=159.5156, train_loss=5.0721416

Batch 232170, train_perplexity=157.68423, train_loss=5.0605946

Batch 232180, train_perplexity=158.76222, train_loss=5.0674076

Batch 232190, train_perplexity=162.05449, train_loss=5.0879326

Batch 232200, train_perplexity=162.77037, train_loss=5.0923405

Batch 232210, train_perplexity=158.83295, train_loss=5.067853

Batch 232220, train_perplexity=180.71037, train_loss=5.1968956

Batch 232230, train_perplexity=165.92455, train_loss=5.111533

Batch 232240, train_perplexity=189.36028, train_loss=5.2436514

Batch 232250, train_perplexity=159.10614, train_loss=5.0695715

Batch 232260, train_perplexity=162.3134, train_loss=5.089529

Batch 232270, train_perplexity=173.40363, train_loss=5.155622

Batch 232280, train_perplexity=164.16061, train_loss=5.1008453

Batch 232290, train_perplexity=186.24593, train_loss=5.227068

Batch 232300, train_perplexity=165.54182, train_loss=5.109224

Batch 232310, train_perplexity=190.37898, train_loss=5.249017

Batch 232320, train_perplexity=149.00227, train_loss=5.0039616

Batch 232330, train_perplexity=164.38002, train_loss=5.102181

Batch 232340, train_perplexity=155.4753, train_loss=5.046487

Batch 232350, train_perplexity=172.13109, train_loss=5.1482563

Batch 232360, train_perplexity=165.47292, train_loss=5.1088076

Batch 232370, train_perplexity=184.20042, train_loss=5.2160244

Batch 232380, train_perplexity=168.54343, train_loss=5.1271935

Batch 232390, train_perplexity=176.4781, train_loss=5.173197

Batch 232400, train_perplexity=158.27295, train_loss=5.064321

Batch 232410, train_perplexity=154.5229, train_loss=5.0403423

Batch 232420, train_perplexity=161.2233, train_loss=5.0827904

Batch 232430, train_perplexity=167.50891, train_loss=5.1210365

Batch 232440, train_perplexity=169.31415, train_loss=5.131756

Batch 232450, train_perplexity=173.8828, train_loss=5.1583815

Batch 232460, train_perplexity=173.8798, train_loss=5.1583643

Batch 232470, train_perplexity=185.3663, train_loss=5.222334

Batch 232480, train_perplexity=154.02841, train_loss=5.037137

Batch 232490, train_perplexity=166.01762, train_loss=5.112094

Batch 232500, train_perplexity=160.51337, train_loss=5.0783772

Batch 232510, train_perplexity=166.22943, train_loss=5.113369

Batch 232520, train_perplexity=147.44038, train_loss=4.993424

Batch 232530, train_perplexity=181.04433, train_loss=5.198742

Batch 232540, train_perplexity=161.7608, train_loss=5.0861187

Batch 232550, train_perplexity=174.87961, train_loss=5.164098

Batch 232560, train_perplexity=163.66977, train_loss=5.097851

Batch 232570, train_perplexity=159.65971, train_loss=5.073045

Batch 232580, train_perplexity=166.48662, train_loss=5.114915

Batch 232590, train_perplexity=176.4855, train_loss=5.1732388

Batch 232600, train_perplexity=133.85742, train_loss=4.8967752

Batch 232610, train_perplexity=156.25285, train_loss=5.0514755

Batch 232620, train_perplexity=172.90106, train_loss=5.1527195

Batch 232630, train_perplexity=211.49963, train_loss=5.3542233

Batch 232640, train_perplexity=168.37907, train_loss=5.126218

Batch 232650, train_perplexity=180.295, train_loss=5.1945944

Batch 232660, train_perplexity=168.12636, train_loss=5.124716

Batch 232670, train_perplexity=167.75734, train_loss=5.1225185

Batch 232680, train_perplexity=165.94054, train_loss=5.1116295

Batch 232690, train_perplexity=156.91289, train_loss=5.055691

Batch 232700, train_perplexity=160.9484, train_loss=5.081084

Batch 232710, train_perplexity=174.00555, train_loss=5.159087

Batch 232720, train_perplexity=162.94884, train_loss=5.0934362

Batch 232730, train_perplexity=168.98715, train_loss=5.1298227

Batch 232740, train_perplexity=177.08688, train_loss=5.1766405

Batch 232750, train_perplexity=169.35548, train_loss=5.132

Batch 232760, train_perplexity=169.62857, train_loss=5.133611

Batch 232770, train_perplexity=184.03835, train_loss=5.215144

Batch 232780, train_perplexity=185.697, train_loss=5.2241163

Batch 232790, train_perplexity=178.89056, train_loss=5.1867743

Batch 232800, train_perplexity=171.51872, train_loss=5.1446924

Batch 232810, train_perplexity=179.80511, train_loss=5.1918736

Batch 232820, train_perplexity=161.41623, train_loss=5.0839863

Batch 232830, train_perplexity=191.25848, train_loss=5.253626

Batch 232840, train_perplexity=169.413, train_loss=5.1323395

Batch 232850, train_perplexity=176.12863, train_loss=5.1712146

Batch 232860, train_perplexity=160.21843, train_loss=5.076538

Batch 232870, train_perplexity=167.49701, train_loss=5.1209655

Batch 232880, train_perplexity=148.78432, train_loss=5.0024977

Batch 232890, train_perplexity=166.02237, train_loss=5.1121225

Batch 232900, train_perplexity=164.1348, train_loss=5.100688

Batch 232910, train_perplexity=171.18617, train_loss=5.1427517

Batch 232920, train_perplexity=184.26122, train_loss=5.2163544

Batch 232930, train_perplexity=187.07742, train_loss=5.2315226

Batch 232940, train_perplexity=162.61615, train_loss=5.0913925

Batch 232950, train_perplexity=173.11835, train_loss=5.1539755

Batch 232960, train_perplexity=178.13945, train_loss=5.1825666

Batch 232970, train_perplexity=178.6397, train_loss=5.185371

Batch 232980, train_perplexity=166.47916, train_loss=5.11487

Batch 232990, train_perplexity=168.13156, train_loss=5.124747

Batch 233000, train_perplexity=166.15034, train_loss=5.112893

Batch 233010, train_perplexity=180.48767, train_loss=5.1956625

Batch 233020, train_perplexity=179.46745, train_loss=5.189994

Batch 233030, train_perplexity=167.04843, train_loss=5.1182837

Batch 233040, train_perplexity=172.7808, train_loss=5.152024

Batch 233050, train_perplexity=201.95116, train_loss=5.308026

Batch 233060, train_perplexity=177.28003, train_loss=5.1777306

Batch 233070, train_perplexity=158.618, train_loss=5.0664988

Batch 233080, train_perplexity=152.65218, train_loss=5.028162

Batch 233090, train_perplexity=162.57443, train_loss=5.091136

Batch 233100, train_perplexity=169.44968, train_loss=5.132556

Batch 233110, train_perplexity=168.77039, train_loss=5.128539

Batch 233120, train_perplexity=190.11392, train_loss=5.2476234

Batch 233130, train_perplexity=166.38614, train_loss=5.114311

Batch 233140, train_perplexity=160.72345, train_loss=5.079685

Batch 233150, train_perplexity=172.3193, train_loss=5.149349

Batch 233160, train_perplexity=166.40486, train_loss=5.1144238

Batch 233170, train_perplexity=170.50163, train_loss=5.138745

Batch 233180, train_perplexity=143.7803, train_loss=4.9682865

Batch 233190, train_perplexity=179.35898, train_loss=5.189389

Batch 233200, train_perplexity=174.89828, train_loss=5.1642046

Batch 233210, train_perplexity=160.08434, train_loss=5.0757008

Batch 233220, train_perplexity=174.30783, train_loss=5.160823

Batch 233230, train_perplexity=162.39445, train_loss=5.0900283

Batch 233240, train_perplexity=193.3937, train_loss=5.264728

Batch 233250, train_perplexity=167.43736, train_loss=5.1206093

Batch 233260, train_perplexity=163.19307, train_loss=5.094934

Batch 233270, train_perplexity=169.1292, train_loss=5.130663

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306362 sentences.
Finished loading
Batch 233280, train_perplexity=165.98326, train_loss=5.111887

Batch 233290, train_perplexity=160.33842, train_loss=5.0772867

Batch 233300, train_perplexity=155.73633, train_loss=5.0481644

Batch 233310, train_perplexity=163.6946, train_loss=5.0980024

Batch 233320, train_perplexity=172.98575, train_loss=5.153209

Batch 233330, train_perplexity=166.06038, train_loss=5.1123514

Batch 233340, train_perplexity=180.85796, train_loss=5.197712

Batch 233350, train_perplexity=168.43738, train_loss=5.126564

Batch 233360, train_perplexity=168.82881, train_loss=5.1288853

Batch 233370, train_perplexity=174.90463, train_loss=5.164241

Batch 233380, train_perplexity=184.60507, train_loss=5.218219

Batch 233390, train_perplexity=162.53506, train_loss=5.0908937

Batch 233400, train_perplexity=177.64195, train_loss=5.17977

Batch 233410, train_perplexity=167.15656, train_loss=5.118931

Batch 233420, train_perplexity=172.69177, train_loss=5.1515083

Batch 233430, train_perplexity=173.30643, train_loss=5.1550612

Batch 233440, train_perplexity=175.01793, train_loss=5.1648884

Batch 233450, train_perplexity=169.71603, train_loss=5.1341267

Batch 233460, train_perplexity=194.73132, train_loss=5.2716208

Batch 233470, train_perplexity=147.38226, train_loss=4.9930296

Batch 233480, train_perplexity=156.76562, train_loss=5.054752

Batch 233490, train_perplexity=154.3352, train_loss=5.039127

Batch 233500, train_perplexity=179.03828, train_loss=5.1875997

Batch 233510, train_perplexity=163.47119, train_loss=5.096637

Batch 233520, train_perplexity=151.688, train_loss=5.021826

Batch 233530, train_perplexity=175.56607, train_loss=5.1680155

Batch 233540, train_perplexity=169.89304, train_loss=5.135169

Batch 233550, train_perplexity=182.72008, train_loss=5.2079554

Batch 233560, train_perplexity=163.86812, train_loss=5.099062

Batch 233570, train_perplexity=177.60121, train_loss=5.1795406

Batch 233580, train_perplexity=177.00938, train_loss=5.176203

Batch 233590, train_perplexity=186.3966, train_loss=5.2278767

Batch 233600, train_perplexity=181.63388, train_loss=5.201993

Batch 233610, train_perplexity=161.6036, train_loss=5.0851464

Batch 233620, train_perplexity=160.31075, train_loss=5.077114

Batch 233630, train_perplexity=164.40675, train_loss=5.1023436

Batch 233640, train_perplexity=182.03064, train_loss=5.204175

Batch 233650, train_perplexity=153.39052, train_loss=5.032987

Batch 233660, train_perplexity=160.29271, train_loss=5.0770016

Batch 233670, train_perplexity=172.33583, train_loss=5.149445

Batch 233680, train_perplexity=157.04118, train_loss=5.056508

Batch 233690, train_perplexity=179.54535, train_loss=5.190428

Batch 233700, train_perplexity=176.92146, train_loss=5.175706

Batch 233710, train_perplexity=181.53949, train_loss=5.201473

Batch 233720, train_perplexity=186.59473, train_loss=5.228939

Batch 233730, train_perplexity=160.83263, train_loss=5.080364

Batch 233740, train_perplexity=186.34923, train_loss=5.2276225

Batch 233750, train_perplexity=163.68147, train_loss=5.0979223

Batch 233760, train_perplexity=187.18959, train_loss=5.232122

Batch 233770, train_perplexity=175.88583, train_loss=5.169835

Batch 233780, train_perplexity=167.58896, train_loss=5.1215143

Batch 233790, train_perplexity=149.02827, train_loss=5.004136

Batch 233800, train_perplexity=187.43584, train_loss=5.2334366

Batch 233810, train_perplexity=191.30792, train_loss=5.2538843

Batch 233820, train_perplexity=178.50635, train_loss=5.184624

Batch 233830, train_perplexity=169.81302, train_loss=5.134698

Batch 233840, train_perplexity=209.11824, train_loss=5.3429

Batch 233850, train_perplexity=168.04637, train_loss=5.12424

Batch 233860, train_perplexity=151.27138, train_loss=5.0190754

Batch 233870, train_perplexity=170.29094, train_loss=5.1375084

Batch 233880, train_perplexity=188.03848, train_loss=5.2366467

Batch 233890, train_perplexity=167.46921, train_loss=5.1207995

Batch 233900, train_perplexity=169.34273, train_loss=5.1319246

Batch 233910, train_perplexity=184.51689, train_loss=5.217741

Batch 233920, train_perplexity=170.70232, train_loss=5.139921

Batch 233930, train_perplexity=177.31038, train_loss=5.1779017

Batch 233940, train_perplexity=172.9149, train_loss=5.1527996

Batch 233950, train_perplexity=174.44678, train_loss=5.1616197

Batch 233960, train_perplexity=181.20393, train_loss=5.199623

Batch 233970, train_perplexity=169.39296, train_loss=5.132221

Batch 233980, train_perplexity=164.26038, train_loss=5.101453

Batch 233990, train_perplexity=186.25462, train_loss=5.2271147

Batch 234000, train_perplexity=168.39224, train_loss=5.126296

Batch 234010, train_perplexity=170.012, train_loss=5.135869

Batch 234020, train_perplexity=174.73108, train_loss=5.163248

Batch 234030, train_perplexity=139.37514, train_loss=4.937169

Batch 234040, train_perplexity=176.75745, train_loss=5.1747785

Batch 234050, train_perplexity=194.69046, train_loss=5.271411

Batch 234060, train_perplexity=167.04134, train_loss=5.1182413

Batch 234070, train_perplexity=159.20479, train_loss=5.0701914

Batch 234080, train_perplexity=187.04924, train_loss=5.231372

Batch 234090, train_perplexity=169.65552, train_loss=5.13377

Batch 234100, train_perplexity=172.84195, train_loss=5.1523776

Batch 234110, train_perplexity=170.95834, train_loss=5.14142

Batch 234120, train_perplexity=165.70862, train_loss=5.110231

Batch 234130, train_perplexity=177.359, train_loss=5.178176

Batch 234140, train_perplexity=169.1976, train_loss=5.1310673

Batch 234150, train_perplexity=175.51427, train_loss=5.1677203

Batch 234160, train_perplexity=186.43945, train_loss=5.2281065

Batch 234170, train_perplexity=170.41637, train_loss=5.1382446

Batch 234180, train_perplexity=196.42976, train_loss=5.280305

Batch 234190, train_perplexity=182.595, train_loss=5.2072706

Batch 234200, train_perplexity=187.2336, train_loss=5.232357

Batch 234210, train_perplexity=184.18074, train_loss=5.2159176

Batch 234220, train_perplexity=157.26208, train_loss=5.057914

Batch 234230, train_perplexity=157.20302, train_loss=5.057538

Batch 234240, train_perplexity=169.40863, train_loss=5.1323137

Batch 234250, train_perplexity=163.13829, train_loss=5.0945983

Batch 234260, train_perplexity=178.56041, train_loss=5.184927

Batch 234270, train_perplexity=179.38258, train_loss=5.189521

Batch 234280, train_perplexity=157.3873, train_loss=5.0587096

Batch 234290, train_perplexity=157.08911, train_loss=5.0568132

Batch 234300, train_perplexity=170.77168, train_loss=5.1403275

Batch 234310, train_perplexity=169.15323, train_loss=5.130805

Batch 234320, train_perplexity=196.9471, train_loss=5.282935

Batch 234330, train_perplexity=166.97429, train_loss=5.11784

Batch 234340, train_perplexity=146.04605, train_loss=4.983922

Batch 234350, train_perplexity=176.58333, train_loss=5.173793

Batch 234360, train_perplexity=158.58336, train_loss=5.0662804

Batch 234370, train_perplexity=157.02808, train_loss=5.0564246

Batch 234380, train_perplexity=158.7869, train_loss=5.067563

Batch 234390, train_perplexity=176.5222, train_loss=5.1734467

Batch 234400, train_perplexity=167.63603, train_loss=5.121795

Batch 234410, train_perplexity=176.28482, train_loss=5.172101

Batch 234420, train_perplexity=177.72252, train_loss=5.1802235

Batch 234430, train_perplexity=174.82675, train_loss=5.1637955

Batch 234440, train_perplexity=185.55609, train_loss=5.223357

Batch 234450, train_perplexity=180.47202, train_loss=5.1955757

Batch 234460, train_perplexity=179.47687, train_loss=5.1900463

Batch 234470, train_perplexity=173.99991, train_loss=5.1590548

Batch 234480, train_perplexity=166.37439, train_loss=5.1142406

Batch 234490, train_perplexity=176.54594, train_loss=5.173581

Batch 234500, train_perplexity=195.24297, train_loss=5.274245

Batch 234510, train_perplexity=194.53976, train_loss=5.2706366

Batch 234520, train_perplexity=200.5215, train_loss=5.3009214

Batch 234530, train_perplexity=179.01746, train_loss=5.1874833

Batch 234540, train_perplexity=181.04855, train_loss=5.1987653

Batch 234550, train_perplexity=163.032, train_loss=5.0939465

Batch 234560, train_perplexity=176.56952, train_loss=5.1737146
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 234570, train_perplexity=176.54274, train_loss=5.173563

Batch 234580, train_perplexity=173.94905, train_loss=5.1587625

Batch 234590, train_perplexity=167.37589, train_loss=5.120242

Batch 234600, train_perplexity=191.3676, train_loss=5.254196

Batch 234610, train_perplexity=169.22842, train_loss=5.1312494

Batch 234620, train_perplexity=182.21873, train_loss=5.205208

Batch 234630, train_perplexity=160.24342, train_loss=5.076694

Batch 234640, train_perplexity=167.33551, train_loss=5.120001

Batch 234650, train_perplexity=149.82623, train_loss=5.009476

Batch 234660, train_perplexity=160.77826, train_loss=5.080026

Batch 234670, train_perplexity=176.88013, train_loss=5.1754723

Batch 234680, train_perplexity=169.48943, train_loss=5.1327906

Batch 234690, train_perplexity=160.70828, train_loss=5.079591

Batch 234700, train_perplexity=171.58597, train_loss=5.1450844

Batch 234710, train_perplexity=182.9218, train_loss=5.209059

Batch 234720, train_perplexity=160.82411, train_loss=5.0803113

Batch 234730, train_perplexity=182.956, train_loss=5.2092457

Batch 234740, train_perplexity=174.50842, train_loss=5.161973

Batch 234750, train_perplexity=184.20735, train_loss=5.216062

Batch 234760, train_perplexity=164.59642, train_loss=5.1034966

Batch 234770, train_perplexity=169.31584, train_loss=5.131766

Batch 234780, train_perplexity=147.87502, train_loss=4.9963675

Batch 234790, train_perplexity=166.881, train_loss=5.117281

Batch 234800, train_perplexity=168.11993, train_loss=5.1246777

Batch 234810, train_perplexity=172.45404, train_loss=5.1501307

Batch 234820, train_perplexity=167.95256, train_loss=5.1236815

Batch 234830, train_perplexity=167.53894, train_loss=5.121216

Batch 234840, train_perplexity=156.42923, train_loss=5.0526037

Batch 234850, train_perplexity=160.07312, train_loss=5.0756307

Batch 234860, train_perplexity=173.37775, train_loss=5.1554728

Batch 234870, train_perplexity=164.17393, train_loss=5.1009264

Batch 234880, train_perplexity=161.41261, train_loss=5.083964

Batch 234890, train_perplexity=170.28006, train_loss=5.1374445

Batch 234900, train_perplexity=176.10637, train_loss=5.171088

Batch 234910, train_perplexity=172.63324, train_loss=5.1511693

Batch 234920, train_perplexity=141.57288, train_loss=4.9528146

Batch 234930, train_perplexity=145.94044, train_loss=4.9831986

Batch 234940, train_perplexity=179.47516, train_loss=5.190037

Batch 234950, train_perplexity=160.42453, train_loss=5.0778236

Batch 234960, train_perplexity=167.63484, train_loss=5.121788

Batch 234970, train_perplexity=162.31851, train_loss=5.0895605

Batch 234980, train_perplexity=172.57948, train_loss=5.150858

Batch 234990, train_perplexity=173.54417, train_loss=5.156432

Batch 235000, train_perplexity=165.90271, train_loss=5.1114016

Batch 235010, train_perplexity=159.8924, train_loss=5.074501

Batch 235020, train_perplexity=160.54813, train_loss=5.0785937

Batch 235030, train_perplexity=167.01721, train_loss=5.118097

Batch 235040, train_perplexity=151.00162, train_loss=5.0172906

Batch 235050, train_perplexity=177.33972, train_loss=5.178067

Batch 235060, train_perplexity=166.10616, train_loss=5.112627

Batch 235070, train_perplexity=185.3458, train_loss=5.2222233

Batch 235080, train_perplexity=161.47173, train_loss=5.08433

Batch 235090, train_perplexity=169.38536, train_loss=5.1321764

Batch 235100, train_perplexity=171.97554, train_loss=5.147352

Batch 235110, train_perplexity=184.06442, train_loss=5.215286

Batch 235120, train_perplexity=166.88927, train_loss=5.1173306

Batch 235130, train_perplexity=159.41476, train_loss=5.0715094

Batch 235140, train_perplexity=160.03526, train_loss=5.075394

Batch 235150, train_perplexity=169.77132, train_loss=5.1344523

Batch 235160, train_perplexity=175.11284, train_loss=5.1654305

Batch 235170, train_perplexity=185.76173, train_loss=5.224465

Batch 235180, train_perplexity=186.25978, train_loss=5.2271423

Batch 235190, train_perplexity=174.58466, train_loss=5.16241

Batch 235200, train_perplexity=184.8741, train_loss=5.219675

Batch 235210, train_perplexity=173.85626, train_loss=5.158229

Batch 235220, train_perplexity=205.45563, train_loss=5.32523

Batch 235230, train_perplexity=167.28302, train_loss=5.119687

Batch 235240, train_perplexity=170.51373, train_loss=5.138816

Batch 235250, train_perplexity=183.16585, train_loss=5.210392

Batch 235260, train_perplexity=175.13547, train_loss=5.16556

Batch 235270, train_perplexity=167.05687, train_loss=5.1183343

Batch 235280, train_perplexity=170.74481, train_loss=5.14017

Batch 235290, train_perplexity=172.70396, train_loss=5.151579

Batch 235300, train_perplexity=200.31497, train_loss=5.299891

Batch 235310, train_perplexity=195.59697, train_loss=5.2760563

Batch 235320, train_perplexity=191.52278, train_loss=5.255007

Batch 235330, train_perplexity=152.30006, train_loss=5.0258527

Batch 235340, train_perplexity=150.72884, train_loss=5.0154824

Batch 235350, train_perplexity=150.41336, train_loss=5.013387

Batch 235360, train_perplexity=174.57468, train_loss=5.1623526

Batch 235370, train_perplexity=174.86226, train_loss=5.1639986

Batch 235380, train_perplexity=181.00238, train_loss=5.19851

Batch 235390, train_perplexity=181.72795, train_loss=5.202511

Batch 235400, train_perplexity=191.93199, train_loss=5.257141

Batch 235410, train_perplexity=160.84674, train_loss=5.080452

Batch 235420, train_perplexity=168.7596, train_loss=5.128475

Batch 235430, train_perplexity=187.14827, train_loss=5.231901

Batch 235440, train_perplexity=157.79962, train_loss=5.061326

Batch 235450, train_perplexity=170.67561, train_loss=5.139765

Batch 235460, train_perplexity=168.74527, train_loss=5.1283903

Batch 235470, train_perplexity=155.98352, train_loss=5.0497503

Batch 235480, train_perplexity=194.13693, train_loss=5.2685637

Batch 235490, train_perplexity=173.24239, train_loss=5.1546917

Batch 235500, train_perplexity=165.92249, train_loss=5.111521

Batch 235510, train_perplexity=142.02603, train_loss=4.9560103

Batch 235520, train_perplexity=177.04796, train_loss=5.1764207

Batch 235530, train_perplexity=174.30217, train_loss=5.1607904

Batch 235540, train_perplexity=175.8626, train_loss=5.169703

Batch 235550, train_perplexity=158.67601, train_loss=5.0668645

Batch 235560, train_perplexity=143.47485, train_loss=4.96616

Batch 235570, train_perplexity=192.53955, train_loss=5.2603016

Batch 235580, train_perplexity=155.88536, train_loss=5.049121

Batch 235590, train_perplexity=174.18105, train_loss=5.160095

Batch 235600, train_perplexity=171.43254, train_loss=5.14419

Batch 235610, train_perplexity=171.78998, train_loss=5.1462727

Batch 235620, train_perplexity=173.8934, train_loss=5.1584425

Batch 235630, train_perplexity=169.5284, train_loss=5.1330204

Batch 235640, train_perplexity=164.26758, train_loss=5.1014967

Batch 235650, train_perplexity=173.09326, train_loss=5.1538305

Batch 235660, train_perplexity=183.38292, train_loss=5.2115765

Batch 235670, train_perplexity=167.29785, train_loss=5.119776

Batch 235680, train_perplexity=152.10274, train_loss=5.024556

Batch 235690, train_perplexity=184.46498, train_loss=5.2174597

Batch 235700, train_perplexity=163.29037, train_loss=5.09553

Batch 235710, train_perplexity=172.28316, train_loss=5.1491394

Batch 235720, train_perplexity=165.12744, train_loss=5.1067176

Batch 235730, train_perplexity=171.05717, train_loss=5.141998

Batch 235740, train_perplexity=153.18448, train_loss=5.031643

Batch 235750, train_perplexity=180.9116, train_loss=5.1980085

Batch 235760, train_perplexity=179.52771, train_loss=5.1903296

Batch 235770, train_perplexity=153.71716, train_loss=5.0351143

Batch 235780, train_perplexity=167.32019, train_loss=5.1199093

Batch 235790, train_perplexity=175.2068, train_loss=5.165967

Batch 235800, train_perplexity=170.97945, train_loss=5.1415434

Batch 235810, train_perplexity=146.97339, train_loss=4.9902515

Batch 235820, train_perplexity=164.93016, train_loss=5.105522

Batch 235830, train_perplexity=151.76527, train_loss=5.022335

Batch 235840, train_perplexity=168.31517, train_loss=5.1258383

Batch 235850, train_perplexity=160.84904, train_loss=5.0804663

Batch 235860, train_perplexity=146.89899, train_loss=4.989745
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 235870, train_perplexity=166.04929, train_loss=5.1122847

Batch 235880, train_perplexity=153.49873, train_loss=5.0336924

Batch 235890, train_perplexity=166.34473, train_loss=5.1140623

Batch 235900, train_perplexity=192.44759, train_loss=5.259824

Batch 235910, train_perplexity=159.64723, train_loss=5.0729666

Batch 235920, train_perplexity=189.74306, train_loss=5.245671

Batch 235930, train_perplexity=181.28482, train_loss=5.2000694

Batch 235940, train_perplexity=184.52815, train_loss=5.217802

Batch 235950, train_perplexity=169.1959, train_loss=5.1310573

Batch 235960, train_perplexity=188.03795, train_loss=5.236644

Batch 235970, train_perplexity=157.68507, train_loss=5.0606

Batch 235980, train_perplexity=149.27371, train_loss=5.0057817

Batch 235990, train_perplexity=189.23805, train_loss=5.2430058

Batch 236000, train_perplexity=180.4147, train_loss=5.195258

Batch 236010, train_perplexity=162.16339, train_loss=5.0886045

Batch 236020, train_perplexity=178.34631, train_loss=5.1837273

Batch 236030, train_perplexity=188.37071, train_loss=5.238412

Batch 236040, train_perplexity=159.81221, train_loss=5.0739994

Batch 236050, train_perplexity=148.09674, train_loss=4.9978657

Batch 236060, train_perplexity=164.25818, train_loss=5.1014395

Batch 236070, train_perplexity=170.57733, train_loss=5.139189

Batch 236080, train_perplexity=170.45148, train_loss=5.1384506

Batch 236090, train_perplexity=166.9754, train_loss=5.1178465

Batch 236100, train_perplexity=175.78622, train_loss=5.1692686

Batch 236110, train_perplexity=175.59521, train_loss=5.1681814

Batch 236120, train_perplexity=172.06577, train_loss=5.1478767

Batch 236130, train_perplexity=171.0553, train_loss=5.141987

Batch 236140, train_perplexity=178.2472, train_loss=5.1831713

Batch 236150, train_perplexity=154.50699, train_loss=5.0402393

Batch 236160, train_perplexity=159.89468, train_loss=5.0745153

Batch 236170, train_perplexity=156.35078, train_loss=5.052102

Batch 236180, train_perplexity=160.29134, train_loss=5.076993

Batch 236190, train_perplexity=181.67545, train_loss=5.202222

Batch 236200, train_perplexity=157.56444, train_loss=5.0598345

Batch 236210, train_perplexity=167.5355, train_loss=5.1211953

Batch 236220, train_perplexity=174.45683, train_loss=5.1616774

Batch 236230, train_perplexity=165.76741, train_loss=5.1105857

Batch 236240, train_perplexity=166.45256, train_loss=5.1147103

Batch 236250, train_perplexity=156.81085, train_loss=5.0550404

Batch 236260, train_perplexity=176.56177, train_loss=5.173671

Batch 236270, train_perplexity=178.72652, train_loss=5.185857

Batch 236280, train_perplexity=167.1945, train_loss=5.119158

Batch 236290, train_perplexity=160.08876, train_loss=5.0757284

Batch 236300, train_perplexity=164.13057, train_loss=5.100662

Batch 236310, train_perplexity=159.58063, train_loss=5.0725493

Batch 236320, train_perplexity=164.9602, train_loss=5.1057043

Batch 236330, train_perplexity=165.91972, train_loss=5.111504

Batch 236340, train_perplexity=161.03207, train_loss=5.0816035

Batch 236350, train_perplexity=162.18388, train_loss=5.088731

Batch 236360, train_perplexity=151.6854, train_loss=5.0218086

Batch 236370, train_perplexity=169.62671, train_loss=5.1336

Batch 236380, train_perplexity=193.07573, train_loss=5.2630825

Batch 236390, train_perplexity=168.2635, train_loss=5.125531

Batch 236400, train_perplexity=163.26086, train_loss=5.0953493

Batch 236410, train_perplexity=170.19385, train_loss=5.136938

Batch 236420, train_perplexity=182.176, train_loss=5.204973

Batch 236430, train_perplexity=158.45515, train_loss=5.0654716

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00045-of-00100
Loaded 306088 sentences.
Finished loading
Batch 236440, train_perplexity=181.17612, train_loss=5.1994696

Batch 236450, train_perplexity=153.08998, train_loss=5.031026

Batch 236460, train_perplexity=153.9334, train_loss=5.03652

Batch 236470, train_perplexity=154.48341, train_loss=5.0400867

Batch 236480, train_perplexity=179.58293, train_loss=5.190637

Batch 236490, train_perplexity=162.37022, train_loss=5.089879

Batch 236500, train_perplexity=171.87257, train_loss=5.1467533

Batch 236510, train_perplexity=163.34964, train_loss=5.095893

Batch 236520, train_perplexity=180.8935, train_loss=5.1979084

Batch 236530, train_perplexity=175.8595, train_loss=5.1696854

Batch 236540, train_perplexity=154.51141, train_loss=5.040268

Batch 236550, train_perplexity=147.45087, train_loss=4.993495

Batch 236560, train_perplexity=167.57674, train_loss=5.1214414

Batch 236570, train_perplexity=161.95206, train_loss=5.0873003

Batch 236580, train_perplexity=159.45915, train_loss=5.071788

Batch 236590, train_perplexity=168.11818, train_loss=5.124667

Batch 236600, train_perplexity=185.14723, train_loss=5.2211514

Batch 236610, train_perplexity=167.39009, train_loss=5.120327

Batch 236620, train_perplexity=181.64687, train_loss=5.2020645

Batch 236630, train_perplexity=172.92992, train_loss=5.1528864

Batch 236640, train_perplexity=181.19789, train_loss=5.1995897

Batch 236650, train_perplexity=163.86281, train_loss=5.0990295

Batch 236660, train_perplexity=166.24925, train_loss=5.113488

Batch 236670, train_perplexity=170.95206, train_loss=5.141383

Batch 236680, train_perplexity=157.55167, train_loss=5.0597534

Batch 236690, train_perplexity=159.25385, train_loss=5.0704994

Batch 236700, train_perplexity=173.6344, train_loss=5.156952

Batch 236710, train_perplexity=157.8441, train_loss=5.061608

Batch 236720, train_perplexity=165.30037, train_loss=5.1077642

Batch 236730, train_perplexity=163.14755, train_loss=5.094655

Batch 236740, train_perplexity=180.0383, train_loss=5.1931696

Batch 236750, train_perplexity=173.65327, train_loss=5.1570606

Batch 236760, train_perplexity=153.31557, train_loss=5.0324984

Batch 236770, train_perplexity=169.60965, train_loss=5.1334996

Batch 236780, train_perplexity=161.87747, train_loss=5.0868397

Batch 236790, train_perplexity=199.92154, train_loss=5.297925

Batch 236800, train_perplexity=179.8027, train_loss=5.19186

Batch 236810, train_perplexity=180.59494, train_loss=5.1962566

Batch 236820, train_perplexity=164.40173, train_loss=5.102313

Batch 236830, train_perplexity=175.41562, train_loss=5.167158

Batch 236840, train_perplexity=178.46976, train_loss=5.184419

Batch 236850, train_perplexity=188.93349, train_loss=5.241395

Batch 236860, train_perplexity=160.21812, train_loss=5.076536

Batch 236870, train_perplexity=163.9311, train_loss=5.0994463

Batch 236880, train_perplexity=188.45041, train_loss=5.238835

Batch 236890, train_perplexity=167.46594, train_loss=5.12078

Batch 236900, train_perplexity=172.20103, train_loss=5.1486626

Batch 236910, train_perplexity=166.66422, train_loss=5.115981

Batch 236920, train_perplexity=168.46187, train_loss=5.1267095

Batch 236930, train_perplexity=174.86526, train_loss=5.164016

Batch 236940, train_perplexity=158.19983, train_loss=5.063859

Batch 236950, train_perplexity=188.41023, train_loss=5.2386217

Batch 236960, train_perplexity=163.75174, train_loss=5.0983515

Batch 236970, train_perplexity=170.72487, train_loss=5.1400533

Batch 236980, train_perplexity=166.69887, train_loss=5.116189

Batch 236990, train_perplexity=170.30475, train_loss=5.1375895

Batch 237000, train_perplexity=182.49908, train_loss=5.206745

Batch 237010, train_perplexity=148.91576, train_loss=5.003381

Batch 237020, train_perplexity=185.67363, train_loss=5.2239904

Batch 237030, train_perplexity=164.29955, train_loss=5.1016912

Batch 237040, train_perplexity=157.30199, train_loss=5.0581675

Batch 237050, train_perplexity=152.60007, train_loss=5.0278206

Batch 237060, train_perplexity=185.00374, train_loss=5.220376

Batch 237070, train_perplexity=177.5223, train_loss=5.179096

Batch 237080, train_perplexity=183.35057, train_loss=5.2114

Batch 237090, train_perplexity=158.78659, train_loss=5.067561

Batch 237100, train_perplexity=171.37042, train_loss=5.1438274
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 237110, train_perplexity=168.94478, train_loss=5.129572

Batch 237120, train_perplexity=155.91882, train_loss=5.0493355

Batch 237130, train_perplexity=174.03085, train_loss=5.1592326

Batch 237140, train_perplexity=144.34236, train_loss=4.972188

Batch 237150, train_perplexity=160.80984, train_loss=5.0802226

Batch 237160, train_perplexity=159.70624, train_loss=5.073336

Batch 237170, train_perplexity=170.60255, train_loss=5.1393366

Batch 237180, train_perplexity=176.5685, train_loss=5.173709

Batch 237190, train_perplexity=166.60796, train_loss=5.1156435

Batch 237200, train_perplexity=188.22939, train_loss=5.2376614

Batch 237210, train_perplexity=179.37437, train_loss=5.189475

Batch 237220, train_perplexity=159.97736, train_loss=5.075032

Batch 237230, train_perplexity=167.43912, train_loss=5.12062

Batch 237240, train_perplexity=175.11476, train_loss=5.1654415

Batch 237250, train_perplexity=194.9305, train_loss=5.272643

Batch 237260, train_perplexity=222.58598, train_loss=5.4053135

Batch 237270, train_perplexity=161.5791, train_loss=5.084995

Batch 237280, train_perplexity=170.10022, train_loss=5.136388

Batch 237290, train_perplexity=160.53366, train_loss=5.0785036

Batch 237300, train_perplexity=184.61212, train_loss=5.218257

Batch 237310, train_perplexity=193.2078, train_loss=5.2637663

Batch 237320, train_perplexity=197.61801, train_loss=5.286336

Batch 237330, train_perplexity=176.1141, train_loss=5.171132

Batch 237340, train_perplexity=171.39143, train_loss=5.14395

Batch 237350, train_perplexity=164.76808, train_loss=5.104539

Batch 237360, train_perplexity=155.38474, train_loss=5.045904

Batch 237370, train_perplexity=166.2261, train_loss=5.113349

Batch 237380, train_perplexity=169.74841, train_loss=5.1343174

Batch 237390, train_perplexity=183.97763, train_loss=5.214814

Batch 237400, train_perplexity=165.38481, train_loss=5.108275

Batch 237410, train_perplexity=170.06795, train_loss=5.136198

Batch 237420, train_perplexity=184.27737, train_loss=5.216442

Batch 237430, train_perplexity=153.1676, train_loss=5.031533

Batch 237440, train_perplexity=171.5709, train_loss=5.1449966

Batch 237450, train_perplexity=159.44128, train_loss=5.071676

Batch 237460, train_perplexity=156.87914, train_loss=5.0554757

Batch 237470, train_perplexity=168.6764, train_loss=5.127982

Batch 237480, train_perplexity=167.457, train_loss=5.1207266

Batch 237490, train_perplexity=160.46233, train_loss=5.078059

Batch 237500, train_perplexity=174.88928, train_loss=5.164153

Batch 237510, train_perplexity=148.4831, train_loss=5.000471

Batch 237520, train_perplexity=168.64601, train_loss=5.127802

Batch 237530, train_perplexity=156.41312, train_loss=5.0525007

Batch 237540, train_perplexity=179.9976, train_loss=5.1929436

Batch 237550, train_perplexity=172.10687, train_loss=5.1481156

Batch 237560, train_perplexity=172.05804, train_loss=5.147832

Batch 237570, train_perplexity=163.5532, train_loss=5.0971384

Batch 237580, train_perplexity=173.80023, train_loss=5.1579065

Batch 237590, train_perplexity=169.53049, train_loss=5.133033

Batch 237600, train_perplexity=154.89752, train_loss=5.0427637

Batch 237610, train_perplexity=186.23456, train_loss=5.227007

Batch 237620, train_perplexity=183.98149, train_loss=5.214835

Batch 237630, train_perplexity=171.16039, train_loss=5.142601

Batch 237640, train_perplexity=170.37517, train_loss=5.138003

Batch 237650, train_perplexity=186.2136, train_loss=5.2268944

Batch 237660, train_perplexity=170.95174, train_loss=5.1413813

Batch 237670, train_perplexity=149.87718, train_loss=5.009816

Batch 237680, train_perplexity=160.18811, train_loss=5.076349

Batch 237690, train_perplexity=164.63065, train_loss=5.1037045

Batch 237700, train_perplexity=161.47589, train_loss=5.084356

Batch 237710, train_perplexity=179.4529, train_loss=5.189913

Batch 237720, train_perplexity=186.96819, train_loss=5.2309384

Batch 237730, train_perplexity=171.84315, train_loss=5.146582

Batch 237740, train_perplexity=171.42305, train_loss=5.1441345

Batch 237750, train_perplexity=161.51477, train_loss=5.0845966

Batch 237760, train_perplexity=168.84991, train_loss=5.12901

Batch 237770, train_perplexity=161.3282, train_loss=5.083441

Batch 237780, train_perplexity=171.19711, train_loss=5.1428156

Batch 237790, train_perplexity=184.52798, train_loss=5.217801

Batch 237800, train_perplexity=174.24019, train_loss=5.1604347

Batch 237810, train_perplexity=176.38724, train_loss=5.172682

Batch 237820, train_perplexity=155.67708, train_loss=5.047784

Batch 237830, train_perplexity=194.58688, train_loss=5.270879

Batch 237840, train_perplexity=173.37445, train_loss=5.1554537

Batch 237850, train_perplexity=164.75905, train_loss=5.104484

Batch 237860, train_perplexity=187.61198, train_loss=5.234376

Batch 237870, train_perplexity=167.19409, train_loss=5.1191554

Batch 237880, train_perplexity=183.67451, train_loss=5.2131653

Batch 237890, train_perplexity=172.31586, train_loss=5.149329

Batch 237900, train_perplexity=177.69507, train_loss=5.180069

Batch 237910, train_perplexity=135.57764, train_loss=4.9095445

Batch 237920, train_perplexity=167.2122, train_loss=5.1192636

Batch 237930, train_perplexity=171.60576, train_loss=5.1452

Batch 237940, train_perplexity=193.71849, train_loss=5.266406

Batch 237950, train_perplexity=155.07407, train_loss=5.043903

Batch 237960, train_perplexity=185.75563, train_loss=5.224432

Batch 237970, train_perplexity=155.40059, train_loss=5.046006

Batch 237980, train_perplexity=167.55301, train_loss=5.1212997

Batch 237990, train_perplexity=168.19467, train_loss=5.125122

Batch 238000, train_perplexity=154.14354, train_loss=5.037884

Batch 238010, train_perplexity=163.40884, train_loss=5.0962553

Batch 238020, train_perplexity=161.84528, train_loss=5.086641

Batch 238030, train_perplexity=156.06178, train_loss=5.050252

Batch 238040, train_perplexity=159.39575, train_loss=5.07139

Batch 238050, train_perplexity=163.92744, train_loss=5.099424

Batch 238060, train_perplexity=168.1217, train_loss=5.124688

Batch 238070, train_perplexity=153.66461, train_loss=5.0347724

Batch 238080, train_perplexity=169.50543, train_loss=5.132885

Batch 238090, train_perplexity=176.66046, train_loss=5.1742296

Batch 238100, train_perplexity=150.59485, train_loss=5.014593

Batch 238110, train_perplexity=169.64613, train_loss=5.1337147

Batch 238120, train_perplexity=179.37659, train_loss=5.1894875

Batch 238130, train_perplexity=161.39467, train_loss=5.083853

Batch 238140, train_perplexity=164.1344, train_loss=5.1006856

Batch 238150, train_perplexity=172.0195, train_loss=5.147608

Batch 238160, train_perplexity=199.20253, train_loss=5.294322

Batch 238170, train_perplexity=156.6421, train_loss=5.0539637

Batch 238180, train_perplexity=170.78911, train_loss=5.1404295

Batch 238190, train_perplexity=172.9949, train_loss=5.153262

Batch 238200, train_perplexity=175.33083, train_loss=5.1666746

Batch 238210, train_perplexity=147.17902, train_loss=4.9916496

Batch 238220, train_perplexity=183.61165, train_loss=5.212823

Batch 238230, train_perplexity=163.15471, train_loss=5.094699

Batch 238240, train_perplexity=174.92873, train_loss=5.1643786

Batch 238250, train_perplexity=155.20871, train_loss=5.0447707

Batch 238260, train_perplexity=164.98665, train_loss=5.1058645

Batch 238270, train_perplexity=196.24448, train_loss=5.2793612

Batch 238280, train_perplexity=163.89423, train_loss=5.099221

Batch 238290, train_perplexity=163.51984, train_loss=5.0969343

Batch 238300, train_perplexity=208.92299, train_loss=5.3419657

Batch 238310, train_perplexity=177.87776, train_loss=5.1810966

Batch 238320, train_perplexity=155.58253, train_loss=5.0471764

Batch 238330, train_perplexity=172.24011, train_loss=5.1488895

Batch 238340, train_perplexity=161.70003, train_loss=5.085743

Batch 238350, train_perplexity=173.17697, train_loss=5.154314

Batch 238360, train_perplexity=189.52087, train_loss=5.244499

Batch 238370, train_perplexity=165.78046, train_loss=5.1106644

Batch 238380, train_perplexity=179.08286, train_loss=5.1878486

Batch 238390, train_perplexity=150.3737, train_loss=5.0131235

Batch 238400, train_perplexity=171.38333, train_loss=5.143903
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 238410, train_perplexity=160.68552, train_loss=5.079449

Batch 238420, train_perplexity=162.37262, train_loss=5.089894

Batch 238430, train_perplexity=163.89125, train_loss=5.099203

Batch 238440, train_perplexity=175.39822, train_loss=5.167059

Batch 238450, train_perplexity=166.01549, train_loss=5.112081

Batch 238460, train_perplexity=166.90854, train_loss=5.117446

Batch 238470, train_perplexity=191.0964, train_loss=5.252778

Batch 238480, train_perplexity=185.43651, train_loss=5.2227125

Batch 238490, train_perplexity=153.73035, train_loss=5.0352

Batch 238500, train_perplexity=179.8058, train_loss=5.1918774

Batch 238510, train_perplexity=164.51686, train_loss=5.103013

Batch 238520, train_perplexity=163.58589, train_loss=5.097338

Batch 238530, train_perplexity=163.14102, train_loss=5.094615

Batch 238540, train_perplexity=169.84946, train_loss=5.1349125

Batch 238550, train_perplexity=170.21674, train_loss=5.1370726

Batch 238560, train_perplexity=186.62854, train_loss=5.2291203

Batch 238570, train_perplexity=181.83943, train_loss=5.203124

Batch 238580, train_perplexity=182.57864, train_loss=5.207181

Batch 238590, train_perplexity=168.01631, train_loss=5.124061

Batch 238600, train_perplexity=174.50992, train_loss=5.1619816

Batch 238610, train_perplexity=138.25015, train_loss=4.9290648

Batch 238620, train_perplexity=192.85306, train_loss=5.2619286

Batch 238630, train_perplexity=167.57489, train_loss=5.1214304

Batch 238640, train_perplexity=182.37225, train_loss=5.20605

Batch 238650, train_perplexity=171.05415, train_loss=5.14198

Batch 238660, train_perplexity=173.04523, train_loss=5.153553

Batch 238670, train_perplexity=180.75656, train_loss=5.197151

Batch 238680, train_perplexity=163.5512, train_loss=5.097126

Batch 238690, train_perplexity=185.12746, train_loss=5.2210445

Batch 238700, train_perplexity=159.21497, train_loss=5.0702553

Batch 238710, train_perplexity=154.0099, train_loss=5.037017

Batch 238720, train_perplexity=169.42786, train_loss=5.132427

Batch 238730, train_perplexity=198.78976, train_loss=5.292248

Batch 238740, train_perplexity=163.00758, train_loss=5.0937967

Batch 238750, train_perplexity=154.55061, train_loss=5.0405216

Batch 238760, train_perplexity=186.90239, train_loss=5.2305865

Batch 238770, train_perplexity=174.34142, train_loss=5.1610155

Batch 238780, train_perplexity=166.03773, train_loss=5.112215

Batch 238790, train_perplexity=194.89928, train_loss=5.272483

Batch 238800, train_perplexity=175.62033, train_loss=5.1683245

Batch 238810, train_perplexity=185.1602, train_loss=5.2212214

Batch 238820, train_perplexity=157.5334, train_loss=5.0596375

Batch 238830, train_perplexity=191.64703, train_loss=5.2556553

Batch 238840, train_perplexity=163.83539, train_loss=5.098862

Batch 238850, train_perplexity=155.76329, train_loss=5.0483375

Batch 238860, train_perplexity=178.71765, train_loss=5.185807

Batch 238870, train_perplexity=167.13257, train_loss=5.1187873

Batch 238880, train_perplexity=158.44353, train_loss=5.065398

Batch 238890, train_perplexity=159.92046, train_loss=5.0746765

Batch 238900, train_perplexity=151.41989, train_loss=5.0200567

Batch 238910, train_perplexity=161.41808, train_loss=5.0839977

Batch 238920, train_perplexity=155.11586, train_loss=5.0441723

Batch 238930, train_perplexity=167.20335, train_loss=5.1192107

Batch 238940, train_perplexity=184.43156, train_loss=5.2172785

Batch 238950, train_perplexity=162.45123, train_loss=5.090378

Batch 238960, train_perplexity=170.56334, train_loss=5.1391068

Batch 238970, train_perplexity=151.44026, train_loss=5.020191

Batch 238980, train_perplexity=165.07195, train_loss=5.1063814

Batch 238990, train_perplexity=170.82893, train_loss=5.1406627

Batch 239000, train_perplexity=167.78023, train_loss=5.122655

Batch 239010, train_perplexity=162.58243, train_loss=5.091185

Batch 239020, train_perplexity=185.43181, train_loss=5.2226872

Batch 239030, train_perplexity=157.7988, train_loss=5.061321

Batch 239040, train_perplexity=162.58978, train_loss=5.0912304

Batch 239050, train_perplexity=165.73415, train_loss=5.110385

Batch 239060, train_perplexity=160.36334, train_loss=5.077442

Batch 239070, train_perplexity=188.99547, train_loss=5.241723

Batch 239080, train_perplexity=188.49974, train_loss=5.2390966

Batch 239090, train_perplexity=159.8285, train_loss=5.0741014

Batch 239100, train_perplexity=153.55232, train_loss=5.0340414

Batch 239110, train_perplexity=169.37115, train_loss=5.1320925

Batch 239120, train_perplexity=181.53629, train_loss=5.2014556

Batch 239130, train_perplexity=161.87546, train_loss=5.0868273

Batch 239140, train_perplexity=170.97359, train_loss=5.141509

Batch 239150, train_perplexity=169.026, train_loss=5.1300526

Batch 239160, train_perplexity=152.27522, train_loss=5.0256896

Batch 239170, train_perplexity=165.81557, train_loss=5.110876

Batch 239180, train_perplexity=164.26523, train_loss=5.1014824

Batch 239190, train_perplexity=175.83618, train_loss=5.169553

Batch 239200, train_perplexity=164.56895, train_loss=5.1033297

Batch 239210, train_perplexity=173.10506, train_loss=5.1538987

Batch 239220, train_perplexity=149.92522, train_loss=5.0101366

Batch 239230, train_perplexity=155.83884, train_loss=5.0488224

Batch 239240, train_perplexity=161.23653, train_loss=5.0828724

Batch 239250, train_perplexity=173.73825, train_loss=5.15755

Batch 239260, train_perplexity=174.7769, train_loss=5.1635103

Batch 239270, train_perplexity=180.00044, train_loss=5.1929593

Batch 239280, train_perplexity=168.42291, train_loss=5.126478

Batch 239290, train_perplexity=185.70125, train_loss=5.224139

Batch 239300, train_perplexity=175.988, train_loss=5.170416

Batch 239310, train_perplexity=172.35481, train_loss=5.149555

Batch 239320, train_perplexity=171.39412, train_loss=5.1439657

Batch 239330, train_perplexity=164.43216, train_loss=5.102498

Batch 239340, train_perplexity=157.59435, train_loss=5.0600243

Batch 239350, train_perplexity=179.39703, train_loss=5.1896014

Batch 239360, train_perplexity=173.40321, train_loss=5.1556196

Batch 239370, train_perplexity=172.34363, train_loss=5.1494904

Batch 239380, train_perplexity=167.77399, train_loss=5.1226177

Batch 239390, train_perplexity=152.22006, train_loss=5.025327

Batch 239400, train_perplexity=169.7311, train_loss=5.1342154

Batch 239410, train_perplexity=178.92451, train_loss=5.186964

Batch 239420, train_perplexity=146.86354, train_loss=4.989504

Batch 239430, train_perplexity=166.48026, train_loss=5.1148767

Batch 239440, train_perplexity=173.63356, train_loss=5.156947

Batch 239450, train_perplexity=155.63744, train_loss=5.047529

Batch 239460, train_perplexity=174.15437, train_loss=5.159942

Batch 239470, train_perplexity=156.71982, train_loss=5.0544596

Batch 239480, train_perplexity=164.28622, train_loss=5.10161

Batch 239490, train_perplexity=186.77936, train_loss=5.229928

Batch 239500, train_perplexity=185.76767, train_loss=5.224497

Batch 239510, train_perplexity=164.74515, train_loss=5.1043997

Batch 239520, train_perplexity=180.01178, train_loss=5.1930223

Batch 239530, train_perplexity=140.42908, train_loss=4.9447026

Batch 239540, train_perplexity=161.22299, train_loss=5.0827885

Batch 239550, train_perplexity=189.01819, train_loss=5.241843

Batch 239560, train_perplexity=165.45888, train_loss=5.1087227

Batch 239570, train_perplexity=166.52122, train_loss=5.115123

Batch 239580, train_perplexity=180.32845, train_loss=5.19478

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Batch 239590, train_perplexity=164.43811, train_loss=5.1025343

Batch 239600, train_perplexity=167.25677, train_loss=5.11953

Batch 239610, train_perplexity=169.4193, train_loss=5.1323767

Batch 239620, train_perplexity=183.51082, train_loss=5.2122736

Batch 239630, train_perplexity=163.63559, train_loss=5.097642

Batch 239640, train_perplexity=161.94942, train_loss=5.087284
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 239650, train_perplexity=160.88156, train_loss=5.0806684

Batch 239660, train_perplexity=153.93875, train_loss=5.036555

Batch 239670, train_perplexity=157.66454, train_loss=5.0604696

Batch 239680, train_perplexity=150.99637, train_loss=5.017256

Batch 239690, train_perplexity=179.44563, train_loss=5.1898723

Batch 239700, train_perplexity=167.49326, train_loss=5.120943

Batch 239710, train_perplexity=168.74222, train_loss=5.128372

Batch 239720, train_perplexity=169.65819, train_loss=5.1337857

Batch 239730, train_perplexity=171.45314, train_loss=5.14431

Batch 239740, train_perplexity=151.09792, train_loss=5.017928

Batch 239750, train_perplexity=159.7482, train_loss=5.073599

Batch 239760, train_perplexity=159.4657, train_loss=5.071829

Batch 239770, train_perplexity=175.60626, train_loss=5.1682444

Batch 239780, train_perplexity=145.42563, train_loss=4.979665

Batch 239790, train_perplexity=179.65924, train_loss=5.191062

Batch 239800, train_perplexity=161.76836, train_loss=5.0861654

Batch 239810, train_perplexity=166.50948, train_loss=5.115052

Batch 239820, train_perplexity=168.08339, train_loss=5.12446

Batch 239830, train_perplexity=156.05568, train_loss=5.050213

Batch 239840, train_perplexity=155.69058, train_loss=5.0478706

Batch 239850, train_perplexity=168.66765, train_loss=5.12793

Batch 239860, train_perplexity=169.95898, train_loss=5.135557

Batch 239870, train_perplexity=195.2223, train_loss=5.274139

Batch 239880, train_perplexity=160.29317, train_loss=5.0770044

Batch 239890, train_perplexity=163.50502, train_loss=5.0968437

Batch 239900, train_perplexity=183.26718, train_loss=5.210945

Batch 239910, train_perplexity=160.24571, train_loss=5.0767083

Batch 239920, train_perplexity=181.84828, train_loss=5.2031727

Batch 239930, train_perplexity=165.78679, train_loss=5.1107025

Batch 239940, train_perplexity=158.53845, train_loss=5.065997

Batch 239950, train_perplexity=173.14246, train_loss=5.1541147

Batch 239960, train_perplexity=159.9248, train_loss=5.0747037

Batch 239970, train_perplexity=171.0734, train_loss=5.1420927

Batch 239980, train_perplexity=165.86198, train_loss=5.111156

Batch 239990, train_perplexity=169.32715, train_loss=5.1318326

Batch 240000, train_perplexity=190.40714, train_loss=5.2491646

Batch 240010, train_perplexity=164.57547, train_loss=5.103369

Batch 240020, train_perplexity=187.21584, train_loss=5.232262

Batch 240030, train_perplexity=181.85834, train_loss=5.203228

Batch 240040, train_perplexity=168.93915, train_loss=5.1295385

Batch 240050, train_perplexity=164.17064, train_loss=5.1009064

Batch 240060, train_perplexity=154.63391, train_loss=5.0410604

Batch 240070, train_perplexity=162.10944, train_loss=5.0882716

Batch 240080, train_perplexity=178.11533, train_loss=5.182431

Batch 240090, train_perplexity=171.12285, train_loss=5.1423817

Batch 240100, train_perplexity=157.44772, train_loss=5.0590935

Batch 240110, train_perplexity=161.38544, train_loss=5.0837955

Batch 240120, train_perplexity=166.62941, train_loss=5.1157722

Batch 240130, train_perplexity=188.75491, train_loss=5.2404494

Batch 240140, train_perplexity=163.85062, train_loss=5.098955

Batch 240150, train_perplexity=170.2903, train_loss=5.1375046

Batch 240160, train_perplexity=151.28363, train_loss=5.0191565

Batch 240170, train_perplexity=163.19719, train_loss=5.0949593

Batch 240180, train_perplexity=178.47026, train_loss=5.184422

Batch 240190, train_perplexity=165.9816, train_loss=5.111877

Batch 240200, train_perplexity=166.95638, train_loss=5.1177325

Batch 240210, train_perplexity=166.57079, train_loss=5.1154203

Batch 240220, train_perplexity=157.07219, train_loss=5.0567055

Batch 240230, train_perplexity=154.62668, train_loss=5.0410137

Batch 240240, train_perplexity=149.74246, train_loss=5.008917

Batch 240250, train_perplexity=167.0611, train_loss=5.1183596

Batch 240260, train_perplexity=164.2671, train_loss=5.101494

Batch 240270, train_perplexity=162.8595, train_loss=5.092888

Batch 240280, train_perplexity=175.29605, train_loss=5.1664762

Batch 240290, train_perplexity=171.50826, train_loss=5.1446314

Batch 240300, train_perplexity=167.17426, train_loss=5.1190367

Batch 240310, train_perplexity=139.50372, train_loss=4.9380913

Batch 240320, train_perplexity=157.26373, train_loss=5.0579243

Batch 240330, train_perplexity=155.58951, train_loss=5.047221

Batch 240340, train_perplexity=169.92188, train_loss=5.135339

Batch 240350, train_perplexity=176.79968, train_loss=5.1750174

Batch 240360, train_perplexity=173.05876, train_loss=5.153631

Batch 240370, train_perplexity=171.9903, train_loss=5.147438

Batch 240380, train_perplexity=161.79984, train_loss=5.08636

Batch 240390, train_perplexity=174.7899, train_loss=5.1635847

Batch 240400, train_perplexity=172.658, train_loss=5.151313

Batch 240410, train_perplexity=173.53275, train_loss=5.1563663

Batch 240420, train_perplexity=151.36993, train_loss=5.0197268

Batch 240430, train_perplexity=163.46043, train_loss=5.096571

Batch 240440, train_perplexity=172.94261, train_loss=5.15296

Batch 240450, train_perplexity=177.02475, train_loss=5.1762896

Batch 240460, train_perplexity=164.99655, train_loss=5.1059246

Batch 240470, train_perplexity=144.05516, train_loss=4.9701962

Batch 240480, train_perplexity=150.83186, train_loss=5.0161657

Batch 240490, train_perplexity=180.4672, train_loss=5.195549

Batch 240500, train_perplexity=179.58919, train_loss=5.190672

Batch 240510, train_perplexity=162.01028, train_loss=5.08766

Batch 240520, train_perplexity=183.86713, train_loss=5.2142134

Batch 240530, train_perplexity=139.38025, train_loss=4.937206

Batch 240540, train_perplexity=168.2327, train_loss=5.125348

Batch 240550, train_perplexity=183.18646, train_loss=5.2105045

Batch 240560, train_perplexity=162.41258, train_loss=5.09014

Batch 240570, train_perplexity=169.12718, train_loss=5.130651

Batch 240580, train_perplexity=171.27026, train_loss=5.143243

Batch 240590, train_perplexity=166.11613, train_loss=5.112687

Batch 240600, train_perplexity=171.85208, train_loss=5.146634

Batch 240610, train_perplexity=200.08577, train_loss=5.298746

Batch 240620, train_perplexity=189.60233, train_loss=5.244929

Batch 240630, train_perplexity=184.96634, train_loss=5.220174

Batch 240640, train_perplexity=200.74182, train_loss=5.3020196

Batch 240650, train_perplexity=157.78111, train_loss=5.0612087

Batch 240660, train_perplexity=193.19933, train_loss=5.2637224

Batch 240670, train_perplexity=165.2296, train_loss=5.107336

Batch 240680, train_perplexity=181.42024, train_loss=5.200816

Batch 240690, train_perplexity=167.33998, train_loss=5.1200275

Batch 240700, train_perplexity=163.0693, train_loss=5.0941753

Batch 240710, train_perplexity=175.01575, train_loss=5.164876

Batch 240720, train_perplexity=179.76877, train_loss=5.1916714

Batch 240730, train_perplexity=169.00642, train_loss=5.1299367

Batch 240740, train_perplexity=195.47148, train_loss=5.2754145

Batch 240750, train_perplexity=163.87094, train_loss=5.099079

Batch 240760, train_perplexity=167.36153, train_loss=5.1201563

Batch 240770, train_perplexity=151.02272, train_loss=5.0174303

Batch 240780, train_perplexity=155.7546, train_loss=5.0482817

Batch 240790, train_perplexity=180.9028, train_loss=5.19796

Batch 240800, train_perplexity=175.47685, train_loss=5.167507

Batch 240810, train_perplexity=149.19495, train_loss=5.005254

Batch 240820, train_perplexity=165.31999, train_loss=5.107883

Batch 240830, train_perplexity=173.72003, train_loss=5.157445

Batch 240840, train_perplexity=164.88048, train_loss=5.105221

Batch 240850, train_perplexity=172.38136, train_loss=5.149709

Batch 240860, train_perplexity=160.58005, train_loss=5.0787926

Batch 240870, train_perplexity=186.28784, train_loss=5.227293

Batch 240880, train_perplexity=188.80415, train_loss=5.2407103

Batch 240890, train_perplexity=170.52464, train_loss=5.13888

Batch 240900, train_perplexity=161.51393, train_loss=5.0845914

Batch 240910, train_perplexity=155.36295, train_loss=5.045764

Batch 240920, train_perplexity=151.40762, train_loss=5.0199757

Batch 240930, train_perplexity=183.6239, train_loss=5.2128897

Batch 240940, train_perplexity=167.02127, train_loss=5.118121
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 240950, train_perplexity=162.85437, train_loss=5.0928564

Batch 240960, train_perplexity=171.72986, train_loss=5.1459227

Batch 240970, train_perplexity=184.42734, train_loss=5.2172556

Batch 240980, train_perplexity=146.00218, train_loss=4.9836216

Batch 240990, train_perplexity=177.76219, train_loss=5.1804466

Batch 241000, train_perplexity=156.89851, train_loss=5.055599

Batch 241010, train_perplexity=164.59462, train_loss=5.1034856

Batch 241020, train_perplexity=155.54277, train_loss=5.046921

Batch 241030, train_perplexity=147.34726, train_loss=4.992792

Batch 241040, train_perplexity=197.25415, train_loss=5.284493

Batch 241050, train_perplexity=179.45358, train_loss=5.1899166

Batch 241060, train_perplexity=160.47136, train_loss=5.0781155

Batch 241070, train_perplexity=166.7139, train_loss=5.116279

Batch 241080, train_perplexity=178.39003, train_loss=5.1839724

Batch 241090, train_perplexity=170.7466, train_loss=5.1401806

Batch 241100, train_perplexity=160.32512, train_loss=5.0772038

Batch 241110, train_perplexity=161.14413, train_loss=5.082299

Batch 241120, train_perplexity=159.8619, train_loss=5.0743103

Batch 241130, train_perplexity=160.06693, train_loss=5.075592

Batch 241140, train_perplexity=170.00398, train_loss=5.135822

Batch 241150, train_perplexity=172.35037, train_loss=5.1495295

Batch 241160, train_perplexity=173.31609, train_loss=5.155117

Batch 241170, train_perplexity=155.77495, train_loss=5.0484123

Batch 241180, train_perplexity=177.78508, train_loss=5.1805754

Batch 241190, train_perplexity=152.19748, train_loss=5.025179

Batch 241200, train_perplexity=179.73344, train_loss=5.191475

Batch 241210, train_perplexity=156.22029, train_loss=5.051267

Batch 241220, train_perplexity=172.99738, train_loss=5.1532764

Batch 241230, train_perplexity=160.47717, train_loss=5.0781517

Batch 241240, train_perplexity=174.801, train_loss=5.163648

Batch 241250, train_perplexity=173.55957, train_loss=5.156521

Batch 241260, train_perplexity=172.06412, train_loss=5.147867

Batch 241270, train_perplexity=172.30025, train_loss=5.1492386

Batch 241280, train_perplexity=181.82928, train_loss=5.2030683

Batch 241290, train_perplexity=186.9746, train_loss=5.230973

Batch 241300, train_perplexity=169.82977, train_loss=5.1347966

Batch 241310, train_perplexity=154.6688, train_loss=5.041286

Batch 241320, train_perplexity=155.12776, train_loss=5.044249

Batch 241330, train_perplexity=142.78145, train_loss=4.961315

Batch 241340, train_perplexity=168.21127, train_loss=5.125221

Batch 241350, train_perplexity=173.83289, train_loss=5.1580944

Batch 241360, train_perplexity=187.66656, train_loss=5.234667

Batch 241370, train_perplexity=161.86226, train_loss=5.0867457

Batch 241380, train_perplexity=157.5676, train_loss=5.0598545

Batch 241390, train_perplexity=177.54143, train_loss=5.179204

Batch 241400, train_perplexity=156.66586, train_loss=5.0541153

Batch 241410, train_perplexity=157.17993, train_loss=5.057391

Batch 241420, train_perplexity=170.38516, train_loss=5.1380615

Batch 241430, train_perplexity=171.58531, train_loss=5.1450806

Batch 241440, train_perplexity=170.28656, train_loss=5.1374826

Batch 241450, train_perplexity=181.90855, train_loss=5.203504

Batch 241460, train_perplexity=171.16054, train_loss=5.142602

Batch 241470, train_perplexity=190.39633, train_loss=5.249108

Batch 241480, train_perplexity=170.6762, train_loss=5.139768

Batch 241490, train_perplexity=177.95624, train_loss=5.1815376

Batch 241500, train_perplexity=158.97919, train_loss=5.0687733

Batch 241510, train_perplexity=171.1377, train_loss=5.1424685

Batch 241520, train_perplexity=160.20293, train_loss=5.0764413

Batch 241530, train_perplexity=169.6145, train_loss=5.133528

Batch 241540, train_perplexity=181.62029, train_loss=5.201918

Batch 241550, train_perplexity=176.67723, train_loss=5.1743245

Batch 241560, train_perplexity=174.11676, train_loss=5.159726

Batch 241570, train_perplexity=184.91316, train_loss=5.2198863

Batch 241580, train_perplexity=167.99854, train_loss=5.1239552

Batch 241590, train_perplexity=157.23982, train_loss=5.057772

Batch 241600, train_perplexity=176.46472, train_loss=5.173121

Batch 241610, train_perplexity=176.17307, train_loss=5.171467

Batch 241620, train_perplexity=160.83876, train_loss=5.0804024

Batch 241630, train_perplexity=166.35725, train_loss=5.1141376

Batch 241640, train_perplexity=157.999, train_loss=5.0625887

Batch 241650, train_perplexity=176.59747, train_loss=5.173873

Batch 241660, train_perplexity=158.43324, train_loss=5.0653334

Batch 241670, train_perplexity=154.01724, train_loss=5.0370646

Batch 241680, train_perplexity=157.82785, train_loss=5.061505

Batch 241690, train_perplexity=170.27194, train_loss=5.137397

Batch 241700, train_perplexity=161.00896, train_loss=5.08146

Batch 241710, train_perplexity=155.06053, train_loss=5.0438156

Batch 241720, train_perplexity=174.58533, train_loss=5.1624136

Batch 241730, train_perplexity=171.01434, train_loss=5.1417475

Batch 241740, train_perplexity=158.3476, train_loss=5.0647926

Batch 241750, train_perplexity=159.87517, train_loss=5.0743933

Batch 241760, train_perplexity=174.90163, train_loss=5.1642237

Batch 241770, train_perplexity=143.7544, train_loss=4.9681063

Batch 241780, train_perplexity=156.17181, train_loss=5.0509567

Batch 241790, train_perplexity=157.0131, train_loss=5.0563293

Batch 241800, train_perplexity=173.03682, train_loss=5.1535044

Batch 241810, train_perplexity=172.68518, train_loss=5.15147

Batch 241820, train_perplexity=162.50677, train_loss=5.0907197

Batch 241830, train_perplexity=148.4419, train_loss=5.0001936

Batch 241840, train_perplexity=151.10368, train_loss=5.0179663

Batch 241850, train_perplexity=152.13654, train_loss=5.0247784

Batch 241860, train_perplexity=164.43591, train_loss=5.102521

Batch 241870, train_perplexity=171.39526, train_loss=5.1439724

Batch 241880, train_perplexity=138.1359, train_loss=4.928238

Batch 241890, train_perplexity=152.1876, train_loss=5.025114

Batch 241900, train_perplexity=192.25626, train_loss=5.258829

Batch 241910, train_perplexity=185.23395, train_loss=5.2216196

Batch 241920, train_perplexity=166.23087, train_loss=5.1133776

Batch 241930, train_perplexity=161.81279, train_loss=5.08644

Batch 241940, train_perplexity=177.85512, train_loss=5.180969

Batch 241950, train_perplexity=159.7584, train_loss=5.0736628

Batch 241960, train_perplexity=176.85971, train_loss=5.175357

Batch 241970, train_perplexity=155.62823, train_loss=5.04747

Batch 241980, train_perplexity=174.61638, train_loss=5.1625915

Batch 241990, train_perplexity=166.89278, train_loss=5.1173515

Batch 242000, train_perplexity=179.09029, train_loss=5.18789

Batch 242010, train_perplexity=172.11122, train_loss=5.148141

Batch 242020, train_perplexity=160.65985, train_loss=5.0792894

Batch 242030, train_perplexity=164.91318, train_loss=5.105419

Batch 242040, train_perplexity=166.9883, train_loss=5.1179237

Batch 242050, train_perplexity=177.29558, train_loss=5.1778183

Batch 242060, train_perplexity=159.81236, train_loss=5.0740004

Batch 242070, train_perplexity=169.14677, train_loss=5.130767

Batch 242080, train_perplexity=168.22644, train_loss=5.125311

Batch 242090, train_perplexity=170.44432, train_loss=5.1384087

Batch 242100, train_perplexity=176.65541, train_loss=5.174201

Batch 242110, train_perplexity=176.55806, train_loss=5.17365

Batch 242120, train_perplexity=167.10619, train_loss=5.1186295

Batch 242130, train_perplexity=169.59517, train_loss=5.1334143

Batch 242140, train_perplexity=164.32219, train_loss=5.101829

Batch 242150, train_perplexity=161.50261, train_loss=5.0845213

Batch 242160, train_perplexity=174.82042, train_loss=5.163759

Batch 242170, train_perplexity=158.31424, train_loss=5.064582

Batch 242180, train_perplexity=162.7218, train_loss=5.092042

Batch 242190, train_perplexity=173.1935, train_loss=5.1544094

Batch 242200, train_perplexity=186.94893, train_loss=5.2308354

Batch 242210, train_perplexity=164.73225, train_loss=5.1043215

Batch 242220, train_perplexity=164.16829, train_loss=5.100892

Batch 242230, train_perplexity=173.59863, train_loss=5.156746

Batch 242240, train_perplexity=154.33093, train_loss=5.039099
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 242250, train_perplexity=163.42084, train_loss=5.0963287

Batch 242260, train_perplexity=175.24767, train_loss=5.1662

Batch 242270, train_perplexity=156.187, train_loss=5.051054

Batch 242280, train_perplexity=185.25302, train_loss=5.2217226

Batch 242290, train_perplexity=158.00864, train_loss=5.0626497

Batch 242300, train_perplexity=164.0948, train_loss=5.1004443

Batch 242310, train_perplexity=166.46915, train_loss=5.11481

Batch 242320, train_perplexity=181.36455, train_loss=5.200509

Batch 242330, train_perplexity=163.33444, train_loss=5.0958

Batch 242340, train_perplexity=167.90628, train_loss=5.123406

Batch 242350, train_perplexity=166.70595, train_loss=5.1162314

Batch 242360, train_perplexity=156.39105, train_loss=5.0523596

Batch 242370, train_perplexity=163.29778, train_loss=5.0955753

Batch 242380, train_perplexity=156.64584, train_loss=5.0539875

Batch 242390, train_perplexity=190.22726, train_loss=5.2482195

Batch 242400, train_perplexity=167.8348, train_loss=5.12298

Batch 242410, train_perplexity=157.9297, train_loss=5.06215

Batch 242420, train_perplexity=155.61473, train_loss=5.0473833

Batch 242430, train_perplexity=151.29143, train_loss=5.019208

Batch 242440, train_perplexity=142.20581, train_loss=4.9572754

Batch 242450, train_perplexity=170.51651, train_loss=5.138832

Batch 242460, train_perplexity=174.79274, train_loss=5.163601

Batch 242470, train_perplexity=151.3581, train_loss=5.0196486

Batch 242480, train_perplexity=175.67729, train_loss=5.1686487

Batch 242490, train_perplexity=154.93947, train_loss=5.0430346

Batch 242500, train_perplexity=174.41475, train_loss=5.161436

Batch 242510, train_perplexity=164.8418, train_loss=5.104986

Batch 242520, train_perplexity=158.31483, train_loss=5.0645857

Batch 242530, train_perplexity=153.04414, train_loss=5.0307264

Batch 242540, train_perplexity=157.2126, train_loss=5.057599

Batch 242550, train_perplexity=177.33237, train_loss=5.1780257

Batch 242560, train_perplexity=153.4323, train_loss=5.0332594

Batch 242570, train_perplexity=167.18669, train_loss=5.119111

Batch 242580, train_perplexity=153.2511, train_loss=5.032078

Batch 242590, train_perplexity=157.93558, train_loss=5.062187

Batch 242600, train_perplexity=200.49922, train_loss=5.3008103

Batch 242610, train_perplexity=174.75906, train_loss=5.1634083

Batch 242620, train_perplexity=168.83751, train_loss=5.128937

Batch 242630, train_perplexity=169.92383, train_loss=5.13535

Batch 242640, train_perplexity=145.47508, train_loss=4.980005

Batch 242650, train_perplexity=154.89116, train_loss=5.0427227

Batch 242660, train_perplexity=163.10416, train_loss=5.094389

Batch 242670, train_perplexity=171.03914, train_loss=5.1418924

Batch 242680, train_perplexity=178.84834, train_loss=5.186538

Batch 242690, train_perplexity=173.68607, train_loss=5.1572495

Batch 242700, train_perplexity=174.69408, train_loss=5.1630363

Batch 242710, train_perplexity=170.49455, train_loss=5.1387033

Batch 242720, train_perplexity=178.11142, train_loss=5.1824093

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00056-of-00100
Loaded 305067 sentences.
Finished loading
Batch 242730, train_perplexity=183.90404, train_loss=5.214414

Batch 242740, train_perplexity=167.7791, train_loss=5.1226482

Batch 242750, train_perplexity=175.79645, train_loss=5.169327

Batch 242760, train_perplexity=180.53743, train_loss=5.195938

Batch 242770, train_perplexity=160.24074, train_loss=5.0766773

Batch 242780, train_perplexity=148.74132, train_loss=5.0022087

Batch 242790, train_perplexity=162.90564, train_loss=5.093171

Batch 242800, train_perplexity=168.31662, train_loss=5.125847

Batch 242810, train_perplexity=162.4321, train_loss=5.09026

Batch 242820, train_perplexity=164.8506, train_loss=5.1050396

Batch 242830, train_perplexity=178.17622, train_loss=5.182773

Batch 242840, train_perplexity=155.16808, train_loss=5.044509

Batch 242850, train_perplexity=153.94896, train_loss=5.036621

Batch 242860, train_perplexity=189.3739, train_loss=5.2437234

Batch 242870, train_perplexity=191.3874, train_loss=5.2542996

Batch 242880, train_perplexity=177.80711, train_loss=5.1806993

Batch 242890, train_perplexity=163.20622, train_loss=5.0950146

Batch 242900, train_perplexity=161.87846, train_loss=5.086846

Batch 242910, train_perplexity=175.26437, train_loss=5.1662955

Batch 242920, train_perplexity=160.61673, train_loss=5.079021

Batch 242930, train_perplexity=179.15681, train_loss=5.1882615

Batch 242940, train_perplexity=169.81268, train_loss=5.134696

Batch 242950, train_perplexity=155.19954, train_loss=5.0447116

Batch 242960, train_perplexity=158.37555, train_loss=5.064969

Batch 242970, train_perplexity=178.63211, train_loss=5.1853285

Batch 242980, train_perplexity=157.0006, train_loss=5.0562496

Batch 242990, train_perplexity=156.80981, train_loss=5.0550337

Batch 243000, train_perplexity=172.00505, train_loss=5.147524

Batch 243010, train_perplexity=176.89008, train_loss=5.1755285

Batch 243020, train_perplexity=162.56638, train_loss=5.0910864

Batch 243030, train_perplexity=175.61682, train_loss=5.1683044

Batch 243040, train_perplexity=173.59665, train_loss=5.1567345

Batch 243050, train_perplexity=160.77557, train_loss=5.0800095

Batch 243060, train_perplexity=165.70506, train_loss=5.1102095

Batch 243070, train_perplexity=178.73112, train_loss=5.1858826

Batch 243080, train_perplexity=151.63304, train_loss=5.0214634

Batch 243090, train_perplexity=193.4913, train_loss=5.2652326

Batch 243100, train_perplexity=157.06094, train_loss=5.056634

Batch 243110, train_perplexity=159.81023, train_loss=5.073987

Batch 243120, train_perplexity=182.2541, train_loss=5.205402

Batch 243130, train_perplexity=176.00613, train_loss=5.170519

Batch 243140, train_perplexity=180.60269, train_loss=5.1962996

Batch 243150, train_perplexity=173.07031, train_loss=5.153698

Batch 243160, train_perplexity=171.77809, train_loss=5.1462035

Batch 243170, train_perplexity=171.04404, train_loss=5.141921

Batch 243180, train_perplexity=155.64546, train_loss=5.0475807

Batch 243190, train_perplexity=161.77638, train_loss=5.086215

Batch 243200, train_perplexity=184.47238, train_loss=5.2174997

Batch 243210, train_perplexity=163.10167, train_loss=5.0943737

Batch 243220, train_perplexity=157.4562, train_loss=5.0591474

Batch 243230, train_perplexity=173.17665, train_loss=5.154312

Batch 243240, train_perplexity=165.09367, train_loss=5.106513

Batch 243250, train_perplexity=163.17245, train_loss=5.0948076

Batch 243260, train_perplexity=181.84377, train_loss=5.203148

Batch 243270, train_perplexity=169.97195, train_loss=5.1356335

Batch 243280, train_perplexity=197.05437, train_loss=5.2834797

Batch 243290, train_perplexity=193.63963, train_loss=5.265999

Batch 243300, train_perplexity=168.01776, train_loss=5.1240697

Batch 243310, train_perplexity=156.52385, train_loss=5.0532084

Batch 243320, train_perplexity=166.44708, train_loss=5.1146774

Batch 243330, train_perplexity=170.68066, train_loss=5.1397943

Batch 243340, train_perplexity=158.2635, train_loss=5.0642614

Batch 243350, train_perplexity=165.5711, train_loss=5.1094007

Batch 243360, train_perplexity=171.74141, train_loss=5.14599

Batch 243370, train_perplexity=178.3385, train_loss=5.1836834

Batch 243380, train_perplexity=150.36896, train_loss=5.013092

Batch 243390, train_perplexity=163.39746, train_loss=5.0961857

Batch 243400, train_perplexity=157.02846, train_loss=5.056427

Batch 243410, train_perplexity=173.39264, train_loss=5.1555586

Batch 243420, train_perplexity=168.10551, train_loss=5.124592

Batch 243430, train_perplexity=167.00279, train_loss=5.1180105

Batch 243440, train_perplexity=159.23547, train_loss=5.070384

Batch 243450, train_perplexity=170.80759, train_loss=5.1405377

Batch 243460, train_perplexity=157.29712, train_loss=5.0581365

Batch 243470, train_perplexity=165.2085, train_loss=5.1072083

Batch 243480, train_perplexity=174.66826, train_loss=5.1628885
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 243490, train_perplexity=164.09973, train_loss=5.1004744

Batch 243500, train_perplexity=170.36566, train_loss=5.137947

Batch 243510, train_perplexity=154.57243, train_loss=5.040663

Batch 243520, train_perplexity=161.88966, train_loss=5.086915

Batch 243530, train_perplexity=174.788, train_loss=5.1635737

Batch 243540, train_perplexity=147.71294, train_loss=4.9952707

Batch 243550, train_perplexity=168.10855, train_loss=5.12461

Batch 243560, train_perplexity=172.39722, train_loss=5.1498013

Batch 243570, train_perplexity=180.71382, train_loss=5.1969147

Batch 243580, train_perplexity=182.00113, train_loss=5.204013

Batch 243590, train_perplexity=174.26361, train_loss=5.160569

Batch 243600, train_perplexity=147.6204, train_loss=4.994644

Batch 243610, train_perplexity=165.44444, train_loss=5.1086354

Batch 243620, train_perplexity=161.27959, train_loss=5.0831394

Batch 243630, train_perplexity=168.52815, train_loss=5.127103

Batch 243640, train_perplexity=172.34479, train_loss=5.149497

Batch 243650, train_perplexity=161.77515, train_loss=5.0862074

Batch 243660, train_perplexity=172.94138, train_loss=5.1529527

Batch 243670, train_perplexity=168.1011, train_loss=5.1245656

Batch 243680, train_perplexity=164.75394, train_loss=5.104453

Batch 243690, train_perplexity=165.66904, train_loss=5.109992

Batch 243700, train_perplexity=159.4733, train_loss=5.0718765

Batch 243710, train_perplexity=155.70128, train_loss=5.0479393

Batch 243720, train_perplexity=161.17027, train_loss=5.0824614

Batch 243730, train_perplexity=155.02771, train_loss=5.043604

Batch 243740, train_perplexity=158.58548, train_loss=5.0662937

Batch 243750, train_perplexity=177.85231, train_loss=5.1809535

Batch 243760, train_perplexity=157.69093, train_loss=5.060637

Batch 243770, train_perplexity=158.77623, train_loss=5.067496

Batch 243780, train_perplexity=163.19766, train_loss=5.094962

Batch 243790, train_perplexity=163.00836, train_loss=5.0938015

Batch 243800, train_perplexity=162.52809, train_loss=5.090851

Batch 243810, train_perplexity=152.96272, train_loss=5.0301943

Batch 243820, train_perplexity=168.34953, train_loss=5.1260424

Batch 243830, train_perplexity=161.57286, train_loss=5.084956

Batch 243840, train_perplexity=142.85397, train_loss=4.961823

Batch 243850, train_perplexity=173.78963, train_loss=5.1578455

Batch 243860, train_perplexity=175.6422, train_loss=5.168449

Batch 243870, train_perplexity=159.16623, train_loss=5.069949

Batch 243880, train_perplexity=157.67702, train_loss=5.060549

Batch 243890, train_perplexity=159.74889, train_loss=5.073603

Batch 243900, train_perplexity=173.05728, train_loss=5.1536226

Batch 243910, train_perplexity=163.46605, train_loss=5.0966053

Batch 243920, train_perplexity=170.84181, train_loss=5.140738

Batch 243930, train_perplexity=152.27225, train_loss=5.02567

Batch 243940, train_perplexity=160.1309, train_loss=5.0759916

Batch 243950, train_perplexity=163.2885, train_loss=5.0955186

Batch 243960, train_perplexity=183.22472, train_loss=5.2107134

Batch 243970, train_perplexity=161.54512, train_loss=5.0847845

Batch 243980, train_perplexity=154.95662, train_loss=5.043145

Batch 243990, train_perplexity=167.25972, train_loss=5.119548

Batch 244000, train_perplexity=169.6285, train_loss=5.1336107

Batch 244010, train_perplexity=172.39903, train_loss=5.1498117

Batch 244020, train_perplexity=149.13676, train_loss=5.0048637

Batch 244030, train_perplexity=174.96085, train_loss=5.164562

Batch 244040, train_perplexity=182.61983, train_loss=5.2074065

Batch 244050, train_perplexity=169.95316, train_loss=5.135523

Batch 244060, train_perplexity=167.85191, train_loss=5.123082

Batch 244070, train_perplexity=161.86758, train_loss=5.0867786

Batch 244080, train_perplexity=160.39716, train_loss=5.077653

Batch 244090, train_perplexity=201.84958, train_loss=5.307523

Batch 244100, train_perplexity=187.02533, train_loss=5.231244

Batch 244110, train_perplexity=145.52164, train_loss=4.9803247

Batch 244120, train_perplexity=171.01419, train_loss=5.1417465

Batch 244130, train_perplexity=179.8804, train_loss=5.192292

Batch 244140, train_perplexity=173.09557, train_loss=5.153844

Batch 244150, train_perplexity=156.6227, train_loss=5.0538397

Batch 244160, train_perplexity=155.39081, train_loss=5.0459433

Batch 244170, train_perplexity=158.95341, train_loss=5.068611

Batch 244180, train_perplexity=144.93108, train_loss=4.9762583

Batch 244190, train_perplexity=168.38542, train_loss=5.1262555

Batch 244200, train_perplexity=163.71738, train_loss=5.0981417

Batch 244210, train_perplexity=159.03075, train_loss=5.0690975

Batch 244220, train_perplexity=150.3514, train_loss=5.012975

Batch 244230, train_perplexity=167.78094, train_loss=5.122659

Batch 244240, train_perplexity=163.6377, train_loss=5.097655

Batch 244250, train_perplexity=175.25209, train_loss=5.1662254

Batch 244260, train_perplexity=175.09647, train_loss=5.165337

Batch 244270, train_perplexity=147.59085, train_loss=4.994444

Batch 244280, train_perplexity=158.58865, train_loss=5.0663137

Batch 244290, train_perplexity=167.48575, train_loss=5.1208982

Batch 244300, train_perplexity=174.80191, train_loss=5.1636534

Batch 244310, train_perplexity=161.89877, train_loss=5.0869713

Batch 244320, train_perplexity=155.58818, train_loss=5.0472126

Batch 244330, train_perplexity=171.1062, train_loss=5.1422844

Batch 244340, train_perplexity=170.73586, train_loss=5.1401176

Batch 244350, train_perplexity=178.85243, train_loss=5.186561

Batch 244360, train_perplexity=168.25026, train_loss=5.1254525

Batch 244370, train_perplexity=141.90417, train_loss=4.955152

Batch 244380, train_perplexity=175.3121, train_loss=5.166568

Batch 244390, train_perplexity=176.44377, train_loss=5.1730022

Batch 244400, train_perplexity=184.29091, train_loss=5.2165155

Batch 244410, train_perplexity=157.73222, train_loss=5.060899

Batch 244420, train_perplexity=163.11496, train_loss=5.0944552

Batch 244430, train_perplexity=160.85287, train_loss=5.08049

Batch 244440, train_perplexity=182.10295, train_loss=5.204572

Batch 244450, train_perplexity=158.00925, train_loss=5.0626535

Batch 244460, train_perplexity=166.3142, train_loss=5.1138787

Batch 244470, train_perplexity=150.88725, train_loss=5.016533

Batch 244480, train_perplexity=167.56323, train_loss=5.121361

Batch 244490, train_perplexity=170.56952, train_loss=5.139143

Batch 244500, train_perplexity=179.31946, train_loss=5.189169

Batch 244510, train_perplexity=176.25851, train_loss=5.171952

Batch 244520, train_perplexity=152.39305, train_loss=5.026463

Batch 244530, train_perplexity=189.38962, train_loss=5.2438064

Batch 244540, train_perplexity=197.76837, train_loss=5.2870965

Batch 244550, train_perplexity=167.2291, train_loss=5.1193647

Batch 244560, train_perplexity=171.92142, train_loss=5.1470375

Batch 244570, train_perplexity=177.54346, train_loss=5.1792154

Batch 244580, train_perplexity=153.0535, train_loss=5.0307875

Batch 244590, train_perplexity=162.50523, train_loss=5.09071

Batch 244600, train_perplexity=173.6099, train_loss=5.1568108

Batch 244610, train_perplexity=159.36536, train_loss=5.0711994

Batch 244620, train_perplexity=165.95328, train_loss=5.1117063

Batch 244630, train_perplexity=161.91452, train_loss=5.0870686

Batch 244640, train_perplexity=175.01442, train_loss=5.1648684

Batch 244650, train_perplexity=156.11015, train_loss=5.050562

Batch 244660, train_perplexity=169.89757, train_loss=5.1351957

Batch 244670, train_perplexity=160.97502, train_loss=5.081249

Batch 244680, train_perplexity=168.73352, train_loss=5.1283207

Batch 244690, train_perplexity=182.97833, train_loss=5.2093678

Batch 244700, train_perplexity=163.90234, train_loss=5.099271

Batch 244710, train_perplexity=168.74898, train_loss=5.1284122

Batch 244720, train_perplexity=151.62393, train_loss=5.0214033

Batch 244730, train_perplexity=162.8661, train_loss=5.0929284

Batch 244740, train_perplexity=166.16081, train_loss=5.112956

Batch 244750, train_perplexity=165.93396, train_loss=5.11159

Batch 244760, train_perplexity=164.19632, train_loss=5.101063

Batch 244770, train_perplexity=178.61601, train_loss=5.1852384

Batch 244780, train_perplexity=143.90884, train_loss=4.96918
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 244790, train_perplexity=160.69035, train_loss=5.079479

Batch 244800, train_perplexity=160.55869, train_loss=5.0786595

Batch 244810, train_perplexity=164.2975, train_loss=5.101679

Batch 244820, train_perplexity=131.7228, train_loss=4.8806996

Batch 244830, train_perplexity=175.01166, train_loss=5.1648526

Batch 244840, train_perplexity=145.81384, train_loss=4.982331

Batch 244850, train_perplexity=146.63109, train_loss=4.98792

Batch 244860, train_perplexity=157.94153, train_loss=5.062225

Batch 244870, train_perplexity=177.2763, train_loss=5.1777096

Batch 244880, train_perplexity=147.86304, train_loss=4.9962864

Batch 244890, train_perplexity=153.69385, train_loss=5.0349627

Batch 244900, train_perplexity=172.8816, train_loss=5.152607

Batch 244910, train_perplexity=173.08864, train_loss=5.153804

Batch 244920, train_perplexity=156.74022, train_loss=5.0545897

Batch 244930, train_perplexity=173.38239, train_loss=5.1554995

Batch 244940, train_perplexity=158.08792, train_loss=5.0631514

Batch 244950, train_perplexity=157.3386, train_loss=5.0584

Batch 244960, train_perplexity=172.47205, train_loss=5.150235

Batch 244970, train_perplexity=162.86913, train_loss=5.092947

Batch 244980, train_perplexity=163.86266, train_loss=5.0990286

Batch 244990, train_perplexity=162.42241, train_loss=5.0902004

Batch 245000, train_perplexity=177.31248, train_loss=5.1779137

Batch 245010, train_perplexity=158.40047, train_loss=5.0651264

Batch 245020, train_perplexity=169.76314, train_loss=5.134404

Batch 245030, train_perplexity=176.88806, train_loss=5.175517

Batch 245040, train_perplexity=171.16568, train_loss=5.142632

Batch 245050, train_perplexity=159.43954, train_loss=5.071665

Batch 245060, train_perplexity=160.95192, train_loss=5.0811057

Batch 245070, train_perplexity=158.97743, train_loss=5.0687623

Batch 245080, train_perplexity=160.16725, train_loss=5.0762186

Batch 245090, train_perplexity=162.00797, train_loss=5.0876455

Batch 245100, train_perplexity=178.37337, train_loss=5.183879

Batch 245110, train_perplexity=174.55986, train_loss=5.1622677

Batch 245120, train_perplexity=170.16562, train_loss=5.136772

Batch 245130, train_perplexity=168.19708, train_loss=5.1251364

Batch 245140, train_perplexity=142.6174, train_loss=4.9601655

Batch 245150, train_perplexity=163.323, train_loss=5.09573

Batch 245160, train_perplexity=176.91472, train_loss=5.175668

Batch 245170, train_perplexity=165.91205, train_loss=5.111458

Batch 245180, train_perplexity=155.00273, train_loss=5.0434427

Batch 245190, train_perplexity=151.96231, train_loss=5.0236325

Batch 245200, train_perplexity=168.94559, train_loss=5.1295767

Batch 245210, train_perplexity=172.3009, train_loss=5.1492424

Batch 245220, train_perplexity=158.82173, train_loss=5.0677824

Batch 245230, train_perplexity=152.86182, train_loss=5.0295343

Batch 245240, train_perplexity=192.24048, train_loss=5.258747

Batch 245250, train_perplexity=143.42151, train_loss=4.965788

Batch 245260, train_perplexity=151.01538, train_loss=5.0173817

Batch 245270, train_perplexity=164.2033, train_loss=5.101105

Batch 245280, train_perplexity=168.58731, train_loss=5.127454

Batch 245290, train_perplexity=171.36479, train_loss=5.1437945

Batch 245300, train_perplexity=187.45264, train_loss=5.233526

Batch 245310, train_perplexity=205.3059, train_loss=5.324501

Batch 245320, train_perplexity=159.06032, train_loss=5.0692835

Batch 245330, train_perplexity=166.1391, train_loss=5.1128254

Batch 245340, train_perplexity=165.89575, train_loss=5.1113596

Batch 245350, train_perplexity=158.66649, train_loss=5.0668044

Batch 245360, train_perplexity=164.98727, train_loss=5.1058683

Batch 245370, train_perplexity=174.47746, train_loss=5.1617956

Batch 245380, train_perplexity=156.14009, train_loss=5.0507536

Batch 245390, train_perplexity=160.40648, train_loss=5.077711

Batch 245400, train_perplexity=167.8913, train_loss=5.123317

Batch 245410, train_perplexity=168.22018, train_loss=5.1252737

Batch 245420, train_perplexity=171.88847, train_loss=5.146846

Batch 245430, train_perplexity=178.85262, train_loss=5.186562

Batch 245440, train_perplexity=158.31046, train_loss=5.064558

Batch 245450, train_perplexity=165.67409, train_loss=5.1100225

Batch 245460, train_perplexity=173.81912, train_loss=5.1580153

Batch 245470, train_perplexity=158.99124, train_loss=5.068849

Batch 245480, train_perplexity=173.79037, train_loss=5.15785

Batch 245490, train_perplexity=163.78891, train_loss=5.0985785

Batch 245500, train_perplexity=170.79301, train_loss=5.1404524

Batch 245510, train_perplexity=158.68222, train_loss=5.0669036

Batch 245520, train_perplexity=175.03787, train_loss=5.1650023

Batch 245530, train_perplexity=173.98712, train_loss=5.1589813

Batch 245540, train_perplexity=153.46185, train_loss=5.033452

Batch 245550, train_perplexity=164.09206, train_loss=5.1004276

Batch 245560, train_perplexity=158.28435, train_loss=5.064393

Batch 245570, train_perplexity=167.3577, train_loss=5.1201334

Batch 245580, train_perplexity=140.25308, train_loss=4.9434485

Batch 245590, train_perplexity=142.71461, train_loss=4.960847

Batch 245600, train_perplexity=192.94522, train_loss=5.2624063

Batch 245610, train_perplexity=174.28049, train_loss=5.160666

Batch 245620, train_perplexity=155.57556, train_loss=5.0471315

Batch 245630, train_perplexity=162.90657, train_loss=5.093177

Batch 245640, train_perplexity=166.54903, train_loss=5.1152897

Batch 245650, train_perplexity=167.7207, train_loss=5.1223

Batch 245660, train_perplexity=166.57983, train_loss=5.1154747

Batch 245670, train_perplexity=136.35689, train_loss=4.9152756

Batch 245680, train_perplexity=160.91187, train_loss=5.080857

Batch 245690, train_perplexity=148.63512, train_loss=5.0014944

Batch 245700, train_perplexity=159.18256, train_loss=5.0700517

Batch 245710, train_perplexity=184.5322, train_loss=5.217824

Batch 245720, train_perplexity=153.43552, train_loss=5.0332804

Batch 245730, train_perplexity=161.14014, train_loss=5.0822744

Batch 245740, train_perplexity=153.97552, train_loss=5.0367937

Batch 245750, train_perplexity=176.01889, train_loss=5.1705914

Batch 245760, train_perplexity=177.17996, train_loss=5.177166

Batch 245770, train_perplexity=160.2108, train_loss=5.0764904

Batch 245780, train_perplexity=154.6488, train_loss=5.041157

Batch 245790, train_perplexity=159.7412, train_loss=5.073555

Batch 245800, train_perplexity=143.8412, train_loss=4.96871

Batch 245810, train_perplexity=167.79863, train_loss=5.1227646

Batch 245820, train_perplexity=179.27731, train_loss=5.188934

Batch 245830, train_perplexity=188.1506, train_loss=5.2372427

Batch 245840, train_perplexity=169.3261, train_loss=5.1318264

Batch 245850, train_perplexity=183.3824, train_loss=5.2115736

Batch 245860, train_perplexity=157.69717, train_loss=5.0606766

Batch 245870, train_perplexity=158.54442, train_loss=5.066035

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Batch 245880, train_perplexity=180.75958, train_loss=5.197168

Batch 245890, train_perplexity=161.0889, train_loss=5.0819564

Batch 245900, train_perplexity=162.92366, train_loss=5.0932817

Batch 245910, train_perplexity=152.42685, train_loss=5.0266848

Batch 245920, train_perplexity=158.99313, train_loss=5.068861

Batch 245930, train_perplexity=187.16594, train_loss=5.2319956

Batch 245940, train_perplexity=182.39104, train_loss=5.206153

Batch 245950, train_perplexity=179.97804, train_loss=5.192835

Batch 245960, train_perplexity=152.40242, train_loss=5.0265245

Batch 245970, train_perplexity=158.60257, train_loss=5.0664015

Batch 245980, train_perplexity=166.69077, train_loss=5.1161404

Batch 245990, train_perplexity=166.0534, train_loss=5.1123095

Batch 246000, train_perplexity=170.16245, train_loss=5.1367536

Batch 246010, train_perplexity=176.45522, train_loss=5.173067

Batch 246020, train_perplexity=175.04897, train_loss=5.165066
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 246030, train_perplexity=168.38358, train_loss=5.1262445

Batch 246040, train_perplexity=182.82135, train_loss=5.2085094

Batch 246050, train_perplexity=165.02771, train_loss=5.1061134

Batch 246060, train_perplexity=171.03416, train_loss=5.1418633

Batch 246070, train_perplexity=154.26994, train_loss=5.038704

Batch 246080, train_perplexity=174.4427, train_loss=5.1615963

Batch 246090, train_perplexity=157.33229, train_loss=5.05836

Batch 246100, train_perplexity=166.55553, train_loss=5.115329

Batch 246110, train_perplexity=185.75563, train_loss=5.224432

Batch 246120, train_perplexity=160.31633, train_loss=5.077149

Batch 246130, train_perplexity=178.06786, train_loss=5.1821647

Batch 246140, train_perplexity=150.30537, train_loss=5.012669

Batch 246150, train_perplexity=147.55257, train_loss=4.9941845

Batch 246160, train_perplexity=168.69298, train_loss=5.1280804

Batch 246170, train_perplexity=164.51968, train_loss=5.10303

Batch 246180, train_perplexity=176.77625, train_loss=5.174885

Batch 246190, train_perplexity=168.44429, train_loss=5.126605

Batch 246200, train_perplexity=146.70459, train_loss=4.988421

Batch 246210, train_perplexity=163.12204, train_loss=5.0944986

Batch 246220, train_perplexity=185.44958, train_loss=5.222783

Batch 246230, train_perplexity=163.64261, train_loss=5.097685

Batch 246240, train_perplexity=180.99219, train_loss=5.198454

Batch 246250, train_perplexity=185.44464, train_loss=5.2227564

Batch 246260, train_perplexity=165.72798, train_loss=5.1103477

Batch 246270, train_perplexity=163.32019, train_loss=5.0957127

Batch 246280, train_perplexity=178.8707, train_loss=5.186663

Batch 246290, train_perplexity=173.53639, train_loss=5.1563873

Batch 246300, train_perplexity=166.46915, train_loss=5.11481

Batch 246310, train_perplexity=162.90237, train_loss=5.093151

Batch 246320, train_perplexity=172.20743, train_loss=5.1486998

Batch 246330, train_perplexity=163.84312, train_loss=5.0989094

Batch 246340, train_perplexity=176.23389, train_loss=5.171812

Batch 246350, train_perplexity=187.82431, train_loss=5.235507

Batch 246360, train_perplexity=161.27574, train_loss=5.0831156

Batch 246370, train_perplexity=170.10014, train_loss=5.1363873

Batch 246380, train_perplexity=172.85481, train_loss=5.152452

Batch 246390, train_perplexity=170.93991, train_loss=5.141312

Batch 246400, train_perplexity=171.49501, train_loss=5.144554

Batch 246410, train_perplexity=170.49147, train_loss=5.138685

Batch 246420, train_perplexity=153.6411, train_loss=5.0346193

Batch 246430, train_perplexity=152.85948, train_loss=5.029519

Batch 246440, train_perplexity=169.57358, train_loss=5.133287

Batch 246450, train_perplexity=151.21382, train_loss=5.018695

Batch 246460, train_perplexity=177.41795, train_loss=5.1785083

Batch 246470, train_perplexity=175.16235, train_loss=5.1657133

Batch 246480, train_perplexity=187.42314, train_loss=5.233369

Batch 246490, train_perplexity=181.65874, train_loss=5.20213

Batch 246500, train_perplexity=169.7625, train_loss=5.1344004

Batch 246510, train_perplexity=168.46765, train_loss=5.126744

Batch 246520, train_perplexity=167.62596, train_loss=5.121735

Batch 246530, train_perplexity=163.27238, train_loss=5.09542

Batch 246540, train_perplexity=173.16995, train_loss=5.1542735

Batch 246550, train_perplexity=162.5656, train_loss=5.0910816

Batch 246560, train_perplexity=191.37326, train_loss=5.2542257

Batch 246570, train_perplexity=154.55348, train_loss=5.04054

Batch 246580, train_perplexity=161.9434, train_loss=5.087247

Batch 246590, train_perplexity=162.25838, train_loss=5.08919

Batch 246600, train_perplexity=161.10558, train_loss=5.08206

Batch 246610, train_perplexity=157.46086, train_loss=5.059177

Batch 246620, train_perplexity=158.8621, train_loss=5.0680366

Batch 246630, train_perplexity=172.88423, train_loss=5.152622

Batch 246640, train_perplexity=169.09396, train_loss=5.1304545

Batch 246650, train_perplexity=179.14067, train_loss=5.1881714

Batch 246660, train_perplexity=151.55612, train_loss=5.020956

Batch 246670, train_perplexity=156.84346, train_loss=5.0552483

Batch 246680, train_perplexity=161.57794, train_loss=5.0849876

Batch 246690, train_perplexity=166.78442, train_loss=5.116702

Batch 246700, train_perplexity=182.59413, train_loss=5.207266

Batch 246710, train_perplexity=161.54405, train_loss=5.084778

Batch 246720, train_perplexity=177.40189, train_loss=5.1784177

Batch 246730, train_perplexity=169.76022, train_loss=5.134387

Batch 246740, train_perplexity=170.40596, train_loss=5.1381836

Batch 246750, train_perplexity=186.47217, train_loss=5.228282

Batch 246760, train_perplexity=149.28311, train_loss=5.0058446

Batch 246770, train_perplexity=161.09366, train_loss=5.081986

Batch 246780, train_perplexity=163.33546, train_loss=5.095806

Batch 246790, train_perplexity=148.11311, train_loss=4.9979763

Batch 246800, train_perplexity=177.57817, train_loss=5.179411

Batch 246810, train_perplexity=173.25015, train_loss=5.1547365

Batch 246820, train_perplexity=180.93188, train_loss=5.1981206

Batch 246830, train_perplexity=174.85068, train_loss=5.1639323

Batch 246840, train_perplexity=155.55168, train_loss=5.046978

Batch 246850, train_perplexity=176.9196, train_loss=5.1756954

Batch 246860, train_perplexity=148.36023, train_loss=4.9996433

Batch 246870, train_perplexity=169.11678, train_loss=5.1305895

Batch 246880, train_perplexity=195.8245, train_loss=5.277219

Batch 246890, train_perplexity=183.22375, train_loss=5.210708

Batch 246900, train_perplexity=182.14525, train_loss=5.2048044

Batch 246910, train_perplexity=171.081, train_loss=5.142137

Batch 246920, train_perplexity=170.73886, train_loss=5.1401353

Batch 246930, train_perplexity=182.98383, train_loss=5.209398

Batch 246940, train_perplexity=165.1317, train_loss=5.1067433

Batch 246950, train_perplexity=169.64824, train_loss=5.133727

Batch 246960, train_perplexity=187.4546, train_loss=5.2335367

Batch 246970, train_perplexity=171.02968, train_loss=5.141837

Batch 246980, train_perplexity=167.24983, train_loss=5.1194887

Batch 246990, train_perplexity=174.7235, train_loss=5.1632047

Batch 247000, train_perplexity=138.282, train_loss=4.929295

Batch 247010, train_perplexity=151.37282, train_loss=5.019746

Batch 247020, train_perplexity=163.0627, train_loss=5.094135

Batch 247030, train_perplexity=198.71991, train_loss=5.2918963

Batch 247040, train_perplexity=150.29456, train_loss=5.012597

Batch 247050, train_perplexity=197.06433, train_loss=5.28353

Batch 247060, train_perplexity=165.79626, train_loss=5.1107597

Batch 247070, train_perplexity=164.06999, train_loss=5.100293

Batch 247080, train_perplexity=172.19183, train_loss=5.148609

Batch 247090, train_perplexity=173.79195, train_loss=5.157859

Batch 247100, train_perplexity=174.3908, train_loss=5.1612988

Batch 247110, train_perplexity=165.8445, train_loss=5.1110506

Batch 247120, train_perplexity=154.88673, train_loss=5.042694

Batch 247130, train_perplexity=162.11461, train_loss=5.0883036

Batch 247140, train_perplexity=178.24991, train_loss=5.1831865

Batch 247150, train_perplexity=183.80437, train_loss=5.213872

Batch 247160, train_perplexity=168.805, train_loss=5.128744

Batch 247170, train_perplexity=142.04622, train_loss=4.9561524

Batch 247180, train_perplexity=159.3801, train_loss=5.071292

Batch 247190, train_perplexity=169.19696, train_loss=5.1310635

Batch 247200, train_perplexity=165.68855, train_loss=5.11011

Batch 247210, train_perplexity=167.45732, train_loss=5.1207285

Batch 247220, train_perplexity=172.83577, train_loss=5.152342

Batch 247230, train_perplexity=187.15808, train_loss=5.2319536

Batch 247240, train_perplexity=161.65385, train_loss=5.0854573

Batch 247250, train_perplexity=184.1127, train_loss=5.215548

Batch 247260, train_perplexity=170.9475, train_loss=5.1413565

Batch 247270, train_perplexity=170.44366, train_loss=5.138405

Batch 247280, train_perplexity=180.65317, train_loss=5.196579

Batch 247290, train_perplexity=153.328, train_loss=5.0325794

Batch 247300, train_perplexity=179.32365, train_loss=5.1891923

Batch 247310, train_perplexity=161.40753, train_loss=5.0839324

Batch 247320, train_perplexity=162.03593, train_loss=5.087818
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 247330, train_perplexity=163.46355, train_loss=5.09659

Batch 247340, train_perplexity=158.76222, train_loss=5.0674076

Batch 247350, train_perplexity=151.72794, train_loss=5.022089

Batch 247360, train_perplexity=183.935, train_loss=5.2145824

Batch 247370, train_perplexity=168.19748, train_loss=5.1251388

Batch 247380, train_perplexity=154.97627, train_loss=5.043272

Batch 247390, train_perplexity=188.30336, train_loss=5.2380543

Batch 247400, train_perplexity=150.40654, train_loss=5.013342

Batch 247410, train_perplexity=159.33353, train_loss=5.0709996

Batch 247420, train_perplexity=172.54938, train_loss=5.1506834

Batch 247430, train_perplexity=172.09268, train_loss=5.148033

Batch 247440, train_perplexity=185.30727, train_loss=5.2220154

Batch 247450, train_perplexity=164.92175, train_loss=5.105471

Batch 247460, train_perplexity=173.51273, train_loss=5.156251

Batch 247470, train_perplexity=184.17873, train_loss=5.2159066

Batch 247480, train_perplexity=162.72832, train_loss=5.092082

Batch 247490, train_perplexity=167.37254, train_loss=5.120222

Batch 247500, train_perplexity=166.59755, train_loss=5.115581

Batch 247510, train_perplexity=167.28915, train_loss=5.119724

Batch 247520, train_perplexity=156.32611, train_loss=5.0519443

Batch 247530, train_perplexity=173.06784, train_loss=5.1536837

Batch 247540, train_perplexity=176.69147, train_loss=5.174405

Batch 247550, train_perplexity=168.00919, train_loss=5.1240187

Batch 247560, train_perplexity=179.28151, train_loss=5.188957

Batch 247570, train_perplexity=180.93283, train_loss=5.198126

Batch 247580, train_perplexity=163.43596, train_loss=5.0964212

Batch 247590, train_perplexity=170.33919, train_loss=5.1377916

Batch 247600, train_perplexity=170.48358, train_loss=5.138639

Batch 247610, train_perplexity=140.09929, train_loss=4.9423513

Batch 247620, train_perplexity=175.35274, train_loss=5.1667995

Batch 247630, train_perplexity=165.13359, train_loss=5.106755

Batch 247640, train_perplexity=159.46236, train_loss=5.071808

Batch 247650, train_perplexity=183.94342, train_loss=5.214628

Batch 247660, train_perplexity=174.13702, train_loss=5.1598425

Batch 247670, train_perplexity=170.17169, train_loss=5.136808

Batch 247680, train_perplexity=177.359, train_loss=5.178176

Batch 247690, train_perplexity=172.2646, train_loss=5.1490316

Batch 247700, train_perplexity=174.77158, train_loss=5.16348

Batch 247710, train_perplexity=181.76747, train_loss=5.2027283

Batch 247720, train_perplexity=149.55179, train_loss=5.0076427

Batch 247730, train_perplexity=165.24474, train_loss=5.1074276

Batch 247740, train_perplexity=160.13121, train_loss=5.0759935

Batch 247750, train_perplexity=159.60521, train_loss=5.0727034

Batch 247760, train_perplexity=178.94183, train_loss=5.187061

Batch 247770, train_perplexity=175.32898, train_loss=5.166664

Batch 247780, train_perplexity=161.95305, train_loss=5.0873065

Batch 247790, train_perplexity=180.8007, train_loss=5.1973953

Batch 247800, train_perplexity=166.01492, train_loss=5.1120777

Batch 247810, train_perplexity=153.51016, train_loss=5.0337667

Batch 247820, train_perplexity=167.6908, train_loss=5.122122

Batch 247830, train_perplexity=163.67273, train_loss=5.097869

Batch 247840, train_perplexity=169.72356, train_loss=5.134171

Batch 247850, train_perplexity=171.66026, train_loss=5.1455173

Batch 247860, train_perplexity=177.31435, train_loss=5.177924

Batch 247870, train_perplexity=165.04124, train_loss=5.1061954

Batch 247880, train_perplexity=154.90504, train_loss=5.0428123

Batch 247890, train_perplexity=148.10104, train_loss=4.997895

Batch 247900, train_perplexity=149.48312, train_loss=5.0071836

Batch 247910, train_perplexity=160.1471, train_loss=5.0760927

Batch 247920, train_perplexity=192.13629, train_loss=5.258205

Batch 247930, train_perplexity=185.66583, train_loss=5.2239485

Batch 247940, train_perplexity=157.6859, train_loss=5.060605

Batch 247950, train_perplexity=167.09662, train_loss=5.118572

Batch 247960, train_perplexity=167.12077, train_loss=5.1187167

Batch 247970, train_perplexity=165.80876, train_loss=5.110835

Batch 247980, train_perplexity=168.72531, train_loss=5.128272

Batch 247990, train_perplexity=156.77632, train_loss=5.05482

Batch 248000, train_perplexity=165.13745, train_loss=5.106778

Batch 248010, train_perplexity=170.7466, train_loss=5.1401806

Batch 248020, train_perplexity=155.65704, train_loss=5.047655

Batch 248030, train_perplexity=159.2663, train_loss=5.0705776

Batch 248040, train_perplexity=150.4305, train_loss=5.013501

Batch 248050, train_perplexity=156.39098, train_loss=5.052359

Batch 248060, train_perplexity=161.40083, train_loss=5.083891

Batch 248070, train_perplexity=162.49066, train_loss=5.0906205

Batch 248080, train_perplexity=185.32327, train_loss=5.2221017

Batch 248090, train_perplexity=172.18642, train_loss=5.1485777

Batch 248100, train_perplexity=166.82706, train_loss=5.1169577

Batch 248110, train_perplexity=175.21248, train_loss=5.1659994

Batch 248120, train_perplexity=154.02959, train_loss=5.0371447

Batch 248130, train_perplexity=154.87425, train_loss=5.0426135

Batch 248140, train_perplexity=189.0427, train_loss=5.241973

Batch 248150, train_perplexity=177.69795, train_loss=5.180085

Batch 248160, train_perplexity=166.88115, train_loss=5.117282

Batch 248170, train_perplexity=167.8287, train_loss=5.122944

Batch 248180, train_perplexity=150.57446, train_loss=5.0144577

Batch 248190, train_perplexity=164.06593, train_loss=5.1002684

Batch 248200, train_perplexity=173.40619, train_loss=5.155637

Batch 248210, train_perplexity=160.67671, train_loss=5.0793943

Batch 248220, train_perplexity=166.1742, train_loss=5.1130366

Batch 248230, train_perplexity=163.54167, train_loss=5.097068

Batch 248240, train_perplexity=154.62196, train_loss=5.040983

Batch 248250, train_perplexity=157.76863, train_loss=5.0611296

Batch 248260, train_perplexity=168.5063, train_loss=5.126973

Batch 248270, train_perplexity=171.16022, train_loss=5.1426

Batch 248280, train_perplexity=166.57141, train_loss=5.115424

Batch 248290, train_perplexity=162.70186, train_loss=5.0919194

Batch 248300, train_perplexity=177.38792, train_loss=5.178339

Batch 248310, train_perplexity=149.1278, train_loss=5.0048037

Batch 248320, train_perplexity=168.653, train_loss=5.1278434

Batch 248330, train_perplexity=144.71872, train_loss=4.974792

Batch 248340, train_perplexity=177.83154, train_loss=5.1808367

Batch 248350, train_perplexity=158.9948, train_loss=5.0688715

Batch 248360, train_perplexity=159.02551, train_loss=5.0690646

Batch 248370, train_perplexity=158.51517, train_loss=5.0658503

Batch 248380, train_perplexity=182.00304, train_loss=5.2040234

Batch 248390, train_perplexity=152.80075, train_loss=5.0291348

Batch 248400, train_perplexity=155.05655, train_loss=5.04379

Batch 248410, train_perplexity=175.32773, train_loss=5.166657

Batch 248420, train_perplexity=176.33191, train_loss=5.172368

Batch 248430, train_perplexity=171.94142, train_loss=5.147154

Batch 248440, train_perplexity=182.58978, train_loss=5.207242

Batch 248450, train_perplexity=160.608, train_loss=5.0789666

Batch 248460, train_perplexity=145.60228, train_loss=4.980879

Batch 248470, train_perplexity=162.8341, train_loss=5.092732

Batch 248480, train_perplexity=162.46053, train_loss=5.090435

Batch 248490, train_perplexity=156.15305, train_loss=5.0508366

Batch 248500, train_perplexity=185.68213, train_loss=5.224036

Batch 248510, train_perplexity=162.61662, train_loss=5.0913954

Batch 248520, train_perplexity=153.85179, train_loss=5.0359898

Batch 248530, train_perplexity=157.98589, train_loss=5.0625057

Batch 248540, train_perplexity=157.3479, train_loss=5.0584593

Batch 248550, train_perplexity=161.46133, train_loss=5.0842657

Batch 248560, train_perplexity=162.5701, train_loss=5.0911093

Batch 248570, train_perplexity=171.65683, train_loss=5.1454973

Batch 248580, train_perplexity=181.69052, train_loss=5.202305

Batch 248590, train_perplexity=162.10518, train_loss=5.0882454

Batch 248600, train_perplexity=156.60133, train_loss=5.0537033

Batch 248610, train_perplexity=153.13182, train_loss=5.031299

Batch 248620, train_perplexity=179.28047, train_loss=5.1889515
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 248630, train_perplexity=174.5472, train_loss=5.162195

Batch 248640, train_perplexity=148.15733, train_loss=4.998275

Batch 248650, train_perplexity=169.95964, train_loss=5.135561

Batch 248660, train_perplexity=158.79819, train_loss=5.067634

Batch 248670, train_perplexity=165.77397, train_loss=5.1106253

Batch 248680, train_perplexity=188.12709, train_loss=5.237118

Batch 248690, train_perplexity=177.94757, train_loss=5.181489

Batch 248700, train_perplexity=162.11476, train_loss=5.0883045

Batch 248710, train_perplexity=194.81685, train_loss=5.27206

Batch 248720, train_perplexity=171.64505, train_loss=5.1454287

Batch 248730, train_perplexity=177.19534, train_loss=5.177253

Batch 248740, train_perplexity=156.59521, train_loss=5.053664

Batch 248750, train_perplexity=169.98395, train_loss=5.135704

Batch 248760, train_perplexity=155.01588, train_loss=5.0435276

Batch 248770, train_perplexity=143.6386, train_loss=4.9673004

Batch 248780, train_perplexity=183.85828, train_loss=5.214165

Batch 248790, train_perplexity=172.15152, train_loss=5.148375

Batch 248800, train_perplexity=175.80182, train_loss=5.1693573

Batch 248810, train_perplexity=169.52411, train_loss=5.132995

Batch 248820, train_perplexity=198.75241, train_loss=5.29206

Batch 248830, train_perplexity=163.47134, train_loss=5.0966377

Batch 248840, train_perplexity=165.00774, train_loss=5.1059923

Batch 248850, train_perplexity=174.3913, train_loss=5.1613016

Batch 248860, train_perplexity=162.87535, train_loss=5.092985

Batch 248870, train_perplexity=151.89095, train_loss=5.023163

Batch 248880, train_perplexity=172.98534, train_loss=5.153207

Batch 248890, train_perplexity=162.07751, train_loss=5.0880747

Batch 248900, train_perplexity=173.74586, train_loss=5.1575937

Batch 248910, train_perplexity=164.03754, train_loss=5.1000953

Batch 248920, train_perplexity=157.11429, train_loss=5.0569735

Batch 248930, train_perplexity=175.08679, train_loss=5.165282

Batch 248940, train_perplexity=179.62703, train_loss=5.1908827

Batch 248950, train_perplexity=154.33836, train_loss=5.0391474

Batch 248960, train_perplexity=178.20282, train_loss=5.1829224

Batch 248970, train_perplexity=162.30984, train_loss=5.089507

Batch 248980, train_perplexity=162.08786, train_loss=5.0881386

Batch 248990, train_perplexity=162.01639, train_loss=5.0876975

Batch 249000, train_perplexity=190.60298, train_loss=5.2501926

Batch 249010, train_perplexity=179.79808, train_loss=5.1918344

Batch 249020, train_perplexity=146.47777, train_loss=4.9868736

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00062-of-00100
Loaded 306328 sentences.
Finished loading
Batch 249030, train_perplexity=164.06741, train_loss=5.1002774

Batch 249040, train_perplexity=160.83493, train_loss=5.0803785

Batch 249050, train_perplexity=170.8475, train_loss=5.1407714

Batch 249060, train_perplexity=159.8369, train_loss=5.074154

Batch 249070, train_perplexity=164.90602, train_loss=5.105376

Batch 249080, train_perplexity=151.27585, train_loss=5.019105

Batch 249090, train_perplexity=172.07741, train_loss=5.1479445

Batch 249100, train_perplexity=181.3533, train_loss=5.200447

Batch 249110, train_perplexity=162.80383, train_loss=5.092546

Batch 249120, train_perplexity=169.11903, train_loss=5.130603

Batch 249130, train_perplexity=158.59349, train_loss=5.0663443

Batch 249140, train_perplexity=175.7036, train_loss=5.1687984

Batch 249150, train_perplexity=175.83308, train_loss=5.169535

Batch 249160, train_perplexity=173.34427, train_loss=5.1552796

Batch 249170, train_perplexity=179.34546, train_loss=5.189314

Batch 249180, train_perplexity=174.17888, train_loss=5.160083

Batch 249190, train_perplexity=172.52107, train_loss=5.1505194

Batch 249200, train_perplexity=173.17499, train_loss=5.1543026

Batch 249210, train_perplexity=144.58415, train_loss=4.9738617

Batch 249220, train_perplexity=161.40483, train_loss=5.0839157

Batch 249230, train_perplexity=160.74461, train_loss=5.079817

Batch 249240, train_perplexity=158.21175, train_loss=5.0639343

Batch 249250, train_perplexity=157.62636, train_loss=5.0602274

Batch 249260, train_perplexity=172.44162, train_loss=5.1500587

Batch 249270, train_perplexity=180.5494, train_loss=5.1960044

Batch 249280, train_perplexity=171.72609, train_loss=5.1459007

Batch 249290, train_perplexity=156.12073, train_loss=5.0506296

Batch 249300, train_perplexity=169.59372, train_loss=5.1334057

Batch 249310, train_perplexity=181.69, train_loss=5.202302

Batch 249320, train_perplexity=146.86186, train_loss=4.9894924

Batch 249330, train_perplexity=162.57722, train_loss=5.091153

Batch 249340, train_perplexity=160.98047, train_loss=5.081283

Batch 249350, train_perplexity=159.06987, train_loss=5.0693436

Batch 249360, train_perplexity=180.30188, train_loss=5.1946325

Batch 249370, train_perplexity=171.51529, train_loss=5.1446724

Batch 249380, train_perplexity=161.8926, train_loss=5.086933

Batch 249390, train_perplexity=160.86575, train_loss=5.08057

Batch 249400, train_perplexity=182.04375, train_loss=5.204247

Batch 249410, train_perplexity=144.24164, train_loss=4.97149

Batch 249420, train_perplexity=168.2213, train_loss=5.1252804

Batch 249430, train_perplexity=152.88318, train_loss=5.029674

Batch 249440, train_perplexity=169.76913, train_loss=5.1344395

Batch 249450, train_perplexity=153.19046, train_loss=5.031682

Batch 249460, train_perplexity=170.977, train_loss=5.141529

Batch 249470, train_perplexity=180.2405, train_loss=5.194292

Batch 249480, train_perplexity=162.62778, train_loss=5.091464

Batch 249490, train_perplexity=178.98033, train_loss=5.187276

Batch 249500, train_perplexity=179.39995, train_loss=5.1896176

Batch 249510, train_perplexity=157.44562, train_loss=5.05908

Batch 249520, train_perplexity=160.0045, train_loss=5.075202

Batch 249530, train_perplexity=153.93002, train_loss=5.036498

Batch 249540, train_perplexity=158.74678, train_loss=5.0673103

Batch 249550, train_perplexity=167.23915, train_loss=5.119425

Batch 249560, train_perplexity=177.83595, train_loss=5.1808615

Batch 249570, train_perplexity=173.90933, train_loss=5.158534

Batch 249580, train_perplexity=160.98976, train_loss=5.081341

Batch 249590, train_perplexity=162.63492, train_loss=5.091508

Batch 249600, train_perplexity=163.41048, train_loss=5.0962653

Batch 249610, train_perplexity=170.14062, train_loss=5.1366253

Batch 249620, train_perplexity=178.94704, train_loss=5.18709

Batch 249630, train_perplexity=159.97758, train_loss=5.0750337

Batch 249640, train_perplexity=159.43475, train_loss=5.071635

Batch 249650, train_perplexity=177.35849, train_loss=5.178173

Batch 249660, train_perplexity=166.39043, train_loss=5.114337

Batch 249670, train_perplexity=162.29204, train_loss=5.0893974

Batch 249680, train_perplexity=138.3653, train_loss=4.9298973

Batch 249690, train_perplexity=160.294, train_loss=5.0770097

Batch 249700, train_perplexity=170.2264, train_loss=5.1371293

Batch 249710, train_perplexity=168.32425, train_loss=5.125892

Batch 249720, train_perplexity=166.846, train_loss=5.117071

Batch 249730, train_perplexity=150.48976, train_loss=5.013895

Batch 249740, train_perplexity=166.76454, train_loss=5.116583

Batch 249750, train_perplexity=168.56473, train_loss=5.12732

Batch 249760, train_perplexity=168.24232, train_loss=5.1254053

Batch 249770, train_perplexity=169.89465, train_loss=5.1351786

Batch 249780, train_perplexity=173.89589, train_loss=5.158457

Batch 249790, train_perplexity=165.62512, train_loss=5.109727

Batch 249800, train_perplexity=170.60857, train_loss=5.139372

Batch 249810, train_perplexity=177.34142, train_loss=5.1780767

Batch 249820, train_perplexity=151.3322, train_loss=5.0194774

Batch 249830, train_perplexity=155.96567, train_loss=5.049636

Batch 249840, train_perplexity=169.21011, train_loss=5.131141

Batch 249850, train_perplexity=165.65514, train_loss=5.109908

Batch 249860, train_perplexity=146.40947, train_loss=4.9864073
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 249870, train_perplexity=165.4539, train_loss=5.1086926

Batch 249880, train_perplexity=189.3749, train_loss=5.2437286

Batch 249890, train_perplexity=176.2086, train_loss=5.1716685

Batch 249900, train_perplexity=166.39162, train_loss=5.114344

Batch 249910, train_perplexity=166.49289, train_loss=5.1149526

Batch 249920, train_perplexity=181.78612, train_loss=5.202831

Batch 249930, train_perplexity=161.0601, train_loss=5.0817776

Batch 249940, train_perplexity=153.80382, train_loss=5.035678

Batch 249950, train_perplexity=162.27377, train_loss=5.089285

Batch 249960, train_perplexity=155.70276, train_loss=5.047949

Batch 249970, train_perplexity=151.5885, train_loss=5.0211697

Batch 249980, train_perplexity=181.89874, train_loss=5.20345

Batch 249990, train_perplexity=162.92474, train_loss=5.0932884

Batch 250000, train_perplexity=175.07133, train_loss=5.1651936

Batch 250010, train_perplexity=176.18684, train_loss=5.171545

Batch 250020, train_perplexity=158.85748, train_loss=5.0680075

Batch 250030, train_perplexity=168.23366, train_loss=5.125354

Batch 250040, train_perplexity=179.23483, train_loss=5.188697

Batch 250050, train_perplexity=167.54182, train_loss=5.121233

Batch 250060, train_perplexity=162.9347, train_loss=5.0933495

Batch 250070, train_perplexity=184.31728, train_loss=5.2166586

Batch 250080, train_perplexity=179.97925, train_loss=5.1928415

Batch 250090, train_perplexity=149.77058, train_loss=5.0091047

Batch 250100, train_perplexity=169.552, train_loss=5.1331596

Batch 250110, train_perplexity=157.69627, train_loss=5.060671

Batch 250120, train_perplexity=145.04376, train_loss=4.9770355

Batch 250130, train_perplexity=151.53221, train_loss=5.020798

Batch 250140, train_perplexity=176.20499, train_loss=5.171648

Batch 250150, train_perplexity=156.92882, train_loss=5.0557923

Batch 250160, train_perplexity=137.91054, train_loss=4.926605

Batch 250170, train_perplexity=179.64279, train_loss=5.1909704

Batch 250180, train_perplexity=154.71991, train_loss=5.0416164

Batch 250190, train_perplexity=155.10165, train_loss=5.0440807

Batch 250200, train_perplexity=164.98413, train_loss=5.1058493

Batch 250210, train_perplexity=150.93259, train_loss=5.0168333

Batch 250220, train_perplexity=173.61577, train_loss=5.1568446

Batch 250230, train_perplexity=162.37534, train_loss=5.0899105

Batch 250240, train_perplexity=149.99057, train_loss=5.0105724

Batch 250250, train_perplexity=157.76727, train_loss=5.061121

Batch 250260, train_perplexity=184.08662, train_loss=5.2154064

Batch 250270, train_perplexity=179.51572, train_loss=5.190263

Batch 250280, train_perplexity=142.5295, train_loss=4.959549

Batch 250290, train_perplexity=179.17758, train_loss=5.1883774

Batch 250300, train_perplexity=158.98904, train_loss=5.0688353

Batch 250310, train_perplexity=158.10391, train_loss=5.0632524

Batch 250320, train_perplexity=157.8785, train_loss=5.0618258

Batch 250330, train_perplexity=185.13928, train_loss=5.2211084

Batch 250340, train_perplexity=151.87154, train_loss=5.023035

Batch 250350, train_perplexity=174.44104, train_loss=5.1615868

Batch 250360, train_perplexity=153.35593, train_loss=5.0327616

Batch 250370, train_perplexity=151.53583, train_loss=5.020822

Batch 250380, train_perplexity=178.96274, train_loss=5.1871777

Batch 250390, train_perplexity=165.3543, train_loss=5.1080904

Batch 250400, train_perplexity=174.0155, train_loss=5.1591444

Batch 250410, train_perplexity=168.29495, train_loss=5.125718

Batch 250420, train_perplexity=161.60144, train_loss=5.085133

Batch 250430, train_perplexity=156.79814, train_loss=5.0549593

Batch 250440, train_perplexity=158.92946, train_loss=5.0684605

Batch 250450, train_perplexity=160.38919, train_loss=5.0776033

Batch 250460, train_perplexity=162.61754, train_loss=5.091401

Batch 250470, train_perplexity=175.65752, train_loss=5.168536

Batch 250480, train_perplexity=173.17574, train_loss=5.154307

Batch 250490, train_perplexity=166.92772, train_loss=5.117561

Batch 250500, train_perplexity=161.89923, train_loss=5.086974

Batch 250510, train_perplexity=170.4539, train_loss=5.138465

Batch 250520, train_perplexity=166.54553, train_loss=5.1152687

Batch 250530, train_perplexity=169.52184, train_loss=5.132982

Batch 250540, train_perplexity=163.9989, train_loss=5.0998597

Batch 250550, train_perplexity=169.45824, train_loss=5.1326065

Batch 250560, train_perplexity=157.14598, train_loss=5.057175

Batch 250570, train_perplexity=183.98518, train_loss=5.214855

Batch 250580, train_perplexity=163.09653, train_loss=5.094342

Batch 250590, train_perplexity=148.30266, train_loss=4.999255

Batch 250600, train_perplexity=169.89627, train_loss=5.135188

Batch 250610, train_perplexity=182.66573, train_loss=5.207658

Batch 250620, train_perplexity=147.678, train_loss=4.995034

Batch 250630, train_perplexity=167.85849, train_loss=5.1231213

Batch 250640, train_perplexity=156.38202, train_loss=5.052302

Batch 250650, train_perplexity=158.7589, train_loss=5.0673866

Batch 250660, train_perplexity=183.65034, train_loss=5.2130337

Batch 250670, train_perplexity=146.46365, train_loss=4.9867773

Batch 250680, train_perplexity=153.0192, train_loss=5.0305634

Batch 250690, train_perplexity=137.45479, train_loss=4.923295

Batch 250700, train_perplexity=165.89038, train_loss=5.111327

Batch 250710, train_perplexity=167.03958, train_loss=5.118231

Batch 250720, train_perplexity=192.52762, train_loss=5.2602396

Batch 250730, train_perplexity=165.54474, train_loss=5.1092415

Batch 250740, train_perplexity=174.84476, train_loss=5.1638985

Batch 250750, train_perplexity=174.77374, train_loss=5.163492

Batch 250760, train_perplexity=139.31812, train_loss=4.93676

Batch 250770, train_perplexity=156.93346, train_loss=5.055822

Batch 250780, train_perplexity=154.83282, train_loss=5.042346

Batch 250790, train_perplexity=158.04256, train_loss=5.0628643

Batch 250800, train_perplexity=175.02017, train_loss=5.1649013

Batch 250810, train_perplexity=168.95348, train_loss=5.1296234

Batch 250820, train_perplexity=153.89075, train_loss=5.036243

Batch 250830, train_perplexity=168.56256, train_loss=5.127307

Batch 250840, train_perplexity=150.92194, train_loss=5.0167627

Batch 250850, train_perplexity=139.50153, train_loss=4.9380755

Batch 250860, train_perplexity=156.92665, train_loss=5.0557785

Batch 250870, train_perplexity=165.93103, train_loss=5.1115723

Batch 250880, train_perplexity=164.42871, train_loss=5.102477

Batch 250890, train_perplexity=152.93034, train_loss=5.0299826

Batch 250900, train_perplexity=179.514, train_loss=5.1902533

Batch 250910, train_perplexity=169.9833, train_loss=5.1357

Batch 250920, train_perplexity=164.43906, train_loss=5.10254

Batch 250930, train_perplexity=182.75162, train_loss=5.208128

Batch 250940, train_perplexity=179.45393, train_loss=5.1899185

Batch 250950, train_perplexity=166.10742, train_loss=5.1126347

Batch 250960, train_perplexity=155.46521, train_loss=5.046422

Batch 250970, train_perplexity=164.75583, train_loss=5.1044645

Batch 250980, train_perplexity=168.80716, train_loss=5.128757

Batch 250990, train_perplexity=174.6686, train_loss=5.1628904

Batch 251000, train_perplexity=157.83455, train_loss=5.0615473

Batch 251010, train_perplexity=167.64555, train_loss=5.121852

Batch 251020, train_perplexity=161.90201, train_loss=5.0869913

Batch 251030, train_perplexity=164.51624, train_loss=5.103009

Batch 251040, train_perplexity=168.31758, train_loss=5.1258526

Batch 251050, train_perplexity=183.6662, train_loss=5.21312

Batch 251060, train_perplexity=179.33075, train_loss=5.189232

Batch 251070, train_perplexity=159.31445, train_loss=5.07088

Batch 251080, train_perplexity=170.94717, train_loss=5.1413546

Batch 251090, train_perplexity=183.20079, train_loss=5.2105827

Batch 251100, train_perplexity=140.47449, train_loss=4.945026

Batch 251110, train_perplexity=169.12033, train_loss=5.1306105

Batch 251120, train_perplexity=183.69624, train_loss=5.2132835

Batch 251130, train_perplexity=159.03029, train_loss=5.0690947

Batch 251140, train_perplexity=175.47762, train_loss=5.1675115

Batch 251150, train_perplexity=163.29185, train_loss=5.095539

Batch 251160, train_perplexity=152.34901, train_loss=5.026174
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 251170, train_perplexity=167.79271, train_loss=5.1227293

Batch 251180, train_perplexity=141.97423, train_loss=4.9556456

Batch 251190, train_perplexity=165.92986, train_loss=5.111565

Batch 251200, train_perplexity=151.59523, train_loss=5.021214

Batch 251210, train_perplexity=160.46095, train_loss=5.0780506

Batch 251220, train_perplexity=196.24477, train_loss=5.2793627

Batch 251230, train_perplexity=174.47531, train_loss=5.161783

Batch 251240, train_perplexity=173.50296, train_loss=5.1561947

Batch 251250, train_perplexity=141.31367, train_loss=4.950982

Batch 251260, train_perplexity=160.19101, train_loss=5.076367

Batch 251270, train_perplexity=168.91513, train_loss=5.1293964

Batch 251280, train_perplexity=164.76259, train_loss=5.1045055

Batch 251290, train_perplexity=154.5422, train_loss=5.0404673

Batch 251300, train_perplexity=134.89879, train_loss=4.904525

Batch 251310, train_perplexity=163.13565, train_loss=5.094582

Batch 251320, train_perplexity=170.86975, train_loss=5.1409016

Batch 251330, train_perplexity=163.78976, train_loss=5.0985837

Batch 251340, train_perplexity=174.08423, train_loss=5.159539

Batch 251350, train_perplexity=169.19873, train_loss=5.131074

Batch 251360, train_perplexity=169.06978, train_loss=5.1303115

Batch 251370, train_perplexity=163.11581, train_loss=5.0944605

Batch 251380, train_perplexity=171.09657, train_loss=5.142228

Batch 251390, train_perplexity=169.26595, train_loss=5.131471

Batch 251400, train_perplexity=156.659, train_loss=5.0540714

Batch 251410, train_perplexity=146.80374, train_loss=4.9890966

Batch 251420, train_perplexity=160.86253, train_loss=5.08055

Batch 251430, train_perplexity=169.75148, train_loss=5.1343355

Batch 251440, train_perplexity=177.54362, train_loss=5.1792164

Batch 251450, train_perplexity=153.80118, train_loss=5.0356607

Batch 251460, train_perplexity=140.84682, train_loss=4.947673

Batch 251470, train_perplexity=176.74742, train_loss=5.1747217

Batch 251480, train_perplexity=160.51176, train_loss=5.078367

Batch 251490, train_perplexity=172.70215, train_loss=5.1515684

Batch 251500, train_perplexity=157.8937, train_loss=5.061922

Batch 251510, train_perplexity=164.82858, train_loss=5.104906

Batch 251520, train_perplexity=163.5774, train_loss=5.097286

Batch 251530, train_perplexity=157.18457, train_loss=5.0574207

Batch 251540, train_perplexity=153.6433, train_loss=5.0346336

Batch 251550, train_perplexity=167.86249, train_loss=5.123145

Batch 251560, train_perplexity=153.76482, train_loss=5.035424

Batch 251570, train_perplexity=162.96507, train_loss=5.093536

Batch 251580, train_perplexity=165.48625, train_loss=5.108888

Batch 251590, train_perplexity=147.8113, train_loss=4.9959364

Batch 251600, train_perplexity=181.85452, train_loss=5.203207

Batch 251610, train_perplexity=162.20555, train_loss=5.0888643

Batch 251620, train_perplexity=153.12517, train_loss=5.0312557

Batch 251630, train_perplexity=163.15752, train_loss=5.094716

Batch 251640, train_perplexity=156.51698, train_loss=5.0531645

Batch 251650, train_perplexity=165.32796, train_loss=5.107931

Batch 251660, train_perplexity=165.02512, train_loss=5.1060977

Batch 251670, train_perplexity=162.29932, train_loss=5.0894423

Batch 251680, train_perplexity=151.4794, train_loss=5.0204496

Batch 251690, train_perplexity=163.02391, train_loss=5.093897

Batch 251700, train_perplexity=162.36743, train_loss=5.089862

Batch 251710, train_perplexity=157.78548, train_loss=5.0612364

Batch 251720, train_perplexity=170.35324, train_loss=5.137874

Batch 251730, train_perplexity=168.04413, train_loss=5.1242266

Batch 251740, train_perplexity=166.85172, train_loss=5.1171055

Batch 251750, train_perplexity=171.18863, train_loss=5.142766

Batch 251760, train_perplexity=173.24701, train_loss=5.1547184

Batch 251770, train_perplexity=180.71512, train_loss=5.196922

Batch 251780, train_perplexity=164.11584, train_loss=5.1005726

Batch 251790, train_perplexity=163.7818, train_loss=5.098535

Batch 251800, train_perplexity=163.53357, train_loss=5.0970182

Batch 251810, train_perplexity=162.34111, train_loss=5.0896997

Batch 251820, train_perplexity=205.59431, train_loss=5.325905

Batch 251830, train_perplexity=163.88469, train_loss=5.099163

Batch 251840, train_perplexity=167.69225, train_loss=5.1221304

Batch 251850, train_perplexity=158.1496, train_loss=5.0635414

Batch 251860, train_perplexity=149.23073, train_loss=5.0054936

Batch 251870, train_perplexity=162.47842, train_loss=5.090545

Batch 251880, train_perplexity=166.64673, train_loss=5.115876

Batch 251890, train_perplexity=173.69733, train_loss=5.1573143

Batch 251900, train_perplexity=176.11528, train_loss=5.171139

Batch 251910, train_perplexity=165.51656, train_loss=5.1090713

Batch 251920, train_perplexity=181.91428, train_loss=5.2035356

Batch 251930, train_perplexity=145.39622, train_loss=4.9794626

Batch 251940, train_perplexity=170.1833, train_loss=5.136876

Batch 251950, train_perplexity=164.31184, train_loss=5.101766

Batch 251960, train_perplexity=170.6822, train_loss=5.1398034

Batch 251970, train_perplexity=170.29752, train_loss=5.137547

Batch 251980, train_perplexity=180.74208, train_loss=5.197071

Batch 251990, train_perplexity=162.21375, train_loss=5.088915

Batch 252000, train_perplexity=169.38715, train_loss=5.132187

Batch 252010, train_perplexity=160.80594, train_loss=5.0801983

Batch 252020, train_perplexity=166.61574, train_loss=5.11569

Batch 252030, train_perplexity=154.4888, train_loss=5.0401216

Batch 252040, train_perplexity=164.7841, train_loss=5.104636

Batch 252050, train_perplexity=168.64616, train_loss=5.127803

Batch 252060, train_perplexity=187.30164, train_loss=5.2327204

Batch 252070, train_perplexity=176.73941, train_loss=5.1746764

Batch 252080, train_perplexity=159.254, train_loss=5.0705004

Batch 252090, train_perplexity=176.38783, train_loss=5.172685

Batch 252100, train_perplexity=152.102, train_loss=5.0245514

Batch 252110, train_perplexity=171.24821, train_loss=5.143114

Batch 252120, train_perplexity=164.2737, train_loss=5.101534

Batch 252130, train_perplexity=170.67708, train_loss=5.1397734

Batch 252140, train_perplexity=156.55005, train_loss=5.0533757

Batch 252150, train_perplexity=172.73245, train_loss=5.151744

Batch 252160, train_perplexity=156.7799, train_loss=5.054843

Batch 252170, train_perplexity=166.9512, train_loss=5.1177015

Batch 252180, train_perplexity=180.46031, train_loss=5.195511

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00072-of-00100
Loaded 306018 sentences.
Finished loading
Batch 252190, train_perplexity=160.25664, train_loss=5.0767765

Batch 252200, train_perplexity=158.91779, train_loss=5.068387

Batch 252210, train_perplexity=134.96146, train_loss=4.9049892

Batch 252220, train_perplexity=157.48376, train_loss=5.0593224

Batch 252230, train_perplexity=163.08751, train_loss=5.094287

Batch 252240, train_perplexity=161.84212, train_loss=5.0866213

Batch 252250, train_perplexity=158.6388, train_loss=5.06663

Batch 252260, train_perplexity=165.92834, train_loss=5.111556

Batch 252270, train_perplexity=157.01797, train_loss=5.0563602

Batch 252280, train_perplexity=163.7572, train_loss=5.098385

Batch 252290, train_perplexity=180.16179, train_loss=5.1938553

Batch 252300, train_perplexity=156.35435, train_loss=5.052125

Batch 252310, train_perplexity=175.81422, train_loss=5.169428

Batch 252320, train_perplexity=163.00291, train_loss=5.093768

Batch 252330, train_perplexity=168.32585, train_loss=5.1259017

Batch 252340, train_perplexity=152.85365, train_loss=5.029481

Batch 252350, train_perplexity=167.57777, train_loss=5.1214476

Batch 252360, train_perplexity=166.00938, train_loss=5.1120443

Batch 252370, train_perplexity=169.97958, train_loss=5.1356783

Batch 252380, train_perplexity=167.95776, train_loss=5.1237125

Batch 252390, train_perplexity=178.38153, train_loss=5.1839247

Batch 252400, train_perplexity=154.07689, train_loss=5.0374517
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 252410, train_perplexity=168.49023, train_loss=5.126878

Batch 252420, train_perplexity=176.19835, train_loss=5.1716104

Batch 252430, train_perplexity=149.8686, train_loss=5.009759

Batch 252440, train_perplexity=180.29646, train_loss=5.1946025

Batch 252450, train_perplexity=161.0009, train_loss=5.08141

Batch 252460, train_perplexity=167.70264, train_loss=5.1221924

Batch 252470, train_perplexity=176.27112, train_loss=5.1720233

Batch 252480, train_perplexity=162.33229, train_loss=5.0896454

Batch 252490, train_perplexity=156.12549, train_loss=5.05066

Batch 252500, train_perplexity=171.90683, train_loss=5.1469526

Batch 252510, train_perplexity=167.83351, train_loss=5.1229725

Batch 252520, train_perplexity=155.2577, train_loss=5.0450864

Batch 252530, train_perplexity=172.13814, train_loss=5.1482973

Batch 252540, train_perplexity=175.90395, train_loss=5.169938

Batch 252550, train_perplexity=164.16673, train_loss=5.1008825

Batch 252560, train_perplexity=162.39453, train_loss=5.090029

Batch 252570, train_perplexity=160.60777, train_loss=5.078965

Batch 252580, train_perplexity=159.575, train_loss=5.072514

Batch 252590, train_perplexity=191.84178, train_loss=5.256671

Batch 252600, train_perplexity=177.07295, train_loss=5.176562

Batch 252610, train_perplexity=166.20201, train_loss=5.113204

Batch 252620, train_perplexity=177.89447, train_loss=5.1811905

Batch 252630, train_perplexity=173.79095, train_loss=5.157853

Batch 252640, train_perplexity=162.12096, train_loss=5.0883427

Batch 252650, train_perplexity=150.9169, train_loss=5.0167294

Batch 252660, train_perplexity=159.53537, train_loss=5.0722656

Batch 252670, train_perplexity=150.29771, train_loss=5.012618

Batch 252680, train_perplexity=151.23949, train_loss=5.0188646

Batch 252690, train_perplexity=162.94168, train_loss=5.0933924

Batch 252700, train_perplexity=161.37921, train_loss=5.083757

Batch 252710, train_perplexity=165.97108, train_loss=5.1118135

Batch 252720, train_perplexity=171.03687, train_loss=5.141879

Batch 252730, train_perplexity=162.53313, train_loss=5.090882

Batch 252740, train_perplexity=181.00618, train_loss=5.198531

Batch 252750, train_perplexity=158.89203, train_loss=5.068225

Batch 252760, train_perplexity=163.85687, train_loss=5.0989933

Batch 252770, train_perplexity=165.76797, train_loss=5.110589

Batch 252780, train_perplexity=165.48112, train_loss=5.108857

Batch 252790, train_perplexity=180.63887, train_loss=5.1965

Batch 252800, train_perplexity=154.35927, train_loss=5.039283

Batch 252810, train_perplexity=167.80391, train_loss=5.122796

Batch 252820, train_perplexity=166.99753, train_loss=5.117979

Batch 252830, train_perplexity=179.33528, train_loss=5.189257

Batch 252840, train_perplexity=169.5883, train_loss=5.1333737

Batch 252850, train_perplexity=182.6537, train_loss=5.207592

Batch 252860, train_perplexity=148.73778, train_loss=5.002185

Batch 252870, train_perplexity=176.75938, train_loss=5.1747894

Batch 252880, train_perplexity=164.7618, train_loss=5.104501

Batch 252890, train_perplexity=161.76497, train_loss=5.0861444

Batch 252900, train_perplexity=160.17505, train_loss=5.0762672

Batch 252910, train_perplexity=173.16162, train_loss=5.1542253

Batch 252920, train_perplexity=165.45438, train_loss=5.1086955

Batch 252930, train_perplexity=161.00789, train_loss=5.0814533

Batch 252940, train_perplexity=178.8892, train_loss=5.1867666

Batch 252950, train_perplexity=169.82095, train_loss=5.1347446

Batch 252960, train_perplexity=170.48512, train_loss=5.138648

Batch 252970, train_perplexity=159.64815, train_loss=5.0729723

Batch 252980, train_perplexity=168.07057, train_loss=5.124384

Batch 252990, train_perplexity=172.22041, train_loss=5.148775

Batch 253000, train_perplexity=171.0473, train_loss=5.14194

Batch 253010, train_perplexity=157.41649, train_loss=5.058895

Batch 253020, train_perplexity=162.0582, train_loss=5.0879555

Batch 253030, train_perplexity=173.42009, train_loss=5.155717

Batch 253040, train_perplexity=150.72173, train_loss=5.015435

Batch 253050, train_perplexity=156.18045, train_loss=5.051012

Batch 253060, train_perplexity=154.2954, train_loss=5.038869

Batch 253070, train_perplexity=169.9209, train_loss=5.135333

Batch 253080, train_perplexity=159.55598, train_loss=5.072395

Batch 253090, train_perplexity=175.84239, train_loss=5.169588

Batch 253100, train_perplexity=167.54294, train_loss=5.1212397

Batch 253110, train_perplexity=160.70131, train_loss=5.0795474

Batch 253120, train_perplexity=172.54114, train_loss=5.1506357

Batch 253130, train_perplexity=182.44914, train_loss=5.2064714

Batch 253140, train_perplexity=185.72162, train_loss=5.224249

Batch 253150, train_perplexity=163.5632, train_loss=5.0971994

Batch 253160, train_perplexity=184.75319, train_loss=5.219021

Batch 253170, train_perplexity=161.51762, train_loss=5.0846143

Batch 253180, train_perplexity=159.83049, train_loss=5.074114

Batch 253190, train_perplexity=179.42151, train_loss=5.189738

Batch 253200, train_perplexity=180.2288, train_loss=5.194227

Batch 253210, train_perplexity=158.38037, train_loss=5.0649996

Batch 253220, train_perplexity=174.56984, train_loss=5.162325

Batch 253230, train_perplexity=155.63863, train_loss=5.047537

Batch 253240, train_perplexity=152.42525, train_loss=5.0266743

Batch 253250, train_perplexity=160.44351, train_loss=5.077942

Batch 253260, train_perplexity=148.52219, train_loss=5.0007343

Batch 253270, train_perplexity=178.75772, train_loss=5.1860313

Batch 253280, train_perplexity=160.41551, train_loss=5.0777674

Batch 253290, train_perplexity=164.50948, train_loss=5.102968

Batch 253300, train_perplexity=163.6935, train_loss=5.0979958

Batch 253310, train_perplexity=169.98225, train_loss=5.135694

Batch 253320, train_perplexity=164.25075, train_loss=5.101394

Batch 253330, train_perplexity=166.06528, train_loss=5.112381

Batch 253340, train_perplexity=178.29709, train_loss=5.183451

Batch 253350, train_perplexity=185.64424, train_loss=5.223832

Batch 253360, train_perplexity=160.18016, train_loss=5.076299

Batch 253370, train_perplexity=174.27924, train_loss=5.160659

Batch 253380, train_perplexity=154.73776, train_loss=5.041732

Batch 253390, train_perplexity=165.64738, train_loss=5.1098614

Batch 253400, train_perplexity=168.75107, train_loss=5.1284246

Batch 253410, train_perplexity=193.52313, train_loss=5.265397

Batch 253420, train_perplexity=166.98894, train_loss=5.1179276

Batch 253430, train_perplexity=166.77487, train_loss=5.116645

Batch 253440, train_perplexity=166.2778, train_loss=5.11366

Batch 253450, train_perplexity=174.13586, train_loss=5.159836

Batch 253460, train_perplexity=159.75963, train_loss=5.0736704

Batch 253470, train_perplexity=166.9711, train_loss=5.1178207

Batch 253480, train_perplexity=186.64287, train_loss=5.229197

Batch 253490, train_perplexity=175.37347, train_loss=5.166918

Batch 253500, train_perplexity=158.56415, train_loss=5.0661592

Batch 253510, train_perplexity=157.75562, train_loss=5.061047

Batch 253520, train_perplexity=162.04668, train_loss=5.0878844

Batch 253530, train_perplexity=165.80228, train_loss=5.110796

Batch 253540, train_perplexity=153.88687, train_loss=5.0362177

Batch 253550, train_perplexity=155.43512, train_loss=5.0462284

Batch 253560, train_perplexity=165.78046, train_loss=5.1106644

Batch 253570, train_perplexity=150.77275, train_loss=5.015774

Batch 253580, train_perplexity=172.94443, train_loss=5.1529703

Batch 253590, train_perplexity=166.911, train_loss=5.1174607

Batch 253600, train_perplexity=149.18356, train_loss=5.0051775

Batch 253610, train_perplexity=153.32573, train_loss=5.0325646

Batch 253620, train_perplexity=151.16855, train_loss=5.0183954

Batch 253630, train_perplexity=161.7807, train_loss=5.0862417

Batch 253640, train_perplexity=154.90668, train_loss=5.042823

Batch 253650, train_perplexity=171.59972, train_loss=5.1451645

Batch 253660, train_perplexity=164.87025, train_loss=5.105159

Batch 253670, train_perplexity=165.16154, train_loss=5.106924

Batch 253680, train_perplexity=172.37306, train_loss=5.149661

Batch 253690, train_perplexity=166.95645, train_loss=5.117733

Batch 253700, train_perplexity=175.4321, train_loss=5.167252
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 253710, train_perplexity=154.9533, train_loss=5.0431237

Batch 253720, train_perplexity=152.14973, train_loss=5.024865

Batch 253730, train_perplexity=171.38448, train_loss=5.1439095

Batch 253740, train_perplexity=184.68343, train_loss=5.218643

Batch 253750, train_perplexity=166.00789, train_loss=5.1120353

Batch 253760, train_perplexity=156.04875, train_loss=5.0501685

Batch 253770, train_perplexity=153.94095, train_loss=5.036569

Batch 253780, train_perplexity=161.05864, train_loss=5.0817685

Batch 253790, train_perplexity=147.89137, train_loss=4.996478

Batch 253800, train_perplexity=183.73303, train_loss=5.213484

Batch 253810, train_perplexity=169.39644, train_loss=5.1322417

Batch 253820, train_perplexity=181.12921, train_loss=5.1992106

Batch 253830, train_perplexity=144.51425, train_loss=4.973378

Batch 253840, train_perplexity=166.89189, train_loss=5.1173463

Batch 253850, train_perplexity=168.2027, train_loss=5.1251698

Batch 253860, train_perplexity=178.15848, train_loss=5.1826735

Batch 253870, train_perplexity=177.33955, train_loss=5.1780663

Batch 253880, train_perplexity=178.94226, train_loss=5.187063

Batch 253890, train_perplexity=162.75043, train_loss=5.092218

Batch 253900, train_perplexity=189.54582, train_loss=5.244631

Batch 253910, train_perplexity=175.66707, train_loss=5.1685905

Batch 253920, train_perplexity=177.7805, train_loss=5.1805496

Batch 253930, train_perplexity=162.30775, train_loss=5.089494

Batch 253940, train_perplexity=160.1878, train_loss=5.076347

Batch 253950, train_perplexity=154.58864, train_loss=5.0407677

Batch 253960, train_perplexity=162.82983, train_loss=5.0927057

Batch 253970, train_perplexity=154.02782, train_loss=5.037133

Batch 253980, train_perplexity=160.40518, train_loss=5.077703

Batch 253990, train_perplexity=151.10974, train_loss=5.0180063

Batch 254000, train_perplexity=156.32245, train_loss=5.051921

Batch 254010, train_perplexity=157.877, train_loss=5.061816

Batch 254020, train_perplexity=171.54465, train_loss=5.1448436

Batch 254030, train_perplexity=171.5557, train_loss=5.144908

Batch 254040, train_perplexity=149.60686, train_loss=5.008011

Batch 254050, train_perplexity=155.84865, train_loss=5.0488853

Batch 254060, train_perplexity=156.43669, train_loss=5.0526514

Batch 254070, train_perplexity=158.76979, train_loss=5.0674553

Batch 254080, train_perplexity=174.31256, train_loss=5.16085

Batch 254090, train_perplexity=166.85793, train_loss=5.1171427

Batch 254100, train_perplexity=155.75726, train_loss=5.048299

Batch 254110, train_perplexity=153.04868, train_loss=5.030756

Batch 254120, train_perplexity=169.0497, train_loss=5.1301928

Batch 254130, train_perplexity=163.75407, train_loss=5.098366

Batch 254140, train_perplexity=152.0828, train_loss=5.024425

Batch 254150, train_perplexity=150.58911, train_loss=5.014555

Batch 254160, train_perplexity=167.23915, train_loss=5.119425

Batch 254170, train_perplexity=167.21156, train_loss=5.11926

Batch 254180, train_perplexity=168.66869, train_loss=5.1279364

Batch 254190, train_perplexity=158.28789, train_loss=5.0644155

Batch 254200, train_perplexity=160.42331, train_loss=5.077816

Batch 254210, train_perplexity=166.18109, train_loss=5.113078

Batch 254220, train_perplexity=168.8694, train_loss=5.1291256

Batch 254230, train_perplexity=146.35175, train_loss=4.986013

Batch 254240, train_perplexity=172.43767, train_loss=5.150036

Batch 254250, train_perplexity=150.94771, train_loss=5.0169334

Batch 254260, train_perplexity=160.21942, train_loss=5.0765443

Batch 254270, train_perplexity=180.56566, train_loss=5.1960945

Batch 254280, train_perplexity=148.41853, train_loss=5.0000362

Batch 254290, train_perplexity=158.02786, train_loss=5.0627713

Batch 254300, train_perplexity=162.62778, train_loss=5.091464

Batch 254310, train_perplexity=159.58345, train_loss=5.072567

Batch 254320, train_perplexity=149.75574, train_loss=5.0090055

Batch 254330, train_perplexity=145.54446, train_loss=4.9804816

Batch 254340, train_perplexity=172.44572, train_loss=5.1500826

Batch 254350, train_perplexity=155.19176, train_loss=5.0446615

Batch 254360, train_perplexity=164.4077, train_loss=5.1023493

Batch 254370, train_perplexity=178.05562, train_loss=5.182096

Batch 254380, train_perplexity=172.88837, train_loss=5.152646

Batch 254390, train_perplexity=169.96677, train_loss=5.135603

Batch 254400, train_perplexity=181.30937, train_loss=5.200205

Batch 254410, train_perplexity=169.15904, train_loss=5.1308393

Batch 254420, train_perplexity=175.56616, train_loss=5.168016

Batch 254430, train_perplexity=179.89035, train_loss=5.1923475

Batch 254440, train_perplexity=158.64938, train_loss=5.0666966

Batch 254450, train_perplexity=163.26009, train_loss=5.0953445

Batch 254460, train_perplexity=160.23509, train_loss=5.076642

Batch 254470, train_perplexity=167.67073, train_loss=5.122002

Batch 254480, train_perplexity=167.02827, train_loss=5.118163

Batch 254490, train_perplexity=150.7095, train_loss=5.015354

Batch 254500, train_perplexity=170.40092, train_loss=5.138154

Batch 254510, train_perplexity=139.47792, train_loss=4.9379063

Batch 254520, train_perplexity=174.53355, train_loss=5.162117

Batch 254530, train_perplexity=152.94173, train_loss=5.030057

Batch 254540, train_perplexity=142.7703, train_loss=4.961237

Batch 254550, train_perplexity=155.3065, train_loss=5.0454006

Batch 254560, train_perplexity=187.79816, train_loss=5.235368

Batch 254570, train_perplexity=149.54323, train_loss=5.0075855

Batch 254580, train_perplexity=156.23378, train_loss=5.0513535

Batch 254590, train_perplexity=140.42647, train_loss=4.944684

Batch 254600, train_perplexity=174.68268, train_loss=5.162971

Batch 254610, train_perplexity=179.23671, train_loss=5.1887074

Batch 254620, train_perplexity=158.32397, train_loss=5.0646434

Batch 254630, train_perplexity=166.79404, train_loss=5.11676

Batch 254640, train_perplexity=165.12831, train_loss=5.106723

Batch 254650, train_perplexity=181.41757, train_loss=5.2008014

Batch 254660, train_perplexity=169.9184, train_loss=5.1353183

Batch 254670, train_perplexity=146.88959, train_loss=4.9896812

Batch 254680, train_perplexity=164.11491, train_loss=5.100567

Batch 254690, train_perplexity=158.12999, train_loss=5.0634174

Batch 254700, train_perplexity=168.76813, train_loss=5.1285257

Batch 254710, train_perplexity=171.69286, train_loss=5.145707

Batch 254720, train_perplexity=164.29547, train_loss=5.1016665

Batch 254730, train_perplexity=160.94556, train_loss=5.081066

Batch 254740, train_perplexity=169.90648, train_loss=5.135248

Batch 254750, train_perplexity=152.29985, train_loss=5.0258512

Batch 254760, train_perplexity=182.04982, train_loss=5.2042804

Batch 254770, train_perplexity=157.46935, train_loss=5.059231

Batch 254780, train_perplexity=190.2163, train_loss=5.248162

Batch 254790, train_perplexity=175.6855, train_loss=5.1686954

Batch 254800, train_perplexity=158.94698, train_loss=5.0685706

Batch 254810, train_perplexity=165.5572, train_loss=5.109317

Batch 254820, train_perplexity=180.30316, train_loss=5.1946397

Batch 254830, train_perplexity=166.73671, train_loss=5.116416

Batch 254840, train_perplexity=172.97618, train_loss=5.153154

Batch 254850, train_perplexity=163.59642, train_loss=5.0974026

Batch 254860, train_perplexity=171.55864, train_loss=5.144925

Batch 254870, train_perplexity=172.48364, train_loss=5.1503024

Batch 254880, train_perplexity=164.27698, train_loss=5.101554

Batch 254890, train_perplexity=183.66235, train_loss=5.213099

Batch 254900, train_perplexity=179.08105, train_loss=5.1878386

Batch 254910, train_perplexity=190.35866, train_loss=5.24891

Batch 254920, train_perplexity=156.28712, train_loss=5.051695

Batch 254930, train_perplexity=158.72702, train_loss=5.067186

Batch 254940, train_perplexity=158.29959, train_loss=5.0644894

Batch 254950, train_perplexity=164.87048, train_loss=5.10516

Batch 254960, train_perplexity=157.76479, train_loss=5.0611053

Batch 254970, train_perplexity=167.91573, train_loss=5.123462

Batch 254980, train_perplexity=170.38094, train_loss=5.1380367

Batch 254990, train_perplexity=173.52232, train_loss=5.1563063

Batch 255000, train_perplexity=161.4578, train_loss=5.084244
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 255010, train_perplexity=160.466, train_loss=5.078082

Batch 255020, train_perplexity=178.642, train_loss=5.185384

Batch 255030, train_perplexity=168.25627, train_loss=5.1254883

Batch 255040, train_perplexity=167.07512, train_loss=5.1184435

Batch 255050, train_perplexity=162.59041, train_loss=5.091234

Batch 255060, train_perplexity=160.09372, train_loss=5.0757594

Batch 255070, train_perplexity=152.24539, train_loss=5.0254936

Batch 255080, train_perplexity=163.34473, train_loss=5.095863

Batch 255090, train_perplexity=166.38281, train_loss=5.114291

Batch 255100, train_perplexity=164.1387, train_loss=5.100712

Batch 255110, train_perplexity=157.47693, train_loss=5.059279

Batch 255120, train_perplexity=173.02081, train_loss=5.153412

Batch 255130, train_perplexity=155.68607, train_loss=5.0478415

Batch 255140, train_perplexity=170.86014, train_loss=5.1408453

Batch 255150, train_perplexity=183.40007, train_loss=5.21167

Batch 255160, train_perplexity=149.42249, train_loss=5.006778

Batch 255170, train_perplexity=164.93253, train_loss=5.1055365

Batch 255180, train_perplexity=175.09288, train_loss=5.1653166

Batch 255190, train_perplexity=156.92696, train_loss=5.0557804

Batch 255200, train_perplexity=167.51674, train_loss=5.1210833

Batch 255210, train_perplexity=155.82071, train_loss=5.048706

Batch 255220, train_perplexity=153.23065, train_loss=5.0319443

Batch 255230, train_perplexity=170.2268, train_loss=5.1371317

Batch 255240, train_perplexity=168.34584, train_loss=5.1260204

Batch 255250, train_perplexity=159.17694, train_loss=5.0700164

Batch 255260, train_perplexity=165.3163, train_loss=5.1078606

Batch 255270, train_perplexity=148.15634, train_loss=4.998268

Batch 255280, train_perplexity=180.9355, train_loss=5.1981406

Batch 255290, train_perplexity=169.94537, train_loss=5.135477

Batch 255300, train_perplexity=154.03781, train_loss=5.037198

Batch 255310, train_perplexity=163.18552, train_loss=5.0948877

Batch 255320, train_perplexity=164.84023, train_loss=5.1049767

Batch 255330, train_perplexity=161.45071, train_loss=5.0842

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Batch 255340, train_perplexity=163.80672, train_loss=5.098687

Batch 255350, train_perplexity=165.85953, train_loss=5.111141

Batch 255360, train_perplexity=164.51112, train_loss=5.102978

Batch 255370, train_perplexity=151.45398, train_loss=5.020282

Batch 255380, train_perplexity=184.81645, train_loss=5.219363

Batch 255390, train_perplexity=156.96481, train_loss=5.0560217

Batch 255400, train_perplexity=181.21136, train_loss=5.199664

Batch 255410, train_perplexity=167.84616, train_loss=5.123048

Batch 255420, train_perplexity=165.25655, train_loss=5.107499

Batch 255430, train_perplexity=160.33139, train_loss=5.077243

Batch 255440, train_perplexity=145.24704, train_loss=4.978436

Batch 255450, train_perplexity=158.2374, train_loss=5.0640965

Batch 255460, train_perplexity=157.75711, train_loss=5.0610566

Batch 255470, train_perplexity=155.61029, train_loss=5.0473547

Batch 255480, train_perplexity=172.64542, train_loss=5.15124

Batch 255490, train_perplexity=166.9992, train_loss=5.117989

Batch 255500, train_perplexity=147.36462, train_loss=4.99291

Batch 255510, train_perplexity=154.14346, train_loss=5.0378838

Batch 255520, train_perplexity=153.0362, train_loss=5.0306745

Batch 255530, train_perplexity=184.74289, train_loss=5.218965

Batch 255540, train_perplexity=178.49129, train_loss=5.18454

Batch 255550, train_perplexity=152.31683, train_loss=5.025963

Batch 255560, train_perplexity=162.62561, train_loss=5.0914507

Batch 255570, train_perplexity=159.19205, train_loss=5.0701113

Batch 255580, train_perplexity=163.75282, train_loss=5.098358

Batch 255590, train_perplexity=156.10495, train_loss=5.0505285

Batch 255600, train_perplexity=153.75507, train_loss=5.035361

Batch 255610, train_perplexity=168.76363, train_loss=5.128499

Batch 255620, train_perplexity=166.01959, train_loss=5.112106

Batch 255630, train_perplexity=158.37509, train_loss=5.064966

Batch 255640, train_perplexity=162.1921, train_loss=5.0887814

Batch 255650, train_perplexity=153.1649, train_loss=5.031515

Batch 255660, train_perplexity=179.45966, train_loss=5.1899505

Batch 255670, train_perplexity=157.48572, train_loss=5.0593348

Batch 255680, train_perplexity=163.90149, train_loss=5.0992656

Batch 255690, train_perplexity=166.02728, train_loss=5.112152

Batch 255700, train_perplexity=178.03432, train_loss=5.1819763

Batch 255710, train_perplexity=150.40439, train_loss=5.0133276

Batch 255720, train_perplexity=170.54936, train_loss=5.1390247

Batch 255730, train_perplexity=166.36313, train_loss=5.114173

Batch 255740, train_perplexity=171.31323, train_loss=5.1434937

Batch 255750, train_perplexity=163.6217, train_loss=5.097557

Batch 255760, train_perplexity=155.36227, train_loss=5.0457597

Batch 255770, train_perplexity=165.1106, train_loss=5.1066155

Batch 255780, train_perplexity=171.06932, train_loss=5.142069

Batch 255790, train_perplexity=161.05994, train_loss=5.0817766

Batch 255800, train_perplexity=167.8556, train_loss=5.123104

Batch 255810, train_perplexity=159.30663, train_loss=5.070831

Batch 255820, train_perplexity=172.1283, train_loss=5.14824

Batch 255830, train_perplexity=150.43365, train_loss=5.013522

Batch 255840, train_perplexity=157.43181, train_loss=5.0589924

Batch 255850, train_perplexity=172.32219, train_loss=5.149366

Batch 255860, train_perplexity=157.0596, train_loss=5.0566254

Batch 255870, train_perplexity=178.07388, train_loss=5.1821985

Batch 255880, train_perplexity=149.19601, train_loss=5.005261

Batch 255890, train_perplexity=179.73448, train_loss=5.1914806

Batch 255900, train_perplexity=173.75548, train_loss=5.157649

Batch 255910, train_perplexity=161.58148, train_loss=5.0850096

Batch 255920, train_perplexity=139.52715, train_loss=4.938259

Batch 255930, train_perplexity=162.0174, train_loss=5.0877037

Batch 255940, train_perplexity=174.05617, train_loss=5.159378

Batch 255950, train_perplexity=167.46538, train_loss=5.1207767

Batch 255960, train_perplexity=169.48653, train_loss=5.1327734

Batch 255970, train_perplexity=155.14056, train_loss=5.0443316

Batch 255980, train_perplexity=148.96817, train_loss=5.0037327

Batch 255990, train_perplexity=182.1846, train_loss=5.2050204

Batch 256000, train_perplexity=166.35797, train_loss=5.114142

Batch 256010, train_perplexity=156.33766, train_loss=5.052018

Batch 256020, train_perplexity=165.84537, train_loss=5.111056

Batch 256030, train_perplexity=192.03893, train_loss=5.257698

Batch 256040, train_perplexity=154.27325, train_loss=5.0387254

Batch 256050, train_perplexity=142.90146, train_loss=4.9621553

Batch 256060, train_perplexity=137.00105, train_loss=4.9199886

Batch 256070, train_perplexity=156.3604, train_loss=5.0521636

Batch 256080, train_perplexity=165.17258, train_loss=5.106991

Batch 256090, train_perplexity=158.17697, train_loss=5.0637145

Batch 256100, train_perplexity=170.49763, train_loss=5.1387215

Batch 256110, train_perplexity=155.06335, train_loss=5.0438337

Batch 256120, train_perplexity=171.37958, train_loss=5.143881

Batch 256130, train_perplexity=155.08775, train_loss=5.043991

Batch 256140, train_perplexity=170.41116, train_loss=5.138214

Batch 256150, train_perplexity=169.58644, train_loss=5.133363

Batch 256160, train_perplexity=167.60574, train_loss=5.1216145

Batch 256170, train_perplexity=168.95139, train_loss=5.129611

Batch 256180, train_perplexity=147.26839, train_loss=4.9922566

Batch 256190, train_perplexity=151.85191, train_loss=5.022906

Batch 256200, train_perplexity=148.766, train_loss=5.0023746

Batch 256210, train_perplexity=172.89644, train_loss=5.152693

Batch 256220, train_perplexity=155.81773, train_loss=5.048687

Batch 256230, train_perplexity=174.50958, train_loss=5.1619797

Batch 256240, train_perplexity=153.03918, train_loss=5.030694
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 256250, train_perplexity=165.65111, train_loss=5.109884

Batch 256260, train_perplexity=196.79689, train_loss=5.282172

Batch 256270, train_perplexity=133.5919, train_loss=4.8947897

Batch 256280, train_perplexity=165.14311, train_loss=5.1068125

Batch 256290, train_perplexity=165.60742, train_loss=5.10962

Batch 256300, train_perplexity=170.42578, train_loss=5.1383

Batch 256310, train_perplexity=172.58229, train_loss=5.150874

Batch 256320, train_perplexity=180.01202, train_loss=5.1930237

Batch 256330, train_perplexity=157.86728, train_loss=5.0617547

Batch 256340, train_perplexity=178.77187, train_loss=5.1861105

Batch 256350, train_perplexity=183.78159, train_loss=5.213748

Batch 256360, train_perplexity=160.13953, train_loss=5.0760455

Batch 256370, train_perplexity=161.0601, train_loss=5.0817776

Batch 256380, train_perplexity=153.57524, train_loss=5.0341907

Batch 256390, train_perplexity=171.62965, train_loss=5.145339

Batch 256400, train_perplexity=172.79787, train_loss=5.1521225

Batch 256410, train_perplexity=157.90703, train_loss=5.0620065

Batch 256420, train_perplexity=165.39798, train_loss=5.1083546

Batch 256430, train_perplexity=162.16162, train_loss=5.0885935

Batch 256440, train_perplexity=167.54758, train_loss=5.1212673

Batch 256450, train_perplexity=172.60945, train_loss=5.1510315

Batch 256460, train_perplexity=160.09624, train_loss=5.075775

Batch 256470, train_perplexity=158.78932, train_loss=5.0675783

Batch 256480, train_perplexity=155.7878, train_loss=5.048495

Batch 256490, train_perplexity=165.50757, train_loss=5.109017

Batch 256500, train_perplexity=167.63531, train_loss=5.121791

Batch 256510, train_perplexity=170.23468, train_loss=5.137178

Batch 256520, train_perplexity=150.21353, train_loss=5.012058

Batch 256530, train_perplexity=164.79503, train_loss=5.1047025

Batch 256540, train_perplexity=158.32027, train_loss=5.06462

Batch 256550, train_perplexity=174.92757, train_loss=5.164372

Batch 256560, train_perplexity=161.84782, train_loss=5.0866566

Batch 256570, train_perplexity=167.69032, train_loss=5.122119

Batch 256580, train_perplexity=151.01854, train_loss=5.0174026

Batch 256590, train_perplexity=151.2206, train_loss=5.0187397

Batch 256600, train_perplexity=170.6844, train_loss=5.1398163

Batch 256610, train_perplexity=165.07935, train_loss=5.1064262

Batch 256620, train_perplexity=162.5563, train_loss=5.0910244

Batch 256630, train_perplexity=182.39172, train_loss=5.2061567

Batch 256640, train_perplexity=159.57751, train_loss=5.07253

Batch 256650, train_perplexity=154.53786, train_loss=5.040439

Batch 256660, train_perplexity=165.83311, train_loss=5.110982

Batch 256670, train_perplexity=165.19186, train_loss=5.1071076

Batch 256680, train_perplexity=165.1313, train_loss=5.106741

Batch 256690, train_perplexity=154.24927, train_loss=5.03857

Batch 256700, train_perplexity=155.74925, train_loss=5.0482473

Batch 256710, train_perplexity=156.16809, train_loss=5.050933

Batch 256720, train_perplexity=150.52112, train_loss=5.0141034

Batch 256730, train_perplexity=176.22928, train_loss=5.171786

Batch 256740, train_perplexity=162.18103, train_loss=5.088713

Batch 256750, train_perplexity=166.6596, train_loss=5.1159534

Batch 256760, train_perplexity=173.15245, train_loss=5.1541724

Batch 256770, train_perplexity=167.82263, train_loss=5.1229076

Batch 256780, train_perplexity=141.42503, train_loss=4.95177

Batch 256790, train_perplexity=161.09543, train_loss=5.081997

Batch 256800, train_perplexity=149.95988, train_loss=5.010368

Batch 256810, train_perplexity=164.38779, train_loss=5.102228

Batch 256820, train_perplexity=156.44565, train_loss=5.0527086

Batch 256830, train_perplexity=179.7223, train_loss=5.191413

Batch 256840, train_perplexity=168.79726, train_loss=5.1286983

Batch 256850, train_perplexity=154.02649, train_loss=5.0371246

Batch 256860, train_perplexity=153.62637, train_loss=5.0345235

Batch 256870, train_perplexity=169.27031, train_loss=5.131497

Batch 256880, train_perplexity=166.21533, train_loss=5.113284

Batch 256890, train_perplexity=159.94882, train_loss=5.074854

Batch 256900, train_perplexity=157.12822, train_loss=5.057062

Batch 256910, train_perplexity=163.44711, train_loss=5.0964894

Batch 256920, train_perplexity=179.64948, train_loss=5.1910076

Batch 256930, train_perplexity=156.7411, train_loss=5.0545955

Batch 256940, train_perplexity=142.83144, train_loss=4.961665

Batch 256950, train_perplexity=163.17921, train_loss=5.094849

Batch 256960, train_perplexity=152.3244, train_loss=5.0260124

Batch 256970, train_perplexity=156.56729, train_loss=5.053486

Batch 256980, train_perplexity=156.5231, train_loss=5.0532036

Batch 256990, train_perplexity=169.97041, train_loss=5.1356244

Batch 257000, train_perplexity=166.80827, train_loss=5.116845

Batch 257010, train_perplexity=178.09213, train_loss=5.182301

Batch 257020, train_perplexity=164.34923, train_loss=5.1019936

Batch 257030, train_perplexity=159.93219, train_loss=5.07475

Batch 257040, train_perplexity=158.03712, train_loss=5.06283

Batch 257050, train_perplexity=163.70544, train_loss=5.0980687

Batch 257060, train_perplexity=165.74821, train_loss=5.11047

Batch 257070, train_perplexity=163.91563, train_loss=5.099352

Batch 257080, train_perplexity=173.19101, train_loss=5.154395

Batch 257090, train_perplexity=157.43481, train_loss=5.0590115

Batch 257100, train_perplexity=169.78767, train_loss=5.1345487

Batch 257110, train_perplexity=156.06624, train_loss=5.0502806

Batch 257120, train_perplexity=168.06415, train_loss=5.124346

Batch 257130, train_perplexity=170.30475, train_loss=5.1375895

Batch 257140, train_perplexity=165.67946, train_loss=5.110055

Batch 257150, train_perplexity=174.97853, train_loss=5.1646633

Batch 257160, train_perplexity=150.32007, train_loss=5.012767

Batch 257170, train_perplexity=171.6823, train_loss=5.1456456

Batch 257180, train_perplexity=185.1603, train_loss=5.221222

Batch 257190, train_perplexity=162.08601, train_loss=5.088127

Batch 257200, train_perplexity=166.90854, train_loss=5.117446

Batch 257210, train_perplexity=159.67632, train_loss=5.0731487

Batch 257220, train_perplexity=142.31842, train_loss=4.958067

Batch 257230, train_perplexity=142.16005, train_loss=4.9569535

Batch 257240, train_perplexity=156.87996, train_loss=5.055481

Batch 257250, train_perplexity=158.34119, train_loss=5.064752

Batch 257260, train_perplexity=153.54823, train_loss=5.0340147

Batch 257270, train_perplexity=170.51715, train_loss=5.138836

Batch 257280, train_perplexity=171.12251, train_loss=5.1423798

Batch 257290, train_perplexity=189.36443, train_loss=5.2436733

Batch 257300, train_perplexity=155.24564, train_loss=5.0450087

Batch 257310, train_perplexity=202.59593, train_loss=5.3112135

Batch 257320, train_perplexity=167.57538, train_loss=5.1214333

Batch 257330, train_perplexity=185.25374, train_loss=5.2217264

Batch 257340, train_perplexity=164.62059, train_loss=5.1036434

Batch 257350, train_perplexity=149.32755, train_loss=5.006142

Batch 257360, train_perplexity=180.60356, train_loss=5.1963043

Batch 257370, train_perplexity=166.9887, train_loss=5.117926

Batch 257380, train_perplexity=160.99146, train_loss=5.0813513

Batch 257390, train_perplexity=149.27863, train_loss=5.0058146

Batch 257400, train_perplexity=176.87381, train_loss=5.1754365

Batch 257410, train_perplexity=167.03854, train_loss=5.1182246

Batch 257420, train_perplexity=151.48344, train_loss=5.0204763

Batch 257430, train_perplexity=167.93254, train_loss=5.1235623

Batch 257440, train_perplexity=153.43544, train_loss=5.03328

Batch 257450, train_perplexity=165.09116, train_loss=5.106498

Batch 257460, train_perplexity=175.1044, train_loss=5.1653824

Batch 257470, train_perplexity=150.72006, train_loss=5.0154243

Batch 257480, train_perplexity=152.78282, train_loss=5.0290174

Batch 257490, train_perplexity=151.37572, train_loss=5.019765

Batch 257500, train_perplexity=171.65396, train_loss=5.1454806

Batch 257510, train_perplexity=177.77371, train_loss=5.1805115

Batch 257520, train_perplexity=155.82903, train_loss=5.0487595

Batch 257530, train_perplexity=173.07205, train_loss=5.153708

Batch 257540, train_perplexity=160.77013, train_loss=5.0799756
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 257550, train_perplexity=172.44162, train_loss=5.1500587

Batch 257560, train_perplexity=157.49667, train_loss=5.0594044

Batch 257570, train_perplexity=175.37204, train_loss=5.1669097

Batch 257580, train_perplexity=165.70988, train_loss=5.1102386

Batch 257590, train_perplexity=184.55719, train_loss=5.2179594

Batch 257600, train_perplexity=178.7434, train_loss=5.185951

Batch 257610, train_perplexity=160.50342, train_loss=5.0783153

Batch 257620, train_perplexity=152.21098, train_loss=5.0252676

Batch 257630, train_perplexity=164.81923, train_loss=5.1048493

Batch 257640, train_perplexity=158.52884, train_loss=5.0659366

Batch 257650, train_perplexity=169.75359, train_loss=5.134348

Batch 257660, train_perplexity=156.30255, train_loss=5.0517936

Batch 257670, train_perplexity=151.37932, train_loss=5.0197887

Batch 257680, train_perplexity=155.2184, train_loss=5.044833

Batch 257690, train_perplexity=154.32512, train_loss=5.0390615

Batch 257700, train_perplexity=153.0192, train_loss=5.0305634

Batch 257710, train_perplexity=151.3187, train_loss=5.019388

Batch 257720, train_perplexity=166.88783, train_loss=5.117322

Batch 257730, train_perplexity=184.71513, train_loss=5.218815

Batch 257740, train_perplexity=164.07204, train_loss=5.1003056

Batch 257750, train_perplexity=166.69823, train_loss=5.116185

Batch 257760, train_perplexity=188.97601, train_loss=5.24162

Batch 257770, train_perplexity=172.53374, train_loss=5.150593

Batch 257780, train_perplexity=170.83096, train_loss=5.1406746

Batch 257790, train_perplexity=165.86206, train_loss=5.1111565

Batch 257800, train_perplexity=156.26567, train_loss=5.0515575

Batch 257810, train_perplexity=190.3267, train_loss=5.248742

Batch 257820, train_perplexity=158.69107, train_loss=5.0669594

Batch 257830, train_perplexity=149.19778, train_loss=5.005273

Batch 257840, train_perplexity=188.37198, train_loss=5.2384186

Batch 257850, train_perplexity=157.94415, train_loss=5.0622416

Batch 257860, train_perplexity=147.2832, train_loss=4.9923573

Batch 257870, train_perplexity=158.06433, train_loss=5.063002

Batch 257880, train_perplexity=164.8506, train_loss=5.1050396

Batch 257890, train_perplexity=145.85008, train_loss=4.982579

Batch 257900, train_perplexity=148.08742, train_loss=4.9978027

Batch 257910, train_perplexity=157.29756, train_loss=5.0581393

Batch 257920, train_perplexity=153.0637, train_loss=5.030854

Batch 257930, train_perplexity=174.4605, train_loss=5.1616983

Batch 257940, train_perplexity=150.86826, train_loss=5.016407

Batch 257950, train_perplexity=148.8868, train_loss=5.003186

Batch 257960, train_perplexity=166.05594, train_loss=5.1123247

Batch 257970, train_perplexity=158.84915, train_loss=5.067955

Batch 257980, train_perplexity=174.17316, train_loss=5.16005

Batch 257990, train_perplexity=164.2137, train_loss=5.1011686

Batch 258000, train_perplexity=176.04358, train_loss=5.1707315

Batch 258010, train_perplexity=152.77, train_loss=5.0289335

Batch 258020, train_perplexity=182.11302, train_loss=5.2046275

Batch 258030, train_perplexity=152.73926, train_loss=5.0287323

Batch 258040, train_perplexity=180.46866, train_loss=5.195557

Batch 258050, train_perplexity=140.2761, train_loss=4.9436126

Batch 258060, train_perplexity=173.03558, train_loss=5.153497

Batch 258070, train_perplexity=166.25853, train_loss=5.113544

Batch 258080, train_perplexity=156.14783, train_loss=5.050803

Batch 258090, train_perplexity=168.68799, train_loss=5.128051

Batch 258100, train_perplexity=177.98102, train_loss=5.181677

Batch 258110, train_perplexity=153.56741, train_loss=5.0341396

Batch 258120, train_perplexity=157.86902, train_loss=5.0617657

Batch 258130, train_perplexity=154.2962, train_loss=5.038874

Batch 258140, train_perplexity=163.6825, train_loss=5.0979285

Batch 258150, train_perplexity=162.683, train_loss=5.0918036

Batch 258160, train_perplexity=166.2166, train_loss=5.1132917

Batch 258170, train_perplexity=165.4651, train_loss=5.1087604

Batch 258180, train_perplexity=163.62865, train_loss=5.0975995

Batch 258190, train_perplexity=166.96425, train_loss=5.1177797

Batch 258200, train_perplexity=156.41849, train_loss=5.052535

Batch 258210, train_perplexity=160.63872, train_loss=5.079158

Batch 258220, train_perplexity=150.0873, train_loss=5.011217

Batch 258230, train_perplexity=157.15834, train_loss=5.057254

Batch 258240, train_perplexity=172.24045, train_loss=5.1488914

Batch 258250, train_perplexity=163.44353, train_loss=5.0964675

Batch 258260, train_perplexity=161.88303, train_loss=5.086874

Batch 258270, train_perplexity=173.65294, train_loss=5.1570587

Batch 258280, train_perplexity=164.09973, train_loss=5.1004744

Batch 258290, train_perplexity=138.05121, train_loss=4.9276247

Batch 258300, train_perplexity=154.8027, train_loss=5.0421515

Batch 258310, train_perplexity=171.29192, train_loss=5.143369

Batch 258320, train_perplexity=147.74773, train_loss=4.9955063

Batch 258330, train_perplexity=155.8265, train_loss=5.0487432

Batch 258340, train_perplexity=158.16513, train_loss=5.0636396

Batch 258350, train_perplexity=182.02196, train_loss=5.2041273

Batch 258360, train_perplexity=170.59694, train_loss=5.1393037

Batch 258370, train_perplexity=157.88881, train_loss=5.061891

Batch 258380, train_perplexity=145.64006, train_loss=4.981138

Batch 258390, train_perplexity=184.26393, train_loss=5.216369

Batch 258400, train_perplexity=171.36618, train_loss=5.1438026

Batch 258410, train_perplexity=177.76999, train_loss=5.1804905

Batch 258420, train_perplexity=156.92995, train_loss=5.0557995

Batch 258430, train_perplexity=169.72437, train_loss=5.134176

Batch 258440, train_perplexity=152.60393, train_loss=5.027846

Batch 258450, train_perplexity=164.89651, train_loss=5.105318

Batch 258460, train_perplexity=157.63289, train_loss=5.060269

Batch 258470, train_perplexity=177.00154, train_loss=5.1761584

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Batch 258480, train_perplexity=154.16905, train_loss=5.0380497

Batch 258490, train_perplexity=167.19067, train_loss=5.119135

Batch 258500, train_perplexity=158.48312, train_loss=5.065648

Batch 258510, train_perplexity=150.31741, train_loss=5.012749

Batch 258520, train_perplexity=170.45554, train_loss=5.1384745

Batch 258530, train_perplexity=161.78394, train_loss=5.0862617

Batch 258540, train_perplexity=175.05899, train_loss=5.165123

Batch 258550, train_perplexity=171.55823, train_loss=5.1449227

Batch 258560, train_perplexity=144.71423, train_loss=4.974761

Batch 258570, train_perplexity=172.26575, train_loss=5.1490383

Batch 258580, train_perplexity=146.80844, train_loss=4.9891286

Batch 258590, train_perplexity=149.36578, train_loss=5.006398

Batch 258600, train_perplexity=176.2381, train_loss=5.171836

Batch 258610, train_perplexity=167.03545, train_loss=5.118206

Batch 258620, train_perplexity=155.97473, train_loss=5.049694

Batch 258630, train_perplexity=153.73409, train_loss=5.0352244

Batch 258640, train_perplexity=167.16882, train_loss=5.1190042

Batch 258650, train_perplexity=168.74414, train_loss=5.1283836

Batch 258660, train_perplexity=174.61346, train_loss=5.162575

Batch 258670, train_perplexity=172.4264, train_loss=5.1499705

Batch 258680, train_perplexity=160.50809, train_loss=5.0783443

Batch 258690, train_perplexity=168.11081, train_loss=5.1246233

Batch 258700, train_perplexity=155.09914, train_loss=5.0440645

Batch 258710, train_perplexity=162.55266, train_loss=5.091002

Batch 258720, train_perplexity=162.72816, train_loss=5.092081

Batch 258730, train_perplexity=175.44339, train_loss=5.1673164

Batch 258740, train_perplexity=165.17651, train_loss=5.1070147

Batch 258750, train_perplexity=175.44757, train_loss=5.1673403

Batch 258760, train_perplexity=154.37457, train_loss=5.039382

Batch 258770, train_perplexity=179.23808, train_loss=5.188715

Batch 258780, train_perplexity=163.24141, train_loss=5.09523
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 258790, train_perplexity=163.10321, train_loss=5.0943832

Batch 258800, train_perplexity=157.17738, train_loss=5.057375

Batch 258810, train_perplexity=166.10085, train_loss=5.112595

Batch 258820, train_perplexity=159.79353, train_loss=5.0738826

Batch 258830, train_perplexity=155.78075, train_loss=5.0484495

Batch 258840, train_perplexity=159.34348, train_loss=5.071062

Batch 258850, train_perplexity=149.56107, train_loss=5.0077047

Batch 258860, train_perplexity=158.76192, train_loss=5.0674057

Batch 258870, train_perplexity=165.3874, train_loss=5.1082907

Batch 258880, train_perplexity=158.14357, train_loss=5.0635033

Batch 258890, train_perplexity=161.95413, train_loss=5.087313

Batch 258900, train_perplexity=166.6186, train_loss=5.1157074

Batch 258910, train_perplexity=157.82236, train_loss=5.06147

Batch 258920, train_perplexity=165.304, train_loss=5.107786

Batch 258930, train_perplexity=157.55663, train_loss=5.059785

Batch 258940, train_perplexity=154.41153, train_loss=5.0396214

Batch 258950, train_perplexity=161.88063, train_loss=5.086859

Batch 258960, train_perplexity=155.53195, train_loss=5.046851

Batch 258970, train_perplexity=162.05154, train_loss=5.0879145

Batch 258980, train_perplexity=161.16173, train_loss=5.0824084

Batch 258990, train_perplexity=167.98468, train_loss=5.1238728

Batch 259000, train_perplexity=158.49068, train_loss=5.065696

Batch 259010, train_perplexity=135.04797, train_loss=4.90563

Batch 259020, train_perplexity=153.00533, train_loss=5.0304728

Batch 259030, train_perplexity=152.61702, train_loss=5.0279317

Batch 259040, train_perplexity=156.42043, train_loss=5.0525475

Batch 259050, train_perplexity=175.21616, train_loss=5.1660204

Batch 259060, train_perplexity=164.55075, train_loss=5.103219

Batch 259070, train_perplexity=138.81522, train_loss=4.9331436

Batch 259080, train_perplexity=151.47594, train_loss=5.0204268

Batch 259090, train_perplexity=135.65065, train_loss=4.910083

Batch 259100, train_perplexity=160.5753, train_loss=5.078763

Batch 259110, train_perplexity=173.94664, train_loss=5.1587486

Batch 259120, train_perplexity=165.59227, train_loss=5.1095285

Batch 259130, train_perplexity=186.45847, train_loss=5.2282085

Batch 259140, train_perplexity=166.21516, train_loss=5.113283

Batch 259150, train_perplexity=157.25864, train_loss=5.057892

Batch 259160, train_perplexity=145.4359, train_loss=4.9797354

Batch 259170, train_perplexity=174.31198, train_loss=5.1608467

Batch 259180, train_perplexity=163.66072, train_loss=5.0977955

Batch 259190, train_perplexity=169.66959, train_loss=5.133853

Batch 259200, train_perplexity=150.83301, train_loss=5.0161734

Batch 259210, train_perplexity=153.02809, train_loss=5.0306215

Batch 259220, train_perplexity=176.77321, train_loss=5.1748676

Batch 259230, train_perplexity=150.81834, train_loss=5.016076

Batch 259240, train_perplexity=157.04103, train_loss=5.056507

Batch 259250, train_perplexity=156.35257, train_loss=5.0521135

Batch 259260, train_perplexity=155.22935, train_loss=5.0449038

Batch 259270, train_perplexity=165.29036, train_loss=5.1077037

Batch 259280, train_perplexity=148.39575, train_loss=4.9998827

Batch 259290, train_perplexity=164.39334, train_loss=5.102262

Batch 259300, train_perplexity=165.09312, train_loss=5.1065097

Batch 259310, train_perplexity=156.96857, train_loss=5.0560455

Batch 259320, train_perplexity=160.91025, train_loss=5.080847

Batch 259330, train_perplexity=162.30241, train_loss=5.0894613

Batch 259340, train_perplexity=144.64098, train_loss=4.9742546

Batch 259350, train_perplexity=171.37598, train_loss=5.14386

Batch 259360, train_perplexity=149.95853, train_loss=5.010359

Batch 259370, train_perplexity=160.1277, train_loss=5.0759716

Batch 259380, train_perplexity=155.20627, train_loss=5.044755

Batch 259390, train_perplexity=173.00777, train_loss=5.1533365

Batch 259400, train_perplexity=175.74968, train_loss=5.1690607

Batch 259410, train_perplexity=157.60329, train_loss=5.060081

Batch 259420, train_perplexity=157.4815, train_loss=5.059308

Batch 259430, train_perplexity=161.29243, train_loss=5.083219

Batch 259440, train_perplexity=168.9559, train_loss=5.1296377

Batch 259450, train_perplexity=157.38573, train_loss=5.0586996

Batch 259460, train_perplexity=169.26562, train_loss=5.1314692

Batch 259470, train_perplexity=155.06061, train_loss=5.043816

Batch 259480, train_perplexity=136.39433, train_loss=4.91555

Batch 259490, train_perplexity=184.07758, train_loss=5.2153573

Batch 259500, train_perplexity=160.30586, train_loss=5.0770836

Batch 259510, train_perplexity=164.88873, train_loss=5.105271

Batch 259520, train_perplexity=151.3975, train_loss=5.019909

Batch 259530, train_perplexity=163.22902, train_loss=5.0951543

Batch 259540, train_perplexity=192.9502, train_loss=5.262432

Batch 259550, train_perplexity=156.93405, train_loss=5.0558257

Batch 259560, train_perplexity=173.15938, train_loss=5.1542125

Batch 259570, train_perplexity=151.68785, train_loss=5.021825

Batch 259580, train_perplexity=155.41422, train_loss=5.046094

Batch 259590, train_perplexity=173.32849, train_loss=5.1551886

Batch 259600, train_perplexity=171.29543, train_loss=5.1433897

Batch 259610, train_perplexity=142.8253, train_loss=4.961622

Batch 259620, train_perplexity=166.15416, train_loss=5.112916

Batch 259630, train_perplexity=144.3361, train_loss=4.9721446

Batch 259640, train_perplexity=170.44774, train_loss=5.1384287

Batch 259650, train_perplexity=155.46848, train_loss=5.046443

Batch 259660, train_perplexity=165.47433, train_loss=5.108816

Batch 259670, train_perplexity=156.42073, train_loss=5.0525494

Batch 259680, train_perplexity=148.97726, train_loss=5.0037937

Batch 259690, train_perplexity=151.60751, train_loss=5.021295

Batch 259700, train_perplexity=163.55812, train_loss=5.0971684

Batch 259710, train_perplexity=152.97061, train_loss=5.030246

Batch 259720, train_perplexity=180.82985, train_loss=5.1975565

Batch 259730, train_perplexity=136.489, train_loss=4.916244

Batch 259740, train_perplexity=183.36508, train_loss=5.211479

Batch 259750, train_perplexity=153.54398, train_loss=5.033987

Batch 259760, train_perplexity=160.87236, train_loss=5.080611

Batch 259770, train_perplexity=148.20114, train_loss=4.9985704

Batch 259780, train_perplexity=172.90988, train_loss=5.1527705

Batch 259790, train_perplexity=167.40128, train_loss=5.1203938

Batch 259800, train_perplexity=165.4319, train_loss=5.1085596

Batch 259810, train_perplexity=165.04755, train_loss=5.1062336

Batch 259820, train_perplexity=153.90639, train_loss=5.0363445

Batch 259830, train_perplexity=170.04395, train_loss=5.136057

Batch 259840, train_perplexity=195.62813, train_loss=5.2762156

Batch 259850, train_perplexity=151.24988, train_loss=5.0189333

Batch 259860, train_perplexity=162.49074, train_loss=5.090621

Batch 259870, train_perplexity=172.03229, train_loss=5.147682

Batch 259880, train_perplexity=159.25612, train_loss=5.0705137

Batch 259890, train_perplexity=167.18326, train_loss=5.1190906

Batch 259900, train_perplexity=168.09492, train_loss=5.124529

Batch 259910, train_perplexity=152.28648, train_loss=5.0257635

Batch 259920, train_perplexity=159.7831, train_loss=5.0738173

Batch 259930, train_perplexity=170.73618, train_loss=5.1401196

Batch 259940, train_perplexity=157.23262, train_loss=5.0577264

Batch 259950, train_perplexity=165.21873, train_loss=5.1072702

Batch 259960, train_perplexity=169.06946, train_loss=5.1303096

Batch 259970, train_perplexity=156.49817, train_loss=5.0530443

Batch 259980, train_perplexity=147.29956, train_loss=4.9924684

Batch 259990, train_perplexity=153.62022, train_loss=5.0344834

Batch 260000, train_perplexity=164.48831, train_loss=5.1028395

Batch 260010, train_perplexity=177.44495, train_loss=5.1786604

Batch 260020, train_perplexity=158.36603, train_loss=5.064909

Batch 260030, train_perplexity=162.5208, train_loss=5.090806

Batch 260040, train_perplexity=153.9259, train_loss=5.0364714

Batch 260050, train_perplexity=163.76016, train_loss=5.098403

Batch 260060, train_perplexity=141.90553, train_loss=4.9551616

Batch 260070, train_perplexity=165.39388, train_loss=5.10833

Batch 260080, train_perplexity=141.93686, train_loss=4.9553823
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 260090, train_perplexity=159.60385, train_loss=5.072695

Batch 260100, train_perplexity=149.65144, train_loss=5.008309

Batch 260110, train_perplexity=169.76509, train_loss=5.1344156

Batch 260120, train_perplexity=150.67667, train_loss=5.0151362

Batch 260130, train_perplexity=157.36943, train_loss=5.058596

Batch 260140, train_perplexity=175.00299, train_loss=5.164803

Batch 260150, train_perplexity=179.87123, train_loss=5.192241

Batch 260160, train_perplexity=156.12282, train_loss=5.050643

Batch 260170, train_perplexity=155.87534, train_loss=5.0490565

Batch 260180, train_perplexity=171.58752, train_loss=5.1450934

Batch 260190, train_perplexity=125.97055, train_loss=4.836048

Batch 260200, train_perplexity=147.35696, train_loss=4.992858

Batch 260210, train_perplexity=160.4709, train_loss=5.0781126

Batch 260220, train_perplexity=168.6724, train_loss=5.1279583

Batch 260230, train_perplexity=186.72227, train_loss=5.2296224

Batch 260240, train_perplexity=170.02716, train_loss=5.135958

Batch 260250, train_perplexity=168.00502, train_loss=5.123994

Batch 260260, train_perplexity=165.31172, train_loss=5.107833

Batch 260270, train_perplexity=177.2829, train_loss=5.177747

Batch 260280, train_perplexity=164.15091, train_loss=5.100786

Batch 260290, train_perplexity=168.63113, train_loss=5.1277137

Batch 260300, train_perplexity=165.97012, train_loss=5.111808

Batch 260310, train_perplexity=162.48741, train_loss=5.0906005

Batch 260320, train_perplexity=156.25516, train_loss=5.0514903

Batch 260330, train_perplexity=158.94946, train_loss=5.0685863

Batch 260340, train_perplexity=153.95042, train_loss=5.0366306

Batch 260350, train_perplexity=159.57729, train_loss=5.0725284

Batch 260360, train_perplexity=148.18701, train_loss=4.998475

Batch 260370, train_perplexity=182.99736, train_loss=5.2094717

Batch 260380, train_perplexity=157.26959, train_loss=5.0579615

Batch 260390, train_perplexity=145.96237, train_loss=4.983349

Batch 260400, train_perplexity=188.84196, train_loss=5.2409105

Batch 260410, train_perplexity=159.04045, train_loss=5.0691586

Batch 260420, train_perplexity=154.46072, train_loss=5.03994

Batch 260430, train_perplexity=164.43701, train_loss=5.1025276

Batch 260440, train_perplexity=159.44167, train_loss=5.071678

Batch 260450, train_perplexity=159.4654, train_loss=5.071827

Batch 260460, train_perplexity=159.18309, train_loss=5.070055

Batch 260470, train_perplexity=183.43628, train_loss=5.2118673

Batch 260480, train_perplexity=185.99796, train_loss=5.2257357

Batch 260490, train_perplexity=142.18846, train_loss=4.9571533

Batch 260500, train_perplexity=158.82886, train_loss=5.067827

Batch 260510, train_perplexity=170.70793, train_loss=5.139954

Batch 260520, train_perplexity=158.17094, train_loss=5.0636764

Batch 260530, train_perplexity=175.76443, train_loss=5.1691446

Batch 260540, train_perplexity=161.0707, train_loss=5.0818434

Batch 260550, train_perplexity=163.0844, train_loss=5.094268

Batch 260560, train_perplexity=146.48265, train_loss=4.986907

Batch 260570, train_perplexity=148.71196, train_loss=5.0020113

Batch 260580, train_perplexity=150.61453, train_loss=5.014724

Batch 260590, train_perplexity=156.14894, train_loss=5.0508103

Batch 260600, train_perplexity=160.41336, train_loss=5.077754

Batch 260610, train_perplexity=153.5822, train_loss=5.034236

Batch 260620, train_perplexity=153.62015, train_loss=5.034483

Batch 260630, train_perplexity=145.63353, train_loss=4.9810934

Batch 260640, train_perplexity=184.96616, train_loss=5.220173

Batch 260650, train_perplexity=182.6511, train_loss=5.2075777

Batch 260660, train_perplexity=154.08534, train_loss=5.0375066

Batch 260670, train_perplexity=168.01544, train_loss=5.124056

Batch 260680, train_perplexity=175.05014, train_loss=5.1650724

Batch 260690, train_perplexity=160.01656, train_loss=5.0752773

Batch 260700, train_perplexity=149.97519, train_loss=5.01047

Batch 260710, train_perplexity=166.58365, train_loss=5.1154976

Batch 260720, train_perplexity=167.93045, train_loss=5.12355

Batch 260730, train_perplexity=168.87221, train_loss=5.1291423

Batch 260740, train_perplexity=162.62297, train_loss=5.0914345

Batch 260750, train_perplexity=138.94673, train_loss=4.9340906

Batch 260760, train_perplexity=153.80646, train_loss=5.035695

Batch 260770, train_perplexity=166.69537, train_loss=5.116168

Batch 260780, train_perplexity=177.50385, train_loss=5.1789923

Batch 260790, train_perplexity=155.56444, train_loss=5.04706

Batch 260800, train_perplexity=172.22032, train_loss=5.1487746

Batch 260810, train_perplexity=158.02943, train_loss=5.0627813

Batch 260820, train_perplexity=165.55492, train_loss=5.109303

Batch 260830, train_perplexity=154.83873, train_loss=5.042384

Batch 260840, train_perplexity=155.31406, train_loss=5.0454493

Batch 260850, train_perplexity=171.24692, train_loss=5.1431065

Batch 260860, train_perplexity=168.44267, train_loss=5.1265955

Batch 260870, train_perplexity=186.18127, train_loss=5.226721

Batch 260880, train_perplexity=174.01128, train_loss=5.15912

Batch 260890, train_perplexity=155.73045, train_loss=5.0481267

Batch 260900, train_perplexity=181.3488, train_loss=5.2004223

Batch 260910, train_perplexity=166.01793, train_loss=5.112096

Batch 260920, train_perplexity=160.61787, train_loss=5.079028

Batch 260930, train_perplexity=178.53658, train_loss=5.1847935

Batch 260940, train_perplexity=159.27678, train_loss=5.0706434

Batch 260950, train_perplexity=172.17525, train_loss=5.148513

Batch 260960, train_perplexity=177.19264, train_loss=5.1772375

Batch 260970, train_perplexity=156.73326, train_loss=5.0545454

Batch 260980, train_perplexity=161.998, train_loss=5.087584

Batch 260990, train_perplexity=156.95966, train_loss=5.055989

Batch 261000, train_perplexity=179.0797, train_loss=5.187831

Batch 261010, train_perplexity=155.70648, train_loss=5.0479727

Batch 261020, train_perplexity=174.22266, train_loss=5.160334

Batch 261030, train_perplexity=174.15215, train_loss=5.1599293

Batch 261040, train_perplexity=187.21127, train_loss=5.232238

Batch 261050, train_perplexity=156.05382, train_loss=5.050201

Batch 261060, train_perplexity=132.2138, train_loss=4.8844204

Batch 261070, train_perplexity=194.61473, train_loss=5.271022

Batch 261080, train_perplexity=162.37874, train_loss=5.0899315

Batch 261090, train_perplexity=158.96432, train_loss=5.06868

Batch 261100, train_perplexity=148.00961, train_loss=4.9972773

Batch 261110, train_perplexity=152.65567, train_loss=5.028185

Batch 261120, train_perplexity=159.5035, train_loss=5.072066

Batch 261130, train_perplexity=162.88933, train_loss=5.093071

Batch 261140, train_perplexity=156.81093, train_loss=5.055041

Batch 261150, train_perplexity=167.1065, train_loss=5.1186314

Batch 261160, train_perplexity=176.32063, train_loss=5.172304

Batch 261170, train_perplexity=159.133, train_loss=5.0697403

Batch 261180, train_perplexity=169.65729, train_loss=5.1337805

Batch 261190, train_perplexity=154.52644, train_loss=5.040365

Batch 261200, train_perplexity=176.0027, train_loss=5.1704993

Batch 261210, train_perplexity=154.67986, train_loss=5.0413575

Batch 261220, train_perplexity=147.86923, train_loss=4.9963284

Batch 261230, train_perplexity=192.94983, train_loss=5.26243

Batch 261240, train_perplexity=151.77179, train_loss=5.022378

Batch 261250, train_perplexity=176.78342, train_loss=5.1749253

Batch 261260, train_perplexity=153.87645, train_loss=5.03615

Batch 261270, train_perplexity=145.8135, train_loss=4.9823284

Batch 261280, train_perplexity=153.50452, train_loss=5.03373

Batch 261290, train_perplexity=162.25583, train_loss=5.0891743

Batch 261300, train_perplexity=157.08432, train_loss=5.0567827

Batch 261310, train_perplexity=164.50525, train_loss=5.1029425

Batch 261320, train_perplexity=176.6646, train_loss=5.174253

Batch 261330, train_perplexity=162.89546, train_loss=5.0931087

Batch 261340, train_perplexity=153.09853, train_loss=5.0310817

Batch 261350, train_perplexity=171.36528, train_loss=5.1437974

Batch 261360, train_perplexity=160.0915, train_loss=5.0757456

Batch 261370, train_perplexity=181.20393, train_loss=5.199623

Batch 261380, train_perplexity=162.60103, train_loss=5.0912995
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 261390, train_perplexity=156.09416, train_loss=5.0504594

Batch 261400, train_perplexity=155.14116, train_loss=5.0443354

Batch 261410, train_perplexity=183.54004, train_loss=5.212433

Batch 261420, train_perplexity=172.1232, train_loss=5.1482105

Batch 261430, train_perplexity=153.10371, train_loss=5.0311155

Batch 261440, train_perplexity=166.00005, train_loss=5.111988

Batch 261450, train_perplexity=169.0135, train_loss=5.1299787

Batch 261460, train_perplexity=165.87036, train_loss=5.1112065

Batch 261470, train_perplexity=170.77249, train_loss=5.140332

Batch 261480, train_perplexity=168.74849, train_loss=5.1284094

Batch 261490, train_perplexity=170.24141, train_loss=5.1372175

Batch 261500, train_perplexity=175.9427, train_loss=5.1701584

Batch 261510, train_perplexity=165.23174, train_loss=5.107349

Batch 261520, train_perplexity=163.76407, train_loss=5.098427

Batch 261530, train_perplexity=183.52167, train_loss=5.2123327

Batch 261540, train_perplexity=149.7483, train_loss=5.008956

Batch 261550, train_perplexity=139.44067, train_loss=4.937639

Batch 261560, train_perplexity=194.4129, train_loss=5.2699842

Batch 261570, train_perplexity=162.26944, train_loss=5.089258

Batch 261580, train_perplexity=168.36816, train_loss=5.126153

Batch 261590, train_perplexity=150.64154, train_loss=5.014903

Batch 261600, train_perplexity=170.26447, train_loss=5.137353

Batch 261610, train_perplexity=142.1991, train_loss=4.957228

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Batch 261620, train_perplexity=164.47545, train_loss=5.1027613

Batch 261630, train_perplexity=158.8365, train_loss=5.0678754

Batch 261640, train_perplexity=164.84038, train_loss=5.1049776

Batch 261650, train_perplexity=146.98277, train_loss=4.9903154

Batch 261660, train_perplexity=154.07071, train_loss=5.0374117

Batch 261670, train_perplexity=156.32707, train_loss=5.0519505

Batch 261680, train_perplexity=143.5274, train_loss=4.966526

Batch 261690, train_perplexity=162.25104, train_loss=5.0891447

Batch 261700, train_perplexity=171.77441, train_loss=5.146182

Batch 261710, train_perplexity=142.13965, train_loss=4.95681

Batch 261720, train_perplexity=157.212, train_loss=5.0575953

Batch 261730, train_perplexity=188.14503, train_loss=5.237213

Batch 261740, train_perplexity=164.7951, train_loss=5.104703

Batch 261750, train_perplexity=152.82814, train_loss=5.029314

Batch 261760, train_perplexity=180.0699, train_loss=5.193345

Batch 261770, train_perplexity=153.4007, train_loss=5.0330534

Batch 261780, train_perplexity=195.7106, train_loss=5.276637

Batch 261790, train_perplexity=143.54001, train_loss=4.966614

Batch 261800, train_perplexity=175.11041, train_loss=5.1654167

Batch 261810, train_perplexity=163.62404, train_loss=5.0975714

Batch 261820, train_perplexity=182.8932, train_loss=5.2089024

Batch 261830, train_perplexity=152.83601, train_loss=5.0293655

Batch 261840, train_perplexity=155.6612, train_loss=5.047682

Batch 261850, train_perplexity=168.90933, train_loss=5.129362

Batch 261860, train_perplexity=169.27927, train_loss=5.13155

Batch 261870, train_perplexity=178.80247, train_loss=5.1862817

Batch 261880, train_perplexity=172.21007, train_loss=5.148715

Batch 261890, train_perplexity=157.17407, train_loss=5.057354

Batch 261900, train_perplexity=175.99959, train_loss=5.1704817

Batch 261910, train_perplexity=153.61194, train_loss=5.0344296

Batch 261920, train_perplexity=151.36877, train_loss=5.019719

Batch 261930, train_perplexity=168.68011, train_loss=5.128004

Batch 261940, train_perplexity=166.5036, train_loss=5.115017

Batch 261950, train_perplexity=163.25276, train_loss=5.0952997

Batch 261960, train_perplexity=153.82098, train_loss=5.0357895

Batch 261970, train_perplexity=160.79007, train_loss=5.0800996

Batch 261980, train_perplexity=168.93109, train_loss=5.129491

Batch 261990, train_perplexity=152.68486, train_loss=5.028376

Batch 262000, train_perplexity=171.92978, train_loss=5.147086

Batch 262010, train_perplexity=168.4552, train_loss=5.12667

Batch 262020, train_perplexity=160.5462, train_loss=5.078582

Batch 262030, train_perplexity=147.35359, train_loss=4.992835

Batch 262040, train_perplexity=157.46957, train_loss=5.059232

Batch 262050, train_perplexity=175.07184, train_loss=5.1651964

Batch 262060, train_perplexity=176.6383, train_loss=5.174104

Batch 262070, train_perplexity=160.526, train_loss=5.078456

Batch 262080, train_perplexity=161.14091, train_loss=5.082279

Batch 262090, train_perplexity=154.61519, train_loss=5.0409393

Batch 262100, train_perplexity=157.52718, train_loss=5.059598

Batch 262110, train_perplexity=166.36615, train_loss=5.114191

Batch 262120, train_perplexity=171.90387, train_loss=5.1469355

Batch 262130, train_perplexity=159.02347, train_loss=5.0690517

Batch 262140, train_perplexity=170.74025, train_loss=5.1401434

Batch 262150, train_perplexity=162.3689, train_loss=5.089871

Batch 262160, train_perplexity=177.22948, train_loss=5.1774454

Batch 262170, train_perplexity=168.77312, train_loss=5.1285553

Batch 262180, train_perplexity=178.2754, train_loss=5.1833296

Batch 262190, train_perplexity=164.85397, train_loss=5.10506

Batch 262200, train_perplexity=150.12474, train_loss=5.0114665

Batch 262210, train_perplexity=159.3639, train_loss=5.0711904

Batch 262220, train_perplexity=154.2261, train_loss=5.0384197

Batch 262230, train_perplexity=149.31337, train_loss=5.0060472

Batch 262240, train_perplexity=187.005, train_loss=5.2311354

Batch 262250, train_perplexity=164.14833, train_loss=5.1007705

Batch 262260, train_perplexity=168.7143, train_loss=5.1282067

Batch 262270, train_perplexity=155.37991, train_loss=5.045873

Batch 262280, train_perplexity=181.16212, train_loss=5.1993923

Batch 262290, train_perplexity=153.33078, train_loss=5.0325975

Batch 262300, train_perplexity=171.11827, train_loss=5.142355

Batch 262310, train_perplexity=173.99037, train_loss=5.159

Batch 262320, train_perplexity=142.52582, train_loss=4.959523

Batch 262330, train_perplexity=164.55719, train_loss=5.103258

Batch 262340, train_perplexity=152.60735, train_loss=5.0278683

Batch 262350, train_perplexity=151.43816, train_loss=5.0201774

Batch 262360, train_perplexity=160.9692, train_loss=5.081213

Batch 262370, train_perplexity=160.84935, train_loss=5.080468

Batch 262380, train_perplexity=169.32545, train_loss=5.1318226

Batch 262390, train_perplexity=162.48329, train_loss=5.090575

Batch 262400, train_perplexity=183.61111, train_loss=5.21282

Batch 262410, train_perplexity=151.67006, train_loss=5.0217075

Batch 262420, train_perplexity=171.84224, train_loss=5.146577

Batch 262430, train_perplexity=170.02692, train_loss=5.135957

Batch 262440, train_perplexity=159.26971, train_loss=5.070599

Batch 262450, train_perplexity=166.91624, train_loss=5.117492

Batch 262460, train_perplexity=161.2559, train_loss=5.0829926

Batch 262470, train_perplexity=179.91626, train_loss=5.1924915

Batch 262480, train_perplexity=167.5121, train_loss=5.1210556

Batch 262490, train_perplexity=164.3602, train_loss=5.1020603

Batch 262500, train_perplexity=144.55376, train_loss=4.9736514

Batch 262510, train_perplexity=164.19757, train_loss=5.1010704

Batch 262520, train_perplexity=159.6104, train_loss=5.072736

Batch 262530, train_perplexity=163.16133, train_loss=5.0947394

Batch 262540, train_perplexity=160.97134, train_loss=5.0812263

Batch 262550, train_perplexity=142.30281, train_loss=4.9579573

Batch 262560, train_perplexity=163.68913, train_loss=5.097969

Batch 262570, train_perplexity=189.1278, train_loss=5.242423

Batch 262580, train_perplexity=168.97894, train_loss=5.129774

Batch 262590, train_perplexity=162.62352, train_loss=5.091438

Batch 262600, train_perplexity=153.07977, train_loss=5.030959

Batch 262610, train_perplexity=163.97849, train_loss=5.0997353

Batch 262620, train_perplexity=155.48183, train_loss=5.046529
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 262630, train_perplexity=161.25928, train_loss=5.0830135

Batch 262640, train_perplexity=164.43419, train_loss=5.1025105

Batch 262650, train_perplexity=175.05774, train_loss=5.165116

Batch 262660, train_perplexity=163.46176, train_loss=5.096579

Batch 262670, train_perplexity=171.59848, train_loss=5.1451573

Batch 262680, train_perplexity=169.1463, train_loss=5.130764

Batch 262690, train_perplexity=159.18916, train_loss=5.070093

Batch 262700, train_perplexity=154.3157, train_loss=5.0390005

Batch 262710, train_perplexity=163.66618, train_loss=5.097829

Batch 262720, train_perplexity=159.25446, train_loss=5.070503

Batch 262730, train_perplexity=159.8202, train_loss=5.0740495

Batch 262740, train_perplexity=159.93593, train_loss=5.0747733

Batch 262750, train_perplexity=160.15764, train_loss=5.0761585

Batch 262760, train_perplexity=158.83908, train_loss=5.0678916

Batch 262770, train_perplexity=181.68282, train_loss=5.2022624

Batch 262780, train_perplexity=168.03403, train_loss=5.1241665

Batch 262790, train_perplexity=180.63387, train_loss=5.196472

Batch 262800, train_perplexity=148.8007, train_loss=5.002608

Batch 262810, train_perplexity=162.59831, train_loss=5.091283

Batch 262820, train_perplexity=149.55101, train_loss=5.0076375

Batch 262830, train_perplexity=174.1548, train_loss=5.1599445

Batch 262840, train_perplexity=160.30922, train_loss=5.0771046

Batch 262850, train_perplexity=172.86215, train_loss=5.1524944

Batch 262860, train_perplexity=160.43816, train_loss=5.0779085

Batch 262870, train_perplexity=182.43488, train_loss=5.2063932

Batch 262880, train_perplexity=136.97206, train_loss=4.919777

Batch 262890, train_perplexity=149.23051, train_loss=5.005492

Batch 262900, train_perplexity=174.27077, train_loss=5.16061

Batch 262910, train_perplexity=164.65923, train_loss=5.103878

Batch 262920, train_perplexity=159.17738, train_loss=5.0700192

Batch 262930, train_perplexity=155.2073, train_loss=5.0447617

Batch 262940, train_perplexity=179.40619, train_loss=5.1896524

Batch 262950, train_perplexity=165.89386, train_loss=5.111348

Batch 262960, train_perplexity=163.98514, train_loss=5.099776

Batch 262970, train_perplexity=163.72824, train_loss=5.098208

Batch 262980, train_perplexity=153.42139, train_loss=5.0331883

Batch 262990, train_perplexity=179.49013, train_loss=5.19012

Batch 263000, train_perplexity=173.36717, train_loss=5.1554117

Batch 263010, train_perplexity=185.95708, train_loss=5.225516

Batch 263020, train_perplexity=149.29414, train_loss=5.0059185

Batch 263030, train_perplexity=165.39427, train_loss=5.108332

Batch 263040, train_perplexity=168.09317, train_loss=5.1245184

Batch 263050, train_perplexity=164.03026, train_loss=5.100051

Batch 263060, train_perplexity=165.92012, train_loss=5.1115065

Batch 263070, train_perplexity=163.29831, train_loss=5.0955787

Batch 263080, train_perplexity=166.99045, train_loss=5.1179366

Batch 263090, train_perplexity=156.58177, train_loss=5.0535784

Batch 263100, train_perplexity=166.73384, train_loss=5.116399

Batch 263110, train_perplexity=156.09512, train_loss=5.0504656

Batch 263120, train_perplexity=164.06085, train_loss=5.1002374

Batch 263130, train_perplexity=169.3718, train_loss=5.1320963

Batch 263140, train_perplexity=169.77837, train_loss=5.134494

Batch 263150, train_perplexity=151.01422, train_loss=5.017374

Batch 263160, train_perplexity=161.29358, train_loss=5.083226

Batch 263170, train_perplexity=181.46991, train_loss=5.20109

Batch 263180, train_perplexity=174.5795, train_loss=5.16238

Batch 263190, train_perplexity=163.2857, train_loss=5.0955014

Batch 263200, train_perplexity=162.50081, train_loss=5.090683

Batch 263210, train_perplexity=152.25774, train_loss=5.0255747

Batch 263220, train_perplexity=162.50197, train_loss=5.09069

Batch 263230, train_perplexity=172.88118, train_loss=5.1526046

Batch 263240, train_perplexity=162.42163, train_loss=5.0901957

Batch 263250, train_perplexity=163.56725, train_loss=5.097224

Batch 263260, train_perplexity=173.44176, train_loss=5.155842

Batch 263270, train_perplexity=150.17636, train_loss=5.0118103

Batch 263280, train_perplexity=173.70544, train_loss=5.157361

Batch 263290, train_perplexity=160.55998, train_loss=5.0786676

Batch 263300, train_perplexity=165.81113, train_loss=5.1108494

Batch 263310, train_perplexity=180.56998, train_loss=5.1961184

Batch 263320, train_perplexity=180.63129, train_loss=5.196458

Batch 263330, train_perplexity=173.24487, train_loss=5.154706

Batch 263340, train_perplexity=149.30603, train_loss=5.005998

Batch 263350, train_perplexity=162.0687, train_loss=5.0880203

Batch 263360, train_perplexity=145.32788, train_loss=4.9789925

Batch 263370, train_perplexity=153.96614, train_loss=5.0367327

Batch 263380, train_perplexity=164.89722, train_loss=5.1053224

Batch 263390, train_perplexity=169.83455, train_loss=5.1348248

Batch 263400, train_perplexity=171.81128, train_loss=5.1463966

Batch 263410, train_perplexity=172.55406, train_loss=5.1507106

Batch 263420, train_perplexity=167.06985, train_loss=5.118412

Batch 263430, train_perplexity=164.50713, train_loss=5.102954

Batch 263440, train_perplexity=156.2436, train_loss=5.0514164

Batch 263450, train_perplexity=173.10283, train_loss=5.153886

Batch 263460, train_perplexity=168.62735, train_loss=5.1276913

Batch 263470, train_perplexity=164.86632, train_loss=5.105135

Batch 263480, train_perplexity=165.743, train_loss=5.1104383

Batch 263490, train_perplexity=147.8858, train_loss=4.9964404

Batch 263500, train_perplexity=165.57016, train_loss=5.109395

Batch 263510, train_perplexity=147.92833, train_loss=4.996728

Batch 263520, train_perplexity=157.20166, train_loss=5.0575294

Batch 263530, train_perplexity=161.50153, train_loss=5.0845146

Batch 263540, train_perplexity=158.8628, train_loss=5.068041

Batch 263550, train_perplexity=161.29373, train_loss=5.083227

Batch 263560, train_perplexity=174.98938, train_loss=5.1647253

Batch 263570, train_perplexity=176.32921, train_loss=5.172353

Batch 263580, train_perplexity=174.79965, train_loss=5.1636405

Batch 263590, train_perplexity=166.76343, train_loss=5.116576

Batch 263600, train_perplexity=157.2477, train_loss=5.057822

Batch 263610, train_perplexity=158.17818, train_loss=5.063722

Batch 263620, train_perplexity=150.89359, train_loss=5.016575

Batch 263630, train_perplexity=158.92484, train_loss=5.0684314

Batch 263640, train_perplexity=149.39783, train_loss=5.006613

Batch 263650, train_perplexity=164.06546, train_loss=5.1002655

Batch 263660, train_perplexity=170.95296, train_loss=5.1413884

Batch 263670, train_perplexity=161.30936, train_loss=5.083324

Batch 263680, train_perplexity=158.67761, train_loss=5.0668745

Batch 263690, train_perplexity=143.66298, train_loss=4.96747

Batch 263700, train_perplexity=170.47227, train_loss=5.1385727

Batch 263710, train_perplexity=167.58224, train_loss=5.1214743

Batch 263720, train_perplexity=161.94772, train_loss=5.0872736

Batch 263730, train_perplexity=150.12961, train_loss=5.011499

Batch 263740, train_perplexity=168.15257, train_loss=5.1248717

Batch 263750, train_perplexity=172.14594, train_loss=5.1483426

Batch 263760, train_perplexity=152.69281, train_loss=5.028428

Batch 263770, train_perplexity=164.9853, train_loss=5.1058564

Batch 263780, train_perplexity=164.79951, train_loss=5.1047297

Batch 263790, train_perplexity=162.41312, train_loss=5.090143

Batch 263800, train_perplexity=153.33282, train_loss=5.032611

Batch 263810, train_perplexity=162.01631, train_loss=5.087697

Batch 263820, train_perplexity=157.21838, train_loss=5.057636

Batch 263830, train_perplexity=167.1454, train_loss=5.118864

Batch 263840, train_perplexity=163.9065, train_loss=5.099296

Batch 263850, train_perplexity=150.74586, train_loss=5.0155954

Batch 263860, train_perplexity=149.86046, train_loss=5.0097046

Batch 263870, train_perplexity=164.50572, train_loss=5.1029453

Batch 263880, train_perplexity=171.03287, train_loss=5.1418557

Batch 263890, train_perplexity=184.6512, train_loss=5.2184687

Batch 263900, train_perplexity=177.72337, train_loss=5.180228

Batch 263910, train_perplexity=162.85655, train_loss=5.0928698

Batch 263920, train_perplexity=150.19899, train_loss=5.011961
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 263930, train_perplexity=135.93672, train_loss=4.9121895

Batch 263940, train_perplexity=172.84517, train_loss=5.152396

Batch 263950, train_perplexity=145.87985, train_loss=4.9827833

Batch 263960, train_perplexity=171.70612, train_loss=5.1457844

Batch 263970, train_perplexity=168.9464, train_loss=5.1295815

Batch 263980, train_perplexity=171.94699, train_loss=5.1471863

Batch 263990, train_perplexity=186.26785, train_loss=5.2271857

Batch 264000, train_perplexity=161.6123, train_loss=5.0852003

Batch 264010, train_perplexity=172.58467, train_loss=5.150888

Batch 264020, train_perplexity=175.21883, train_loss=5.1660357

Batch 264030, train_perplexity=167.90123, train_loss=5.123376

Batch 264040, train_perplexity=152.15627, train_loss=5.024908

Batch 264050, train_perplexity=166.62941, train_loss=5.1157722

Batch 264060, train_perplexity=150.35355, train_loss=5.0129895

Batch 264070, train_perplexity=195.17567, train_loss=5.2739

Batch 264080, train_perplexity=154.7306, train_loss=5.0416856

Batch 264090, train_perplexity=178.48141, train_loss=5.1844845

Batch 264100, train_perplexity=177.5609, train_loss=5.1793137

Batch 264110, train_perplexity=142.90038, train_loss=4.9621477

Batch 264120, train_perplexity=159.4203, train_loss=5.071544

Batch 264130, train_perplexity=151.76926, train_loss=5.0223613

Batch 264140, train_perplexity=144.70898, train_loss=4.974725

Batch 264150, train_perplexity=159.8215, train_loss=5.0740576

Batch 264160, train_perplexity=169.90356, train_loss=5.135231

Batch 264170, train_perplexity=170.43604, train_loss=5.13836

Batch 264180, train_perplexity=167.07686, train_loss=5.118454

Batch 264190, train_perplexity=175.38669, train_loss=5.166993

Batch 264200, train_perplexity=180.97717, train_loss=5.198371

Batch 264210, train_perplexity=163.0065, train_loss=5.09379

Batch 264220, train_perplexity=163.9699, train_loss=5.099683

Batch 264230, train_perplexity=159.01436, train_loss=5.0689945

Batch 264240, train_perplexity=167.09177, train_loss=5.118543

Batch 264250, train_perplexity=155.7982, train_loss=5.0485616

Batch 264260, train_perplexity=165.56036, train_loss=5.109336

Batch 264270, train_perplexity=190.19461, train_loss=5.248048

Batch 264280, train_perplexity=176.55554, train_loss=5.1736355

Batch 264290, train_perplexity=174.6128, train_loss=5.162571

Batch 264300, train_perplexity=153.05203, train_loss=5.030778

Batch 264310, train_perplexity=169.37793, train_loss=5.1321325

Batch 264320, train_perplexity=163.68507, train_loss=5.0979443

Batch 264330, train_perplexity=148.40637, train_loss=4.999954

Batch 264340, train_perplexity=159.74593, train_loss=5.0735846

Batch 264350, train_perplexity=155.0913, train_loss=5.044014

Batch 264360, train_perplexity=149.47664, train_loss=5.00714

Batch 264370, train_perplexity=161.87755, train_loss=5.08684

Batch 264380, train_perplexity=151.44719, train_loss=5.020237

Batch 264390, train_perplexity=192.50403, train_loss=5.260117

Batch 264400, train_perplexity=171.05643, train_loss=5.1419935

Batch 264410, train_perplexity=161.58534, train_loss=5.0850334

Batch 264420, train_perplexity=169.4458, train_loss=5.132533

Batch 264430, train_perplexity=141.73073, train_loss=4.953929

Batch 264440, train_perplexity=169.26749, train_loss=5.13148

Batch 264450, train_perplexity=182.46045, train_loss=5.2065334

Batch 264460, train_perplexity=161.81511, train_loss=5.0864544

Batch 264470, train_perplexity=166.46661, train_loss=5.1147947

Batch 264480, train_perplexity=152.71494, train_loss=5.028573

Batch 264490, train_perplexity=162.24345, train_loss=5.089098

Batch 264500, train_perplexity=151.1831, train_loss=5.0184917

Batch 264510, train_perplexity=147.79114, train_loss=4.9958

Batch 264520, train_perplexity=160.09296, train_loss=5.0757546

Batch 264530, train_perplexity=163.67648, train_loss=5.097892

Batch 264540, train_perplexity=176.67749, train_loss=5.174326

Batch 264550, train_perplexity=174.00613, train_loss=5.1590905

Batch 264560, train_perplexity=163.68155, train_loss=5.097923

Batch 264570, train_perplexity=159.75887, train_loss=5.0736656

Batch 264580, train_perplexity=158.22353, train_loss=5.0640087

Batch 264590, train_perplexity=163.9108, train_loss=5.0993223

Batch 264600, train_perplexity=168.66298, train_loss=5.1279025

Batch 264610, train_perplexity=155.05359, train_loss=5.043771

Batch 264620, train_perplexity=167.97475, train_loss=5.1238136

Batch 264630, train_perplexity=150.86855, train_loss=5.016409

Batch 264640, train_perplexity=164.80989, train_loss=5.1047926

Batch 264650, train_perplexity=174.99248, train_loss=5.164743

Batch 264660, train_perplexity=169.95274, train_loss=5.1355205

Batch 264670, train_perplexity=165.95367, train_loss=5.1117086

Batch 264680, train_perplexity=167.28366, train_loss=5.119691

Batch 264690, train_perplexity=146.65178, train_loss=4.988061

Batch 264700, train_perplexity=176.68945, train_loss=5.1743937

Batch 264710, train_perplexity=167.51929, train_loss=5.1210985

Batch 264720, train_perplexity=167.68568, train_loss=5.1220913

Batch 264730, train_perplexity=174.08273, train_loss=5.1595306

Batch 264740, train_perplexity=156.5387, train_loss=5.0533032

Batch 264750, train_perplexity=160.44044, train_loss=5.077923

Batch 264760, train_perplexity=147.33167, train_loss=4.9926863

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Batch 264770, train_perplexity=157.20233, train_loss=5.0575337

Batch 264780, train_perplexity=183.96394, train_loss=5.21474

Batch 264790, train_perplexity=165.39671, train_loss=5.108347

Batch 264800, train_perplexity=167.85272, train_loss=5.123087

Batch 264810, train_perplexity=145.82213, train_loss=4.9823875

Batch 264820, train_perplexity=149.39577, train_loss=5.006599

Batch 264830, train_perplexity=169.47763, train_loss=5.132721

Batch 264840, train_perplexity=148.95702, train_loss=5.003658

Batch 264850, train_perplexity=164.02596, train_loss=5.1000247

Batch 264860, train_perplexity=154.16022, train_loss=5.0379925

Batch 264870, train_perplexity=170.94447, train_loss=5.141339

Batch 264880, train_perplexity=152.17172, train_loss=5.0250096

Batch 264890, train_perplexity=165.49501, train_loss=5.108941

Batch 264900, train_perplexity=176.25894, train_loss=5.171954

Batch 264910, train_perplexity=160.67917, train_loss=5.0794096

Batch 264920, train_perplexity=160.01045, train_loss=5.075239

Batch 264930, train_perplexity=169.01158, train_loss=5.129967

Batch 264940, train_perplexity=145.78207, train_loss=4.982113

Batch 264950, train_perplexity=162.99467, train_loss=5.0937176

Batch 264960, train_perplexity=141.22435, train_loss=4.95035

Batch 264970, train_perplexity=169.21414, train_loss=5.131165

Batch 264980, train_perplexity=160.56137, train_loss=5.078676

Batch 264990, train_perplexity=143.79663, train_loss=4.9684

Batch 265000, train_perplexity=143.67662, train_loss=4.967565

Batch 265010, train_perplexity=180.72304, train_loss=5.1969657

Batch 265020, train_perplexity=159.29523, train_loss=5.0707593

Batch 265030, train_perplexity=151.05484, train_loss=5.017643

Batch 265040, train_perplexity=152.70671, train_loss=5.028519

Batch 265050, train_perplexity=144.0246, train_loss=4.969984

Batch 265060, train_perplexity=171.17703, train_loss=5.1426983

Batch 265070, train_perplexity=153.18799, train_loss=5.031666

Batch 265080, train_perplexity=151.97803, train_loss=5.023736

Batch 265090, train_perplexity=164.69267, train_loss=5.104081

Batch 265100, train_perplexity=183.54985, train_loss=5.2124863

Batch 265110, train_perplexity=168.44525, train_loss=5.1266108

Batch 265120, train_perplexity=171.23172, train_loss=5.143018

Batch 265130, train_perplexity=167.32904, train_loss=5.119962

Batch 265140, train_perplexity=159.79567, train_loss=5.073896

Batch 265150, train_perplexity=152.03719, train_loss=5.024125

Batch 265160, train_perplexity=154.99571, train_loss=5.0433974
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 265170, train_perplexity=178.48788, train_loss=5.1845207

Batch 265180, train_perplexity=147.09952, train_loss=4.9911094

Batch 265190, train_perplexity=167.3592, train_loss=5.1201425

Batch 265200, train_perplexity=168.83333, train_loss=5.128912

Batch 265210, train_perplexity=158.59515, train_loss=5.0663548

Batch 265220, train_perplexity=148.57396, train_loss=5.001083

Batch 265230, train_perplexity=170.60124, train_loss=5.139329

Batch 265240, train_perplexity=180.75294, train_loss=5.197131

Batch 265250, train_perplexity=168.30522, train_loss=5.125779

Batch 265260, train_perplexity=146.6171, train_loss=4.9878244

Batch 265270, train_perplexity=144.0147, train_loss=4.9699154

Batch 265280, train_perplexity=158.15926, train_loss=5.0636024

Batch 265290, train_perplexity=157.18068, train_loss=5.057396

Batch 265300, train_perplexity=163.70575, train_loss=5.0980706

Batch 265310, train_perplexity=162.88637, train_loss=5.093053

Batch 265320, train_perplexity=162.00581, train_loss=5.087632

Batch 265330, train_perplexity=162.91014, train_loss=5.093199

Batch 265340, train_perplexity=163.2889, train_loss=5.095521

Batch 265350, train_perplexity=175.46205, train_loss=5.167423

Batch 265360, train_perplexity=162.88094, train_loss=5.0930195

Batch 265370, train_perplexity=159.32935, train_loss=5.0709734

Batch 265380, train_perplexity=170.57896, train_loss=5.1391983

Batch 265390, train_perplexity=141.04373, train_loss=4.94907

Batch 265400, train_perplexity=165.13524, train_loss=5.106765

Batch 265410, train_perplexity=163.00027, train_loss=5.093752

Batch 265420, train_perplexity=170.67717, train_loss=5.139774

Batch 265430, train_perplexity=153.55467, train_loss=5.0340567

Batch 265440, train_perplexity=158.05069, train_loss=5.062916

Batch 265450, train_perplexity=160.18192, train_loss=5.07631

Batch 265460, train_perplexity=156.25583, train_loss=5.0514946

Batch 265470, train_perplexity=171.81734, train_loss=5.146432

Batch 265480, train_perplexity=150.59615, train_loss=5.0146017

Batch 265490, train_perplexity=169.32262, train_loss=5.131806

Batch 265500, train_perplexity=155.01617, train_loss=5.0435295

Batch 265510, train_perplexity=154.78441, train_loss=5.042033

Batch 265520, train_perplexity=185.92204, train_loss=5.2253275

Batch 265530, train_perplexity=147.20415, train_loss=4.9918203

Batch 265540, train_perplexity=171.17686, train_loss=5.1426973

Batch 265550, train_perplexity=159.39409, train_loss=5.0713797

Batch 265560, train_perplexity=157.55151, train_loss=5.0597525

Batch 265570, train_perplexity=168.38036, train_loss=5.1262255

Batch 265580, train_perplexity=138.45229, train_loss=4.930526

Batch 265590, train_perplexity=167.05823, train_loss=5.1183424

Batch 265600, train_perplexity=148.43481, train_loss=5.000146

Batch 265610, train_perplexity=155.6842, train_loss=5.0478296

Batch 265620, train_perplexity=146.40012, train_loss=4.9863434

Batch 265630, train_perplexity=170.02919, train_loss=5.13597

Batch 265640, train_perplexity=169.63658, train_loss=5.1336584

Batch 265650, train_perplexity=174.59715, train_loss=5.1624813

Batch 265660, train_perplexity=185.76466, train_loss=5.2244806

Batch 265670, train_perplexity=164.66245, train_loss=5.1038976

Batch 265680, train_perplexity=174.20247, train_loss=5.1602182

Batch 265690, train_perplexity=159.24883, train_loss=5.070468

Batch 265700, train_perplexity=155.51147, train_loss=5.0467196

Batch 265710, train_perplexity=149.74217, train_loss=5.008915

Batch 265720, train_perplexity=154.88976, train_loss=5.0427136

Batch 265730, train_perplexity=151.1834, train_loss=5.0184937

Batch 265740, train_perplexity=156.04526, train_loss=5.050146

Batch 265750, train_perplexity=163.25775, train_loss=5.09533

Batch 265760, train_perplexity=146.03575, train_loss=4.9838514

Batch 265770, train_perplexity=162.23517, train_loss=5.089047

Batch 265780, train_perplexity=155.78088, train_loss=5.0484505

Batch 265790, train_perplexity=190.63916, train_loss=5.2503824

Batch 265800, train_perplexity=180.00336, train_loss=5.1929755

Batch 265810, train_perplexity=141.90059, train_loss=4.955127

Batch 265820, train_perplexity=155.20471, train_loss=5.044745

Batch 265830, train_perplexity=173.52365, train_loss=5.156314

Batch 265840, train_perplexity=157.82596, train_loss=5.061493

Batch 265850, train_perplexity=158.19727, train_loss=5.063843

Batch 265860, train_perplexity=157.80804, train_loss=5.0613794

Batch 265870, train_perplexity=155.18932, train_loss=5.044646

Batch 265880, train_perplexity=168.49152, train_loss=5.1268854

Batch 265890, train_perplexity=152.6449, train_loss=5.0281143

Batch 265900, train_perplexity=172.7252, train_loss=5.151702

Batch 265910, train_perplexity=175.38585, train_loss=5.1669884

Batch 265920, train_perplexity=157.43083, train_loss=5.058986

Batch 265930, train_perplexity=167.47137, train_loss=5.1208124

Batch 265940, train_perplexity=139.75333, train_loss=4.939879

Batch 265950, train_perplexity=169.08759, train_loss=5.130417

Batch 265960, train_perplexity=169.65001, train_loss=5.1337376

Batch 265970, train_perplexity=153.96526, train_loss=5.036727

Batch 265980, train_perplexity=156.46698, train_loss=5.052845

Batch 265990, train_perplexity=136.78069, train_loss=4.918379

Batch 266000, train_perplexity=160.49461, train_loss=5.0782604

Batch 266010, train_perplexity=166.29556, train_loss=5.1137667

Batch 266020, train_perplexity=152.08337, train_loss=5.024429

Batch 266030, train_perplexity=160.5055, train_loss=5.078328

Batch 266040, train_perplexity=164.07281, train_loss=5.1003103

Batch 266050, train_perplexity=173.18704, train_loss=5.154372

Batch 266060, train_perplexity=156.50183, train_loss=5.0530677

Batch 266070, train_perplexity=167.75966, train_loss=5.1225324

Batch 266080, train_perplexity=152.95354, train_loss=5.030134

Batch 266090, train_perplexity=155.47278, train_loss=5.0464706

Batch 266100, train_perplexity=151.06485, train_loss=5.0177093

Batch 266110, train_perplexity=178.14098, train_loss=5.182575

Batch 266120, train_perplexity=180.04482, train_loss=5.193206

Batch 266130, train_perplexity=149.56377, train_loss=5.007723

Batch 266140, train_perplexity=142.19666, train_loss=4.957211

Batch 266150, train_perplexity=175.89488, train_loss=5.1698866

Batch 266160, train_perplexity=171.58752, train_loss=5.1450934

Batch 266170, train_perplexity=150.41084, train_loss=5.0133705

Batch 266180, train_perplexity=172.49097, train_loss=5.150345

Batch 266190, train_perplexity=150.85301, train_loss=5.016306

Batch 266200, train_perplexity=167.838, train_loss=5.122999

Batch 266210, train_perplexity=167.68448, train_loss=5.122084

Batch 266220, train_perplexity=164.37672, train_loss=5.102161

Batch 266230, train_perplexity=161.03015, train_loss=5.0815916

Batch 266240, train_perplexity=166.85394, train_loss=5.117119

Batch 266250, train_perplexity=170.97212, train_loss=5.1415005

Batch 266260, train_perplexity=148.11722, train_loss=4.998004

Batch 266270, train_perplexity=156.35362, train_loss=5.05212

Batch 266280, train_perplexity=161.06439, train_loss=5.0818043

Batch 266290, train_perplexity=176.41635, train_loss=5.172847

Batch 266300, train_perplexity=172.64384, train_loss=5.151231

Batch 266310, train_perplexity=160.46355, train_loss=5.078067

Batch 266320, train_perplexity=171.66608, train_loss=5.145551

Batch 266330, train_perplexity=158.38657, train_loss=5.0650387

Batch 266340, train_perplexity=161.78009, train_loss=5.086238

Batch 266350, train_perplexity=163.24545, train_loss=5.095255

Batch 266360, train_perplexity=164.01149, train_loss=5.0999365

Batch 266370, train_perplexity=154.69157, train_loss=5.0414333

Batch 266380, train_perplexity=165.84544, train_loss=5.1110563

Batch 266390, train_perplexity=156.54414, train_loss=5.053338

Batch 266400, train_perplexity=183.38625, train_loss=5.2115946

Batch 266410, train_perplexity=159.10052, train_loss=5.069536

Batch 266420, train_perplexity=154.63303, train_loss=5.0410547

Batch 266430, train_perplexity=146.04034, train_loss=4.983883

Batch 266440, train_perplexity=167.67545, train_loss=5.1220303

Batch 266450, train_perplexity=174.61313, train_loss=5.162573

Batch 266460, train_perplexity=153.54018, train_loss=5.0339622
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 266470, train_perplexity=149.40553, train_loss=5.0066643

Batch 266480, train_perplexity=184.8191, train_loss=5.2193775

Batch 266490, train_perplexity=162.70969, train_loss=5.0919676

Batch 266500, train_perplexity=173.53424, train_loss=5.156375

Batch 266510, train_perplexity=168.41473, train_loss=5.1264296

Batch 266520, train_perplexity=196.64006, train_loss=5.281375

Batch 266530, train_perplexity=143.79498, train_loss=4.9683886

Batch 266540, train_perplexity=166.68567, train_loss=5.11611

Batch 266550, train_perplexity=171.553, train_loss=5.144892

Batch 266560, train_perplexity=175.48322, train_loss=5.1675434

Batch 266570, train_perplexity=154.17918, train_loss=5.0381155

Batch 266580, train_perplexity=165.87456, train_loss=5.111232

Batch 266590, train_perplexity=137.9729, train_loss=4.9270573

Batch 266600, train_perplexity=157.4767, train_loss=5.0592775

Batch 266610, train_perplexity=159.82272, train_loss=5.074065

Batch 266620, train_perplexity=162.35675, train_loss=5.089796

Batch 266630, train_perplexity=163.76344, train_loss=5.098423

Batch 266640, train_perplexity=169.10977, train_loss=5.130548

Batch 266650, train_perplexity=160.96198, train_loss=5.081168

Batch 266660, train_perplexity=135.00098, train_loss=4.905282

Batch 266670, train_perplexity=147.89914, train_loss=4.9965305

Batch 266680, train_perplexity=168.76877, train_loss=5.1285295

Batch 266690, train_perplexity=183.66472, train_loss=5.213112

Batch 266700, train_perplexity=174.57767, train_loss=5.1623697

Batch 266710, train_perplexity=174.50867, train_loss=5.1619744

Batch 266720, train_perplexity=175.56892, train_loss=5.1680317

Batch 266730, train_perplexity=159.73633, train_loss=5.0735245

Batch 266740, train_perplexity=160.41321, train_loss=5.077753

Batch 266750, train_perplexity=156.91214, train_loss=5.055686

Batch 266760, train_perplexity=193.00366, train_loss=5.262709

Batch 266770, train_perplexity=174.66643, train_loss=5.162878

Batch 266780, train_perplexity=180.17296, train_loss=5.1939173

Batch 266790, train_perplexity=179.8196, train_loss=5.191954

Batch 266800, train_perplexity=157.14531, train_loss=5.057171

Batch 266810, train_perplexity=157.75343, train_loss=5.0610332

Batch 266820, train_perplexity=149.07852, train_loss=5.004473

Batch 266830, train_perplexity=147.15936, train_loss=4.991516

Batch 266840, train_perplexity=148.99048, train_loss=5.0038824

Batch 266850, train_perplexity=148.32861, train_loss=4.99943

Batch 266860, train_perplexity=153.64388, train_loss=5.0346375

Batch 266870, train_perplexity=155.45128, train_loss=5.0463324

Batch 266880, train_perplexity=163.37044, train_loss=5.09602

Batch 266890, train_perplexity=158.02951, train_loss=5.062782

Batch 266900, train_perplexity=170.98842, train_loss=5.141596

Batch 266910, train_perplexity=148.2174, train_loss=4.99868

Batch 266920, train_perplexity=155.63046, train_loss=5.0474844

Batch 266930, train_perplexity=182.30391, train_loss=5.205675

Batch 266940, train_perplexity=170.85931, train_loss=5.1408405

Batch 266950, train_perplexity=159.7751, train_loss=5.073767

Batch 266960, train_perplexity=155.0221, train_loss=5.0435677

Batch 266970, train_perplexity=154.8634, train_loss=5.0425434

Batch 266980, train_perplexity=164.67398, train_loss=5.1039677

Batch 266990, train_perplexity=155.46254, train_loss=5.046405

Batch 267000, train_perplexity=154.73576, train_loss=5.041719

Batch 267010, train_perplexity=168.48428, train_loss=5.1268425

Batch 267020, train_perplexity=174.92398, train_loss=5.1643515

Batch 267030, train_perplexity=155.27689, train_loss=5.04521

Batch 267040, train_perplexity=149.55264, train_loss=5.0076485

Batch 267050, train_perplexity=161.3322, train_loss=5.0834656

Batch 267060, train_perplexity=172.86858, train_loss=5.1525316

Batch 267070, train_perplexity=155.53825, train_loss=5.0468917

Batch 267080, train_perplexity=166.19489, train_loss=5.113161

Batch 267090, train_perplexity=152.08061, train_loss=5.0244107

Batch 267100, train_perplexity=176.85457, train_loss=5.175328

Batch 267110, train_perplexity=153.43025, train_loss=5.033246

Batch 267120, train_perplexity=161.56453, train_loss=5.0849047

Batch 267130, train_perplexity=140.84372, train_loss=4.947651

Batch 267140, train_perplexity=159.01709, train_loss=5.0690117

Batch 267150, train_perplexity=169.1505, train_loss=5.130789

Batch 267160, train_perplexity=163.81758, train_loss=5.0987535

Batch 267170, train_perplexity=151.22406, train_loss=5.0187626

Batch 267180, train_perplexity=161.42715, train_loss=5.084054

Batch 267190, train_perplexity=149.48357, train_loss=5.0071864

Batch 267200, train_perplexity=146.72446, train_loss=4.9885564

Batch 267210, train_perplexity=176.95015, train_loss=5.175868

Batch 267220, train_perplexity=161.13661, train_loss=5.0822525

Batch 267230, train_perplexity=160.83815, train_loss=5.0803986

Batch 267240, train_perplexity=139.89635, train_loss=4.9409018

Batch 267250, train_perplexity=191.81561, train_loss=5.2565346

Batch 267260, train_perplexity=176.06859, train_loss=5.1708736

Batch 267270, train_perplexity=172.27461, train_loss=5.14909

Batch 267280, train_perplexity=148.84023, train_loss=5.0028734

Batch 267290, train_perplexity=171.66992, train_loss=5.1455736

Batch 267300, train_perplexity=166.55728, train_loss=5.1153393

Batch 267310, train_perplexity=161.68045, train_loss=5.085622

Batch 267320, train_perplexity=168.16435, train_loss=5.124942

Batch 267330, train_perplexity=151.77554, train_loss=5.022403

Batch 267340, train_perplexity=156.20354, train_loss=5.05116

Batch 267350, train_perplexity=152.91869, train_loss=5.0299063

Batch 267360, train_perplexity=173.40636, train_loss=5.1556377

Batch 267370, train_perplexity=165.68736, train_loss=5.1101027

Batch 267380, train_perplexity=163.5409, train_loss=5.097063

Batch 267390, train_perplexity=159.965, train_loss=5.074955

Batch 267400, train_perplexity=169.42381, train_loss=5.1324034

Batch 267410, train_perplexity=158.09697, train_loss=5.0632086

Batch 267420, train_perplexity=176.5323, train_loss=5.173504

Batch 267430, train_perplexity=168.52045, train_loss=5.127057

Batch 267440, train_perplexity=129.72508, train_loss=4.8654175

Batch 267450, train_perplexity=141.83025, train_loss=4.954631

Batch 267460, train_perplexity=155.04132, train_loss=5.0436916

Batch 267470, train_perplexity=162.17476, train_loss=5.0886745

Batch 267480, train_perplexity=157.71869, train_loss=5.060813

Batch 267490, train_perplexity=183.92245, train_loss=5.2145143

Batch 267500, train_perplexity=154.34131, train_loss=5.0391665

Batch 267510, train_perplexity=155.96404, train_loss=5.0496254

Batch 267520, train_perplexity=150.48904, train_loss=5.0138903

Batch 267530, train_perplexity=152.87917, train_loss=5.029648

Batch 267540, train_perplexity=158.88672, train_loss=5.0681915

Batch 267550, train_perplexity=168.5423, train_loss=5.127187

Batch 267560, train_perplexity=175.78807, train_loss=5.169279

Batch 267570, train_perplexity=156.69493, train_loss=5.054301

Batch 267580, train_perplexity=157.22078, train_loss=5.057651

Batch 267590, train_perplexity=170.4053, train_loss=5.13818

Batch 267600, train_perplexity=158.09343, train_loss=5.063186

Batch 267610, train_perplexity=149.9398, train_loss=5.010234

Batch 267620, train_perplexity=163.9287, train_loss=5.0994315

Batch 267630, train_perplexity=158.52492, train_loss=5.065912

Batch 267640, train_perplexity=153.11656, train_loss=5.0311995

Batch 267650, train_perplexity=168.21112, train_loss=5.12522

Batch 267660, train_perplexity=157.67657, train_loss=5.060546

Batch 267670, train_perplexity=186.04027, train_loss=5.225963

Batch 267680, train_perplexity=141.71248, train_loss=4.9538

Batch 267690, train_perplexity=142.06145, train_loss=4.9562597

Batch 267700, train_perplexity=165.69298, train_loss=5.1101365

Batch 267710, train_perplexity=169.87619, train_loss=5.13507

Batch 267720, train_perplexity=182.41173, train_loss=5.2062664

Batch 267730, train_perplexity=160.84596, train_loss=5.080447

Batch 267740, train_perplexity=141.07494, train_loss=4.949291

Batch 267750, train_perplexity=151.1171, train_loss=5.018055

Batch 267760, train_perplexity=161.4471, train_loss=5.0841775
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 267770, train_perplexity=176.68565, train_loss=5.174372

Batch 267780, train_perplexity=171.42453, train_loss=5.144143

Batch 267790, train_perplexity=174.73248, train_loss=5.163256

Batch 267800, train_perplexity=156.05359, train_loss=5.0501995

Batch 267810, train_perplexity=161.30081, train_loss=5.083271

Batch 267820, train_perplexity=158.09004, train_loss=5.0631647

Batch 267830, train_perplexity=168.2611, train_loss=5.125517

Batch 267840, train_perplexity=152.86298, train_loss=5.029542

Batch 267850, train_perplexity=149.96353, train_loss=5.010392

Batch 267860, train_perplexity=170.80255, train_loss=5.140508

Batch 267870, train_perplexity=168.03033, train_loss=5.1241446

Batch 267880, train_perplexity=152.83047, train_loss=5.0293293

Batch 267890, train_perplexity=161.95451, train_loss=5.0873156

Batch 267900, train_perplexity=163.03261, train_loss=5.0939503

Batch 267910, train_perplexity=166.76549, train_loss=5.1165886

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Batch 267920, train_perplexity=184.72826, train_loss=5.218886

Batch 267930, train_perplexity=170.13559, train_loss=5.1365957

Batch 267940, train_perplexity=155.89175, train_loss=5.049162

Batch 267950, train_perplexity=151.50418, train_loss=5.020613

Batch 267960, train_perplexity=174.69733, train_loss=5.163055

Batch 267970, train_perplexity=156.5982, train_loss=5.0536833

Batch 267980, train_perplexity=166.80765, train_loss=5.1168413

Batch 267990, train_perplexity=173.07394, train_loss=5.153719

Batch 268000, train_perplexity=157.85847, train_loss=5.061699

Batch 268010, train_perplexity=158.64296, train_loss=5.066656

Batch 268020, train_perplexity=140.7951, train_loss=4.9473057

Batch 268030, train_perplexity=149.06119, train_loss=5.004357

Batch 268040, train_perplexity=154.55827, train_loss=5.040571

Batch 268050, train_perplexity=180.82924, train_loss=5.197553

Batch 268060, train_perplexity=176.56564, train_loss=5.1736927

Batch 268070, train_perplexity=150.88841, train_loss=5.0165405

Batch 268080, train_perplexity=170.91669, train_loss=5.141176

Batch 268090, train_perplexity=187.11507, train_loss=5.231724

Batch 268100, train_perplexity=169.66336, train_loss=5.1338162

Batch 268110, train_perplexity=168.03307, train_loss=5.124161

Batch 268120, train_perplexity=170.75311, train_loss=5.1402187

Batch 268130, train_perplexity=164.67964, train_loss=5.104002

Batch 268140, train_perplexity=154.12083, train_loss=5.037737

Batch 268150, train_perplexity=166.08208, train_loss=5.112482

Batch 268160, train_perplexity=148.48338, train_loss=5.000473

Batch 268170, train_perplexity=163.62015, train_loss=5.0975475

Batch 268180, train_perplexity=155.6309, train_loss=5.0474873

Batch 268190, train_perplexity=177.73303, train_loss=5.1802826

Batch 268200, train_perplexity=169.77058, train_loss=5.134448

Batch 268210, train_perplexity=170.91898, train_loss=5.1411896

Batch 268220, train_perplexity=155.35716, train_loss=5.045727

Batch 268230, train_perplexity=139.05058, train_loss=4.934838

Batch 268240, train_perplexity=153.64754, train_loss=5.0346613

Batch 268250, train_perplexity=174.89845, train_loss=5.1642056

Batch 268260, train_perplexity=163.10555, train_loss=5.0943975

Batch 268270, train_perplexity=158.26895, train_loss=5.064296

Batch 268280, train_perplexity=145.9138, train_loss=4.983016

Batch 268290, train_perplexity=159.68195, train_loss=5.073184

Batch 268300, train_perplexity=157.99516, train_loss=5.0625644

Batch 268310, train_perplexity=147.74506, train_loss=4.995488

Batch 268320, train_perplexity=156.19527, train_loss=5.051107

Batch 268330, train_perplexity=157.66258, train_loss=5.060457

Batch 268340, train_perplexity=170.60417, train_loss=5.139346

Batch 268350, train_perplexity=151.89893, train_loss=5.0232153

Batch 268360, train_perplexity=168.79533, train_loss=5.128687

Batch 268370, train_perplexity=163.20918, train_loss=5.0950327

Batch 268380, train_perplexity=168.5264, train_loss=5.1270924

Batch 268390, train_perplexity=160.9282, train_loss=5.0809584

Batch 268400, train_perplexity=161.91916, train_loss=5.087097

Batch 268410, train_perplexity=173.13222, train_loss=5.1540556

Batch 268420, train_perplexity=164.00616, train_loss=5.099904

Batch 268430, train_perplexity=151.33934, train_loss=5.0195246

Batch 268440, train_perplexity=162.8185, train_loss=5.092636

Batch 268450, train_perplexity=156.35779, train_loss=5.052147

Batch 268460, train_perplexity=164.59218, train_loss=5.103471

Batch 268470, train_perplexity=161.65825, train_loss=5.0854845

Batch 268480, train_perplexity=160.7115, train_loss=5.079611

Batch 268490, train_perplexity=157.47325, train_loss=5.0592556

Batch 268500, train_perplexity=183.8005, train_loss=5.213851

Batch 268510, train_perplexity=156.73924, train_loss=5.0545835

Batch 268520, train_perplexity=145.62048, train_loss=4.9810038

Batch 268530, train_perplexity=164.52698, train_loss=5.1030746

Batch 268540, train_perplexity=159.57675, train_loss=5.072525

Batch 268550, train_perplexity=169.84006, train_loss=5.134857

Batch 268560, train_perplexity=156.40761, train_loss=5.0524654

Batch 268570, train_perplexity=157.84087, train_loss=5.0615873

Batch 268580, train_perplexity=173.45581, train_loss=5.155923

Batch 268590, train_perplexity=144.50159, train_loss=4.9732904

Batch 268600, train_perplexity=161.49976, train_loss=5.0845037

Batch 268610, train_perplexity=168.94397, train_loss=5.129567

Batch 268620, train_perplexity=166.33965, train_loss=5.114032

Batch 268630, train_perplexity=140.5856, train_loss=4.9458165

Batch 268640, train_perplexity=166.35933, train_loss=5.11415

Batch 268650, train_perplexity=147.22752, train_loss=4.991979

Batch 268660, train_perplexity=159.307, train_loss=5.070833

Batch 268670, train_perplexity=174.96819, train_loss=5.164604

Batch 268680, train_perplexity=153.19316, train_loss=5.0316997

Batch 268690, train_perplexity=184.07793, train_loss=5.215359

Batch 268700, train_perplexity=184.30101, train_loss=5.2165704

Batch 268710, train_perplexity=161.49991, train_loss=5.0845046

Batch 268720, train_perplexity=150.49033, train_loss=5.013899

Batch 268730, train_perplexity=151.28688, train_loss=5.019178

Batch 268740, train_perplexity=172.62442, train_loss=5.1511183

Batch 268750, train_perplexity=157.39075, train_loss=5.0587316

Batch 268760, train_perplexity=159.01353, train_loss=5.0689893

Batch 268770, train_perplexity=145.08112, train_loss=4.977293

Batch 268780, train_perplexity=161.30028, train_loss=5.0832677

Batch 268790, train_perplexity=180.7564, train_loss=5.19715

Batch 268800, train_perplexity=184.02817, train_loss=5.215089

Batch 268810, train_perplexity=171.84535, train_loss=5.146595

Batch 268820, train_perplexity=150.42526, train_loss=5.0134664

Batch 268830, train_perplexity=166.71286, train_loss=5.116273

Batch 268840, train_perplexity=163.6008, train_loss=5.0974293

Batch 268850, train_perplexity=153.26529, train_loss=5.0321703

Batch 268860, train_perplexity=163.5161, train_loss=5.0969114

Batch 268870, train_perplexity=149.34918, train_loss=5.006287

Batch 268880, train_perplexity=164.60858, train_loss=5.1035705

Batch 268890, train_perplexity=162.6996, train_loss=5.0919056

Batch 268900, train_perplexity=141.1748, train_loss=4.949999

Batch 268910, train_perplexity=173.64929, train_loss=5.1570377

Batch 268920, train_perplexity=149.08627, train_loss=5.004525

Batch 268930, train_perplexity=172.91614, train_loss=5.1528068

Batch 268940, train_perplexity=174.25955, train_loss=5.160546

Batch 268950, train_perplexity=166.87161, train_loss=5.1172247

Batch 268960, train_perplexity=159.43513, train_loss=5.071637

Batch 268970, train_perplexity=174.11975, train_loss=5.1597433

Batch 268980, train_perplexity=140.34561, train_loss=4.944108

Batch 268990, train_perplexity=161.5197, train_loss=5.084627

Batch 269000, train_perplexity=157.41168, train_loss=5.0588646
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 269010, train_perplexity=143.85725, train_loss=4.9688215

Batch 269020, train_perplexity=156.56056, train_loss=5.053443

Batch 269030, train_perplexity=153.59334, train_loss=5.0343084

Batch 269040, train_perplexity=166.90582, train_loss=5.1174297

Batch 269050, train_perplexity=168.4626, train_loss=5.1267138

Batch 269060, train_perplexity=150.95187, train_loss=5.016961

Batch 269070, train_perplexity=181.36523, train_loss=5.200513

Batch 269080, train_perplexity=157.85388, train_loss=5.06167

Batch 269090, train_perplexity=154.93637, train_loss=5.0430145

Batch 269100, train_perplexity=165.35036, train_loss=5.1080666

Batch 269110, train_perplexity=163.571, train_loss=5.097247

Batch 269120, train_perplexity=149.33025, train_loss=5.0061603

Batch 269130, train_perplexity=192.05028, train_loss=5.257757

Batch 269140, train_perplexity=149.06111, train_loss=5.0043564

Batch 269150, train_perplexity=157.53453, train_loss=5.0596447

Batch 269160, train_perplexity=172.93272, train_loss=5.1529026

Batch 269170, train_perplexity=180.39053, train_loss=5.195124

Batch 269180, train_perplexity=154.4888, train_loss=5.0401216

Batch 269190, train_perplexity=155.23047, train_loss=5.044911

Batch 269200, train_perplexity=172.99193, train_loss=5.153245

Batch 269210, train_perplexity=165.528, train_loss=5.1091404

Batch 269220, train_perplexity=161.69579, train_loss=5.0857167

Batch 269230, train_perplexity=135.91351, train_loss=4.912019

Batch 269240, train_perplexity=171.2773, train_loss=5.143284

Batch 269250, train_perplexity=160.56091, train_loss=5.0786734

Batch 269260, train_perplexity=158.7816, train_loss=5.0675297

Batch 269270, train_perplexity=164.77979, train_loss=5.10461

Batch 269280, train_perplexity=159.75201, train_loss=5.0736227

Batch 269290, train_perplexity=178.92128, train_loss=5.186946

Batch 269300, train_perplexity=166.39487, train_loss=5.1143637

Batch 269310, train_perplexity=174.07709, train_loss=5.159498

Batch 269320, train_perplexity=148.5821, train_loss=5.0011377

Batch 269330, train_perplexity=164.83755, train_loss=5.1049604

Batch 269340, train_perplexity=168.21938, train_loss=5.125269

Batch 269350, train_perplexity=144.59428, train_loss=4.973932

Batch 269360, train_perplexity=167.1438, train_loss=5.1188545

Batch 269370, train_perplexity=169.42511, train_loss=5.132411

Batch 269380, train_perplexity=157.81633, train_loss=5.061432

Batch 269390, train_perplexity=162.55971, train_loss=5.0910454

Batch 269400, train_perplexity=164.00578, train_loss=5.0999017

Batch 269410, train_perplexity=151.27902, train_loss=5.019126

Batch 269420, train_perplexity=147.83455, train_loss=4.9960938

Batch 269430, train_perplexity=153.47562, train_loss=5.0335417

Batch 269440, train_perplexity=144.43262, train_loss=4.972813

Batch 269450, train_perplexity=166.24538, train_loss=5.113465

Batch 269460, train_perplexity=168.32578, train_loss=5.125901

Batch 269470, train_perplexity=170.22388, train_loss=5.1371145

Batch 269480, train_perplexity=151.77916, train_loss=5.0224266

Batch 269490, train_perplexity=167.48798, train_loss=5.1209116

Batch 269500, train_perplexity=168.99231, train_loss=5.1298532

Batch 269510, train_perplexity=176.72702, train_loss=5.1746063

Batch 269520, train_perplexity=171.35596, train_loss=5.143743

Batch 269530, train_perplexity=152.42714, train_loss=5.0266867

Batch 269540, train_perplexity=159.08853, train_loss=5.069461

Batch 269550, train_perplexity=150.45654, train_loss=5.0136743

Batch 269560, train_perplexity=148.91193, train_loss=5.003355

Batch 269570, train_perplexity=146.8653, train_loss=4.989516

Batch 269580, train_perplexity=182.15984, train_loss=5.2048845

Batch 269590, train_perplexity=172.16063, train_loss=5.148428

Batch 269600, train_perplexity=154.664, train_loss=5.041255

Batch 269610, train_perplexity=158.09607, train_loss=5.063203

Batch 269620, train_perplexity=151.01437, train_loss=5.017375

Batch 269630, train_perplexity=156.49667, train_loss=5.053035

Batch 269640, train_perplexity=144.4424, train_loss=4.972881

Batch 269650, train_perplexity=156.60866, train_loss=5.05375

Batch 269660, train_perplexity=147.59943, train_loss=4.994502

Batch 269670, train_perplexity=145.56522, train_loss=4.980624

Batch 269680, train_perplexity=174.51866, train_loss=5.1620317

Batch 269690, train_perplexity=176.20818, train_loss=5.171666

Batch 269700, train_perplexity=162.58731, train_loss=5.091215

Batch 269710, train_perplexity=171.85077, train_loss=5.1466265

Batch 269720, train_perplexity=171.07527, train_loss=5.1421037

Batch 269730, train_perplexity=154.54494, train_loss=5.040485

Batch 269740, train_perplexity=156.9384, train_loss=5.0558534

Batch 269750, train_perplexity=150.32867, train_loss=5.012824

Batch 269760, train_perplexity=164.0484, train_loss=5.1001616

Batch 269770, train_perplexity=158.64076, train_loss=5.0666423

Batch 269780, train_perplexity=150.51645, train_loss=5.0140724

Batch 269790, train_perplexity=139.747, train_loss=4.9398336

Batch 269800, train_perplexity=163.22763, train_loss=5.0951457

Batch 269810, train_perplexity=168.00462, train_loss=5.1239915

Batch 269820, train_perplexity=146.98082, train_loss=4.990302

Batch 269830, train_perplexity=156.55325, train_loss=5.053396

Batch 269840, train_perplexity=167.923, train_loss=5.1235056

Batch 269850, train_perplexity=151.40054, train_loss=5.019929

Batch 269860, train_perplexity=152.36885, train_loss=5.0263042

Batch 269870, train_perplexity=158.52666, train_loss=5.0659227

Batch 269880, train_perplexity=180.91196, train_loss=5.1980104

Batch 269890, train_perplexity=162.51584, train_loss=5.0907755

Batch 269900, train_perplexity=157.70973, train_loss=5.060756

Batch 269910, train_perplexity=154.09717, train_loss=5.0375834

Batch 269920, train_perplexity=172.38867, train_loss=5.1497517

Batch 269930, train_perplexity=157.81227, train_loss=5.061406

Batch 269940, train_perplexity=166.039, train_loss=5.1122227

Batch 269950, train_perplexity=173.93835, train_loss=5.158701

Batch 269960, train_perplexity=156.46474, train_loss=5.0528307

Batch 269970, train_perplexity=179.56195, train_loss=5.1905203

Batch 269980, train_perplexity=179.6368, train_loss=5.190937

Batch 269990, train_perplexity=155.78772, train_loss=5.0484943

Batch 270000, train_perplexity=163.23868, train_loss=5.0952134

Batch 270010, train_perplexity=146.10895, train_loss=4.9843526

Batch 270020, train_perplexity=159.43187, train_loss=5.0716166

Batch 270030, train_perplexity=164.26093, train_loss=5.101456

Batch 270040, train_perplexity=181.97319, train_loss=5.2038593

Batch 270050, train_perplexity=160.80125, train_loss=5.080169

Batch 270060, train_perplexity=158.00728, train_loss=5.062641

Batch 270070, train_perplexity=162.86183, train_loss=5.092902

Batch 270080, train_perplexity=160.58954, train_loss=5.0788517

Batch 270090, train_perplexity=156.01476, train_loss=5.0499506

Batch 270100, train_perplexity=163.73932, train_loss=5.0982757

Batch 270110, train_perplexity=165.90675, train_loss=5.111426

Batch 270120, train_perplexity=150.31018, train_loss=5.012701

Batch 270130, train_perplexity=154.73128, train_loss=5.04169

Batch 270140, train_perplexity=183.32942, train_loss=5.2112846

Batch 270150, train_perplexity=151.13452, train_loss=5.0181704

Batch 270160, train_perplexity=156.9316, train_loss=5.05581

Batch 270170, train_perplexity=153.32004, train_loss=5.0325274

Batch 270180, train_perplexity=157.52132, train_loss=5.059561

Batch 270190, train_perplexity=163.5267, train_loss=5.0969763

Batch 270200, train_perplexity=156.22394, train_loss=5.0512905

Batch 270210, train_perplexity=138.71675, train_loss=4.932434

Batch 270220, train_perplexity=156.22678, train_loss=5.0513086

Batch 270230, train_perplexity=160.78003, train_loss=5.080037

Batch 270240, train_perplexity=166.05745, train_loss=5.112334

Batch 270250, train_perplexity=159.42677, train_loss=5.0715847

Batch 270260, train_perplexity=153.79239, train_loss=5.0356035

Batch 270270, train_perplexity=154.31268, train_loss=5.038981

Batch 270280, train_perplexity=175.12679, train_loss=5.16551

Batch 270290, train_perplexity=143.88875, train_loss=4.9690404

Batch 270300, train_perplexity=164.73273, train_loss=5.1043243
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 270310, train_perplexity=169.33691, train_loss=5.1318903

Batch 270320, train_perplexity=144.87946, train_loss=4.975902

Batch 270330, train_perplexity=175.40474, train_loss=5.167096

Batch 270340, train_perplexity=155.64471, train_loss=5.047576

Batch 270350, train_perplexity=161.7753, train_loss=5.0862083

Batch 270360, train_perplexity=158.98889, train_loss=5.0688343

Batch 270370, train_perplexity=165.44768, train_loss=5.108655

Batch 270380, train_perplexity=164.97673, train_loss=5.1058044

Batch 270390, train_perplexity=150.02777, train_loss=5.0108204

Batch 270400, train_perplexity=155.89465, train_loss=5.0491805

Batch 270410, train_perplexity=164.79488, train_loss=5.1047015

Batch 270420, train_perplexity=163.86311, train_loss=5.0990314

Batch 270430, train_perplexity=156.76794, train_loss=5.0547667

Batch 270440, train_perplexity=170.64583, train_loss=5.1395903

Batch 270450, train_perplexity=147.89885, train_loss=4.9965286

Batch 270460, train_perplexity=165.04802, train_loss=5.1062365

Batch 270470, train_perplexity=160.8544, train_loss=5.0804996

Batch 270480, train_perplexity=154.24934, train_loss=5.0385704

Batch 270490, train_perplexity=153.47313, train_loss=5.0335255

Batch 270500, train_perplexity=147.28777, train_loss=4.9923882

Batch 270510, train_perplexity=162.9594, train_loss=5.093501

Batch 270520, train_perplexity=144.81088, train_loss=4.9754286

Batch 270530, train_perplexity=153.1097, train_loss=5.0311546

Batch 270540, train_perplexity=152.48994, train_loss=5.0270987

Batch 270550, train_perplexity=156.7169, train_loss=5.054441

Batch 270560, train_perplexity=154.26178, train_loss=5.038651

Batch 270570, train_perplexity=158.40228, train_loss=5.065138

Batch 270580, train_perplexity=147.83652, train_loss=4.996107

Batch 270590, train_perplexity=157.01901, train_loss=5.056367

Batch 270600, train_perplexity=179.51495, train_loss=5.1902585

Batch 270610, train_perplexity=152.9938, train_loss=5.0303974

Batch 270620, train_perplexity=165.48949, train_loss=5.1089077

Batch 270630, train_perplexity=166.21184, train_loss=5.113263

Batch 270640, train_perplexity=146.54126, train_loss=4.987307

Batch 270650, train_perplexity=157.74162, train_loss=5.0609584

Batch 270660, train_perplexity=160.3427, train_loss=5.0773134

Batch 270670, train_perplexity=171.17964, train_loss=5.1427135

Batch 270680, train_perplexity=166.3625, train_loss=5.114169

Batch 270690, train_perplexity=155.59248, train_loss=5.0472403

Batch 270700, train_perplexity=170.3465, train_loss=5.1378345

Batch 270710, train_perplexity=174.88803, train_loss=5.164146

Batch 270720, train_perplexity=162.32834, train_loss=5.089621

Batch 270730, train_perplexity=149.75081, train_loss=5.0089726

Batch 270740, train_perplexity=137.68733, train_loss=4.9249854

Batch 270750, train_perplexity=166.43533, train_loss=5.114607

Batch 270760, train_perplexity=165.36533, train_loss=5.108157

Batch 270770, train_perplexity=161.16873, train_loss=5.082452

Batch 270780, train_perplexity=181.1211, train_loss=5.199166

Batch 270790, train_perplexity=168.37634, train_loss=5.1262016

Batch 270800, train_perplexity=153.81967, train_loss=5.035781

Batch 270810, train_perplexity=174.95668, train_loss=5.1645384

Batch 270820, train_perplexity=161.89423, train_loss=5.086943

Batch 270830, train_perplexity=152.47177, train_loss=5.0269794

Batch 270840, train_perplexity=166.2916, train_loss=5.113743

Batch 270850, train_perplexity=137.6303, train_loss=4.924571

Batch 270860, train_perplexity=171.3321, train_loss=5.143604

Batch 270870, train_perplexity=157.10515, train_loss=5.0569153

Batch 270880, train_perplexity=158.9096, train_loss=5.0683355

Batch 270890, train_perplexity=155.14648, train_loss=5.0443697

Batch 270900, train_perplexity=162.35234, train_loss=5.089769

Batch 270910, train_perplexity=166.85219, train_loss=5.1171083

Batch 270920, train_perplexity=165.69621, train_loss=5.110156

Batch 270930, train_perplexity=160.0132, train_loss=5.0752563

Batch 270940, train_perplexity=167.1363, train_loss=5.1188097

Batch 270950, train_perplexity=165.10847, train_loss=5.1066027

Batch 270960, train_perplexity=146.83414, train_loss=4.9893036

Batch 270970, train_perplexity=158.15382, train_loss=5.063568

Batch 270980, train_perplexity=169.47699, train_loss=5.132717

Batch 270990, train_perplexity=180.11455, train_loss=5.193593

Batch 271000, train_perplexity=169.27185, train_loss=5.131506

Batch 271010, train_perplexity=173.21819, train_loss=5.154552

Batch 271020, train_perplexity=154.19978, train_loss=5.038249

Batch 271030, train_perplexity=160.77113, train_loss=5.079982

Batch 271040, train_perplexity=143.18439, train_loss=4.9641333

Batch 271050, train_perplexity=150.29857, train_loss=5.012624

Batch 271060, train_perplexity=164.59172, train_loss=5.103468

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Batch 271070, train_perplexity=159.01633, train_loss=5.069007

Batch 271080, train_perplexity=168.39465, train_loss=5.1263103

Batch 271090, train_perplexity=175.19928, train_loss=5.165924

Batch 271100, train_perplexity=157.48143, train_loss=5.0593076

Batch 271110, train_perplexity=165.9156, train_loss=5.1114793

Batch 271120, train_perplexity=146.77043, train_loss=4.9888697

Batch 271130, train_perplexity=163.25183, train_loss=5.095294

Batch 271140, train_perplexity=162.537, train_loss=5.0909057

Batch 271150, train_perplexity=148.73154, train_loss=5.002143

Batch 271160, train_perplexity=172.63257, train_loss=5.1511655

Batch 271170, train_perplexity=176.12166, train_loss=5.171175

Batch 271180, train_perplexity=158.99715, train_loss=5.0688863

Batch 271190, train_perplexity=156.62927, train_loss=5.0538816

Batch 271200, train_perplexity=180.26904, train_loss=5.1944504

Batch 271210, train_perplexity=155.1541, train_loss=5.044419

Batch 271220, train_perplexity=168.1116, train_loss=5.124628

Batch 271230, train_perplexity=149.50038, train_loss=5.007299

Batch 271240, train_perplexity=156.90585, train_loss=5.055646

Batch 271250, train_perplexity=162.37347, train_loss=5.089899

Batch 271260, train_perplexity=170.51324, train_loss=5.138813

Batch 271270, train_perplexity=150.5265, train_loss=5.014139

Batch 271280, train_perplexity=160.48283, train_loss=5.078187

Batch 271290, train_perplexity=158.89029, train_loss=5.068214

Batch 271300, train_perplexity=148.27734, train_loss=4.9990845

Batch 271310, train_perplexity=158.22774, train_loss=5.0640354

Batch 271320, train_perplexity=155.96492, train_loss=5.049631

Batch 271330, train_perplexity=178.68349, train_loss=5.185616

Batch 271340, train_perplexity=186.42398, train_loss=5.2280235

Batch 271350, train_perplexity=150.50282, train_loss=5.013982

Batch 271360, train_perplexity=170.94733, train_loss=5.1413555

Batch 271370, train_perplexity=169.44757, train_loss=5.1325436

Batch 271380, train_perplexity=146.00497, train_loss=4.9836407

Batch 271390, train_perplexity=170.21268, train_loss=5.1370487

Batch 271400, train_perplexity=175.67259, train_loss=5.168622

Batch 271410, train_perplexity=166.45454, train_loss=5.1147223

Batch 271420, train_perplexity=166.05807, train_loss=5.1123376

Batch 271430, train_perplexity=162.5153, train_loss=5.090772

Batch 271440, train_perplexity=158.56369, train_loss=5.0661564

Batch 271450, train_perplexity=139.99765, train_loss=4.9416256

Batch 271460, train_perplexity=168.38503, train_loss=5.126253

Batch 271470, train_perplexity=156.70367, train_loss=5.0543566

Batch 271480, train_perplexity=156.4276, train_loss=5.052593

Batch 271490, train_perplexity=158.89142, train_loss=5.068221

Batch 271500, train_perplexity=166.24156, train_loss=5.113442

Batch 271510, train_perplexity=164.08392, train_loss=5.100378

Batch 271520, train_perplexity=156.71907, train_loss=5.054455

Batch 271530, train_perplexity=169.36436, train_loss=5.1320524

Batch 271540, train_perplexity=161.32742, train_loss=5.083436
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 271550, train_perplexity=150.45876, train_loss=5.013689

Batch 271560, train_perplexity=176.90105, train_loss=5.1755905

Batch 271570, train_perplexity=167.89803, train_loss=5.123357

Batch 271580, train_perplexity=137.81877, train_loss=4.9259396

Batch 271590, train_perplexity=165.39088, train_loss=5.1083117

Batch 271600, train_perplexity=165.92812, train_loss=5.1115546

Batch 271610, train_perplexity=153.21735, train_loss=5.0318575

Batch 271620, train_perplexity=168.01865, train_loss=5.124075

Batch 271630, train_perplexity=140.07143, train_loss=4.9421525

Batch 271640, train_perplexity=162.59537, train_loss=5.0912647

Batch 271650, train_perplexity=166.15764, train_loss=5.112937

Batch 271660, train_perplexity=170.25578, train_loss=5.137302

Batch 271670, train_perplexity=192.12923, train_loss=5.258168

Batch 271680, train_perplexity=188.70326, train_loss=5.2401757

Batch 271690, train_perplexity=150.06776, train_loss=5.011087

Batch 271700, train_perplexity=161.52502, train_loss=5.08466

Batch 271710, train_perplexity=137.83533, train_loss=4.9260597

Batch 271720, train_perplexity=172.77281, train_loss=5.1519775

Batch 271730, train_perplexity=163.33609, train_loss=5.09581

Batch 271740, train_perplexity=170.68864, train_loss=5.139841

Batch 271750, train_perplexity=160.24303, train_loss=5.0766916

Batch 271760, train_perplexity=142.83688, train_loss=4.9617033

Batch 271770, train_perplexity=152.08199, train_loss=5.02442

Batch 271780, train_perplexity=160.67159, train_loss=5.0793624

Batch 271790, train_perplexity=165.68152, train_loss=5.1100674

Batch 271800, train_perplexity=158.05605, train_loss=5.0629497

Batch 271810, train_perplexity=177.72676, train_loss=5.1802473

Batch 271820, train_perplexity=167.75702, train_loss=5.1225166

Batch 271830, train_perplexity=168.16452, train_loss=5.124943

Batch 271840, train_perplexity=151.688, train_loss=5.021826

Batch 271850, train_perplexity=154.14803, train_loss=5.0379133

Batch 271860, train_perplexity=147.3188, train_loss=4.992599

Batch 271870, train_perplexity=169.31042, train_loss=5.131734

Batch 271880, train_perplexity=144.03174, train_loss=4.9700336

Batch 271890, train_perplexity=147.88362, train_loss=4.9964256

Batch 271900, train_perplexity=155.67693, train_loss=5.047783

Batch 271910, train_perplexity=150.98729, train_loss=5.0171957

Batch 271920, train_perplexity=178.0008, train_loss=5.181788

Batch 271930, train_perplexity=160.8107, train_loss=5.080228

Batch 271940, train_perplexity=169.86226, train_loss=5.134988

Batch 271950, train_perplexity=157.69717, train_loss=5.0606766

Batch 271960, train_perplexity=153.5125, train_loss=5.033782

Batch 271970, train_perplexity=145.37357, train_loss=4.9793067

Batch 271980, train_perplexity=151.8753, train_loss=5.02306

Batch 271990, train_perplexity=169.26248, train_loss=5.1314507

Batch 272000, train_perplexity=155.2378, train_loss=5.044958

Batch 272010, train_perplexity=179.20355, train_loss=5.1885223

Batch 272020, train_perplexity=161.6954, train_loss=5.0857143

Batch 272030, train_perplexity=162.82037, train_loss=5.0926476

Batch 272040, train_perplexity=168.23622, train_loss=5.125369

Batch 272050, train_perplexity=146.442, train_loss=4.9866295

Batch 272060, train_perplexity=138.76306, train_loss=4.932768

Batch 272070, train_perplexity=146.75748, train_loss=4.9887815

Batch 272080, train_perplexity=165.3409, train_loss=5.1080093

Batch 272090, train_perplexity=166.1181, train_loss=5.112699

Batch 272100, train_perplexity=151.15355, train_loss=5.0182962

Batch 272110, train_perplexity=143.67833, train_loss=4.967577

Batch 272120, train_perplexity=173.54036, train_loss=5.15641

Batch 272130, train_perplexity=155.85289, train_loss=5.0489125

Batch 272140, train_perplexity=162.05263, train_loss=5.087921

Batch 272150, train_perplexity=161.49953, train_loss=5.084502

Batch 272160, train_perplexity=175.55495, train_loss=5.167952

Batch 272170, train_perplexity=144.3169, train_loss=4.9720116

Batch 272180, train_perplexity=165.47443, train_loss=5.1088166

Batch 272190, train_perplexity=160.28842, train_loss=5.076975

Batch 272200, train_perplexity=179.6864, train_loss=5.191213

Batch 272210, train_perplexity=149.28333, train_loss=5.005846

Batch 272220, train_perplexity=167.20964, train_loss=5.1192484

Batch 272230, train_perplexity=166.74545, train_loss=5.1164684

Batch 272240, train_perplexity=146.70213, train_loss=4.9884043

Batch 272250, train_perplexity=166.41486, train_loss=5.114484

Batch 272260, train_perplexity=181.1134, train_loss=5.1991234

Batch 272270, train_perplexity=158.48659, train_loss=5.06567

Batch 272280, train_perplexity=163.75111, train_loss=5.0983477

Batch 272290, train_perplexity=165.09343, train_loss=5.1065116

Batch 272300, train_perplexity=157.53964, train_loss=5.059677

Batch 272310, train_perplexity=161.53188, train_loss=5.0847025

Batch 272320, train_perplexity=161.26382, train_loss=5.0830417

Batch 272330, train_perplexity=168.21361, train_loss=5.1252346

Batch 272340, train_perplexity=156.12297, train_loss=5.050644

Batch 272350, train_perplexity=161.07085, train_loss=5.0818443

Batch 272360, train_perplexity=151.35233, train_loss=5.0196104

Batch 272370, train_perplexity=171.28432, train_loss=5.143325

Batch 272380, train_perplexity=177.31874, train_loss=5.177949

Batch 272390, train_perplexity=152.79272, train_loss=5.0290823

Batch 272400, train_perplexity=175.3432, train_loss=5.166745

Batch 272410, train_perplexity=167.35019, train_loss=5.1200886

Batch 272420, train_perplexity=182.01909, train_loss=5.2041116

Batch 272430, train_perplexity=161.508, train_loss=5.0845547

Batch 272440, train_perplexity=156.64584, train_loss=5.0539875

Batch 272450, train_perplexity=150.00188, train_loss=5.010648

Batch 272460, train_perplexity=159.72414, train_loss=5.073448

Batch 272470, train_perplexity=172.04723, train_loss=5.147769

Batch 272480, train_perplexity=154.59277, train_loss=5.0407944

Batch 272490, train_perplexity=152.86604, train_loss=5.029562

Batch 272500, train_perplexity=178.8892, train_loss=5.1867666

Batch 272510, train_perplexity=151.46632, train_loss=5.0203633

Batch 272520, train_perplexity=157.78345, train_loss=5.0612235

Batch 272530, train_perplexity=152.93385, train_loss=5.0300055

Batch 272540, train_perplexity=149.4892, train_loss=5.007224

Batch 272550, train_perplexity=182.3826, train_loss=5.2061067

Batch 272560, train_perplexity=178.92017, train_loss=5.1869397

Batch 272570, train_perplexity=173.85402, train_loss=5.158216

Batch 272580, train_perplexity=147.70287, train_loss=4.9952025

Batch 272590, train_perplexity=166.68639, train_loss=5.116114

Batch 272600, train_perplexity=146.96918, train_loss=4.990223

Batch 272610, train_perplexity=170.71875, train_loss=5.1400175

Batch 272620, train_perplexity=148.35811, train_loss=4.999629

Batch 272630, train_perplexity=153.43814, train_loss=5.0332975

Batch 272640, train_perplexity=166.28383, train_loss=5.113696

Batch 272650, train_perplexity=136.20248, train_loss=4.9141426

Batch 272660, train_perplexity=165.57347, train_loss=5.109415

Batch 272670, train_perplexity=171.94995, train_loss=5.1472034

Batch 272680, train_perplexity=179.98868, train_loss=5.192894

Batch 272690, train_perplexity=151.46394, train_loss=5.0203476

Batch 272700, train_perplexity=169.67024, train_loss=5.133857

Batch 272710, train_perplexity=164.79039, train_loss=5.1046743

Batch 272720, train_perplexity=164.35345, train_loss=5.1020193

Batch 272730, train_perplexity=166.20328, train_loss=5.1132116

Batch 272740, train_perplexity=139.68896, train_loss=4.9394183

Batch 272750, train_perplexity=170.68425, train_loss=5.1398153

Batch 272760, train_perplexity=171.90765, train_loss=5.1469574

Batch 272770, train_perplexity=166.26813, train_loss=5.1136017

Batch 272780, train_perplexity=150.60835, train_loss=5.014683

Batch 272790, train_perplexity=141.58057, train_loss=4.952869

Batch 272800, train_perplexity=145.27655, train_loss=4.978639

Batch 272810, train_perplexity=142.57544, train_loss=4.9598713

Batch 272820, train_perplexity=161.14737, train_loss=5.0823193

Batch 272830, train_perplexity=149.1086, train_loss=5.004675

Batch 272840, train_perplexity=148.21599, train_loss=4.9986706
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 272850, train_perplexity=148.76402, train_loss=5.0023613

Batch 272860, train_perplexity=162.80469, train_loss=5.092551

Batch 272870, train_perplexity=165.75026, train_loss=5.110482

Batch 272880, train_perplexity=145.93661, train_loss=4.9831724

Batch 272890, train_perplexity=144.32164, train_loss=4.9720445

Batch 272900, train_perplexity=155.28555, train_loss=5.0452657

Batch 272910, train_perplexity=156.85483, train_loss=5.0553207

Batch 272920, train_perplexity=150.79626, train_loss=5.0159297

Batch 272930, train_perplexity=156.92763, train_loss=5.0557847

Batch 272940, train_perplexity=174.05534, train_loss=5.1593733

Batch 272950, train_perplexity=169.25142, train_loss=5.1313853

Batch 272960, train_perplexity=168.73465, train_loss=5.1283274

Batch 272970, train_perplexity=160.4556, train_loss=5.078017

Batch 272980, train_perplexity=171.89519, train_loss=5.146885

Batch 272990, train_perplexity=174.7334, train_loss=5.1632614

Batch 273000, train_perplexity=149.35046, train_loss=5.0062957

Batch 273010, train_perplexity=148.75317, train_loss=5.0022883

Batch 273020, train_perplexity=172.19676, train_loss=5.148638

Batch 273030, train_perplexity=166.18585, train_loss=5.1131067

Batch 273040, train_perplexity=155.76945, train_loss=5.048377

Batch 273050, train_perplexity=161.75293, train_loss=5.08607

Batch 273060, train_perplexity=140.70525, train_loss=4.946667

Batch 273070, train_perplexity=170.60141, train_loss=5.13933

Batch 273080, train_perplexity=161.77946, train_loss=5.086234

Batch 273090, train_perplexity=155.69237, train_loss=5.047882

Batch 273100, train_perplexity=148.6174, train_loss=5.001375

Batch 273110, train_perplexity=153.80455, train_loss=5.0356827

Batch 273120, train_perplexity=149.94452, train_loss=5.0102654

Batch 273130, train_perplexity=169.68448, train_loss=5.1339407

Batch 273140, train_perplexity=166.66382, train_loss=5.1159787

Batch 273150, train_perplexity=170.70459, train_loss=5.1399345

Batch 273160, train_perplexity=161.82114, train_loss=5.0864916

Batch 273170, train_perplexity=158.45773, train_loss=5.065488

Batch 273180, train_perplexity=159.71957, train_loss=5.0734196

Batch 273190, train_perplexity=159.98528, train_loss=5.075082

Batch 273200, train_perplexity=143.703, train_loss=4.9677486

Batch 273210, train_perplexity=170.0875, train_loss=5.136313

Batch 273220, train_perplexity=161.55637, train_loss=5.084854

Batch 273230, train_perplexity=166.4151, train_loss=5.1144853

Batch 273240, train_perplexity=160.12196, train_loss=5.075936

Batch 273250, train_perplexity=151.80986, train_loss=5.022629

Batch 273260, train_perplexity=159.3462, train_loss=5.0710793

Batch 273270, train_perplexity=174.03027, train_loss=5.1592293

Batch 273280, train_perplexity=167.71239, train_loss=5.1222506

Batch 273290, train_perplexity=160.8897, train_loss=5.080719

Batch 273300, train_perplexity=165.78267, train_loss=5.1106777

Batch 273310, train_perplexity=171.21198, train_loss=5.1429024

Batch 273320, train_perplexity=148.59245, train_loss=5.0012074

Batch 273330, train_perplexity=178.89423, train_loss=5.1867948

Batch 273340, train_perplexity=144.69658, train_loss=4.974639

Batch 273350, train_perplexity=162.51785, train_loss=5.090788

Batch 273360, train_perplexity=162.94992, train_loss=5.093443

Batch 273370, train_perplexity=153.22166, train_loss=5.0318856

Batch 273380, train_perplexity=153.1893, train_loss=5.0316744

Batch 273390, train_perplexity=158.8293, train_loss=5.06783

Batch 273400, train_perplexity=154.21507, train_loss=5.038348

Batch 273410, train_perplexity=157.39427, train_loss=5.058754

Batch 273420, train_perplexity=154.19617, train_loss=5.0382257

Batch 273430, train_perplexity=161.65115, train_loss=5.0854406

Batch 273440, train_perplexity=169.50624, train_loss=5.1328897

Batch 273450, train_perplexity=157.40906, train_loss=5.058848

Batch 273460, train_perplexity=168.75252, train_loss=5.128433

Batch 273470, train_perplexity=175.48422, train_loss=5.167549

Batch 273480, train_perplexity=183.03314, train_loss=5.209667

Batch 273490, train_perplexity=167.58688, train_loss=5.121502

Batch 273500, train_perplexity=142.51482, train_loss=4.959446

Batch 273510, train_perplexity=151.31004, train_loss=5.019331

Batch 273520, train_perplexity=162.11894, train_loss=5.0883303

Batch 273530, train_perplexity=155.05234, train_loss=5.0437627

Batch 273540, train_perplexity=138.65736, train_loss=4.932006

Batch 273550, train_perplexity=141.08046, train_loss=4.9493303

Batch 273560, train_perplexity=166.41406, train_loss=5.114479

Batch 273570, train_perplexity=150.958, train_loss=5.0170016

Batch 273580, train_perplexity=169.02907, train_loss=5.1300707

Batch 273590, train_perplexity=163.37473, train_loss=5.0960464

Batch 273600, train_perplexity=150.52155, train_loss=5.0141063

Batch 273610, train_perplexity=145.87032, train_loss=4.982718

Batch 273620, train_perplexity=151.47398, train_loss=5.020414

Batch 273630, train_perplexity=163.47992, train_loss=5.09669

Batch 273640, train_perplexity=161.35127, train_loss=5.083584

Batch 273650, train_perplexity=175.24106, train_loss=5.1661625

Batch 273660, train_perplexity=168.06232, train_loss=5.124335

Batch 273670, train_perplexity=152.08257, train_loss=5.0244236

Batch 273680, train_perplexity=168.11456, train_loss=5.1246457

Batch 273690, train_perplexity=165.79373, train_loss=5.1107445

Batch 273700, train_perplexity=168.14134, train_loss=5.124805

Batch 273710, train_perplexity=162.24422, train_loss=5.0891027

Batch 273720, train_perplexity=165.03818, train_loss=5.106177

Batch 273730, train_perplexity=167.41428, train_loss=5.1204715

Batch 273740, train_perplexity=172.46227, train_loss=5.1501784

Batch 273750, train_perplexity=142.35588, train_loss=4.95833

Batch 273760, train_perplexity=157.1023, train_loss=5.056897

Batch 273770, train_perplexity=139.68211, train_loss=4.939369

Batch 273780, train_perplexity=168.53233, train_loss=5.1271276

Batch 273790, train_perplexity=171.88904, train_loss=5.146849

Batch 273800, train_perplexity=147.05505, train_loss=4.990807

Batch 273810, train_perplexity=163.97552, train_loss=5.099717

Batch 273820, train_perplexity=150.25471, train_loss=5.012332

Batch 273830, train_perplexity=163.98561, train_loss=5.0997787

Batch 273840, train_perplexity=148.64278, train_loss=5.001546

Batch 273850, train_perplexity=150.25694, train_loss=5.0123467

Batch 273860, train_perplexity=153.18951, train_loss=5.031676

Batch 273870, train_perplexity=163.17984, train_loss=5.094853

Batch 273880, train_perplexity=155.62157, train_loss=5.047427

Batch 273890, train_perplexity=150.68047, train_loss=5.0151615

Batch 273900, train_perplexity=137.91547, train_loss=4.926641

Batch 273910, train_perplexity=161.56647, train_loss=5.0849166

Batch 273920, train_perplexity=158.20111, train_loss=5.063867

Batch 273930, train_perplexity=161.7598, train_loss=5.0861125

Batch 273940, train_perplexity=157.93173, train_loss=5.062163

Batch 273950, train_perplexity=173.79369, train_loss=5.157869

Batch 273960, train_perplexity=149.60379, train_loss=5.0079904

Batch 273970, train_perplexity=177.30843, train_loss=5.177891

Batch 273980, train_perplexity=170.92223, train_loss=5.1412086

Batch 273990, train_perplexity=141.62102, train_loss=4.9531546

Batch 274000, train_perplexity=154.09613, train_loss=5.0375767

Batch 274010, train_perplexity=162.80243, train_loss=5.0925374

Batch 274020, train_perplexity=158.82597, train_loss=5.067809

Batch 274030, train_perplexity=149.20177, train_loss=5.0052996

Batch 274040, train_perplexity=161.60522, train_loss=5.0851564

Batch 274050, train_perplexity=163.15463, train_loss=5.0946984

Batch 274060, train_perplexity=178.40399, train_loss=5.1840506

Batch 274070, train_perplexity=156.63016, train_loss=5.0538874

Batch 274080, train_perplexity=142.36566, train_loss=4.958399

Batch 274090, train_perplexity=161.1735, train_loss=5.0824814

Batch 274100, train_perplexity=143.179, train_loss=4.9640956

Batch 274110, train_perplexity=144.11526, train_loss=4.9706135

Batch 274120, train_perplexity=154.40292, train_loss=5.0395656

Batch 274130, train_perplexity=139.34091, train_loss=4.9369235

Batch 274140, train_perplexity=158.33673, train_loss=5.064724
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 274150, train_perplexity=162.60483, train_loss=5.091323

Batch 274160, train_perplexity=161.76295, train_loss=5.086132

Batch 274170, train_perplexity=155.41022, train_loss=5.046068

Batch 274180, train_perplexity=155.20331, train_loss=5.044736

Batch 274190, train_perplexity=160.03992, train_loss=5.0754232

Batch 274200, train_perplexity=183.9864, train_loss=5.214862

Batch 274210, train_perplexity=151.17691, train_loss=5.0184507

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00073-of-00100
Loaded 306690 sentences.
Finished loading
Batch 274220, train_perplexity=164.83786, train_loss=5.1049623

Batch 274230, train_perplexity=164.54887, train_loss=5.1032076

Batch 274240, train_perplexity=154.78647, train_loss=5.0420465

Batch 274250, train_perplexity=179.6225, train_loss=5.1908574

Batch 274260, train_perplexity=175.24892, train_loss=5.1662073

Batch 274270, train_perplexity=160.6669, train_loss=5.0793333

Batch 274280, train_perplexity=137.27544, train_loss=4.9219894

Batch 274290, train_perplexity=167.40254, train_loss=5.1204014

Batch 274300, train_perplexity=145.00006, train_loss=4.976734

Batch 274310, train_perplexity=163.41461, train_loss=5.0962906

Batch 274320, train_perplexity=143.42383, train_loss=4.965804

Batch 274330, train_perplexity=164.73737, train_loss=5.1043525

Batch 274340, train_perplexity=152.42001, train_loss=5.02664

Batch 274350, train_perplexity=149.91649, train_loss=5.0100784

Batch 274360, train_perplexity=168.35146, train_loss=5.126054

Batch 274370, train_perplexity=166.00575, train_loss=5.1120224

Batch 274380, train_perplexity=155.32547, train_loss=5.0455227

Batch 274390, train_perplexity=172.38086, train_loss=5.1497064

Batch 274400, train_perplexity=127.170364, train_loss=4.8455276

Batch 274410, train_perplexity=159.17276, train_loss=5.06999

Batch 274420, train_perplexity=153.09874, train_loss=5.031083

Batch 274430, train_perplexity=173.44159, train_loss=5.155841

Batch 274440, train_perplexity=168.87398, train_loss=5.129153

Batch 274450, train_perplexity=164.408, train_loss=5.102351

Batch 274460, train_perplexity=168.03531, train_loss=5.124174

Batch 274470, train_perplexity=151.21123, train_loss=5.0186777

Batch 274480, train_perplexity=160.51009, train_loss=5.0783567

Batch 274490, train_perplexity=153.37114, train_loss=5.0328608

Batch 274500, train_perplexity=168.90233, train_loss=5.1293206

Batch 274510, train_perplexity=147.06958, train_loss=4.990906

Batch 274520, train_perplexity=146.42831, train_loss=4.986536

Batch 274530, train_perplexity=152.65582, train_loss=5.028186

Batch 274540, train_perplexity=160.18405, train_loss=5.0763235

Batch 274550, train_perplexity=176.71405, train_loss=5.174533

Batch 274560, train_perplexity=165.579, train_loss=5.1094484

Batch 274570, train_perplexity=161.7419, train_loss=5.086002

Batch 274580, train_perplexity=172.38292, train_loss=5.1497183

Batch 274590, train_perplexity=159.02126, train_loss=5.069038

Batch 274600, train_perplexity=158.13663, train_loss=5.0634594

Batch 274610, train_perplexity=159.20866, train_loss=5.0702157

Batch 274620, train_perplexity=163.88765, train_loss=5.099181

Batch 274630, train_perplexity=156.32089, train_loss=5.051911

Batch 274640, train_perplexity=149.56976, train_loss=5.007763

Batch 274650, train_perplexity=167.5546, train_loss=5.1213093

Batch 274660, train_perplexity=143.43655, train_loss=4.965893

Batch 274670, train_perplexity=160.11021, train_loss=5.0758624

Batch 274680, train_perplexity=143.30597, train_loss=4.964982

Batch 274690, train_perplexity=152.82407, train_loss=5.0292873

Batch 274700, train_perplexity=153.12985, train_loss=5.0312862

Batch 274710, train_perplexity=160.03595, train_loss=5.0753984

Batch 274720, train_perplexity=172.4329, train_loss=5.150008

Batch 274730, train_perplexity=172.01212, train_loss=5.147565

Batch 274740, train_perplexity=160.41412, train_loss=5.077759

Batch 274750, train_perplexity=170.47723, train_loss=5.138602

Batch 274760, train_perplexity=162.37038, train_loss=5.08988

Batch 274770, train_perplexity=159.76268, train_loss=5.0736895

Batch 274780, train_perplexity=152.2822, train_loss=5.0257354

Batch 274790, train_perplexity=156.14648, train_loss=5.0507946

Batch 274800, train_perplexity=173.1342, train_loss=5.154067

Batch 274810, train_perplexity=146.10533, train_loss=4.984328

Batch 274820, train_perplexity=193.18634, train_loss=5.263655

Batch 274830, train_perplexity=161.99129, train_loss=5.0875425

Batch 274840, train_perplexity=147.25743, train_loss=4.9921823

Batch 274850, train_perplexity=166.66422, train_loss=5.115981

Batch 274860, train_perplexity=138.92381, train_loss=4.9339256

Batch 274870, train_perplexity=160.19566, train_loss=5.076396

Batch 274880, train_perplexity=159.3639, train_loss=5.0711904

Batch 274890, train_perplexity=162.32385, train_loss=5.0895934

Batch 274900, train_perplexity=152.4022, train_loss=5.026523

Batch 274910, train_perplexity=158.52303, train_loss=5.0659

Batch 274920, train_perplexity=144.14359, train_loss=4.97081

Batch 274930, train_perplexity=150.59097, train_loss=5.0145674

Batch 274940, train_perplexity=177.53601, train_loss=5.1791735

Batch 274950, train_perplexity=158.26366, train_loss=5.0642624

Batch 274960, train_perplexity=155.54826, train_loss=5.046956

Batch 274970, train_perplexity=152.38797, train_loss=5.0264297

Batch 274980, train_perplexity=135.97893, train_loss=4.9125

Batch 274990, train_perplexity=155.47574, train_loss=5.0464897

Batch 275000, train_perplexity=145.9163, train_loss=4.983033

Batch 275010, train_perplexity=171.42061, train_loss=5.14412

Batch 275020, train_perplexity=149.64153, train_loss=5.0082426

Batch 275030, train_perplexity=151.55223, train_loss=5.0209303

Batch 275040, train_perplexity=155.9157, train_loss=5.0493155

Batch 275050, train_perplexity=162.21884, train_loss=5.0889463

Batch 275060, train_perplexity=146.78954, train_loss=4.989

Batch 275070, train_perplexity=142.46922, train_loss=4.959126

Batch 275080, train_perplexity=165.93436, train_loss=5.1115923

Batch 275090, train_perplexity=162.73639, train_loss=5.0921316

Batch 275100, train_perplexity=156.72803, train_loss=5.054512

Batch 275110, train_perplexity=141.13226, train_loss=4.9496975

Batch 275120, train_perplexity=161.49529, train_loss=5.084476

Batch 275130, train_perplexity=168.32721, train_loss=5.12591

Batch 275140, train_perplexity=171.51741, train_loss=5.144685

Batch 275150, train_perplexity=167.12276, train_loss=5.1187286

Batch 275160, train_perplexity=167.1497, train_loss=5.11889

Batch 275170, train_perplexity=154.34418, train_loss=5.039185

Batch 275180, train_perplexity=167.33966, train_loss=5.1200256

Batch 275190, train_perplexity=152.10216, train_loss=5.0245523

Batch 275200, train_perplexity=166.68646, train_loss=5.1161146

Batch 275210, train_perplexity=175.75287, train_loss=5.169079

Batch 275220, train_perplexity=171.40834, train_loss=5.1440487

Batch 275230, train_perplexity=159.97736, train_loss=5.075032

Batch 275240, train_perplexity=136.69319, train_loss=4.917739

Batch 275250, train_perplexity=165.81557, train_loss=5.110876

Batch 275260, train_perplexity=159.23987, train_loss=5.0704117

Batch 275270, train_perplexity=153.89288, train_loss=5.036257

Batch 275280, train_perplexity=175.53343, train_loss=5.1678295

Batch 275290, train_perplexity=153.59525, train_loss=5.034321

Batch 275300, train_perplexity=166.64658, train_loss=5.1158752

Batch 275310, train_perplexity=158.27362, train_loss=5.0643253

Batch 275320, train_perplexity=141.08812, train_loss=4.9493847

Batch 275330, train_perplexity=155.69542, train_loss=5.0479016

Batch 275340, train_perplexity=160.48636, train_loss=5.078209

Batch 275350, train_perplexity=164.78725, train_loss=5.1046553

Batch 275360, train_perplexity=148.95255, train_loss=5.003628

Batch 275370, train_perplexity=161.99005, train_loss=5.087535

Batch 275380, train_perplexity=165.04376, train_loss=5.1062107
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 275390, train_perplexity=146.38936, train_loss=4.98627

Batch 275400, train_perplexity=152.9119, train_loss=5.029862

Batch 275410, train_perplexity=157.99304, train_loss=5.062551

Batch 275420, train_perplexity=140.21223, train_loss=4.943157

Batch 275430, train_perplexity=157.96465, train_loss=5.0623713

Batch 275440, train_perplexity=165.23087, train_loss=5.1073437

Batch 275450, train_perplexity=154.38239, train_loss=5.0394325

Batch 275460, train_perplexity=148.08092, train_loss=4.997759

Batch 275470, train_perplexity=157.69371, train_loss=5.0606546

Batch 275480, train_perplexity=180.13086, train_loss=5.1936836

Batch 275490, train_perplexity=147.34108, train_loss=4.99275

Batch 275500, train_perplexity=162.31108, train_loss=5.0895147

Batch 275510, train_perplexity=167.67593, train_loss=5.122033

Batch 275520, train_perplexity=152.03545, train_loss=5.0241137

Batch 275530, train_perplexity=155.02238, train_loss=5.0435696

Batch 275540, train_perplexity=154.67918, train_loss=5.041353

Batch 275550, train_perplexity=155.28317, train_loss=5.0452504

Batch 275560, train_perplexity=152.73984, train_loss=5.028736

Batch 275570, train_perplexity=162.70038, train_loss=5.0919104

Batch 275580, train_perplexity=155.48479, train_loss=5.046548

Batch 275590, train_perplexity=169.51933, train_loss=5.132967

Batch 275600, train_perplexity=151.8478, train_loss=5.0228786

Batch 275610, train_perplexity=166.93257, train_loss=5.11759

Batch 275620, train_perplexity=159.61998, train_loss=5.072796

Batch 275630, train_perplexity=155.10329, train_loss=5.044091

Batch 275640, train_perplexity=157.04515, train_loss=5.0565333

Batch 275650, train_perplexity=160.67603, train_loss=5.07939

Batch 275660, train_perplexity=154.5036, train_loss=5.0402174

Batch 275670, train_perplexity=181.94333, train_loss=5.2036953

Batch 275680, train_perplexity=155.6226, train_loss=5.047434

Batch 275690, train_perplexity=156.95412, train_loss=5.0559535

Batch 275700, train_perplexity=170.33934, train_loss=5.1377926

Batch 275710, train_perplexity=152.53285, train_loss=5.02738

Batch 275720, train_perplexity=152.67584, train_loss=5.028317

Batch 275730, train_perplexity=164.56001, train_loss=5.1032753

Batch 275740, train_perplexity=147.2325, train_loss=4.992013

Batch 275750, train_perplexity=148.24101, train_loss=4.9988394

Batch 275760, train_perplexity=148.90141, train_loss=5.0032845

Batch 275770, train_perplexity=143.29874, train_loss=4.9649315

Batch 275780, train_perplexity=147.51619, train_loss=4.993938

Batch 275790, train_perplexity=165.02733, train_loss=5.106111

Batch 275800, train_perplexity=188.58101, train_loss=5.2395277

Batch 275810, train_perplexity=162.2265, train_loss=5.0889935

Batch 275820, train_perplexity=151.08208, train_loss=5.017823

Batch 275830, train_perplexity=159.77617, train_loss=5.073774

Batch 275840, train_perplexity=157.3945, train_loss=5.0587554

Batch 275850, train_perplexity=166.3598, train_loss=5.114153

Batch 275860, train_perplexity=153.02759, train_loss=5.030618

Batch 275870, train_perplexity=148.53648, train_loss=5.0008307

Batch 275880, train_perplexity=170.56505, train_loss=5.139117

Batch 275890, train_perplexity=166.23055, train_loss=5.1133757

Batch 275900, train_perplexity=154.92906, train_loss=5.0429673

Batch 275910, train_perplexity=166.58063, train_loss=5.1154795

Batch 275920, train_perplexity=182.13603, train_loss=5.204754

Batch 275930, train_perplexity=168.30724, train_loss=5.125791

Batch 275940, train_perplexity=140.7463, train_loss=4.946959

Batch 275950, train_perplexity=165.2348, train_loss=5.1073675

Batch 275960, train_perplexity=163.14087, train_loss=5.094614

Batch 275970, train_perplexity=160.79182, train_loss=5.0801105

Batch 275980, train_perplexity=160.91792, train_loss=5.0808945

Batch 275990, train_perplexity=161.34196, train_loss=5.083526

Batch 276000, train_perplexity=149.84946, train_loss=5.009631

Batch 276010, train_perplexity=156.64674, train_loss=5.053993

Batch 276020, train_perplexity=168.91547, train_loss=5.1293983

Batch 276030, train_perplexity=158.46165, train_loss=5.0655127

Batch 276040, train_perplexity=156.36226, train_loss=5.0521755

Batch 276050, train_perplexity=160.88586, train_loss=5.080695

Batch 276060, train_perplexity=167.22743, train_loss=5.1193547

Batch 276070, train_perplexity=151.51132, train_loss=5.0206604

Batch 276080, train_perplexity=148.60677, train_loss=5.0013037

Batch 276090, train_perplexity=142.32901, train_loss=4.9581413

Batch 276100, train_perplexity=172.48495, train_loss=5.15031

Batch 276110, train_perplexity=151.06651, train_loss=5.01772

Batch 276120, train_perplexity=163.45895, train_loss=5.096562

Batch 276130, train_perplexity=149.85817, train_loss=5.0096893

Batch 276140, train_perplexity=144.12283, train_loss=4.970666

Batch 276150, train_perplexity=139.5113, train_loss=4.9381456

Batch 276160, train_perplexity=157.3416, train_loss=5.058419

Batch 276170, train_perplexity=153.52795, train_loss=5.0338826

Batch 276180, train_perplexity=172.72981, train_loss=5.1517286

Batch 276190, train_perplexity=143.21867, train_loss=4.9643726

Batch 276200, train_perplexity=173.95535, train_loss=5.1587987

Batch 276210, train_perplexity=154.24126, train_loss=5.038518

Batch 276220, train_perplexity=150.56311, train_loss=5.0143824

Batch 276230, train_perplexity=181.93709, train_loss=5.203661

Batch 276240, train_perplexity=156.71317, train_loss=5.054417

Batch 276250, train_perplexity=158.28804, train_loss=5.0644164

Batch 276260, train_perplexity=158.12411, train_loss=5.0633802

Batch 276270, train_perplexity=174.8741, train_loss=5.1640663

Batch 276280, train_perplexity=160.70369, train_loss=5.079562

Batch 276290, train_perplexity=156.32082, train_loss=5.0519104

Batch 276300, train_perplexity=148.51305, train_loss=5.000673

Batch 276310, train_perplexity=160.99168, train_loss=5.0813527

Batch 276320, train_perplexity=161.73535, train_loss=5.0859613

Batch 276330, train_perplexity=141.33383, train_loss=4.9511247

Batch 276340, train_perplexity=158.84749, train_loss=5.0679445

Batch 276350, train_perplexity=156.8591, train_loss=5.055348

Batch 276360, train_perplexity=166.09958, train_loss=5.1125875

Batch 276370, train_perplexity=168.44604, train_loss=5.1266155

Batch 276380, train_perplexity=150.17578, train_loss=5.0118065

Batch 276390, train_perplexity=142.7794, train_loss=4.961301

Batch 276400, train_perplexity=156.15245, train_loss=5.0508327

Batch 276410, train_perplexity=162.19278, train_loss=5.0887856

Batch 276420, train_perplexity=169.33012, train_loss=5.1318502

Batch 276430, train_perplexity=157.47775, train_loss=5.059284

Batch 276440, train_perplexity=159.92754, train_loss=5.074721

Batch 276450, train_perplexity=151.57838, train_loss=5.021103

Batch 276460, train_perplexity=153.0986, train_loss=5.031082

Batch 276470, train_perplexity=143.1287, train_loss=4.963744

Batch 276480, train_perplexity=151.61237, train_loss=5.021327

Batch 276490, train_perplexity=151.2043, train_loss=5.018632

Batch 276500, train_perplexity=161.075, train_loss=5.08187

Batch 276510, train_perplexity=139.18373, train_loss=4.935795

Batch 276520, train_perplexity=153.93141, train_loss=5.036507

Batch 276530, train_perplexity=163.50035, train_loss=5.096815

Batch 276540, train_perplexity=156.99783, train_loss=5.056232

Batch 276550, train_perplexity=163.25276, train_loss=5.0952997

Batch 276560, train_perplexity=148.74777, train_loss=5.002252

Batch 276570, train_perplexity=135.81465, train_loss=4.911291

Batch 276580, train_perplexity=165.84663, train_loss=5.1110635

Batch 276590, train_perplexity=170.16731, train_loss=5.136782

Batch 276600, train_perplexity=147.84464, train_loss=4.996162

Batch 276610, train_perplexity=170.24857, train_loss=5.1372595

Batch 276620, train_perplexity=141.12924, train_loss=4.949676

Batch 276630, train_perplexity=158.84013, train_loss=5.0678983

Batch 276640, train_perplexity=175.86218, train_loss=5.1697006

Batch 276650, train_perplexity=165.18636, train_loss=5.1070743

Batch 276660, train_perplexity=140.62012, train_loss=4.946062

Batch 276670, train_perplexity=171.08473, train_loss=5.142159

Batch 276680, train_perplexity=172.90121, train_loss=5.1527205
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 276690, train_perplexity=166.12247, train_loss=5.1127253

Batch 276700, train_perplexity=153.33057, train_loss=5.032596

Batch 276710, train_perplexity=154.21964, train_loss=5.038378

Batch 276720, train_perplexity=142.89302, train_loss=4.962096

Batch 276730, train_perplexity=178.37149, train_loss=5.1838684

Batch 276740, train_perplexity=172.81122, train_loss=5.1521997

Batch 276750, train_perplexity=157.2822, train_loss=5.0580416

Batch 276760, train_perplexity=158.06644, train_loss=5.0630155

Batch 276770, train_perplexity=153.49077, train_loss=5.0336404

Batch 276780, train_perplexity=141.93219, train_loss=4.9553494

Batch 276790, train_perplexity=169.7468, train_loss=5.134308

Batch 276800, train_perplexity=155.17386, train_loss=5.044546

Batch 276810, train_perplexity=172.70149, train_loss=5.1515646

Batch 276820, train_perplexity=181.882, train_loss=5.203358

Batch 276830, train_perplexity=164.40605, train_loss=5.1023393

Batch 276840, train_perplexity=166.69084, train_loss=5.116141

Batch 276850, train_perplexity=162.88995, train_loss=5.093075

Batch 276860, train_perplexity=143.64983, train_loss=4.9673786

Batch 276870, train_perplexity=152.59468, train_loss=5.0277853

Batch 276880, train_perplexity=149.72874, train_loss=5.0088253

Batch 276890, train_perplexity=148.16356, train_loss=4.998317

Batch 276900, train_perplexity=154.01665, train_loss=5.0370607

Batch 276910, train_perplexity=153.01314, train_loss=5.030524

Batch 276920, train_perplexity=167.36519, train_loss=5.120178

Batch 276930, train_perplexity=160.4934, train_loss=5.078253

Batch 276940, train_perplexity=169.48425, train_loss=5.13276

Batch 276950, train_perplexity=185.81853, train_loss=5.2247705

Batch 276960, train_perplexity=149.42099, train_loss=5.0067677

Batch 276970, train_perplexity=176.07631, train_loss=5.1709175

Batch 276980, train_perplexity=163.32277, train_loss=5.0957284

Batch 276990, train_perplexity=145.60674, train_loss=4.9809093

Batch 277000, train_perplexity=148.65837, train_loss=5.001651

Batch 277010, train_perplexity=148.9838, train_loss=5.0038376

Batch 277020, train_perplexity=164.23515, train_loss=5.1012993

Batch 277030, train_perplexity=158.44942, train_loss=5.0654354

Batch 277040, train_perplexity=148.18298, train_loss=4.998448

Batch 277050, train_perplexity=161.43054, train_loss=5.084075

Batch 277060, train_perplexity=157.65387, train_loss=5.060402

Batch 277070, train_perplexity=154.06961, train_loss=5.0374045

Batch 277080, train_perplexity=156.44707, train_loss=5.0527177

Batch 277090, train_perplexity=158.10028, train_loss=5.0632296

Batch 277100, train_perplexity=165.691, train_loss=5.1101246

Batch 277110, train_perplexity=161.63574, train_loss=5.0853453

Batch 277120, train_perplexity=138.96501, train_loss=4.934222

Batch 277130, train_perplexity=164.60693, train_loss=5.1035604

Batch 277140, train_perplexity=162.10023, train_loss=5.088215

Batch 277150, train_perplexity=171.20184, train_loss=5.1428432

Batch 277160, train_perplexity=163.78423, train_loss=5.09855

Batch 277170, train_perplexity=169.69913, train_loss=5.134027

Batch 277180, train_perplexity=148.44444, train_loss=5.000211

Batch 277190, train_perplexity=145.94107, train_loss=4.983203

Batch 277200, train_perplexity=152.82938, train_loss=5.029322

Batch 277210, train_perplexity=168.96654, train_loss=5.1297007

Batch 277220, train_perplexity=164.69016, train_loss=5.104066

Batch 277230, train_perplexity=142.45836, train_loss=4.9590497

Batch 277240, train_perplexity=161.7369, train_loss=5.085971

Batch 277250, train_perplexity=151.26387, train_loss=5.019026

Batch 277260, train_perplexity=168.73755, train_loss=5.1283445

Batch 277270, train_perplexity=177.40205, train_loss=5.1784186

Batch 277280, train_perplexity=144.02686, train_loss=4.97

Batch 277290, train_perplexity=152.37292, train_loss=5.026331

Batch 277300, train_perplexity=169.231, train_loss=5.1312647

Batch 277310, train_perplexity=149.62411, train_loss=5.0081263

Batch 277320, train_perplexity=177.17929, train_loss=5.177162

Batch 277330, train_perplexity=152.89798, train_loss=5.029771

Batch 277340, train_perplexity=164.10443, train_loss=5.100503

Batch 277350, train_perplexity=155.70403, train_loss=5.047957

Batch 277360, train_perplexity=148.95738, train_loss=5.00366

Batch 277370, train_perplexity=148.45343, train_loss=5.0002713

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00042-of-00100
Loaded 306879 sentences.
Finished loading
Batch 277380, train_perplexity=154.99327, train_loss=5.0433817

Batch 277390, train_perplexity=152.58698, train_loss=5.0277348

Batch 277400, train_perplexity=155.55212, train_loss=5.046981

Batch 277410, train_perplexity=154.41139, train_loss=5.0396204

Batch 277420, train_perplexity=168.46164, train_loss=5.126708

Batch 277430, train_perplexity=153.87476, train_loss=5.036139

Batch 277440, train_perplexity=155.60703, train_loss=5.0473337

Batch 277450, train_perplexity=140.50946, train_loss=4.945275

Batch 277460, train_perplexity=156.93593, train_loss=5.0558376

Batch 277470, train_perplexity=172.59059, train_loss=5.1509223

Batch 277480, train_perplexity=169.78322, train_loss=5.1345224

Batch 277490, train_perplexity=166.78036, train_loss=5.1166778

Batch 277500, train_perplexity=153.8543, train_loss=5.036006

Batch 277510, train_perplexity=165.786, train_loss=5.1106977

Batch 277520, train_perplexity=168.3436, train_loss=5.126007

Batch 277530, train_perplexity=145.20265, train_loss=4.9781303

Batch 277540, train_perplexity=158.61784, train_loss=5.066498

Batch 277550, train_perplexity=163.14024, train_loss=5.09461

Batch 277560, train_perplexity=163.38266, train_loss=5.096095

Batch 277570, train_perplexity=152.49547, train_loss=5.027135

Batch 277580, train_perplexity=154.57066, train_loss=5.0406513

Batch 277590, train_perplexity=179.40524, train_loss=5.189647

Batch 277600, train_perplexity=139.75246, train_loss=4.9398727

Batch 277610, train_perplexity=160.63373, train_loss=5.079127

Batch 277620, train_perplexity=146.55, train_loss=4.9873667

Batch 277630, train_perplexity=166.66199, train_loss=5.1159678

Batch 277640, train_perplexity=156.356, train_loss=5.0521355

Batch 277650, train_perplexity=161.39467, train_loss=5.083853

Batch 277660, train_perplexity=166.99928, train_loss=5.1179895

Batch 277670, train_perplexity=133.50543, train_loss=4.894142

Batch 277680, train_perplexity=163.8389, train_loss=5.0988836

Batch 277690, train_perplexity=159.09552, train_loss=5.0695047

Batch 277700, train_perplexity=167.77959, train_loss=5.122651

Batch 277710, train_perplexity=158.70772, train_loss=5.0670643

Batch 277720, train_perplexity=147.22661, train_loss=4.991973

Batch 277730, train_perplexity=158.04745, train_loss=5.0628953

Batch 277740, train_perplexity=156.49236, train_loss=5.053007

Batch 277750, train_perplexity=138.52426, train_loss=4.9310455

Batch 277760, train_perplexity=170.95638, train_loss=5.1414084

Batch 277770, train_perplexity=161.01678, train_loss=5.0815086

Batch 277780, train_perplexity=185.60733, train_loss=5.2236333

Batch 277790, train_perplexity=167.39297, train_loss=5.120344

Batch 277800, train_perplexity=147.8399, train_loss=4.99613

Batch 277810, train_perplexity=159.16487, train_loss=5.0699406

Batch 277820, train_perplexity=154.91287, train_loss=5.042863

Batch 277830, train_perplexity=173.96921, train_loss=5.1588783

Batch 277840, train_perplexity=151.82838, train_loss=5.022751

Batch 277850, train_perplexity=155.4885, train_loss=5.0465717

Batch 277860, train_perplexity=157.87218, train_loss=5.0617857

Batch 277870, train_perplexity=152.16432, train_loss=5.024961

Batch 277880, train_perplexity=166.90956, train_loss=5.117452

Batch 277890, train_perplexity=146.79814, train_loss=4.9890585

Batch 277900, train_perplexity=147.89801, train_loss=4.996523

Batch 277910, train_perplexity=163.68866, train_loss=5.097966

Batch 277920, train_perplexity=160.20331, train_loss=5.0764437
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 277930, train_perplexity=154.03664, train_loss=5.0371904

Batch 277940, train_perplexity=178.51053, train_loss=5.1846476

Batch 277950, train_perplexity=164.09348, train_loss=5.100436

Batch 277960, train_perplexity=185.41519, train_loss=5.2225976

Batch 277970, train_perplexity=170.86258, train_loss=5.1408596

Batch 277980, train_perplexity=192.89592, train_loss=5.262151

Batch 277990, train_perplexity=150.75679, train_loss=5.015668

Batch 278000, train_perplexity=152.92407, train_loss=5.0299416

Batch 278010, train_perplexity=155.73232, train_loss=5.0481386

Batch 278020, train_perplexity=157.18022, train_loss=5.057393

Batch 278030, train_perplexity=151.70485, train_loss=5.021937

Batch 278040, train_perplexity=148.66417, train_loss=5.00169

Batch 278050, train_perplexity=156.28296, train_loss=5.051668

Batch 278060, train_perplexity=163.88586, train_loss=5.09917

Batch 278070, train_perplexity=169.54092, train_loss=5.1330943

Batch 278080, train_perplexity=151.74617, train_loss=5.022209

Batch 278090, train_perplexity=148.54485, train_loss=5.000887

Batch 278100, train_perplexity=160.32344, train_loss=5.0771933

Batch 278110, train_perplexity=157.16344, train_loss=5.0572863

Batch 278120, train_perplexity=137.99803, train_loss=4.9272394

Batch 278130, train_perplexity=162.26959, train_loss=5.089259

Batch 278140, train_perplexity=171.92339, train_loss=5.147049

Batch 278150, train_perplexity=184.90002, train_loss=5.2198153

Batch 278160, train_perplexity=169.347, train_loss=5.13195

Batch 278170, train_perplexity=153.9704, train_loss=5.0367603

Batch 278180, train_perplexity=151.97513, train_loss=5.023717

Batch 278190, train_perplexity=164.22952, train_loss=5.101265

Batch 278200, train_perplexity=161.60738, train_loss=5.08517

Batch 278210, train_perplexity=148.37283, train_loss=4.999728

Batch 278220, train_perplexity=138.59232, train_loss=4.9315367

Batch 278230, train_perplexity=172.7215, train_loss=5.1516805

Batch 278240, train_perplexity=161.28351, train_loss=5.0831637

Batch 278250, train_perplexity=153.51851, train_loss=5.033821

Batch 278260, train_perplexity=152.20474, train_loss=5.0252266

Batch 278270, train_perplexity=158.07872, train_loss=5.063093

Batch 278280, train_perplexity=162.60855, train_loss=5.091346

Batch 278290, train_perplexity=160.18428, train_loss=5.076325

Batch 278300, train_perplexity=160.2185, train_loss=5.0765386

Batch 278310, train_perplexity=137.85085, train_loss=4.9261723

Batch 278320, train_perplexity=169.62979, train_loss=5.1336184

Batch 278330, train_perplexity=156.04259, train_loss=5.050129

Batch 278340, train_perplexity=168.51595, train_loss=5.1270304

Batch 278350, train_perplexity=143.04123, train_loss=4.963133

Batch 278360, train_perplexity=155.09425, train_loss=5.044033

Batch 278370, train_perplexity=165.05455, train_loss=5.106276

Batch 278380, train_perplexity=145.30682, train_loss=4.9788475

Batch 278390, train_perplexity=146.33102, train_loss=4.9858713

Batch 278400, train_perplexity=150.97145, train_loss=5.017091

Batch 278410, train_perplexity=155.05048, train_loss=5.043751

Batch 278420, train_perplexity=163.85405, train_loss=5.098976

Batch 278430, train_perplexity=166.70682, train_loss=5.1162367

Batch 278440, train_perplexity=151.60788, train_loss=5.0212975

Batch 278450, train_perplexity=134.36671, train_loss=4.900573

Batch 278460, train_perplexity=167.3486, train_loss=5.120079

Batch 278470, train_perplexity=158.09743, train_loss=5.0632114

Batch 278480, train_perplexity=172.1562, train_loss=5.148402

Batch 278490, train_perplexity=164.3711, train_loss=5.1021266

Batch 278500, train_perplexity=166.00685, train_loss=5.112029

Batch 278510, train_perplexity=145.74954, train_loss=4.9818897

Batch 278520, train_perplexity=174.87068, train_loss=5.164047

Batch 278530, train_perplexity=165.44562, train_loss=5.1086426

Batch 278540, train_perplexity=152.61739, train_loss=5.027934

Batch 278550, train_perplexity=152.0237, train_loss=5.0240364

Batch 278560, train_perplexity=146.67863, train_loss=4.988244

Batch 278570, train_perplexity=131.9486, train_loss=4.8824124

Batch 278580, train_perplexity=168.0253, train_loss=5.1241145

Batch 278590, train_perplexity=157.10034, train_loss=5.056885

Batch 278600, train_perplexity=147.81079, train_loss=4.995933

Batch 278610, train_perplexity=138.0578, train_loss=4.9276724

Batch 278620, train_perplexity=158.49567, train_loss=5.065727

Batch 278630, train_perplexity=168.15962, train_loss=5.1249137

Batch 278640, train_perplexity=158.58759, train_loss=5.066307

Batch 278650, train_perplexity=158.67291, train_loss=5.066845

Batch 278660, train_perplexity=152.53198, train_loss=5.0273743

Batch 278670, train_perplexity=161.82082, train_loss=5.0864897

Batch 278680, train_perplexity=167.73479, train_loss=5.122384

Batch 278690, train_perplexity=159.82866, train_loss=5.0741024

Batch 278700, train_perplexity=165.92004, train_loss=5.111506

Batch 278710, train_perplexity=167.8937, train_loss=5.123331

Batch 278720, train_perplexity=173.16632, train_loss=5.1542525

Batch 278730, train_perplexity=168.96443, train_loss=5.1296883

Batch 278740, train_perplexity=157.21756, train_loss=5.0576305

Batch 278750, train_perplexity=176.3721, train_loss=5.172596

Batch 278760, train_perplexity=150.0405, train_loss=5.0109053

Batch 278770, train_perplexity=139.4829, train_loss=4.937942

Batch 278780, train_perplexity=149.97412, train_loss=5.0104628

Batch 278790, train_perplexity=151.83028, train_loss=5.0227633

Batch 278800, train_perplexity=157.88513, train_loss=5.0618677

Batch 278810, train_perplexity=148.43948, train_loss=5.0001774

Batch 278820, train_perplexity=177.95624, train_loss=5.1815376

Batch 278830, train_perplexity=161.48428, train_loss=5.084408

Batch 278840, train_perplexity=158.99829, train_loss=5.0688934

Batch 278850, train_perplexity=163.63902, train_loss=5.097663

Batch 278860, train_perplexity=147.57431, train_loss=4.994332

Batch 278870, train_perplexity=163.98271, train_loss=5.099761

Batch 278880, train_perplexity=142.93867, train_loss=4.9624157

Batch 278890, train_perplexity=142.57, train_loss=4.959833

Batch 278900, train_perplexity=168.47183, train_loss=5.1267686

Batch 278910, train_perplexity=157.36058, train_loss=5.05854

Batch 278920, train_perplexity=146.13954, train_loss=4.984562

Batch 278930, train_perplexity=161.42485, train_loss=5.0840397

Batch 278940, train_perplexity=152.48224, train_loss=5.027048

Batch 278950, train_perplexity=152.52689, train_loss=5.027341

Batch 278960, train_perplexity=156.98053, train_loss=5.056122

Batch 278970, train_perplexity=162.77736, train_loss=5.0923834

Batch 278980, train_perplexity=148.40085, train_loss=4.999917

Batch 278990, train_perplexity=145.98569, train_loss=4.9835086

Batch 279000, train_perplexity=168.02, train_loss=5.124083

Batch 279010, train_perplexity=157.2138, train_loss=5.0576067

Batch 279020, train_perplexity=154.50066, train_loss=5.0401983

Batch 279030, train_perplexity=172.24997, train_loss=5.148947

Batch 279040, train_perplexity=164.38934, train_loss=5.1022377

Batch 279050, train_perplexity=143.94028, train_loss=4.9693985

Batch 279060, train_perplexity=167.6035, train_loss=5.121601

Batch 279070, train_perplexity=147.2971, train_loss=4.9924517

Batch 279080, train_perplexity=155.56711, train_loss=5.047077

Batch 279090, train_perplexity=161.53342, train_loss=5.084712

Batch 279100, train_perplexity=160.19353, train_loss=5.0763826

Batch 279110, train_perplexity=147.96121, train_loss=4.99695

Batch 279120, train_perplexity=141.9607, train_loss=4.95555

Batch 279130, train_perplexity=153.93883, train_loss=5.0365553

Batch 279140, train_perplexity=155.43333, train_loss=5.046217

Batch 279150, train_perplexity=156.41438, train_loss=5.052509

Batch 279160, train_perplexity=165.27657, train_loss=5.1076202

Batch 279170, train_perplexity=161.14491, train_loss=5.082304

Batch 279180, train_perplexity=175.24974, train_loss=5.166212

Batch 279190, train_perplexity=161.09052, train_loss=5.0819664

Batch 279200, train_perplexity=156.0132, train_loss=5.0499406

Batch 279210, train_perplexity=153.66, train_loss=5.0347424

Batch 279220, train_perplexity=161.09036, train_loss=5.0819654
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 279230, train_perplexity=164.35, train_loss=5.1019983

Batch 279240, train_perplexity=144.68712, train_loss=4.9745736

Batch 279250, train_perplexity=148.36908, train_loss=4.999703

Batch 279260, train_perplexity=147.07553, train_loss=4.9909463

Batch 279270, train_perplexity=148.76927, train_loss=5.0023966

Batch 279280, train_perplexity=166.73193, train_loss=5.1163874

Batch 279290, train_perplexity=165.6812, train_loss=5.1100655

Batch 279300, train_perplexity=152.82407, train_loss=5.0292873

Batch 279310, train_perplexity=164.13683, train_loss=5.1007004

Batch 279320, train_perplexity=168.61803, train_loss=5.127636

Batch 279330, train_perplexity=172.57504, train_loss=5.150832

Batch 279340, train_perplexity=154.51834, train_loss=5.040313

Batch 279350, train_perplexity=146.3572, train_loss=4.98605

Batch 279360, train_perplexity=150.81273, train_loss=5.016039

Batch 279370, train_perplexity=172.28578, train_loss=5.1491547

Batch 279380, train_perplexity=162.02188, train_loss=5.0877314

Batch 279390, train_perplexity=155.58884, train_loss=5.047217

Batch 279400, train_perplexity=161.72887, train_loss=5.0859213

Batch 279410, train_perplexity=160.05212, train_loss=5.0754995

Batch 279420, train_perplexity=155.49776, train_loss=5.0466313

Batch 279430, train_perplexity=158.73398, train_loss=5.0672297

Batch 279440, train_perplexity=154.84796, train_loss=5.0424438

Batch 279450, train_perplexity=159.85703, train_loss=5.07428

Batch 279460, train_perplexity=165.20108, train_loss=5.1071634

Batch 279470, train_perplexity=172.8867, train_loss=5.1526365

Batch 279480, train_perplexity=147.90915, train_loss=4.9965982

Batch 279490, train_perplexity=156.62381, train_loss=5.053847

Batch 279500, train_perplexity=160.27437, train_loss=5.076887

Batch 279510, train_perplexity=182.66032, train_loss=5.2076283

Batch 279520, train_perplexity=146.97696, train_loss=4.990276

Batch 279530, train_perplexity=157.2981, train_loss=5.0581427

Batch 279540, train_perplexity=146.3983, train_loss=4.986331

Batch 279550, train_perplexity=146.40277, train_loss=4.9863615

Batch 279560, train_perplexity=167.88202, train_loss=5.1232615

Batch 279570, train_perplexity=156.88596, train_loss=5.055519

Batch 279580, train_perplexity=152.41202, train_loss=5.0265875

Batch 279590, train_perplexity=151.38957, train_loss=5.0198565

Batch 279600, train_perplexity=158.42244, train_loss=5.065265

Batch 279610, train_perplexity=170.1171, train_loss=5.136487

Batch 279620, train_perplexity=155.29332, train_loss=5.0453157

Batch 279630, train_perplexity=159.67868, train_loss=5.0731635

Batch 279640, train_perplexity=156.58626, train_loss=5.053607

Batch 279650, train_perplexity=148.6551, train_loss=5.001629

Batch 279660, train_perplexity=156.93338, train_loss=5.0558214

Batch 279670, train_perplexity=140.50523, train_loss=4.945245

Batch 279680, train_perplexity=158.61807, train_loss=5.066499

Batch 279690, train_perplexity=149.6825, train_loss=5.0085163

Batch 279700, train_perplexity=192.062, train_loss=5.257818

Batch 279710, train_perplexity=161.28836, train_loss=5.083194

Batch 279720, train_perplexity=163.15253, train_loss=5.0946856

Batch 279730, train_perplexity=151.74385, train_loss=5.022194

Batch 279740, train_perplexity=177.58554, train_loss=5.1794524

Batch 279750, train_perplexity=150.26009, train_loss=5.0123677

Batch 279760, train_perplexity=158.67284, train_loss=5.0668445

Batch 279770, train_perplexity=155.25275, train_loss=5.0450544

Batch 279780, train_perplexity=146.1946, train_loss=4.9849386

Batch 279790, train_perplexity=142.92456, train_loss=4.962317

Batch 279800, train_perplexity=164.60788, train_loss=5.103566

Batch 279810, train_perplexity=150.11264, train_loss=5.011386

Batch 279820, train_perplexity=147.571, train_loss=4.9943094

Batch 279830, train_perplexity=156.52548, train_loss=5.053219

Batch 279840, train_perplexity=166.17444, train_loss=5.113038

Batch 279850, train_perplexity=155.67255, train_loss=5.047755

Batch 279860, train_perplexity=146.20018, train_loss=4.984977

Batch 279870, train_perplexity=162.607, train_loss=5.0913363

Batch 279880, train_perplexity=136.52734, train_loss=4.916525

Batch 279890, train_perplexity=158.08815, train_loss=5.063153

Batch 279900, train_perplexity=180.2044, train_loss=5.194092

Batch 279910, train_perplexity=166.06766, train_loss=5.1123953

Batch 279920, train_perplexity=151.00357, train_loss=5.0173035

Batch 279930, train_perplexity=160.35669, train_loss=5.0774007

Batch 279940, train_perplexity=162.12883, train_loss=5.0883913

Batch 279950, train_perplexity=140.33717, train_loss=4.944048

Batch 279960, train_perplexity=174.71416, train_loss=5.1631513

Batch 279970, train_perplexity=167.02605, train_loss=5.1181498

Batch 279980, train_perplexity=161.36713, train_loss=5.083682

Batch 279990, train_perplexity=153.65517, train_loss=5.034711

Batch 280000, train_perplexity=151.13078, train_loss=5.0181456

Batch 280010, train_perplexity=157.82355, train_loss=5.0614777

Batch 280020, train_perplexity=136.24931, train_loss=4.9144864

Batch 280030, train_perplexity=152.3966, train_loss=5.0264864

Batch 280040, train_perplexity=138.46945, train_loss=4.9306498

Batch 280050, train_perplexity=160.66008, train_loss=5.079291

Batch 280060, train_perplexity=178.23291, train_loss=5.183091

Batch 280070, train_perplexity=179.6883, train_loss=5.1912236

Batch 280080, train_perplexity=159.48578, train_loss=5.0719547

Batch 280090, train_perplexity=152.22246, train_loss=5.025343

Batch 280100, train_perplexity=170.09496, train_loss=5.136357

Batch 280110, train_perplexity=150.7527, train_loss=5.0156407

Batch 280120, train_perplexity=148.37778, train_loss=4.9997616

Batch 280130, train_perplexity=157.8529, train_loss=5.0616636

Batch 280140, train_perplexity=176.34451, train_loss=5.1724396

Batch 280150, train_perplexity=168.07794, train_loss=5.124428

Batch 280160, train_perplexity=149.42462, train_loss=5.006792

Batch 280170, train_perplexity=162.12675, train_loss=5.0883784

Batch 280180, train_perplexity=186.52312, train_loss=5.228555

Batch 280190, train_perplexity=177.8524, train_loss=5.180954

Batch 280200, train_perplexity=153.49991, train_loss=5.0337

Batch 280210, train_perplexity=134.46414, train_loss=4.9012976

Batch 280220, train_perplexity=158.56195, train_loss=5.0661454

Batch 280230, train_perplexity=140.63783, train_loss=4.946188

Batch 280240, train_perplexity=150.23903, train_loss=5.0122275

Batch 280250, train_perplexity=145.38084, train_loss=4.979357

Batch 280260, train_perplexity=150.77002, train_loss=5.0157557

Batch 280270, train_perplexity=155.52705, train_loss=5.0468197

Batch 280280, train_perplexity=154.22581, train_loss=5.038418

Batch 280290, train_perplexity=154.56064, train_loss=5.0405865

Batch 280300, train_perplexity=161.01141, train_loss=5.0814753

Batch 280310, train_perplexity=157.60892, train_loss=5.060117

Batch 280320, train_perplexity=168.3521, train_loss=5.1260576

Batch 280330, train_perplexity=148.6573, train_loss=5.0016437

Batch 280340, train_perplexity=179.96329, train_loss=5.192753

Batch 280350, train_perplexity=148.21472, train_loss=4.998662

Batch 280360, train_perplexity=175.1581, train_loss=5.165689

Batch 280370, train_perplexity=143.28603, train_loss=4.964843

Batch 280380, train_perplexity=165.67433, train_loss=5.110024

Batch 280390, train_perplexity=169.73343, train_loss=5.134229

Batch 280400, train_perplexity=164.20094, train_loss=5.101091

Batch 280410, train_perplexity=156.02257, train_loss=5.0500007

Batch 280420, train_perplexity=168.85982, train_loss=5.129069

Batch 280430, train_perplexity=159.36201, train_loss=5.0711784

Batch 280440, train_perplexity=158.31711, train_loss=5.0646

Batch 280450, train_perplexity=158.94917, train_loss=5.0685844

Batch 280460, train_perplexity=167.6908, train_loss=5.122122

Batch 280470, train_perplexity=163.2221, train_loss=5.095112

Batch 280480, train_perplexity=167.61134, train_loss=5.121648

Batch 280490, train_perplexity=171.99686, train_loss=5.147476

Batch 280500, train_perplexity=162.51833, train_loss=5.0907907

Batch 280510, train_perplexity=134.703, train_loss=4.9030724

Batch 280520, train_perplexity=150.07013, train_loss=5.0111027
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 280530, train_perplexity=163.0274, train_loss=5.0939183

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Batch 280540, train_perplexity=186.30241, train_loss=5.227371

Batch 280550, train_perplexity=149.81609, train_loss=5.0094085

Batch 280560, train_perplexity=160.0406, train_loss=5.0754275

Batch 280570, train_perplexity=182.29721, train_loss=5.2056384

Batch 280580, train_perplexity=164.24283, train_loss=5.101346

Batch 280590, train_perplexity=166.40057, train_loss=5.114398

Batch 280600, train_perplexity=146.68716, train_loss=4.988302

Batch 280610, train_perplexity=154.4991, train_loss=5.0401883

Batch 280620, train_perplexity=156.36964, train_loss=5.0522227

Batch 280630, train_perplexity=146.65765, train_loss=4.988101

Batch 280640, train_perplexity=154.1639, train_loss=5.0380163

Batch 280650, train_perplexity=172.82324, train_loss=5.1522694

Batch 280660, train_perplexity=143.63216, train_loss=4.9672556

Batch 280670, train_perplexity=152.56937, train_loss=5.0276194

Batch 280680, train_perplexity=168.5571, train_loss=5.1272745

Batch 280690, train_perplexity=166.29778, train_loss=5.11378

Batch 280700, train_perplexity=147.12877, train_loss=4.991308

Batch 280710, train_perplexity=142.52039, train_loss=4.959485

Batch 280720, train_perplexity=150.24841, train_loss=5.01229

Batch 280730, train_perplexity=154.79495, train_loss=5.0421014

Batch 280740, train_perplexity=156.97813, train_loss=5.0561066

Batch 280750, train_perplexity=152.8484, train_loss=5.0294466

Batch 280760, train_perplexity=139.41249, train_loss=4.937437

Batch 280770, train_perplexity=155.03392, train_loss=5.043644

Batch 280780, train_perplexity=155.56013, train_loss=5.0470324

Batch 280790, train_perplexity=167.41771, train_loss=5.120492

Batch 280800, train_perplexity=158.6192, train_loss=5.0665064

Batch 280810, train_perplexity=157.26892, train_loss=5.057957

Batch 280820, train_perplexity=184.59988, train_loss=5.2181907

Batch 280830, train_perplexity=149.90706, train_loss=5.0100155

Batch 280840, train_perplexity=134.257, train_loss=4.899756

Batch 280850, train_perplexity=159.92397, train_loss=5.0746984

Batch 280860, train_perplexity=167.7303, train_loss=5.1223574

Batch 280870, train_perplexity=166.34695, train_loss=5.1140757

Batch 280880, train_perplexity=159.8468, train_loss=5.074216

Batch 280890, train_perplexity=147.40967, train_loss=4.9932156

Batch 280900, train_perplexity=142.2735, train_loss=4.9577513

Batch 280910, train_perplexity=142.78813, train_loss=4.961362

Batch 280920, train_perplexity=162.53862, train_loss=5.0909157

Batch 280930, train_perplexity=161.6123, train_loss=5.0852003

Batch 280940, train_perplexity=160.65741, train_loss=5.079274

Batch 280950, train_perplexity=130.51816, train_loss=4.8715124

Batch 280960, train_perplexity=161.34966, train_loss=5.083574

Batch 280970, train_perplexity=140.74423, train_loss=4.946944

Batch 280980, train_perplexity=145.71175, train_loss=4.9816303

Batch 280990, train_perplexity=162.57863, train_loss=5.0911617

Batch 281000, train_perplexity=153.55518, train_loss=5.03406

Batch 281010, train_perplexity=162.4754, train_loss=5.0905266

Batch 281020, train_perplexity=142.06058, train_loss=4.9562535

Batch 281030, train_perplexity=175.58667, train_loss=5.168133

Batch 281040, train_perplexity=162.80562, train_loss=5.092557

Batch 281050, train_perplexity=167.45396, train_loss=5.1207085

Batch 281060, train_perplexity=164.11171, train_loss=5.1005473

Batch 281070, train_perplexity=187.01231, train_loss=5.2311745

Batch 281080, train_perplexity=155.48152, train_loss=5.046527

Batch 281090, train_perplexity=166.06133, train_loss=5.112357

Batch 281100, train_perplexity=162.7928, train_loss=5.0924783

Batch 281110, train_perplexity=153.5081, train_loss=5.0337534

Batch 281120, train_perplexity=168.55789, train_loss=5.1272793

Batch 281130, train_perplexity=157.21216, train_loss=5.057596

Batch 281140, train_perplexity=182.64882, train_loss=5.2075653

Batch 281150, train_perplexity=137.39961, train_loss=4.9228935

Batch 281160, train_perplexity=149.4634, train_loss=5.0070515

Batch 281170, train_perplexity=154.38695, train_loss=5.039462

Batch 281180, train_perplexity=164.1412, train_loss=5.100727

Batch 281190, train_perplexity=162.48238, train_loss=5.0905695

Batch 281200, train_perplexity=154.53558, train_loss=5.0404243

Batch 281210, train_perplexity=160.20087, train_loss=5.0764284

Batch 281220, train_perplexity=165.5768, train_loss=5.109435

Batch 281230, train_perplexity=160.43631, train_loss=5.077897

Batch 281240, train_perplexity=163.59955, train_loss=5.0974216

Batch 281250, train_perplexity=169.36324, train_loss=5.1320457

Batch 281260, train_perplexity=161.14299, train_loss=5.082292

Batch 281270, train_perplexity=157.6632, train_loss=5.060461

Batch 281280, train_perplexity=160.65396, train_loss=5.0792527

Batch 281290, train_perplexity=150.71783, train_loss=5.0154095

Batch 281300, train_perplexity=149.604, train_loss=5.007992

Batch 281310, train_perplexity=144.09879, train_loss=4.970499

Batch 281320, train_perplexity=173.12115, train_loss=5.1539917

Batch 281330, train_perplexity=182.68314, train_loss=5.207753

Batch 281340, train_perplexity=162.7502, train_loss=5.0922165

Batch 281350, train_perplexity=165.27357, train_loss=5.107602

Batch 281360, train_perplexity=147.31664, train_loss=4.992584

Batch 281370, train_perplexity=162.27486, train_loss=5.0892916

Batch 281380, train_perplexity=163.61101, train_loss=5.0974917

Batch 281390, train_perplexity=157.72493, train_loss=5.0608525

Batch 281400, train_perplexity=145.97058, train_loss=4.983405

Batch 281410, train_perplexity=156.16629, train_loss=5.0509214

Batch 281420, train_perplexity=149.2419, train_loss=5.0055685

Batch 281430, train_perplexity=169.5355, train_loss=5.1330624

Batch 281440, train_perplexity=168.51804, train_loss=5.127043

Batch 281450, train_perplexity=162.61165, train_loss=5.091365

Batch 281460, train_perplexity=156.11984, train_loss=5.050624

Batch 281470, train_perplexity=160.86407, train_loss=5.0805597

Batch 281480, train_perplexity=156.61351, train_loss=5.053781

Batch 281490, train_perplexity=165.35019, train_loss=5.1080656

Batch 281500, train_perplexity=165.26924, train_loss=5.107576

Batch 281510, train_perplexity=153.38423, train_loss=5.032946

Batch 281520, train_perplexity=167.07663, train_loss=5.1184525

Batch 281530, train_perplexity=154.90335, train_loss=5.0428014

Batch 281540, train_perplexity=171.32695, train_loss=5.1435738

Batch 281550, train_perplexity=156.46355, train_loss=5.052823

Batch 281560, train_perplexity=157.15579, train_loss=5.0572376

Batch 281570, train_perplexity=157.54956, train_loss=5.05974

Batch 281580, train_perplexity=155.81752, train_loss=5.0486856

Batch 281590, train_perplexity=149.12695, train_loss=5.004798

Batch 281600, train_perplexity=167.7863, train_loss=5.122691

Batch 281610, train_perplexity=160.21278, train_loss=5.076503

Batch 281620, train_perplexity=171.7858, train_loss=5.1462483

Batch 281630, train_perplexity=143.95764, train_loss=4.969519

Batch 281640, train_perplexity=149.77173, train_loss=5.0091124

Batch 281650, train_perplexity=176.07858, train_loss=5.1709304

Batch 281660, train_perplexity=139.55441, train_loss=4.9384546

Batch 281670, train_perplexity=155.24446, train_loss=5.045001

Batch 281680, train_perplexity=154.52702, train_loss=5.040369

Batch 281690, train_perplexity=168.09854, train_loss=5.1245503

Batch 281700, train_perplexity=154.13332, train_loss=5.037818

Batch 281710, train_perplexity=143.80252, train_loss=4.968441

Batch 281720, train_perplexity=160.55003, train_loss=5.0786057

Batch 281730, train_perplexity=153.24855, train_loss=5.032061

Batch 281740, train_perplexity=167.55476, train_loss=5.12131

Batch 281750, train_perplexity=154.73482, train_loss=5.0417128

Batch 281760, train_perplexity=158.7772, train_loss=5.067502
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 281770, train_perplexity=147.64561, train_loss=4.994815

Batch 281780, train_perplexity=158.03938, train_loss=5.0628443

Batch 281790, train_perplexity=171.7813, train_loss=5.146222

Batch 281800, train_perplexity=171.186, train_loss=5.1427507

Batch 281810, train_perplexity=171.01183, train_loss=5.1417327

Batch 281820, train_perplexity=162.04645, train_loss=5.087883

Batch 281830, train_perplexity=161.14221, train_loss=5.0822873

Batch 281840, train_perplexity=144.24252, train_loss=4.971496

Batch 281850, train_perplexity=173.2276, train_loss=5.1546063

Batch 281860, train_perplexity=165.30998, train_loss=5.1078224

Batch 281870, train_perplexity=174.16493, train_loss=5.1600027

Batch 281880, train_perplexity=137.47694, train_loss=4.923456

Batch 281890, train_perplexity=163.07834, train_loss=5.0942307

Batch 281900, train_perplexity=163.445, train_loss=5.0964766

Batch 281910, train_perplexity=152.99577, train_loss=5.0304103

Batch 281920, train_perplexity=157.59923, train_loss=5.0600553

Batch 281930, train_perplexity=144.72362, train_loss=4.974826

Batch 281940, train_perplexity=177.89871, train_loss=5.1812143

Batch 281950, train_perplexity=156.46295, train_loss=5.0528193

Batch 281960, train_perplexity=146.96806, train_loss=4.9902153

Batch 281970, train_perplexity=156.2016, train_loss=5.0511475

Batch 281980, train_perplexity=142.93901, train_loss=4.962418

Batch 281990, train_perplexity=144.32358, train_loss=4.972058

Batch 282000, train_perplexity=160.79375, train_loss=5.0801225

Batch 282010, train_perplexity=154.3313, train_loss=5.0391016

Batch 282020, train_perplexity=158.21326, train_loss=5.063944

Batch 282030, train_perplexity=167.86264, train_loss=5.123146

Batch 282040, train_perplexity=141.34993, train_loss=4.9512386

Batch 282050, train_perplexity=143.87317, train_loss=4.968932

Batch 282060, train_perplexity=176.82295, train_loss=5.175149

Batch 282070, train_perplexity=156.38388, train_loss=5.052314

Batch 282080, train_perplexity=177.17879, train_loss=5.1771593

Batch 282090, train_perplexity=158.13603, train_loss=5.0634556

Batch 282100, train_perplexity=166.43692, train_loss=5.1146164

Batch 282110, train_perplexity=154.40977, train_loss=5.03961

Batch 282120, train_perplexity=154.23228, train_loss=5.03846

Batch 282130, train_perplexity=140.45755, train_loss=4.9449053

Batch 282140, train_perplexity=151.59544, train_loss=5.0212154

Batch 282150, train_perplexity=177.39334, train_loss=5.1783695

Batch 282160, train_perplexity=155.89473, train_loss=5.049181

Batch 282170, train_perplexity=159.40169, train_loss=5.0714273

Batch 282180, train_perplexity=156.91528, train_loss=5.055706

Batch 282190, train_perplexity=161.2869, train_loss=5.0831847

Batch 282200, train_perplexity=128.53462, train_loss=4.8561983

Batch 282210, train_perplexity=162.04459, train_loss=5.0878716

Batch 282220, train_perplexity=160.01755, train_loss=5.0752835

Batch 282230, train_perplexity=161.14629, train_loss=5.0823126

Batch 282240, train_perplexity=152.9084, train_loss=5.029839

Batch 282250, train_perplexity=148.68445, train_loss=5.0018263

Batch 282260, train_perplexity=158.28253, train_loss=5.0643816

Batch 282270, train_perplexity=161.09221, train_loss=5.081977

Batch 282280, train_perplexity=162.80344, train_loss=5.0925436

Batch 282290, train_perplexity=149.8599, train_loss=5.009701

Batch 282300, train_perplexity=160.41711, train_loss=5.0777774

Batch 282310, train_perplexity=159.17458, train_loss=5.0700016

Batch 282320, train_perplexity=149.66637, train_loss=5.0084085

Batch 282330, train_perplexity=185.63565, train_loss=5.223786

Batch 282340, train_perplexity=158.41194, train_loss=5.065199

Batch 282350, train_perplexity=148.97038, train_loss=5.0037475

Batch 282360, train_perplexity=160.86661, train_loss=5.0805755

Batch 282370, train_perplexity=146.94725, train_loss=4.9900737

Batch 282380, train_perplexity=165.57245, train_loss=5.109409

Batch 282390, train_perplexity=150.99248, train_loss=5.01723

Batch 282400, train_perplexity=159.68773, train_loss=5.0732203

Batch 282410, train_perplexity=170.36436, train_loss=5.1379395

Batch 282420, train_perplexity=165.51309, train_loss=5.1090503

Batch 282430, train_perplexity=159.2739, train_loss=5.0706253

Batch 282440, train_perplexity=155.85362, train_loss=5.0489173

Batch 282450, train_perplexity=152.86429, train_loss=5.0295506

Batch 282460, train_perplexity=153.14328, train_loss=5.031374

Batch 282470, train_perplexity=154.6671, train_loss=5.041275

Batch 282480, train_perplexity=154.8521, train_loss=5.0424705

Batch 282490, train_perplexity=153.07758, train_loss=5.030945

Batch 282500, train_perplexity=176.30534, train_loss=5.1722174

Batch 282510, train_perplexity=169.41461, train_loss=5.132349

Batch 282520, train_perplexity=171.55872, train_loss=5.1449256

Batch 282530, train_perplexity=163.77048, train_loss=5.098466

Batch 282540, train_perplexity=165.62212, train_loss=5.109709

Batch 282550, train_perplexity=158.1701, train_loss=5.063671

Batch 282560, train_perplexity=151.14418, train_loss=5.0182343

Batch 282570, train_perplexity=141.4531, train_loss=4.951968

Batch 282580, train_perplexity=154.31578, train_loss=5.039001

Batch 282590, train_perplexity=149.989, train_loss=5.010562

Batch 282600, train_perplexity=174.04745, train_loss=5.159328

Batch 282610, train_perplexity=179.37813, train_loss=5.189496

Batch 282620, train_perplexity=157.87051, train_loss=5.061775

Batch 282630, train_perplexity=171.6845, train_loss=5.1456585

Batch 282640, train_perplexity=163.49722, train_loss=5.096796

Batch 282650, train_perplexity=173.45168, train_loss=5.155899

Batch 282660, train_perplexity=147.41438, train_loss=4.9932475

Batch 282670, train_perplexity=166.7434, train_loss=5.116456

Batch 282680, train_perplexity=156.32797, train_loss=5.051956

Batch 282690, train_perplexity=142.46916, train_loss=4.9591255

Batch 282700, train_perplexity=168.96202, train_loss=5.129674

Batch 282710, train_perplexity=168.63217, train_loss=5.12772

Batch 282720, train_perplexity=181.77103, train_loss=5.202748

Batch 282730, train_perplexity=142.69788, train_loss=4.9607296

Batch 282740, train_perplexity=163.63747, train_loss=5.0976534

Batch 282750, train_perplexity=159.50069, train_loss=5.072048

Batch 282760, train_perplexity=155.75772, train_loss=5.0483017

Batch 282770, train_perplexity=144.08449, train_loss=4.9704

Batch 282780, train_perplexity=159.55058, train_loss=5.072361

Batch 282790, train_perplexity=159.60475, train_loss=5.0727005

Batch 282800, train_perplexity=151.06651, train_loss=5.01772

Batch 282810, train_perplexity=160.2905, train_loss=5.0769877

Batch 282820, train_perplexity=151.71202, train_loss=5.021984

Batch 282830, train_perplexity=167.69543, train_loss=5.1221495

Batch 282840, train_perplexity=163.72987, train_loss=5.098218

Batch 282850, train_perplexity=165.0632, train_loss=5.1063285

Batch 282860, train_perplexity=153.63245, train_loss=5.034563

Batch 282870, train_perplexity=161.04565, train_loss=5.081688

Batch 282880, train_perplexity=159.34605, train_loss=5.0710783

Batch 282890, train_perplexity=178.63603, train_loss=5.1853504

Batch 282900, train_perplexity=152.85802, train_loss=5.0295095

Batch 282910, train_perplexity=153.5352, train_loss=5.03393

Batch 282920, train_perplexity=159.1446, train_loss=5.0698133

Batch 282930, train_perplexity=146.94368, train_loss=4.9900494

Batch 282940, train_perplexity=160.43356, train_loss=5.07788

Batch 282950, train_perplexity=165.73462, train_loss=5.110388

Batch 282960, train_perplexity=154.03868, train_loss=5.037204

Batch 282970, train_perplexity=143.05241, train_loss=4.963211

Batch 282980, train_perplexity=148.89247, train_loss=5.0032244

Batch 282990, train_perplexity=138.8938, train_loss=4.9337096

Batch 283000, train_perplexity=165.5974, train_loss=5.1095595

Batch 283010, train_perplexity=161.90834, train_loss=5.0870304

Batch 283020, train_perplexity=162.3566, train_loss=5.089795

Batch 283030, train_perplexity=159.48349, train_loss=5.0719404

Batch 283040, train_perplexity=161.82506, train_loss=5.086516

Batch 283050, train_perplexity=139.68665, train_loss=4.9394016

Batch 283060, train_perplexity=171.74248, train_loss=5.145996
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 283070, train_perplexity=168.07353, train_loss=5.1244016

Batch 283080, train_perplexity=163.53513, train_loss=5.097028

Batch 283090, train_perplexity=157.9419, train_loss=5.0622272

Batch 283100, train_perplexity=157.47595, train_loss=5.059273

Batch 283110, train_perplexity=155.95496, train_loss=5.049567

Batch 283120, train_perplexity=157.34355, train_loss=5.0584316

Batch 283130, train_perplexity=145.3642, train_loss=4.9792423

Batch 283140, train_perplexity=134.21048, train_loss=4.8994093

Batch 283150, train_perplexity=159.21754, train_loss=5.0702715

Batch 283160, train_perplexity=145.07552, train_loss=4.9772544

Batch 283170, train_perplexity=159.17079, train_loss=5.0699778

Batch 283180, train_perplexity=168.01352, train_loss=5.1240444

Batch 283190, train_perplexity=150.00717, train_loss=5.010683

Batch 283200, train_perplexity=161.30159, train_loss=5.083276

Batch 283210, train_perplexity=151.73604, train_loss=5.0221424

Batch 283220, train_perplexity=180.61855, train_loss=5.1963873

Batch 283230, train_perplexity=141.92834, train_loss=4.9553223

Batch 283240, train_perplexity=158.92969, train_loss=5.068462

Batch 283250, train_perplexity=166.75897, train_loss=5.1165495

Batch 283260, train_perplexity=159.79346, train_loss=5.073882

Batch 283270, train_perplexity=153.08531, train_loss=5.0309954

Batch 283280, train_perplexity=155.01352, train_loss=5.0435123

Batch 283290, train_perplexity=144.70471, train_loss=4.974695

Batch 283300, train_perplexity=169.37439, train_loss=5.1321115

Batch 283310, train_perplexity=149.13235, train_loss=5.004834

Batch 283320, train_perplexity=166.45915, train_loss=5.11475

Batch 283330, train_perplexity=148.03905, train_loss=4.997476

Batch 283340, train_perplexity=141.17366, train_loss=4.9499907

Batch 283350, train_perplexity=156.63405, train_loss=5.053912

Batch 283360, train_perplexity=166.35075, train_loss=5.1140985

Batch 283370, train_perplexity=174.91388, train_loss=5.164294

Batch 283380, train_perplexity=140.09662, train_loss=4.9423323

Batch 283390, train_perplexity=149.49875, train_loss=5.007288

Batch 283400, train_perplexity=151.37527, train_loss=5.019762

Batch 283410, train_perplexity=163.88078, train_loss=5.099139

Batch 283420, train_perplexity=146.63632, train_loss=4.9879556

Batch 283430, train_perplexity=143.9989, train_loss=4.9698057

Batch 283440, train_perplexity=150.316, train_loss=5.0127397

Batch 283450, train_perplexity=151.24771, train_loss=5.018919

Batch 283460, train_perplexity=149.4299, train_loss=5.0068274

Batch 283470, train_perplexity=165.91696, train_loss=5.1114874

Batch 283480, train_perplexity=146.685, train_loss=4.9882874

Batch 283490, train_perplexity=162.95178, train_loss=5.0934544

Batch 283500, train_perplexity=151.23308, train_loss=5.018822

Batch 283510, train_perplexity=145.77582, train_loss=4.98207

Batch 283520, train_perplexity=142.26997, train_loss=4.9577265

Batch 283530, train_perplexity=186.66486, train_loss=5.229315

Batch 283540, train_perplexity=166.34274, train_loss=5.1140504

Batch 283550, train_perplexity=158.57368, train_loss=5.0662193

Batch 283560, train_perplexity=179.69823, train_loss=5.191279

Batch 283570, train_perplexity=151.5909, train_loss=5.0211854

Batch 283580, train_perplexity=155.73239, train_loss=5.048139

Batch 283590, train_perplexity=136.10834, train_loss=4.913451

Batch 283600, train_perplexity=150.7926, train_loss=5.0159054

Batch 283610, train_perplexity=157.68965, train_loss=5.060629

Batch 283620, train_perplexity=135.45894, train_loss=4.9086685

Batch 283630, train_perplexity=157.58284, train_loss=5.0599513

Batch 283640, train_perplexity=193.17915, train_loss=5.263618

Batch 283650, train_perplexity=142.16425, train_loss=4.956983

Batch 283660, train_perplexity=153.41298, train_loss=5.0331335

Batch 283670, train_perplexity=158.89854, train_loss=5.068266

Batch 283680, train_perplexity=167.19913, train_loss=5.1191854

Batch 283690, train_perplexity=159.11554, train_loss=5.0696306

Batch 283700, train_perplexity=136.77156, train_loss=4.918312

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Batch 283710, train_perplexity=141.81468, train_loss=4.954521

Batch 283720, train_perplexity=145.30565, train_loss=4.9788394

Batch 283730, train_perplexity=163.63474, train_loss=5.0976367

Batch 283740, train_perplexity=160.9256, train_loss=5.080942

Batch 283750, train_perplexity=138.74995, train_loss=4.9326735

Batch 283760, train_perplexity=149.90813, train_loss=5.0100226

Batch 283770, train_perplexity=152.51424, train_loss=5.027258

Batch 283780, train_perplexity=157.66936, train_loss=5.0605

Batch 283790, train_perplexity=155.94046, train_loss=5.0494742

Batch 283800, train_perplexity=161.38483, train_loss=5.0837917

Batch 283810, train_perplexity=159.03098, train_loss=5.069099

Batch 283820, train_perplexity=160.78247, train_loss=5.0800524

Batch 283830, train_perplexity=168.41168, train_loss=5.1264114

Batch 283840, train_perplexity=135.81918, train_loss=4.9113245

Batch 283850, train_perplexity=152.493, train_loss=5.0271187

Batch 283860, train_perplexity=149.77473, train_loss=5.0091324

Batch 283870, train_perplexity=143.72986, train_loss=4.9679356

Batch 283880, train_perplexity=156.05405, train_loss=5.0502024

Batch 283890, train_perplexity=168.81279, train_loss=5.1287904

Batch 283900, train_perplexity=147.3318, train_loss=4.992687

Batch 283910, train_perplexity=146.47498, train_loss=4.9868546

Batch 283920, train_perplexity=166.03471, train_loss=5.112197

Batch 283930, train_perplexity=161.17073, train_loss=5.082464

Batch 283940, train_perplexity=157.27829, train_loss=5.058017

Batch 283950, train_perplexity=150.69211, train_loss=5.015239

Batch 283960, train_perplexity=154.72699, train_loss=5.041662

Batch 283970, train_perplexity=154.89397, train_loss=5.042741

Batch 283980, train_perplexity=163.9344, train_loss=5.0994663

Batch 283990, train_perplexity=157.34865, train_loss=5.058464

Batch 284000, train_perplexity=186.14719, train_loss=5.2265377

Batch 284010, train_perplexity=156.6847, train_loss=5.0542355

Batch 284020, train_perplexity=159.26782, train_loss=5.070587

Batch 284030, train_perplexity=175.9365, train_loss=5.170123

Batch 284040, train_perplexity=171.3098, train_loss=5.1434736

Batch 284050, train_perplexity=156.64958, train_loss=5.0540113

Batch 284060, train_perplexity=137.89595, train_loss=4.9264994

Batch 284070, train_perplexity=155.60739, train_loss=5.047336

Batch 284080, train_perplexity=153.31038, train_loss=5.0324645

Batch 284090, train_perplexity=163.26367, train_loss=5.0953665

Batch 284100, train_perplexity=148.11862, train_loss=4.9980135

Batch 284110, train_perplexity=161.827, train_loss=5.086528

Batch 284120, train_perplexity=159.97224, train_loss=5.0750003

Batch 284130, train_perplexity=170.44627, train_loss=5.13842

Batch 284140, train_perplexity=157.57976, train_loss=5.0599318

Batch 284150, train_perplexity=146.10234, train_loss=4.9843073

Batch 284160, train_perplexity=156.40492, train_loss=5.0524483

Batch 284170, train_perplexity=147.57431, train_loss=4.994332

Batch 284180, train_perplexity=159.20343, train_loss=5.070183

Batch 284190, train_perplexity=157.4246, train_loss=5.0589466

Batch 284200, train_perplexity=137.29718, train_loss=4.9221478

Batch 284210, train_perplexity=151.78242, train_loss=5.022448

Batch 284220, train_perplexity=154.76906, train_loss=5.041934

Batch 284230, train_perplexity=162.19348, train_loss=5.08879

Batch 284240, train_perplexity=138.60765, train_loss=4.9316473

Batch 284250, train_perplexity=150.1962, train_loss=5.0119424

Batch 284260, train_perplexity=156.4898, train_loss=5.052991

Batch 284270, train_perplexity=157.20691, train_loss=5.057563

Batch 284280, train_perplexity=140.27924, train_loss=4.943635

Batch 284290, train_perplexity=156.356, train_loss=5.0521355

Batch 284300, train_perplexity=162.83559, train_loss=5.092741
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 284310, train_perplexity=145.92737, train_loss=4.983109

Batch 284320, train_perplexity=161.23384, train_loss=5.0828557

Batch 284330, train_perplexity=154.14655, train_loss=5.037904

Batch 284340, train_perplexity=146.33702, train_loss=4.9859123

Batch 284350, train_perplexity=145.47057, train_loss=4.979974

Batch 284360, train_perplexity=160.22272, train_loss=5.076565

Batch 284370, train_perplexity=148.13898, train_loss=4.998151

Batch 284380, train_perplexity=158.74586, train_loss=5.0673046

Batch 284390, train_perplexity=169.41704, train_loss=5.1323633

Batch 284400, train_perplexity=143.27188, train_loss=4.964744

Batch 284410, train_perplexity=159.51802, train_loss=5.072157

Batch 284420, train_perplexity=166.49463, train_loss=5.114963

Batch 284430, train_perplexity=146.25136, train_loss=4.985327

Batch 284440, train_perplexity=153.06297, train_loss=5.0308495

Batch 284450, train_perplexity=152.46646, train_loss=5.0269446

Batch 284460, train_perplexity=157.53242, train_loss=5.0596313

Batch 284470, train_perplexity=160.0525, train_loss=5.075502

Batch 284480, train_perplexity=175.59856, train_loss=5.1682005

Batch 284490, train_perplexity=152.27545, train_loss=5.025691

Batch 284500, train_perplexity=155.87376, train_loss=5.0490465

Batch 284510, train_perplexity=170.82999, train_loss=5.140669

Batch 284520, train_perplexity=151.12602, train_loss=5.018114

Batch 284530, train_perplexity=147.55693, train_loss=4.994214

Batch 284540, train_perplexity=160.98776, train_loss=5.0813284

Batch 284550, train_perplexity=141.08301, train_loss=4.9493484

Batch 284560, train_perplexity=155.84344, train_loss=5.048852

Batch 284570, train_perplexity=168.72371, train_loss=5.1282625

Batch 284580, train_perplexity=181.18466, train_loss=5.199517

Batch 284590, train_perplexity=170.9012, train_loss=5.1410856

Batch 284600, train_perplexity=139.7582, train_loss=4.9399137

Batch 284610, train_perplexity=166.35313, train_loss=5.114113

Batch 284620, train_perplexity=151.50706, train_loss=5.0206323

Batch 284630, train_perplexity=160.42224, train_loss=5.0778093

Batch 284640, train_perplexity=140.80042, train_loss=4.9473433

Batch 284650, train_perplexity=170.1343, train_loss=5.136588

Batch 284660, train_perplexity=155.51186, train_loss=5.046722

Batch 284670, train_perplexity=148.7399, train_loss=5.002199

Batch 284680, train_perplexity=156.31664, train_loss=5.0518837

Batch 284690, train_perplexity=157.03174, train_loss=5.056448

Batch 284700, train_perplexity=160.04343, train_loss=5.075445

Batch 284710, train_perplexity=165.44089, train_loss=5.108614

Batch 284720, train_perplexity=156.5422, train_loss=5.0533257

Batch 284730, train_perplexity=138.04779, train_loss=4.9276

Batch 284740, train_perplexity=150.07793, train_loss=5.0111547

Batch 284750, train_perplexity=152.41681, train_loss=5.026619

Batch 284760, train_perplexity=161.04727, train_loss=5.081698

Batch 284770, train_perplexity=157.9404, train_loss=5.0622177

Batch 284780, train_perplexity=167.74638, train_loss=5.122453

Batch 284790, train_perplexity=177.35071, train_loss=5.178129

Batch 284800, train_perplexity=140.24767, train_loss=4.94341

Batch 284810, train_perplexity=170.87595, train_loss=5.140938

Batch 284820, train_perplexity=151.60933, train_loss=5.021307

Batch 284830, train_perplexity=159.15804, train_loss=5.0698977

Batch 284840, train_perplexity=166.68933, train_loss=5.116132

Batch 284850, train_perplexity=143.98373, train_loss=4.9697003

Batch 284860, train_perplexity=176.71034, train_loss=5.174512

Batch 284870, train_perplexity=159.22392, train_loss=5.0703115

Batch 284880, train_perplexity=173.84631, train_loss=5.1581717

Batch 284890, train_perplexity=159.10083, train_loss=5.069538

Batch 284900, train_perplexity=158.8712, train_loss=5.068094

Batch 284910, train_perplexity=154.76123, train_loss=5.0418835

Batch 284920, train_perplexity=163.20941, train_loss=5.095034

Batch 284930, train_perplexity=152.29869, train_loss=5.0258436

Batch 284940, train_perplexity=167.49358, train_loss=5.120945

Batch 284950, train_perplexity=148.0339, train_loss=4.9974413

Batch 284960, train_perplexity=159.14165, train_loss=5.0697947

Batch 284970, train_perplexity=151.78871, train_loss=5.0224895

Batch 284980, train_perplexity=159.6148, train_loss=5.0727634

Batch 284990, train_perplexity=182.91447, train_loss=5.2090187

Batch 285000, train_perplexity=137.47575, train_loss=4.9234476

Batch 285010, train_perplexity=176.11125, train_loss=5.171116

Batch 285020, train_perplexity=152.4405, train_loss=5.0267744

Batch 285030, train_perplexity=157.39052, train_loss=5.05873

Batch 285040, train_perplexity=168.01256, train_loss=5.1240387

Batch 285050, train_perplexity=157.45598, train_loss=5.059146

Batch 285060, train_perplexity=140.5909, train_loss=4.945854

Batch 285070, train_perplexity=158.0115, train_loss=5.062668

Batch 285080, train_perplexity=153.45015, train_loss=5.0333757

Batch 285090, train_perplexity=166.85045, train_loss=5.117098

Batch 285100, train_perplexity=177.34302, train_loss=5.178086

Batch 285110, train_perplexity=157.8219, train_loss=5.061467

Batch 285120, train_perplexity=163.36818, train_loss=5.0960064

Batch 285130, train_perplexity=172.0656, train_loss=5.147876

Batch 285140, train_perplexity=164.83968, train_loss=5.1049733

Batch 285150, train_perplexity=141.799, train_loss=4.9544106

Batch 285160, train_perplexity=154.69261, train_loss=5.04144

Batch 285170, train_perplexity=161.32889, train_loss=5.083445

Batch 285180, train_perplexity=154.65073, train_loss=5.041169

Batch 285190, train_perplexity=151.23207, train_loss=5.0188155

Batch 285200, train_perplexity=148.96448, train_loss=5.003708

Batch 285210, train_perplexity=154.75356, train_loss=5.041834

Batch 285220, train_perplexity=153.73857, train_loss=5.0352535

Batch 285230, train_perplexity=157.38602, train_loss=5.0587015

Batch 285240, train_perplexity=152.81525, train_loss=5.0292296

Batch 285250, train_perplexity=155.53966, train_loss=5.0469007

Batch 285260, train_perplexity=171.02316, train_loss=5.141799

Batch 285270, train_perplexity=151.3358, train_loss=5.019501

Batch 285280, train_perplexity=153.20844, train_loss=5.0317993

Batch 285290, train_perplexity=162.48082, train_loss=5.09056

Batch 285300, train_perplexity=154.13185, train_loss=5.0378084

Batch 285310, train_perplexity=156.17001, train_loss=5.0509453

Batch 285320, train_perplexity=168.18738, train_loss=5.1250787

Batch 285330, train_perplexity=156.71593, train_loss=5.054435

Batch 285340, train_perplexity=177.53687, train_loss=5.179178

Batch 285350, train_perplexity=156.947, train_loss=5.055908

Batch 285360, train_perplexity=175.52063, train_loss=5.1677566

Batch 285370, train_perplexity=154.93356, train_loss=5.0429964

Batch 285380, train_perplexity=164.68324, train_loss=5.104024

Batch 285390, train_perplexity=128.11627, train_loss=4.852938

Batch 285400, train_perplexity=144.4674, train_loss=4.973054

Batch 285410, train_perplexity=132.72375, train_loss=4.88827

Batch 285420, train_perplexity=147.34326, train_loss=4.992765

Batch 285430, train_perplexity=152.06343, train_loss=5.0242977

Batch 285440, train_perplexity=173.75267, train_loss=5.157633

Batch 285450, train_perplexity=158.6068, train_loss=5.066428

Batch 285460, train_perplexity=156.40782, train_loss=5.052467

Batch 285470, train_perplexity=140.81418, train_loss=4.947441

Batch 285480, train_perplexity=174.6601, train_loss=5.162842

Batch 285490, train_perplexity=166.64403, train_loss=5.11586

Batch 285500, train_perplexity=149.83252, train_loss=5.009518

Batch 285510, train_perplexity=139.82892, train_loss=4.9404197

Batch 285520, train_perplexity=154.50412, train_loss=5.0402207

Batch 285530, train_perplexity=147.30975, train_loss=4.9925375

Batch 285540, train_perplexity=154.13156, train_loss=5.0378065

Batch 285550, train_perplexity=165.82504, train_loss=5.1109333

Batch 285560, train_perplexity=158.44896, train_loss=5.0654325

Batch 285570, train_perplexity=172.8404, train_loss=5.1523685

Batch 285580, train_perplexity=170.00633, train_loss=5.1358356

Batch 285590, train_perplexity=147.25644, train_loss=4.9921756

Batch 285600, train_perplexity=152.41849, train_loss=5.02663
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 285610, train_perplexity=139.20457, train_loss=4.9359446

Batch 285620, train_perplexity=160.26955, train_loss=5.076857

Batch 285630, train_perplexity=150.13326, train_loss=5.0115232

Batch 285640, train_perplexity=145.37813, train_loss=4.979338

Batch 285650, train_perplexity=165.22519, train_loss=5.1073093

Batch 285660, train_perplexity=148.55058, train_loss=5.0009255

Batch 285670, train_perplexity=164.77028, train_loss=5.1045523

Batch 285680, train_perplexity=156.33095, train_loss=5.0519753

Batch 285690, train_perplexity=155.17015, train_loss=5.0445223

Batch 285700, train_perplexity=138.36253, train_loss=4.9298773

Batch 285710, train_perplexity=165.29422, train_loss=5.107727

Batch 285720, train_perplexity=157.97354, train_loss=5.0624275

Batch 285730, train_perplexity=160.20209, train_loss=5.076436

Batch 285740, train_perplexity=142.67242, train_loss=4.9605513

Batch 285750, train_perplexity=144.23132, train_loss=4.9714184

Batch 285760, train_perplexity=177.97015, train_loss=5.181616

Batch 285770, train_perplexity=157.39638, train_loss=5.0587673

Batch 285780, train_perplexity=147.39758, train_loss=4.9931335

Batch 285790, train_perplexity=159.4397, train_loss=5.071666

Batch 285800, train_perplexity=151.81274, train_loss=5.022648

Batch 285810, train_perplexity=147.08401, train_loss=4.991004

Batch 285820, train_perplexity=163.53278, train_loss=5.0970135

Batch 285830, train_perplexity=161.33159, train_loss=5.0834618

Batch 285840, train_perplexity=156.6938, train_loss=5.0542936

Batch 285850, train_perplexity=146.82237, train_loss=4.9892235

Batch 285860, train_perplexity=165.74742, train_loss=5.110465

Batch 285870, train_perplexity=145.86893, train_loss=4.9827085

Batch 285880, train_perplexity=167.74518, train_loss=5.122446

Batch 285890, train_perplexity=143.92038, train_loss=4.96926

Batch 285900, train_perplexity=172.02327, train_loss=5.1476297

Batch 285910, train_perplexity=167.8175, train_loss=5.122877

Batch 285920, train_perplexity=143.05473, train_loss=4.9632273

Batch 285930, train_perplexity=152.58669, train_loss=5.027733

Batch 285940, train_perplexity=143.54588, train_loss=4.966655

Batch 285950, train_perplexity=160.283, train_loss=5.076941

Batch 285960, train_perplexity=151.11867, train_loss=5.0180655

Batch 285970, train_perplexity=159.72704, train_loss=5.0734663

Batch 285980, train_perplexity=174.34848, train_loss=5.161056

Batch 285990, train_perplexity=160.93013, train_loss=5.0809703

Batch 286000, train_perplexity=160.15106, train_loss=5.0761175

Batch 286010, train_perplexity=164.833, train_loss=5.104933

Batch 286020, train_perplexity=169.66248, train_loss=5.133811

Batch 286030, train_perplexity=150.25572, train_loss=5.0123386

Batch 286040, train_perplexity=168.69185, train_loss=5.1280737

Batch 286050, train_perplexity=160.87987, train_loss=5.080658

Batch 286060, train_perplexity=161.53711, train_loss=5.084735

Batch 286070, train_perplexity=148.84264, train_loss=5.0028896

Batch 286080, train_perplexity=151.277, train_loss=5.0191126

Batch 286090, train_perplexity=162.13742, train_loss=5.088444

Batch 286100, train_perplexity=150.33685, train_loss=5.0128784

Batch 286110, train_perplexity=180.57195, train_loss=5.1961293

Batch 286120, train_perplexity=166.58429, train_loss=5.1155014

Batch 286130, train_perplexity=146.17885, train_loss=4.984831

Batch 286140, train_perplexity=153.0192, train_loss=5.0305634

Batch 286150, train_perplexity=143.13129, train_loss=4.9637623

Batch 286160, train_perplexity=154.79857, train_loss=5.0421247

Batch 286170, train_perplexity=152.91664, train_loss=5.029893

Batch 286180, train_perplexity=167.80423, train_loss=5.122798

Batch 286190, train_perplexity=166.67209, train_loss=5.1160283

Batch 286200, train_perplexity=155.63046, train_loss=5.0474844

Batch 286210, train_perplexity=158.92325, train_loss=5.0684214

Batch 286220, train_perplexity=166.29247, train_loss=5.113748

Batch 286230, train_perplexity=164.44423, train_loss=5.1025715

Batch 286240, train_perplexity=150.66194, train_loss=5.0150385

Batch 286250, train_perplexity=176.43755, train_loss=5.172967

Batch 286260, train_perplexity=156.92201, train_loss=5.055749

Batch 286270, train_perplexity=134.96165, train_loss=4.9049907

Batch 286280, train_perplexity=143.3679, train_loss=4.965414

Batch 286290, train_perplexity=153.67891, train_loss=5.0348654

Batch 286300, train_perplexity=158.69455, train_loss=5.0669813

Batch 286310, train_perplexity=156.51056, train_loss=5.0531235

Batch 286320, train_perplexity=156.86769, train_loss=5.0554028

Batch 286330, train_perplexity=161.42392, train_loss=5.084034

Batch 286340, train_perplexity=130.41713, train_loss=4.870738

Batch 286350, train_perplexity=164.57162, train_loss=5.103346

Batch 286360, train_perplexity=134.77805, train_loss=4.9036293

Batch 286370, train_perplexity=145.0029, train_loss=4.9767537

Batch 286380, train_perplexity=167.22096, train_loss=5.119316

Batch 286390, train_perplexity=161.21216, train_loss=5.082721

Batch 286400, train_perplexity=174.5954, train_loss=5.1624713

Batch 286410, train_perplexity=136.60483, train_loss=4.9170923

Batch 286420, train_perplexity=143.00671, train_loss=4.9628916

Batch 286430, train_perplexity=194.94174, train_loss=5.272701

Batch 286440, train_perplexity=162.89003, train_loss=5.0930753

Batch 286450, train_perplexity=147.30814, train_loss=4.9925265

Batch 286460, train_perplexity=169.1805, train_loss=5.130966

Batch 286470, train_perplexity=152.14088, train_loss=5.024807

Batch 286480, train_perplexity=152.87231, train_loss=5.029603

Batch 286490, train_perplexity=163.07071, train_loss=5.094184

Batch 286500, train_perplexity=151.37918, train_loss=5.019788

Batch 286510, train_perplexity=146.98502, train_loss=4.9903307

Batch 286520, train_perplexity=182.59082, train_loss=5.2072477

Batch 286530, train_perplexity=151.01178, train_loss=5.017358

Batch 286540, train_perplexity=128.18843, train_loss=4.8535013

Batch 286550, train_perplexity=161.33897, train_loss=5.0835075

Batch 286560, train_perplexity=159.89125, train_loss=5.074494

Batch 286570, train_perplexity=152.19879, train_loss=5.0251875

Batch 286580, train_perplexity=156.54848, train_loss=5.0533657

Batch 286590, train_perplexity=153.95374, train_loss=5.036652

Batch 286600, train_perplexity=159.62828, train_loss=5.072848

Batch 286610, train_perplexity=141.38782, train_loss=4.9515066

Batch 286620, train_perplexity=165.8426, train_loss=5.111039

Batch 286630, train_perplexity=140.93784, train_loss=4.948319

Batch 286640, train_perplexity=181.53915, train_loss=5.2014713

Batch 286650, train_perplexity=147.8275, train_loss=4.996046

Batch 286660, train_perplexity=144.7406, train_loss=4.974943

Batch 286670, train_perplexity=164.94826, train_loss=5.105632

Batch 286680, train_perplexity=140.63286, train_loss=4.9461527

Batch 286690, train_perplexity=151.39375, train_loss=5.019884

Batch 286700, train_perplexity=162.00095, train_loss=5.087602

Batch 286710, train_perplexity=152.26498, train_loss=5.0256224

Batch 286720, train_perplexity=160.6286, train_loss=5.079095

Batch 286730, train_perplexity=149.54224, train_loss=5.007579

Batch 286740, train_perplexity=149.47864, train_loss=5.0071535

Batch 286750, train_perplexity=159.6186, train_loss=5.0727873

Batch 286760, train_perplexity=156.31947, train_loss=5.051902

Batch 286770, train_perplexity=161.96965, train_loss=5.087409

Batch 286780, train_perplexity=153.6797, train_loss=5.0348706

Batch 286790, train_perplexity=163.03075, train_loss=5.093939

Batch 286800, train_perplexity=152.639, train_loss=5.0280757

Batch 286810, train_perplexity=140.42688, train_loss=4.944687

Batch 286820, train_perplexity=141.19164, train_loss=4.950118

Batch 286830, train_perplexity=150.44571, train_loss=5.0136023

Batch 286840, train_perplexity=150.92813, train_loss=5.0168037

Batch 286850, train_perplexity=163.94237, train_loss=5.099515

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306744 sentences.
Finished loading
Batch 286860, train_perplexity=152.4626, train_loss=5.0269194

Batch 286870, train_perplexity=134.88188, train_loss=4.9043994

Batch 286880, train_perplexity=158.33778, train_loss=5.0647306

Batch 286890, train_perplexity=164.67964, train_loss=5.104002

Batch 286900, train_perplexity=145.18784, train_loss=4.9780283

Batch 286910, train_perplexity=172.15373, train_loss=5.148388

Batch 286920, train_perplexity=150.87027, train_loss=5.0164204

Batch 286930, train_perplexity=161.03114, train_loss=5.081598

Batch 286940, train_perplexity=163.04039, train_loss=5.093998

Batch 286950, train_perplexity=155.141, train_loss=5.0443344

Batch 286960, train_perplexity=142.91652, train_loss=4.9622607

Batch 286970, train_perplexity=151.00961, train_loss=5.0173435

Batch 286980, train_perplexity=168.67545, train_loss=5.1279764

Batch 286990, train_perplexity=153.89699, train_loss=5.0362835

Batch 287000, train_perplexity=150.7867, train_loss=5.0158663

Batch 287010, train_perplexity=151.03238, train_loss=5.017494

Batch 287020, train_perplexity=154.39594, train_loss=5.0395203

Batch 287030, train_perplexity=142.39172, train_loss=4.958582

Batch 287040, train_perplexity=147.16174, train_loss=4.9915323

Batch 287050, train_perplexity=150.17278, train_loss=5.0117865

Batch 287060, train_perplexity=127.70353, train_loss=4.8497114

Batch 287070, train_perplexity=148.56886, train_loss=5.0010486

Batch 287080, train_perplexity=135.43355, train_loss=4.908481

Batch 287090, train_perplexity=161.11586, train_loss=5.0821238

Batch 287100, train_perplexity=154.30098, train_loss=5.038905

Batch 287110, train_perplexity=156.76392, train_loss=5.054741

Batch 287120, train_perplexity=171.4843, train_loss=5.1444917

Batch 287130, train_perplexity=147.52141, train_loss=4.9939733

Batch 287140, train_perplexity=150.38202, train_loss=5.013179

Batch 287150, train_perplexity=147.99014, train_loss=4.9971457

Batch 287160, train_perplexity=154.58038, train_loss=5.0407143

Batch 287170, train_perplexity=169.59712, train_loss=5.1334257

Batch 287180, train_perplexity=162.41513, train_loss=5.0901556

Batch 287190, train_perplexity=143.2908, train_loss=4.964876

Batch 287200, train_perplexity=153.96672, train_loss=5.0367365

Batch 287210, train_perplexity=163.69365, train_loss=5.0979967

Batch 287220, train_perplexity=150.27077, train_loss=5.012439

Batch 287230, train_perplexity=152.12276, train_loss=5.024688

Batch 287240, train_perplexity=158.72429, train_loss=5.0671687

Batch 287250, train_perplexity=140.81216, train_loss=4.947427

Batch 287260, train_perplexity=170.47586, train_loss=5.1385937

Batch 287270, train_perplexity=164.65356, train_loss=5.1038437

Batch 287280, train_perplexity=162.0384, train_loss=5.0878334

Batch 287290, train_perplexity=144.14194, train_loss=4.9707985

Batch 287300, train_perplexity=139.07797, train_loss=4.9350348

Batch 287310, train_perplexity=144.84583, train_loss=4.97567

Batch 287320, train_perplexity=155.15617, train_loss=5.044432

Batch 287330, train_perplexity=151.02777, train_loss=5.0174637

Batch 287340, train_perplexity=159.15515, train_loss=5.0698795

Batch 287350, train_perplexity=154.69652, train_loss=5.0414653

Batch 287360, train_perplexity=163.23853, train_loss=5.0952125

Batch 287370, train_perplexity=161.53041, train_loss=5.0846934

Batch 287380, train_perplexity=153.80264, train_loss=5.0356703

Batch 287390, train_perplexity=173.19035, train_loss=5.1543913

Batch 287400, train_perplexity=151.64186, train_loss=5.0215216

Batch 287410, train_perplexity=166.50995, train_loss=5.115055

Batch 287420, train_perplexity=136.70244, train_loss=4.9178066

Batch 287430, train_perplexity=153.16716, train_loss=5.03153

Batch 287440, train_perplexity=170.64339, train_loss=5.139576

Batch 287450, train_perplexity=161.17195, train_loss=5.082472

Batch 287460, train_perplexity=149.77802, train_loss=5.0091543

Batch 287470, train_perplexity=151.3607, train_loss=5.0196657

Batch 287480, train_perplexity=164.06131, train_loss=5.10024

Batch 287490, train_perplexity=167.95856, train_loss=5.1237173

Batch 287500, train_perplexity=161.85085, train_loss=5.086675

Batch 287510, train_perplexity=155.84389, train_loss=5.048855

Batch 287520, train_perplexity=156.7281, train_loss=5.0545125

Batch 287530, train_perplexity=180.69693, train_loss=5.196821

Batch 287540, train_perplexity=137.71452, train_loss=4.925183

Batch 287550, train_perplexity=149.2466, train_loss=5.0056

Batch 287560, train_perplexity=137.82771, train_loss=4.9260044

Batch 287570, train_perplexity=150.65173, train_loss=5.014971

Batch 287580, train_perplexity=144.61917, train_loss=4.974104

Batch 287590, train_perplexity=163.19066, train_loss=5.094919

Batch 287600, train_perplexity=155.1342, train_loss=5.0442905

Batch 287610, train_perplexity=170.74757, train_loss=5.1401863

Batch 287620, train_perplexity=152.46625, train_loss=5.026943

Batch 287630, train_perplexity=155.94937, train_loss=5.0495315

Batch 287640, train_perplexity=153.58125, train_loss=5.0342298

Batch 287650, train_perplexity=156.6705, train_loss=5.054145

Batch 287660, train_perplexity=140.4155, train_loss=4.944606

Batch 287670, train_perplexity=159.11964, train_loss=5.0696564

Batch 287680, train_perplexity=173.06, train_loss=5.1536384

Batch 287690, train_perplexity=155.49828, train_loss=5.0466347

Batch 287700, train_perplexity=151.3708, train_loss=5.0197325

Batch 287710, train_perplexity=148.89716, train_loss=5.003256

Batch 287720, train_perplexity=154.57832, train_loss=5.040701

Batch 287730, train_perplexity=153.1278, train_loss=5.031273

Batch 287740, train_perplexity=157.43076, train_loss=5.0589857

Batch 287750, train_perplexity=141.07077, train_loss=4.9492617

Batch 287760, train_perplexity=137.17029, train_loss=4.921223

Batch 287770, train_perplexity=166.59732, train_loss=5.1155796

Batch 287780, train_perplexity=151.93195, train_loss=5.0234327

Batch 287790, train_perplexity=149.72925, train_loss=5.0088286

Batch 287800, train_perplexity=161.01924, train_loss=5.081524

Batch 287810, train_perplexity=138.49521, train_loss=4.9308357

Batch 287820, train_perplexity=169.82289, train_loss=5.134756

Batch 287830, train_perplexity=175.94505, train_loss=5.1701717

Batch 287840, train_perplexity=153.00693, train_loss=5.0304832

Batch 287850, train_perplexity=185.28484, train_loss=5.2218943

Batch 287860, train_perplexity=173.78441, train_loss=5.1578155

Batch 287870, train_perplexity=163.37222, train_loss=5.096031

Batch 287880, train_perplexity=138.09842, train_loss=4.9279666

Batch 287890, train_perplexity=142.28667, train_loss=4.957844

Batch 287900, train_perplexity=148.97507, train_loss=5.003779

Batch 287910, train_perplexity=165.45074, train_loss=5.1086736

Batch 287920, train_perplexity=161.74475, train_loss=5.0860195

Batch 287930, train_perplexity=170.15474, train_loss=5.1367083

Batch 287940, train_perplexity=162.44844, train_loss=5.0903606

Batch 287950, train_perplexity=161.67722, train_loss=5.085602

Batch 287960, train_perplexity=170.56807, train_loss=5.1391344

Batch 287970, train_perplexity=145.66458, train_loss=4.9813066

Batch 287980, train_perplexity=148.2215, train_loss=4.998708

Batch 287990, train_perplexity=154.35912, train_loss=5.039282

Batch 288000, train_perplexity=161.18164, train_loss=5.082532

Batch 288010, train_perplexity=153.48038, train_loss=5.0335727

Batch 288020, train_perplexity=152.00652, train_loss=5.0239234

Batch 288030, train_perplexity=143.73027, train_loss=4.9679384

Batch 288040, train_perplexity=163.74315, train_loss=5.098299

Batch 288050, train_perplexity=154.30908, train_loss=5.0389576

Batch 288060, train_perplexity=160.29675, train_loss=5.077027

Batch 288070, train_perplexity=150.13927, train_loss=5.0115633

Batch 288080, train_perplexity=176.60413, train_loss=5.1739106

Batch 288090, train_perplexity=163.74908, train_loss=5.0983353

Batch 288100, train_perplexity=151.00932, train_loss=5.0173416

Batch 288110, train_perplexity=151.47377, train_loss=5.0204124

Batch 288120, train_perplexity=151.20625, train_loss=5.018645

Batch 288130, train_perplexity=145.48167, train_loss=4.98005

Batch 288140, train_perplexity=147.03633, train_loss=4.9906797
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

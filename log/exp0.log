nohup: ignoring input
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/Upgrade_ELMo/bilm-tf/bilm/training.py:217: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/Upgrade_ELMo/py36_bilm-tf/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-27 06:31:39.896955: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-27 06:31:40.125871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:05:00.0
totalMemory: 22.38GiB freeMemory: 22.23GiB
2020-10-27 06:31:40.125979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-27 06:31:40.567879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-27 06:31:40.567973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-27 06:31:40.567990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-27 06:31:40.568163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21568 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:05:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 99 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
Found 99 shards at /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/*
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00081-of-00100
Loaded 306530 sentences.
Finished loading
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=98058.03

Batch 10, train_perplexity=15179.843

Batch 20, train_perplexity=3698.6318

Batch 30, train_perplexity=2316.2969

Batch 40, train_perplexity=2170.7644

Batch 50, train_perplexity=1710.5162

Batch 60, train_perplexity=1662.8755

Batch 70, train_perplexity=1475.2112

Batch 80, train_perplexity=1325.5526

Batch 90, train_perplexity=1328.3025

Batch 100, train_perplexity=1249.3882

Batch 110, train_perplexity=1146.4418

Batch 120, train_perplexity=1097.3204

Batch 130, train_perplexity=1056.7048

Batch 140, train_perplexity=1049.8387

Batch 150, train_perplexity=1073.2052

Batch 160, train_perplexity=957.74915

Batch 170, train_perplexity=922.6628

Batch 180, train_perplexity=898.3082

Batch 190, train_perplexity=842.1813

Batch 200, train_perplexity=805.3101

Batch 210, train_perplexity=858.8172

Batch 220, train_perplexity=769.86743

Batch 230, train_perplexity=829.3269

Batch 240, train_perplexity=775.9241

Batch 250, train_perplexity=789.3011

Batch 260, train_perplexity=856.5368

Batch 270, train_perplexity=802.35144

Batch 280, train_perplexity=766.1251

Batch 290, train_perplexity=740.87646

Batch 300, train_perplexity=734.0132

Batch 310, train_perplexity=729.92535

Batch 320, train_perplexity=687.54535

Batch 330, train_perplexity=703.69696

Batch 340, train_perplexity=717.5016

Batch 350, train_perplexity=651.1082

Batch 360, train_perplexity=658.3361

Batch 370, train_perplexity=691.0827

Batch 380, train_perplexity=684.8162

Batch 390, train_perplexity=591.9097

Batch 400, train_perplexity=656.73016

Batch 410, train_perplexity=692.004

Batch 420, train_perplexity=626.0855

Batch 430, train_perplexity=660.29596

Batch 440, train_perplexity=608.2517

Batch 450, train_perplexity=589.44806

Batch 460, train_perplexity=617.7501

Batch 470, train_perplexity=615.05426

Batch 480, train_perplexity=639.49274

Batch 490, train_perplexity=605.07367

Batch 500, train_perplexity=636.9024

Batch 510, train_perplexity=608.5415

Batch 520, train_perplexity=613.5039

Batch 530, train_perplexity=551.06116

Batch 540, train_perplexity=509.79263

Batch 550, train_perplexity=574.73004

Batch 560, train_perplexity=573.2239

Batch 570, train_perplexity=585.07184

Batch 580, train_perplexity=585.6602

Batch 590, train_perplexity=564.8726

Batch 600, train_perplexity=534.6536

Batch 610, train_perplexity=551.62585

Batch 620, train_perplexity=554.8056

Batch 630, train_perplexity=525.0941

Batch 640, train_perplexity=500.40408

Batch 650, train_perplexity=510.2844

Batch 660, train_perplexity=567.56775

Batch 670, train_perplexity=528.60364

Batch 680, train_perplexity=508.64362

Batch 690, train_perplexity=484.13116

Batch 700, train_perplexity=477.99008

Batch 710, train_perplexity=506.42633

Batch 720, train_perplexity=504.65067

Batch 730, train_perplexity=462.29877

Batch 740, train_perplexity=530.0347

Batch 750, train_perplexity=486.3705

Batch 760, train_perplexity=511.0178

Batch 770, train_perplexity=474.46216

Batch 780, train_perplexity=506.0372

Batch 790, train_perplexity=461.807

Batch 800, train_perplexity=463.32187

Batch 810, train_perplexity=485.2058

Batch 820, train_perplexity=471.7656

Batch 830, train_perplexity=513.9723

Batch 840, train_perplexity=434.89856

Batch 850, train_perplexity=505.64597

Batch 860, train_perplexity=438.76935

Batch 870, train_perplexity=432.96494

Batch 880, train_perplexity=477.35483

Batch 890, train_perplexity=446.8421

Batch 900, train_perplexity=436.6971

Batch 910, train_perplexity=472.48914

Batch 920, train_perplexity=448.80945

Batch 930, train_perplexity=472.25333

Batch 940, train_perplexity=445.66132

Batch 950, train_perplexity=396.93262

Batch 960, train_perplexity=386.2857

Batch 970, train_perplexity=430.44943

Batch 980, train_perplexity=397.1708

Batch 990, train_perplexity=401.63376

Batch 1000, train_perplexity=451.6857

Batch 1010, train_perplexity=411.32825

Batch 1020, train_perplexity=400.7339

Batch 1030, train_perplexity=395.93

Batch 1040, train_perplexity=410.3638

Batch 1050, train_perplexity=456.27927

Batch 1060, train_perplexity=409.12253

Batch 1070, train_perplexity=380.90814

Batch 1080, train_perplexity=425.84167

Batch 1090, train_perplexity=437.59134

Batch 1100, train_perplexity=388.13077

Batch 1110, train_perplexity=409.62323

Batch 1120, train_perplexity=413.85278

Batch 1130, train_perplexity=385.41876

Batch 1140, train_perplexity=376.7538

Batch 1150, train_perplexity=398.59375

Batch 1160, train_perplexity=373.53415

Batch 1170, train_perplexity=370.50446

Batch 1180, train_perplexity=379.9221

Batch 1190, train_perplexity=375.30612

Batch 1200, train_perplexity=391.99857

Batch 1210, train_perplexity=404.26724

Batch 1220, train_perplexity=403.4461

Batch 1230, train_perplexity=357.24448

Batch 1240, train_perplexity=375.24457

Batch 1250, train_perplexity=389.4981

Batch 1260, train_perplexity=370.40518

Batch 1270, train_perplexity=378.8309

Batch 1280, train_perplexity=405.42746

Batch 1290, train_perplexity=373.20868

Batch 1300, train_perplexity=376.8352

Batch 1310, train_perplexity=394.31195

Batch 1320, train_perplexity=358.60526

Batch 1330, train_perplexity=363.0331

Batch 1340, train_perplexity=397.9044

Batch 1350, train_perplexity=362.3614

Batch 1360, train_perplexity=328.6356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 1370, train_perplexity=380.01666

Batch 1380, train_perplexity=366.04584

Batch 1390, train_perplexity=363.20593

Batch 1400, train_perplexity=414.44305

Batch 1410, train_perplexity=387.57538

Batch 1420, train_perplexity=369.60382

Batch 1430, train_perplexity=360.16434

Batch 1440, train_perplexity=352.4404

Batch 1450, train_perplexity=376.95544

Batch 1460, train_perplexity=364.16943

Batch 1470, train_perplexity=362.66113

Batch 1480, train_perplexity=362.5878

Batch 1490, train_perplexity=349.68884

Batch 1500, train_perplexity=348.66635

Batch 1510, train_perplexity=359.46432

Batch 1520, train_perplexity=350.4268

Batch 1530, train_perplexity=304.87128

Batch 1540, train_perplexity=348.19186

Batch 1550, train_perplexity=380.43658

Batch 1560, train_perplexity=384.27698

Batch 1570, train_perplexity=375.59775

Batch 1580, train_perplexity=333.18588

Batch 1590, train_perplexity=349.95825

Batch 1600, train_perplexity=359.02887

Batch 1610, train_perplexity=332.04315

Batch 1620, train_perplexity=322.36835

Batch 1630, train_perplexity=319.41797

Batch 1640, train_perplexity=367.24554

Batch 1650, train_perplexity=359.4986

Batch 1660, train_perplexity=330.8955

Batch 1670, train_perplexity=327.4549

Batch 1680, train_perplexity=322.15475

Batch 1690, train_perplexity=339.8744

Batch 1700, train_perplexity=331.43463

Batch 1710, train_perplexity=338.94055

Batch 1720, train_perplexity=347.7737

Batch 1730, train_perplexity=330.81866

Batch 1740, train_perplexity=304.32095

Batch 1750, train_perplexity=369.55307

Batch 1760, train_perplexity=357.27396

Batch 1770, train_perplexity=322.595

Batch 1780, train_perplexity=357.82394

Batch 1790, train_perplexity=311.2023

Batch 1800, train_perplexity=356.20703

Batch 1810, train_perplexity=340.95026

Batch 1820, train_perplexity=343.46054

Batch 1830, train_perplexity=320.63022

Batch 1840, train_perplexity=324.0677

Batch 1850, train_perplexity=327.04733

Batch 1860, train_perplexity=325.86426

Batch 1870, train_perplexity=336.7535

Batch 1880, train_perplexity=318.25067

Batch 1890, train_perplexity=313.29214

Batch 1900, train_perplexity=323.9163

Batch 1910, train_perplexity=324.02783

Batch 1920, train_perplexity=318.0169

Batch 1930, train_perplexity=321.92932

Batch 1940, train_perplexity=301.70172

Batch 1950, train_perplexity=309.24908

Batch 1960, train_perplexity=332.84988

Batch 1970, train_perplexity=323.4494

Batch 1980, train_perplexity=289.85648

Batch 1990, train_perplexity=333.44415

Batch 2000, train_perplexity=347.74918

Batch 2010, train_perplexity=310.7299

Batch 2020, train_perplexity=330.53912

Batch 2030, train_perplexity=329.3836

Batch 2040, train_perplexity=314.34067

Batch 2050, train_perplexity=316.11963

Batch 2060, train_perplexity=320.47583

Batch 2070, train_perplexity=322.4474

Batch 2080, train_perplexity=328.5203

Batch 2090, train_perplexity=267.8437

Batch 2100, train_perplexity=305.25735

Batch 2110, train_perplexity=301.7505

Batch 2120, train_perplexity=308.61682

Batch 2130, train_perplexity=303.43155

Batch 2140, train_perplexity=310.1829

Batch 2150, train_perplexity=275.18237

Batch 2160, train_perplexity=319.89215

Batch 2170, train_perplexity=313.90674

Batch 2180, train_perplexity=287.21735

Batch 2190, train_perplexity=308.53827

Batch 2200, train_perplexity=304.81342

Batch 2210, train_perplexity=301.10355

Batch 2220, train_perplexity=305.6815

Batch 2230, train_perplexity=295.37585

Batch 2240, train_perplexity=316.86484

Batch 2250, train_perplexity=314.18286

Batch 2260, train_perplexity=313.15817

Batch 2270, train_perplexity=293.8631

Batch 2280, train_perplexity=291.85278

Batch 2290, train_perplexity=267.4182

Batch 2300, train_perplexity=295.02115

Batch 2310, train_perplexity=291.96805

Batch 2320, train_perplexity=291.95828

Batch 2330, train_perplexity=292.10367

Batch 2340, train_perplexity=304.92944

Batch 2350, train_perplexity=304.31732

Batch 2360, train_perplexity=284.51755

Batch 2370, train_perplexity=306.9868

Batch 2380, train_perplexity=293.81058

Batch 2390, train_perplexity=299.312

Batch 2400, train_perplexity=279.31082

Batch 2410, train_perplexity=278.3043

Batch 2420, train_perplexity=282.39984

Batch 2430, train_perplexity=260.08475

Batch 2440, train_perplexity=289.26526

Batch 2450, train_perplexity=292.80762

Batch 2460, train_perplexity=267.90564

Batch 2470, train_perplexity=281.12265

Batch 2480, train_perplexity=304.07117

Batch 2490, train_perplexity=261.4705

Batch 2500, train_perplexity=300.64716

Batch 2510, train_perplexity=294.83832

Batch 2520, train_perplexity=289.80023

Batch 2530, train_perplexity=303.28952

Batch 2540, train_perplexity=284.62613

Batch 2550, train_perplexity=300.96817

Batch 2560, train_perplexity=278.77765

Batch 2570, train_perplexity=284.34125

Batch 2580, train_perplexity=271.54318

Batch 2590, train_perplexity=310.38885

Batch 2600, train_perplexity=280.28314

Batch 2610, train_perplexity=299.51932

Batch 2620, train_perplexity=274.243

Batch 2630, train_perplexity=268.71716

Batch 2640, train_perplexity=273.94086

Batch 2650, train_perplexity=263.90564

Batch 2660, train_perplexity=268.88058

Batch 2670, train_perplexity=266.45416

Batch 2680, train_perplexity=281.48602

Batch 2690, train_perplexity=272.35327

Batch 2700, train_perplexity=272.7979

Batch 2710, train_perplexity=286.2256

Batch 2720, train_perplexity=285.87994

Batch 2730, train_perplexity=281.816

Batch 2740, train_perplexity=278.1261

Batch 2750, train_perplexity=277.39883

Batch 2760, train_perplexity=259.7766

Batch 2770, train_perplexity=254.29962

Batch 2780, train_perplexity=279.3561

Batch 2790, train_perplexity=294.17212

Batch 2800, train_perplexity=290.48645

Batch 2810, train_perplexity=280.02774

Batch 2820, train_perplexity=259.63086

Batch 2830, train_perplexity=271.3607

Batch 2840, train_perplexity=263.03613

Batch 2850, train_perplexity=290.8604

Batch 2860, train_perplexity=286.79474

Batch 2870, train_perplexity=299.0377

Batch 2880, train_perplexity=259.18295

Batch 2890, train_perplexity=265.45407

Batch 2900, train_perplexity=301.34988

Batch 2910, train_perplexity=253.96674

Batch 2920, train_perplexity=295.52576

Batch 2930, train_perplexity=261.11414

Batch 2940, train_perplexity=290.0163

Batch 2950, train_perplexity=266.32916

Batch 2960, train_perplexity=267.2033

Batch 2970, train_perplexity=265.9865

Batch 2980, train_perplexity=252.29225

Batch 2990, train_perplexity=265.32272

Batch 3000, train_perplexity=276.10992

Batch 3010, train_perplexity=245.71382

Batch 3020, train_perplexity=253.13698

Batch 3030, train_perplexity=219.20148

Batch 3040, train_perplexity=281.36218

Batch 3050, train_perplexity=248.14784

Batch 3060, train_perplexity=246.68541

Batch 3070, train_perplexity=240.12364

Batch 3080, train_perplexity=250.96066

Batch 3090, train_perplexity=264.59033

Batch 3100, train_perplexity=268.70026

Batch 3110, train_perplexity=262.70096

Batch 3120, train_perplexity=262.51025

Batch 3130, train_perplexity=268.42865

Batch 3140, train_perplexity=232.18314

Batch 3150, train_perplexity=229.58516

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00015-of-00100
Loaded 306329 sentences.
Finished loading
Batch 3160, train_perplexity=267.63098

Batch 3170, train_perplexity=278.84824

Batch 3180, train_perplexity=245.89175

Batch 3190, train_perplexity=265.91144

Batch 3200, train_perplexity=262.20386

Batch 3210, train_perplexity=262.63107

Batch 3220, train_perplexity=258.39517

Batch 3230, train_perplexity=270.08667

Batch 3240, train_perplexity=251.57771

Batch 3250, train_perplexity=268.92532

Batch 3260, train_perplexity=245.21451

Batch 3270, train_perplexity=235.47037

Batch 3280, train_perplexity=253.13385

Batch 3290, train_perplexity=270.30145

Batch 3300, train_perplexity=272.1288

Batch 3310, train_perplexity=243.08798

Batch 3320, train_perplexity=244.80608
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3330, train_perplexity=267.57077

Batch 3340, train_perplexity=249.90169

Batch 3350, train_perplexity=262.6501

Batch 3360, train_perplexity=268.65054

Batch 3370, train_perplexity=242.36923

Batch 3380, train_perplexity=263.17087

Batch 3390, train_perplexity=234.44702

Batch 3400, train_perplexity=250.41988

Batch 3410, train_perplexity=230.9132

Batch 3420, train_perplexity=239.82144

Batch 3430, train_perplexity=258.38705

Batch 3440, train_perplexity=268.9397

Batch 3450, train_perplexity=248.60027

Batch 3460, train_perplexity=239.17825

Batch 3470, train_perplexity=252.66595

Batch 3480, train_perplexity=233.16258

Batch 3490, train_perplexity=267.5565

Batch 3500, train_perplexity=229.96524

Batch 3510, train_perplexity=248.73343

Batch 3520, train_perplexity=258.44583

Batch 3530, train_perplexity=261.22623

Batch 3540, train_perplexity=245.38832

Batch 3550, train_perplexity=242.42216

Batch 3560, train_perplexity=227.81633

Batch 3570, train_perplexity=234.8776

Batch 3580, train_perplexity=240.12651

Batch 3590, train_perplexity=267.41464

Batch 3600, train_perplexity=231.4559

Batch 3610, train_perplexity=223.90086

Batch 3620, train_perplexity=273.2429

Batch 3630, train_perplexity=236.757

Batch 3640, train_perplexity=254.10446

Batch 3650, train_perplexity=233.39763

Batch 3660, train_perplexity=267.28513

Batch 3670, train_perplexity=241.12921

Batch 3680, train_perplexity=245.54025

Batch 3690, train_perplexity=243.56555

Batch 3700, train_perplexity=249.64658

Batch 3710, train_perplexity=247.07211

Batch 3720, train_perplexity=263.84308

Batch 3730, train_perplexity=257.94644

Batch 3740, train_perplexity=229.03528

Batch 3750, train_perplexity=225.43282

Batch 3760, train_perplexity=209.6219

Batch 3770, train_perplexity=285.63126

Batch 3780, train_perplexity=213.51448

Batch 3790, train_perplexity=246.70258

Batch 3800, train_perplexity=238.24362

Batch 3810, train_perplexity=257.18045

Batch 3820, train_perplexity=238.28839

Batch 3830, train_perplexity=257.70648

Batch 3840, train_perplexity=273.1888

Batch 3850, train_perplexity=251.07294

Batch 3860, train_perplexity=250.30133

Batch 3870, train_perplexity=221.80217

Batch 3880, train_perplexity=239.12009

Batch 3890, train_perplexity=223.08603

Batch 3900, train_perplexity=219.70659

Batch 3910, train_perplexity=239.79251

Batch 3920, train_perplexity=233.89586

Batch 3930, train_perplexity=231.23593

Batch 3940, train_perplexity=244.59744

Batch 3950, train_perplexity=234.24097

Batch 3960, train_perplexity=232.56575

Batch 3970, train_perplexity=225.64145

Batch 3980, train_perplexity=242.7207

Batch 3990, train_perplexity=251.47192

Batch 4000, train_perplexity=228.87631

Batch 4010, train_perplexity=250.01671

Batch 4020, train_perplexity=254.39592

Batch 4030, train_perplexity=240.94843

Batch 4040, train_perplexity=251.5765

Batch 4050, train_perplexity=240.02003

Batch 4060, train_perplexity=226.26614

Batch 4070, train_perplexity=234.54362

Batch 4080, train_perplexity=228.13594

Batch 4090, train_perplexity=254.25476

Batch 4100, train_perplexity=227.77452

Batch 4110, train_perplexity=224.47931

Batch 4120, train_perplexity=247.78462

Batch 4130, train_perplexity=216.74762

Batch 4140, train_perplexity=237.22282

Batch 4150, train_perplexity=233.06099

Batch 4160, train_perplexity=216.36772

Batch 4170, train_perplexity=242.79976

Batch 4180, train_perplexity=232.99898

Batch 4190, train_perplexity=213.00238

Batch 4200, train_perplexity=229.16003

Batch 4210, train_perplexity=219.81998

Batch 4220, train_perplexity=222.72412

Batch 4230, train_perplexity=237.55788

Batch 4240, train_perplexity=228.50447

Batch 4250, train_perplexity=233.0252

Batch 4260, train_perplexity=199.29677

Batch 4270, train_perplexity=216.09871

Batch 4280, train_perplexity=212.9447

Batch 4290, train_perplexity=249.3287

Batch 4300, train_perplexity=226.0386

Batch 4310, train_perplexity=241.39255

Batch 4320, train_perplexity=212.49808

Batch 4330, train_perplexity=227.74628

Batch 4340, train_perplexity=248.19519

Batch 4350, train_perplexity=218.63622

Batch 4360, train_perplexity=213.88397

Batch 4370, train_perplexity=226.34923

Batch 4380, train_perplexity=230.99712

Batch 4390, train_perplexity=234.47787

Batch 4400, train_perplexity=210.47334

Batch 4410, train_perplexity=223.06284

Batch 4420, train_perplexity=214.2541

Batch 4430, train_perplexity=234.67216

Batch 4440, train_perplexity=226.96225

Batch 4450, train_perplexity=220.0486

Batch 4460, train_perplexity=224.11824

Batch 4470, train_perplexity=212.36073

Batch 4480, train_perplexity=230.08151

Batch 4490, train_perplexity=223.87984

Batch 4500, train_perplexity=243.02425

Batch 4510, train_perplexity=227.29778

Batch 4520, train_perplexity=218.79703

Batch 4530, train_perplexity=234.07526

Batch 4540, train_perplexity=222.85915

Batch 4550, train_perplexity=208.49931

Batch 4560, train_perplexity=246.13388

Batch 4570, train_perplexity=217.57239

Batch 4580, train_perplexity=211.06148

Batch 4590, train_perplexity=219.0998

Batch 4600, train_perplexity=210.03133

Batch 4610, train_perplexity=205.65686

Batch 4620, train_perplexity=220.12646

Batch 4630, train_perplexity=215.2515

Batch 4640, train_perplexity=221.85336

Batch 4650, train_perplexity=211.82886

Batch 4660, train_perplexity=217.94505

Batch 4670, train_perplexity=219.6539

Batch 4680, train_perplexity=233.48611

Batch 4690, train_perplexity=221.05579

Batch 4700, train_perplexity=240.31047

Batch 4710, train_perplexity=221.5377

Batch 4720, train_perplexity=200.09322

Batch 4730, train_perplexity=211.49902

Batch 4740, train_perplexity=224.05626

Batch 4750, train_perplexity=241.03506

Batch 4760, train_perplexity=228.08678

Batch 4770, train_perplexity=211.6211

Batch 4780, train_perplexity=203.56972

Batch 4790, train_perplexity=219.33646

Batch 4800, train_perplexity=224.27507

Batch 4810, train_perplexity=210.46742

Batch 4820, train_perplexity=216.87706

Batch 4830, train_perplexity=222.57814

Batch 4840, train_perplexity=214.93192

Batch 4850, train_perplexity=196.89526

Batch 4860, train_perplexity=212.18785

Batch 4870, train_perplexity=228.16595

Batch 4880, train_perplexity=219.83003

Batch 4890, train_perplexity=216.11478

Batch 4900, train_perplexity=204.88028

Batch 4910, train_perplexity=222.98978

Batch 4920, train_perplexity=200.5127

Batch 4930, train_perplexity=210.42328

Batch 4940, train_perplexity=218.0121

Batch 4950, train_perplexity=220.37495

Batch 4960, train_perplexity=224.89545

Batch 4970, train_perplexity=224.31293

Batch 4980, train_perplexity=197.60312

Batch 4990, train_perplexity=222.81218

Batch 5000, train_perplexity=230.80586

Batch 5010, train_perplexity=219.17221

Batch 5020, train_perplexity=206.00293

Batch 5030, train_perplexity=197.2311

Batch 5040, train_perplexity=202.85887

Batch 5050, train_perplexity=222.77298

Batch 5060, train_perplexity=211.06119

Batch 5070, train_perplexity=220.56628

Batch 5080, train_perplexity=204.11131

Batch 5090, train_perplexity=212.10681

Batch 5100, train_perplexity=201.83351

Batch 5110, train_perplexity=222.16396

Batch 5120, train_perplexity=209.66489

Batch 5130, train_perplexity=212.52524

Batch 5140, train_perplexity=208.17923

Batch 5150, train_perplexity=217.59346

Batch 5160, train_perplexity=248.09035

Batch 5170, train_perplexity=202.51884

Batch 5180, train_perplexity=207.95412

Batch 5190, train_perplexity=221.88574

Batch 5200, train_perplexity=200.40315

Batch 5210, train_perplexity=221.13116

Batch 5220, train_perplexity=222.79752

Batch 5230, train_perplexity=216.11417

Batch 5240, train_perplexity=180.82518

Batch 5250, train_perplexity=185.85344

Batch 5260, train_perplexity=198.49489

Batch 5270, train_perplexity=213.63618

Batch 5280, train_perplexity=200.40851

Batch 5290, train_perplexity=204.31006

Batch 5300, train_perplexity=227.31772

Batch 5310, train_perplexity=195.75615

Batch 5320, train_perplexity=213.88937

Batch 5330, train_perplexity=231.21477

Batch 5340, train_perplexity=196.99425

Batch 5350, train_perplexity=198.41039

Batch 5360, train_perplexity=208.7953

Batch 5370, train_perplexity=196.75523

Batch 5380, train_perplexity=191.69382
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 5390, train_perplexity=194.72696

Batch 5400, train_perplexity=186.95053

Batch 5410, train_perplexity=186.82353

Batch 5420, train_perplexity=219.23837

Batch 5430, train_perplexity=205.98781

Batch 5440, train_perplexity=195.533

Batch 5450, train_perplexity=209.72629

Batch 5460, train_perplexity=190.26645

Batch 5470, train_perplexity=236.17767

Batch 5480, train_perplexity=195.24399

Batch 5490, train_perplexity=225.24477

Batch 5500, train_perplexity=207.30623

Batch 5510, train_perplexity=220.0253

Batch 5520, train_perplexity=201.85246

Batch 5530, train_perplexity=187.08331

Batch 5540, train_perplexity=206.39899

Batch 5550, train_perplexity=211.46585

Batch 5560, train_perplexity=190.40277

Batch 5570, train_perplexity=207.41202

Batch 5580, train_perplexity=207.86708

Batch 5590, train_perplexity=205.19392

Batch 5600, train_perplexity=200.5806

Batch 5610, train_perplexity=198.33301

Batch 5620, train_perplexity=215.2784

Batch 5630, train_perplexity=194.52306

Batch 5640, train_perplexity=210.42609

Batch 5650, train_perplexity=198.25633

Batch 5660, train_perplexity=202.05527

Batch 5670, train_perplexity=198.00475

Batch 5680, train_perplexity=188.58713

Batch 5690, train_perplexity=196.84738

Batch 5700, train_perplexity=190.20622

Batch 5710, train_perplexity=193.00311

Batch 5720, train_perplexity=207.05174

Batch 5730, train_perplexity=206.11623

Batch 5740, train_perplexity=188.61447

Batch 5750, train_perplexity=205.75357

Batch 5760, train_perplexity=200.51173

Batch 5770, train_perplexity=201.08806

Batch 5780, train_perplexity=208.39177

Batch 5790, train_perplexity=205.92653

Batch 5800, train_perplexity=208.8443

Batch 5810, train_perplexity=193.06929

Batch 5820, train_perplexity=196.90709

Batch 5830, train_perplexity=205.40656

Batch 5840, train_perplexity=184.21588

Batch 5850, train_perplexity=205.89706

Batch 5860, train_perplexity=203.37897

Batch 5870, train_perplexity=196.09192

Batch 5880, train_perplexity=184.87251

Batch 5890, train_perplexity=212.98775

Batch 5900, train_perplexity=210.333

Batch 5910, train_perplexity=190.26373

Batch 5920, train_perplexity=182.81865

Batch 5930, train_perplexity=198.05658

Batch 5940, train_perplexity=209.07726

Batch 5950, train_perplexity=194.92752

Batch 5960, train_perplexity=191.91663

Batch 5970, train_perplexity=201.70381

Batch 5980, train_perplexity=197.63809

Batch 5990, train_perplexity=189.17381

Batch 6000, train_perplexity=185.8709

Batch 6010, train_perplexity=188.98772

Batch 6020, train_perplexity=204.95221

Batch 6030, train_perplexity=198.37889

Batch 6040, train_perplexity=202.0284

Batch 6050, train_perplexity=185.89616

Batch 6060, train_perplexity=177.97966

Batch 6070, train_perplexity=191.1037

Batch 6080, train_perplexity=193.21121

Batch 6090, train_perplexity=214.81326

Batch 6100, train_perplexity=201.9942

Batch 6110, train_perplexity=186.77998

Batch 6120, train_perplexity=187.88002

Batch 6130, train_perplexity=185.4159

Batch 6140, train_perplexity=204.79921

Batch 6150, train_perplexity=193.75157

Batch 6160, train_perplexity=195.12886

Batch 6170, train_perplexity=182.43913

Batch 6180, train_perplexity=187.28557

Batch 6190, train_perplexity=192.58784

Batch 6200, train_perplexity=173.18912

Batch 6210, train_perplexity=197.51439

Batch 6220, train_perplexity=188.30893

Batch 6230, train_perplexity=188.81334

Batch 6240, train_perplexity=199.33156

Batch 6250, train_perplexity=209.0926

Batch 6260, train_perplexity=184.55359

Batch 6270, train_perplexity=202.40445

Batch 6280, train_perplexity=204.21742

Batch 6290, train_perplexity=196.1789

Batch 6300, train_perplexity=198.57982

Batch 6310, train_perplexity=185.74092

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00029-of-00100
Loaded 306680 sentences.
Finished loading
Batch 6320, train_perplexity=171.74132

Batch 6330, train_perplexity=179.16699

Batch 6340, train_perplexity=177.19467

Batch 6350, train_perplexity=189.73698

Batch 6360, train_perplexity=189.25212

Batch 6370, train_perplexity=176.78215

Batch 6380, train_perplexity=169.47069

Batch 6390, train_perplexity=198.76634

Batch 6400, train_perplexity=191.18555

Batch 6410, train_perplexity=199.3923

Batch 6420, train_perplexity=197.228

Batch 6430, train_perplexity=171.26595

Batch 6440, train_perplexity=203.93134

Batch 6450, train_perplexity=181.84584

Batch 6460, train_perplexity=195.54233

Batch 6470, train_perplexity=178.20265

Batch 6480, train_perplexity=176.84842

Batch 6490, train_perplexity=193.0183

Batch 6500, train_perplexity=194.68787

Batch 6510, train_perplexity=210.90485

Batch 6520, train_perplexity=206.73851

Batch 6530, train_perplexity=184.5681

Batch 6540, train_perplexity=180.81122

Batch 6550, train_perplexity=188.69373

Batch 6560, train_perplexity=171.88043

Batch 6570, train_perplexity=202.93404

Batch 6580, train_perplexity=180.29533

Batch 6590, train_perplexity=187.65144

Batch 6600, train_perplexity=183.99913

Batch 6610, train_perplexity=191.48444

Batch 6620, train_perplexity=194.44757

Batch 6630, train_perplexity=182.41539

Batch 6640, train_perplexity=181.3393

Batch 6650, train_perplexity=184.86803

Batch 6660, train_perplexity=191.95772

Batch 6670, train_perplexity=202.82713

Batch 6680, train_perplexity=182.75197

Batch 6690, train_perplexity=183.09677

Batch 6700, train_perplexity=202.3868

Batch 6710, train_perplexity=176.47028

Batch 6720, train_perplexity=184.64557

Batch 6730, train_perplexity=172.17845

Batch 6740, train_perplexity=185.32053

Batch 6750, train_perplexity=179.9553

Batch 6760, train_perplexity=188.56267

Batch 6770, train_perplexity=175.0074

Batch 6780, train_perplexity=196.4249

Batch 6790, train_perplexity=183.70027

Batch 6800, train_perplexity=194.32133

Batch 6810, train_perplexity=168.85788

Batch 6820, train_perplexity=187.99474

Batch 6830, train_perplexity=164.91145

Batch 6840, train_perplexity=192.0924

Batch 6850, train_perplexity=187.9078

Batch 6860, train_perplexity=175.95906

Batch 6870, train_perplexity=186.67892

Batch 6880, train_perplexity=178.53282

Batch 6890, train_perplexity=178.84119

Batch 6900, train_perplexity=192.12758

Batch 6910, train_perplexity=167.80182

Batch 6920, train_perplexity=169.50899

Batch 6930, train_perplexity=177.39401

Batch 6940, train_perplexity=185.7272

Batch 6950, train_perplexity=185.57962

Batch 6960, train_perplexity=188.38167

Batch 6970, train_perplexity=194.63097

Batch 6980, train_perplexity=181.16989

Batch 6990, train_perplexity=195.7219

Batch 7000, train_perplexity=179.74031

Batch 7010, train_perplexity=175.19627

Batch 7020, train_perplexity=170.92264

Batch 7030, train_perplexity=179.33383

Batch 7040, train_perplexity=190.75919

Batch 7050, train_perplexity=182.93907

Batch 7060, train_perplexity=185.96327

Batch 7070, train_perplexity=169.83795

Batch 7080, train_perplexity=185.76245

Batch 7090, train_perplexity=198.22504

Batch 7100, train_perplexity=171.88445

Batch 7110, train_perplexity=167.53639

Batch 7120, train_perplexity=194.14499

Batch 7130, train_perplexity=173.43431

Batch 7140, train_perplexity=173.07065

Batch 7150, train_perplexity=190.54828

Batch 7160, train_perplexity=193.97916

Batch 7170, train_perplexity=174.145

Batch 7180, train_perplexity=164.231

Batch 7190, train_perplexity=181.35191

Batch 7200, train_perplexity=159.47269

Batch 7210, train_perplexity=192.62386

Batch 7220, train_perplexity=195.01138

Batch 7230, train_perplexity=183.11484

Batch 7240, train_perplexity=181.37405

Batch 7250, train_perplexity=182.44922

Batch 7260, train_perplexity=175.04846

Batch 7270, train_perplexity=178.33134

Batch 7280, train_perplexity=160.30142

Batch 7290, train_perplexity=185.47365

Batch 7300, train_perplexity=184.48003

Batch 7310, train_perplexity=163.72707

Batch 7320, train_perplexity=180.67952

Batch 7330, train_perplexity=175.48112

Batch 7340, train_perplexity=179.32819
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 7350, train_perplexity=196.66368

Batch 7360, train_perplexity=180.8551

Batch 7370, train_perplexity=193.84286

Batch 7380, train_perplexity=168.36044

Batch 7390, train_perplexity=181.62262

Batch 7400, train_perplexity=161.12831

Batch 7410, train_perplexity=186.09679

Batch 7420, train_perplexity=182.85309

Batch 7430, train_perplexity=174.1005

Batch 7440, train_perplexity=172.50436

Batch 7450, train_perplexity=173.29196

Batch 7460, train_perplexity=189.61824

Batch 7470, train_perplexity=175.00824

Batch 7480, train_perplexity=172.19897

Batch 7490, train_perplexity=173.21216

Batch 7500, train_perplexity=177.16737

Batch 7510, train_perplexity=165.5127

Batch 7520, train_perplexity=184.44528

Batch 7530, train_perplexity=160.19322

Batch 7540, train_perplexity=180.77191

Batch 7550, train_perplexity=176.0769

Batch 7560, train_perplexity=190.35992

Batch 7570, train_perplexity=181.37181

Batch 7580, train_perplexity=182.15341

Batch 7590, train_perplexity=187.92204

Batch 7600, train_perplexity=180.15381

Batch 7610, train_perplexity=158.30971

Batch 7620, train_perplexity=183.47073

Batch 7630, train_perplexity=178.45929

Batch 7640, train_perplexity=191.30219

Batch 7650, train_perplexity=180.70917

Batch 7660, train_perplexity=186.20436

Batch 7670, train_perplexity=178.16501

Batch 7680, train_perplexity=181.28534

Batch 7690, train_perplexity=186.52365

Batch 7700, train_perplexity=192.59831

Batch 7710, train_perplexity=175.4106

Batch 7720, train_perplexity=159.79407

Batch 7730, train_perplexity=173.69882

Batch 7740, train_perplexity=180.27333

Batch 7750, train_perplexity=183.17249

Batch 7760, train_perplexity=183.68442

Batch 7770, train_perplexity=176.22331

Batch 7780, train_perplexity=170.14703

Batch 7790, train_perplexity=175.83904

Batch 7800, train_perplexity=176.61624

Batch 7810, train_perplexity=169.4765

Batch 7820, train_perplexity=180.32819

Batch 7830, train_perplexity=172.95944

Batch 7840, train_perplexity=181.83058

Batch 7850, train_perplexity=150.39062

Batch 7860, train_perplexity=176.41937

Batch 7870, train_perplexity=172.34627

Batch 7880, train_perplexity=171.38922

Batch 7890, train_perplexity=185.93747

Batch 7900, train_perplexity=172.77034

Batch 7910, train_perplexity=178.0389

Batch 7920, train_perplexity=164.45944

Batch 7930, train_perplexity=171.73518

Batch 7940, train_perplexity=166.72287

Batch 7950, train_perplexity=161.84335

Batch 7960, train_perplexity=173.60352

Batch 7970, train_perplexity=177.55887

Batch 7980, train_perplexity=166.51218

Batch 7990, train_perplexity=168.76257

Batch 8000, train_perplexity=170.58498

Batch 8010, train_perplexity=161.89893

Batch 8020, train_perplexity=172.87624

Batch 8030, train_perplexity=167.43065

Batch 8040, train_perplexity=167.2563

Batch 8050, train_perplexity=196.33012

Batch 8060, train_perplexity=177.03606

Batch 8070, train_perplexity=183.97684

Batch 8080, train_perplexity=176.2692

Batch 8090, train_perplexity=171.27542

Batch 8100, train_perplexity=158.50586

Batch 8110, train_perplexity=178.48158

Batch 8120, train_perplexity=179.4618

Batch 8130, train_perplexity=184.78755

Batch 8140, train_perplexity=171.75311

Batch 8150, train_perplexity=175.28535

Batch 8160, train_perplexity=176.46573

Batch 8170, train_perplexity=171.75041

Batch 8180, train_perplexity=163.72003

Batch 8190, train_perplexity=168.02737

Batch 8200, train_perplexity=170.45976

Batch 8210, train_perplexity=157.69958

Batch 8220, train_perplexity=177.76778

Batch 8230, train_perplexity=152.5455

Batch 8240, train_perplexity=155.57155

Batch 8250, train_perplexity=174.1141

Batch 8260, train_perplexity=161.08652

Batch 8270, train_perplexity=177.20972

Batch 8280, train_perplexity=198.43575

Batch 8290, train_perplexity=177.14972

Batch 8300, train_perplexity=189.92943

Batch 8310, train_perplexity=175.2383

Batch 8320, train_perplexity=159.4061

Batch 8330, train_perplexity=170.52968

Batch 8340, train_perplexity=181.87498

Batch 8350, train_perplexity=192.59721

Batch 8360, train_perplexity=161.96455

Batch 8370, train_perplexity=164.74812

Batch 8380, train_perplexity=179.91455

Batch 8390, train_perplexity=165.01183

Batch 8400, train_perplexity=153.10152

Batch 8410, train_perplexity=163.56842

Batch 8420, train_perplexity=178.13435

Batch 8430, train_perplexity=164.00664

Batch 8440, train_perplexity=189.20197

Batch 8450, train_perplexity=151.58995

Batch 8460, train_perplexity=157.22258

Batch 8470, train_perplexity=170.34178

Batch 8480, train_perplexity=181.53654

Batch 8490, train_perplexity=167.83191

Batch 8500, train_perplexity=169.77756

Batch 8510, train_perplexity=181.88339

Batch 8520, train_perplexity=156.1613

Batch 8530, train_perplexity=177.76558

Batch 8540, train_perplexity=163.41898

Batch 8550, train_perplexity=161.8743

Batch 8560, train_perplexity=163.60469

Batch 8570, train_perplexity=163.72153

Batch 8580, train_perplexity=166.96744

Batch 8590, train_perplexity=159.83614

Batch 8600, train_perplexity=156.02353

Batch 8610, train_perplexity=168.75598

Batch 8620, train_perplexity=161.99313

Batch 8630, train_perplexity=178.26895

Batch 8640, train_perplexity=164.47136

Batch 8650, train_perplexity=189.62375

Batch 8660, train_perplexity=178.1341

Batch 8670, train_perplexity=166.77313

Batch 8680, train_perplexity=179.33905

Batch 8690, train_perplexity=162.76036

Batch 8700, train_perplexity=185.46436

Batch 8710, train_perplexity=149.97798

Batch 8720, train_perplexity=165.09557

Batch 8730, train_perplexity=156.44684

Batch 8740, train_perplexity=175.90051

Batch 8750, train_perplexity=164.53223

Batch 8760, train_perplexity=183.14664

Batch 8770, train_perplexity=185.6036

Batch 8780, train_perplexity=163.16125

Batch 8790, train_perplexity=166.72876

Batch 8800, train_perplexity=174.69424

Batch 8810, train_perplexity=185.47435

Batch 8820, train_perplexity=151.99136

Batch 8830, train_perplexity=177.98271

Batch 8840, train_perplexity=166.92715

Batch 8850, train_perplexity=165.10698

Batch 8860, train_perplexity=140.71013

Batch 8870, train_perplexity=173.05258

Batch 8880, train_perplexity=175.88222

Batch 8890, train_perplexity=174.21709

Batch 8900, train_perplexity=177.94707

Batch 8910, train_perplexity=159.07974

Batch 8920, train_perplexity=162.98225

Batch 8930, train_perplexity=160.14052

Batch 8940, train_perplexity=165.0219

Batch 8950, train_perplexity=171.22797

Batch 8960, train_perplexity=172.26936

Batch 8970, train_perplexity=164.38637

Batch 8980, train_perplexity=170.48854

Batch 8990, train_perplexity=150.44112

Batch 9000, train_perplexity=161.16995

Batch 9010, train_perplexity=154.98204

Batch 9020, train_perplexity=172.43143

Batch 9030, train_perplexity=145.4959

Batch 9040, train_perplexity=161.3979

Batch 9050, train_perplexity=170.31076

Batch 9060, train_perplexity=146.87187

Batch 9070, train_perplexity=180.9342

Batch 9080, train_perplexity=172.36401

Batch 9090, train_perplexity=158.15367

Batch 9100, train_perplexity=164.2776

Batch 9110, train_perplexity=163.58223

Batch 9120, train_perplexity=166.88275

Batch 9130, train_perplexity=167.88185

Batch 9140, train_perplexity=156.67484

Batch 9150, train_perplexity=166.79683

Batch 9160, train_perplexity=179.30714

Batch 9170, train_perplexity=177.02695

Batch 9180, train_perplexity=143.30748

Batch 9190, train_perplexity=159.70152

Batch 9200, train_perplexity=179.64108

Batch 9210, train_perplexity=151.22781

Batch 9220, train_perplexity=173.76543

Batch 9230, train_perplexity=156.73087

Batch 9240, train_perplexity=174.25148

Batch 9250, train_perplexity=158.45341

Batch 9260, train_perplexity=160.33093

Batch 9270, train_perplexity=149.40453

Batch 9280, train_perplexity=169.54941

Batch 9290, train_perplexity=153.53989

Batch 9300, train_perplexity=153.849

Batch 9310, train_perplexity=155.75117

Batch 9320, train_perplexity=142.32921

Batch 9330, train_perplexity=173.89275

Batch 9340, train_perplexity=149.74031

Batch 9350, train_perplexity=154.10489

Batch 9360, train_perplexity=160.9312

Batch 9370, train_perplexity=152.69106

Batch 9380, train_perplexity=176.9763

Batch 9390, train_perplexity=152.13654

Batch 9400, train_perplexity=154.63081
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9410, train_perplexity=160.26794

Batch 9420, train_perplexity=164.64877

Batch 9430, train_perplexity=176.856

Batch 9440, train_perplexity=143.76248

Batch 9450, train_perplexity=179.8545

Batch 9460, train_perplexity=150.02304

Batch 9470, train_perplexity=159.38922

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00077-of-00100
Loaded 305798 sentences.
Finished loading
Batch 9480, train_perplexity=155.99095

Batch 9490, train_perplexity=169.05276

Batch 9500, train_perplexity=165.5899

Batch 9510, train_perplexity=158.67368

Batch 9520, train_perplexity=158.02281

Batch 9530, train_perplexity=155.94753

Batch 9540, train_perplexity=159.54343

Batch 9550, train_perplexity=155.62497

Batch 9560, train_perplexity=167.3276

Batch 9570, train_perplexity=159.89415

Batch 9580, train_perplexity=157.30792

Batch 9590, train_perplexity=165.41423

Batch 9600, train_perplexity=158.00307

Batch 9610, train_perplexity=155.57564

Batch 9620, train_perplexity=152.35258

Batch 9630, train_perplexity=157.89612

Batch 9640, train_perplexity=152.74538

Batch 9650, train_perplexity=162.57738

Batch 9660, train_perplexity=164.66817

Batch 9670, train_perplexity=154.98277

Batch 9680, train_perplexity=156.45773

Batch 9690, train_perplexity=166.4358

Batch 9700, train_perplexity=157.89822

Batch 9710, train_perplexity=136.28511

Batch 9720, train_perplexity=161.93044

Batch 9730, train_perplexity=155.82063

Batch 9740, train_perplexity=144.49069

Batch 9750, train_perplexity=164.63167

Batch 9760, train_perplexity=154.58127

Batch 9770, train_perplexity=158.31831

Batch 9780, train_perplexity=153.09334

Batch 9790, train_perplexity=164.81987

Batch 9800, train_perplexity=162.27702

Batch 9810, train_perplexity=154.09863

Batch 9820, train_perplexity=143.22728

Batch 9830, train_perplexity=142.98312

Batch 9840, train_perplexity=167.32889

Batch 9850, train_perplexity=160.09196

Batch 9860, train_perplexity=177.47643

Batch 9870, train_perplexity=156.74477

Batch 9880, train_perplexity=167.07216

Batch 9890, train_perplexity=145.45634

Batch 9900, train_perplexity=143.79779

Batch 9910, train_perplexity=155.29324

Batch 9920, train_perplexity=171.83455

Batch 9930, train_perplexity=154.63759

Batch 9940, train_perplexity=151.76108

Batch 9950, train_perplexity=151.01321

Batch 9960, train_perplexity=158.25461

Batch 9970, train_perplexity=157.71538

Batch 9980, train_perplexity=165.32086

Batch 9990, train_perplexity=151.15413

Batch 10000, train_perplexity=150.99521

Batch 10010, train_perplexity=171.17818

Batch 10020, train_perplexity=139.40298

Batch 10030, train_perplexity=163.38594

Batch 10040, train_perplexity=158.45554

Batch 10050, train_perplexity=150.91963

Batch 10060, train_perplexity=151.66913

Batch 10070, train_perplexity=144.79112

Batch 10080, train_perplexity=148.22404

Batch 10090, train_perplexity=171.55847

Batch 10100, train_perplexity=153.61223

Batch 10110, train_perplexity=161.46126

Batch 10120, train_perplexity=180.40146

Batch 10130, train_perplexity=163.68172

Batch 10140, train_perplexity=166.33347

Batch 10150, train_perplexity=155.86076

Batch 10160, train_perplexity=164.78365

Batch 10170, train_perplexity=167.3379

Batch 10180, train_perplexity=154.15302

Batch 10190, train_perplexity=156.57893

Batch 10200, train_perplexity=168.11417

Batch 10210, train_perplexity=145.43846

Batch 10220, train_perplexity=158.3042

Batch 10230, train_perplexity=150.46135

Batch 10240, train_perplexity=145.24628

Batch 10250, train_perplexity=160.8215

Batch 10260, train_perplexity=148.3393

Batch 10270, train_perplexity=164.62994

Batch 10280, train_perplexity=148.61215

Batch 10290, train_perplexity=158.85233

Batch 10300, train_perplexity=166.16002

Batch 10310, train_perplexity=142.03964

Batch 10320, train_perplexity=159.73807

Batch 10330, train_perplexity=142.70604

Batch 10340, train_perplexity=151.93282

Batch 10350, train_perplexity=147.78691

Batch 10360, train_perplexity=154.2795

Batch 10370, train_perplexity=144.61186

Batch 10380, train_perplexity=160.78447

Batch 10390, train_perplexity=148.9061

Batch 10400, train_perplexity=143.26799

Batch 10410, train_perplexity=152.8898

Batch 10420, train_perplexity=147.26186

Batch 10430, train_perplexity=135.2484

Batch 10440, train_perplexity=149.17758

Batch 10450, train_perplexity=150.5377

Batch 10460, train_perplexity=142.58441

Batch 10470, train_perplexity=142.00226

Batch 10480, train_perplexity=154.65633

Batch 10490, train_perplexity=157.89815

Batch 10500, train_perplexity=148.33025

Batch 10510, train_perplexity=156.60776

Batch 10520, train_perplexity=141.46794

Batch 10530, train_perplexity=146.51828

Batch 10540, train_perplexity=154.92928

Batch 10550, train_perplexity=154.84508

Batch 10560, train_perplexity=160.29713

Batch 10570, train_perplexity=160.79697

Batch 10580, train_perplexity=149.01463

Batch 10590, train_perplexity=159.82082

Batch 10600, train_perplexity=147.99318

Batch 10610, train_perplexity=160.18796

Batch 10620, train_perplexity=146.96736

Batch 10630, train_perplexity=156.64473

Batch 10640, train_perplexity=146.08353

Batch 10650, train_perplexity=153.87512

Batch 10660, train_perplexity=145.49443

Batch 10670, train_perplexity=165.58824

Batch 10680, train_perplexity=164.16469

Batch 10690, train_perplexity=137.7712

Batch 10700, train_perplexity=150.21144

Batch 10710, train_perplexity=156.73453

Batch 10720, train_perplexity=158.40575

Batch 10730, train_perplexity=150.49637

Batch 10740, train_perplexity=139.4194

Batch 10750, train_perplexity=148.16977

Batch 10760, train_perplexity=152.70285

Batch 10770, train_perplexity=141.99467

Batch 10780, train_perplexity=147.7824

Batch 10790, train_perplexity=144.58002

Batch 10800, train_perplexity=145.71828

Batch 10810, train_perplexity=147.02028

Batch 10820, train_perplexity=153.17827

Batch 10830, train_perplexity=150.89934

Batch 10840, train_perplexity=150.3722

Batch 10850, train_perplexity=149.26482

Batch 10860, train_perplexity=147.70631

Batch 10870, train_perplexity=153.01094

Batch 10880, train_perplexity=168.31903

Batch 10890, train_perplexity=151.31682

Batch 10900, train_perplexity=152.61623

Batch 10910, train_perplexity=152.94377

Batch 10920, train_perplexity=159.86006

Batch 10930, train_perplexity=170.58107

Batch 10940, train_perplexity=153.57079

Batch 10950, train_perplexity=151.05196

Batch 10960, train_perplexity=138.25325

Batch 10970, train_perplexity=156.05731

Batch 10980, train_perplexity=158.1125

Batch 10990, train_perplexity=154.34352

Batch 11000, train_perplexity=147.82376

Batch 11010, train_perplexity=156.42491

Batch 11020, train_perplexity=147.94965

Batch 11030, train_perplexity=149.1783

Batch 11040, train_perplexity=165.1962

Batch 11050, train_perplexity=156.8715

Batch 11060, train_perplexity=142.31625

Batch 11070, train_perplexity=151.4716

Batch 11080, train_perplexity=155.27252

Batch 11090, train_perplexity=138.34908

Batch 11100, train_perplexity=165.70586

Batch 11110, train_perplexity=142.97064

Batch 11120, train_perplexity=143.05322

Batch 11130, train_perplexity=150.34818

Batch 11140, train_perplexity=154.3054

Batch 11150, train_perplexity=150.94525

Batch 11160, train_perplexity=147.65111

Batch 11170, train_perplexity=153.08853

Batch 11180, train_perplexity=148.04356

Batch 11190, train_perplexity=147.11124

Batch 11200, train_perplexity=158.55765

Batch 11210, train_perplexity=147.96982

Batch 11220, train_perplexity=141.78223

Batch 11230, train_perplexity=142.70631

Batch 11240, train_perplexity=156.80167

Batch 11250, train_perplexity=140.94987

Batch 11260, train_perplexity=170.56937

Batch 11270, train_perplexity=158.60915

Batch 11280, train_perplexity=147.74104

Batch 11290, train_perplexity=156.70218

Batch 11300, train_perplexity=149.98128

Batch 11310, train_perplexity=154.10275

Batch 11320, train_perplexity=133.39877
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11330, train_perplexity=144.0577

Batch 11340, train_perplexity=132.14348

Batch 11350, train_perplexity=145.5943

Batch 11360, train_perplexity=146.69809

Batch 11370, train_perplexity=134.26726

Batch 11380, train_perplexity=139.09834

Batch 11390, train_perplexity=146.72606

Batch 11400, train_perplexity=145.47514

Batch 11410, train_perplexity=151.86778

Batch 11420, train_perplexity=152.69586

Batch 11430, train_perplexity=155.06866

Batch 11440, train_perplexity=145.23506

Batch 11450, train_perplexity=139.71162

Batch 11460, train_perplexity=147.13495

Batch 11470, train_perplexity=149.31366

Batch 11480, train_perplexity=152.07155

Batch 11490, train_perplexity=153.75536

Batch 11500, train_perplexity=170.34251

Batch 11510, train_perplexity=156.36935

Batch 11520, train_perplexity=144.51495

Batch 11530, train_perplexity=150.25299

Batch 11540, train_perplexity=133.96585

Batch 11550, train_perplexity=148.28738

Batch 11560, train_perplexity=152.29832

Batch 11570, train_perplexity=147.63104

Batch 11580, train_perplexity=150.45001

Batch 11590, train_perplexity=146.60661

Batch 11600, train_perplexity=144.08731

Batch 11610, train_perplexity=155.3865

Batch 11620, train_perplexity=145.64047

Batch 11630, train_perplexity=134.40164

Batch 11640, train_perplexity=138.10547

Batch 11650, train_perplexity=126.95952

Batch 11660, train_perplexity=163.06978

Batch 11670, train_perplexity=139.9612

Batch 11680, train_perplexity=150.42699

Batch 11690, train_perplexity=148.22949

Batch 11700, train_perplexity=147.20709

Batch 11710, train_perplexity=144.77367

Batch 11720, train_perplexity=152.97316

Batch 11730, train_perplexity=155.56993

Batch 11740, train_perplexity=149.68362

Batch 11750, train_perplexity=154.0647

Batch 11760, train_perplexity=147.64941

Batch 11770, train_perplexity=127.010925

Batch 11780, train_perplexity=156.32841

Batch 11790, train_perplexity=144.56554

Batch 11800, train_perplexity=148.87422

Batch 11810, train_perplexity=157.8581

Batch 11820, train_perplexity=141.37245

Batch 11830, train_perplexity=128.60562

Batch 11840, train_perplexity=144.95554

Batch 11850, train_perplexity=154.86694

Batch 11860, train_perplexity=138.95706

Batch 11870, train_perplexity=145.95102

Batch 11880, train_perplexity=150.28682

Batch 11890, train_perplexity=145.91476

Batch 11900, train_perplexity=144.51729

Batch 11910, train_perplexity=136.20949

Batch 11920, train_perplexity=148.05917

Batch 11930, train_perplexity=151.59407

Batch 11940, train_perplexity=151.46957

Batch 11950, train_perplexity=136.4594

Batch 11960, train_perplexity=147.6697

Batch 11970, train_perplexity=144.8738

Batch 11980, train_perplexity=152.03023

Batch 11990, train_perplexity=143.49573

Batch 12000, train_perplexity=139.98376

Batch 12010, train_perplexity=131.51875

Batch 12020, train_perplexity=149.81787

Batch 12030, train_perplexity=135.28903

Batch 12040, train_perplexity=163.05142

Batch 12050, train_perplexity=143.52222

Batch 12060, train_perplexity=133.46953

Batch 12070, train_perplexity=140.86078

Batch 12080, train_perplexity=152.83696

Batch 12090, train_perplexity=150.70218

Batch 12100, train_perplexity=140.2349

Batch 12110, train_perplexity=145.73218

Batch 12120, train_perplexity=131.78223

Batch 12130, train_perplexity=144.02281

Batch 12140, train_perplexity=154.93016

Batch 12150, train_perplexity=145.74483

Batch 12160, train_perplexity=149.22084

Batch 12170, train_perplexity=159.08376

Batch 12180, train_perplexity=156.37062

Batch 12190, train_perplexity=139.07825

Batch 12200, train_perplexity=147.70476

Batch 12210, train_perplexity=148.01279

Batch 12220, train_perplexity=134.40555

Batch 12230, train_perplexity=140.3926

Batch 12240, train_perplexity=146.54889

Batch 12250, train_perplexity=140.011

Batch 12260, train_perplexity=163.19174

Batch 12270, train_perplexity=152.99475

Batch 12280, train_perplexity=143.058

Batch 12290, train_perplexity=136.42336

Batch 12300, train_perplexity=136.99739

Batch 12310, train_perplexity=156.39433

Batch 12320, train_perplexity=148.13713

Batch 12330, train_perplexity=150.45511

Batch 12340, train_perplexity=154.44128

Batch 12350, train_perplexity=156.6106

Batch 12360, train_perplexity=150.07248

Batch 12370, train_perplexity=139.71608

Batch 12380, train_perplexity=135.45015

Batch 12390, train_perplexity=137.74873

Batch 12400, train_perplexity=155.10817

Batch 12410, train_perplexity=144.99301

Batch 12420, train_perplexity=155.12903

Batch 12430, train_perplexity=139.86

Batch 12440, train_perplexity=147.69504

Batch 12450, train_perplexity=139.05145

Batch 12460, train_perplexity=136.5341

Batch 12470, train_perplexity=140.60283

Batch 12480, train_perplexity=135.31123

Batch 12490, train_perplexity=145.57298

Batch 12500, train_perplexity=142.98912

Batch 12510, train_perplexity=124.92547

Batch 12520, train_perplexity=141.1639

Batch 12530, train_perplexity=148.5802

Batch 12540, train_perplexity=158.16182

Batch 12550, train_perplexity=145.3204

Batch 12560, train_perplexity=141.6745

Batch 12570, train_perplexity=139.5845

Batch 12580, train_perplexity=154.12494

Batch 12590, train_perplexity=145.78542

Batch 12600, train_perplexity=130.54866

Batch 12610, train_perplexity=147.6932

Batch 12620, train_perplexity=134.31195

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00039-of-00100
Loaded 305933 sentences.
Finished loading
Batch 12630, train_perplexity=136.08212

Batch 12640, train_perplexity=138.17792

Batch 12650, train_perplexity=133.51447

Batch 12660, train_perplexity=137.50723

Batch 12670, train_perplexity=135.95319

Batch 12680, train_perplexity=149.95804

Batch 12690, train_perplexity=154.63539

Batch 12700, train_perplexity=147.68912

Batch 12710, train_perplexity=132.83829

Batch 12720, train_perplexity=128.83505

Batch 12730, train_perplexity=138.3368

Batch 12740, train_perplexity=133.07997

Batch 12750, train_perplexity=143.66833

Batch 12760, train_perplexity=147.39082

Batch 12770, train_perplexity=132.79724

Batch 12780, train_perplexity=131.78065

Batch 12790, train_perplexity=138.78093

Batch 12800, train_perplexity=143.67847

Batch 12810, train_perplexity=159.50008

Batch 12820, train_perplexity=131.04712

Batch 12830, train_perplexity=127.009895

Batch 12840, train_perplexity=130.20375

Batch 12850, train_perplexity=132.5294

Batch 12860, train_perplexity=138.24857

Batch 12870, train_perplexity=134.66106

Batch 12880, train_perplexity=141.31044

Batch 12890, train_perplexity=146.037

Batch 12900, train_perplexity=139.38164

Batch 12910, train_perplexity=155.21234

Batch 12920, train_perplexity=152.2719

Batch 12930, train_perplexity=137.68149

Batch 12940, train_perplexity=129.61824

Batch 12950, train_perplexity=138.5836

Batch 12960, train_perplexity=135.35486

Batch 12970, train_perplexity=125.19029

Batch 12980, train_perplexity=133.11195

Batch 12990, train_perplexity=135.6207

Batch 13000, train_perplexity=123.16198

Batch 13010, train_perplexity=135.3084

Batch 13020, train_perplexity=137.68793

Batch 13030, train_perplexity=134.5617

Batch 13040, train_perplexity=138.74599

Batch 13050, train_perplexity=141.6125

Batch 13060, train_perplexity=138.28865

Batch 13070, train_perplexity=137.40707

Batch 13080, train_perplexity=135.02377

Batch 13090, train_perplexity=138.22353

Batch 13100, train_perplexity=146.34511

Batch 13110, train_perplexity=132.6298

Batch 13120, train_perplexity=143.30789

Batch 13130, train_perplexity=137.23343

Batch 13140, train_perplexity=139.17775

Batch 13150, train_perplexity=138.02509

Batch 13160, train_perplexity=140.95874

Batch 13170, train_perplexity=134.6935

Batch 13180, train_perplexity=145.28992

Batch 13190, train_perplexity=124.772644

Batch 13200, train_perplexity=145.87206

Batch 13210, train_perplexity=137.06418

Batch 13220, train_perplexity=145.24724

Batch 13230, train_perplexity=131.94916
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13240, train_perplexity=131.21469

Batch 13250, train_perplexity=133.51804

Batch 13260, train_perplexity=147.88785

Batch 13270, train_perplexity=139.1061

Batch 13280, train_perplexity=133.24048

Batch 13290, train_perplexity=136.64705

Batch 13300, train_perplexity=125.96202

Batch 13310, train_perplexity=143.51851

Batch 13320, train_perplexity=130.22337

Batch 13330, train_perplexity=129.34578

Batch 13340, train_perplexity=133.71312

Batch 13350, train_perplexity=129.61676

Batch 13360, train_perplexity=134.28108

Batch 13370, train_perplexity=137.66455

Batch 13380, train_perplexity=132.01865

Batch 13390, train_perplexity=136.13826

Batch 13400, train_perplexity=140.13817

Batch 13410, train_perplexity=131.49655

Batch 13420, train_perplexity=136.69254

Batch 13430, train_perplexity=129.2414

Batch 13440, train_perplexity=138.29941

Batch 13450, train_perplexity=128.37598

Batch 13460, train_perplexity=122.075165

Batch 13470, train_perplexity=131.02588

Batch 13480, train_perplexity=140.06361

Batch 13490, train_perplexity=130.12096

Batch 13500, train_perplexity=141.79575

Batch 13510, train_perplexity=133.8079

Batch 13520, train_perplexity=130.07921

Batch 13530, train_perplexity=137.6723

Batch 13540, train_perplexity=134.06075

Batch 13550, train_perplexity=140.97205

Batch 13560, train_perplexity=145.92792

Batch 13570, train_perplexity=124.77087

Batch 13580, train_perplexity=125.66535

Batch 13590, train_perplexity=140.80504

Batch 13600, train_perplexity=136.39062

Batch 13610, train_perplexity=133.03682

Batch 13620, train_perplexity=140.16524

Batch 13630, train_perplexity=130.05682

Batch 13640, train_perplexity=151.94571

Batch 13650, train_perplexity=136.17047

Batch 13660, train_perplexity=141.12238

Batch 13670, train_perplexity=121.38327

Batch 13680, train_perplexity=140.44348

Batch 13690, train_perplexity=151.64099

Batch 13700, train_perplexity=132.8236

Batch 13710, train_perplexity=128.09152

Batch 13720, train_perplexity=141.08987

Batch 13730, train_perplexity=140.16971

Batch 13740, train_perplexity=146.48985

Batch 13750, train_perplexity=138.08835

Batch 13760, train_perplexity=140.24084

Batch 13770, train_perplexity=138.8176

Batch 13780, train_perplexity=130.88756

Batch 13790, train_perplexity=131.24611

Batch 13800, train_perplexity=121.928444

Batch 13810, train_perplexity=133.63919

Batch 13820, train_perplexity=125.824066

Batch 13830, train_perplexity=138.26492

Batch 13840, train_perplexity=140.48962

Batch 13850, train_perplexity=117.01698

Batch 13860, train_perplexity=116.365295

Batch 13870, train_perplexity=146.66298

Batch 13880, train_perplexity=127.1965

Batch 13890, train_perplexity=150.76866

Batch 13900, train_perplexity=125.24445

Batch 13910, train_perplexity=146.10693

Batch 13920, train_perplexity=131.26175

Batch 13930, train_perplexity=147.88623

Batch 13940, train_perplexity=147.57466

Batch 13950, train_perplexity=133.03346

Batch 13960, train_perplexity=134.60316

Batch 13970, train_perplexity=128.12329

Batch 13980, train_perplexity=137.7957

Batch 13990, train_perplexity=140.53232

Batch 14000, train_perplexity=131.715

Batch 14010, train_perplexity=136.87001

Batch 14020, train_perplexity=132.71875

Batch 14030, train_perplexity=141.1102

Batch 14040, train_perplexity=136.50925

Batch 14050, train_perplexity=124.48223

Batch 14060, train_perplexity=135.89615

Batch 14070, train_perplexity=139.56067

Batch 14080, train_perplexity=141.39484

Batch 14090, train_perplexity=142.13423

Batch 14100, train_perplexity=132.25366

Batch 14110, train_perplexity=145.04639

Batch 14120, train_perplexity=133.21126

Batch 14130, train_perplexity=131.1213

Batch 14140, train_perplexity=134.13338

Batch 14150, train_perplexity=131.58888

Batch 14160, train_perplexity=130.19855

Batch 14170, train_perplexity=126.43374

Batch 14180, train_perplexity=133.19716

Batch 14190, train_perplexity=125.64361

Batch 14200, train_perplexity=134.31956

Batch 14210, train_perplexity=116.577896

Batch 14220, train_perplexity=137.46579

Batch 14230, train_perplexity=125.21358

Batch 14240, train_perplexity=144.49138

Batch 14250, train_perplexity=134.56125

Batch 14260, train_perplexity=141.64243

Batch 14270, train_perplexity=127.0423

Batch 14280, train_perplexity=136.76712

Batch 14290, train_perplexity=142.78003

Batch 14300, train_perplexity=138.7591

Batch 14310, train_perplexity=137.30719

Batch 14320, train_perplexity=141.15002

Batch 14330, train_perplexity=123.93727

Batch 14340, train_perplexity=128.4807

Batch 14350, train_perplexity=138.2497

Batch 14360, train_perplexity=124.43433

Batch 14370, train_perplexity=121.48124

Batch 14380, train_perplexity=140.95538

Batch 14390, train_perplexity=128.77295

Batch 14400, train_perplexity=153.18477

Batch 14410, train_perplexity=134.93263

Batch 14420, train_perplexity=147.23643

Batch 14430, train_perplexity=114.15138

Batch 14440, train_perplexity=135.82663

Batch 14450, train_perplexity=134.96674

Batch 14460, train_perplexity=149.05876

Batch 14470, train_perplexity=123.02182

Batch 14480, train_perplexity=125.84753

Batch 14490, train_perplexity=120.088905

Batch 14500, train_perplexity=117.217026

Batch 14510, train_perplexity=131.00513

Batch 14520, train_perplexity=136.40865

Batch 14530, train_perplexity=127.065865

Batch 14540, train_perplexity=130.41153

Batch 14550, train_perplexity=141.60493

Batch 14560, train_perplexity=126.80185

Batch 14570, train_perplexity=132.7542

Batch 14580, train_perplexity=129.58469

Batch 14590, train_perplexity=127.68916

Batch 14600, train_perplexity=125.54904

Batch 14610, train_perplexity=131.91066

Batch 14620, train_perplexity=118.27068

Batch 14630, train_perplexity=127.975105

Batch 14640, train_perplexity=125.274254

Batch 14650, train_perplexity=119.28221

Batch 14660, train_perplexity=134.43913

Batch 14670, train_perplexity=135.78928

Batch 14680, train_perplexity=151.77866

Batch 14690, train_perplexity=129.51672

Batch 14700, train_perplexity=137.59644

Batch 14710, train_perplexity=136.47826

Batch 14720, train_perplexity=129.89381

Batch 14730, train_perplexity=121.27456

Batch 14740, train_perplexity=128.25795

Batch 14750, train_perplexity=135.81984

Batch 14760, train_perplexity=121.735985

Batch 14770, train_perplexity=120.54437

Batch 14780, train_perplexity=124.02927

Batch 14790, train_perplexity=143.26894

Batch 14800, train_perplexity=119.85555

Batch 14810, train_perplexity=125.46837

Batch 14820, train_perplexity=133.54529

Batch 14830, train_perplexity=134.8836

Batch 14840, train_perplexity=132.5887

Batch 14850, train_perplexity=119.876816

Batch 14860, train_perplexity=144.16077

Batch 14870, train_perplexity=113.319336

Batch 14880, train_perplexity=137.19482

Batch 14890, train_perplexity=130.65303

Batch 14900, train_perplexity=119.54699

Batch 14910, train_perplexity=136.70303

Batch 14920, train_perplexity=127.478546

Batch 14930, train_perplexity=133.77032

Batch 14940, train_perplexity=132.74545

Batch 14950, train_perplexity=142.48532

Batch 14960, train_perplexity=134.34302

Batch 14970, train_perplexity=128.8788

Batch 14980, train_perplexity=121.10496

Batch 14990, train_perplexity=124.410484

Batch 15000, train_perplexity=134.61888

Batch 15010, train_perplexity=134.63814

Batch 15020, train_perplexity=135.16194

Batch 15030, train_perplexity=146.23434

Batch 15040, train_perplexity=128.75189

Batch 15050, train_perplexity=130.1392

Batch 15060, train_perplexity=128.05812

Batch 15070, train_perplexity=112.3783

Batch 15080, train_perplexity=134.47427

Batch 15090, train_perplexity=124.28206

Batch 15100, train_perplexity=128.19032

Batch 15110, train_perplexity=151.66399

Batch 15120, train_perplexity=115.583954

Batch 15130, train_perplexity=133.21355

Batch 15140, train_perplexity=132.23524

Batch 15150, train_perplexity=151.75905

Batch 15160, train_perplexity=130.78056

Batch 15170, train_perplexity=131.5375

Batch 15180, train_perplexity=124.726845

Batch 15190, train_perplexity=145.75546

Batch 15200, train_perplexity=113.00448

Batch 15210, train_perplexity=115.45385

Batch 15220, train_perplexity=128.9485

Batch 15230, train_perplexity=117.042595
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15240, train_perplexity=138.98291

Batch 15250, train_perplexity=142.29602

Batch 15260, train_perplexity=113.038

Batch 15270, train_perplexity=137.17546

Batch 15280, train_perplexity=123.688484

Batch 15290, train_perplexity=131.2446

Batch 15300, train_perplexity=141.98642

Batch 15310, train_perplexity=142.95769

Batch 15320, train_perplexity=129.00435

Batch 15330, train_perplexity=136.81194

Batch 15340, train_perplexity=130.01831

Batch 15350, train_perplexity=134.10442

Batch 15360, train_perplexity=114.4416

Batch 15370, train_perplexity=127.036545

Batch 15380, train_perplexity=135.70131

Batch 15390, train_perplexity=135.6984

Batch 15400, train_perplexity=137.70007

Batch 15410, train_perplexity=143.70477

Batch 15420, train_perplexity=130.21928

Batch 15430, train_perplexity=127.36814

Batch 15440, train_perplexity=128.42258

Batch 15450, train_perplexity=136.38634

Batch 15460, train_perplexity=127.50657

Batch 15470, train_perplexity=128.17474

Batch 15480, train_perplexity=135.8197

Batch 15490, train_perplexity=133.72989

Batch 15500, train_perplexity=125.00025

Batch 15510, train_perplexity=127.82367

Batch 15520, train_perplexity=140.01393

Batch 15530, train_perplexity=117.96417

Batch 15540, train_perplexity=111.32128

Batch 15550, train_perplexity=140.60564

Batch 15560, train_perplexity=131.11412

Batch 15570, train_perplexity=129.36823

Batch 15580, train_perplexity=127.56161

Batch 15590, train_perplexity=122.89928

Batch 15600, train_perplexity=143.17682

Batch 15610, train_perplexity=122.879585

Batch 15620, train_perplexity=137.0285

Batch 15630, train_perplexity=126.459

Batch 15640, train_perplexity=129.16093

Batch 15650, train_perplexity=143.98175

Batch 15660, train_perplexity=125.04258

Batch 15670, train_perplexity=113.41837

Batch 15680, train_perplexity=144.10669

Batch 15690, train_perplexity=124.70389

Batch 15700, train_perplexity=135.83026

Batch 15710, train_perplexity=133.7755

Batch 15720, train_perplexity=130.9046

Batch 15730, train_perplexity=139.10728

Batch 15740, train_perplexity=129.93991

Batch 15750, train_perplexity=131.79982

Batch 15760, train_perplexity=120.24929

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00052-of-00100
Loaded 305378 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00052-of-00100
Loaded 305378 sentences.
Finished loading
Batch 15770, train_perplexity=112.84736

Batch 15780, train_perplexity=121.91496

Batch 15790, train_perplexity=138.06332

Batch 15800, train_perplexity=138.17133

Batch 15810, train_perplexity=131.87808

Batch 15820, train_perplexity=129.76543

Batch 15830, train_perplexity=131.75514

Batch 15840, train_perplexity=141.1063

Batch 15850, train_perplexity=120.042244

Batch 15860, train_perplexity=121.77273

Batch 15870, train_perplexity=123.0

Batch 15880, train_perplexity=134.42043

Batch 15890, train_perplexity=127.85317

Batch 15900, train_perplexity=120.65818

Batch 15910, train_perplexity=127.52135

Batch 15920, train_perplexity=133.40196

Batch 15930, train_perplexity=145.08112

Batch 15940, train_perplexity=133.16058

Batch 15950, train_perplexity=126.80282

Batch 15960, train_perplexity=131.86017

Batch 15970, train_perplexity=129.66968

Batch 15980, train_perplexity=122.46626

Batch 15990, train_perplexity=131.47675

Batch 16000, train_perplexity=124.822395

Batch 16010, train_perplexity=129.68408

Batch 16020, train_perplexity=125.94844

Batch 16030, train_perplexity=117.93111

Batch 16040, train_perplexity=124.38225

Batch 16050, train_perplexity=125.524254

Batch 16060, train_perplexity=132.61778

Batch 16070, train_perplexity=128.94273

Batch 16080, train_perplexity=133.3736

Batch 16090, train_perplexity=119.03312

Batch 16100, train_perplexity=121.06604

Batch 16110, train_perplexity=130.79092

Batch 16120, train_perplexity=120.60733

Batch 16130, train_perplexity=117.78465

Batch 16140, train_perplexity=124.73101

Batch 16150, train_perplexity=135.5887

Batch 16160, train_perplexity=130.18538

Batch 16170, train_perplexity=132.04352

Batch 16180, train_perplexity=128.5822

Batch 16190, train_perplexity=130.2835

Batch 16200, train_perplexity=127.51387

Batch 16210, train_perplexity=126.80946

Batch 16220, train_perplexity=129.64928

Batch 16230, train_perplexity=116.58273

Batch 16240, train_perplexity=123.101974

Batch 16250, train_perplexity=111.21501

Batch 16260, train_perplexity=115.480606

Batch 16270, train_perplexity=130.92595

Batch 16280, train_perplexity=137.53398

Batch 16290, train_perplexity=129.49074

Batch 16300, train_perplexity=112.50436

Batch 16310, train_perplexity=132.5595

Batch 16320, train_perplexity=123.82138

Batch 16330, train_perplexity=124.61342

Batch 16340, train_perplexity=131.14476

Batch 16350, train_perplexity=120.54857

Batch 16360, train_perplexity=117.34503

Batch 16370, train_perplexity=127.707794

Batch 16380, train_perplexity=130.05067

Batch 16390, train_perplexity=122.89084

Batch 16400, train_perplexity=119.83772

Batch 16410, train_perplexity=132.85654

Batch 16420, train_perplexity=136.8971

Batch 16430, train_perplexity=122.74373

Batch 16440, train_perplexity=132.41154

Batch 16450, train_perplexity=119.61097

Batch 16460, train_perplexity=123.04681

Batch 16470, train_perplexity=116.76956

Batch 16480, train_perplexity=107.04857

Batch 16490, train_perplexity=133.28853

Batch 16500, train_perplexity=117.5897

Batch 16510, train_perplexity=129.89934

Batch 16520, train_perplexity=131.87456

Batch 16530, train_perplexity=140.17961

Batch 16540, train_perplexity=129.77211

Batch 16550, train_perplexity=122.6355

Batch 16560, train_perplexity=136.68797

Batch 16570, train_perplexity=124.398384

Batch 16580, train_perplexity=123.468155

Batch 16590, train_perplexity=135.71198

Batch 16600, train_perplexity=128.03302

Batch 16610, train_perplexity=117.78858

Batch 16620, train_perplexity=126.45936

Batch 16630, train_perplexity=127.45812

Batch 16640, train_perplexity=135.86777

Batch 16650, train_perplexity=131.42271

Batch 16660, train_perplexity=132.53175

Batch 16670, train_perplexity=116.52949

Batch 16680, train_perplexity=116.72914

Batch 16690, train_perplexity=122.10503

Batch 16700, train_perplexity=121.113556

Batch 16710, train_perplexity=132.75926

Batch 16720, train_perplexity=109.04468

Batch 16730, train_perplexity=125.648994

Batch 16740, train_perplexity=126.859184

Batch 16750, train_perplexity=135.1276

Batch 16760, train_perplexity=132.3907

Batch 16770, train_perplexity=136.58504

Batch 16780, train_perplexity=129.12361

Batch 16790, train_perplexity=122.18948

Batch 16800, train_perplexity=114.79187

Batch 16810, train_perplexity=128.73692

Batch 16820, train_perplexity=131.7393

Batch 16830, train_perplexity=115.01262

Batch 16840, train_perplexity=133.16795

Batch 16850, train_perplexity=128.10014

Batch 16860, train_perplexity=125.47016

Batch 16870, train_perplexity=122.88205

Batch 16880, train_perplexity=127.13302

Batch 16890, train_perplexity=123.85492

Batch 16900, train_perplexity=126.01741

Batch 16910, train_perplexity=129.43307

Batch 16920, train_perplexity=123.555084

Batch 16930, train_perplexity=129.74489

Batch 16940, train_perplexity=124.82055

Batch 16950, train_perplexity=104.69472

Batch 16960, train_perplexity=135.92926

Batch 16970, train_perplexity=124.91707

Batch 16980, train_perplexity=124.38877

Batch 16990, train_perplexity=126.64432

Batch 17000, train_perplexity=131.68083

Batch 17010, train_perplexity=114.07526

Batch 17020, train_perplexity=138.32579

Batch 17030, train_perplexity=118.19863

Batch 17040, train_perplexity=135.98787

Batch 17050, train_perplexity=128.81607

Batch 17060, train_perplexity=117.70195

Batch 17070, train_perplexity=135.71101

Batch 17080, train_perplexity=116.35864

Batch 17090, train_perplexity=125.44138

Batch 17100, train_perplexity=107.34939

Batch 17110, train_perplexity=113.47436

Batch 17120, train_perplexity=126.020775

Batch 17130, train_perplexity=130.67596
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17140, train_perplexity=127.17819

Batch 17150, train_perplexity=123.21955

Batch 17160, train_perplexity=124.52491

Batch 17170, train_perplexity=121.11402

Batch 17180, train_perplexity=119.56951

Batch 17190, train_perplexity=123.17784

Batch 17200, train_perplexity=133.60019

Batch 17210, train_perplexity=119.40889

Batch 17220, train_perplexity=129.90756

Batch 17230, train_perplexity=136.65018

Batch 17240, train_perplexity=129.51753

Batch 17250, train_perplexity=113.75597

Batch 17260, train_perplexity=129.70195

Batch 17270, train_perplexity=115.690704

Batch 17280, train_perplexity=122.74431

Batch 17290, train_perplexity=122.91721

Batch 17300, train_perplexity=124.53495

Batch 17310, train_perplexity=124.212975

Batch 17320, train_perplexity=120.33086

Batch 17330, train_perplexity=131.77191

Batch 17340, train_perplexity=114.50448

Batch 17350, train_perplexity=111.50744

Batch 17360, train_perplexity=135.83855

Batch 17370, train_perplexity=114.896576

Batch 17380, train_perplexity=127.11956

Batch 17390, train_perplexity=123.69108

Batch 17400, train_perplexity=136.91171

Batch 17410, train_perplexity=120.71049

Batch 17420, train_perplexity=125.08718

Batch 17430, train_perplexity=110.08627

Batch 17440, train_perplexity=128.37488

Batch 17450, train_perplexity=106.30843

Batch 17460, train_perplexity=121.98608

Batch 17470, train_perplexity=130.81499

Batch 17480, train_perplexity=117.12612

Batch 17490, train_perplexity=118.90123

Batch 17500, train_perplexity=112.616745

Batch 17510, train_perplexity=128.1613

Batch 17520, train_perplexity=116.99539

Batch 17530, train_perplexity=113.93234

Batch 17540, train_perplexity=121.41615

Batch 17550, train_perplexity=132.32721

Batch 17560, train_perplexity=118.053085

Batch 17570, train_perplexity=127.67205

Batch 17580, train_perplexity=131.08612

Batch 17590, train_perplexity=133.7269

Batch 17600, train_perplexity=132.1799

Batch 17610, train_perplexity=124.85353

Batch 17620, train_perplexity=113.8978

Batch 17630, train_perplexity=123.42101

Batch 17640, train_perplexity=133.71733

Batch 17650, train_perplexity=134.10223

Batch 17660, train_perplexity=125.93583

Batch 17670, train_perplexity=127.64807

Batch 17680, train_perplexity=121.0014

Batch 17690, train_perplexity=120.96252

Batch 17700, train_perplexity=128.52132

Batch 17710, train_perplexity=132.35075

Batch 17720, train_perplexity=125.344406

Batch 17730, train_perplexity=130.51306

Batch 17740, train_perplexity=125.896385

Batch 17750, train_perplexity=137.58081

Batch 17760, train_perplexity=123.621506

Batch 17770, train_perplexity=109.35736

Batch 17780, train_perplexity=124.18881

Batch 17790, train_perplexity=126.05167

Batch 17800, train_perplexity=126.899475

Batch 17810, train_perplexity=118.734604

Batch 17820, train_perplexity=119.659004

Batch 17830, train_perplexity=109.19854

Batch 17840, train_perplexity=126.26733

Batch 17850, train_perplexity=117.660706

Batch 17860, train_perplexity=107.82819

Batch 17870, train_perplexity=123.64968

Batch 17880, train_perplexity=119.7726

Batch 17890, train_perplexity=119.170784

Batch 17900, train_perplexity=119.83841

Batch 17910, train_perplexity=116.56372

Batch 17920, train_perplexity=118.073296

Batch 17930, train_perplexity=113.50391

Batch 17940, train_perplexity=117.77505

Batch 17950, train_perplexity=121.87776

Batch 17960, train_perplexity=130.49539

Batch 17970, train_perplexity=111.96671

Batch 17980, train_perplexity=120.50414

Batch 17990, train_perplexity=110.29803

Batch 18000, train_perplexity=129.07874

Batch 18010, train_perplexity=124.58145

Batch 18020, train_perplexity=123.56964

Batch 18030, train_perplexity=123.09787

Batch 18040, train_perplexity=112.30229

Batch 18050, train_perplexity=127.12962

Batch 18060, train_perplexity=123.69904

Batch 18070, train_perplexity=119.25639

Batch 18080, train_perplexity=115.16189

Batch 18090, train_perplexity=121.07331

Batch 18100, train_perplexity=115.95636

Batch 18110, train_perplexity=127.31119

Batch 18120, train_perplexity=119.742966

Batch 18130, train_perplexity=112.72592

Batch 18140, train_perplexity=119.49387

Batch 18150, train_perplexity=126.454895

Batch 18160, train_perplexity=133.72295

Batch 18170, train_perplexity=114.73414

Batch 18180, train_perplexity=114.886

Batch 18190, train_perplexity=126.51208

Batch 18200, train_perplexity=126.17434

Batch 18210, train_perplexity=114.82762

Batch 18220, train_perplexity=123.018295

Batch 18230, train_perplexity=122.95684

Batch 18240, train_perplexity=120.871475

Batch 18250, train_perplexity=133.83368

Batch 18260, train_perplexity=108.05095

Batch 18270, train_perplexity=109.99482

Batch 18280, train_perplexity=123.04904

Batch 18290, train_perplexity=125.272224

Batch 18300, train_perplexity=125.206055

Batch 18310, train_perplexity=123.189354

Batch 18320, train_perplexity=121.60556

Batch 18330, train_perplexity=116.7827

Batch 18340, train_perplexity=127.32461

Batch 18350, train_perplexity=111.66749

Batch 18360, train_perplexity=116.96259

Batch 18370, train_perplexity=119.72081

Batch 18380, train_perplexity=126.00743

Batch 18390, train_perplexity=122.321526

Batch 18400, train_perplexity=147.00774

Batch 18410, train_perplexity=106.16994

Batch 18420, train_perplexity=124.95794

Batch 18430, train_perplexity=121.41968

Batch 18440, train_perplexity=123.68943

Batch 18450, train_perplexity=125.381584

Batch 18460, train_perplexity=115.39622

Batch 18470, train_perplexity=130.14839

Batch 18480, train_perplexity=117.48444

Batch 18490, train_perplexity=117.716095

Batch 18500, train_perplexity=130.84169

Batch 18510, train_perplexity=125.155556

Batch 18520, train_perplexity=119.184875

Batch 18530, train_perplexity=114.3733

Batch 18540, train_perplexity=131.07599

Batch 18550, train_perplexity=120.44716

Batch 18560, train_perplexity=131.21677

Batch 18570, train_perplexity=121.89322

Batch 18580, train_perplexity=122.69112

Batch 18590, train_perplexity=116.43501

Batch 18600, train_perplexity=121.77064

Batch 18610, train_perplexity=116.966835

Batch 18620, train_perplexity=121.05888

Batch 18630, train_perplexity=122.46229

Batch 18640, train_perplexity=118.31897

Batch 18650, train_perplexity=118.86909

Batch 18660, train_perplexity=128.14566

Batch 18670, train_perplexity=115.27913

Batch 18680, train_perplexity=114.50366

Batch 18690, train_perplexity=122.42259

Batch 18700, train_perplexity=116.417854

Batch 18710, train_perplexity=121.80816

Batch 18720, train_perplexity=118.42181

Batch 18730, train_perplexity=137.45819

Batch 18740, train_perplexity=114.676704

Batch 18750, train_perplexity=113.11656

Batch 18760, train_perplexity=117.6174

Batch 18770, train_perplexity=114.7314

Batch 18780, train_perplexity=109.27313

Batch 18790, train_perplexity=113.625534

Batch 18800, train_perplexity=123.818665

Batch 18810, train_perplexity=112.17331

Batch 18820, train_perplexity=120.54961

Batch 18830, train_perplexity=133.71478

Batch 18840, train_perplexity=125.61401

Batch 18850, train_perplexity=123.019585

Batch 18860, train_perplexity=114.88491

Batch 18870, train_perplexity=114.625206

Batch 18880, train_perplexity=116.781364

Batch 18890, train_perplexity=116.399315

Batch 18900, train_perplexity=119.173965

Batch 18910, train_perplexity=121.87555

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00008-of-00100
Loaded 307045 sentences.
Finished loading
Batch 18920, train_perplexity=123.71574

Batch 18930, train_perplexity=124.04938

Batch 18940, train_perplexity=120.288055

Batch 18950, train_perplexity=117.721985

Batch 18960, train_perplexity=119.45149

Batch 18970, train_perplexity=120.23163

Batch 18980, train_perplexity=120.76622

Batch 18990, train_perplexity=110.65256

Batch 19000, train_perplexity=112.555115

Batch 19010, train_perplexity=112.09091

Batch 19020, train_perplexity=132.15028

Batch 19030, train_perplexity=122.190475
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19040, train_perplexity=121.32731

Batch 19050, train_perplexity=124.75444

Batch 19060, train_perplexity=119.75444

Batch 19070, train_perplexity=130.79503

Batch 19080, train_perplexity=115.835495

Batch 19090, train_perplexity=123.35899

Batch 19100, train_perplexity=118.51891

Batch 19110, train_perplexity=110.44987

Batch 19120, train_perplexity=119.58599

Batch 19130, train_perplexity=118.93962

Batch 19140, train_perplexity=118.3579

Batch 19150, train_perplexity=113.457535

Batch 19160, train_perplexity=120.76749

Batch 19170, train_perplexity=116.184715

Batch 19180, train_perplexity=119.59078

Batch 19190, train_perplexity=110.521515

Batch 19200, train_perplexity=126.77452

Batch 19210, train_perplexity=113.21234

Batch 19220, train_perplexity=110.88797

Batch 19230, train_perplexity=122.36831

Batch 19240, train_perplexity=119.06389

Batch 19250, train_perplexity=115.75598

Batch 19260, train_perplexity=125.37309

Batch 19270, train_perplexity=127.30718

Batch 19280, train_perplexity=118.73075

Batch 19290, train_perplexity=114.5521

Batch 19300, train_perplexity=102.19498

Batch 19310, train_perplexity=118.43276

Batch 19320, train_perplexity=117.00984

Batch 19330, train_perplexity=125.01134

Batch 19340, train_perplexity=128.57545

Batch 19350, train_perplexity=115.34918

Batch 19360, train_perplexity=117.02245

Batch 19370, train_perplexity=111.14763

Batch 19380, train_perplexity=109.58286

Batch 19390, train_perplexity=115.04268

Batch 19400, train_perplexity=111.236916

Batch 19410, train_perplexity=111.84004

Batch 19420, train_perplexity=121.751015

Batch 19430, train_perplexity=121.26011

Batch 19440, train_perplexity=118.976204

Batch 19450, train_perplexity=111.95796

Batch 19460, train_perplexity=117.215065

Batch 19470, train_perplexity=121.31198

Batch 19480, train_perplexity=113.78146

Batch 19490, train_perplexity=129.37224

Batch 19500, train_perplexity=120.9631

Batch 19510, train_perplexity=112.5109

Batch 19520, train_perplexity=114.31986

Batch 19530, train_perplexity=123.448555

Batch 19540, train_perplexity=118.623116

Batch 19550, train_perplexity=110.926414

Batch 19560, train_perplexity=123.2105

Batch 19570, train_perplexity=123.21214

Batch 19580, train_perplexity=126.65078

Batch 19590, train_perplexity=116.92941

Batch 19600, train_perplexity=124.58151

Batch 19610, train_perplexity=116.7415

Batch 19620, train_perplexity=119.595

Batch 19630, train_perplexity=127.18237

Batch 19640, train_perplexity=114.67682

Batch 19650, train_perplexity=110.03285

Batch 19660, train_perplexity=116.94798

Batch 19670, train_perplexity=120.22154

Batch 19680, train_perplexity=121.3879

Batch 19690, train_perplexity=114.558

Batch 19700, train_perplexity=116.88621

Batch 19710, train_perplexity=117.031105

Batch 19720, train_perplexity=109.00662

Batch 19730, train_perplexity=116.80486

Batch 19740, train_perplexity=116.90143

Batch 19750, train_perplexity=107.887955

Batch 19760, train_perplexity=113.166466

Batch 19770, train_perplexity=113.361435

Batch 19780, train_perplexity=119.783165

Batch 19790, train_perplexity=114.09495

Batch 19800, train_perplexity=111.31831

Batch 19810, train_perplexity=111.30589

Batch 19820, train_perplexity=117.95428

Batch 19830, train_perplexity=115.650055

Batch 19840, train_perplexity=121.56492

Batch 19850, train_perplexity=113.69231

Batch 19860, train_perplexity=115.70483

Batch 19870, train_perplexity=103.66592

Batch 19880, train_perplexity=107.58619

Batch 19890, train_perplexity=113.37771

Batch 19900, train_perplexity=115.08372

Batch 19910, train_perplexity=107.71816

Batch 19920, train_perplexity=120.0861

Batch 19930, train_perplexity=124.32781

Batch 19940, train_perplexity=101.745605

Batch 19950, train_perplexity=123.515564

Batch 19960, train_perplexity=120.52857

Batch 19970, train_perplexity=115.91191

Batch 19980, train_perplexity=112.92099

Batch 19990, train_perplexity=121.29816

Batch 20000, train_perplexity=119.2672

Batch 20010, train_perplexity=119.84229

Batch 20020, train_perplexity=121.05351

Batch 20030, train_perplexity=111.11027

Batch 20040, train_perplexity=115.853004

Batch 20050, train_perplexity=114.249016

Batch 20060, train_perplexity=127.00541

Batch 20070, train_perplexity=112.198135

Batch 20080, train_perplexity=117.41264

Batch 20090, train_perplexity=112.071144

Batch 20100, train_perplexity=110.952446

Batch 20110, train_perplexity=118.46761

Batch 20120, train_perplexity=114.144684

Batch 20130, train_perplexity=127.39323

Batch 20140, train_perplexity=121.27982

Batch 20150, train_perplexity=119.42108

Batch 20160, train_perplexity=127.27313

Batch 20170, train_perplexity=121.4802

Batch 20180, train_perplexity=106.71841

Batch 20190, train_perplexity=125.05319

Batch 20200, train_perplexity=125.87045

Batch 20210, train_perplexity=122.88791

Batch 20220, train_perplexity=116.10054

Batch 20230, train_perplexity=105.22714

Batch 20240, train_perplexity=111.00981

Batch 20250, train_perplexity=106.95061

Batch 20260, train_perplexity=115.673935

Batch 20270, train_perplexity=117.142204

Batch 20280, train_perplexity=111.5051

Batch 20290, train_perplexity=123.528755

Batch 20300, train_perplexity=119.66014

Batch 20310, train_perplexity=125.97956

Batch 20320, train_perplexity=101.693756

Batch 20330, train_perplexity=103.60148

Batch 20340, train_perplexity=109.01468

Batch 20350, train_perplexity=108.413246

Batch 20360, train_perplexity=116.17352

Batch 20370, train_perplexity=114.25991

Batch 20380, train_perplexity=119.2025

Batch 20390, train_perplexity=117.88512

Batch 20400, train_perplexity=116.904495

Batch 20410, train_perplexity=123.8346

Batch 20420, train_perplexity=117.60736

Batch 20430, train_perplexity=116.60692

Batch 20440, train_perplexity=116.852325

Batch 20450, train_perplexity=111.09772

Batch 20460, train_perplexity=103.05752

Batch 20470, train_perplexity=112.21006

Batch 20480, train_perplexity=121.18779

Batch 20490, train_perplexity=108.79761

Batch 20500, train_perplexity=118.8093

Batch 20510, train_perplexity=112.11989

Batch 20520, train_perplexity=101.48086

Batch 20530, train_perplexity=114.82423

Batch 20540, train_perplexity=115.52456

Batch 20550, train_perplexity=113.26813

Batch 20560, train_perplexity=110.91541

Batch 20570, train_perplexity=108.8822

Batch 20580, train_perplexity=127.78698

Batch 20590, train_perplexity=106.95356

Batch 20600, train_perplexity=112.66084

Batch 20610, train_perplexity=112.95793

Batch 20620, train_perplexity=110.83516

Batch 20630, train_perplexity=115.36447

Batch 20640, train_perplexity=110.2158

Batch 20650, train_perplexity=133.21367

Batch 20660, train_perplexity=109.801186

Batch 20670, train_perplexity=113.49276

Batch 20680, train_perplexity=117.91576

Batch 20690, train_perplexity=111.86916

Batch 20700, train_perplexity=107.80845

Batch 20710, train_perplexity=118.56408

Batch 20720, train_perplexity=110.23519

Batch 20730, train_perplexity=124.624825

Batch 20740, train_perplexity=113.27071

Batch 20750, train_perplexity=116.18361

Batch 20760, train_perplexity=111.997154

Batch 20770, train_perplexity=116.969955

Batch 20780, train_perplexity=114.74437

Batch 20790, train_perplexity=121.68474

Batch 20800, train_perplexity=121.26398

Batch 20810, train_perplexity=121.24669

Batch 20820, train_perplexity=112.36148

Batch 20830, train_perplexity=120.81743

Batch 20840, train_perplexity=122.49336

Batch 20850, train_perplexity=115.609085

Batch 20860, train_perplexity=125.46113

Batch 20870, train_perplexity=107.42492

Batch 20880, train_perplexity=123.59015

Batch 20890, train_perplexity=113.465004

Batch 20900, train_perplexity=107.20064

Batch 20910, train_perplexity=124.463295

Batch 20920, train_perplexity=107.07593

Batch 20930, train_perplexity=115.5705

Batch 20940, train_perplexity=107.73023

Batch 20950, train_perplexity=109.39752

Batch 20960, train_perplexity=114.47489

Batch 20970, train_perplexity=121.87415

Batch 20980, train_perplexity=112.969246

Batch 20990, train_perplexity=108.27014

Batch 21000, train_perplexity=110.68759

Batch 21010, train_perplexity=108.06126

Batch 21020, train_perplexity=117.86971

Batch 21030, train_perplexity=103.424965
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 21040, train_perplexity=114.23703

Batch 21050, train_perplexity=107.48609

Batch 21060, train_perplexity=105.189316

Batch 21070, train_perplexity=111.881325

Batch 21080, train_perplexity=120.081055

Batch 21090, train_perplexity=107.130424

Batch 21100, train_perplexity=121.36087

Batch 21110, train_perplexity=111.66105

Batch 21120, train_perplexity=99.3079

Batch 21130, train_perplexity=111.21973

Batch 21140, train_perplexity=116.06744

Batch 21150, train_perplexity=114.38175

Batch 21160, train_perplexity=118.81446

Batch 21170, train_perplexity=111.89423

Batch 21180, train_perplexity=115.87842

Batch 21190, train_perplexity=103.78823

Batch 21200, train_perplexity=117.56817

Batch 21210, train_perplexity=112.6436

Batch 21220, train_perplexity=119.64029

Batch 21230, train_perplexity=108.67006

Batch 21240, train_perplexity=116.897415

Batch 21250, train_perplexity=114.10964

Batch 21260, train_perplexity=118.831345

Batch 21270, train_perplexity=114.007225

Batch 21280, train_perplexity=117.58235

Batch 21290, train_perplexity=104.4998

Batch 21300, train_perplexity=101.39312

Batch 21310, train_perplexity=107.26854

Batch 21320, train_perplexity=118.19694

Batch 21330, train_perplexity=105.31905

Batch 21340, train_perplexity=107.97236

Batch 21350, train_perplexity=118.16989

Batch 21360, train_perplexity=108.426895

Batch 21370, train_perplexity=112.66702

Batch 21380, train_perplexity=109.027626

Batch 21390, train_perplexity=110.06691

Batch 21400, train_perplexity=116.34494

Batch 21410, train_perplexity=104.12039

Batch 21420, train_perplexity=113.28509

Batch 21430, train_perplexity=114.15922

Batch 21440, train_perplexity=109.7557

Batch 21450, train_perplexity=126.814545

Batch 21460, train_perplexity=102.339714

Batch 21470, train_perplexity=112.44622

Batch 21480, train_perplexity=104.850945

Batch 21490, train_perplexity=109.265205

Batch 21500, train_perplexity=104.761734

Batch 21510, train_perplexity=129.26715

Batch 21520, train_perplexity=120.51564

Batch 21530, train_perplexity=108.64602

Batch 21540, train_perplexity=107.770004

Batch 21550, train_perplexity=112.34611

Batch 21560, train_perplexity=121.83952

Batch 21570, train_perplexity=122.84853

Batch 21580, train_perplexity=117.38275

Batch 21590, train_perplexity=113.00745

Batch 21600, train_perplexity=113.809845

Batch 21610, train_perplexity=115.85588

Batch 21620, train_perplexity=122.10468

Batch 21630, train_perplexity=108.60759

Batch 21640, train_perplexity=104.06942

Batch 21650, train_perplexity=113.14073

Batch 21660, train_perplexity=100.90395

Batch 21670, train_perplexity=105.710846

Batch 21680, train_perplexity=119.1526

Batch 21690, train_perplexity=107.93447

Batch 21700, train_perplexity=106.93419

Batch 21710, train_perplexity=114.28171

Batch 21720, train_perplexity=118.450836

Batch 21730, train_perplexity=105.33286

Batch 21740, train_perplexity=119.2978

Batch 21750, train_perplexity=107.47236

Batch 21760, train_perplexity=116.79723

Batch 21770, train_perplexity=113.48648

Batch 21780, train_perplexity=116.92434

Batch 21790, train_perplexity=121.89752

Batch 21800, train_perplexity=102.87517

Batch 21810, train_perplexity=115.37922

Batch 21820, train_perplexity=117.94561

Batch 21830, train_perplexity=119.15874

Batch 21840, train_perplexity=116.00873

Batch 21850, train_perplexity=113.621254

Batch 21860, train_perplexity=120.62292

Batch 21870, train_perplexity=114.569145

Batch 21880, train_perplexity=113.060646

Batch 21890, train_perplexity=97.1697

Batch 21900, train_perplexity=110.319595

Batch 21910, train_perplexity=108.66716

Batch 21920, train_perplexity=119.08342

Batch 21930, train_perplexity=109.42027

Batch 21940, train_perplexity=107.868

Batch 21950, train_perplexity=112.74774

Batch 21960, train_perplexity=108.51161

Batch 21970, train_perplexity=120.813736

Batch 21980, train_perplexity=109.08467

Batch 21990, train_perplexity=100.33722

Batch 22000, train_perplexity=114.81831

Batch 22010, train_perplexity=97.20886

Batch 22020, train_perplexity=110.30455

Batch 22030, train_perplexity=103.77561

Batch 22040, train_perplexity=124.321236

Batch 22050, train_perplexity=110.47804

Batch 22060, train_perplexity=108.004486

Batch 22070, train_perplexity=108.55664

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100
Loaded 306068 sentences.
Finished loading
Batch 22080, train_perplexity=109.97033

Batch 22090, train_perplexity=107.5229

Batch 22100, train_perplexity=112.13855

Batch 22110, train_perplexity=104.251396

Batch 22120, train_perplexity=104.63952

Batch 22130, train_perplexity=115.73932

Batch 22140, train_perplexity=118.40736

Batch 22150, train_perplexity=108.884026

Batch 22160, train_perplexity=125.871475

Batch 22170, train_perplexity=105.69915

Batch 22180, train_perplexity=111.22005

Batch 22190, train_perplexity=113.29086

Batch 22200, train_perplexity=116.02361

Batch 22210, train_perplexity=120.62913

Batch 22220, train_perplexity=104.042175

Batch 22230, train_perplexity=114.23229

Batch 22240, train_perplexity=116.026375

Batch 22250, train_perplexity=126.64063

Batch 22260, train_perplexity=107.55008

Batch 22270, train_perplexity=98.18238

Batch 22280, train_perplexity=123.929886

Batch 22290, train_perplexity=126.38618

Batch 22300, train_perplexity=109.66504

Batch 22310, train_perplexity=115.211975

Batch 22320, train_perplexity=115.34632

Batch 22330, train_perplexity=129.41351

Batch 22340, train_perplexity=107.9722

Batch 22350, train_perplexity=107.494606

Batch 22360, train_perplexity=118.84302

Batch 22370, train_perplexity=110.19836

Batch 22380, train_perplexity=116.285255

Batch 22390, train_perplexity=108.43362

Batch 22400, train_perplexity=113.966736

Batch 22410, train_perplexity=115.02271

Batch 22420, train_perplexity=112.97523

Batch 22430, train_perplexity=101.096985

Batch 22440, train_perplexity=118.40419

Batch 22450, train_perplexity=111.47352

Batch 22460, train_perplexity=99.47316

Batch 22470, train_perplexity=114.98679

Batch 22480, train_perplexity=121.67162

Batch 22490, train_perplexity=110.39784

Batch 22500, train_perplexity=123.95571

Batch 22510, train_perplexity=112.0401

Batch 22520, train_perplexity=109.91213

Batch 22530, train_perplexity=103.948204

Batch 22540, train_perplexity=112.020546

Batch 22550, train_perplexity=119.381676

Batch 22560, train_perplexity=112.41652

Batch 22570, train_perplexity=114.032234

Batch 22580, train_perplexity=107.16767

Batch 22590, train_perplexity=109.38208

Batch 22600, train_perplexity=119.287674

Batch 22610, train_perplexity=104.65878

Batch 22620, train_perplexity=100.17282

Batch 22630, train_perplexity=108.00433

Batch 22640, train_perplexity=107.01519

Batch 22650, train_perplexity=113.90822

Batch 22660, train_perplexity=122.44676

Batch 22670, train_perplexity=112.31343

Batch 22680, train_perplexity=113.33614

Batch 22690, train_perplexity=108.44447

Batch 22700, train_perplexity=118.96032

Batch 22710, train_perplexity=113.58545

Batch 22720, train_perplexity=103.80199

Batch 22730, train_perplexity=101.40356

Batch 22740, train_perplexity=113.11527

Batch 22750, train_perplexity=108.838806

Batch 22760, train_perplexity=106.28481

Batch 22770, train_perplexity=99.02527

Batch 22780, train_perplexity=98.50581

Batch 22790, train_perplexity=106.63905

Batch 22800, train_perplexity=110.93361

Batch 22810, train_perplexity=112.482414

Batch 22820, train_perplexity=112.33507

Batch 22830, train_perplexity=112.1348

Batch 22840, train_perplexity=112.805336

Batch 22850, train_perplexity=109.32008

Batch 22860, train_perplexity=115.8709

Batch 22870, train_perplexity=104.31256

Batch 22880, train_perplexity=106.33551

Batch 22890, train_perplexity=112.87459

Batch 22900, train_perplexity=113.78038

Batch 22910, train_perplexity=112.89617

Batch 22920, train_perplexity=107.17053

Batch 22930, train_perplexity=101.5678
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22940, train_perplexity=106.94377

Batch 22950, train_perplexity=109.465775

Batch 22960, train_perplexity=119.57869

Batch 22970, train_perplexity=106.739784

Batch 22980, train_perplexity=103.04799

Batch 22990, train_perplexity=114.4381

Batch 23000, train_perplexity=103.56196

Batch 23010, train_perplexity=121.34589

Batch 23020, train_perplexity=104.8228

Batch 23030, train_perplexity=108.72604

Batch 23040, train_perplexity=104.785965

Batch 23050, train_perplexity=101.5896

Batch 23060, train_perplexity=108.510994

Batch 23070, train_perplexity=113.6374

Batch 23080, train_perplexity=117.16131

Batch 23090, train_perplexity=110.41022

Batch 23100, train_perplexity=123.21673

Batch 23110, train_perplexity=114.851166

Batch 23120, train_perplexity=97.59542

Batch 23130, train_perplexity=106.61404

Batch 23140, train_perplexity=100.1507

Batch 23150, train_perplexity=111.74073

Batch 23160, train_perplexity=124.990234

Batch 23170, train_perplexity=104.404175

Batch 23180, train_perplexity=108.25791

Batch 23190, train_perplexity=110.10686

Batch 23200, train_perplexity=108.62054

Batch 23210, train_perplexity=111.5119

Batch 23220, train_perplexity=119.962875

Batch 23230, train_perplexity=118.03609

Batch 23240, train_perplexity=110.36011

Batch 23250, train_perplexity=110.350426

Batch 23260, train_perplexity=109.75617

Batch 23270, train_perplexity=108.746056

Batch 23280, train_perplexity=110.63377

Batch 23290, train_perplexity=115.73231

Batch 23300, train_perplexity=105.39893

Batch 23310, train_perplexity=106.53934

Batch 23320, train_perplexity=107.68473

Batch 23330, train_perplexity=110.26532

Batch 23340, train_perplexity=109.30883

Batch 23350, train_perplexity=102.39653

Batch 23360, train_perplexity=105.01796

Batch 23370, train_perplexity=101.29347

Batch 23380, train_perplexity=111.04591

Batch 23390, train_perplexity=98.36598

Batch 23400, train_perplexity=101.25127

Batch 23410, train_perplexity=103.73519

Batch 23420, train_perplexity=113.68721

Batch 23430, train_perplexity=110.94282

Batch 23440, train_perplexity=116.63934

Batch 23450, train_perplexity=110.05437

Batch 23460, train_perplexity=109.71122

Batch 23470, train_perplexity=117.01386

Batch 23480, train_perplexity=102.71979

Batch 23490, train_perplexity=96.82614

Batch 23500, train_perplexity=104.455666

Batch 23510, train_perplexity=110.15732

Batch 23520, train_perplexity=108.58164

Batch 23530, train_perplexity=115.23428

Batch 23540, train_perplexity=121.75264

Batch 23550, train_perplexity=100.84248

Batch 23560, train_perplexity=109.65322

Batch 23570, train_perplexity=110.33432

Batch 23580, train_perplexity=119.11971

Batch 23590, train_perplexity=100.38565

Batch 23600, train_perplexity=113.059784

Batch 23610, train_perplexity=121.16468

Batch 23620, train_perplexity=104.782616

Batch 23630, train_perplexity=96.18631

Batch 23640, train_perplexity=109.254684

Batch 23650, train_perplexity=108.10568

Batch 23660, train_perplexity=111.33742

Batch 23670, train_perplexity=103.13229

Batch 23680, train_perplexity=113.77772

Batch 23690, train_perplexity=105.24169

Batch 23700, train_perplexity=109.24223

Batch 23710, train_perplexity=110.719795

Batch 23720, train_perplexity=105.00194

Batch 23730, train_perplexity=108.16333

Batch 23740, train_perplexity=99.40488

Batch 23750, train_perplexity=108.08228

Batch 23760, train_perplexity=116.74495

Batch 23770, train_perplexity=100.446465

Batch 23780, train_perplexity=100.353294

Batch 23790, train_perplexity=112.27316

Batch 23800, train_perplexity=103.86938

Batch 23810, train_perplexity=107.95779

Batch 23820, train_perplexity=107.84331

Batch 23830, train_perplexity=110.3086

Batch 23840, train_perplexity=100.89842

Batch 23850, train_perplexity=120.71797

Batch 23860, train_perplexity=107.76281

Batch 23870, train_perplexity=121.82035

Batch 23880, train_perplexity=103.67778

Batch 23890, train_perplexity=109.38834

Batch 23900, train_perplexity=115.2078

Batch 23910, train_perplexity=108.38885

Batch 23920, train_perplexity=109.25156

Batch 23930, train_perplexity=121.00192

Batch 23940, train_perplexity=111.4614

Batch 23950, train_perplexity=112.84278

Batch 23960, train_perplexity=111.309235

Batch 23970, train_perplexity=113.084045

Batch 23980, train_perplexity=103.802635

Batch 23990, train_perplexity=101.62225

Batch 24000, train_perplexity=106.323135

Batch 24010, train_perplexity=113.891655

Batch 24020, train_perplexity=107.12353

Batch 24030, train_perplexity=122.0161

Batch 24040, train_perplexity=108.48564

Batch 24050, train_perplexity=109.82538

Batch 24060, train_perplexity=108.95346

Batch 24070, train_perplexity=108.04477

Batch 24080, train_perplexity=115.9855

Batch 24090, train_perplexity=109.58172

Batch 24100, train_perplexity=111.678085

Batch 24110, train_perplexity=111.59999

Batch 24120, train_perplexity=111.8266

Batch 24130, train_perplexity=117.23385

Batch 24140, train_perplexity=111.24296

Batch 24150, train_perplexity=105.526215

Batch 24160, train_perplexity=106.84428

Batch 24170, train_perplexity=108.263695

Batch 24180, train_perplexity=110.45624

Batch 24190, train_perplexity=111.869804

Batch 24200, train_perplexity=113.13097

Batch 24210, train_perplexity=103.62974

Batch 24220, train_perplexity=113.63659

Batch 24230, train_perplexity=116.217964

Batch 24240, train_perplexity=106.43413

Batch 24250, train_perplexity=121.451584

Batch 24260, train_perplexity=107.18228

Batch 24270, train_perplexity=104.11166

Batch 24280, train_perplexity=103.32894

Batch 24290, train_perplexity=100.92883

Batch 24300, train_perplexity=104.626945

Batch 24310, train_perplexity=114.95603

Batch 24320, train_perplexity=109.87745

Batch 24330, train_perplexity=98.47434

Batch 24340, train_perplexity=122.6124

Batch 24350, train_perplexity=101.52103

Batch 24360, train_perplexity=112.16545

Batch 24370, train_perplexity=105.37788

Batch 24380, train_perplexity=107.49993

Batch 24390, train_perplexity=108.5379

Batch 24400, train_perplexity=101.93771

Batch 24410, train_perplexity=104.28511

Batch 24420, train_perplexity=111.79898

Batch 24430, train_perplexity=107.2555

Batch 24440, train_perplexity=99.84868

Batch 24450, train_perplexity=104.741554

Batch 24460, train_perplexity=96.56899

Batch 24470, train_perplexity=116.76833

Batch 24480, train_perplexity=101.147224

Batch 24490, train_perplexity=99.93165

Batch 24500, train_perplexity=98.57719

Batch 24510, train_perplexity=105.99982

Batch 24520, train_perplexity=102.92502

Batch 24530, train_perplexity=96.03297

Batch 24540, train_perplexity=109.18938

Batch 24550, train_perplexity=103.71961

Batch 24560, train_perplexity=108.853546

Batch 24570, train_perplexity=106.94612

Batch 24580, train_perplexity=109.62159

Batch 24590, train_perplexity=107.25846

Batch 24600, train_perplexity=103.98474

Batch 24610, train_perplexity=103.952866

Batch 24620, train_perplexity=114.83507

Batch 24630, train_perplexity=103.27235

Batch 24640, train_perplexity=110.92219

Batch 24650, train_perplexity=103.77452

Batch 24660, train_perplexity=116.92713

Batch 24670, train_perplexity=111.7807

Batch 24680, train_perplexity=116.64212

Batch 24690, train_perplexity=115.34082

Batch 24700, train_perplexity=107.60086

Batch 24710, train_perplexity=108.64576

Batch 24720, train_perplexity=107.77833

Batch 24730, train_perplexity=111.53084

Batch 24740, train_perplexity=118.952774

Batch 24750, train_perplexity=115.26242

Batch 24760, train_perplexity=104.042274

Batch 24770, train_perplexity=108.48078

Batch 24780, train_perplexity=102.40727

Batch 24790, train_perplexity=108.67141

Batch 24800, train_perplexity=101.87552

Batch 24810, train_perplexity=100.42798

Batch 24820, train_perplexity=103.1113

Batch 24830, train_perplexity=103.86363

Batch 24840, train_perplexity=108.440445

Batch 24850, train_perplexity=109.421524

Batch 24860, train_perplexity=113.01591

Batch 24870, train_perplexity=112.2251

Batch 24880, train_perplexity=111.68932

Batch 24890, train_perplexity=97.95152

Batch 24900, train_perplexity=102.44727

Batch 24910, train_perplexity=102.877426

Batch 24920, train_perplexity=109.13935

Batch 24930, train_perplexity=113.72315
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24940, train_perplexity=97.34797

Batch 24950, train_perplexity=106.92083

Batch 24960, train_perplexity=112.86264

Batch 24970, train_perplexity=115.16414

Batch 24980, train_perplexity=114.96437

Batch 24990, train_perplexity=106.55142

Batch 25000, train_perplexity=107.3564

Batch 25010, train_perplexity=102.01454

Batch 25020, train_perplexity=113.42795

Batch 25030, train_perplexity=99.90411

Batch 25040, train_perplexity=109.805374

Batch 25050, train_perplexity=102.4483

Batch 25060, train_perplexity=107.31054

Batch 25070, train_perplexity=112.13064

Batch 25080, train_perplexity=114.4417

Batch 25090, train_perplexity=103.58695

Batch 25100, train_perplexity=105.99138

Batch 25110, train_perplexity=108.46909

Batch 25120, train_perplexity=103.860954

Batch 25130, train_perplexity=104.40457

Batch 25140, train_perplexity=99.94061

Batch 25150, train_perplexity=105.708626

Batch 25160, train_perplexity=111.47703

Batch 25170, train_perplexity=104.273125

Batch 25180, train_perplexity=98.468475

Batch 25190, train_perplexity=105.36085

Batch 25200, train_perplexity=94.20346

Batch 25210, train_perplexity=99.2873

Batch 25220, train_perplexity=105.04676

Batch 25230, train_perplexity=113.44087

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00090-of-00100
Loaded 306997 sentences.
Finished loading
Batch 25240, train_perplexity=112.71334

Batch 25250, train_perplexity=115.05277

Batch 25260, train_perplexity=111.73189

Batch 25270, train_perplexity=103.85908

Batch 25280, train_perplexity=105.08428

Batch 25290, train_perplexity=110.35074

Batch 25300, train_perplexity=106.806076

Batch 25310, train_perplexity=105.589836

Batch 25320, train_perplexity=94.491295

Batch 25330, train_perplexity=101.5083

Batch 25340, train_perplexity=110.01365

Batch 25350, train_perplexity=113.39874

Batch 25360, train_perplexity=101.48148

Batch 25370, train_perplexity=98.59143

Batch 25380, train_perplexity=100.107544

Batch 25390, train_perplexity=100.45877

Batch 25400, train_perplexity=102.682465

Batch 25410, train_perplexity=104.362564

Batch 25420, train_perplexity=103.414215

Batch 25430, train_perplexity=92.25396

Batch 25440, train_perplexity=109.56907

Batch 25450, train_perplexity=112.92595

Batch 25460, train_perplexity=101.325935

Batch 25470, train_perplexity=108.0997

Batch 25480, train_perplexity=98.90786

Batch 25490, train_perplexity=103.839615

Batch 25500, train_perplexity=102.90971

Batch 25510, train_perplexity=105.37858

Batch 25520, train_perplexity=95.71314

Batch 25530, train_perplexity=107.49783

Batch 25540, train_perplexity=94.39474

Batch 25550, train_perplexity=114.12096

Batch 25560, train_perplexity=112.243614

Batch 25570, train_perplexity=116.51816

Batch 25580, train_perplexity=107.49163

Batch 25590, train_perplexity=102.79314

Batch 25600, train_perplexity=105.70006

Batch 25610, train_perplexity=99.21868

Batch 25620, train_perplexity=106.192314

Batch 25630, train_perplexity=111.9586

Batch 25640, train_perplexity=108.88033

Batch 25650, train_perplexity=103.791794

Batch 25660, train_perplexity=119.43133

Batch 25670, train_perplexity=102.261665

Batch 25680, train_perplexity=102.84853

Batch 25690, train_perplexity=106.45849

Batch 25700, train_perplexity=104.26174

Batch 25710, train_perplexity=106.435745

Batch 25720, train_perplexity=100.448906

Batch 25730, train_perplexity=98.663574

Batch 25740, train_perplexity=105.733734

Batch 25750, train_perplexity=106.88158

Batch 25760, train_perplexity=102.5307

Batch 25770, train_perplexity=106.31219

Batch 25780, train_perplexity=97.95301

Batch 25790, train_perplexity=99.51581

Batch 25800, train_perplexity=117.918625

Batch 25810, train_perplexity=102.91305

Batch 25820, train_perplexity=102.68462

Batch 25830, train_perplexity=106.82268

Batch 25840, train_perplexity=113.87493

Batch 25850, train_perplexity=102.8274

Batch 25860, train_perplexity=100.70025

Batch 25870, train_perplexity=106.19961

Batch 25880, train_perplexity=110.818146

Batch 25890, train_perplexity=108.06213

Batch 25900, train_perplexity=111.689

Batch 25910, train_perplexity=104.56007

Batch 25920, train_perplexity=105.66004

Batch 25930, train_perplexity=101.509026

Batch 25940, train_perplexity=103.241425

Batch 25950, train_perplexity=109.5487

Batch 25960, train_perplexity=101.865555

Batch 25970, train_perplexity=101.14091

Batch 25980, train_perplexity=112.13651

Batch 25990, train_perplexity=101.13734

Batch 26000, train_perplexity=110.08596

Batch 26010, train_perplexity=110.80356

Batch 26020, train_perplexity=103.64347

Batch 26030, train_perplexity=118.20427

Batch 26040, train_perplexity=102.720566

Batch 26050, train_perplexity=101.17048

Batch 26060, train_perplexity=107.123116

Batch 26070, train_perplexity=106.03126

Batch 26080, train_perplexity=103.81501

Batch 26090, train_perplexity=97.614784

Batch 26100, train_perplexity=112.6219

Batch 26110, train_perplexity=100.2324

Batch 26120, train_perplexity=99.14698

Batch 26130, train_perplexity=105.53386

Batch 26140, train_perplexity=110.16862

Batch 26150, train_perplexity=108.89685

Batch 26160, train_perplexity=107.23801

Batch 26170, train_perplexity=110.99139

Batch 26180, train_perplexity=103.77828

Batch 26190, train_perplexity=100.93995

Batch 26200, train_perplexity=95.42185

Batch 26210, train_perplexity=102.65667

Batch 26220, train_perplexity=109.918846

Batch 26230, train_perplexity=102.803474

Batch 26240, train_perplexity=105.757835

Batch 26250, train_perplexity=103.824814

Batch 26260, train_perplexity=109.95155

Batch 26270, train_perplexity=93.46457

Batch 26280, train_perplexity=104.509766

Batch 26290, train_perplexity=99.89806

Batch 26300, train_perplexity=95.5098

Batch 26310, train_perplexity=95.33349

Batch 26320, train_perplexity=100.68825

Batch 26330, train_perplexity=107.653

Batch 26340, train_perplexity=101.36865

Batch 26350, train_perplexity=105.748

Batch 26360, train_perplexity=110.87999

Batch 26370, train_perplexity=119.76129

Batch 26380, train_perplexity=105.58772

Batch 26390, train_perplexity=114.259315

Batch 26400, train_perplexity=100.15596

Batch 26410, train_perplexity=105.45332

Batch 26420, train_perplexity=106.65273

Batch 26430, train_perplexity=98.74038

Batch 26440, train_perplexity=102.88891

Batch 26450, train_perplexity=106.3596

Batch 26460, train_perplexity=101.592316

Batch 26470, train_perplexity=92.22084

Batch 26480, train_perplexity=117.61235

Batch 26490, train_perplexity=105.02863

Batch 26500, train_perplexity=108.98958

Batch 26510, train_perplexity=105.22995

Batch 26520, train_perplexity=105.03163

Batch 26530, train_perplexity=106.13157

Batch 26540, train_perplexity=96.03823

Batch 26550, train_perplexity=110.22011

Batch 26560, train_perplexity=106.55478

Batch 26570, train_perplexity=111.343895

Batch 26580, train_perplexity=96.74007

Batch 26590, train_perplexity=104.94052

Batch 26600, train_perplexity=102.31425

Batch 26610, train_perplexity=106.4188

Batch 26620, train_perplexity=102.04442

Batch 26630, train_perplexity=99.17431

Batch 26640, train_perplexity=114.31485

Batch 26650, train_perplexity=103.63824

Batch 26660, train_perplexity=96.03141

Batch 26670, train_perplexity=94.691284

Batch 26680, train_perplexity=100.84075

Batch 26690, train_perplexity=97.41057

Batch 26700, train_perplexity=101.23374

Batch 26710, train_perplexity=93.31049

Batch 26720, train_perplexity=106.0076

Batch 26730, train_perplexity=111.20085

Batch 26740, train_perplexity=101.58001

Batch 26750, train_perplexity=99.08072

Batch 26760, train_perplexity=105.92418

Batch 26770, train_perplexity=104.77383

Batch 26780, train_perplexity=109.04832

Batch 26790, train_perplexity=107.21986

Batch 26800, train_perplexity=106.3632

Batch 26810, train_perplexity=96.61942

Batch 26820, train_perplexity=109.786316

Batch 26830, train_perplexity=101.922745

Batch 26840, train_perplexity=107.27412
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 26850, train_perplexity=96.68994

Batch 26860, train_perplexity=97.4837

Batch 26870, train_perplexity=104.90995

Batch 26880, train_perplexity=110.5046

Batch 26890, train_perplexity=113.91023

Batch 26900, train_perplexity=113.153946

Batch 26910, train_perplexity=93.55214

Batch 26920, train_perplexity=101.31811

Batch 26930, train_perplexity=89.443275

Batch 26940, train_perplexity=98.477585

Batch 26950, train_perplexity=107.164856

Batch 26960, train_perplexity=103.27289

Batch 26970, train_perplexity=110.76574

Batch 26980, train_perplexity=94.909805

Batch 26990, train_perplexity=103.02907

Batch 27000, train_perplexity=108.26684

Batch 27010, train_perplexity=109.23374

Batch 27020, train_perplexity=98.313416

Batch 27030, train_perplexity=98.969475

Batch 27040, train_perplexity=92.85802

Batch 27050, train_perplexity=108.86632

Batch 27060, train_perplexity=104.11791

Batch 27070, train_perplexity=100.05038

Batch 27080, train_perplexity=103.34412

Batch 27090, train_perplexity=103.3731

Batch 27100, train_perplexity=111.3087

Batch 27110, train_perplexity=96.45021

Batch 27120, train_perplexity=104.248566

Batch 27130, train_perplexity=94.9625

Batch 27140, train_perplexity=105.09085

Batch 27150, train_perplexity=106.17925

Batch 27160, train_perplexity=118.29025

Batch 27170, train_perplexity=91.28284

Batch 27180, train_perplexity=111.724

Batch 27190, train_perplexity=101.53052

Batch 27200, train_perplexity=94.09733

Batch 27210, train_perplexity=102.18182

Batch 27220, train_perplexity=107.869125

Batch 27230, train_perplexity=112.61036

Batch 27240, train_perplexity=111.81498

Batch 27250, train_perplexity=93.21034

Batch 27260, train_perplexity=103.36965

Batch 27270, train_perplexity=99.02909

Batch 27280, train_perplexity=85.25699

Batch 27290, train_perplexity=103.716545

Batch 27300, train_perplexity=103.85898

Batch 27310, train_perplexity=102.879974

Batch 27320, train_perplexity=103.006134

Batch 27330, train_perplexity=118.53909

Batch 27340, train_perplexity=109.51778

Batch 27350, train_perplexity=101.8388

Batch 27360, train_perplexity=102.99901

Batch 27370, train_perplexity=101.91317

Batch 27380, train_perplexity=109.57868

Batch 27390, train_perplexity=94.44607

Batch 27400, train_perplexity=97.058655

Batch 27410, train_perplexity=104.71464

Batch 27420, train_perplexity=103.760666

Batch 27430, train_perplexity=100.99879

Batch 27440, train_perplexity=99.22899

Batch 27450, train_perplexity=97.68285

Batch 27460, train_perplexity=109.762344

Batch 27470, train_perplexity=104.820694

Batch 27480, train_perplexity=98.06995

Batch 27490, train_perplexity=115.9176

Batch 27500, train_perplexity=92.04164

Batch 27510, train_perplexity=105.780426

Batch 27520, train_perplexity=96.22984

Batch 27530, train_perplexity=116.20577

Batch 27540, train_perplexity=103.16992

Batch 27550, train_perplexity=100.46232

Batch 27560, train_perplexity=100.01188

Batch 27570, train_perplexity=105.54604

Batch 27580, train_perplexity=109.4509

Batch 27590, train_perplexity=98.819984

Batch 27600, train_perplexity=108.19552

Batch 27610, train_perplexity=102.82603

Batch 27620, train_perplexity=97.80898

Batch 27630, train_perplexity=94.62056

Batch 27640, train_perplexity=99.66488

Batch 27650, train_perplexity=105.216705

Batch 27660, train_perplexity=108.351746

Batch 27670, train_perplexity=99.98589

Batch 27680, train_perplexity=103.119606

Batch 27690, train_perplexity=100.04665

Batch 27700, train_perplexity=94.90148

Batch 27710, train_perplexity=103.086914

Batch 27720, train_perplexity=97.31854

Batch 27730, train_perplexity=113.28525

Batch 27740, train_perplexity=99.05474

Batch 27750, train_perplexity=102.929436

Batch 27760, train_perplexity=91.8053

Batch 27770, train_perplexity=107.76867

Batch 27780, train_perplexity=101.01718

Batch 27790, train_perplexity=105.14568

Batch 27800, train_perplexity=119.94657

Batch 27810, train_perplexity=102.37783

Batch 27820, train_perplexity=109.83366

Batch 27830, train_perplexity=106.794464

Batch 27840, train_perplexity=101.19137

Batch 27850, train_perplexity=100.13838

Batch 27860, train_perplexity=104.92116

Batch 27870, train_perplexity=103.92685

Batch 27880, train_perplexity=110.049324

Batch 27890, train_perplexity=111.16628

Batch 27900, train_perplexity=102.52918

Batch 27910, train_perplexity=108.760574

Batch 27920, train_perplexity=98.91696

Batch 27930, train_perplexity=85.93354

Batch 27940, train_perplexity=112.437

Batch 27950, train_perplexity=93.86582

Batch 27960, train_perplexity=90.69075

Batch 27970, train_perplexity=98.30526

Batch 27980, train_perplexity=99.55758

Batch 27990, train_perplexity=103.70962

Batch 28000, train_perplexity=98.53597

Batch 28010, train_perplexity=101.63568

Batch 28020, train_perplexity=110.36816

Batch 28030, train_perplexity=97.635544

Batch 28040, train_perplexity=104.927414

Batch 28050, train_perplexity=101.122826

Batch 28060, train_perplexity=102.794945

Batch 28070, train_perplexity=101.39027

Batch 28080, train_perplexity=99.58644

Batch 28090, train_perplexity=99.148636

Batch 28100, train_perplexity=102.464035

Batch 28110, train_perplexity=92.61746

Batch 28120, train_perplexity=105.777504

Batch 28130, train_perplexity=97.88774

Batch 28140, train_perplexity=99.7087

Batch 28150, train_perplexity=101.88139

Batch 28160, train_perplexity=106.38227

Batch 28170, train_perplexity=111.86404

Batch 28180, train_perplexity=97.25815

Batch 28190, train_perplexity=111.5436

Batch 28200, train_perplexity=117.69123

Batch 28210, train_perplexity=104.21815

Batch 28220, train_perplexity=100.49614

Batch 28230, train_perplexity=105.98774

Batch 28240, train_perplexity=103.94434

Batch 28250, train_perplexity=95.99835

Batch 28260, train_perplexity=94.43282

Batch 28270, train_perplexity=97.90389

Batch 28280, train_perplexity=92.372765

Batch 28290, train_perplexity=101.69526

Batch 28300, train_perplexity=102.28283

Batch 28310, train_perplexity=104.69377

Batch 28320, train_perplexity=103.466995

Batch 28330, train_perplexity=100.66372

Batch 28340, train_perplexity=102.980446

Batch 28350, train_perplexity=106.54777

Batch 28360, train_perplexity=104.108185

Batch 28370, train_perplexity=101.21632

Batch 28380, train_perplexity=102.21769

Batch 28390, train_perplexity=103.39297

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00069-of-00100
Loaded 305307 sentences.
Finished loading
Batch 28400, train_perplexity=107.50752

Batch 28410, train_perplexity=109.8125

Batch 28420, train_perplexity=105.07356

Batch 28430, train_perplexity=103.724556

Batch 28440, train_perplexity=105.340096

Batch 28450, train_perplexity=95.5262

Batch 28460, train_perplexity=116.56995

Batch 28470, train_perplexity=116.11382

Batch 28480, train_perplexity=99.55658

Batch 28490, train_perplexity=96.54601

Batch 28500, train_perplexity=103.12526

Batch 28510, train_perplexity=103.94796

Batch 28520, train_perplexity=99.44281

Batch 28530, train_perplexity=105.98723

Batch 28540, train_perplexity=85.82993

Batch 28550, train_perplexity=96.23494

Batch 28560, train_perplexity=97.70992

Batch 28570, train_perplexity=109.38344

Batch 28580, train_perplexity=99.07576

Batch 28590, train_perplexity=100.73195

Batch 28600, train_perplexity=102.34898

Batch 28610, train_perplexity=108.68271

Batch 28620, train_perplexity=107.6758

Batch 28630, train_perplexity=105.18891

Batch 28640, train_perplexity=89.21799

Batch 28650, train_perplexity=95.24575

Batch 28660, train_perplexity=99.501816

Batch 28670, train_perplexity=112.48381

Batch 28680, train_perplexity=91.23636

Batch 28690, train_perplexity=93.43293

Batch 28700, train_perplexity=90.99879

Batch 28710, train_perplexity=109.098816

Batch 28720, train_perplexity=99.808784

Batch 28730, train_perplexity=99.61138

Batch 28740, train_perplexity=101.52635

Batch 28750, train_perplexity=96.62053
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28760, train_perplexity=98.837326

Batch 28770, train_perplexity=99.03504

Batch 28780, train_perplexity=95.69776

Batch 28790, train_perplexity=102.66641

Batch 28800, train_perplexity=104.46393

Batch 28810, train_perplexity=105.73403

Batch 28820, train_perplexity=101.29584

Batch 28830, train_perplexity=102.444

Batch 28840, train_perplexity=92.99201

Batch 28850, train_perplexity=101.30487

Batch 28860, train_perplexity=97.44569

Batch 28870, train_perplexity=108.18138

Batch 28880, train_perplexity=107.390396

Batch 28890, train_perplexity=101.92527

Batch 28900, train_perplexity=109.39982

Batch 28910, train_perplexity=96.172005

Batch 28920, train_perplexity=106.86313

Batch 28930, train_perplexity=104.67485

Batch 28940, train_perplexity=99.60872

Batch 28950, train_perplexity=108.51772

Batch 28960, train_perplexity=100.07943

Batch 28970, train_perplexity=98.1346

Batch 28980, train_perplexity=111.27421

Batch 28990, train_perplexity=102.931694

Batch 29000, train_perplexity=106.61043

Batch 29010, train_perplexity=106.51028

Batch 29020, train_perplexity=98.67863

Batch 29030, train_perplexity=99.12108

Batch 29040, train_perplexity=101.68333

Batch 29050, train_perplexity=94.86954

Batch 29060, train_perplexity=99.089554

Batch 29070, train_perplexity=98.17175

Batch 29080, train_perplexity=115.33609

Batch 29090, train_perplexity=106.309044

Batch 29100, train_perplexity=117.32316

Batch 29110, train_perplexity=104.055374

Batch 29120, train_perplexity=95.92121

Batch 29130, train_perplexity=108.26689

Batch 29140, train_perplexity=94.400635

Batch 29150, train_perplexity=96.01236

Batch 29160, train_perplexity=92.73607

Batch 29170, train_perplexity=110.99425

Batch 29180, train_perplexity=98.93442

Batch 29190, train_perplexity=96.23907

Batch 29200, train_perplexity=101.0697

Batch 29210, train_perplexity=102.52283

Batch 29220, train_perplexity=95.07037

Batch 29230, train_perplexity=94.31933

Batch 29240, train_perplexity=105.22383

Batch 29250, train_perplexity=105.254845

Batch 29260, train_perplexity=94.380066

Batch 29270, train_perplexity=95.98133

Batch 29280, train_perplexity=91.51635

Batch 29290, train_perplexity=106.95274

Batch 29300, train_perplexity=94.589836

Batch 29310, train_perplexity=102.95359

Batch 29320, train_perplexity=92.720505

Batch 29330, train_perplexity=101.42929

Batch 29340, train_perplexity=103.641495

Batch 29350, train_perplexity=99.42327

Batch 29360, train_perplexity=98.440025

Batch 29370, train_perplexity=96.592705

Batch 29380, train_perplexity=109.534904

Batch 29390, train_perplexity=104.84889

Batch 29400, train_perplexity=94.46985

Batch 29410, train_perplexity=101.09062

Batch 29420, train_perplexity=95.64142

Batch 29430, train_perplexity=105.02612

Batch 29440, train_perplexity=99.358246

Batch 29450, train_perplexity=102.161995

Batch 29460, train_perplexity=91.11828

Batch 29470, train_perplexity=94.565125

Batch 29480, train_perplexity=90.50839

Batch 29490, train_perplexity=110.34206

Batch 29500, train_perplexity=98.28829

Batch 29510, train_perplexity=94.8206

Batch 29520, train_perplexity=102.36095

Batch 29530, train_perplexity=95.816154

Batch 29540, train_perplexity=96.665276

Batch 29550, train_perplexity=103.73059

Batch 29560, train_perplexity=96.13074

Batch 29570, train_perplexity=106.6118

Batch 29580, train_perplexity=104.20911

Batch 29590, train_perplexity=101.64445

Batch 29600, train_perplexity=103.29633

Batch 29610, train_perplexity=96.88973

Batch 29620, train_perplexity=108.07733

Batch 29630, train_perplexity=101.86327

Batch 29640, train_perplexity=97.8449

Batch 29650, train_perplexity=105.92079

Batch 29660, train_perplexity=102.92865

Batch 29670, train_perplexity=96.87546

Batch 29680, train_perplexity=101.86473

Batch 29690, train_perplexity=96.68722

Batch 29700, train_perplexity=98.62284

Batch 29710, train_perplexity=92.65536

Batch 29720, train_perplexity=110.73959

Batch 29730, train_perplexity=99.56474

Batch 29740, train_perplexity=97.20566

Batch 29750, train_perplexity=100.026764

Batch 29760, train_perplexity=90.86654

Batch 29770, train_perplexity=103.42354

Batch 29780, train_perplexity=97.00137

Batch 29790, train_perplexity=108.25373

Batch 29800, train_perplexity=97.97058

Batch 29810, train_perplexity=96.69363

Batch 29820, train_perplexity=99.80284

Batch 29830, train_perplexity=100.3735

Batch 29840, train_perplexity=102.20985

Batch 29850, train_perplexity=107.574905

Batch 29860, train_perplexity=101.44728

Batch 29870, train_perplexity=100.38556

Batch 29880, train_perplexity=95.97254

Batch 29890, train_perplexity=92.61905

Batch 29900, train_perplexity=105.20175

Batch 29910, train_perplexity=100.72239

Batch 29920, train_perplexity=101.25295

Batch 29930, train_perplexity=103.79768

Batch 29940, train_perplexity=101.52345

Batch 29950, train_perplexity=100.78446

Batch 29960, train_perplexity=109.63011

Batch 29970, train_perplexity=106.02555

Batch 29980, train_perplexity=98.83012

Batch 29990, train_perplexity=97.88214

Batch 30000, train_perplexity=99.140076

Batch 30010, train_perplexity=92.71264

Batch 30020, train_perplexity=96.224754

Batch 30030, train_perplexity=109.837425

Batch 30040, train_perplexity=101.527466

Batch 30050, train_perplexity=105.52571

Batch 30060, train_perplexity=98.816124

Batch 30070, train_perplexity=105.03048

Batch 30080, train_perplexity=97.40165

Batch 30090, train_perplexity=98.536255

Batch 30100, train_perplexity=88.306816

Batch 30110, train_perplexity=101.93616

Batch 30120, train_perplexity=97.20992

Batch 30130, train_perplexity=100.077286

Batch 30140, train_perplexity=113.52664

Batch 30150, train_perplexity=103.29559

Batch 30160, train_perplexity=101.3499

Batch 30170, train_perplexity=97.395096

Batch 30180, train_perplexity=103.43335

Batch 30190, train_perplexity=108.40187

Batch 30200, train_perplexity=102.13029

Batch 30210, train_perplexity=89.28697

Batch 30220, train_perplexity=86.990746

Batch 30230, train_perplexity=99.87906

Batch 30240, train_perplexity=93.9703

Batch 30250, train_perplexity=95.2835

Batch 30260, train_perplexity=112.11834

Batch 30270, train_perplexity=97.738205

Batch 30280, train_perplexity=98.2509

Batch 30290, train_perplexity=94.57112

Batch 30300, train_perplexity=91.66239

Batch 30310, train_perplexity=100.66693

Batch 30320, train_perplexity=85.31511

Batch 30330, train_perplexity=102.98678

Batch 30340, train_perplexity=94.71639

Batch 30350, train_perplexity=99.26723

Batch 30360, train_perplexity=108.19407

Batch 30370, train_perplexity=99.41042

Batch 30380, train_perplexity=85.84704

Batch 30390, train_perplexity=99.97693

Batch 30400, train_perplexity=101.59144

Batch 30410, train_perplexity=95.8184

Batch 30420, train_perplexity=109.23322

Batch 30430, train_perplexity=92.253296

Batch 30440, train_perplexity=94.3295

Batch 30450, train_perplexity=104.052345

Batch 30460, train_perplexity=101.31144

Batch 30470, train_perplexity=96.92823

Batch 30480, train_perplexity=106.91287

Batch 30490, train_perplexity=98.27077

Batch 30500, train_perplexity=104.14905

Batch 30510, train_perplexity=101.385765

Batch 30520, train_perplexity=109.83779

Batch 30530, train_perplexity=94.69237

Batch 30540, train_perplexity=101.29337

Batch 30550, train_perplexity=96.38465

Batch 30560, train_perplexity=94.06063

Batch 30570, train_perplexity=103.96303

Batch 30580, train_perplexity=93.87638

Batch 30590, train_perplexity=98.37888

Batch 30600, train_perplexity=97.88036

Batch 30610, train_perplexity=97.66432

Batch 30620, train_perplexity=108.13321

Batch 30630, train_perplexity=91.75773

Batch 30640, train_perplexity=93.05527

Batch 30650, train_perplexity=88.660515

Batch 30660, train_perplexity=104.38834

Batch 30670, train_perplexity=99.21082

Batch 30680, train_perplexity=101.16237

Batch 30690, train_perplexity=97.617294

Batch 30700, train_perplexity=105.17617

Batch 30710, train_perplexity=91.94159

Batch 30720, train_perplexity=95.14579

Batch 30730, train_perplexity=100.2748

Batch 30740, train_perplexity=85.77097

Batch 30750, train_perplexity=98.56957

Batch 30760, train_perplexity=97.725204

Batch 30770, train_perplexity=97.5609
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 30780, train_perplexity=89.105446

Batch 30790, train_perplexity=100.38029

Batch 30800, train_perplexity=111.87759

Batch 30810, train_perplexity=106.24955

Batch 30820, train_perplexity=104.11116

Batch 30830, train_perplexity=101.74517

Batch 30840, train_perplexity=98.750275

Batch 30850, train_perplexity=91.05252

Batch 30860, train_perplexity=101.35155

Batch 30870, train_perplexity=91.49475

Batch 30880, train_perplexity=106.80877

Batch 30890, train_perplexity=107.89407

Batch 30900, train_perplexity=103.3116

Batch 30910, train_perplexity=95.95387

Batch 30920, train_perplexity=103.134605

Batch 30930, train_perplexity=102.26225

Batch 30940, train_perplexity=101.331345

Batch 30950, train_perplexity=94.78642

Batch 30960, train_perplexity=100.17678

Batch 30970, train_perplexity=95.98444

Batch 30980, train_perplexity=90.72003

Batch 30990, train_perplexity=93.07506

Batch 31000, train_perplexity=94.19389

Batch 31010, train_perplexity=100.10564

Batch 31020, train_perplexity=106.47676

Batch 31030, train_perplexity=91.78109

Batch 31040, train_perplexity=95.61424

Batch 31050, train_perplexity=98.70686

Batch 31060, train_perplexity=93.7755

Batch 31070, train_perplexity=98.18098

Batch 31080, train_perplexity=100.390434

Batch 31090, train_perplexity=95.9514

Batch 31100, train_perplexity=102.49252

Batch 31110, train_perplexity=101.732506

Batch 31120, train_perplexity=95.426636

Batch 31130, train_perplexity=92.96851

Batch 31140, train_perplexity=93.94991

Batch 31150, train_perplexity=105.99153

Batch 31160, train_perplexity=101.5757

Batch 31170, train_perplexity=102.13603

Batch 31180, train_perplexity=92.030716

Batch 31190, train_perplexity=98.104744

Batch 31200, train_perplexity=94.30989

Batch 31210, train_perplexity=100.926956

Batch 31220, train_perplexity=97.62591

Batch 31230, train_perplexity=95.11576

Batch 31240, train_perplexity=106.071465

Batch 31250, train_perplexity=109.25229

Batch 31260, train_perplexity=104.33032

Batch 31270, train_perplexity=89.8172

Batch 31280, train_perplexity=90.7936

Batch 31290, train_perplexity=92.12301

Batch 31300, train_perplexity=106.69673

Batch 31310, train_perplexity=94.240166

Batch 31320, train_perplexity=103.856895

Batch 31330, train_perplexity=93.50313

Batch 31340, train_perplexity=107.46262

Batch 31350, train_perplexity=96.038

Batch 31360, train_perplexity=100.84946

Batch 31370, train_perplexity=98.40239

Batch 31380, train_perplexity=99.20061

Batch 31390, train_perplexity=92.96577

Batch 31400, train_perplexity=90.358505

Batch 31410, train_perplexity=103.0532

Batch 31420, train_perplexity=97.81784

Batch 31430, train_perplexity=102.26098

Batch 31440, train_perplexity=92.958145

Batch 31450, train_perplexity=97.73215

Batch 31460, train_perplexity=99.63066

Batch 31470, train_perplexity=80.218346

Batch 31480, train_perplexity=93.9621

Batch 31490, train_perplexity=89.07032

Batch 31500, train_perplexity=98.32767

Batch 31510, train_perplexity=107.058624

Batch 31520, train_perplexity=99.43858

Batch 31530, train_perplexity=101.08484

Batch 31540, train_perplexity=103.04062

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00076-of-00100
Loaded 306032 sentences.
Finished loading
Batch 31550, train_perplexity=98.56624

Batch 31560, train_perplexity=102.828674

Batch 31570, train_perplexity=101.638054

Batch 31580, train_perplexity=97.00392

Batch 31590, train_perplexity=97.70205

Batch 31600, train_perplexity=99.965775

Batch 31610, train_perplexity=83.33731

Batch 31620, train_perplexity=102.12142

Batch 31630, train_perplexity=104.71219

Batch 31640, train_perplexity=98.84025

Batch 31650, train_perplexity=85.10378

Batch 31660, train_perplexity=102.48631

Batch 31670, train_perplexity=100.014595

Batch 31680, train_perplexity=98.38039

Batch 31690, train_perplexity=94.05767

Batch 31700, train_perplexity=102.94191

Batch 31710, train_perplexity=90.40307

Batch 31720, train_perplexity=91.6332

Batch 31730, train_perplexity=103.364624

Batch 31740, train_perplexity=102.276634

Batch 31750, train_perplexity=98.92154

Batch 31760, train_perplexity=93.53166

Batch 31770, train_perplexity=91.063675

Batch 31780, train_perplexity=94.03076

Batch 31790, train_perplexity=106.15809

Batch 31800, train_perplexity=101.65536

Batch 31810, train_perplexity=104.73486

Batch 31820, train_perplexity=98.9758

Batch 31830, train_perplexity=102.10862

Batch 31840, train_perplexity=99.97555

Batch 31850, train_perplexity=92.28427

Batch 31860, train_perplexity=91.85824

Batch 31870, train_perplexity=96.83385

Batch 31880, train_perplexity=106.42139

Batch 31890, train_perplexity=99.00393

Batch 31900, train_perplexity=103.80343

Batch 31910, train_perplexity=91.45366

Batch 31920, train_perplexity=97.90291

Batch 31930, train_perplexity=95.7157

Batch 31940, train_perplexity=96.366684

Batch 31950, train_perplexity=94.58528

Batch 31960, train_perplexity=102.65378

Batch 31970, train_perplexity=100.99773

Batch 31980, train_perplexity=94.14023

Batch 31990, train_perplexity=98.455376

Batch 32000, train_perplexity=94.13224

Batch 32010, train_perplexity=97.96198

Batch 32020, train_perplexity=100.11418

Batch 32030, train_perplexity=98.55937

Batch 32040, train_perplexity=98.95635

Batch 32050, train_perplexity=104.99398

Batch 32060, train_perplexity=91.33412

Batch 32070, train_perplexity=98.364624

Batch 32080, train_perplexity=99.599266

Batch 32090, train_perplexity=94.64005

Batch 32100, train_perplexity=93.19195

Batch 32110, train_perplexity=105.50951

Batch 32120, train_perplexity=98.24659

Batch 32130, train_perplexity=100.83027

Batch 32140, train_perplexity=94.51959

Batch 32150, train_perplexity=102.530205

Batch 32160, train_perplexity=91.86056

Batch 32170, train_perplexity=108.510376

Batch 32180, train_perplexity=91.029076

Batch 32190, train_perplexity=102.7409

Batch 32200, train_perplexity=92.23025

Batch 32210, train_perplexity=98.5841

Batch 32220, train_perplexity=98.75752

Batch 32230, train_perplexity=94.16097

Batch 32240, train_perplexity=99.00983

Batch 32250, train_perplexity=93.49684

Batch 32260, train_perplexity=96.26156

Batch 32270, train_perplexity=107.51921

Batch 32280, train_perplexity=97.74147

Batch 32290, train_perplexity=111.501114

Batch 32300, train_perplexity=97.74967

Batch 32310, train_perplexity=87.02381

Batch 32320, train_perplexity=95.03113

Batch 32330, train_perplexity=89.3236

Batch 32340, train_perplexity=96.58524

Batch 32350, train_perplexity=90.113335

Batch 32360, train_perplexity=101.21854

Batch 32370, train_perplexity=91.866646

Batch 32380, train_perplexity=88.43319

Batch 32390, train_perplexity=97.05689

Batch 32400, train_perplexity=108.64416

Batch 32410, train_perplexity=103.85987

Batch 32420, train_perplexity=109.933205

Batch 32430, train_perplexity=90.15094

Batch 32440, train_perplexity=87.196144

Batch 32450, train_perplexity=103.21056

Batch 32460, train_perplexity=93.47883

Batch 32470, train_perplexity=84.46961

Batch 32480, train_perplexity=88.978325

Batch 32490, train_perplexity=98.83507

Batch 32500, train_perplexity=96.79641

Batch 32510, train_perplexity=95.4508

Batch 32520, train_perplexity=89.22109

Batch 32530, train_perplexity=102.24733

Batch 32540, train_perplexity=103.809364

Batch 32550, train_perplexity=97.77181

Batch 32560, train_perplexity=98.58743

Batch 32570, train_perplexity=97.149315

Batch 32580, train_perplexity=93.22777

Batch 32590, train_perplexity=98.34469

Batch 32600, train_perplexity=102.10944

Batch 32610, train_perplexity=88.4865

Batch 32620, train_perplexity=96.17915

Batch 32630, train_perplexity=97.261024

Batch 32640, train_perplexity=96.62412

Batch 32650, train_perplexity=87.21245

Batch 32660, train_perplexity=103.69276

Batch 32670, train_perplexity=100.20669

Batch 32680, train_perplexity=89.754524

Batch 32690, train_perplexity=97.62954

Batch 32700, train_perplexity=91.605675
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 32710, train_perplexity=90.402374

Batch 32720, train_perplexity=88.122406

Batch 32730, train_perplexity=96.065575

Batch 32740, train_perplexity=93.534386

Batch 32750, train_perplexity=92.74491

Batch 32760, train_perplexity=104.61846

Batch 32770, train_perplexity=90.52583

Batch 32780, train_perplexity=87.709946

Batch 32790, train_perplexity=92.49737

Batch 32800, train_perplexity=96.80855

Batch 32810, train_perplexity=88.08212

Batch 32820, train_perplexity=102.54424

Batch 32830, train_perplexity=91.23148

Batch 32840, train_perplexity=99.336975

Batch 32850, train_perplexity=98.86288

Batch 32860, train_perplexity=93.13775

Batch 32870, train_perplexity=102.92782

Batch 32880, train_perplexity=103.22016

Batch 32890, train_perplexity=97.372116

Batch 32900, train_perplexity=99.4283

Batch 32910, train_perplexity=92.03625

Batch 32920, train_perplexity=94.7201

Batch 32930, train_perplexity=98.92861

Batch 32940, train_perplexity=100.97827

Batch 32950, train_perplexity=87.03883

Batch 32960, train_perplexity=97.7206

Batch 32970, train_perplexity=98.49665

Batch 32980, train_perplexity=102.39512

Batch 32990, train_perplexity=101.20985

Batch 33000, train_perplexity=98.61052

Batch 33010, train_perplexity=97.49858

Batch 33020, train_perplexity=98.422424

Batch 33030, train_perplexity=104.19659

Batch 33040, train_perplexity=85.46156

Batch 33050, train_perplexity=98.54622

Batch 33060, train_perplexity=91.92449

Batch 33070, train_perplexity=98.595665

Batch 33080, train_perplexity=102.2761

Batch 33090, train_perplexity=89.43283

Batch 33100, train_perplexity=100.48517

Batch 33110, train_perplexity=95.73122

Batch 33120, train_perplexity=98.76134

Batch 33130, train_perplexity=93.98074

Batch 33140, train_perplexity=98.27554

Batch 33150, train_perplexity=89.98786

Batch 33160, train_perplexity=90.553116

Batch 33170, train_perplexity=105.82573

Batch 33180, train_perplexity=103.67511

Batch 33190, train_perplexity=85.34359

Batch 33200, train_perplexity=95.586525

Batch 33210, train_perplexity=91.902405

Batch 33220, train_perplexity=95.642654

Batch 33230, train_perplexity=97.52592

Batch 33240, train_perplexity=102.30058

Batch 33250, train_perplexity=94.58632

Batch 33260, train_perplexity=99.58331

Batch 33270, train_perplexity=87.347206

Batch 33280, train_perplexity=91.27243

Batch 33290, train_perplexity=102.38247

Batch 33300, train_perplexity=96.10393

Batch 33310, train_perplexity=106.07166

Batch 33320, train_perplexity=108.34276

Batch 33330, train_perplexity=103.410866

Batch 33340, train_perplexity=88.201485

Batch 33350, train_perplexity=91.61218

Batch 33360, train_perplexity=109.07801

Batch 33370, train_perplexity=102.55074

Batch 33380, train_perplexity=99.88411

Batch 33390, train_perplexity=88.91399

Batch 33400, train_perplexity=103.21849

Batch 33410, train_perplexity=91.034325

Batch 33420, train_perplexity=100.49748

Batch 33430, train_perplexity=102.87787

Batch 33440, train_perplexity=89.410995

Batch 33450, train_perplexity=92.57291

Batch 33460, train_perplexity=92.13636

Batch 33470, train_perplexity=96.13931

Batch 33480, train_perplexity=84.78703

Batch 33490, train_perplexity=100.47137

Batch 33500, train_perplexity=85.71488

Batch 33510, train_perplexity=98.047874

Batch 33520, train_perplexity=91.73773

Batch 33530, train_perplexity=97.40165

Batch 33540, train_perplexity=96.16077

Batch 33550, train_perplexity=96.33392

Batch 33560, train_perplexity=97.705124

Batch 33570, train_perplexity=100.81613

Batch 33580, train_perplexity=98.525635

Batch 33590, train_perplexity=90.755684

Batch 33600, train_perplexity=95.127464

Batch 33610, train_perplexity=86.4197

Batch 33620, train_perplexity=103.19225

Batch 33630, train_perplexity=104.053986

Batch 33640, train_perplexity=91.06624

Batch 33650, train_perplexity=91.678825

Batch 33660, train_perplexity=106.05786

Batch 33670, train_perplexity=98.4152

Batch 33680, train_perplexity=110.05541

Batch 33690, train_perplexity=95.75961

Batch 33700, train_perplexity=100.4615

Batch 33710, train_perplexity=89.68013

Batch 33720, train_perplexity=94.00732

Batch 33730, train_perplexity=96.428505

Batch 33740, train_perplexity=88.68132

Batch 33750, train_perplexity=96.60394

Batch 33760, train_perplexity=103.393654

Batch 33770, train_perplexity=102.06296

Batch 33780, train_perplexity=110.73384

Batch 33790, train_perplexity=100.36957

Batch 33800, train_perplexity=103.17208

Batch 33810, train_perplexity=106.62797

Batch 33820, train_perplexity=88.59231

Batch 33830, train_perplexity=97.36394

Batch 33840, train_perplexity=100.27208

Batch 33850, train_perplexity=92.89398

Batch 33860, train_perplexity=91.8359

Batch 33870, train_perplexity=99.77348

Batch 33880, train_perplexity=105.69915

Batch 33890, train_perplexity=97.3905

Batch 33900, train_perplexity=96.84341

Batch 33910, train_perplexity=93.37067

Batch 33920, train_perplexity=98.87221

Batch 33930, train_perplexity=103.43789

Batch 33940, train_perplexity=94.70366

Batch 33950, train_perplexity=91.90398

Batch 33960, train_perplexity=90.1545

Batch 33970, train_perplexity=93.92058

Batch 33980, train_perplexity=95.26319

Batch 33990, train_perplexity=99.91212

Batch 34000, train_perplexity=93.78153

Batch 34010, train_perplexity=97.82946

Batch 34020, train_perplexity=94.60066

Batch 34030, train_perplexity=95.46746

Batch 34040, train_perplexity=101.32526

Batch 34050, train_perplexity=89.818275

Batch 34060, train_perplexity=103.49917

Batch 34070, train_perplexity=93.22154

Batch 34080, train_perplexity=97.05902

Batch 34090, train_perplexity=102.77662

Batch 34100, train_perplexity=93.83503

Batch 34110, train_perplexity=100.6606

Batch 34120, train_perplexity=97.61022

Batch 34130, train_perplexity=98.49792

Batch 34140, train_perplexity=90.069855

Batch 34150, train_perplexity=93.63242

Batch 34160, train_perplexity=88.186516

Batch 34170, train_perplexity=96.75478

Batch 34180, train_perplexity=106.09681

Batch 34190, train_perplexity=99.451294

Batch 34200, train_perplexity=102.42124

Batch 34210, train_perplexity=94.99547

Batch 34220, train_perplexity=101.65211

Batch 34230, train_perplexity=101.25778

Batch 34240, train_perplexity=103.44859

Batch 34250, train_perplexity=83.28289

Batch 34260, train_perplexity=87.881

Batch 34270, train_perplexity=92.086205

Batch 34280, train_perplexity=99.64739

Batch 34290, train_perplexity=87.47329

Batch 34300, train_perplexity=87.26353

Batch 34310, train_perplexity=92.74076

Batch 34320, train_perplexity=99.301125

Batch 34330, train_perplexity=97.359116

Batch 34340, train_perplexity=96.142746

Batch 34350, train_perplexity=102.38096

Batch 34360, train_perplexity=97.10453

Batch 34370, train_perplexity=95.24103

Batch 34380, train_perplexity=95.58215

Batch 34390, train_perplexity=106.30479

Batch 34400, train_perplexity=91.57345

Batch 34410, train_perplexity=90.24091

Batch 34420, train_perplexity=91.26699

Batch 34430, train_perplexity=95.06544

Batch 34440, train_perplexity=98.37565

Batch 34450, train_perplexity=94.44534

Batch 34460, train_perplexity=100.75126

Batch 34470, train_perplexity=99.42593

Batch 34480, train_perplexity=93.00541

Batch 34490, train_perplexity=89.316185

Batch 34500, train_perplexity=86.31941

Batch 34510, train_perplexity=85.85064

Batch 34520, train_perplexity=98.985

Batch 34530, train_perplexity=92.652794

Batch 34540, train_perplexity=99.206665

Batch 34550, train_perplexity=98.359276

Batch 34560, train_perplexity=86.17235

Batch 34570, train_perplexity=89.55817

Batch 34580, train_perplexity=104.10342

Batch 34590, train_perplexity=101.927605

Batch 34600, train_perplexity=101.88455

Batch 34610, train_perplexity=95.60335

Batch 34620, train_perplexity=91.4968

Batch 34630, train_perplexity=95.6406

Batch 34640, train_perplexity=89.08837

Batch 34650, train_perplexity=100.178406

Batch 34660, train_perplexity=92.48198

Batch 34670, train_perplexity=90.24414

Batch 34680, train_perplexity=97.32551

Batch 34690, train_perplexity=102.40796

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305615 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00080-of-00100
Loaded 305615 sentences.
Finished loading
Batch 34700, train_perplexity=110.1507

Batch 34710, train_perplexity=91.163734

Batch 34720, train_perplexity=94.369446

Batch 34730, train_perplexity=102.43106

Batch 34740, train_perplexity=95.23431

Batch 34750, train_perplexity=104.423

Batch 34760, train_perplexity=94.4204

Batch 34770, train_perplexity=90.77044

Batch 34780, train_perplexity=92.12125

Batch 34790, train_perplexity=98.23413

Batch 34800, train_perplexity=95.75423

Batch 34810, train_perplexity=94.255486

Batch 34820, train_perplexity=94.14768

Batch 34830, train_perplexity=98.61926

Batch 34840, train_perplexity=90.20567

Batch 34850, train_perplexity=92.66093

Batch 34860, train_perplexity=95.17011

Batch 34870, train_perplexity=95.52119

Batch 34880, train_perplexity=91.91081

Batch 34890, train_perplexity=88.15994

Batch 34900, train_perplexity=88.83796

Batch 34910, train_perplexity=101.425995

Batch 34920, train_perplexity=94.42328

Batch 34930, train_perplexity=101.53981

Batch 34940, train_perplexity=97.19759

Batch 34950, train_perplexity=95.065346

Batch 34960, train_perplexity=101.871925

Batch 34970, train_perplexity=91.821236

Batch 34980, train_perplexity=99.72025

Batch 34990, train_perplexity=87.336334

Batch 35000, train_perplexity=90.75413

Batch 35010, train_perplexity=93.26583

Batch 35020, train_perplexity=84.60215

Batch 35030, train_perplexity=89.836174

Batch 35040, train_perplexity=86.71182

Batch 35050, train_perplexity=87.63578

Batch 35060, train_perplexity=84.87493

Batch 35070, train_perplexity=97.196205

Batch 35080, train_perplexity=96.33792

Batch 35090, train_perplexity=93.32295

Batch 35100, train_perplexity=99.157005

Batch 35110, train_perplexity=89.14888

Batch 35120, train_perplexity=108.10048

Batch 35130, train_perplexity=91.63482

Batch 35140, train_perplexity=88.223946

Batch 35150, train_perplexity=86.619255

Batch 35160, train_perplexity=93.621574

Batch 35170, train_perplexity=86.66346

Batch 35180, train_perplexity=95.04118

Batch 35190, train_perplexity=89.09219

Batch 35200, train_perplexity=89.48722

Batch 35210, train_perplexity=99.57405

Batch 35220, train_perplexity=96.8987

Batch 35230, train_perplexity=97.658455

Batch 35240, train_perplexity=92.71829

Batch 35250, train_perplexity=86.03981

Batch 35260, train_perplexity=83.07224

Batch 35270, train_perplexity=89.50437

Batch 35280, train_perplexity=91.45942

Batch 35290, train_perplexity=95.591545

Batch 35300, train_perplexity=95.34308

Batch 35310, train_perplexity=101.0592

Batch 35320, train_perplexity=101.407524

Batch 35330, train_perplexity=98.658585

Batch 35340, train_perplexity=101.325836

Batch 35350, train_perplexity=92.856735

Batch 35360, train_perplexity=97.347694

Batch 35370, train_perplexity=96.934654

Batch 35380, train_perplexity=83.64007

Batch 35390, train_perplexity=92.67542

Batch 35400, train_perplexity=95.90218

Batch 35410, train_perplexity=86.05274

Batch 35420, train_perplexity=91.71555

Batch 35430, train_perplexity=92.82358

Batch 35440, train_perplexity=89.4423

Batch 35450, train_perplexity=102.24001

Batch 35460, train_perplexity=87.55558

Batch 35470, train_perplexity=108.247635

Batch 35480, train_perplexity=93.05669

Batch 35490, train_perplexity=100.509224

Batch 35500, train_perplexity=85.28245

Batch 35510, train_perplexity=89.14676

Batch 35520, train_perplexity=85.6146

Batch 35530, train_perplexity=94.964035

Batch 35540, train_perplexity=93.45414

Batch 35550, train_perplexity=103.91842

Batch 35560, train_perplexity=92.86475

Batch 35570, train_perplexity=88.55387

Batch 35580, train_perplexity=92.624794

Batch 35590, train_perplexity=93.43894

Batch 35600, train_perplexity=87.649734

Batch 35610, train_perplexity=94.72069

Batch 35620, train_perplexity=92.71414

Batch 35630, train_perplexity=98.06901

Batch 35640, train_perplexity=106.75546

Batch 35650, train_perplexity=91.79772

Batch 35660, train_perplexity=86.48

Batch 35670, train_perplexity=97.27744

Batch 35680, train_perplexity=96.1334

Batch 35690, train_perplexity=81.92761

Batch 35700, train_perplexity=91.04392

Batch 35710, train_perplexity=89.731285

Batch 35720, train_perplexity=100.48603

Batch 35730, train_perplexity=92.706665

Batch 35740, train_perplexity=93.31645

Batch 35750, train_perplexity=100.95516

Batch 35760, train_perplexity=105.972275

Batch 35770, train_perplexity=98.49041

Batch 35780, train_perplexity=96.16058

Batch 35790, train_perplexity=99.45836

Batch 35800, train_perplexity=91.05156

Batch 35810, train_perplexity=88.972046

Batch 35820, train_perplexity=100.28011

Batch 35830, train_perplexity=95.697716

Batch 35840, train_perplexity=93.70331

Batch 35850, train_perplexity=104.71389

Batch 35860, train_perplexity=90.640686

Batch 35870, train_perplexity=90.17471

Batch 35880, train_perplexity=104.33992

Batch 35890, train_perplexity=92.06649

Batch 35900, train_perplexity=108.676796

Batch 35910, train_perplexity=92.86856

Batch 35920, train_perplexity=90.81092

Batch 35930, train_perplexity=97.88942

Batch 35940, train_perplexity=106.619225

Batch 35950, train_perplexity=90.174286

Batch 35960, train_perplexity=92.07636

Batch 35970, train_perplexity=106.14235

Batch 35980, train_perplexity=97.12407

Batch 35990, train_perplexity=94.40644

Batch 36000, train_perplexity=99.30084

Batch 36010, train_perplexity=96.81451

Batch 36020, train_perplexity=94.24147

Batch 36030, train_perplexity=95.13313

Batch 36040, train_perplexity=93.72619

Batch 36050, train_perplexity=93.224655

Batch 36060, train_perplexity=85.94239

Batch 36070, train_perplexity=88.54602

Batch 36080, train_perplexity=96.91783

Batch 36090, train_perplexity=98.00969

Batch 36100, train_perplexity=90.763084

Batch 36110, train_perplexity=96.42106

Batch 36120, train_perplexity=86.21016

Batch 36130, train_perplexity=86.29571

Batch 36140, train_perplexity=90.98854

Batch 36150, train_perplexity=94.17417

Batch 36160, train_perplexity=94.79424

Batch 36170, train_perplexity=92.16585

Batch 36180, train_perplexity=85.16289

Batch 36190, train_perplexity=104.74275

Batch 36200, train_perplexity=93.33942

Batch 36210, train_perplexity=94.12811

Batch 36220, train_perplexity=91.21496

Batch 36230, train_perplexity=94.849815

Batch 36240, train_perplexity=94.93714

Batch 36250, train_perplexity=97.86114

Batch 36260, train_perplexity=88.228325

Batch 36270, train_perplexity=107.902565

Batch 36280, train_perplexity=90.75867

Batch 36290, train_perplexity=88.110725

Batch 36300, train_perplexity=95.036156

Batch 36310, train_perplexity=88.08985

Batch 36320, train_perplexity=87.55517

Batch 36330, train_perplexity=93.43605

Batch 36340, train_perplexity=97.84658

Batch 36350, train_perplexity=82.31002

Batch 36360, train_perplexity=97.85572

Batch 36370, train_perplexity=97.53313

Batch 36380, train_perplexity=93.88757

Batch 36390, train_perplexity=89.72838

Batch 36400, train_perplexity=94.91116

Batch 36410, train_perplexity=98.306946

Batch 36420, train_perplexity=93.72297

Batch 36430, train_perplexity=102.238655

Batch 36440, train_perplexity=91.73205

Batch 36450, train_perplexity=89.16436

Batch 36460, train_perplexity=91.46749

Batch 36470, train_perplexity=96.74123

Batch 36480, train_perplexity=94.62638

Batch 36490, train_perplexity=104.602455

Batch 36500, train_perplexity=93.7278

Batch 36510, train_perplexity=100.34076

Batch 36520, train_perplexity=92.70017

Batch 36530, train_perplexity=100.40255

Batch 36540, train_perplexity=92.62947

Batch 36550, train_perplexity=94.994514

Batch 36560, train_perplexity=100.10706

Batch 36570, train_perplexity=97.95988

Batch 36580, train_perplexity=102.38139

Batch 36590, train_perplexity=90.78658

Batch 36600, train_perplexity=101.33125

Batch 36610, train_perplexity=97.95488

Batch 36620, train_perplexity=94.81039

Batch 36630, train_perplexity=99.78257

Batch 36640, train_perplexity=89.72239

Batch 36650, train_perplexity=98.72329

Batch 36660, train_perplexity=87.38383
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 36670, train_perplexity=108.2719

Batch 36680, train_perplexity=106.59645

Batch 36690, train_perplexity=93.211235

Batch 36700, train_perplexity=88.16751

Batch 36710, train_perplexity=86.957275

Batch 36720, train_perplexity=86.52076

Batch 36730, train_perplexity=92.94321

Batch 36740, train_perplexity=97.00799

Batch 36750, train_perplexity=91.22757

Batch 36760, train_perplexity=102.71969

Batch 36770, train_perplexity=98.26786

Batch 36780, train_perplexity=92.344795

Batch 36790, train_perplexity=94.57239

Batch 36800, train_perplexity=88.30829

Batch 36810, train_perplexity=93.10884

Batch 36820, train_perplexity=100.07079

Batch 36830, train_perplexity=84.05817

Batch 36840, train_perplexity=95.656654

Batch 36850, train_perplexity=94.04386

Batch 36860, train_perplexity=100.663185

Batch 36870, train_perplexity=93.000175

Batch 36880, train_perplexity=97.039955

Batch 36890, train_perplexity=99.354645

Batch 36900, train_perplexity=88.46482

Batch 36910, train_perplexity=82.98716

Batch 36920, train_perplexity=102.1055

Batch 36930, train_perplexity=88.25272

Batch 36940, train_perplexity=92.47042

Batch 36950, train_perplexity=95.882706

Batch 36960, train_perplexity=84.39569

Batch 36970, train_perplexity=104.39063

Batch 36980, train_perplexity=83.99502

Batch 36990, train_perplexity=99.43057

Batch 37000, train_perplexity=95.40912

Batch 37010, train_perplexity=102.30185

Batch 37020, train_perplexity=93.196075

Batch 37030, train_perplexity=80.41582

Batch 37040, train_perplexity=96.20039

Batch 37050, train_perplexity=100.10391

Batch 37060, train_perplexity=93.24577

Batch 37070, train_perplexity=98.23741

Batch 37080, train_perplexity=97.54071

Batch 37090, train_perplexity=90.72176

Batch 37100, train_perplexity=79.711754

Batch 37110, train_perplexity=96.57327

Batch 37120, train_perplexity=99.48786

Batch 37130, train_perplexity=99.281715

Batch 37140, train_perplexity=87.07565

Batch 37150, train_perplexity=94.59525

Batch 37160, train_perplexity=86.3979

Batch 37170, train_perplexity=90.44006

Batch 37180, train_perplexity=84.23994

Batch 37190, train_perplexity=88.936035

Batch 37200, train_perplexity=95.94646

Batch 37210, train_perplexity=93.45289

Batch 37220, train_perplexity=92.73589

Batch 37230, train_perplexity=83.66656

Batch 37240, train_perplexity=88.33962

Batch 37250, train_perplexity=88.53217

Batch 37260, train_perplexity=95.45367

Batch 37270, train_perplexity=85.59215

Batch 37280, train_perplexity=85.71549

Batch 37290, train_perplexity=81.82095

Batch 37300, train_perplexity=95.56019

Batch 37310, train_perplexity=97.41405

Batch 37320, train_perplexity=93.67409

Batch 37330, train_perplexity=81.930115

Batch 37340, train_perplexity=90.904236

Batch 37350, train_perplexity=94.56323

Batch 37360, train_perplexity=97.90183

Batch 37370, train_perplexity=91.272125

Batch 37380, train_perplexity=82.798584

Batch 37390, train_perplexity=91.47342

Batch 37400, train_perplexity=97.87467

Batch 37410, train_perplexity=92.75111

Batch 37420, train_perplexity=98.36345

Batch 37430, train_perplexity=103.35477

Batch 37440, train_perplexity=102.38882

Batch 37450, train_perplexity=100.54

Batch 37460, train_perplexity=100.65675

Batch 37470, train_perplexity=94.40329

Batch 37480, train_perplexity=96.34541

Batch 37490, train_perplexity=103.75602

Batch 37500, train_perplexity=95.465546

Batch 37510, train_perplexity=83.85412

Batch 37520, train_perplexity=90.60737

Batch 37530, train_perplexity=96.21906

Batch 37540, train_perplexity=97.280266

Batch 37550, train_perplexity=90.85653

Batch 37560, train_perplexity=88.47064

Batch 37570, train_perplexity=90.66671

Batch 37580, train_perplexity=99.25459

Batch 37590, train_perplexity=96.01364

Batch 37600, train_perplexity=91.52259

Batch 37610, train_perplexity=91.57685

Batch 37620, train_perplexity=87.63996

Batch 37630, train_perplexity=85.02197

Batch 37640, train_perplexity=91.90135

Batch 37650, train_perplexity=89.95183

Batch 37660, train_perplexity=98.053955

Batch 37670, train_perplexity=94.780594

Batch 37680, train_perplexity=82.98882

Batch 37690, train_perplexity=92.150856

Batch 37700, train_perplexity=98.46542

Batch 37710, train_perplexity=88.04013

Batch 37720, train_perplexity=94.74169

Batch 37730, train_perplexity=82.84546

Batch 37740, train_perplexity=94.13691

Batch 37750, train_perplexity=101.28183

Batch 37760, train_perplexity=85.49776

Batch 37770, train_perplexity=92.33639

Batch 37780, train_perplexity=95.85765

Batch 37790, train_perplexity=100.28638

Batch 37800, train_perplexity=86.824066

Batch 37810, train_perplexity=100.37704

Batch 37820, train_perplexity=87.13243

Batch 37830, train_perplexity=100.22069

Batch 37840, train_perplexity=104.731964

Batch 37850, train_perplexity=88.87071

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00009-of-00100
Loaded 305917 sentences.
Finished loading
Batch 37860, train_perplexity=90.33244

Batch 37870, train_perplexity=88.079475

Batch 37880, train_perplexity=93.93298

Batch 37890, train_perplexity=88.27781

Batch 37900, train_perplexity=87.927734

Batch 37910, train_perplexity=92.29505

Batch 37920, train_perplexity=93.156715

Batch 37930, train_perplexity=94.13157

Batch 37940, train_perplexity=92.64538

Batch 37950, train_perplexity=94.403336

Batch 37960, train_perplexity=96.6562

Batch 37970, train_perplexity=97.10883

Batch 37980, train_perplexity=93.36759

Batch 37990, train_perplexity=96.47266

Batch 38000, train_perplexity=88.02309

Batch 38010, train_perplexity=87.7414

Batch 38020, train_perplexity=85.12696

Batch 38030, train_perplexity=101.58902

Batch 38040, train_perplexity=97.28171

Batch 38050, train_perplexity=88.3751

Batch 38060, train_perplexity=96.097466

Batch 38070, train_perplexity=91.47486

Batch 38080, train_perplexity=88.88359

Batch 38090, train_perplexity=99.898636

Batch 38100, train_perplexity=87.57942

Batch 38110, train_perplexity=90.33645

Batch 38120, train_perplexity=89.871994

Batch 38130, train_perplexity=96.14073

Batch 38140, train_perplexity=98.83035

Batch 38150, train_perplexity=94.50463

Batch 38160, train_perplexity=87.535965

Batch 38170, train_perplexity=93.771736

Batch 38180, train_perplexity=85.16126

Batch 38190, train_perplexity=91.80867

Batch 38200, train_perplexity=85.95354

Batch 38210, train_perplexity=93.18084

Batch 38220, train_perplexity=91.65304

Batch 38230, train_perplexity=90.1671

Batch 38240, train_perplexity=93.50023

Batch 38250, train_perplexity=87.08857

Batch 38260, train_perplexity=90.01838

Batch 38270, train_perplexity=84.79787

Batch 38280, train_perplexity=84.37968

Batch 38290, train_perplexity=87.446014

Batch 38300, train_perplexity=93.87844

Batch 38310, train_perplexity=93.01126

Batch 38320, train_perplexity=95.47871

Batch 38330, train_perplexity=100.404274

Batch 38340, train_perplexity=97.92901

Batch 38350, train_perplexity=91.728195

Batch 38360, train_perplexity=88.27899

Batch 38370, train_perplexity=84.0225

Batch 38380, train_perplexity=90.1521

Batch 38390, train_perplexity=93.62988

Batch 38400, train_perplexity=95.92016

Batch 38410, train_perplexity=88.16566

Batch 38420, train_perplexity=92.38312

Batch 38430, train_perplexity=101.304146

Batch 38440, train_perplexity=83.89135

Batch 38450, train_perplexity=90.62737

Batch 38460, train_perplexity=93.137924

Batch 38470, train_perplexity=87.08313

Batch 38480, train_perplexity=83.792206

Batch 38490, train_perplexity=92.89035

Batch 38500, train_perplexity=94.687584

Batch 38510, train_perplexity=102.43316

Batch 38520, train_perplexity=94.0426

Batch 38530, train_perplexity=95.48339

Batch 38540, train_perplexity=81.43903

Batch 38550, train_perplexity=99.52288

Batch 38560, train_perplexity=104.4208

Batch 38570, train_perplexity=82.23007

Batch 38580, train_perplexity=90.379105

Batch 38590, train_perplexity=84.51489

Batch 38600, train_perplexity=88.68225
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 38610, train_perplexity=88.32421

Batch 38620, train_perplexity=91.38596

Batch 38630, train_perplexity=89.24505

Batch 38640, train_perplexity=94.318436

Batch 38650, train_perplexity=96.93378

Batch 38660, train_perplexity=89.62353

Batch 38670, train_perplexity=88.58555

Batch 38680, train_perplexity=76.09878

Batch 38690, train_perplexity=89.94668

Batch 38700, train_perplexity=90.8869

Batch 38710, train_perplexity=91.456276

Batch 38720, train_perplexity=97.47478

Batch 38730, train_perplexity=100.92816

Batch 38740, train_perplexity=89.713486

Batch 38750, train_perplexity=96.53119

Batch 38760, train_perplexity=92.131966

Batch 38770, train_perplexity=87.07059

Batch 38780, train_perplexity=85.882

Batch 38790, train_perplexity=93.9698

Batch 38800, train_perplexity=82.223724

Batch 38810, train_perplexity=87.641884

Batch 38820, train_perplexity=92.120636

Batch 38830, train_perplexity=95.18128

Batch 38840, train_perplexity=93.36154

Batch 38850, train_perplexity=88.24759

Batch 38860, train_perplexity=94.78001

Batch 38870, train_perplexity=94.39375

Batch 38880, train_perplexity=91.96

Batch 38890, train_perplexity=93.32446

Batch 38900, train_perplexity=91.862885

Batch 38910, train_perplexity=87.552956

Batch 38920, train_perplexity=89.9312

Batch 38930, train_perplexity=97.96152

Batch 38940, train_perplexity=94.53564

Batch 38950, train_perplexity=90.67812

Batch 38960, train_perplexity=87.91612

Batch 38970, train_perplexity=85.995964

Batch 38980, train_perplexity=90.94985

Batch 38990, train_perplexity=86.99456

Batch 39000, train_perplexity=84.21174

Batch 39010, train_perplexity=86.52633

Batch 39020, train_perplexity=88.80293

Batch 39030, train_perplexity=93.29794

Batch 39040, train_perplexity=86.24626

Batch 39050, train_perplexity=96.678505

Batch 39060, train_perplexity=93.9483

Batch 39070, train_perplexity=93.92823

Batch 39080, train_perplexity=94.77178

Batch 39090, train_perplexity=86.06681

Batch 39100, train_perplexity=91.53206

Batch 39110, train_perplexity=96.76244

Batch 39120, train_perplexity=92.54382

Batch 39130, train_perplexity=79.37002

Batch 39140, train_perplexity=90.878845

Batch 39150, train_perplexity=97.06291

Batch 39160, train_perplexity=90.20293

Batch 39170, train_perplexity=89.41462

Batch 39180, train_perplexity=91.26543

Batch 39190, train_perplexity=96.8447

Batch 39200, train_perplexity=90.07132

Batch 39210, train_perplexity=93.3793

Batch 39220, train_perplexity=92.91338

Batch 39230, train_perplexity=92.79313

Batch 39240, train_perplexity=102.17568

Batch 39250, train_perplexity=87.488266

Batch 39260, train_perplexity=86.80466

Batch 39270, train_perplexity=84.03036

Batch 39280, train_perplexity=98.02862

Batch 39290, train_perplexity=88.112366

Batch 39300, train_perplexity=97.76585

Batch 39310, train_perplexity=89.22228

Batch 39320, train_perplexity=84.260826

Batch 39330, train_perplexity=79.853424

Batch 39340, train_perplexity=86.09986

Batch 39350, train_perplexity=86.50261

Batch 39360, train_perplexity=92.92694

Batch 39370, train_perplexity=84.39284

Batch 39380, train_perplexity=83.49666

Batch 39390, train_perplexity=87.986244

Batch 39400, train_perplexity=107.62529

Batch 39410, train_perplexity=101.132324

Batch 39420, train_perplexity=89.73955

Batch 39430, train_perplexity=90.391945

Batch 39440, train_perplexity=85.92149

Batch 39450, train_perplexity=100.04098

Batch 39460, train_perplexity=102.20824

Batch 39470, train_perplexity=100.462265

Batch 39480, train_perplexity=90.417206

Batch 39490, train_perplexity=94.885185

Batch 39500, train_perplexity=92.17921

Batch 39510, train_perplexity=98.8058

Batch 39520, train_perplexity=95.01912

Batch 39530, train_perplexity=90.34454

Batch 39540, train_perplexity=94.55638

Batch 39550, train_perplexity=96.203926

Batch 39560, train_perplexity=99.44731

Batch 39570, train_perplexity=92.51744

Batch 39580, train_perplexity=94.05166

Batch 39590, train_perplexity=87.77421

Batch 39600, train_perplexity=94.69896

Batch 39610, train_perplexity=92.26667

Batch 39620, train_perplexity=95.03969

Batch 39630, train_perplexity=89.51743

Batch 39640, train_perplexity=100.860466

Batch 39650, train_perplexity=95.34226

Batch 39660, train_perplexity=85.13398

Batch 39670, train_perplexity=95.797745

Batch 39680, train_perplexity=81.88235

Batch 39690, train_perplexity=90.41592

Batch 39700, train_perplexity=89.390236

Batch 39710, train_perplexity=93.7038

Batch 39720, train_perplexity=85.67815

Batch 39730, train_perplexity=100.912186

Batch 39740, train_perplexity=84.60409

Batch 39750, train_perplexity=90.33386

Batch 39760, train_perplexity=84.67984

Batch 39770, train_perplexity=85.92789

Batch 39780, train_perplexity=88.08527

Batch 39790, train_perplexity=87.04622

Batch 39800, train_perplexity=87.72902

Batch 39810, train_perplexity=88.05113

Batch 39820, train_perplexity=96.265686

Batch 39830, train_perplexity=80.58771

Batch 39840, train_perplexity=94.042465

Batch 39850, train_perplexity=97.51932

Batch 39860, train_perplexity=98.34005

Batch 39870, train_perplexity=98.93767

Batch 39880, train_perplexity=93.402016

Batch 39890, train_perplexity=90.877106

Batch 39900, train_perplexity=96.19044

Batch 39910, train_perplexity=97.78338

Batch 39920, train_perplexity=88.77008

Batch 39930, train_perplexity=87.66512

Batch 39940, train_perplexity=97.18935

Batch 39950, train_perplexity=88.111824

Batch 39960, train_perplexity=87.846054

Batch 39970, train_perplexity=91.30878

Batch 39980, train_perplexity=90.97345

Batch 39990, train_perplexity=77.003624

Batch 40000, train_perplexity=90.97427

Batch 40010, train_perplexity=93.85185

Batch 40020, train_perplexity=96.796776

Batch 40030, train_perplexity=90.71406

Batch 40040, train_perplexity=86.325836

Batch 40050, train_perplexity=89.278885

Batch 40060, train_perplexity=96.018776

Batch 40070, train_perplexity=96.25036

Batch 40080, train_perplexity=93.08234

Batch 40090, train_perplexity=96.918755

Batch 40100, train_perplexity=85.515945

Batch 40110, train_perplexity=86.63586

Batch 40120, train_perplexity=89.46861

Batch 40130, train_perplexity=84.304955

Batch 40140, train_perplexity=87.428795

Batch 40150, train_perplexity=85.50689

Batch 40160, train_perplexity=95.48845

Batch 40170, train_perplexity=94.08454

Batch 40180, train_perplexity=109.30544

Batch 40190, train_perplexity=86.80308

Batch 40200, train_perplexity=94.03704

Batch 40210, train_perplexity=94.485886

Batch 40220, train_perplexity=90.09898

Batch 40230, train_perplexity=90.45714

Batch 40240, train_perplexity=84.8511

Batch 40250, train_perplexity=97.40109

Batch 40260, train_perplexity=83.27812

Batch 40270, train_perplexity=87.08923

Batch 40280, train_perplexity=87.995476

Batch 40290, train_perplexity=92.99902

Batch 40300, train_perplexity=86.37179

Batch 40310, train_perplexity=84.53013

Batch 40320, train_perplexity=87.899185

Batch 40330, train_perplexity=84.73781

Batch 40340, train_perplexity=88.466255

Batch 40350, train_perplexity=93.07284

Batch 40360, train_perplexity=90.13052

Batch 40370, train_perplexity=85.42009

Batch 40380, train_perplexity=97.73402

Batch 40390, train_perplexity=84.60497

Batch 40400, train_perplexity=96.39178

Batch 40410, train_perplexity=85.75981

Batch 40420, train_perplexity=83.90215

Batch 40430, train_perplexity=88.01431

Batch 40440, train_perplexity=89.51982

Batch 40450, train_perplexity=86.88756

Batch 40460, train_perplexity=90.84747

Batch 40470, train_perplexity=85.79654

Batch 40480, train_perplexity=88.6847

Batch 40490, train_perplexity=86.99991

Batch 40500, train_perplexity=95.87932

Batch 40510, train_perplexity=85.55354

Batch 40520, train_perplexity=93.18964

Batch 40530, train_perplexity=90.36376

Batch 40540, train_perplexity=86.07174

Batch 40550, train_perplexity=87.66211

Batch 40560, train_perplexity=86.85177

Batch 40570, train_perplexity=88.59138

Batch 40580, train_perplexity=93.58837

Batch 40590, train_perplexity=89.823494

Batch 40600, train_perplexity=85.56268

Batch 40610, train_perplexity=92.964615

Batch 40620, train_perplexity=81.56721

Batch 40630, train_perplexity=84.10247

Batch 40640, train_perplexity=88.622475
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 40650, train_perplexity=99.011765

Batch 40660, train_perplexity=90.78161

Batch 40670, train_perplexity=89.54493

Batch 40680, train_perplexity=89.1278

Batch 40690, train_perplexity=93.72109

Batch 40700, train_perplexity=98.02488

Batch 40710, train_perplexity=88.41451

Batch 40720, train_perplexity=95.74637

Batch 40730, train_perplexity=97.95974

Batch 40740, train_perplexity=94.6299

Batch 40750, train_perplexity=87.30395

Batch 40760, train_perplexity=86.26436

Batch 40770, train_perplexity=96.01443

Batch 40780, train_perplexity=94.1361

Batch 40790, train_perplexity=88.78287

Batch 40800, train_perplexity=85.26407

Batch 40810, train_perplexity=99.466705

Batch 40820, train_perplexity=98.19105

Batch 40830, train_perplexity=93.85105

Batch 40840, train_perplexity=89.929695

Batch 40850, train_perplexity=97.56257

Batch 40860, train_perplexity=95.048485

Batch 40870, train_perplexity=85.22862

Batch 40880, train_perplexity=89.55723

Batch 40890, train_perplexity=90.54008

Batch 40900, train_perplexity=99.013084

Batch 40910, train_perplexity=96.72683

Batch 40920, train_perplexity=91.474205

Batch 40930, train_perplexity=94.54357

Batch 40940, train_perplexity=88.866005

Batch 40950, train_perplexity=83.5882

Batch 40960, train_perplexity=99.29213

Batch 40970, train_perplexity=85.85244

Batch 40980, train_perplexity=99.994286

Batch 40990, train_perplexity=87.39774

Batch 41000, train_perplexity=96.176956

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00020-of-00100
Loaded 305446 sentences.
Finished loading
Batch 41010, train_perplexity=90.83487

Batch 41020, train_perplexity=87.63419

Batch 41030, train_perplexity=92.66376

Batch 41040, train_perplexity=86.00109

Batch 41050, train_perplexity=83.14897

Batch 41060, train_perplexity=76.395164

Batch 41070, train_perplexity=88.94032

Batch 41080, train_perplexity=95.17007

Batch 41090, train_perplexity=82.28114

Batch 41100, train_perplexity=88.869354

Batch 41110, train_perplexity=91.19447

Batch 41120, train_perplexity=92.88468

Batch 41130, train_perplexity=89.18009

Batch 41140, train_perplexity=91.56965

Batch 41150, train_perplexity=85.16033

Batch 41160, train_perplexity=83.34403

Batch 41170, train_perplexity=85.51586

Batch 41180, train_perplexity=94.33647

Batch 41190, train_perplexity=94.65341

Batch 41200, train_perplexity=83.88367

Batch 41210, train_perplexity=93.592606

Batch 41220, train_perplexity=87.048546

Batch 41230, train_perplexity=83.429

Batch 41240, train_perplexity=93.38866

Batch 41250, train_perplexity=84.69426

Batch 41260, train_perplexity=84.312996

Batch 41270, train_perplexity=86.978096

Batch 41280, train_perplexity=91.718796

Batch 41290, train_perplexity=96.17728

Batch 41300, train_perplexity=84.54891

Batch 41310, train_perplexity=87.78153

Batch 41320, train_perplexity=88.11102

Batch 41330, train_perplexity=80.40999

Batch 41340, train_perplexity=91.53171

Batch 41350, train_perplexity=92.69946

Batch 41360, train_perplexity=83.4117

Batch 41370, train_perplexity=93.27606

Batch 41380, train_perplexity=83.04099

Batch 41390, train_perplexity=99.926125

Batch 41400, train_perplexity=84.98748

Batch 41410, train_perplexity=81.724335

Batch 41420, train_perplexity=91.10142

Batch 41430, train_perplexity=93.39026

Batch 41440, train_perplexity=85.32621

Batch 41450, train_perplexity=84.78388

Batch 41460, train_perplexity=97.62009

Batch 41470, train_perplexity=86.936134

Batch 41480, train_perplexity=84.12689

Batch 41490, train_perplexity=86.79824

Batch 41500, train_perplexity=83.34423

Batch 41510, train_perplexity=96.13881

Batch 41520, train_perplexity=87.82055

Batch 41530, train_perplexity=87.71672

Batch 41540, train_perplexity=82.46575

Batch 41550, train_perplexity=95.657295

Batch 41560, train_perplexity=90.338776

Batch 41570, train_perplexity=84.06498

Batch 41580, train_perplexity=93.897156

Batch 41590, train_perplexity=89.49763

Batch 41600, train_perplexity=84.45705

Batch 41610, train_perplexity=89.263306

Batch 41620, train_perplexity=84.953445

Batch 41630, train_perplexity=91.20269

Batch 41640, train_perplexity=95.33822

Batch 41650, train_perplexity=85.1847

Batch 41660, train_perplexity=95.27205

Batch 41670, train_perplexity=90.3576

Batch 41680, train_perplexity=83.88667

Batch 41690, train_perplexity=93.31058

Batch 41700, train_perplexity=86.015656

Batch 41710, train_perplexity=90.56849

Batch 41720, train_perplexity=88.50292

Batch 41730, train_perplexity=99.851395

Batch 41740, train_perplexity=87.077774

Batch 41750, train_perplexity=96.102135

Batch 41760, train_perplexity=84.92011

Batch 41770, train_perplexity=85.76865

Batch 41780, train_perplexity=95.84495

Batch 41790, train_perplexity=87.701996

Batch 41800, train_perplexity=99.2465

Batch 41810, train_perplexity=84.14419

Batch 41820, train_perplexity=88.29414

Batch 41830, train_perplexity=87.29104

Batch 41840, train_perplexity=94.7683

Batch 41850, train_perplexity=91.504745

Batch 41860, train_perplexity=98.45397

Batch 41870, train_perplexity=90.95271

Batch 41880, train_perplexity=85.32101

Batch 41890, train_perplexity=86.5953

Batch 41900, train_perplexity=83.98465

Batch 41910, train_perplexity=85.664825

Batch 41920, train_perplexity=86.805565

Batch 41930, train_perplexity=87.08558

Batch 41940, train_perplexity=86.31036

Batch 41950, train_perplexity=82.38216

Batch 41960, train_perplexity=97.70419

Batch 41970, train_perplexity=85.50999

Batch 41980, train_perplexity=90.74564

Batch 41990, train_perplexity=81.332695

Batch 42000, train_perplexity=96.0119

Batch 42010, train_perplexity=84.8942

Batch 42020, train_perplexity=89.52281

Batch 42030, train_perplexity=91.8401

Batch 42040, train_perplexity=85.16492

Batch 42050, train_perplexity=93.44906

Batch 42060, train_perplexity=84.44029

Batch 42070, train_perplexity=82.551994

Batch 42080, train_perplexity=93.74335

Batch 42090, train_perplexity=87.19378

Batch 42100, train_perplexity=90.8317

Batch 42110, train_perplexity=89.348305

Batch 42120, train_perplexity=99.1719

Batch 42130, train_perplexity=80.57987

Batch 42140, train_perplexity=85.87103

Batch 42150, train_perplexity=92.22242

Batch 42160, train_perplexity=81.868645

Batch 42170, train_perplexity=80.79213

Batch 42180, train_perplexity=94.978165

Batch 42190, train_perplexity=88.97935

Batch 42200, train_perplexity=94.06054

Batch 42210, train_perplexity=94.12385

Batch 42220, train_perplexity=86.01578

Batch 42230, train_perplexity=100.3134

Batch 42240, train_perplexity=93.24137

Batch 42250, train_perplexity=91.84186

Batch 42260, train_perplexity=82.45741

Batch 42270, train_perplexity=88.45052

Batch 42280, train_perplexity=88.582886

Batch 42290, train_perplexity=94.52897

Batch 42300, train_perplexity=93.690704

Batch 42310, train_perplexity=93.297676

Batch 42320, train_perplexity=92.84917

Batch 42330, train_perplexity=86.84481

Batch 42340, train_perplexity=93.72663

Batch 42350, train_perplexity=88.88758

Batch 42360, train_perplexity=83.82793

Batch 42370, train_perplexity=92.376686

Batch 42380, train_perplexity=84.19231

Batch 42390, train_perplexity=84.96608

Batch 42400, train_perplexity=86.12191

Batch 42410, train_perplexity=83.04171

Batch 42420, train_perplexity=88.23362

Batch 42430, train_perplexity=81.46012

Batch 42440, train_perplexity=96.53533

Batch 42450, train_perplexity=86.97652

Batch 42460, train_perplexity=87.18513

Batch 42470, train_perplexity=95.55144

Batch 42480, train_perplexity=84.40825

Batch 42490, train_perplexity=97.505646

Batch 42500, train_perplexity=95.454254

Batch 42510, train_perplexity=95.15659

Batch 42520, train_perplexity=89.18315

Batch 42530, train_perplexity=89.585075

Batch 42540, train_perplexity=86.68842

Batch 42550, train_perplexity=92.17319

Batch 42560, train_perplexity=85.07891

Batch 42570, train_perplexity=89.07588

Batch 42580, train_perplexity=88.78925
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42590, train_perplexity=92.893005

Batch 42600, train_perplexity=93.368484

Batch 42610, train_perplexity=93.30853

Batch 42620, train_perplexity=92.299675

Batch 42630, train_perplexity=83.30386

Batch 42640, train_perplexity=87.636444

Batch 42650, train_perplexity=88.19042

Batch 42660, train_perplexity=81.66317

Batch 42670, train_perplexity=79.740265

Batch 42680, train_perplexity=88.0225

Batch 42690, train_perplexity=87.06606

Batch 42700, train_perplexity=96.30141

Batch 42710, train_perplexity=92.34757

Batch 42720, train_perplexity=89.468704

Batch 42730, train_perplexity=84.1058

Batch 42740, train_perplexity=93.36363

Batch 42750, train_perplexity=100.687

Batch 42760, train_perplexity=94.91822

Batch 42770, train_perplexity=92.769066

Batch 42780, train_perplexity=97.76631

Batch 42790, train_perplexity=93.83123

Batch 42800, train_perplexity=90.6486

Batch 42810, train_perplexity=86.93306

Batch 42820, train_perplexity=82.702736

Batch 42830, train_perplexity=92.983765

Batch 42840, train_perplexity=84.96937

Batch 42850, train_perplexity=87.949165

Batch 42860, train_perplexity=87.423256

Batch 42870, train_perplexity=85.21813

Batch 42880, train_perplexity=87.01211

Batch 42890, train_perplexity=87.37899

Batch 42900, train_perplexity=80.020836

Batch 42910, train_perplexity=86.832726

Batch 42920, train_perplexity=89.918465

Batch 42930, train_perplexity=86.6821

Batch 42940, train_perplexity=93.528145

Batch 42950, train_perplexity=99.76749

Batch 42960, train_perplexity=87.564766

Batch 42970, train_perplexity=96.12565

Batch 42980, train_perplexity=96.14582

Batch 42990, train_perplexity=87.11906

Batch 43000, train_perplexity=91.11024

Batch 43010, train_perplexity=91.46915

Batch 43020, train_perplexity=90.17416

Batch 43030, train_perplexity=94.5608

Batch 43040, train_perplexity=95.46628

Batch 43050, train_perplexity=89.19387

Batch 43060, train_perplexity=85.08593

Batch 43070, train_perplexity=94.609505

Batch 43080, train_perplexity=96.45923

Batch 43090, train_perplexity=95.024826

Batch 43100, train_perplexity=88.943924

Batch 43110, train_perplexity=84.84479

Batch 43120, train_perplexity=86.766335

Batch 43130, train_perplexity=97.026634

Batch 43140, train_perplexity=93.21452

Batch 43150, train_perplexity=89.66743

Batch 43160, train_perplexity=92.42925

Batch 43170, train_perplexity=82.52641

Batch 43180, train_perplexity=88.83232

Batch 43190, train_perplexity=89.10209

Batch 43200, train_perplexity=88.08053

Batch 43210, train_perplexity=92.61247

Batch 43220, train_perplexity=80.44818

Batch 43230, train_perplexity=85.64506

Batch 43240, train_perplexity=95.39301

Batch 43250, train_perplexity=83.29766

Batch 43260, train_perplexity=77.26945

Batch 43270, train_perplexity=87.62993

Batch 43280, train_perplexity=89.11667

Batch 43290, train_perplexity=93.53108

Batch 43300, train_perplexity=77.935585

Batch 43310, train_perplexity=89.802216

Batch 43320, train_perplexity=90.35084

Batch 43330, train_perplexity=87.82553

Batch 43340, train_perplexity=90.75274

Batch 43350, train_perplexity=84.70298

Batch 43360, train_perplexity=77.771866

Batch 43370, train_perplexity=95.198166

Batch 43380, train_perplexity=88.57292

Batch 43390, train_perplexity=91.75423

Batch 43400, train_perplexity=82.28455

Batch 43410, train_perplexity=92.832695

Batch 43420, train_perplexity=92.31834

Batch 43430, train_perplexity=92.1187

Batch 43440, train_perplexity=87.70041

Batch 43450, train_perplexity=91.99105

Batch 43460, train_perplexity=94.992615

Batch 43470, train_perplexity=85.26358

Batch 43480, train_perplexity=80.51319

Batch 43490, train_perplexity=90.699486

Batch 43500, train_perplexity=90.82175

Batch 43510, train_perplexity=86.4213

Batch 43520, train_perplexity=96.38658

Batch 43530, train_perplexity=87.451935

Batch 43540, train_perplexity=86.22299

Batch 43550, train_perplexity=85.381645

Batch 43560, train_perplexity=79.259056

Batch 43570, train_perplexity=80.89536

Batch 43580, train_perplexity=93.007

Batch 43590, train_perplexity=82.96445

Batch 43600, train_perplexity=89.43807

Batch 43610, train_perplexity=93.80277

Batch 43620, train_perplexity=83.97564

Batch 43630, train_perplexity=79.11247

Batch 43640, train_perplexity=94.68474

Batch 43650, train_perplexity=87.43205

Batch 43660, train_perplexity=85.716064

Batch 43670, train_perplexity=80.80935

Batch 43680, train_perplexity=81.295944

Batch 43690, train_perplexity=79.317276

Batch 43700, train_perplexity=92.53341

Batch 43710, train_perplexity=90.22211

Batch 43720, train_perplexity=86.96233

Batch 43730, train_perplexity=85.88684

Batch 43740, train_perplexity=89.50813

Batch 43750, train_perplexity=96.40653

Batch 43760, train_perplexity=91.95671

Batch 43770, train_perplexity=95.18958

Batch 43780, train_perplexity=81.676414

Batch 43790, train_perplexity=92.85058

Batch 43800, train_perplexity=90.10474

Batch 43810, train_perplexity=89.32445

Batch 43820, train_perplexity=82.75212

Batch 43830, train_perplexity=91.703316

Batch 43840, train_perplexity=97.390686

Batch 43850, train_perplexity=89.03121

Batch 43860, train_perplexity=87.28488

Batch 43870, train_perplexity=91.95548

Batch 43880, train_perplexity=88.16028

Batch 43890, train_perplexity=88.70999

Batch 43900, train_perplexity=88.5967

Batch 43910, train_perplexity=96.732506

Batch 43920, train_perplexity=81.32541

Batch 43930, train_perplexity=85.08731

Batch 43940, train_perplexity=83.692535

Batch 43950, train_perplexity=91.01163

Batch 43960, train_perplexity=90.6991

Batch 43970, train_perplexity=84.24648

Batch 43980, train_perplexity=92.30029

Batch 43990, train_perplexity=83.37618

Batch 44000, train_perplexity=86.24248

Batch 44010, train_perplexity=82.43666

Batch 44020, train_perplexity=82.47661

Batch 44030, train_perplexity=90.85761

Batch 44040, train_perplexity=92.41841

Batch 44050, train_perplexity=87.27939

Batch 44060, train_perplexity=80.29634

Batch 44070, train_perplexity=79.29822

Batch 44080, train_perplexity=87.534294

Batch 44090, train_perplexity=92.66411

Batch 44100, train_perplexity=93.66176

Batch 44110, train_perplexity=88.53166

Batch 44120, train_perplexity=87.05933

Batch 44130, train_perplexity=89.60207

Batch 44140, train_perplexity=100.77048

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00093-of-00100
Loaded 306407 sentences.
Finished loading
Batch 44150, train_perplexity=97.52043

Batch 44160, train_perplexity=84.328514

Batch 44170, train_perplexity=85.8242

Batch 44180, train_perplexity=94.79759

Batch 44190, train_perplexity=101.93373

Batch 44200, train_perplexity=79.74015

Batch 44210, train_perplexity=76.2549

Batch 44220, train_perplexity=82.67072

Batch 44230, train_perplexity=88.67747

Batch 44240, train_perplexity=93.98585

Batch 44250, train_perplexity=80.71354

Batch 44260, train_perplexity=88.05172

Batch 44270, train_perplexity=92.38779

Batch 44280, train_perplexity=91.51321

Batch 44290, train_perplexity=98.0038

Batch 44300, train_perplexity=87.42801

Batch 44310, train_perplexity=78.8912

Batch 44320, train_perplexity=82.2064

Batch 44330, train_perplexity=79.98521

Batch 44340, train_perplexity=95.8604

Batch 44350, train_perplexity=91.910904

Batch 44360, train_perplexity=81.67482

Batch 44370, train_perplexity=91.43604

Batch 44380, train_perplexity=87.13542

Batch 44390, train_perplexity=89.18102

Batch 44400, train_perplexity=86.28855

Batch 44410, train_perplexity=94.906364

Batch 44420, train_perplexity=86.60851

Batch 44430, train_perplexity=84.49257

Batch 44440, train_perplexity=91.53564

Batch 44450, train_perplexity=79.42757

Batch 44460, train_perplexity=95.90209

Batch 44470, train_perplexity=89.444305

Batch 44480, train_perplexity=84.52308

Batch 44490, train_perplexity=91.780396

Batch 44500, train_perplexity=85.39211

Batch 44510, train_perplexity=82.30359

Batch 44520, train_perplexity=94.867004
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44530, train_perplexity=83.23477

Batch 44540, train_perplexity=81.6037

Batch 44550, train_perplexity=88.64902

Batch 44560, train_perplexity=87.45961

Batch 44570, train_perplexity=80.76721

Batch 44580, train_perplexity=97.236305

Batch 44590, train_perplexity=93.30035

Batch 44600, train_perplexity=89.755424

Batch 44610, train_perplexity=89.12151

Batch 44620, train_perplexity=91.36121

Batch 44630, train_perplexity=86.44084

Batch 44640, train_perplexity=80.19364

Batch 44650, train_perplexity=99.64492

Batch 44660, train_perplexity=95.70479

Batch 44670, train_perplexity=84.258575

Batch 44680, train_perplexity=90.49053

Batch 44690, train_perplexity=77.813

Batch 44700, train_perplexity=89.96941

Batch 44710, train_perplexity=86.4293

Batch 44720, train_perplexity=95.14951

Batch 44730, train_perplexity=92.05336

Batch 44740, train_perplexity=90.03731

Batch 44750, train_perplexity=86.05081

Batch 44760, train_perplexity=82.28377

Batch 44770, train_perplexity=80.737175

Batch 44780, train_perplexity=93.32451

Batch 44790, train_perplexity=87.24036

Batch 44800, train_perplexity=91.99601

Batch 44810, train_perplexity=92.199646

Batch 44820, train_perplexity=80.0516

Batch 44830, train_perplexity=89.78081

Batch 44840, train_perplexity=85.617294

Batch 44850, train_perplexity=95.24166

Batch 44860, train_perplexity=85.92084

Batch 44870, train_perplexity=101.89413

Batch 44880, train_perplexity=78.953445

Batch 44890, train_perplexity=87.04049

Batch 44900, train_perplexity=89.54792

Batch 44910, train_perplexity=79.5101

Batch 44920, train_perplexity=88.711685

Batch 44930, train_perplexity=87.04472

Batch 44940, train_perplexity=94.93035

Batch 44950, train_perplexity=94.85235

Batch 44960, train_perplexity=98.73313

Batch 44970, train_perplexity=82.29417

Batch 44980, train_perplexity=95.26828

Batch 44990, train_perplexity=84.94283

Batch 45000, train_perplexity=97.19815

Batch 45010, train_perplexity=80.594315

Batch 45020, train_perplexity=85.87693

Batch 45030, train_perplexity=82.07033

Batch 45040, train_perplexity=87.42534

Batch 45050, train_perplexity=80.64556

Batch 45060, train_perplexity=87.47062

Batch 45070, train_perplexity=91.57231

Batch 45080, train_perplexity=85.15988

Batch 45090, train_perplexity=84.76424

Batch 45100, train_perplexity=94.3897

Batch 45110, train_perplexity=92.23236

Batch 45120, train_perplexity=88.69646

Batch 45130, train_perplexity=83.752335

Batch 45140, train_perplexity=88.10842

Batch 45150, train_perplexity=91.18252

Batch 45160, train_perplexity=94.64028

Batch 45170, train_perplexity=88.61622

Batch 45180, train_perplexity=99.88206

Batch 45190, train_perplexity=92.16461

Batch 45200, train_perplexity=89.37029

Batch 45210, train_perplexity=85.146164

Batch 45220, train_perplexity=84.728676

Batch 45230, train_perplexity=84.18027

Batch 45240, train_perplexity=83.18391

Batch 45250, train_perplexity=92.8088

Batch 45260, train_perplexity=84.45974

Batch 45270, train_perplexity=93.80505

Batch 45280, train_perplexity=83.019775

Batch 45290, train_perplexity=85.305504

Batch 45300, train_perplexity=94.46075

Batch 45310, train_perplexity=87.048874

Batch 45320, train_perplexity=89.286674

Batch 45330, train_perplexity=75.23279

Batch 45340, train_perplexity=86.91893

Batch 45350, train_perplexity=92.89407

Batch 45360, train_perplexity=89.092445

Batch 45370, train_perplexity=89.54347

Batch 45380, train_perplexity=80.737366

Batch 45390, train_perplexity=93.64484

Batch 45400, train_perplexity=84.28518

Batch 45410, train_perplexity=92.8026

Batch 45420, train_perplexity=84.37964

Batch 45430, train_perplexity=90.82919

Batch 45440, train_perplexity=89.730515

Batch 45450, train_perplexity=88.83063

Batch 45460, train_perplexity=83.78753

Batch 45470, train_perplexity=91.36418

Batch 45480, train_perplexity=91.3316

Batch 45490, train_perplexity=82.59972

Batch 45500, train_perplexity=76.19442

Batch 45510, train_perplexity=91.79681

Batch 45520, train_perplexity=86.997505

Batch 45530, train_perplexity=99.85234

Batch 45540, train_perplexity=90.19699

Batch 45550, train_perplexity=90.17488

Batch 45560, train_perplexity=86.997505

Batch 45570, train_perplexity=84.90128

Batch 45580, train_perplexity=86.570526

Batch 45590, train_perplexity=75.9508

Batch 45600, train_perplexity=93.56026

Batch 45610, train_perplexity=84.78105

Batch 45620, train_perplexity=94.20377

Batch 45630, train_perplexity=89.03307

Batch 45640, train_perplexity=89.7235

Batch 45650, train_perplexity=78.491356

Batch 45660, train_perplexity=83.25629

Batch 45670, train_perplexity=85.60684

Batch 45680, train_perplexity=88.689735

Batch 45690, train_perplexity=88.57127

Batch 45700, train_perplexity=78.0856

Batch 45710, train_perplexity=89.72324

Batch 45720, train_perplexity=82.51925

Batch 45730, train_perplexity=83.495384

Batch 45740, train_perplexity=82.23196

Batch 45750, train_perplexity=93.803894

Batch 45760, train_perplexity=93.462204

Batch 45770, train_perplexity=101.86464

Batch 45780, train_perplexity=85.41398

Batch 45790, train_perplexity=108.457504

Batch 45800, train_perplexity=84.02034

Batch 45810, train_perplexity=88.20611

Batch 45820, train_perplexity=78.32531

Batch 45830, train_perplexity=80.698685

Batch 45840, train_perplexity=84.2507

Batch 45850, train_perplexity=87.37016

Batch 45860, train_perplexity=77.383606

Batch 45870, train_perplexity=90.98169

Batch 45880, train_perplexity=87.70818

Batch 45890, train_perplexity=86.27148

Batch 45900, train_perplexity=90.48242

Batch 45910, train_perplexity=94.88876

Batch 45920, train_perplexity=82.403015

Batch 45930, train_perplexity=92.52044

Batch 45940, train_perplexity=92.800125

Batch 45950, train_perplexity=84.170876

Batch 45960, train_perplexity=92.84235

Batch 45970, train_perplexity=83.56381

Batch 45980, train_perplexity=79.76365

Batch 45990, train_perplexity=85.640236

Batch 46000, train_perplexity=103.57056

Batch 46010, train_perplexity=88.515625

Batch 46020, train_perplexity=84.26508

Batch 46030, train_perplexity=83.38732

Batch 46040, train_perplexity=84.42379

Batch 46050, train_perplexity=82.94479

Batch 46060, train_perplexity=89.78401

Batch 46070, train_perplexity=85.92075

Batch 46080, train_perplexity=86.78512

Batch 46090, train_perplexity=96.36976

Batch 46100, train_perplexity=79.95912

Batch 46110, train_perplexity=80.245895

Batch 46120, train_perplexity=85.906746

Batch 46130, train_perplexity=80.14356

Batch 46140, train_perplexity=84.05007

Batch 46150, train_perplexity=91.363304

Batch 46160, train_perplexity=83.63373

Batch 46170, train_perplexity=89.67465

Batch 46180, train_perplexity=87.60244

Batch 46190, train_perplexity=84.13744

Batch 46200, train_perplexity=80.61357

Batch 46210, train_perplexity=88.51714

Batch 46220, train_perplexity=93.70331

Batch 46230, train_perplexity=96.73652

Batch 46240, train_perplexity=89.07681

Batch 46250, train_perplexity=92.06798

Batch 46260, train_perplexity=91.29642

Batch 46270, train_perplexity=77.686104

Batch 46280, train_perplexity=91.25202

Batch 46290, train_perplexity=85.88864

Batch 46300, train_perplexity=90.65348

Batch 46310, train_perplexity=88.881134

Batch 46320, train_perplexity=86.15345

Batch 46330, train_perplexity=83.79364

Batch 46340, train_perplexity=92.30839

Batch 46350, train_perplexity=88.12669

Batch 46360, train_perplexity=85.64228

Batch 46370, train_perplexity=92.72678

Batch 46380, train_perplexity=91.584755

Batch 46390, train_perplexity=83.921036

Batch 46400, train_perplexity=90.078186

Batch 46410, train_perplexity=91.402306

Batch 46420, train_perplexity=76.59797

Batch 46430, train_perplexity=86.553024

Batch 46440, train_perplexity=79.2252

Batch 46450, train_perplexity=86.0015

Batch 46460, train_perplexity=94.36288

Batch 46470, train_perplexity=97.05754

Batch 46480, train_perplexity=88.96182

Batch 46490, train_perplexity=86.92776

Batch 46500, train_perplexity=84.35272

Batch 46510, train_perplexity=90.19828

Batch 46520, train_perplexity=91.35825

Batch 46530, train_perplexity=86.72662

Batch 46540, train_perplexity=87.919395

Batch 46550, train_perplexity=87.698235

Batch 46560, train_perplexity=85.31604
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 46570, train_perplexity=102.6973

Batch 46580, train_perplexity=93.4315

Batch 46590, train_perplexity=92.90683

Batch 46600, train_perplexity=82.55597

Batch 46610, train_perplexity=84.00936

Batch 46620, train_perplexity=87.5507

Batch 46630, train_perplexity=84.19905

Batch 46640, train_perplexity=80.695915

Batch 46650, train_perplexity=83.82169

Batch 46660, train_perplexity=98.15303

Batch 46670, train_perplexity=86.85194

Batch 46680, train_perplexity=94.11622

Batch 46690, train_perplexity=85.65584

Batch 46700, train_perplexity=87.45077

Batch 46710, train_perplexity=85.55192

Batch 46720, train_perplexity=97.46534

Batch 46730, train_perplexity=88.99598

Batch 46740, train_perplexity=92.33383

Batch 46750, train_perplexity=94.76283

Batch 46760, train_perplexity=82.21486

Batch 46770, train_perplexity=86.07995

Batch 46780, train_perplexity=82.02022

Batch 46790, train_perplexity=86.70892

Batch 46800, train_perplexity=92.60682

Batch 46810, train_perplexity=85.070274

Batch 46820, train_perplexity=85.583336

Batch 46830, train_perplexity=80.970505

Batch 46840, train_perplexity=84.42318

Batch 46850, train_perplexity=97.44573

Batch 46860, train_perplexity=85.40616

Batch 46870, train_perplexity=94.35037

Batch 46880, train_perplexity=84.97759

Batch 46890, train_perplexity=85.733154

Batch 46900, train_perplexity=79.36313

Batch 46910, train_perplexity=79.26556

Batch 46920, train_perplexity=85.70217

Batch 46930, train_perplexity=94.53501

Batch 46940, train_perplexity=77.87652

Batch 46950, train_perplexity=97.34036

Batch 46960, train_perplexity=82.957016

Batch 46970, train_perplexity=87.01908

Batch 46980, train_perplexity=94.79212

Batch 46990, train_perplexity=93.33532

Batch 47000, train_perplexity=92.16026

Batch 47010, train_perplexity=74.58947

Batch 47020, train_perplexity=83.668915

Batch 47030, train_perplexity=97.40866

Batch 47040, train_perplexity=94.60788

Batch 47050, train_perplexity=79.82582

Batch 47060, train_perplexity=88.39078

Batch 47070, train_perplexity=89.894196

Batch 47080, train_perplexity=90.84539

Batch 47090, train_perplexity=82.16383

Batch 47100, train_perplexity=87.90883

Batch 47110, train_perplexity=82.31858

Batch 47120, train_perplexity=86.35111

Batch 47130, train_perplexity=94.189445

Batch 47140, train_perplexity=87.843506

Batch 47150, train_perplexity=97.1887

Batch 47160, train_perplexity=90.8499

Batch 47170, train_perplexity=88.19837

Batch 47180, train_perplexity=88.8774

Batch 47190, train_perplexity=101.39873

Batch 47200, train_perplexity=79.71289

Batch 47210, train_perplexity=77.87451

Batch 47220, train_perplexity=84.84244

Batch 47230, train_perplexity=81.54784

Batch 47240, train_perplexity=92.36497

Batch 47250, train_perplexity=88.06675

Batch 47260, train_perplexity=81.136154

Batch 47270, train_perplexity=89.66833

Batch 47280, train_perplexity=89.25386

Batch 47290, train_perplexity=88.637184

Batch 47300, train_perplexity=83.64497

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00019-of-00100
Loaded 305591 sentences.
Finished loading
Batch 47310, train_perplexity=94.858315

Batch 47320, train_perplexity=80.587326

Batch 47330, train_perplexity=81.68576

Batch 47340, train_perplexity=76.72555

Batch 47350, train_perplexity=83.22136

Batch 47360, train_perplexity=90.88517

Batch 47370, train_perplexity=93.098404

Batch 47380, train_perplexity=88.49199

Batch 47390, train_perplexity=76.08136

Batch 47400, train_perplexity=89.719734

Batch 47410, train_perplexity=81.31951

Batch 47420, train_perplexity=91.79221

Batch 47430, train_perplexity=86.82304

Batch 47440, train_perplexity=93.60292

Batch 47450, train_perplexity=91.620094

Batch 47460, train_perplexity=87.77396

Batch 47470, train_perplexity=89.54518

Batch 47480, train_perplexity=88.05222

Batch 47490, train_perplexity=84.72067

Batch 47500, train_perplexity=83.20434

Batch 47510, train_perplexity=83.13624

Batch 47520, train_perplexity=85.46988

Batch 47530, train_perplexity=80.7929

Batch 47540, train_perplexity=90.37901

Batch 47550, train_perplexity=81.75439

Batch 47560, train_perplexity=89.877655

Batch 47570, train_perplexity=84.33631

Batch 47580, train_perplexity=80.08924

Batch 47590, train_perplexity=82.49769

Batch 47600, train_perplexity=84.91294

Batch 47610, train_perplexity=86.420525

Batch 47620, train_perplexity=85.890976

Batch 47630, train_perplexity=81.71591

Batch 47640, train_perplexity=85.55248

Batch 47650, train_perplexity=86.0198

Batch 47660, train_perplexity=86.8184

Batch 47670, train_perplexity=90.92314

Batch 47680, train_perplexity=83.48686

Batch 47690, train_perplexity=87.938934

Batch 47700, train_perplexity=87.08815

Batch 47710, train_perplexity=88.57562

Batch 47720, train_perplexity=96.86502

Batch 47730, train_perplexity=89.65708

Batch 47740, train_perplexity=80.53124

Batch 47750, train_perplexity=87.550156

Batch 47760, train_perplexity=85.07518

Batch 47770, train_perplexity=96.523186

Batch 47780, train_perplexity=105.44191

Batch 47790, train_perplexity=83.47667

Batch 47800, train_perplexity=86.19676

Batch 47810, train_perplexity=87.95948

Batch 47820, train_perplexity=83.54628

Batch 47830, train_perplexity=92.515854

Batch 47840, train_perplexity=92.19112

Batch 47850, train_perplexity=84.839165

Batch 47860, train_perplexity=78.13071

Batch 47870, train_perplexity=83.09633

Batch 47880, train_perplexity=88.37767

Batch 47890, train_perplexity=84.9966

Batch 47900, train_perplexity=88.696075

Batch 47910, train_perplexity=89.51914

Batch 47920, train_perplexity=91.69728

Batch 47930, train_perplexity=85.235695

Batch 47940, train_perplexity=89.630875

Batch 47950, train_perplexity=91.33203

Batch 47960, train_perplexity=79.68918

Batch 47970, train_perplexity=79.35795

Batch 47980, train_perplexity=84.0334

Batch 47990, train_perplexity=95.07486

Batch 48000, train_perplexity=84.677055

Batch 48010, train_perplexity=79.460945

Batch 48020, train_perplexity=82.93119

Batch 48030, train_perplexity=83.50314

Batch 48040, train_perplexity=86.383194

Batch 48050, train_perplexity=88.43049

Batch 48060, train_perplexity=88.8708

Batch 48070, train_perplexity=87.13642

Batch 48080, train_perplexity=92.607834

Batch 48090, train_perplexity=79.92596

Batch 48100, train_perplexity=86.80594

Batch 48110, train_perplexity=82.055695

Batch 48120, train_perplexity=83.88303

Batch 48130, train_perplexity=89.00863

Batch 48140, train_perplexity=86.83885

Batch 48150, train_perplexity=86.04564

Batch 48160, train_perplexity=81.69667

Batch 48170, train_perplexity=86.68214

Batch 48180, train_perplexity=89.79391

Batch 48190, train_perplexity=84.07051

Batch 48200, train_perplexity=83.725945

Batch 48210, train_perplexity=87.19249

Batch 48220, train_perplexity=84.648315

Batch 48230, train_perplexity=86.69656

Batch 48240, train_perplexity=75.599014

Batch 48250, train_perplexity=93.15418

Batch 48260, train_perplexity=84.99323

Batch 48270, train_perplexity=83.4329

Batch 48280, train_perplexity=94.41955

Batch 48290, train_perplexity=82.16086

Batch 48300, train_perplexity=89.33731

Batch 48310, train_perplexity=102.86458

Batch 48320, train_perplexity=88.20039

Batch 48330, train_perplexity=88.88576

Batch 48340, train_perplexity=82.4038

Batch 48350, train_perplexity=85.358765

Batch 48360, train_perplexity=90.10362

Batch 48370, train_perplexity=83.95858

Batch 48380, train_perplexity=81.83469

Batch 48390, train_perplexity=79.390724

Batch 48400, train_perplexity=86.18082

Batch 48410, train_perplexity=80.30354

Batch 48420, train_perplexity=78.24569

Batch 48430, train_perplexity=82.95812

Batch 48440, train_perplexity=77.73316

Batch 48450, train_perplexity=83.456215

Batch 48460, train_perplexity=97.920044

Batch 48470, train_perplexity=73.65217

Batch 48480, train_perplexity=93.00199

Batch 48490, train_perplexity=85.39093

Batch 48500, train_perplexity=78.15456
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 48510, train_perplexity=87.09542

Batch 48520, train_perplexity=89.3365

Batch 48530, train_perplexity=85.702415

Batch 48540, train_perplexity=93.135086

Batch 48550, train_perplexity=82.337975

Batch 48560, train_perplexity=79.134056

Batch 48570, train_perplexity=90.8033

Batch 48580, train_perplexity=81.04389

Batch 48590, train_perplexity=83.69701

Batch 48600, train_perplexity=76.87086

Batch 48610, train_perplexity=84.65336

Batch 48620, train_perplexity=88.98096

Batch 48630, train_perplexity=81.58864

Batch 48640, train_perplexity=90.55225

Batch 48650, train_perplexity=86.49271

Batch 48660, train_perplexity=84.68368

Batch 48670, train_perplexity=81.37638

Batch 48680, train_perplexity=96.979546

Batch 48690, train_perplexity=85.46951

Batch 48700, train_perplexity=85.67524

Batch 48710, train_perplexity=92.88158

Batch 48720, train_perplexity=82.78401

Batch 48730, train_perplexity=88.80082

Batch 48740, train_perplexity=82.95484

Batch 48750, train_perplexity=86.36174

Batch 48760, train_perplexity=82.22337

Batch 48770, train_perplexity=80.01698

Batch 48780, train_perplexity=87.35025

Batch 48790, train_perplexity=96.24045

Batch 48800, train_perplexity=83.58788

Batch 48810, train_perplexity=84.31195

Batch 48820, train_perplexity=87.143814

Batch 48830, train_perplexity=89.4943

Batch 48840, train_perplexity=79.53243

Batch 48850, train_perplexity=83.725266

Batch 48860, train_perplexity=91.030594

Batch 48870, train_perplexity=84.8855

Batch 48880, train_perplexity=89.28144

Batch 48890, train_perplexity=81.90695

Batch 48900, train_perplexity=84.67415

Batch 48910, train_perplexity=78.73768

Batch 48920, train_perplexity=88.50584

Batch 48930, train_perplexity=91.08057

Batch 48940, train_perplexity=89.861496

Batch 48950, train_perplexity=86.58713

Batch 48960, train_perplexity=84.76529

Batch 48970, train_perplexity=85.56293

Batch 48980, train_perplexity=84.23897

Batch 48990, train_perplexity=84.61111

Batch 49000, train_perplexity=82.34245

Batch 49010, train_perplexity=81.983185

Batch 49020, train_perplexity=81.01785

Batch 49030, train_perplexity=94.09823

Batch 49040, train_perplexity=84.49716

Batch 49050, train_perplexity=89.952515

Batch 49060, train_perplexity=85.909615

Batch 49070, train_perplexity=89.2273

Batch 49080, train_perplexity=79.46799

Batch 49090, train_perplexity=83.56907

Batch 49100, train_perplexity=86.827095

Batch 49110, train_perplexity=84.21873

Batch 49120, train_perplexity=80.03457

Batch 49130, train_perplexity=93.022125

Batch 49140, train_perplexity=79.03209

Batch 49150, train_perplexity=86.67974

Batch 49160, train_perplexity=92.40889

Batch 49170, train_perplexity=81.387634

Batch 49180, train_perplexity=85.60966

Batch 49190, train_perplexity=84.43603

Batch 49200, train_perplexity=86.18665

Batch 49210, train_perplexity=86.88607

Batch 49220, train_perplexity=89.74109

Batch 49230, train_perplexity=87.496155

Batch 49240, train_perplexity=79.09392

Batch 49250, train_perplexity=90.6901

Batch 49260, train_perplexity=90.512886

Batch 49270, train_perplexity=79.38399

Batch 49280, train_perplexity=87.39062

Batch 49290, train_perplexity=91.069885

Batch 49300, train_perplexity=88.72065

Batch 49310, train_perplexity=84.522514

Batch 49320, train_perplexity=84.73017

Batch 49330, train_perplexity=83.611244

Batch 49340, train_perplexity=81.578636

Batch 49350, train_perplexity=87.12878

Batch 49360, train_perplexity=91.054214

Batch 49370, train_perplexity=80.70938

Batch 49380, train_perplexity=79.663795

Batch 49390, train_perplexity=88.61512

Batch 49400, train_perplexity=79.209335

Batch 49410, train_perplexity=85.043015

Batch 49420, train_perplexity=79.820076

Batch 49430, train_perplexity=86.5496

Batch 49440, train_perplexity=86.328636

Batch 49450, train_perplexity=79.48588

Batch 49460, train_perplexity=82.919044

Batch 49470, train_perplexity=82.43493

Batch 49480, train_perplexity=84.65336

Batch 49490, train_perplexity=85.59607

Batch 49500, train_perplexity=86.498116

Batch 49510, train_perplexity=91.907135

Batch 49520, train_perplexity=82.80952

Batch 49530, train_perplexity=88.44922

Batch 49540, train_perplexity=90.88634

Batch 49550, train_perplexity=81.61482

Batch 49560, train_perplexity=85.53421

Batch 49570, train_perplexity=90.34631

Batch 49580, train_perplexity=80.4009

Batch 49590, train_perplexity=81.597084

Batch 49600, train_perplexity=80.68079

Batch 49610, train_perplexity=74.64803

Batch 49620, train_perplexity=84.27079

Batch 49630, train_perplexity=80.7381

Batch 49640, train_perplexity=81.86755

Batch 49650, train_perplexity=91.95781

Batch 49660, train_perplexity=89.50531

Batch 49670, train_perplexity=83.74978

Batch 49680, train_perplexity=83.83513

Batch 49690, train_perplexity=91.20908

Batch 49700, train_perplexity=87.416084

Batch 49710, train_perplexity=84.43856

Batch 49720, train_perplexity=81.06194

Batch 49730, train_perplexity=80.73883

Batch 49740, train_perplexity=84.2515

Batch 49750, train_perplexity=78.54426

Batch 49760, train_perplexity=81.93253

Batch 49770, train_perplexity=87.45477

Batch 49780, train_perplexity=82.322075

Batch 49790, train_perplexity=90.42462

Batch 49800, train_perplexity=75.561714

Batch 49810, train_perplexity=91.64386

Batch 49820, train_perplexity=75.47504

Batch 49830, train_perplexity=85.61096

Batch 49840, train_perplexity=87.78132

Batch 49850, train_perplexity=82.2387

Batch 49860, train_perplexity=91.88185

Batch 49870, train_perplexity=93.54652

Batch 49880, train_perplexity=86.007576

Batch 49890, train_perplexity=87.410416

Batch 49900, train_perplexity=88.36364

Batch 49910, train_perplexity=78.16059

Batch 49920, train_perplexity=97.20677

Batch 49930, train_perplexity=83.26768

Batch 49940, train_perplexity=84.189255

Batch 49950, train_perplexity=82.521454

Batch 49960, train_perplexity=79.46606

Batch 49970, train_perplexity=77.53955

Batch 49980, train_perplexity=87.0916

Batch 49990, train_perplexity=78.615036

Batch 50000, train_perplexity=87.2565

Batch 50010, train_perplexity=89.892914

Batch 50020, train_perplexity=83.20961

Batch 50030, train_perplexity=82.625244

Batch 50040, train_perplexity=86.02242

Batch 50050, train_perplexity=83.00545

Batch 50060, train_perplexity=96.13927

Batch 50070, train_perplexity=77.9883

Batch 50080, train_perplexity=86.129135

Batch 50090, train_perplexity=81.789085

Batch 50100, train_perplexity=82.59334

Batch 50110, train_perplexity=95.41485

Batch 50120, train_perplexity=87.388245

Batch 50130, train_perplexity=81.54134

Batch 50140, train_perplexity=87.33833

Batch 50150, train_perplexity=80.988655

Batch 50160, train_perplexity=86.99223

Batch 50170, train_perplexity=84.67827

Batch 50180, train_perplexity=90.137825

Batch 50190, train_perplexity=78.85577

Batch 50200, train_perplexity=80.45985

Batch 50210, train_perplexity=85.22289

Batch 50220, train_perplexity=85.42318

Batch 50230, train_perplexity=84.3835

Batch 50240, train_perplexity=89.00591

Batch 50250, train_perplexity=92.48846

Batch 50260, train_perplexity=86.655525

Batch 50270, train_perplexity=83.69565

Batch 50280, train_perplexity=84.549034

Batch 50290, train_perplexity=83.42852

Batch 50300, train_perplexity=82.90802

Batch 50310, train_perplexity=92.0197

Batch 50320, train_perplexity=77.78878

Batch 50330, train_perplexity=89.31615

Batch 50340, train_perplexity=83.308586

Batch 50350, train_perplexity=88.95809

Batch 50360, train_perplexity=89.56252

Batch 50370, train_perplexity=88.97655

Batch 50380, train_perplexity=81.81787

Batch 50390, train_perplexity=90.60046

Batch 50400, train_perplexity=84.9289

Batch 50410, train_perplexity=85.161674

Batch 50420, train_perplexity=90.94777

Batch 50430, train_perplexity=98.69562

Batch 50440, train_perplexity=89.4026

Batch 50450, train_perplexity=79.64587

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100
Loaded 306964 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00037-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306964 sentences.
Finished loading
Batch 50460, train_perplexity=87.36683

Batch 50470, train_perplexity=82.668945

Batch 50480, train_perplexity=75.11543

Batch 50490, train_perplexity=79.44393

Batch 50500, train_perplexity=80.484634

Batch 50510, train_perplexity=80.12782

Batch 50520, train_perplexity=81.8542

Batch 50530, train_perplexity=78.21846

Batch 50540, train_perplexity=90.154594

Batch 50550, train_perplexity=94.00279

Batch 50560, train_perplexity=74.004776

Batch 50570, train_perplexity=85.12805

Batch 50580, train_perplexity=86.964035

Batch 50590, train_perplexity=84.24387

Batch 50600, train_perplexity=85.32926

Batch 50610, train_perplexity=89.48398

Batch 50620, train_perplexity=86.420815

Batch 50630, train_perplexity=88.65333

Batch 50640, train_perplexity=80.68752

Batch 50650, train_perplexity=87.096664

Batch 50660, train_perplexity=79.65362

Batch 50670, train_perplexity=85.563095

Batch 50680, train_perplexity=81.5513

Batch 50690, train_perplexity=75.57242

Batch 50700, train_perplexity=85.48088

Batch 50710, train_perplexity=89.68025

Batch 50720, train_perplexity=85.2489

Batch 50730, train_perplexity=82.20185

Batch 50740, train_perplexity=84.62434

Batch 50750, train_perplexity=72.09602

Batch 50760, train_perplexity=87.18455

Batch 50770, train_perplexity=84.32127

Batch 50780, train_perplexity=86.307686

Batch 50790, train_perplexity=86.19265

Batch 50800, train_perplexity=91.00625

Batch 50810, train_perplexity=80.49753

Batch 50820, train_perplexity=90.97132

Batch 50830, train_perplexity=88.05667

Batch 50840, train_perplexity=89.38035

Batch 50850, train_perplexity=88.242966

Batch 50860, train_perplexity=89.99177

Batch 50870, train_perplexity=84.80507

Batch 50880, train_perplexity=78.87637

Batch 50890, train_perplexity=89.15033

Batch 50900, train_perplexity=87.011444

Batch 50910, train_perplexity=97.62744

Batch 50920, train_perplexity=88.31014

Batch 50930, train_perplexity=78.293724

Batch 50940, train_perplexity=89.29072

Batch 50950, train_perplexity=91.68609

Batch 50960, train_perplexity=92.0301

Batch 50970, train_perplexity=80.3797

Batch 50980, train_perplexity=79.94036

Batch 50990, train_perplexity=98.19994

Batch 51000, train_perplexity=83.38191

Batch 51010, train_perplexity=83.21596

Batch 51020, train_perplexity=89.33987

Batch 51030, train_perplexity=83.83245

Batch 51040, train_perplexity=79.97483

Batch 51050, train_perplexity=88.59691

Batch 51060, train_perplexity=92.606285

Batch 51070, train_perplexity=82.315125

Batch 51080, train_perplexity=91.26416

Batch 51090, train_perplexity=75.44726

Batch 51100, train_perplexity=83.660614

Batch 51110, train_perplexity=85.60121

Batch 51120, train_perplexity=90.78001

Batch 51130, train_perplexity=83.108574

Batch 51140, train_perplexity=79.23162

Batch 51150, train_perplexity=86.8598

Batch 51160, train_perplexity=83.72471

Batch 51170, train_perplexity=77.90292

Batch 51180, train_perplexity=80.57975

Batch 51190, train_perplexity=84.21905

Batch 51200, train_perplexity=79.52891

Batch 51210, train_perplexity=92.748985

Batch 51220, train_perplexity=90.29407

Batch 51230, train_perplexity=73.260254

Batch 51240, train_perplexity=92.32534

Batch 51250, train_perplexity=84.525536

Batch 51260, train_perplexity=89.546036

Batch 51270, train_perplexity=90.086266

Batch 51280, train_perplexity=82.22505

Batch 51290, train_perplexity=78.84535

Batch 51300, train_perplexity=98.79237

Batch 51310, train_perplexity=86.190025

Batch 51320, train_perplexity=83.16911

Batch 51330, train_perplexity=83.66033

Batch 51340, train_perplexity=81.42296

Batch 51350, train_perplexity=89.11008

Batch 51360, train_perplexity=80.94772

Batch 51370, train_perplexity=89.532974

Batch 51380, train_perplexity=77.61901

Batch 51390, train_perplexity=89.820885

Batch 51400, train_perplexity=90.406

Batch 51410, train_perplexity=88.92959

Batch 51420, train_perplexity=86.72613

Batch 51430, train_perplexity=82.9711

Batch 51440, train_perplexity=84.84066

Batch 51450, train_perplexity=86.34609

Batch 51460, train_perplexity=85.71721

Batch 51470, train_perplexity=80.812744

Batch 51480, train_perplexity=89.39322

Batch 51490, train_perplexity=84.77228

Batch 51500, train_perplexity=86.25646

Batch 51510, train_perplexity=79.73327

Batch 51520, train_perplexity=81.59042

Batch 51530, train_perplexity=81.617744

Batch 51540, train_perplexity=80.65883

Batch 51550, train_perplexity=82.480064

Batch 51560, train_perplexity=89.94831

Batch 51570, train_perplexity=86.983444

Batch 51580, train_perplexity=80.878204

Batch 51590, train_perplexity=74.4474

Batch 51600, train_perplexity=85.83083

Batch 51610, train_perplexity=84.78886

Batch 51620, train_perplexity=81.07358

Batch 51630, train_perplexity=77.98573

Batch 51640, train_perplexity=78.958336

Batch 51650, train_perplexity=83.21977

Batch 51660, train_perplexity=82.235016

Batch 51670, train_perplexity=83.48065

Batch 51680, train_perplexity=94.91374

Batch 51690, train_perplexity=85.10387

Batch 51700, train_perplexity=90.054825

Batch 51710, train_perplexity=88.40174

Batch 51720, train_perplexity=82.98439

Batch 51730, train_perplexity=81.83961

Batch 51740, train_perplexity=73.926155

Batch 51750, train_perplexity=85.99814

Batch 51760, train_perplexity=79.06281

Batch 51770, train_perplexity=87.18821

Batch 51780, train_perplexity=87.43026

Batch 51790, train_perplexity=80.50529

Batch 51800, train_perplexity=81.40751

Batch 51810, train_perplexity=85.63093

Batch 51820, train_perplexity=85.8003

Batch 51830, train_perplexity=85.7226

Batch 51840, train_perplexity=85.24609

Batch 51850, train_perplexity=80.266365

Batch 51860, train_perplexity=69.56605

Batch 51870, train_perplexity=85.58023

Batch 51880, train_perplexity=84.427246

Batch 51890, train_perplexity=84.07861

Batch 51900, train_perplexity=78.411674

Batch 51910, train_perplexity=79.498344

Batch 51920, train_perplexity=96.68215

Batch 51930, train_perplexity=73.221176

Batch 51940, train_perplexity=89.41701

Batch 51950, train_perplexity=92.37928

Batch 51960, train_perplexity=84.15193

Batch 51970, train_perplexity=85.03105

Batch 51980, train_perplexity=81.36156

Batch 51990, train_perplexity=79.85979

Batch 52000, train_perplexity=86.94231

Batch 52010, train_perplexity=86.99597

Batch 52020, train_perplexity=83.95778

Batch 52030, train_perplexity=88.41506

Batch 52040, train_perplexity=80.10207

Batch 52050, train_perplexity=84.99076

Batch 52060, train_perplexity=76.42103

Batch 52070, train_perplexity=88.57689

Batch 52080, train_perplexity=80.78481

Batch 52090, train_perplexity=90.105515

Batch 52100, train_perplexity=80.06435

Batch 52110, train_perplexity=83.28178

Batch 52120, train_perplexity=77.803696

Batch 52130, train_perplexity=81.724174

Batch 52140, train_perplexity=81.961685

Batch 52150, train_perplexity=82.83392

Batch 52160, train_perplexity=88.97307

Batch 52170, train_perplexity=78.906395

Batch 52180, train_perplexity=82.02299

Batch 52190, train_perplexity=81.44699

Batch 52200, train_perplexity=85.098915

Batch 52210, train_perplexity=84.55657

Batch 52220, train_perplexity=79.4287

Batch 52230, train_perplexity=81.59101

Batch 52240, train_perplexity=87.49014

Batch 52250, train_perplexity=87.31527

Batch 52260, train_perplexity=88.46917

Batch 52270, train_perplexity=88.122116

Batch 52280, train_perplexity=91.00655

Batch 52290, train_perplexity=81.58922

Batch 52300, train_perplexity=88.18945

Batch 52310, train_perplexity=81.38181

Batch 52320, train_perplexity=78.12326

Batch 52330, train_perplexity=90.32714

Batch 52340, train_perplexity=92.80419

Batch 52350, train_perplexity=84.8808

Batch 52360, train_perplexity=79.39848

Batch 52370, train_perplexity=89.797676

Batch 52380, train_perplexity=86.56219

Batch 52390, train_perplexity=84.785255

Batch 52400, train_perplexity=74.854774

Batch 52410, train_perplexity=84.5255

Batch 52420, train_perplexity=89.78988

Batch 52430, train_perplexity=81.35768

Batch 52440, train_perplexity=84.76213

Batch 52450, train_perplexity=77.703514

Batch 52460, train_perplexity=95.47156

Batch 52470, train_perplexity=83.094154

Batch 52480, train_perplexity=84.61966
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52490, train_perplexity=81.43802

Batch 52500, train_perplexity=87.89986

Batch 52510, train_perplexity=76.12538

Batch 52520, train_perplexity=81.67396

Batch 52530, train_perplexity=83.07145

Batch 52540, train_perplexity=79.216286

Batch 52550, train_perplexity=84.73223

Batch 52560, train_perplexity=86.03711

Batch 52570, train_perplexity=84.26517

Batch 52580, train_perplexity=76.65355

Batch 52590, train_perplexity=87.754456

Batch 52600, train_perplexity=89.491486

Batch 52610, train_perplexity=84.93327

Batch 52620, train_perplexity=79.47981

Batch 52630, train_perplexity=71.16589

Batch 52640, train_perplexity=88.27036

Batch 52650, train_perplexity=81.149

Batch 52660, train_perplexity=93.86286

Batch 52670, train_perplexity=80.108795

Batch 52680, train_perplexity=91.96105

Batch 52690, train_perplexity=85.71819

Batch 52700, train_perplexity=82.00637

Batch 52710, train_perplexity=84.81461

Batch 52720, train_perplexity=95.97117

Batch 52730, train_perplexity=83.91488

Batch 52740, train_perplexity=80.98429

Batch 52750, train_perplexity=88.93905

Batch 52760, train_perplexity=84.297

Batch 52770, train_perplexity=77.006676

Batch 52780, train_perplexity=86.238945

Batch 52790, train_perplexity=78.68937

Batch 52800, train_perplexity=84.59852

Batch 52810, train_perplexity=77.17699

Batch 52820, train_perplexity=81.56526

Batch 52830, train_perplexity=81.88762

Batch 52840, train_perplexity=81.64978

Batch 52850, train_perplexity=79.085846

Batch 52860, train_perplexity=83.29933

Batch 52870, train_perplexity=81.33557

Batch 52880, train_perplexity=88.99386

Batch 52890, train_perplexity=73.55039

Batch 52900, train_perplexity=85.69203

Batch 52910, train_perplexity=83.086945

Batch 52920, train_perplexity=84.904686

Batch 52930, train_perplexity=86.58205

Batch 52940, train_perplexity=75.96348

Batch 52950, train_perplexity=82.78023

Batch 52960, train_perplexity=83.20747

Batch 52970, train_perplexity=90.9058

Batch 52980, train_perplexity=83.56704

Batch 52990, train_perplexity=90.62867

Batch 53000, train_perplexity=81.16154

Batch 53010, train_perplexity=75.00394

Batch 53020, train_perplexity=94.64714

Batch 53030, train_perplexity=86.71149

Batch 53040, train_perplexity=87.6657

Batch 53050, train_perplexity=77.49501

Batch 53060, train_perplexity=88.44508

Batch 53070, train_perplexity=84.45922

Batch 53080, train_perplexity=82.323524

Batch 53090, train_perplexity=78.07403

Batch 53100, train_perplexity=86.8357

Batch 53110, train_perplexity=79.14119

Batch 53120, train_perplexity=80.26602

Batch 53130, train_perplexity=81.44831

Batch 53140, train_perplexity=83.719315

Batch 53150, train_perplexity=88.690285

Batch 53160, train_perplexity=87.03676

Batch 53170, train_perplexity=81.71198

Batch 53180, train_perplexity=80.95417

Batch 53190, train_perplexity=81.657875

Batch 53200, train_perplexity=90.04023

Batch 53210, train_perplexity=83.80866

Batch 53220, train_perplexity=83.12423

Batch 53230, train_perplexity=84.36382

Batch 53240, train_perplexity=84.504616

Batch 53250, train_perplexity=81.07945

Batch 53260, train_perplexity=86.822205

Batch 53270, train_perplexity=82.366875

Batch 53280, train_perplexity=78.23647

Batch 53290, train_perplexity=77.489136

Batch 53300, train_perplexity=82.882996

Batch 53310, train_perplexity=74.01388

Batch 53320, train_perplexity=83.92564

Batch 53330, train_perplexity=82.31234

Batch 53340, train_perplexity=78.76768

Batch 53350, train_perplexity=88.099594

Batch 53360, train_perplexity=89.11004

Batch 53370, train_perplexity=84.05071

Batch 53380, train_perplexity=80.10398

Batch 53390, train_perplexity=89.071594

Batch 53400, train_perplexity=90.09812

Batch 53410, train_perplexity=81.82848

Batch 53420, train_perplexity=80.30561

Batch 53430, train_perplexity=84.52449

Batch 53440, train_perplexity=85.96485

Batch 53450, train_perplexity=83.26772

Batch 53460, train_perplexity=82.952545

Batch 53470, train_perplexity=88.69988

Batch 53480, train_perplexity=87.32435

Batch 53490, train_perplexity=81.06871

Batch 53500, train_perplexity=81.04471

Batch 53510, train_perplexity=80.65602

Batch 53520, train_perplexity=78.872055

Batch 53530, train_perplexity=78.08225

Batch 53540, train_perplexity=82.28177

Batch 53550, train_perplexity=81.778404

Batch 53560, train_perplexity=89.8181

Batch 53570, train_perplexity=86.274315

Batch 53580, train_perplexity=76.19591

Batch 53590, train_perplexity=75.321915

Batch 53600, train_perplexity=92.22427

Batch 53610, train_perplexity=85.85683

Batch 53620, train_perplexity=85.15404

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100
Loaded 307000 sentences.
Finished loading
Batch 53630, train_perplexity=83.16899

Batch 53640, train_perplexity=81.91902

Batch 53650, train_perplexity=81.87208

Batch 53660, train_perplexity=83.35003

Batch 53670, train_perplexity=79.689026

Batch 53680, train_perplexity=86.48537

Batch 53690, train_perplexity=87.6031

Batch 53700, train_perplexity=86.54799

Batch 53710, train_perplexity=85.96862

Batch 53720, train_perplexity=92.656815

Batch 53730, train_perplexity=86.37335

Batch 53740, train_perplexity=92.18874

Batch 53750, train_perplexity=85.30351

Batch 53760, train_perplexity=74.53827

Batch 53770, train_perplexity=90.47349

Batch 53780, train_perplexity=86.441

Batch 53790, train_perplexity=83.162766

Batch 53800, train_perplexity=79.57215

Batch 53810, train_perplexity=84.20845

Batch 53820, train_perplexity=84.55738

Batch 53830, train_perplexity=91.45489

Batch 53840, train_perplexity=81.49785

Batch 53850, train_perplexity=79.577194

Batch 53860, train_perplexity=78.9615

Batch 53870, train_perplexity=84.27602

Batch 53880, train_perplexity=83.56652

Batch 53890, train_perplexity=82.396416

Batch 53900, train_perplexity=75.87265

Batch 53910, train_perplexity=87.86361

Batch 53920, train_perplexity=83.153725

Batch 53930, train_perplexity=81.50107

Batch 53940, train_perplexity=84.98019

Batch 53950, train_perplexity=78.57505

Batch 53960, train_perplexity=78.451164

Batch 53970, train_perplexity=75.040535

Batch 53980, train_perplexity=84.54823

Batch 53990, train_perplexity=80.21498

Batch 54000, train_perplexity=86.66569

Batch 54010, train_perplexity=78.663635

Batch 54020, train_perplexity=91.6204

Batch 54030, train_perplexity=88.982185

Batch 54040, train_perplexity=79.01928

Batch 54050, train_perplexity=86.255394

Batch 54060, train_perplexity=87.35691

Batch 54070, train_perplexity=91.152695

Batch 54080, train_perplexity=95.84293

Batch 54090, train_perplexity=80.67837

Batch 54100, train_perplexity=76.98971

Batch 54110, train_perplexity=79.58771

Batch 54120, train_perplexity=78.38304

Batch 54130, train_perplexity=92.82973

Batch 54140, train_perplexity=83.735886

Batch 54150, train_perplexity=81.38131

Batch 54160, train_perplexity=83.254616

Batch 54170, train_perplexity=81.351006

Batch 54180, train_perplexity=82.06454

Batch 54190, train_perplexity=81.05874

Batch 54200, train_perplexity=86.12396

Batch 54210, train_perplexity=87.568565

Batch 54220, train_perplexity=83.30084

Batch 54230, train_perplexity=84.93368

Batch 54240, train_perplexity=79.38773

Batch 54250, train_perplexity=78.733284

Batch 54260, train_perplexity=84.51505

Batch 54270, train_perplexity=77.64462

Batch 54280, train_perplexity=76.2288

Batch 54290, train_perplexity=84.991325

Batch 54300, train_perplexity=89.19472

Batch 54310, train_perplexity=87.91923

Batch 54320, train_perplexity=83.83337

Batch 54330, train_perplexity=80.032585

Batch 54340, train_perplexity=80.30002

Batch 54350, train_perplexity=85.98551

Batch 54360, train_perplexity=94.03498

Batch 54370, train_perplexity=88.42484

Batch 54380, train_perplexity=85.848755

Batch 54390, train_perplexity=80.53355

Batch 54400, train_perplexity=71.84841

Batch 54410, train_perplexity=87.95822

Batch 54420, train_perplexity=89.51666
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 54430, train_perplexity=84.576454

Batch 54440, train_perplexity=88.83131

Batch 54450, train_perplexity=84.58755

Batch 54460, train_perplexity=81.23332

Batch 54470, train_perplexity=81.59179

Batch 54480, train_perplexity=80.5433

Batch 54490, train_perplexity=82.03378

Batch 54500, train_perplexity=85.90371

Batch 54510, train_perplexity=79.504715

Batch 54520, train_perplexity=76.71216

Batch 54530, train_perplexity=80.021065

Batch 54540, train_perplexity=82.185974

Batch 54550, train_perplexity=83.76432

Batch 54560, train_perplexity=95.02492

Batch 54570, train_perplexity=76.87023

Batch 54580, train_perplexity=78.32065

Batch 54590, train_perplexity=75.68486

Batch 54600, train_perplexity=82.09612

Batch 54610, train_perplexity=87.075195

Batch 54620, train_perplexity=78.99362

Batch 54630, train_perplexity=80.66514

Batch 54640, train_perplexity=79.998474

Batch 54650, train_perplexity=85.843765

Batch 54660, train_perplexity=77.07956

Batch 54670, train_perplexity=103.006325

Batch 54680, train_perplexity=85.08609

Batch 54690, train_perplexity=82.71922

Batch 54700, train_perplexity=85.6124

Batch 54710, train_perplexity=75.81735

Batch 54720, train_perplexity=87.00522

Batch 54730, train_perplexity=88.574104

Batch 54740, train_perplexity=82.45686

Batch 54750, train_perplexity=84.8318

Batch 54760, train_perplexity=81.32254

Batch 54770, train_perplexity=76.75603

Batch 54780, train_perplexity=78.50947

Batch 54790, train_perplexity=92.698135

Batch 54800, train_perplexity=83.935646

Batch 54810, train_perplexity=85.0517

Batch 54820, train_perplexity=91.488335

Batch 54830, train_perplexity=84.88185

Batch 54840, train_perplexity=85.30604

Batch 54850, train_perplexity=77.82062

Batch 54860, train_perplexity=93.82626

Batch 54870, train_perplexity=80.2413

Batch 54880, train_perplexity=85.8574

Batch 54890, train_perplexity=85.01447

Batch 54900, train_perplexity=93.64515

Batch 54910, train_perplexity=81.149345

Batch 54920, train_perplexity=90.10233

Batch 54930, train_perplexity=86.84498

Batch 54940, train_perplexity=80.623375

Batch 54950, train_perplexity=82.37795

Batch 54960, train_perplexity=83.14822

Batch 54970, train_perplexity=89.950836

Batch 54980, train_perplexity=85.9487

Batch 54990, train_perplexity=91.56795

Batch 55000, train_perplexity=86.37895

Batch 55010, train_perplexity=76.70053

Batch 55020, train_perplexity=82.92395

Batch 55030, train_perplexity=80.10078

Batch 55040, train_perplexity=78.70108

Batch 55050, train_perplexity=82.19267

Batch 55060, train_perplexity=84.75171

Batch 55070, train_perplexity=85.50689

Batch 55080, train_perplexity=77.22068

Batch 55090, train_perplexity=86.5934

Batch 55100, train_perplexity=78.518715

Batch 55110, train_perplexity=87.65701

Batch 55120, train_perplexity=90.09864

Batch 55130, train_perplexity=87.37416

Batch 55140, train_perplexity=91.20269

Batch 55150, train_perplexity=83.96531

Batch 55160, train_perplexity=81.51938

Batch 55170, train_perplexity=78.81615

Batch 55180, train_perplexity=91.85412

Batch 55190, train_perplexity=91.91204

Batch 55200, train_perplexity=95.5272

Batch 55210, train_perplexity=75.5811

Batch 55220, train_perplexity=81.557945

Batch 55230, train_perplexity=89.19412

Batch 55240, train_perplexity=90.2883

Batch 55250, train_perplexity=94.22116

Batch 55260, train_perplexity=80.59754

Batch 55270, train_perplexity=81.54372

Batch 55280, train_perplexity=93.00678

Batch 55290, train_perplexity=79.27954

Batch 55300, train_perplexity=78.900375

Batch 55310, train_perplexity=89.99031

Batch 55320, train_perplexity=79.43772

Batch 55330, train_perplexity=87.584564

Batch 55340, train_perplexity=80.96305

Batch 55350, train_perplexity=79.15425

Batch 55360, train_perplexity=82.263565

Batch 55370, train_perplexity=84.09273

Batch 55380, train_perplexity=83.937126

Batch 55390, train_perplexity=75.42942

Batch 55400, train_perplexity=81.46335

Batch 55410, train_perplexity=84.031395

Batch 55420, train_perplexity=77.45101

Batch 55430, train_perplexity=79.10723

Batch 55440, train_perplexity=83.74962

Batch 55450, train_perplexity=75.0996

Batch 55460, train_perplexity=75.71193

Batch 55470, train_perplexity=80.955215

Batch 55480, train_perplexity=88.59839

Batch 55490, train_perplexity=85.644325

Batch 55500, train_perplexity=75.928566

Batch 55510, train_perplexity=76.03828

Batch 55520, train_perplexity=80.558586

Batch 55530, train_perplexity=83.612915

Batch 55540, train_perplexity=81.213295

Batch 55550, train_perplexity=83.036995

Batch 55560, train_perplexity=83.258354

Batch 55570, train_perplexity=77.61309

Batch 55580, train_perplexity=92.06899

Batch 55590, train_perplexity=82.466225

Batch 55600, train_perplexity=82.23431

Batch 55610, train_perplexity=79.11934

Batch 55620, train_perplexity=79.85822

Batch 55630, train_perplexity=82.2558

Batch 55640, train_perplexity=79.54521

Batch 55650, train_perplexity=91.50605

Batch 55660, train_perplexity=77.605835

Batch 55670, train_perplexity=78.54287

Batch 55680, train_perplexity=84.42113

Batch 55690, train_perplexity=84.38668

Batch 55700, train_perplexity=81.08711

Batch 55710, train_perplexity=79.65913

Batch 55720, train_perplexity=86.68164

Batch 55730, train_perplexity=93.74147

Batch 55740, train_perplexity=86.851395

Batch 55750, train_perplexity=96.1532

Batch 55760, train_perplexity=85.17442

Batch 55770, train_perplexity=82.67797

Batch 55780, train_perplexity=85.15867

Batch 55790, train_perplexity=84.48258

Batch 55800, train_perplexity=82.25501

Batch 55810, train_perplexity=72.53478

Batch 55820, train_perplexity=87.265236

Batch 55830, train_perplexity=79.51518

Batch 55840, train_perplexity=83.15726

Batch 55850, train_perplexity=86.111725

Batch 55860, train_perplexity=75.58622

Batch 55870, train_perplexity=84.64178

Batch 55880, train_perplexity=89.755

Batch 55890, train_perplexity=87.941444

Batch 55900, train_perplexity=83.53855

Batch 55910, train_perplexity=80.476425

Batch 55920, train_perplexity=93.52805

Batch 55930, train_perplexity=77.179085

Batch 55940, train_perplexity=73.79655

Batch 55950, train_perplexity=85.172066

Batch 55960, train_perplexity=81.36396

Batch 55970, train_perplexity=77.83327

Batch 55980, train_perplexity=79.61405

Batch 55990, train_perplexity=75.033165

Batch 56000, train_perplexity=90.78057

Batch 56010, train_perplexity=82.35246

Batch 56020, train_perplexity=76.391884

Batch 56030, train_perplexity=79.13062

Batch 56040, train_perplexity=78.510185

Batch 56050, train_perplexity=86.97096

Batch 56060, train_perplexity=79.38637

Batch 56070, train_perplexity=84.136284

Batch 56080, train_perplexity=90.20202

Batch 56090, train_perplexity=84.41219

Batch 56100, train_perplexity=84.48979

Batch 56110, train_perplexity=83.955215

Batch 56120, train_perplexity=72.18626

Batch 56130, train_perplexity=81.28102

Batch 56140, train_perplexity=80.778915

Batch 56150, train_perplexity=83.24112

Batch 56160, train_perplexity=88.26262

Batch 56170, train_perplexity=85.40241

Batch 56180, train_perplexity=90.944214

Batch 56190, train_perplexity=90.97058

Batch 56200, train_perplexity=79.587555

Batch 56210, train_perplexity=79.14311

Batch 56220, train_perplexity=83.29481

Batch 56230, train_perplexity=78.61684

Batch 56240, train_perplexity=80.42598

Batch 56250, train_perplexity=74.160065

Batch 56260, train_perplexity=82.94926

Batch 56270, train_perplexity=85.74583

Batch 56280, train_perplexity=75.93947

Batch 56290, train_perplexity=83.25915

Batch 56300, train_perplexity=82.76002

Batch 56310, train_perplexity=84.52324

Batch 56320, train_perplexity=84.246124

Batch 56330, train_perplexity=85.12679

Batch 56340, train_perplexity=84.27987

Batch 56350, train_perplexity=73.81411

Batch 56360, train_perplexity=82.64242

Batch 56370, train_perplexity=75.94819

Batch 56380, train_perplexity=77.76

Batch 56390, train_perplexity=81.43833

Batch 56400, train_perplexity=79.5346

Batch 56410, train_perplexity=89.12049

Batch 56420, train_perplexity=82.777855

Batch 56430, train_perplexity=88.2704

Batch 56440, train_perplexity=83.97764

Batch 56450, train_perplexity=80.12155

Batch 56460, train_perplexity=90.85857

Batch 56470, train_perplexity=78.307274
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 56480, train_perplexity=83.116264

Batch 56490, train_perplexity=85.81266

Batch 56500, train_perplexity=76.56927

Batch 56510, train_perplexity=90.94144

Batch 56520, train_perplexity=71.2287

Batch 56530, train_perplexity=86.335884

Batch 56540, train_perplexity=79.858376

Batch 56550, train_perplexity=77.02401

Batch 56560, train_perplexity=83.79987

Batch 56570, train_perplexity=74.33393

Batch 56580, train_perplexity=86.16077

Batch 56590, train_perplexity=87.14598

Batch 56600, train_perplexity=81.75871

Batch 56610, train_perplexity=83.306366

Batch 56620, train_perplexity=84.67395

Batch 56630, train_perplexity=78.0396

Batch 56640, train_perplexity=74.88126

Batch 56650, train_perplexity=80.06801

Batch 56660, train_perplexity=71.26247

Batch 56670, train_perplexity=82.2089

Batch 56680, train_perplexity=85.96657

Batch 56690, train_perplexity=75.9277

Batch 56700, train_perplexity=81.729324

Batch 56710, train_perplexity=75.271576

Batch 56720, train_perplexity=83.75394

Batch 56730, train_perplexity=77.65455

Batch 56740, train_perplexity=89.380264

Batch 56750, train_perplexity=92.50333

Batch 56760, train_perplexity=86.488174

Batch 56770, train_perplexity=83.443405

Batch 56780, train_perplexity=78.304436

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00034-of-00100
Loaded 305408 sentences.
Finished loading
Batch 56790, train_perplexity=83.49316

Batch 56800, train_perplexity=73.842064

Batch 56810, train_perplexity=86.61186

Batch 56820, train_perplexity=88.02334

Batch 56830, train_perplexity=85.91551

Batch 56840, train_perplexity=81.423225

Batch 56850, train_perplexity=82.07573

Batch 56860, train_perplexity=80.86779

Batch 56870, train_perplexity=79.091354

Batch 56880, train_perplexity=81.9833

Batch 56890, train_perplexity=81.78827

Batch 56900, train_perplexity=82.51724

Batch 56910, train_perplexity=88.59484

Batch 56920, train_perplexity=83.586525

Batch 56930, train_perplexity=93.9362

Batch 56940, train_perplexity=77.08162

Batch 56950, train_perplexity=73.32736

Batch 56960, train_perplexity=78.78429

Batch 56970, train_perplexity=83.166214

Batch 56980, train_perplexity=74.466225

Batch 56990, train_perplexity=84.01104

Batch 57000, train_perplexity=73.88817

Batch 57010, train_perplexity=87.167755

Batch 57020, train_perplexity=78.33805

Batch 57030, train_perplexity=85.37733

Batch 57040, train_perplexity=85.73274

Batch 57050, train_perplexity=74.87598

Batch 57060, train_perplexity=82.75895

Batch 57070, train_perplexity=91.68464

Batch 57080, train_perplexity=76.63246

Batch 57090, train_perplexity=73.38497

Batch 57100, train_perplexity=89.49942

Batch 57110, train_perplexity=87.57316

Batch 57120, train_perplexity=86.31011

Batch 57130, train_perplexity=83.10168

Batch 57140, train_perplexity=77.433655

Batch 57150, train_perplexity=85.42885

Batch 57160, train_perplexity=76.60469

Batch 57170, train_perplexity=87.85008

Batch 57180, train_perplexity=85.6837

Batch 57190, train_perplexity=83.719795

Batch 57200, train_perplexity=86.48034

Batch 57210, train_perplexity=92.904305

Batch 57220, train_perplexity=86.68214

Batch 57230, train_perplexity=82.72711

Batch 57240, train_perplexity=85.4671

Batch 57250, train_perplexity=84.66559

Batch 57260, train_perplexity=78.07127

Batch 57270, train_perplexity=85.42363

Batch 57280, train_perplexity=84.60642

Batch 57290, train_perplexity=75.61088

Batch 57300, train_perplexity=78.54759

Batch 57310, train_perplexity=75.7084

Batch 57320, train_perplexity=84.76088

Batch 57330, train_perplexity=82.84633

Batch 57340, train_perplexity=78.39208

Batch 57350, train_perplexity=79.195625

Batch 57360, train_perplexity=74.53863

Batch 57370, train_perplexity=84.77094

Batch 57380, train_perplexity=84.87421

Batch 57390, train_perplexity=78.60499

Batch 57400, train_perplexity=83.69198

Batch 57410, train_perplexity=79.33653

Batch 57420, train_perplexity=81.807144

Batch 57430, train_perplexity=74.81316

Batch 57440, train_perplexity=71.85801

Batch 57450, train_perplexity=80.51672

Batch 57460, train_perplexity=90.17884

Batch 57470, train_perplexity=75.70384

Batch 57480, train_perplexity=81.1182

Batch 57490, train_perplexity=84.12705

Batch 57500, train_perplexity=86.27753

Batch 57510, train_perplexity=70.07547

Batch 57520, train_perplexity=78.83505

Batch 57530, train_perplexity=81.190025

Batch 57540, train_perplexity=84.08061

Batch 57550, train_perplexity=84.732956

Batch 57560, train_perplexity=77.4423

Batch 57570, train_perplexity=82.90434

Batch 57580, train_perplexity=78.23357

Batch 57590, train_perplexity=84.894806

Batch 57600, train_perplexity=84.926834

Batch 57610, train_perplexity=75.99021

Batch 57620, train_perplexity=78.142555

Batch 57630, train_perplexity=81.76577

Batch 57640, train_perplexity=76.28003

Batch 57650, train_perplexity=83.607414

Batch 57660, train_perplexity=78.28682

Batch 57670, train_perplexity=81.32044

Batch 57680, train_perplexity=78.53029

Batch 57690, train_perplexity=84.73021

Batch 57700, train_perplexity=91.9536

Batch 57710, train_perplexity=83.14369

Batch 57720, train_perplexity=83.847755

Batch 57730, train_perplexity=75.93581

Batch 57740, train_perplexity=78.51086

Batch 57750, train_perplexity=84.180504

Batch 57760, train_perplexity=80.20795

Batch 57770, train_perplexity=79.09294

Batch 57780, train_perplexity=91.51242

Batch 57790, train_perplexity=80.21498

Batch 57800, train_perplexity=86.12856

Batch 57810, train_perplexity=95.78263

Batch 57820, train_perplexity=87.05054

Batch 57830, train_perplexity=85.21968

Batch 57840, train_perplexity=87.30019

Batch 57850, train_perplexity=77.64536

Batch 57860, train_perplexity=82.15961

Batch 57870, train_perplexity=80.93233

Batch 57880, train_perplexity=84.3866

Batch 57890, train_perplexity=86.1123

Batch 57900, train_perplexity=81.8754

Batch 57910, train_perplexity=81.79084

Batch 57920, train_perplexity=85.12051

Batch 57930, train_perplexity=86.8006

Batch 57940, train_perplexity=83.67194

Batch 57950, train_perplexity=80.832245

Batch 57960, train_perplexity=84.07709

Batch 57970, train_perplexity=83.1682

Batch 57980, train_perplexity=74.27069

Batch 57990, train_perplexity=87.817665

Batch 58000, train_perplexity=82.49674

Batch 58010, train_perplexity=76.496574

Batch 58020, train_perplexity=70.34893

Batch 58030, train_perplexity=81.49031

Batch 58040, train_perplexity=79.36177

Batch 58050, train_perplexity=85.19132

Batch 58060, train_perplexity=82.93913

Batch 58070, train_perplexity=79.813835

Batch 58080, train_perplexity=85.56497

Batch 58090, train_perplexity=82.67636

Batch 58100, train_perplexity=71.65213

Batch 58110, train_perplexity=80.45785

Batch 58120, train_perplexity=78.58131

Batch 58130, train_perplexity=79.241295

Batch 58140, train_perplexity=82.571594

Batch 58150, train_perplexity=76.49081

Batch 58160, train_perplexity=80.28524

Batch 58170, train_perplexity=83.38533

Batch 58180, train_perplexity=79.12568

Batch 58190, train_perplexity=80.01237

Batch 58200, train_perplexity=75.406006

Batch 58210, train_perplexity=73.13464

Batch 58220, train_perplexity=82.14264

Batch 58230, train_perplexity=81.25579

Batch 58240, train_perplexity=88.1895

Batch 58250, train_perplexity=88.95245

Batch 58260, train_perplexity=79.57169

Batch 58270, train_perplexity=87.91528

Batch 58280, train_perplexity=79.126884

Batch 58290, train_perplexity=80.17185

Batch 58300, train_perplexity=72.87606

Batch 58310, train_perplexity=79.576324

Batch 58320, train_perplexity=73.923584

Batch 58330, train_perplexity=81.143585

Batch 58340, train_perplexity=86.00003

Batch 58350, train_perplexity=80.938

Batch 58360, train_perplexity=84.017456

Batch 58370, train_perplexity=73.86573

Batch 58380, train_perplexity=79.59955

Batch 58390, train_perplexity=83.53995

Batch 58400, train_perplexity=82.04005

Batch 58410, train_perplexity=74.70572
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 58420, train_perplexity=79.40231

Batch 58430, train_perplexity=75.72905

Batch 58440, train_perplexity=78.46037

Batch 58450, train_perplexity=82.45671

Batch 58460, train_perplexity=75.2817

Batch 58470, train_perplexity=79.8893

Batch 58480, train_perplexity=82.75615

Batch 58490, train_perplexity=82.016815

Batch 58500, train_perplexity=80.40642

Batch 58510, train_perplexity=79.31418

Batch 58520, train_perplexity=81.62876

Batch 58530, train_perplexity=75.210625

Batch 58540, train_perplexity=81.63553

Batch 58550, train_perplexity=84.20519

Batch 58560, train_perplexity=81.609764

Batch 58570, train_perplexity=77.273796

Batch 58580, train_perplexity=74.8834

Batch 58590, train_perplexity=86.25013

Batch 58600, train_perplexity=74.15157

Batch 58610, train_perplexity=79.45174

Batch 58620, train_perplexity=78.86314

Batch 58630, train_perplexity=81.632576

Batch 58640, train_perplexity=78.13958

Batch 58650, train_perplexity=80.33035

Batch 58660, train_perplexity=77.912994

Batch 58670, train_perplexity=88.4817

Batch 58680, train_perplexity=91.98412

Batch 58690, train_perplexity=79.79062

Batch 58700, train_perplexity=76.878334

Batch 58710, train_perplexity=79.16191

Batch 58720, train_perplexity=80.703415

Batch 58730, train_perplexity=84.40278

Batch 58740, train_perplexity=94.806404

Batch 58750, train_perplexity=84.012245

Batch 58760, train_perplexity=82.042076

Batch 58770, train_perplexity=79.530235

Batch 58780, train_perplexity=78.478294

Batch 58790, train_perplexity=74.9139

Batch 58800, train_perplexity=81.905624

Batch 58810, train_perplexity=90.81135

Batch 58820, train_perplexity=79.19506

Batch 58830, train_perplexity=91.09282

Batch 58840, train_perplexity=81.447495

Batch 58850, train_perplexity=80.06259

Batch 58860, train_perplexity=75.503044

Batch 58870, train_perplexity=67.64352

Batch 58880, train_perplexity=82.014305

Batch 58890, train_perplexity=74.81495

Batch 58900, train_perplexity=84.57174

Batch 58910, train_perplexity=81.52272

Batch 58920, train_perplexity=79.10335

Batch 58930, train_perplexity=73.590385

Batch 58940, train_perplexity=80.307755

Batch 58950, train_perplexity=85.53021

Batch 58960, train_perplexity=89.197525

Batch 58970, train_perplexity=72.81208

Batch 58980, train_perplexity=88.8713

Batch 58990, train_perplexity=73.140915

Batch 59000, train_perplexity=85.72858

Batch 59010, train_perplexity=87.174904

Batch 59020, train_perplexity=77.41446

Batch 59030, train_perplexity=72.98435

Batch 59040, train_perplexity=73.950096

Batch 59050, train_perplexity=73.09503

Batch 59060, train_perplexity=91.12462

Batch 59070, train_perplexity=84.426125

Batch 59080, train_perplexity=76.261154

Batch 59090, train_perplexity=76.53433

Batch 59100, train_perplexity=83.97884

Batch 59110, train_perplexity=83.37348

Batch 59120, train_perplexity=83.79472

Batch 59130, train_perplexity=80.09115

Batch 59140, train_perplexity=85.28505

Batch 59150, train_perplexity=80.49162

Batch 59160, train_perplexity=79.332634

Batch 59170, train_perplexity=77.32241

Batch 59180, train_perplexity=84.21885

Batch 59190, train_perplexity=87.49261

Batch 59200, train_perplexity=79.68538

Batch 59210, train_perplexity=81.09659

Batch 59220, train_perplexity=81.13221

Batch 59230, train_perplexity=78.44331

Batch 59240, train_perplexity=80.11873

Batch 59250, train_perplexity=81.31013

Batch 59260, train_perplexity=74.5788

Batch 59270, train_perplexity=85.92461

Batch 59280, train_perplexity=82.32793

Batch 59290, train_perplexity=89.84483

Batch 59300, train_perplexity=82.81899

Batch 59310, train_perplexity=80.12828

Batch 59320, train_perplexity=80.801956

Batch 59330, train_perplexity=79.17829

Batch 59340, train_perplexity=82.50241

Batch 59350, train_perplexity=83.91608

Batch 59360, train_perplexity=82.44271

Batch 59370, train_perplexity=79.75403

Batch 59380, train_perplexity=73.737564

Batch 59390, train_perplexity=84.74677

Batch 59400, train_perplexity=86.206665

Batch 59410, train_perplexity=84.41638

Batch 59420, train_perplexity=79.75989

Batch 59430, train_perplexity=86.710205

Batch 59440, train_perplexity=72.92239

Batch 59450, train_perplexity=80.22195

Batch 59460, train_perplexity=82.52342

Batch 59470, train_perplexity=79.93381

Batch 59480, train_perplexity=80.70938

Batch 59490, train_perplexity=86.35688

Batch 59500, train_perplexity=81.54162

Batch 59510, train_perplexity=82.54239

Batch 59520, train_perplexity=77.7239

Batch 59530, train_perplexity=82.79633

Batch 59540, train_perplexity=82.38062

Batch 59550, train_perplexity=85.52915

Batch 59560, train_perplexity=81.2375

Batch 59570, train_perplexity=84.401405

Batch 59580, train_perplexity=83.75374

Batch 59590, train_perplexity=93.9457

Batch 59600, train_perplexity=80.08939

Batch 59610, train_perplexity=80.37219

Batch 59620, train_perplexity=88.43724

Batch 59630, train_perplexity=74.86248

Batch 59640, train_perplexity=71.979576

Batch 59650, train_perplexity=84.57601

Batch 59660, train_perplexity=91.29119

Batch 59670, train_perplexity=78.16551

Batch 59680, train_perplexity=81.5319

Batch 59690, train_perplexity=72.71739

Batch 59700, train_perplexity=86.57705

Batch 59710, train_perplexity=81.95117

Batch 59720, train_perplexity=78.15989

Batch 59730, train_perplexity=70.16129

Batch 59740, train_perplexity=77.21862

Batch 59750, train_perplexity=85.77666

Batch 59760, train_perplexity=76.69556

Batch 59770, train_perplexity=82.54109

Batch 59780, train_perplexity=77.20636

Batch 59790, train_perplexity=83.65004

Batch 59800, train_perplexity=84.34339

Batch 59810, train_perplexity=77.53149

Batch 59820, train_perplexity=82.60507

Batch 59830, train_perplexity=82.98843

Batch 59840, train_perplexity=75.21586

Batch 59850, train_perplexity=86.037766

Batch 59860, train_perplexity=79.44734

Batch 59870, train_perplexity=78.32176

Batch 59880, train_perplexity=87.73148

Batch 59890, train_perplexity=77.691475

Batch 59900, train_perplexity=80.12744

Batch 59910, train_perplexity=77.745544

Batch 59920, train_perplexity=80.06709

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00065-of-00100
Loaded 305213 sentences.
Finished loading
Batch 59930, train_perplexity=81.67867

Batch 59940, train_perplexity=76.67296

Batch 59950, train_perplexity=79.95851

Batch 59960, train_perplexity=80.499756

Batch 59970, train_perplexity=83.69457

Batch 59980, train_perplexity=82.12208

Batch 59990, train_perplexity=82.82594

Batch 60000, train_perplexity=71.625694

Batch 60010, train_perplexity=79.588844

Batch 60020, train_perplexity=79.91475

Batch 60030, train_perplexity=74.59395

Batch 60040, train_perplexity=78.24446

Batch 60050, train_perplexity=84.26709

Batch 60060, train_perplexity=83.04749

Batch 60070, train_perplexity=95.49268

Batch 60080, train_perplexity=83.49602

Batch 60090, train_perplexity=75.593895

Batch 60100, train_perplexity=83.90647

Batch 60110, train_perplexity=83.129425

Batch 60120, train_perplexity=80.27276

Batch 60130, train_perplexity=75.01946

Batch 60140, train_perplexity=82.14006

Batch 60150, train_perplexity=76.9504

Batch 60160, train_perplexity=71.573494

Batch 60170, train_perplexity=82.538765

Batch 60180, train_perplexity=83.41305

Batch 60190, train_perplexity=82.64707

Batch 60200, train_perplexity=79.71646

Batch 60210, train_perplexity=84.02274

Batch 60220, train_perplexity=94.99724

Batch 60230, train_perplexity=74.74249

Batch 60240, train_perplexity=76.97745

Batch 60250, train_perplexity=80.16077

Batch 60260, train_perplexity=73.61962

Batch 60270, train_perplexity=80.18794

Batch 60280, train_perplexity=79.30896

Batch 60290, train_perplexity=82.19432

Batch 60300, train_perplexity=72.96083

Batch 60310, train_perplexity=76.84989

Batch 60320, train_perplexity=75.425316

Batch 60330, train_perplexity=82.161644

Batch 60340, train_perplexity=78.15404

Batch 60350, train_perplexity=72.23491
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 60360, train_perplexity=89.117134

Batch 60370, train_perplexity=83.115234

Batch 60380, train_perplexity=72.2139

Batch 60390, train_perplexity=79.00383

Batch 60400, train_perplexity=75.086136

Batch 60410, train_perplexity=74.66409

Batch 60420, train_perplexity=86.07248

Batch 60430, train_perplexity=81.15229

Batch 60440, train_perplexity=80.64971

Batch 60450, train_perplexity=69.173645

Batch 60460, train_perplexity=81.58234

Batch 60470, train_perplexity=90.5773

Batch 60480, train_perplexity=78.61954

Batch 60490, train_perplexity=81.421326

Batch 60500, train_perplexity=83.213264

Batch 60510, train_perplexity=81.31525

Batch 60520, train_perplexity=89.50134

Batch 60530, train_perplexity=85.90822

Batch 60540, train_perplexity=90.52372

Batch 60550, train_perplexity=74.70832

Batch 60560, train_perplexity=82.03297

Batch 60570, train_perplexity=78.15366

Batch 60580, train_perplexity=72.79611

Batch 60590, train_perplexity=76.36515

Batch 60600, train_perplexity=82.45301

Batch 60610, train_perplexity=79.746574

Batch 60620, train_perplexity=78.806564

Batch 60630, train_perplexity=79.99649

Batch 60640, train_perplexity=75.79899

Batch 60650, train_perplexity=85.922806

Batch 60660, train_perplexity=83.62568

Batch 60670, train_perplexity=99.28522

Batch 60680, train_perplexity=80.00389

Batch 60690, train_perplexity=81.43499

Batch 60700, train_perplexity=79.2085

Batch 60710, train_perplexity=80.981895

Batch 60720, train_perplexity=77.65818

Batch 60730, train_perplexity=81.52964

Batch 60740, train_perplexity=81.95708

Batch 60750, train_perplexity=79.55902

Batch 60760, train_perplexity=82.71706

Batch 60770, train_perplexity=72.647835

Batch 60780, train_perplexity=78.95495

Batch 60790, train_perplexity=86.37986

Batch 60800, train_perplexity=87.45261

Batch 60810, train_perplexity=85.26959

Batch 60820, train_perplexity=73.45842

Batch 60830, train_perplexity=92.09248

Batch 60840, train_perplexity=72.5224

Batch 60850, train_perplexity=75.433914

Batch 60860, train_perplexity=72.68913

Batch 60870, train_perplexity=78.655754

Batch 60880, train_perplexity=84.72827

Batch 60890, train_perplexity=86.57713

Batch 60900, train_perplexity=81.159294

Batch 60910, train_perplexity=89.05252

Batch 60920, train_perplexity=79.98139

Batch 60930, train_perplexity=88.95796

Batch 60940, train_perplexity=85.49515

Batch 60950, train_perplexity=78.13574

Batch 60960, train_perplexity=80.22041

Batch 60970, train_perplexity=86.14869

Batch 60980, train_perplexity=82.17355

Batch 60990, train_perplexity=85.17337

Batch 61000, train_perplexity=89.126656

Batch 61010, train_perplexity=84.53904

Batch 61020, train_perplexity=85.28529

Batch 61030, train_perplexity=84.598434

Batch 61040, train_perplexity=79.449615

Batch 61050, train_perplexity=83.402115

Batch 61060, train_perplexity=83.74375

Batch 61070, train_perplexity=82.184364

Batch 61080, train_perplexity=74.130226

Batch 61090, train_perplexity=81.31971

Batch 61100, train_perplexity=75.83297

Batch 61110, train_perplexity=69.68916

Batch 61120, train_perplexity=81.207756

Batch 61130, train_perplexity=80.30783

Batch 61140, train_perplexity=77.37349

Batch 61150, train_perplexity=75.75974

Batch 61160, train_perplexity=80.92681

Batch 61170, train_perplexity=78.9158

Batch 61180, train_perplexity=77.81349

Batch 61190, train_perplexity=86.842995

Batch 61200, train_perplexity=77.38405

Batch 61210, train_perplexity=87.97357

Batch 61220, train_perplexity=77.23534

Batch 61230, train_perplexity=80.69637

Batch 61240, train_perplexity=85.89798

Batch 61250, train_perplexity=89.18809

Batch 61260, train_perplexity=82.26721

Batch 61270, train_perplexity=82.03418

Batch 61280, train_perplexity=74.60832

Batch 61290, train_perplexity=80.37188

Batch 61300, train_perplexity=81.741714

Batch 61310, train_perplexity=79.9335

Batch 61320, train_perplexity=92.68258

Batch 61330, train_perplexity=75.186455

Batch 61340, train_perplexity=75.031845

Batch 61350, train_perplexity=75.55556

Batch 61360, train_perplexity=83.246956

Batch 61370, train_perplexity=77.431404

Batch 61380, train_perplexity=76.487015

Batch 61390, train_perplexity=83.91164

Batch 61400, train_perplexity=71.062805

Batch 61410, train_perplexity=86.27103

Batch 61420, train_perplexity=85.02315

Batch 61430, train_perplexity=73.11658

Batch 61440, train_perplexity=71.87024

Batch 61450, train_perplexity=79.83462

Batch 61460, train_perplexity=79.06994

Batch 61470, train_perplexity=76.384384

Batch 61480, train_perplexity=71.85838

Batch 61490, train_perplexity=79.69966

Batch 61500, train_perplexity=83.78757

Batch 61510, train_perplexity=97.019

Batch 61520, train_perplexity=79.274025

Batch 61530, train_perplexity=74.32376

Batch 61540, train_perplexity=85.83411

Batch 61550, train_perplexity=75.972244

Batch 61560, train_perplexity=84.93951

Batch 61570, train_perplexity=73.00823

Batch 61580, train_perplexity=74.66961

Batch 61590, train_perplexity=87.16634

Batch 61600, train_perplexity=73.88954

Batch 61610, train_perplexity=80.35678

Batch 61620, train_perplexity=70.09625

Batch 61630, train_perplexity=80.360115

Batch 61640, train_perplexity=78.27894

Batch 61650, train_perplexity=78.3122

Batch 61660, train_perplexity=75.23369

Batch 61670, train_perplexity=84.79706

Batch 61680, train_perplexity=71.61094

Batch 61690, train_perplexity=86.42477

Batch 61700, train_perplexity=73.620285

Batch 61710, train_perplexity=76.68041

Batch 61720, train_perplexity=86.4

Batch 61730, train_perplexity=87.462364

Batch 61740, train_perplexity=74.76951

Batch 61750, train_perplexity=76.39531

Batch 61760, train_perplexity=82.73713

Batch 61770, train_perplexity=82.284676

Batch 61780, train_perplexity=82.72826

Batch 61790, train_perplexity=78.58243

Batch 61800, train_perplexity=76.17691

Batch 61810, train_perplexity=80.349464

Batch 61820, train_perplexity=83.06832

Batch 61830, train_perplexity=79.14289

Batch 61840, train_perplexity=77.10364

Batch 61850, train_perplexity=82.19777

Batch 61860, train_perplexity=74.87755

Batch 61870, train_perplexity=71.52821

Batch 61880, train_perplexity=78.37048

Batch 61890, train_perplexity=79.450676

Batch 61900, train_perplexity=82.89019

Batch 61910, train_perplexity=74.76341

Batch 61920, train_perplexity=81.898476

Batch 61930, train_perplexity=77.41571

Batch 61940, train_perplexity=78.953476

Batch 61950, train_perplexity=79.81372

Batch 61960, train_perplexity=79.45317

Batch 61970, train_perplexity=86.99626

Batch 61980, train_perplexity=79.97773

Batch 61990, train_perplexity=83.94373

Batch 62000, train_perplexity=77.49297

Batch 62010, train_perplexity=69.47386

Batch 62020, train_perplexity=81.87528

Batch 62030, train_perplexity=80.65171

Batch 62040, train_perplexity=82.19001

Batch 62050, train_perplexity=76.76441

Batch 62060, train_perplexity=77.82076

Batch 62070, train_perplexity=70.24733

Batch 62080, train_perplexity=71.306694

Batch 62090, train_perplexity=79.508736

Batch 62100, train_perplexity=79.315956

Batch 62110, train_perplexity=73.92563

Batch 62120, train_perplexity=76.68144

Batch 62130, train_perplexity=76.44356

Batch 62140, train_perplexity=78.10106

Batch 62150, train_perplexity=77.29812

Batch 62160, train_perplexity=78.54819

Batch 62170, train_perplexity=75.88368

Batch 62180, train_perplexity=76.141136

Batch 62190, train_perplexity=68.830376

Batch 62200, train_perplexity=73.79877

Batch 62210, train_perplexity=87.61472

Batch 62220, train_perplexity=83.53421

Batch 62230, train_perplexity=84.68481

Batch 62240, train_perplexity=79.68933

Batch 62250, train_perplexity=83.59039

Batch 62260, train_perplexity=75.12123

Batch 62270, train_perplexity=75.610405

Batch 62280, train_perplexity=81.71572

Batch 62290, train_perplexity=79.288086

Batch 62300, train_perplexity=86.68102

Batch 62310, train_perplexity=73.405685

Batch 62320, train_perplexity=76.70148

Batch 62330, train_perplexity=76.63085

Batch 62340, train_perplexity=73.64311

Batch 62350, train_perplexity=85.361046

Batch 62360, train_perplexity=85.31759

Batch 62370, train_perplexity=73.07482

Batch 62380, train_perplexity=77.76253

Batch 62390, train_perplexity=91.87887
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 62400, train_perplexity=83.63174

Batch 62410, train_perplexity=74.75618

Batch 62420, train_perplexity=80.99866

Batch 62430, train_perplexity=82.62634

Batch 62440, train_perplexity=80.33533

Batch 62450, train_perplexity=76.03679

Batch 62460, train_perplexity=88.37426

Batch 62470, train_perplexity=77.152664

Batch 62480, train_perplexity=76.14404

Batch 62490, train_perplexity=81.45728

Batch 62500, train_perplexity=77.9877

Batch 62510, train_perplexity=77.26864

Batch 62520, train_perplexity=86.377556

Batch 62530, train_perplexity=73.94266

Batch 62540, train_perplexity=79.865074

Batch 62550, train_perplexity=77.55475

Batch 62560, train_perplexity=85.04881

Batch 62570, train_perplexity=75.25453

Batch 62580, train_perplexity=80.52264

Batch 62590, train_perplexity=75.91061

Batch 62600, train_perplexity=81.32661

Batch 62610, train_perplexity=75.6178

Batch 62620, train_perplexity=83.38955

Batch 62630, train_perplexity=81.24843

Batch 62640, train_perplexity=72.731125

Batch 62650, train_perplexity=77.51212

Batch 62660, train_perplexity=72.80479

Batch 62670, train_perplexity=73.22543

Batch 62680, train_perplexity=78.89398

Batch 62690, train_perplexity=79.373726

Batch 62700, train_perplexity=84.66712

Batch 62710, train_perplexity=83.48376

Batch 62720, train_perplexity=80.0187

Batch 62730, train_perplexity=77.46682

Batch 62740, train_perplexity=70.618675

Batch 62750, train_perplexity=78.90086

Batch 62760, train_perplexity=81.04157

Batch 62770, train_perplexity=74.59921

Batch 62780, train_perplexity=76.135544

Batch 62790, train_perplexity=80.40757

Batch 62800, train_perplexity=76.45486

Batch 62810, train_perplexity=79.14402

Batch 62820, train_perplexity=80.20489

Batch 62830, train_perplexity=77.04822

Batch 62840, train_perplexity=82.62907

Batch 62850, train_perplexity=77.74644

Batch 62860, train_perplexity=81.17965

Batch 62870, train_perplexity=80.39227

Batch 62880, train_perplexity=87.82922

Batch 62890, train_perplexity=83.13002

Batch 62900, train_perplexity=78.258675

Batch 62910, train_perplexity=68.42469

Batch 62920, train_perplexity=79.18652

Batch 62930, train_perplexity=77.00796

Batch 62940, train_perplexity=79.89982

Batch 62950, train_perplexity=81.950745

Batch 62960, train_perplexity=74.99039

Batch 62970, train_perplexity=86.1832

Batch 62980, train_perplexity=79.39962

Batch 62990, train_perplexity=77.66795

Batch 63000, train_perplexity=80.24459

Batch 63010, train_perplexity=77.155975

Batch 63020, train_perplexity=88.22925

Batch 63030, train_perplexity=78.23412

Batch 63040, train_perplexity=80.51903

Batch 63050, train_perplexity=80.98668

Batch 63060, train_perplexity=78.143005

Batch 63070, train_perplexity=82.39068

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00070-of-00100
Loaded 305476 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00070-of-00100
Loaded 305476 sentences.
Finished loading
Batch 63080, train_perplexity=77.81776

Batch 63090, train_perplexity=82.81951

Batch 63100, train_perplexity=79.884766

Batch 63110, train_perplexity=76.16935

Batch 63120, train_perplexity=74.42841

Batch 63130, train_perplexity=76.0361

Batch 63140, train_perplexity=73.24248

Batch 63150, train_perplexity=91.85614

Batch 63160, train_perplexity=84.7893

Batch 63170, train_perplexity=76.98997

Batch 63180, train_perplexity=85.52777

Batch 63190, train_perplexity=84.53057

Batch 63200, train_perplexity=77.445915

Batch 63210, train_perplexity=85.01423

Batch 63220, train_perplexity=79.04845

Batch 63230, train_perplexity=74.94241

Batch 63240, train_perplexity=87.61631

Batch 63250, train_perplexity=79.73866

Batch 63260, train_perplexity=82.41827

Batch 63270, train_perplexity=81.05522

Batch 63280, train_perplexity=75.28799

Batch 63290, train_perplexity=77.06898

Batch 63300, train_perplexity=87.162186

Batch 63310, train_perplexity=80.266716

Batch 63320, train_perplexity=80.76301

Batch 63330, train_perplexity=87.22222

Batch 63340, train_perplexity=82.412766

Batch 63350, train_perplexity=75.2752

Batch 63360, train_perplexity=81.16193

Batch 63370, train_perplexity=83.85144

Batch 63380, train_perplexity=84.619255

Batch 63390, train_perplexity=73.08465

Batch 63400, train_perplexity=77.16502

Batch 63410, train_perplexity=77.64588

Batch 63420, train_perplexity=84.18998

Batch 63430, train_perplexity=80.43997

Batch 63440, train_perplexity=77.35394

Batch 63450, train_perplexity=75.8525

Batch 63460, train_perplexity=77.46586

Batch 63470, train_perplexity=83.46919

Batch 63480, train_perplexity=80.31698

Batch 63490, train_perplexity=81.137314

Batch 63500, train_perplexity=70.83641

Batch 63510, train_perplexity=85.429535

Batch 63520, train_perplexity=79.68249

Batch 63530, train_perplexity=81.37359

Batch 63540, train_perplexity=74.701164

Batch 63550, train_perplexity=76.231995

Batch 63560, train_perplexity=81.98186

Batch 63570, train_perplexity=75.21209

Batch 63580, train_perplexity=74.23893

Batch 63590, train_perplexity=82.1437

Batch 63600, train_perplexity=83.890114

Batch 63610, train_perplexity=76.76127

Batch 63620, train_perplexity=78.43052

Batch 63630, train_perplexity=77.28839

Batch 63640, train_perplexity=91.53529

Batch 63650, train_perplexity=77.26071

Batch 63660, train_perplexity=70.91823

Batch 63670, train_perplexity=77.21784

Batch 63680, train_perplexity=77.832115

Batch 63690, train_perplexity=80.47842

Batch 63700, train_perplexity=79.95157

Batch 63710, train_perplexity=69.62479

Batch 63720, train_perplexity=76.08314

Batch 63730, train_perplexity=80.29918

Batch 63740, train_perplexity=75.5766

Batch 63750, train_perplexity=79.32152

Batch 63760, train_perplexity=81.16014

Batch 63770, train_perplexity=80.19647

Batch 63780, train_perplexity=77.30774

Batch 63790, train_perplexity=80.80138

Batch 63800, train_perplexity=72.556786

Batch 63810, train_perplexity=85.48431

Batch 63820, train_perplexity=81.344376

Batch 63830, train_perplexity=77.21884

Batch 63840, train_perplexity=83.443

Batch 63850, train_perplexity=79.70259

Batch 63860, train_perplexity=87.33118

Batch 63870, train_perplexity=74.54534

Batch 63880, train_perplexity=81.30986

Batch 63890, train_perplexity=74.17987

Batch 63900, train_perplexity=76.35598

Batch 63910, train_perplexity=83.197235

Batch 63920, train_perplexity=78.55725

Batch 63930, train_perplexity=77.67618

Batch 63940, train_perplexity=74.49567

Batch 63950, train_perplexity=79.31992

Batch 63960, train_perplexity=75.00637

Batch 63970, train_perplexity=82.91873

Batch 63980, train_perplexity=77.049355

Batch 63990, train_perplexity=80.91145

Batch 64000, train_perplexity=81.52494

Batch 64010, train_perplexity=75.19441

Batch 64020, train_perplexity=79.06699

Batch 64030, train_perplexity=80.91732

Batch 64040, train_perplexity=82.4855

Batch 64050, train_perplexity=74.26325

Batch 64060, train_perplexity=75.938194

Batch 64070, train_perplexity=72.14417

Batch 64080, train_perplexity=89.70032

Batch 64090, train_perplexity=78.81453

Batch 64100, train_perplexity=79.804016

Batch 64110, train_perplexity=72.80944

Batch 64120, train_perplexity=74.36467

Batch 64130, train_perplexity=76.09504

Batch 64140, train_perplexity=77.765045

Batch 64150, train_perplexity=71.43687

Batch 64160, train_perplexity=75.60226

Batch 64170, train_perplexity=71.99114

Batch 64180, train_perplexity=78.52849

Batch 64190, train_perplexity=75.15505

Batch 64200, train_perplexity=75.30342

Batch 64210, train_perplexity=78.73629

Batch 64220, train_perplexity=78.01515

Batch 64230, train_perplexity=77.32186

Batch 64240, train_perplexity=82.37198

Batch 64250, train_perplexity=82.37799

Batch 64260, train_perplexity=80.20274

Batch 64270, train_perplexity=73.678795

Batch 64280, train_perplexity=77.31563

Batch 64290, train_perplexity=79.43923

Batch 64300, train_perplexity=74.06154

Batch 64310, train_perplexity=73.64813

Batch 64320, train_perplexity=82.877464

Batch 64330, train_perplexity=75.75249
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 64340, train_perplexity=81.48172

Batch 64350, train_perplexity=83.49467

Batch 64360, train_perplexity=88.07456

Batch 64370, train_perplexity=79.94784

Batch 64380, train_perplexity=74.82215

Batch 64390, train_perplexity=73.01087

Batch 64400, train_perplexity=79.26737

Batch 64410, train_perplexity=79.6527

Batch 64420, train_perplexity=81.17237

Batch 64430, train_perplexity=75.13019

Batch 64440, train_perplexity=87.0517

Batch 64450, train_perplexity=76.57642

Batch 64460, train_perplexity=79.805084

Batch 64470, train_perplexity=71.570114

Batch 64480, train_perplexity=79.16788

Batch 64490, train_perplexity=77.145744

Batch 64500, train_perplexity=81.71116

Batch 64510, train_perplexity=72.01072

Batch 64520, train_perplexity=74.13514

Batch 64530, train_perplexity=81.17296

Batch 64540, train_perplexity=76.53919

Batch 64550, train_perplexity=81.96684

Batch 64560, train_perplexity=81.25044

Batch 64570, train_perplexity=88.19101

Batch 64580, train_perplexity=79.12855

Batch 64590, train_perplexity=86.67974

Batch 64600, train_perplexity=77.51241

Batch 64610, train_perplexity=81.893005

Batch 64620, train_perplexity=78.85336

Batch 64630, train_perplexity=73.818474

Batch 64640, train_perplexity=82.87525

Batch 64650, train_perplexity=80.23962

Batch 64660, train_perplexity=71.019684

Batch 64670, train_perplexity=71.68364

Batch 64680, train_perplexity=76.14415

Batch 64690, train_perplexity=80.40891

Batch 64700, train_perplexity=70.91901

Batch 64710, train_perplexity=75.35816

Batch 64720, train_perplexity=85.15071

Batch 64730, train_perplexity=86.93372

Batch 64740, train_perplexity=81.1312

Batch 64750, train_perplexity=80.45605

Batch 64760, train_perplexity=78.15511

Batch 64770, train_perplexity=81.04297

Batch 64780, train_perplexity=81.78397

Batch 64790, train_perplexity=71.50162

Batch 64800, train_perplexity=80.70403

Batch 64810, train_perplexity=78.01025

Batch 64820, train_perplexity=83.08829

Batch 64830, train_perplexity=70.83668

Batch 64840, train_perplexity=86.364044

Batch 64850, train_perplexity=77.15498

Batch 64860, train_perplexity=76.59432

Batch 64870, train_perplexity=71.850914

Batch 64880, train_perplexity=81.63506

Batch 64890, train_perplexity=76.78931

Batch 64900, train_perplexity=78.22894

Batch 64910, train_perplexity=75.34135

Batch 64920, train_perplexity=73.77375

Batch 64930, train_perplexity=82.70139

Batch 64940, train_perplexity=75.13291

Batch 64950, train_perplexity=75.91604

Batch 64960, train_perplexity=80.00984

Batch 64970, train_perplexity=81.89488

Batch 64980, train_perplexity=83.34888

Batch 64990, train_perplexity=75.32852

Batch 65000, train_perplexity=77.27726

Batch 65010, train_perplexity=84.678185

Batch 65020, train_perplexity=85.274635

Batch 65030, train_perplexity=75.4844

Batch 65040, train_perplexity=77.73998

Batch 65050, train_perplexity=79.46681

Batch 65060, train_perplexity=86.85666

Batch 65070, train_perplexity=81.653984

Batch 65080, train_perplexity=78.19563

Batch 65090, train_perplexity=76.42125

Batch 65100, train_perplexity=77.01923

Batch 65110, train_perplexity=79.79314

Batch 65120, train_perplexity=78.64698

Batch 65130, train_perplexity=89.322784

Batch 65140, train_perplexity=79.055534

Batch 65150, train_perplexity=81.81866

Batch 65160, train_perplexity=77.511826

Batch 65170, train_perplexity=73.63012

Batch 65180, train_perplexity=67.291115

Batch 65190, train_perplexity=78.708206

Batch 65200, train_perplexity=83.55684

Batch 65210, train_perplexity=80.36805

Batch 65220, train_perplexity=70.539925

Batch 65230, train_perplexity=76.21804

Batch 65240, train_perplexity=73.80746

Batch 65250, train_perplexity=76.94875

Batch 65260, train_perplexity=84.97054

Batch 65270, train_perplexity=83.18335

Batch 65280, train_perplexity=81.396484

Batch 65290, train_perplexity=78.92879

Batch 65300, train_perplexity=79.839035

Batch 65310, train_perplexity=86.558556

Batch 65320, train_perplexity=79.73152

Batch 65330, train_perplexity=78.30989

Batch 65340, train_perplexity=78.50513

Batch 65350, train_perplexity=85.39313

Batch 65360, train_perplexity=76.76398

Batch 65370, train_perplexity=71.13407

Batch 65380, train_perplexity=75.76892

Batch 65390, train_perplexity=76.13892

Batch 65400, train_perplexity=81.93496

Batch 65410, train_perplexity=83.914955

Batch 65420, train_perplexity=88.35664

Batch 65430, train_perplexity=71.497665

Batch 65440, train_perplexity=81.96864

Batch 65450, train_perplexity=88.16633

Batch 65460, train_perplexity=78.13943

Batch 65470, train_perplexity=82.643524

Batch 65480, train_perplexity=77.278145

Batch 65490, train_perplexity=81.21585

Batch 65500, train_perplexity=81.23793

Batch 65510, train_perplexity=78.24942

Batch 65520, train_perplexity=81.11321

Batch 65530, train_perplexity=82.289536

Batch 65540, train_perplexity=79.96278

Batch 65550, train_perplexity=76.6927

Batch 65560, train_perplexity=77.79999

Batch 65570, train_perplexity=79.57063

Batch 65580, train_perplexity=80.17051

Batch 65590, train_perplexity=85.87881

Batch 65600, train_perplexity=70.792885

Batch 65610, train_perplexity=73.16966

Batch 65620, train_perplexity=77.68507

Batch 65630, train_perplexity=87.11274

Batch 65640, train_perplexity=84.91598

Batch 65650, train_perplexity=74.4489

Batch 65660, train_perplexity=79.03918

Batch 65670, train_perplexity=83.459

Batch 65680, train_perplexity=76.390205

Batch 65690, train_perplexity=86.37105

Batch 65700, train_perplexity=82.44983

Batch 65710, train_perplexity=81.97404

Batch 65720, train_perplexity=72.50366

Batch 65730, train_perplexity=84.006

Batch 65740, train_perplexity=86.195366

Batch 65750, train_perplexity=75.51619

Batch 65760, train_perplexity=77.20923

Batch 65770, train_perplexity=80.24964

Batch 65780, train_perplexity=80.04575

Batch 65790, train_perplexity=69.897995

Batch 65800, train_perplexity=75.4542

Batch 65810, train_perplexity=77.04869

Batch 65820, train_perplexity=77.881714

Batch 65830, train_perplexity=77.54924

Batch 65840, train_perplexity=83.942406

Batch 65850, train_perplexity=87.91864

Batch 65860, train_perplexity=86.36429

Batch 65870, train_perplexity=71.93573

Batch 65880, train_perplexity=79.6068

Batch 65890, train_perplexity=80.18355

Batch 65900, train_perplexity=86.12088

Batch 65910, train_perplexity=82.5542

Batch 65920, train_perplexity=77.3108

Batch 65930, train_perplexity=76.11743

Batch 65940, train_perplexity=80.66022

Batch 65950, train_perplexity=77.43883

Batch 65960, train_perplexity=85.843475

Batch 65970, train_perplexity=74.65779

Batch 65980, train_perplexity=92.27182

Batch 65990, train_perplexity=79.41567

Batch 66000, train_perplexity=81.322845

Batch 66010, train_perplexity=63.547173

Batch 66020, train_perplexity=76.39862

Batch 66030, train_perplexity=79.08637

Batch 66040, train_perplexity=80.5259

Batch 66050, train_perplexity=81.40933

Batch 66060, train_perplexity=84.550285

Batch 66070, train_perplexity=82.12913

Batch 66080, train_perplexity=74.44269

Batch 66090, train_perplexity=84.46929

Batch 66100, train_perplexity=85.17914

Batch 66110, train_perplexity=80.87766

Batch 66120, train_perplexity=73.80366

Batch 66130, train_perplexity=75.52832

Batch 66140, train_perplexity=79.51078

Batch 66150, train_perplexity=75.23096

Batch 66160, train_perplexity=82.47661

Batch 66170, train_perplexity=77.601616

Batch 66180, train_perplexity=83.5564

Batch 66190, train_perplexity=75.63724

Batch 66200, train_perplexity=84.92534

Batch 66210, train_perplexity=82.36051

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00055-of-00100
Loaded 306641 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00055-of-00100
Loaded 306641 sentences.
Finished loading
Batch 66220, train_perplexity=75.99826

Batch 66230, train_perplexity=72.90665

Batch 66240, train_perplexity=77.43971

Batch 66250, train_perplexity=75.78446

Batch 66260, train_perplexity=71.374084

Batch 66270, train_perplexity=83.61786
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 66280, train_perplexity=80.072136

Batch 66290, train_perplexity=81.07451

Batch 66300, train_perplexity=71.56029

Batch 66310, train_perplexity=76.36577

Batch 66320, train_perplexity=75.93334

Batch 66330, train_perplexity=72.57928

Batch 66340, train_perplexity=78.651855

Batch 66350, train_perplexity=78.1717

Batch 66360, train_perplexity=81.81588

Batch 66370, train_perplexity=81.63608

Batch 66380, train_perplexity=70.74371

Batch 66390, train_perplexity=77.20455

Batch 66400, train_perplexity=76.221054

Batch 66410, train_perplexity=79.23801

Batch 66420, train_perplexity=83.52393

Batch 66430, train_perplexity=80.51596

Batch 66440, train_perplexity=82.40954

Batch 66450, train_perplexity=84.3411

Batch 66460, train_perplexity=79.27712

Batch 66470, train_perplexity=80.87817

Batch 66480, train_perplexity=79.024025

Batch 66490, train_perplexity=78.96839

Batch 66500, train_perplexity=83.666595

Batch 66510, train_perplexity=82.956894

Batch 66520, train_perplexity=86.00404

Batch 66530, train_perplexity=87.356705

Batch 66540, train_perplexity=75.461685

Batch 66550, train_perplexity=72.703415

Batch 66560, train_perplexity=79.92634

Batch 66570, train_perplexity=79.277504

Batch 66580, train_perplexity=72.29301

Batch 66590, train_perplexity=87.64957

Batch 66600, train_perplexity=75.061295

Batch 66610, train_perplexity=79.91037

Batch 66620, train_perplexity=83.74906

Batch 66630, train_perplexity=86.22307

Batch 66640, train_perplexity=82.12788

Batch 66650, train_perplexity=73.31373

Batch 66660, train_perplexity=81.87384

Batch 66670, train_perplexity=77.50007

Batch 66680, train_perplexity=78.4658

Batch 66690, train_perplexity=75.43704

Batch 66700, train_perplexity=74.81959

Batch 66710, train_perplexity=88.0471

Batch 66720, train_perplexity=83.25708

Batch 66730, train_perplexity=84.787315

Batch 66740, train_perplexity=85.87545

Batch 66750, train_perplexity=77.431885

Batch 66760, train_perplexity=81.25253

Batch 66770, train_perplexity=77.24741

Batch 66780, train_perplexity=80.09604

Batch 66790, train_perplexity=77.38168

Batch 66800, train_perplexity=75.23085

Batch 66810, train_perplexity=83.61738

Batch 66820, train_perplexity=76.71823

Batch 66830, train_perplexity=80.12813

Batch 66840, train_perplexity=76.91045

Batch 66850, train_perplexity=82.270355

Batch 66860, train_perplexity=81.23394

Batch 66870, train_perplexity=82.66134

Batch 66880, train_perplexity=82.13485

Batch 66890, train_perplexity=78.30328

Batch 66900, train_perplexity=70.98654

Batch 66910, train_perplexity=84.719055

Batch 66920, train_perplexity=82.58471

Batch 66930, train_perplexity=75.41813

Batch 66940, train_perplexity=77.77573

Batch 66950, train_perplexity=77.20794

Batch 66960, train_perplexity=78.70018

Batch 66970, train_perplexity=84.54287

Batch 66980, train_perplexity=78.368576

Batch 66990, train_perplexity=73.54618

Batch 67000, train_perplexity=78.23401

Batch 67010, train_perplexity=82.6488

Batch 67020, train_perplexity=80.13435

Batch 67030, train_perplexity=81.55002

Batch 67040, train_perplexity=67.37884

Batch 67050, train_perplexity=82.59802

Batch 67060, train_perplexity=82.15812

Batch 67070, train_perplexity=79.42317

Batch 67080, train_perplexity=79.353294

Batch 67090, train_perplexity=80.12629

Batch 67100, train_perplexity=78.16715

Batch 67110, train_perplexity=73.37223

Batch 67120, train_perplexity=71.683846

Batch 67130, train_perplexity=77.75077

Batch 67140, train_perplexity=71.1374

Batch 67150, train_perplexity=81.13735

Batch 67160, train_perplexity=78.3494

Batch 67170, train_perplexity=79.04008

Batch 67180, train_perplexity=70.39926

Batch 67190, train_perplexity=73.831955

Batch 67200, train_perplexity=72.93408

Batch 67210, train_perplexity=71.3703

Batch 67220, train_perplexity=76.89733

Batch 67230, train_perplexity=82.68093

Batch 67240, train_perplexity=84.74613

Batch 67250, train_perplexity=75.216896

Batch 67260, train_perplexity=78.71654

Batch 67270, train_perplexity=78.63131

Batch 67280, train_perplexity=78.779625

Batch 67290, train_perplexity=88.65527

Batch 67300, train_perplexity=79.6967

Batch 67310, train_perplexity=73.86087

Batch 67320, train_perplexity=82.205574

Batch 67330, train_perplexity=83.28722

Batch 67340, train_perplexity=86.36899

Batch 67350, train_perplexity=84.22339

Batch 67360, train_perplexity=80.27127

Batch 67370, train_perplexity=74.00767

Batch 67380, train_perplexity=71.68282

Batch 67390, train_perplexity=74.563866

Batch 67400, train_perplexity=78.29832

Batch 67410, train_perplexity=78.22289

Batch 67420, train_perplexity=77.69289

Batch 67430, train_perplexity=77.85008

Batch 67440, train_perplexity=83.04432

Batch 67450, train_perplexity=85.11823

Batch 67460, train_perplexity=76.338104

Batch 67470, train_perplexity=77.71249

Batch 67480, train_perplexity=71.91752

Batch 67490, train_perplexity=76.27585

Batch 67500, train_perplexity=73.26078

Batch 67510, train_perplexity=80.29075

Batch 67520, train_perplexity=86.25605

Batch 67530, train_perplexity=74.83307

Batch 67540, train_perplexity=79.48815

Batch 67550, train_perplexity=79.01581

Batch 67560, train_perplexity=76.395454

Batch 67570, train_perplexity=67.56686

Batch 67580, train_perplexity=77.21722

Batch 67590, train_perplexity=78.81855

Batch 67600, train_perplexity=82.68759

Batch 67610, train_perplexity=74.5719

Batch 67620, train_perplexity=70.30446

Batch 67630, train_perplexity=75.246925

Batch 67640, train_perplexity=72.50152

Batch 67650, train_perplexity=77.89211

Batch 67660, train_perplexity=70.22352

Batch 67670, train_perplexity=87.220184

Batch 67680, train_perplexity=79.94181

Batch 67690, train_perplexity=73.83967

Batch 67700, train_perplexity=78.33596

Batch 67710, train_perplexity=83.75194

Batch 67720, train_perplexity=82.76314

Batch 67730, train_perplexity=83.88175

Batch 67740, train_perplexity=77.38604

Batch 67750, train_perplexity=82.28162

Batch 67760, train_perplexity=75.44769

Batch 67770, train_perplexity=71.84468

Batch 67780, train_perplexity=74.21758

Batch 67790, train_perplexity=78.22838

Batch 67800, train_perplexity=81.56744

Batch 67810, train_perplexity=80.2529

Batch 67820, train_perplexity=90.577866

Batch 67830, train_perplexity=82.79333

Batch 67840, train_perplexity=84.41561

Batch 67850, train_perplexity=79.112625

Batch 67860, train_perplexity=76.97257

Batch 67870, train_perplexity=84.990555

Batch 67880, train_perplexity=81.5029

Batch 67890, train_perplexity=76.27614

Batch 67900, train_perplexity=78.87126

Batch 67910, train_perplexity=81.401764

Batch 67920, train_perplexity=79.23268

Batch 67930, train_perplexity=76.746185

Batch 67940, train_perplexity=84.87732

Batch 67950, train_perplexity=79.48402

Batch 67960, train_perplexity=76.90426

Batch 67970, train_perplexity=69.05316

Batch 67980, train_perplexity=79.78575

Batch 67990, train_perplexity=80.26733

Batch 68000, train_perplexity=91.1108

Batch 68010, train_perplexity=74.87491

Batch 68020, train_perplexity=74.79754

Batch 68030, train_perplexity=74.57759

Batch 68040, train_perplexity=72.76151

Batch 68050, train_perplexity=81.754036

Batch 68060, train_perplexity=78.33129

Batch 68070, train_perplexity=74.0792

Batch 68080, train_perplexity=78.7212

Batch 68090, train_perplexity=82.903076

Batch 68100, train_perplexity=78.0121

Batch 68110, train_perplexity=84.5577

Batch 68120, train_perplexity=77.08033

Batch 68130, train_perplexity=77.07728

Batch 68140, train_perplexity=80.56838

Batch 68150, train_perplexity=85.11539

Batch 68160, train_perplexity=79.08769

Batch 68170, train_perplexity=76.34917

Batch 68180, train_perplexity=88.30021

Batch 68190, train_perplexity=84.75805

Batch 68200, train_perplexity=81.69032

Batch 68210, train_perplexity=84.46417

Batch 68220, train_perplexity=69.92926

Batch 68230, train_perplexity=86.15871

Batch 68240, train_perplexity=75.59058

Batch 68250, train_perplexity=79.126205

Batch 68260, train_perplexity=73.93109

Batch 68270, train_perplexity=78.309296

Batch 68280, train_perplexity=82.24686

Batch 68290, train_perplexity=75.06147

Batch 68300, train_perplexity=69.437035

Batch 68310, train_perplexity=79.487885

Batch 68320, train_perplexity=83.95746
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 68330, train_perplexity=82.23517

Batch 68340, train_perplexity=84.556656

Batch 68350, train_perplexity=77.25162

Batch 68360, train_perplexity=77.80511

Batch 68370, train_perplexity=74.07058

Batch 68380, train_perplexity=80.39097

Batch 68390, train_perplexity=80.5198

Batch 68400, train_perplexity=73.55801

Batch 68410, train_perplexity=78.342606

Batch 68420, train_perplexity=82.430214

Batch 68430, train_perplexity=71.057724

Batch 68440, train_perplexity=75.66978

Batch 68450, train_perplexity=74.33301

Batch 68460, train_perplexity=74.32915

Batch 68470, train_perplexity=85.079765

Batch 68480, train_perplexity=74.04417

Batch 68490, train_perplexity=85.06942

Batch 68500, train_perplexity=92.59132

Batch 68510, train_perplexity=80.25894

Batch 68520, train_perplexity=74.77832

Batch 68530, train_perplexity=75.55

Batch 68540, train_perplexity=68.955536

Batch 68550, train_perplexity=77.945175

Batch 68560, train_perplexity=73.57004

Batch 68570, train_perplexity=83.72566

Batch 68580, train_perplexity=73.579994

Batch 68590, train_perplexity=78.25218

Batch 68600, train_perplexity=81.192116

Batch 68610, train_perplexity=77.29576

Batch 68620, train_perplexity=80.68499

Batch 68630, train_perplexity=97.651375

Batch 68640, train_perplexity=80.379395

Batch 68650, train_perplexity=67.753044

Batch 68660, train_perplexity=79.46265

Batch 68670, train_perplexity=69.4132

Batch 68680, train_perplexity=81.30431

Batch 68690, train_perplexity=73.50607

Batch 68700, train_perplexity=70.74898

Batch 68710, train_perplexity=77.24499

Batch 68720, train_perplexity=74.82832

Batch 68730, train_perplexity=68.52758

Batch 68740, train_perplexity=80.385025

Batch 68750, train_perplexity=87.0997

Batch 68760, train_perplexity=75.65588

Batch 68770, train_perplexity=73.61509

Batch 68780, train_perplexity=72.87794

Batch 68790, train_perplexity=84.974144

Batch 68800, train_perplexity=72.67985

Batch 68810, train_perplexity=71.92276

Batch 68820, train_perplexity=78.25166

Batch 68830, train_perplexity=80.04236

Batch 68840, train_perplexity=84.22403

Batch 68850, train_perplexity=73.125015

Batch 68860, train_perplexity=79.06628

Batch 68870, train_perplexity=80.2929

Batch 68880, train_perplexity=72.883255

Batch 68890, train_perplexity=76.08535

Batch 68900, train_perplexity=83.38294

Batch 68910, train_perplexity=83.95566

Batch 68920, train_perplexity=80.90528

Batch 68930, train_perplexity=73.05141

Batch 68940, train_perplexity=76.6549

Batch 68950, train_perplexity=73.650345

Batch 68960, train_perplexity=74.4501

Batch 68970, train_perplexity=81.69667

Batch 68980, train_perplexity=75.66371

Batch 68990, train_perplexity=73.36832

Batch 69000, train_perplexity=76.10756

Batch 69010, train_perplexity=73.41185

Batch 69020, train_perplexity=74.70672

Batch 69030, train_perplexity=81.147606

Batch 69040, train_perplexity=73.82045

Batch 69050, train_perplexity=77.74251

Batch 69060, train_perplexity=84.04831

Batch 69070, train_perplexity=85.88188

Batch 69080, train_perplexity=77.42705

Batch 69090, train_perplexity=75.51651

Batch 69100, train_perplexity=82.23635

Batch 69110, train_perplexity=80.84211

Batch 69120, train_perplexity=74.4216

Batch 69130, train_perplexity=74.84806

Batch 69140, train_perplexity=80.60969

Batch 69150, train_perplexity=80.071106

Batch 69160, train_perplexity=75.91039

Batch 69170, train_perplexity=78.48222

Batch 69180, train_perplexity=82.86794

Batch 69190, train_perplexity=79.422646

Batch 69200, train_perplexity=72.928345

Batch 69210, train_perplexity=83.20112

Batch 69220, train_perplexity=79.153114

Batch 69230, train_perplexity=73.54229

Batch 69240, train_perplexity=78.450195

Batch 69250, train_perplexity=69.328186

Batch 69260, train_perplexity=88.966446

Batch 69270, train_perplexity=69.51621

Batch 69280, train_perplexity=84.16252

Batch 69290, train_perplexity=84.06033

Batch 69300, train_perplexity=74.18694

Batch 69310, train_perplexity=85.398544

Batch 69320, train_perplexity=69.03663

Batch 69330, train_perplexity=78.32255

Batch 69340, train_perplexity=75.357086

Batch 69350, train_perplexity=75.15975

Batch 69360, train_perplexity=77.24418

Batch 69370, train_perplexity=79.15013

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00098-of-00100
Loaded 306180 sentences.
Finished loading
Batch 69380, train_perplexity=78.25054

Batch 69390, train_perplexity=80.92226

Batch 69400, train_perplexity=73.51053

Batch 69410, train_perplexity=82.234505

Batch 69420, train_perplexity=79.21878

Batch 69430, train_perplexity=78.01832

Batch 69440, train_perplexity=71.66109

Batch 69450, train_perplexity=89.64263

Batch 69460, train_perplexity=79.84992

Batch 69470, train_perplexity=82.52633

Batch 69480, train_perplexity=77.41228

Batch 69490, train_perplexity=84.61425

Batch 69500, train_perplexity=82.13019

Batch 69510, train_perplexity=78.44646

Batch 69520, train_perplexity=76.252205

Batch 69530, train_perplexity=75.68465

Batch 69540, train_perplexity=68.37772

Batch 69550, train_perplexity=81.402145

Batch 69560, train_perplexity=71.65712

Batch 69570, train_perplexity=78.97543

Batch 69580, train_perplexity=72.68231

Batch 69590, train_perplexity=72.02249

Batch 69600, train_perplexity=79.3919

Batch 69610, train_perplexity=72.903656

Batch 69620, train_perplexity=72.18147

Batch 69630, train_perplexity=79.24447

Batch 69640, train_perplexity=69.76879

Batch 69650, train_perplexity=83.95838

Batch 69660, train_perplexity=78.50138

Batch 69670, train_perplexity=77.7893

Batch 69680, train_perplexity=77.40497

Batch 69690, train_perplexity=75.06283

Batch 69700, train_perplexity=77.630486

Batch 69710, train_perplexity=73.05963

Batch 69720, train_perplexity=65.14911

Batch 69730, train_perplexity=78.58854

Batch 69740, train_perplexity=74.97222

Batch 69750, train_perplexity=77.72382

Batch 69760, train_perplexity=73.070465

Batch 69770, train_perplexity=86.178764

Batch 69780, train_perplexity=80.25714

Batch 69790, train_perplexity=79.44991

Batch 69800, train_perplexity=74.54836

Batch 69810, train_perplexity=79.02319

Batch 69820, train_perplexity=72.227745

Batch 69830, train_perplexity=70.38238

Batch 69840, train_perplexity=79.51973

Batch 69850, train_perplexity=79.12395

Batch 69860, train_perplexity=77.02944

Batch 69870, train_perplexity=75.89787

Batch 69880, train_perplexity=89.08735

Batch 69890, train_perplexity=77.05538

Batch 69900, train_perplexity=77.924805

Batch 69910, train_perplexity=75.21593

Batch 69920, train_perplexity=77.39416

Batch 69930, train_perplexity=81.06856

Batch 69940, train_perplexity=73.39204

Batch 69950, train_perplexity=77.50732

Batch 69960, train_perplexity=82.03558

Batch 69970, train_perplexity=78.963005

Batch 69980, train_perplexity=77.009796

Batch 69990, train_perplexity=80.183395

Batch 70000, train_perplexity=73.93504

Batch 70010, train_perplexity=72.853966

Batch 70020, train_perplexity=80.74372

Batch 70030, train_perplexity=83.44456

Batch 70040, train_perplexity=80.697296

Batch 70050, train_perplexity=73.67479

Batch 70060, train_perplexity=70.73205

Batch 70070, train_perplexity=77.07324

Batch 70080, train_perplexity=73.3016

Batch 70090, train_perplexity=71.56875

Batch 70100, train_perplexity=76.76822

Batch 70110, train_perplexity=75.216576

Batch 70120, train_perplexity=64.25438

Batch 70130, train_perplexity=84.011406

Batch 70140, train_perplexity=76.771225

Batch 70150, train_perplexity=84.30455

Batch 70160, train_perplexity=75.153656

Batch 70170, train_perplexity=77.09585

Batch 70180, train_perplexity=75.45586

Batch 70190, train_perplexity=80.305145

Batch 70200, train_perplexity=74.12867

Batch 70210, train_perplexity=81.24157

Batch 70220, train_perplexity=63.069817

Batch 70230, train_perplexity=76.57055

Batch 70240, train_perplexity=81.11024

Batch 70250, train_perplexity=78.28114

Batch 70260, train_perplexity=69.9356
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 70270, train_perplexity=77.176285

Batch 70280, train_perplexity=80.254845

Batch 70290, train_perplexity=71.66303

Batch 70300, train_perplexity=79.25826

Batch 70310, train_perplexity=73.49164

Batch 70320, train_perplexity=82.14664

Batch 70330, train_perplexity=78.86713

Batch 70340, train_perplexity=75.3254

Batch 70350, train_perplexity=82.03887

Batch 70360, train_perplexity=69.39671

Batch 70370, train_perplexity=82.60003

Batch 70380, train_perplexity=70.466034

Batch 70390, train_perplexity=79.51199

Batch 70400, train_perplexity=72.79986

Batch 70410, train_perplexity=81.309044

Batch 70420, train_perplexity=73.51631

Batch 70430, train_perplexity=75.14803

Batch 70440, train_perplexity=72.1666

Batch 70450, train_perplexity=76.50292

Batch 70460, train_perplexity=76.12375

Batch 70470, train_perplexity=82.64006

Batch 70480, train_perplexity=74.254715

Batch 70490, train_perplexity=80.19353

Batch 70500, train_perplexity=71.27634

Batch 70510, train_perplexity=74.237656

Batch 70520, train_perplexity=75.17441

Batch 70530, train_perplexity=78.97987

Batch 70540, train_perplexity=77.181915

Batch 70550, train_perplexity=79.18981

Batch 70560, train_perplexity=76.592094

Batch 70570, train_perplexity=70.21518

Batch 70580, train_perplexity=78.330284

Batch 70590, train_perplexity=83.17296

Batch 70600, train_perplexity=82.89869

Batch 70610, train_perplexity=76.32296

Batch 70620, train_perplexity=85.63632

Batch 70630, train_perplexity=72.14221

Batch 70640, train_perplexity=72.98804

Batch 70650, train_perplexity=73.002106

Batch 70660, train_perplexity=77.3056

Batch 70670, train_perplexity=80.899414

Batch 70680, train_perplexity=70.62356

Batch 70690, train_perplexity=75.94088

Batch 70700, train_perplexity=77.08235

Batch 70710, train_perplexity=79.413025

Batch 70720, train_perplexity=73.5445

Batch 70730, train_perplexity=71.603905

Batch 70740, train_perplexity=87.172

Batch 70750, train_perplexity=71.134476

Batch 70760, train_perplexity=73.49034

Batch 70770, train_perplexity=72.29687

Batch 70780, train_perplexity=82.09926

Batch 70790, train_perplexity=71.30258

Batch 70800, train_perplexity=73.533806

Batch 70810, train_perplexity=74.701836

Batch 70820, train_perplexity=81.874115

Batch 70830, train_perplexity=71.05335

Batch 70840, train_perplexity=84.30326

Batch 70850, train_perplexity=77.62493

Batch 70860, train_perplexity=74.164734

Batch 70870, train_perplexity=82.91114

Batch 70880, train_perplexity=79.33317

Batch 70890, train_perplexity=82.3214

Batch 70900, train_perplexity=71.71978

Batch 70910, train_perplexity=76.09442

Batch 70920, train_perplexity=79.51184

Batch 70930, train_perplexity=82.58066

Batch 70940, train_perplexity=75.62847

Batch 70950, train_perplexity=67.942535

Batch 70960, train_perplexity=72.41721

Batch 70970, train_perplexity=76.12571

Batch 70980, train_perplexity=77.31559

Batch 70990, train_perplexity=72.96828

Batch 71000, train_perplexity=78.39343

Batch 71010, train_perplexity=84.071556

Batch 71020, train_perplexity=75.55285

Batch 71030, train_perplexity=78.74913

Batch 71040, train_perplexity=79.020744

Batch 71050, train_perplexity=77.77647

Batch 71060, train_perplexity=79.78872

Batch 71070, train_perplexity=75.907425

Batch 71080, train_perplexity=75.673454

Batch 71090, train_perplexity=76.333336

Batch 71100, train_perplexity=82.45482

Batch 71110, train_perplexity=81.21605

Batch 71120, train_perplexity=77.94466

Batch 71130, train_perplexity=79.821785

Batch 71140, train_perplexity=83.16689

Batch 71150, train_perplexity=76.04223

Batch 71160, train_perplexity=80.183205

Batch 71170, train_perplexity=72.79455

Batch 71180, train_perplexity=76.69446

Batch 71190, train_perplexity=73.57319

Batch 71200, train_perplexity=73.3694

Batch 71210, train_perplexity=78.82276

Batch 71220, train_perplexity=78.58528

Batch 71230, train_perplexity=76.995514

Batch 71240, train_perplexity=81.545425

Batch 71250, train_perplexity=75.52692

Batch 71260, train_perplexity=74.22544

Batch 71270, train_perplexity=77.377625

Batch 71280, train_perplexity=80.95085

Batch 71290, train_perplexity=78.37676

Batch 71300, train_perplexity=74.294914

Batch 71310, train_perplexity=76.38052

Batch 71320, train_perplexity=80.141426

Batch 71330, train_perplexity=75.60882

Batch 71340, train_perplexity=77.43118

Batch 71350, train_perplexity=71.68617

Batch 71360, train_perplexity=76.61799

Batch 71370, train_perplexity=75.949135

Batch 71380, train_perplexity=78.322586

Batch 71390, train_perplexity=67.67223

Batch 71400, train_perplexity=82.69674

Batch 71410, train_perplexity=71.88549

Batch 71420, train_perplexity=73.319984

Batch 71430, train_perplexity=76.08985

Batch 71440, train_perplexity=82.66354

Batch 71450, train_perplexity=80.576294

Batch 71460, train_perplexity=74.93641

Batch 71470, train_perplexity=73.67507

Batch 71480, train_perplexity=77.20551

Batch 71490, train_perplexity=71.06776

Batch 71500, train_perplexity=75.70327

Batch 71510, train_perplexity=77.8251

Batch 71520, train_perplexity=76.685165

Batch 71530, train_perplexity=77.2297

Batch 71540, train_perplexity=78.49203

Batch 71550, train_perplexity=84.09778

Batch 71560, train_perplexity=81.5003

Batch 71570, train_perplexity=74.35566

Batch 71580, train_perplexity=78.13187

Batch 71590, train_perplexity=77.3021

Batch 71600, train_perplexity=69.52609

Batch 71610, train_perplexity=73.06939

Batch 71620, train_perplexity=70.338394

Batch 71630, train_perplexity=77.80778

Batch 71640, train_perplexity=75.771164

Batch 71650, train_perplexity=83.92824

Batch 71660, train_perplexity=75.42557

Batch 71670, train_perplexity=74.64085

Batch 71680, train_perplexity=79.75939

Batch 71690, train_perplexity=74.30898

Batch 71700, train_perplexity=76.41469

Batch 71710, train_perplexity=71.546844

Batch 71720, train_perplexity=69.591064

Batch 71730, train_perplexity=77.326096

Batch 71740, train_perplexity=72.10854

Batch 71750, train_perplexity=76.66733

Batch 71760, train_perplexity=71.15005

Batch 71770, train_perplexity=74.41557

Batch 71780, train_perplexity=69.80433

Batch 71790, train_perplexity=74.29562

Batch 71800, train_perplexity=75.94993

Batch 71810, train_perplexity=67.51546

Batch 71820, train_perplexity=78.396904

Batch 71830, train_perplexity=80.10429

Batch 71840, train_perplexity=78.25714

Batch 71850, train_perplexity=72.122574

Batch 71860, train_perplexity=74.211784

Batch 71870, train_perplexity=78.857574

Batch 71880, train_perplexity=75.31857

Batch 71890, train_perplexity=80.35985

Batch 71900, train_perplexity=82.67427

Batch 71910, train_perplexity=76.49883

Batch 71920, train_perplexity=74.71099

Batch 71930, train_perplexity=81.065575

Batch 71940, train_perplexity=77.63952

Batch 71950, train_perplexity=70.17026

Batch 71960, train_perplexity=75.60605

Batch 71970, train_perplexity=70.82823

Batch 71980, train_perplexity=81.44509

Batch 71990, train_perplexity=71.09792

Batch 72000, train_perplexity=71.11467

Batch 72010, train_perplexity=73.30299

Batch 72020, train_perplexity=71.448074

Batch 72030, train_perplexity=82.68515

Batch 72040, train_perplexity=78.51965

Batch 72050, train_perplexity=80.514496

Batch 72060, train_perplexity=78.82216

Batch 72070, train_perplexity=80.40354

Batch 72080, train_perplexity=82.220276

Batch 72090, train_perplexity=75.5074

Batch 72100, train_perplexity=78.83675

Batch 72110, train_perplexity=77.992355

Batch 72120, train_perplexity=68.51392

Batch 72130, train_perplexity=76.16132

Batch 72140, train_perplexity=78.04514

Batch 72150, train_perplexity=74.27005

Batch 72160, train_perplexity=77.88588

Batch 72170, train_perplexity=79.480385

Batch 72180, train_perplexity=70.08188

Batch 72190, train_perplexity=74.21259

Batch 72200, train_perplexity=74.912796

Batch 72210, train_perplexity=73.04187

Batch 72220, train_perplexity=79.92737

Batch 72230, train_perplexity=83.63557

Batch 72240, train_perplexity=78.293465

Batch 72250, train_perplexity=74.23054

Batch 72260, train_perplexity=70.50825

Batch 72270, train_perplexity=75.60515

Batch 72280, train_perplexity=80.49388

Batch 72290, train_perplexity=79.77853

Batch 72300, train_perplexity=76.430176
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 72310, train_perplexity=68.047745

Batch 72320, train_perplexity=82.81868

Batch 72330, train_perplexity=71.36353

Batch 72340, train_perplexity=87.21241

Batch 72350, train_perplexity=74.92512

Batch 72360, train_perplexity=74.80186

Batch 72370, train_perplexity=76.0266

Batch 72380, train_perplexity=77.13578

Batch 72390, train_perplexity=76.97011

Batch 72400, train_perplexity=80.8102

Batch 72410, train_perplexity=72.46882

Batch 72420, train_perplexity=76.34047

Batch 72430, train_perplexity=79.1954

Batch 72440, train_perplexity=82.94871

Batch 72450, train_perplexity=76.803185

Batch 72460, train_perplexity=74.66961

Batch 72470, train_perplexity=82.963974

Batch 72480, train_perplexity=78.94685

Batch 72490, train_perplexity=77.031136

Batch 72500, train_perplexity=74.905975

Batch 72510, train_perplexity=74.79768

Batch 72520, train_perplexity=70.70669

Batch 72530, train_perplexity=75.543884

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00012-of-00100
Loaded 305594 sentences.
Finished loading
Batch 72540, train_perplexity=81.81986

Batch 72550, train_perplexity=84.00327

Batch 72560, train_perplexity=77.99328

Batch 72570, train_perplexity=75.320946

Batch 72580, train_perplexity=85.05291

Batch 72590, train_perplexity=77.41313

Batch 72600, train_perplexity=75.35723

Batch 72610, train_perplexity=78.19385

Batch 72620, train_perplexity=75.72298

Batch 72630, train_perplexity=72.70619

Batch 72640, train_perplexity=79.625595

Batch 72650, train_perplexity=74.19246

Batch 72660, train_perplexity=73.894684

Batch 72670, train_perplexity=87.92837

Batch 72680, train_perplexity=78.271996

Batch 72690, train_perplexity=71.83721

Batch 72700, train_perplexity=76.25628

Batch 72710, train_perplexity=78.38274

Batch 72720, train_perplexity=73.473206

Batch 72730, train_perplexity=77.545204

Batch 72740, train_perplexity=70.8179

Batch 72750, train_perplexity=84.9955

Batch 72760, train_perplexity=67.99504

Batch 72770, train_perplexity=76.930664

Batch 72780, train_perplexity=75.74313

Batch 72790, train_perplexity=73.27273

Batch 72800, train_perplexity=76.94677

Batch 72810, train_perplexity=74.33156

Batch 72820, train_perplexity=78.60855

Batch 72830, train_perplexity=80.138824

Batch 72840, train_perplexity=74.514244

Batch 72850, train_perplexity=80.854294

Batch 72860, train_perplexity=80.32361

Batch 72870, train_perplexity=76.75889

Batch 72880, train_perplexity=81.163124

Batch 72890, train_perplexity=83.327065

Batch 72900, train_perplexity=69.30657

Batch 72910, train_perplexity=69.282715

Batch 72920, train_perplexity=76.51817

Batch 72930, train_perplexity=78.43127

Batch 72940, train_perplexity=78.548904

Batch 72950, train_perplexity=69.638435

Batch 72960, train_perplexity=71.183136

Batch 72970, train_perplexity=75.1567

Batch 72980, train_perplexity=88.40014

Batch 72990, train_perplexity=73.88525

Batch 73000, train_perplexity=70.84668

Batch 73010, train_perplexity=76.92549

Batch 73020, train_perplexity=76.09392

Batch 73030, train_perplexity=72.90908

Batch 73040, train_perplexity=72.45279

Batch 73050, train_perplexity=76.785645

Batch 73060, train_perplexity=76.908844

Batch 73070, train_perplexity=67.616554

Batch 73080, train_perplexity=74.91651

Batch 73090, train_perplexity=86.106346

Batch 73100, train_perplexity=85.26423

Batch 73110, train_perplexity=75.16129

Batch 73120, train_perplexity=85.26301

Batch 73130, train_perplexity=78.83802

Batch 73140, train_perplexity=73.01095

Batch 73150, train_perplexity=86.24404

Batch 73160, train_perplexity=80.17881

Batch 73170, train_perplexity=74.48078

Batch 73180, train_perplexity=85.56628

Batch 73190, train_perplexity=74.6011

Batch 73200, train_perplexity=73.16017

Batch 73210, train_perplexity=70.224014

Batch 73220, train_perplexity=75.79104

Batch 73230, train_perplexity=74.07821

Batch 73240, train_perplexity=79.42681

Batch 73250, train_perplexity=71.11087

Batch 73260, train_perplexity=76.107925

Batch 73270, train_perplexity=74.93105

Batch 73280, train_perplexity=77.99964

Batch 73290, train_perplexity=76.611046

Batch 73300, train_perplexity=75.58341

Batch 73310, train_perplexity=78.23916

Batch 73320, train_perplexity=71.65907

Batch 73330, train_perplexity=79.93335

Batch 73340, train_perplexity=80.21758

Batch 73350, train_perplexity=73.2821

Batch 73360, train_perplexity=79.76552

Batch 73370, train_perplexity=78.54722

Batch 73380, train_perplexity=68.29991

Batch 73390, train_perplexity=78.09186

Batch 73400, train_perplexity=72.380135

Batch 73410, train_perplexity=75.13499

Batch 73420, train_perplexity=69.411476

Batch 73430, train_perplexity=87.2808

Batch 73440, train_perplexity=76.93892

Batch 73450, train_perplexity=80.950195

Batch 73460, train_perplexity=79.587326

Batch 73470, train_perplexity=72.0352

Batch 73480, train_perplexity=73.84544

Batch 73490, train_perplexity=73.92661

Batch 73500, train_perplexity=77.47687

Batch 73510, train_perplexity=75.01471

Batch 73520, train_perplexity=74.22806

Batch 73530, train_perplexity=75.72053

Batch 73540, train_perplexity=81.03083

Batch 73550, train_perplexity=83.26212

Batch 73560, train_perplexity=76.42238

Batch 73570, train_perplexity=70.88689

Batch 73580, train_perplexity=76.20546

Batch 73590, train_perplexity=82.28946

Batch 73600, train_perplexity=74.47333

Batch 73610, train_perplexity=81.34701

Batch 73620, train_perplexity=68.69031

Batch 73630, train_perplexity=73.511406

Batch 73640, train_perplexity=78.183334

Batch 73650, train_perplexity=75.84837

Batch 73660, train_perplexity=87.309525

Batch 73670, train_perplexity=63.953632

Batch 73680, train_perplexity=74.37623

Batch 73690, train_perplexity=71.2639

Batch 73700, train_perplexity=76.98483

Batch 73710, train_perplexity=77.61775

Batch 73720, train_perplexity=78.71564

Batch 73730, train_perplexity=77.576126

Batch 73740, train_perplexity=72.83541

Batch 73750, train_perplexity=80.48157

Batch 73760, train_perplexity=73.5567

Batch 73770, train_perplexity=69.13408

Batch 73780, train_perplexity=72.87175

Batch 73790, train_perplexity=70.53878

Batch 73800, train_perplexity=87.736755

Batch 73810, train_perplexity=73.39372

Batch 73820, train_perplexity=76.87126

Batch 73830, train_perplexity=73.79324

Batch 73840, train_perplexity=74.67883

Batch 73850, train_perplexity=76.06852

Batch 73860, train_perplexity=80.74064

Batch 73870, train_perplexity=86.67044

Batch 73880, train_perplexity=78.49225

Batch 73890, train_perplexity=78.82622

Batch 73900, train_perplexity=78.276474

Batch 73910, train_perplexity=81.33087

Batch 73920, train_perplexity=71.5798

Batch 73930, train_perplexity=83.70914

Batch 73940, train_perplexity=72.50539

Batch 73950, train_perplexity=77.15682

Batch 73960, train_perplexity=72.983444

Batch 73970, train_perplexity=75.35982

Batch 73980, train_perplexity=78.48297

Batch 73990, train_perplexity=82.96801

Batch 74000, train_perplexity=71.22748

Batch 74010, train_perplexity=67.65048

Batch 74020, train_perplexity=67.64642

Batch 74030, train_perplexity=74.1128

Batch 74040, train_perplexity=69.96775

Batch 74050, train_perplexity=75.44402

Batch 74060, train_perplexity=72.1375

Batch 74070, train_perplexity=78.09849

Batch 74080, train_perplexity=79.42628

Batch 74090, train_perplexity=73.96392

Batch 74100, train_perplexity=68.787094

Batch 74110, train_perplexity=73.7055

Batch 74120, train_perplexity=68.853516

Batch 74130, train_perplexity=77.3344

Batch 74140, train_perplexity=74.78755

Batch 74150, train_perplexity=75.74739

Batch 74160, train_perplexity=81.95004

Batch 74170, train_perplexity=71.32781

Batch 74180, train_perplexity=74.17103

Batch 74190, train_perplexity=70.11564

Batch 74200, train_perplexity=78.01515

Batch 74210, train_perplexity=76.47801

Batch 74220, train_perplexity=68.5654

Batch 74230, train_perplexity=77.0448

Batch 74240, train_perplexity=81.28532
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 74250, train_perplexity=83.165665

Batch 74260, train_perplexity=79.46386

Batch 74270, train_perplexity=73.11616

Batch 74280, train_perplexity=83.81962

Batch 74290, train_perplexity=74.94363

Batch 74300, train_perplexity=74.01134

Batch 74310, train_perplexity=78.7453

Batch 74320, train_perplexity=75.78576

Batch 74330, train_perplexity=85.79491

Batch 74340, train_perplexity=77.07015

Batch 74350, train_perplexity=74.0633

Batch 74360, train_perplexity=85.712875

Batch 74370, train_perplexity=72.08798

Batch 74380, train_perplexity=73.73116

Batch 74390, train_perplexity=74.24342

Batch 74400, train_perplexity=66.53373

Batch 74410, train_perplexity=75.820244

Batch 74420, train_perplexity=78.92521

Batch 74430, train_perplexity=79.25384

Batch 74440, train_perplexity=73.527245

Batch 74450, train_perplexity=76.781105

Batch 74460, train_perplexity=71.13367

Batch 74470, train_perplexity=73.604355

Batch 74480, train_perplexity=74.95278

Batch 74490, train_perplexity=80.76312

Batch 74500, train_perplexity=77.950676

Batch 74510, train_perplexity=71.49934

Batch 74520, train_perplexity=77.63626

Batch 74530, train_perplexity=81.33429

Batch 74540, train_perplexity=78.28719

Batch 74550, train_perplexity=78.52085

Batch 74560, train_perplexity=73.83671

Batch 74570, train_perplexity=86.063774

Batch 74580, train_perplexity=81.607895

Batch 74590, train_perplexity=81.68163

Batch 74600, train_perplexity=76.34877

Batch 74610, train_perplexity=86.94898

Batch 74620, train_perplexity=80.821144

Batch 74630, train_perplexity=79.83286

Batch 74640, train_perplexity=73.41258

Batch 74650, train_perplexity=76.14074

Batch 74660, train_perplexity=70.63282

Batch 74670, train_perplexity=77.20367

Batch 74680, train_perplexity=78.597984

Batch 74690, train_perplexity=77.28684

Batch 74700, train_perplexity=75.29875

Batch 74710, train_perplexity=72.2336

Batch 74720, train_perplexity=80.17781

Batch 74730, train_perplexity=77.53419

Batch 74740, train_perplexity=79.31675

Batch 74750, train_perplexity=74.666336

Batch 74760, train_perplexity=74.235

Batch 74770, train_perplexity=75.32483

Batch 74780, train_perplexity=68.224846

Batch 74790, train_perplexity=83.064835

Batch 74800, train_perplexity=75.06688

Batch 74810, train_perplexity=82.57518

Batch 74820, train_perplexity=72.36847

Batch 74830, train_perplexity=69.33718

Batch 74840, train_perplexity=69.54943

Batch 74850, train_perplexity=86.6518

Batch 74860, train_perplexity=71.30254

Batch 74870, train_perplexity=80.13608

Batch 74880, train_perplexity=72.2481

Batch 74890, train_perplexity=78.643456

Batch 74900, train_perplexity=72.51172

Batch 74910, train_perplexity=75.78453

Batch 74920, train_perplexity=75.35213

Batch 74930, train_perplexity=72.172455

Batch 74940, train_perplexity=74.21734

Batch 74950, train_perplexity=75.59707

Batch 74960, train_perplexity=84.55932

Batch 74970, train_perplexity=78.706856

Batch 74980, train_perplexity=83.00596

Batch 74990, train_perplexity=69.22816

Batch 75000, train_perplexity=76.016884

Batch 75010, train_perplexity=73.29586

Batch 75020, train_perplexity=73.5025

Batch 75030, train_perplexity=85.592964

Batch 75040, train_perplexity=73.23256

Batch 75050, train_perplexity=73.10769

Batch 75060, train_perplexity=75.42683

Batch 75070, train_perplexity=68.4839

Batch 75080, train_perplexity=74.87062

Batch 75090, train_perplexity=74.114105

Batch 75100, train_perplexity=77.2735

Batch 75110, train_perplexity=71.54718

Batch 75120, train_perplexity=76.345894

Batch 75130, train_perplexity=86.497284

Batch 75140, train_perplexity=72.063614

Batch 75150, train_perplexity=72.05173

Batch 75160, train_perplexity=72.616035

Batch 75170, train_perplexity=78.47549

Batch 75180, train_perplexity=69.03334

Batch 75190, train_perplexity=85.139824

Batch 75200, train_perplexity=68.299065

Batch 75210, train_perplexity=68.04246

Batch 75220, train_perplexity=81.96278

Batch 75230, train_perplexity=73.023476

Batch 75240, train_perplexity=73.26476

Batch 75250, train_perplexity=72.5084

Batch 75260, train_perplexity=77.64422

Batch 75270, train_perplexity=80.62234

Batch 75280, train_perplexity=74.93176

Batch 75290, train_perplexity=75.8847

Batch 75300, train_perplexity=66.69659

Batch 75310, train_perplexity=72.81187

Batch 75320, train_perplexity=68.95856

Batch 75330, train_perplexity=71.43884

Batch 75340, train_perplexity=75.889984

Batch 75350, train_perplexity=76.53576

Batch 75360, train_perplexity=74.816376

Batch 75370, train_perplexity=73.860306

Batch 75380, train_perplexity=78.4208

Batch 75390, train_perplexity=71.50039

Batch 75400, train_perplexity=79.188866

Batch 75410, train_perplexity=79.80428

Batch 75420, train_perplexity=73.58639

Batch 75430, train_perplexity=76.74172

Batch 75440, train_perplexity=71.69605

Batch 75450, train_perplexity=79.27837

Batch 75460, train_perplexity=72.902824

Batch 75470, train_perplexity=77.15929

Batch 75480, train_perplexity=72.461876

Batch 75490, train_perplexity=70.245285

Batch 75500, train_perplexity=71.65726

Batch 75510, train_perplexity=69.012276

Batch 75520, train_perplexity=74.63195

Batch 75530, train_perplexity=77.15428

Batch 75540, train_perplexity=70.60716

Batch 75550, train_perplexity=74.743774

Batch 75560, train_perplexity=73.9129

Batch 75570, train_perplexity=73.86249

Batch 75580, train_perplexity=75.13843

Batch 75590, train_perplexity=78.76869

Batch 75600, train_perplexity=72.78646

Batch 75610, train_perplexity=80.34667

Batch 75620, train_perplexity=77.58327

Batch 75630, train_perplexity=74.31008

Batch 75640, train_perplexity=83.43823

Batch 75650, train_perplexity=81.12679

Batch 75660, train_perplexity=75.93827

Batch 75670, train_perplexity=81.240295

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00004-of-00100
Loaded 306362 sentences.
Finished loading
Batch 75680, train_perplexity=80.33571

Batch 75690, train_perplexity=73.09116

Batch 75700, train_perplexity=71.05976

Batch 75710, train_perplexity=76.15515

Batch 75720, train_perplexity=76.65808

Batch 75730, train_perplexity=70.39477

Batch 75740, train_perplexity=72.71257

Batch 75750, train_perplexity=75.893745

Batch 75760, train_perplexity=75.35213

Batch 75770, train_perplexity=77.24753

Batch 75780, train_perplexity=86.21764

Batch 75790, train_perplexity=81.626656

Batch 75800, train_perplexity=73.32016

Batch 75810, train_perplexity=84.47557

Batch 75820, train_perplexity=78.26532

Batch 75830, train_perplexity=76.14455

Batch 75840, train_perplexity=78.24069

Batch 75850, train_perplexity=74.960495

Batch 75860, train_perplexity=79.89258

Batch 75870, train_perplexity=78.00206

Batch 75880, train_perplexity=75.73757

Batch 75890, train_perplexity=86.027016

Batch 75900, train_perplexity=79.37002

Batch 75910, train_perplexity=77.56869

Batch 75920, train_perplexity=75.37427

Batch 75930, train_perplexity=73.54527

Batch 75940, train_perplexity=70.0601

Batch 75950, train_perplexity=80.75558

Batch 75960, train_perplexity=69.212776

Batch 75970, train_perplexity=77.104706

Batch 75980, train_perplexity=82.792854

Batch 75990, train_perplexity=74.7297

Batch 76000, train_perplexity=77.54809

Batch 76010, train_perplexity=81.38918

Batch 76020, train_perplexity=79.06624

Batch 76030, train_perplexity=71.78503

Batch 76040, train_perplexity=75.18581

Batch 76050, train_perplexity=69.76553

Batch 76060, train_perplexity=77.07044

Batch 76070, train_perplexity=82.21047

Batch 76080, train_perplexity=85.921326

Batch 76090, train_perplexity=84.421936

Batch 76100, train_perplexity=73.959335

Batch 76110, train_perplexity=76.5011

Batch 76120, train_perplexity=73.50545

Batch 76130, train_perplexity=76.93253

Batch 76140, train_perplexity=71.98033

Batch 76150, train_perplexity=70.15812

Batch 76160, train_perplexity=78.200554

Batch 76170, train_perplexity=68.404205

Batch 76180, train_perplexity=81.22368
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 76190, train_perplexity=74.633514

Batch 76200, train_perplexity=72.896355

Batch 76210, train_perplexity=75.41863

Batch 76220, train_perplexity=74.60889

Batch 76230, train_perplexity=71.50326

Batch 76240, train_perplexity=89.948395

Batch 76250, train_perplexity=72.72523

Batch 76260, train_perplexity=72.9584

Batch 76270, train_perplexity=77.88892

Batch 76280, train_perplexity=74.10245

Batch 76290, train_perplexity=70.1885

Batch 76300, train_perplexity=77.271324

Batch 76310, train_perplexity=79.46708

Batch 76320, train_perplexity=73.4482

Batch 76330, train_perplexity=81.47706

Batch 76340, train_perplexity=71.6711

Batch 76350, train_perplexity=81.861855

Batch 76360, train_perplexity=78.03647

Batch 76370, train_perplexity=76.45373

Batch 76380, train_perplexity=79.38698

Batch 76390, train_perplexity=72.71618

Batch 76400, train_perplexity=70.023026

Batch 76410, train_perplexity=71.82913

Batch 76420, train_perplexity=85.38222

Batch 76430, train_perplexity=66.16096

Batch 76440, train_perplexity=78.612564

Batch 76450, train_perplexity=77.45596

Batch 76460, train_perplexity=72.15518

Batch 76470, train_perplexity=84.76076

Batch 76480, train_perplexity=73.73211

Batch 76490, train_perplexity=75.79884

Batch 76500, train_perplexity=73.65196

Batch 76510, train_perplexity=77.72742

Batch 76520, train_perplexity=77.81119

Batch 76530, train_perplexity=78.490234

Batch 76540, train_perplexity=82.175156

Batch 76550, train_perplexity=77.33048

Batch 76560, train_perplexity=81.799965

Batch 76570, train_perplexity=76.49479

Batch 76580, train_perplexity=74.00202

Batch 76590, train_perplexity=77.94417

Batch 76600, train_perplexity=80.65348

Batch 76610, train_perplexity=73.56772

Batch 76620, train_perplexity=77.29738

Batch 76630, train_perplexity=73.181595

Batch 76640, train_perplexity=78.32475

Batch 76650, train_perplexity=73.12393

Batch 76660, train_perplexity=71.91707

Batch 76670, train_perplexity=85.093155

Batch 76680, train_perplexity=69.40671

Batch 76690, train_perplexity=86.906784

Batch 76700, train_perplexity=87.84698

Batch 76710, train_perplexity=69.793915

Batch 76720, train_perplexity=75.65527

Batch 76730, train_perplexity=80.1253

Batch 76740, train_perplexity=79.07755

Batch 76750, train_perplexity=77.64026

Batch 76760, train_perplexity=77.5255

Batch 76770, train_perplexity=72.01968

Batch 76780, train_perplexity=64.484

Batch 76790, train_perplexity=78.080284

Batch 76800, train_perplexity=75.94088

Batch 76810, train_perplexity=77.37604

Batch 76820, train_perplexity=76.4097

Batch 76830, train_perplexity=67.87321

Batch 76840, train_perplexity=78.49592

Batch 76850, train_perplexity=72.77959

Batch 76860, train_perplexity=75.22404

Batch 76870, train_perplexity=70.24663

Batch 76880, train_perplexity=75.61044

Batch 76890, train_perplexity=80.18102

Batch 76900, train_perplexity=71.34284

Batch 76910, train_perplexity=71.73103

Batch 76920, train_perplexity=68.13739

Batch 76930, train_perplexity=77.44758

Batch 76940, train_perplexity=75.25295

Batch 76950, train_perplexity=70.82614

Batch 76960, train_perplexity=91.34523

Batch 76970, train_perplexity=77.342804

Batch 76980, train_perplexity=71.17214

Batch 76990, train_perplexity=76.429634

Batch 77000, train_perplexity=81.5083

Batch 77010, train_perplexity=77.740944

Batch 77020, train_perplexity=74.90533

Batch 77030, train_perplexity=72.38383

Batch 77040, train_perplexity=71.86658

Batch 77050, train_perplexity=70.37241

Batch 77060, train_perplexity=80.28677

Batch 77070, train_perplexity=69.72306

Batch 77080, train_perplexity=73.58884

Batch 77090, train_perplexity=78.786766

Batch 77100, train_perplexity=75.60334

Batch 77110, train_perplexity=77.34841

Batch 77120, train_perplexity=74.43186

Batch 77130, train_perplexity=85.603165

Batch 77140, train_perplexity=82.69394

Batch 77150, train_perplexity=75.67717

Batch 77160, train_perplexity=75.063225

Batch 77170, train_perplexity=69.44349

Batch 77180, train_perplexity=76.20997

Batch 77190, train_perplexity=68.16534

Batch 77200, train_perplexity=69.32806

Batch 77210, train_perplexity=75.08334

Batch 77220, train_perplexity=75.450356

Batch 77230, train_perplexity=85.6634

Batch 77240, train_perplexity=83.319435

Batch 77250, train_perplexity=68.96395

Batch 77260, train_perplexity=73.97976

Batch 77270, train_perplexity=77.74777

Batch 77280, train_perplexity=75.9726

Batch 77290, train_perplexity=71.13773

Batch 77300, train_perplexity=69.649796

Batch 77310, train_perplexity=80.080536

Batch 77320, train_perplexity=74.59666

Batch 77330, train_perplexity=71.25792

Batch 77340, train_perplexity=74.72033

Batch 77350, train_perplexity=67.89493

Batch 77360, train_perplexity=74.61821

Batch 77370, train_perplexity=81.211555

Batch 77380, train_perplexity=76.7096

Batch 77390, train_perplexity=81.806366

Batch 77400, train_perplexity=74.25086

Batch 77410, train_perplexity=75.66696

Batch 77420, train_perplexity=77.66788

Batch 77430, train_perplexity=80.92503

Batch 77440, train_perplexity=83.99414

Batch 77450, train_perplexity=69.909996

Batch 77460, train_perplexity=73.86759

Batch 77470, train_perplexity=72.46046

Batch 77480, train_perplexity=78.535866

Batch 77490, train_perplexity=76.044586

Batch 77500, train_perplexity=80.44362

Batch 77510, train_perplexity=73.58765

Batch 77520, train_perplexity=77.013725

Batch 77530, train_perplexity=81.67945

Batch 77540, train_perplexity=73.080956

Batch 77550, train_perplexity=75.6994

Batch 77560, train_perplexity=73.55905

Batch 77570, train_perplexity=76.919624

Batch 77580, train_perplexity=82.85032

Batch 77590, train_perplexity=73.725746

Batch 77600, train_perplexity=80.654175

Batch 77610, train_perplexity=72.976906

Batch 77620, train_perplexity=81.1788

Batch 77630, train_perplexity=74.14914

Batch 77640, train_perplexity=75.15194

Batch 77650, train_perplexity=75.55912

Batch 77660, train_perplexity=82.85099

Batch 77670, train_perplexity=81.96626

Batch 77680, train_perplexity=78.70588

Batch 77690, train_perplexity=78.58505

Batch 77700, train_perplexity=74.04664

Batch 77710, train_perplexity=78.10866

Batch 77720, train_perplexity=76.80593

Batch 77730, train_perplexity=72.08214

Batch 77740, train_perplexity=71.07067

Batch 77750, train_perplexity=79.23317

Batch 77760, train_perplexity=72.62224

Batch 77770, train_perplexity=76.24384

Batch 77780, train_perplexity=80.40216

Batch 77790, train_perplexity=76.71377

Batch 77800, train_perplexity=78.73073

Batch 77810, train_perplexity=70.116714

Batch 77820, train_perplexity=73.836426

Batch 77830, train_perplexity=80.22661

Batch 77840, train_perplexity=79.65316

Batch 77850, train_perplexity=83.28154

Batch 77860, train_perplexity=81.02055

Batch 77870, train_perplexity=70.40846

Batch 77880, train_perplexity=80.273254

Batch 77890, train_perplexity=70.62878

Batch 77900, train_perplexity=75.22604

Batch 77910, train_perplexity=73.81682

Batch 77920, train_perplexity=70.707664

Batch 77930, train_perplexity=77.133606

Batch 77940, train_perplexity=72.63259

Batch 77950, train_perplexity=76.612434

Batch 77960, train_perplexity=75.10064

Batch 77970, train_perplexity=78.73591

Batch 77980, train_perplexity=71.62214

Batch 77990, train_perplexity=82.71268

Batch 78000, train_perplexity=72.97343

Batch 78010, train_perplexity=81.513084

Batch 78020, train_perplexity=77.496155

Batch 78030, train_perplexity=80.36905

Batch 78040, train_perplexity=65.48848

Batch 78050, train_perplexity=83.15111

Batch 78060, train_perplexity=77.19513

Batch 78070, train_perplexity=73.691055

Batch 78080, train_perplexity=74.986595

Batch 78090, train_perplexity=78.32296

Batch 78100, train_perplexity=71.857155

Batch 78110, train_perplexity=69.186775

Batch 78120, train_perplexity=72.54457

Batch 78130, train_perplexity=78.62062

Batch 78140, train_perplexity=73.44105

Batch 78150, train_perplexity=74.6736

Batch 78160, train_perplexity=83.08623

Batch 78170, train_perplexity=72.175896

Batch 78180, train_perplexity=72.434616

Batch 78190, train_perplexity=72.51109

Batch 78200, train_perplexity=78.985146

Batch 78210, train_perplexity=72.40513

Batch 78220, train_perplexity=70.00279
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 78230, train_perplexity=69.787094

Batch 78240, train_perplexity=82.097885

Batch 78250, train_perplexity=74.325745

Batch 78260, train_perplexity=68.38888

Batch 78270, train_perplexity=78.10597

Batch 78280, train_perplexity=68.79805

Batch 78290, train_perplexity=74.255104

Batch 78300, train_perplexity=70.71997

Batch 78310, train_perplexity=79.54446

Batch 78320, train_perplexity=72.36837

Batch 78330, train_perplexity=81.399124

Batch 78340, train_perplexity=73.92767

Batch 78350, train_perplexity=72.67957

Batch 78360, train_perplexity=73.40114

Batch 78370, train_perplexity=69.32938

Batch 78380, train_perplexity=71.50499

Batch 78390, train_perplexity=82.7376

Batch 78400, train_perplexity=77.94963

Batch 78410, train_perplexity=83.376144

Batch 78420, train_perplexity=83.48758

Batch 78430, train_perplexity=71.35305

Batch 78440, train_perplexity=72.26585

Batch 78450, train_perplexity=75.447334

Batch 78460, train_perplexity=71.74581

Batch 78470, train_perplexity=67.92423

Batch 78480, train_perplexity=72.65816

Batch 78490, train_perplexity=81.85584

Batch 78500, train_perplexity=72.266846

Batch 78510, train_perplexity=76.66901

Batch 78520, train_perplexity=66.77166

Batch 78530, train_perplexity=81.92027

Batch 78540, train_perplexity=74.354706

Batch 78550, train_perplexity=72.84653

Batch 78560, train_perplexity=73.14608

Batch 78570, train_perplexity=73.910965

Batch 78580, train_perplexity=83.422676

Batch 78590, train_perplexity=82.17348

Batch 78600, train_perplexity=75.83738

Batch 78610, train_perplexity=81.55095

Batch 78620, train_perplexity=81.49039

Batch 78630, train_perplexity=74.19055

Batch 78640, train_perplexity=81.346115

Batch 78650, train_perplexity=70.77415

Batch 78660, train_perplexity=74.632484

Batch 78670, train_perplexity=75.61993

Batch 78680, train_perplexity=74.4007

Batch 78690, train_perplexity=71.93381

Batch 78700, train_perplexity=77.034035

Batch 78710, train_perplexity=70.08439

Batch 78720, train_perplexity=76.553314

Batch 78730, train_perplexity=75.0352

Batch 78740, train_perplexity=70.46442

Batch 78750, train_perplexity=75.97275

Batch 78760, train_perplexity=65.55962

Batch 78770, train_perplexity=64.69429

Batch 78780, train_perplexity=70.780464

Batch 78790, train_perplexity=72.74073

Batch 78800, train_perplexity=65.63132

Batch 78810, train_perplexity=72.19827

Batch 78820, train_perplexity=79.0186

Batch 78830, train_perplexity=77.109634

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00005-of-00100
Loaded 305714 sentences.
Finished loading
Batch 78840, train_perplexity=73.56309

Batch 78850, train_perplexity=78.75919

Batch 78860, train_perplexity=77.19436

Batch 78870, train_perplexity=76.90525

Batch 78880, train_perplexity=75.019646

Batch 78890, train_perplexity=70.2688

Batch 78900, train_perplexity=73.201836

Batch 78910, train_perplexity=81.25408

Batch 78920, train_perplexity=75.85843

Batch 78930, train_perplexity=68.25667

Batch 78940, train_perplexity=75.59887

Batch 78950, train_perplexity=80.45816

Batch 78960, train_perplexity=71.28592

Batch 78970, train_perplexity=74.782776

Batch 78980, train_perplexity=80.444885

Batch 78990, train_perplexity=77.616196

Batch 79000, train_perplexity=67.17186

Batch 79010, train_perplexity=70.78495

Batch 79020, train_perplexity=72.088875

Batch 79030, train_perplexity=75.281166

Batch 79040, train_perplexity=76.35605

Batch 79050, train_perplexity=77.19616

Batch 79060, train_perplexity=76.73184

Batch 79070, train_perplexity=74.99261

Batch 79080, train_perplexity=68.2919

Batch 79090, train_perplexity=72.78577

Batch 79100, train_perplexity=73.32558

Batch 79110, train_perplexity=72.81708

Batch 79120, train_perplexity=74.50337

Batch 79130, train_perplexity=79.23744

Batch 79140, train_perplexity=75.19173

Batch 79150, train_perplexity=77.437164

Batch 79160, train_perplexity=73.58666

Batch 79170, train_perplexity=77.46534

Batch 79180, train_perplexity=74.21942

Batch 79190, train_perplexity=81.78281

Batch 79200, train_perplexity=77.74514

Batch 79210, train_perplexity=78.062485

Batch 79220, train_perplexity=76.79484

Batch 79230, train_perplexity=75.91199

Batch 79240, train_perplexity=68.437874

Batch 79250, train_perplexity=70.25912

Batch 79260, train_perplexity=76.60849

Batch 79270, train_perplexity=74.43637

Batch 79280, train_perplexity=78.26532

Batch 79290, train_perplexity=72.20426

Batch 79300, train_perplexity=75.0933

Batch 79310, train_perplexity=80.4478

Batch 79320, train_perplexity=74.346016

Batch 79330, train_perplexity=75.31219

Batch 79340, train_perplexity=81.78792

Batch 79350, train_perplexity=81.51767

Batch 79360, train_perplexity=77.52646

Batch 79370, train_perplexity=71.19054

Batch 79380, train_perplexity=68.92598

Batch 79390, train_perplexity=76.68041

Batch 79400, train_perplexity=71.03977

Batch 79410, train_perplexity=70.61905

Batch 79420, train_perplexity=71.4274

Batch 79430, train_perplexity=72.86292

Batch 79440, train_perplexity=70.04112

Batch 79450, train_perplexity=70.40168

Batch 79460, train_perplexity=73.15369

Batch 79470, train_perplexity=73.3262

Batch 79480, train_perplexity=71.96489

Batch 79490, train_perplexity=74.70287

Batch 79500, train_perplexity=72.32783

Batch 79510, train_perplexity=77.60195

Batch 79520, train_perplexity=77.98205

Batch 79530, train_perplexity=72.063446

Batch 79540, train_perplexity=83.04211

Batch 79550, train_perplexity=75.30716

Batch 79560, train_perplexity=71.81608

Batch 79570, train_perplexity=86.581795

Batch 79580, train_perplexity=72.26326

Batch 79590, train_perplexity=75.26964

Batch 79600, train_perplexity=76.85371

Batch 79610, train_perplexity=72.386696

Batch 79620, train_perplexity=72.93147

Batch 79630, train_perplexity=76.267265

Batch 79640, train_perplexity=75.63266

Batch 79650, train_perplexity=71.94355

Batch 79660, train_perplexity=79.42708

Batch 79670, train_perplexity=78.740715

Batch 79680, train_perplexity=74.18949

Batch 79690, train_perplexity=71.951164

Batch 79700, train_perplexity=73.70705

Batch 79710, train_perplexity=79.325905

Batch 79720, train_perplexity=74.096794

Batch 79730, train_perplexity=65.71264

Batch 79740, train_perplexity=74.93241

Batch 79750, train_perplexity=67.35391

Batch 79760, train_perplexity=79.60114

Batch 79770, train_perplexity=78.8205

Batch 79780, train_perplexity=75.484184

Batch 79790, train_perplexity=82.653534

Batch 79800, train_perplexity=79.96892

Batch 79810, train_perplexity=75.94544

Batch 79820, train_perplexity=79.899055

Batch 79830, train_perplexity=68.95083

Batch 79840, train_perplexity=80.2867

Batch 79850, train_perplexity=74.09711

Batch 79860, train_perplexity=70.73677

Batch 79870, train_perplexity=78.346794

Batch 79880, train_perplexity=78.82945

Batch 79890, train_perplexity=82.1578

Batch 79900, train_perplexity=75.04648

Batch 79910, train_perplexity=79.389854

Batch 79920, train_perplexity=71.026764

Batch 79930, train_perplexity=77.87503

Batch 79940, train_perplexity=75.33417

Batch 79950, train_perplexity=75.253815

Batch 79960, train_perplexity=77.04017

Batch 79970, train_perplexity=70.1128

Batch 79980, train_perplexity=71.62535

Batch 79990, train_perplexity=73.44063

Batch 80000, train_perplexity=72.898964

Batch 80010, train_perplexity=73.737526

Batch 80020, train_perplexity=72.94058

Batch 80030, train_perplexity=82.00078

Batch 80040, train_perplexity=77.79453

Batch 80050, train_perplexity=78.38999

Batch 80060, train_perplexity=75.31746

Batch 80070, train_perplexity=80.73067

Batch 80080, train_perplexity=76.10168

Batch 80090, train_perplexity=80.63418

Batch 80100, train_perplexity=73.32939

Batch 80110, train_perplexity=75.90286

Batch 80120, train_perplexity=80.200294

Batch 80130, train_perplexity=83.05802

Batch 80140, train_perplexity=73.54373

Batch 80150, train_perplexity=76.7661

Batch 80160, train_perplexity=78.61631
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 80170, train_perplexity=78.102844

Batch 80180, train_perplexity=81.79716

Batch 80190, train_perplexity=79.8168

Batch 80200, train_perplexity=66.73091

Batch 80210, train_perplexity=73.591156

Batch 80220, train_perplexity=73.72364

Batch 80230, train_perplexity=71.813515

Batch 80240, train_perplexity=76.45074

Batch 80250, train_perplexity=74.69368

Batch 80260, train_perplexity=67.60727

Batch 80270, train_perplexity=78.41313

Batch 80280, train_perplexity=73.61362

Batch 80290, train_perplexity=67.57833

Batch 80300, train_perplexity=74.783775

Batch 80310, train_perplexity=72.55007

Batch 80320, train_perplexity=65.6846

Batch 80330, train_perplexity=74.07093

Batch 80340, train_perplexity=71.47789

Batch 80350, train_perplexity=78.86637

Batch 80360, train_perplexity=74.344246

Batch 80370, train_perplexity=75.18502

Batch 80380, train_perplexity=82.37174

Batch 80390, train_perplexity=83.70714

Batch 80400, train_perplexity=73.19709

Batch 80410, train_perplexity=78.56175

Batch 80420, train_perplexity=71.13692

Batch 80430, train_perplexity=72.65587

Batch 80440, train_perplexity=72.27288

Batch 80450, train_perplexity=69.54691

Batch 80460, train_perplexity=67.95543

Batch 80470, train_perplexity=74.76281

Batch 80480, train_perplexity=73.69046

Batch 80490, train_perplexity=75.3568

Batch 80500, train_perplexity=69.508224

Batch 80510, train_perplexity=77.187546

Batch 80520, train_perplexity=75.89736

Batch 80530, train_perplexity=80.364944

Batch 80540, train_perplexity=70.33719

Batch 80550, train_perplexity=69.599396

Batch 80560, train_perplexity=75.52868

Batch 80570, train_perplexity=78.26651

Batch 80580, train_perplexity=70.84073

Batch 80590, train_perplexity=70.232895

Batch 80600, train_perplexity=71.10199

Batch 80610, train_perplexity=76.76035

Batch 80620, train_perplexity=78.7735

Batch 80630, train_perplexity=79.06364

Batch 80640, train_perplexity=68.69204

Batch 80650, train_perplexity=81.766624

Batch 80660, train_perplexity=78.996185

Batch 80670, train_perplexity=75.90337

Batch 80680, train_perplexity=85.18832

Batch 80690, train_perplexity=76.46412

Batch 80700, train_perplexity=83.61778

Batch 80710, train_perplexity=84.12305

Batch 80720, train_perplexity=65.3445

Batch 80730, train_perplexity=69.340614

Batch 80740, train_perplexity=74.954926

Batch 80750, train_perplexity=70.08325

Batch 80760, train_perplexity=79.63539

Batch 80770, train_perplexity=68.082504

Batch 80780, train_perplexity=72.64582

Batch 80790, train_perplexity=68.64633

Batch 80800, train_perplexity=75.654625

Batch 80810, train_perplexity=77.95499

Batch 80820, train_perplexity=73.650345

Batch 80830, train_perplexity=73.95666

Batch 80840, train_perplexity=80.46077

Batch 80850, train_perplexity=76.7881

Batch 80860, train_perplexity=71.94814

Batch 80870, train_perplexity=68.98694

Batch 80880, train_perplexity=82.60216

Batch 80890, train_perplexity=76.10139

Batch 80900, train_perplexity=76.91926

Batch 80910, train_perplexity=73.80091

Batch 80920, train_perplexity=80.89776

Batch 80930, train_perplexity=67.75702

Batch 80940, train_perplexity=81.81366

Batch 80950, train_perplexity=78.854225

Batch 80960, train_perplexity=76.46612

Batch 80970, train_perplexity=71.70135

Batch 80980, train_perplexity=79.1738

Batch 80990, train_perplexity=81.74756

Batch 81000, train_perplexity=75.741684

Batch 81010, train_perplexity=72.59613

Batch 81020, train_perplexity=75.4059

Batch 81030, train_perplexity=76.1101

Batch 81040, train_perplexity=72.44775

Batch 81050, train_perplexity=78.32288

Batch 81060, train_perplexity=67.540184

Batch 81070, train_perplexity=69.514656

Batch 81080, train_perplexity=77.80533

Batch 81090, train_perplexity=75.74255

Batch 81100, train_perplexity=71.18249

Batch 81110, train_perplexity=74.88951

Batch 81120, train_perplexity=69.96495

Batch 81130, train_perplexity=79.932014

Batch 81140, train_perplexity=74.90197

Batch 81150, train_perplexity=78.052956

Batch 81160, train_perplexity=72.42246

Batch 81170, train_perplexity=71.31638

Batch 81180, train_perplexity=75.66663

Batch 81190, train_perplexity=75.37577

Batch 81200, train_perplexity=75.81941

Batch 81210, train_perplexity=73.257286

Batch 81220, train_perplexity=71.68026

Batch 81230, train_perplexity=69.39678

Batch 81240, train_perplexity=62.247883

Batch 81250, train_perplexity=77.86627

Batch 81260, train_perplexity=75.33007

Batch 81270, train_perplexity=85.72007

Batch 81280, train_perplexity=71.12396

Batch 81290, train_perplexity=79.377174

Batch 81300, train_perplexity=77.60576

Batch 81310, train_perplexity=75.59289

Batch 81320, train_perplexity=72.22065

Batch 81330, train_perplexity=77.44484

Batch 81340, train_perplexity=74.08796

Batch 81350, train_perplexity=71.624466

Batch 81360, train_perplexity=79.127266

Batch 81370, train_perplexity=74.18037

Batch 81380, train_perplexity=74.06514

Batch 81390, train_perplexity=80.13863

Batch 81400, train_perplexity=76.09907

Batch 81410, train_perplexity=74.11311

Batch 81420, train_perplexity=69.66899

Batch 81430, train_perplexity=75.8576

Batch 81440, train_perplexity=82.929924

Batch 81450, train_perplexity=78.68089

Batch 81460, train_perplexity=75.9408

Batch 81470, train_perplexity=78.77658

Batch 81480, train_perplexity=80.357475

Batch 81490, train_perplexity=73.09454

Batch 81500, train_perplexity=78.31415

Batch 81510, train_perplexity=78.00533

Batch 81520, train_perplexity=79.50214

Batch 81530, train_perplexity=71.05556

Batch 81540, train_perplexity=68.653076

Batch 81550, train_perplexity=76.98457

Batch 81560, train_perplexity=76.74955

Batch 81570, train_perplexity=79.02892

Batch 81580, train_perplexity=82.44114

Batch 81590, train_perplexity=71.692116

Batch 81600, train_perplexity=72.39947

Batch 81610, train_perplexity=71.64243

Batch 81620, train_perplexity=71.63601

Batch 81630, train_perplexity=77.3608

Batch 81640, train_perplexity=73.97895

Batch 81650, train_perplexity=77.570984

Batch 81660, train_perplexity=68.576385

Batch 81670, train_perplexity=80.47919

Batch 81680, train_perplexity=78.3882

Batch 81690, train_perplexity=70.48989

Batch 81700, train_perplexity=73.67858

Batch 81710, train_perplexity=70.89811

Batch 81720, train_perplexity=70.115875

Batch 81730, train_perplexity=75.52191

Batch 81740, train_perplexity=74.96547

Batch 81750, train_perplexity=71.560356

Batch 81760, train_perplexity=71.99242

Batch 81770, train_perplexity=77.18431

Batch 81780, train_perplexity=71.571815

Batch 81790, train_perplexity=67.124664

Batch 81800, train_perplexity=66.70934

Batch 81810, train_perplexity=73.53363

Batch 81820, train_perplexity=74.17736

Batch 81830, train_perplexity=75.45388

Batch 81840, train_perplexity=79.66562

Batch 81850, train_perplexity=80.67895

Batch 81860, train_perplexity=71.800705

Batch 81870, train_perplexity=76.0871

Batch 81880, train_perplexity=78.72641

Batch 81890, train_perplexity=71.55131

Batch 81900, train_perplexity=77.96287

Batch 81910, train_perplexity=73.35278

Batch 81920, train_perplexity=65.26755

Batch 81930, train_perplexity=78.622986

Batch 81940, train_perplexity=77.47088

Batch 81950, train_perplexity=78.51853

Batch 81960, train_perplexity=73.080086

Batch 81970, train_perplexity=73.3046

Batch 81980, train_perplexity=79.03473

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00064-of-00100
Loaded 307521 sentences.
Finished loading
Batch 81990, train_perplexity=70.6406

Batch 82000, train_perplexity=75.89526

Batch 82010, train_perplexity=73.531494

Batch 82020, train_perplexity=73.40744

Batch 82030, train_perplexity=74.87923

Batch 82040, train_perplexity=82.91221

Batch 82050, train_perplexity=78.89767

Batch 82060, train_perplexity=74.80267

Batch 82070, train_perplexity=77.0214

Batch 82080, train_perplexity=78.78417

Batch 82090, train_perplexity=78.37377

Batch 82100, train_perplexity=81.398735
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 82110, train_perplexity=65.26758

Batch 82120, train_perplexity=67.81585

Batch 82130, train_perplexity=69.0232

Batch 82140, train_perplexity=77.645996

Batch 82150, train_perplexity=70.76423

Batch 82160, train_perplexity=73.6455

Batch 82170, train_perplexity=74.19494

Batch 82180, train_perplexity=78.90681

Batch 82190, train_perplexity=70.76537

Batch 82200, train_perplexity=72.69825

Batch 82210, train_perplexity=82.926956

Batch 82220, train_perplexity=71.16148

Batch 82230, train_perplexity=76.67325

Batch 82240, train_perplexity=75.48188

Batch 82250, train_perplexity=71.61255

Batch 82260, train_perplexity=68.49507

Batch 82270, train_perplexity=77.10331

Batch 82280, train_perplexity=75.83608

Batch 82290, train_perplexity=71.07989

Batch 82300, train_perplexity=75.30277

Batch 82310, train_perplexity=74.23457

Batch 82320, train_perplexity=62.238148

Batch 82330, train_perplexity=73.51533

Batch 82340, train_perplexity=79.393265

Batch 82350, train_perplexity=79.8722

Batch 82360, train_perplexity=74.94706

Batch 82370, train_perplexity=70.735756

Batch 82380, train_perplexity=73.82995

Batch 82390, train_perplexity=69.24213

Batch 82400, train_perplexity=74.10117

Batch 82410, train_perplexity=81.65663

Batch 82420, train_perplexity=74.50373

Batch 82430, train_perplexity=78.00444

Batch 82440, train_perplexity=83.99943

Batch 82450, train_perplexity=70.65533

Batch 82460, train_perplexity=77.41338

Batch 82470, train_perplexity=78.064606

Batch 82480, train_perplexity=68.28024

Batch 82490, train_perplexity=80.398026

Batch 82500, train_perplexity=70.876686

Batch 82510, train_perplexity=72.9997

Batch 82520, train_perplexity=76.662834

Batch 82530, train_perplexity=80.28654

Batch 82540, train_perplexity=70.23504

Batch 82550, train_perplexity=76.21531

Batch 82560, train_perplexity=81.45309

Batch 82570, train_perplexity=72.42066

Batch 82580, train_perplexity=74.156136

Batch 82590, train_perplexity=74.539055

Batch 82600, train_perplexity=77.73071

Batch 82610, train_perplexity=80.48609

Batch 82620, train_perplexity=74.63458

Batch 82630, train_perplexity=68.27087

Batch 82640, train_perplexity=69.15053

Batch 82650, train_perplexity=73.50095

Batch 82660, train_perplexity=77.082756

Batch 82670, train_perplexity=75.979126

Batch 82680, train_perplexity=65.51228

Batch 82690, train_perplexity=75.777664

Batch 82700, train_perplexity=76.65848

Batch 82710, train_perplexity=77.89702

Batch 82720, train_perplexity=73.4854

Batch 82730, train_perplexity=84.75381

Batch 82740, train_perplexity=83.88607

Batch 82750, train_perplexity=69.73393

Batch 82760, train_perplexity=79.428024

Batch 82770, train_perplexity=80.886375

Batch 82780, train_perplexity=80.21276

Batch 82790, train_perplexity=80.284164

Batch 82800, train_perplexity=71.05549

Batch 82810, train_perplexity=71.12898

Batch 82820, train_perplexity=76.32008

Batch 82830, train_perplexity=70.990295

Batch 82840, train_perplexity=73.22006

Batch 82850, train_perplexity=72.20729

Batch 82860, train_perplexity=65.08152

Batch 82870, train_perplexity=66.33922

Batch 82880, train_perplexity=78.53351

Batch 82890, train_perplexity=69.730675

Batch 82900, train_perplexity=76.72284

Batch 82910, train_perplexity=78.28734

Batch 82920, train_perplexity=74.720825

Batch 82930, train_perplexity=70.387115

Batch 82940, train_perplexity=79.66847

Batch 82950, train_perplexity=82.0528

Batch 82960, train_perplexity=72.27966

Batch 82970, train_perplexity=66.11002

Batch 82980, train_perplexity=68.42919

Batch 82990, train_perplexity=73.15647

Batch 83000, train_perplexity=70.37167

Batch 83010, train_perplexity=78.85742

Batch 83020, train_perplexity=75.31078

Batch 83030, train_perplexity=72.72069

Batch 83040, train_perplexity=79.73053

Batch 83050, train_perplexity=76.32678

Batch 83060, train_perplexity=82.9319

Batch 83070, train_perplexity=73.96967

Batch 83080, train_perplexity=73.28468

Batch 83090, train_perplexity=73.17381

Batch 83100, train_perplexity=71.07446

Batch 83110, train_perplexity=74.88805

Batch 83120, train_perplexity=79.45211

Batch 83130, train_perplexity=78.87329

Batch 83140, train_perplexity=69.852615

Batch 83150, train_perplexity=70.023094

Batch 83160, train_perplexity=77.6844

Batch 83170, train_perplexity=71.90699

Batch 83180, train_perplexity=73.391266

Batch 83190, train_perplexity=78.691505

Batch 83200, train_perplexity=71.82707

Batch 83210, train_perplexity=79.89227

Batch 83220, train_perplexity=74.69938

Batch 83230, train_perplexity=69.20535

Batch 83240, train_perplexity=69.809395

Batch 83250, train_perplexity=72.885025

Batch 83260, train_perplexity=70.15377

Batch 83270, train_perplexity=72.19249

Batch 83280, train_perplexity=71.985344

Batch 83290, train_perplexity=68.63258

Batch 83300, train_perplexity=75.74678

Batch 83310, train_perplexity=73.948265

Batch 83320, train_perplexity=80.08214

Batch 83330, train_perplexity=71.3553

Batch 83340, train_perplexity=76.663086

Batch 83350, train_perplexity=68.53062

Batch 83360, train_perplexity=78.62602

Batch 83370, train_perplexity=74.839066

Batch 83380, train_perplexity=70.81122

Batch 83390, train_perplexity=76.154465

Batch 83400, train_perplexity=80.55421

Batch 83410, train_perplexity=79.20461

Batch 83420, train_perplexity=69.34009

Batch 83430, train_perplexity=72.51687

Batch 83440, train_perplexity=77.11449

Batch 83450, train_perplexity=77.95852

Batch 83460, train_perplexity=73.49223

Batch 83470, train_perplexity=73.00913

Batch 83480, train_perplexity=74.302284

Batch 83490, train_perplexity=67.142365

Batch 83500, train_perplexity=77.554115

Batch 83510, train_perplexity=89.97954

Batch 83520, train_perplexity=77.615234

Batch 83530, train_perplexity=67.32456

Batch 83540, train_perplexity=74.36562

Batch 83550, train_perplexity=70.93416

Batch 83560, train_perplexity=75.90905

Batch 83570, train_perplexity=81.07277

Batch 83580, train_perplexity=73.377235

Batch 83590, train_perplexity=71.734245

Batch 83600, train_perplexity=86.893364

Batch 83610, train_perplexity=76.53857

Batch 83620, train_perplexity=75.47252

Batch 83630, train_perplexity=78.41728

Batch 83640, train_perplexity=73.345856

Batch 83650, train_perplexity=70.48492

Batch 83660, train_perplexity=70.1889

Batch 83670, train_perplexity=78.59168

Batch 83680, train_perplexity=72.273735

Batch 83690, train_perplexity=78.67931

Batch 83700, train_perplexity=71.799545

Batch 83710, train_perplexity=76.18628

Batch 83720, train_perplexity=67.912415

Batch 83730, train_perplexity=74.868835

Batch 83740, train_perplexity=74.07348

Batch 83750, train_perplexity=74.914116

Batch 83760, train_perplexity=72.24373

Batch 83770, train_perplexity=76.0908

Batch 83780, train_perplexity=81.550095

Batch 83790, train_perplexity=68.4681

Batch 83800, train_perplexity=83.62803

Batch 83810, train_perplexity=73.72491

Batch 83820, train_perplexity=66.0238

Batch 83830, train_perplexity=72.848854

Batch 83840, train_perplexity=74.95574

Batch 83850, train_perplexity=72.54398

Batch 83860, train_perplexity=70.206505

Batch 83870, train_perplexity=75.24646

Batch 83880, train_perplexity=78.325165

Batch 83890, train_perplexity=70.57787

Batch 83900, train_perplexity=80.871956

Batch 83910, train_perplexity=78.044846

Batch 83920, train_perplexity=77.5974

Batch 83930, train_perplexity=72.67126

Batch 83940, train_perplexity=68.96645

Batch 83950, train_perplexity=71.749985

Batch 83960, train_perplexity=84.03789

Batch 83970, train_perplexity=75.446106

Batch 83980, train_perplexity=73.06064

Batch 83990, train_perplexity=66.31753

Batch 84000, train_perplexity=69.94927

Batch 84010, train_perplexity=76.76046

Batch 84020, train_perplexity=73.64248

Batch 84030, train_perplexity=68.74755

Batch 84040, train_perplexity=73.5673

Batch 84050, train_perplexity=73.46346

Batch 84060, train_perplexity=87.88389

Batch 84070, train_perplexity=68.03376

Batch 84080, train_perplexity=68.53127

Batch 84090, train_perplexity=65.619514

Batch 84100, train_perplexity=69.731575

Batch 84110, train_perplexity=76.04227

Batch 84120, train_perplexity=73.25428

Batch 84130, train_perplexity=75.44323

Batch 84140, train_perplexity=73.902794
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 84150, train_perplexity=80.0322

Batch 84160, train_perplexity=69.61304

Batch 84170, train_perplexity=74.72742

Batch 84180, train_perplexity=75.356155

Batch 84190, train_perplexity=74.24951

Batch 84200, train_perplexity=77.46087

Batch 84210, train_perplexity=69.15128

Batch 84220, train_perplexity=69.67424

Batch 84230, train_perplexity=67.30992

Batch 84240, train_perplexity=77.90337

Batch 84250, train_perplexity=78.8378

Batch 84260, train_perplexity=71.55452

Batch 84270, train_perplexity=78.675

Batch 84280, train_perplexity=80.92218

Batch 84290, train_perplexity=70.18475

Batch 84300, train_perplexity=69.091896

Batch 84310, train_perplexity=71.29622

Batch 84320, train_perplexity=71.116264

Batch 84330, train_perplexity=64.63885

Batch 84340, train_perplexity=75.204384

Batch 84350, train_perplexity=73.8152

Batch 84360, train_perplexity=72.382996

Batch 84370, train_perplexity=76.35474

Batch 84380, train_perplexity=70.85033

Batch 84390, train_perplexity=76.04451

Batch 84400, train_perplexity=71.62081

Batch 84410, train_perplexity=70.039055

Batch 84420, train_perplexity=67.93648

Batch 84430, train_perplexity=75.21736

Batch 84440, train_perplexity=78.78015

Batch 84450, train_perplexity=76.55565

Batch 84460, train_perplexity=72.86779

Batch 84470, train_perplexity=75.46791

Batch 84480, train_perplexity=74.42753

Batch 84490, train_perplexity=71.627266

Batch 84500, train_perplexity=73.87038

Batch 84510, train_perplexity=73.24576

Batch 84520, train_perplexity=62.600014

Batch 84530, train_perplexity=75.08191

Batch 84540, train_perplexity=68.755875

Batch 84550, train_perplexity=68.3984

Batch 84560, train_perplexity=74.941666

Batch 84570, train_perplexity=82.73011

Batch 84580, train_perplexity=81.735634

Batch 84590, train_perplexity=68.02968

Batch 84600, train_perplexity=74.980194

Batch 84610, train_perplexity=81.07567

Batch 84620, train_perplexity=80.08646

Batch 84630, train_perplexity=81.80668

Batch 84640, train_perplexity=74.844955

Batch 84650, train_perplexity=75.40439

Batch 84660, train_perplexity=68.01352

Batch 84670, train_perplexity=80.17399

Batch 84680, train_perplexity=76.22218

Batch 84690, train_perplexity=64.19099

Batch 84700, train_perplexity=68.54454

Batch 84710, train_perplexity=71.68877

Batch 84720, train_perplexity=75.429344

Batch 84730, train_perplexity=71.00689

Batch 84740, train_perplexity=73.37553

Batch 84750, train_perplexity=77.72953

Batch 84760, train_perplexity=72.15263

Batch 84770, train_perplexity=75.92154

Batch 84780, train_perplexity=69.826004

Batch 84790, train_perplexity=69.581116

Batch 84800, train_perplexity=73.77301

Batch 84810, train_perplexity=74.89715

Batch 84820, train_perplexity=76.9292

Batch 84830, train_perplexity=85.211876

Batch 84840, train_perplexity=75.95855

Batch 84850, train_perplexity=73.856674

Batch 84860, train_perplexity=72.82312

Batch 84870, train_perplexity=67.52499

Batch 84880, train_perplexity=73.99634

Batch 84890, train_perplexity=66.72582

Batch 84900, train_perplexity=77.79272

Batch 84910, train_perplexity=77.683914

Batch 84920, train_perplexity=68.13356

Batch 84930, train_perplexity=73.30313

Batch 84940, train_perplexity=76.622955

Batch 84950, train_perplexity=69.76297

Batch 84960, train_perplexity=75.14438

Batch 84970, train_perplexity=68.890625

Batch 84980, train_perplexity=81.741325

Batch 84990, train_perplexity=71.51232

Batch 85000, train_perplexity=75.56146

Batch 85010, train_perplexity=68.455696

Batch 85020, train_perplexity=69.66919

Batch 85030, train_perplexity=71.41974

Batch 85040, train_perplexity=72.937386

Batch 85050, train_perplexity=75.461006

Batch 85060, train_perplexity=78.8275

Batch 85070, train_perplexity=68.837494

Batch 85080, train_perplexity=73.6645

Batch 85090, train_perplexity=69.870735

Batch 85100, train_perplexity=71.32903

Batch 85110, train_perplexity=76.4507

Batch 85120, train_perplexity=79.46985

Batch 85130, train_perplexity=75.67623

Batch 85140, train_perplexity=77.153984

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00026-of-00100
Loaded 306324 sentences.
Finished loading
Batch 85150, train_perplexity=71.742256

Batch 85160, train_perplexity=74.01384

Batch 85170, train_perplexity=70.297356

Batch 85180, train_perplexity=76.66813

Batch 85190, train_perplexity=78.02386

Batch 85200, train_perplexity=68.509544

Batch 85210, train_perplexity=80.186874

Batch 85220, train_perplexity=71.91055

Batch 85230, train_perplexity=76.51365

Batch 85240, train_perplexity=69.69305

Batch 85250, train_perplexity=67.07021

Batch 85260, train_perplexity=70.122765

Batch 85270, train_perplexity=78.78263

Batch 85280, train_perplexity=76.09421

Batch 85290, train_perplexity=75.87623

Batch 85300, train_perplexity=74.87851

Batch 85310, train_perplexity=76.38293

Batch 85320, train_perplexity=75.06931

Batch 85330, train_perplexity=74.70896

Batch 85340, train_perplexity=70.1717

Batch 85350, train_perplexity=72.976135

Batch 85360, train_perplexity=80.009995

Batch 85370, train_perplexity=77.93402

Batch 85380, train_perplexity=72.97433

Batch 85390, train_perplexity=68.85867

Batch 85400, train_perplexity=72.093445

Batch 85410, train_perplexity=65.62537

Batch 85420, train_perplexity=74.749016

Batch 85430, train_perplexity=62.62073

Batch 85440, train_perplexity=66.6164

Batch 85450, train_perplexity=64.63638

Batch 85460, train_perplexity=80.31369

Batch 85470, train_perplexity=73.023445

Batch 85480, train_perplexity=70.41511

Batch 85490, train_perplexity=73.81073

Batch 85500, train_perplexity=71.50987

Batch 85510, train_perplexity=75.07733

Batch 85520, train_perplexity=73.36629

Batch 85530, train_perplexity=79.058395

Batch 85540, train_perplexity=74.85995

Batch 85550, train_perplexity=70.874214

Batch 85560, train_perplexity=77.21505

Batch 85570, train_perplexity=73.018814

Batch 85580, train_perplexity=72.04809

Batch 85590, train_perplexity=78.80329

Batch 85600, train_perplexity=71.130104

Batch 85610, train_perplexity=68.38653

Batch 85620, train_perplexity=72.99438

Batch 85630, train_perplexity=67.23018

Batch 85640, train_perplexity=76.34542

Batch 85650, train_perplexity=66.99264

Batch 85660, train_perplexity=78.183105

Batch 85670, train_perplexity=75.63406

Batch 85680, train_perplexity=73.33862

Batch 85690, train_perplexity=77.427155

Batch 85700, train_perplexity=73.24866

Batch 85710, train_perplexity=79.454994

Batch 85720, train_perplexity=67.29285

Batch 85730, train_perplexity=73.37045

Batch 85740, train_perplexity=71.86894

Batch 85750, train_perplexity=84.41042

Batch 85760, train_perplexity=63.18414

Batch 85770, train_perplexity=74.83079

Batch 85780, train_perplexity=73.738335

Batch 85790, train_perplexity=74.06472

Batch 85800, train_perplexity=65.91883

Batch 85810, train_perplexity=72.31748

Batch 85820, train_perplexity=68.604935

Batch 85830, train_perplexity=70.37879

Batch 85840, train_perplexity=71.01481

Batch 85850, train_perplexity=77.999306

Batch 85860, train_perplexity=74.31008

Batch 85870, train_perplexity=73.98939

Batch 85880, train_perplexity=67.9682

Batch 85890, train_perplexity=75.06029

Batch 85900, train_perplexity=76.98788

Batch 85910, train_perplexity=67.72678

Batch 85920, train_perplexity=71.6928

Batch 85930, train_perplexity=71.18279

Batch 85940, train_perplexity=71.40431

Batch 85950, train_perplexity=71.0101

Batch 85960, train_perplexity=76.105675

Batch 85970, train_perplexity=74.21079

Batch 85980, train_perplexity=66.597084

Batch 85990, train_perplexity=73.688

Batch 86000, train_perplexity=72.0135

Batch 86010, train_perplexity=71.189514

Batch 86020, train_perplexity=72.841805

Batch 86030, train_perplexity=76.56664

Batch 86040, train_perplexity=75.35845

Batch 86050, train_perplexity=79.072426

Batch 86060, train_perplexity=72.59066

Batch 86070, train_perplexity=73.09563

Batch 86080, train_perplexity=79.02682
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 86090, train_perplexity=71.608345

Batch 86100, train_perplexity=69.73241

Batch 86110, train_perplexity=70.5556

Batch 86120, train_perplexity=72.657326

Batch 86130, train_perplexity=74.644905

Batch 86140, train_perplexity=78.387

Batch 86150, train_perplexity=81.24889

Batch 86160, train_perplexity=76.83964

Batch 86170, train_perplexity=70.625145

Batch 86180, train_perplexity=78.7634

Batch 86190, train_perplexity=76.75519

Batch 86200, train_perplexity=71.037125

Batch 86210, train_perplexity=67.20287

Batch 86220, train_perplexity=81.33883

Batch 86230, train_perplexity=76.48519

Batch 86240, train_perplexity=71.35642

Batch 86250, train_perplexity=71.201805

Batch 86260, train_perplexity=68.41451

Batch 86270, train_perplexity=71.853455

Batch 86280, train_perplexity=74.36964

Batch 86290, train_perplexity=67.75024

Batch 86300, train_perplexity=75.73406

Batch 86310, train_perplexity=73.4484

Batch 86320, train_perplexity=75.08757

Batch 86330, train_perplexity=72.788124

Batch 86340, train_perplexity=81.94273

Batch 86350, train_perplexity=70.029106

Batch 86360, train_perplexity=73.457825

Batch 86370, train_perplexity=73.424034

Batch 86380, train_perplexity=67.46165

Batch 86390, train_perplexity=73.47671

Batch 86400, train_perplexity=66.904175

Batch 86410, train_perplexity=69.45296

Batch 86420, train_perplexity=72.40565

Batch 86430, train_perplexity=69.29348

Batch 86440, train_perplexity=71.995125

Batch 86450, train_perplexity=71.10667

Batch 86460, train_perplexity=81.55561

Batch 86470, train_perplexity=70.59114

Batch 86480, train_perplexity=71.671745

Batch 86490, train_perplexity=69.71196

Batch 86500, train_perplexity=73.06764

Batch 86510, train_perplexity=73.846924

Batch 86520, train_perplexity=67.84612

Batch 86530, train_perplexity=76.29425

Batch 86540, train_perplexity=68.9624

Batch 86550, train_perplexity=70.77199

Batch 86560, train_perplexity=81.63732

Batch 86570, train_perplexity=65.08934

Batch 86580, train_perplexity=72.888504

Batch 86590, train_perplexity=79.32533

Batch 86600, train_perplexity=76.7598

Batch 86610, train_perplexity=67.41078

Batch 86620, train_perplexity=73.15354

Batch 86630, train_perplexity=76.045746

Batch 86640, train_perplexity=78.01285

Batch 86650, train_perplexity=70.0764

Batch 86660, train_perplexity=68.13746

Batch 86670, train_perplexity=68.6443

Batch 86680, train_perplexity=64.145615

Batch 86690, train_perplexity=69.96952

Batch 86700, train_perplexity=68.86842

Batch 86710, train_perplexity=75.270325

Batch 86720, train_perplexity=66.06609

Batch 86730, train_perplexity=70.32059

Batch 86740, train_perplexity=76.47589

Batch 86750, train_perplexity=68.240204

Batch 86760, train_perplexity=72.85688

Batch 86770, train_perplexity=68.70914

Batch 86780, train_perplexity=69.0778

Batch 86790, train_perplexity=74.70622

Batch 86800, train_perplexity=78.561

Batch 86810, train_perplexity=77.92655

Batch 86820, train_perplexity=77.4584

Batch 86830, train_perplexity=70.384865

Batch 86840, train_perplexity=68.40401

Batch 86850, train_perplexity=79.05961

Batch 86860, train_perplexity=70.87499

Batch 86870, train_perplexity=77.45914

Batch 86880, train_perplexity=78.893074

Batch 86890, train_perplexity=67.13036

Batch 86900, train_perplexity=80.915695

Batch 86910, train_perplexity=71.59469

Batch 86920, train_perplexity=71.827896

Batch 86930, train_perplexity=66.68931

Batch 86940, train_perplexity=65.06001

Batch 86950, train_perplexity=76.86807

Batch 86960, train_perplexity=72.182884

Batch 86970, train_perplexity=72.914085

Batch 86980, train_perplexity=77.75192

Batch 86990, train_perplexity=72.42502

Batch 87000, train_perplexity=68.89943

Batch 87010, train_perplexity=67.29163

Batch 87020, train_perplexity=74.13284

Batch 87030, train_perplexity=69.556

Batch 87040, train_perplexity=72.56889

Batch 87050, train_perplexity=77.6644

Batch 87060, train_perplexity=72.894966

Batch 87070, train_perplexity=67.373955

Batch 87080, train_perplexity=74.72596

Batch 87090, train_perplexity=71.53234

Batch 87100, train_perplexity=72.04097

Batch 87110, train_perplexity=69.7653

Batch 87120, train_perplexity=73.68357

Batch 87130, train_perplexity=69.48174

Batch 87140, train_perplexity=74.723816

Batch 87150, train_perplexity=75.634895

Batch 87160, train_perplexity=65.77477

Batch 87170, train_perplexity=68.04077

Batch 87180, train_perplexity=67.17894

Batch 87190, train_perplexity=69.319855

Batch 87200, train_perplexity=74.14603

Batch 87210, train_perplexity=71.95223

Batch 87220, train_perplexity=72.032455

Batch 87230, train_perplexity=73.655365

Batch 87240, train_perplexity=73.179085

Batch 87250, train_perplexity=84.33427

Batch 87260, train_perplexity=72.90032

Batch 87270, train_perplexity=74.155396

Batch 87280, train_perplexity=73.1239

Batch 87290, train_perplexity=74.16261

Batch 87300, train_perplexity=79.49603

Batch 87310, train_perplexity=73.47888

Batch 87320, train_perplexity=77.956696

Batch 87330, train_perplexity=72.94556

Batch 87340, train_perplexity=75.183586

Batch 87350, train_perplexity=88.17297

Batch 87360, train_perplexity=74.97186

Batch 87370, train_perplexity=73.95052

Batch 87380, train_perplexity=65.28654

Batch 87390, train_perplexity=72.73171

Batch 87400, train_perplexity=75.3143

Batch 87410, train_perplexity=71.09501

Batch 87420, train_perplexity=74.71399

Batch 87430, train_perplexity=80.40308

Batch 87440, train_perplexity=70.04981

Batch 87450, train_perplexity=72.09455

Batch 87460, train_perplexity=74.8899

Batch 87470, train_perplexity=76.67164

Batch 87480, train_perplexity=75.55217

Batch 87490, train_perplexity=68.449394

Batch 87500, train_perplexity=72.198204

Batch 87510, train_perplexity=76.74348

Batch 87520, train_perplexity=69.73779

Batch 87530, train_perplexity=76.52776

Batch 87540, train_perplexity=71.45165

Batch 87550, train_perplexity=76.15236

Batch 87560, train_perplexity=78.50528

Batch 87570, train_perplexity=74.3485

Batch 87580, train_perplexity=70.85773

Batch 87590, train_perplexity=77.30929

Batch 87600, train_perplexity=70.90136

Batch 87610, train_perplexity=72.7469

Batch 87620, train_perplexity=70.769356

Batch 87630, train_perplexity=70.21856

Batch 87640, train_perplexity=77.25047

Batch 87650, train_perplexity=81.750755

Batch 87660, train_perplexity=80.04404

Batch 87670, train_perplexity=78.90279

Batch 87680, train_perplexity=63.31636

Batch 87690, train_perplexity=81.60206

Batch 87700, train_perplexity=74.91201

Batch 87710, train_perplexity=76.75482

Batch 87720, train_perplexity=70.33095

Batch 87730, train_perplexity=74.822296

Batch 87740, train_perplexity=70.80244

Batch 87750, train_perplexity=65.22477

Batch 87760, train_perplexity=66.39935

Batch 87770, train_perplexity=70.78569

Batch 87780, train_perplexity=85.21793

Batch 87790, train_perplexity=71.4629

Batch 87800, train_perplexity=72.03616

Batch 87810, train_perplexity=69.97797

Batch 87820, train_perplexity=76.345345

Batch 87830, train_perplexity=72.8958

Batch 87840, train_perplexity=75.97873

Batch 87850, train_perplexity=78.56291

Batch 87860, train_perplexity=68.09114

Batch 87870, train_perplexity=66.96916

Batch 87880, train_perplexity=65.30765

Batch 87890, train_perplexity=72.41027

Batch 87900, train_perplexity=75.14552

Batch 87910, train_perplexity=73.72491

Batch 87920, train_perplexity=73.48708

Batch 87930, train_perplexity=68.55114

Batch 87940, train_perplexity=71.69841

Batch 87950, train_perplexity=76.52357

Batch 87960, train_perplexity=72.36709

Batch 87970, train_perplexity=71.68405

Batch 87980, train_perplexity=74.601494

Batch 87990, train_perplexity=76.708206

Batch 88000, train_perplexity=72.923996

Batch 88010, train_perplexity=77.86701

Batch 88020, train_perplexity=71.3828

Batch 88030, train_perplexity=69.643684

Batch 88040, train_perplexity=73.619446

Batch 88050, train_perplexity=80.861

Batch 88060, train_perplexity=62.258034

Batch 88070, train_perplexity=75.16839

Batch 88080, train_perplexity=70.17919

Batch 88090, train_perplexity=71.85838

Batch 88100, train_perplexity=72.01501

Batch 88110, train_perplexity=74.2144

Batch 88120, train_perplexity=73.8384

Batch 88130, train_perplexity=65.26776
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 88140, train_perplexity=77.990456

Batch 88150, train_perplexity=83.59011

Batch 88160, train_perplexity=78.225136

Batch 88170, train_perplexity=76.32496

Batch 88180, train_perplexity=78.90361

Batch 88190, train_perplexity=68.65595

Batch 88200, train_perplexity=69.13856

Batch 88210, train_perplexity=67.27585

Batch 88220, train_perplexity=75.177704

Batch 88230, train_perplexity=82.24442

Batch 88240, train_perplexity=77.5587

Batch 88250, train_perplexity=74.20509

Batch 88260, train_perplexity=76.71721

Batch 88270, train_perplexity=72.74912

Batch 88280, train_perplexity=72.54876

Batch 88290, train_perplexity=83.26256

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00027-of-00100
Loaded 306804 sentences.
Finished loading
Batch 88300, train_perplexity=76.5892

Batch 88310, train_perplexity=68.976814

Batch 88320, train_perplexity=65.90412

Batch 88330, train_perplexity=80.41719

Batch 88340, train_perplexity=70.86688

Batch 88350, train_perplexity=72.858406

Batch 88360, train_perplexity=78.12698

Batch 88370, train_perplexity=72.97179

Batch 88380, train_perplexity=75.18061

Batch 88390, train_perplexity=72.87867

Batch 88400, train_perplexity=71.482254

Batch 88410, train_perplexity=67.78223

Batch 88420, train_perplexity=72.133644

Batch 88430, train_perplexity=79.31671

Batch 88440, train_perplexity=72.528076

Batch 88450, train_perplexity=73.870026

Batch 88460, train_perplexity=74.50266

Batch 88470, train_perplexity=76.23254

Batch 88480, train_perplexity=73.00917

Batch 88490, train_perplexity=70.91813

Batch 88500, train_perplexity=75.90269

Batch 88510, train_perplexity=71.75758

Batch 88520, train_perplexity=74.23224

Batch 88530, train_perplexity=67.49164

Batch 88540, train_perplexity=77.26945

Batch 88550, train_perplexity=62.495266

Batch 88560, train_perplexity=71.741196

Batch 88570, train_perplexity=73.76928

Batch 88580, train_perplexity=77.24491

Batch 88590, train_perplexity=69.75578

Batch 88600, train_perplexity=75.06795

Batch 88610, train_perplexity=72.673225

Batch 88620, train_perplexity=69.67092

Batch 88630, train_perplexity=75.04519

Batch 88640, train_perplexity=76.16852

Batch 88650, train_perplexity=73.32264

Batch 88660, train_perplexity=67.12345

Batch 88670, train_perplexity=83.56824

Batch 88680, train_perplexity=73.90089

Batch 88690, train_perplexity=66.42091

Batch 88700, train_perplexity=73.82059

Batch 88710, train_perplexity=80.295845

Batch 88720, train_perplexity=73.040405

Batch 88730, train_perplexity=78.003174

Batch 88740, train_perplexity=75.71511

Batch 88750, train_perplexity=68.409065

Batch 88760, train_perplexity=64.32949

Batch 88770, train_perplexity=79.05546

Batch 88780, train_perplexity=65.336395

Batch 88790, train_perplexity=71.48761

Batch 88800, train_perplexity=68.638016

Batch 88810, train_perplexity=70.82986

Batch 88820, train_perplexity=69.639366

Batch 88830, train_perplexity=65.50453

Batch 88840, train_perplexity=78.08244

Batch 88850, train_perplexity=75.26365

Batch 88860, train_perplexity=70.25969

Batch 88870, train_perplexity=76.0662

Batch 88880, train_perplexity=69.03307

Batch 88890, train_perplexity=77.26989

Batch 88900, train_perplexity=70.14721

Batch 88910, train_perplexity=73.57972

Batch 88920, train_perplexity=70.70854

Batch 88930, train_perplexity=72.63599

Batch 88940, train_perplexity=74.503586

Batch 88950, train_perplexity=77.222885

Batch 88960, train_perplexity=73.339424

Batch 88970, train_perplexity=68.903435

Batch 88980, train_perplexity=78.52003

Batch 88990, train_perplexity=73.19758

Batch 89000, train_perplexity=69.95214

Batch 89010, train_perplexity=77.99224

Batch 89020, train_perplexity=72.10221

Batch 89030, train_perplexity=68.71389

Batch 89040, train_perplexity=83.269226

Batch 89050, train_perplexity=63.058086

Batch 89060, train_perplexity=64.57902

Batch 89070, train_perplexity=67.51552

Batch 89080, train_perplexity=75.04741

Batch 89090, train_perplexity=74.614296

Batch 89100, train_perplexity=67.03715

Batch 89110, train_perplexity=75.51255

Batch 89120, train_perplexity=80.048615

Batch 89130, train_perplexity=79.7538

Batch 89140, train_perplexity=73.01679

Batch 89150, train_perplexity=76.98956

Batch 89160, train_perplexity=71.204384

Batch 89170, train_perplexity=72.150085

Batch 89180, train_perplexity=77.89416

Batch 89190, train_perplexity=81.63666

Batch 89200, train_perplexity=73.43587

Batch 89210, train_perplexity=72.58945

Batch 89220, train_perplexity=76.60448

Batch 89230, train_perplexity=79.21104

Batch 89240, train_perplexity=73.37273

Batch 89250, train_perplexity=73.35285

Batch 89260, train_perplexity=72.38455

Batch 89270, train_perplexity=75.93331

Batch 89280, train_perplexity=75.647736

Batch 89290, train_perplexity=68.73221

Batch 89300, train_perplexity=84.59856

Batch 89310, train_perplexity=67.46641

Batch 89320, train_perplexity=80.53262

Batch 89330, train_perplexity=69.18816

Batch 89340, train_perplexity=69.61473

Batch 89350, train_perplexity=76.92029

Batch 89360, train_perplexity=72.21748

Batch 89370, train_perplexity=68.25856

Batch 89380, train_perplexity=63.45261

Batch 89390, train_perplexity=81.02407

Batch 89400, train_perplexity=69.71096

Batch 89410, train_perplexity=63.572754

Batch 89420, train_perplexity=77.31766

Batch 89430, train_perplexity=73.1913

Batch 89440, train_perplexity=73.49233

Batch 89450, train_perplexity=67.25503

Batch 89460, train_perplexity=78.170654

Batch 89470, train_perplexity=74.058395

Batch 89480, train_perplexity=73.713165

Batch 89490, train_perplexity=70.70729

Batch 89500, train_perplexity=72.963684

Batch 89510, train_perplexity=75.850044

Batch 89520, train_perplexity=67.458885

Batch 89530, train_perplexity=77.43055

Batch 89540, train_perplexity=74.858986

Batch 89550, train_perplexity=64.639465

Batch 89560, train_perplexity=73.076775

Batch 89570, train_perplexity=65.9392

Batch 89580, train_perplexity=79.685455

Batch 89590, train_perplexity=76.94549

Batch 89600, train_perplexity=74.55053

Batch 89610, train_perplexity=71.6358

Batch 89620, train_perplexity=73.4943

Batch 89630, train_perplexity=65.37987

Batch 89640, train_perplexity=67.696045

Batch 89650, train_perplexity=76.38988

Batch 89660, train_perplexity=82.7434

Batch 89670, train_perplexity=70.57205

Batch 89680, train_perplexity=70.444466

Batch 89690, train_perplexity=77.861855

Batch 89700, train_perplexity=66.7686

Batch 89710, train_perplexity=77.29472

Batch 89720, train_perplexity=72.22871

Batch 89730, train_perplexity=76.4771

Batch 89740, train_perplexity=71.34223

Batch 89750, train_perplexity=73.78934

Batch 89760, train_perplexity=80.44005

Batch 89770, train_perplexity=74.636826

Batch 89780, train_perplexity=73.55292

Batch 89790, train_perplexity=68.30082

Batch 89800, train_perplexity=70.586525

Batch 89810, train_perplexity=70.67403

Batch 89820, train_perplexity=75.70413

Batch 89830, train_perplexity=73.48648

Batch 89840, train_perplexity=73.4517

Batch 89850, train_perplexity=84.90323

Batch 89860, train_perplexity=77.07824

Batch 89870, train_perplexity=78.52209

Batch 89880, train_perplexity=71.28034

Batch 89890, train_perplexity=80.4798

Batch 89900, train_perplexity=71.763535

Batch 89910, train_perplexity=69.41869

Batch 89920, train_perplexity=76.850044

Batch 89930, train_perplexity=73.17182

Batch 89940, train_perplexity=79.80752

Batch 89950, train_perplexity=72.518875

Batch 89960, train_perplexity=75.02122

Batch 89970, train_perplexity=76.92975

Batch 89980, train_perplexity=66.67958

Batch 89990, train_perplexity=77.43336

Batch 90000, train_perplexity=71.33509

Batch 90010, train_perplexity=65.186714

Batch 90020, train_perplexity=67.527916

Batch 90030, train_perplexity=71.09928

Batch 90040, train_perplexity=78.80014

Batch 90050, train_perplexity=76.168335

Batch 90060, train_perplexity=68.347275

Batch 90070, train_perplexity=69.942
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 90080, train_perplexity=69.3304

Batch 90090, train_perplexity=73.38182

Batch 90100, train_perplexity=71.20934

Batch 90110, train_perplexity=72.56391

Batch 90120, train_perplexity=70.9232

Batch 90130, train_perplexity=72.30476

Batch 90140, train_perplexity=70.86513

Batch 90150, train_perplexity=72.80208

Batch 90160, train_perplexity=72.155525

Batch 90170, train_perplexity=73.14315

Batch 90180, train_perplexity=77.474724

Batch 90190, train_perplexity=74.50785

Batch 90200, train_perplexity=65.70907

Batch 90210, train_perplexity=79.127754

Batch 90220, train_perplexity=73.77336

Batch 90230, train_perplexity=66.53652

Batch 90240, train_perplexity=71.6741

Batch 90250, train_perplexity=84.71562

Batch 90260, train_perplexity=69.91796

Batch 90270, train_perplexity=74.82968

Batch 90280, train_perplexity=76.091446

Batch 90290, train_perplexity=75.483246

Batch 90300, train_perplexity=75.16151

Batch 90310, train_perplexity=80.30806

Batch 90320, train_perplexity=67.227036

Batch 90330, train_perplexity=76.76434

Batch 90340, train_perplexity=64.460884

Batch 90350, train_perplexity=70.38691

Batch 90360, train_perplexity=73.5181

Batch 90370, train_perplexity=66.9874

Batch 90380, train_perplexity=69.311

Batch 90390, train_perplexity=78.18796

Batch 90400, train_perplexity=70.47793

Batch 90410, train_perplexity=70.50509

Batch 90420, train_perplexity=77.28916

Batch 90430, train_perplexity=71.12037

Batch 90440, train_perplexity=70.43351

Batch 90450, train_perplexity=67.130615

Batch 90460, train_perplexity=80.71062

Batch 90470, train_perplexity=71.40414

Batch 90480, train_perplexity=71.58827

Batch 90490, train_perplexity=73.77111

Batch 90500, train_perplexity=67.9695

Batch 90510, train_perplexity=65.59193

Batch 90520, train_perplexity=79.26109

Batch 90530, train_perplexity=74.610916

Batch 90540, train_perplexity=66.8947

Batch 90550, train_perplexity=83.67649

Batch 90560, train_perplexity=76.15359

Batch 90570, train_perplexity=72.46226

Batch 90580, train_perplexity=77.168045

Batch 90590, train_perplexity=75.15061

Batch 90600, train_perplexity=71.256355

Batch 90610, train_perplexity=65.9743

Batch 90620, train_perplexity=67.5571

Batch 90630, train_perplexity=74.06055

Batch 90640, train_perplexity=73.44259

Batch 90650, train_perplexity=73.35733

Batch 90660, train_perplexity=70.47282

Batch 90670, train_perplexity=74.836494

Batch 90680, train_perplexity=69.16402

Batch 90690, train_perplexity=71.54094

Batch 90700, train_perplexity=73.22718

Batch 90710, train_perplexity=81.00333

Batch 90720, train_perplexity=70.696266

Batch 90730, train_perplexity=75.717094

Batch 90740, train_perplexity=74.66391

Batch 90750, train_perplexity=78.36543

Batch 90760, train_perplexity=72.88127

Batch 90770, train_perplexity=65.85304

Batch 90780, train_perplexity=74.043175

Batch 90790, train_perplexity=70.60225

Batch 90800, train_perplexity=68.91599

Batch 90810, train_perplexity=69.04058

Batch 90820, train_perplexity=75.55548

Batch 90830, train_perplexity=75.982025

Batch 90840, train_perplexity=69.25606

Batch 90850, train_perplexity=67.862816

Batch 90860, train_perplexity=74.70943

Batch 90870, train_perplexity=70.24348

Batch 90880, train_perplexity=65.190285

Batch 90890, train_perplexity=74.53681

Batch 90900, train_perplexity=78.09399

Batch 90910, train_perplexity=68.25579

Batch 90920, train_perplexity=70.01995

Batch 90930, train_perplexity=72.832565

Batch 90940, train_perplexity=77.876816

Batch 90950, train_perplexity=74.78288

Batch 90960, train_perplexity=69.55474

Batch 90970, train_perplexity=68.14094

Batch 90980, train_perplexity=73.36293

Batch 90990, train_perplexity=71.88488

Batch 91000, train_perplexity=64.373215

Batch 91010, train_perplexity=79.81265

Batch 91020, train_perplexity=78.17192

Batch 91030, train_perplexity=74.06306

Batch 91040, train_perplexity=73.688774

Batch 91050, train_perplexity=72.38528

Batch 91060, train_perplexity=67.65526

Batch 91070, train_perplexity=72.28222

Batch 91080, train_perplexity=68.07932

Batch 91090, train_perplexity=72.18113

Batch 91100, train_perplexity=76.06845

Batch 91110, train_perplexity=67.50484

Batch 91120, train_perplexity=72.249306

Batch 91130, train_perplexity=68.866714

Batch 91140, train_perplexity=77.7824

Batch 91150, train_perplexity=66.31256

Batch 91160, train_perplexity=74.04579

Batch 91170, train_perplexity=69.75915

Batch 91180, train_perplexity=79.876274

Batch 91190, train_perplexity=72.51721

Batch 91200, train_perplexity=77.50806

Batch 91210, train_perplexity=71.472374

Batch 91220, train_perplexity=71.14292

Batch 91230, train_perplexity=72.08399

Batch 91240, train_perplexity=74.87819

Batch 91250, train_perplexity=76.28581

Batch 91260, train_perplexity=80.09023

Batch 91270, train_perplexity=73.89208

Batch 91280, train_perplexity=68.03409

Batch 91290, train_perplexity=77.3129

Batch 91300, train_perplexity=74.01166

Batch 91310, train_perplexity=77.002785

Batch 91320, train_perplexity=69.16003

Batch 91330, train_perplexity=70.60231

Batch 91340, train_perplexity=71.740715

Batch 91350, train_perplexity=82.08512

Batch 91360, train_perplexity=78.3895

Batch 91370, train_perplexity=63.54363

Batch 91380, train_perplexity=71.542336

Batch 91390, train_perplexity=63.49689

Batch 91400, train_perplexity=69.912125

Batch 91410, train_perplexity=70.32596

Batch 91420, train_perplexity=70.85485

Batch 91430, train_perplexity=73.892006

Batch 91440, train_perplexity=71.10206

Batch 91450, train_perplexity=74.85659

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00047-of-00100
Loaded 306016 sentences.
Finished loading
Batch 91460, train_perplexity=73.55015

Batch 91470, train_perplexity=71.96015

Batch 91480, train_perplexity=78.641396

Batch 91490, train_perplexity=65.722984

Batch 91500, train_perplexity=67.23114

Batch 91510, train_perplexity=76.19696

Batch 91520, train_perplexity=81.45045

Batch 91530, train_perplexity=79.76217

Batch 91540, train_perplexity=69.719376

Batch 91550, train_perplexity=70.83097

Batch 91560, train_perplexity=74.82519

Batch 91570, train_perplexity=76.21353

Batch 91580, train_perplexity=68.25882

Batch 91590, train_perplexity=80.061325

Batch 91600, train_perplexity=73.12648

Batch 91610, train_perplexity=70.86695

Batch 91620, train_perplexity=71.02914

Batch 91630, train_perplexity=76.36894

Batch 91640, train_perplexity=72.631584

Batch 91650, train_perplexity=76.6166

Batch 91660, train_perplexity=76.53459

Batch 91670, train_perplexity=73.23047

Batch 91680, train_perplexity=71.14547

Batch 91690, train_perplexity=75.36967

Batch 91700, train_perplexity=73.93825

Batch 91710, train_perplexity=74.167946

Batch 91720, train_perplexity=84.464775

Batch 91730, train_perplexity=73.06088

Batch 91740, train_perplexity=69.62237

Batch 91750, train_perplexity=70.937744

Batch 91760, train_perplexity=70.39846

Batch 91770, train_perplexity=74.52014

Batch 91780, train_perplexity=76.06228

Batch 91790, train_perplexity=69.39569

Batch 91800, train_perplexity=71.00313

Batch 91810, train_perplexity=71.88776

Batch 91820, train_perplexity=66.3934

Batch 91830, train_perplexity=72.71742

Batch 91840, train_perplexity=72.57378

Batch 91850, train_perplexity=72.86001

Batch 91860, train_perplexity=70.68114

Batch 91870, train_perplexity=76.33195

Batch 91880, train_perplexity=75.35328

Batch 91890, train_perplexity=80.30201

Batch 91900, train_perplexity=67.573494

Batch 91910, train_perplexity=74.777855

Batch 91920, train_perplexity=71.79037

Batch 91930, train_perplexity=71.08606

Batch 91940, train_perplexity=72.3819

Batch 91950, train_perplexity=73.73633

Batch 91960, train_perplexity=74.727455

Batch 91970, train_perplexity=70.22703

Batch 91980, train_perplexity=76.957855

Batch 91990, train_perplexity=69.20033

Batch 92000, train_perplexity=75.04959

Batch 92010, train_perplexity=81.684906
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 92020, train_perplexity=72.40803

Batch 92030, train_perplexity=77.66284

Batch 92040, train_perplexity=74.70312

Batch 92050, train_perplexity=69.73307

Batch 92060, train_perplexity=71.28959

Batch 92070, train_perplexity=74.1989

Batch 92080, train_perplexity=77.59614

Batch 92090, train_perplexity=73.010666

Batch 92100, train_perplexity=66.42621

Batch 92110, train_perplexity=82.13289

Batch 92120, train_perplexity=76.00377

Batch 92130, train_perplexity=74.89176

Batch 92140, train_perplexity=73.54492

Batch 92150, train_perplexity=73.46711

Batch 92160, train_perplexity=76.29403

Batch 92170, train_perplexity=73.69496

Batch 92180, train_perplexity=71.94725

Batch 92190, train_perplexity=66.78468

Batch 92200, train_perplexity=78.24293

Batch 92210, train_perplexity=71.44886

Batch 92220, train_perplexity=71.10497

Batch 92230, train_perplexity=79.68272

Batch 92240, train_perplexity=68.49265

Batch 92250, train_perplexity=69.05609

Batch 92260, train_perplexity=75.25152

Batch 92270, train_perplexity=73.95729

Batch 92280, train_perplexity=68.53817

Batch 92290, train_perplexity=69.42267

Batch 92300, train_perplexity=76.43787

Batch 92310, train_perplexity=70.00713

Batch 92320, train_perplexity=67.80343

Batch 92330, train_perplexity=75.37304

Batch 92340, train_perplexity=74.9422

Batch 92350, train_perplexity=69.65617

Batch 92360, train_perplexity=67.36881

Batch 92370, train_perplexity=72.04293

Batch 92380, train_perplexity=70.04089

Batch 92390, train_perplexity=73.4944

Batch 92400, train_perplexity=69.28218

Batch 92410, train_perplexity=72.15092

Batch 92420, train_perplexity=69.25437

Batch 92430, train_perplexity=68.96287

Batch 92440, train_perplexity=78.13078

Batch 92450, train_perplexity=72.17015

Batch 92460, train_perplexity=70.56623

Batch 92470, train_perplexity=73.60772

Batch 92480, train_perplexity=71.26777

Batch 92490, train_perplexity=70.42112

Batch 92500, train_perplexity=71.91433

Batch 92510, train_perplexity=76.33428

Batch 92520, train_perplexity=74.17131

Batch 92530, train_perplexity=73.5283

Batch 92540, train_perplexity=82.24953

Batch 92550, train_perplexity=74.78142

Batch 92560, train_perplexity=73.7119

Batch 92570, train_perplexity=64.6988

Batch 92580, train_perplexity=73.524025

Batch 92590, train_perplexity=70.94336

Batch 92600, train_perplexity=73.486694

Batch 92610, train_perplexity=74.61693

Batch 92620, train_perplexity=78.45034

Batch 92630, train_perplexity=68.63003

Batch 92640, train_perplexity=70.96664

Batch 92650, train_perplexity=77.123314

Batch 92660, train_perplexity=72.01988

Batch 92670, train_perplexity=67.5532

Batch 92680, train_perplexity=71.903625

Batch 92690, train_perplexity=79.34811

Batch 92700, train_perplexity=72.48527

Batch 92710, train_perplexity=69.98377

Batch 92720, train_perplexity=76.8507

Batch 92730, train_perplexity=70.02022

Batch 92740, train_perplexity=74.92458

Batch 92750, train_perplexity=73.20274

Batch 92760, train_perplexity=71.66983

Batch 92770, train_perplexity=77.18534

Batch 92780, train_perplexity=70.0056

Batch 92790, train_perplexity=71.6885

Batch 92800, train_perplexity=72.439316

Batch 92810, train_perplexity=75.02601

Batch 92820, train_perplexity=69.866135

Batch 92830, train_perplexity=75.22845

Batch 92840, train_perplexity=74.26616

Batch 92850, train_perplexity=77.5458

Batch 92860, train_perplexity=76.272865

Batch 92870, train_perplexity=74.66164

Batch 92880, train_perplexity=68.80363

Batch 92890, train_perplexity=83.88527

Batch 92900, train_perplexity=79.65225

Batch 92910, train_perplexity=71.879395

Batch 92920, train_perplexity=71.52876

Batch 92930, train_perplexity=67.96891

Batch 92940, train_perplexity=68.46594

Batch 92950, train_perplexity=75.6718

Batch 92960, train_perplexity=75.036026

Batch 92970, train_perplexity=68.85476

Batch 92980, train_perplexity=67.327

Batch 92990, train_perplexity=72.342735

Batch 93000, train_perplexity=74.16802

Batch 93010, train_perplexity=68.41993

Batch 93020, train_perplexity=66.974205

Batch 93030, train_perplexity=71.37027

Batch 93040, train_perplexity=67.42338

Batch 93050, train_perplexity=71.49623

Batch 93060, train_perplexity=64.50307

Batch 93070, train_perplexity=66.60941

Batch 93080, train_perplexity=75.41712

Batch 93090, train_perplexity=73.97281

Batch 93100, train_perplexity=68.643776

Batch 93110, train_perplexity=79.76924

Batch 93120, train_perplexity=71.692566

Batch 93130, train_perplexity=65.1928

Batch 93140, train_perplexity=70.33444

Batch 93150, train_perplexity=68.68811

Batch 93160, train_perplexity=78.5401

Batch 93170, train_perplexity=69.835396

Batch 93180, train_perplexity=77.349365

Batch 93190, train_perplexity=70.671295

Batch 93200, train_perplexity=68.538

Batch 93210, train_perplexity=76.70357

Batch 93220, train_perplexity=73.98692

Batch 93230, train_perplexity=66.715836

Batch 93240, train_perplexity=80.57299

Batch 93250, train_perplexity=70.00727

Batch 93260, train_perplexity=68.234314

Batch 93270, train_perplexity=73.44371

Batch 93280, train_perplexity=74.95028

Batch 93290, train_perplexity=77.049835

Batch 93300, train_perplexity=75.33467

Batch 93310, train_perplexity=76.28207

Batch 93320, train_perplexity=64.29833

Batch 93330, train_perplexity=79.66965

Batch 93340, train_perplexity=74.30023

Batch 93350, train_perplexity=72.185394

Batch 93360, train_perplexity=68.21415

Batch 93370, train_perplexity=71.66327

Batch 93380, train_perplexity=66.42013

Batch 93390, train_perplexity=68.94899

Batch 93400, train_perplexity=83.915436

Batch 93410, train_perplexity=69.036964

Batch 93420, train_perplexity=77.96063

Batch 93430, train_perplexity=73.13499

Batch 93440, train_perplexity=68.83585

Batch 93450, train_perplexity=71.662796

Batch 93460, train_perplexity=78.0316

Batch 93470, train_perplexity=78.10001

Batch 93480, train_perplexity=69.87597

Batch 93490, train_perplexity=76.81158

Batch 93500, train_perplexity=71.156395

Batch 93510, train_perplexity=67.12329

Batch 93520, train_perplexity=76.833916

Batch 93530, train_perplexity=79.29323

Batch 93540, train_perplexity=70.527275

Batch 93550, train_perplexity=70.32015

Batch 93560, train_perplexity=68.78539

Batch 93570, train_perplexity=73.24332

Batch 93580, train_perplexity=76.672516

Batch 93590, train_perplexity=73.49094

Batch 93600, train_perplexity=76.29196

Batch 93610, train_perplexity=74.29676

Batch 93620, train_perplexity=72.54516

Batch 93630, train_perplexity=68.707634

Batch 93640, train_perplexity=75.365456

Batch 93650, train_perplexity=69.851746

Batch 93660, train_perplexity=65.72361

Batch 93670, train_perplexity=76.00203

Batch 93680, train_perplexity=69.77085

Batch 93690, train_perplexity=72.496506

Batch 93700, train_perplexity=72.07877

Batch 93710, train_perplexity=74.92216

Batch 93720, train_perplexity=68.4367

Batch 93730, train_perplexity=72.899345

Batch 93740, train_perplexity=79.99821

Batch 93750, train_perplexity=74.84849

Batch 93760, train_perplexity=75.91684

Batch 93770, train_perplexity=64.12341

Batch 93780, train_perplexity=72.68394

Batch 93790, train_perplexity=67.12063

Batch 93800, train_perplexity=71.977036

Batch 93810, train_perplexity=75.76856

Batch 93820, train_perplexity=74.10174

Batch 93830, train_perplexity=75.78984

Batch 93840, train_perplexity=75.68732

Batch 93850, train_perplexity=76.36486

Batch 93860, train_perplexity=77.99603

Batch 93870, train_perplexity=65.48524

Batch 93880, train_perplexity=68.019424

Batch 93890, train_perplexity=77.76831

Batch 93900, train_perplexity=70.78306

Batch 93910, train_perplexity=74.302536

Batch 93920, train_perplexity=69.60726

Batch 93930, train_perplexity=71.74821

Batch 93940, train_perplexity=82.741394

Batch 93950, train_perplexity=70.26324

Batch 93960, train_perplexity=72.45804

Batch 93970, train_perplexity=74.28139

Batch 93980, train_perplexity=71.56683

Batch 93990, train_perplexity=77.17838

Batch 94000, train_perplexity=79.90999

Batch 94010, train_perplexity=75.30418

Batch 94020, train_perplexity=64.11935

Batch 94030, train_perplexity=68.253746

Batch 94040, train_perplexity=71.75522

Batch 94050, train_perplexity=80.705185

Batch 94060, train_perplexity=73.45709
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 94070, train_perplexity=80.493614

Batch 94080, train_perplexity=71.322914

Batch 94090, train_perplexity=71.64359

Batch 94100, train_perplexity=69.63983

Batch 94110, train_perplexity=67.1873

Batch 94120, train_perplexity=66.60776

Batch 94130, train_perplexity=71.48113

Batch 94140, train_perplexity=72.30831

Batch 94150, train_perplexity=70.19171

Batch 94160, train_perplexity=74.37995

Batch 94170, train_perplexity=73.4718

Batch 94180, train_perplexity=71.99808

Batch 94190, train_perplexity=71.51328

Batch 94200, train_perplexity=75.73591

Batch 94210, train_perplexity=74.56237

Batch 94220, train_perplexity=71.99942

Batch 94230, train_perplexity=76.83993

Batch 94240, train_perplexity=70.974014

Batch 94250, train_perplexity=74.59189

Batch 94260, train_perplexity=82.70455

Batch 94270, train_perplexity=81.07335

Batch 94280, train_perplexity=75.66155

Batch 94290, train_perplexity=70.980446

Batch 94300, train_perplexity=70.02937

Batch 94310, train_perplexity=77.777504

Batch 94320, train_perplexity=74.593666

Batch 94330, train_perplexity=78.48533

Batch 94340, train_perplexity=66.99005

Batch 94350, train_perplexity=63.462475

Batch 94360, train_perplexity=64.977425

Batch 94370, train_perplexity=74.34804

Batch 94380, train_perplexity=66.626755

Batch 94390, train_perplexity=72.53291

Batch 94400, train_perplexity=73.09723

Batch 94410, train_perplexity=70.24153

Batch 94420, train_perplexity=77.34826

Batch 94430, train_perplexity=64.24059

Batch 94440, train_perplexity=74.22063

Batch 94450, train_perplexity=69.17104

Batch 94460, train_perplexity=74.426

Batch 94470, train_perplexity=73.6665

Batch 94480, train_perplexity=74.83764

Batch 94490, train_perplexity=70.50498

Batch 94500, train_perplexity=68.05998

Batch 94510, train_perplexity=70.78927

Batch 94520, train_perplexity=75.90873

Batch 94530, train_perplexity=72.30318

Batch 94540, train_perplexity=68.97826

Batch 94550, train_perplexity=66.82045

Batch 94560, train_perplexity=70.0748

Batch 94570, train_perplexity=73.17172

Batch 94580, train_perplexity=73.794334

Batch 94590, train_perplexity=72.65199

Batch 94600, train_perplexity=75.921394

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00071-of-00100
Loaded 306430 sentences.
Finished loading
Batch 94610, train_perplexity=70.79548

Batch 94620, train_perplexity=70.41122

Batch 94630, train_perplexity=80.64398

Batch 94640, train_perplexity=62.84781

Batch 94650, train_perplexity=72.37213

Batch 94660, train_perplexity=68.14006

Batch 94670, train_perplexity=65.78556

Batch 94680, train_perplexity=68.78487

Batch 94690, train_perplexity=74.82276

Batch 94700, train_perplexity=72.886696

Batch 94710, train_perplexity=74.29183

Batch 94720, train_perplexity=72.75578

Batch 94730, train_perplexity=74.68841

Batch 94740, train_perplexity=67.03587

Batch 94750, train_perplexity=75.74999

Batch 94760, train_perplexity=79.08615

Batch 94770, train_perplexity=70.39322

Batch 94780, train_perplexity=68.121346

Batch 94790, train_perplexity=67.28522

Batch 94800, train_perplexity=69.3036

Batch 94810, train_perplexity=77.11493

Batch 94820, train_perplexity=71.42393

Batch 94830, train_perplexity=76.1865

Batch 94840, train_perplexity=69.97012

Batch 94850, train_perplexity=71.08047

Batch 94860, train_perplexity=74.43374

Batch 94870, train_perplexity=80.62453

Batch 94880, train_perplexity=77.15697

Batch 94890, train_perplexity=72.02747

Batch 94900, train_perplexity=76.60111

Batch 94910, train_perplexity=64.75364

Batch 94920, train_perplexity=69.60239

Batch 94930, train_perplexity=70.55045

Batch 94940, train_perplexity=74.4903

Batch 94950, train_perplexity=70.8433

Batch 94960, train_perplexity=71.28415

Batch 94970, train_perplexity=74.420074

Batch 94980, train_perplexity=71.27188

Batch 94990, train_perplexity=74.840744

Batch 95000, train_perplexity=74.14224

Batch 95010, train_perplexity=71.29958

Batch 95020, train_perplexity=75.96478

Batch 95030, train_perplexity=74.824295

Batch 95040, train_perplexity=79.017654

Batch 95050, train_perplexity=77.643

Batch 95060, train_perplexity=68.53127

Batch 95070, train_perplexity=77.19557

Batch 95080, train_perplexity=71.28823

Batch 95090, train_perplexity=68.50425

Batch 95100, train_perplexity=69.281525

Batch 95110, train_perplexity=73.830376

Batch 95120, train_perplexity=70.31724

Batch 95130, train_perplexity=78.684944

Batch 95140, train_perplexity=74.066444

Batch 95150, train_perplexity=74.88212

Batch 95160, train_perplexity=75.31699

Batch 95170, train_perplexity=70.8806

Batch 95180, train_perplexity=67.49161

Batch 95190, train_perplexity=73.54808

Batch 95200, train_perplexity=70.48405

Batch 95210, train_perplexity=66.96709

Batch 95220, train_perplexity=63.422787

Batch 95230, train_perplexity=73.9556

Batch 95240, train_perplexity=72.92654

Batch 95250, train_perplexity=71.340805

Batch 95260, train_perplexity=68.24144

Batch 95270, train_perplexity=68.245735

Batch 95280, train_perplexity=83.692696

Batch 95290, train_perplexity=74.51414

Batch 95300, train_perplexity=80.30875

Batch 95310, train_perplexity=70.65088

Batch 95320, train_perplexity=65.77973

Batch 95330, train_perplexity=76.37979

Batch 95340, train_perplexity=74.74548

Batch 95350, train_perplexity=76.92216

Batch 95360, train_perplexity=68.88356

Batch 95370, train_perplexity=77.203964

Batch 95380, train_perplexity=76.38562

Batch 95390, train_perplexity=75.57999

Batch 95400, train_perplexity=81.484634

Batch 95410, train_perplexity=70.30493

Batch 95420, train_perplexity=78.75634

Batch 95430, train_perplexity=73.89782

Batch 95440, train_perplexity=68.76027

Batch 95450, train_perplexity=68.56304

Batch 95460, train_perplexity=72.927505

Batch 95470, train_perplexity=75.7905

Batch 95480, train_perplexity=77.553825

Batch 95490, train_perplexity=69.34254

Batch 95500, train_perplexity=72.174034

Batch 95510, train_perplexity=66.43761

Batch 95520, train_perplexity=74.57866

Batch 95530, train_perplexity=71.79605

Batch 95540, train_perplexity=70.80298

Batch 95550, train_perplexity=71.81214

Batch 95560, train_perplexity=70.28636

Batch 95570, train_perplexity=75.23609

Batch 95580, train_perplexity=82.67884

Batch 95590, train_perplexity=68.2862

Batch 95600, train_perplexity=73.16722

Batch 95610, train_perplexity=66.186485

Batch 95620, train_perplexity=71.69554

Batch 95630, train_perplexity=67.04284

Batch 95640, train_perplexity=77.98986

Batch 95650, train_perplexity=65.93895

Batch 95660, train_perplexity=79.87315

Batch 95670, train_perplexity=71.49275

Batch 95680, train_perplexity=76.33129

Batch 95690, train_perplexity=71.68631

Batch 95700, train_perplexity=76.811134

Batch 95710, train_perplexity=72.10266

Batch 95720, train_perplexity=73.090294

Batch 95730, train_perplexity=69.08455

Batch 95740, train_perplexity=68.14376

Batch 95750, train_perplexity=70.63706

Batch 95760, train_perplexity=64.02061

Batch 95770, train_perplexity=77.090515

Batch 95780, train_perplexity=70.92056

Batch 95790, train_perplexity=76.746735

Batch 95800, train_perplexity=72.14582

Batch 95810, train_perplexity=68.95212

Batch 95820, train_perplexity=73.58993

Batch 95830, train_perplexity=72.325554

Batch 95840, train_perplexity=70.86104

Batch 95850, train_perplexity=75.945366

Batch 95860, train_perplexity=72.90685

Batch 95870, train_perplexity=71.586494

Batch 95880, train_perplexity=73.59986

Batch 95890, train_perplexity=71.090836

Batch 95900, train_perplexity=84.13355

Batch 95910, train_perplexity=70.57794

Batch 95920, train_perplexity=80.237976

Batch 95930, train_perplexity=73.0551

Batch 95940, train_perplexity=76.67047

Batch 95950, train_perplexity=81.98401

Batch 95960, train_perplexity=71.575676

Batch 95970, train_perplexity=69.03294

Batch 95980, train_perplexity=68.96267

Batch 95990, train_perplexity=70.34722

Batch 96000, train_perplexity=66.953
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 96010, train_perplexity=73.892006

Batch 96020, train_perplexity=72.06994

Batch 96030, train_perplexity=73.60853

Batch 96040, train_perplexity=71.4616

Batch 96050, train_perplexity=80.72948

Batch 96060, train_perplexity=73.06061

Batch 96070, train_perplexity=64.903534

Batch 96080, train_perplexity=71.11759

Batch 96090, train_perplexity=65.442345

Batch 96100, train_perplexity=69.12884

Batch 96110, train_perplexity=70.00039

Batch 96120, train_perplexity=70.70581

Batch 96130, train_perplexity=70.51043

Batch 96140, train_perplexity=83.38493

Batch 96150, train_perplexity=80.3266

Batch 96160, train_perplexity=66.00784

Batch 96170, train_perplexity=71.394844

Batch 96180, train_perplexity=71.98589

Batch 96190, train_perplexity=70.439964

Batch 96200, train_perplexity=64.31578

Batch 96210, train_perplexity=69.602715

Batch 96220, train_perplexity=71.66573

Batch 96230, train_perplexity=75.17688

Batch 96240, train_perplexity=68.65955

Batch 96250, train_perplexity=68.79444

Batch 96260, train_perplexity=70.34611

Batch 96270, train_perplexity=68.49377

Batch 96280, train_perplexity=65.9934

Batch 96290, train_perplexity=72.56446

Batch 96300, train_perplexity=72.735664

Batch 96310, train_perplexity=75.06953

Batch 96320, train_perplexity=75.66357

Batch 96330, train_perplexity=65.6408

Batch 96340, train_perplexity=70.07881

Batch 96350, train_perplexity=67.838036

Batch 96360, train_perplexity=70.32532

Batch 96370, train_perplexity=69.11163

Batch 96380, train_perplexity=67.53864

Batch 96390, train_perplexity=75.52353

Batch 96400, train_perplexity=76.105095

Batch 96410, train_perplexity=77.14178

Batch 96420, train_perplexity=76.6602

Batch 96430, train_perplexity=77.37017

Batch 96440, train_perplexity=67.60147

Batch 96450, train_perplexity=67.3817

Batch 96460, train_perplexity=70.919685

Batch 96470, train_perplexity=69.782364

Batch 96480, train_perplexity=77.23475

Batch 96490, train_perplexity=69.56213

Batch 96500, train_perplexity=74.35077

Batch 96510, train_perplexity=75.05728

Batch 96520, train_perplexity=67.18698

Batch 96530, train_perplexity=68.971085

Batch 96540, train_perplexity=73.50418

Batch 96550, train_perplexity=66.70966

Batch 96560, train_perplexity=71.307

Batch 96570, train_perplexity=80.39657

Batch 96580, train_perplexity=68.4506

Batch 96590, train_perplexity=75.88289

Batch 96600, train_perplexity=72.35046

Batch 96610, train_perplexity=72.49267

Batch 96620, train_perplexity=75.956085

Batch 96630, train_perplexity=70.15467

Batch 96640, train_perplexity=70.10588

Batch 96650, train_perplexity=74.46214

Batch 96660, train_perplexity=74.62241

Batch 96670, train_perplexity=76.44487

Batch 96680, train_perplexity=69.28351

Batch 96690, train_perplexity=73.96646

Batch 96700, train_perplexity=73.31886

Batch 96710, train_perplexity=67.88366

Batch 96720, train_perplexity=75.69894

Batch 96730, train_perplexity=76.954994

Batch 96740, train_perplexity=71.00262

Batch 96750, train_perplexity=73.31222

Batch 96760, train_perplexity=72.359634

Batch 96770, train_perplexity=77.44762

Batch 96780, train_perplexity=73.2793

Batch 96790, train_perplexity=65.109604

Batch 96800, train_perplexity=66.92223

Batch 96810, train_perplexity=80.26602

Batch 96820, train_perplexity=73.93603

Batch 96830, train_perplexity=62.27347

Batch 96840, train_perplexity=70.27071

Batch 96850, train_perplexity=77.02298

Batch 96860, train_perplexity=69.275375

Batch 96870, train_perplexity=68.97862

Batch 96880, train_perplexity=76.175674

Batch 96890, train_perplexity=72.19283

Batch 96900, train_perplexity=68.462875

Batch 96910, train_perplexity=74.13149

Batch 96920, train_perplexity=73.059425

Batch 96930, train_perplexity=70.6502

Batch 96940, train_perplexity=67.49492

Batch 96950, train_perplexity=60.654575

Batch 96960, train_perplexity=76.68652

Batch 96970, train_perplexity=69.597336

Batch 96980, train_perplexity=69.8662

Batch 96990, train_perplexity=74.00943

Batch 97000, train_perplexity=69.31503

Batch 97010, train_perplexity=73.92972

Batch 97020, train_perplexity=73.8634

Batch 97030, train_perplexity=75.36291

Batch 97040, train_perplexity=65.379745

Batch 97050, train_perplexity=70.75664

Batch 97060, train_perplexity=72.764046

Batch 97070, train_perplexity=76.082054

Batch 97080, train_perplexity=62.879494

Batch 97090, train_perplexity=69.68966

Batch 97100, train_perplexity=66.16648

Batch 97110, train_perplexity=64.46445

Batch 97120, train_perplexity=69.024254

Batch 97130, train_perplexity=74.000824

Batch 97140, train_perplexity=71.5113

Batch 97150, train_perplexity=75.40647

Batch 97160, train_perplexity=73.91029

Batch 97170, train_perplexity=71.96791

Batch 97180, train_perplexity=67.14618

Batch 97190, train_perplexity=71.85955

Batch 97200, train_perplexity=76.1456

Batch 97210, train_perplexity=67.99595

Batch 97220, train_perplexity=79.86325

Batch 97230, train_perplexity=69.939804

Batch 97240, train_perplexity=69.295204

Batch 97250, train_perplexity=68.040054

Batch 97260, train_perplexity=72.38842

Batch 97270, train_perplexity=74.17598

Batch 97280, train_perplexity=70.0276

Batch 97290, train_perplexity=78.85092

Batch 97300, train_perplexity=66.6042

Batch 97310, train_perplexity=73.57782

Batch 97320, train_perplexity=69.509186

Batch 97330, train_perplexity=75.46698

Batch 97340, train_perplexity=71.963486

Batch 97350, train_perplexity=62.11958

Batch 97360, train_perplexity=71.19335

Batch 97370, train_perplexity=73.86207

Batch 97380, train_perplexity=68.64175

Batch 97390, train_perplexity=63.719913

Batch 97400, train_perplexity=72.55803

Batch 97410, train_perplexity=69.65119

Batch 97420, train_perplexity=70.894394

Batch 97430, train_perplexity=61.621555

Batch 97440, train_perplexity=73.46522

Batch 97450, train_perplexity=70.42982

Batch 97460, train_perplexity=68.716286

Batch 97470, train_perplexity=74.70223

Batch 97480, train_perplexity=75.249794

Batch 97490, train_perplexity=74.34971

Batch 97500, train_perplexity=75.63208

Batch 97510, train_perplexity=64.78258

Batch 97520, train_perplexity=72.29001

Batch 97530, train_perplexity=74.85955

Batch 97540, train_perplexity=72.063034

Batch 97550, train_perplexity=74.2799

Batch 97560, train_perplexity=67.371475

Batch 97570, train_perplexity=69.399895

Batch 97580, train_perplexity=69.32131

Batch 97590, train_perplexity=71.745056

Batch 97600, train_perplexity=71.94746

Batch 97610, train_perplexity=69.16573

Batch 97620, train_perplexity=73.06263

Batch 97630, train_perplexity=71.44

Batch 97640, train_perplexity=79.64659

Batch 97650, train_perplexity=69.78064

Batch 97660, train_perplexity=71.334885

Batch 97670, train_perplexity=68.714874

Batch 97680, train_perplexity=72.409

Batch 97690, train_perplexity=81.235725

Batch 97700, train_perplexity=72.186356

Batch 97710, train_perplexity=71.62193

Batch 97720, train_perplexity=67.417534

Batch 97730, train_perplexity=64.36011

Batch 97740, train_perplexity=74.517586

Batch 97750, train_perplexity=77.05336

Batch 97760, train_perplexity=69.775444

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00068-of-00100
Loaded 306324 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00068-of-00100
Loaded 306324 sentences.
Finished loading
Batch 97770, train_perplexity=71.60732

Batch 97780, train_perplexity=62.626198

Batch 97790, train_perplexity=65.508125

Batch 97800, train_perplexity=77.23445

Batch 97810, train_perplexity=69.26547

Batch 97820, train_perplexity=75.81605

Batch 97830, train_perplexity=72.46146

Batch 97840, train_perplexity=78.35576

Batch 97850, train_perplexity=68.37186

Batch 97860, train_perplexity=69.06327

Batch 97870, train_perplexity=74.16763

Batch 97880, train_perplexity=78.96971

Batch 97890, train_perplexity=69.63827

Batch 97900, train_perplexity=67.874794

Batch 97910, train_perplexity=74.88954

Batch 97920, train_perplexity=70.74318

Batch 97930, train_perplexity=75.70886

Batch 97940, train_perplexity=68.77673
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 97950, train_perplexity=75.23178

Batch 97960, train_perplexity=71.23658

Batch 97970, train_perplexity=68.60346

Batch 97980, train_perplexity=67.95041

Batch 97990, train_perplexity=71.96012

Batch 98000, train_perplexity=60.3946

Batch 98010, train_perplexity=66.90163

Batch 98020, train_perplexity=65.66634

Batch 98030, train_perplexity=72.27543

Batch 98040, train_perplexity=69.522575

Batch 98050, train_perplexity=72.087944

Batch 98060, train_perplexity=75.748695

Batch 98070, train_perplexity=70.71336

Batch 98080, train_perplexity=69.090805

Batch 98090, train_perplexity=66.71103

Batch 98100, train_perplexity=77.19896

Batch 98110, train_perplexity=60.583813

Batch 98120, train_perplexity=71.31792

Batch 98130, train_perplexity=82.38812

Batch 98140, train_perplexity=65.04022

Batch 98150, train_perplexity=76.60689

Batch 98160, train_perplexity=69.12353

Batch 98170, train_perplexity=70.30024

Batch 98180, train_perplexity=59.885956

Batch 98190, train_perplexity=64.63836

Batch 98200, train_perplexity=65.000046

Batch 98210, train_perplexity=76.59607

Batch 98220, train_perplexity=73.006905

Batch 98230, train_perplexity=72.033966

Batch 98240, train_perplexity=74.18822

Batch 98250, train_perplexity=67.96262

Batch 98260, train_perplexity=74.21468

Batch 98270, train_perplexity=67.15543

Batch 98280, train_perplexity=71.55169

Batch 98290, train_perplexity=74.13012

Batch 98300, train_perplexity=71.56547

Batch 98310, train_perplexity=72.61348

Batch 98320, train_perplexity=72.2857

Batch 98330, train_perplexity=75.76394

Batch 98340, train_perplexity=73.413704

Batch 98350, train_perplexity=75.375916

Batch 98360, train_perplexity=66.16354

Batch 98370, train_perplexity=70.36262

Batch 98380, train_perplexity=73.71953

Batch 98390, train_perplexity=69.85767

Batch 98400, train_perplexity=68.1014

Batch 98410, train_perplexity=78.45315

Batch 98420, train_perplexity=69.70684

Batch 98430, train_perplexity=69.1505

Batch 98440, train_perplexity=79.08559

Batch 98450, train_perplexity=73.697556

Batch 98460, train_perplexity=73.39253

Batch 98470, train_perplexity=71.22408

Batch 98480, train_perplexity=77.8123

Batch 98490, train_perplexity=69.21004

Batch 98500, train_perplexity=75.18889

Batch 98510, train_perplexity=76.80392

Batch 98520, train_perplexity=73.25027

Batch 98530, train_perplexity=71.78503

Batch 98540, train_perplexity=74.93441

Batch 98550, train_perplexity=74.6564

Batch 98560, train_perplexity=68.187645

Batch 98570, train_perplexity=67.119545

Batch 98580, train_perplexity=76.311424

Batch 98590, train_perplexity=74.95428

Batch 98600, train_perplexity=74.31709

Batch 98610, train_perplexity=73.957115

Batch 98620, train_perplexity=69.20162

Batch 98630, train_perplexity=61.33807

Batch 98640, train_perplexity=72.35702

Batch 98650, train_perplexity=66.8137

Batch 98660, train_perplexity=69.20341

Batch 98670, train_perplexity=71.66772

Batch 98680, train_perplexity=66.636444

Batch 98690, train_perplexity=72.10111

Batch 98700, train_perplexity=77.8024

Batch 98710, train_perplexity=67.07197

Batch 98720, train_perplexity=71.728226

Batch 98730, train_perplexity=72.176926

Batch 98740, train_perplexity=74.72364

Batch 98750, train_perplexity=75.28407

Batch 98760, train_perplexity=76.35605

Batch 98770, train_perplexity=73.0161

Batch 98780, train_perplexity=70.463844

Batch 98790, train_perplexity=75.36765

Batch 98800, train_perplexity=73.25086

Batch 98810, train_perplexity=71.94447

Batch 98820, train_perplexity=64.79716

Batch 98830, train_perplexity=74.49414

Batch 98840, train_perplexity=63.16694

Batch 98850, train_perplexity=72.99284

Batch 98860, train_perplexity=68.42737

Batch 98870, train_perplexity=68.90889

Batch 98880, train_perplexity=70.0064

Batch 98890, train_perplexity=72.050354

Batch 98900, train_perplexity=64.79586

Batch 98910, train_perplexity=77.09309

Batch 98920, train_perplexity=67.16654

Batch 98930, train_perplexity=78.54714

Batch 98940, train_perplexity=71.910965

Batch 98950, train_perplexity=74.822296

Batch 98960, train_perplexity=81.52645

Batch 98970, train_perplexity=69.44826

Batch 98980, train_perplexity=76.41651

Batch 98990, train_perplexity=70.127945

Batch 99000, train_perplexity=63.962902

Batch 99010, train_perplexity=64.48129

Batch 99020, train_perplexity=71.33379

Batch 99030, train_perplexity=67.34822

Batch 99040, train_perplexity=70.572495

Batch 99050, train_perplexity=74.00146

Batch 99060, train_perplexity=74.772934

Batch 99070, train_perplexity=69.14245

Batch 99080, train_perplexity=76.557655

Batch 99090, train_perplexity=69.90386

Batch 99100, train_perplexity=62.639008

Batch 99110, train_perplexity=83.410225

Batch 99120, train_perplexity=70.339066

Batch 99130, train_perplexity=67.16705

Batch 99140, train_perplexity=70.942924

Batch 99150, train_perplexity=77.389435

Batch 99160, train_perplexity=66.11242

Batch 99170, train_perplexity=63.799206

Batch 99180, train_perplexity=68.45289

Batch 99190, train_perplexity=74.547226

Batch 99200, train_perplexity=69.87044

Batch 99210, train_perplexity=72.95325

Batch 99220, train_perplexity=72.542534

Batch 99230, train_perplexity=73.35502

Batch 99240, train_perplexity=69.26739

Batch 99250, train_perplexity=69.52198

Batch 99260, train_perplexity=77.1913

Batch 99270, train_perplexity=71.29353

Batch 99280, train_perplexity=76.29549

Batch 99290, train_perplexity=73.647675

Batch 99300, train_perplexity=70.18619

Batch 99310, train_perplexity=73.614006

Batch 99320, train_perplexity=74.03015

Batch 99330, train_perplexity=66.76331

Batch 99340, train_perplexity=74.30569

Batch 99350, train_perplexity=66.54011

Batch 99360, train_perplexity=72.89045

Batch 99370, train_perplexity=70.99839

Batch 99380, train_perplexity=74.437325

Batch 99390, train_perplexity=73.84963

Batch 99400, train_perplexity=64.940125

Batch 99410, train_perplexity=69.40062

Batch 99420, train_perplexity=76.171135

Batch 99430, train_perplexity=72.2788

Batch 99440, train_perplexity=75.764656

Batch 99450, train_perplexity=69.823105

Batch 99460, train_perplexity=81.65449

Batch 99470, train_perplexity=69.05909

Batch 99480, train_perplexity=76.69921

Batch 99490, train_perplexity=69.12907

Batch 99500, train_perplexity=74.43655

Batch 99510, train_perplexity=62.71866

Batch 99520, train_perplexity=67.42968

Batch 99530, train_perplexity=68.09072

Batch 99540, train_perplexity=70.73737

Batch 99550, train_perplexity=66.40043

Batch 99560, train_perplexity=75.9194

Batch 99570, train_perplexity=74.10782

Batch 99580, train_perplexity=68.8096

Batch 99590, train_perplexity=72.56135

Batch 99600, train_perplexity=72.52316

Batch 99610, train_perplexity=75.115

Batch 99620, train_perplexity=67.89017

Batch 99630, train_perplexity=81.53046

Batch 99640, train_perplexity=70.533806

Batch 99650, train_perplexity=73.5774

Batch 99660, train_perplexity=75.3504

Batch 99670, train_perplexity=71.893654

Batch 99680, train_perplexity=67.419205

Batch 99690, train_perplexity=71.67968

Batch 99700, train_perplexity=65.7001

Batch 99710, train_perplexity=70.05642

Batch 99720, train_perplexity=73.93748

Batch 99730, train_perplexity=67.88897

Batch 99740, train_perplexity=71.458466

Batch 99750, train_perplexity=70.320625

Batch 99760, train_perplexity=71.77188

Batch 99770, train_perplexity=64.0673

Batch 99780, train_perplexity=72.855804

Batch 99790, train_perplexity=73.85481

Batch 99800, train_perplexity=71.827385

Batch 99810, train_perplexity=69.29956

Batch 99820, train_perplexity=74.9318

Batch 99830, train_perplexity=74.55618

Batch 99840, train_perplexity=70.58602

Batch 99850, train_perplexity=70.41122

Batch 99860, train_perplexity=69.28443

Batch 99870, train_perplexity=71.397026

Batch 99880, train_perplexity=71.620605

Batch 99890, train_perplexity=72.159996

Batch 99900, train_perplexity=72.82218

Batch 99910, train_perplexity=78.44039

Batch 99920, train_perplexity=64.14155

Batch 99930, train_perplexity=70.96001

Batch 99940, train_perplexity=76.43255

Batch 99950, train_perplexity=70.366844

Batch 99960, train_perplexity=76.77254

Batch 99970, train_perplexity=71.079414

Batch 99980, train_perplexity=73.300545
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 99990, train_perplexity=76.7661

Batch 100000, train_perplexity=75.192764

Batch 100010, train_perplexity=75.66956

Batch 100020, train_perplexity=78.95401

Batch 100030, train_perplexity=72.72956

Batch 100040, train_perplexity=65.584854

Batch 100050, train_perplexity=68.87315

Batch 100060, train_perplexity=77.904785

Batch 100070, train_perplexity=69.36482

Batch 100080, train_perplexity=69.73766

Batch 100090, train_perplexity=65.2715

Batch 100100, train_perplexity=80.26258

Batch 100110, train_perplexity=74.539764

Batch 100120, train_perplexity=68.69332

Batch 100130, train_perplexity=74.51439

Batch 100140, train_perplexity=70.69111

Batch 100150, train_perplexity=82.29456

Batch 100160, train_perplexity=67.02328

Batch 100170, train_perplexity=71.19607

Batch 100180, train_perplexity=70.202286

Batch 100190, train_perplexity=81.39198

Batch 100200, train_perplexity=72.440765

Batch 100210, train_perplexity=82.16846

Batch 100220, train_perplexity=67.938225

Batch 100230, train_perplexity=76.013626

Batch 100240, train_perplexity=74.94463

Batch 100250, train_perplexity=71.62788

Batch 100260, train_perplexity=71.987885

Batch 100270, train_perplexity=73.879814

Batch 100280, train_perplexity=79.690926

Batch 100290, train_perplexity=68.84485

Batch 100300, train_perplexity=70.29464

Batch 100310, train_perplexity=79.187775

Batch 100320, train_perplexity=66.00359

Batch 100330, train_perplexity=71.44031

Batch 100340, train_perplexity=64.10537

Batch 100350, train_perplexity=68.7406

Batch 100360, train_perplexity=72.1245

Batch 100370, train_perplexity=68.40949

Batch 100380, train_perplexity=72.07836

Batch 100390, train_perplexity=72.72429

Batch 100400, train_perplexity=73.87253

Batch 100410, train_perplexity=69.0583

Batch 100420, train_perplexity=70.22998

Batch 100430, train_perplexity=71.605034

Batch 100440, train_perplexity=69.645676

Batch 100450, train_perplexity=74.64074

Batch 100460, train_perplexity=69.90952

Batch 100470, train_perplexity=61.817974

Batch 100480, train_perplexity=78.25345

Batch 100490, train_perplexity=64.50005

Batch 100500, train_perplexity=73.37888

Batch 100510, train_perplexity=82.01016

Batch 100520, train_perplexity=79.96114

Batch 100530, train_perplexity=64.22265

Batch 100540, train_perplexity=73.566

Batch 100550, train_perplexity=71.67691

Batch 100560, train_perplexity=79.97994

Batch 100570, train_perplexity=70.4704

Batch 100580, train_perplexity=75.966194

Batch 100590, train_perplexity=79.348755

Batch 100600, train_perplexity=68.19662

Batch 100610, train_perplexity=67.3969

Batch 100620, train_perplexity=69.32395

Batch 100630, train_perplexity=62.542667

Batch 100640, train_perplexity=66.357605

Batch 100650, train_perplexity=74.96443

Batch 100660, train_perplexity=64.705765

Batch 100670, train_perplexity=72.558685

Batch 100680, train_perplexity=67.4347

Batch 100690, train_perplexity=69.26474

Batch 100700, train_perplexity=74.676765

Batch 100710, train_perplexity=74.04579

Batch 100720, train_perplexity=68.04989

Batch 100730, train_perplexity=68.15826

Batch 100740, train_perplexity=73.387314

Batch 100750, train_perplexity=69.790855

Batch 100760, train_perplexity=70.12146

Batch 100770, train_perplexity=72.53638

Batch 100780, train_perplexity=74.65743

Batch 100790, train_perplexity=73.094475

Batch 100800, train_perplexity=67.961266

Batch 100810, train_perplexity=75.1728

Batch 100820, train_perplexity=79.16644

Batch 100830, train_perplexity=77.12221

Batch 100840, train_perplexity=70.62417

Batch 100850, train_perplexity=75.59682

Batch 100860, train_perplexity=67.74801

Batch 100870, train_perplexity=66.305824

Batch 100880, train_perplexity=80.38813

Batch 100890, train_perplexity=77.19112

Batch 100900, train_perplexity=71.73138

Batch 100910, train_perplexity=74.13863

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00046-of-00100
Loaded 305308 sentences.
Finished loading
Batch 100920, train_perplexity=69.95968

Batch 100930, train_perplexity=66.77083

Batch 100940, train_perplexity=70.78623

Batch 100950, train_perplexity=71.70921

Batch 100960, train_perplexity=73.3821

Batch 100970, train_perplexity=80.01626

Batch 100980, train_perplexity=65.88888

Batch 100990, train_perplexity=68.874825

Batch 101000, train_perplexity=73.78589

Batch 101010, train_perplexity=74.04558

Batch 101020, train_perplexity=69.5696

Batch 101030, train_perplexity=70.79865

Batch 101040, train_perplexity=63.30987

Batch 101050, train_perplexity=68.736305

Batch 101060, train_perplexity=75.52018

Batch 101070, train_perplexity=73.11389

Batch 101080, train_perplexity=70.044266

Batch 101090, train_perplexity=73.62675

Batch 101100, train_perplexity=71.82118

Batch 101110, train_perplexity=69.03745

Batch 101120, train_perplexity=75.26903

Batch 101130, train_perplexity=70.21759

Batch 101140, train_perplexity=69.741684

Batch 101150, train_perplexity=68.19883

Batch 101160, train_perplexity=68.027596

Batch 101170, train_perplexity=69.56363

Batch 101180, train_perplexity=66.013695

Batch 101190, train_perplexity=60.563854

Batch 101200, train_perplexity=74.020584

Batch 101210, train_perplexity=68.61236

Batch 101220, train_perplexity=66.64706

Batch 101230, train_perplexity=72.11858

Batch 101240, train_perplexity=71.36898

Batch 101250, train_perplexity=72.02813

Batch 101260, train_perplexity=71.06877

Batch 101270, train_perplexity=72.824646

Batch 101280, train_perplexity=69.824974

Batch 101290, train_perplexity=64.014435

Batch 101300, train_perplexity=72.05753

Batch 101310, train_perplexity=70.5632

Batch 101320, train_perplexity=74.50877

Batch 101330, train_perplexity=77.97075

Batch 101340, train_perplexity=67.065346

Batch 101350, train_perplexity=71.18218

Batch 101360, train_perplexity=71.57714

Batch 101370, train_perplexity=65.02838

Batch 101380, train_perplexity=74.07637

Batch 101390, train_perplexity=68.980034

Batch 101400, train_perplexity=67.12207

Batch 101410, train_perplexity=65.34194

Batch 101420, train_perplexity=73.2547

Batch 101430, train_perplexity=71.66915

Batch 101440, train_perplexity=78.36682

Batch 101450, train_perplexity=69.165596

Batch 101460, train_perplexity=78.2436

Batch 101470, train_perplexity=71.12173

Batch 101480, train_perplexity=78.09354

Batch 101490, train_perplexity=73.19765

Batch 101500, train_perplexity=71.93933

Batch 101510, train_perplexity=68.64214

Batch 101520, train_perplexity=65.24393

Batch 101530, train_perplexity=73.35075

Batch 101540, train_perplexity=73.46886

Batch 101550, train_perplexity=63.93924

Batch 101560, train_perplexity=67.202484

Batch 101570, train_perplexity=69.53319

Batch 101580, train_perplexity=70.0255

Batch 101590, train_perplexity=73.045555

Batch 101600, train_perplexity=62.860847

Batch 101610, train_perplexity=69.00175

Batch 101620, train_perplexity=67.75835

Batch 101630, train_perplexity=65.871

Batch 101640, train_perplexity=70.30584

Batch 101650, train_perplexity=76.61821

Batch 101660, train_perplexity=70.56556

Batch 101670, train_perplexity=72.83736

Batch 101680, train_perplexity=79.88968

Batch 101690, train_perplexity=65.21388

Batch 101700, train_perplexity=63.01306

Batch 101710, train_perplexity=74.85941

Batch 101720, train_perplexity=67.148094

Batch 101730, train_perplexity=66.543

Batch 101740, train_perplexity=69.136055

Batch 101750, train_perplexity=77.49885

Batch 101760, train_perplexity=70.70177

Batch 101770, train_perplexity=70.24046

Batch 101780, train_perplexity=70.40967

Batch 101790, train_perplexity=67.3673

Batch 101800, train_perplexity=66.368454

Batch 101810, train_perplexity=66.241516

Batch 101820, train_perplexity=70.705605

Batch 101830, train_perplexity=65.37254

Batch 101840, train_perplexity=66.82001

Batch 101850, train_perplexity=62.290283

Batch 101860, train_perplexity=66.0747

Batch 101870, train_perplexity=75.68594

Batch 101880, train_perplexity=68.215675
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 101890, train_perplexity=76.38256

Batch 101900, train_perplexity=79.38766

Batch 101910, train_perplexity=68.05904

Batch 101920, train_perplexity=68.92719

Batch 101930, train_perplexity=67.93942

Batch 101940, train_perplexity=67.46104

Batch 101950, train_perplexity=59.547985

Batch 101960, train_perplexity=69.53796

Batch 101970, train_perplexity=66.50481

Batch 101980, train_perplexity=75.01406

Batch 101990, train_perplexity=71.637344

Batch 102000, train_perplexity=69.20294

Batch 102010, train_perplexity=75.31527

Batch 102020, train_perplexity=73.06068

Batch 102030, train_perplexity=73.06012

Batch 102040, train_perplexity=66.54293

Batch 102050, train_perplexity=71.0122

Batch 102060, train_perplexity=64.85596

Batch 102070, train_perplexity=74.630486

Batch 102080, train_perplexity=72.9362

Batch 102090, train_perplexity=69.8654

Batch 102100, train_perplexity=67.51482

Batch 102110, train_perplexity=68.00794

Batch 102120, train_perplexity=64.86616

Batch 102130, train_perplexity=67.49009

Batch 102140, train_perplexity=67.10066

Batch 102150, train_perplexity=72.24287

Batch 102160, train_perplexity=73.732285

Batch 102170, train_perplexity=74.871124

Batch 102180, train_perplexity=68.09595

Batch 102190, train_perplexity=74.57468

Batch 102200, train_perplexity=72.91395

Batch 102210, train_perplexity=64.94614

Batch 102220, train_perplexity=76.331696

Batch 102230, train_perplexity=69.24932

Batch 102240, train_perplexity=67.12415

Batch 102250, train_perplexity=67.63568

Batch 102260, train_perplexity=67.819275

Batch 102270, train_perplexity=75.69237

Batch 102280, train_perplexity=74.81188

Batch 102290, train_perplexity=68.43001

Batch 102300, train_perplexity=58.671986

Batch 102310, train_perplexity=72.33445

Batch 102320, train_perplexity=74.25404

Batch 102330, train_perplexity=80.32315

Batch 102340, train_perplexity=75.59891

Batch 102350, train_perplexity=77.287575

Batch 102360, train_perplexity=74.55

Batch 102370, train_perplexity=75.893456

Batch 102380, train_perplexity=67.38719

Batch 102390, train_perplexity=73.682625

Batch 102400, train_perplexity=72.144966

Batch 102410, train_perplexity=66.70814

Batch 102420, train_perplexity=73.01812

Batch 102430, train_perplexity=63.766907

Batch 102440, train_perplexity=68.757355

Batch 102450, train_perplexity=69.9418

Batch 102460, train_perplexity=71.96894

Batch 102470, train_perplexity=78.057724

Batch 102480, train_perplexity=75.4118

Batch 102490, train_perplexity=75.57054

Batch 102500, train_perplexity=67.43168

Batch 102510, train_perplexity=68.82703

Batch 102520, train_perplexity=71.31842

Batch 102530, train_perplexity=68.12368

Batch 102540, train_perplexity=67.13949

Batch 102550, train_perplexity=74.32581

Batch 102560, train_perplexity=62.842625

Batch 102570, train_perplexity=75.136246

Batch 102580, train_perplexity=67.32109

Batch 102590, train_perplexity=67.195564

Batch 102600, train_perplexity=76.36348

Batch 102610, train_perplexity=71.74252

Batch 102620, train_perplexity=79.94509

Batch 102630, train_perplexity=68.74902

Batch 102640, train_perplexity=68.20221

Batch 102650, train_perplexity=64.67224

Batch 102660, train_perplexity=64.38734

Batch 102670, train_perplexity=70.3333

Batch 102680, train_perplexity=73.76028

Batch 102690, train_perplexity=73.7905

Batch 102700, train_perplexity=71.71342

Batch 102710, train_perplexity=70.74314

Batch 102720, train_perplexity=69.735664

Batch 102730, train_perplexity=73.10249

Batch 102740, train_perplexity=73.85354

Batch 102750, train_perplexity=71.996056

Batch 102760, train_perplexity=74.151505

Batch 102770, train_perplexity=59.94847

Batch 102780, train_perplexity=76.20368

Batch 102790, train_perplexity=73.98805

Batch 102800, train_perplexity=74.8839

Batch 102810, train_perplexity=70.86719

Batch 102820, train_perplexity=73.75247

Batch 102830, train_perplexity=68.32962

Batch 102840, train_perplexity=76.19383

Batch 102850, train_perplexity=72.00519

Batch 102860, train_perplexity=70.91309

Batch 102870, train_perplexity=70.387856

Batch 102880, train_perplexity=68.72815

Batch 102890, train_perplexity=63.996216

Batch 102900, train_perplexity=65.42044

Batch 102910, train_perplexity=68.60353

Batch 102920, train_perplexity=71.43475

Batch 102930, train_perplexity=64.15411

Batch 102940, train_perplexity=69.25358

Batch 102950, train_perplexity=66.713165

Batch 102960, train_perplexity=69.9829

Batch 102970, train_perplexity=71.17105

Batch 102980, train_perplexity=71.89245

Batch 102990, train_perplexity=77.100586

Batch 103000, train_perplexity=74.089905

Batch 103010, train_perplexity=68.60611

Batch 103020, train_perplexity=68.24147

Batch 103030, train_perplexity=75.52195

Batch 103040, train_perplexity=72.05025

Batch 103050, train_perplexity=75.04741

Batch 103060, train_perplexity=72.638306

Batch 103070, train_perplexity=69.09298

Batch 103080, train_perplexity=74.98223

Batch 103090, train_perplexity=69.96188

Batch 103100, train_perplexity=68.92914

Batch 103110, train_perplexity=75.790565

Batch 103120, train_perplexity=73.67476

Batch 103130, train_perplexity=71.60056

Batch 103140, train_perplexity=66.09881

Batch 103150, train_perplexity=68.78631

Batch 103160, train_perplexity=73.546394

Batch 103170, train_perplexity=67.34717

Batch 103180, train_perplexity=75.2587

Batch 103190, train_perplexity=72.9234

Batch 103200, train_perplexity=72.759186

Batch 103210, train_perplexity=66.35339

Batch 103220, train_perplexity=63.216686

Batch 103230, train_perplexity=64.17584

Batch 103240, train_perplexity=66.86323

Batch 103250, train_perplexity=72.7908

Batch 103260, train_perplexity=72.35066

Batch 103270, train_perplexity=72.65213

Batch 103280, train_perplexity=60.24268

Batch 103290, train_perplexity=78.56497

Batch 103300, train_perplexity=63.335777

Batch 103310, train_perplexity=71.60968

Batch 103320, train_perplexity=75.23892

Batch 103330, train_perplexity=76.204155

Batch 103340, train_perplexity=71.05027

Batch 103350, train_perplexity=73.50891

Batch 103360, train_perplexity=75.690414

Batch 103370, train_perplexity=64.55303

Batch 103380, train_perplexity=76.20492

Batch 103390, train_perplexity=60.88146

Batch 103400, train_perplexity=71.28806

Batch 103410, train_perplexity=76.1897

Batch 103420, train_perplexity=68.406685

Batch 103430, train_perplexity=66.3565

Batch 103440, train_perplexity=68.22407

Batch 103450, train_perplexity=77.09448

Batch 103460, train_perplexity=65.90736

Batch 103470, train_perplexity=67.2155

Batch 103480, train_perplexity=69.977066

Batch 103490, train_perplexity=69.73683

Batch 103500, train_perplexity=73.8465

Batch 103510, train_perplexity=74.21766

Batch 103520, train_perplexity=72.29859

Batch 103530, train_perplexity=67.09919

Batch 103540, train_perplexity=66.77663

Batch 103550, train_perplexity=72.69139

Batch 103560, train_perplexity=71.7534

Batch 103570, train_perplexity=69.849045

Batch 103580, train_perplexity=66.55924

Batch 103590, train_perplexity=72.1159

Batch 103600, train_perplexity=71.04058

Batch 103610, train_perplexity=62.101128

Batch 103620, train_perplexity=70.96495

Batch 103630, train_perplexity=70.1732

Batch 103640, train_perplexity=67.16625

Batch 103650, train_perplexity=66.43761

Batch 103660, train_perplexity=73.48648

Batch 103670, train_perplexity=71.6953

Batch 103680, train_perplexity=66.89764

Batch 103690, train_perplexity=67.04195

Batch 103700, train_perplexity=62.713127

Batch 103710, train_perplexity=72.72665

Batch 103720, train_perplexity=72.27773

Batch 103730, train_perplexity=61.110535

Batch 103740, train_perplexity=76.18915

Batch 103750, train_perplexity=58.871044

Batch 103760, train_perplexity=75.6067

Batch 103770, train_perplexity=69.74435

Batch 103780, train_perplexity=79.85731

Batch 103790, train_perplexity=72.78841

Batch 103800, train_perplexity=74.07008

Batch 103810, train_perplexity=67.77182

Batch 103820, train_perplexity=72.05155

Batch 103830, train_perplexity=73.00423

Batch 103840, train_perplexity=76.09812

Batch 103850, train_perplexity=73.87066

Batch 103860, train_perplexity=62.12109

Batch 103870, train_perplexity=70.559235

Batch 103880, train_perplexity=77.13784
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 103890, train_perplexity=66.95949

Batch 103900, train_perplexity=71.1978

Batch 103910, train_perplexity=70.98417

Batch 103920, train_perplexity=67.844925

Batch 103930, train_perplexity=69.83582

Batch 103940, train_perplexity=70.93913

Batch 103950, train_perplexity=63.23113

Batch 103960, train_perplexity=72.59444

Batch 103970, train_perplexity=73.622536

Batch 103980, train_perplexity=66.3677

Batch 103990, train_perplexity=69.75529

Batch 104000, train_perplexity=71.42393

Batch 104010, train_perplexity=74.72803

Batch 104020, train_perplexity=72.21652

Batch 104030, train_perplexity=77.30173

Batch 104040, train_perplexity=74.62796

Batch 104050, train_perplexity=74.07051

Batch 104060, train_perplexity=66.16515

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00032-of-00100
Loaded 305639 sentences.
Finished loading
Batch 104070, train_perplexity=69.69338

Batch 104080, train_perplexity=69.43796

Batch 104090, train_perplexity=74.57112

Batch 104100, train_perplexity=65.52672

Batch 104110, train_perplexity=74.41972

Batch 104120, train_perplexity=71.520035

Batch 104130, train_perplexity=69.89326

Batch 104140, train_perplexity=69.6223

Batch 104150, train_perplexity=71.197525

Batch 104160, train_perplexity=71.91103

Batch 104170, train_perplexity=67.176476

Batch 104180, train_perplexity=68.2917

Batch 104190, train_perplexity=66.57921

Batch 104200, train_perplexity=74.53255

Batch 104210, train_perplexity=70.02209

Batch 104220, train_perplexity=70.96224

Batch 104230, train_perplexity=73.858826

Batch 104240, train_perplexity=71.51553

Batch 104250, train_perplexity=64.83128

Batch 104260, train_perplexity=65.298836

Batch 104270, train_perplexity=68.31193

Batch 104280, train_perplexity=70.73542

Batch 104290, train_perplexity=60.688538

Batch 104300, train_perplexity=67.792114

Batch 104310, train_perplexity=65.74718

Batch 104320, train_perplexity=71.42195

Batch 104330, train_perplexity=76.41097

Batch 104340, train_perplexity=66.36054

Batch 104350, train_perplexity=71.19244

Batch 104360, train_perplexity=64.27102

Batch 104370, train_perplexity=75.32507

Batch 104380, train_perplexity=76.56642

Batch 104390, train_perplexity=66.86105

Batch 104400, train_perplexity=72.3459

Batch 104410, train_perplexity=62.7084

Batch 104420, train_perplexity=75.764915

Batch 104430, train_perplexity=73.189095

Batch 104440, train_perplexity=78.10996

Batch 104450, train_perplexity=71.10016

Batch 104460, train_perplexity=74.28021

Batch 104470, train_perplexity=63.11145

Batch 104480, train_perplexity=67.947395

Batch 104490, train_perplexity=77.50111

Batch 104500, train_perplexity=71.181435

Batch 104510, train_perplexity=72.668724

Batch 104520, train_perplexity=82.706604

Batch 104530, train_perplexity=69.76234

Batch 104540, train_perplexity=65.66158

Batch 104550, train_perplexity=66.80484

Batch 104560, train_perplexity=71.75224

Batch 104570, train_perplexity=70.692055

Batch 104580, train_perplexity=71.49242

Batch 104590, train_perplexity=73.49976

Batch 104600, train_perplexity=77.20257

Batch 104610, train_perplexity=69.76234

Batch 104620, train_perplexity=68.11595

Batch 104630, train_perplexity=65.97357

Batch 104640, train_perplexity=78.000275

Batch 104650, train_perplexity=68.68844

Batch 104660, train_perplexity=69.97503

Batch 104670, train_perplexity=74.48569

Batch 104680, train_perplexity=66.89748

Batch 104690, train_perplexity=71.81324

Batch 104700, train_perplexity=70.0984

Batch 104710, train_perplexity=67.52293

Batch 104720, train_perplexity=73.079216

Batch 104730, train_perplexity=77.14796

Batch 104740, train_perplexity=72.35405

Batch 104750, train_perplexity=67.99624

Batch 104760, train_perplexity=65.628746

Batch 104770, train_perplexity=65.83571

Batch 104780, train_perplexity=75.1203

Batch 104790, train_perplexity=62.01194

Batch 104800, train_perplexity=77.37663

Batch 104810, train_perplexity=62.108234

Batch 104820, train_perplexity=67.251915

Batch 104830, train_perplexity=67.09401

Batch 104840, train_perplexity=76.947945

Batch 104850, train_perplexity=69.983604

Batch 104860, train_perplexity=72.291664

Batch 104870, train_perplexity=68.734764

Batch 104880, train_perplexity=79.40326

Batch 104890, train_perplexity=68.68716

Batch 104900, train_perplexity=72.98108

Batch 104910, train_perplexity=72.34245

Batch 104920, train_perplexity=70.22395

Batch 104930, train_perplexity=76.29476

Batch 104940, train_perplexity=77.03268

Batch 104950, train_perplexity=73.08109

Batch 104960, train_perplexity=70.951546

Batch 104970, train_perplexity=77.09353

Batch 104980, train_perplexity=69.229805

Batch 104990, train_perplexity=67.91837

Batch 105000, train_perplexity=64.72336

Batch 105010, train_perplexity=70.18007

Batch 105020, train_perplexity=74.90579

Batch 105030, train_perplexity=65.26758

Batch 105040, train_perplexity=67.28733

Batch 105050, train_perplexity=71.99001

Batch 105060, train_perplexity=69.933395

Batch 105070, train_perplexity=71.247894

Batch 105080, train_perplexity=66.88003

Batch 105090, train_perplexity=79.80394

Batch 105100, train_perplexity=62.352985

Batch 105110, train_perplexity=74.471405

Batch 105120, train_perplexity=60.088303

Batch 105130, train_perplexity=71.18059

Batch 105140, train_perplexity=70.3898

Batch 105150, train_perplexity=70.312645

Batch 105160, train_perplexity=65.3574

Batch 105170, train_perplexity=68.43826

Batch 105180, train_perplexity=71.25809

Batch 105190, train_perplexity=75.41503

Batch 105200, train_perplexity=67.43438

Batch 105210, train_perplexity=76.42336

Batch 105220, train_perplexity=63.234352

Batch 105230, train_perplexity=69.10751

Batch 105240, train_perplexity=67.831635

Batch 105250, train_perplexity=73.06402

Batch 105260, train_perplexity=68.339584

Batch 105270, train_perplexity=76.34047

Batch 105280, train_perplexity=66.43584

Batch 105290, train_perplexity=66.50373

Batch 105300, train_perplexity=66.61805

Batch 105310, train_perplexity=61.385883

Batch 105320, train_perplexity=68.20767

Batch 105330, train_perplexity=77.470436

Batch 105340, train_perplexity=71.355095

Batch 105350, train_perplexity=78.977425

Batch 105360, train_perplexity=64.531494

Batch 105370, train_perplexity=74.99933

Batch 105380, train_perplexity=77.24952

Batch 105390, train_perplexity=73.70775

Batch 105400, train_perplexity=67.11148

Batch 105410, train_perplexity=71.17255

Batch 105420, train_perplexity=73.34502

Batch 105430, train_perplexity=61.10488

Batch 105440, train_perplexity=71.722

Batch 105450, train_perplexity=71.95144

Batch 105460, train_perplexity=74.44332

Batch 105470, train_perplexity=62.609684

Batch 105480, train_perplexity=68.12336

Batch 105490, train_perplexity=72.87262

Batch 105500, train_perplexity=69.084114

Batch 105510, train_perplexity=68.11517

Batch 105520, train_perplexity=70.49836

Batch 105530, train_perplexity=62.03986

Batch 105540, train_perplexity=70.4579

Batch 105550, train_perplexity=75.104576

Batch 105560, train_perplexity=75.19359

Batch 105570, train_perplexity=65.52468

Batch 105580, train_perplexity=71.74653

Batch 105590, train_perplexity=70.45491

Batch 105600, train_perplexity=65.36036

Batch 105610, train_perplexity=72.5473

Batch 105620, train_perplexity=66.09779

Batch 105630, train_perplexity=64.08227

Batch 105640, train_perplexity=69.90979

Batch 105650, train_perplexity=67.82345

Batch 105660, train_perplexity=72.73005

Batch 105670, train_perplexity=64.18202

Batch 105680, train_perplexity=66.77828

Batch 105690, train_perplexity=72.13798

Batch 105700, train_perplexity=74.89519

Batch 105710, train_perplexity=71.37851

Batch 105720, train_perplexity=69.54622

Batch 105730, train_perplexity=71.34605

Batch 105740, train_perplexity=71.036995

Batch 105750, train_perplexity=69.46452

Batch 105760, train_perplexity=78.6336

Batch 105770, train_perplexity=64.62751
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 105780, train_perplexity=67.23791

Batch 105790, train_perplexity=69.43657

Batch 105800, train_perplexity=66.249825

Batch 105810, train_perplexity=67.32594

Batch 105820, train_perplexity=66.19425

Batch 105830, train_perplexity=67.3253

Batch 105840, train_perplexity=72.68699

Batch 105850, train_perplexity=74.18917

Batch 105860, train_perplexity=67.36637

Batch 105870, train_perplexity=67.150276

Batch 105880, train_perplexity=73.14552

Batch 105890, train_perplexity=71.834885

Batch 105900, train_perplexity=77.36028

Batch 105910, train_perplexity=69.47648

Batch 105920, train_perplexity=76.8559

Batch 105930, train_perplexity=72.618256

Batch 105940, train_perplexity=66.65675

Batch 105950, train_perplexity=70.070755

Batch 105960, train_perplexity=75.960144

Batch 105970, train_perplexity=73.0981

Batch 105980, train_perplexity=69.98557

Batch 105990, train_perplexity=69.154915

Batch 106000, train_perplexity=71.978035

Batch 106010, train_perplexity=70.37892

Batch 106020, train_perplexity=66.311104

Batch 106030, train_perplexity=63.699986

Batch 106040, train_perplexity=65.04196

Batch 106050, train_perplexity=67.535645

Batch 106060, train_perplexity=70.61993

Batch 106070, train_perplexity=69.52417

Batch 106080, train_perplexity=74.3008

Batch 106090, train_perplexity=69.19836

Batch 106100, train_perplexity=71.71373

Batch 106110, train_perplexity=66.588326

Batch 106120, train_perplexity=73.20023

Batch 106130, train_perplexity=71.42563

Batch 106140, train_perplexity=67.5446

Batch 106150, train_perplexity=67.734406

Batch 106160, train_perplexity=66.530876

Batch 106170, train_perplexity=70.1632

Batch 106180, train_perplexity=72.479256

Batch 106190, train_perplexity=71.57987

Batch 106200, train_perplexity=70.38815

Batch 106210, train_perplexity=65.85555

Batch 106220, train_perplexity=66.24054

Batch 106230, train_perplexity=71.12152

Batch 106240, train_perplexity=63.05478

Batch 106250, train_perplexity=69.16527

Batch 106260, train_perplexity=70.27741

Batch 106270, train_perplexity=76.04937

Batch 106280, train_perplexity=69.99585

Batch 106290, train_perplexity=67.49711

Batch 106300, train_perplexity=69.870804

Batch 106310, train_perplexity=66.63406

Batch 106320, train_perplexity=75.00855

Batch 106330, train_perplexity=65.92983

Batch 106340, train_perplexity=68.318214

Batch 106350, train_perplexity=73.14608

Batch 106360, train_perplexity=64.24442

Batch 106370, train_perplexity=69.915794

Batch 106380, train_perplexity=63.018047

Batch 106390, train_perplexity=80.63322

Batch 106400, train_perplexity=67.25012

Batch 106410, train_perplexity=64.85438

Batch 106420, train_perplexity=73.40618

Batch 106430, train_perplexity=61.8815

Batch 106440, train_perplexity=68.94123

Batch 106450, train_perplexity=73.51803

Batch 106460, train_perplexity=72.85334

Batch 106470, train_perplexity=71.2251

Batch 106480, train_perplexity=68.18059

Batch 106490, train_perplexity=74.16318

Batch 106500, train_perplexity=73.53636

Batch 106510, train_perplexity=66.07608

Batch 106520, train_perplexity=73.69471

Batch 106530, train_perplexity=65.41339

Batch 106540, train_perplexity=63.53084

Batch 106550, train_perplexity=65.69234

Batch 106560, train_perplexity=69.81279

Batch 106570, train_perplexity=66.57054

Batch 106580, train_perplexity=69.73985

Batch 106590, train_perplexity=72.65247

Batch 106600, train_perplexity=71.31431

Batch 106610, train_perplexity=79.394775

Batch 106620, train_perplexity=72.585365

Batch 106630, train_perplexity=68.15468

Batch 106640, train_perplexity=66.74435

Batch 106650, train_perplexity=68.8764

Batch 106660, train_perplexity=64.3417

Batch 106670, train_perplexity=60.883026

Batch 106680, train_perplexity=67.37138

Batch 106690, train_perplexity=68.03818

Batch 106700, train_perplexity=74.48

Batch 106710, train_perplexity=68.32922

Batch 106720, train_perplexity=75.45377

Batch 106730, train_perplexity=70.29635

Batch 106740, train_perplexity=69.20776

Batch 106750, train_perplexity=63.17661

Batch 106760, train_perplexity=65.32829

Batch 106770, train_perplexity=71.51737

Batch 106780, train_perplexity=68.54604

Batch 106790, train_perplexity=70.82283

Batch 106800, train_perplexity=69.63139

Batch 106810, train_perplexity=77.96205

Batch 106820, train_perplexity=68.82368

Batch 106830, train_perplexity=69.31675

Batch 106840, train_perplexity=64.08141

Batch 106850, train_perplexity=67.4275

Batch 106860, train_perplexity=70.128716

Batch 106870, train_perplexity=69.38441

Batch 106880, train_perplexity=70.14935

Batch 106890, train_perplexity=60.664814

Batch 106900, train_perplexity=64.64791

Batch 106910, train_perplexity=59.07962

Batch 106920, train_perplexity=68.530685

Batch 106930, train_perplexity=61.179314

Batch 106940, train_perplexity=76.74794

Batch 106950, train_perplexity=73.95101

Batch 106960, train_perplexity=67.22159

Batch 106970, train_perplexity=75.17211

Batch 106980, train_perplexity=70.5807

Batch 106990, train_perplexity=72.35915

Batch 107000, train_perplexity=63.598587

Batch 107010, train_perplexity=65.59531

Batch 107020, train_perplexity=71.16875

Batch 107030, train_perplexity=73.017456

Batch 107040, train_perplexity=67.253716

Batch 107050, train_perplexity=76.508316

Batch 107060, train_perplexity=74.28932

Batch 107070, train_perplexity=64.57625

Batch 107080, train_perplexity=72.632034

Batch 107090, train_perplexity=77.00425

Batch 107100, train_perplexity=73.99651

Batch 107110, train_perplexity=72.43144

Batch 107120, train_perplexity=66.94831

Batch 107130, train_perplexity=74.601845

Batch 107140, train_perplexity=66.58477

Batch 107150, train_perplexity=84.13444

Batch 107160, train_perplexity=66.6183

Batch 107170, train_perplexity=73.64248

Batch 107180, train_perplexity=71.782906

Batch 107190, train_perplexity=67.19813

Batch 107200, train_perplexity=67.912796

Batch 107210, train_perplexity=61.82947

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00023-of-00100
Loaded 305909 sentences.
Finished loading
Batch 107220, train_perplexity=67.43843

Batch 107230, train_perplexity=75.05993

Batch 107240, train_perplexity=70.58259

Batch 107250, train_perplexity=71.26828

Batch 107260, train_perplexity=71.66631

Batch 107270, train_perplexity=64.717064

Batch 107280, train_perplexity=66.859146

Batch 107290, train_perplexity=70.742775

Batch 107300, train_perplexity=63.419277

Batch 107310, train_perplexity=74.48249

Batch 107320, train_perplexity=78.069374

Batch 107330, train_perplexity=66.6613

Batch 107340, train_perplexity=66.05539

Batch 107350, train_perplexity=69.7484

Batch 107360, train_perplexity=66.66648

Batch 107370, train_perplexity=66.405624

Batch 107380, train_perplexity=72.33859

Batch 107390, train_perplexity=71.062805

Batch 107400, train_perplexity=66.11728

Batch 107410, train_perplexity=64.91861

Batch 107420, train_perplexity=70.52257

Batch 107430, train_perplexity=64.58765

Batch 107440, train_perplexity=68.812294

Batch 107450, train_perplexity=68.53813

Batch 107460, train_perplexity=67.27969

Batch 107470, train_perplexity=73.007675

Batch 107480, train_perplexity=67.017395

Batch 107490, train_perplexity=72.897675

Batch 107500, train_perplexity=72.65531

Batch 107510, train_perplexity=75.64993

Batch 107520, train_perplexity=64.90651

Batch 107530, train_perplexity=67.879326

Batch 107540, train_perplexity=67.55426

Batch 107550, train_perplexity=72.752106

Batch 107560, train_perplexity=71.09426

Batch 107570, train_perplexity=72.08846

Batch 107580, train_perplexity=68.56915

Batch 107590, train_perplexity=69.2409

Batch 107600, train_perplexity=60.734653

Batch 107610, train_perplexity=67.567116

Batch 107620, train_perplexity=65.66672

Batch 107630, train_perplexity=67.70277

Batch 107640, train_perplexity=73.28538

Batch 107650, train_perplexity=75.5316

Batch 107660, train_perplexity=70.94749

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 107670, train_perplexity=72.10077

Batch 107680, train_perplexity=67.74798

Batch 107690, train_perplexity=79.109535

Batch 107700, train_perplexity=69.79521

Batch 107710, train_perplexity=71.16698

Batch 107720, train_perplexity=80.37686

Batch 107730, train_perplexity=75.84939

Batch 107740, train_perplexity=66.1974

Batch 107750, train_perplexity=65.41782

Batch 107760, train_perplexity=72.924416

Batch 107770, train_perplexity=72.4102

Batch 107780, train_perplexity=74.50036

Batch 107790, train_perplexity=71.711334

Batch 107800, train_perplexity=67.82193

Batch 107810, train_perplexity=71.051285

Batch 107820, train_perplexity=66.40806

Batch 107830, train_perplexity=66.09424

Batch 107840, train_perplexity=68.372574

Batch 107850, train_perplexity=78.884575

Batch 107860, train_perplexity=70.222946

Batch 107870, train_perplexity=69.50083

Batch 107880, train_perplexity=72.12621

Batch 107890, train_perplexity=68.48214

Batch 107900, train_perplexity=69.74518

Batch 107910, train_perplexity=72.29142

Batch 107920, train_perplexity=72.60059

Batch 107930, train_perplexity=71.62241

Batch 107940, train_perplexity=74.76345

Batch 107950, train_perplexity=69.50845

Batch 107960, train_perplexity=69.25302

Batch 107970, train_perplexity=62.76931

Batch 107980, train_perplexity=69.72702

Batch 107990, train_perplexity=62.29456

Batch 108000, train_perplexity=68.641685

Batch 108010, train_perplexity=73.05134

Batch 108020, train_perplexity=67.611206

Batch 108030, train_perplexity=67.759026

Batch 108040, train_perplexity=77.8169

Batch 108050, train_perplexity=73.184875

Batch 108060, train_perplexity=70.74473

Batch 108070, train_perplexity=69.920525

Batch 108080, train_perplexity=63.05821

Batch 108090, train_perplexity=71.85838

Batch 108100, train_perplexity=75.91825

Batch 108110, train_perplexity=69.63047

Batch 108120, train_perplexity=67.94895

Batch 108130, train_perplexity=64.293884

Batch 108140, train_perplexity=67.14714

Batch 108150, train_perplexity=66.77134

Batch 108160, train_perplexity=73.61155

Batch 108170, train_perplexity=69.27346

Batch 108180, train_perplexity=67.94513

Batch 108190, train_perplexity=72.128624

Batch 108200, train_perplexity=66.39713

Batch 108210, train_perplexity=74.637

Batch 108220, train_perplexity=74.67328

Batch 108230, train_perplexity=78.80067

Batch 108240, train_perplexity=80.988655

Batch 108250, train_perplexity=70.8684

Batch 108260, train_perplexity=65.991196

Batch 108270, train_perplexity=68.37672

Batch 108280, train_perplexity=78.42326

Batch 108290, train_perplexity=70.93842

Batch 108300, train_perplexity=68.81669

Batch 108310, train_perplexity=65.57472

Batch 108320, train_perplexity=68.2665

Batch 108330, train_perplexity=67.48092

Batch 108340, train_perplexity=67.059845

Batch 108350, train_perplexity=80.22305

Batch 108360, train_perplexity=71.05468

Batch 108370, train_perplexity=67.01624

Batch 108380, train_perplexity=71.9216

Batch 108390, train_perplexity=77.21435

Batch 108400, train_perplexity=73.81598

Batch 108410, train_perplexity=66.81822

Batch 108420, train_perplexity=70.542885

Batch 108430, train_perplexity=68.4031

Batch 108440, train_perplexity=72.677284

Batch 108450, train_perplexity=71.099144

Batch 108460, train_perplexity=71.82605

Batch 108470, train_perplexity=67.79276

Batch 108480, train_perplexity=78.76847

Batch 108490, train_perplexity=67.1039

Batch 108500, train_perplexity=68.6405

Batch 108510, train_perplexity=65.339195

Batch 108520, train_perplexity=72.670944

Batch 108530, train_perplexity=71.65862

Batch 108540, train_perplexity=65.730064

Batch 108550, train_perplexity=66.941574

Batch 108560, train_perplexity=71.03879

Batch 108570, train_perplexity=70.20028

Batch 108580, train_perplexity=62.1723

Batch 108590, train_perplexity=72.52112

Batch 108600, train_perplexity=62.77919

Batch 108610, train_perplexity=68.35282

Batch 108620, train_perplexity=73.871506

Batch 108630, train_perplexity=73.0804

Batch 108640, train_perplexity=75.18975

Batch 108650, train_perplexity=71.05654

Batch 108660, train_perplexity=64.97538

Batch 108670, train_perplexity=72.209526

Batch 108680, train_perplexity=67.811066

Batch 108690, train_perplexity=66.938095

Batch 108700, train_perplexity=67.57536

Batch 108710, train_perplexity=61.599815

Batch 108720, train_perplexity=73.28349

Batch 108730, train_perplexity=65.95096

Batch 108740, train_perplexity=74.01967

Batch 108750, train_perplexity=68.26728

Batch 108760, train_perplexity=68.22472

Batch 108770, train_perplexity=72.21469

Batch 108780, train_perplexity=65.23653

Batch 108790, train_perplexity=66.56416

Batch 108800, train_perplexity=71.610664

Batch 108810, train_perplexity=72.84465

Batch 108820, train_perplexity=71.18239

Batch 108830, train_perplexity=73.33309

Batch 108840, train_perplexity=81.34325

Batch 108850, train_perplexity=73.909485

Batch 108860, train_perplexity=75.40619

Batch 108870, train_perplexity=70.350876

Batch 108880, train_perplexity=66.069786

Batch 108890, train_perplexity=72.19545

Batch 108900, train_perplexity=70.99067

Batch 108910, train_perplexity=66.350136

Batch 108920, train_perplexity=70.01434

Batch 108930, train_perplexity=73.372406

Batch 108940, train_perplexity=69.710266

Batch 108950, train_perplexity=70.11781

Batch 108960, train_perplexity=68.145584

Batch 108970, train_perplexity=68.27842

Batch 108980, train_perplexity=71.51894

Batch 108990, train_perplexity=68.564156

Batch 109000, train_perplexity=74.13348

Batch 109010, train_perplexity=69.27789

Batch 109020, train_perplexity=72.49965

Batch 109030, train_perplexity=73.680626

Batch 109040, train_perplexity=70.50858

Batch 109050, train_perplexity=74.028984

Batch 109060, train_perplexity=78.39971

Batch 109070, train_perplexity=74.71007

Batch 109080, train_perplexity=64.56917

Batch 109090, train_perplexity=80.273834

Batch 109100, train_perplexity=70.09148

Batch 109110, train_perplexity=79.06266

Batch 109120, train_perplexity=67.42891

Batch 109130, train_perplexity=66.27431

Batch 109140, train_perplexity=67.46783

Batch 109150, train_perplexity=62.58449

Batch 109160, train_perplexity=70.32082

Batch 109170, train_perplexity=66.35687

Batch 109180, train_perplexity=73.53994

Batch 109190, train_perplexity=70.63403

Batch 109200, train_perplexity=72.229126

Batch 109210, train_perplexity=75.13105

Batch 109220, train_perplexity=58.914288

Batch 109230, train_perplexity=68.75096

Batch 109240, train_perplexity=69.49092

Batch 109250, train_perplexity=68.485664

Batch 109260, train_perplexity=70.63424

Batch 109270, train_perplexity=74.78727

Batch 109280, train_perplexity=72.05554

Batch 109290, train_perplexity=70.6936

Batch 109300, train_perplexity=72.29173

Batch 109310, train_perplexity=70.330315

Batch 109320, train_perplexity=72.92191

Batch 109330, train_perplexity=74.06479

Batch 109340, train_perplexity=75.357735

Batch 109350, train_perplexity=71.97546

Batch 109360, train_perplexity=68.02543

Batch 109370, train_perplexity=69.60424

Batch 109380, train_perplexity=62.141827

Batch 109390, train_perplexity=68.11173

Batch 109400, train_perplexity=70.755455

Batch 109410, train_perplexity=70.3906

Batch 109420, train_perplexity=77.41904

Batch 109430, train_perplexity=64.05227

Batch 109440, train_perplexity=78.89082

Batch 109450, train_perplexity=73.906845

Batch 109460, train_perplexity=66.47868

Batch 109470, train_perplexity=73.773224

Batch 109480, train_perplexity=71.48478

Batch 109490, train_perplexity=67.593796

Batch 109500, train_perplexity=72.67156

Batch 109510, train_perplexity=73.415665

Batch 109520, train_perplexity=73.570595

Batch 109530, train_perplexity=64.46131

Batch 109540, train_perplexity=71.23597

Batch 109550, train_perplexity=70.18495

Batch 109560, train_perplexity=70.765915

Batch 109570, train_perplexity=61.45049

Batch 109580, train_perplexity=71.13862

Batch 109590, train_perplexity=63.15974

Batch 109600, train_perplexity=65.38084

Batch 109610, train_perplexity=69.35074

Batch 109620, train_perplexity=64.478096

Batch 109630, train_perplexity=66.39251

Batch 109640, train_perplexity=71.03936

Batch 109650, train_perplexity=68.449135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 109660, train_perplexity=63.811436

Batch 109670, train_perplexity=71.623405

Batch 109680, train_perplexity=80.98672

Batch 109690, train_perplexity=69.73367

Batch 109700, train_perplexity=70.15899

Batch 109710, train_perplexity=73.997505

Batch 109720, train_perplexity=73.754684

Batch 109730, train_perplexity=75.21037

Batch 109740, train_perplexity=64.22292

Batch 109750, train_perplexity=65.589516

Batch 109760, train_perplexity=70.409065

Batch 109770, train_perplexity=65.58092

Batch 109780, train_perplexity=70.527885

Batch 109790, train_perplexity=62.1202

Batch 109800, train_perplexity=68.20484

Batch 109810, train_perplexity=81.69534

Batch 109820, train_perplexity=74.850845

Batch 109830, train_perplexity=69.95491

Batch 109840, train_perplexity=70.24716

Batch 109850, train_perplexity=66.13815

Batch 109860, train_perplexity=71.400635

Batch 109870, train_perplexity=70.35923

Batch 109880, train_perplexity=76.022545

Batch 109890, train_perplexity=67.54089

Batch 109900, train_perplexity=71.97772

Batch 109910, train_perplexity=69.518265

Batch 109920, train_perplexity=64.9422

Batch 109930, train_perplexity=71.84948

Batch 109940, train_perplexity=64.70188

Batch 109950, train_perplexity=73.10225

Batch 109960, train_perplexity=78.2524

Batch 109970, train_perplexity=72.63003

Batch 109980, train_perplexity=74.942986

Batch 109990, train_perplexity=67.465256

Batch 110000, train_perplexity=74.64683

Batch 110010, train_perplexity=68.29828

Batch 110020, train_perplexity=68.528595

Batch 110030, train_perplexity=69.36053

Batch 110040, train_perplexity=68.8098

Batch 110050, train_perplexity=67.70057

Batch 110060, train_perplexity=72.844025

Batch 110070, train_perplexity=68.3118

Batch 110080, train_perplexity=72.15026

Batch 110090, train_perplexity=80.53381

Batch 110100, train_perplexity=66.33581

Batch 110110, train_perplexity=70.8803

Batch 110120, train_perplexity=68.23171

Batch 110130, train_perplexity=67.75909

Batch 110140, train_perplexity=64.45351

Batch 110150, train_perplexity=75.191185

Batch 110160, train_perplexity=75.387886

Batch 110170, train_perplexity=64.54079

Batch 110180, train_perplexity=69.7648

Batch 110190, train_perplexity=66.80373

Batch 110200, train_perplexity=67.42927

Batch 110210, train_perplexity=74.80428

Batch 110220, train_perplexity=68.743385

Batch 110230, train_perplexity=73.39162

Batch 110240, train_perplexity=73.282936

Batch 110250, train_perplexity=71.869965

Batch 110260, train_perplexity=66.23706

Batch 110270, train_perplexity=73.391685

Batch 110280, train_perplexity=70.52506

Batch 110290, train_perplexity=69.70554

Batch 110300, train_perplexity=70.57424

Batch 110310, train_perplexity=66.8808

Batch 110320, train_perplexity=74.41532

Batch 110330, train_perplexity=71.65381

Batch 110340, train_perplexity=65.456825

Batch 110350, train_perplexity=68.56494

Batch 110360, train_perplexity=76.06667

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00033-of-00100
Loaded 306700 sentences.
Finished loading
Batch 110370, train_perplexity=71.75888

Batch 110380, train_perplexity=67.1285

Batch 110390, train_perplexity=69.79738

Batch 110400, train_perplexity=70.12213

Batch 110410, train_perplexity=70.666176

Batch 110420, train_perplexity=72.25103

Batch 110430, train_perplexity=67.148994

Batch 110440, train_perplexity=66.599625

Batch 110450, train_perplexity=70.25326

Batch 110460, train_perplexity=68.3712

Batch 110470, train_perplexity=68.20211

Batch 110480, train_perplexity=67.66407

Batch 110490, train_perplexity=71.130066

Batch 110500, train_perplexity=74.55796

Batch 110510, train_perplexity=73.10211

Batch 110520, train_perplexity=61.331753

Batch 110530, train_perplexity=65.5736

Batch 110540, train_perplexity=73.23661

Batch 110550, train_perplexity=61.985806

Batch 110560, train_perplexity=65.631096

Batch 110570, train_perplexity=68.09598

Batch 110580, train_perplexity=70.64144

Batch 110590, train_perplexity=68.07601

Batch 110600, train_perplexity=70.05542

Batch 110610, train_perplexity=70.11591

Batch 110620, train_perplexity=65.93989

Batch 110630, train_perplexity=63.145077

Batch 110640, train_perplexity=70.61201

Batch 110650, train_perplexity=68.20247

Batch 110660, train_perplexity=68.015076

Batch 110670, train_perplexity=72.04503

Batch 110680, train_perplexity=65.78086

Batch 110690, train_perplexity=64.204796

Batch 110700, train_perplexity=74.8774

Batch 110710, train_perplexity=69.18977

Batch 110720, train_perplexity=72.522125

Batch 110730, train_perplexity=69.20578

Batch 110740, train_perplexity=69.83017

Batch 110750, train_perplexity=62.15771

Batch 110760, train_perplexity=70.28542

Batch 110770, train_perplexity=65.5751

Batch 110780, train_perplexity=70.053986

Batch 110790, train_perplexity=66.76424

Batch 110800, train_perplexity=66.08771

Batch 110810, train_perplexity=67.67707

Batch 110820, train_perplexity=67.279724

Batch 110830, train_perplexity=67.81416

Batch 110840, train_perplexity=69.17668

Batch 110850, train_perplexity=73.35537

Batch 110860, train_perplexity=61.54104

Batch 110870, train_perplexity=63.73286

Batch 110880, train_perplexity=78.43486

Batch 110890, train_perplexity=75.806725

Batch 110900, train_perplexity=69.08537

Batch 110910, train_perplexity=65.625084

Batch 110920, train_perplexity=68.387375

Batch 110930, train_perplexity=64.69112

Batch 110940, train_perplexity=72.15366

Batch 110950, train_perplexity=64.09492

Batch 110960, train_perplexity=77.95402

Batch 110970, train_perplexity=71.85859

Batch 110980, train_perplexity=64.07808

Batch 110990, train_perplexity=75.49995

Batch 111000, train_perplexity=72.06667

Batch 111010, train_perplexity=73.26249

Batch 111020, train_perplexity=75.910034

Batch 111030, train_perplexity=74.61508

Batch 111040, train_perplexity=71.876274

Batch 111050, train_perplexity=72.69815

Batch 111060, train_perplexity=70.75674

Batch 111070, train_perplexity=74.10626

Batch 111080, train_perplexity=77.235886

Batch 111090, train_perplexity=65.172356

Batch 111100, train_perplexity=72.62019

Batch 111110, train_perplexity=69.63478

Batch 111120, train_perplexity=69.80224

Batch 111130, train_perplexity=68.850365

Batch 111140, train_perplexity=69.782364

Batch 111150, train_perplexity=63.110306

Batch 111160, train_perplexity=65.986946

Batch 111170, train_perplexity=69.33526

Batch 111180, train_perplexity=64.5158

Batch 111190, train_perplexity=68.435394

Batch 111200, train_perplexity=61.08111

Batch 111210, train_perplexity=70.54995

Batch 111220, train_perplexity=67.10636

Batch 111230, train_perplexity=70.99927

Batch 111240, train_perplexity=62.390102

Batch 111250, train_perplexity=66.19538

Batch 111260, train_perplexity=71.85163

Batch 111270, train_perplexity=67.16725

Batch 111280, train_perplexity=71.75987

Batch 111290, train_perplexity=64.4556

Batch 111300, train_perplexity=67.573364

Batch 111310, train_perplexity=73.02383

Batch 111320, train_perplexity=73.15644

Batch 111330, train_perplexity=67.71839

Batch 111340, train_perplexity=68.121735

Batch 111350, train_perplexity=72.58644

Batch 111360, train_perplexity=71.45254

Batch 111370, train_perplexity=66.4506

Batch 111380, train_perplexity=69.95608

Batch 111390, train_perplexity=70.40994

Batch 111400, train_perplexity=73.17308

Batch 111410, train_perplexity=67.55284

Batch 111420, train_perplexity=67.52576

Batch 111430, train_perplexity=69.531525

Batch 111440, train_perplexity=67.760185

Batch 111450, train_perplexity=64.304924

Batch 111460, train_perplexity=63.913937

Batch 111470, train_perplexity=63.5559

Batch 111480, train_perplexity=61.881557

Batch 111490, train_perplexity=69.847115

Batch 111500, train_perplexity=67.63352

Batch 111510, train_perplexity=66.39143

Batch 111520, train_perplexity=70.09024

Batch 111530, train_perplexity=80.26105

Batch 111540, train_perplexity=70.97713
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 111550, train_perplexity=66.37533

Batch 111560, train_perplexity=65.571915

Batch 111570, train_perplexity=68.74043

Batch 111580, train_perplexity=69.598465

Batch 111590, train_perplexity=65.066284

Batch 111600, train_perplexity=73.59129

Batch 111610, train_perplexity=72.57221

Batch 111620, train_perplexity=65.15091

Batch 111630, train_perplexity=65.541306

Batch 111640, train_perplexity=64.94161

Batch 111650, train_perplexity=68.545456

Batch 111660, train_perplexity=62.740047

Batch 111670, train_perplexity=67.0431

Batch 111680, train_perplexity=66.62841

Batch 111690, train_perplexity=63.236706

Batch 111700, train_perplexity=64.0126

Batch 111710, train_perplexity=72.52116

Batch 111720, train_perplexity=71.79393

Batch 111730, train_perplexity=69.03354

Batch 111740, train_perplexity=62.4616

Batch 111750, train_perplexity=67.38105

Batch 111760, train_perplexity=68.3686

Batch 111770, train_perplexity=73.39925

Batch 111780, train_perplexity=67.87615

Batch 111790, train_perplexity=64.646675

Batch 111800, train_perplexity=70.58838

Batch 111810, train_perplexity=60.957508

Batch 111820, train_perplexity=71.57861

Batch 111830, train_perplexity=66.94553

Batch 111840, train_perplexity=68.45018

Batch 111850, train_perplexity=75.16645

Batch 111860, train_perplexity=65.32892

Batch 111870, train_perplexity=73.74164

Batch 111880, train_perplexity=73.3842

Batch 111890, train_perplexity=69.85062

Batch 111900, train_perplexity=77.79268

Batch 111910, train_perplexity=71.093376

Batch 111920, train_perplexity=68.8201

Batch 111930, train_perplexity=67.47304

Batch 111940, train_perplexity=73.48518

Batch 111950, train_perplexity=62.73448

Batch 111960, train_perplexity=66.16623

Batch 111970, train_perplexity=71.7019

Batch 111980, train_perplexity=69.26454

Batch 111990, train_perplexity=66.19807

Batch 112000, train_perplexity=65.7552

Batch 112010, train_perplexity=63.556778

Batch 112020, train_perplexity=77.48854

Batch 112030, train_perplexity=74.17322

Batch 112040, train_perplexity=71.04838

Batch 112050, train_perplexity=74.235

Batch 112060, train_perplexity=71.66922

Batch 112070, train_perplexity=76.21749

Batch 112080, train_perplexity=66.35093

Batch 112090, train_perplexity=62.02856

Batch 112100, train_perplexity=69.36836

Batch 112110, train_perplexity=69.5428

Batch 112120, train_perplexity=71.52559

Batch 112130, train_perplexity=76.43809

Batch 112140, train_perplexity=70.70149

Batch 112150, train_perplexity=75.407875

Batch 112160, train_perplexity=70.94796

Batch 112170, train_perplexity=67.832214

Batch 112180, train_perplexity=76.77803

Batch 112190, train_perplexity=63.345592

Batch 112200, train_perplexity=66.11608

Batch 112210, train_perplexity=68.07173

Batch 112220, train_perplexity=78.252106

Batch 112230, train_perplexity=69.127945

Batch 112240, train_perplexity=68.82319

Batch 112250, train_perplexity=76.080345

Batch 112260, train_perplexity=73.545204

Batch 112270, train_perplexity=69.73872

Batch 112280, train_perplexity=65.8069

Batch 112290, train_perplexity=65.701294

Batch 112300, train_perplexity=72.32124

Batch 112310, train_perplexity=68.027405

Batch 112320, train_perplexity=67.96787

Batch 112330, train_perplexity=69.06956

Batch 112340, train_perplexity=69.863174

Batch 112350, train_perplexity=82.2129

Batch 112360, train_perplexity=70.85884

Batch 112370, train_perplexity=67.06918

Batch 112380, train_perplexity=65.49416

Batch 112390, train_perplexity=63.30151

Batch 112400, train_perplexity=68.50578

Batch 112410, train_perplexity=69.84442

Batch 112420, train_perplexity=64.675446

Batch 112430, train_perplexity=64.51497

Batch 112440, train_perplexity=67.957794

Batch 112450, train_perplexity=73.92732

Batch 112460, train_perplexity=71.73028

Batch 112470, train_perplexity=63.185825

Batch 112480, train_perplexity=69.48658

Batch 112490, train_perplexity=69.663414

Batch 112500, train_perplexity=69.08952

Batch 112510, train_perplexity=70.84668

Batch 112520, train_perplexity=64.95546

Batch 112530, train_perplexity=73.249954

Batch 112540, train_perplexity=72.87943

Batch 112550, train_perplexity=75.03696

Batch 112560, train_perplexity=74.499825

Batch 112570, train_perplexity=64.8975

Batch 112580, train_perplexity=60.781677

Batch 112590, train_perplexity=81.38531

Batch 112600, train_perplexity=64.00336

Batch 112610, train_perplexity=70.657616

Batch 112620, train_perplexity=67.696175

Batch 112630, train_perplexity=72.09293

Batch 112640, train_perplexity=71.04959

Batch 112650, train_perplexity=72.91516

Batch 112660, train_perplexity=72.81958

Batch 112670, train_perplexity=70.72904

Batch 112680, train_perplexity=72.15828

Batch 112690, train_perplexity=60.006382

Batch 112700, train_perplexity=74.43335

Batch 112710, train_perplexity=63.551144

Batch 112720, train_perplexity=68.42939

Batch 112730, train_perplexity=60.920895

Batch 112740, train_perplexity=71.30098

Batch 112750, train_perplexity=76.54116

Batch 112760, train_perplexity=75.2728

Batch 112770, train_perplexity=77.57594

Batch 112780, train_perplexity=63.75435

Batch 112790, train_perplexity=70.121994

Batch 112800, train_perplexity=73.32697

Batch 112810, train_perplexity=68.95675

Batch 112820, train_perplexity=70.039825

Batch 112830, train_perplexity=66.17096

Batch 112840, train_perplexity=77.50358

Batch 112850, train_perplexity=66.365486

Batch 112860, train_perplexity=64.314064

Batch 112870, train_perplexity=67.3631

Batch 112880, train_perplexity=71.42481

Batch 112890, train_perplexity=66.07687

Batch 112900, train_perplexity=73.79398

Batch 112910, train_perplexity=74.92605

Batch 112920, train_perplexity=73.21318

Batch 112930, train_perplexity=62.55946

Batch 112940, train_perplexity=72.45047

Batch 112950, train_perplexity=59.787834

Batch 112960, train_perplexity=72.68692

Batch 112970, train_perplexity=69.425446

Batch 112980, train_perplexity=70.75714

Batch 112990, train_perplexity=64.24185

Batch 113000, train_perplexity=71.02392

Batch 113010, train_perplexity=67.782745

Batch 113020, train_perplexity=67.24464

Batch 113030, train_perplexity=72.10665

Batch 113040, train_perplexity=64.00592

Batch 113050, train_perplexity=67.14567

Batch 113060, train_perplexity=64.23499

Batch 113070, train_perplexity=69.92006

Batch 113080, train_perplexity=76.491135

Batch 113090, train_perplexity=72.34632

Batch 113100, train_perplexity=64.18067

Batch 113110, train_perplexity=67.99967

Batch 113120, train_perplexity=62.819016

Batch 113130, train_perplexity=76.346725

Batch 113140, train_perplexity=69.20954

Batch 113150, train_perplexity=69.28205

Batch 113160, train_perplexity=67.415665

Batch 113170, train_perplexity=69.17071

Batch 113180, train_perplexity=62.709896

Batch 113190, train_perplexity=71.9978

Batch 113200, train_perplexity=76.19914

Batch 113210, train_perplexity=73.73067

Batch 113220, train_perplexity=67.715225

Batch 113230, train_perplexity=72.07581

Batch 113240, train_perplexity=68.16814

Batch 113250, train_perplexity=67.6916

Batch 113260, train_perplexity=68.43928

Batch 113270, train_perplexity=68.07056

Batch 113280, train_perplexity=67.003914

Batch 113290, train_perplexity=66.016655

Batch 113300, train_perplexity=80.082825

Batch 113310, train_perplexity=72.158966

Batch 113320, train_perplexity=65.49285

Batch 113330, train_perplexity=68.24902

Batch 113340, train_perplexity=71.888306

Batch 113350, train_perplexity=67.594124

Batch 113360, train_perplexity=66.78567

Batch 113370, train_perplexity=67.60069

Batch 113380, train_perplexity=67.36451

Batch 113390, train_perplexity=68.72874

Batch 113400, train_perplexity=74.01183

Batch 113410, train_perplexity=76.63816

Batch 113420, train_perplexity=64.428246

Batch 113430, train_perplexity=75.13589

Batch 113440, train_perplexity=65.97329

Batch 113450, train_perplexity=75.856445

Batch 113460, train_perplexity=65.60897

Batch 113470, train_perplexity=70.329575

Batch 113480, train_perplexity=69.51018

Batch 113490, train_perplexity=67.69263

Batch 113500, train_perplexity=69.941536

Batch 113510, train_perplexity=69.47996

Batch 113520, train_perplexity=62.256966

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 306683 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00087-of-00100
Loaded 306683 sentences.
Finished loading
Batch 113530, train_perplexity=77.64559

Batch 113540, train_perplexity=70.93481

Batch 113550, train_perplexity=68.49821

Batch 113560, train_perplexity=67.50767

Batch 113570, train_perplexity=70.1358

Batch 113580, train_perplexity=69.95868

Batch 113590, train_perplexity=66.550896

Batch 113600, train_perplexity=75.05399

Batch 113610, train_perplexity=66.89282

Batch 113620, train_perplexity=65.0826

Batch 113630, train_perplexity=70.48001

Batch 113640, train_perplexity=73.31404

Batch 113650, train_perplexity=67.12633

Batch 113660, train_perplexity=67.23929

Batch 113670, train_perplexity=71.57441

Batch 113680, train_perplexity=68.394485

Batch 113690, train_perplexity=66.96086

Batch 113700, train_perplexity=70.249306

Batch 113710, train_perplexity=71.08619

Batch 113720, train_perplexity=68.60981

Batch 113730, train_perplexity=70.6242

Batch 113740, train_perplexity=67.48437

Batch 113750, train_perplexity=74.132454

Batch 113760, train_perplexity=69.35795

Batch 113770, train_perplexity=64.2617

Batch 113780, train_perplexity=74.06874

Batch 113790, train_perplexity=68.07906

Batch 113800, train_perplexity=71.49841

Batch 113810, train_perplexity=72.97328

Batch 113820, train_perplexity=69.67809

Batch 113830, train_perplexity=69.07865

Batch 113840, train_perplexity=63.747326

Batch 113850, train_perplexity=73.924995

Batch 113860, train_perplexity=62.988247

Batch 113870, train_perplexity=69.12768

Batch 113880, train_perplexity=63.87625

Batch 113890, train_perplexity=62.420692

Batch 113900, train_perplexity=66.72041

Batch 113910, train_perplexity=65.71947

Batch 113920, train_perplexity=75.405396

Batch 113930, train_perplexity=66.48566

Batch 113940, train_perplexity=68.93515

Batch 113950, train_perplexity=66.1951

Batch 113960, train_perplexity=71.626175

Batch 113970, train_perplexity=71.415924

Batch 113980, train_perplexity=67.62246

Batch 113990, train_perplexity=63.572147

Batch 114000, train_perplexity=63.91601

Batch 114010, train_perplexity=69.23077

Batch 114020, train_perplexity=67.59908

Batch 114030, train_perplexity=65.398705

Batch 114040, train_perplexity=70.978485

Batch 114050, train_perplexity=65.2753

Batch 114060, train_perplexity=67.62326

Batch 114070, train_perplexity=68.73339

Batch 114080, train_perplexity=64.71503

Batch 114090, train_perplexity=68.08741

Batch 114100, train_perplexity=67.55001

Batch 114110, train_perplexity=72.655624

Batch 114120, train_perplexity=68.05823

Batch 114130, train_perplexity=71.42379

Batch 114140, train_perplexity=64.17694

Batch 114150, train_perplexity=75.75476

Batch 114160, train_perplexity=75.0346

Batch 114170, train_perplexity=66.915215

Batch 114180, train_perplexity=67.190186

Batch 114190, train_perplexity=82.04216

Batch 114200, train_perplexity=73.17102

Batch 114210, train_perplexity=74.97308

Batch 114220, train_perplexity=66.009544

Batch 114230, train_perplexity=73.43233

Batch 114240, train_perplexity=71.85633

Batch 114250, train_perplexity=68.53365

Batch 114260, train_perplexity=74.90426

Batch 114270, train_perplexity=68.352036

Batch 114280, train_perplexity=67.06752

Batch 114290, train_perplexity=67.39825

Batch 114300, train_perplexity=68.54095

Batch 114310, train_perplexity=61.118694

Batch 114320, train_perplexity=69.42313

Batch 114330, train_perplexity=68.82095

Batch 114340, train_perplexity=70.81186

Batch 114350, train_perplexity=61.81939

Batch 114360, train_perplexity=66.23397

Batch 114370, train_perplexity=69.34601

Batch 114380, train_perplexity=69.336845

Batch 114390, train_perplexity=63.0752

Batch 114400, train_perplexity=68.215866

Batch 114410, train_perplexity=69.82291

Batch 114420, train_perplexity=70.13096

Batch 114430, train_perplexity=63.45013

Batch 114440, train_perplexity=67.92446

Batch 114450, train_perplexity=65.23428

Batch 114460, train_perplexity=68.83333

Batch 114470, train_perplexity=76.30313

Batch 114480, train_perplexity=71.03371

Batch 114490, train_perplexity=76.675446

Batch 114500, train_perplexity=71.33822

Batch 114510, train_perplexity=74.71334

Batch 114520, train_perplexity=68.18114

Batch 114530, train_perplexity=73.66836

Batch 114540, train_perplexity=61.930737

Batch 114550, train_perplexity=66.23902

Batch 114560, train_perplexity=80.30308

Batch 114570, train_perplexity=63.968147

Batch 114580, train_perplexity=64.62165

Batch 114590, train_perplexity=69.90946

Batch 114600, train_perplexity=66.16837

Batch 114610, train_perplexity=67.41917

Batch 114620, train_perplexity=72.63945

Batch 114630, train_perplexity=68.20744

Batch 114640, train_perplexity=60.73399

Batch 114650, train_perplexity=67.24114

Batch 114660, train_perplexity=64.98049

Batch 114670, train_perplexity=71.709724

Batch 114680, train_perplexity=75.542114

Batch 114690, train_perplexity=69.15828

Batch 114700, train_perplexity=76.6908

Batch 114710, train_perplexity=69.10942

Batch 114720, train_perplexity=65.56998

Batch 114730, train_perplexity=65.44284

Batch 114740, train_perplexity=66.62885

Batch 114750, train_perplexity=69.86647

Batch 114760, train_perplexity=62.029152

Batch 114770, train_perplexity=65.16738

Batch 114780, train_perplexity=75.00791

Batch 114790, train_perplexity=71.29248

Batch 114800, train_perplexity=68.66342

Batch 114810, train_perplexity=67.02171

Batch 114820, train_perplexity=64.1756

Batch 114830, train_perplexity=74.812096

Batch 114840, train_perplexity=69.66703

Batch 114850, train_perplexity=64.489876

Batch 114860, train_perplexity=72.721344

Batch 114870, train_perplexity=65.863686

Batch 114880, train_perplexity=69.04605

Batch 114890, train_perplexity=71.52078

Batch 114900, train_perplexity=70.998116

Batch 114910, train_perplexity=66.02648

Batch 114920, train_perplexity=73.63654

Batch 114930, train_perplexity=66.965294

Batch 114940, train_perplexity=71.461395

Batch 114950, train_perplexity=69.191986

Batch 114960, train_perplexity=65.4931

Batch 114970, train_perplexity=62.23785

Batch 114980, train_perplexity=64.8661

Batch 114990, train_perplexity=72.55055

Batch 115000, train_perplexity=72.78424

Batch 115010, train_perplexity=69.848885

Batch 115020, train_perplexity=68.530785

Batch 115030, train_perplexity=62.42474

Batch 115040, train_perplexity=72.39836

Batch 115050, train_perplexity=67.26648

Batch 115060, train_perplexity=63.62792

Batch 115070, train_perplexity=62.30329

Batch 115080, train_perplexity=73.255196

Batch 115090, train_perplexity=58.895218

Batch 115100, train_perplexity=65.8374

Batch 115110, train_perplexity=71.069145

Batch 115120, train_perplexity=68.38102

Batch 115130, train_perplexity=71.54353

Batch 115140, train_perplexity=63.12623

Batch 115150, train_perplexity=74.21125

Batch 115160, train_perplexity=68.94883

Batch 115170, train_perplexity=64.05187

Batch 115180, train_perplexity=64.80451

Batch 115190, train_perplexity=69.71632

Batch 115200, train_perplexity=71.67127

Batch 115210, train_perplexity=63.26922

Batch 115220, train_perplexity=77.292145

Batch 115230, train_perplexity=71.98109

Batch 115240, train_perplexity=61.403065

Batch 115250, train_perplexity=74.507286

Batch 115260, train_perplexity=64.25337

Batch 115270, train_perplexity=66.17254

Batch 115280, train_perplexity=69.438354

Batch 115290, train_perplexity=71.10535

Batch 115300, train_perplexity=67.51256

Batch 115310, train_perplexity=72.26157

Batch 115320, train_perplexity=68.9583

Batch 115330, train_perplexity=63.166336

Batch 115340, train_perplexity=70.49702

Batch 115350, train_perplexity=65.34845

Batch 115360, train_perplexity=72.12271

Batch 115370, train_perplexity=66.206116

Batch 115380, train_perplexity=65.39309

Batch 115390, train_perplexity=76.17444

Batch 115400, train_perplexity=70.86624

Batch 115410, train_perplexity=72.95958

Batch 115420, train_perplexity=67.85557

Batch 115430, train_perplexity=64.71836

Batch 115440, train_perplexity=69.88353

Batch 115450, train_perplexity=66.781784
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 115460, train_perplexity=72.54461

Batch 115470, train_perplexity=63.15227

Batch 115480, train_perplexity=66.06033

Batch 115490, train_perplexity=66.55182

Batch 115500, train_perplexity=67.12338

Batch 115510, train_perplexity=67.06471

Batch 115520, train_perplexity=69.1381

Batch 115530, train_perplexity=70.39127

Batch 115540, train_perplexity=70.08797

Batch 115550, train_perplexity=74.10301

Batch 115560, train_perplexity=75.19506

Batch 115570, train_perplexity=64.532104

Batch 115580, train_perplexity=64.59454

Batch 115590, train_perplexity=76.33217

Batch 115600, train_perplexity=66.92606

Batch 115610, train_perplexity=72.20047

Batch 115620, train_perplexity=62.24776

Batch 115630, train_perplexity=67.0012

Batch 115640, train_perplexity=69.23522

Batch 115650, train_perplexity=70.5149

Batch 115660, train_perplexity=65.905754

Batch 115670, train_perplexity=78.59809

Batch 115680, train_perplexity=63.337982

Batch 115690, train_perplexity=62.782574

Batch 115700, train_perplexity=72.11288

Batch 115710, train_perplexity=73.73573

Batch 115720, train_perplexity=64.853355

Batch 115730, train_perplexity=62.902645

Batch 115740, train_perplexity=73.54103

Batch 115750, train_perplexity=69.96408

Batch 115760, train_perplexity=67.13209

Batch 115770, train_perplexity=64.89536

Batch 115780, train_perplexity=76.21971

Batch 115790, train_perplexity=66.43238

Batch 115800, train_perplexity=77.56836

Batch 115810, train_perplexity=75.71612

Batch 115820, train_perplexity=68.28145

Batch 115830, train_perplexity=65.7389

Batch 115840, train_perplexity=67.73163

Batch 115850, train_perplexity=66.241295

Batch 115860, train_perplexity=70.69728

Batch 115870, train_perplexity=62.368774

Batch 115880, train_perplexity=65.592926

Batch 115890, train_perplexity=68.73473

Batch 115900, train_perplexity=67.868935

Batch 115910, train_perplexity=69.643745

Batch 115920, train_perplexity=66.0552

Batch 115930, train_perplexity=62.30751

Batch 115940, train_perplexity=74.54204

Batch 115950, train_perplexity=60.550716

Batch 115960, train_perplexity=63.056736

Batch 115970, train_perplexity=67.82394

Batch 115980, train_perplexity=62.740406

Batch 115990, train_perplexity=72.72006

Batch 116000, train_perplexity=73.75208

Batch 116010, train_perplexity=66.69446

Batch 116020, train_perplexity=70.25671

Batch 116030, train_perplexity=67.38622

Batch 116040, train_perplexity=69.39446

Batch 116050, train_perplexity=69.17384

Batch 116060, train_perplexity=63.447227

Batch 116070, train_perplexity=70.633965

Batch 116080, train_perplexity=73.36632

Batch 116090, train_perplexity=65.6815

Batch 116100, train_perplexity=63.269463

Batch 116110, train_perplexity=68.25928

Batch 116120, train_perplexity=63.396236

Batch 116130, train_perplexity=70.35275

Batch 116140, train_perplexity=68.19564

Batch 116150, train_perplexity=79.32651

Batch 116160, train_perplexity=68.75506

Batch 116170, train_perplexity=64.880295

Batch 116180, train_perplexity=62.364166

Batch 116190, train_perplexity=63.56781

Batch 116200, train_perplexity=74.639496

Batch 116210, train_perplexity=72.37903

Batch 116220, train_perplexity=67.65307

Batch 116230, train_perplexity=65.88558

Batch 116240, train_perplexity=71.31033

Batch 116250, train_perplexity=61.034847

Batch 116260, train_perplexity=72.55623

Batch 116270, train_perplexity=65.30325

Batch 116280, train_perplexity=69.6975

Batch 116290, train_perplexity=70.425186

Batch 116300, train_perplexity=59.986015

Batch 116310, train_perplexity=65.72715

Batch 116320, train_perplexity=66.88459

Batch 116330, train_perplexity=61.477978

Batch 116340, train_perplexity=78.70251

Batch 116350, train_perplexity=73.042984

Batch 116360, train_perplexity=66.31942

Batch 116370, train_perplexity=64.59861

Batch 116380, train_perplexity=65.27729

Batch 116390, train_perplexity=63.34952

Batch 116400, train_perplexity=73.85087

Batch 116410, train_perplexity=65.35677

Batch 116420, train_perplexity=72.35474

Batch 116430, train_perplexity=65.20735

Batch 116440, train_perplexity=74.51091

Batch 116450, train_perplexity=70.183075

Batch 116460, train_perplexity=71.43796

Batch 116470, train_perplexity=70.48556

Batch 116480, train_perplexity=71.49575

Batch 116490, train_perplexity=65.349884

Batch 116500, train_perplexity=66.92619

Batch 116510, train_perplexity=60.594994

Batch 116520, train_perplexity=70.25835

Batch 116530, train_perplexity=60.132557

Batch 116540, train_perplexity=65.85747

Batch 116550, train_perplexity=76.45129

Batch 116560, train_perplexity=64.92508

Batch 116570, train_perplexity=72.159035

Batch 116580, train_perplexity=72.488655

Batch 116590, train_perplexity=72.62172

Batch 116600, train_perplexity=73.18554

Batch 116610, train_perplexity=65.29136

Batch 116620, train_perplexity=73.92232

Batch 116630, train_perplexity=64.37033

Batch 116640, train_perplexity=67.1102

Batch 116650, train_perplexity=75.76661

Batch 116660, train_perplexity=66.10372

Batch 116670, train_perplexity=72.19751

Batch 116680, train_perplexity=66.55417

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00013-of-00100
Loaded 305575 sentences.
Finished loading
Batch 116690, train_perplexity=71.48409

Batch 116700, train_perplexity=68.307205

Batch 116710, train_perplexity=66.47779

Batch 116720, train_perplexity=66.71577

Batch 116730, train_perplexity=69.00846

Batch 116740, train_perplexity=75.029015

Batch 116750, train_perplexity=71.483894

Batch 116760, train_perplexity=66.48337

Batch 116770, train_perplexity=73.77913

Batch 116780, train_perplexity=68.75463

Batch 116790, train_perplexity=76.09646

Batch 116800, train_perplexity=64.79565

Batch 116810, train_perplexity=74.55753

Batch 116820, train_perplexity=73.437096

Batch 116830, train_perplexity=69.552284

Batch 116840, train_perplexity=63.758026

Batch 116850, train_perplexity=70.760414

Batch 116860, train_perplexity=62.965996

Batch 116870, train_perplexity=64.4205

Batch 116880, train_perplexity=69.67338

Batch 116890, train_perplexity=80.24261

Batch 116900, train_perplexity=73.686104

Batch 116910, train_perplexity=76.06471

Batch 116920, train_perplexity=69.912796

Batch 116930, train_perplexity=73.661194

Batch 116940, train_perplexity=65.5812

Batch 116950, train_perplexity=67.45998

Batch 116960, train_perplexity=60.856964

Batch 116970, train_perplexity=82.089035

Batch 116980, train_perplexity=71.37027

Batch 116990, train_perplexity=70.65943

Batch 117000, train_perplexity=66.18175

Batch 117010, train_perplexity=67.72756

Batch 117020, train_perplexity=68.27949

Batch 117030, train_perplexity=71.46153

Batch 117040, train_perplexity=67.371445

Batch 117050, train_perplexity=67.7883

Batch 117060, train_perplexity=60.5353

Batch 117070, train_perplexity=67.38051

Batch 117080, train_perplexity=68.1842

Batch 117090, train_perplexity=61.67935

Batch 117100, train_perplexity=64.28417

Batch 117110, train_perplexity=69.43074

Batch 117120, train_perplexity=64.19142

Batch 117130, train_perplexity=68.68877

Batch 117140, train_perplexity=71.031235

Batch 117150, train_perplexity=70.66395

Batch 117160, train_perplexity=68.17945

Batch 117170, train_perplexity=68.53611

Batch 117180, train_perplexity=71.388245

Batch 117190, train_perplexity=64.31744

Batch 117200, train_perplexity=66.848114

Batch 117210, train_perplexity=67.74506

Batch 117220, train_perplexity=64.404594

Batch 117230, train_perplexity=59.185356

Batch 117240, train_perplexity=72.06193

Batch 117250, train_perplexity=70.145775

Batch 117260, train_perplexity=70.33786

Batch 117270, train_perplexity=71.716805

Batch 117280, train_perplexity=65.7263

Batch 117290, train_perplexity=65.824036

Batch 117300, train_perplexity=70.221375

Batch 117310, train_perplexity=68.57135

Batch 117320, train_perplexity=70.44742

Batch 117330, train_perplexity=72.26716

Batch 117340, train_perplexity=73.86125
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 117350, train_perplexity=76.191

Batch 117360, train_perplexity=74.04953

Batch 117370, train_perplexity=70.65997

Batch 117380, train_perplexity=71.391884

Batch 117390, train_perplexity=70.458374

Batch 117400, train_perplexity=65.87484

Batch 117410, train_perplexity=70.667725

Batch 117420, train_perplexity=66.48363

Batch 117430, train_perplexity=71.4978

Batch 117440, train_perplexity=65.020256

Batch 117450, train_perplexity=76.29057

Batch 117460, train_perplexity=67.27322

Batch 117470, train_perplexity=63.752434

Batch 117480, train_perplexity=70.82003

Batch 117490, train_perplexity=62.640324

Batch 117500, train_perplexity=64.61666

Batch 117510, train_perplexity=68.98457

Batch 117520, train_perplexity=67.63539

Batch 117530, train_perplexity=61.02248

Batch 117540, train_perplexity=63.97065

Batch 117550, train_perplexity=78.33013

Batch 117560, train_perplexity=68.92105

Batch 117570, train_perplexity=65.98028

Batch 117580, train_perplexity=61.444336

Batch 117590, train_perplexity=69.82361

Batch 117600, train_perplexity=69.94417

Batch 117610, train_perplexity=70.75417

Batch 117620, train_perplexity=75.63947

Batch 117630, train_perplexity=68.668594

Batch 117640, train_perplexity=72.0719

Batch 117650, train_perplexity=76.02019

Batch 117660, train_perplexity=69.05207

Batch 117670, train_perplexity=68.8524

Batch 117680, train_perplexity=71.633514

Batch 117690, train_perplexity=72.200645

Batch 117700, train_perplexity=83.96075

Batch 117710, train_perplexity=66.64893

Batch 117720, train_perplexity=79.39462

Batch 117730, train_perplexity=70.35379

Batch 117740, train_perplexity=72.24362

Batch 117750, train_perplexity=74.87398

Batch 117760, train_perplexity=69.21014

Batch 117770, train_perplexity=68.929955

Batch 117780, train_perplexity=70.611336

Batch 117790, train_perplexity=68.465355

Batch 117800, train_perplexity=64.259895

Batch 117810, train_perplexity=66.029976

Batch 117820, train_perplexity=72.32328

Batch 117830, train_perplexity=68.86622

Batch 117840, train_perplexity=68.06232

Batch 117850, train_perplexity=69.27306

Batch 117860, train_perplexity=63.59322

Batch 117870, train_perplexity=71.44838

Batch 117880, train_perplexity=74.71937

Batch 117890, train_perplexity=76.25897

Batch 117900, train_perplexity=71.420006

Batch 117910, train_perplexity=66.183205

Batch 117920, train_perplexity=63.35629

Batch 117930, train_perplexity=64.67082

Batch 117940, train_perplexity=64.22975

Batch 117950, train_perplexity=68.9198

Batch 117960, train_perplexity=62.809223

Batch 117970, train_perplexity=64.344246

Batch 117980, train_perplexity=75.661766

Batch 117990, train_perplexity=69.01787

Batch 118000, train_perplexity=63.941647

Batch 118010, train_perplexity=73.1728

Batch 118020, train_perplexity=74.93984

Batch 118030, train_perplexity=71.12912

Batch 118040, train_perplexity=74.24852

Batch 118050, train_perplexity=69.616455

Batch 118060, train_perplexity=73.09807

Batch 118070, train_perplexity=68.20403

Batch 118080, train_perplexity=70.935646

Batch 118090, train_perplexity=70.281136

Batch 118100, train_perplexity=72.36902

Batch 118110, train_perplexity=65.66444

Batch 118120, train_perplexity=68.0581

Batch 118130, train_perplexity=74.36304

Batch 118140, train_perplexity=65.14824

Batch 118150, train_perplexity=67.759415

Batch 118160, train_perplexity=69.11516

Batch 118170, train_perplexity=75.56586

Batch 118180, train_perplexity=75.432945

Batch 118190, train_perplexity=77.83486

Batch 118200, train_perplexity=75.26795

Batch 118210, train_perplexity=65.76549

Batch 118220, train_perplexity=65.45982

Batch 118230, train_perplexity=65.7343

Batch 118240, train_perplexity=68.86626

Batch 118250, train_perplexity=73.23776

Batch 118260, train_perplexity=76.63545

Batch 118270, train_perplexity=71.59121

Batch 118280, train_perplexity=66.67084

Batch 118290, train_perplexity=68.31157

Batch 118300, train_perplexity=60.585026

Batch 118310, train_perplexity=66.07227

Batch 118320, train_perplexity=66.58636

Batch 118330, train_perplexity=71.56131

Batch 118340, train_perplexity=65.640045

Batch 118350, train_perplexity=67.77305

Batch 118360, train_perplexity=65.86937

Batch 118370, train_perplexity=64.95617

Batch 118380, train_perplexity=66.0397

Batch 118390, train_perplexity=78.14651

Batch 118400, train_perplexity=65.15635

Batch 118410, train_perplexity=67.025

Batch 118420, train_perplexity=75.21683

Batch 118430, train_perplexity=58.088398

Batch 118440, train_perplexity=72.4796

Batch 118450, train_perplexity=68.8785

Batch 118460, train_perplexity=64.76686

Batch 118470, train_perplexity=70.338264

Batch 118480, train_perplexity=75.06215

Batch 118490, train_perplexity=67.46912

Batch 118500, train_perplexity=70.83766

Batch 118510, train_perplexity=69.28337

Batch 118520, train_perplexity=72.79208

Batch 118530, train_perplexity=66.33644

Batch 118540, train_perplexity=71.65774

Batch 118550, train_perplexity=63.89864

Batch 118560, train_perplexity=76.049805

Batch 118570, train_perplexity=65.85298

Batch 118580, train_perplexity=73.20547

Batch 118590, train_perplexity=66.161995

Batch 118600, train_perplexity=64.92517

Batch 118610, train_perplexity=66.34795

Batch 118620, train_perplexity=67.65739

Batch 118630, train_perplexity=78.59465

Batch 118640, train_perplexity=75.923355

Batch 118650, train_perplexity=71.88073

Batch 118660, train_perplexity=64.75626

Batch 118670, train_perplexity=76.88376

Batch 118680, train_perplexity=64.95899

Batch 118690, train_perplexity=67.10348

Batch 118700, train_perplexity=62.400394

Batch 118710, train_perplexity=68.86041

Batch 118720, train_perplexity=65.988144

Batch 118730, train_perplexity=67.85304

Batch 118740, train_perplexity=70.610664

Batch 118750, train_perplexity=61.079945

Batch 118760, train_perplexity=64.83314

Batch 118770, train_perplexity=67.84793

Batch 118780, train_perplexity=68.16433

Batch 118790, train_perplexity=68.48684

Batch 118800, train_perplexity=56.229652

Batch 118810, train_perplexity=65.27371

Batch 118820, train_perplexity=71.78208

Batch 118830, train_perplexity=71.29537

Batch 118840, train_perplexity=71.43333

Batch 118850, train_perplexity=64.26857

Batch 118860, train_perplexity=76.13878

Batch 118870, train_perplexity=70.797104

Batch 118880, train_perplexity=70.82513

Batch 118890, train_perplexity=69.46813

Batch 118900, train_perplexity=66.30345

Batch 118910, train_perplexity=74.49599

Batch 118920, train_perplexity=70.13701

Batch 118930, train_perplexity=73.52139

Batch 118940, train_perplexity=67.4798

Batch 118950, train_perplexity=74.28875

Batch 118960, train_perplexity=69.02208

Batch 118970, train_perplexity=73.60835

Batch 118980, train_perplexity=67.67474

Batch 118990, train_perplexity=66.71806

Batch 119000, train_perplexity=68.74181

Batch 119010, train_perplexity=66.29356

Batch 119020, train_perplexity=69.83849

Batch 119030, train_perplexity=63.49259

Batch 119040, train_perplexity=70.63127

Batch 119050, train_perplexity=64.49264

Batch 119060, train_perplexity=67.80136

Batch 119070, train_perplexity=61.506306

Batch 119080, train_perplexity=67.84955

Batch 119090, train_perplexity=77.24922

Batch 119100, train_perplexity=74.55597

Batch 119110, train_perplexity=69.60969

Batch 119120, train_perplexity=68.89082

Batch 119130, train_perplexity=72.54312

Batch 119140, train_perplexity=67.92465

Batch 119150, train_perplexity=69.16596

Batch 119160, train_perplexity=68.4335

Batch 119170, train_perplexity=71.90263

Batch 119180, train_perplexity=68.83008

Batch 119190, train_perplexity=68.30336

Batch 119200, train_perplexity=62.813206

Batch 119210, train_perplexity=65.71439

Batch 119220, train_perplexity=77.97212

Batch 119230, train_perplexity=72.99274

Batch 119240, train_perplexity=63.2298

Batch 119250, train_perplexity=68.42287

Batch 119260, train_perplexity=64.69281

Batch 119270, train_perplexity=70.290184

Batch 119280, train_perplexity=62.067795

Batch 119290, train_perplexity=70.753365

Batch 119300, train_perplexity=68.524345

Batch 119310, train_perplexity=73.78891

Batch 119320, train_perplexity=70.28948

Batch 119330, train_perplexity=67.5013

Batch 119340, train_perplexity=62.744534
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 119350, train_perplexity=70.659836

Batch 119360, train_perplexity=68.68768

Batch 119370, train_perplexity=65.50835

Batch 119380, train_perplexity=66.237755

Batch 119390, train_perplexity=66.95377

Batch 119400, train_perplexity=63.99182

Batch 119410, train_perplexity=79.18743

Batch 119420, train_perplexity=66.12377

Batch 119430, train_perplexity=61.243904

Batch 119440, train_perplexity=70.36403

Batch 119450, train_perplexity=67.339874

Batch 119460, train_perplexity=62.1375

Batch 119470, train_perplexity=70.10053

Batch 119480, train_perplexity=62.453682

Batch 119490, train_perplexity=80.276474

Batch 119500, train_perplexity=68.99062

Batch 119510, train_perplexity=67.87

Batch 119520, train_perplexity=71.62891

Batch 119530, train_perplexity=71.431244

Batch 119540, train_perplexity=70.81122

Batch 119550, train_perplexity=70.10394

Batch 119560, train_perplexity=63.550476

Batch 119570, train_perplexity=63.26204

Batch 119580, train_perplexity=73.30229

Batch 119590, train_perplexity=67.683136

Batch 119600, train_perplexity=63.97425

Batch 119610, train_perplexity=67.06336

Batch 119620, train_perplexity=70.34088

Batch 119630, train_perplexity=67.188835

Batch 119640, train_perplexity=67.14694

Batch 119650, train_perplexity=69.1498

Batch 119660, train_perplexity=72.14314

Batch 119670, train_perplexity=63.739365

Batch 119680, train_perplexity=68.61903

Batch 119690, train_perplexity=66.88558

Batch 119700, train_perplexity=73.38833

Batch 119710, train_perplexity=63.745564

Batch 119720, train_perplexity=65.24617

Batch 119730, train_perplexity=68.46915

Batch 119740, train_perplexity=67.55023

Batch 119750, train_perplexity=67.18685

Batch 119760, train_perplexity=62.78356

Batch 119770, train_perplexity=69.6991

Batch 119780, train_perplexity=68.028114

Batch 119790, train_perplexity=74.396126

Batch 119800, train_perplexity=66.82096

Batch 119810, train_perplexity=65.00122

Batch 119820, train_perplexity=75.33998

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00088-of-00100
Loaded 305749 sentences.
Finished loading
Batch 119830, train_perplexity=69.360855

Batch 119840, train_perplexity=65.646935

Batch 119850, train_perplexity=64.06009

Batch 119860, train_perplexity=68.98181

Batch 119870, train_perplexity=73.76949

Batch 119880, train_perplexity=64.74287

Batch 119890, train_perplexity=69.12366

Batch 119900, train_perplexity=62.696503

Batch 119910, train_perplexity=66.66393

Batch 119920, train_perplexity=65.2341

Batch 119930, train_perplexity=71.9122

Batch 119940, train_perplexity=63.788864

Batch 119950, train_perplexity=75.48843

Batch 119960, train_perplexity=76.01949

Batch 119970, train_perplexity=71.324066

Batch 119980, train_perplexity=68.94064

Batch 119990, train_perplexity=68.60232

Batch 120000, train_perplexity=69.03058

Batch 120010, train_perplexity=71.70631

Batch 120020, train_perplexity=66.13925

Batch 120030, train_perplexity=72.49965

Batch 120040, train_perplexity=70.69186

Batch 120050, train_perplexity=76.04901

Batch 120060, train_perplexity=67.80369

Batch 120070, train_perplexity=76.81121

Batch 120080, train_perplexity=65.2809

Batch 120090, train_perplexity=66.585976

Batch 120100, train_perplexity=56.74416

Batch 120110, train_perplexity=63.537113

Batch 120120, train_perplexity=68.67711

Batch 120130, train_perplexity=65.410645

Batch 120140, train_perplexity=68.17896

Batch 120150, train_perplexity=62.298897

Batch 120160, train_perplexity=65.398735

Batch 120170, train_perplexity=68.52088

Batch 120180, train_perplexity=68.53349

Batch 120190, train_perplexity=66.3267

Batch 120200, train_perplexity=64.922356

Batch 120210, train_perplexity=73.06911

Batch 120220, train_perplexity=70.96055

Batch 120230, train_perplexity=68.78053

Batch 120240, train_perplexity=66.999886

Batch 120250, train_perplexity=65.86868

Batch 120260, train_perplexity=69.20423

Batch 120270, train_perplexity=70.59222

Batch 120280, train_perplexity=75.40259

Batch 120290, train_perplexity=65.436325

Batch 120300, train_perplexity=73.76914

Batch 120310, train_perplexity=72.66474

Batch 120320, train_perplexity=70.56489

Batch 120330, train_perplexity=76.46751

Batch 120340, train_perplexity=68.64053

Batch 120350, train_perplexity=66.98078

Batch 120360, train_perplexity=65.00854

Batch 120370, train_perplexity=67.827

Batch 120380, train_perplexity=63.449493

Batch 120390, train_perplexity=63.13872

Batch 120400, train_perplexity=71.712906

Batch 120410, train_perplexity=63.55272

Batch 120420, train_perplexity=63.956985

Batch 120430, train_perplexity=65.45208

Batch 120440, train_perplexity=61.95407

Batch 120450, train_perplexity=70.39517

Batch 120460, train_perplexity=75.99217

Batch 120470, train_perplexity=64.93728

Batch 120480, train_perplexity=70.54639

Batch 120490, train_perplexity=69.80293

Batch 120500, train_perplexity=67.60463

Batch 120510, train_perplexity=68.96211

Batch 120520, train_perplexity=68.845505

Batch 120530, train_perplexity=74.25922

Batch 120540, train_perplexity=64.51353

Batch 120550, train_perplexity=64.82714

Batch 120560, train_perplexity=66.86488

Batch 120570, train_perplexity=69.938736

Batch 120580, train_perplexity=64.43949

Batch 120590, train_perplexity=71.61705

Batch 120600, train_perplexity=67.456535

Batch 120610, train_perplexity=67.89405

Batch 120620, train_perplexity=67.86369

Batch 120630, train_perplexity=68.43272

Batch 120640, train_perplexity=74.43015

Batch 120650, train_perplexity=60.19794

Batch 120660, train_perplexity=65.43005

Batch 120670, train_perplexity=60.200294

Batch 120680, train_perplexity=70.021355

Batch 120690, train_perplexity=65.17748

Batch 120700, train_perplexity=65.849525

Batch 120710, train_perplexity=68.40127

Batch 120720, train_perplexity=68.98648

Batch 120730, train_perplexity=73.82925

Batch 120740, train_perplexity=73.38273

Batch 120750, train_perplexity=74.93019

Batch 120760, train_perplexity=71.28157

Batch 120770, train_perplexity=73.94449

Batch 120780, train_perplexity=70.32217

Batch 120790, train_perplexity=70.528755

Batch 120800, train_perplexity=68.896935

Batch 120810, train_perplexity=70.059364

Batch 120820, train_perplexity=76.547295

Batch 120830, train_perplexity=67.692726

Batch 120840, train_perplexity=69.25233

Batch 120850, train_perplexity=70.79751

Batch 120860, train_perplexity=69.48125

Batch 120870, train_perplexity=63.78552

Batch 120880, train_perplexity=71.70737

Batch 120890, train_perplexity=70.70699

Batch 120900, train_perplexity=73.8763

Batch 120910, train_perplexity=71.24375

Batch 120920, train_perplexity=59.800663

Batch 120930, train_perplexity=69.69258

Batch 120940, train_perplexity=67.2737

Batch 120950, train_perplexity=72.18168

Batch 120960, train_perplexity=66.82342

Batch 120970, train_perplexity=65.4157

Batch 120980, train_perplexity=68.13717

Batch 120990, train_perplexity=66.96779

Batch 121000, train_perplexity=66.425385

Batch 121010, train_perplexity=64.15717

Batch 121020, train_perplexity=68.87571

Batch 121030, train_perplexity=67.0846

Batch 121040, train_perplexity=67.64307

Batch 121050, train_perplexity=70.79734

Batch 121060, train_perplexity=72.77869

Batch 121070, train_perplexity=72.80333

Batch 121080, train_perplexity=64.33075

Batch 121090, train_perplexity=64.54504

Batch 121100, train_perplexity=66.487114

Batch 121110, train_perplexity=70.24016

Batch 121120, train_perplexity=66.24657

Batch 121130, train_perplexity=69.19684

Batch 121140, train_perplexity=70.02182

Batch 121150, train_perplexity=69.260315

Batch 121160, train_perplexity=67.195984

Batch 121170, train_perplexity=64.42204

Batch 121180, train_perplexity=65.10787

Batch 121190, train_perplexity=73.89257

Batch 121200, train_perplexity=58.351574

Batch 121210, train_perplexity=71.31094

Batch 121220, train_perplexity=60.108593

Batch 121230, train_perplexity=68.682014
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 121240, train_perplexity=60.32719

Batch 121250, train_perplexity=70.85952

Batch 121260, train_perplexity=65.30724

Batch 121270, train_perplexity=75.326584

Batch 121280, train_perplexity=71.355705

Batch 121290, train_perplexity=76.19478

Batch 121300, train_perplexity=71.58513

Batch 121310, train_perplexity=68.34539

Batch 121320, train_perplexity=76.60893

Batch 121330, train_perplexity=67.29718

Batch 121340, train_perplexity=74.145744

Batch 121350, train_perplexity=65.67843

Batch 121360, train_perplexity=61.41592

Batch 121370, train_perplexity=68.89818

Batch 121380, train_perplexity=63.915707

Batch 121390, train_perplexity=67.235954

Batch 121400, train_perplexity=78.85994

Batch 121410, train_perplexity=69.44991

Batch 121420, train_perplexity=65.80941

Batch 121430, train_perplexity=69.05915

Batch 121440, train_perplexity=66.5588

Batch 121450, train_perplexity=68.25215

Batch 121460, train_perplexity=70.153435

Batch 121470, train_perplexity=70.60066

Batch 121480, train_perplexity=64.20253

Batch 121490, train_perplexity=71.488594

Batch 121500, train_perplexity=65.376816

Batch 121510, train_perplexity=62.140644

Batch 121520, train_perplexity=69.77854

Batch 121530, train_perplexity=67.11058

Batch 121540, train_perplexity=72.9377

Batch 121550, train_perplexity=70.59114

Batch 121560, train_perplexity=63.544083

Batch 121570, train_perplexity=71.11155

Batch 121580, train_perplexity=65.301544

Batch 121590, train_perplexity=66.22828

Batch 121600, train_perplexity=72.22051

Batch 121610, train_perplexity=68.21359

Batch 121620, train_perplexity=72.58689

Batch 121630, train_perplexity=75.049126

Batch 121640, train_perplexity=69.034325

Batch 121650, train_perplexity=70.301445

Batch 121660, train_perplexity=67.4075

Batch 121670, train_perplexity=64.83543

Batch 121680, train_perplexity=65.12246

Batch 121690, train_perplexity=70.472786

Batch 121700, train_perplexity=76.461525

Batch 121710, train_perplexity=70.64023

Batch 121720, train_perplexity=63.763195

Batch 121730, train_perplexity=61.96967

Batch 121740, train_perplexity=70.90278

Batch 121750, train_perplexity=67.22486

Batch 121760, train_perplexity=67.66158

Batch 121770, train_perplexity=66.48489

Batch 121780, train_perplexity=66.22304

Batch 121790, train_perplexity=67.906586

Batch 121800, train_perplexity=68.30343

Batch 121810, train_perplexity=74.72475

Batch 121820, train_perplexity=65.51816

Batch 121830, train_perplexity=74.4039

Batch 121840, train_perplexity=72.04221

Batch 121850, train_perplexity=67.419846

Batch 121860, train_perplexity=70.255066

Batch 121870, train_perplexity=71.125595

Batch 121880, train_perplexity=74.620346

Batch 121890, train_perplexity=67.67623

Batch 121900, train_perplexity=66.02056

Batch 121910, train_perplexity=65.64753

Batch 121920, train_perplexity=62.701763

Batch 121930, train_perplexity=64.85645

Batch 121940, train_perplexity=68.21776

Batch 121950, train_perplexity=70.449974

Batch 121960, train_perplexity=65.22374

Batch 121970, train_perplexity=73.01262

Batch 121980, train_perplexity=73.942444

Batch 121990, train_perplexity=72.81694

Batch 122000, train_perplexity=66.04556

Batch 122010, train_perplexity=72.99806

Batch 122020, train_perplexity=64.04793

Batch 122030, train_perplexity=65.91782

Batch 122040, train_perplexity=70.985115

Batch 122050, train_perplexity=69.1437

Batch 122060, train_perplexity=69.25345

Batch 122070, train_perplexity=71.384125

Batch 122080, train_perplexity=62.900665

Batch 122090, train_perplexity=67.818756

Batch 122100, train_perplexity=66.94869

Batch 122110, train_perplexity=66.546486

Batch 122120, train_perplexity=70.029305

Batch 122130, train_perplexity=70.14029

Batch 122140, train_perplexity=74.073654

Batch 122150, train_perplexity=68.06258

Batch 122160, train_perplexity=70.125206

Batch 122170, train_perplexity=66.63155

Batch 122180, train_perplexity=62.582703

Batch 122190, train_perplexity=69.47041

Batch 122200, train_perplexity=66.21281

Batch 122210, train_perplexity=72.46025

Batch 122220, train_perplexity=75.418846

Batch 122230, train_perplexity=69.21621

Batch 122240, train_perplexity=70.74885

Batch 122250, train_perplexity=64.624916

Batch 122260, train_perplexity=67.10514

Batch 122270, train_perplexity=62.676025

Batch 122280, train_perplexity=67.260574

Batch 122290, train_perplexity=65.24175

Batch 122300, train_perplexity=73.05552

Batch 122310, train_perplexity=67.922966

Batch 122320, train_perplexity=62.099586

Batch 122330, train_perplexity=72.497505

Batch 122340, train_perplexity=70.269035

Batch 122350, train_perplexity=63.053818

Batch 122360, train_perplexity=64.58789

Batch 122370, train_perplexity=65.851974

Batch 122380, train_perplexity=64.913506

Batch 122390, train_perplexity=68.13837

Batch 122400, train_perplexity=64.89363

Batch 122410, train_perplexity=74.85063

Batch 122420, train_perplexity=68.203835

Batch 122430, train_perplexity=60.800404

Batch 122440, train_perplexity=63.516968

Batch 122450, train_perplexity=67.40978

Batch 122460, train_perplexity=65.332596

Batch 122470, train_perplexity=75.743996

Batch 122480, train_perplexity=75.655525

Batch 122490, train_perplexity=61.75239

Batch 122500, train_perplexity=68.640175

Batch 122510, train_perplexity=67.5246

Batch 122520, train_perplexity=67.95815

Batch 122530, train_perplexity=62.38561

Batch 122540, train_perplexity=70.069115

Batch 122550, train_perplexity=67.62543

Batch 122560, train_perplexity=65.321594

Batch 122570, train_perplexity=67.37511

Batch 122580, train_perplexity=69.395195

Batch 122590, train_perplexity=69.8734

Batch 122600, train_perplexity=72.2695

Batch 122610, train_perplexity=67.22948

Batch 122620, train_perplexity=64.43937

Batch 122630, train_perplexity=64.672424

Batch 122640, train_perplexity=65.565155

Batch 122650, train_perplexity=56.33958

Batch 122660, train_perplexity=74.19416

Batch 122670, train_perplexity=71.64359

Batch 122680, train_perplexity=68.773155

Batch 122690, train_perplexity=66.79446

Batch 122700, train_perplexity=66.13143

Batch 122710, train_perplexity=70.837425

Batch 122720, train_perplexity=61.94574

Batch 122730, train_perplexity=70.67062

Batch 122740, train_perplexity=68.71635

Batch 122750, train_perplexity=69.91146

Batch 122760, train_perplexity=65.927505

Batch 122770, train_perplexity=67.79516

Batch 122780, train_perplexity=70.64582

Batch 122790, train_perplexity=70.98931

Batch 122800, train_perplexity=61.042065

Batch 122810, train_perplexity=61.022015

Batch 122820, train_perplexity=72.346695

Batch 122830, train_perplexity=65.907265

Batch 122840, train_perplexity=69.33341

Batch 122850, train_perplexity=70.11374

Batch 122860, train_perplexity=66.81484

Batch 122870, train_perplexity=62.707745

Batch 122880, train_perplexity=70.83289

Batch 122890, train_perplexity=62.558594

Batch 122900, train_perplexity=70.07306

Batch 122910, train_perplexity=69.065735

Batch 122920, train_perplexity=74.193985

Batch 122930, train_perplexity=63.913544

Batch 122940, train_perplexity=76.78784

Batch 122950, train_perplexity=64.79673

Batch 122960, train_perplexity=61.126534

Batch 122970, train_perplexity=64.21698

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00063-of-00100
Loaded 306817 sentences.
Finished loading
Batch 122980, train_perplexity=69.36224

Batch 122990, train_perplexity=68.44633

Batch 123000, train_perplexity=63.37629

Batch 123010, train_perplexity=68.0288

Batch 123020, train_perplexity=68.21522

Batch 123030, train_perplexity=63.858616

Batch 123040, train_perplexity=65.76615

Batch 123050, train_perplexity=67.70483

Batch 123060, train_perplexity=66.852516

Batch 123070, train_perplexity=70.09826

Batch 123080, train_perplexity=70.74952

Batch 123090, train_perplexity=67.55481

Batch 123100, train_perplexity=70.712555

Batch 123110, train_perplexity=69.9865

Batch 123120, train_perplexity=66.938065
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 123130, train_perplexity=69.258835

Batch 123140, train_perplexity=65.10681

Batch 123150, train_perplexity=67.039894

Batch 123160, train_perplexity=61.032604

Batch 123170, train_perplexity=70.044334

Batch 123180, train_perplexity=70.61955

Batch 123190, train_perplexity=70.66466

Batch 123200, train_perplexity=64.68032

Batch 123210, train_perplexity=61.28416

Batch 123220, train_perplexity=65.38171

Batch 123230, train_perplexity=65.329475

Batch 123240, train_perplexity=71.838036

Batch 123250, train_perplexity=70.0771

Batch 123260, train_perplexity=67.19192

Batch 123270, train_perplexity=62.718212

Batch 123280, train_perplexity=68.86527

Batch 123290, train_perplexity=68.273926

Batch 123300, train_perplexity=60.421562

Batch 123310, train_perplexity=73.22963

Batch 123320, train_perplexity=70.456795

Batch 123330, train_perplexity=66.21565

Batch 123340, train_perplexity=77.626816

Batch 123350, train_perplexity=74.24714

Batch 123360, train_perplexity=61.71801

Batch 123370, train_perplexity=70.73663

Batch 123380, train_perplexity=73.31484

Batch 123390, train_perplexity=62.30763

Batch 123400, train_perplexity=64.49778

Batch 123410, train_perplexity=73.8246

Batch 123420, train_perplexity=69.30776

Batch 123430, train_perplexity=62.377846

Batch 123440, train_perplexity=66.02525

Batch 123450, train_perplexity=70.07413

Batch 123460, train_perplexity=66.24907

Batch 123470, train_perplexity=76.81011

Batch 123480, train_perplexity=69.940865

Batch 123490, train_perplexity=67.932495

Batch 123500, train_perplexity=73.706696

Batch 123510, train_perplexity=67.798225

Batch 123520, train_perplexity=60.075924

Batch 123530, train_perplexity=68.48792

Batch 123540, train_perplexity=62.50108

Batch 123550, train_perplexity=67.48269

Batch 123560, train_perplexity=68.659355

Batch 123570, train_perplexity=58.98622

Batch 123580, train_perplexity=68.16417

Batch 123590, train_perplexity=66.07577

Batch 123600, train_perplexity=63.596977

Batch 123610, train_perplexity=67.6863

Batch 123620, train_perplexity=69.11516

Batch 123630, train_perplexity=67.070404

Batch 123640, train_perplexity=70.56718

Batch 123650, train_perplexity=61.0205

Batch 123660, train_perplexity=71.78366

Batch 123670, train_perplexity=63.192215

Batch 123680, train_perplexity=65.72323

Batch 123690, train_perplexity=66.29422

Batch 123700, train_perplexity=65.76176

Batch 123710, train_perplexity=63.177753

Batch 123720, train_perplexity=65.75483

Batch 123730, train_perplexity=65.436165

Batch 123740, train_perplexity=70.53266

Batch 123750, train_perplexity=61.231293

Batch 123760, train_perplexity=68.23949

Batch 123770, train_perplexity=68.93472

Batch 123780, train_perplexity=66.43295

Batch 123790, train_perplexity=72.93252

Batch 123800, train_perplexity=66.534874

Batch 123810, train_perplexity=60.790836

Batch 123820, train_perplexity=71.75799

Batch 123830, train_perplexity=76.89689

Batch 123840, train_perplexity=70.37798

Batch 123850, train_perplexity=67.569115

Batch 123860, train_perplexity=68.47359

Batch 123870, train_perplexity=68.55209

Batch 123880, train_perplexity=63.352814

Batch 123890, train_perplexity=69.434715

Batch 123900, train_perplexity=75.01482

Batch 123910, train_perplexity=67.29709

Batch 123920, train_perplexity=70.99514

Batch 123930, train_perplexity=70.26723

Batch 123940, train_perplexity=66.69659

Batch 123950, train_perplexity=70.7994

Batch 123960, train_perplexity=68.48237

Batch 123970, train_perplexity=66.7256

Batch 123980, train_perplexity=71.12173

Batch 123990, train_perplexity=69.03449

Batch 124000, train_perplexity=61.920876

Batch 124010, train_perplexity=66.03177

Batch 124020, train_perplexity=67.38263

Batch 124030, train_perplexity=65.34163

Batch 124040, train_perplexity=68.74827

Batch 124050, train_perplexity=70.89524

Batch 124060, train_perplexity=71.889915

Batch 124070, train_perplexity=69.003456

Batch 124080, train_perplexity=71.272736

Batch 124090, train_perplexity=67.80984

Batch 124100, train_perplexity=67.01884

Batch 124110, train_perplexity=63.236465

Batch 124120, train_perplexity=68.98477

Batch 124130, train_perplexity=62.097633

Batch 124140, train_perplexity=63.478153

Batch 124150, train_perplexity=67.056526

Batch 124160, train_perplexity=70.33568

Batch 124170, train_perplexity=68.49769

Batch 124180, train_perplexity=65.22956

Batch 124190, train_perplexity=64.06736

Batch 124200, train_perplexity=72.15366

Batch 124210, train_perplexity=76.28145

Batch 124220, train_perplexity=61.45623

Batch 124230, train_perplexity=68.236496

Batch 124240, train_perplexity=73.55916

Batch 124250, train_perplexity=66.64156

Batch 124260, train_perplexity=71.042816

Batch 124270, train_perplexity=69.6852

Batch 124280, train_perplexity=65.421005

Batch 124290, train_perplexity=80.87886

Batch 124300, train_perplexity=68.832504

Batch 124310, train_perplexity=61.28609

Batch 124320, train_perplexity=71.34101

Batch 124330, train_perplexity=66.23694

Batch 124340, train_perplexity=59.772896

Batch 124350, train_perplexity=63.8314

Batch 124360, train_perplexity=74.541466

Batch 124370, train_perplexity=65.89488

Batch 124380, train_perplexity=66.98826

Batch 124390, train_perplexity=68.32421

Batch 124400, train_perplexity=61.07674

Batch 124410, train_perplexity=68.15156

Batch 124420, train_perplexity=70.80696

Batch 124430, train_perplexity=69.41088

Batch 124440, train_perplexity=70.84962

Batch 124450, train_perplexity=66.644005

Batch 124460, train_perplexity=67.98386

Batch 124470, train_perplexity=69.034035

Batch 124480, train_perplexity=66.67973

Batch 124490, train_perplexity=66.87611

Batch 124500, train_perplexity=63.22193

Batch 124510, train_perplexity=61.493694

Batch 124520, train_perplexity=63.36215

Batch 124530, train_perplexity=72.03792

Batch 124540, train_perplexity=64.19188

Batch 124550, train_perplexity=63.161366

Batch 124560, train_perplexity=70.215546

Batch 124570, train_perplexity=61.84194

Batch 124580, train_perplexity=66.87706

Batch 124590, train_perplexity=66.9728

Batch 124600, train_perplexity=69.74338

Batch 124610, train_perplexity=65.542404

Batch 124620, train_perplexity=66.389694

Batch 124630, train_perplexity=64.477974

Batch 124640, train_perplexity=71.545105

Batch 124650, train_perplexity=75.88336

Batch 124660, train_perplexity=61.46655

Batch 124670, train_perplexity=67.98726

Batch 124680, train_perplexity=67.142654

Batch 124690, train_perplexity=75.813446

Batch 124700, train_perplexity=67.35905

Batch 124710, train_perplexity=66.4576

Batch 124720, train_perplexity=66.26562

Batch 124730, train_perplexity=65.8402

Batch 124740, train_perplexity=69.988106

Batch 124750, train_perplexity=65.693214

Batch 124760, train_perplexity=67.55101

Batch 124770, train_perplexity=67.63255

Batch 124780, train_perplexity=73.729614

Batch 124790, train_perplexity=65.68463

Batch 124800, train_perplexity=71.08924

Batch 124810, train_perplexity=64.100784

Batch 124820, train_perplexity=71.314545

Batch 124830, train_perplexity=70.33635

Batch 124840, train_perplexity=61.62238

Batch 124850, train_perplexity=66.23883

Batch 124860, train_perplexity=63.625793

Batch 124870, train_perplexity=68.418365

Batch 124880, train_perplexity=70.726585

Batch 124890, train_perplexity=63.246296

Batch 124900, train_perplexity=62.25542

Batch 124910, train_perplexity=70.69951

Batch 124920, train_perplexity=68.001526

Batch 124930, train_perplexity=73.497734

Batch 124940, train_perplexity=73.250404

Batch 124950, train_perplexity=67.55178

Batch 124960, train_perplexity=68.17899

Batch 124970, train_perplexity=70.47564

Batch 124980, train_perplexity=68.16147

Batch 124990, train_perplexity=63.828175

Batch 125000, train_perplexity=60.135624

Batch 125010, train_perplexity=65.16875

Batch 125020, train_perplexity=65.4381

Batch 125030, train_perplexity=70.97517

Batch 125040, train_perplexity=64.6268

Batch 125050, train_perplexity=62.6971

Batch 125060, train_perplexity=76.44075

Batch 125070, train_perplexity=63.758698

Batch 125080, train_perplexity=60.27986

Batch 125090, train_perplexity=70.80135

Batch 125100, train_perplexity=62.225445

Batch 125110, train_perplexity=63.01883
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 125120, train_perplexity=65.25915

Batch 125130, train_perplexity=76.52671

Batch 125140, train_perplexity=68.135315

Batch 125150, train_perplexity=68.242935

Batch 125160, train_perplexity=71.84855

Batch 125170, train_perplexity=70.37013

Batch 125180, train_perplexity=65.25078

Batch 125190, train_perplexity=68.20517

Batch 125200, train_perplexity=72.46226

Batch 125210, train_perplexity=75.27039

Batch 125220, train_perplexity=73.01042

Batch 125230, train_perplexity=69.18077

Batch 125240, train_perplexity=68.75522

Batch 125250, train_perplexity=68.11388

Batch 125260, train_perplexity=68.45889

Batch 125270, train_perplexity=68.06329

Batch 125280, train_perplexity=71.03293

Batch 125290, train_perplexity=67.36746

Batch 125300, train_perplexity=66.77707

Batch 125310, train_perplexity=71.17988

Batch 125320, train_perplexity=64.38359

Batch 125330, train_perplexity=69.98888

Batch 125340, train_perplexity=74.23868

Batch 125350, train_perplexity=69.44458

Batch 125360, train_perplexity=76.36508

Batch 125370, train_perplexity=74.17704

Batch 125380, train_perplexity=69.10632

Batch 125390, train_perplexity=62.06815

Batch 125400, train_perplexity=67.12946

Batch 125410, train_perplexity=65.699356

Batch 125420, train_perplexity=65.39028

Batch 125430, train_perplexity=61.45424

Batch 125440, train_perplexity=69.87154

Batch 125450, train_perplexity=67.789986

Batch 125460, train_perplexity=63.875305

Batch 125470, train_perplexity=69.02653

Batch 125480, train_perplexity=64.71052

Batch 125490, train_perplexity=69.56648

Batch 125500, train_perplexity=73.352715

Batch 125510, train_perplexity=65.96927

Batch 125520, train_perplexity=64.56002

Batch 125530, train_perplexity=67.4834

Batch 125540, train_perplexity=63.65617

Batch 125550, train_perplexity=73.63278

Batch 125560, train_perplexity=70.32374

Batch 125570, train_perplexity=65.18012

Batch 125580, train_perplexity=71.85307

Batch 125590, train_perplexity=73.40338

Batch 125600, train_perplexity=66.37143

Batch 125610, train_perplexity=63.629738

Batch 125620, train_perplexity=71.92262

Batch 125630, train_perplexity=71.22914

Batch 125640, train_perplexity=66.94904

Batch 125650, train_perplexity=71.335976

Batch 125660, train_perplexity=65.07363

Batch 125670, train_perplexity=69.68896

Batch 125680, train_perplexity=74.78827

Batch 125690, train_perplexity=77.65796

Batch 125700, train_perplexity=59.34016

Batch 125710, train_perplexity=66.38121

Batch 125720, train_perplexity=71.69041

Batch 125730, train_perplexity=65.56679

Batch 125740, train_perplexity=75.76177

Batch 125750, train_perplexity=69.978294

Batch 125760, train_perplexity=70.19459

Batch 125770, train_perplexity=61.038513

Batch 125780, train_perplexity=61.75404

Batch 125790, train_perplexity=65.42125

Batch 125800, train_perplexity=63.04432

Batch 125810, train_perplexity=64.79639

Batch 125820, train_perplexity=67.76432

Batch 125830, train_perplexity=72.352875

Batch 125840, train_perplexity=68.84898

Batch 125850, train_perplexity=69.34002

Batch 125860, train_perplexity=65.84312

Batch 125870, train_perplexity=66.26461

Batch 125880, train_perplexity=68.12115

Batch 125890, train_perplexity=67.36958

Batch 125900, train_perplexity=63.732555

Batch 125910, train_perplexity=68.21704

Batch 125920, train_perplexity=66.50069

Batch 125930, train_perplexity=70.451416

Batch 125940, train_perplexity=65.2565

Batch 125950, train_perplexity=65.53027

Batch 125960, train_perplexity=61.38038

Batch 125970, train_perplexity=62.605267

Batch 125980, train_perplexity=76.992134

Batch 125990, train_perplexity=65.367584

Batch 126000, train_perplexity=67.45187

Batch 126010, train_perplexity=68.99099

Batch 126020, train_perplexity=65.19467

Batch 126030, train_perplexity=64.55328

Batch 126040, train_perplexity=64.788635

Batch 126050, train_perplexity=71.49858

Batch 126060, train_perplexity=68.86448

Batch 126070, train_perplexity=64.15442

Batch 126080, train_perplexity=66.44591

Batch 126090, train_perplexity=63.412262

Batch 126100, train_perplexity=68.21545

Batch 126110, train_perplexity=71.284256

Batch 126120, train_perplexity=67.303024

Batch 126130, train_perplexity=72.98609

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00036-of-00100
Loaded 305511 sentences.
Finished loading
Batch 126140, train_perplexity=67.33939

Batch 126150, train_perplexity=66.12456

Batch 126160, train_perplexity=65.95693

Batch 126170, train_perplexity=76.340614

Batch 126180, train_perplexity=64.560425

Batch 126190, train_perplexity=63.038124

Batch 126200, train_perplexity=69.22235

Batch 126210, train_perplexity=69.42181

Batch 126220, train_perplexity=72.00649

Batch 126230, train_perplexity=62.328194

Batch 126240, train_perplexity=71.05457

Batch 126250, train_perplexity=69.986305

Batch 126260, train_perplexity=74.07496

Batch 126270, train_perplexity=65.14992

Batch 126280, train_perplexity=69.99121

Batch 126290, train_perplexity=65.362946

Batch 126300, train_perplexity=61.773186

Batch 126310, train_perplexity=67.87292

Batch 126320, train_perplexity=65.6264

Batch 126330, train_perplexity=71.79807

Batch 126340, train_perplexity=61.195507

Batch 126350, train_perplexity=77.69299

Batch 126360, train_perplexity=64.324615

Batch 126370, train_perplexity=66.97242

Batch 126380, train_perplexity=68.63985

Batch 126390, train_perplexity=68.82121

Batch 126400, train_perplexity=65.86045

Batch 126410, train_perplexity=75.20947

Batch 126420, train_perplexity=68.46118

Batch 126430, train_perplexity=64.76121

Batch 126440, train_perplexity=69.56316

Batch 126450, train_perplexity=65.38015

Batch 126460, train_perplexity=65.236465

Batch 126470, train_perplexity=76.21651

Batch 126480, train_perplexity=67.14675

Batch 126490, train_perplexity=61.67297

Batch 126500, train_perplexity=71.553665

Batch 126510, train_perplexity=67.34742

Batch 126520, train_perplexity=70.1351

Batch 126530, train_perplexity=59.997143

Batch 126540, train_perplexity=75.58121

Batch 126550, train_perplexity=64.988205

Batch 126560, train_perplexity=65.58267

Batch 126570, train_perplexity=65.022766

Batch 126580, train_perplexity=71.106804

Batch 126590, train_perplexity=70.88449

Batch 126600, train_perplexity=65.798805

Batch 126610, train_perplexity=63.964123

Batch 126620, train_perplexity=65.020134

Batch 126630, train_perplexity=63.6352

Batch 126640, train_perplexity=60.73758

Batch 126650, train_perplexity=65.95137

Batch 126660, train_perplexity=69.867134

Batch 126670, train_perplexity=74.11923

Batch 126680, train_perplexity=67.03951

Batch 126690, train_perplexity=68.40042

Batch 126700, train_perplexity=70.239655

Batch 126710, train_perplexity=67.17814

Batch 126720, train_perplexity=69.80576

Batch 126730, train_perplexity=73.94075

Batch 126740, train_perplexity=69.50574

Batch 126750, train_perplexity=63.833836

Batch 126760, train_perplexity=64.149956

Batch 126770, train_perplexity=72.2117

Batch 126780, train_perplexity=68.5667

Batch 126790, train_perplexity=76.78155

Batch 126800, train_perplexity=62.02442

Batch 126810, train_perplexity=67.95025

Batch 126820, train_perplexity=66.01052

Batch 126830, train_perplexity=64.905266

Batch 126840, train_perplexity=65.46032

Batch 126850, train_perplexity=68.02254

Batch 126860, train_perplexity=73.90346

Batch 126870, train_perplexity=75.58662

Batch 126880, train_perplexity=69.62422

Batch 126890, train_perplexity=62.13445

Batch 126900, train_perplexity=65.567474

Batch 126910, train_perplexity=73.88602

Batch 126920, train_perplexity=62.96284

Batch 126930, train_perplexity=71.29007

Batch 126940, train_perplexity=63.82842

Batch 126950, train_perplexity=71.93202

Batch 126960, train_perplexity=70.37825

Batch 126970, train_perplexity=66.91113

Batch 126980, train_perplexity=72.11941

Batch 126990, train_perplexity=72.37865

Batch 127000, train_perplexity=71.06413
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 127010, train_perplexity=64.6604

Batch 127020, train_perplexity=64.52995

Batch 127030, train_perplexity=67.87107

Batch 127040, train_perplexity=70.3651

Batch 127050, train_perplexity=64.497406

Batch 127060, train_perplexity=70.59733

Batch 127070, train_perplexity=60.454067

Batch 127080, train_perplexity=62.795536

Batch 127090, train_perplexity=59.56008

Batch 127100, train_perplexity=75.21048

Batch 127110, train_perplexity=73.42908

Batch 127120, train_perplexity=65.31266

Batch 127130, train_perplexity=66.39828

Batch 127140, train_perplexity=72.595955

Batch 127150, train_perplexity=66.01729

Batch 127160, train_perplexity=58.87927

Batch 127170, train_perplexity=69.33007

Batch 127180, train_perplexity=71.508095

Batch 127190, train_perplexity=69.15063

Batch 127200, train_perplexity=68.44408

Batch 127210, train_perplexity=61.83289

Batch 127220, train_perplexity=71.1087

Batch 127230, train_perplexity=67.48224

Batch 127240, train_perplexity=63.298733

Batch 127250, train_perplexity=66.43996

Batch 127260, train_perplexity=71.283646

Batch 127270, train_perplexity=63.329887

Batch 127280, train_perplexity=67.06893

Batch 127290, train_perplexity=65.43049

Batch 127300, train_perplexity=61.475986

Batch 127310, train_perplexity=74.16629

Batch 127320, train_perplexity=81.10506

Batch 127330, train_perplexity=65.11985

Batch 127340, train_perplexity=77.229256

Batch 127350, train_perplexity=64.24295

Batch 127360, train_perplexity=76.46433

Batch 127370, train_perplexity=64.717865

Batch 127380, train_perplexity=70.842384

Batch 127390, train_perplexity=66.225

Batch 127400, train_perplexity=69.2847

Batch 127410, train_perplexity=70.904945

Batch 127420, train_perplexity=64.42634

Batch 127430, train_perplexity=58.338943

Batch 127440, train_perplexity=63.841446

Batch 127450, train_perplexity=67.47253

Batch 127460, train_perplexity=69.44991

Batch 127470, train_perplexity=66.63527

Batch 127480, train_perplexity=68.50977

Batch 127490, train_perplexity=68.28747

Batch 127500, train_perplexity=68.086075

Batch 127510, train_perplexity=68.868

Batch 127520, train_perplexity=67.819244

Batch 127530, train_perplexity=60.339592

Batch 127540, train_perplexity=60.928738

Batch 127550, train_perplexity=61.76379

Batch 127560, train_perplexity=65.72746

Batch 127570, train_perplexity=64.31744

Batch 127580, train_perplexity=64.72941

Batch 127590, train_perplexity=66.02484

Batch 127600, train_perplexity=61.133587

Batch 127610, train_perplexity=72.594574

Batch 127620, train_perplexity=67.979996

Batch 127630, train_perplexity=73.17894

Batch 127640, train_perplexity=71.590454

Batch 127650, train_perplexity=74.98524

Batch 127660, train_perplexity=68.18585

Batch 127670, train_perplexity=69.09924

Batch 127680, train_perplexity=66.53773

Batch 127690, train_perplexity=65.387634

Batch 127700, train_perplexity=67.57359

Batch 127710, train_perplexity=68.984505

Batch 127720, train_perplexity=68.46996

Batch 127730, train_perplexity=66.10139

Batch 127740, train_perplexity=68.35872

Batch 127750, train_perplexity=66.16225

Batch 127760, train_perplexity=61.95785

Batch 127770, train_perplexity=68.701935

Batch 127780, train_perplexity=65.2885

Batch 127790, train_perplexity=64.90821

Batch 127800, train_perplexity=64.20253

Batch 127810, train_perplexity=74.71071

Batch 127820, train_perplexity=68.74798

Batch 127830, train_perplexity=78.541374

Batch 127840, train_perplexity=69.13236

Batch 127850, train_perplexity=67.38584

Batch 127860, train_perplexity=66.28696

Batch 127870, train_perplexity=61.95732

Batch 127880, train_perplexity=66.639435

Batch 127890, train_perplexity=71.39634

Batch 127900, train_perplexity=72.42136

Batch 127910, train_perplexity=76.578545

Batch 127920, train_perplexity=70.254395

Batch 127930, train_perplexity=67.759605

Batch 127940, train_perplexity=70.058525

Batch 127950, train_perplexity=64.67603

Batch 127960, train_perplexity=62.933968

Batch 127970, train_perplexity=67.99108

Batch 127980, train_perplexity=72.59935

Batch 127990, train_perplexity=60.24423

Batch 128000, train_perplexity=67.096375

Batch 128010, train_perplexity=64.636536

Batch 128020, train_perplexity=60.443924

Batch 128030, train_perplexity=68.28613

Batch 128040, train_perplexity=72.720406

Batch 128050, train_perplexity=65.85392

Batch 128060, train_perplexity=67.87505

Batch 128070, train_perplexity=76.802414

Batch 128080, train_perplexity=67.05294

Batch 128090, train_perplexity=61.569393

Batch 128100, train_perplexity=65.402534

Batch 128110, train_perplexity=64.48246

Batch 128120, train_perplexity=65.61501

Batch 128130, train_perplexity=70.19666

Batch 128140, train_perplexity=65.20455

Batch 128150, train_perplexity=70.77618

Batch 128160, train_perplexity=63.817764

Batch 128170, train_perplexity=61.760696

Batch 128180, train_perplexity=67.65116

Batch 128190, train_perplexity=68.22927

Batch 128200, train_perplexity=63.67217

Batch 128210, train_perplexity=72.602325

Batch 128220, train_perplexity=72.710075

Batch 128230, train_perplexity=70.83104

Batch 128240, train_perplexity=69.14898

Batch 128250, train_perplexity=66.92172

Batch 128260, train_perplexity=62.044003

Batch 128270, train_perplexity=67.92634

Batch 128280, train_perplexity=72.073135

Batch 128290, train_perplexity=65.59361

Batch 128300, train_perplexity=64.870186

Batch 128310, train_perplexity=62.61016

Batch 128320, train_perplexity=65.02801

Batch 128330, train_perplexity=75.88567

Batch 128340, train_perplexity=66.05054

Batch 128350, train_perplexity=65.218735

Batch 128360, train_perplexity=64.94539

Batch 128370, train_perplexity=67.03088

Batch 128380, train_perplexity=67.150116

Batch 128390, train_perplexity=73.155396

Batch 128400, train_perplexity=65.99595

Batch 128410, train_perplexity=70.3492

Batch 128420, train_perplexity=65.741005

Batch 128430, train_perplexity=71.392395

Batch 128440, train_perplexity=72.52551

Batch 128450, train_perplexity=64.79129

Batch 128460, train_perplexity=72.06983

Batch 128470, train_perplexity=70.101135

Batch 128480, train_perplexity=67.48385

Batch 128490, train_perplexity=66.46014

Batch 128500, train_perplexity=63.54363

Batch 128510, train_perplexity=68.387245

Batch 128520, train_perplexity=66.46673

Batch 128530, train_perplexity=67.87434

Batch 128540, train_perplexity=67.12389

Batch 128550, train_perplexity=67.292206

Batch 128560, train_perplexity=68.871315

Batch 128570, train_perplexity=72.191315

Batch 128580, train_perplexity=67.00295

Batch 128590, train_perplexity=64.18891

Batch 128600, train_perplexity=67.656746

Batch 128610, train_perplexity=71.408394

Batch 128620, train_perplexity=69.59432

Batch 128630, train_perplexity=70.44409

Batch 128640, train_perplexity=70.14708

Batch 128650, train_perplexity=71.34403

Batch 128660, train_perplexity=67.92614

Batch 128670, train_perplexity=68.17493

Batch 128680, train_perplexity=77.80488

Batch 128690, train_perplexity=64.75972

Batch 128700, train_perplexity=68.62149

Batch 128710, train_perplexity=62.302402

Batch 128720, train_perplexity=70.464485

Batch 128730, train_perplexity=69.73929

Batch 128740, train_perplexity=70.33961

Batch 128750, train_perplexity=65.40419

Batch 128760, train_perplexity=72.599625

Batch 128770, train_perplexity=69.7481

Batch 128780, train_perplexity=65.19703

Batch 128790, train_perplexity=65.00863

Batch 128800, train_perplexity=66.34827

Batch 128810, train_perplexity=61.373882

Batch 128820, train_perplexity=63.499374

Batch 128830, train_perplexity=66.550674

Batch 128840, train_perplexity=69.66085

Batch 128850, train_perplexity=68.07569

Batch 128860, train_perplexity=71.2177

Batch 128870, train_perplexity=62.675697

Batch 128880, train_perplexity=65.15557

Batch 128890, train_perplexity=76.39414

Batch 128900, train_perplexity=64.33805

Batch 128910, train_perplexity=61.765675

Batch 128920, train_perplexity=65.47169

Batch 128930, train_perplexity=62.017384

Batch 128940, train_perplexity=67.07763

Batch 128950, train_perplexity=66.65726

Batch 128960, train_perplexity=63.82729

Batch 128970, train_perplexity=65.51266

Batch 128980, train_perplexity=72.592285

Batch 128990, train_perplexity=65.65201
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 129000, train_perplexity=68.474106

Batch 129010, train_perplexity=63.846012

Batch 129020, train_perplexity=70.43398

Batch 129030, train_perplexity=69.7221

Batch 129040, train_perplexity=60.924324

Batch 129050, train_perplexity=66.54109

Batch 129060, train_perplexity=58.515076

Batch 129070, train_perplexity=63.324512

Batch 129080, train_perplexity=78.4878

Batch 129090, train_perplexity=65.8548

Batch 129100, train_perplexity=67.758316

Batch 129110, train_perplexity=70.18455

Batch 129120, train_perplexity=65.05536

Batch 129130, train_perplexity=69.77558

Batch 129140, train_perplexity=60.662556

Batch 129150, train_perplexity=64.13595

Batch 129160, train_perplexity=69.22644

Batch 129170, train_perplexity=69.38361

Batch 129180, train_perplexity=66.28152

Batch 129190, train_perplexity=62.76063

Batch 129200, train_perplexity=72.24773

Batch 129210, train_perplexity=66.46394

Batch 129220, train_perplexity=63.653347

Batch 129230, train_perplexity=64.225555

Batch 129240, train_perplexity=62.249187

Batch 129250, train_perplexity=72.921524

Batch 129260, train_perplexity=71.813545

Batch 129270, train_perplexity=59.655926

Batch 129280, train_perplexity=64.95815

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00091-of-00100
Loaded 307290 sentences.
Finished loading
Batch 129290, train_perplexity=68.75594

Batch 129300, train_perplexity=65.98625

Batch 129310, train_perplexity=64.691826

Batch 129320, train_perplexity=68.476295

Batch 129330, train_perplexity=73.56014

Batch 129340, train_perplexity=70.39383

Batch 129350, train_perplexity=71.450424

Batch 129360, train_perplexity=68.38007

Batch 129370, train_perplexity=67.75482

Batch 129380, train_perplexity=73.07963

Batch 129390, train_perplexity=63.733162

Batch 129400, train_perplexity=71.54507

Batch 129410, train_perplexity=65.8951

Batch 129420, train_perplexity=70.137375

Batch 129430, train_perplexity=72.18347

Batch 129440, train_perplexity=71.3668

Batch 129450, train_perplexity=70.2127

Batch 129460, train_perplexity=63.077515

Batch 129470, train_perplexity=69.455345

Batch 129480, train_perplexity=62.41617

Batch 129490, train_perplexity=74.06691

Batch 129500, train_perplexity=68.817406

Batch 129510, train_perplexity=73.57442

Batch 129520, train_perplexity=65.78192

Batch 129530, train_perplexity=72.73844

Batch 129540, train_perplexity=75.99261

Batch 129550, train_perplexity=59.354706

Batch 129560, train_perplexity=61.94506

Batch 129570, train_perplexity=65.0876

Batch 129580, train_perplexity=68.04031

Batch 129590, train_perplexity=69.426674

Batch 129600, train_perplexity=71.05464

Batch 129610, train_perplexity=65.00835

Batch 129620, train_perplexity=68.32655

Batch 129630, train_perplexity=69.649994

Batch 129640, train_perplexity=62.07904

Batch 129650, train_perplexity=72.90317

Batch 129660, train_perplexity=64.09113

Batch 129670, train_perplexity=61.46822

Batch 129680, train_perplexity=68.84084

Batch 129690, train_perplexity=70.65391

Batch 129700, train_perplexity=72.17865

Batch 129710, train_perplexity=74.62693

Batch 129720, train_perplexity=64.81187

Batch 129730, train_perplexity=68.53918

Batch 129740, train_perplexity=62.2556

Batch 129750, train_perplexity=73.46196

Batch 129760, train_perplexity=69.61672

Batch 129770, train_perplexity=66.44886

Batch 129780, train_perplexity=66.37206

Batch 129790, train_perplexity=62.779068

Batch 129800, train_perplexity=70.35792

Batch 129810, train_perplexity=66.61551

Batch 129820, train_perplexity=68.8244

Batch 129830, train_perplexity=70.55601

Batch 129840, train_perplexity=73.12648

Batch 129850, train_perplexity=70.008804

Batch 129860, train_perplexity=63.57363

Batch 129870, train_perplexity=65.86045

Batch 129880, train_perplexity=71.46471

Batch 129890, train_perplexity=71.25832

Batch 129900, train_perplexity=73.01645

Batch 129910, train_perplexity=65.37516

Batch 129920, train_perplexity=67.49409

Batch 129930, train_perplexity=63.977512

Batch 129940, train_perplexity=65.916504

Batch 129950, train_perplexity=61.31468

Batch 129960, train_perplexity=66.646835

Batch 129970, train_perplexity=71.61149

Batch 129980, train_perplexity=58.201378

Batch 129990, train_perplexity=68.10212

Batch 130000, train_perplexity=70.21434

Batch 130010, train_perplexity=60.8787

Batch 130020, train_perplexity=75.13592

Batch 130030, train_perplexity=64.9751

Batch 130040, train_perplexity=58.12131

Batch 130050, train_perplexity=63.40597

Batch 130060, train_perplexity=73.22882

Batch 130070, train_perplexity=68.37596

Batch 130080, train_perplexity=74.210686

Batch 130090, train_perplexity=64.7811

Batch 130100, train_perplexity=67.91079

Batch 130110, train_perplexity=64.78802

Batch 130120, train_perplexity=72.97363

Batch 130130, train_perplexity=69.447136

Batch 130140, train_perplexity=68.54666

Batch 130150, train_perplexity=62.15804

Batch 130160, train_perplexity=71.91971

Batch 130170, train_perplexity=68.47731

Batch 130180, train_perplexity=67.031715

Batch 130190, train_perplexity=69.33718

Batch 130200, train_perplexity=63.161304

Batch 130210, train_perplexity=68.022865

Batch 130220, train_perplexity=72.57595

Batch 130230, train_perplexity=73.4129

Batch 130240, train_perplexity=65.10297

Batch 130250, train_perplexity=72.96355

Batch 130260, train_perplexity=69.67517

Batch 130270, train_perplexity=65.24499

Batch 130280, train_perplexity=65.52178

Batch 130290, train_perplexity=71.74369

Batch 130300, train_perplexity=76.963875

Batch 130310, train_perplexity=71.88587

Batch 130320, train_perplexity=63.642483

Batch 130330, train_perplexity=65.61138

Batch 130340, train_perplexity=75.80246

Batch 130350, train_perplexity=74.96429

Batch 130360, train_perplexity=56.578568

Batch 130370, train_perplexity=63.40839

Batch 130380, train_perplexity=75.929

Batch 130390, train_perplexity=68.69184

Batch 130400, train_perplexity=62.526386

Batch 130410, train_perplexity=61.804768

Batch 130420, train_perplexity=61.028824

Batch 130430, train_perplexity=60.828213

Batch 130440, train_perplexity=66.67207

Batch 130450, train_perplexity=73.58463

Batch 130460, train_perplexity=72.12425

Batch 130470, train_perplexity=68.19356

Batch 130480, train_perplexity=64.16874

Batch 130490, train_perplexity=59.136215

Batch 130500, train_perplexity=65.5065

Batch 130510, train_perplexity=73.41339

Batch 130520, train_perplexity=68.435394

Batch 130530, train_perplexity=66.05353

Batch 130540, train_perplexity=66.98663

Batch 130550, train_perplexity=63.92991

Batch 130560, train_perplexity=72.83097

Batch 130570, train_perplexity=65.413704

Batch 130580, train_perplexity=67.63255

Batch 130590, train_perplexity=67.525635

Batch 130600, train_perplexity=69.70069

Batch 130610, train_perplexity=64.45978

Batch 130620, train_perplexity=70.24026

Batch 130630, train_perplexity=60.08495

Batch 130640, train_perplexity=76.34927

Batch 130650, train_perplexity=64.854034

Batch 130660, train_perplexity=67.32055

Batch 130670, train_perplexity=70.2201

Batch 130680, train_perplexity=72.22113

Batch 130690, train_perplexity=61.949818

Batch 130700, train_perplexity=69.62628

Batch 130710, train_perplexity=65.63598

Batch 130720, train_perplexity=65.982414

Batch 130730, train_perplexity=64.07683

Batch 130740, train_perplexity=73.15958

Batch 130750, train_perplexity=68.87525

Batch 130760, train_perplexity=65.858696

Batch 130770, train_perplexity=63.52872

Batch 130780, train_perplexity=72.35736

Batch 130790, train_perplexity=67.44229

Batch 130800, train_perplexity=66.89173

Batch 130810, train_perplexity=69.069954

Batch 130820, train_perplexity=68.568794

Batch 130830, train_perplexity=67.16549

Batch 130840, train_perplexity=65.477806

Batch 130850, train_perplexity=66.76064

Batch 130860, train_perplexity=61.993908

Batch 130870, train_perplexity=65.17897

Batch 130880, train_perplexity=67.32129
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 130890, train_perplexity=72.67032

Batch 130900, train_perplexity=61.47528

Batch 130910, train_perplexity=64.941986

Batch 130920, train_perplexity=63.83341

Batch 130930, train_perplexity=71.83091

Batch 130940, train_perplexity=63.252087

Batch 130950, train_perplexity=74.421074

Batch 130960, train_perplexity=63.46145

Batch 130970, train_perplexity=71.587585

Batch 130980, train_perplexity=67.71323

Batch 130990, train_perplexity=71.463615

Batch 131000, train_perplexity=65.84086

Batch 131010, train_perplexity=72.94653

Batch 131020, train_perplexity=71.2933

Batch 131030, train_perplexity=63.882614

Batch 131040, train_perplexity=64.109344

Batch 131050, train_perplexity=65.27374

Batch 131060, train_perplexity=75.53765

Batch 131070, train_perplexity=63.12009

Batch 131080, train_perplexity=72.85091

Batch 131090, train_perplexity=62.768055

Batch 131100, train_perplexity=72.56412

Batch 131110, train_perplexity=71.00346

Batch 131120, train_perplexity=63.02364

Batch 131130, train_perplexity=62.337704

Batch 131140, train_perplexity=59.67863

Batch 131150, train_perplexity=60.719685

Batch 131160, train_perplexity=69.38673

Batch 131170, train_perplexity=62.961823

Batch 131180, train_perplexity=58.48233

Batch 131190, train_perplexity=70.94627

Batch 131200, train_perplexity=66.06011

Batch 131210, train_perplexity=63.23562

Batch 131220, train_perplexity=67.682655

Batch 131230, train_perplexity=62.74956

Batch 131240, train_perplexity=73.533455

Batch 131250, train_perplexity=69.39473

Batch 131260, train_perplexity=69.419716

Batch 131270, train_perplexity=77.12085

Batch 131280, train_perplexity=58.73267

Batch 131290, train_perplexity=67.51269

Batch 131300, train_perplexity=61.182377

Batch 131310, train_perplexity=73.92299

Batch 131320, train_perplexity=64.28065

Batch 131330, train_perplexity=62.549168

Batch 131340, train_perplexity=66.0421

Batch 131350, train_perplexity=70.91827

Batch 131360, train_perplexity=61.149475

Batch 131370, train_perplexity=61.245686

Batch 131380, train_perplexity=66.36229

Batch 131390, train_perplexity=68.11388

Batch 131400, train_perplexity=65.47918

Batch 131410, train_perplexity=68.6847

Batch 131420, train_perplexity=67.44518

Batch 131430, train_perplexity=68.798836

Batch 131440, train_perplexity=64.85484

Batch 131450, train_perplexity=66.95211

Batch 131460, train_perplexity=65.40703

Batch 131470, train_perplexity=65.103676

Batch 131480, train_perplexity=67.529625

Batch 131490, train_perplexity=72.5673

Batch 131500, train_perplexity=61.614033

Batch 131510, train_perplexity=66.15991

Batch 131520, train_perplexity=62.75782

Batch 131530, train_perplexity=71.4508

Batch 131540, train_perplexity=70.16059

Batch 131550, train_perplexity=69.680855

Batch 131560, train_perplexity=74.30653

Batch 131570, train_perplexity=69.97756

Batch 131580, train_perplexity=70.807335

Batch 131590, train_perplexity=65.76935

Batch 131600, train_perplexity=73.81869

Batch 131610, train_perplexity=60.800488

Batch 131620, train_perplexity=69.8028

Batch 131630, train_perplexity=63.426746

Batch 131640, train_perplexity=60.585487

Batch 131650, train_perplexity=65.36017

Batch 131660, train_perplexity=61.689617

Batch 131670, train_perplexity=74.12662

Batch 131680, train_perplexity=70.63794

Batch 131690, train_perplexity=69.81851

Batch 131700, train_perplexity=67.68917

Batch 131710, train_perplexity=61.62173

Batch 131720, train_perplexity=67.961266

Batch 131730, train_perplexity=64.53195

Batch 131740, train_perplexity=69.02208

Batch 131750, train_perplexity=67.768394

Batch 131760, train_perplexity=69.20809

Batch 131770, train_perplexity=63.044075

Batch 131780, train_perplexity=65.35066

Batch 131790, train_perplexity=76.284065

Batch 131800, train_perplexity=63.3359

Batch 131810, train_perplexity=59.775036

Batch 131820, train_perplexity=70.32733

Batch 131830, train_perplexity=66.11413

Batch 131840, train_perplexity=67.98171

Batch 131850, train_perplexity=71.978134

Batch 131860, train_perplexity=75.14108

Batch 131870, train_perplexity=67.375496

Batch 131880, train_perplexity=68.666885

Batch 131890, train_perplexity=69.859474

Batch 131900, train_perplexity=71.08592

Batch 131910, train_perplexity=66.79481

Batch 131920, train_perplexity=59.72634

Batch 131930, train_perplexity=74.76537

Batch 131940, train_perplexity=63.172573

Batch 131950, train_perplexity=70.617195

Batch 131960, train_perplexity=65.30771

Batch 131970, train_perplexity=66.89735

Batch 131980, train_perplexity=69.85574

Batch 131990, train_perplexity=70.45642

Batch 132000, train_perplexity=67.532135

Batch 132010, train_perplexity=62.61228

Batch 132020, train_perplexity=61.78918

Batch 132030, train_perplexity=66.84735

Batch 132040, train_perplexity=71.16576

Batch 132050, train_perplexity=67.68024

Batch 132060, train_perplexity=61.901215

Batch 132070, train_perplexity=71.69014

Batch 132080, train_perplexity=71.2745

Batch 132090, train_perplexity=63.645943

Batch 132100, train_perplexity=67.912926

Batch 132110, train_perplexity=64.80977

Batch 132120, train_perplexity=61.543095

Batch 132130, train_perplexity=65.54015

Batch 132140, train_perplexity=68.12628

Batch 132150, train_perplexity=68.83316

Batch 132160, train_perplexity=73.11888

Batch 132170, train_perplexity=64.97581

Batch 132180, train_perplexity=71.16556

Batch 132190, train_perplexity=65.003456

Batch 132200, train_perplexity=65.36332

Batch 132210, train_perplexity=70.4535

Batch 132220, train_perplexity=66.75619

Batch 132230, train_perplexity=66.95348

Batch 132240, train_perplexity=62.625988

Batch 132250, train_perplexity=68.177956

Batch 132260, train_perplexity=63.109165

Batch 132270, train_perplexity=65.56065

Batch 132280, train_perplexity=70.251144

Batch 132290, train_perplexity=64.63786

Batch 132300, train_perplexity=66.175125

Batch 132310, train_perplexity=63.44275

Batch 132320, train_perplexity=72.12739

Batch 132330, train_perplexity=64.31989

Batch 132340, train_perplexity=65.03576

Batch 132350, train_perplexity=62.23797

Batch 132360, train_perplexity=69.43263

Batch 132370, train_perplexity=64.307625

Batch 132380, train_perplexity=64.15124

Batch 132390, train_perplexity=58.40352

Batch 132400, train_perplexity=69.09605

Batch 132410, train_perplexity=61.07342

Batch 132420, train_perplexity=61.091713

Batch 132430, train_perplexity=67.07085

Batch 132440, train_perplexity=70.790085

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00007-of-00100
Loaded 306552 sentences.
Finished loading
Batch 132450, train_perplexity=64.09394

Batch 132460, train_perplexity=67.42821

Batch 132470, train_perplexity=73.4358

Batch 132480, train_perplexity=69.60188

Batch 132490, train_perplexity=63.230858

Batch 132500, train_perplexity=61.429276

Batch 132510, train_perplexity=65.628

Batch 132520, train_perplexity=68.61197

Batch 132530, train_perplexity=65.650566

Batch 132540, train_perplexity=60.391922

Batch 132550, train_perplexity=66.045815

Batch 132560, train_perplexity=65.4022

Batch 132570, train_perplexity=69.26098

Batch 132580, train_perplexity=62.927906

Batch 132590, train_perplexity=72.3259

Batch 132600, train_perplexity=69.41442

Batch 132610, train_perplexity=67.87939

Batch 132620, train_perplexity=60.785965

Batch 132630, train_perplexity=68.12933

Batch 132640, train_perplexity=71.61415

Batch 132650, train_perplexity=62.033115

Batch 132660, train_perplexity=72.4324

Batch 132670, train_perplexity=66.028366

Batch 132680, train_perplexity=64.93263

Batch 132690, train_perplexity=62.628494

Batch 132700, train_perplexity=69.1714

Batch 132710, train_perplexity=69.38034

Batch 132720, train_perplexity=71.64001

Batch 132730, train_perplexity=69.44425

Batch 132740, train_perplexity=70.05789

Batch 132750, train_perplexity=64.4092

Batch 132760, train_perplexity=71.1625

Batch 132770, train_perplexity=69.08273
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 132780, train_perplexity=67.69682

Batch 132790, train_perplexity=65.272125

Batch 132800, train_perplexity=66.25083

Batch 132810, train_perplexity=65.24493

Batch 132820, train_perplexity=68.20097

Batch 132830, train_perplexity=72.5372

Batch 132840, train_perplexity=66.84378

Batch 132850, train_perplexity=65.799934

Batch 132860, train_perplexity=66.43609

Batch 132870, train_perplexity=64.37887

Batch 132880, train_perplexity=72.32342

Batch 132890, train_perplexity=67.78207

Batch 132900, train_perplexity=63.07081

Batch 132910, train_perplexity=67.228065

Batch 132920, train_perplexity=72.401985

Batch 132930, train_perplexity=66.40879

Batch 132940, train_perplexity=76.37269

Batch 132950, train_perplexity=70.09358

Batch 132960, train_perplexity=67.07008

Batch 132970, train_perplexity=68.89953

Batch 132980, train_perplexity=67.87625

Batch 132990, train_perplexity=69.44018

Batch 133000, train_perplexity=63.9953

Batch 133010, train_perplexity=67.38574

Batch 133020, train_perplexity=72.34535

Batch 133030, train_perplexity=59.629192

Batch 133040, train_perplexity=70.45958

Batch 133050, train_perplexity=69.92259

Batch 133060, train_perplexity=64.93254

Batch 133070, train_perplexity=66.65307

Batch 133080, train_perplexity=70.409134

Batch 133090, train_perplexity=70.26378

Batch 133100, train_perplexity=64.35235

Batch 133110, train_perplexity=65.52775

Batch 133120, train_perplexity=65.35699

Batch 133130, train_perplexity=67.26821

Batch 133140, train_perplexity=62.794518

Batch 133150, train_perplexity=70.58878

Batch 133160, train_perplexity=71.02338

Batch 133170, train_perplexity=62.259697

Batch 133180, train_perplexity=67.9632

Batch 133190, train_perplexity=69.94054

Batch 133200, train_perplexity=60.729904

Batch 133210, train_perplexity=59.59434

Batch 133220, train_perplexity=64.32265

Batch 133230, train_perplexity=62.771885

Batch 133240, train_perplexity=67.88389

Batch 133250, train_perplexity=74.12623

Batch 133260, train_perplexity=68.06949

Batch 133270, train_perplexity=67.78805

Batch 133280, train_perplexity=58.94725

Batch 133290, train_perplexity=70.23068

Batch 133300, train_perplexity=67.91581

Batch 133310, train_perplexity=59.991848

Batch 133320, train_perplexity=74.83914

Batch 133330, train_perplexity=69.26091

Batch 133340, train_perplexity=67.23832

Batch 133350, train_perplexity=65.66847

Batch 133360, train_perplexity=63.79172

Batch 133370, train_perplexity=69.78596

Batch 133380, train_perplexity=65.78795

Batch 133390, train_perplexity=59.75842

Batch 133400, train_perplexity=66.13103

Batch 133410, train_perplexity=66.49891

Batch 133420, train_perplexity=66.05252

Batch 133430, train_perplexity=64.9431

Batch 133440, train_perplexity=63.456696

Batch 133450, train_perplexity=65.51588

Batch 133460, train_perplexity=65.88354

Batch 133470, train_perplexity=70.60326

Batch 133480, train_perplexity=65.94156

Batch 133490, train_perplexity=70.857216

Batch 133500, train_perplexity=66.88

Batch 133510, train_perplexity=63.70381

Batch 133520, train_perplexity=68.006

Batch 133530, train_perplexity=66.02635

Batch 133540, train_perplexity=61.707474

Batch 133550, train_perplexity=67.25224

Batch 133560, train_perplexity=67.49203

Batch 133570, train_perplexity=58.20252

Batch 133580, train_perplexity=70.7674

Batch 133590, train_perplexity=70.823235

Batch 133600, train_perplexity=69.88316

Batch 133610, train_perplexity=66.7793

Batch 133620, train_perplexity=62.015667

Batch 133630, train_perplexity=67.449875

Batch 133640, train_perplexity=68.453995

Batch 133650, train_perplexity=71.909424

Batch 133660, train_perplexity=65.54215

Batch 133670, train_perplexity=59.8188

Batch 133680, train_perplexity=64.759415

Batch 133690, train_perplexity=65.54971

Batch 133700, train_perplexity=57.90466

Batch 133710, train_perplexity=66.62399

Batch 133720, train_perplexity=71.012

Batch 133730, train_perplexity=70.99744

Batch 133740, train_perplexity=61.065384

Batch 133750, train_perplexity=66.61081

Batch 133760, train_perplexity=69.871666

Batch 133770, train_perplexity=58.93927

Batch 133780, train_perplexity=63.50095

Batch 133790, train_perplexity=63.63611

Batch 133800, train_perplexity=68.118355

Batch 133810, train_perplexity=65.38109

Batch 133820, train_perplexity=67.280655

Batch 133830, train_perplexity=71.563354

Batch 133840, train_perplexity=66.27466

Batch 133850, train_perplexity=63.82839

Batch 133860, train_perplexity=70.48068

Batch 133870, train_perplexity=66.62936

Batch 133880, train_perplexity=61.301582

Batch 133890, train_perplexity=67.79855

Batch 133900, train_perplexity=67.7295

Batch 133910, train_perplexity=64.3691

Batch 133920, train_perplexity=72.7612

Batch 133930, train_perplexity=68.0816

Batch 133940, train_perplexity=62.88696

Batch 133950, train_perplexity=64.75046

Batch 133960, train_perplexity=65.350975

Batch 133970, train_perplexity=64.70777

Batch 133980, train_perplexity=63.78631

Batch 133990, train_perplexity=72.471275

Batch 134000, train_perplexity=68.07218

Batch 134010, train_perplexity=66.95438

Batch 134020, train_perplexity=68.906

Batch 134030, train_perplexity=59.053932

Batch 134040, train_perplexity=72.671875

Batch 134050, train_perplexity=71.46767

Batch 134060, train_perplexity=59.842968

Batch 134070, train_perplexity=62.035305

Batch 134080, train_perplexity=65.47299

Batch 134090, train_perplexity=69.90623

Batch 134100, train_perplexity=70.31868

Batch 134110, train_perplexity=69.88886

Batch 134120, train_perplexity=62.71199

Batch 134130, train_perplexity=72.888916

Batch 134140, train_perplexity=67.56287

Batch 134150, train_perplexity=63.847717

Batch 134160, train_perplexity=71.006714

Batch 134170, train_perplexity=64.50319

Batch 134180, train_perplexity=71.608826

Batch 134190, train_perplexity=73.16931

Batch 134200, train_perplexity=70.123665

Batch 134210, train_perplexity=68.72454

Batch 134220, train_perplexity=70.74854

Batch 134230, train_perplexity=65.95927

Batch 134240, train_perplexity=67.78032

Batch 134250, train_perplexity=67.17967

Batch 134260, train_perplexity=71.822075

Batch 134270, train_perplexity=68.39064

Batch 134280, train_perplexity=65.09328

Batch 134290, train_perplexity=65.24393

Batch 134300, train_perplexity=68.100815

Batch 134310, train_perplexity=71.82516

Batch 134320, train_perplexity=65.95033

Batch 134330, train_perplexity=72.993675

Batch 134340, train_perplexity=73.62769

Batch 134350, train_perplexity=73.025635

Batch 134360, train_perplexity=65.76079

Batch 134370, train_perplexity=72.57772

Batch 134380, train_perplexity=61.143616

Batch 134390, train_perplexity=69.61911

Batch 134400, train_perplexity=66.0438

Batch 134410, train_perplexity=65.95429

Batch 134420, train_perplexity=66.78353

Batch 134430, train_perplexity=64.00357

Batch 134440, train_perplexity=67.51411

Batch 134450, train_perplexity=61.584545

Batch 134460, train_perplexity=70.39957

Batch 134470, train_perplexity=64.31918

Batch 134480, train_perplexity=67.25878

Batch 134490, train_perplexity=75.94819

Batch 134500, train_perplexity=59.97097

Batch 134510, train_perplexity=73.26253

Batch 134520, train_perplexity=63.072674

Batch 134530, train_perplexity=74.082306

Batch 134540, train_perplexity=66.855606

Batch 134550, train_perplexity=65.04742

Batch 134560, train_perplexity=64.86799

Batch 134570, train_perplexity=70.374695

Batch 134580, train_perplexity=64.69972

Batch 134590, train_perplexity=69.3104

Batch 134600, train_perplexity=69.869804

Batch 134610, train_perplexity=65.5944

Batch 134620, train_perplexity=64.132095

Batch 134630, train_perplexity=67.79613

Batch 134640, train_perplexity=63.82507

Batch 134650, train_perplexity=68.64132

Batch 134660, train_perplexity=69.47857

Batch 134670, train_perplexity=66.79627

Batch 134680, train_perplexity=64.211105

Batch 134690, train_perplexity=59.99202

Batch 134700, train_perplexity=64.58783

Batch 134710, train_perplexity=69.919395

Batch 134720, train_perplexity=69.327095

Batch 134730, train_perplexity=68.66093

Batch 134740, train_perplexity=69.49606

Batch 134750, train_perplexity=65.232605

Batch 134760, train_perplexity=67.645325

Batch 134770, train_perplexity=70.05956
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 134780, train_perplexity=63.08347

Batch 134790, train_perplexity=72.49267

Batch 134800, train_perplexity=67.77938

Batch 134810, train_perplexity=68.712875

Batch 134820, train_perplexity=67.234604

Batch 134830, train_perplexity=64.95902

Batch 134840, train_perplexity=66.76564

Batch 134850, train_perplexity=67.530464

Batch 134860, train_perplexity=72.53963

Batch 134870, train_perplexity=72.136635

Batch 134880, train_perplexity=61.716595

Batch 134890, train_perplexity=73.43745

Batch 134900, train_perplexity=81.26183

Batch 134910, train_perplexity=57.538757

Batch 134920, train_perplexity=70.957535

Batch 134930, train_perplexity=65.014114

Batch 134940, train_perplexity=69.23186

Batch 134950, train_perplexity=66.52444

Batch 134960, train_perplexity=65.33478

Batch 134970, train_perplexity=58.27294

Batch 134980, train_perplexity=73.879326

Batch 134990, train_perplexity=61.761932

Batch 135000, train_perplexity=70.45548

Batch 135010, train_perplexity=61.373295

Batch 135020, train_perplexity=67.87159

Batch 135030, train_perplexity=65.94624

Batch 135040, train_perplexity=67.0991

Batch 135050, train_perplexity=68.38373

Batch 135060, train_perplexity=69.22736

Batch 135070, train_perplexity=66.196236

Batch 135080, train_perplexity=67.292145

Batch 135090, train_perplexity=67.40557

Batch 135100, train_perplexity=73.15198

Batch 135110, train_perplexity=68.77909

Batch 135120, train_perplexity=66.038635

Batch 135130, train_perplexity=68.59005

Batch 135140, train_perplexity=65.43195

Batch 135150, train_perplexity=77.93648

Batch 135160, train_perplexity=62.677044

Batch 135170, train_perplexity=72.16881

Batch 135180, train_perplexity=64.236275

Batch 135190, train_perplexity=63.334538

Batch 135200, train_perplexity=70.15735

Batch 135210, train_perplexity=67.55803

Batch 135220, train_perplexity=70.06357

Batch 135230, train_perplexity=65.37108

Batch 135240, train_perplexity=66.054504

Batch 135250, train_perplexity=61.97493

Batch 135260, train_perplexity=61.53077

Batch 135270, train_perplexity=64.93009

Batch 135280, train_perplexity=62.197086

Batch 135290, train_perplexity=70.85438

Batch 135300, train_perplexity=61.597816

Batch 135310, train_perplexity=75.14567

Batch 135320, train_perplexity=69.65537

Batch 135330, train_perplexity=65.45651

Batch 135340, train_perplexity=64.556175

Batch 135350, train_perplexity=62.505817

Batch 135360, train_perplexity=76.451355

Batch 135370, train_perplexity=68.59201

Batch 135380, train_perplexity=68.32558

Batch 135390, train_perplexity=69.82444

Batch 135400, train_perplexity=64.703514

Batch 135410, train_perplexity=65.58001

Batch 135420, train_perplexity=72.26902

Batch 135430, train_perplexity=69.98037

Batch 135440, train_perplexity=65.69641

Batch 135450, train_perplexity=69.662285

Batch 135460, train_perplexity=67.933205

Batch 135470, train_perplexity=64.48947

Batch 135480, train_perplexity=72.03163

Batch 135490, train_perplexity=64.19929

Batch 135500, train_perplexity=70.05395

Batch 135510, train_perplexity=65.8869

Batch 135520, train_perplexity=55.69922

Batch 135530, train_perplexity=62.999813

Batch 135540, train_perplexity=65.892494

Batch 135550, train_perplexity=66.98832

Batch 135560, train_perplexity=63.401104

Batch 135570, train_perplexity=66.327774

Batch 135580, train_perplexity=73.29209

Batch 135590, train_perplexity=67.50242

Batch 135600, train_perplexity=69.81978

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00035-of-00100
Loaded 305297 sentences.
Finished loading
Batch 135610, train_perplexity=67.55558

Batch 135620, train_perplexity=65.42281

Batch 135630, train_perplexity=62.752132

Batch 135640, train_perplexity=61.99704

Batch 135650, train_perplexity=64.96205

Batch 135660, train_perplexity=65.78767

Batch 135670, train_perplexity=64.93901

Batch 135680, train_perplexity=65.229866

Batch 135690, train_perplexity=61.084866

Batch 135700, train_perplexity=74.253975

Batch 135710, train_perplexity=71.656334

Batch 135720, train_perplexity=60.515213

Batch 135730, train_perplexity=74.04466

Batch 135740, train_perplexity=71.8417

Batch 135750, train_perplexity=62.388973

Batch 135760, train_perplexity=61.874596

Batch 135770, train_perplexity=63.5276

Batch 135780, train_perplexity=66.23558

Batch 135790, train_perplexity=63.67396

Batch 135800, train_perplexity=71.57851

Batch 135810, train_perplexity=71.05109

Batch 135820, train_perplexity=64.63786

Batch 135830, train_perplexity=68.49955

Batch 135840, train_perplexity=72.49204

Batch 135850, train_perplexity=63.201675

Batch 135860, train_perplexity=73.009445

Batch 135870, train_perplexity=64.06119

Batch 135880, train_perplexity=64.27378

Batch 135890, train_perplexity=60.592884

Batch 135900, train_perplexity=72.46281

Batch 135910, train_perplexity=67.22377

Batch 135920, train_perplexity=66.3477

Batch 135930, train_perplexity=64.45098

Batch 135940, train_perplexity=65.70361

Batch 135950, train_perplexity=62.48776

Batch 135960, train_perplexity=68.12946

Batch 135970, train_perplexity=74.11782

Batch 135980, train_perplexity=65.50803

Batch 135990, train_perplexity=62.785774

Batch 136000, train_perplexity=64.85246

Batch 136010, train_perplexity=69.73227

Batch 136020, train_perplexity=70.58188

Batch 136030, train_perplexity=63.75608

Batch 136040, train_perplexity=65.02583

Batch 136050, train_perplexity=69.809456

Batch 136060, train_perplexity=64.30103

Batch 136070, train_perplexity=68.80986

Batch 136080, train_perplexity=66.12787

Batch 136090, train_perplexity=65.73213

Batch 136100, train_perplexity=70.22251

Batch 136110, train_perplexity=67.1118

Batch 136120, train_perplexity=72.28274

Batch 136130, train_perplexity=67.49257

Batch 136140, train_perplexity=65.01381

Batch 136150, train_perplexity=59.346214

Batch 136160, train_perplexity=72.09163

Batch 136170, train_perplexity=68.10114

Batch 136180, train_perplexity=67.90561

Batch 136190, train_perplexity=66.926

Batch 136200, train_perplexity=64.717186

Batch 136210, train_perplexity=72.478806

Batch 136220, train_perplexity=65.66822

Batch 136230, train_perplexity=58.289894

Batch 136240, train_perplexity=63.81022

Batch 136250, train_perplexity=72.21783

Batch 136260, train_perplexity=66.04506

Batch 136270, train_perplexity=64.36824

Batch 136280, train_perplexity=70.70011

Batch 136290, train_perplexity=57.83479

Batch 136300, train_perplexity=69.08392

Batch 136310, train_perplexity=75.270355

Batch 136320, train_perplexity=57.633793

Batch 136330, train_perplexity=65.28078

Batch 136340, train_perplexity=66.68422

Batch 136350, train_perplexity=66.21

Batch 136360, train_perplexity=65.649

Batch 136370, train_perplexity=65.08931

Batch 136380, train_perplexity=71.25649

Batch 136390, train_perplexity=67.33082

Batch 136400, train_perplexity=64.475266

Batch 136410, train_perplexity=67.44196

Batch 136420, train_perplexity=64.737526

Batch 136430, train_perplexity=65.173965

Batch 136440, train_perplexity=67.87673

Batch 136450, train_perplexity=64.31842

Batch 136460, train_perplexity=68.88123

Batch 136470, train_perplexity=61.102375

Batch 136480, train_perplexity=65.023544

Batch 136490, train_perplexity=68.63952

Batch 136500, train_perplexity=64.40171

Batch 136510, train_perplexity=64.26967

Batch 136520, train_perplexity=70.21159

Batch 136530, train_perplexity=70.08669

Batch 136540, train_perplexity=72.23484

Batch 136550, train_perplexity=63.41571

Batch 136560, train_perplexity=65.11482

Batch 136570, train_perplexity=65.38299

Batch 136580, train_perplexity=68.68864

Batch 136590, train_perplexity=63.855083

Batch 136600, train_perplexity=64.6015

Batch 136610, train_perplexity=64.79877

Batch 136620, train_perplexity=78.12259

Batch 136630, train_perplexity=69.712395

Batch 136640, train_perplexity=65.55403

Batch 136650, train_perplexity=60.81629

Batch 136660, train_perplexity=69.83037
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 136670, train_perplexity=71.15914

Batch 136680, train_perplexity=68.984116

Batch 136690, train_perplexity=64.61099

Batch 136700, train_perplexity=68.402054

Batch 136710, train_perplexity=66.757965

Batch 136720, train_perplexity=65.179596

Batch 136730, train_perplexity=66.40242

Batch 136740, train_perplexity=67.87266

Batch 136750, train_perplexity=67.79263

Batch 136760, train_perplexity=70.45891

Batch 136770, train_perplexity=65.49192

Batch 136780, train_perplexity=72.22836

Batch 136790, train_perplexity=72.97259

Batch 136800, train_perplexity=68.117905

Batch 136810, train_perplexity=66.962074

Batch 136820, train_perplexity=71.25299

Batch 136830, train_perplexity=66.63339

Batch 136840, train_perplexity=73.70083

Batch 136850, train_perplexity=65.78851

Batch 136860, train_perplexity=61.739998

Batch 136870, train_perplexity=68.51971

Batch 136880, train_perplexity=69.51131

Batch 136890, train_perplexity=73.56814

Batch 136900, train_perplexity=71.23057

Batch 136910, train_perplexity=62.77931

Batch 136920, train_perplexity=61.559853

Batch 136930, train_perplexity=62.243073

Batch 136940, train_perplexity=61.66747

Batch 136950, train_perplexity=65.68839

Batch 136960, train_perplexity=63.739487

Batch 136970, train_perplexity=70.331085

Batch 136980, train_perplexity=66.41306

Batch 136990, train_perplexity=72.35025

Batch 137000, train_perplexity=68.79841

Batch 137010, train_perplexity=64.68337

Batch 137020, train_perplexity=67.742546

Batch 137030, train_perplexity=68.33516

Batch 137040, train_perplexity=67.28101

Batch 137050, train_perplexity=64.07701

Batch 137060, train_perplexity=70.54928

Batch 137070, train_perplexity=62.74226

Batch 137080, train_perplexity=65.33191

Batch 137090, train_perplexity=63.707214

Batch 137100, train_perplexity=70.823135

Batch 137110, train_perplexity=62.120766

Batch 137120, train_perplexity=67.805695

Batch 137130, train_perplexity=71.6063

Batch 137140, train_perplexity=67.58964

Batch 137150, train_perplexity=69.18961

Batch 137160, train_perplexity=67.48543

Batch 137170, train_perplexity=73.56695

Batch 137180, train_perplexity=72.39991

Batch 137190, train_perplexity=61.54603

Batch 137200, train_perplexity=74.21801

Batch 137210, train_perplexity=66.661835

Batch 137220, train_perplexity=66.89866

Batch 137230, train_perplexity=69.25919

Batch 137240, train_perplexity=60.988823

Batch 137250, train_perplexity=70.173706

Batch 137260, train_perplexity=74.78185

Batch 137270, train_perplexity=63.695915

Batch 137280, train_perplexity=61.270252

Batch 137290, train_perplexity=64.577484

Batch 137300, train_perplexity=67.0894

Batch 137310, train_perplexity=67.15662

Batch 137320, train_perplexity=68.72618

Batch 137330, train_perplexity=71.13105

Batch 137340, train_perplexity=60.662354

Batch 137350, train_perplexity=66.46689

Batch 137360, train_perplexity=64.45424

Batch 137370, train_perplexity=73.54899

Batch 137380, train_perplexity=61.74503

Batch 137390, train_perplexity=63.313946

Batch 137400, train_perplexity=71.04177

Batch 137410, train_perplexity=63.723988

Batch 137420, train_perplexity=60.686916

Batch 137430, train_perplexity=66.19646

Batch 137440, train_perplexity=67.79451

Batch 137450, train_perplexity=67.42441

Batch 137460, train_perplexity=59.696903

Batch 137470, train_perplexity=66.87011

Batch 137480, train_perplexity=68.769714

Batch 137490, train_perplexity=65.922195

Batch 137500, train_perplexity=67.39677

Batch 137510, train_perplexity=66.553314

Batch 137520, train_perplexity=63.131016

Batch 137530, train_perplexity=73.426796

Batch 137540, train_perplexity=71.22659

Batch 137550, train_perplexity=74.37829

Batch 137560, train_perplexity=71.77941

Batch 137570, train_perplexity=73.081375

Batch 137580, train_perplexity=71.989876

Batch 137590, train_perplexity=68.05261

Batch 137600, train_perplexity=61.129566

Batch 137610, train_perplexity=67.11564

Batch 137620, train_perplexity=61.66506

Batch 137630, train_perplexity=67.60624

Batch 137640, train_perplexity=69.76859

Batch 137650, train_perplexity=67.913055

Batch 137660, train_perplexity=67.21371

Batch 137670, train_perplexity=68.74568

Batch 137680, train_perplexity=63.527874

Batch 137690, train_perplexity=64.60945

Batch 137700, train_perplexity=65.97096

Batch 137710, train_perplexity=62.839508

Batch 137720, train_perplexity=64.46709

Batch 137730, train_perplexity=66.49707

Batch 137740, train_perplexity=64.00391

Batch 137750, train_perplexity=66.675156

Batch 137760, train_perplexity=69.00033

Batch 137770, train_perplexity=59.73261

Batch 137780, train_perplexity=69.52682

Batch 137790, train_perplexity=61.38629

Batch 137800, train_perplexity=67.534195

Batch 137810, train_perplexity=65.09434

Batch 137820, train_perplexity=62.919773

Batch 137830, train_perplexity=64.682014

Batch 137840, train_perplexity=57.90963

Batch 137850, train_perplexity=63.3778

Batch 137860, train_perplexity=63.204872

Batch 137870, train_perplexity=65.24822

Batch 137880, train_perplexity=60.31856

Batch 137890, train_perplexity=66.60122

Batch 137900, train_perplexity=66.990875

Batch 137910, train_perplexity=61.613506

Batch 137920, train_perplexity=60.01806

Batch 137930, train_perplexity=63.521877

Batch 137940, train_perplexity=71.53408

Batch 137950, train_perplexity=69.527084

Batch 137960, train_perplexity=73.56449

Batch 137970, train_perplexity=60.704487

Batch 137980, train_perplexity=64.03902

Batch 137990, train_perplexity=68.50396

Batch 138000, train_perplexity=66.51118

Batch 138010, train_perplexity=64.77368

Batch 138020, train_perplexity=68.10277

Batch 138030, train_perplexity=65.42986

Batch 138040, train_perplexity=66.77309

Batch 138050, train_perplexity=67.74752

Batch 138060, train_perplexity=73.66053

Batch 138070, train_perplexity=63.25471

Batch 138080, train_perplexity=69.74744

Batch 138090, train_perplexity=70.838135

Batch 138100, train_perplexity=75.2136

Batch 138110, train_perplexity=70.865364

Batch 138120, train_perplexity=62.14485

Batch 138130, train_perplexity=58.916958

Batch 138140, train_perplexity=65.93788

Batch 138150, train_perplexity=67.25583

Batch 138160, train_perplexity=62.74711

Batch 138170, train_perplexity=69.30921

Batch 138180, train_perplexity=58.94326

Batch 138190, train_perplexity=59.89992

Batch 138200, train_perplexity=73.42704

Batch 138210, train_perplexity=66.33204

Batch 138220, train_perplexity=66.67347

Batch 138230, train_perplexity=64.20351

Batch 138240, train_perplexity=71.58936

Batch 138250, train_perplexity=67.389824

Batch 138260, train_perplexity=60.916595

Batch 138270, train_perplexity=63.55169

Batch 138280, train_perplexity=71.34577

Batch 138290, train_perplexity=71.990425

Batch 138300, train_perplexity=73.01129

Batch 138310, train_perplexity=57.676105

Batch 138320, train_perplexity=60.767242

Batch 138330, train_perplexity=65.11141

Batch 138340, train_perplexity=63.200863

Batch 138350, train_perplexity=55.56674

Batch 138360, train_perplexity=63.552475

Batch 138370, train_perplexity=69.70714

Batch 138380, train_perplexity=64.31839

Batch 138390, train_perplexity=64.50756

Batch 138400, train_perplexity=70.80264

Batch 138410, train_perplexity=75.78894

Batch 138420, train_perplexity=69.19743

Batch 138430, train_perplexity=66.7068

Batch 138440, train_perplexity=64.85626

Batch 138450, train_perplexity=68.06154

Batch 138460, train_perplexity=65.74075

Batch 138470, train_perplexity=60.89667

Batch 138480, train_perplexity=58.943542

Batch 138490, train_perplexity=64.67159

Batch 138500, train_perplexity=60.53172

Batch 138510, train_perplexity=69.51562

Batch 138520, train_perplexity=67.62385

Batch 138530, train_perplexity=71.78951

Batch 138540, train_perplexity=67.065445

Batch 138550, train_perplexity=69.082306

Batch 138560, train_perplexity=67.659195

Batch 138570, train_perplexity=61.883564

Batch 138580, train_perplexity=67.76604

Batch 138590, train_perplexity=60.837116

Batch 138600, train_perplexity=72.9729

Batch 138610, train_perplexity=66.755165

Batch 138620, train_perplexity=66.760895

Batch 138630, train_perplexity=72.38224

Batch 138640, train_perplexity=77.76883

Batch 138650, train_perplexity=57.712364
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 138660, train_perplexity=68.82798

Batch 138670, train_perplexity=62.50692

Batch 138680, train_perplexity=61.409477

Batch 138690, train_perplexity=61.69706

Batch 138700, train_perplexity=68.978226

Batch 138710, train_perplexity=61.537342

Batch 138720, train_perplexity=70.19543

Batch 138730, train_perplexity=70.94248

Batch 138740, train_perplexity=67.63506

Batch 138750, train_perplexity=68.71779

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00011-of-00100
Loaded 306290 sentences.
Finished loading
Batch 138760, train_perplexity=65.2091

Batch 138770, train_perplexity=67.93557

Batch 138780, train_perplexity=73.19664

Batch 138790, train_perplexity=69.36876

Batch 138800, train_perplexity=61.722717

Batch 138810, train_perplexity=67.373634

Batch 138820, train_perplexity=67.65694

Batch 138830, train_perplexity=65.21345

Batch 138840, train_perplexity=72.1803

Batch 138850, train_perplexity=67.63784

Batch 138860, train_perplexity=67.66142

Batch 138870, train_perplexity=68.44623

Batch 138880, train_perplexity=69.91723

Batch 138890, train_perplexity=62.033264

Batch 138900, train_perplexity=60.971027

Batch 138910, train_perplexity=59.449932

Batch 138920, train_perplexity=66.12327

Batch 138930, train_perplexity=66.39277

Batch 138940, train_perplexity=66.11116

Batch 138950, train_perplexity=63.48024

Batch 138960, train_perplexity=63.134205

Batch 138970, train_perplexity=65.247665

Batch 138980, train_perplexity=64.98972

Batch 138990, train_perplexity=69.65557

Batch 139000, train_perplexity=59.83027

Batch 139010, train_perplexity=63.275524

Batch 139020, train_perplexity=66.94442

Batch 139030, train_perplexity=67.096504

Batch 139040, train_perplexity=64.927864

Batch 139050, train_perplexity=78.100464

Batch 139060, train_perplexity=67.96509

Batch 139070, train_perplexity=67.76978

Batch 139080, train_perplexity=68.53434

Batch 139090, train_perplexity=69.13289

Batch 139100, train_perplexity=69.60082

Batch 139110, train_perplexity=59.98524

Batch 139120, train_perplexity=64.49362

Batch 139130, train_perplexity=63.056644

Batch 139140, train_perplexity=65.33795

Batch 139150, train_perplexity=59.627712

Batch 139160, train_perplexity=68.10316

Batch 139170, train_perplexity=64.99022

Batch 139180, train_perplexity=67.70902

Batch 139190, train_perplexity=64.649025

Batch 139200, train_perplexity=63.543205

Batch 139210, train_perplexity=68.87279

Batch 139220, train_perplexity=62.73837

Batch 139230, train_perplexity=70.74864

Batch 139240, train_perplexity=64.3121

Batch 139250, train_perplexity=60.208046

Batch 139260, train_perplexity=58.26044

Batch 139270, train_perplexity=68.63396

Batch 139280, train_perplexity=64.08221

Batch 139290, train_perplexity=61.78364

Batch 139300, train_perplexity=66.407745

Batch 139310, train_perplexity=63.227238

Batch 139320, train_perplexity=62.063797

Batch 139330, train_perplexity=59.113323

Batch 139340, train_perplexity=66.47811

Batch 139350, train_perplexity=67.142944

Batch 139360, train_perplexity=57.12241

Batch 139370, train_perplexity=68.34376

Batch 139380, train_perplexity=63.59983

Batch 139390, train_perplexity=61.40014

Batch 139400, train_perplexity=63.915768

Batch 139410, train_perplexity=68.25231

Batch 139420, train_perplexity=57.22672

Batch 139430, train_perplexity=65.29024

Batch 139440, train_perplexity=66.92032

Batch 139450, train_perplexity=67.42862

Batch 139460, train_perplexity=71.74964

Batch 139470, train_perplexity=71.17438

Batch 139480, train_perplexity=58.35327

Batch 139490, train_perplexity=61.753098

Batch 139500, train_perplexity=70.39302

Batch 139510, train_perplexity=66.01294

Batch 139520, train_perplexity=65.44247

Batch 139530, train_perplexity=70.96948

Batch 139540, train_perplexity=66.27873

Batch 139550, train_perplexity=66.93449

Batch 139560, train_perplexity=67.34588

Batch 139570, train_perplexity=60.821484

Batch 139580, train_perplexity=58.057827

Batch 139590, train_perplexity=61.21722

Batch 139600, train_perplexity=64.33934

Batch 139610, train_perplexity=67.54904

Batch 139620, train_perplexity=71.42028

Batch 139630, train_perplexity=76.55671

Batch 139640, train_perplexity=70.58898

Batch 139650, train_perplexity=64.57991

Batch 139660, train_perplexity=67.055756

Batch 139670, train_perplexity=59.26186

Batch 139680, train_perplexity=61.016777

Batch 139690, train_perplexity=61.281006

Batch 139700, train_perplexity=68.642006

Batch 139710, train_perplexity=71.941696

Batch 139720, train_perplexity=65.278786

Batch 139730, train_perplexity=69.865074

Batch 139740, train_perplexity=66.18446

Batch 139750, train_perplexity=67.3834

Batch 139760, train_perplexity=66.889824

Batch 139770, train_perplexity=64.72706

Batch 139780, train_perplexity=61.009968

Batch 139790, train_perplexity=62.06951

Batch 139800, train_perplexity=63.83828

Batch 139810, train_perplexity=66.76083

Batch 139820, train_perplexity=62.335358

Batch 139830, train_perplexity=71.08667

Batch 139840, train_perplexity=73.552704

Batch 139850, train_perplexity=68.19619

Batch 139860, train_perplexity=70.644646

Batch 139870, train_perplexity=62.784218

Batch 139880, train_perplexity=64.94006

Batch 139890, train_perplexity=62.13522

Batch 139900, train_perplexity=69.54758

Batch 139910, train_perplexity=75.90855

Batch 139920, train_perplexity=70.102135

Batch 139930, train_perplexity=70.449974

Batch 139940, train_perplexity=62.265694

Batch 139950, train_perplexity=66.32284

Batch 139960, train_perplexity=63.490593

Batch 139970, train_perplexity=66.893364

Batch 139980, train_perplexity=68.12011

Batch 139990, train_perplexity=64.765526

Batch 140000, train_perplexity=61.531414

Batch 140010, train_perplexity=65.23957

Batch 140020, train_perplexity=61.563168

Batch 140030, train_perplexity=62.360657

Batch 140040, train_perplexity=78.47111

Batch 140050, train_perplexity=60.46583

Batch 140060, train_perplexity=73.84192

Batch 140070, train_perplexity=66.28342

Batch 140080, train_perplexity=65.84017

Batch 140090, train_perplexity=68.80698

Batch 140100, train_perplexity=62.61076

Batch 140110, train_perplexity=64.3979

Batch 140120, train_perplexity=65.266335

Batch 140130, train_perplexity=65.640045

Batch 140140, train_perplexity=68.31893

Batch 140150, train_perplexity=74.28911

Batch 140160, train_perplexity=69.08991

Batch 140170, train_perplexity=63.259808

Batch 140180, train_perplexity=68.3436

Batch 140190, train_perplexity=73.33425

Batch 140200, train_perplexity=61.79911

Batch 140210, train_perplexity=67.984665

Batch 140220, train_perplexity=65.6245

Batch 140230, train_perplexity=65.77032

Batch 140240, train_perplexity=57.150417

Batch 140250, train_perplexity=65.591866

Batch 140260, train_perplexity=60.718845

Batch 140270, train_perplexity=64.452705

Batch 140280, train_perplexity=68.99799

Batch 140290, train_perplexity=72.08894

Batch 140300, train_perplexity=63.7718

Batch 140310, train_perplexity=63.715237

Batch 140320, train_perplexity=64.24755

Batch 140330, train_perplexity=68.04635

Batch 140340, train_perplexity=66.003

Batch 140350, train_perplexity=67.60631

Batch 140360, train_perplexity=67.62365

Batch 140370, train_perplexity=71.191284

Batch 140380, train_perplexity=61.194046

Batch 140390, train_perplexity=71.89917

Batch 140400, train_perplexity=65.68404

Batch 140410, train_perplexity=71.78085

Batch 140420, train_perplexity=63.99472

Batch 140430, train_perplexity=67.789986

Batch 140440, train_perplexity=62.99248

Batch 140450, train_perplexity=64.7882

Batch 140460, train_perplexity=63.026283

Batch 140470, train_perplexity=60.786198

Batch 140480, train_perplexity=63.54775

Batch 140490, train_perplexity=68.135574

Batch 140500, train_perplexity=66.20567

Batch 140510, train_perplexity=68.00399

Batch 140520, train_perplexity=58.408195

Batch 140530, train_perplexity=68.399055

Batch 140540, train_perplexity=74.16749
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 140550, train_perplexity=60.86204

Batch 140560, train_perplexity=68.37394

Batch 140570, train_perplexity=65.41642

Batch 140580, train_perplexity=67.211876

Batch 140590, train_perplexity=62.56629

Batch 140600, train_perplexity=64.09938

Batch 140610, train_perplexity=60.359535

Batch 140620, train_perplexity=68.63763

Batch 140630, train_perplexity=67.661064

Batch 140640, train_perplexity=60.54829

Batch 140650, train_perplexity=66.4842

Batch 140660, train_perplexity=63.883896

Batch 140670, train_perplexity=70.30017

Batch 140680, train_perplexity=69.901726

Batch 140690, train_perplexity=62.932556

Batch 140700, train_perplexity=64.113594

Batch 140710, train_perplexity=61.53018

Batch 140720, train_perplexity=74.88465

Batch 140730, train_perplexity=65.511

Batch 140740, train_perplexity=65.82353

Batch 140750, train_perplexity=65.510254

Batch 140760, train_perplexity=73.320335

Batch 140770, train_perplexity=67.32299

Batch 140780, train_perplexity=62.367764

Batch 140790, train_perplexity=59.744915

Batch 140800, train_perplexity=65.63979

Batch 140810, train_perplexity=74.40106

Batch 140820, train_perplexity=64.9179

Batch 140830, train_perplexity=68.42893

Batch 140840, train_perplexity=68.10608

Batch 140850, train_perplexity=62.132584

Batch 140860, train_perplexity=64.97057

Batch 140870, train_perplexity=66.39834

Batch 140880, train_perplexity=64.60921

Batch 140890, train_perplexity=69.75422

Batch 140900, train_perplexity=65.4783

Batch 140910, train_perplexity=64.06265

Batch 140920, train_perplexity=63.491444

Batch 140930, train_perplexity=65.01964

Batch 140940, train_perplexity=67.10553

Batch 140950, train_perplexity=75.97746

Batch 140960, train_perplexity=71.93044

Batch 140970, train_perplexity=67.961716

Batch 140980, train_perplexity=67.09305

Batch 140990, train_perplexity=57.978703

Batch 141000, train_perplexity=60.498592

Batch 141010, train_perplexity=61.60892

Batch 141020, train_perplexity=71.4164

Batch 141030, train_perplexity=70.293365

Batch 141040, train_perplexity=66.78306

Batch 141050, train_perplexity=66.322266

Batch 141060, train_perplexity=64.99509

Batch 141070, train_perplexity=62.502003

Batch 141080, train_perplexity=73.24824

Batch 141090, train_perplexity=68.158806

Batch 141100, train_perplexity=66.29833

Batch 141110, train_perplexity=63.0445

Batch 141120, train_perplexity=62.121326

Batch 141130, train_perplexity=63.565628

Batch 141140, train_perplexity=67.71926

Batch 141150, train_perplexity=66.99864

Batch 141160, train_perplexity=73.250755

Batch 141170, train_perplexity=65.97511

Batch 141180, train_perplexity=65.01461

Batch 141190, train_perplexity=62.24153

Batch 141200, train_perplexity=62.81728

Batch 141210, train_perplexity=65.603874

Batch 141220, train_perplexity=64.561226

Batch 141230, train_perplexity=70.054985

Batch 141240, train_perplexity=67.81145

Batch 141250, train_perplexity=67.24999

Batch 141260, train_perplexity=65.9693

Batch 141270, train_perplexity=66.46556

Batch 141280, train_perplexity=66.063736

Batch 141290, train_perplexity=65.02543

Batch 141300, train_perplexity=73.966675

Batch 141310, train_perplexity=66.45893

Batch 141320, train_perplexity=56.833603

Batch 141330, train_perplexity=66.47767

Batch 141340, train_perplexity=69.19569

Batch 141350, train_perplexity=62.458923

Batch 141360, train_perplexity=68.5284

Batch 141370, train_perplexity=66.55373

Batch 141380, train_perplexity=68.00314

Batch 141390, train_perplexity=60.84196

Batch 141400, train_perplexity=67.850945

Batch 141410, train_perplexity=58.18362

Batch 141420, train_perplexity=69.60394

Batch 141430, train_perplexity=66.33476

Batch 141440, train_perplexity=70.84881

Batch 141450, train_perplexity=69.67823

Batch 141460, train_perplexity=62.468513

Batch 141470, train_perplexity=60.015396

Batch 141480, train_perplexity=79.14953

Batch 141490, train_perplexity=68.19324

Batch 141500, train_perplexity=67.1766

Batch 141510, train_perplexity=67.94513

Batch 141520, train_perplexity=65.527214

Batch 141530, train_perplexity=64.80418

Batch 141540, train_perplexity=64.07643

Batch 141550, train_perplexity=67.62107

Batch 141560, train_perplexity=66.92198

Batch 141570, train_perplexity=70.880974

Batch 141580, train_perplexity=65.57529

Batch 141590, train_perplexity=52.57129

Batch 141600, train_perplexity=66.83559

Batch 141610, train_perplexity=65.52509

Batch 141620, train_perplexity=65.22723

Batch 141630, train_perplexity=67.90633

Batch 141640, train_perplexity=63.687046

Batch 141650, train_perplexity=68.20823

Batch 141660, train_perplexity=55.13789

Batch 141670, train_perplexity=72.61874

Batch 141680, train_perplexity=59.19648

Batch 141690, train_perplexity=71.76107

Batch 141700, train_perplexity=62.040836

Batch 141710, train_perplexity=67.93055

Batch 141720, train_perplexity=69.16659

Batch 141730, train_perplexity=64.47576

Batch 141740, train_perplexity=63.8202

Batch 141750, train_perplexity=67.6223

Batch 141760, train_perplexity=63.32089

Batch 141770, train_perplexity=64.14011

Batch 141780, train_perplexity=66.1555

Batch 141790, train_perplexity=60.3221

Batch 141800, train_perplexity=64.77306

Batch 141810, train_perplexity=67.32263

Batch 141820, train_perplexity=69.5008

Batch 141830, train_perplexity=67.144226

Batch 141840, train_perplexity=65.3751

Batch 141850, train_perplexity=73.562805

Batch 141860, train_perplexity=67.24089

Batch 141870, train_perplexity=63.76435

Batch 141880, train_perplexity=61.978184

Batch 141890, train_perplexity=66.1087

Batch 141900, train_perplexity=67.33509

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00074-of-00100
Loaded 306892 sentences.
Finished loading
Batch 141910, train_perplexity=62.175232

Batch 141920, train_perplexity=70.29937

Batch 141930, train_perplexity=59.913006

Batch 141940, train_perplexity=58.59591

Batch 141950, train_perplexity=66.766945

Batch 141960, train_perplexity=67.47858

Batch 141970, train_perplexity=66.256645

Batch 141980, train_perplexity=70.2057

Batch 141990, train_perplexity=62.241943

Batch 142000, train_perplexity=70.67574

Batch 142010, train_perplexity=65.31365

Batch 142020, train_perplexity=66.53912

Batch 142030, train_perplexity=64.49384

Batch 142040, train_perplexity=65.68341

Batch 142050, train_perplexity=70.6275

Batch 142060, train_perplexity=66.42817

Batch 142070, train_perplexity=58.095158

Batch 142080, train_perplexity=68.18387

Batch 142090, train_perplexity=66.0884

Batch 142100, train_perplexity=65.04456

Batch 142110, train_perplexity=66.28395

Batch 142120, train_perplexity=66.10788

Batch 142130, train_perplexity=63.863487

Batch 142140, train_perplexity=67.38321

Batch 142150, train_perplexity=62.597744

Batch 142160, train_perplexity=63.542477

Batch 142170, train_perplexity=64.58327

Batch 142180, train_perplexity=67.45853

Batch 142190, train_perplexity=64.90713

Batch 142200, train_perplexity=65.020065

Batch 142210, train_perplexity=69.88153

Batch 142220, train_perplexity=57.822407

Batch 142230, train_perplexity=63.741188

Batch 142240, train_perplexity=59.005352

Batch 142250, train_perplexity=62.401527

Batch 142260, train_perplexity=71.53521

Batch 142270, train_perplexity=68.893974

Batch 142280, train_perplexity=67.72

Batch 142290, train_perplexity=61.811222

Batch 142300, train_perplexity=67.38616

Batch 142310, train_perplexity=71.180824

Batch 142320, train_perplexity=69.75266

Batch 142330, train_perplexity=60.205288

Batch 142340, train_perplexity=57.9312

Batch 142350, train_perplexity=69.42512

Batch 142360, train_perplexity=66.64378

Batch 142370, train_perplexity=67.47867

Batch 142380, train_perplexity=65.50347

Batch 142390, train_perplexity=71.63492

Batch 142400, train_perplexity=68.1016

Batch 142410, train_perplexity=57.484623

Batch 142420, train_perplexity=66.604935

Batch 142430, train_perplexity=71.011795
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 142440, train_perplexity=68.0938

Batch 142450, train_perplexity=69.105995

Batch 142460, train_perplexity=60.52462

Batch 142470, train_perplexity=60.019318

Batch 142480, train_perplexity=64.24313

Batch 142490, train_perplexity=68.64188

Batch 142500, train_perplexity=70.72402

Batch 142510, train_perplexity=69.46505

Batch 142520, train_perplexity=59.286762

Batch 142530, train_perplexity=63.710556

Batch 142540, train_perplexity=67.261154

Batch 142550, train_perplexity=69.275215

Batch 142560, train_perplexity=62.918785

Batch 142570, train_perplexity=67.7379

Batch 142580, train_perplexity=65.9336

Batch 142590, train_perplexity=77.38522

Batch 142600, train_perplexity=66.70298

Batch 142610, train_perplexity=65.88407

Batch 142620, train_perplexity=72.54772

Batch 142630, train_perplexity=63.74511

Batch 142640, train_perplexity=65.565254

Batch 142650, train_perplexity=62.791103

Batch 142660, train_perplexity=68.31909

Batch 142670, train_perplexity=65.781044

Batch 142680, train_perplexity=66.31082

Batch 142690, train_perplexity=60.087185

Batch 142700, train_perplexity=62.062645

Batch 142710, train_perplexity=65.22828

Batch 142720, train_perplexity=67.34694

Batch 142730, train_perplexity=67.798775

Batch 142740, train_perplexity=68.898705

Batch 142750, train_perplexity=65.49795

Batch 142760, train_perplexity=61.060783

Batch 142770, train_perplexity=66.48401

Batch 142780, train_perplexity=63.149952

Batch 142790, train_perplexity=65.95546

Batch 142800, train_perplexity=64.84358

Batch 142810, train_perplexity=65.732635

Batch 142820, train_perplexity=67.14471

Batch 142830, train_perplexity=67.08768

Batch 142840, train_perplexity=70.35691

Batch 142850, train_perplexity=63.9382

Batch 142860, train_perplexity=67.51797

Batch 142870, train_perplexity=71.266754

Batch 142880, train_perplexity=66.365036

Batch 142890, train_perplexity=67.06285

Batch 142900, train_perplexity=74.13015

Batch 142910, train_perplexity=63.09096

Batch 142920, train_perplexity=66.92635

Batch 142930, train_perplexity=71.7534

Batch 142940, train_perplexity=65.86221

Batch 142950, train_perplexity=64.62171

Batch 142960, train_perplexity=67.13532

Batch 142970, train_perplexity=67.25753

Batch 142980, train_perplexity=67.725105

Batch 142990, train_perplexity=70.411354

Batch 143000, train_perplexity=58.615917

Batch 143010, train_perplexity=58.635822

Batch 143020, train_perplexity=64.1343

Batch 143030, train_perplexity=66.564705

Batch 143040, train_perplexity=57.595634

Batch 143050, train_perplexity=60.08025

Batch 143060, train_perplexity=63.3163

Batch 143070, train_perplexity=63.882553

Batch 143080, train_perplexity=65.76446

Batch 143090, train_perplexity=64.15032

Batch 143100, train_perplexity=61.38436

Batch 143110, train_perplexity=76.52773

Batch 143120, train_perplexity=69.83143

Batch 143130, train_perplexity=66.83613

Batch 143140, train_perplexity=66.77051

Batch 143150, train_perplexity=63.924942

Batch 143160, train_perplexity=67.44383

Batch 143170, train_perplexity=64.14519

Batch 143180, train_perplexity=67.26584

Batch 143190, train_perplexity=61.27116

Batch 143200, train_perplexity=69.06646

Batch 143210, train_perplexity=64.3688

Batch 143220, train_perplexity=66.97759

Batch 143230, train_perplexity=66.2065

Batch 143240, train_perplexity=66.27892

Batch 143250, train_perplexity=73.916885

Batch 143260, train_perplexity=65.77656

Batch 143270, train_perplexity=61.81659

Batch 143280, train_perplexity=65.02063

Batch 143290, train_perplexity=70.601776

Batch 143300, train_perplexity=69.333176

Batch 143310, train_perplexity=62.36788

Batch 143320, train_perplexity=67.694565

Batch 143330, train_perplexity=77.51541

Batch 143340, train_perplexity=63.534813

Batch 143350, train_perplexity=66.04109

Batch 143360, train_perplexity=65.23335

Batch 143370, train_perplexity=69.1028

Batch 143380, train_perplexity=59.17091

Batch 143390, train_perplexity=66.546745

Batch 143400, train_perplexity=66.45535

Batch 143410, train_perplexity=64.62683

Batch 143420, train_perplexity=68.924995

Batch 143430, train_perplexity=62.320465

Batch 143440, train_perplexity=63.189983

Batch 143450, train_perplexity=59.259655

Batch 143460, train_perplexity=62.23613

Batch 143470, train_perplexity=62.65723

Batch 143480, train_perplexity=75.36118

Batch 143490, train_perplexity=66.20833

Batch 143500, train_perplexity=66.97363

Batch 143510, train_perplexity=67.283676

Batch 143520, train_perplexity=67.37061

Batch 143530, train_perplexity=71.15049

Batch 143540, train_perplexity=70.27269

Batch 143550, train_perplexity=66.29078

Batch 143560, train_perplexity=66.48363

Batch 143570, train_perplexity=64.353455

Batch 143580, train_perplexity=64.93072

Batch 143590, train_perplexity=69.82817

Batch 143600, train_perplexity=73.613335

Batch 143610, train_perplexity=60.799995

Batch 143620, train_perplexity=68.32727

Batch 143630, train_perplexity=59.911293

Batch 143640, train_perplexity=65.700485

Batch 143650, train_perplexity=70.93213

Batch 143660, train_perplexity=68.14295

Batch 143670, train_perplexity=66.557846

Batch 143680, train_perplexity=61.006798

Batch 143690, train_perplexity=66.946106

Batch 143700, train_perplexity=63.636837

Batch 143710, train_perplexity=64.12879

Batch 143720, train_perplexity=65.84911

Batch 143730, train_perplexity=68.815506

Batch 143740, train_perplexity=68.832405

Batch 143750, train_perplexity=69.812454

Batch 143760, train_perplexity=71.69684

Batch 143770, train_perplexity=71.01027

Batch 143780, train_perplexity=65.79789

Batch 143790, train_perplexity=65.02562

Batch 143800, train_perplexity=62.445522

Batch 143810, train_perplexity=62.23506

Batch 143820, train_perplexity=63.14992

Batch 143830, train_perplexity=61.26184

Batch 143840, train_perplexity=66.85806

Batch 143850, train_perplexity=62.31378

Batch 143860, train_perplexity=63.737965

Batch 143870, train_perplexity=64.4089

Batch 143880, train_perplexity=65.85358

Batch 143890, train_perplexity=69.05879

Batch 143900, train_perplexity=70.32217

Batch 143910, train_perplexity=66.406

Batch 143920, train_perplexity=70.18

Batch 143930, train_perplexity=61.329647

Batch 143940, train_perplexity=67.01196

Batch 143950, train_perplexity=63.371998

Batch 143960, train_perplexity=67.90946

Batch 143970, train_perplexity=66.723915

Batch 143980, train_perplexity=60.170647

Batch 143990, train_perplexity=66.02163

Batch 144000, train_perplexity=59.797073

Batch 144010, train_perplexity=68.755875

Batch 144020, train_perplexity=67.28826

Batch 144030, train_perplexity=67.03664

Batch 144040, train_perplexity=65.19296

Batch 144050, train_perplexity=63.708916

Batch 144060, train_perplexity=64.89825

Batch 144070, train_perplexity=70.527885

Batch 144080, train_perplexity=67.19355

Batch 144090, train_perplexity=57.757072

Batch 144100, train_perplexity=70.384834

Batch 144110, train_perplexity=63.751583

Batch 144120, train_perplexity=73.56337

Batch 144130, train_perplexity=59.549545

Batch 144140, train_perplexity=66.46549

Batch 144150, train_perplexity=65.94193

Batch 144160, train_perplexity=69.40443

Batch 144170, train_perplexity=60.096127

Batch 144180, train_perplexity=72.11266

Batch 144190, train_perplexity=65.97474

Batch 144200, train_perplexity=69.16111

Batch 144210, train_perplexity=65.78384

Batch 144220, train_perplexity=65.72674

Batch 144230, train_perplexity=60.162987

Batch 144240, train_perplexity=65.09731

Batch 144250, train_perplexity=67.174675

Batch 144260, train_perplexity=64.75963

Batch 144270, train_perplexity=69.52566

Batch 144280, train_perplexity=63.088345

Batch 144290, train_perplexity=67.097015

Batch 144300, train_perplexity=68.179054

Batch 144310, train_perplexity=64.499374

Batch 144320, train_perplexity=68.38516

Batch 144330, train_perplexity=62.480965

Batch 144340, train_perplexity=63.027607

Batch 144350, train_perplexity=62.438286

Batch 144360, train_perplexity=64.048294

Batch 144370, train_perplexity=63.587215

Batch 144380, train_perplexity=62.759975

Batch 144390, train_perplexity=67.92511

Batch 144400, train_perplexity=62.640205

Batch 144410, train_perplexity=65.256004

Batch 144420, train_perplexity=67.290474
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 144430, train_perplexity=70.55863

Batch 144440, train_perplexity=66.04102

Batch 144450, train_perplexity=63.35387

Batch 144460, train_perplexity=63.058838

Batch 144470, train_perplexity=60.850464

Batch 144480, train_perplexity=63.433704

Batch 144490, train_perplexity=71.61866

Batch 144500, train_perplexity=61.30497

Batch 144510, train_perplexity=63.426476

Batch 144520, train_perplexity=71.915665

Batch 144530, train_perplexity=68.97471

Batch 144540, train_perplexity=73.2793

Batch 144550, train_perplexity=68.22179

Batch 144560, train_perplexity=65.49288

Batch 144570, train_perplexity=65.9541

Batch 144580, train_perplexity=63.770554

Batch 144590, train_perplexity=69.45451

Batch 144600, train_perplexity=68.203575

Batch 144610, train_perplexity=69.583336

Batch 144620, train_perplexity=66.889885

Batch 144630, train_perplexity=67.734146

Batch 144640, train_perplexity=66.76173

Batch 144650, train_perplexity=69.82737

Batch 144660, train_perplexity=61.36926

Batch 144670, train_perplexity=65.98978

Batch 144680, train_perplexity=70.909134

Batch 144690, train_perplexity=63.207073

Batch 144700, train_perplexity=63.50946

Batch 144710, train_perplexity=62.363926

Batch 144720, train_perplexity=55.84104

Batch 144730, train_perplexity=73.37664

Batch 144740, train_perplexity=63.706123

Batch 144750, train_perplexity=74.42149

Batch 144760, train_perplexity=63.65183

Batch 144770, train_perplexity=63.250275

Batch 144780, train_perplexity=57.150772

Batch 144790, train_perplexity=65.34393

Batch 144800, train_perplexity=68.894104

Batch 144810, train_perplexity=63.621666

Batch 144820, train_perplexity=65.61826

Batch 144830, train_perplexity=63.91912

Batch 144840, train_perplexity=65.79764

Batch 144850, train_perplexity=67.15803

Batch 144860, train_perplexity=69.678795

Batch 144870, train_perplexity=64.414604

Batch 144880, train_perplexity=62.093548

Batch 144890, train_perplexity=67.51871

Batch 144900, train_perplexity=70.63585

Batch 144910, train_perplexity=67.01158

Batch 144920, train_perplexity=62.353935

Batch 144930, train_perplexity=65.19302

Batch 144940, train_perplexity=67.23698

Batch 144950, train_perplexity=71.04634

Batch 144960, train_perplexity=63.54884

Batch 144970, train_perplexity=65.62459

Batch 144980, train_perplexity=65.69804

Batch 144990, train_perplexity=68.11134

Batch 145000, train_perplexity=63.64934

Batch 145010, train_perplexity=64.269424

Batch 145020, train_perplexity=62.63698

Batch 145030, train_perplexity=65.98493

Batch 145040, train_perplexity=64.588135

Batch 145050, train_perplexity=62.61488

Batch 145060, train_perplexity=63.50201

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00041-of-00100
Loaded 306092 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00041-of-00100
Loaded 306092 sentences.
Finished loading
Batch 145070, train_perplexity=73.25226

Batch 145080, train_perplexity=68.02549

Batch 145090, train_perplexity=70.72752

Batch 145100, train_perplexity=71.20235

Batch 145110, train_perplexity=66.10246

Batch 145120, train_perplexity=65.45196

Batch 145130, train_perplexity=60.489853

Batch 145140, train_perplexity=64.09953

Batch 145150, train_perplexity=68.047554

Batch 145160, train_perplexity=69.24982

Batch 145170, train_perplexity=65.29155

Batch 145180, train_perplexity=61.691998

Batch 145190, train_perplexity=70.207275

Batch 145200, train_perplexity=67.95997

Batch 145210, train_perplexity=64.29247

Batch 145220, train_perplexity=71.00478

Batch 145230, train_perplexity=66.826126

Batch 145240, train_perplexity=70.956116

Batch 145250, train_perplexity=61.954308

Batch 145260, train_perplexity=64.180336

Batch 145270, train_perplexity=58.294285

Batch 145280, train_perplexity=72.37792

Batch 145290, train_perplexity=65.23746

Batch 145300, train_perplexity=68.13258

Batch 145310, train_perplexity=59.972942

Batch 145320, train_perplexity=70.59299

Batch 145330, train_perplexity=69.91173

Batch 145340, train_perplexity=71.13712

Batch 145350, train_perplexity=64.763

Batch 145360, train_perplexity=68.15396

Batch 145370, train_perplexity=66.47183

Batch 145380, train_perplexity=65.05462

Batch 145390, train_perplexity=70.07366

Batch 145400, train_perplexity=60.416836

Batch 145410, train_perplexity=66.23302

Batch 145420, train_perplexity=72.13784

Batch 145430, train_perplexity=66.938705

Batch 145440, train_perplexity=63.899464

Batch 145450, train_perplexity=70.73336

Batch 145460, train_perplexity=66.09553

Batch 145470, train_perplexity=69.176414

Batch 145480, train_perplexity=66.311455

Batch 145490, train_perplexity=64.18674

Batch 145500, train_perplexity=60.92903

Batch 145510, train_perplexity=65.13507

Batch 145520, train_perplexity=60.84254

Batch 145530, train_perplexity=64.186676

Batch 145540, train_perplexity=73.118774

Batch 145550, train_perplexity=66.57099

Batch 145560, train_perplexity=66.002365

Batch 145570, train_perplexity=64.59621

Batch 145580, train_perplexity=67.776924

Batch 145590, train_perplexity=66.72446

Batch 145600, train_perplexity=71.71756

Batch 145610, train_perplexity=63.84583

Batch 145620, train_perplexity=62.325726

Batch 145630, train_perplexity=61.220024

Batch 145640, train_perplexity=63.21072

Batch 145650, train_perplexity=62.801167

Batch 145660, train_perplexity=62.78515

Batch 145670, train_perplexity=60.057594

Batch 145680, train_perplexity=59.87228

Batch 145690, train_perplexity=61.822983

Batch 145700, train_perplexity=67.564606

Batch 145710, train_perplexity=68.812126

Batch 145720, train_perplexity=67.196915

Batch 145730, train_perplexity=63.398777

Batch 145740, train_perplexity=69.17401

Batch 145750, train_perplexity=59.168034

Batch 145760, train_perplexity=69.74534

Batch 145770, train_perplexity=67.1848

Batch 145780, train_perplexity=70.33145

Batch 145790, train_perplexity=74.429375

Batch 145800, train_perplexity=63.70916

Batch 145810, train_perplexity=59.499054

Batch 145820, train_perplexity=69.29289

Batch 145830, train_perplexity=68.698265

Batch 145840, train_perplexity=59.80343

Batch 145850, train_perplexity=70.91346

Batch 145860, train_perplexity=67.49164

Batch 145870, train_perplexity=63.49922

Batch 145880, train_perplexity=61.54729

Batch 145890, train_perplexity=64.36168

Batch 145900, train_perplexity=66.04644

Batch 145910, train_perplexity=69.411545

Batch 145920, train_perplexity=63.03999

Batch 145930, train_perplexity=64.976494

Batch 145940, train_perplexity=75.643875

Batch 145950, train_perplexity=63.652317

Batch 145960, train_perplexity=70.46378

Batch 145970, train_perplexity=63.36517

Batch 145980, train_perplexity=67.15662

Batch 145990, train_perplexity=66.87027

Batch 146000, train_perplexity=67.530136

Batch 146010, train_perplexity=63.90888

Batch 146020, train_perplexity=64.55962

Batch 146030, train_perplexity=62.35257

Batch 146040, train_perplexity=67.4863

Batch 146050, train_perplexity=68.580765

Batch 146060, train_perplexity=63.587517

Batch 146070, train_perplexity=65.060135

Batch 146080, train_perplexity=70.773476

Batch 146090, train_perplexity=59.004227

Batch 146100, train_perplexity=66.55531

Batch 146110, train_perplexity=64.64915

Batch 146120, train_perplexity=66.46017

Batch 146130, train_perplexity=66.18068

Batch 146140, train_perplexity=66.73594

Batch 146150, train_perplexity=66.173386

Batch 146160, train_perplexity=60.253536

Batch 146170, train_perplexity=58.293118

Batch 146180, train_perplexity=59.83258

Batch 146190, train_perplexity=65.1078

Batch 146200, train_perplexity=66.08538

Batch 146210, train_perplexity=67.82277

Batch 146220, train_perplexity=58.782936

Batch 146230, train_perplexity=70.73151

Batch 146240, train_perplexity=71.97676

Batch 146250, train_perplexity=61.375523

Batch 146260, train_perplexity=65.044754

Batch 146270, train_perplexity=59.77797

Batch 146280, train_perplexity=64.56483

Batch 146290, train_perplexity=67.90347

Batch 146300, train_perplexity=73.3758

Batch 146310, train_perplexity=65.92634
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 146320, train_perplexity=63.312256

Batch 146330, train_perplexity=64.82915

Batch 146340, train_perplexity=66.638824

Batch 146350, train_perplexity=64.98176

Batch 146360, train_perplexity=66.67557

Batch 146370, train_perplexity=64.23284

Batch 146380, train_perplexity=67.91679

Batch 146390, train_perplexity=69.38785

Batch 146400, train_perplexity=63.41087

Batch 146410, train_perplexity=66.65386

Batch 146420, train_perplexity=65.84249

Batch 146430, train_perplexity=66.65456

Batch 146440, train_perplexity=71.53671

Batch 146450, train_perplexity=74.55014

Batch 146460, train_perplexity=67.51501

Batch 146470, train_perplexity=69.381

Batch 146480, train_perplexity=64.34701

Batch 146490, train_perplexity=71.14133

Batch 146500, train_perplexity=66.98909

Batch 146510, train_perplexity=64.62116

Batch 146520, train_perplexity=65.14948

Batch 146530, train_perplexity=65.93096

Batch 146540, train_perplexity=66.72773

Batch 146550, train_perplexity=59.327824

Batch 146560, train_perplexity=64.983

Batch 146570, train_perplexity=69.381165

Batch 146580, train_perplexity=69.06528

Batch 146590, train_perplexity=65.04668

Batch 146600, train_perplexity=65.938385

Batch 146610, train_perplexity=62.84712

Batch 146620, train_perplexity=63.634834

Batch 146630, train_perplexity=65.060135

Batch 146640, train_perplexity=65.35347

Batch 146650, train_perplexity=77.82009

Batch 146660, train_perplexity=72.77959

Batch 146670, train_perplexity=69.89586

Batch 146680, train_perplexity=69.186905

Batch 146690, train_perplexity=70.99642

Batch 146700, train_perplexity=60.80452

Batch 146710, train_perplexity=64.35793

Batch 146720, train_perplexity=60.052067

Batch 146730, train_perplexity=61.37605

Batch 146740, train_perplexity=67.13882

Batch 146750, train_perplexity=68.074295

Batch 146760, train_perplexity=70.84465

Batch 146770, train_perplexity=64.54032

Batch 146780, train_perplexity=67.254295

Batch 146790, train_perplexity=69.89266

Batch 146800, train_perplexity=61.094772

Batch 146810, train_perplexity=69.30799

Batch 146820, train_perplexity=64.65235

Batch 146830, train_perplexity=61.744442

Batch 146840, train_perplexity=73.65913

Batch 146850, train_perplexity=60.50829

Batch 146860, train_perplexity=65.73558

Batch 146870, train_perplexity=62.2586

Batch 146880, train_perplexity=64.56726

Batch 146890, train_perplexity=62.11777

Batch 146900, train_perplexity=63.422787

Batch 146910, train_perplexity=57.46593

Batch 146920, train_perplexity=64.87198

Batch 146930, train_perplexity=62.76135

Batch 146940, train_perplexity=66.8578

Batch 146950, train_perplexity=64.24789

Batch 146960, train_perplexity=64.44527

Batch 146970, train_perplexity=70.08108

Batch 146980, train_perplexity=58.14177

Batch 146990, train_perplexity=73.3197

Batch 147000, train_perplexity=68.7723

Batch 147010, train_perplexity=68.93867

Batch 147020, train_perplexity=68.18026

Batch 147030, train_perplexity=67.49563

Batch 147040, train_perplexity=61.24227

Batch 147050, train_perplexity=62.12998

Batch 147060, train_perplexity=64.29024

Batch 147070, train_perplexity=65.53346

Batch 147080, train_perplexity=64.06919

Batch 147090, train_perplexity=65.17621

Batch 147100, train_perplexity=65.893

Batch 147110, train_perplexity=61.626198

Batch 147120, train_perplexity=68.683

Batch 147130, train_perplexity=67.86984

Batch 147140, train_perplexity=65.17751

Batch 147150, train_perplexity=67.43161

Batch 147160, train_perplexity=68.30147

Batch 147170, train_perplexity=61.751625

Batch 147180, train_perplexity=67.40178

Batch 147190, train_perplexity=72.37875

Batch 147200, train_perplexity=68.015335

Batch 147210, train_perplexity=63.65881

Batch 147220, train_perplexity=67.36467

Batch 147230, train_perplexity=66.255135

Batch 147240, train_perplexity=59.166397

Batch 147250, train_perplexity=63.270485

Batch 147260, train_perplexity=71.03272

Batch 147270, train_perplexity=71.42917

Batch 147280, train_perplexity=67.55526

Batch 147290, train_perplexity=65.872955

Batch 147300, train_perplexity=59.61074

Batch 147310, train_perplexity=68.92447

Batch 147320, train_perplexity=63.34082

Batch 147330, train_perplexity=65.767876

Batch 147340, train_perplexity=66.036995

Batch 147350, train_perplexity=63.94799

Batch 147360, train_perplexity=72.37882

Batch 147370, train_perplexity=66.50807

Batch 147380, train_perplexity=73.89627

Batch 147390, train_perplexity=71.977104

Batch 147400, train_perplexity=62.20883

Batch 147410, train_perplexity=59.826504

Batch 147420, train_perplexity=68.64626

Batch 147430, train_perplexity=70.00774

Batch 147440, train_perplexity=60.978615

Batch 147450, train_perplexity=67.72956

Batch 147460, train_perplexity=61.54503

Batch 147470, train_perplexity=63.31666

Batch 147480, train_perplexity=61.83345

Batch 147490, train_perplexity=69.07163

Batch 147500, train_perplexity=62.328697

Batch 147510, train_perplexity=71.756485

Batch 147520, train_perplexity=73.74881

Batch 147530, train_perplexity=67.87644

Batch 147540, train_perplexity=68.553986

Batch 147550, train_perplexity=58.5259

Batch 147560, train_perplexity=64.279785

Batch 147570, train_perplexity=73.042496

Batch 147580, train_perplexity=64.45522

Batch 147590, train_perplexity=68.83691

Batch 147600, train_perplexity=58.25572

Batch 147610, train_perplexity=63.81539

Batch 147620, train_perplexity=66.24477

Batch 147630, train_perplexity=62.806976

Batch 147640, train_perplexity=66.246284

Batch 147650, train_perplexity=64.97469

Batch 147660, train_perplexity=65.33842

Batch 147670, train_perplexity=68.6387

Batch 147680, train_perplexity=71.42757

Batch 147690, train_perplexity=70.5871

Batch 147700, train_perplexity=60.925777

Batch 147710, train_perplexity=76.233086

Batch 147720, train_perplexity=60.66704

Batch 147730, train_perplexity=61.72793

Batch 147740, train_perplexity=59.789745

Batch 147750, train_perplexity=69.208885

Batch 147760, train_perplexity=58.37395

Batch 147770, train_perplexity=56.87459

Batch 147780, train_perplexity=65.75094

Batch 147790, train_perplexity=62.663982

Batch 147800, train_perplexity=67.46191

Batch 147810, train_perplexity=66.30855

Batch 147820, train_perplexity=63.089546

Batch 147830, train_perplexity=66.0159

Batch 147840, train_perplexity=57.6286

Batch 147850, train_perplexity=64.84352

Batch 147860, train_perplexity=60.416145

Batch 147870, train_perplexity=74.41575

Batch 147880, train_perplexity=64.85837

Batch 147890, train_perplexity=59.310966

Batch 147900, train_perplexity=65.09148

Batch 147910, train_perplexity=65.71396

Batch 147920, train_perplexity=65.21357

Batch 147930, train_perplexity=66.78207

Batch 147940, train_perplexity=63.24503

Batch 147950, train_perplexity=67.431

Batch 147960, train_perplexity=66.841354

Batch 147970, train_perplexity=70.56102

Batch 147980, train_perplexity=68.951096

Batch 147990, train_perplexity=70.491844

Batch 148000, train_perplexity=69.27313

Batch 148010, train_perplexity=68.8204

Batch 148020, train_perplexity=63.814114

Batch 148030, train_perplexity=66.31057

Batch 148040, train_perplexity=64.2961

Batch 148050, train_perplexity=58.737682

Batch 148060, train_perplexity=67.7007

Batch 148070, train_perplexity=73.517715

Batch 148080, train_perplexity=65.481865

Batch 148090, train_perplexity=70.69017

Batch 148100, train_perplexity=68.75483

Batch 148110, train_perplexity=75.1107

Batch 148120, train_perplexity=69.04595

Batch 148130, train_perplexity=63.561172

Batch 148140, train_perplexity=62.331642

Batch 148150, train_perplexity=68.30613

Batch 148160, train_perplexity=70.17093

Batch 148170, train_perplexity=66.21357

Batch 148180, train_perplexity=60.50021

Batch 148190, train_perplexity=65.66052

Batch 148200, train_perplexity=65.96455

Batch 148210, train_perplexity=63.44402

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00028-of-00100
Loaded 305485 sentences.WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Finished loading
Batch 148220, train_perplexity=65.318794

Batch 148230, train_perplexity=68.0975

Batch 148240, train_perplexity=68.991745

Batch 148250, train_perplexity=63.62367

Batch 148260, train_perplexity=66.00066

Batch 148270, train_perplexity=69.210266

Batch 148280, train_perplexity=61.65321

Batch 148290, train_perplexity=68.99339

Batch 148300, train_perplexity=66.37694

Batch 148310, train_perplexity=62.828423

Batch 148320, train_perplexity=71.48778

Batch 148330, train_perplexity=62.732327

Batch 148340, train_perplexity=66.11747

Batch 148350, train_perplexity=69.15514

Batch 148360, train_perplexity=64.39569

Batch 148370, train_perplexity=61.095673

Batch 148380, train_perplexity=63.05496

Batch 148390, train_perplexity=67.531876

Batch 148400, train_perplexity=67.66203

Batch 148410, train_perplexity=63.861176

Batch 148420, train_perplexity=67.61114

Batch 148430, train_perplexity=67.59818

Batch 148440, train_perplexity=63.953266

Batch 148450, train_perplexity=69.32627

Batch 148460, train_perplexity=60.357807

Batch 148470, train_perplexity=69.84635

Batch 148480, train_perplexity=60.12648

Batch 148490, train_perplexity=57.76897

Batch 148500, train_perplexity=70.35765

Batch 148510, train_perplexity=71.240654

Batch 148520, train_perplexity=62.88735

Batch 148530, train_perplexity=71.121185

Batch 148540, train_perplexity=74.59338

Batch 148550, train_perplexity=59.190693

Batch 148560, train_perplexity=58.80539

Batch 148570, train_perplexity=67.12969

Batch 148580, train_perplexity=69.10912

Batch 148590, train_perplexity=72.95787

Batch 148600, train_perplexity=68.53225

Batch 148610, train_perplexity=66.143196

Batch 148620, train_perplexity=69.31632

Batch 148630, train_perplexity=68.453804

Batch 148640, train_perplexity=64.363335

Batch 148650, train_perplexity=63.61293

Batch 148660, train_perplexity=65.69895

Batch 148670, train_perplexity=68.90685

Batch 148680, train_perplexity=65.77195

Batch 148690, train_perplexity=66.46068

Batch 148700, train_perplexity=68.11598

Batch 148710, train_perplexity=59.141544

Batch 148720, train_perplexity=69.74917

Batch 148730, train_perplexity=68.251465

Batch 148740, train_perplexity=65.3516

Batch 148750, train_perplexity=64.70771

Batch 148760, train_perplexity=57.35993

Batch 148770, train_perplexity=66.836006

Batch 148780, train_perplexity=68.49713

Batch 148790, train_perplexity=67.961655

Batch 148800, train_perplexity=68.40323

Batch 148810, train_perplexity=66.66336

Batch 148820, train_perplexity=59.579712

Batch 148830, train_perplexity=64.68519

Batch 148840, train_perplexity=65.38707

Batch 148850, train_perplexity=64.42339

Batch 148860, train_perplexity=66.85564

Batch 148870, train_perplexity=69.533516

Batch 148880, train_perplexity=68.14636

Batch 148890, train_perplexity=67.90995

Batch 148900, train_perplexity=66.17597

Batch 148910, train_perplexity=61.065384

Batch 148920, train_perplexity=66.90411

Batch 148930, train_perplexity=68.7847

Batch 148940, train_perplexity=69.74232

Batch 148950, train_perplexity=63.366257

Batch 148960, train_perplexity=65.74837

Batch 148970, train_perplexity=67.390175

Batch 148980, train_perplexity=66.05387

Batch 148990, train_perplexity=67.84625

Batch 149000, train_perplexity=67.2339

Batch 149010, train_perplexity=64.515495

Batch 149020, train_perplexity=64.22568

Batch 149030, train_perplexity=64.62443

Batch 149040, train_perplexity=65.07869

Batch 149050, train_perplexity=67.92439

Batch 149060, train_perplexity=64.735794

Batch 149070, train_perplexity=71.783936

Batch 149080, train_perplexity=66.92383

Batch 149090, train_perplexity=68.07234

Batch 149100, train_perplexity=67.14384

Batch 149110, train_perplexity=62.86777

Batch 149120, train_perplexity=63.148117

Batch 149130, train_perplexity=60.46468

Batch 149140, train_perplexity=55.019337

Batch 149150, train_perplexity=61.41979

Batch 149160, train_perplexity=70.088165

Batch 149170, train_perplexity=64.06611

Batch 149180, train_perplexity=61.29129

Batch 149190, train_perplexity=65.517

Batch 149200, train_perplexity=63.429195

Batch 149210, train_perplexity=64.05969

Batch 149220, train_perplexity=60.245808

Batch 149230, train_perplexity=60.865494

Batch 149240, train_perplexity=64.37748

Batch 149250, train_perplexity=67.397316

Batch 149260, train_perplexity=66.52888

Batch 149270, train_perplexity=63.350063

Batch 149280, train_perplexity=66.40268

Batch 149290, train_perplexity=62.472622

Batch 149300, train_perplexity=72.34294

Batch 149310, train_perplexity=63.187424

Batch 149320, train_perplexity=64.26795

Batch 149330, train_perplexity=66.60623

Batch 149340, train_perplexity=65.703804

Batch 149350, train_perplexity=61.03674

Batch 149360, train_perplexity=68.31075

Batch 149370, train_perplexity=68.42841

Batch 149380, train_perplexity=59.437237

Batch 149390, train_perplexity=62.61721

Batch 149400, train_perplexity=69.287735

Batch 149410, train_perplexity=71.85944

Batch 149420, train_perplexity=69.99952

Batch 149430, train_perplexity=68.62427

Batch 149440, train_perplexity=66.37479

Batch 149450, train_perplexity=68.257614

Batch 149460, train_perplexity=65.93084

Batch 149470, train_perplexity=64.96713

Batch 149480, train_perplexity=59.689247

Batch 149490, train_perplexity=64.11638

Batch 149500, train_perplexity=67.039

Batch 149510, train_perplexity=66.555504

Batch 149520, train_perplexity=58.44957

Batch 149530, train_perplexity=58.365685

Batch 149540, train_perplexity=63.819378

Batch 149550, train_perplexity=66.89843

Batch 149560, train_perplexity=69.31325

Batch 149570, train_perplexity=69.39596

Batch 149580, train_perplexity=60.14251

Batch 149590, train_perplexity=58.262135

Batch 149600, train_perplexity=62.611507

Batch 149610, train_perplexity=66.5595

Batch 149620, train_perplexity=64.432915

Batch 149630, train_perplexity=63.299095

Batch 149640, train_perplexity=67.23037

Batch 149650, train_perplexity=60.32003

Batch 149660, train_perplexity=64.58715

Batch 149670, train_perplexity=68.74358

Batch 149680, train_perplexity=60.00527

Batch 149690, train_perplexity=63.71572

Batch 149700, train_perplexity=73.85421

Batch 149710, train_perplexity=67.156166

Batch 149720, train_perplexity=67.24983

Batch 149730, train_perplexity=72.254166

Batch 149740, train_perplexity=61.80406

Batch 149750, train_perplexity=70.83688

Batch 149760, train_perplexity=69.89536

Batch 149770, train_perplexity=65.22564

Batch 149780, train_perplexity=62.864384

Batch 149790, train_perplexity=70.16795

Batch 149800, train_perplexity=66.999695

Batch 149810, train_perplexity=57.122574

Batch 149820, train_perplexity=64.27415

Batch 149830, train_perplexity=62.387157

Batch 149840, train_perplexity=67.20044

Batch 149850, train_perplexity=59.913406

Batch 149860, train_perplexity=66.7333

Batch 149870, train_perplexity=73.84185

Batch 149880, train_perplexity=69.144226

Batch 149890, train_perplexity=64.96519

Batch 149900, train_perplexity=71.049866

Batch 149910, train_perplexity=63.80237

Batch 149920, train_perplexity=66.04323

Batch 149930, train_perplexity=56.909264

Batch 149940, train_perplexity=59.026993

Batch 149950, train_perplexity=70.67612

Batch 149960, train_perplexity=60.12473

Batch 149970, train_perplexity=65.26319

Batch 149980, train_perplexity=66.13654

Batch 149990, train_perplexity=61.973366

Batch 150000, train_perplexity=69.84279

Batch 150010, train_perplexity=66.72907

Batch 150020, train_perplexity=60.2644

Batch 150030, train_perplexity=67.97377

Batch 150040, train_perplexity=64.05062

Batch 150050, train_perplexity=73.80883

Batch 150060, train_perplexity=66.70769

Batch 150070, train_perplexity=63.40791

Batch 150080, train_perplexity=70.61241

Batch 150090, train_perplexity=69.10619

Batch 150100, train_perplexity=66.72286

Batch 150110, train_perplexity=60.144

Batch 150120, train_perplexity=64.27868

Batch 150130, train_perplexity=71.495415

Batch 150140, train_perplexity=62.984882

Batch 150150, train_perplexity=70.899025

Batch 150160, train_perplexity=71.79126

Batch 150170, train_perplexity=66.62332

Batch 150180, train_perplexity=63.660847

Batch 150190, train_perplexity=58.320698

Batch 150200, train_perplexity=65.326584
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 150210, train_perplexity=67.15412

Batch 150220, train_perplexity=56.06668

Batch 150230, train_perplexity=64.8872

Batch 150240, train_perplexity=63.95903

Batch 150250, train_perplexity=63.112473

Batch 150260, train_perplexity=58.550022

Batch 150270, train_perplexity=68.387085

Batch 150280, train_perplexity=63.588852

Batch 150290, train_perplexity=66.79255

Batch 150300, train_perplexity=60.560936

Batch 150310, train_perplexity=61.1377

Batch 150320, train_perplexity=70.38986

Batch 150330, train_perplexity=69.110214

Batch 150340, train_perplexity=60.32722

Batch 150350, train_perplexity=60.588203

Batch 150360, train_perplexity=65.47727

Batch 150370, train_perplexity=57.53489

Batch 150380, train_perplexity=62.993774

Batch 150390, train_perplexity=67.95967

Batch 150400, train_perplexity=61.758812

Batch 150410, train_perplexity=66.17361

Batch 150420, train_perplexity=68.71779

Batch 150430, train_perplexity=70.1711

Batch 150440, train_perplexity=68.07108

Batch 150450, train_perplexity=63.64133

Batch 150460, train_perplexity=61.789562

Batch 150470, train_perplexity=66.37352

Batch 150480, train_perplexity=60.31632

Batch 150490, train_perplexity=59.266975

Batch 150500, train_perplexity=59.485718

Batch 150510, train_perplexity=68.55925

Batch 150520, train_perplexity=63.2087

Batch 150530, train_perplexity=57.90303

Batch 150540, train_perplexity=67.5275

Batch 150550, train_perplexity=59.053482

Batch 150560, train_perplexity=63.216778

Batch 150570, train_perplexity=62.257618

Batch 150580, train_perplexity=64.56307

Batch 150590, train_perplexity=66.25381

Batch 150600, train_perplexity=63.134327

Batch 150610, train_perplexity=65.64781

Batch 150620, train_perplexity=69.44498

Batch 150630, train_perplexity=72.66148

Batch 150640, train_perplexity=69.95191

Batch 150650, train_perplexity=62.790115

Batch 150660, train_perplexity=65.33786

Batch 150670, train_perplexity=59.953957

Batch 150680, train_perplexity=66.41898

Batch 150690, train_perplexity=66.82093

Batch 150700, train_perplexity=69.52397

Batch 150710, train_perplexity=68.57344

Batch 150720, train_perplexity=58.932777

Batch 150730, train_perplexity=65.20393

Batch 150740, train_perplexity=62.039684

Batch 150750, train_perplexity=61.848843

Batch 150760, train_perplexity=61.651417

Batch 150770, train_perplexity=62.17793

Batch 150780, train_perplexity=64.95728

Batch 150790, train_perplexity=68.45067

Batch 150800, train_perplexity=67.19951

Batch 150810, train_perplexity=67.86508

Batch 150820, train_perplexity=61.702885

Batch 150830, train_perplexity=68.10595

Batch 150840, train_perplexity=69.36774

Batch 150850, train_perplexity=69.463326

Batch 150860, train_perplexity=64.53552

Batch 150870, train_perplexity=64.213

Batch 150880, train_perplexity=57.836277

Batch 150890, train_perplexity=68.582924

Batch 150900, train_perplexity=62.653107

Batch 150910, train_perplexity=63.293118

Batch 150920, train_perplexity=63.856606

Batch 150930, train_perplexity=68.107414

Batch 150940, train_perplexity=72.23643

Batch 150950, train_perplexity=60.729588

Batch 150960, train_perplexity=66.77064

Batch 150970, train_perplexity=56.71703

Batch 150980, train_perplexity=66.48984

Batch 150990, train_perplexity=59.07317

Batch 151000, train_perplexity=59.308475

Batch 151010, train_perplexity=61.67394

Batch 151020, train_perplexity=59.684235

Batch 151030, train_perplexity=60.809044

Batch 151040, train_perplexity=63.22151

Batch 151050, train_perplexity=57.735073

Batch 151060, train_perplexity=68.13655

Batch 151070, train_perplexity=66.85296

Batch 151080, train_perplexity=68.06799

Batch 151090, train_perplexity=64.18683

Batch 151100, train_perplexity=58.49895

Batch 151110, train_perplexity=67.21659

Batch 151120, train_perplexity=70.643906

Batch 151130, train_perplexity=71.85921

Batch 151140, train_perplexity=66.20814

Batch 151150, train_perplexity=60.94196

Batch 151160, train_perplexity=61.3114

Batch 151170, train_perplexity=67.863205

Batch 151180, train_perplexity=65.74194

Batch 151190, train_perplexity=64.14384

Batch 151200, train_perplexity=58.226807

Batch 151210, train_perplexity=65.68185

Batch 151220, train_perplexity=61.44885

Batch 151230, train_perplexity=60.595196

Batch 151240, train_perplexity=59.933178

Batch 151250, train_perplexity=69.28331

Batch 151260, train_perplexity=67.24679

Batch 151270, train_perplexity=56.993015

Batch 151280, train_perplexity=61.7478

Batch 151290, train_perplexity=65.03259

Batch 151300, train_perplexity=61.623173

Batch 151310, train_perplexity=59.352554

Batch 151320, train_perplexity=65.74329

Batch 151330, train_perplexity=58.38403

Batch 151340, train_perplexity=64.63161

Batch 151350, train_perplexity=67.84839

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00057-of-00100
Loaded 305084 sentences.
Finished loading
Batch 151360, train_perplexity=58.020798

Batch 151370, train_perplexity=65.32848

Batch 151380, train_perplexity=79.906975

Batch 151390, train_perplexity=61.27791

Batch 151400, train_perplexity=69.85321

Batch 151410, train_perplexity=69.52095

Batch 151420, train_perplexity=62.394535

Batch 151430, train_perplexity=63.952106

Batch 151440, train_perplexity=62.30668

Batch 151450, train_perplexity=60.377785

Batch 151460, train_perplexity=65.258865

Batch 151470, train_perplexity=62.318237

Batch 151480, train_perplexity=65.94442

Batch 151490, train_perplexity=63.264874

Batch 151500, train_perplexity=71.48846

Batch 151510, train_perplexity=61.86011

Batch 151520, train_perplexity=57.43065

Batch 151530, train_perplexity=68.125244

Batch 151540, train_perplexity=62.872837

Batch 151550, train_perplexity=62.910233

Batch 151560, train_perplexity=62.14266

Batch 151570, train_perplexity=69.65797

Batch 151580, train_perplexity=70.09582

Batch 151590, train_perplexity=62.225803

Batch 151600, train_perplexity=70.06702

Batch 151610, train_perplexity=63.358402

Batch 151620, train_perplexity=57.27146

Batch 151630, train_perplexity=59.06334

Batch 151640, train_perplexity=61.081314

Batch 151650, train_perplexity=60.174465

Batch 151660, train_perplexity=66.48597

Batch 151670, train_perplexity=58.27636

Batch 151680, train_perplexity=60.54061

Batch 151690, train_perplexity=69.91042

Batch 151700, train_perplexity=71.933464

Batch 151710, train_perplexity=62.98008

Batch 151720, train_perplexity=66.22519

Batch 151730, train_perplexity=60.36051

Batch 151740, train_perplexity=66.0013

Batch 151750, train_perplexity=63.75052

Batch 151760, train_perplexity=66.20056

Batch 151770, train_perplexity=65.57698

Batch 151780, train_perplexity=65.080055

Batch 151790, train_perplexity=67.7747

Batch 151800, train_perplexity=63.894436

Batch 151810, train_perplexity=65.577415

Batch 151820, train_perplexity=71.9594

Batch 151830, train_perplexity=64.70543

Batch 151840, train_perplexity=68.54889

Batch 151850, train_perplexity=62.599415

Batch 151860, train_perplexity=70.480484

Batch 151870, train_perplexity=65.19299

Batch 151880, train_perplexity=73.68319

Batch 151890, train_perplexity=57.763958

Batch 151900, train_perplexity=61.571476

Batch 151910, train_perplexity=70.103806

Batch 151920, train_perplexity=62.643192

Batch 151930, train_perplexity=70.06477

Batch 151940, train_perplexity=71.14669

Batch 151950, train_perplexity=63.12641

Batch 151960, train_perplexity=64.48738

Batch 151970, train_perplexity=65.38024

Batch 151980, train_perplexity=68.32235

Batch 151990, train_perplexity=65.23939

Batch 152000, train_perplexity=68.516304

Batch 152010, train_perplexity=65.94002

Batch 152020, train_perplexity=58.463284

Batch 152030, train_perplexity=64.60933

Batch 152040, train_perplexity=70.62386

Batch 152050, train_perplexity=59.79439

Batch 152060, train_perplexity=65.10222

Batch 152070, train_perplexity=65.60006

Batch 152080, train_perplexity=67.10124

Batch 152090, train_perplexity=62.851135
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 152100, train_perplexity=70.26478

Batch 152110, train_perplexity=69.19641

Batch 152120, train_perplexity=65.286064

Batch 152130, train_perplexity=65.195786

Batch 152140, train_perplexity=62.857967

Batch 152150, train_perplexity=66.12837

Batch 152160, train_perplexity=68.06498

Batch 152170, train_perplexity=63.09809

Batch 152180, train_perplexity=66.361176

Batch 152190, train_perplexity=63.109524

Batch 152200, train_perplexity=67.65258

Batch 152210, train_perplexity=61.00735

Batch 152220, train_perplexity=70.21923

Batch 152230, train_perplexity=65.87484

Batch 152240, train_perplexity=66.510574

Batch 152250, train_perplexity=66.40847

Batch 152260, train_perplexity=62.154217

Batch 152270, train_perplexity=59.90358

Batch 152280, train_perplexity=61.884186

Batch 152290, train_perplexity=73.06869

Batch 152300, train_perplexity=64.08221

Batch 152310, train_perplexity=64.26023

Batch 152320, train_perplexity=66.02692

Batch 152330, train_perplexity=62.199043

Batch 152340, train_perplexity=60.81713

Batch 152350, train_perplexity=60.624416

Batch 152360, train_perplexity=67.80945

Batch 152370, train_perplexity=64.24234

Batch 152380, train_perplexity=64.161156

Batch 152390, train_perplexity=62.55728

Batch 152400, train_perplexity=70.76888

Batch 152410, train_perplexity=67.34701

Batch 152420, train_perplexity=64.0915

Batch 152430, train_perplexity=68.748924

Batch 152440, train_perplexity=66.89014

Batch 152450, train_perplexity=72.962814

Batch 152460, train_perplexity=71.1604

Batch 152470, train_perplexity=65.975555

Batch 152480, train_perplexity=71.180145

Batch 152490, train_perplexity=64.85292

Batch 152500, train_perplexity=67.78145

Batch 152510, train_perplexity=67.101944

Batch 152520, train_perplexity=66.06229

Batch 152530, train_perplexity=71.13251

Batch 152540, train_perplexity=68.600746

Batch 152550, train_perplexity=62.216427

Batch 152560, train_perplexity=61.330147

Batch 152570, train_perplexity=64.76485

Batch 152580, train_perplexity=68.59545

Batch 152590, train_perplexity=62.523228

Batch 152600, train_perplexity=65.38102

Batch 152610, train_perplexity=66.229294

Batch 152620, train_perplexity=63.940456

Batch 152630, train_perplexity=66.43919

Batch 152640, train_perplexity=67.58381

Batch 152650, train_perplexity=64.99676

Batch 152660, train_perplexity=69.56671

Batch 152670, train_perplexity=62.910263

Batch 152680, train_perplexity=62.491154

Batch 152690, train_perplexity=65.27219

Batch 152700, train_perplexity=68.39096

Batch 152710, train_perplexity=65.67395

Batch 152720, train_perplexity=63.30682

Batch 152730, train_perplexity=61.7163

Batch 152740, train_perplexity=67.534515

Batch 152750, train_perplexity=61.637775

Batch 152760, train_perplexity=62.89077

Batch 152770, train_perplexity=61.83867

Batch 152780, train_perplexity=62.986954

Batch 152790, train_perplexity=70.33511

Batch 152800, train_perplexity=65.31434

Batch 152810, train_perplexity=61.784115

Batch 152820, train_perplexity=66.16875

Batch 152830, train_perplexity=64.027626

Batch 152840, train_perplexity=61.059414

Batch 152850, train_perplexity=65.84905

Batch 152860, train_perplexity=70.93463

Batch 152870, train_perplexity=65.49963

Batch 152880, train_perplexity=65.546875

Batch 152890, train_perplexity=61.193756

Batch 152900, train_perplexity=61.433907

Batch 152910, train_perplexity=62.13709

Batch 152920, train_perplexity=64.73524

Batch 152930, train_perplexity=68.90235

Batch 152940, train_perplexity=67.4012

Batch 152950, train_perplexity=68.23832

Batch 152960, train_perplexity=64.36413

Batch 152970, train_perplexity=62.371304

Batch 152980, train_perplexity=65.81048

Batch 152990, train_perplexity=63.00567

Batch 153000, train_perplexity=73.813934

Batch 153010, train_perplexity=61.665413

Batch 153020, train_perplexity=64.2819

Batch 153030, train_perplexity=60.318676

Batch 153040, train_perplexity=63.885693

Batch 153050, train_perplexity=66.941

Batch 153060, train_perplexity=63.182453

Batch 153070, train_perplexity=67.988716

Batch 153080, train_perplexity=64.935486

Batch 153090, train_perplexity=66.22001

Batch 153100, train_perplexity=61.89147

Batch 153110, train_perplexity=63.412987

Batch 153120, train_perplexity=70.119156

Batch 153130, train_perplexity=66.3496

Batch 153140, train_perplexity=71.78845

Batch 153150, train_perplexity=64.692444

Batch 153160, train_perplexity=71.83725

Batch 153170, train_perplexity=66.00602

Batch 153180, train_perplexity=67.10533

Batch 153190, train_perplexity=63.147514

Batch 153200, train_perplexity=64.36481

Batch 153210, train_perplexity=67.683914

Batch 153220, train_perplexity=61.11304

Batch 153230, train_perplexity=70.938896

Batch 153240, train_perplexity=64.19139

Batch 153250, train_perplexity=63.26065

Batch 153260, train_perplexity=69.57763

Batch 153270, train_perplexity=62.33467

Batch 153280, train_perplexity=64.25294

Batch 153290, train_perplexity=59.507053

Batch 153300, train_perplexity=69.46462

Batch 153310, train_perplexity=68.647934

Batch 153320, train_perplexity=62.642532

Batch 153330, train_perplexity=66.677986

Batch 153340, train_perplexity=63.232212

Batch 153350, train_perplexity=61.015785

Batch 153360, train_perplexity=68.323685

Batch 153370, train_perplexity=66.17197

Batch 153380, train_perplexity=64.365234

Batch 153390, train_perplexity=63.82139

Batch 153400, train_perplexity=63.814724

Batch 153410, train_perplexity=63.288982

Batch 153420, train_perplexity=69.537895

Batch 153430, train_perplexity=70.17518

Batch 153440, train_perplexity=74.329926

Batch 153450, train_perplexity=66.84295

Batch 153460, train_perplexity=61.39595

Batch 153470, train_perplexity=63.21283

Batch 153480, train_perplexity=66.378334

Batch 153490, train_perplexity=64.011536

Batch 153500, train_perplexity=65.20272

Batch 153510, train_perplexity=63.609745

Batch 153520, train_perplexity=67.80841

Batch 153530, train_perplexity=64.77903

Batch 153540, train_perplexity=65.44621

Batch 153550, train_perplexity=61.964825

Batch 153560, train_perplexity=72.536514

Batch 153570, train_perplexity=60.44891

Batch 153580, train_perplexity=67.523315

Batch 153590, train_perplexity=67.759155

Batch 153600, train_perplexity=65.924736

Batch 153610, train_perplexity=61.146095

Batch 153620, train_perplexity=58.19147

Batch 153630, train_perplexity=65.39237

Batch 153640, train_perplexity=75.64099

Batch 153650, train_perplexity=64.80235

Batch 153660, train_perplexity=55.954773

Batch 153670, train_perplexity=59.973774

Batch 153680, train_perplexity=69.78091

Batch 153690, train_perplexity=65.47099

Batch 153700, train_perplexity=71.97649

Batch 153710, train_perplexity=65.349045

Batch 153720, train_perplexity=63.305614

Batch 153730, train_perplexity=63.85399

Batch 153740, train_perplexity=65.01629

Batch 153750, train_perplexity=67.961716

Batch 153760, train_perplexity=64.85447

Batch 153770, train_perplexity=63.61897

Batch 153780, train_perplexity=66.93468

Batch 153790, train_perplexity=63.79884

Batch 153800, train_perplexity=66.41785

Batch 153810, train_perplexity=61.195446

Batch 153820, train_perplexity=67.92679

Batch 153830, train_perplexity=67.6341

Batch 153840, train_perplexity=62.85809

Batch 153850, train_perplexity=60.030994

Batch 153860, train_perplexity=67.54804

Batch 153870, train_perplexity=58.708702

Batch 153880, train_perplexity=62.301304

Batch 153890, train_perplexity=60.463036

Batch 153900, train_perplexity=64.029915

Batch 153910, train_perplexity=66.991776

Batch 153920, train_perplexity=65.72962

Batch 153930, train_perplexity=66.7255

Batch 153940, train_perplexity=66.199776

Batch 153950, train_perplexity=66.679924

Batch 153960, train_perplexity=68.77936

Batch 153970, train_perplexity=67.3209

Batch 153980, train_perplexity=61.87241

Batch 153990, train_perplexity=73.14737

Batch 154000, train_perplexity=67.1579

Batch 154010, train_perplexity=63.08034

Batch 154020, train_perplexity=67.791855

Batch 154030, train_perplexity=69.065834

Batch 154040, train_perplexity=66.94426

Batch 154050, train_perplexity=62.34793

Batch 154060, train_perplexity=58.234165

Batch 154070, train_perplexity=66.662476

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 154080, train_perplexity=68.36928

Batch 154090, train_perplexity=65.52672

Batch 154100, train_perplexity=65.014175

Batch 154110, train_perplexity=66.12841

Batch 154120, train_perplexity=63.990814

Batch 154130, train_perplexity=64.1961

Batch 154140, train_perplexity=70.05976

Batch 154150, train_perplexity=62.793888

Batch 154160, train_perplexity=69.58652

Batch 154170, train_perplexity=63.05096

Batch 154180, train_perplexity=63.43924

Batch 154190, train_perplexity=69.205315

Batch 154200, train_perplexity=69.77292

Batch 154210, train_perplexity=69.77106

Batch 154220, train_perplexity=68.52934

Batch 154230, train_perplexity=71.469505

Batch 154240, train_perplexity=63.86818

Batch 154250, train_perplexity=58.838066

Batch 154260, train_perplexity=68.35262

Batch 154270, train_perplexity=63.32065

Batch 154280, train_perplexity=65.94052

Batch 154290, train_perplexity=58.792774

Batch 154300, train_perplexity=63.556232

Batch 154310, train_perplexity=60.659634

Batch 154320, train_perplexity=67.09868

Batch 154330, train_perplexity=71.09741

Batch 154340, train_perplexity=67.91066

Batch 154350, train_perplexity=66.79248

Batch 154360, train_perplexity=65.88659

Batch 154370, train_perplexity=66.57423

Batch 154380, train_perplexity=64.94006

Batch 154390, train_perplexity=68.57121

Batch 154400, train_perplexity=62.77841

Batch 154410, train_perplexity=56.158268

Batch 154420, train_perplexity=57.20863

Batch 154430, train_perplexity=70.67608

Batch 154440, train_perplexity=66.86973

Batch 154450, train_perplexity=65.89941

Batch 154460, train_perplexity=69.386925

Batch 154470, train_perplexity=65.288124

Batch 154480, train_perplexity=72.93391

Batch 154490, train_perplexity=65.097565

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00089-of-00100
Loaded 306744 sentences.
Finished loading
Batch 154500, train_perplexity=64.48302

Batch 154510, train_perplexity=61.328537

Batch 154520, train_perplexity=61.069633

Batch 154530, train_perplexity=63.062298

Batch 154540, train_perplexity=62.306206

Batch 154550, train_perplexity=68.148506

Batch 154560, train_perplexity=72.08248

Batch 154570, train_perplexity=65.096756

Batch 154580, train_perplexity=68.687386

Batch 154590, train_perplexity=63.18161

Batch 154600, train_perplexity=58.847046

Batch 154610, train_perplexity=62.30888

Batch 154620, train_perplexity=63.835358

Batch 154630, train_perplexity=61.41592

Batch 154640, train_perplexity=62.726795

Batch 154650, train_perplexity=62.038765

Batch 154660, train_perplexity=62.18926

Batch 154670, train_perplexity=61.30918

Batch 154680, train_perplexity=62.223843

Batch 154690, train_perplexity=63.121593

Batch 154700, train_perplexity=60.26871

Batch 154710, train_perplexity=67.0942

Batch 154720, train_perplexity=63.392097

Batch 154730, train_perplexity=71.442184

Batch 154740, train_perplexity=61.035427

Batch 154750, train_perplexity=66.90153

Batch 154760, train_perplexity=65.27467

Batch 154770, train_perplexity=62.464878

Batch 154780, train_perplexity=68.03603

Batch 154790, train_perplexity=67.64016

Batch 154800, train_perplexity=64.20357

Batch 154810, train_perplexity=71.3781

Batch 154820, train_perplexity=68.88918

Batch 154830, train_perplexity=66.29407

Batch 154840, train_perplexity=63.181187

Batch 154850, train_perplexity=65.64503

Batch 154860, train_perplexity=72.264534

Batch 154870, train_perplexity=62.316425

Batch 154880, train_perplexity=69.26597

Batch 154890, train_perplexity=60.186287

Batch 154900, train_perplexity=67.81171

Batch 154910, train_perplexity=65.91336

Batch 154920, train_perplexity=71.56192

Batch 154930, train_perplexity=66.17613

Batch 154940, train_perplexity=65.19784

Batch 154950, train_perplexity=68.100464

Batch 154960, train_perplexity=69.691185

Batch 154970, train_perplexity=69.639496

Batch 154980, train_perplexity=63.251392

Batch 154990, train_perplexity=58.909344

Batch 155000, train_perplexity=62.927303

Batch 155010, train_perplexity=67.51047

Batch 155020, train_perplexity=61.334503

Batch 155030, train_perplexity=62.80934

Batch 155040, train_perplexity=67.16766

Batch 155050, train_perplexity=62.38427

Batch 155060, train_perplexity=58.102192

Batch 155070, train_perplexity=69.90196

Batch 155080, train_perplexity=71.84156

Batch 155090, train_perplexity=68.514084

Batch 155100, train_perplexity=64.927925

Batch 155110, train_perplexity=72.23229

Batch 155120, train_perplexity=70.12243

Batch 155130, train_perplexity=57.39985

Batch 155140, train_perplexity=69.43624

Batch 155150, train_perplexity=57.626236

Batch 155160, train_perplexity=54.396587

Batch 155170, train_perplexity=67.00075

Batch 155180, train_perplexity=67.39744

Batch 155190, train_perplexity=68.0168

Batch 155200, train_perplexity=65.33876

Batch 155210, train_perplexity=60.91137

Batch 155220, train_perplexity=66.70393

Batch 155230, train_perplexity=67.51791

Batch 155240, train_perplexity=72.851395

Batch 155250, train_perplexity=72.39194

Batch 155260, train_perplexity=62.951134

Batch 155270, train_perplexity=65.18777

Batch 155280, train_perplexity=66.53358

Batch 155290, train_perplexity=65.66434

Batch 155300, train_perplexity=60.739258

Batch 155310, train_perplexity=63.931374

Batch 155320, train_perplexity=70.41605

Batch 155330, train_perplexity=69.90492

Batch 155340, train_perplexity=66.54109

Batch 155350, train_perplexity=63.83819

Batch 155360, train_perplexity=65.19622

Batch 155370, train_perplexity=64.42585

Batch 155380, train_perplexity=62.425873

Batch 155390, train_perplexity=57.743744

Batch 155400, train_perplexity=64.14476

Batch 155410, train_perplexity=61.779312

Batch 155420, train_perplexity=63.68395

Batch 155430, train_perplexity=64.08398

Batch 155440, train_perplexity=75.81779

Batch 155450, train_perplexity=59.615658

Batch 155460, train_perplexity=65.82143

Batch 155470, train_perplexity=71.67756

Batch 155480, train_perplexity=71.624466

Batch 155490, train_perplexity=65.9214

Batch 155500, train_perplexity=71.943344

Batch 155510, train_perplexity=65.41851

Batch 155520, train_perplexity=63.887672

Batch 155530, train_perplexity=65.77581

Batch 155540, train_perplexity=59.76882

Batch 155550, train_perplexity=62.914104

Batch 155560, train_perplexity=66.274124

Batch 155570, train_perplexity=60.248825

Batch 155580, train_perplexity=63.614025

Batch 155590, train_perplexity=60.685413

Batch 155600, train_perplexity=65.51947

Batch 155610, train_perplexity=58.530228

Batch 155620, train_perplexity=57.905018

Batch 155630, train_perplexity=67.391304

Batch 155640, train_perplexity=63.332787

Batch 155650, train_perplexity=66.35947

Batch 155660, train_perplexity=68.20585

Batch 155670, train_perplexity=61.378914

Batch 155680, train_perplexity=62.467617

Batch 155690, train_perplexity=62.01821

Batch 155700, train_perplexity=62.47051

Batch 155710, train_perplexity=61.57165

Batch 155720, train_perplexity=69.2199

Batch 155730, train_perplexity=62.15943

Batch 155740, train_perplexity=63.958755

Batch 155750, train_perplexity=63.371456

Batch 155760, train_perplexity=65.825386

Batch 155770, train_perplexity=69.17526

Batch 155780, train_perplexity=68.866646

Batch 155790, train_perplexity=60.12086

Batch 155800, train_perplexity=59.338516

Batch 155810, train_perplexity=64.64285

Batch 155820, train_perplexity=62.50194

Batch 155830, train_perplexity=64.15375

Batch 155840, train_perplexity=58.03563

Batch 155850, train_perplexity=57.876087

Batch 155860, train_perplexity=63.41214

Batch 155870, train_perplexity=62.997257

Batch 155880, train_perplexity=67.061

Batch 155890, train_perplexity=60.315285

Batch 155900, train_perplexity=69.546814

Batch 155910, train_perplexity=64.851814

Batch 155920, train_perplexity=60.195415

Batch 155930, train_perplexity=59.042305

Batch 155940, train_perplexity=67.2693

Batch 155950, train_perplexity=61.89354

WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Batch 155960, train_perplexity=74.431366

Batch 155970, train_perplexity=66.04947

Batch 155980, train_perplexity=69.85501

Batch 155990, train_perplexity=66.904366

Batch 156000, train_perplexity=61.866158

Batch 156010, train_perplexity=62.926376

Batch 156020, train_perplexity=63.607956

Batch 156030, train_perplexity=73.981026

Batch 156040, train_perplexity=66.70547

Batch 156050, train_perplexity=64.21334

Batch 156060, train_perplexity=63.94991

Batch 156070, train_perplexity=61.156124

Batch 156080, train_perplexity=65.37504

Batch 156090, train_perplexity=66.19182

Batch 156100, train_perplexity=70.62205

Batch 156110, train_perplexity=65.650314

Batch 156120, train_perplexity=65.48779

Batch 156130, train_perplexity=61.74574

Batch 156140, train_perplexity=66.60395

Batch 156150, train_perplexity=65.05021

Batch 156160, train_perplexity=68.44852

Batch 156170, train_perplexity=68.006454

Batch 156180, train_perplexity=63.032715

Batch 156190, train_perplexity=61.401104

Batch 156200, train_perplexity=68.89102

Batch 156210, train_perplexity=64.58697

Batch 156220, train_perplexity=63.037945

Batch 156230, train_perplexity=66.71189

Batch 156240, train_perplexity=64.817986

Batch 156250, train_perplexity=60.341576

Batch 156260, train_perplexity=62.701405

Batch 156270, train_perplexity=65.86315

Batch 156280, train_perplexity=62.493896

Batch 156290, train_perplexity=64.81663

Batch 156300, train_perplexity=57.76608

Batch 156310, train_perplexity=66.91783

Batch 156320, train_perplexity=63.0943

Batch 156330, train_perplexity=63.014023

Batch 156340, train_perplexity=57.07991

Batch 156350, train_perplexity=62.658367

Batch 156360, train_perplexity=59.399155

Batch 156370, train_perplexity=62.42254

Batch 156380, train_perplexity=67.87045

Batch 156390, train_perplexity=68.62198

Batch 156400, train_perplexity=65.32693

Batch 156410, train_perplexity=64.39283

Batch 156420, train_perplexity=67.857185

Batch 156430, train_perplexity=68.45531

Batch 156440, train_perplexity=67.65174

Batch 156450, train_perplexity=63.199207

Batch 156460, train_perplexity=64.33069

Batch 156470, train_perplexity=60.342842

Batch 156480, train_perplexity=72.41314

Batch 156490, train_perplexity=69.42591

Batch 156500, train_perplexity=64.04002

Batch 156510, train_perplexity=63.931465

Batch 156520, train_perplexity=63.78351

Batch 156530, train_perplexity=68.96688

Batch 156540, train_perplexity=70.90092

Batch 156550, train_perplexity=67.72149

Batch 156560, train_perplexity=69.97787

Batch 156570, train_perplexity=64.075424

Batch 156580, train_perplexity=69.275345

Batch 156590, train_perplexity=63.05529

Batch 156600, train_perplexity=68.662766

Batch 156610, train_perplexity=63.11494

Batch 156620, train_perplexity=59.51182

Batch 156630, train_perplexity=66.87384

Batch 156640, train_perplexity=62.27151

Batch 156650, train_perplexity=66.19173

Batch 156660, train_perplexity=65.08732

Batch 156670, train_perplexity=64.19129

Batch 156680, train_perplexity=65.95357

Batch 156690, train_perplexity=68.16719

Batch 156700, train_perplexity=68.03674

Batch 156710, train_perplexity=65.84393

Batch 156720, train_perplexity=62.0881

Batch 156730, train_perplexity=69.43227

Batch 156740, train_perplexity=64.64218

Batch 156750, train_perplexity=70.673416

Batch 156760, train_perplexity=60.67132

Batch 156770, train_perplexity=67.057

Batch 156780, train_perplexity=63.147305

Batch 156790, train_perplexity=56.85572

Batch 156800, train_perplexity=63.448105

Batch 156810, train_perplexity=67.48102

Batch 156820, train_perplexity=64.62591

Batch 156830, train_perplexity=63.888588

Batch 156840, train_perplexity=64.21117

Batch 156850, train_perplexity=69.9821

Batch 156860, train_perplexity=58.68922

Batch 156870, train_perplexity=62.82363

Batch 156880, train_perplexity=59.765144

Batch 156890, train_perplexity=60.920345

Batch 156900, train_perplexity=63.150345

Batch 156910, train_perplexity=69.49841

Batch 156920, train_perplexity=70.955475

Batch 156930, train_perplexity=64.26336

Batch 156940, train_perplexity=56.829483

Batch 156950, train_perplexity=61.732197

Batch 156960, train_perplexity=62.168087

Batch 156970, train_perplexity=68.709335

Batch 156980, train_perplexity=60.35982

Batch 156990, train_perplexity=64.06137

Batch 157000, train_perplexity=62.718987

Batch 157010, train_perplexity=59.770447

Batch 157020, train_perplexity=64.48

Batch 157030, train_perplexity=68.19311

Batch 157040, train_perplexity=73.16094

Batch 157050, train_perplexity=63.407547

Batch 157060, train_perplexity=65.86912

Batch 157070, train_perplexity=73.45667

Batch 157080, train_perplexity=69.45336

Batch 157090, train_perplexity=62.122097

Batch 157100, train_perplexity=58.160652

Batch 157110, train_perplexity=65.086266

Batch 157120, train_perplexity=60.892376

Batch 157130, train_perplexity=68.580696

Batch 157140, train_perplexity=69.675804

Batch 157150, train_perplexity=63.678425

Batch 157160, train_perplexity=58.396667

Batch 157170, train_perplexity=59.324936

Batch 157180, train_perplexity=69.42856

Batch 157190, train_perplexity=60.817974

Batch 157200, train_perplexity=63.387352

Batch 157210, train_perplexity=69.373024

Batch 157220, train_perplexity=65.71841

Batch 157230, train_perplexity=64.65028

Batch 157240, train_perplexity=63.42847

Batch 157250, train_perplexity=59.39134

Batch 157260, train_perplexity=60.614384

Batch 157270, train_perplexity=61.21903

Batch 157280, train_perplexity=62.663685

Batch 157290, train_perplexity=66.75008

Batch 157300, train_perplexity=61.778664

Batch 157310, train_perplexity=61.74209

Batch 157320, train_perplexity=61.913967

Batch 157330, train_perplexity=65.07819

Batch 157340, train_perplexity=64.049126

Batch 157350, train_perplexity=69.78104

Batch 157360, train_perplexity=59.083282

Batch 157370, train_perplexity=64.202286

Batch 157380, train_perplexity=65.94687

Batch 157390, train_perplexity=65.15073

Batch 157400, train_perplexity=63.37118

Batch 157410, train_perplexity=61.74091

Batch 157420, train_perplexity=62.45106

Batch 157430, train_perplexity=68.1563

Batch 157440, train_perplexity=62.876976

Batch 157450, train_perplexity=59.235332

Batch 157460, train_perplexity=64.59805

Batch 157470, train_perplexity=64.172966

Batch 157480, train_perplexity=60.47898

Batch 157490, train_perplexity=68.71058

Batch 157500, train_perplexity=62.905914

Batch 157510, train_perplexity=71.087074

Batch 157520, train_perplexity=63.778736

Batch 157530, train_perplexity=67.079834

Batch 157540, train_perplexity=62.609295

Batch 157550, train_perplexity=66.74358

Batch 157560, train_perplexity=63.05553

Batch 157570, train_perplexity=66.832054

Batch 157580, train_perplexity=60.81931

Batch 157590, train_perplexity=70.21448

Batch 157600, train_perplexity=66.02875

Batch 157610, train_perplexity=62.100475

Batch 157620, train_perplexity=64.25766

Batch 157630, train_perplexity=63.792118

Batch 157640, train_perplexity=61.200233

Batch 157650, train_perplexity=71.73945

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00017-of-00100
Loaded 306553 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00017-of-00100
Loaded 306553 sentences.
Finished loading
Batch 157660, train_perplexity=67.464676

Batch 157670, train_perplexity=58.74662

Batch 157680, train_perplexity=65.01566

Batch 157690, train_perplexity=63.66209

Batch 157700, train_perplexity=66.18383

Batch 157710, train_perplexity=65.754265

Batch 157720, train_perplexity=64.427444

Batch 157730, train_perplexity=67.28377

Batch 157740, train_perplexity=59.902977

Batch 157750, train_perplexity=68.34291

Batch 157760, train_perplexity=61.49862

Batch 157770, train_perplexity=64.770164

Batch 157780, train_perplexity=67.32954

Batch 157790, train_perplexity=62.913143

Batch 157800, train_perplexity=68.87552

Batch 157810, train_perplexity=63.116207

Batch 157820, train_perplexity=75.11643

Batch 157830, train_perplexity=65.12712

Batch 157840, train_perplexity=63.8546
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 157850, train_perplexity=64.34762

Batch 157860, train_perplexity=62.61694

Batch 157870, train_perplexity=62.94045

Batch 157880, train_perplexity=64.67184

Batch 157890, train_perplexity=64.786964

Batch 157900, train_perplexity=60.35461

Batch 157910, train_perplexity=66.535065

Batch 157920, train_perplexity=61.210506

Batch 157930, train_perplexity=67.031075

Batch 157940, train_perplexity=60.777504

Batch 157950, train_perplexity=63.789593

Batch 157960, train_perplexity=67.084984

Batch 157970, train_perplexity=60.837524

Batch 157980, train_perplexity=59.33314

Batch 157990, train_perplexity=60.732136

Batch 158000, train_perplexity=62.876316

Batch 158010, train_perplexity=63.410538

Batch 158020, train_perplexity=60.795387

Batch 158030, train_perplexity=69.054115

Batch 158040, train_perplexity=62.25958

Batch 158050, train_perplexity=74.52498

Batch 158060, train_perplexity=64.94973

Batch 158070, train_perplexity=67.73893

Batch 158080, train_perplexity=73.254425

Batch 158090, train_perplexity=61.821568

Batch 158100, train_perplexity=65.28252

Batch 158110, train_perplexity=62.831837

Batch 158120, train_perplexity=63.31953

Batch 158130, train_perplexity=61.97582

Batch 158140, train_perplexity=63.450977

Batch 158150, train_perplexity=61.307953

Batch 158160, train_perplexity=56.283596

Batch 158170, train_perplexity=68.598946

Batch 158180, train_perplexity=73.863335

Batch 158190, train_perplexity=65.451584

Batch 158200, train_perplexity=62.62091

Batch 158210, train_perplexity=60.43565

Batch 158220, train_perplexity=59.441513

Batch 158230, train_perplexity=69.81971

Batch 158240, train_perplexity=60.87667

Batch 158250, train_perplexity=73.30921

Batch 158260, train_perplexity=58.17444

Batch 158270, train_perplexity=67.96372

Batch 158280, train_perplexity=67.03229

Batch 158290, train_perplexity=61.041103

Batch 158300, train_perplexity=67.28496

Batch 158310, train_perplexity=63.40138

Batch 158320, train_perplexity=61.178146

Batch 158330, train_perplexity=65.49891

Batch 158340, train_perplexity=70.25882

Batch 158350, train_perplexity=64.33406

Batch 158360, train_perplexity=69.03107

Batch 158370, train_perplexity=58.582275

Batch 158380, train_perplexity=69.68793

Batch 158390, train_perplexity=65.14047

Batch 158400, train_perplexity=62.004963

Batch 158410, train_perplexity=68.074455

Batch 158420, train_perplexity=60.956345

Batch 158430, train_perplexity=63.63787

Batch 158440, train_perplexity=63.91854

Batch 158450, train_perplexity=66.27302

Batch 158460, train_perplexity=69.238655

Batch 158470, train_perplexity=66.46892

Batch 158480, train_perplexity=65.87207

Batch 158490, train_perplexity=65.17403

Batch 158500, train_perplexity=65.70675

Batch 158510, train_perplexity=57.73689

Batch 158520, train_perplexity=59.33334

Batch 158530, train_perplexity=73.4888

Batch 158540, train_perplexity=67.07955

Batch 158550, train_perplexity=63.426445

Batch 158560, train_perplexity=62.94471

Batch 158570, train_perplexity=63.550114

Batch 158580, train_perplexity=57.450314

Batch 158590, train_perplexity=65.74411

Batch 158600, train_perplexity=59.839516

Batch 158610, train_perplexity=62.192577

Batch 158620, train_perplexity=61.338333

Batch 158630, train_perplexity=71.398285

Batch 158640, train_perplexity=61.048847

Batch 158650, train_perplexity=66.67042

Batch 158660, train_perplexity=64.60366

Batch 158670, train_perplexity=63.10495

Batch 158680, train_perplexity=68.06842

Batch 158690, train_perplexity=69.80866

Batch 158700, train_perplexity=52.380554

Batch 158710, train_perplexity=63.494137

Batch 158720, train_perplexity=67.80104

Batch 158730, train_perplexity=67.12402

Batch 158740, train_perplexity=65.92911

Batch 158750, train_perplexity=61.983532

Batch 158760, train_perplexity=64.408585

Batch 158770, train_perplexity=60.58162

Batch 158780, train_perplexity=68.22973

Batch 158790, train_perplexity=65.581856

Batch 158800, train_perplexity=65.769066

Batch 158810, train_perplexity=68.234085

Batch 158820, train_perplexity=68.61547

Batch 158830, train_perplexity=68.35344

Batch 158840, train_perplexity=63.596767

Batch 158850, train_perplexity=61.114204

Batch 158860, train_perplexity=69.059875

Batch 158870, train_perplexity=65.25532

Batch 158880, train_perplexity=69.13526

Batch 158890, train_perplexity=68.28968

Batch 158900, train_perplexity=63.644424

Batch 158910, train_perplexity=62.109093

Batch 158920, train_perplexity=66.699005

Batch 158930, train_perplexity=66.13815

Batch 158940, train_perplexity=59.284725

Batch 158950, train_perplexity=66.63975

Batch 158960, train_perplexity=62.4616

Batch 158970, train_perplexity=69.52509

Batch 158980, train_perplexity=70.40484

Batch 158990, train_perplexity=70.52022

Batch 159000, train_perplexity=62.99113

Batch 159010, train_perplexity=71.52818

Batch 159020, train_perplexity=60.962452

Batch 159030, train_perplexity=62.679283

Batch 159040, train_perplexity=68.49011

Batch 159050, train_perplexity=67.7097

Batch 159060, train_perplexity=66.86105

Batch 159070, train_perplexity=61.73605

Batch 159080, train_perplexity=61.111584

Batch 159090, train_perplexity=66.02453

Batch 159100, train_perplexity=57.029823

Batch 159110, train_perplexity=62.526924

Batch 159120, train_perplexity=59.922718

Batch 159130, train_perplexity=65.55141

Batch 159140, train_perplexity=67.49067

Batch 159150, train_perplexity=65.22828

Batch 159160, train_perplexity=63.864037

Batch 159170, train_perplexity=68.558266

Batch 159180, train_perplexity=63.49326

Batch 159190, train_perplexity=72.40278

Batch 159200, train_perplexity=60.47973

Batch 159210, train_perplexity=61.033974

Batch 159220, train_perplexity=60.52641

Batch 159230, train_perplexity=63.58482

Batch 159240, train_perplexity=63.899982

Batch 159250, train_perplexity=62.821533

Batch 159260, train_perplexity=60.847965

Batch 159270, train_perplexity=65.87518

Batch 159280, train_perplexity=65.90456

Batch 159290, train_perplexity=64.09083

Batch 159300, train_perplexity=63.67499

Batch 159310, train_perplexity=67.93502

Batch 159320, train_perplexity=65.158714

Batch 159330, train_perplexity=59.268528

Batch 159340, train_perplexity=63.280716

Batch 159350, train_perplexity=59.429695

Batch 159360, train_perplexity=63.45612

Batch 159370, train_perplexity=63.76791

Batch 159380, train_perplexity=68.10322

Batch 159390, train_perplexity=64.047104

Batch 159400, train_perplexity=61.914497

Batch 159410, train_perplexity=67.64262

Batch 159420, train_perplexity=74.916084

Batch 159430, train_perplexity=68.32284

Batch 159440, train_perplexity=64.338875

Batch 159450, train_perplexity=74.08351

Batch 159460, train_perplexity=64.94124

Batch 159470, train_perplexity=69.652916

Batch 159480, train_perplexity=71.10294

Batch 159490, train_perplexity=61.28726

Batch 159500, train_perplexity=58.782207

Batch 159510, train_perplexity=68.01696

Batch 159520, train_perplexity=64.20516

Batch 159530, train_perplexity=66.70651

Batch 159540, train_perplexity=63.145103

Batch 159550, train_perplexity=64.195946

Batch 159560, train_perplexity=65.21494

Batch 159570, train_perplexity=66.447685

Batch 159580, train_perplexity=66.62784

Batch 159590, train_perplexity=61.166916

Batch 159600, train_perplexity=66.94888

Batch 159610, train_perplexity=62.2242

Batch 159620, train_perplexity=65.23777

Batch 159630, train_perplexity=56.5017

Batch 159640, train_perplexity=66.69557

Batch 159650, train_perplexity=64.94629

Batch 159660, train_perplexity=68.10965

Batch 159670, train_perplexity=57.679733

Batch 159680, train_perplexity=65.7908

Batch 159690, train_perplexity=68.32453

Batch 159700, train_perplexity=64.09327

Batch 159710, train_perplexity=77.62471

Batch 159720, train_perplexity=67.55832

Batch 159730, train_perplexity=69.00293

Batch 159740, train_perplexity=60.087585

Batch 159750, train_perplexity=63.955215

Batch 159760, train_perplexity=64.15595

Batch 159770, train_perplexity=66.39397

Batch 159780, train_perplexity=60.882214

Batch 159790, train_perplexity=64.58352

Batch 159800, train_perplexity=66.63292

Batch 159810, train_perplexity=64.779366

Batch 159820, train_perplexity=59.786125
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 159830, train_perplexity=62.380554

Batch 159840, train_perplexity=58.920162

Batch 159850, train_perplexity=67.98658

Batch 159860, train_perplexity=66.27975

Batch 159870, train_perplexity=63.88664

Batch 159880, train_perplexity=61.46461

Batch 159890, train_perplexity=58.57979

Batch 159900, train_perplexity=63.99637

Batch 159910, train_perplexity=70.52109

Batch 159920, train_perplexity=63.06302

Batch 159930, train_perplexity=62.927544

Batch 159940, train_perplexity=57.962227

Batch 159950, train_perplexity=57.109447

Batch 159960, train_perplexity=61.97635

Batch 159970, train_perplexity=61.401222

Batch 159980, train_perplexity=60.082314

Batch 159990, train_perplexity=64.50922

Batch 160000, train_perplexity=64.75895

Batch 160010, train_perplexity=69.41595

Batch 160020, train_perplexity=59.323128

Batch 160030, train_perplexity=67.532715

Batch 160040, train_perplexity=74.799965

Batch 160050, train_perplexity=65.45926

Batch 160060, train_perplexity=59.19634

Batch 160070, train_perplexity=67.36059

Batch 160080, train_perplexity=65.72806

Batch 160090, train_perplexity=59.032986

Batch 160100, train_perplexity=64.428154

Batch 160110, train_perplexity=71.55732

Batch 160120, train_perplexity=65.22623

Batch 160130, train_perplexity=69.51505

Batch 160140, train_perplexity=60.191338

Batch 160150, train_perplexity=64.041

Batch 160160, train_perplexity=64.239555

Batch 160170, train_perplexity=67.83305

Batch 160180, train_perplexity=64.66055

Batch 160190, train_perplexity=56.82908

Batch 160200, train_perplexity=57.968307

Batch 160210, train_perplexity=60.85264

Batch 160220, train_perplexity=67.612206

Batch 160230, train_perplexity=65.50229

Batch 160240, train_perplexity=63.200623

Batch 160250, train_perplexity=59.60335

Batch 160260, train_perplexity=61.394722

Batch 160270, train_perplexity=76.00253

Batch 160280, train_perplexity=69.78443

Batch 160290, train_perplexity=65.17142

Batch 160300, train_perplexity=67.00829

Batch 160310, train_perplexity=64.47102

Batch 160320, train_perplexity=62.0746

Batch 160330, train_perplexity=61.068935

Batch 160340, train_perplexity=63.741188

Batch 160350, train_perplexity=62.503315

Batch 160360, train_perplexity=71.66163

Batch 160370, train_perplexity=68.524734

Batch 160380, train_perplexity=67.3114

Batch 160390, train_perplexity=60.55701

Batch 160400, train_perplexity=62.83885

Batch 160410, train_perplexity=56.672478

Batch 160420, train_perplexity=58.09765

Batch 160430, train_perplexity=62.94411

Batch 160440, train_perplexity=64.47539

Batch 160450, train_perplexity=58.883705

Batch 160460, train_perplexity=67.93871

Batch 160470, train_perplexity=57.65383

Batch 160480, train_perplexity=61.931297

Batch 160490, train_perplexity=61.732197

Batch 160500, train_perplexity=60.348366

Batch 160510, train_perplexity=56.42661

Batch 160520, train_perplexity=62.46321

Batch 160530, train_perplexity=64.65303

Batch 160540, train_perplexity=64.571785

Batch 160550, train_perplexity=63.319168

Batch 160560, train_perplexity=63.98316

Batch 160570, train_perplexity=67.82361

Batch 160580, train_perplexity=71.40741

Batch 160590, train_perplexity=63.487263

Batch 160600, train_perplexity=61.817974

Batch 160610, train_perplexity=66.202774

Batch 160620, train_perplexity=68.94018

Batch 160630, train_perplexity=63.52751

Batch 160640, train_perplexity=61.002026

Batch 160650, train_perplexity=72.154045

Batch 160660, train_perplexity=70.34413

Batch 160670, train_perplexity=61.29319

Batch 160680, train_perplexity=65.666466

Batch 160690, train_perplexity=69.07516

Batch 160700, train_perplexity=58.15297

Batch 160710, train_perplexity=62.4326

Batch 160720, train_perplexity=69.25655

Batch 160730, train_perplexity=69.01221

Batch 160740, train_perplexity=66.09988

Batch 160750, train_perplexity=65.94593

Batch 160760, train_perplexity=61.192646

Batch 160770, train_perplexity=64.80161

Batch 160780, train_perplexity=63.75608

Batch 160790, train_perplexity=63.278664

Batch 160800, train_perplexity=66.05532

Batch 160810, train_perplexity=66.056366

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00060-of-00100
Loaded 306404 sentences.
Finished loading
Batch 160820, train_perplexity=63.13812

Batch 160830, train_perplexity=63.092556

Batch 160840, train_perplexity=76.982

Batch 160850, train_perplexity=67.175446

Batch 160860, train_perplexity=68.17659

Batch 160870, train_perplexity=67.17304

Batch 160880, train_perplexity=63.654835

Batch 160890, train_perplexity=68.51451

Batch 160900, train_perplexity=59.943066

Batch 160910, train_perplexity=65.81151

Batch 160920, train_perplexity=69.356155

Batch 160930, train_perplexity=68.923584

Batch 160940, train_perplexity=66.66953

Batch 160950, train_perplexity=58.663425

Batch 160960, train_perplexity=66.848114

Batch 160970, train_perplexity=65.46741

Batch 160980, train_perplexity=66.48172

Batch 160990, train_perplexity=67.49351

Batch 161000, train_perplexity=69.77688

Batch 161010, train_perplexity=65.238396

Batch 161020, train_perplexity=64.78063

Batch 161030, train_perplexity=60.015427

Batch 161040, train_perplexity=63.157814

Batch 161050, train_perplexity=62.243103

Batch 161060, train_perplexity=68.47456

Batch 161070, train_perplexity=57.493805

Batch 161080, train_perplexity=60.59083

Batch 161090, train_perplexity=68.917564

Batch 161100, train_perplexity=57.882133

Batch 161110, train_perplexity=67.58871

Batch 161120, train_perplexity=62.040096

Batch 161130, train_perplexity=63.91473

Batch 161140, train_perplexity=59.870167

Batch 161150, train_perplexity=66.29919

Batch 161160, train_perplexity=65.17984

Batch 161170, train_perplexity=66.815735

Batch 161180, train_perplexity=72.22747

Batch 161190, train_perplexity=62.94327

Batch 161200, train_perplexity=59.006588

Batch 161210, train_perplexity=62.815212

Batch 161220, train_perplexity=62.03335

Batch 161230, train_perplexity=66.59232

Batch 161240, train_perplexity=65.21065

Batch 161250, train_perplexity=68.46095

Batch 161260, train_perplexity=66.21054

Batch 161270, train_perplexity=62.85893

Batch 161280, train_perplexity=70.06605

Batch 161290, train_perplexity=64.72478

Batch 161300, train_perplexity=67.21713

Batch 161310, train_perplexity=63.75596

Batch 161320, train_perplexity=65.1152

Batch 161330, train_perplexity=58.664097

Batch 161340, train_perplexity=69.27194

Batch 161350, train_perplexity=60.76597

Batch 161360, train_perplexity=66.68711

Batch 161370, train_perplexity=71.13129

Batch 161380, train_perplexity=66.78347

Batch 161390, train_perplexity=62.601387

Batch 161400, train_perplexity=65.04171

Batch 161410, train_perplexity=61.546703

Batch 161420, train_perplexity=65.22299

Batch 161430, train_perplexity=67.77014

Batch 161440, train_perplexity=67.1095

Batch 161450, train_perplexity=59.485832

Batch 161460, train_perplexity=65.75703

Batch 161470, train_perplexity=68.01398

Batch 161480, train_perplexity=65.10638

Batch 161490, train_perplexity=58.33928

Batch 161500, train_perplexity=66.286224

Batch 161510, train_perplexity=63.891876

Batch 161520, train_perplexity=60.433434

Batch 161530, train_perplexity=67.30883

Batch 161540, train_perplexity=59.532654

Batch 161550, train_perplexity=60.16098

Batch 161560, train_perplexity=62.896946

Batch 161570, train_perplexity=64.97544

Batch 161580, train_perplexity=63.417706

Batch 161590, train_perplexity=65.04848

Batch 161600, train_perplexity=64.24908

Batch 161610, train_perplexity=60.823948

Batch 161620, train_perplexity=64.86498

Batch 161630, train_perplexity=68.883

Batch 161640, train_perplexity=63.833775

Batch 161650, train_perplexity=65.14731

Batch 161660, train_perplexity=65.82934

Batch 161670, train_perplexity=66.098015

Batch 161680, train_perplexity=63.791416

Batch 161690, train_perplexity=59.512474

Batch 161700, train_perplexity=59.49922

Batch 161710, train_perplexity=63.092403
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 161720, train_perplexity=68.09569

Batch 161730, train_perplexity=67.46455

Batch 161740, train_perplexity=59.448063

Batch 161750, train_perplexity=59.716602

Batch 161760, train_perplexity=62.404385

Batch 161770, train_perplexity=64.67794

Batch 161780, train_perplexity=60.91468

Batch 161790, train_perplexity=69.68627

Batch 161800, train_perplexity=58.55957

Batch 161810, train_perplexity=64.81428

Batch 161820, train_perplexity=61.394547

Batch 161830, train_perplexity=57.283016

Batch 161840, train_perplexity=61.559765

Batch 161850, train_perplexity=60.65064

Batch 161860, train_perplexity=60.607216

Batch 161870, train_perplexity=67.35195

Batch 161880, train_perplexity=61.086002

Batch 161890, train_perplexity=60.07349

Batch 161900, train_perplexity=65.11221

Batch 161910, train_perplexity=72.414246

Batch 161920, train_perplexity=61.774597

Batch 161930, train_perplexity=61.882385

Batch 161940, train_perplexity=64.5587

Batch 161950, train_perplexity=67.10028

Batch 161960, train_perplexity=69.129166

Batch 161970, train_perplexity=63.812836

Batch 161980, train_perplexity=66.38181

Batch 161990, train_perplexity=65.70571

Batch 162000, train_perplexity=65.023445

Batch 162010, train_perplexity=66.83345

Batch 162020, train_perplexity=60.448593

Batch 162030, train_perplexity=64.09168

Batch 162040, train_perplexity=58.8987

Batch 162050, train_perplexity=64.69824

Batch 162060, train_perplexity=65.87496

Batch 162070, train_perplexity=66.87764

Batch 162080, train_perplexity=64.82092

Batch 162090, train_perplexity=68.32916

Batch 162100, train_perplexity=73.90772

Batch 162110, train_perplexity=63.085754

Batch 162120, train_perplexity=57.764538

Batch 162130, train_perplexity=64.7895

Batch 162140, train_perplexity=68.1512

Batch 162150, train_perplexity=57.289135

Batch 162160, train_perplexity=64.09278

Batch 162170, train_perplexity=65.60706

Batch 162180, train_perplexity=66.597786

Batch 162190, train_perplexity=66.470375

Batch 162200, train_perplexity=60.7793

Batch 162210, train_perplexity=66.533066

Batch 162220, train_perplexity=68.73303

Batch 162230, train_perplexity=63.67906

Batch 162240, train_perplexity=71.5868

Batch 162250, train_perplexity=69.1503

Batch 162260, train_perplexity=55.816444

Batch 162270, train_perplexity=70.32304

Batch 162280, train_perplexity=65.597466

Batch 162290, train_perplexity=61.46215

Batch 162300, train_perplexity=59.814354

Batch 162310, train_perplexity=66.01067

Batch 162320, train_perplexity=64.95131

Batch 162330, train_perplexity=59.395645

Batch 162340, train_perplexity=63.142696

Batch 162350, train_perplexity=62.526535

Batch 162360, train_perplexity=65.59699

Batch 162370, train_perplexity=65.958824

Batch 162380, train_perplexity=64.08117

Batch 162390, train_perplexity=62.75339

Batch 162400, train_perplexity=78.311905

Batch 162410, train_perplexity=73.04653

Batch 162420, train_perplexity=67.01183

Batch 162430, train_perplexity=62.49622

Batch 162440, train_perplexity=64.792

Batch 162450, train_perplexity=62.2583

Batch 162460, train_perplexity=73.23005

Batch 162470, train_perplexity=72.17638

Batch 162480, train_perplexity=64.94279

Batch 162490, train_perplexity=57.651604

Batch 162500, train_perplexity=62.917824

Batch 162510, train_perplexity=65.428925

Batch 162520, train_perplexity=61.80586

Batch 162530, train_perplexity=61.361736

Batch 162540, train_perplexity=68.2528

Batch 162550, train_perplexity=64.50651

Batch 162560, train_perplexity=61.11374

Batch 162570, train_perplexity=64.65013

Batch 162580, train_perplexity=60.66678

Batch 162590, train_perplexity=66.10662

Batch 162600, train_perplexity=63.00017

Batch 162610, train_perplexity=64.45839

Batch 162620, train_perplexity=61.014362

Batch 162630, train_perplexity=59.758675

Batch 162640, train_perplexity=62.042347

Batch 162650, train_perplexity=65.63235

Batch 162660, train_perplexity=63.206257

Batch 162670, train_perplexity=63.011528

Batch 162680, train_perplexity=65.5289

Batch 162690, train_perplexity=61.41311

Batch 162700, train_perplexity=61.50126

Batch 162710, train_perplexity=64.68084

Batch 162720, train_perplexity=68.94399

Batch 162730, train_perplexity=59.039238

Batch 162740, train_perplexity=64.52349

Batch 162750, train_perplexity=71.23923

Batch 162760, train_perplexity=75.54795

Batch 162770, train_perplexity=63.942745

Batch 162780, train_perplexity=68.19005

Batch 162790, train_perplexity=66.248245

Batch 162800, train_perplexity=65.61555

Batch 162810, train_perplexity=60.218494

Batch 162820, train_perplexity=63.357132

Batch 162830, train_perplexity=66.26341

Batch 162840, train_perplexity=63.81454

Batch 162850, train_perplexity=60.218037

Batch 162860, train_perplexity=62.87296

Batch 162870, train_perplexity=59.30412

Batch 162880, train_perplexity=58.371586

Batch 162890, train_perplexity=61.05077

Batch 162900, train_perplexity=54.746433

Batch 162910, train_perplexity=68.741974

Batch 162920, train_perplexity=61.950527

Batch 162930, train_perplexity=71.146454

Batch 162940, train_perplexity=60.708916

Batch 162950, train_perplexity=61.09789

Batch 162960, train_perplexity=68.70685

Batch 162970, train_perplexity=62.439686

Batch 162980, train_perplexity=69.916725

Batch 162990, train_perplexity=67.90778

Batch 163000, train_perplexity=72.55969

Batch 163010, train_perplexity=64.5523

Batch 163020, train_perplexity=65.63973

Batch 163030, train_perplexity=62.20705

Batch 163040, train_perplexity=68.38376

Batch 163050, train_perplexity=62.710106

Batch 163060, train_perplexity=63.342392

Batch 163070, train_perplexity=68.912964

Batch 163080, train_perplexity=61.7269

Batch 163090, train_perplexity=59.752693

Batch 163100, train_perplexity=63.98209

Batch 163110, train_perplexity=63.519302

Batch 163120, train_perplexity=64.28938

Batch 163130, train_perplexity=59.138275

Batch 163140, train_perplexity=64.165436

Batch 163150, train_perplexity=63.08305

Batch 163160, train_perplexity=65.31839

Batch 163170, train_perplexity=58.975872

Batch 163180, train_perplexity=65.08344

Batch 163190, train_perplexity=59.803745

Batch 163200, train_perplexity=65.05276

Batch 163210, train_perplexity=59.240303

Batch 163220, train_perplexity=63.280987

Batch 163230, train_perplexity=66.01927

Batch 163240, train_perplexity=66.941734

Batch 163250, train_perplexity=62.713486

Batch 163260, train_perplexity=60.23613

Batch 163270, train_perplexity=63.372875

Batch 163280, train_perplexity=65.502754

Batch 163290, train_perplexity=55.24039

Batch 163300, train_perplexity=62.331226

Batch 163310, train_perplexity=64.60526

Batch 163320, train_perplexity=71.28225

Batch 163330, train_perplexity=62.2602

Batch 163340, train_perplexity=67.58626

Batch 163350, train_perplexity=71.52351

Batch 163360, train_perplexity=64.9763

Batch 163370, train_perplexity=64.6145

Batch 163380, train_perplexity=62.170517

Batch 163390, train_perplexity=68.1168

Batch 163400, train_perplexity=64.164795

Batch 163410, train_perplexity=60.28331

Batch 163420, train_perplexity=66.29128

Batch 163430, train_perplexity=67.525536

Batch 163440, train_perplexity=61.30018

Batch 163450, train_perplexity=60.705498

Batch 163460, train_perplexity=67.42695

Batch 163470, train_perplexity=66.56375

Batch 163480, train_perplexity=70.548836

Batch 163490, train_perplexity=64.8417

Batch 163500, train_perplexity=68.570496

Batch 163510, train_perplexity=58.583252

Batch 163520, train_perplexity=65.345085

Batch 163530, train_perplexity=66.316765

Batch 163540, train_perplexity=68.60834

Batch 163550, train_perplexity=62.714684

Batch 163560, train_perplexity=65.688515

Batch 163570, train_perplexity=52.605453

Batch 163580, train_perplexity=65.9398

Batch 163590, train_perplexity=67.329185

Batch 163600, train_perplexity=64.44766

Batch 163610, train_perplexity=73.087364

Batch 163620, train_perplexity=67.06222

Batch 163630, train_perplexity=64.13907

Batch 163640, train_perplexity=66.39527

Batch 163650, train_perplexity=61.153793

Batch 163660, train_perplexity=69.04611

Batch 163670, train_perplexity=62.442184

Batch 163680, train_perplexity=66.938705

Batch 163690, train_perplexity=64.16207

Batch 163700, train_perplexity=72.55927
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 163710, train_perplexity=68.87302

Batch 163720, train_perplexity=67.37678

Batch 163730, train_perplexity=66.8441

Batch 163740, train_perplexity=70.7137

Batch 163750, train_perplexity=67.19672

Batch 163760, train_perplexity=60.922348

Batch 163770, train_perplexity=63.60844

Batch 163780, train_perplexity=59.941982

Batch 163790, train_perplexity=66.15862

Batch 163800, train_perplexity=63.437

Batch 163810, train_perplexity=65.86812

Batch 163820, train_perplexity=59.883015

Batch 163830, train_perplexity=62.741962

Batch 163840, train_perplexity=62.54556

Batch 163850, train_perplexity=60.892143

Batch 163860, train_perplexity=64.013794

Batch 163870, train_perplexity=55.561016

Batch 163880, train_perplexity=61.27604

Batch 163890, train_perplexity=65.98663

Batch 163900, train_perplexity=60.963146

Batch 163910, train_perplexity=67.3358

Batch 163920, train_perplexity=57.151237

Batch 163930, train_perplexity=71.462555

Batch 163940, train_perplexity=63.935673

Batch 163950, train_perplexity=60.78701

Batch 163960, train_perplexity=67.47523

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00006-of-00100
Loaded 305440 sentences.
Finished loading
Batch 163970, train_perplexity=58.913223

Batch 163980, train_perplexity=61.99521

Batch 163990, train_perplexity=68.178535

Batch 164000, train_perplexity=63.72365

Batch 164010, train_perplexity=60.780228

Batch 164020, train_perplexity=69.34836

Batch 164030, train_perplexity=61.568455

Batch 164040, train_perplexity=62.877094

Batch 164050, train_perplexity=72.21011

Batch 164060, train_perplexity=62.878532

Batch 164070, train_perplexity=67.09846

Batch 164080, train_perplexity=60.512558

Batch 164090, train_perplexity=61.522083

Batch 164100, train_perplexity=64.314064

Batch 164110, train_perplexity=63.600407

Batch 164120, train_perplexity=65.61902

Batch 164130, train_perplexity=62.665836

Batch 164140, train_perplexity=67.272766

Batch 164150, train_perplexity=65.02487

Batch 164160, train_perplexity=61.11237

Batch 164170, train_perplexity=72.37744

Batch 164180, train_perplexity=61.197956

Batch 164190, train_perplexity=71.572845

Batch 164200, train_perplexity=64.79753

Batch 164210, train_perplexity=60.93545

Batch 164220, train_perplexity=68.54039

Batch 164230, train_perplexity=74.79947

Batch 164240, train_perplexity=64.984146

Batch 164250, train_perplexity=64.57114

Batch 164260, train_perplexity=67.25958

Batch 164270, train_perplexity=67.518486

Batch 164280, train_perplexity=68.33437

Batch 164290, train_perplexity=63.900894

Batch 164300, train_perplexity=64.48538

Batch 164310, train_perplexity=64.06198

Batch 164320, train_perplexity=67.64158

Batch 164330, train_perplexity=65.27225

Batch 164340, train_perplexity=60.31011

Batch 164350, train_perplexity=62.238564

Batch 164360, train_perplexity=71.937546

Batch 164370, train_perplexity=65.71164

Batch 164380, train_perplexity=64.3026

Batch 164390, train_perplexity=65.00636

Batch 164400, train_perplexity=63.294327

Batch 164410, train_perplexity=57.06075

Batch 164420, train_perplexity=63.892426

Batch 164430, train_perplexity=63.740093

Batch 164440, train_perplexity=60.83636

Batch 164450, train_perplexity=63.372574

Batch 164460, train_perplexity=65.645935

Batch 164470, train_perplexity=63.25468

Batch 164480, train_perplexity=62.446175

Batch 164490, train_perplexity=69.18209

Batch 164500, train_perplexity=68.04343

Batch 164510, train_perplexity=58.923588

Batch 164520, train_perplexity=54.628593

Batch 164530, train_perplexity=61.401455

Batch 164540, train_perplexity=62.55239

Batch 164550, train_perplexity=62.630466

Batch 164560, train_perplexity=59.27975

Batch 164570, train_perplexity=62.73843

Batch 164580, train_perplexity=66.812996

Batch 164590, train_perplexity=64.17933

Batch 164600, train_perplexity=66.604965

Batch 164610, train_perplexity=69.82021

Batch 164620, train_perplexity=66.09862

Batch 164630, train_perplexity=63.517998

Batch 164640, train_perplexity=64.63567

Batch 164650, train_perplexity=70.70857

Batch 164660, train_perplexity=59.903866

Batch 164670, train_perplexity=62.15297

Batch 164680, train_perplexity=58.397335

Batch 164690, train_perplexity=65.76242

Batch 164700, train_perplexity=63.515213

Batch 164710, train_perplexity=75.78493

Batch 164720, train_perplexity=62.86912

Batch 164730, train_perplexity=63.444504

Batch 164740, train_perplexity=62.030396

Batch 164750, train_perplexity=57.87932

Batch 164760, train_perplexity=64.34909

Batch 164770, train_perplexity=68.69538

Batch 164780, train_perplexity=63.537872

Batch 164790, train_perplexity=60.566597

Batch 164800, train_perplexity=62.75674

Batch 164810, train_perplexity=54.955334

Batch 164820, train_perplexity=63.90489

Batch 164830, train_perplexity=65.28566

Batch 164840, train_perplexity=66.94215

Batch 164850, train_perplexity=58.55884

Batch 164860, train_perplexity=60.361088

Batch 164870, train_perplexity=66.39017

Batch 164880, train_perplexity=63.014023

Batch 164890, train_perplexity=69.27383

Batch 164900, train_perplexity=65.22315

Batch 164910, train_perplexity=66.569084

Batch 164920, train_perplexity=64.33535

Batch 164930, train_perplexity=59.471767

Batch 164940, train_perplexity=62.37761

Batch 164950, train_perplexity=61.15245

Batch 164960, train_perplexity=67.9343

Batch 164970, train_perplexity=58.62383

Batch 164980, train_perplexity=61.697353

Batch 164990, train_perplexity=65.06833

Batch 165000, train_perplexity=69.911194

Batch 165010, train_perplexity=63.922836

Batch 165020, train_perplexity=58.72934

Batch 165030, train_perplexity=62.97209

Batch 165040, train_perplexity=65.319695

Batch 165050, train_perplexity=66.96941

Batch 165060, train_perplexity=67.25872

Batch 165070, train_perplexity=56.13313

Batch 165080, train_perplexity=60.023926

Batch 165090, train_perplexity=68.716385

Batch 165100, train_perplexity=66.49469

Batch 165110, train_perplexity=59.349014

Batch 165120, train_perplexity=64.2277

Batch 165130, train_perplexity=62.35376

Batch 165140, train_perplexity=65.64893

Batch 165150, train_perplexity=63.410084

Batch 165160, train_perplexity=62.362293

Batch 165170, train_perplexity=60.15622

Batch 165180, train_perplexity=63.420185

Batch 165190, train_perplexity=64.55328

Batch 165200, train_perplexity=65.145325

Batch 165210, train_perplexity=69.6973

Batch 165220, train_perplexity=61.095673

Batch 165230, train_perplexity=66.85624

Batch 165240, train_perplexity=62.70021

Batch 165250, train_perplexity=58.734295

Batch 165260, train_perplexity=62.559727

Batch 165270, train_perplexity=67.76096

Batch 165280, train_perplexity=63.402134

Batch 165290, train_perplexity=71.033806

Batch 165300, train_perplexity=63.73836

Batch 165310, train_perplexity=57.02694

Batch 165320, train_perplexity=66.36061

Batch 165330, train_perplexity=64.639465

Batch 165340, train_perplexity=63.71946

Batch 165350, train_perplexity=65.84607

Batch 165360, train_perplexity=64.83471

Batch 165370, train_perplexity=65.23217

Batch 165380, train_perplexity=60.243797

Batch 165390, train_perplexity=65.30101

Batch 165400, train_perplexity=66.78681

Batch 165410, train_perplexity=60.32863

Batch 165420, train_perplexity=68.44839

Batch 165430, train_perplexity=65.53459

Batch 165440, train_perplexity=59.203537

Batch 165450, train_perplexity=66.05863

Batch 165460, train_perplexity=56.000233

Batch 165470, train_perplexity=66.61818

Batch 165480, train_perplexity=67.86968

Batch 165490, train_perplexity=63.65954

Batch 165500, train_perplexity=59.624672

Batch 165510, train_perplexity=75.12725

Batch 165520, train_perplexity=61.062794

Batch 165530, train_perplexity=66.5409

Batch 165540, train_perplexity=62.743607

Batch 165550, train_perplexity=63.691967

Batch 165560, train_perplexity=64.577545

Batch 165570, train_perplexity=59.280315

Batch 165580, train_perplexity=69.65039
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 165590, train_perplexity=58.64133

Batch 165600, train_perplexity=64.840614

Batch 165610, train_perplexity=57.873577

Batch 165620, train_perplexity=59.967968

Batch 165630, train_perplexity=64.385

Batch 165640, train_perplexity=62.076347

Batch 165650, train_perplexity=69.55056

Batch 165660, train_perplexity=65.54978

Batch 165670, train_perplexity=59.42624

Batch 165680, train_perplexity=67.095

Batch 165690, train_perplexity=66.16796

Batch 165700, train_perplexity=62.560204

Batch 165710, train_perplexity=71.60029

Batch 165720, train_perplexity=69.11137

Batch 165730, train_perplexity=60.630745

Batch 165740, train_perplexity=66.124626

Batch 165750, train_perplexity=63.118763

Batch 165760, train_perplexity=63.468136

Batch 165770, train_perplexity=67.66055

Batch 165780, train_perplexity=64.44376

Batch 165790, train_perplexity=64.38338

Batch 165800, train_perplexity=63.538536

Batch 165810, train_perplexity=63.102726

Batch 165820, train_perplexity=64.97101

Batch 165830, train_perplexity=65.64443

Batch 165840, train_perplexity=65.52256

Batch 165850, train_perplexity=62.39546

Batch 165860, train_perplexity=67.069954

Batch 165870, train_perplexity=65.633286

Batch 165880, train_perplexity=65.13122

Batch 165890, train_perplexity=63.553474

Batch 165900, train_perplexity=67.68694

Batch 165910, train_perplexity=65.86818

Batch 165920, train_perplexity=65.45823

Batch 165930, train_perplexity=63.64652

Batch 165940, train_perplexity=65.06085

Batch 165950, train_perplexity=62.014366

Batch 165960, train_perplexity=64.669

Batch 165970, train_perplexity=60.891766

Batch 165980, train_perplexity=65.14833

Batch 165990, train_perplexity=69.00451

Batch 166000, train_perplexity=61.338657

Batch 166010, train_perplexity=68.952675

Batch 166020, train_perplexity=65.85021

Batch 166030, train_perplexity=64.17792

Batch 166040, train_perplexity=66.969765

Batch 166050, train_perplexity=65.96354

Batch 166060, train_perplexity=65.78673

Batch 166070, train_perplexity=68.06439

Batch 166080, train_perplexity=65.77192

Batch 166090, train_perplexity=76.388535

Batch 166100, train_perplexity=58.20568

Batch 166110, train_perplexity=60.783356

Batch 166120, train_perplexity=66.08655

Batch 166130, train_perplexity=64.26207

Batch 166140, train_perplexity=68.25198

Batch 166150, train_perplexity=58.218338

Batch 166160, train_perplexity=59.265053

Batch 166170, train_perplexity=64.73608

Batch 166180, train_perplexity=62.128674

Batch 166190, train_perplexity=60.999466

Batch 166200, train_perplexity=64.791756

Batch 166210, train_perplexity=62.42373

Batch 166220, train_perplexity=68.68484

Batch 166230, train_perplexity=61.42916

Batch 166240, train_perplexity=62.103584

Batch 166250, train_perplexity=66.88612

Batch 166260, train_perplexity=66.11759

Batch 166270, train_perplexity=63.963818

Batch 166280, train_perplexity=66.04084

Batch 166290, train_perplexity=68.07309

Batch 166300, train_perplexity=59.755287

Batch 166310, train_perplexity=72.87759

Batch 166320, train_perplexity=63.375626

Batch 166330, train_perplexity=58.044262

Batch 166340, train_perplexity=63.56072

Batch 166350, train_perplexity=55.577656

Batch 166360, train_perplexity=64.40496

Batch 166370, train_perplexity=68.260445

Batch 166380, train_perplexity=68.590576

Batch 166390, train_perplexity=62.540283

Batch 166400, train_perplexity=60.62794

Batch 166410, train_perplexity=64.45178

Batch 166420, train_perplexity=61.660088

Batch 166430, train_perplexity=67.328766

Batch 166440, train_perplexity=65.73332

Batch 166450, train_perplexity=62.44707

Batch 166460, train_perplexity=63.166397

Batch 166470, train_perplexity=64.128914

Batch 166480, train_perplexity=71.97882

Batch 166490, train_perplexity=68.40088

Batch 166500, train_perplexity=62.07324

Batch 166510, train_perplexity=68.62885

Batch 166520, train_perplexity=68.685555

Batch 166530, train_perplexity=60.470387

Batch 166540, train_perplexity=67.7872

Batch 166550, train_perplexity=72.87453

Batch 166560, train_perplexity=65.338326

Batch 166570, train_perplexity=62.51321

Batch 166580, train_perplexity=71.1045

Batch 166590, train_perplexity=61.821037

Batch 166600, train_perplexity=60.091396

Batch 166610, train_perplexity=68.7686

Batch 166620, train_perplexity=65.2146

Batch 166630, train_perplexity=57.693516

Batch 166640, train_perplexity=72.17617

Batch 166650, train_perplexity=66.78968

Batch 166660, train_perplexity=67.34524

Batch 166670, train_perplexity=66.06307

Batch 166680, train_perplexity=68.85082

Batch 166690, train_perplexity=60.21689

Batch 166700, train_perplexity=60.185715

Batch 166710, train_perplexity=63.164318

Batch 166720, train_perplexity=68.02941

Batch 166730, train_perplexity=69.867805

Batch 166740, train_perplexity=62.35507

Batch 166750, train_perplexity=67.38141

Batch 166760, train_perplexity=66.07504

Batch 166770, train_perplexity=63.610626

Batch 166780, train_perplexity=61.108376

Batch 166790, train_perplexity=57.288807

Batch 166800, train_perplexity=62.729187

Batch 166810, train_perplexity=61.993435

Batch 166820, train_perplexity=62.258865

Batch 166830, train_perplexity=69.94827

Batch 166840, train_perplexity=66.170105

Batch 166850, train_perplexity=65.373726

Batch 166860, train_perplexity=61.934105

Batch 166870, train_perplexity=68.700554

Batch 166880, train_perplexity=64.83311

Batch 166890, train_perplexity=59.474915

Batch 166900, train_perplexity=66.233055

Batch 166910, train_perplexity=67.20024

Batch 166920, train_perplexity=64.77992

Batch 166930, train_perplexity=63.180977

Batch 166940, train_perplexity=67.414955

Batch 166950, train_perplexity=66.75739

Batch 166960, train_perplexity=69.22238

Batch 166970, train_perplexity=64.3394

Batch 166980, train_perplexity=62.622433

Batch 166990, train_perplexity=67.60295

Batch 167000, train_perplexity=63.946434

Batch 167010, train_perplexity=67.343956

Batch 167020, train_perplexity=60.941467

Batch 167030, train_perplexity=69.05895

Batch 167040, train_perplexity=63.971138

Batch 167050, train_perplexity=63.977757

Batch 167060, train_perplexity=69.079506

Batch 167070, train_perplexity=63.17191

Batch 167080, train_perplexity=66.79784

Batch 167090, train_perplexity=61.959682

Batch 167100, train_perplexity=64.137566

Batch 167110, train_perplexity=63.232998

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00097-of-00100
Loaded 305532 sentences.
Finished loading
Batch 167120, train_perplexity=60.63277

Batch 167130, train_perplexity=62.501198

Batch 167140, train_perplexity=66.253555

Batch 167150, train_perplexity=64.84606

Batch 167160, train_perplexity=61.96459

Batch 167170, train_perplexity=67.68604

Batch 167180, train_perplexity=65.6831

Batch 167190, train_perplexity=62.59506

Batch 167200, train_perplexity=67.22249

Batch 167210, train_perplexity=63.765415

Batch 167220, train_perplexity=62.954975

Batch 167230, train_perplexity=62.77434

Batch 167240, train_perplexity=59.94055

Batch 167250, train_perplexity=64.987274

Batch 167260, train_perplexity=72.8892

Batch 167270, train_perplexity=67.80169

Batch 167280, train_perplexity=64.49919

Batch 167290, train_perplexity=60.327423

Batch 167300, train_perplexity=61.21074

Batch 167310, train_perplexity=60.18052

Batch 167320, train_perplexity=64.12249

Batch 167330, train_perplexity=65.63592

Batch 167340, train_perplexity=62.03918

Batch 167350, train_perplexity=61.203503

Batch 167360, train_perplexity=59.906094

Batch 167370, train_perplexity=60.442886

Batch 167380, train_perplexity=65.81989

Batch 167390, train_perplexity=68.19557

Batch 167400, train_perplexity=65.16558

Batch 167410, train_perplexity=62.68066

Batch 167420, train_perplexity=66.0489

Batch 167430, train_perplexity=61.987198

Batch 167440, train_perplexity=68.75479

Batch 167450, train_perplexity=58.80842

Batch 167460, train_perplexity=68.11134

Batch 167470, train_perplexity=68.4367
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 167480, train_perplexity=69.476776

Batch 167490, train_perplexity=62.1967

Batch 167500, train_perplexity=56.694695

Batch 167510, train_perplexity=62.865105

Batch 167520, train_perplexity=57.138756

Batch 167530, train_perplexity=71.59414

Batch 167540, train_perplexity=62.227642

Batch 167550, train_perplexity=63.601284

Batch 167560, train_perplexity=65.76662

Batch 167570, train_perplexity=66.107

Batch 167580, train_perplexity=66.07942

Batch 167590, train_perplexity=66.76453

Batch 167600, train_perplexity=63.842113

Batch 167610, train_perplexity=63.76709

Batch 167620, train_perplexity=63.89267

Batch 167630, train_perplexity=59.33685

Batch 167640, train_perplexity=61.4669

Batch 167650, train_perplexity=62.517056

Batch 167660, train_perplexity=62.388287

Batch 167670, train_perplexity=62.356968

Batch 167680, train_perplexity=66.67913

Batch 167690, train_perplexity=68.674355

Batch 167700, train_perplexity=63.913147

Batch 167710, train_perplexity=65.71252

Batch 167720, train_perplexity=66.01578

Batch 167730, train_perplexity=61.721745

Batch 167740, train_perplexity=62.862823

Batch 167750, train_perplexity=53.98812

Batch 167760, train_perplexity=56.93665

Batch 167770, train_perplexity=60.70243

Batch 167780, train_perplexity=65.236404

Batch 167790, train_perplexity=62.050217

Batch 167800, train_perplexity=65.65326

Batch 167810, train_perplexity=60.54552

Batch 167820, train_perplexity=58.004753

Batch 167830, train_perplexity=66.604454

Batch 167840, train_perplexity=71.10816

Batch 167850, train_perplexity=63.713657

Batch 167860, train_perplexity=63.196705

Batch 167870, train_perplexity=67.938095

Batch 167880, train_perplexity=60.73422

Batch 167890, train_perplexity=66.01779

Batch 167900, train_perplexity=70.41189

Batch 167910, train_perplexity=64.18

Batch 167920, train_perplexity=68.055725

Batch 167930, train_perplexity=63.955093

Batch 167940, train_perplexity=58.358143

Batch 167950, train_perplexity=62.502895

Batch 167960, train_perplexity=58.50944

Batch 167970, train_perplexity=61.421017

Batch 167980, train_perplexity=61.29246

Batch 167990, train_perplexity=65.725365

Batch 168000, train_perplexity=67.339455

Batch 168010, train_perplexity=69.401054

Batch 168020, train_perplexity=64.25674

Batch 168030, train_perplexity=65.61095

Batch 168040, train_perplexity=62.87254

Batch 168050, train_perplexity=69.174965

Batch 168060, train_perplexity=64.84909

Batch 168070, train_perplexity=64.08557

Batch 168080, train_perplexity=59.46147

Batch 168090, train_perplexity=68.87407

Batch 168100, train_perplexity=56.920662

Batch 168110, train_perplexity=60.94501

Batch 168120, train_perplexity=65.357086

Batch 168130, train_perplexity=62.677433

Batch 168140, train_perplexity=63.15962

Batch 168150, train_perplexity=57.34907

Batch 168160, train_perplexity=60.512325

Batch 168170, train_perplexity=64.0618

Batch 168180, train_perplexity=68.32291

Batch 168190, train_perplexity=71.17017

Batch 168200, train_perplexity=65.59336

Batch 168210, train_perplexity=67.71929

Batch 168220, train_perplexity=60.37928

Batch 168230, train_perplexity=66.73451

Batch 168240, train_perplexity=56.59756

Batch 168250, train_perplexity=63.145798

Batch 168260, train_perplexity=62.81854

Batch 168270, train_perplexity=65.57488

Batch 168280, train_perplexity=62.72491

Batch 168290, train_perplexity=65.00767

Batch 168300, train_perplexity=64.3991

Batch 168310, train_perplexity=60.483334

Batch 168320, train_perplexity=60.872635

Batch 168330, train_perplexity=60.620167

Batch 168340, train_perplexity=64.50817

Batch 168350, train_perplexity=57.1232

Batch 168360, train_perplexity=70.252655

Batch 168370, train_perplexity=62.704903

Batch 168380, train_perplexity=65.50229

Batch 168390, train_perplexity=62.8485

Batch 168400, train_perplexity=66.1982

Batch 168410, train_perplexity=63.402103

Batch 168420, train_perplexity=66.66457

Batch 168430, train_perplexity=62.01212

Batch 168440, train_perplexity=67.63874

Batch 168450, train_perplexity=61.559296

Batch 168460, train_perplexity=72.57616

Batch 168470, train_perplexity=61.301872

Batch 168480, train_perplexity=62.363422

Batch 168490, train_perplexity=58.523003

Batch 168500, train_perplexity=63.77974

Batch 168510, train_perplexity=66.48622

Batch 168520, train_perplexity=72.0986

Batch 168530, train_perplexity=68.91014

Batch 168540, train_perplexity=60.129807

Batch 168550, train_perplexity=69.27105

Batch 168560, train_perplexity=62.853115

Batch 168570, train_perplexity=64.79024

Batch 168580, train_perplexity=60.635

Batch 168590, train_perplexity=59.404

Batch 168600, train_perplexity=66.74867

Batch 168610, train_perplexity=65.2063

Batch 168620, train_perplexity=66.05564

Batch 168630, train_perplexity=54.9437

Batch 168640, train_perplexity=60.459488

Batch 168650, train_perplexity=62.739746

Batch 168660, train_perplexity=64.2789

Batch 168670, train_perplexity=68.53349

Batch 168680, train_perplexity=60.654053

Batch 168690, train_perplexity=61.57145

Batch 168700, train_perplexity=61.989532

Batch 168710, train_perplexity=65.089615

Batch 168720, train_perplexity=67.01219

Batch 168730, train_perplexity=61.317425

Batch 168740, train_perplexity=66.11441

Batch 168750, train_perplexity=68.6409

Batch 168760, train_perplexity=63.27242

Batch 168770, train_perplexity=60.397884

Batch 168780, train_perplexity=62.584732

Batch 168790, train_perplexity=64.748856

Batch 168800, train_perplexity=62.061638

Batch 168810, train_perplexity=60.19303

Batch 168820, train_perplexity=67.967094

Batch 168830, train_perplexity=64.826675

Batch 168840, train_perplexity=58.77772

Batch 168850, train_perplexity=67.455605

Batch 168860, train_perplexity=68.73883

Batch 168870, train_perplexity=61.238884

Batch 168880, train_perplexity=60.548466

Batch 168890, train_perplexity=63.577087

Batch 168900, train_perplexity=60.60323

Batch 168910, train_perplexity=64.332954

Batch 168920, train_perplexity=61.0738

Batch 168930, train_perplexity=62.16536

Batch 168940, train_perplexity=70.89378

Batch 168950, train_perplexity=66.46118

Batch 168960, train_perplexity=64.10207

Batch 168970, train_perplexity=70.30115

Batch 168980, train_perplexity=61.54045

Batch 168990, train_perplexity=67.86787

Batch 169000, train_perplexity=65.70252

Batch 169010, train_perplexity=70.15323

Batch 169020, train_perplexity=58.800373

Batch 169030, train_perplexity=63.730488

Batch 169040, train_perplexity=62.83172

Batch 169050, train_perplexity=65.79698

Batch 169060, train_perplexity=62.484867

Batch 169070, train_perplexity=63.818863

Batch 169080, train_perplexity=64.930405

Batch 169090, train_perplexity=66.53449

Batch 169100, train_perplexity=67.51977

Batch 169110, train_perplexity=59.60807

Batch 169120, train_perplexity=65.283264

Batch 169130, train_perplexity=67.212425

Batch 169140, train_perplexity=67.18704

Batch 169150, train_perplexity=63.492443

Batch 169160, train_perplexity=61.902187

Batch 169170, train_perplexity=68.28089

Batch 169180, train_perplexity=63.257637

Batch 169190, train_perplexity=58.68256

Batch 169200, train_perplexity=62.031254

Batch 169210, train_perplexity=63.368523

Batch 169220, train_perplexity=69.30283

Batch 169230, train_perplexity=62.18727

Batch 169240, train_perplexity=59.72196

Batch 169250, train_perplexity=64.33787

Batch 169260, train_perplexity=68.3714

Batch 169270, train_perplexity=61.98634

Batch 169280, train_perplexity=60.10329

Batch 169290, train_perplexity=63.223076

Batch 169300, train_perplexity=62.819016

Batch 169310, train_perplexity=67.22961

Batch 169320, train_perplexity=62.3191

Batch 169330, train_perplexity=62.572617

Batch 169340, train_perplexity=61.625404

Batch 169350, train_perplexity=66.27144

Batch 169360, train_perplexity=59.807907

Batch 169370, train_perplexity=64.472565

Batch 169380, train_perplexity=66.57931

Batch 169390, train_perplexity=63.817703

Batch 169400, train_perplexity=65.05728

Batch 169410, train_perplexity=62.08792

Batch 169420, train_perplexity=68.17461

Batch 169430, train_perplexity=63.579998

Batch 169440, train_perplexity=65.44984

Batch 169450, train_perplexity=71.59448

Batch 169460, train_perplexity=59.663948
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 169470, train_perplexity=68.5314

Batch 169480, train_perplexity=64.42978

Batch 169490, train_perplexity=65.808876

Batch 169500, train_perplexity=65.92788

Batch 169510, train_perplexity=63.73608

Batch 169520, train_perplexity=72.567406

Batch 169530, train_perplexity=63.27296

Batch 169540, train_perplexity=66.03942

Batch 169550, train_perplexity=69.68248

Batch 169560, train_perplexity=68.869576

Batch 169570, train_perplexity=61.072723

Batch 169580, train_perplexity=64.30137

Batch 169590, train_perplexity=66.62275

Batch 169600, train_perplexity=57.980667

Batch 169610, train_perplexity=67.856155

Batch 169620, train_perplexity=64.409935

Batch 169630, train_perplexity=60.67401

Batch 169640, train_perplexity=52.67586

Batch 169650, train_perplexity=67.84897

Batch 169660, train_perplexity=63.53981

Batch 169670, train_perplexity=67.81449

Batch 169680, train_perplexity=62.978996

Batch 169690, train_perplexity=60.682636

Batch 169700, train_perplexity=57.744846

Batch 169710, train_perplexity=64.89697

Batch 169720, train_perplexity=63.68887

Batch 169730, train_perplexity=62.253403

Batch 169740, train_perplexity=66.32929

Batch 169750, train_perplexity=64.93839

Batch 169760, train_perplexity=61.062645

Batch 169770, train_perplexity=62.50653

Batch 169780, train_perplexity=61.76326

Batch 169790, train_perplexity=61.203094

Batch 169800, train_perplexity=64.66132

Batch 169810, train_perplexity=69.63824

Batch 169820, train_perplexity=66.27352

Batch 169830, train_perplexity=67.34967

Batch 169840, train_perplexity=72.87186

Batch 169850, train_perplexity=64.00659

Batch 169860, train_perplexity=62.02927

Batch 169870, train_perplexity=62.82189

Batch 169880, train_perplexity=64.637215

Batch 169890, train_perplexity=62.248238

Batch 169900, train_perplexity=66.2106

Batch 169910, train_perplexity=70.84448

Batch 169920, train_perplexity=70.3276

Batch 169930, train_perplexity=59.607048

Batch 169940, train_perplexity=65.58423

Batch 169950, train_perplexity=64.41516

Batch 169960, train_perplexity=64.50614

Batch 169970, train_perplexity=69.27703

Batch 169980, train_perplexity=71.10616

Batch 169990, train_perplexity=69.00684

Batch 170000, train_perplexity=57.07479

Batch 170010, train_perplexity=70.481895

Batch 170020, train_perplexity=63.220486

Batch 170030, train_perplexity=62.245327

Batch 170040, train_perplexity=64.43276

Batch 170050, train_perplexity=67.68707

Batch 170060, train_perplexity=64.08343

Batch 170070, train_perplexity=66.69678

Batch 170080, train_perplexity=62.881233

Batch 170090, train_perplexity=60.766838

Batch 170100, train_perplexity=62.94495

Batch 170110, train_perplexity=64.367935

Batch 170120, train_perplexity=67.43451

Batch 170130, train_perplexity=64.32811

Batch 170140, train_perplexity=65.83615

Batch 170150, train_perplexity=61.70291

Batch 170160, train_perplexity=60.480537

Batch 170170, train_perplexity=59.74312

Batch 170180, train_perplexity=63.210598

Batch 170190, train_perplexity=60.15989

Batch 170200, train_perplexity=59.0505

Batch 170210, train_perplexity=65.37236

Batch 170220, train_perplexity=68.100555

Batch 170230, train_perplexity=67.81449

Batch 170240, train_perplexity=67.51887

Batch 170250, train_perplexity=60.85667

Batch 170260, train_perplexity=62.259697

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00059-of-00100
Loaded 306839 sentences.
Finished loading
Batch 170270, train_perplexity=63.997894

Batch 170280, train_perplexity=64.41454

Batch 170290, train_perplexity=62.918003

Batch 170300, train_perplexity=73.81981

Batch 170310, train_perplexity=62.17814

Batch 170320, train_perplexity=64.88058

Batch 170330, train_perplexity=61.55947

Batch 170340, train_perplexity=69.3105

Batch 170350, train_perplexity=59.462437

Batch 170360, train_perplexity=66.79299

Batch 170370, train_perplexity=66.411385

Batch 170380, train_perplexity=62.415752

Batch 170390, train_perplexity=62.256725

Batch 170400, train_perplexity=59.59173

Batch 170410, train_perplexity=64.28046

Batch 170420, train_perplexity=67.37858

Batch 170430, train_perplexity=59.69992

Batch 170440, train_perplexity=62.618523

Batch 170450, train_perplexity=66.763

Batch 170460, train_perplexity=64.26189

Batch 170470, train_perplexity=64.12928

Batch 170480, train_perplexity=67.07328

Batch 170490, train_perplexity=61.39891

Batch 170500, train_perplexity=67.88379

Batch 170510, train_perplexity=57.474426

Batch 170520, train_perplexity=63.486355

Batch 170530, train_perplexity=61.972626

Batch 170540, train_perplexity=66.41056

Batch 170550, train_perplexity=69.10929

Batch 170560, train_perplexity=63.581455

Batch 170570, train_perplexity=69.51472

Batch 170580, train_perplexity=65.664215

Batch 170590, train_perplexity=64.11399

Batch 170600, train_perplexity=65.45533

Batch 170610, train_perplexity=72.11831

Batch 170620, train_perplexity=63.19053

Batch 170630, train_perplexity=61.801674

Batch 170640, train_perplexity=63.672592

Batch 170650, train_perplexity=56.73023

Batch 170660, train_perplexity=65.2313

Batch 170670, train_perplexity=65.49941

Batch 170680, train_perplexity=65.47424

Batch 170690, train_perplexity=63.632347

Batch 170700, train_perplexity=70.48398

Batch 170710, train_perplexity=63.41217

Batch 170720, train_perplexity=62.78003

Batch 170730, train_perplexity=64.27243

Batch 170740, train_perplexity=64.468445

Batch 170750, train_perplexity=61.681526

Batch 170760, train_perplexity=64.44914

Batch 170770, train_perplexity=63.652256

Batch 170780, train_perplexity=56.944878

Batch 170790, train_perplexity=63.730186

Batch 170800, train_perplexity=61.403564

Batch 170810, train_perplexity=65.80351

Batch 170820, train_perplexity=59.27017

Batch 170830, train_perplexity=63.64054

Batch 170840, train_perplexity=62.376537

Batch 170850, train_perplexity=65.96474

Batch 170860, train_perplexity=57.535275

Batch 170870, train_perplexity=66.76777

Batch 170880, train_perplexity=65.93209

Batch 170890, train_perplexity=62.058

Batch 170900, train_perplexity=61.435867

Batch 170910, train_perplexity=70.14172

Batch 170920, train_perplexity=60.294006

Batch 170930, train_perplexity=63.748634

Batch 170940, train_perplexity=62.99996

Batch 170950, train_perplexity=66.91496

Batch 170960, train_perplexity=59.066044

Batch 170970, train_perplexity=59.110054

Batch 170980, train_perplexity=75.79986

Batch 170990, train_perplexity=65.452644

Batch 171000, train_perplexity=61.830563

Batch 171010, train_perplexity=62.865105

Batch 171020, train_perplexity=62.26884

Batch 171030, train_perplexity=62.641487

Batch 171040, train_perplexity=61.771595

Batch 171050, train_perplexity=65.66509

Batch 171060, train_perplexity=67.341515

Batch 171070, train_perplexity=60.57584

Batch 171080, train_perplexity=67.57369

Batch 171090, train_perplexity=70.19077

Batch 171100, train_perplexity=59.68142

Batch 171110, train_perplexity=66.03838

Batch 171120, train_perplexity=63.631165

Batch 171130, train_perplexity=56.866188

Batch 171140, train_perplexity=64.06693

Batch 171150, train_perplexity=69.69175

Batch 171160, train_perplexity=62.378887

Batch 171170, train_perplexity=59.413036

Batch 171180, train_perplexity=61.1619

Batch 171190, train_perplexity=65.348045

Batch 171200, train_perplexity=70.25406

Batch 171210, train_perplexity=64.61389

Batch 171220, train_perplexity=62.834534

Batch 171230, train_perplexity=67.420105

Batch 171240, train_perplexity=62.42352

Batch 171250, train_perplexity=66.9546

Batch 171260, train_perplexity=64.26483

Batch 171270, train_perplexity=60.613144

Batch 171280, train_perplexity=61.41352

Batch 171290, train_perplexity=64.3917

Batch 171300, train_perplexity=64.66416

Batch 171310, train_perplexity=62.449093

Batch 171320, train_perplexity=61.954514

Batch 171330, train_perplexity=64.66752

Batch 171340, train_perplexity=67.416954

Batch 171350, train_perplexity=58.882862
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 171360, train_perplexity=62.10459

Batch 171370, train_perplexity=66.66066

Batch 171380, train_perplexity=59.399723

Batch 171390, train_perplexity=63.20469

Batch 171400, train_perplexity=55.513638

Batch 171410, train_perplexity=63.61184

Batch 171420, train_perplexity=65.34144

Batch 171430, train_perplexity=63.694275

Batch 171440, train_perplexity=60.696033

Batch 171450, train_perplexity=67.314865

Batch 171460, train_perplexity=63.143116

Batch 171470, train_perplexity=63.341667

Batch 171480, train_perplexity=61.978477

Batch 171490, train_perplexity=65.1483

Batch 171500, train_perplexity=58.471344

Batch 171510, train_perplexity=58.77506

Batch 171520, train_perplexity=62.235653

Batch 171530, train_perplexity=65.372734

Batch 171540, train_perplexity=61.140903

Batch 171550, train_perplexity=71.51935

Batch 171560, train_perplexity=65.706314

Batch 171570, train_perplexity=64.69386

Batch 171580, train_perplexity=59.7896

Batch 171590, train_perplexity=68.24362

Batch 171600, train_perplexity=61.98504

Batch 171610, train_perplexity=64.974014

Batch 171620, train_perplexity=59.028877

Batch 171630, train_perplexity=70.146675

Batch 171640, train_perplexity=59.033043

Batch 171650, train_perplexity=62.83573

Batch 171660, train_perplexity=56.533585

Batch 171670, train_perplexity=65.3002

Batch 171680, train_perplexity=68.837006

Batch 171690, train_perplexity=55.962914

Batch 171700, train_perplexity=65.06908

Batch 171710, train_perplexity=70.51716

Batch 171720, train_perplexity=63.295532

Batch 171730, train_perplexity=56.594433

Batch 171740, train_perplexity=65.07124

Batch 171750, train_perplexity=67.14765

Batch 171760, train_perplexity=70.98004

Batch 171770, train_perplexity=60.92444

Batch 171780, train_perplexity=65.049065

Batch 171790, train_perplexity=66.50975

Batch 171800, train_perplexity=57.963444

Batch 171810, train_perplexity=66.48534

Batch 171820, train_perplexity=63.7252

Batch 171830, train_perplexity=60.704052

Batch 171840, train_perplexity=60.915813

Batch 171850, train_perplexity=67.14547

Batch 171860, train_perplexity=64.66795

Batch 171870, train_perplexity=67.06295

Batch 171880, train_perplexity=66.832115

Batch 171890, train_perplexity=61.037727

Batch 171900, train_perplexity=61.599346

Batch 171910, train_perplexity=63.97571

Batch 171920, train_perplexity=63.344505

Batch 171930, train_perplexity=66.76497

Batch 171940, train_perplexity=62.461422

Batch 171950, train_perplexity=59.44605

Batch 171960, train_perplexity=61.19965

Batch 171970, train_perplexity=64.82157

Batch 171980, train_perplexity=61.44952

Batch 171990, train_perplexity=67.943504

Batch 172000, train_perplexity=62.554268

Batch 172010, train_perplexity=59.6421

Batch 172020, train_perplexity=63.388016

Batch 172030, train_perplexity=64.46131

Batch 172040, train_perplexity=65.07137

Batch 172050, train_perplexity=67.304436

Batch 172060, train_perplexity=57.742836

Batch 172070, train_perplexity=58.348625

Batch 172080, train_perplexity=65.96235

Batch 172090, train_perplexity=58.90572

Batch 172100, train_perplexity=61.795338

Batch 172110, train_perplexity=62.00437

Batch 172120, train_perplexity=68.72926

Batch 172130, train_perplexity=59.004112

Batch 172140, train_perplexity=63.417492

Batch 172150, train_perplexity=62.351616

Batch 172160, train_perplexity=68.670296

Batch 172170, train_perplexity=68.031296

Batch 172180, train_perplexity=67.51086

Batch 172190, train_perplexity=64.85051

Batch 172200, train_perplexity=61.310642

Batch 172210, train_perplexity=70.54086

Batch 172220, train_perplexity=56.35436

Batch 172230, train_perplexity=60.85673

Batch 172240, train_perplexity=64.469734

Batch 172250, train_perplexity=65.70377

Batch 172260, train_perplexity=63.109825

Batch 172270, train_perplexity=63.409966

Batch 172280, train_perplexity=64.67128

Batch 172290, train_perplexity=63.637566

Batch 172300, train_perplexity=67.62317

Batch 172310, train_perplexity=62.565277

Batch 172320, train_perplexity=65.09557

Batch 172330, train_perplexity=69.059906

Batch 172340, train_perplexity=63.117832

Batch 172350, train_perplexity=61.730957

Batch 172360, train_perplexity=65.67342

Batch 172370, train_perplexity=63.613358

Batch 172380, train_perplexity=61.56775

Batch 172390, train_perplexity=63.94509

Batch 172400, train_perplexity=73.6258

Batch 172410, train_perplexity=68.91034

Batch 172420, train_perplexity=65.859764

Batch 172430, train_perplexity=66.0147

Batch 172440, train_perplexity=58.685696

Batch 172450, train_perplexity=63.641937

Batch 172460, train_perplexity=66.44455

Batch 172470, train_perplexity=64.1815

Batch 172480, train_perplexity=61.78217

Batch 172490, train_perplexity=58.65397

Batch 172500, train_perplexity=61.50211

Batch 172510, train_perplexity=67.353745

Batch 172520, train_perplexity=69.40704

Batch 172530, train_perplexity=64.59497

Batch 172540, train_perplexity=63.80894

Batch 172550, train_perplexity=67.11855

Batch 172560, train_perplexity=57.643578

Batch 172570, train_perplexity=62.04542

Batch 172580, train_perplexity=65.731255

Batch 172590, train_perplexity=63.595673

Batch 172600, train_perplexity=56.328274

Batch 172610, train_perplexity=63.088825

Batch 172620, train_perplexity=64.44207

Batch 172630, train_perplexity=63.68486

Batch 172640, train_perplexity=65.771576

Batch 172650, train_perplexity=58.34598

Batch 172660, train_perplexity=65.366745

Batch 172670, train_perplexity=63.034428

Batch 172680, train_perplexity=61.8119

Batch 172690, train_perplexity=63.907387

Batch 172700, train_perplexity=61.007202

Batch 172710, train_perplexity=59.377068

Batch 172720, train_perplexity=63.354717

Batch 172730, train_perplexity=62.11573

Batch 172740, train_perplexity=62.219303

Batch 172750, train_perplexity=62.869358

Batch 172760, train_perplexity=63.600708

Batch 172770, train_perplexity=61.511173

Batch 172780, train_perplexity=71.358086

Batch 172790, train_perplexity=60.43539

Batch 172800, train_perplexity=58.840057

Batch 172810, train_perplexity=65.91851

Batch 172820, train_perplexity=58.116463

Batch 172830, train_perplexity=57.26772

Batch 172840, train_perplexity=64.308975

Batch 172850, train_perplexity=64.60298

Batch 172860, train_perplexity=56.858757

Batch 172870, train_perplexity=62.4616

Batch 172880, train_perplexity=61.803295

Batch 172890, train_perplexity=67.923225

Batch 172900, train_perplexity=65.06821

Batch 172910, train_perplexity=61.69535

Batch 172920, train_perplexity=61.383656

Batch 172930, train_perplexity=62.48585

Batch 172940, train_perplexity=59.445255

Batch 172950, train_perplexity=58.507263

Batch 172960, train_perplexity=63.880363

Batch 172970, train_perplexity=57.783848

Batch 172980, train_perplexity=66.63508

Batch 172990, train_perplexity=54.463406

Batch 173000, train_perplexity=59.772156

Batch 173010, train_perplexity=62.49378

Batch 173020, train_perplexity=61.18891

Batch 173030, train_perplexity=68.51735

Batch 173040, train_perplexity=61.195507

Batch 173050, train_perplexity=60.793617

Batch 173060, train_perplexity=57.08783

Batch 173070, train_perplexity=63.236282

Batch 173080, train_perplexity=61.905937

Batch 173090, train_perplexity=63.218254

Batch 173100, train_perplexity=59.27226

Batch 173110, train_perplexity=62.669125

Batch 173120, train_perplexity=60.11613

Batch 173130, train_perplexity=60.791386

Batch 173140, train_perplexity=68.28701

Batch 173150, train_perplexity=67.01081

Batch 173160, train_perplexity=64.46442

Batch 173170, train_perplexity=71.165825

Batch 173180, train_perplexity=68.64993

Batch 173190, train_perplexity=62.773563

Batch 173200, train_perplexity=61.084255

Batch 173210, train_perplexity=65.072426

Batch 173220, train_perplexity=63.378407

Batch 173230, train_perplexity=69.66361

Batch 173240, train_perplexity=62.35037

Batch 173250, train_perplexity=62.676716

Batch 173260, train_perplexity=67.33673

Batch 173270, train_perplexity=69.57149

Batch 173280, train_perplexity=61.489998

Batch 173290, train_perplexity=61.696262

Batch 173300, train_perplexity=67.38057

Batch 173310, train_perplexity=59.916607

Batch 173320, train_perplexity=59.93078

Batch 173330, train_perplexity=66.746925
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 173340, train_perplexity=64.57563

Batch 173350, train_perplexity=71.74369

Batch 173360, train_perplexity=54.636852

Batch 173370, train_perplexity=69.03064

Batch 173380, train_perplexity=64.62245

Batch 173390, train_perplexity=66.313095

Batch 173400, train_perplexity=73.82157

Batch 173410, train_perplexity=62.214706

Batch 173420, train_perplexity=66.71434

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00016-of-00100
Loaded 306534 sentences.
Finished loading
Batch 173430, train_perplexity=62.32451

Batch 173440, train_perplexity=65.85003

Batch 173450, train_perplexity=69.350204

Batch 173460, train_perplexity=66.96839

Batch 173470, train_perplexity=61.749508

Batch 173480, train_perplexity=59.904922

Batch 173490, train_perplexity=67.96924

Batch 173500, train_perplexity=64.20308

Batch 173510, train_perplexity=61.891354

Batch 173520, train_perplexity=68.42267

Batch 173530, train_perplexity=64.15935

Batch 173540, train_perplexity=63.227116

Batch 173550, train_perplexity=63.73031

Batch 173560, train_perplexity=63.945152

Batch 173570, train_perplexity=62.840527

Batch 173580, train_perplexity=74.06867

Batch 173590, train_perplexity=61.22008

Batch 173600, train_perplexity=59.621742

Batch 173610, train_perplexity=60.00827

Batch 173620, train_perplexity=66.35621

Batch 173630, train_perplexity=63.180645

Batch 173640, train_perplexity=66.9636

Batch 173650, train_perplexity=65.06951

Batch 173660, train_perplexity=67.22941

Batch 173670, train_perplexity=61.887875

Batch 173680, train_perplexity=63.525845

Batch 173690, train_perplexity=60.313126

Batch 173700, train_perplexity=67.89771

Batch 173710, train_perplexity=62.006264

Batch 173720, train_perplexity=60.70949

Batch 173730, train_perplexity=62.989418

Batch 173740, train_perplexity=57.594208

Batch 173750, train_perplexity=62.000793

Batch 173760, train_perplexity=65.66669

Batch 173770, train_perplexity=62.02383

Batch 173780, train_perplexity=59.742805

Batch 173790, train_perplexity=66.40717

Batch 173800, train_perplexity=61.11144

Batch 173810, train_perplexity=71.99125

Batch 173820, train_perplexity=63.59898

Batch 173830, train_perplexity=59.72828

Batch 173840, train_perplexity=65.14377

Batch 173850, train_perplexity=64.36849

Batch 173860, train_perplexity=65.149796

Batch 173870, train_perplexity=61.427433

Batch 173880, train_perplexity=64.04738

Batch 173890, train_perplexity=65.514595

Batch 173900, train_perplexity=66.44252

Batch 173910, train_perplexity=70.34511

Batch 173920, train_perplexity=57.801704

Batch 173930, train_perplexity=62.034477

Batch 173940, train_perplexity=63.361183

Batch 173950, train_perplexity=61.277996

Batch 173960, train_perplexity=65.30475

Batch 173970, train_perplexity=66.7922

Batch 173980, train_perplexity=61.310993

Batch 173990, train_perplexity=73.548004

Batch 174000, train_perplexity=65.4499

Batch 174010, train_perplexity=65.15166

Batch 174020, train_perplexity=58.596664

Batch 174030, train_perplexity=57.108658

Batch 174040, train_perplexity=63.85795

Batch 174050, train_perplexity=63.237732

Batch 174060, train_perplexity=54.236057

Batch 174070, train_perplexity=66.28177

Batch 174080, train_perplexity=64.2209

Batch 174090, train_perplexity=69.25553

Batch 174100, train_perplexity=66.053116

Batch 174110, train_perplexity=61.528275

Batch 174120, train_perplexity=56.619347

Batch 174130, train_perplexity=65.51272

Batch 174140, train_perplexity=60.8417

Batch 174150, train_perplexity=58.86487

Batch 174160, train_perplexity=64.117386

Batch 174170, train_perplexity=63.699863

Batch 174180, train_perplexity=61.896225

Batch 174190, train_perplexity=62.515327

Batch 174200, train_perplexity=56.467037

Batch 174210, train_perplexity=66.76064

Batch 174220, train_perplexity=63.527058

Batch 174230, train_perplexity=60.384808

Batch 174240, train_perplexity=58.657635

Batch 174250, train_perplexity=66.662254

Batch 174260, train_perplexity=64.28506

Batch 174270, train_perplexity=66.81395

Batch 174280, train_perplexity=59.53268

Batch 174290, train_perplexity=73.0567

Batch 174300, train_perplexity=61.42327

Batch 174310, train_perplexity=59.6363

Batch 174320, train_perplexity=65.093994

Batch 174330, train_perplexity=67.90043

Batch 174340, train_perplexity=65.18074

Batch 174350, train_perplexity=64.111

Batch 174360, train_perplexity=58.887356

Batch 174370, train_perplexity=59.294846

Batch 174380, train_perplexity=67.47858

Batch 174390, train_perplexity=62.906273

Batch 174400, train_perplexity=66.75163

Batch 174410, train_perplexity=57.61489

Batch 174420, train_perplexity=56.947838

Batch 174430, train_perplexity=62.773144

Batch 174440, train_perplexity=75.50744

Batch 174450, train_perplexity=62.31595

Batch 174460, train_perplexity=61.557766

Batch 174470, train_perplexity=61.50894

Batch 174480, train_perplexity=54.151493

Batch 174490, train_perplexity=60.25486

Batch 174500, train_perplexity=62.052643

Batch 174510, train_perplexity=69.77898

Batch 174520, train_perplexity=71.49521

Batch 174530, train_perplexity=66.875595

Batch 174540, train_perplexity=59.96777

Batch 174550, train_perplexity=63.109283

Batch 174560, train_perplexity=64.94248

Batch 174570, train_perplexity=61.209457

Batch 174580, train_perplexity=65.06318

Batch 174590, train_perplexity=65.268326

Batch 174600, train_perplexity=59.356346

Batch 174610, train_perplexity=62.63698

Batch 174620, train_perplexity=58.94149

Batch 174630, train_perplexity=66.65593

Batch 174640, train_perplexity=68.13382

Batch 174650, train_perplexity=59.28388

Batch 174660, train_perplexity=64.54722

Batch 174670, train_perplexity=68.589264

Batch 174680, train_perplexity=71.60923

Batch 174690, train_perplexity=58.70092

Batch 174700, train_perplexity=68.99112

Batch 174710, train_perplexity=72.67056

Batch 174720, train_perplexity=61.83861

Batch 174730, train_perplexity=61.636955

Batch 174740, train_perplexity=61.632957

Batch 174750, train_perplexity=63.78798

Batch 174760, train_perplexity=63.089397

Batch 174770, train_perplexity=61.102898

Batch 174780, train_perplexity=68.33183

Batch 174790, train_perplexity=60.670166

Batch 174800, train_perplexity=71.879974

Batch 174810, train_perplexity=63.56545

Batch 174820, train_perplexity=66.32986

Batch 174830, train_perplexity=67.63545

Batch 174840, train_perplexity=63.64546

Batch 174850, train_perplexity=71.99389

Batch 174860, train_perplexity=61.303596

Batch 174870, train_perplexity=60.602478

Batch 174880, train_perplexity=67.73942

Batch 174890, train_perplexity=59.737595

Batch 174900, train_perplexity=61.315994

Batch 174910, train_perplexity=63.601707

Batch 174920, train_perplexity=62.94399

Batch 174930, train_perplexity=64.54716

Batch 174940, train_perplexity=55.178158

Batch 174950, train_perplexity=63.356228

Batch 174960, train_perplexity=69.53458

Batch 174970, train_perplexity=59.508926

Batch 174980, train_perplexity=61.944263

Batch 174990, train_perplexity=64.5527

Batch 175000, train_perplexity=60.326157

Batch 175010, train_perplexity=62.01691

Batch 175020, train_perplexity=59.23968

Batch 175030, train_perplexity=61.44041

Batch 175040, train_perplexity=75.41539

Batch 175050, train_perplexity=57.94327

Batch 175060, train_perplexity=67.694244

Batch 175070, train_perplexity=51.92924

Batch 175080, train_perplexity=62.285618

Batch 175090, train_perplexity=66.318344

Batch 175100, train_perplexity=63.455757

Batch 175110, train_perplexity=58.19153

Batch 175120, train_perplexity=56.660156

Batch 175130, train_perplexity=59.504898

Batch 175140, train_perplexity=68.3906

Batch 175150, train_perplexity=58.739616

Batch 175160, train_perplexity=59.908436

Batch 175170, train_perplexity=61.866276

Batch 175180, train_perplexity=68.55474

Batch 175190, train_perplexity=67.06336

Batch 175200, train_perplexity=61.765907

Batch 175210, train_perplexity=63.846985
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 175220, train_perplexity=62.176952

Batch 175230, train_perplexity=65.476776

Batch 175240, train_perplexity=70.7759

Batch 175250, train_perplexity=62.872417

Batch 175260, train_perplexity=66.04758

Batch 175270, train_perplexity=58.605328

Batch 175280, train_perplexity=59.93249

Batch 175290, train_perplexity=64.591805

Batch 175300, train_perplexity=67.58858

Batch 175310, train_perplexity=63.649857

Batch 175320, train_perplexity=59.04278

Batch 175330, train_perplexity=69.51495

Batch 175340, train_perplexity=60.461334

Batch 175350, train_perplexity=61.504925

Batch 175360, train_perplexity=58.003094

Batch 175370, train_perplexity=59.31102

Batch 175380, train_perplexity=63.155464

Batch 175390, train_perplexity=60.575897

Batch 175400, train_perplexity=68.44832

Batch 175410, train_perplexity=69.031166

Batch 175420, train_perplexity=57.142136

Batch 175430, train_perplexity=67.6552

Batch 175440, train_perplexity=59.800438

Batch 175450, train_perplexity=61.755985

Batch 175460, train_perplexity=60.64604

Batch 175470, train_perplexity=66.533035

Batch 175480, train_perplexity=61.830563

Batch 175490, train_perplexity=60.001606

Batch 175500, train_perplexity=58.40285

Batch 175510, train_perplexity=58.888927

Batch 175520, train_perplexity=70.062836

Batch 175530, train_perplexity=57.095398

Batch 175540, train_perplexity=65.38402

Batch 175550, train_perplexity=67.12476

Batch 175560, train_perplexity=64.23205

Batch 175570, train_perplexity=64.13081

Batch 175580, train_perplexity=57.897644

Batch 175590, train_perplexity=67.02782

Batch 175600, train_perplexity=70.69563

Batch 175610, train_perplexity=71.08341

Batch 175620, train_perplexity=65.09154

Batch 175630, train_perplexity=59.519142

Batch 175640, train_perplexity=60.039898

Batch 175650, train_perplexity=65.29491

Batch 175660, train_perplexity=64.941734

Batch 175670, train_perplexity=66.272

Batch 175680, train_perplexity=75.713844

Batch 175690, train_perplexity=68.74545

Batch 175700, train_perplexity=61.47018

Batch 175710, train_perplexity=67.2147

Batch 175720, train_perplexity=60.9981

Batch 175730, train_perplexity=64.549095

Batch 175740, train_perplexity=61.170242

Batch 175750, train_perplexity=67.26529

Batch 175760, train_perplexity=61.774952

Batch 175770, train_perplexity=62.72443

Batch 175780, train_perplexity=60.348225

Batch 175790, train_perplexity=65.60378

Batch 175800, train_perplexity=64.144325

Batch 175810, train_perplexity=61.665764

Batch 175820, train_perplexity=66.74705

Batch 175830, train_perplexity=64.66971

Batch 175840, train_perplexity=59.30033

Batch 175850, train_perplexity=69.215546

Batch 175860, train_perplexity=65.01449

Batch 175870, train_perplexity=66.27194

Batch 175880, train_perplexity=71.80077

Batch 175890, train_perplexity=73.16882

Batch 175900, train_perplexity=71.54896

Batch 175910, train_perplexity=59.975403

Batch 175920, train_perplexity=60.377525

Batch 175930, train_perplexity=63.747326

Batch 175940, train_perplexity=63.762222

Batch 175950, train_perplexity=60.29182

Batch 175960, train_perplexity=59.878334

Batch 175970, train_perplexity=63.67824

Batch 175980, train_perplexity=61.93664

Batch 175990, train_perplexity=68.38995

Batch 176000, train_perplexity=58.916004

Batch 176010, train_perplexity=64.51442

Batch 176020, train_perplexity=59.807564

Batch 176030, train_perplexity=62.791344

Batch 176040, train_perplexity=69.86214

Batch 176050, train_perplexity=56.713352

Batch 176060, train_perplexity=63.653408

Batch 176070, train_perplexity=65.257835

Batch 176080, train_perplexity=66.217735

Batch 176090, train_perplexity=68.93005

Batch 176100, train_perplexity=64.94663

Batch 176110, train_perplexity=61.783142

Batch 176120, train_perplexity=62.931206

Batch 176130, train_perplexity=63.54157

Batch 176140, train_perplexity=63.486507

Batch 176150, train_perplexity=65.06671

Batch 176160, train_perplexity=63.439724

Batch 176170, train_perplexity=71.34414

Batch 176180, train_perplexity=64.79667

Batch 176190, train_perplexity=56.909153

Batch 176200, train_perplexity=63.22561

Batch 176210, train_perplexity=66.08488

Batch 176220, train_perplexity=66.80147

Batch 176230, train_perplexity=62.24966

Batch 176240, train_perplexity=61.186985

Batch 176250, train_perplexity=62.212986

Batch 176260, train_perplexity=70.535385

Batch 176270, train_perplexity=72.36516

Batch 176280, train_perplexity=59.997456

Batch 176290, train_perplexity=62.50567

Batch 176300, train_perplexity=67.95203

Batch 176310, train_perplexity=67.16023

Batch 176320, train_perplexity=65.413925

Batch 176330, train_perplexity=68.19603

Batch 176340, train_perplexity=62.921005

Batch 176350, train_perplexity=67.97257

Batch 176360, train_perplexity=66.0273

Batch 176370, train_perplexity=58.539215

Batch 176380, train_perplexity=59.547787

Batch 176390, train_perplexity=62.470417

Batch 176400, train_perplexity=67.538414

Batch 176410, train_perplexity=62.217495

Batch 176420, train_perplexity=66.65103

Batch 176430, train_perplexity=62.71492

Batch 176440, train_perplexity=65.81741

Batch 176450, train_perplexity=58.291977

Batch 176460, train_perplexity=69.092026

Batch 176470, train_perplexity=63.850883

Batch 176480, train_perplexity=60.29369

Batch 176490, train_perplexity=62.05146

Batch 176500, train_perplexity=62.82282

Batch 176510, train_perplexity=65.94756

Batch 176520, train_perplexity=68.06329

Batch 176530, train_perplexity=62.785206

Batch 176540, train_perplexity=59.979206

Batch 176550, train_perplexity=62.409946

Batch 176560, train_perplexity=56.608303

Batch 176570, train_perplexity=63.5107

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00014-of-00100
Loaded 306408 sentences.
Finished loading
Batch 176580, train_perplexity=68.1984

Batch 176590, train_perplexity=62.399326

Batch 176600, train_perplexity=65.540215

Batch 176610, train_perplexity=64.130196

Batch 176620, train_perplexity=68.05881

Batch 176630, train_perplexity=67.14137

Batch 176640, train_perplexity=58.302765

Batch 176650, train_perplexity=68.35588

Batch 176660, train_perplexity=59.8622

Batch 176670, train_perplexity=64.606834

Batch 176680, train_perplexity=63.784695

Batch 176690, train_perplexity=64.052635

Batch 176700, train_perplexity=67.354355

Batch 176710, train_perplexity=65.656265

Batch 176720, train_perplexity=61.242912

Batch 176730, train_perplexity=60.547657

Batch 176740, train_perplexity=62.655827

Batch 176750, train_perplexity=57.66868

Batch 176760, train_perplexity=62.30784

Batch 176770, train_perplexity=61.181442

Batch 176780, train_perplexity=68.08777

Batch 176790, train_perplexity=67.612946

Batch 176800, train_perplexity=67.84525

Batch 176810, train_perplexity=66.10063

Batch 176820, train_perplexity=64.17146

Batch 176830, train_perplexity=63.14565

Batch 176840, train_perplexity=62.695965

Batch 176850, train_perplexity=63.505886

Batch 176860, train_perplexity=66.42861

Batch 176870, train_perplexity=61.64286

Batch 176880, train_perplexity=70.37946

Batch 176890, train_perplexity=59.49718

Batch 176900, train_perplexity=62.146362

Batch 176910, train_perplexity=66.76847

Batch 176920, train_perplexity=60.5915

Batch 176930, train_perplexity=68.33793

Batch 176940, train_perplexity=63.565113

Batch 176950, train_perplexity=65.01492

Batch 176960, train_perplexity=70.405914

Batch 176970, train_perplexity=59.36008

Batch 176980, train_perplexity=65.682625

Batch 176990, train_perplexity=65.32886

Batch 177000, train_perplexity=64.20568

Batch 177010, train_perplexity=57.999607

Batch 177020, train_perplexity=61.89853

Batch 177030, train_perplexity=55.774593

Batch 177040, train_perplexity=62.967285

Batch 177050, train_perplexity=58.168556

Batch 177060, train_perplexity=63.83408

Batch 177070, train_perplexity=63.53802

Batch 177080, train_perplexity=64.64477

Batch 177090, train_perplexity=65.31633
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 177100, train_perplexity=65.5766

Batch 177110, train_perplexity=65.38614

Batch 177120, train_perplexity=68.323944

Batch 177130, train_perplexity=64.48535

Batch 177140, train_perplexity=59.881016

Batch 177150, train_perplexity=64.50343

Batch 177160, train_perplexity=64.63762

Batch 177170, train_perplexity=62.486687

Batch 177180, train_perplexity=67.455025

Batch 177190, train_perplexity=58.211704

Batch 177200, train_perplexity=66.99535

Batch 177210, train_perplexity=67.00353

Batch 177220, train_perplexity=67.291695

Batch 177230, train_perplexity=62.862347

Batch 177240, train_perplexity=57.136875

Batch 177250, train_perplexity=66.041275

Batch 177260, train_perplexity=67.59032

Batch 177270, train_perplexity=65.63285

Batch 177280, train_perplexity=64.877975

Batch 177290, train_perplexity=59.970455

Batch 177300, train_perplexity=65.85493

Batch 177310, train_perplexity=65.93876

Batch 177320, train_perplexity=64.890724

Batch 177330, train_perplexity=58.24791

Batch 177340, train_perplexity=66.64906

Batch 177350, train_perplexity=63.565567

Batch 177360, train_perplexity=64.482956

Batch 177370, train_perplexity=61.51569

Batch 177380, train_perplexity=67.46664

Batch 177390, train_perplexity=62.60061

Batch 177400, train_perplexity=57.284054

Batch 177410, train_perplexity=65.601685

Batch 177420, train_perplexity=61.943523

Batch 177430, train_perplexity=67.25506

Batch 177440, train_perplexity=77.5968

Batch 177450, train_perplexity=70.07046

Batch 177460, train_perplexity=65.005066

Batch 177470, train_perplexity=60.078876

Batch 177480, train_perplexity=60.726517

Batch 177490, train_perplexity=64.01398

Batch 177500, train_perplexity=61.39967

Batch 177510, train_perplexity=67.26699

Batch 177520, train_perplexity=68.92703

Batch 177530, train_perplexity=63.001762

Batch 177540, train_perplexity=56.57711

Batch 177550, train_perplexity=56.02507

Batch 177560, train_perplexity=62.673367

Batch 177570, train_perplexity=63.489292

Batch 177580, train_perplexity=70.730835

Batch 177590, train_perplexity=65.16632

Batch 177600, train_perplexity=67.68317

Batch 177610, train_perplexity=59.661415

Batch 177620, train_perplexity=58.45342

Batch 177630, train_perplexity=60.491497

Batch 177640, train_perplexity=66.90028

Batch 177650, train_perplexity=66.83279

Batch 177660, train_perplexity=65.49404

Batch 177670, train_perplexity=56.818645

Batch 177680, train_perplexity=55.754383

Batch 177690, train_perplexity=63.335323

Batch 177700, train_perplexity=64.88986

Batch 177710, train_perplexity=62.035896

Batch 177720, train_perplexity=60.111084

Batch 177730, train_perplexity=62.859768

Batch 177740, train_perplexity=64.92341

Batch 177750, train_perplexity=64.60791

Batch 177760, train_perplexity=67.05278

Batch 177770, train_perplexity=65.0365

Batch 177780, train_perplexity=61.447792

Batch 177790, train_perplexity=56.297123

Batch 177800, train_perplexity=64.10519

Batch 177810, train_perplexity=64.649055

Batch 177820, train_perplexity=63.61827

Batch 177830, train_perplexity=62.78377

Batch 177840, train_perplexity=63.971138

Batch 177850, train_perplexity=68.36533

Batch 177860, train_perplexity=62.31782

Batch 177870, train_perplexity=66.054

Batch 177880, train_perplexity=66.05353

Batch 177890, train_perplexity=59.457417

Batch 177900, train_perplexity=69.15557

Batch 177910, train_perplexity=67.52325

Batch 177920, train_perplexity=65.82582

Batch 177930, train_perplexity=62.895805

Batch 177940, train_perplexity=66.42393

Batch 177950, train_perplexity=72.26468

Batch 177960, train_perplexity=58.948486

Batch 177970, train_perplexity=65.249374

Batch 177980, train_perplexity=61.477802

Batch 177990, train_perplexity=64.62757

Batch 178000, train_perplexity=64.756325

Batch 178010, train_perplexity=66.87075

Batch 178020, train_perplexity=63.57572

Batch 178030, train_perplexity=61.380848

Batch 178040, train_perplexity=59.50697

Batch 178050, train_perplexity=63.12752

Batch 178060, train_perplexity=67.327446

Batch 178070, train_perplexity=61.007523

Batch 178080, train_perplexity=66.71179

Batch 178090, train_perplexity=65.25706

Batch 178100, train_perplexity=63.70846

Batch 178110, train_perplexity=58.693222

Batch 178120, train_perplexity=65.32424

Batch 178130, train_perplexity=61.46168

Batch 178140, train_perplexity=62.59694

Batch 178150, train_perplexity=61.7918

Batch 178160, train_perplexity=63.263187

Batch 178170, train_perplexity=61.342342

Batch 178180, train_perplexity=64.954346

Batch 178190, train_perplexity=70.47652

Batch 178200, train_perplexity=71.150085

Batch 178210, train_perplexity=55.086437

Batch 178220, train_perplexity=63.393215

Batch 178230, train_perplexity=62.7323

Batch 178240, train_perplexity=69.376564

Batch 178250, train_perplexity=61.087574

Batch 178260, train_perplexity=58.228443

Batch 178270, train_perplexity=63.769917

Batch 178280, train_perplexity=70.43092

Batch 178290, train_perplexity=60.719975

Batch 178300, train_perplexity=60.76052

Batch 178310, train_perplexity=59.21816

Batch 178320, train_perplexity=57.078384

Batch 178330, train_perplexity=66.431145

Batch 178340, train_perplexity=59.44279

Batch 178350, train_perplexity=64.0687

Batch 178360, train_perplexity=76.20263

Batch 178370, train_perplexity=56.141052

Batch 178380, train_perplexity=63.03548

Batch 178390, train_perplexity=65.77804

Batch 178400, train_perplexity=75.14717

Batch 178410, train_perplexity=59.030342

Batch 178420, train_perplexity=64.98356

Batch 178430, train_perplexity=64.8324

Batch 178440, train_perplexity=57.080727

Batch 178450, train_perplexity=69.42035

Batch 178460, train_perplexity=57.843723

Batch 178470, train_perplexity=60.94623

Batch 178480, train_perplexity=62.4145

Batch 178490, train_perplexity=68.17964

Batch 178500, train_perplexity=66.01495

Batch 178510, train_perplexity=65.09824

Batch 178520, train_perplexity=71.025406

Batch 178530, train_perplexity=62.10403

Batch 178540, train_perplexity=78.93116

Batch 178550, train_perplexity=64.36766

Batch 178560, train_perplexity=66.5114

Batch 178570, train_perplexity=71.662964

Batch 178580, train_perplexity=63.19556

Batch 178590, train_perplexity=61.21004

Batch 178600, train_perplexity=68.01216

Batch 178610, train_perplexity=64.95803

Batch 178620, train_perplexity=68.63065

Batch 178630, train_perplexity=61.6905

Batch 178640, train_perplexity=66.486984

Batch 178650, train_perplexity=64.763954

Batch 178660, train_perplexity=64.42173

Batch 178670, train_perplexity=62.881233

Batch 178680, train_perplexity=68.14012

Batch 178690, train_perplexity=66.22399

Batch 178700, train_perplexity=64.48566

Batch 178710, train_perplexity=64.509094

Batch 178720, train_perplexity=72.007935

Batch 178730, train_perplexity=63.783905

Batch 178740, train_perplexity=60.912006

Batch 178750, train_perplexity=68.3909

Batch 178760, train_perplexity=66.57496

Batch 178770, train_perplexity=64.63025

Batch 178780, train_perplexity=62.925323

Batch 178790, train_perplexity=66.01779

Batch 178800, train_perplexity=70.17565

Batch 178810, train_perplexity=66.57239

Batch 178820, train_perplexity=67.29301

Batch 178830, train_perplexity=68.145744

Batch 178840, train_perplexity=67.962135

Batch 178850, train_perplexity=61.838257

Batch 178860, train_perplexity=68.83815

Batch 178870, train_perplexity=61.412174

Batch 178880, train_perplexity=65.207634

Batch 178890, train_perplexity=67.00768

Batch 178900, train_perplexity=64.946075

Batch 178910, train_perplexity=66.47133

Batch 178920, train_perplexity=60.42012

Batch 178930, train_perplexity=62.743336

Batch 178940, train_perplexity=66.57527

Batch 178950, train_perplexity=74.69656

Batch 178960, train_perplexity=66.85634

Batch 178970, train_perplexity=65.72113

Batch 178980, train_perplexity=63.92308

Batch 178990, train_perplexity=63.155857

Batch 179000, train_perplexity=65.792274

Batch 179010, train_perplexity=63.478516

Batch 179020, train_perplexity=61.880142

Batch 179030, train_perplexity=67.6974

Batch 179040, train_perplexity=64.298546

Batch 179050, train_perplexity=61.021168

Batch 179060, train_perplexity=59.825962

Batch 179070, train_perplexity=65.72342

Batch 179080, train_perplexity=62.35923
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 179090, train_perplexity=63.321857

Batch 179100, train_perplexity=65.54159

Batch 179110, train_perplexity=64.88819

Batch 179120, train_perplexity=59.089764

Batch 179130, train_perplexity=67.81016

Batch 179140, train_perplexity=57.5623

Batch 179150, train_perplexity=62.290756

Batch 179160, train_perplexity=59.276554

Batch 179170, train_perplexity=58.966816

Batch 179180, train_perplexity=69.561935

Batch 179190, train_perplexity=71.30312

Batch 179200, train_perplexity=64.99496

Batch 179210, train_perplexity=58.65358

Batch 179220, train_perplexity=58.738663

Batch 179230, train_perplexity=58.194027

Batch 179240, train_perplexity=58.998993

Batch 179250, train_perplexity=63.609596

Batch 179260, train_perplexity=58.160515

Batch 179270, train_perplexity=70.733734

Batch 179280, train_perplexity=61.926426

Batch 179290, train_perplexity=61.696117

Batch 179300, train_perplexity=63.026585

Batch 179310, train_perplexity=63.405308

Batch 179320, train_perplexity=60.78756

Batch 179330, train_perplexity=59.81595

Batch 179340, train_perplexity=67.540764

Batch 179350, train_perplexity=58.967663

Batch 179360, train_perplexity=62.524208

Batch 179370, train_perplexity=67.83454

Batch 179380, train_perplexity=66.059105

Batch 179390, train_perplexity=65.6475

Batch 179400, train_perplexity=57.960213

Batch 179410, train_perplexity=66.19848

Batch 179420, train_perplexity=66.56064

Batch 179430, train_perplexity=68.165405

Batch 179440, train_perplexity=59.467087

Batch 179450, train_perplexity=62.684486

Batch 179460, train_perplexity=57.690517

Batch 179470, train_perplexity=63.669678

Batch 179480, train_perplexity=61.18999

Batch 179490, train_perplexity=62.163906

Batch 179500, train_perplexity=63.35653

Batch 179510, train_perplexity=57.17048

Batch 179520, train_perplexity=61.99846

Batch 179530, train_perplexity=62.026787

Batch 179540, train_perplexity=66.822075

Batch 179550, train_perplexity=63.784424

Batch 179560, train_perplexity=60.19194

Batch 179570, train_perplexity=69.362015

Batch 179580, train_perplexity=66.995605

Batch 179590, train_perplexity=58.690285

Batch 179600, train_perplexity=67.74962

Batch 179610, train_perplexity=67.24801

Batch 179620, train_perplexity=62.492405

Batch 179630, train_perplexity=65.592926

Batch 179640, train_perplexity=70.45393

Batch 179650, train_perplexity=67.211815

Batch 179660, train_perplexity=59.339054

Batch 179670, train_perplexity=66.00419

Batch 179680, train_perplexity=65.87772

Batch 179690, train_perplexity=59.56636

Batch 179700, train_perplexity=64.32265

Batch 179710, train_perplexity=63.953938

Batch 179720, train_perplexity=60.209538

Batch 179730, train_perplexity=61.56593

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00018-of-00100
Loaded 306372 sentences.
Finished loading
Batch 179740, train_perplexity=60.05622

Batch 179750, train_perplexity=58.766315

Batch 179760, train_perplexity=64.21692

Batch 179770, train_perplexity=60.766315

Batch 179780, train_perplexity=61.170124

Batch 179790, train_perplexity=57.816536

Batch 179800, train_perplexity=59.736996

Batch 179810, train_perplexity=63.033825

Batch 179820, train_perplexity=61.829117

Batch 179830, train_perplexity=61.746033

Batch 179840, train_perplexity=60.916885

Batch 179850, train_perplexity=62.145947

Batch 179860, train_perplexity=61.209457

Batch 179870, train_perplexity=69.76373

Batch 179880, train_perplexity=64.13992

Batch 179890, train_perplexity=58.818317

Batch 179900, train_perplexity=64.28978

Batch 179910, train_perplexity=57.9724

Batch 179920, train_perplexity=65.34867

Batch 179930, train_perplexity=57.92703

Batch 179940, train_perplexity=58.7803

Batch 179950, train_perplexity=65.3106

Batch 179960, train_perplexity=61.063114

Batch 179970, train_perplexity=62.199875

Batch 179980, train_perplexity=72.85896

Batch 179990, train_perplexity=63.485237

Batch 180000, train_perplexity=65.08987

Batch 180010, train_perplexity=65.31004

Batch 180020, train_perplexity=64.5067

Batch 180030, train_perplexity=62.196136

Batch 180040, train_perplexity=67.95977

Batch 180050, train_perplexity=68.60706

Batch 180060, train_perplexity=61.440174

Batch 180070, train_perplexity=66.348526

Batch 180080, train_perplexity=68.2419

Batch 180090, train_perplexity=66.01118

Batch 180100, train_perplexity=68.43964

Batch 180110, train_perplexity=66.66877

Batch 180120, train_perplexity=65.29055

Batch 180130, train_perplexity=63.41498

Batch 180140, train_perplexity=59.78139

Batch 180150, train_perplexity=61.90328

Batch 180160, train_perplexity=61.596703

Batch 180170, train_perplexity=62.449272

Batch 180180, train_perplexity=61.810635

Batch 180190, train_perplexity=61.718685

Batch 180200, train_perplexity=62.55561

Batch 180210, train_perplexity=60.22154

Batch 180220, train_perplexity=71.543365

Batch 180230, train_perplexity=67.881004

Batch 180240, train_perplexity=68.63769

Batch 180250, train_perplexity=56.562786

Batch 180260, train_perplexity=63.1433

Batch 180270, train_perplexity=69.48317

Batch 180280, train_perplexity=67.31249

Batch 180290, train_perplexity=70.57848

Batch 180300, train_perplexity=64.4433

Batch 180310, train_perplexity=59.630615

Batch 180320, train_perplexity=60.767475

Batch 180330, train_perplexity=54.507442

Batch 180340, train_perplexity=56.904705

Batch 180350, train_perplexity=63.331158

Batch 180360, train_perplexity=69.94681

Batch 180370, train_perplexity=59.688904

Batch 180380, train_perplexity=66.04635

Batch 180390, train_perplexity=64.478584

Batch 180400, train_perplexity=68.62794

Batch 180410, train_perplexity=63.003025

Batch 180420, train_perplexity=58.079758

Batch 180430, train_perplexity=63.248135

Batch 180440, train_perplexity=64.32161

Batch 180450, train_perplexity=67.355774

Batch 180460, train_perplexity=69.232086

Batch 180470, train_perplexity=63.079018

Batch 180480, train_perplexity=61.297665

Batch 180490, train_perplexity=64.34728

Batch 180500, train_perplexity=64.11546

Batch 180510, train_perplexity=57.637394

Batch 180520, train_perplexity=62.131104

Batch 180530, train_perplexity=63.919483

Batch 180540, train_perplexity=70.34665

Batch 180550, train_perplexity=69.54267

Batch 180560, train_perplexity=65.43467

Batch 180570, train_perplexity=65.9243

Batch 180580, train_perplexity=58.534416

Batch 180590, train_perplexity=58.7136

Batch 180600, train_perplexity=67.075745

Batch 180610, train_perplexity=54.980522

Batch 180620, train_perplexity=61.26844

Batch 180630, train_perplexity=64.10354

Batch 180640, train_perplexity=62.398907

Batch 180650, train_perplexity=65.67042

Batch 180660, train_perplexity=64.034004

Batch 180670, train_perplexity=65.34724

Batch 180680, train_perplexity=64.503494

Batch 180690, train_perplexity=65.98852

Batch 180700, train_perplexity=66.335556

Batch 180710, train_perplexity=70.721054

Batch 180720, train_perplexity=56.61967

Batch 180730, train_perplexity=65.783745

Batch 180740, train_perplexity=61.50777

Batch 180750, train_perplexity=63.389618

Batch 180760, train_perplexity=66.35599

Batch 180770, train_perplexity=62.86558

Batch 180780, train_perplexity=61.90511

Batch 180790, train_perplexity=69.63153

Batch 180800, train_perplexity=54.22958

Batch 180810, train_perplexity=57.116745

Batch 180820, train_perplexity=63.754715

Batch 180830, train_perplexity=63.801517

Batch 180840, train_perplexity=62.25738

Batch 180850, train_perplexity=59.00538

Batch 180860, train_perplexity=59.91052

Batch 180870, train_perplexity=55.742527

Batch 180880, train_perplexity=61.851204

Batch 180890, train_perplexity=61.778545

Batch 180900, train_perplexity=61.932037

Batch 180910, train_perplexity=70.98025

Batch 180920, train_perplexity=61.84893

Batch 180930, train_perplexity=64.53451

Batch 180940, train_perplexity=54.66604

Batch 180950, train_perplexity=64.82454

Batch 180960, train_perplexity=66.841866
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 180970, train_perplexity=65.97993

Batch 180980, train_perplexity=57.732677

Batch 180990, train_perplexity=64.86468

Batch 181000, train_perplexity=61.150322

Batch 181010, train_perplexity=68.940506

Batch 181020, train_perplexity=65.46169

Batch 181030, train_perplexity=60.495766

Batch 181040, train_perplexity=65.11451

Batch 181050, train_perplexity=58.997868

Batch 181060, train_perplexity=62.82249

Batch 181070, train_perplexity=65.65113

Batch 181080, train_perplexity=66.344444

Batch 181090, train_perplexity=66.09924

Batch 181100, train_perplexity=61.088215

Batch 181110, train_perplexity=60.22214

Batch 181120, train_perplexity=64.34185

Batch 181130, train_perplexity=61.87967

Batch 181140, train_perplexity=70.218796

Batch 181150, train_perplexity=60.70489

Batch 181160, train_perplexity=62.067646

Batch 181170, train_perplexity=68.70213

Batch 181180, train_perplexity=65.72552

Batch 181190, train_perplexity=64.22543

Batch 181200, train_perplexity=68.87975

Batch 181210, train_perplexity=60.854176

Batch 181220, train_perplexity=63.386776

Batch 181230, train_perplexity=65.06734

Batch 181240, train_perplexity=66.88335

Batch 181250, train_perplexity=58.44815

Batch 181260, train_perplexity=62.996296

Batch 181270, train_perplexity=64.6624

Batch 181280, train_perplexity=70.66944

Batch 181290, train_perplexity=64.11167

Batch 181300, train_perplexity=63.292816

Batch 181310, train_perplexity=64.26189

Batch 181320, train_perplexity=64.45283

Batch 181330, train_perplexity=56.539623

Batch 181340, train_perplexity=57.154507

Batch 181350, train_perplexity=63.65456

Batch 181360, train_perplexity=60.785877

Batch 181370, train_perplexity=65.56522

Batch 181380, train_perplexity=64.596634

Batch 181390, train_perplexity=63.69731

Batch 181400, train_perplexity=63.51388

Batch 181410, train_perplexity=64.87309

Batch 181420, train_perplexity=61.85076

Batch 181430, train_perplexity=70.3219

Batch 181440, train_perplexity=63.084072

Batch 181450, train_perplexity=63.584576

Batch 181460, train_perplexity=66.45418

Batch 181470, train_perplexity=62.23844

Batch 181480, train_perplexity=62.38924

Batch 181490, train_perplexity=59.627487

Batch 181500, train_perplexity=65.12637

Batch 181510, train_perplexity=57.58619

Batch 181520, train_perplexity=60.703125

Batch 181530, train_perplexity=65.3116

Batch 181540, train_perplexity=58.97559

Batch 181550, train_perplexity=62.66787

Batch 181560, train_perplexity=63.932835

Batch 181570, train_perplexity=57.02977

Batch 181580, train_perplexity=66.421585

Batch 181590, train_perplexity=63.927532

Batch 181600, train_perplexity=61.09075

Batch 181610, train_perplexity=60.270004

Batch 181620, train_perplexity=67.77793

Batch 181630, train_perplexity=67.514366

Batch 181640, train_perplexity=61.26634

Batch 181650, train_perplexity=63.51376

Batch 181660, train_perplexity=65.465065

Batch 181670, train_perplexity=59.945354

Batch 181680, train_perplexity=61.854122

Batch 181690, train_perplexity=61.577496

Batch 181700, train_perplexity=70.70578

Batch 181710, train_perplexity=69.06613

Batch 181720, train_perplexity=64.08074

Batch 181730, train_perplexity=64.55279

Batch 181740, train_perplexity=63.70132

Batch 181750, train_perplexity=65.65923

Batch 181760, train_perplexity=66.526054

Batch 181770, train_perplexity=61.1836

Batch 181780, train_perplexity=56.45573

Batch 181790, train_perplexity=56.775475

Batch 181800, train_perplexity=63.56472

Batch 181810, train_perplexity=58.684967

Batch 181820, train_perplexity=64.89023

Batch 181830, train_perplexity=66.42519

Batch 181840, train_perplexity=67.22993

Batch 181850, train_perplexity=61.08315

Batch 181860, train_perplexity=60.208046

Batch 181870, train_perplexity=64.16813

Batch 181880, train_perplexity=68.981155

Batch 181890, train_perplexity=58.143154

Batch 181900, train_perplexity=60.766666

Batch 181910, train_perplexity=66.51394

Batch 181920, train_perplexity=71.85102

Batch 181930, train_perplexity=65.17182

Batch 181940, train_perplexity=64.19433

Batch 181950, train_perplexity=64.58857

Batch 181960, train_perplexity=65.246635

Batch 181970, train_perplexity=65.28962

Batch 181980, train_perplexity=64.49907

Batch 181990, train_perplexity=69.50308

Batch 182000, train_perplexity=60.260376

Batch 182010, train_perplexity=67.79729

Batch 182020, train_perplexity=64.43513

Batch 182030, train_perplexity=61.366184

Batch 182040, train_perplexity=66.44702

Batch 182050, train_perplexity=58.087814

Batch 182060, train_perplexity=60.797676

Batch 182070, train_perplexity=64.00629

Batch 182080, train_perplexity=61.99917

Batch 182090, train_perplexity=67.33323

Batch 182100, train_perplexity=64.52491

Batch 182110, train_perplexity=57.99828

Batch 182120, train_perplexity=61.381783

Batch 182130, train_perplexity=63.55193

Batch 182140, train_perplexity=60.90556

Batch 182150, train_perplexity=61.295853

Batch 182160, train_perplexity=57.672146

Batch 182170, train_perplexity=55.438354

Batch 182180, train_perplexity=66.30209

Batch 182190, train_perplexity=67.48797

Batch 182200, train_perplexity=61.098675

Batch 182210, train_perplexity=65.978134

Batch 182220, train_perplexity=64.41286

Batch 182230, train_perplexity=59.891182

Batch 182240, train_perplexity=61.387756

Batch 182250, train_perplexity=66.62529

Batch 182260, train_perplexity=61.441875

Batch 182270, train_perplexity=68.312805

Batch 182280, train_perplexity=57.01555

Batch 182290, train_perplexity=64.33608

Batch 182300, train_perplexity=63.211502

Batch 182310, train_perplexity=65.27228

Batch 182320, train_perplexity=63.532177

Batch 182330, train_perplexity=66.468094

Batch 182340, train_perplexity=65.53446

Batch 182350, train_perplexity=62.24815

Batch 182360, train_perplexity=66.00051

Batch 182370, train_perplexity=59.88747

Batch 182380, train_perplexity=62.222893

Batch 182390, train_perplexity=64.22286

Batch 182400, train_perplexity=63.473038

Batch 182410, train_perplexity=61.819622

Batch 182420, train_perplexity=62.187447

Batch 182430, train_perplexity=63.81722

Batch 182440, train_perplexity=59.23273

Batch 182450, train_perplexity=61.16881

Batch 182460, train_perplexity=62.865524

Batch 182470, train_perplexity=60.654514

Batch 182480, train_perplexity=68.0335

Batch 182490, train_perplexity=67.37935

Batch 182500, train_perplexity=64.31097

Batch 182510, train_perplexity=63.58297

Batch 182520, train_perplexity=67.58075

Batch 182530, train_perplexity=60.644104

Batch 182540, train_perplexity=60.915028

Batch 182550, train_perplexity=68.39083

Batch 182560, train_perplexity=65.53612

Batch 182570, train_perplexity=67.43181

Batch 182580, train_perplexity=68.219315

Batch 182590, train_perplexity=68.80074

Batch 182600, train_perplexity=60.93987

Batch 182610, train_perplexity=61.961605

Batch 182620, train_perplexity=71.368126

Batch 182630, train_perplexity=70.10508

Batch 182640, train_perplexity=60.671032

Batch 182650, train_perplexity=61.609627

Batch 182660, train_perplexity=62.410038

Batch 182670, train_perplexity=57.57029

Batch 182680, train_perplexity=65.92725

Batch 182690, train_perplexity=61.41393

Batch 182700, train_perplexity=72.89823

Batch 182710, train_perplexity=65.57429

Batch 182720, train_perplexity=63.601345

Batch 182730, train_perplexity=59.590252

Batch 182740, train_perplexity=57.385864

Batch 182750, train_perplexity=65.688705

Batch 182760, train_perplexity=63.51385

Batch 182770, train_perplexity=64.266975

Batch 182780, train_perplexity=62.78284

Batch 182790, train_perplexity=60.09661

Batch 182800, train_perplexity=60.08054

Batch 182810, train_perplexity=59.209324

Batch 182820, train_perplexity=62.280838

Batch 182830, train_perplexity=59.297108

Batch 182840, train_perplexity=58.36827

Batch 182850, train_perplexity=67.93567

Batch 182860, train_perplexity=66.63483

Batch 182870, train_perplexity=61.406433

Batch 182880, train_perplexity=59.43514

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100
Loaded 305032 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00038-of-00100WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 305032 sentences.
Finished loading
Batch 182890, train_perplexity=53.42932

Batch 182900, train_perplexity=67.71929

Batch 182910, train_perplexity=62.34478

Batch 182920, train_perplexity=68.869606

Batch 182930, train_perplexity=61.47675

Batch 182940, train_perplexity=57.085705

Batch 182950, train_perplexity=70.990875

Batch 182960, train_perplexity=66.85347

Batch 182970, train_perplexity=62.180183

Batch 182980, train_perplexity=62.928146

Batch 182990, train_perplexity=58.15824

Batch 183000, train_perplexity=68.38627

Batch 183010, train_perplexity=57.820118

Batch 183020, train_perplexity=61.848637

Batch 183030, train_perplexity=71.165146

Batch 183040, train_perplexity=65.62337

Batch 183050, train_perplexity=58.550243

Batch 183060, train_perplexity=62.989807

Batch 183070, train_perplexity=66.38479

Batch 183080, train_perplexity=64.344246

Batch 183090, train_perplexity=67.162575

Batch 183100, train_perplexity=63.180645

Batch 183110, train_perplexity=74.83714

Batch 183120, train_perplexity=63.67575

Batch 183130, train_perplexity=62.841007

Batch 183140, train_perplexity=59.11913

Batch 183150, train_perplexity=63.148777

Batch 183160, train_perplexity=63.50322

Batch 183170, train_perplexity=64.7899

Batch 183180, train_perplexity=70.57822

Batch 183190, train_perplexity=62.0081

Batch 183200, train_perplexity=64.00351

Batch 183210, train_perplexity=66.07555

Batch 183220, train_perplexity=62.370827

Batch 183230, train_perplexity=60.86445

Batch 183240, train_perplexity=56.03819

Batch 183250, train_perplexity=60.09687

Batch 183260, train_perplexity=64.437004

Batch 183270, train_perplexity=58.616898

Batch 183280, train_perplexity=65.57366

Batch 183290, train_perplexity=60.557587

Batch 183300, train_perplexity=60.76003

Batch 183310, train_perplexity=64.569046

Batch 183320, train_perplexity=70.96562

Batch 183330, train_perplexity=61.234737

Batch 183340, train_perplexity=64.86121

Batch 183350, train_perplexity=66.395775

Batch 183360, train_perplexity=65.872986

Batch 183370, train_perplexity=60.62849

Batch 183380, train_perplexity=62.003605

Batch 183390, train_perplexity=63.226757

Batch 183400, train_perplexity=56.716244

Batch 183410, train_perplexity=59.612816

Batch 183420, train_perplexity=60.205574

Batch 183430, train_perplexity=59.098553

Batch 183440, train_perplexity=62.017857

Batch 183450, train_perplexity=57.48731

Batch 183460, train_perplexity=64.681305

Batch 183470, train_perplexity=64.454796

Batch 183480, train_perplexity=61.70112

Batch 183490, train_perplexity=60.70787

Batch 183500, train_perplexity=63.759304

Batch 183510, train_perplexity=65.55712

Batch 183520, train_perplexity=63.667675

Batch 183530, train_perplexity=65.734955

Batch 183540, train_perplexity=72.46837

Batch 183550, train_perplexity=56.908993

Batch 183560, train_perplexity=68.57403

Batch 183570, train_perplexity=61.187393

Batch 183580, train_perplexity=64.61512

Batch 183590, train_perplexity=62.613804

Batch 183600, train_perplexity=62.468422

Batch 183610, train_perplexity=66.42009

Batch 183620, train_perplexity=60.315514

Batch 183630, train_perplexity=62.876137

Batch 183640, train_perplexity=64.219215

Batch 183650, train_perplexity=68.60078

Batch 183660, train_perplexity=64.27295

Batch 183670, train_perplexity=65.223335

Batch 183680, train_perplexity=67.42364

Batch 183690, train_perplexity=71.17995

Batch 183700, train_perplexity=58.853024

Batch 183710, train_perplexity=70.01762

Batch 183720, train_perplexity=64.002106

Batch 183730, train_perplexity=61.8142

Batch 183740, train_perplexity=59.83375

Batch 183750, train_perplexity=65.43748

Batch 183760, train_perplexity=58.819664

Batch 183770, train_perplexity=57.126495

Batch 183780, train_perplexity=62.305016

Batch 183790, train_perplexity=72.209526

Batch 183800, train_perplexity=66.146164

Batch 183810, train_perplexity=56.99845

Batch 183820, train_perplexity=58.516777

Batch 183830, train_perplexity=56.941784

Batch 183840, train_perplexity=60.622593

Batch 183850, train_perplexity=65.94077

Batch 183860, train_perplexity=69.43279

Batch 183870, train_perplexity=62.57065

Batch 183880, train_perplexity=66.607185

Batch 183890, train_perplexity=61.28381

Batch 183900, train_perplexity=58.07067

Batch 183910, train_perplexity=66.4022

Batch 183920, train_perplexity=65.46775

Batch 183930, train_perplexity=60.098476

Batch 183940, train_perplexity=61.80854

Batch 183950, train_perplexity=59.815067

Batch 183960, train_perplexity=63.50322

Batch 183970, train_perplexity=66.14995

Batch 183980, train_perplexity=68.10283

Batch 183990, train_perplexity=67.56525

Batch 184000, train_perplexity=66.539955

Batch 184010, train_perplexity=65.187454

Batch 184020, train_perplexity=65.93317

Batch 184030, train_perplexity=54.114994

Batch 184040, train_perplexity=65.5801

Batch 184050, train_perplexity=59.224148

Batch 184060, train_perplexity=68.36892

Batch 184070, train_perplexity=61.43701

Batch 184080, train_perplexity=68.57556

Batch 184090, train_perplexity=65.95706

Batch 184100, train_perplexity=61.1514

Batch 184110, train_perplexity=62.074425

Batch 184120, train_perplexity=67.10374

Batch 184130, train_perplexity=63.822636

Batch 184140, train_perplexity=64.252266

Batch 184150, train_perplexity=58.013687

Batch 184160, train_perplexity=61.496098

Batch 184170, train_perplexity=59.84017

Batch 184180, train_perplexity=67.49132

Batch 184190, train_perplexity=60.174236

Batch 184200, train_perplexity=65.80298

Batch 184210, train_perplexity=61.758633

Batch 184220, train_perplexity=62.507904

Batch 184230, train_perplexity=59.48294

Batch 184240, train_perplexity=62.201534

Batch 184250, train_perplexity=63.81448

Batch 184260, train_perplexity=64.93022

Batch 184270, train_perplexity=64.35554

Batch 184280, train_perplexity=65.36656

Batch 184290, train_perplexity=62.795265

Batch 184300, train_perplexity=69.4842

Batch 184310, train_perplexity=61.977947

Batch 184320, train_perplexity=63.593582

Batch 184330, train_perplexity=60.627045

Batch 184340, train_perplexity=71.80848

Batch 184350, train_perplexity=65.71383

Batch 184360, train_perplexity=61.11575

Batch 184370, train_perplexity=63.2917

Batch 184380, train_perplexity=58.735416

Batch 184390, train_perplexity=68.792274

Batch 184400, train_perplexity=68.50017

Batch 184410, train_perplexity=64.500084

Batch 184420, train_perplexity=67.81721

Batch 184430, train_perplexity=63.84537

Batch 184440, train_perplexity=61.031033

Batch 184450, train_perplexity=62.80907

Batch 184460, train_perplexity=61.969376

Batch 184470, train_perplexity=57.982517

Batch 184480, train_perplexity=62.780056

Batch 184490, train_perplexity=60.901234

Batch 184500, train_perplexity=62.14894

Batch 184510, train_perplexity=58.17832

Batch 184520, train_perplexity=57.239437

Batch 184530, train_perplexity=70.89291

Batch 184540, train_perplexity=67.324814

Batch 184550, train_perplexity=64.08963

Batch 184560, train_perplexity=63.987553

Batch 184570, train_perplexity=69.0717

Batch 184580, train_perplexity=58.891144

Batch 184590, train_perplexity=64.85998

Batch 184600, train_perplexity=66.58934

Batch 184610, train_perplexity=65.6316

Batch 184620, train_perplexity=62.601387

Batch 184630, train_perplexity=62.750187

Batch 184640, train_perplexity=64.836845

Batch 184650, train_perplexity=54.80949

Batch 184660, train_perplexity=62.074306

Batch 184670, train_perplexity=64.91223

Batch 184680, train_perplexity=53.539753

Batch 184690, train_perplexity=55.124485

Batch 184700, train_perplexity=67.00359

Batch 184710, train_perplexity=61.345444

Batch 184720, train_perplexity=58.284

Batch 184730, train_perplexity=60.60407

Batch 184740, train_perplexity=62.247852

Batch 184750, train_perplexity=56.550514

Batch 184760, train_perplexity=54.874786

Batch 184770, train_perplexity=59.239227

Batch 184780, train_perplexity=61.893482

Batch 184790, train_perplexity=68.262726

Batch 184800, train_perplexity=67.96256

Batch 184810, train_perplexity=66.55195

Batch 184820, train_perplexity=57.295334

Batch 184830, train_perplexity=61.458515

Batch 184840, train_perplexity=53.056232

Batch 184850, train_perplexity=62.94855
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 184860, train_perplexity=62.803772

Batch 184870, train_perplexity=60.805996

Batch 184880, train_perplexity=68.5686

Batch 184890, train_perplexity=65.24132

Batch 184900, train_perplexity=54.71715

Batch 184910, train_perplexity=62.307243

Batch 184920, train_perplexity=59.01942

Batch 184930, train_perplexity=68.91093

Batch 184940, train_perplexity=59.34644

Batch 184950, train_perplexity=56.270798

Batch 184960, train_perplexity=59.930893

Batch 184970, train_perplexity=56.549274

Batch 184980, train_perplexity=58.503277

Batch 184990, train_perplexity=56.2705

Batch 185000, train_perplexity=61.96663

Batch 185010, train_perplexity=60.707466

Batch 185020, train_perplexity=60.942802

Batch 185030, train_perplexity=70.16802

Batch 185040, train_perplexity=67.938774

Batch 185050, train_perplexity=67.22986

Batch 185060, train_perplexity=64.60511

Batch 185070, train_perplexity=69.8263

Batch 185080, train_perplexity=58.8163

Batch 185090, train_perplexity=63.996155

Batch 185100, train_perplexity=64.65346

Batch 185110, train_perplexity=59.33238

Batch 185120, train_perplexity=64.39449

Batch 185130, train_perplexity=64.85849

Batch 185140, train_perplexity=67.51536

Batch 185150, train_perplexity=62.186855

Batch 185160, train_perplexity=66.85991

Batch 185170, train_perplexity=59.204494

Batch 185180, train_perplexity=63.124092

Batch 185190, train_perplexity=62.9412

Batch 185200, train_perplexity=63.91839

Batch 185210, train_perplexity=72.37572

Batch 185220, train_perplexity=60.064785

Batch 185230, train_perplexity=63.156067

Batch 185240, train_perplexity=66.34188

Batch 185250, train_perplexity=66.813286

Batch 185260, train_perplexity=62.009045

Batch 185270, train_perplexity=63.635624

Batch 185280, train_perplexity=68.46627

Batch 185290, train_perplexity=69.2203

Batch 185300, train_perplexity=56.891087

Batch 185310, train_perplexity=61.788208

Batch 185320, train_perplexity=62.953537

Batch 185330, train_perplexity=64.02961

Batch 185340, train_perplexity=67.60637

Batch 185350, train_perplexity=64.28966

Batch 185360, train_perplexity=56.980732

Batch 185370, train_perplexity=60.665073

Batch 185380, train_perplexity=62.801826

Batch 185390, train_perplexity=63.630558

Batch 185400, train_perplexity=64.30198

Batch 185410, train_perplexity=64.47004

Batch 185420, train_perplexity=60.850346

Batch 185430, train_perplexity=58.48757

Batch 185440, train_perplexity=59.894096

Batch 185450, train_perplexity=56.51628

Batch 185460, train_perplexity=66.580574

Batch 185470, train_perplexity=62.020664

Batch 185480, train_perplexity=53.101475

Batch 185490, train_perplexity=64.52891

Batch 185500, train_perplexity=64.158585

Batch 185510, train_perplexity=61.18585

Batch 185520, train_perplexity=67.10908

Batch 185530, train_perplexity=61.22236

Batch 185540, train_perplexity=66.256805

Batch 185550, train_perplexity=61.956257

Batch 185560, train_perplexity=64.69849

Batch 185570, train_perplexity=64.35376

Batch 185580, train_perplexity=64.48585

Batch 185590, train_perplexity=58.030426

Batch 185600, train_perplexity=65.96518

Batch 185610, train_perplexity=61.754513

Batch 185620, train_perplexity=60.072346

Batch 185630, train_perplexity=62.66784

Batch 185640, train_perplexity=64.79126

Batch 185650, train_perplexity=66.99311

Batch 185660, train_perplexity=62.612732

Batch 185670, train_perplexity=64.48314

Batch 185680, train_perplexity=59.088524

Batch 185690, train_perplexity=60.942917

Batch 185700, train_perplexity=63.09731

Batch 185710, train_perplexity=63.56378

Batch 185720, train_perplexity=66.26799

Batch 185730, train_perplexity=66.718155

Batch 185740, train_perplexity=58.24608

Batch 185750, train_perplexity=61.276154

Batch 185760, train_perplexity=60.5465

Batch 185770, train_perplexity=66.63606

Batch 185780, train_perplexity=58.032143

Batch 185790, train_perplexity=60.053444

Batch 185800, train_perplexity=63.61008

Batch 185810, train_perplexity=62.166485

Batch 185820, train_perplexity=62.499798

Batch 185830, train_perplexity=59.106613

Batch 185840, train_perplexity=61.61039

Batch 185850, train_perplexity=65.58348

Batch 185860, train_perplexity=56.08989

Batch 185870, train_perplexity=57.453465

Batch 185880, train_perplexity=64.39225

Batch 185890, train_perplexity=58.374676

Batch 185900, train_perplexity=59.2724

Batch 185910, train_perplexity=70.18957

Batch 185920, train_perplexity=67.97734

Batch 185930, train_perplexity=65.32104

Batch 185940, train_perplexity=63.576725

Batch 185950, train_perplexity=58.707443

Batch 185960, train_perplexity=59.124035

Batch 185970, train_perplexity=63.996155

Batch 185980, train_perplexity=63.49686

Batch 185990, train_perplexity=64.078415

Batch 186000, train_perplexity=67.10041

Batch 186010, train_perplexity=65.69077

Batch 186020, train_perplexity=64.13252

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00092-of-00100
Loaded 305511 sentences.
Finished loading
Batch 186030, train_perplexity=68.90094

Batch 186040, train_perplexity=61.378914

Batch 186050, train_perplexity=60.937283

Batch 186060, train_perplexity=59.813354

Batch 186070, train_perplexity=60.648907

Batch 186080, train_perplexity=62.39254

Batch 186090, train_perplexity=59.631184

Batch 186100, train_perplexity=61.28381

Batch 186110, train_perplexity=58.086514

Batch 186120, train_perplexity=62.62354

Batch 186130, train_perplexity=61.61518

Batch 186140, train_perplexity=61.86483

Batch 186150, train_perplexity=56.146057

Batch 186160, train_perplexity=58.966763

Batch 186170, train_perplexity=67.91565

Batch 186180, train_perplexity=61.97493

Batch 186190, train_perplexity=66.772804

Batch 186200, train_perplexity=61.96589

Batch 186210, train_perplexity=64.77368

Batch 186220, train_perplexity=58.45746

Batch 186230, train_perplexity=63.54496

Batch 186240, train_perplexity=61.996803

Batch 186250, train_perplexity=64.379845

Batch 186260, train_perplexity=61.769768

Batch 186270, train_perplexity=60.489246

Batch 186280, train_perplexity=63.415165

Batch 186290, train_perplexity=65.09483

Batch 186300, train_perplexity=64.808716

Batch 186310, train_perplexity=66.64054

Batch 186320, train_perplexity=66.88842

Batch 186330, train_perplexity=68.709465

Batch 186340, train_perplexity=65.2926

Batch 186350, train_perplexity=65.13203

Batch 186360, train_perplexity=63.92214

Batch 186370, train_perplexity=66.52574

Batch 186380, train_perplexity=55.82765

Batch 186390, train_perplexity=60.911137

Batch 186400, train_perplexity=66.56769

Batch 186410, train_perplexity=58.67635

Batch 186420, train_perplexity=62.51938

Batch 186430, train_perplexity=63.047382

Batch 186440, train_perplexity=64.20691

Batch 186450, train_perplexity=58.431206

Batch 186460, train_perplexity=60.523064

Batch 186470, train_perplexity=68.066505

Batch 186480, train_perplexity=67.27293

Batch 186490, train_perplexity=64.05166

Batch 186500, train_perplexity=61.979305

Batch 186510, train_perplexity=62.81518

Batch 186520, train_perplexity=61.363113

Batch 186530, train_perplexity=58.83672

Batch 186540, train_perplexity=61.604633

Batch 186550, train_perplexity=57.920895

Batch 186560, train_perplexity=71.43591

Batch 186570, train_perplexity=55.192448

Batch 186580, train_perplexity=64.82862

Batch 186590, train_perplexity=65.740875

Batch 186600, train_perplexity=60.823456

Batch 186610, train_perplexity=63.06777

Batch 186620, train_perplexity=69.07371

Batch 186630, train_perplexity=63.20954

Batch 186640, train_perplexity=62.307213

Batch 186650, train_perplexity=61.174557

Batch 186660, train_perplexity=74.27129

Batch 186670, train_perplexity=63.472614

Batch 186680, train_perplexity=59.14101

Batch 186690, train_perplexity=60.01614

Batch 186700, train_perplexity=65.14638

Batch 186710, train_perplexity=63.003178

Batch 186720, train_perplexity=62.766796

Batch 186730, train_perplexity=60.146923
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 186740, train_perplexity=66.52698

Batch 186750, train_perplexity=59.39162

Batch 186760, train_perplexity=61.90552

Batch 186770, train_perplexity=57.500305

Batch 186780, train_perplexity=72.8285

Batch 186790, train_perplexity=65.86011

Batch 186800, train_perplexity=64.7827

Batch 186810, train_perplexity=71.13285

Batch 186820, train_perplexity=61.368263

Batch 186830, train_perplexity=62.276054

Batch 186840, train_perplexity=59.48538

Batch 186850, train_perplexity=56.565834

Batch 186860, train_perplexity=67.096695

Batch 186870, train_perplexity=56.4141

Batch 186880, train_perplexity=61.99828

Batch 186890, train_perplexity=68.85496

Batch 186900, train_perplexity=68.21164

Batch 186910, train_perplexity=64.936226

Batch 186920, train_perplexity=61.953804

Batch 186930, train_perplexity=60.28561

Batch 186940, train_perplexity=64.47834

Batch 186950, train_perplexity=61.5056

Batch 186960, train_perplexity=58.53545

Batch 186970, train_perplexity=56.815582

Batch 186980, train_perplexity=68.847374

Batch 186990, train_perplexity=56.93353

Batch 187000, train_perplexity=59.318855

Batch 187010, train_perplexity=56.97307

Batch 187020, train_perplexity=61.15038

Batch 187030, train_perplexity=61.415657

Batch 187040, train_perplexity=70.03661

Batch 187050, train_perplexity=60.69583

Batch 187060, train_perplexity=60.922756

Batch 187070, train_perplexity=62.40242

Batch 187080, train_perplexity=67.72059

Batch 187090, train_perplexity=62.15223

Batch 187100, train_perplexity=61.325672

Batch 187110, train_perplexity=63.43821

Batch 187120, train_perplexity=63.834232

Batch 187130, train_perplexity=59.14978

Batch 187140, train_perplexity=66.59925

Batch 187150, train_perplexity=64.36622

Batch 187160, train_perplexity=59.05044

Batch 187170, train_perplexity=62.591774

Batch 187180, train_perplexity=62.86819

Batch 187190, train_perplexity=62.092216

Batch 187200, train_perplexity=54.387287

Batch 187210, train_perplexity=62.83729

Batch 187220, train_perplexity=66.53938

Batch 187230, train_perplexity=57.71806

Batch 187240, train_perplexity=59.865173

Batch 187250, train_perplexity=65.15843

Batch 187260, train_perplexity=62.987434

Batch 187270, train_perplexity=57.954464

Batch 187280, train_perplexity=65.0678

Batch 187290, train_perplexity=65.13843

Batch 187300, train_perplexity=63.95107

Batch 187310, train_perplexity=67.454765

Batch 187320, train_perplexity=66.519775

Batch 187330, train_perplexity=69.76191

Batch 187340, train_perplexity=61.88616

Batch 187350, train_perplexity=61.85153

Batch 187360, train_perplexity=68.08672

Batch 187370, train_perplexity=60.66898

Batch 187380, train_perplexity=67.23044

Batch 187390, train_perplexity=60.012394

Batch 187400, train_perplexity=61.32538

Batch 187410, train_perplexity=63.72192

Batch 187420, train_perplexity=54.677925

Batch 187430, train_perplexity=60.436256

Batch 187440, train_perplexity=65.15315

Batch 187450, train_perplexity=60.405373

Batch 187460, train_perplexity=61.180275

Batch 187470, train_perplexity=68.17467

Batch 187480, train_perplexity=60.326904

Batch 187490, train_perplexity=64.78604

Batch 187500, train_perplexity=62.392006

Batch 187510, train_perplexity=69.638535

Batch 187520, train_perplexity=58.578365

Batch 187530, train_perplexity=66.76408

Batch 187540, train_perplexity=66.3057

Batch 187550, train_perplexity=63.562115

Batch 187560, train_perplexity=62.378025

Batch 187570, train_perplexity=58.061424

Batch 187580, train_perplexity=60.492596

Batch 187590, train_perplexity=59.78253

Batch 187600, train_perplexity=71.12905

Batch 187610, train_perplexity=61.093693

Batch 187620, train_perplexity=71.88673

Batch 187630, train_perplexity=62.11416

Batch 187640, train_perplexity=71.0374

Batch 187650, train_perplexity=62.74507

Batch 187660, train_perplexity=68.044174

Batch 187670, train_perplexity=66.82447

Batch 187680, train_perplexity=62.82291

Batch 187690, train_perplexity=59.956474

Batch 187700, train_perplexity=62.031696

Batch 187710, train_perplexity=64.75139

Batch 187720, train_perplexity=60.98478

Batch 187730, train_perplexity=66.15345

Batch 187740, train_perplexity=64.84525

Batch 187750, train_perplexity=60.610397

Batch 187760, train_perplexity=64.66172

Batch 187770, train_perplexity=63.43428

Batch 187780, train_perplexity=60.740562

Batch 187790, train_perplexity=58.08823

Batch 187800, train_perplexity=55.33514

Batch 187810, train_perplexity=64.39507

Batch 187820, train_perplexity=55.598118

Batch 187830, train_perplexity=62.93823

Batch 187840, train_perplexity=65.61276

Batch 187850, train_perplexity=63.189323

Batch 187860, train_perplexity=67.93671

Batch 187870, train_perplexity=59.04892

Batch 187880, train_perplexity=61.683525

Batch 187890, train_perplexity=60.456203

Batch 187900, train_perplexity=63.96568

Batch 187910, train_perplexity=66.841866

Batch 187920, train_perplexity=64.2621

Batch 187930, train_perplexity=62.2315

Batch 187940, train_perplexity=56.850735

Batch 187950, train_perplexity=67.0958

Batch 187960, train_perplexity=58.629364

Batch 187970, train_perplexity=67.04332

Batch 187980, train_perplexity=63.387352

Batch 187990, train_perplexity=57.838207

Batch 188000, train_perplexity=56.8989

Batch 188010, train_perplexity=67.02903

Batch 188020, train_perplexity=59.258442

Batch 188030, train_perplexity=62.66954

Batch 188040, train_perplexity=62.065277

Batch 188050, train_perplexity=63.708584

Batch 188060, train_perplexity=62.356106

Batch 188070, train_perplexity=65.63526

Batch 188080, train_perplexity=55.6035

Batch 188090, train_perplexity=64.78678

Batch 188100, train_perplexity=62.61703

Batch 188110, train_perplexity=66.8844

Batch 188120, train_perplexity=59.904663

Batch 188130, train_perplexity=67.35696

Batch 188140, train_perplexity=57.90974

Batch 188150, train_perplexity=65.66481

Batch 188160, train_perplexity=58.719284

Batch 188170, train_perplexity=58.987965

Batch 188180, train_perplexity=56.366158

Batch 188190, train_perplexity=56.11001

Batch 188200, train_perplexity=64.524475

Batch 188210, train_perplexity=60.59294

Batch 188220, train_perplexity=65.6753

Batch 188230, train_perplexity=65.88784

Batch 188240, train_perplexity=65.79805

Batch 188250, train_perplexity=60.80643

Batch 188260, train_perplexity=67.743065

Batch 188270, train_perplexity=67.913315

Batch 188280, train_perplexity=61.282932

Batch 188290, train_perplexity=56.487965

Batch 188300, train_perplexity=56.641327

Batch 188310, train_perplexity=60.227226

Batch 188320, train_perplexity=56.86757

Batch 188330, train_perplexity=61.73075

Batch 188340, train_perplexity=67.94448

Batch 188350, train_perplexity=57.263462

Batch 188360, train_perplexity=58.329376

Batch 188370, train_perplexity=62.635784

Batch 188380, train_perplexity=62.904503

Batch 188390, train_perplexity=71.05007

Batch 188400, train_perplexity=64.458664

Batch 188410, train_perplexity=64.547554

Batch 188420, train_perplexity=62.61285

Batch 188430, train_perplexity=63.032593

Batch 188440, train_perplexity=67.19884

Batch 188450, train_perplexity=65.28566

Batch 188460, train_perplexity=71.67831

Batch 188470, train_perplexity=56.0103

Batch 188480, train_perplexity=55.151093

Batch 188490, train_perplexity=67.49013

Batch 188500, train_perplexity=56.723248

Batch 188510, train_perplexity=55.960484

Batch 188520, train_perplexity=59.703392

Batch 188530, train_perplexity=60.44712

Batch 188540, train_perplexity=67.82665

Batch 188550, train_perplexity=63.33883

Batch 188560, train_perplexity=56.91404

Batch 188570, train_perplexity=65.14066

Batch 188580, train_perplexity=65.91965

Batch 188590, train_perplexity=67.03395

Batch 188600, train_perplexity=65.45426

Batch 188610, train_perplexity=62.597027

Batch 188620, train_perplexity=60.874027

Batch 188630, train_perplexity=63.521393

Batch 188640, train_perplexity=58.47335

Batch 188650, train_perplexity=62.07312

Batch 188660, train_perplexity=62.77012

Batch 188670, train_perplexity=62.184303

Batch 188680, train_perplexity=62.77964

Batch 188690, train_perplexity=59.7552

Batch 188700, train_perplexity=62.67325

Batch 188710, train_perplexity=62.821594

Batch 188720, train_perplexity=63.214878
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 188730, train_perplexity=69.71259

Batch 188740, train_perplexity=65.522964

Batch 188750, train_perplexity=63.802856

Batch 188760, train_perplexity=62.134186

Batch 188770, train_perplexity=62.80913

Batch 188780, train_perplexity=67.524086

Batch 188790, train_perplexity=61.407078

Batch 188800, train_perplexity=64.172226

Batch 188810, train_perplexity=67.30595

Batch 188820, train_perplexity=64.50122

Batch 188830, train_perplexity=64.01373

Batch 188840, train_perplexity=60.960648

Batch 188850, train_perplexity=67.18775

Batch 188860, train_perplexity=63.891663

Batch 188870, train_perplexity=56.679234

Batch 188880, train_perplexity=61.93765

Batch 188890, train_perplexity=67.90328

Batch 188900, train_perplexity=58.36755

Batch 188910, train_perplexity=60.021263

Batch 188920, train_perplexity=64.15534

Batch 188930, train_perplexity=56.600155

Batch 188940, train_perplexity=66.47095

Batch 188950, train_perplexity=65.00624

Batch 188960, train_perplexity=62.01407

Batch 188970, train_perplexity=62.983833

Batch 188980, train_perplexity=60.826904

Batch 188990, train_perplexity=65.23973

Batch 189000, train_perplexity=62.970978

Batch 189010, train_perplexity=58.27972

Batch 189020, train_perplexity=66.269035

Batch 189030, train_perplexity=62.637577

Batch 189040, train_perplexity=64.62659

Batch 189050, train_perplexity=59.701942

Batch 189060, train_perplexity=62.388973

Batch 189070, train_perplexity=65.9475

Batch 189080, train_perplexity=68.5858

Batch 189090, train_perplexity=58.57454

Batch 189100, train_perplexity=62.726288

Batch 189110, train_perplexity=68.328835

Batch 189120, train_perplexity=64.613396

Batch 189130, train_perplexity=61.869373

Batch 189140, train_perplexity=62.659023

Batch 189150, train_perplexity=63.75216

Batch 189160, train_perplexity=57.642094

Batch 189170, train_perplexity=65.53868

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00031-of-00100
Loaded 306259 sentences.
Finished loading
Batch 189180, train_perplexity=59.413773

Batch 189190, train_perplexity=55.944263

Batch 189200, train_perplexity=61.299652

Batch 189210, train_perplexity=59.041374

Batch 189220, train_perplexity=61.660767

Batch 189230, train_perplexity=59.408504

Batch 189240, train_perplexity=66.241295

Batch 189250, train_perplexity=63.87159

Batch 189260, train_perplexity=64.26005

Batch 189270, train_perplexity=56.94037

Batch 189280, train_perplexity=61.39642

Batch 189290, train_perplexity=72.22361

Batch 189300, train_perplexity=59.28272

Batch 189310, train_perplexity=61.292637

Batch 189320, train_perplexity=66.39815

Batch 189330, train_perplexity=64.93149

Batch 189340, train_perplexity=70.81392

Batch 189350, train_perplexity=62.203465

Batch 189360, train_perplexity=55.92015

Batch 189370, train_perplexity=60.977276

Batch 189380, train_perplexity=55.962353

Batch 189390, train_perplexity=66.806145

Batch 189400, train_perplexity=54.209

Batch 189410, train_perplexity=62.28149

Batch 189420, train_perplexity=52.111336

Batch 189430, train_perplexity=61.734196

Batch 189440, train_perplexity=57.488514

Batch 189450, train_perplexity=55.169346

Batch 189460, train_perplexity=68.03389

Batch 189470, train_perplexity=67.84182

Batch 189480, train_perplexity=57.60129

Batch 189490, train_perplexity=65.62553

Batch 189500, train_perplexity=57.337505

Batch 189510, train_perplexity=63.7108

Batch 189520, train_perplexity=65.07255

Batch 189530, train_perplexity=59.39151

Batch 189540, train_perplexity=61.752216

Batch 189550, train_perplexity=63.927532

Batch 189560, train_perplexity=61.705414

Batch 189570, train_perplexity=67.88207

Batch 189580, train_perplexity=63.192276

Batch 189590, train_perplexity=63.737114

Batch 189600, train_perplexity=61.378975

Batch 189610, train_perplexity=61.840263

Batch 189620, train_perplexity=61.124317

Batch 189630, train_perplexity=62.863846

Batch 189640, train_perplexity=61.569424

Batch 189650, train_perplexity=68.21154

Batch 189660, train_perplexity=64.56242

Batch 189670, train_perplexity=67.68778

Batch 189680, train_perplexity=61.628315

Batch 189690, train_perplexity=66.90456

Batch 189700, train_perplexity=64.95193

Batch 189710, train_perplexity=64.220474

Batch 189720, train_perplexity=64.02897

Batch 189730, train_perplexity=55.79348

Batch 189740, train_perplexity=66.42076

Batch 189750, train_perplexity=67.97293

Batch 189760, train_perplexity=72.60281

Batch 189770, train_perplexity=63.925034

Batch 189780, train_perplexity=69.9828

Batch 189790, train_perplexity=62.910625

Batch 189800, train_perplexity=67.85046

Batch 189810, train_perplexity=63.456303

Batch 189820, train_perplexity=64.83654

Batch 189830, train_perplexity=61.738964

Batch 189840, train_perplexity=61.10832

Batch 189850, train_perplexity=63.073753

Batch 189860, train_perplexity=62.615147

Batch 189870, train_perplexity=59.036983

Batch 189880, train_perplexity=60.5151

Batch 189890, train_perplexity=64.82788

Batch 189900, train_perplexity=64.3064

Batch 189910, train_perplexity=63.764656

Batch 189920, train_perplexity=64.67495

Batch 189930, train_perplexity=55.95061

Batch 189940, train_perplexity=62.56859

Batch 189950, train_perplexity=64.74509

Batch 189960, train_perplexity=59.867256

Batch 189970, train_perplexity=57.803967

Batch 189980, train_perplexity=56.727604

Batch 189990, train_perplexity=69.7847

Batch 190000, train_perplexity=68.85578

Batch 190010, train_perplexity=59.820915

Batch 190020, train_perplexity=63.38028

Batch 190030, train_perplexity=66.253456

Batch 190040, train_perplexity=65.30089

Batch 190050, train_perplexity=62.460262

Batch 190060, train_perplexity=63.602257

Batch 190070, train_perplexity=65.626656

Batch 190080, train_perplexity=58.297123

Batch 190090, train_perplexity=65.4259

Batch 190100, train_perplexity=64.08184

Batch 190110, train_perplexity=59.45895

Batch 190120, train_perplexity=59.210423

Batch 190130, train_perplexity=64.94322

Batch 190140, train_perplexity=66.83642

Batch 190150, train_perplexity=60.46119

Batch 190160, train_perplexity=64.43562

Batch 190170, train_perplexity=63.909367

Batch 190180, train_perplexity=58.77478

Batch 190190, train_perplexity=65.77032

Batch 190200, train_perplexity=73.032745

Batch 190210, train_perplexity=58.499565

Batch 190220, train_perplexity=60.828472

Batch 190230, train_perplexity=63.10733

Batch 190240, train_perplexity=62.037315

Batch 190250, train_perplexity=63.482815

Batch 190260, train_perplexity=64.245094

Batch 190270, train_perplexity=58.33149

Batch 190280, train_perplexity=61.28051

Batch 190290, train_perplexity=65.10973

Batch 190300, train_perplexity=53.788376

Batch 190310, train_perplexity=57.27288

Batch 190320, train_perplexity=56.149513

Batch 190330, train_perplexity=57.586353

Batch 190340, train_perplexity=57.727226

Batch 190350, train_perplexity=62.975754

Batch 190360, train_perplexity=59.01534

Batch 190370, train_perplexity=63.260532

Batch 190380, train_perplexity=65.75401

Batch 190390, train_perplexity=61.32994

Batch 190400, train_perplexity=67.774925

Batch 190410, train_perplexity=60.87548

Batch 190420, train_perplexity=65.82588

Batch 190430, train_perplexity=61.357613

Batch 190440, train_perplexity=60.39731

Batch 190450, train_perplexity=63.801945

Batch 190460, train_perplexity=61.873768

Batch 190470, train_perplexity=68.251724

Batch 190480, train_perplexity=55.97655

Batch 190490, train_perplexity=63.641632

Batch 190500, train_perplexity=60.282246

Batch 190510, train_perplexity=60.31218

Batch 190520, train_perplexity=68.69951

Batch 190530, train_perplexity=59.870052

Batch 190540, train_perplexity=64.339584

Batch 190550, train_perplexity=61.198017

Batch 190560, train_perplexity=61.744617

Batch 190570, train_perplexity=66.862335

Batch 190580, train_perplexity=65.9398

Batch 190590, train_perplexity=60.871475

Batch 190600, train_perplexity=55.22293
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 190610, train_perplexity=62.450165

Batch 190620, train_perplexity=61.418705

Batch 190630, train_perplexity=57.38288

Batch 190640, train_perplexity=57.382034

Batch 190650, train_perplexity=63.640114

Batch 190660, train_perplexity=63.432343

Batch 190670, train_perplexity=61.4676

Batch 190680, train_perplexity=65.72041

Batch 190690, train_perplexity=68.31134

Batch 190700, train_perplexity=67.08457

Batch 190710, train_perplexity=56.48659

Batch 190720, train_perplexity=55.775867

Batch 190730, train_perplexity=57.77043

Batch 190740, train_perplexity=65.44228

Batch 190750, train_perplexity=61.402744

Batch 190760, train_perplexity=61.17625

Batch 190770, train_perplexity=58.388092

Batch 190780, train_perplexity=66.62285

Batch 190790, train_perplexity=57.92581

Batch 190800, train_perplexity=65.534966

Batch 190810, train_perplexity=66.055824

Batch 190820, train_perplexity=61.314327

Batch 190830, train_perplexity=58.096153

Batch 190840, train_perplexity=60.9263

Batch 190850, train_perplexity=60.16313

Batch 190860, train_perplexity=66.026764

Batch 190870, train_perplexity=59.98327

Batch 190880, train_perplexity=60.76458

Batch 190890, train_perplexity=62.79859

Batch 190900, train_perplexity=72.22134

Batch 190910, train_perplexity=64.36382

Batch 190920, train_perplexity=63.15173

Batch 190930, train_perplexity=60.894875

Batch 190940, train_perplexity=61.46508

Batch 190950, train_perplexity=58.32615

Batch 190960, train_perplexity=62.86885

Batch 190970, train_perplexity=53.925484

Batch 190980, train_perplexity=62.161625

Batch 190990, train_perplexity=66.273834

Batch 191000, train_perplexity=62.41745

Batch 191010, train_perplexity=59.57133

Batch 191020, train_perplexity=58.216396

Batch 191030, train_perplexity=57.081352

Batch 191040, train_perplexity=66.945854

Batch 191050, train_perplexity=60.226536

Batch 191060, train_perplexity=64.21961

Batch 191070, train_perplexity=62.17105

Batch 191080, train_perplexity=59.150063

Batch 191090, train_perplexity=65.183975

Batch 191100, train_perplexity=61.883034

Batch 191110, train_perplexity=73.06869

Batch 191120, train_perplexity=58.302624

Batch 191130, train_perplexity=59.782074

Batch 191140, train_perplexity=63.36257

Batch 191150, train_perplexity=64.46967

Batch 191160, train_perplexity=66.70406

Batch 191170, train_perplexity=58.986305

Batch 191180, train_perplexity=65.0656

Batch 191190, train_perplexity=59.305393

Batch 191200, train_perplexity=60.986755

Batch 191210, train_perplexity=61.96178

Batch 191220, train_perplexity=57.81317

Batch 191230, train_perplexity=67.466286

Batch 191240, train_perplexity=61.44905

Batch 191250, train_perplexity=62.681915

Batch 191260, train_perplexity=64.87476

Batch 191270, train_perplexity=60.376057

Batch 191280, train_perplexity=66.89894

Batch 191290, train_perplexity=62.03019

Batch 191300, train_perplexity=61.67394

Batch 191310, train_perplexity=61.220257

Batch 191320, train_perplexity=64.009796

Batch 191330, train_perplexity=61.098003

Batch 191340, train_perplexity=56.650753

Batch 191350, train_perplexity=57.807163

Batch 191360, train_perplexity=67.993225

Batch 191370, train_perplexity=63.794003

Batch 191380, train_perplexity=56.720787

Batch 191390, train_perplexity=69.3422

Batch 191400, train_perplexity=63.230915

Batch 191410, train_perplexity=61.40421

Batch 191420, train_perplexity=64.276566

Batch 191430, train_perplexity=64.047806

Batch 191440, train_perplexity=61.05988

Batch 191450, train_perplexity=57.11337

Batch 191460, train_perplexity=63.563084

Batch 191470, train_perplexity=61.984035

Batch 191480, train_perplexity=62.06519

Batch 191490, train_perplexity=72.040146

Batch 191500, train_perplexity=72.3436

Batch 191510, train_perplexity=66.71755

Batch 191520, train_perplexity=63.565083

Batch 191530, train_perplexity=60.489246

Batch 191540, train_perplexity=58.224586

Batch 191550, train_perplexity=60.942715

Batch 191560, train_perplexity=58.34907

Batch 191570, train_perplexity=61.182056

Batch 191580, train_perplexity=57.32414

Batch 191590, train_perplexity=72.536026

Batch 191600, train_perplexity=59.791256

Batch 191610, train_perplexity=57.92932

Batch 191620, train_perplexity=71.67093

Batch 191630, train_perplexity=60.45695

Batch 191640, train_perplexity=62.545948

Batch 191650, train_perplexity=60.050922

Batch 191660, train_perplexity=57.17702

Batch 191670, train_perplexity=70.03391

Batch 191680, train_perplexity=63.585274

Batch 191690, train_perplexity=55.437878

Batch 191700, train_perplexity=59.793736

Batch 191710, train_perplexity=68.7389

Batch 191720, train_perplexity=60.98103

Batch 191730, train_perplexity=68.12927

Batch 191740, train_perplexity=58.4965

Batch 191750, train_perplexity=63.16016

Batch 191760, train_perplexity=65.698105

Batch 191770, train_perplexity=56.682583

Batch 191780, train_perplexity=59.456963

Batch 191790, train_perplexity=61.44293

Batch 191800, train_perplexity=59.938496

Batch 191810, train_perplexity=59.702198

Batch 191820, train_perplexity=63.37928

Batch 191830, train_perplexity=65.378716

Batch 191840, train_perplexity=61.15012

Batch 191850, train_perplexity=68.104324

Batch 191860, train_perplexity=57.005245

Batch 191870, train_perplexity=59.409126

Batch 191880, train_perplexity=62.900185

Batch 191890, train_perplexity=60.1955

Batch 191900, train_perplexity=58.749645

Batch 191910, train_perplexity=63.40289

Batch 191920, train_perplexity=64.76041

Batch 191930, train_perplexity=61.953243

Batch 191940, train_perplexity=66.67901

Batch 191950, train_perplexity=61.77454

Batch 191960, train_perplexity=63.544174

Batch 191970, train_perplexity=61.296524

Batch 191980, train_perplexity=59.950127

Batch 191990, train_perplexity=69.40744

Batch 192000, train_perplexity=67.113846

Batch 192010, train_perplexity=70.662834

Batch 192020, train_perplexity=60.60268

Batch 192030, train_perplexity=57.58358

Batch 192040, train_perplexity=62.7393

Batch 192050, train_perplexity=65.315895

Batch 192060, train_perplexity=63.595642

Batch 192070, train_perplexity=59.881332

Batch 192080, train_perplexity=61.619087

Batch 192090, train_perplexity=54.306667

Batch 192100, train_perplexity=59.831867

Batch 192110, train_perplexity=59.564457

Batch 192120, train_perplexity=65.17332

Batch 192130, train_perplexity=64.49722

Batch 192140, train_perplexity=65.15396

Batch 192150, train_perplexity=69.327324

Batch 192160, train_perplexity=61.787678

Batch 192170, train_perplexity=66.8888

Batch 192180, train_perplexity=66.32322

Batch 192190, train_perplexity=62.819614

Batch 192200, train_perplexity=54.931465

Batch 192210, train_perplexity=63.106575

Batch 192220, train_perplexity=61.046635

Batch 192230, train_perplexity=62.872807

Batch 192240, train_perplexity=60.022438

Batch 192250, train_perplexity=56.568153

Batch 192260, train_perplexity=64.29695

Batch 192270, train_perplexity=59.263866

Batch 192280, train_perplexity=59.932377

Batch 192290, train_perplexity=69.87933

Batch 192300, train_perplexity=62.175262

Batch 192310, train_perplexity=55.860455

Batch 192320, train_perplexity=65.37747

Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Loading data from: /docker/Upgrade_ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00054-of-00100
Loaded 306524 sentences.
Finished loading
Batch 192330, train_perplexity=59.34109

Batch 192340, train_perplexity=62.88789

Batch 192350, train_perplexity=60.635025

Batch 192360, train_perplexity=63.048588

Batch 192370, train_perplexity=62.07318

Batch 192380, train_perplexity=59.112366

Batch 192390, train_perplexity=64.09938

Batch 192400, train_perplexity=62.67077

Batch 192410, train_perplexity=57.70942

Batch 192420, train_perplexity=60.584564

Batch 192430, train_perplexity=59.149754

Batch 192440, train_perplexity=60.701157

Batch 192450, train_perplexity=59.348988

Batch 192460, train_perplexity=58.543713

Batch 192470, train_perplexity=65.58698

Batch 192480, train_perplexity=67.16434
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

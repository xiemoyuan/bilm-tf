nohup: ignoring input
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /docker/ELMo/bilm-tf/bilm/training.py:228: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.
WARNING:tensorflow:From /docker/py36-tf1.9/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2020-10-29 13:27:07.864804: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-29 13:27:08.061060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0d:00.0
totalMemory: 22.38GiB freeMemory: 21.80GiB
2020-10-29 13:27:08.061115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2020-10-29 13:27:08.606565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 13:27:08.606623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2020-10-29 13:27:08.606634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2020-10-29 13:27:08.608034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21153 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:0d:00.0, compute capability: 6.1)
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Found 51 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Found 51 shards at /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/*
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading

ids:  (128, 20, 50)

char_embeding:  (128, 20, 50, 16)

ids_reverse:  (128, 20, 50)

char_embeding_reverse:  (128, 20, 50, 16)

after conv:  (128, 20, 50, 32)
after pool:  (128, 20, 1, 32)
after squeeze:  (128, 20, 32)

after conv:  (128, 20, 49, 32)
after pool:  (128, 20, 1, 32)
after squeeze:  (128, 20, 32)

after conv:  (128, 20, 48, 64)
after pool:  (128, 20, 1, 64)
after squeeze:  (128, 20, 64)

after conv:  (128, 20, 47, 128)
after pool:  (128, 20, 1, 128)
after squeeze:  (128, 20, 128)

after conv:  (128, 20, 46, 256)
after pool:  (128, 20, 1, 256)
after squeeze:  (128, 20, 256)

after conv:  (128, 20, 45, 512)
after pool:  (128, 20, 1, 512)
after squeeze:  (128, 20, 512)

after conv:  (128, 20, 44, 1024)
after pool:  (128, 20, 1, 1024)
after squeeze:  (128, 20, 1024)

after cnn:  (128, 20, 2048)

after conv:  (128, 20, 50, 32)
after pool:  (128, 20, 1, 32)
after squeeze:  (128, 20, 32)

after conv:  (128, 20, 49, 32)
after pool:  (128, 20, 1, 32)
after squeeze:  (128, 20, 32)

after conv:  (128, 20, 48, 64)
after pool:  (128, 20, 1, 64)
after squeeze:  (128, 20, 64)

after conv:  (128, 20, 47, 128)
after pool:  (128, 20, 1, 128)
after squeeze:  (128, 20, 128)

after conv:  (128, 20, 46, 256)
after pool:  (128, 20, 1, 256)
after squeeze:  (128, 20, 256)

after conv:  (128, 20, 45, 512)
after pool:  (128, 20, 1, 512)
after squeeze:  (128, 20, 512)

after conv:  (128, 20, 44, 1024)
after pool:  (128, 20, 1, 1024)
after squeeze:  (128, 20, 1024)
after cnn reverse:  (128, 20, 2048)

use_highway:  False
use_proj:  True

before highway&proj:  (2560, 2048)
before highway&proj:  (2560, 2048)

after proj:  (2560, 512)
after proj:  (2560, 512)

token_embedding:  (128, 20, 512)
token_embedding_reverse:  (128, 20, 512)

no cell_clip, no proj.

_lstm_output_unpacked:  [<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_2:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_5:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_8:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_11:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_14:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_17:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_20:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_23:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_26:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_29:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_32:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_35:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_38:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_41:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_44:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_47:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_50:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_53:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_56:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>]
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_0/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)

lstm_output_flat:  (2560, 512)

no cell_clip, no proj.

_lstm_output_unpacked:  [<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_2:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_5:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_8:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_11:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_14:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_17:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_20:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_23:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_26:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_29:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_32:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_35:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_38:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_41:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_44:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_47:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_50:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_53:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_56:0' shape=(128, 512) dtype=float32>, <tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>]
final_state:  LSTMStateTuple(c=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/add_39:0' shape=(128, 512) dtype=float32>, h=<tf.Tensor 'lm/RNN_1/rnn/lstm_cell/mul_59:0' shape=(128, 512) dtype=float32>)

lstm_output_flat:  (2560, 512)

next_token_id_flat: (2560, 1)
output_scores:  (2560, 150000)
losses:  (2560,)

next_token_id_flat: (2560, 1)
output_scores:  (2560, 150000)
losses:  (2560,)

total_loss:  ()
[['global_step:0', TensorShape([])],
 ['lm/CNN/W_cnn_0:0',
  TensorShape([Dimension(1), Dimension(1), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_1:0',
  TensorShape([Dimension(1), Dimension(2), Dimension(16), Dimension(32)])],
 ['lm/CNN/W_cnn_2:0',
  TensorShape([Dimension(1), Dimension(3), Dimension(16), Dimension(64)])],
 ['lm/CNN/W_cnn_3:0',
  TensorShape([Dimension(1), Dimension(4), Dimension(16), Dimension(128)])],
 ['lm/CNN/W_cnn_4:0',
  TensorShape([Dimension(1), Dimension(5), Dimension(16), Dimension(256)])],
 ['lm/CNN/W_cnn_5:0',
  TensorShape([Dimension(1), Dimension(6), Dimension(16), Dimension(512)])],
 ['lm/CNN/W_cnn_6:0',
  TensorShape([Dimension(1), Dimension(7), Dimension(16), Dimension(1024)])],
 ['lm/CNN/b_cnn_0:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_1:0', TensorShape([Dimension(32)])],
 ['lm/CNN/b_cnn_2:0', TensorShape([Dimension(64)])],
 ['lm/CNN/b_cnn_3:0', TensorShape([Dimension(128)])],
 ['lm/CNN/b_cnn_4:0', TensorShape([Dimension(256)])],
 ['lm/CNN/b_cnn_5:0', TensorShape([Dimension(512)])],
 ['lm/CNN/b_cnn_6:0', TensorShape([Dimension(1024)])],
 ['lm/CNN_proj/W_proj:0', TensorShape([Dimension(2048), Dimension(512)])],
 ['lm/CNN_proj/b_proj:0', TensorShape([Dimension(512)])],
 ['lm/RNN_0/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_0/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/bias:0', TensorShape([Dimension(2048)])],
 ['lm/RNN_1/rnn/lstm_cell/kernel:0',
  TensorShape([Dimension(1024), Dimension(2048)])],
 ['lm/char_embed:0', TensorShape([Dimension(261), Dimension(16)])],
 ['lm/softmax/W:0', TensorShape([Dimension(150000), Dimension(512)])],
 ['lm/softmax/b:0', TensorShape([Dimension(150000)])],
 ['train_loss:0', TensorShape([])],
 ['train_perplexity:0', TensorShape([])]]
Training for 10 epochs and 3002530 batches
Batch 0, train_perplexity=149999.81, train_loss=11.918389

Batch 10, train_perplexity=143048.53, train_loss=11.870939

Batch 20, train_perplexity=135910.36, train_loss=11.819751

Batch 30, train_perplexity=129556.02, train_loss=11.771869

Batch 40, train_perplexity=124415.06, train_loss=11.731379

Batch 50, train_perplexity=117588.64, train_loss=11.674948

Batch 60, train_perplexity=113496.4, train_loss=11.639526

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 70, train_perplexity=109044.586, train_loss=11.599512

Batch 80, train_perplexity=103298.76, train_loss=11.545381

Batch 90, train_perplexity=97081.3, train_loss=11.483304

Batch 100, train_perplexity=93117.46, train_loss=11.441617

Batch 110, train_perplexity=90040.625, train_loss=11.408016

Batch 120, train_perplexity=83508.89, train_loss=11.332708

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 130, train_perplexity=80102.73, train_loss=11.291065

Batch 140, train_perplexity=77018.18, train_loss=11.251797

Batch 150, train_perplexity=75938.89, train_loss=11.237684

Batch 160, train_perplexity=70818.75, train_loss=11.167879

Batch 170, train_perplexity=67674.266, train_loss=11.122461

Batch 180, train_perplexity=64632.043, train_loss=11.076466

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 190, train_perplexity=59781.426, train_loss=10.99845

Batch 200, train_perplexity=58708.008, train_loss=10.980331

Batch 210, train_perplexity=56249.395, train_loss=10.937551

Batch 220, train_perplexity=53096.26, train_loss=10.879862

Batch 230, train_perplexity=49562.293, train_loss=10.810986

Batch 240, train_perplexity=49361.91, train_loss=10.806934

Batch 250, train_perplexity=44234.902, train_loss=10.697269

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 260, train_perplexity=41488.125, train_loss=10.6331625

Batch 270, train_perplexity=42407.066, train_loss=10.65507

Batch 280, train_perplexity=39681.38, train_loss=10.588637

Batch 290, train_perplexity=38285.406, train_loss=10.552824

Batch 300, train_perplexity=33685.406, train_loss=10.42482

Batch 310, train_perplexity=37442.992, train_loss=10.530575

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 320, train_perplexity=34954.0, train_loss=10.461788

Batch 330, train_perplexity=33679.75, train_loss=10.424652

Batch 340, train_perplexity=31779.21, train_loss=10.366568

Batch 350, train_perplexity=32486.451, train_loss=10.388578

Batch 360, train_perplexity=30125.852, train_loss=10.313139

Batch 370, train_perplexity=27752.23, train_loss=10.231071

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 380, train_perplexity=27948.242, train_loss=10.23811

Batch 390, train_perplexity=28825.791, train_loss=10.269026

Batch 400, train_perplexity=25632.615, train_loss=10.151621

Batch 410, train_perplexity=25841.096, train_loss=10.159721

Batch 420, train_perplexity=24233.041, train_loss=10.095472

Batch 430, train_perplexity=24383.912, train_loss=10.101679

Batch 440, train_perplexity=23728.416, train_loss=10.074429

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 450, train_perplexity=22468.547, train_loss=10.019872

Batch 460, train_perplexity=21436.918, train_loss=9.97287

Batch 470, train_perplexity=22476.135, train_loss=10.020209

Batch 480, train_perplexity=22355.629, train_loss=10.014833

Batch 490, train_perplexity=23167.564, train_loss=10.0505085

Batch 500, train_perplexity=21309.484, train_loss=9.9669075

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 510, train_perplexity=19214.28, train_loss=9.863409

Batch 520, train_perplexity=21760.094, train_loss=9.987833

Batch 530, train_perplexity=18815.098, train_loss=9.842415

Batch 540, train_perplexity=18726.629, train_loss=9.837702

Batch 550, train_perplexity=19586.82, train_loss=9.882612

Batch 560, train_perplexity=17099.951, train_loss=9.746831

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 570, train_perplexity=16529.912, train_loss=9.712927

Batch 580, train_perplexity=18971.312, train_loss=9.850683

Batch 590, train_perplexity=18655.293, train_loss=9.833885

Batch 600, train_perplexity=17137.223, train_loss=9.749008

Batch 610, train_perplexity=16413.34, train_loss=9.70585

Batch 620, train_perplexity=16720.582, train_loss=9.724396

Batch 630, train_perplexity=16976.607, train_loss=9.739592

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 640, train_perplexity=17412.309, train_loss=9.764933

Batch 650, train_perplexity=17573.092, train_loss=9.774124

Batch 660, train_perplexity=16014.114, train_loss=9.681226

Batch 670, train_perplexity=14575.47, train_loss=9.587095

Batch 680, train_perplexity=14424.88, train_loss=9.57671

Batch 690, train_perplexity=16183.966, train_loss=9.691776

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 700, train_perplexity=16714.35, train_loss=9.724023

Batch 710, train_perplexity=15169.857, train_loss=9.627066

Batch 720, train_perplexity=14982.574, train_loss=9.614643

Batch 730, train_perplexity=14333.305, train_loss=9.570341

Batch 740, train_perplexity=14312.174, train_loss=9.568866

Batch 750, train_perplexity=14102.087, train_loss=9.554078

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 760, train_perplexity=13729.129, train_loss=9.527275

Batch 770, train_perplexity=14487.0, train_loss=9.581007

Batch 780, train_perplexity=13338.391, train_loss=9.498402

Batch 790, train_perplexity=13071.337, train_loss=9.478177

Batch 800, train_perplexity=14553.552, train_loss=9.58559

Batch 810, train_perplexity=13894.224, train_loss=9.539228

Batch 820, train_perplexity=13610.523, train_loss=9.518599

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 830, train_perplexity=12010.741, train_loss=9.393557

Batch 840, train_perplexity=12116.653, train_loss=9.402336

Batch 850, train_perplexity=13793.094, train_loss=9.531923

Batch 860, train_perplexity=13231.031, train_loss=9.49032

Batch 870, train_perplexity=12751.114, train_loss=9.453374

Batch 880, train_perplexity=11861.17, train_loss=9.381025

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 890, train_perplexity=13396.573, train_loss=9.502754

Batch 900, train_perplexity=12161.131, train_loss=9.406

Batch 910, train_perplexity=12525.312, train_loss=9.435507

Batch 920, train_perplexity=11176.637, train_loss=9.321581

Batch 930, train_perplexity=12048.99, train_loss=9.396736

Batch 940, train_perplexity=11047.415, train_loss=9.309952

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 950, train_perplexity=12394.343, train_loss=9.424995

Batch 960, train_perplexity=12762.951, train_loss=9.454302

Batch 970, train_perplexity=11481.021, train_loss=9.348451

Batch 980, train_perplexity=12263.772, train_loss=9.414405

Batch 990, train_perplexity=11982.174, train_loss=9.391175

Batch 1000, train_perplexity=11243.325, train_loss=9.32753

Batch 1010, train_perplexity=11192.68, train_loss=9.323015

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 1020, train_perplexity=12326.469, train_loss=9.419504

Batch 1030, train_perplexity=11436.971, train_loss=9.344606

Batch 1040, train_perplexity=10749.502, train_loss=9.282615

Batch 1050, train_perplexity=10772.829, train_loss=9.284782

Batch 1060, train_perplexity=11116.258, train_loss=9.316164

Batch 1070, train_perplexity=11269.357, train_loss=9.329843

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 1080, train_perplexity=11202.376, train_loss=9.323881

Batch 1090, train_perplexity=11238.255, train_loss=9.327079

Batch 1100, train_perplexity=11347.426, train_loss=9.336746

Batch 1110, train_perplexity=10084.797, train_loss=9.218784

Batch 1120, train_perplexity=10552.44, train_loss=9.264112

Batch 1130, train_perplexity=10730.963, train_loss=9.280889

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 1140, train_perplexity=11061.838, train_loss=9.311256

Batch 1150, train_perplexity=11088.454, train_loss=9.31366

Batch 1160, train_perplexity=11108.978, train_loss=9.315509

Batch 1170, train_perplexity=10306.116, train_loss=9.240493

Batch 1180, train_perplexity=10248.593, train_loss=9.234896

Batch 1190, train_perplexity=10533.097, train_loss=9.262278

Batch 1200, train_perplexity=10890.423, train_loss=9.295639

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 1210, train_perplexity=9921.348, train_loss=9.202444

Batch 1220, train_perplexity=9846.093, train_loss=9.19483

Batch 1230, train_perplexity=9648.646, train_loss=9.174573

Batch 1240, train_perplexity=9657.208, train_loss=9.17546

Batch 1250, train_perplexity=10337.971, train_loss=9.243579

Batch 1260, train_perplexity=10786.111, train_loss=9.286015

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 1270, train_perplexity=10184.512, train_loss=9.228623

Batch 1280, train_perplexity=10339.845, train_loss=9.24376

Batch 1290, train_perplexity=9423.923, train_loss=9.151007

Batch 1300, train_perplexity=9284.005, train_loss=9.136048

Batch 1310, train_perplexity=9837.42, train_loss=9.193949

Batch 1320, train_perplexity=10464.691, train_loss=9.255762

Batch 1330, train_perplexity=9792.136, train_loss=9.189335

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 1340, train_perplexity=10323.498, train_loss=9.242178

Batch 1350, train_perplexity=10480.211, train_loss=9.257244

Batch 1360, train_perplexity=10675.628, train_loss=9.275719

Batch 1370, train_perplexity=9284.988, train_loss=9.136154

Batch 1380, train_perplexity=9634.726, train_loss=9.173129

Batch 1390, train_perplexity=9898.986, train_loss=9.200188

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 1400, train_perplexity=9391.176, train_loss=9.147526

Batch 1410, train_perplexity=9868.898, train_loss=9.197144

Batch 1420, train_perplexity=9339.863, train_loss=9.142047

Batch 1430, train_perplexity=10141.875, train_loss=9.224428

Batch 1440, train_perplexity=9233.947, train_loss=9.130642

Batch 1450, train_perplexity=9923.903, train_loss=9.202702

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 1460, train_perplexity=9687.547, train_loss=9.1785965

Batch 1470, train_perplexity=9071.986, train_loss=9.1129465

Batch 1480, train_perplexity=9288.345, train_loss=9.136516

Batch 1490, train_perplexity=10120.928, train_loss=9.222361

Batch 1500, train_perplexity=10062.893, train_loss=9.21661

Batch 1510, train_perplexity=8673.655, train_loss=9.068046

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 1520, train_perplexity=8947.237, train_loss=9.0991

Batch 1530, train_perplexity=8674.342, train_loss=9.068125

Batch 1540, train_perplexity=9547.767, train_loss=9.1640625

Batch 1550, train_perplexity=8773.613, train_loss=9.079504

Batch 1560, train_perplexity=9265.007, train_loss=9.134

Batch 1570, train_perplexity=9381.821, train_loss=9.146529

Batch 1580, train_perplexity=9411.3125, train_loss=9.149668

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 1590, train_perplexity=9647.129, train_loss=9.174416

Batch 1600, train_perplexity=8552.622, train_loss=9.053993

Batch 1610, train_perplexity=8495.013, train_loss=9.047235

Batch 1620, train_perplexity=9248.763, train_loss=9.132245

Batch 1630, train_perplexity=8794.623, train_loss=9.081896

Batch 1640, train_perplexity=8499.072, train_loss=9.047712

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 1650, train_perplexity=8800.429, train_loss=9.082556

Batch 1660, train_perplexity=8394.634, train_loss=9.035348

Batch 1670, train_perplexity=8252.177, train_loss=9.018232

Batch 1680, train_perplexity=9431.259, train_loss=9.151785

Batch 1690, train_perplexity=8571.247, train_loss=9.056169

Batch 1700, train_perplexity=8736.417, train_loss=9.075255

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 1710, train_perplexity=8832.564, train_loss=9.086201

Batch 1720, train_perplexity=8140.743, train_loss=9.004637

Batch 1730, train_perplexity=8746.7295, train_loss=9.076435

Batch 1740, train_perplexity=7965.127, train_loss=8.982828

Batch 1750, train_perplexity=8776.601, train_loss=9.079844

Batch 1760, train_perplexity=8168.32, train_loss=9.0080185

Batch 1770, train_perplexity=8219.88, train_loss=9.014311

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 1780, train_perplexity=7969.4272, train_loss=8.983368

Batch 1790, train_perplexity=8315.52, train_loss=9.025879

Batch 1800, train_perplexity=8058.4946, train_loss=8.994482

Batch 1810, train_perplexity=8404.479, train_loss=9.03652

Batch 1820, train_perplexity=8453.353, train_loss=9.042318

Batch 1830, train_perplexity=8867.861, train_loss=9.090189

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 1840, train_perplexity=7350.811, train_loss=8.902566

Batch 1850, train_perplexity=8561.632, train_loss=9.055046

Batch 1860, train_perplexity=7939.6294, train_loss=8.979622

Batch 1870, train_perplexity=7842.088, train_loss=8.96726

Batch 1880, train_perplexity=8157.6855, train_loss=9.006716

Batch 1890, train_perplexity=8680.251, train_loss=9.068806

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 1900, train_perplexity=8346.554, train_loss=9.029604

Batch 1910, train_perplexity=7716.4443, train_loss=8.951109

Batch 1920, train_perplexity=7852.176, train_loss=8.968546

Batch 1930, train_perplexity=8298.218, train_loss=9.023796

Batch 1940, train_perplexity=8204.968, train_loss=9.012495

Batch 1950, train_perplexity=8141.939, train_loss=9.004784

Batch 1960, train_perplexity=7830.347, train_loss=8.965762

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 1970, train_perplexity=8021.7207, train_loss=8.989908

Batch 1980, train_perplexity=7829.9517, train_loss=8.965712

Batch 1990, train_perplexity=7706.185, train_loss=8.949779

Batch 2000, train_perplexity=7519.8506, train_loss=8.925302

Batch 2010, train_perplexity=7423.649, train_loss=8.912426

Batch 2020, train_perplexity=7968.0063, train_loss=8.98319

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 2030, train_perplexity=7711.971, train_loss=8.950529

Batch 2040, train_perplexity=7257.644, train_loss=8.889811

Batch 2050, train_perplexity=7303.1074, train_loss=8.896055

Batch 2060, train_perplexity=8506.533, train_loss=9.04859

Batch 2070, train_perplexity=7351.4634, train_loss=8.902655

Batch 2080, train_perplexity=7659.1694, train_loss=8.943659

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 2090, train_perplexity=7691.853, train_loss=8.947917

Batch 2100, train_perplexity=7785.6025, train_loss=8.9600315

Batch 2110, train_perplexity=7346.7256, train_loss=8.90201

Batch 2120, train_perplexity=7683.561, train_loss=8.946838

Batch 2130, train_perplexity=8052.9785, train_loss=8.993797

Batch 2140, train_perplexity=7431.6606, train_loss=8.913505

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 2150, train_perplexity=8249.376, train_loss=9.017893

Batch 2160, train_perplexity=7572.248, train_loss=8.932245

Batch 2170, train_perplexity=7938.297, train_loss=8.979454

Batch 2180, train_perplexity=7438.957, train_loss=8.914486

Batch 2190, train_perplexity=7290.6025, train_loss=8.894341

Batch 2200, train_perplexity=6784.0693, train_loss=8.822332

Batch 2210, train_perplexity=6796.937, train_loss=8.824227

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 2220, train_perplexity=7127.9805, train_loss=8.871783

Batch 2230, train_perplexity=7898.9824, train_loss=8.974489

Batch 2240, train_perplexity=7202.882, train_loss=8.8822365

Batch 2250, train_perplexity=7136.4487, train_loss=8.872971

Batch 2260, train_perplexity=7733.8896, train_loss=8.953367

Batch 2270, train_perplexity=6992.72, train_loss=8.852625

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 2280, train_perplexity=7208.613, train_loss=8.883032

Batch 2290, train_perplexity=7619.958, train_loss=8.938526

Batch 2300, train_perplexity=7354.647, train_loss=8.903088

Batch 2310, train_perplexity=7810.471, train_loss=8.963221

Batch 2320, train_perplexity=6759.2964, train_loss=8.818674

Batch 2330, train_perplexity=6792.472, train_loss=8.82357

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 2340, train_perplexity=7201.5356, train_loss=8.88205

Batch 2350, train_perplexity=7361.524, train_loss=8.904022

Batch 2360, train_perplexity=7693.3936, train_loss=8.948117

Batch 2370, train_perplexity=6992.6265, train_loss=8.852612

Batch 2380, train_perplexity=7005.3223, train_loss=8.854425

Batch 2390, train_perplexity=7182.4956, train_loss=8.879402

Batch 2400, train_perplexity=6620.2397, train_loss=8.797887

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 2410, train_perplexity=6785.7905, train_loss=8.822586

Batch 2420, train_perplexity=6751.179, train_loss=8.817472

Batch 2430, train_perplexity=6923.9453, train_loss=8.842741

Batch 2440, train_perplexity=6998.7915, train_loss=8.853493

Batch 2450, train_perplexity=7098.6274, train_loss=8.867657

Batch 2460, train_perplexity=6835.4126, train_loss=8.829872

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 2470, train_perplexity=7599.522, train_loss=8.935841

Batch 2480, train_perplexity=6563.78, train_loss=8.789322

Batch 2490, train_perplexity=6844.9497, train_loss=8.831266

Batch 2500, train_perplexity=7042.398, train_loss=8.859704

Batch 2510, train_perplexity=6956.9473, train_loss=8.847496

Batch 2520, train_perplexity=7212.6426, train_loss=8.883591

Batch 2530, train_perplexity=7254.703, train_loss=8.889405

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 2540, train_perplexity=6708.2817, train_loss=8.811098

Batch 2550, train_perplexity=6649.371, train_loss=8.802278

Batch 2560, train_perplexity=7304.793, train_loss=8.896286

Batch 2570, train_perplexity=6719.326, train_loss=8.812743

Batch 2580, train_perplexity=6784.8975, train_loss=8.822454

Batch 2590, train_perplexity=6801.677, train_loss=8.824924

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 2600, train_perplexity=6650.3667, train_loss=8.802427

Batch 2610, train_perplexity=6723.4736, train_loss=8.81336

Batch 2620, train_perplexity=7005.763, train_loss=8.854488

Batch 2630, train_perplexity=6348.076, train_loss=8.755907

Batch 2640, train_perplexity=6536.052, train_loss=8.785089

Batch 2650, train_perplexity=7312.9067, train_loss=8.897396

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 2660, train_perplexity=7252.607, train_loss=8.889116

Batch 2670, train_perplexity=6508.776, train_loss=8.780907

Batch 2680, train_perplexity=6584.7207, train_loss=8.792507

Batch 2690, train_perplexity=6839.4033, train_loss=8.830456

Batch 2700, train_perplexity=6844.356, train_loss=8.83118

Batch 2710, train_perplexity=6426.7407, train_loss=8.768223

Batch 2720, train_perplexity=6283.8135, train_loss=8.745732

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 2730, train_perplexity=6701.6187, train_loss=8.810104

Batch 2740, train_perplexity=6431.934, train_loss=8.769031

Batch 2750, train_perplexity=6225.21, train_loss=8.736362

Batch 2760, train_perplexity=6456.0737, train_loss=8.772777

Batch 2770, train_perplexity=6365.948, train_loss=8.7587185

Batch 2780, train_perplexity=6380.341, train_loss=8.760977

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 2790, train_perplexity=6696.572, train_loss=8.809351

Batch 2800, train_perplexity=6652.498, train_loss=8.802748

Batch 2810, train_perplexity=6466.117, train_loss=8.774331

Batch 2820, train_perplexity=6163.7485, train_loss=8.72644

Batch 2830, train_perplexity=6310.4834, train_loss=8.749968

Batch 2840, train_perplexity=6731.3135, train_loss=8.814526

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 2850, train_perplexity=7034.0146, train_loss=8.858513

Batch 2860, train_perplexity=6711.686, train_loss=8.811605

Batch 2870, train_perplexity=6316.685, train_loss=8.75095

Batch 2880, train_perplexity=6773.2993, train_loss=8.820744

Batch 2890, train_perplexity=6545.2646, train_loss=8.786497

Batch 2900, train_perplexity=6850.5005, train_loss=8.832077

Batch 2910, train_perplexity=6580.1255, train_loss=8.791809

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 2920, train_perplexity=6819.8184, train_loss=8.827588

Batch 2930, train_perplexity=6640.765, train_loss=8.800982

Batch 2940, train_perplexity=6636.5107, train_loss=8.800342

Batch 2950, train_perplexity=6645.384, train_loss=8.801678

Batch 2960, train_perplexity=6435.6094, train_loss=8.769602

Batch 2970, train_perplexity=6806.868, train_loss=8.825687

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 2980, train_perplexity=6415.6445, train_loss=8.766495

Batch 2990, train_perplexity=5994.633, train_loss=8.69862

Batch 3000, train_perplexity=6327.1816, train_loss=8.75261

Batch 3010, train_perplexity=6433.621, train_loss=8.769293

Batch 3020, train_perplexity=5866.365, train_loss=8.6769905

Batch 3030, train_perplexity=6428.9478, train_loss=8.768566

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 3040, train_perplexity=6368.948, train_loss=8.75919

Batch 3050, train_perplexity=6103.3364, train_loss=8.716591

Batch 3060, train_perplexity=6211.298, train_loss=8.734125

Batch 3070, train_perplexity=6478.7837, train_loss=8.776288

Batch 3080, train_perplexity=5334.8037, train_loss=8.582007

Batch 3090, train_perplexity=6431.4004, train_loss=8.768948

Batch 3100, train_perplexity=6854.507, train_loss=8.832662

Batch 3110, train_perplexity=6089.1567, train_loss=8.714265

Batch 3120, train_perplexity=6059.4854, train_loss=8.70938

Batch 3130, train_perplexity=6302.4966, train_loss=8.748701

Batch 3140, train_perplexity=6312.446, train_loss=8.750278

Batch 3150, train_perplexity=5677.4233, train_loss=8.644253

Batch 3160, train_perplexity=6493.2144, train_loss=8.778513

Batch 3170, train_perplexity=6264.7256, train_loss=8.74269

Batch 3180, train_perplexity=5629.6646, train_loss=8.635805

Batch 3190, train_perplexity=5956.0996, train_loss=8.692171

Batch 3200, train_perplexity=6461.673, train_loss=8.7736435

Batch 3210, train_perplexity=6294.5557, train_loss=8.74744

Batch 3220, train_perplexity=6541.2837, train_loss=8.785889

Batch 3230, train_perplexity=6506.951, train_loss=8.780626

Batch 3240, train_perplexity=6447.798, train_loss=8.771494

Batch 3250, train_perplexity=5861.6787, train_loss=8.676191

Batch 3260, train_perplexity=6349.868, train_loss=8.756189

Batch 3270, train_perplexity=5885.3843, train_loss=8.680227

Batch 3280, train_perplexity=6210.587, train_loss=8.734011

Batch 3290, train_perplexity=5853.936, train_loss=8.67487

Batch 3300, train_perplexity=6497.0176, train_loss=8.7790985

Batch 3310, train_perplexity=5760.893, train_loss=8.658848

Batch 3320, train_perplexity=5502.443, train_loss=8.612947

Batch 3330, train_perplexity=6149.1177, train_loss=8.724064

Batch 3340, train_perplexity=5884.4414, train_loss=8.680067

Batch 3350, train_perplexity=6250.3496, train_loss=8.740393

Batch 3360, train_perplexity=6102.068, train_loss=8.716383

Batch 3370, train_perplexity=5799.126, train_loss=8.6654625

Batch 3380, train_perplexity=5586.4136, train_loss=8.628093

Batch 3390, train_perplexity=6059.318, train_loss=8.7093525

Batch 3400, train_perplexity=6318.071, train_loss=8.751169

Batch 3410, train_perplexity=6164.172, train_loss=8.726509

Batch 3420, train_perplexity=6446.0396, train_loss=8.771221

Batch 3430, train_perplexity=5759.2397, train_loss=8.658561

Batch 3440, train_perplexity=6355.903, train_loss=8.757139

Batch 3450, train_perplexity=6202.5312, train_loss=8.732713

Batch 3460, train_perplexity=6116.552, train_loss=8.718754

Batch 3470, train_perplexity=5486.911, train_loss=8.610121

Batch 3480, train_perplexity=5584.2134, train_loss=8.627699

Batch 3490, train_perplexity=6251.2915, train_loss=8.740543

Batch 3500, train_perplexity=5936.9146, train_loss=8.688945

Batch 3510, train_perplexity=6120.625, train_loss=8.7194195

Batch 3520, train_perplexity=5965.6724, train_loss=8.693777

Batch 3530, train_perplexity=5450.6016, train_loss=8.603481

Batch 3540, train_perplexity=5861.3994, train_loss=8.676144

Batch 3550, train_perplexity=5749.34, train_loss=8.65684

Batch 3560, train_perplexity=6043.2397, train_loss=8.706696

Batch 3570, train_perplexity=5991.964, train_loss=8.698174

Batch 3580, train_perplexity=5570.31, train_loss=8.625206

Batch 3590, train_perplexity=5862.204, train_loss=8.676281

Batch 3600, train_perplexity=5640.0146, train_loss=8.637642

Batch 3610, train_perplexity=5943.979, train_loss=8.690134

Batch 3620, train_perplexity=5586.1416, train_loss=8.628044
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 3630, train_perplexity=5989.8325, train_loss=8.697819

Batch 3640, train_perplexity=5883.437, train_loss=8.679896

Batch 3650, train_perplexity=5891.9546, train_loss=8.681343

Batch 3660, train_perplexity=6509.9863, train_loss=8.781093

Batch 3670, train_perplexity=5685.209, train_loss=8.645623

Batch 3680, train_perplexity=5704.9517, train_loss=8.64909

Batch 3690, train_perplexity=6006.937, train_loss=8.70067

Batch 3700, train_perplexity=5731.444, train_loss=8.653723

Batch 3710, train_perplexity=5911.361, train_loss=8.684631

Batch 3720, train_perplexity=5633.2466, train_loss=8.636441

Batch 3730, train_perplexity=5709.894, train_loss=8.649956

Batch 3740, train_perplexity=5718.297, train_loss=8.651426

Batch 3750, train_perplexity=5715.3037, train_loss=8.650903

Batch 3760, train_perplexity=5863.0093, train_loss=8.676418

Batch 3770, train_perplexity=5673.391, train_loss=8.643542

Batch 3780, train_perplexity=5822.2783, train_loss=8.669447

Batch 3790, train_perplexity=6169.606, train_loss=8.72739

Batch 3800, train_perplexity=5726.046, train_loss=8.652781

Batch 3810, train_perplexity=5347.936, train_loss=8.584466

Batch 3820, train_perplexity=5466.5625, train_loss=8.606405

Batch 3830, train_perplexity=6130.0767, train_loss=8.720963

Batch 3840, train_perplexity=5396.9478, train_loss=8.593589

Batch 3850, train_perplexity=5760.959, train_loss=8.658859

Batch 3860, train_perplexity=5640.391, train_loss=8.637709

Batch 3870, train_perplexity=5345.7075, train_loss=8.584049

Batch 3880, train_perplexity=5560.571, train_loss=8.623456

Batch 3890, train_perplexity=5787.005, train_loss=8.66337

Batch 3900, train_perplexity=5504.175, train_loss=8.613262

Batch 3910, train_perplexity=5611.1724, train_loss=8.632515

Batch 3920, train_perplexity=5508.7964, train_loss=8.614101

Batch 3930, train_perplexity=5579.4814, train_loss=8.626851

Batch 3940, train_perplexity=5877.924, train_loss=8.678959

Batch 3950, train_perplexity=5337.9033, train_loss=8.582588

Batch 3960, train_perplexity=6389.7974, train_loss=8.762458

Batch 3970, train_perplexity=5821.912, train_loss=8.669384

Batch 3980, train_perplexity=5760.7666, train_loss=8.658826

Batch 3990, train_perplexity=5685.1606, train_loss=8.645615

Batch 4000, train_perplexity=5064.8647, train_loss=8.530083

Batch 4010, train_perplexity=5885.9004, train_loss=8.680315

Batch 4020, train_perplexity=5383.11, train_loss=8.591022

Batch 4030, train_perplexity=5111.8916, train_loss=8.539325

Batch 4040, train_perplexity=5693.9395, train_loss=8.647158

Batch 4050, train_perplexity=5612.221, train_loss=8.632702

Batch 4060, train_perplexity=5232.842, train_loss=8.56271

Batch 4070, train_perplexity=5395.085, train_loss=8.593244

Batch 4080, train_perplexity=5508.1133, train_loss=8.613977

Batch 4090, train_perplexity=5652.049, train_loss=8.639773

Batch 4100, train_perplexity=5821.9565, train_loss=8.669392

Batch 4110, train_perplexity=5215.494, train_loss=8.559389

Batch 4120, train_perplexity=5294.3584, train_loss=8.574397

Batch 4130, train_perplexity=5572.584, train_loss=8.625614

Batch 4140, train_perplexity=6237.131, train_loss=8.738276

Batch 4150, train_perplexity=5579.662, train_loss=8.6268835

Batch 4160, train_perplexity=5820.374, train_loss=8.66912

Batch 4170, train_perplexity=4865.1133, train_loss=8.489845

Batch 4180, train_perplexity=5664.6274, train_loss=8.641996

Batch 4190, train_perplexity=5121.3384, train_loss=8.541171

Batch 4200, train_perplexity=5452.3486, train_loss=8.603802

Batch 4210, train_perplexity=5323.902, train_loss=8.579962

Batch 4220, train_perplexity=5548.117, train_loss=8.621214

Batch 4230, train_perplexity=5274.8794, train_loss=8.570711

Batch 4240, train_perplexity=5601.228, train_loss=8.630741

Batch 4250, train_perplexity=5286.266, train_loss=8.572867

Batch 4260, train_perplexity=5392.693, train_loss=8.5928

Batch 4270, train_perplexity=5028.767, train_loss=8.52293

Batch 4280, train_perplexity=5346.136, train_loss=8.584129

Batch 4290, train_perplexity=5324.4556, train_loss=8.580066

Batch 4300, train_perplexity=5568.276, train_loss=8.624841

Batch 4310, train_perplexity=5179.2715, train_loss=8.55242

Batch 4320, train_perplexity=4945.528, train_loss=8.506239

Batch 4330, train_perplexity=4633.928, train_loss=8.44116

Batch 4340, train_perplexity=5637.5625, train_loss=8.637207

Batch 4350, train_perplexity=5064.1694, train_loss=8.529945

Batch 4360, train_perplexity=5264.537, train_loss=8.568748

Batch 4370, train_perplexity=4812.966, train_loss=8.479069

Batch 4380, train_perplexity=5485.279, train_loss=8.609823

Batch 4390, train_perplexity=5103.2554, train_loss=8.537634

Batch 4400, train_perplexity=5440.781, train_loss=8.601678

Batch 4410, train_perplexity=5514.0, train_loss=8.615046

Batch 4420, train_perplexity=5157.6133, train_loss=8.548229

Batch 4430, train_perplexity=5338.779, train_loss=8.582752

Batch 4440, train_perplexity=5407.844, train_loss=8.595606

Batch 4450, train_perplexity=5352.11, train_loss=8.585246

Batch 4460, train_perplexity=5490.1567, train_loss=8.610712

Batch 4470, train_perplexity=5191.9907, train_loss=8.5548725

Batch 4480, train_perplexity=5158.8823, train_loss=8.548475

Batch 4490, train_perplexity=5057.234, train_loss=8.528575

Batch 4500, train_perplexity=5558.3066, train_loss=8.623049

Batch 4510, train_perplexity=4887.8687, train_loss=8.494512

Batch 4520, train_perplexity=4779.2925, train_loss=8.472048

Batch 4530, train_perplexity=5076.0977, train_loss=8.532298

Batch 4540, train_perplexity=5435.424, train_loss=8.600693

Batch 4550, train_perplexity=5188.0063, train_loss=8.554105

Batch 4560, train_perplexity=5217.3, train_loss=8.559735

Batch 4570, train_perplexity=5217.7427, train_loss=8.55982

Batch 4580, train_perplexity=5314.203, train_loss=8.578138

Batch 4590, train_perplexity=5100.983, train_loss=8.537189

Batch 4600, train_perplexity=5054.9146, train_loss=8.528116

Batch 4610, train_perplexity=5823.2334, train_loss=8.669611

Batch 4620, train_perplexity=5144.1733, train_loss=8.54562

Batch 4630, train_perplexity=5413.5566, train_loss=8.596662

Batch 4640, train_perplexity=5060.075, train_loss=8.529137

Batch 4650, train_perplexity=5309.5225, train_loss=8.577257

Batch 4660, train_perplexity=5153.4883, train_loss=8.547429

Batch 4670, train_perplexity=5345.881, train_loss=8.584082

Batch 4680, train_perplexity=5191.3325, train_loss=8.554746

Batch 4690, train_perplexity=4575.929, train_loss=8.428565

Batch 4700, train_perplexity=4911.7515, train_loss=8.499386

Batch 4710, train_perplexity=5330.634, train_loss=8.581225

Batch 4720, train_perplexity=5588.401, train_loss=8.6284485

Batch 4730, train_perplexity=5167.273, train_loss=8.5501

Batch 4740, train_perplexity=4906.0024, train_loss=8.498215

Batch 4750, train_perplexity=5253.6685, train_loss=8.566682

Batch 4760, train_perplexity=5198.9673, train_loss=8.556215

Batch 4770, train_perplexity=5152.481, train_loss=8.547234

Batch 4780, train_perplexity=5137.329, train_loss=8.544289

Batch 4790, train_perplexity=4542.4756, train_loss=8.421227

Batch 4800, train_perplexity=4963.312, train_loss=8.509829

Batch 4810, train_perplexity=4966.987, train_loss=8.510569

Batch 4820, train_perplexity=5026.619, train_loss=8.522503

Batch 4830, train_perplexity=4904.4214, train_loss=8.497892

Batch 4840, train_perplexity=5131.101, train_loss=8.543076

Batch 4850, train_perplexity=5385.051, train_loss=8.591382

Batch 4860, train_perplexity=4513.5864, train_loss=8.414847

Batch 4870, train_perplexity=4881.1235, train_loss=8.493131

Batch 4880, train_perplexity=4600.3623, train_loss=8.43389

Batch 4890, train_perplexity=4902.3354, train_loss=8.497467

Batch 4900, train_perplexity=5118.2915, train_loss=8.540576

Batch 4910, train_perplexity=5105.6406, train_loss=8.538101

Batch 4920, train_perplexity=5093.7544, train_loss=8.53577

Batch 4930, train_perplexity=5050.029, train_loss=8.527149

Batch 4940, train_perplexity=4994.0317, train_loss=8.515999

Batch 4950, train_perplexity=4754.2896, train_loss=8.466803

Batch 4960, train_perplexity=5015.041, train_loss=8.520197

Batch 4970, train_perplexity=5201.4966, train_loss=8.556702

Batch 4980, train_perplexity=4904.047, train_loss=8.497816
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 4990, train_perplexity=4697.664, train_loss=8.454821

Batch 5000, train_perplexity=5034.29, train_loss=8.524028

Batch 5010, train_perplexity=4671.8228, train_loss=8.449305

Batch 5020, train_perplexity=5124.5044, train_loss=8.541789

Batch 5030, train_perplexity=4775.903, train_loss=8.471338

Batch 5040, train_perplexity=5188.704, train_loss=8.554239

Batch 5050, train_perplexity=4962.8765, train_loss=8.509741

Batch 5060, train_perplexity=4850.469, train_loss=8.486831

Batch 5070, train_perplexity=4759.2437, train_loss=8.467844

Batch 5080, train_perplexity=5187.0615, train_loss=8.553923

Batch 5090, train_perplexity=4876.159, train_loss=8.492113

Batch 5100, train_perplexity=5050.8716, train_loss=8.527316

Batch 5110, train_perplexity=5043.1655, train_loss=8.525789

Batch 5120, train_perplexity=5171.572, train_loss=8.550932

Batch 5130, train_perplexity=4597.7393, train_loss=8.43332

Batch 5140, train_perplexity=4960.9746, train_loss=8.509357

Batch 5150, train_perplexity=4384.007, train_loss=8.385718

Batch 5160, train_perplexity=5241.7974, train_loss=8.56442

Batch 5170, train_perplexity=5092.3022, train_loss=8.535485

Batch 5180, train_perplexity=4558.7456, train_loss=8.424803

Batch 5190, train_perplexity=5034.0166, train_loss=8.523973

Batch 5200, train_perplexity=5629.847, train_loss=8.635838

Batch 5210, train_perplexity=5056.6504, train_loss=8.52846

Batch 5220, train_perplexity=5585.7793, train_loss=8.627979

Batch 5230, train_perplexity=4894.973, train_loss=8.495964

Batch 5240, train_perplexity=4902.635, train_loss=8.497528

Batch 5250, train_perplexity=5221.775, train_loss=8.560593

Batch 5260, train_perplexity=5000.532, train_loss=8.5173

Batch 5270, train_perplexity=4873.574, train_loss=8.491583

Batch 5280, train_perplexity=4652.9077, train_loss=8.445248

Batch 5290, train_perplexity=4903.972, train_loss=8.497801

Batch 5300, train_perplexity=4780.423, train_loss=8.472284

Batch 5310, train_perplexity=4641.447, train_loss=8.442781

Batch 5320, train_perplexity=5167.5093, train_loss=8.550146

Batch 5330, train_perplexity=4879.7227, train_loss=8.492844

Batch 5340, train_perplexity=4684.967, train_loss=8.452114

Batch 5350, train_perplexity=4534.1616, train_loss=8.419395

Batch 5360, train_perplexity=4943.726, train_loss=8.505875

Batch 5370, train_perplexity=4529.8525, train_loss=8.418445

Batch 5380, train_perplexity=4524.4644, train_loss=8.417254

Batch 5390, train_perplexity=5398.3633, train_loss=8.593851

Batch 5400, train_perplexity=4974.3438, train_loss=8.512049

Batch 5410, train_perplexity=5262.439, train_loss=8.56835

Batch 5420, train_perplexity=4802.8877, train_loss=8.476973

Batch 5430, train_perplexity=4808.268, train_loss=8.478092

Batch 5440, train_perplexity=5075.8604, train_loss=8.532251

Batch 5450, train_perplexity=4604.6943, train_loss=8.434832

Batch 5460, train_perplexity=5279.112, train_loss=8.571513

Batch 5470, train_perplexity=4618.68, train_loss=8.437864

Batch 5480, train_perplexity=5293.692, train_loss=8.574271

Batch 5490, train_perplexity=4565.1846, train_loss=8.426214

Batch 5500, train_perplexity=4846.6265, train_loss=8.486038

Batch 5510, train_perplexity=4942.5664, train_loss=8.50564

Batch 5520, train_perplexity=5235.0684, train_loss=8.563135

Batch 5530, train_perplexity=5054.765, train_loss=8.528087

Batch 5540, train_perplexity=4715.4385, train_loss=8.458597

Batch 5550, train_perplexity=4969.8677, train_loss=8.511148

Batch 5560, train_perplexity=5340.989, train_loss=8.583166

Batch 5570, train_perplexity=4289.5264, train_loss=8.363932

Batch 5580, train_perplexity=4462.81, train_loss=8.403534

Batch 5590, train_perplexity=4578.343, train_loss=8.429092

Batch 5600, train_perplexity=4761.8086, train_loss=8.468383

Batch 5610, train_perplexity=4619.676, train_loss=8.43808

Batch 5620, train_perplexity=4753.501, train_loss=8.466637

Batch 5630, train_perplexity=4784.9565, train_loss=8.473232

Batch 5640, train_perplexity=4619.35, train_loss=8.438009

Batch 5650, train_perplexity=4519.0093, train_loss=8.416048

Batch 5660, train_perplexity=4596.0474, train_loss=8.432952

Batch 5670, train_perplexity=4497.581, train_loss=8.411295

Batch 5680, train_perplexity=5198.9326, train_loss=8.556209

Batch 5690, train_perplexity=4687.836, train_loss=8.452726

Batch 5700, train_perplexity=4695.156, train_loss=8.454287

Batch 5710, train_perplexity=4865.0483, train_loss=8.489832

Batch 5720, train_perplexity=5199.2153, train_loss=8.556263

Batch 5730, train_perplexity=4641.6597, train_loss=8.442827

Batch 5740, train_perplexity=4726.821, train_loss=8.461008

Batch 5750, train_perplexity=4687.273, train_loss=8.452606

Batch 5760, train_perplexity=4639.376, train_loss=8.442335

Batch 5770, train_perplexity=4479.5083, train_loss=8.407269

Batch 5780, train_perplexity=5057.692, train_loss=8.528666

Batch 5790, train_perplexity=4493.3877, train_loss=8.410362

Batch 5800, train_perplexity=4649.785, train_loss=8.444576

Batch 5810, train_perplexity=4711.438, train_loss=8.457748

Batch 5820, train_perplexity=4548.206, train_loss=8.422488

Batch 5830, train_perplexity=4526.7993, train_loss=8.41777

Batch 5840, train_perplexity=4479.995, train_loss=8.407377

Batch 5850, train_perplexity=4590.7773, train_loss=8.431805

Batch 5860, train_perplexity=4762.422, train_loss=8.468512

Batch 5870, train_perplexity=4617.35, train_loss=8.437576

Batch 5880, train_perplexity=4886.787, train_loss=8.49429

Batch 5890, train_perplexity=4797.4673, train_loss=8.475843

Batch 5900, train_perplexity=4822.7847, train_loss=8.481107

Batch 5910, train_perplexity=4906.2646, train_loss=8.498268

Batch 5920, train_perplexity=4574.8076, train_loss=8.42832

Batch 5930, train_perplexity=4439.8613, train_loss=8.398378

Batch 5940, train_perplexity=4677.895, train_loss=8.4506035

Batch 5950, train_perplexity=4959.877, train_loss=8.509136

Batch 5960, train_perplexity=4300.4673, train_loss=8.366479

Batch 5970, train_perplexity=4690.524, train_loss=8.4532995

Batch 5980, train_perplexity=4597.8975, train_loss=8.433354

Batch 5990, train_perplexity=4878.9873, train_loss=8.492693

Batch 6000, train_perplexity=4925.8896, train_loss=8.50226

Batch 6010, train_perplexity=4269.2676, train_loss=8.359198

Batch 6020, train_perplexity=4644.573, train_loss=8.443455

Batch 6030, train_perplexity=4653.458, train_loss=8.445366

Batch 6040, train_perplexity=4643.085, train_loss=8.443134

Batch 6050, train_perplexity=4288.86, train_loss=8.363776

Batch 6060, train_perplexity=4697.5205, train_loss=8.45479

Batch 6070, train_perplexity=5057.5376, train_loss=8.528635

Batch 6080, train_perplexity=4509.925, train_loss=8.414036

Batch 6090, train_perplexity=4290.2056, train_loss=8.36409

Batch 6100, train_perplexity=4786.148, train_loss=8.473481

Batch 6110, train_perplexity=4604.7734, train_loss=8.434849

Batch 6120, train_perplexity=4582.633, train_loss=8.430029

Batch 6130, train_perplexity=4641.341, train_loss=8.442759

Batch 6140, train_perplexity=4359.579, train_loss=8.380131

Batch 6150, train_perplexity=4920.035, train_loss=8.501071

Batch 6160, train_perplexity=4782.3657, train_loss=8.472691

Batch 6170, train_perplexity=4663.9165, train_loss=8.447611

Batch 6180, train_perplexity=4885.8135, train_loss=8.494091

Batch 6190, train_perplexity=4698.179, train_loss=8.45493

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 6200, train_perplexity=4588.536, train_loss=8.431316

Batch 6210, train_perplexity=4579.854, train_loss=8.429422

Batch 6220, train_perplexity=4652.278, train_loss=8.445112

Batch 6230, train_perplexity=4623.5986, train_loss=8.438929

Batch 6240, train_perplexity=4458.743, train_loss=8.402622

Batch 6250, train_perplexity=4273.529, train_loss=8.360195

Batch 6260, train_perplexity=4503.461, train_loss=8.412601

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 6270, train_perplexity=4715.6724, train_loss=8.458647

Batch 6280, train_perplexity=4357.6587, train_loss=8.37969

Batch 6290, train_perplexity=4363.6177, train_loss=8.381057

Batch 6300, train_perplexity=4418.9683, train_loss=8.3936615

Batch 6310, train_perplexity=4457.213, train_loss=8.402279

Batch 6320, train_perplexity=4230.4688, train_loss=8.350068

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 6330, train_perplexity=4407.302, train_loss=8.391018

Batch 6340, train_perplexity=4573.7783, train_loss=8.428095

Batch 6350, train_perplexity=4144.1914, train_loss=8.329463

Batch 6360, train_perplexity=4180.9473, train_loss=8.338293

Batch 6370, train_perplexity=4767.3477, train_loss=8.469545

Batch 6380, train_perplexity=4713.2803, train_loss=8.458139

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 6390, train_perplexity=4507.569, train_loss=8.413513

Batch 6400, train_perplexity=4721.2114, train_loss=8.459821

Batch 6410, train_perplexity=4462.789, train_loss=8.403529

Batch 6420, train_perplexity=4322.5186, train_loss=8.371593

Batch 6430, train_perplexity=4208.2207, train_loss=8.344795

Batch 6440, train_perplexity=4273.382, train_loss=8.360161

Batch 6450, train_perplexity=4174.636, train_loss=8.336782

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 6460, train_perplexity=4647.001, train_loss=8.443977

Batch 6470, train_perplexity=4293.267, train_loss=8.364803

Batch 6480, train_perplexity=4839.4307, train_loss=8.484552

Batch 6490, train_perplexity=4598.621, train_loss=8.433512

Batch 6500, train_perplexity=4236.133, train_loss=8.351406

Batch 6510, train_perplexity=4361.1094, train_loss=8.380482

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 6520, train_perplexity=4561.659, train_loss=8.425442

Batch 6530, train_perplexity=4453.8306, train_loss=8.40152

Batch 6540, train_perplexity=4545.158, train_loss=8.421818

Batch 6550, train_perplexity=4287.2075, train_loss=8.363391

Batch 6560, train_perplexity=4526.4453, train_loss=8.417692

Batch 6570, train_perplexity=4599.6123, train_loss=8.433727

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 6580, train_perplexity=4570.238, train_loss=8.4273205

Batch 6590, train_perplexity=4692.9087, train_loss=8.453808

Batch 6600, train_perplexity=4477.791, train_loss=8.406885

Batch 6610, train_perplexity=4759.189, train_loss=8.467833

Batch 6620, train_perplexity=4458.573, train_loss=8.402584

Batch 6630, train_perplexity=4399.3228, train_loss=8.389206

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 6640, train_perplexity=4710.036, train_loss=8.457451

Batch 6650, train_perplexity=4254.9644, train_loss=8.355842

Batch 6660, train_perplexity=4391.811, train_loss=8.387497

Batch 6670, train_perplexity=4261.2544, train_loss=8.357319

Batch 6680, train_perplexity=4227.2466, train_loss=8.349306

Batch 6690, train_perplexity=4346.162, train_loss=8.3770485

Batch 6700, train_perplexity=4615.0435, train_loss=8.437077

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 6710, train_perplexity=4109.2207, train_loss=8.320989

Batch 6720, train_perplexity=4075.4736, train_loss=8.312742

Batch 6730, train_perplexity=4560.585, train_loss=8.425206

Batch 6740, train_perplexity=4806.058, train_loss=8.4776325

Batch 6750, train_perplexity=4495.814, train_loss=8.410902

Batch 6760, train_perplexity=4078.1484, train_loss=8.313398

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 6770, train_perplexity=4245.192, train_loss=8.353542

Batch 6780, train_perplexity=4367.8145, train_loss=8.382018

Batch 6790, train_perplexity=4025.8767, train_loss=8.300498

Batch 6800, train_perplexity=4432.28, train_loss=8.396669

Batch 6810, train_perplexity=4084.4304, train_loss=8.314938

Batch 6820, train_perplexity=4392.657, train_loss=8.38769

Batch 6830, train_perplexity=4295.2085, train_loss=8.365255

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 6840, train_perplexity=4255.496, train_loss=8.355967

Batch 6850, train_perplexity=4321.591, train_loss=8.371379

Batch 6860, train_perplexity=4147.0264, train_loss=8.330147

Batch 6870, train_perplexity=4118.083, train_loss=8.323143

Batch 6880, train_perplexity=4398.333, train_loss=8.388981

Batch 6890, train_perplexity=3936.5613, train_loss=8.278063

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 6900, train_perplexity=4279.948, train_loss=8.361696
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 6910, train_perplexity=4731.574, train_loss=8.462013

Batch 6920, train_perplexity=4342.744, train_loss=8.376262

Batch 6930, train_perplexity=4738.899, train_loss=8.46356

Batch 6940, train_perplexity=4234.6665, train_loss=8.35106

Batch 6950, train_perplexity=4316.1665, train_loss=8.370123

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 6960, train_perplexity=4279.8013, train_loss=8.361662

Batch 6970, train_perplexity=4289.854, train_loss=8.364008

Batch 6980, train_perplexity=4445.174, train_loss=8.399574

Batch 6990, train_perplexity=4521.76, train_loss=8.4166565

Batch 7000, train_perplexity=4405.562, train_loss=8.390623

Batch 7010, train_perplexity=4025.965, train_loss=8.30052

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 7020, train_perplexity=3909.5862, train_loss=8.271187

Batch 7030, train_perplexity=4383.095, train_loss=8.38551

Batch 7040, train_perplexity=4524.913, train_loss=8.417354

Batch 7050, train_perplexity=4274.5356, train_loss=8.360431

Batch 7060, train_perplexity=4339.2954, train_loss=8.375467

Batch 7070, train_perplexity=4510.4756, train_loss=8.414158

Batch 7080, train_perplexity=4391.1997, train_loss=8.387358

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 7090, train_perplexity=4363.9717, train_loss=8.381138

Batch 7100, train_perplexity=4314.607, train_loss=8.369761

Batch 7110, train_perplexity=4418.9175, train_loss=8.39365

Batch 7120, train_perplexity=4142.54, train_loss=8.329064

Batch 7130, train_perplexity=4247.241, train_loss=8.354025

Batch 7140, train_perplexity=4467.23, train_loss=8.404524

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 7150, train_perplexity=4024.5908, train_loss=8.300179

Batch 7160, train_perplexity=4326.4033, train_loss=8.372492

Batch 7170, train_perplexity=4281.663, train_loss=8.362097

Batch 7180, train_perplexity=4015.5698, train_loss=8.297935

Batch 7190, train_perplexity=4407.781, train_loss=8.391127

Batch 7200, train_perplexity=4484.911, train_loss=8.408474

Batch 7210, train_perplexity=4514.202, train_loss=8.414984

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 7220, train_perplexity=4681.608, train_loss=8.451397

Batch 7230, train_perplexity=4052.7456, train_loss=8.30715

Batch 7240, train_perplexity=4228.5044, train_loss=8.349604

Batch 7250, train_perplexity=3991.1277, train_loss=8.291829

Batch 7260, train_perplexity=4153.1846, train_loss=8.331631

Batch 7270, train_perplexity=4303.938, train_loss=8.367286

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 7280, train_perplexity=4243.8604, train_loss=8.353229

Batch 7290, train_perplexity=4683.5195, train_loss=8.451805

Batch 7300, train_perplexity=4471.6885, train_loss=8.405521

Batch 7310, train_perplexity=4391.9785, train_loss=8.387535

Batch 7320, train_perplexity=4133.8774, train_loss=8.326971

Batch 7330, train_perplexity=4559.3193, train_loss=8.424929

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 7340, train_perplexity=4084.9993, train_loss=8.315077

Batch 7350, train_perplexity=3962.203, train_loss=8.284555

Batch 7360, train_perplexity=4337.9097, train_loss=8.375148

Batch 7370, train_perplexity=4506.735, train_loss=8.413328

Batch 7380, train_perplexity=4166.6772, train_loss=8.334874

Batch 7390, train_perplexity=4058.8262, train_loss=8.308649

Batch 7400, train_perplexity=4577.522, train_loss=8.428913

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 7410, train_perplexity=4224.901, train_loss=8.348751

Batch 7420, train_perplexity=4401.7905, train_loss=8.389767

Batch 7430, train_perplexity=4138.974, train_loss=8.328203

Batch 7440, train_perplexity=4330.6387, train_loss=8.37347

Batch 7450, train_perplexity=4364.7124, train_loss=8.381308

Batch 7460, train_perplexity=4165.95, train_loss=8.3347

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 7470, train_perplexity=4060.1113, train_loss=8.308966

Batch 7480, train_perplexity=3799.9004, train_loss=8.24273

Batch 7490, train_perplexity=4420.671, train_loss=8.394047

Batch 7500, train_perplexity=4476.8306, train_loss=8.406671

Batch 7510, train_perplexity=3956.3464, train_loss=8.283076

Batch 7520, train_perplexity=4354.273, train_loss=8.378913

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 7530, train_perplexity=4313.8335, train_loss=8.369582

Batch 7540, train_perplexity=4576.1777, train_loss=8.428619

Batch 7550, train_perplexity=4336.863, train_loss=8.374907

Batch 7560, train_perplexity=3965.673, train_loss=8.285431

Batch 7570, train_perplexity=4470.098, train_loss=8.405166

Batch 7580, train_perplexity=4022.488, train_loss=8.299656

Batch 7590, train_perplexity=4078.942, train_loss=8.313593

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 7600, train_perplexity=4232.757, train_loss=8.350609

Batch 7610, train_perplexity=4193.4897, train_loss=8.341289

Batch 7620, train_perplexity=3882.3445, train_loss=8.2641945

Batch 7630, train_perplexity=4498.2114, train_loss=8.411435

Batch 7640, train_perplexity=4429.3853, train_loss=8.396016

Batch 7650, train_perplexity=4068.8755, train_loss=8.311122

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 7660, train_perplexity=4179.6875, train_loss=8.337992

Batch 7670, train_perplexity=4397.5693, train_loss=8.388807

Batch 7680, train_perplexity=4337.0947, train_loss=8.37496

Batch 7690, train_perplexity=4302.641, train_loss=8.366984

Batch 7700, train_perplexity=4052.4055, train_loss=8.307066

Batch 7710, train_perplexity=4007.6924, train_loss=8.295971

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 7720, train_perplexity=4440.4497, train_loss=8.398511

Batch 7730, train_perplexity=4061.0562, train_loss=8.309198

Batch 7740, train_perplexity=3695.4446, train_loss=8.214856

Batch 7750, train_perplexity=4176.432, train_loss=8.337213

Batch 7760, train_perplexity=4350.7075, train_loss=8.378094

Batch 7770, train_perplexity=3880.3418, train_loss=8.263679

Batch 7780, train_perplexity=4414.5117, train_loss=8.3926525

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 7790, train_perplexity=4218.403, train_loss=8.347212

Batch 7800, train_perplexity=3930.5815, train_loss=8.276543

Batch 7810, train_perplexity=4466.5444, train_loss=8.40437

Batch 7820, train_perplexity=4502.147, train_loss=8.41231

Batch 7830, train_perplexity=4119.548, train_loss=8.323499

Batch 7840, train_perplexity=4176.8584, train_loss=8.337315

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 7850, train_perplexity=4065.7065, train_loss=8.310343

Batch 7860, train_perplexity=4112.314, train_loss=8.321741

Batch 7870, train_perplexity=4240.175, train_loss=8.35236

Batch 7880, train_perplexity=3942.8057, train_loss=8.279648

Batch 7890, train_perplexity=4101.386, train_loss=8.31908

Batch 7900, train_perplexity=4208.582, train_loss=8.344881

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 7910, train_perplexity=4248.3794, train_loss=8.354293

Batch 7920, train_perplexity=4066.924, train_loss=8.310642

Batch 7930, train_perplexity=3970.7595, train_loss=8.286713

Batch 7940, train_perplexity=4392.8833, train_loss=8.387741

Batch 7950, train_perplexity=4333.919, train_loss=8.374228

Batch 7960, train_perplexity=4048.6973, train_loss=8.30615

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 7970, train_perplexity=4294.9424, train_loss=8.365193

Batch 7980, train_perplexity=4102.7637, train_loss=8.319416

Batch 7990, train_perplexity=3827.239, train_loss=8.249899

Batch 8000, train_perplexity=4209.947, train_loss=8.345205

Batch 8010, train_perplexity=4259.613, train_loss=8.356934

Batch 8020, train_perplexity=4637.1465, train_loss=8.4418545

Batch 8030, train_perplexity=4148.316, train_loss=8.330458

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 8040, train_perplexity=4136.038, train_loss=8.327494

Batch 8050, train_perplexity=3855.0867, train_loss=8.257149

Batch 8060, train_perplexity=4299.7656, train_loss=8.366316

Batch 8070, train_perplexity=3957.241, train_loss=8.283302

Batch 8080, train_perplexity=4507.358, train_loss=8.413466

Batch 8090, train_perplexity=4021.5676, train_loss=8.299427

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 8100, train_perplexity=4406.625, train_loss=8.390864

Batch 8110, train_perplexity=4290.1484, train_loss=8.364077

Batch 8120, train_perplexity=4237.527, train_loss=8.351735

Batch 8130, train_perplexity=4108.041, train_loss=8.320702

Batch 8140, train_perplexity=4133.747, train_loss=8.32694

Batch 8150, train_perplexity=4407.0664, train_loss=8.3909645

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 8160, train_perplexity=4126.9443, train_loss=8.325293

Batch 8170, train_perplexity=4191.7827, train_loss=8.340881

Batch 8180, train_perplexity=4119.206, train_loss=8.323416

Batch 8190, train_perplexity=4134.8154, train_loss=8.327198

Batch 8200, train_perplexity=3899.2273, train_loss=8.268534

Batch 8210, train_perplexity=4046.5432, train_loss=8.305618

Batch 8220, train_perplexity=4334.663, train_loss=8.374399

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6101 sentences.
Finished loading
Batch 8230, train_perplexity=4096.586, train_loss=8.317909

Batch 8240, train_perplexity=3816.9053, train_loss=8.247195

Batch 8250, train_perplexity=4063.1946, train_loss=8.309725

Batch 8260, train_perplexity=4196.7944, train_loss=8.342076

Batch 8270, train_perplexity=3784.9236, train_loss=8.238781

Batch 8280, train_perplexity=3758.5002, train_loss=8.231775

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 8290, train_perplexity=4473.0024, train_loss=8.405815

Batch 8300, train_perplexity=4014.356, train_loss=8.297632

Batch 8310, train_perplexity=3841.0532, train_loss=8.253502

Batch 8320, train_perplexity=4396.37, train_loss=8.388535

Batch 8330, train_perplexity=3784.1655, train_loss=8.238581

Batch 8340, train_perplexity=4169.559, train_loss=8.335566

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 8350, train_perplexity=4012.3582, train_loss=8.297134

Batch 8360, train_perplexity=4323.8047, train_loss=8.371891

Batch 8370, train_perplexity=4109.1855, train_loss=8.32098

Batch 8380, train_perplexity=4286.3696, train_loss=8.363195

Batch 8390, train_perplexity=3949.7002, train_loss=8.281395

Batch 8400, train_perplexity=4343.875, train_loss=8.376522

Batch 8410, train_perplexity=3942.1965, train_loss=8.279493

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 8420, train_perplexity=4436.5386, train_loss=8.39763

Batch 8430, train_perplexity=3899.908, train_loss=8.268708

Batch 8440, train_perplexity=4016.5962, train_loss=8.29819

Batch 8450, train_perplexity=4321.612, train_loss=8.371384

Batch 8460, train_perplexity=4308.3115, train_loss=8.368301

Batch 8470, train_perplexity=4199.569, train_loss=8.342737

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 8480, train_perplexity=3743.6187, train_loss=8.227808

Batch 8490, train_perplexity=4412.0034, train_loss=8.392084

Batch 8500, train_perplexity=3966.1875, train_loss=8.285561

Batch 8510, train_perplexity=3790.1501, train_loss=8.240161

Batch 8520, train_perplexity=3943.4863, train_loss=8.27982

Batch 8530, train_perplexity=3892.51, train_loss=8.266809

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 8540, train_perplexity=4007.509, train_loss=8.295925

Batch 8550, train_perplexity=3766.9546, train_loss=8.234022

Batch 8560, train_perplexity=4034.8518, train_loss=8.302725

Batch 8570, train_perplexity=3772.2607, train_loss=8.23543

Batch 8580, train_perplexity=4228.0044, train_loss=8.349485

Batch 8590, train_perplexity=4137.4033, train_loss=8.327824

Batch 8600, train_perplexity=3871.1052, train_loss=8.261295

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 8610, train_perplexity=3890.2166, train_loss=8.26622

Batch 8620, train_perplexity=3892.3318, train_loss=8.266764

Batch 8630, train_perplexity=3936.8088, train_loss=8.278126

Batch 8640, train_perplexity=4034.567, train_loss=8.302654

Batch 8650, train_perplexity=3711.1643, train_loss=8.219101

Batch 8660, train_perplexity=4242.7637, train_loss=8.35297

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 8670, train_perplexity=3836.887, train_loss=8.252417

Batch 8680, train_perplexity=3795.0874, train_loss=8.241463

Batch 8690, train_perplexity=4176.4717, train_loss=8.337222

Batch 8700, train_perplexity=4130.094, train_loss=8.326056

Batch 8710, train_perplexity=4091.8772, train_loss=8.316759

Batch 8720, train_perplexity=4433.1973, train_loss=8.396876

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 8730, train_perplexity=4014.8728, train_loss=8.297761

Batch 8740, train_perplexity=4080.8562, train_loss=8.314062

Batch 8750, train_perplexity=4084.1733, train_loss=8.314875

Batch 8760, train_perplexity=3993.0657, train_loss=8.292315

Batch 8770, train_perplexity=3940.7532, train_loss=8.279127

Batch 8780, train_perplexity=3958.4336, train_loss=8.283604

Batch 8790, train_perplexity=3718.959, train_loss=8.221199

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 8800, train_perplexity=3886.575, train_loss=8.265284

Batch 8810, train_perplexity=4332.655, train_loss=8.373936

Batch 8820, train_perplexity=4065.0317, train_loss=8.310177

Batch 8830, train_perplexity=4395.3267, train_loss=8.388297

Batch 8840, train_perplexity=3482.4583, train_loss=8.155494

Batch 8850, train_perplexity=4283.82, train_loss=8.3626

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 8860, train_perplexity=4070.859, train_loss=8.311609

Batch 8870, train_perplexity=4017.9026, train_loss=8.298515

Batch 8880, train_perplexity=4345.0435, train_loss=8.376791

Batch 8890, train_perplexity=4098.9307, train_loss=8.318481

Batch 8900, train_perplexity=4167.8096, train_loss=8.335146

Batch 8910, train_perplexity=3969.0405, train_loss=8.28628

Batch 8920, train_perplexity=3926.0034, train_loss=8.275377

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 8930, train_perplexity=3863.8467, train_loss=8.2594185

Batch 8940, train_perplexity=4021.9702, train_loss=8.299527

Batch 8950, train_perplexity=4124.391, train_loss=8.324674

Batch 8960, train_perplexity=3768.8447, train_loss=8.234524

Batch 8970, train_perplexity=4113.145, train_loss=8.321943

Batch 8980, train_perplexity=3677.02, train_loss=8.209858

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 8990, train_perplexity=4141.4536, train_loss=8.328802

Batch 9000, train_perplexity=4205.6973, train_loss=8.344195

Batch 9010, train_perplexity=3812.427, train_loss=8.246021

Batch 9020, train_perplexity=3674.2158, train_loss=8.209095

Batch 9030, train_perplexity=4020.5015, train_loss=8.299162

Batch 9040, train_perplexity=4024.3682, train_loss=8.300123

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 9050, train_perplexity=3873.1953, train_loss=8.261835

Batch 9060, train_perplexity=4028.6729, train_loss=8.301192

Batch 9070, train_perplexity=4152.773, train_loss=8.331532

Batch 9080, train_perplexity=4022.3115, train_loss=8.299612

Batch 9090, train_perplexity=3428.6606, train_loss=8.139925

Batch 9100, train_perplexity=4123.2544, train_loss=8.324398

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 9110, train_perplexity=3560.3774, train_loss=8.177622

Batch 9120, train_perplexity=4047.9058, train_loss=8.305955

Batch 9130, train_perplexity=3862.3286, train_loss=8.259026

Batch 9140, train_perplexity=3894.3962, train_loss=8.267294

Batch 9150, train_perplexity=4534.3345, train_loss=8.419434

Batch 9160, train_perplexity=3508.4094, train_loss=8.162918

Batch 9170, train_perplexity=3847.9534, train_loss=8.255297

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 9180, train_perplexity=4087.127, train_loss=8.315598

Batch 9190, train_perplexity=3962.0518, train_loss=8.284517

Batch 9200, train_perplexity=3631.6003, train_loss=8.197429

Batch 9210, train_perplexity=3714.5176, train_loss=8.220004

Batch 9220, train_perplexity=3591.6365, train_loss=8.186363

Batch 9230, train_perplexity=4058.9578, train_loss=8.3086815

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 9240, train_perplexity=3807.0425, train_loss=8.244608

Batch 9250, train_perplexity=3944.863, train_loss=8.2801695

Batch 9260, train_perplexity=3861.7983, train_loss=8.258888

Batch 9270, train_perplexity=3966.441, train_loss=8.2856245

Batch 9280, train_perplexity=3765.2808, train_loss=8.233578

Batch 9290, train_perplexity=3790.8408, train_loss=8.240343

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 9300, train_perplexity=3834.129, train_loss=8.251698

Batch 9310, train_perplexity=3874.6877, train_loss=8.26222

Batch 9320, train_perplexity=3673.1157, train_loss=8.208796

Batch 9330, train_perplexity=4003.4065, train_loss=8.294901

Batch 9340, train_perplexity=4015.1487, train_loss=8.29783

Batch 9350, train_perplexity=3745.2043, train_loss=8.228231

Batch 9360, train_perplexity=3863.3013, train_loss=8.259277

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 9370, train_perplexity=3559.987, train_loss=8.177512

Batch 9380, train_perplexity=3987.16, train_loss=8.290834

Batch 9390, train_perplexity=3871.6775, train_loss=8.261443

Batch 9400, train_perplexity=3634.854, train_loss=8.198324

Batch 9410, train_perplexity=3989.168, train_loss=8.291338

Batch 9420, train_perplexity=3840.438, train_loss=8.253342

Batch 9430, train_perplexity=4115.6406, train_loss=8.32255

Batch 9440, train_perplexity=3935.7053, train_loss=8.277845

Batch 9450, train_perplexity=4013.9272, train_loss=8.297525

Batch 9460, train_perplexity=4166.4507, train_loss=8.33482

Batch 9470, train_perplexity=3921.1392, train_loss=8.2741375

Batch 9480, train_perplexity=3731.6208, train_loss=8.224598

Batch 9490, train_perplexity=3848.104, train_loss=8.255336

Batch 9500, train_perplexity=3978.0063, train_loss=8.288536

Batch 9510, train_perplexity=3813.4888, train_loss=8.2463

Batch 9520, train_perplexity=3893.921, train_loss=8.267172

Batch 9530, train_perplexity=4001.7117, train_loss=8.294477

Batch 9540, train_perplexity=3714.4502, train_loss=8.219986

Batch 9550, train_perplexity=3698.7378, train_loss=8.215747

Batch 9560, train_perplexity=3869.8132, train_loss=8.260962

Batch 9570, train_perplexity=3727.7437, train_loss=8.223558

Batch 9580, train_perplexity=3901.5188, train_loss=8.269121

Batch 9590, train_perplexity=3888.2507, train_loss=8.265715

Batch 9600, train_perplexity=4178.739, train_loss=8.337765

Batch 9610, train_perplexity=3769.6858, train_loss=8.234747

Batch 9620, train_perplexity=3879.502, train_loss=8.263462

Batch 9630, train_perplexity=3560.7034, train_loss=8.177713

Batch 9640, train_perplexity=3657.0269, train_loss=8.204406

Batch 9650, train_perplexity=3596.092, train_loss=8.187603

Batch 9660, train_perplexity=4049.6704, train_loss=8.306391

Batch 9670, train_perplexity=3948.4048, train_loss=8.281067

Batch 9680, train_perplexity=3641.4392, train_loss=8.200134

Batch 9690, train_perplexity=3394.2659, train_loss=8.129843

Batch 9700, train_perplexity=3788.9395, train_loss=8.239841

Batch 9710, train_perplexity=3465.3865, train_loss=8.150579

Batch 9720, train_perplexity=3940.5125, train_loss=8.279066
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 9730, train_perplexity=4220.157, train_loss=8.347628

Batch 9740, train_perplexity=3491.114, train_loss=8.157976

Batch 9750, train_perplexity=3690.799, train_loss=8.213598

Batch 9760, train_perplexity=3998.942, train_loss=8.293785

Batch 9770, train_perplexity=3797.676, train_loss=8.242145

Batch 9780, train_perplexity=3773.153, train_loss=8.235666

Batch 9790, train_perplexity=4032.2437, train_loss=8.302078

Batch 9800, train_perplexity=3979.0461, train_loss=8.288797

Batch 9810, train_perplexity=4169.9766, train_loss=8.335666

Batch 9820, train_perplexity=3922.725, train_loss=8.274542

Batch 9830, train_perplexity=3542.998, train_loss=8.172729

Batch 9840, train_perplexity=3790.913, train_loss=8.240362

Batch 9850, train_perplexity=3696.3398, train_loss=8.215098

Batch 9860, train_perplexity=3979.3496, train_loss=8.288874

Batch 9870, train_perplexity=3867.6328, train_loss=8.260398

Batch 9880, train_perplexity=3685.414, train_loss=8.212138

Batch 9890, train_perplexity=3996.8716, train_loss=8.293267

Batch 9900, train_perplexity=3779.6357, train_loss=8.237383

Batch 9910, train_perplexity=3723.8352, train_loss=8.222509

Batch 9920, train_perplexity=4093.501, train_loss=8.317156

Batch 9930, train_perplexity=3760.3682, train_loss=8.232272

Batch 9940, train_perplexity=3628.4604, train_loss=8.196564

Batch 9950, train_perplexity=3882.7073, train_loss=8.264288

Batch 9960, train_perplexity=4011.019, train_loss=8.296801

Batch 9970, train_perplexity=4164.2656, train_loss=8.334295

Batch 9980, train_perplexity=3910.0261, train_loss=8.271299

Batch 9990, train_perplexity=3912.2866, train_loss=8.271877

Batch 10000, train_perplexity=3873.8232, train_loss=8.261997

Batch 10010, train_perplexity=3608.6487, train_loss=8.191089

Batch 10020, train_perplexity=3790.671, train_loss=8.240298

Batch 10030, train_perplexity=3784.1477, train_loss=8.238576

Batch 10040, train_perplexity=3286.8623, train_loss=8.097689

Batch 10050, train_perplexity=3838.2043, train_loss=8.25276

Batch 10060, train_perplexity=3843.7283, train_loss=8.254198

Batch 10070, train_perplexity=3797.2305, train_loss=8.242027

Batch 10080, train_perplexity=3681.406, train_loss=8.21105

Batch 10090, train_perplexity=3599.0288, train_loss=8.188419

Batch 10100, train_perplexity=3869.4958, train_loss=8.2608795

Batch 10110, train_perplexity=3887.7131, train_loss=8.265576

Batch 10120, train_perplexity=3871.6443, train_loss=8.261435

Batch 10130, train_perplexity=3936.3623, train_loss=8.278012

Batch 10140, train_perplexity=3755.9705, train_loss=8.231102

Batch 10150, train_perplexity=3942.58, train_loss=8.279591

Batch 10160, train_perplexity=3473.8938, train_loss=8.153031

Batch 10170, train_perplexity=3599.8115, train_loss=8.188637

Batch 10180, train_perplexity=4002.7766, train_loss=8.294744

Batch 10190, train_perplexity=3831.4314, train_loss=8.250994

Batch 10200, train_perplexity=3891.9866, train_loss=8.266675

Batch 10210, train_perplexity=3515.9727, train_loss=8.1650715

Batch 10220, train_perplexity=4122.1455, train_loss=8.324129

Batch 10230, train_perplexity=3926.9546, train_loss=8.2756195

Batch 10240, train_perplexity=3663.9949, train_loss=8.206309

Batch 10250, train_perplexity=3712.9592, train_loss=8.219584

Batch 10260, train_perplexity=3705.4634, train_loss=8.217564

Batch 10270, train_perplexity=3736.0789, train_loss=8.225792

Batch 10280, train_perplexity=3738.6096, train_loss=8.226469

Batch 10290, train_perplexity=4280.095, train_loss=8.361731

Batch 10300, train_perplexity=4022.3616, train_loss=8.299624

Batch 10310, train_perplexity=3808.6768, train_loss=8.245037

Batch 10320, train_perplexity=3808.161, train_loss=8.244902

Batch 10330, train_perplexity=3717.3135, train_loss=8.220757

Batch 10340, train_perplexity=3791.0938, train_loss=8.24041

Batch 10350, train_perplexity=3611.0208, train_loss=8.191746

Batch 10360, train_perplexity=3928.764, train_loss=8.27608

Batch 10370, train_perplexity=3511.6968, train_loss=8.163855

Batch 10380, train_perplexity=3718.9163, train_loss=8.221188

Batch 10390, train_perplexity=3622.1128, train_loss=8.194813

Batch 10400, train_perplexity=3855.1455, train_loss=8.257164

Batch 10410, train_perplexity=3976.5613, train_loss=8.288173

Batch 10420, train_perplexity=4111.647, train_loss=8.321579

Batch 10430, train_perplexity=3628.869, train_loss=8.196676

Batch 10440, train_perplexity=3461.9546, train_loss=8.149589

Batch 10450, train_perplexity=3692.3198, train_loss=8.21401

Batch 10460, train_perplexity=3578.6167, train_loss=8.182732

Batch 10470, train_perplexity=3869.1416, train_loss=8.260788

Batch 10480, train_perplexity=4026.4758, train_loss=8.300647

Batch 10490, train_perplexity=3783.7039, train_loss=8.238459

Batch 10500, train_perplexity=3871.253, train_loss=8.261333

Batch 10510, train_perplexity=3440.5967, train_loss=8.1434

Batch 10520, train_perplexity=3637.878, train_loss=8.199156

Batch 10530, train_perplexity=3778.94, train_loss=8.237199

Batch 10540, train_perplexity=3283.0403, train_loss=8.096525

Batch 10550, train_perplexity=3793.6833, train_loss=8.241093

Batch 10560, train_perplexity=3893.9617, train_loss=8.267182

Batch 10570, train_perplexity=3655.1545, train_loss=8.203894

Batch 10580, train_perplexity=3722.9617, train_loss=8.222275

Batch 10590, train_perplexity=3679.595, train_loss=8.210558

Batch 10600, train_perplexity=4092.8684, train_loss=8.317001

Batch 10610, train_perplexity=3593.37, train_loss=8.186846

Batch 10620, train_perplexity=4065.8228, train_loss=8.310371

Batch 10630, train_perplexity=3675.1235, train_loss=8.209342

Batch 10640, train_perplexity=3705.4, train_loss=8.217546

Batch 10650, train_perplexity=4037.7388, train_loss=8.30344

Batch 10660, train_perplexity=3830.423, train_loss=8.2507305

Batch 10670, train_perplexity=3807.5508, train_loss=8.244741

Batch 10680, train_perplexity=3480.881, train_loss=8.155041

Batch 10690, train_perplexity=3679.4475, train_loss=8.210518

Batch 10700, train_perplexity=4048.0564, train_loss=8.305992

Batch 10710, train_perplexity=3788.1843, train_loss=8.239642

Batch 10720, train_perplexity=3870.4517, train_loss=8.2611265

Batch 10730, train_perplexity=3604.896, train_loss=8.190048

Batch 10740, train_perplexity=3589.137, train_loss=8.185667

Batch 10750, train_perplexity=3653.527, train_loss=8.203448

Batch 10760, train_perplexity=3955.66, train_loss=8.282903

Batch 10770, train_perplexity=3575.3794, train_loss=8.181827

Batch 10780, train_perplexity=3389.2844, train_loss=8.128374

Batch 10790, train_perplexity=3804.9954, train_loss=8.24407

Batch 10800, train_perplexity=3573.992, train_loss=8.181438

Batch 10810, train_perplexity=3891.0105, train_loss=8.266424

Batch 10820, train_perplexity=3991.1125, train_loss=8.291825

Batch 10830, train_perplexity=3749.271, train_loss=8.229317

Batch 10840, train_perplexity=3915.9224, train_loss=8.272806

Batch 10850, train_perplexity=3552.9458, train_loss=8.175532

Batch 10860, train_perplexity=3783.5884, train_loss=8.238428

Batch 10870, train_perplexity=3721.4, train_loss=8.221855

Batch 10880, train_perplexity=3655.0361, train_loss=8.203861

Batch 10890, train_perplexity=3258.952, train_loss=8.089161

Batch 10900, train_perplexity=3460.2776, train_loss=8.149104

Batch 10910, train_perplexity=3598.5896, train_loss=8.188297

Batch 10920, train_perplexity=3543.711, train_loss=8.17293

Batch 10930, train_perplexity=3843.7356, train_loss=8.2542

Batch 10940, train_perplexity=4133.763, train_loss=8.326943

Batch 10950, train_perplexity=3579.5452, train_loss=8.182991

Batch 10960, train_perplexity=3769.711, train_loss=8.234754

Batch 10970, train_perplexity=3449.579, train_loss=8.146008

Batch 10980, train_perplexity=3522.3225, train_loss=8.166876

Batch 10990, train_perplexity=3500.9895, train_loss=8.160801

Batch 11000, train_perplexity=3905.4126, train_loss=8.270119

Batch 11010, train_perplexity=3306.1968, train_loss=8.103554

Batch 11020, train_perplexity=3737.198, train_loss=8.226091

Batch 11030, train_perplexity=3995.5493, train_loss=8.292936

Batch 11040, train_perplexity=3486.6753, train_loss=8.156704

Batch 11050, train_perplexity=3579.0571, train_loss=8.182855

Batch 11060, train_perplexity=3832.937, train_loss=8.251387
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 11070, train_perplexity=3530.69, train_loss=8.169249

Batch 11080, train_perplexity=3473.2478, train_loss=8.152845

Batch 11090, train_perplexity=3677.953, train_loss=8.210112

Batch 11100, train_perplexity=3796.0212, train_loss=8.241709

Batch 11110, train_perplexity=3811.024, train_loss=8.245653

Batch 11120, train_perplexity=3523.9622, train_loss=8.167341

Batch 11130, train_perplexity=3805.5542, train_loss=8.244217

Batch 11140, train_perplexity=3694.4473, train_loss=8.214586

Batch 11150, train_perplexity=3616.5762, train_loss=8.193283

Batch 11160, train_perplexity=3417.61, train_loss=8.136697

Batch 11170, train_perplexity=3921.2214, train_loss=8.2741585

Batch 11180, train_perplexity=3769.7864, train_loss=8.234774

Batch 11190, train_perplexity=3589.8215, train_loss=8.185858

Batch 11200, train_perplexity=3584.789, train_loss=8.184455

Batch 11210, train_perplexity=3678.058, train_loss=8.21014

Batch 11220, train_perplexity=3462.1758, train_loss=8.1496525

Batch 11230, train_perplexity=3780.775, train_loss=8.237684

Batch 11240, train_perplexity=3769.8403, train_loss=8.234788

Batch 11250, train_perplexity=3812.0125, train_loss=8.245913

Batch 11260, train_perplexity=3557.0989, train_loss=8.176701

Batch 11270, train_perplexity=3465.9814, train_loss=8.150751

Batch 11280, train_perplexity=3935.709, train_loss=8.277846

Batch 11290, train_perplexity=3739.7007, train_loss=8.226761

Batch 11300, train_perplexity=3502.9868, train_loss=8.161371

Batch 11310, train_perplexity=3784.8767, train_loss=8.238769

Batch 11320, train_perplexity=3860.9146, train_loss=8.258659

Batch 11330, train_perplexity=3806.4907, train_loss=8.244463

Batch 11340, train_perplexity=3809.9773, train_loss=8.2453785

Batch 11350, train_perplexity=3741.313, train_loss=8.227192

Batch 11360, train_perplexity=3793.6472, train_loss=8.241083

Batch 11370, train_perplexity=3720.1294, train_loss=8.221514

Batch 11380, train_perplexity=3336.815, train_loss=8.112772

Batch 11390, train_perplexity=3591.979, train_loss=8.186459

Batch 11400, train_perplexity=3650.5493, train_loss=8.202633

Batch 11410, train_perplexity=3586.971, train_loss=8.185063

Batch 11420, train_perplexity=3363.2168, train_loss=8.120653

Batch 11430, train_perplexity=3791.0322, train_loss=8.240394

Batch 11440, train_perplexity=4085.8018, train_loss=8.315273

Batch 11450, train_perplexity=3802.826, train_loss=8.2435

Batch 11460, train_perplexity=3634.1953, train_loss=8.198143

Batch 11470, train_perplexity=3520.7642, train_loss=8.166433

Batch 11480, train_perplexity=3393.2236, train_loss=8.129536

Batch 11490, train_perplexity=3517.512, train_loss=8.165509

Batch 11500, train_perplexity=3644.5452, train_loss=8.200987

Batch 11510, train_perplexity=3530.6428, train_loss=8.169235

Batch 11520, train_perplexity=3454.4646, train_loss=8.147423

Batch 11530, train_perplexity=3793.3433, train_loss=8.241003

Batch 11540, train_perplexity=3574.851, train_loss=8.181679

Batch 11550, train_perplexity=3577.6204, train_loss=8.182453

Batch 11560, train_perplexity=3742.5264, train_loss=8.227516

Batch 11570, train_perplexity=3562.524, train_loss=8.178225

Batch 11580, train_perplexity=3853.3516, train_loss=8.256699

Batch 11590, train_perplexity=3695.4905, train_loss=8.214869

Batch 11600, train_perplexity=3454.3792, train_loss=8.147398

Batch 11610, train_perplexity=3600.924, train_loss=8.188946

Batch 11620, train_perplexity=3263.362, train_loss=8.090513

Batch 11630, train_perplexity=3712.814, train_loss=8.219545

Batch 11640, train_perplexity=3348.2332, train_loss=8.116188

Batch 11650, train_perplexity=3736.2927, train_loss=8.225849

Batch 11660, train_perplexity=3431.0225, train_loss=8.140614

Batch 11670, train_perplexity=3734.0095, train_loss=8.225238

Batch 11680, train_perplexity=3698.2368, train_loss=8.215611

Batch 11690, train_perplexity=3729.735, train_loss=8.2240925

Batch 11700, train_perplexity=3447.093, train_loss=8.145287

Batch 11710, train_perplexity=3577.2075, train_loss=8.182338

Batch 11720, train_perplexity=3642.0645, train_loss=8.200306

Batch 11730, train_perplexity=3613.7837, train_loss=8.192511

Batch 11740, train_perplexity=3614.3833, train_loss=8.192677

Batch 11750, train_perplexity=3593.1885, train_loss=8.186795

Batch 11760, train_perplexity=3689.0078, train_loss=8.213113

Batch 11770, train_perplexity=3599.2966, train_loss=8.188494

Batch 11780, train_perplexity=3704.1208, train_loss=8.217201

Batch 11790, train_perplexity=3171.0835, train_loss=8.061829

Batch 11800, train_perplexity=3390.1897, train_loss=8.128641

Batch 11810, train_perplexity=3588.0076, train_loss=8.185352

Batch 11820, train_perplexity=3749.9326, train_loss=8.229493

Batch 11830, train_perplexity=3524.9604, train_loss=8.167624

Batch 11840, train_perplexity=3963.2383, train_loss=8.284817

Batch 11850, train_perplexity=4004.514, train_loss=8.295177

Batch 11860, train_perplexity=3740.1465, train_loss=8.22688

Batch 11870, train_perplexity=3726.8196, train_loss=8.22331

Batch 11880, train_perplexity=3687.6396, train_loss=8.212742

Batch 11890, train_perplexity=3721.8115, train_loss=8.221966

Batch 11900, train_perplexity=3353.6274, train_loss=8.117798

Batch 11910, train_perplexity=3598.6514, train_loss=8.188314

Batch 11920, train_perplexity=3431.4314, train_loss=8.140733

Batch 11930, train_perplexity=3517.3208, train_loss=8.165455

Batch 11940, train_perplexity=3528.6165, train_loss=8.168661

Batch 11950, train_perplexity=3167.1213, train_loss=8.060578

Batch 11960, train_perplexity=3790.9094, train_loss=8.240361

Batch 11970, train_perplexity=3764.0925, train_loss=8.233262

Batch 11980, train_perplexity=3343.332, train_loss=8.114723

Batch 11990, train_perplexity=3529.0269, train_loss=8.168777

Batch 12000, train_perplexity=3412.5198, train_loss=8.135206

Batch 12010, train_perplexity=3588.2437, train_loss=8.185418

Batch 12020, train_perplexity=3172.9832, train_loss=8.0624275

Batch 12030, train_perplexity=3590.157, train_loss=8.185951

Batch 12040, train_perplexity=3505.2925, train_loss=8.162029

Batch 12050, train_perplexity=3404.4355, train_loss=8.132834

Batch 12060, train_perplexity=3827.509, train_loss=8.2499695

Batch 12070, train_perplexity=3835.8552, train_loss=8.252148

Batch 12080, train_perplexity=3518.9817, train_loss=8.165927

Batch 12090, train_perplexity=3853.3296, train_loss=8.256693

Batch 12100, train_perplexity=3464.8147, train_loss=8.150414

Batch 12110, train_perplexity=3535.5352, train_loss=8.17062

Batch 12120, train_perplexity=3363.3418, train_loss=8.12069

Batch 12130, train_perplexity=3394.9392, train_loss=8.130041

Batch 12140, train_perplexity=3571.0278, train_loss=8.180609

Batch 12150, train_perplexity=3557.7097, train_loss=8.176872

Batch 12160, train_perplexity=3775.5503, train_loss=8.236301

Batch 12170, train_perplexity=3391.2761, train_loss=8.128962

Batch 12180, train_perplexity=3040.8423, train_loss=8.01989

Batch 12190, train_perplexity=3373.6055, train_loss=8.123737

Batch 12200, train_perplexity=3728.629, train_loss=8.223796

Batch 12210, train_perplexity=3568.8557, train_loss=8.18

Batch 12220, train_perplexity=3384.8784, train_loss=8.127073

Batch 12230, train_perplexity=3434.748, train_loss=8.141699

Batch 12240, train_perplexity=3629.6094, train_loss=8.19688

Batch 12250, train_perplexity=3384.375, train_loss=8.1269245

Batch 12260, train_perplexity=3537.2417, train_loss=8.171103

Batch 12270, train_perplexity=3449.0989, train_loss=8.145868

Batch 12280, train_perplexity=3661.5847, train_loss=8.205651

Batch 12290, train_perplexity=3640.3628, train_loss=8.199839

Batch 12300, train_perplexity=3296.4744, train_loss=8.100609

Batch 12310, train_perplexity=3244.1853, train_loss=8.0846195

Batch 12320, train_perplexity=3275.6313, train_loss=8.094266

Batch 12330, train_perplexity=3726.528, train_loss=8.223232

Batch 12340, train_perplexity=4029.6372, train_loss=8.301432

Batch 12350, train_perplexity=3320.8528, train_loss=8.107977

Batch 12360, train_perplexity=3828.239, train_loss=8.25016

Batch 12370, train_perplexity=3424.6934, train_loss=8.138767

Batch 12380, train_perplexity=3486.825, train_loss=8.156747

Batch 12390, train_perplexity=3287.596, train_loss=8.097912
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 12400, train_perplexity=3245.8254, train_loss=8.085125

Batch 12410, train_perplexity=3551.103, train_loss=8.175014

Batch 12420, train_perplexity=3636.553, train_loss=8.1987915

Batch 12430, train_perplexity=3902.5383, train_loss=8.269382

Batch 12440, train_perplexity=3325.074, train_loss=8.109247

Batch 12450, train_perplexity=3496.9453, train_loss=8.159645

Batch 12460, train_perplexity=3584.4849, train_loss=8.18437

Batch 12470, train_perplexity=3570.374, train_loss=8.180426

Batch 12480, train_perplexity=3401.1384, train_loss=8.1318655

Batch 12490, train_perplexity=3540.09, train_loss=8.171907

Batch 12500, train_perplexity=4068.3904, train_loss=8.311003

Batch 12510, train_perplexity=3504.741, train_loss=8.161872

Batch 12520, train_perplexity=3321.974, train_loss=8.1083145

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 12530, train_perplexity=3277.9126, train_loss=8.094962

Batch 12540, train_perplexity=3415.476, train_loss=8.136072

Batch 12550, train_perplexity=3225.4534, train_loss=8.078829

Batch 12560, train_perplexity=3538.6353, train_loss=8.171496

Batch 12570, train_perplexity=3697.521, train_loss=8.215418

Batch 12580, train_perplexity=3550.5884, train_loss=8.174869

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 12590, train_perplexity=3166.7197, train_loss=8.0604515

Batch 12600, train_perplexity=3472.5522, train_loss=8.152645

Batch 12610, train_perplexity=3343.249, train_loss=8.114698

Batch 12620, train_perplexity=3483.468, train_loss=8.155784

Batch 12630, train_perplexity=3507.8474, train_loss=8.162758

Batch 12640, train_perplexity=3498.5264, train_loss=8.160097

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 12650, train_perplexity=3589.041, train_loss=8.18564

Batch 12660, train_perplexity=3508.296, train_loss=8.162886

Batch 12670, train_perplexity=3242.583, train_loss=8.0841255

Batch 12680, train_perplexity=3444.2737, train_loss=8.144468

Batch 12690, train_perplexity=3685.5723, train_loss=8.212181

Batch 12700, train_perplexity=3645.0144, train_loss=8.201116

Batch 12710, train_perplexity=3332.7632, train_loss=8.111557

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 12720, train_perplexity=3495.5515, train_loss=8.159246

Batch 12730, train_perplexity=3659.4866, train_loss=8.205078

Batch 12740, train_perplexity=3814.9766, train_loss=8.24669

Batch 12750, train_perplexity=3528.3337, train_loss=8.168581

Batch 12760, train_perplexity=3480.5425, train_loss=8.154943

Batch 12770, train_perplexity=3218.7793, train_loss=8.076757

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 12780, train_perplexity=3142.9812, train_loss=8.052927

Batch 12790, train_perplexity=3734.9104, train_loss=8.225479

Batch 12800, train_perplexity=3683.2708, train_loss=8.211556

Batch 12810, train_perplexity=3308.3416, train_loss=8.104202

Batch 12820, train_perplexity=3497.829, train_loss=8.159898

Batch 12830, train_perplexity=3574.1487, train_loss=8.181482

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 12840, train_perplexity=3259.3745, train_loss=8.089291

Batch 12850, train_perplexity=3674.356, train_loss=8.209133

Batch 12860, train_perplexity=3294.5479, train_loss=8.100024

Batch 12870, train_perplexity=3592.8972, train_loss=8.186714

Batch 12880, train_perplexity=3491.9197, train_loss=8.158207

Batch 12890, train_perplexity=3544.0254, train_loss=8.173018

Batch 12900, train_perplexity=3505.764, train_loss=8.162164

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 12910, train_perplexity=3191.6965, train_loss=8.068308

Batch 12920, train_perplexity=3525.4478, train_loss=8.167763

Batch 12930, train_perplexity=3574.2034, train_loss=8.181498

Batch 12940, train_perplexity=3408.1746, train_loss=8.133932

Batch 12950, train_perplexity=3268.0522, train_loss=8.091949

Batch 12960, train_perplexity=3521.3484, train_loss=8.166599

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 12970, train_perplexity=3609.327, train_loss=8.191277

Batch 12980, train_perplexity=3863.5627, train_loss=8.259345

Batch 12990, train_perplexity=3345.7688, train_loss=8.115452

Batch 13000, train_perplexity=3366.6567, train_loss=8.1216755

Batch 13010, train_perplexity=3297.984, train_loss=8.101067

Batch 13020, train_perplexity=3358.0056, train_loss=8.1191025

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 13030, train_perplexity=3607.1453, train_loss=8.190672

Batch 13040, train_perplexity=3491.7566, train_loss=8.15816

Batch 13050, train_perplexity=3238.374, train_loss=8.082827

Batch 13060, train_perplexity=3471.5522, train_loss=8.152357

Batch 13070, train_perplexity=3515.0876, train_loss=8.16482

Batch 13080, train_perplexity=3240.79, train_loss=8.083572

Batch 13090, train_perplexity=3382.3035, train_loss=8.126312

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 13100, train_perplexity=3487.9126, train_loss=8.157059

Batch 13110, train_perplexity=3388.3247, train_loss=8.128091

Batch 13120, train_perplexity=3184.8855, train_loss=8.066172

Batch 13130, train_perplexity=3477.583, train_loss=8.154093

Batch 13140, train_perplexity=3483.6775, train_loss=8.155844

Batch 13150, train_perplexity=3483.0728, train_loss=8.15567

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 13160, train_perplexity=3423.4983, train_loss=8.138418

Batch 13170, train_perplexity=3418.174, train_loss=8.136862

Batch 13180, train_perplexity=3426.5686, train_loss=8.139315

Batch 13190, train_perplexity=3516.6736, train_loss=8.165271

Batch 13200, train_perplexity=3302.4688, train_loss=8.102426

Batch 13210, train_perplexity=3531.7607, train_loss=8.169552

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 13220, train_perplexity=3051.405, train_loss=8.023357

Batch 13230, train_perplexity=3425.5295, train_loss=8.139011

Batch 13240, train_perplexity=3213.2585, train_loss=8.075041

Batch 13250, train_perplexity=3712.6724, train_loss=8.219507

Batch 13260, train_perplexity=3470.7908, train_loss=8.152138

Batch 13270, train_perplexity=3301.3823, train_loss=8.102097

Batch 13280, train_perplexity=3446.4421, train_loss=8.145098

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 13290, train_perplexity=3538.3113, train_loss=8.171405

Batch 13300, train_perplexity=3347.9744, train_loss=8.116111

Batch 13310, train_perplexity=3702.4646, train_loss=8.216754

Batch 13320, train_perplexity=3202.5754, train_loss=8.071711

Batch 13330, train_perplexity=3220.806, train_loss=8.077387

Batch 13340, train_perplexity=3499.6743, train_loss=8.160425

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 13350, train_perplexity=3565.2634, train_loss=8.178993

Batch 13360, train_perplexity=3477.0525, train_loss=8.15394

Batch 13370, train_perplexity=3377.52, train_loss=8.124897

Batch 13380, train_perplexity=3505.9077, train_loss=8.162205

Batch 13390, train_perplexity=3111.5173, train_loss=8.042866

Batch 13400, train_perplexity=3475.6267, train_loss=8.15353

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 13410, train_perplexity=3316.479, train_loss=8.106659

Batch 13420, train_perplexity=3316.5486, train_loss=8.10668

Batch 13430, train_perplexity=3516.4756, train_loss=8.165215

Batch 13440, train_perplexity=3629.9453, train_loss=8.196973

Batch 13450, train_perplexity=3454.399, train_loss=8.147404

Batch 13460, train_perplexity=3456.9133, train_loss=8.148131

Batch 13470, train_perplexity=3337.7695, train_loss=8.113058

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 13480, train_perplexity=3348.7122, train_loss=8.116331

Batch 13490, train_perplexity=3327.1072, train_loss=8.1098585

Batch 13500, train_perplexity=3349.0442, train_loss=8.11643

Batch 13510, train_perplexity=3593.4866, train_loss=8.186878

Batch 13520, train_perplexity=3254.318, train_loss=8.087738

Batch 13530, train_perplexity=3565.7632, train_loss=8.179133

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 13540, train_perplexity=3262.6182, train_loss=8.090285

Batch 13550, train_perplexity=3266.9429, train_loss=8.09161

Batch 13560, train_perplexity=3429.9165, train_loss=8.140291

Batch 13570, train_perplexity=3103.125, train_loss=8.040165

Batch 13580, train_perplexity=3600.4915, train_loss=8.188826

Batch 13590, train_perplexity=3672.8916, train_loss=8.2087345

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 13600, train_perplexity=3316.8774, train_loss=8.106779

Batch 13610, train_perplexity=3592.5203, train_loss=8.186609

Batch 13620, train_perplexity=3316.6055, train_loss=8.106697

Batch 13630, train_perplexity=3563.5435, train_loss=8.178511

Batch 13640, train_perplexity=3599.4478, train_loss=8.188536

Batch 13650, train_perplexity=3349.6, train_loss=8.116596

Batch 13660, train_perplexity=3220.3145, train_loss=8.077234

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 13670, train_perplexity=3351.498, train_loss=8.117163

Batch 13680, train_perplexity=3228.7803, train_loss=8.07986

Batch 13690, train_perplexity=3191.456, train_loss=8.068233

Batch 13700, train_perplexity=3532.7244, train_loss=8.169825

Batch 13710, train_perplexity=3241.2166, train_loss=8.083704

Batch 13720, train_perplexity=3462.6313, train_loss=8.149784

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 13730, train_perplexity=3374.1333, train_loss=8.123894
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 13740, train_perplexity=3663.2786, train_loss=8.206114

Batch 13750, train_perplexity=3104.1165, train_loss=8.040484

Batch 13760, train_perplexity=3090.4553, train_loss=8.036074

Batch 13770, train_perplexity=3275.6409, train_loss=8.094269

Batch 13780, train_perplexity=3437.9727, train_loss=8.142637

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 13790, train_perplexity=3703.8313, train_loss=8.217123

Batch 13800, train_perplexity=3474.4536, train_loss=8.1531925

Batch 13810, train_perplexity=3529.7607, train_loss=8.168985

Batch 13820, train_perplexity=3386.2087, train_loss=8.127466

Batch 13830, train_perplexity=3402.5237, train_loss=8.132273

Batch 13840, train_perplexity=3491.0273, train_loss=8.157951

Batch 13850, train_perplexity=3204.3108, train_loss=8.072252

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 13860, train_perplexity=3432.8618, train_loss=8.1411495

Batch 13870, train_perplexity=3319.4534, train_loss=8.107555

Batch 13880, train_perplexity=3014.1873, train_loss=8.0110855

Batch 13890, train_perplexity=3154.5122, train_loss=8.056589

Batch 13900, train_perplexity=3728.846, train_loss=8.223854

Batch 13910, train_perplexity=3315.0686, train_loss=8.106234

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 13920, train_perplexity=2949.006, train_loss=7.9892235

Batch 13930, train_perplexity=3361.3057, train_loss=8.120085

Batch 13940, train_perplexity=3297.0845, train_loss=8.100794

Batch 13950, train_perplexity=3508.289, train_loss=8.162884

Batch 13960, train_perplexity=3448.9607, train_loss=8.145828

Batch 13970, train_perplexity=2933.177, train_loss=7.9838414

Batch 13980, train_perplexity=3125.2747, train_loss=8.047277

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 13990, train_perplexity=3292.1953, train_loss=8.09931

Batch 14000, train_perplexity=3386.945, train_loss=8.127684

Batch 14010, train_perplexity=3365.5623, train_loss=8.12135

Batch 14020, train_perplexity=3368.0505, train_loss=8.122089

Batch 14030, train_perplexity=3254.9294, train_loss=8.087926

Batch 14040, train_perplexity=3186.4592, train_loss=8.066666

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 14050, train_perplexity=3216.3704, train_loss=8.076009

Batch 14060, train_perplexity=3338.5686, train_loss=8.113297

Batch 14070, train_perplexity=3492.1362, train_loss=8.158269

Batch 14080, train_perplexity=3349.6958, train_loss=8.116625

Batch 14090, train_perplexity=3285.4583, train_loss=8.097261

Batch 14100, train_perplexity=3647.313, train_loss=8.201746

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 14110, train_perplexity=3249.8582, train_loss=8.086367

Batch 14120, train_perplexity=3080.2656, train_loss=8.032771

Batch 14130, train_perplexity=3078.3127, train_loss=8.032137

Batch 14140, train_perplexity=3604.339, train_loss=8.189894

Batch 14150, train_perplexity=3548.9463, train_loss=8.174406

Batch 14160, train_perplexity=3319.3203, train_loss=8.107515

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 14170, train_perplexity=3252.934, train_loss=8.087313

Batch 14180, train_perplexity=3460.6902, train_loss=8.149223

Batch 14190, train_perplexity=3463.9988, train_loss=8.150179

Batch 14200, train_perplexity=3365.4307, train_loss=8.121311

Batch 14210, train_perplexity=3242.9448, train_loss=8.084237

Batch 14220, train_perplexity=3269.8325, train_loss=8.092494

Batch 14230, train_perplexity=3330.1992, train_loss=8.110787

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 14240, train_perplexity=3185.4993, train_loss=8.066364

Batch 14250, train_perplexity=3372.097, train_loss=8.12329

Batch 14260, train_perplexity=3716.2927, train_loss=8.220482

Batch 14270, train_perplexity=3172.118, train_loss=8.062155

Batch 14280, train_perplexity=3143.5867, train_loss=8.05312

Batch 14290, train_perplexity=3433.1826, train_loss=8.141243

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 14300, train_perplexity=3577.4907, train_loss=8.182417

Batch 14310, train_perplexity=3296.286, train_loss=8.100552

Batch 14320, train_perplexity=3390.096, train_loss=8.128613

Batch 14330, train_perplexity=3682.4243, train_loss=8.211327

Batch 14340, train_perplexity=3240.3572, train_loss=8.083439

Batch 14350, train_perplexity=3341.649, train_loss=8.11422

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 14360, train_perplexity=3585.0112, train_loss=8.184517

Batch 14370, train_perplexity=3229.3315, train_loss=8.08003

Batch 14380, train_perplexity=3128.012, train_loss=8.048153

Batch 14390, train_perplexity=3498.1328, train_loss=8.159985

Batch 14400, train_perplexity=3317.3457, train_loss=8.10692
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 14410, train_perplexity=3307.8586, train_loss=8.104056

Batch 14420, train_perplexity=3190.89, train_loss=8.068055

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 14430, train_perplexity=3313.1375, train_loss=8.105651

Batch 14440, train_perplexity=3264.794, train_loss=8.090952

Batch 14450, train_perplexity=3051.3933, train_loss=8.023354

Batch 14460, train_perplexity=3284.9915, train_loss=8.097119

Batch 14470, train_perplexity=3163.176, train_loss=8.059332

Batch 14480, train_perplexity=3240.4314, train_loss=8.083462

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 14490, train_perplexity=3461.479, train_loss=8.149451

Batch 14500, train_perplexity=2985.6838, train_loss=8.001584

Batch 14510, train_perplexity=3181.783, train_loss=8.065197

Batch 14520, train_perplexity=3219.8845, train_loss=8.077101

Batch 14530, train_perplexity=3205.4722, train_loss=8.072615

Batch 14540, train_perplexity=3316.811, train_loss=8.106759

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 14550, train_perplexity=3527.3984, train_loss=8.168316

Batch 14560, train_perplexity=3088.817, train_loss=8.035543

Batch 14570, train_perplexity=3133.3264, train_loss=8.04985

Batch 14580, train_perplexity=3174.1575, train_loss=8.062798

Batch 14590, train_perplexity=3323.381, train_loss=8.108738

Batch 14600, train_perplexity=3377.1433, train_loss=8.124785

Batch 14610, train_perplexity=3163.9485, train_loss=8.059576

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 14620, train_perplexity=3042.7744, train_loss=8.020525

Batch 14630, train_perplexity=3451.399, train_loss=8.146535

Batch 14640, train_perplexity=3277.0845, train_loss=8.094709

Batch 14650, train_perplexity=3267.6348, train_loss=8.091822

Batch 14660, train_perplexity=3575.1987, train_loss=8.181776

Batch 14670, train_perplexity=3599.894, train_loss=8.18866

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 14680, train_perplexity=3216.5085, train_loss=8.076052

Batch 14690, train_perplexity=3255.625, train_loss=8.08814

Batch 14700, train_perplexity=3138.141, train_loss=8.051386

Batch 14710, train_perplexity=3265.1987, train_loss=8.091076

Batch 14720, train_perplexity=3209.8342, train_loss=8.073975

Batch 14730, train_perplexity=3301.4202, train_loss=8.102108

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 14740, train_perplexity=3309.0608, train_loss=8.10442

Batch 14750, train_perplexity=3153.4595, train_loss=8.056255

Batch 14760, train_perplexity=3188.3591, train_loss=8.067262

Batch 14770, train_perplexity=3420.4373, train_loss=8.137524

Batch 14780, train_perplexity=3238.825, train_loss=8.082966

Batch 14790, train_perplexity=3216.5452, train_loss=8.076063

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 14800, train_perplexity=3379.1375, train_loss=8.125376

Batch 14810, train_perplexity=3407.843, train_loss=8.133835

Batch 14820, train_perplexity=3379.4146, train_loss=8.125458

Batch 14830, train_perplexity=3174.5632, train_loss=8.062925

Batch 14840, train_perplexity=3185.7026, train_loss=8.066428

Batch 14850, train_perplexity=3278.1033, train_loss=8.09502

Batch 14860, train_perplexity=3403.1274, train_loss=8.13245

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 14870, train_perplexity=3213.6814, train_loss=8.075172

Batch 14880, train_perplexity=3252.289, train_loss=8.087114

Batch 14890, train_perplexity=3046.6245, train_loss=8.02179

Batch 14900, train_perplexity=3115.407, train_loss=8.044115

Batch 14910, train_perplexity=3459.3835, train_loss=8.148846

Batch 14920, train_perplexity=3613.1082, train_loss=8.192324

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 14930, train_perplexity=3404.9844, train_loss=8.132996

Batch 14940, train_perplexity=3352.5945, train_loss=8.11749

Batch 14950, train_perplexity=3025.4275, train_loss=8.014808

Batch 14960, train_perplexity=3292.4153, train_loss=8.099377

Batch 14970, train_perplexity=3567.0625, train_loss=8.179498

Batch 14980, train_perplexity=3263.3123, train_loss=8.090498

Batch 14990, train_perplexity=3234.6392, train_loss=8.081673

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 15000, train_perplexity=3361.668, train_loss=8.120193

Batch 15010, train_perplexity=3243.3438, train_loss=8.08436

Batch 15020, train_perplexity=3272.918, train_loss=8.093437

Batch 15030, train_perplexity=3220.253, train_loss=8.077215

Batch 15040, train_perplexity=3266.6377, train_loss=8.0915165

Batch 15050, train_perplexity=3020.852, train_loss=8.013294

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 15060, train_perplexity=3268.0867, train_loss=8.09196

Batch 15070, train_perplexity=3279.0444, train_loss=8.095307

Batch 15080, train_perplexity=2952.6982, train_loss=7.9904747

Batch 15090, train_perplexity=3235.4167, train_loss=8.081913

Batch 15100, train_perplexity=3094.9207, train_loss=8.037518

Batch 15110, train_perplexity=3167.4263, train_loss=8.060675

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 15120, train_perplexity=3482.9932, train_loss=8.155647

Batch 15130, train_perplexity=3097.918, train_loss=8.038486

Batch 15140, train_perplexity=3265.1924, train_loss=8.091074

Batch 15150, train_perplexity=3196.2534, train_loss=8.069735

Batch 15160, train_perplexity=3158.9617, train_loss=8.057999

Batch 15170, train_perplexity=3307.2751, train_loss=8.10388

Batch 15180, train_perplexity=3441.8406, train_loss=8.143762

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 15190, train_perplexity=3324.2876, train_loss=8.109011

Batch 15200, train_perplexity=3300.6772, train_loss=8.101883

Batch 15210, train_perplexity=3448.2373, train_loss=8.145618

Batch 15220, train_perplexity=3163.2002, train_loss=8.05934

Batch 15230, train_perplexity=3310.8506, train_loss=8.10496

Batch 15240, train_perplexity=3278.044, train_loss=8.095002

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 15250, train_perplexity=3346.286, train_loss=8.115606

Batch 15260, train_perplexity=3098.4084, train_loss=8.038644

Batch 15270, train_perplexity=3276.353, train_loss=8.094486

Batch 15280, train_perplexity=3266.0178, train_loss=8.091327

Batch 15290, train_perplexity=2910.233, train_loss=7.9759884

Batch 15300, train_perplexity=3649.0664, train_loss=8.202227

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 15310, train_perplexity=3405.8223, train_loss=8.133242

Batch 15320, train_perplexity=3024.5103, train_loss=8.014504

Batch 15330, train_perplexity=2908.446, train_loss=7.975374

Batch 15340, train_perplexity=3178.465, train_loss=8.064154

Batch 15350, train_perplexity=3260.1082, train_loss=8.089516

Batch 15360, train_perplexity=3192.4424, train_loss=8.068542

Batch 15370, train_perplexity=2903.2556, train_loss=7.973588

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 15380, train_perplexity=3344.1006, train_loss=8.114953

Batch 15390, train_perplexity=3488.3516, train_loss=8.157185

Batch 15400, train_perplexity=3119.8845, train_loss=8.045551

Batch 15410, train_perplexity=3236.4043, train_loss=8.082218

Batch 15420, train_perplexity=3154.765, train_loss=8.056669

Batch 15430, train_perplexity=3257.5754, train_loss=8.088738

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 15440, train_perplexity=3219.4238, train_loss=8.076958

Batch 15450, train_perplexity=2986.5667, train_loss=8.00188

Batch 15460, train_perplexity=3082.449, train_loss=8.03348

Batch 15470, train_perplexity=3062.2554, train_loss=8.026907

Batch 15480, train_perplexity=3439.8945, train_loss=8.143196

Batch 15490, train_perplexity=3387.4265, train_loss=8.127826

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 15500, train_perplexity=3100.646, train_loss=8.039366

Batch 15510, train_perplexity=3534.6519, train_loss=8.17037

Batch 15520, train_perplexity=3127.0544, train_loss=8.047847

Batch 15530, train_perplexity=3295.789, train_loss=8.100401

Batch 15540, train_perplexity=3217.165, train_loss=8.076256

Batch 15550, train_perplexity=3324.5476, train_loss=8.109089

Batch 15560, train_perplexity=3200.612, train_loss=8.071097

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 15570, train_perplexity=3324.8584, train_loss=8.109182

Batch 15580, train_perplexity=2975.1416, train_loss=7.998047

Batch 15590, train_perplexity=3241.702, train_loss=8.083854

Batch 15600, train_perplexity=3318.6904, train_loss=8.107326

Batch 15610, train_perplexity=3399.3484, train_loss=8.131339

Batch 15620, train_perplexity=3304.715, train_loss=8.103106

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 15630, train_perplexity=3164.5913, train_loss=8.059779

Batch 15640, train_perplexity=3004.673, train_loss=8.007924

Batch 15650, train_perplexity=3214.0615, train_loss=8.075291

Batch 15660, train_perplexity=3446.015, train_loss=8.144974

Batch 15670, train_perplexity=3168.4656, train_loss=8.061003

Batch 15680, train_perplexity=3088.4224, train_loss=8.035416

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loadingWARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 15690, train_perplexity=3712.4104, train_loss=8.219437

Batch 15700, train_perplexity=3148.2368, train_loss=8.054598

Batch 15710, train_perplexity=3056.0354, train_loss=8.024874

Batch 15720, train_perplexity=3176.9287, train_loss=8.06367

Batch 15730, train_perplexity=3165.2039, train_loss=8.059973

Batch 15740, train_perplexity=3326.425, train_loss=8.109653

Batch 15750, train_perplexity=3329.9038, train_loss=8.110699

Batch 15760, train_perplexity=2951.6565, train_loss=7.990122

Batch 15770, train_perplexity=3089.4592, train_loss=8.035751

Batch 15780, train_perplexity=3218.3003, train_loss=8.076609

Batch 15790, train_perplexity=3047.2695, train_loss=8.022001

Batch 15800, train_perplexity=2937.4053, train_loss=7.985282

Batch 15810, train_perplexity=3281.2844, train_loss=8.09599

Batch 15820, train_perplexity=3045.84, train_loss=8.021532

Batch 15830, train_perplexity=3165.4333, train_loss=8.060045

Batch 15840, train_perplexity=2882.4292, train_loss=7.9663887

Batch 15850, train_perplexity=3200.9326, train_loss=8.0711975

Batch 15860, train_perplexity=3112.8232, train_loss=8.043285

Batch 15870, train_perplexity=3190.8625, train_loss=8.068047

Batch 15880, train_perplexity=3280.624, train_loss=8.095789

Batch 15890, train_perplexity=3298.4556, train_loss=8.10121

Batch 15900, train_perplexity=2931.1008, train_loss=7.9831333

Batch 15910, train_perplexity=3042.8787, train_loss=8.020559

Batch 15920, train_perplexity=3336.802, train_loss=8.112768

Batch 15930, train_perplexity=3090.1428, train_loss=8.035973

Batch 15940, train_perplexity=3099.8506, train_loss=8.039109

Batch 15950, train_perplexity=2940.5586, train_loss=7.986355

Batch 15960, train_perplexity=3142.8762, train_loss=8.052894

Batch 15970, train_perplexity=3009.8384, train_loss=8.009642

Batch 15980, train_perplexity=3192.951, train_loss=8.068701

Batch 15990, train_perplexity=3384.856, train_loss=8.127067

Batch 16000, train_perplexity=3039.0217, train_loss=8.019291

Batch 16010, train_perplexity=3014.2534, train_loss=8.011107

Batch 16020, train_perplexity=3091.0063, train_loss=8.036252

Batch 16030, train_perplexity=3384.6267, train_loss=8.126999

Batch 16040, train_perplexity=3190.9114, train_loss=8.068062

Batch 16050, train_perplexity=3357.4355, train_loss=8.118933

Batch 16060, train_perplexity=3284.3054, train_loss=8.09691

Batch 16070, train_perplexity=3261.6104, train_loss=8.089976

Batch 16080, train_perplexity=3066.0044, train_loss=8.028131

Batch 16090, train_perplexity=3190.8535, train_loss=8.068044

Batch 16100, train_perplexity=3499.0002, train_loss=8.160233

Batch 16110, train_perplexity=2943.0386, train_loss=7.987198

Batch 16120, train_perplexity=3011.3716, train_loss=8.010151

Batch 16130, train_perplexity=3034.9841, train_loss=8.0179615

Batch 16140, train_perplexity=3074.1528, train_loss=8.030785

Batch 16150, train_perplexity=2955.9102, train_loss=7.991562

Batch 16160, train_perplexity=3132.6123, train_loss=8.049623

Batch 16170, train_perplexity=2981.7456, train_loss=8.000264

Batch 16180, train_perplexity=3197.4424, train_loss=8.0701065

Batch 16190, train_perplexity=2985.752, train_loss=8.001607

Batch 16200, train_perplexity=3142.6873, train_loss=8.052834

Batch 16210, train_perplexity=3143.9614, train_loss=8.053239

Batch 16220, train_perplexity=3153.4111, train_loss=8.05624

Batch 16230, train_perplexity=3289.189, train_loss=8.098396

Batch 16240, train_perplexity=3520.143, train_loss=8.166257

Batch 16250, train_perplexity=2961.2627, train_loss=7.993371

Batch 16260, train_perplexity=3112.8677, train_loss=8.0433

Batch 16270, train_perplexity=3080.862, train_loss=8.032965

Batch 16280, train_perplexity=3252.016, train_loss=8.08703

Batch 16290, train_perplexity=3054.229, train_loss=8.024282

Batch 16300, train_perplexity=3050.6948, train_loss=8.023125

Batch 16310, train_perplexity=3315.8591, train_loss=8.106472

Batch 16320, train_perplexity=3164.377, train_loss=8.059711

Batch 16330, train_perplexity=3052.6333, train_loss=8.02376

Batch 16340, train_perplexity=3149.7324, train_loss=8.055073

Batch 16350, train_perplexity=3107.2117, train_loss=8.041481

Batch 16360, train_perplexity=3321.3818, train_loss=8.108136

Batch 16370, train_perplexity=2836.0044, train_loss=7.9501514

Batch 16380, train_perplexity=2926.481, train_loss=7.981556

Batch 16390, train_perplexity=3152.828, train_loss=8.056055

Batch 16400, train_perplexity=3203.6445, train_loss=8.072044

Batch 16410, train_perplexity=3268.8098, train_loss=8.092181

Batch 16420, train_perplexity=3107.7805, train_loss=8.041664

Batch 16430, train_perplexity=3236.6265, train_loss=8.082287

Batch 16440, train_perplexity=3339.7756, train_loss=8.113659

Batch 16450, train_perplexity=3018.528, train_loss=8.012525

Batch 16460, train_perplexity=3208.243, train_loss=8.073479

Batch 16470, train_perplexity=3125.9333, train_loss=8.047488

Batch 16480, train_perplexity=3380.6394, train_loss=8.12582

Batch 16490, train_perplexity=2962.2075, train_loss=7.99369

Batch 16500, train_perplexity=3592.068, train_loss=8.186483

Batch 16510, train_perplexity=2885.1382, train_loss=7.967328

Batch 16520, train_perplexity=3262.9824, train_loss=8.090397

Batch 16530, train_perplexity=2993.8298, train_loss=8.004309

Batch 16540, train_perplexity=3127.4214, train_loss=8.047964

Batch 16550, train_perplexity=3028.6694, train_loss=8.015879

Batch 16560, train_perplexity=3267.0613, train_loss=8.091646

Batch 16570, train_perplexity=3242.886, train_loss=8.084219

Batch 16580, train_perplexity=3013.3997, train_loss=8.010824

Batch 16590, train_perplexity=3062.127, train_loss=8.026865

Batch 16600, train_perplexity=3359.3958, train_loss=8.119516

Batch 16610, train_perplexity=3332.8774, train_loss=8.111591

Batch 16620, train_perplexity=3200.0476, train_loss=8.070921

Batch 16630, train_perplexity=3157.233, train_loss=8.057451

Batch 16640, train_perplexity=2913.2097, train_loss=7.9770107

Batch 16650, train_perplexity=2793.344, train_loss=7.9349947

Batch 16660, train_perplexity=3179.6929, train_loss=8.06454

Batch 16670, train_perplexity=3191.94, train_loss=8.068384

Batch 16680, train_perplexity=2979.0142, train_loss=7.9993477

Batch 16690, train_perplexity=3138.8564, train_loss=8.051614

Batch 16700, train_perplexity=3308.0134, train_loss=8.104103

Batch 16710, train_perplexity=3019.101, train_loss=8.012714

Batch 16720, train_perplexity=3497.7424, train_loss=8.159873

Batch 16730, train_perplexity=3159.9138, train_loss=8.0583

Batch 16740, train_perplexity=3326.0286, train_loss=8.109534

Batch 16750, train_perplexity=3109.945, train_loss=8.04236

Batch 16760, train_perplexity=3111.7783, train_loss=8.04295

Batch 16770, train_perplexity=3102.1013, train_loss=8.039835

Batch 16780, train_perplexity=3069.9573, train_loss=8.029419

Batch 16790, train_perplexity=3368.5774, train_loss=8.122246

Batch 16800, train_perplexity=3232.5544, train_loss=8.081028

Batch 16810, train_perplexity=3378.9375, train_loss=8.125317

Batch 16820, train_perplexity=3210.8386, train_loss=8.074287

Batch 16830, train_perplexity=3311.8992, train_loss=8.105277

Batch 16840, train_perplexity=3096.5415, train_loss=8.038041

Batch 16850, train_perplexity=3418.1545, train_loss=8.136856

Batch 16860, train_perplexity=3072.7634, train_loss=8.030333

Batch 16870, train_perplexity=3185.1377, train_loss=8.066251

Batch 16880, train_perplexity=3116.6194, train_loss=8.044504

Batch 16890, train_perplexity=3263.779, train_loss=8.090641

Batch 16900, train_perplexity=3215.429, train_loss=8.075716

Batch 16910, train_perplexity=3279.4417, train_loss=8.095428

Batch 16920, train_perplexity=3087.0942, train_loss=8.034986

Batch 16930, train_perplexity=2877.1426, train_loss=7.964553

Batch 16940, train_perplexity=3166.7227, train_loss=8.060452

Batch 16950, train_perplexity=3343.8296, train_loss=8.114872

Batch 16960, train_perplexity=3072.8162, train_loss=8.03035

Batch 16970, train_perplexity=2951.8733, train_loss=7.9901953

Batch 16980, train_perplexity=3131.1519, train_loss=8.049156

Batch 16990, train_perplexity=3329.0337, train_loss=8.110437

Batch 17000, train_perplexity=3120.5422, train_loss=8.045762

Batch 17010, train_perplexity=2980.5657, train_loss=7.9998684
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 17020, train_perplexity=2790.805, train_loss=7.9340854

Batch 17030, train_perplexity=3221.8657, train_loss=8.077716

Batch 17040, train_perplexity=3046.6072, train_loss=8.021784

Batch 17050, train_perplexity=3247.0732, train_loss=8.085509

Batch 17060, train_perplexity=3057.1665, train_loss=8.025244

Batch 17070, train_perplexity=2864.0676, train_loss=7.959998

Batch 17080, train_perplexity=2914.7102, train_loss=7.9775257

Batch 17090, train_perplexity=3234.01, train_loss=8.081478

Batch 17100, train_perplexity=3231.9165, train_loss=8.080831

Batch 17110, train_perplexity=3243.18, train_loss=8.08431

Batch 17120, train_perplexity=3008.4236, train_loss=8.0091715

Batch 17130, train_perplexity=2979.0027, train_loss=7.999344

Batch 17140, train_perplexity=3110.5264, train_loss=8.042547

Batch 17150, train_perplexity=2977.357, train_loss=7.998791

Batch 17160, train_perplexity=3109.248, train_loss=8.042136

Batch 17170, train_perplexity=3105.8606, train_loss=8.041046

Batch 17180, train_perplexity=2967.666, train_loss=7.995531

Batch 17190, train_perplexity=2961.7837, train_loss=7.993547

Batch 17200, train_perplexity=3328.5703, train_loss=8.110298

Batch 17210, train_perplexity=3458.9612, train_loss=8.148724

Batch 17220, train_perplexity=3157.3171, train_loss=8.057478

Batch 17230, train_perplexity=3132.451, train_loss=8.049571

Batch 17240, train_perplexity=3065.1128, train_loss=8.02784

Batch 17250, train_perplexity=3053.9436, train_loss=8.024189

Batch 17260, train_perplexity=3060.9502, train_loss=8.026481

Batch 17270, train_perplexity=3095.387, train_loss=8.037668

Batch 17280, train_perplexity=3075.6775, train_loss=8.0312805

Batch 17290, train_perplexity=2881.8604, train_loss=7.9661913

Batch 17300, train_perplexity=3033.3232, train_loss=8.017414

Batch 17310, train_perplexity=3124.053, train_loss=8.046886

Batch 17320, train_perplexity=2989.8809, train_loss=8.002989

Batch 17330, train_perplexity=2982.7524, train_loss=8.000602

Batch 17340, train_perplexity=2987.4668, train_loss=8.002181

Batch 17350, train_perplexity=3223.596, train_loss=8.078253

Batch 17360, train_perplexity=2993.2817, train_loss=8.004126

Batch 17370, train_perplexity=3160.0403, train_loss=8.05834

Batch 17380, train_perplexity=3028.7358, train_loss=8.015901

Batch 17390, train_perplexity=3029.5708, train_loss=8.016176

Batch 17400, train_perplexity=3195.8816, train_loss=8.069618

Batch 17410, train_perplexity=2882.344, train_loss=7.966359

Batch 17420, train_perplexity=2947.2544, train_loss=7.9886293

Batch 17430, train_perplexity=3004.0227, train_loss=8.007708

Batch 17440, train_perplexity=3101.761, train_loss=8.039725

Batch 17450, train_perplexity=2804.5469, train_loss=7.9389973

Batch 17460, train_perplexity=3306.2566, train_loss=8.103572

Batch 17470, train_perplexity=3041.2715, train_loss=8.020031

Batch 17480, train_perplexity=2759.5042, train_loss=7.9228063

Batch 17490, train_perplexity=3298.0466, train_loss=8.101086

Batch 17500, train_perplexity=2823.7627, train_loss=7.9458256

Batch 17510, train_perplexity=2947.1533, train_loss=7.988595

Batch 17520, train_perplexity=3036.0322, train_loss=8.018307

Batch 17530, train_perplexity=2818.335, train_loss=7.9439015

Batch 17540, train_perplexity=3222.5632, train_loss=8.077932

Batch 17550, train_perplexity=3398.82, train_loss=8.131184

Batch 17560, train_perplexity=2932.2527, train_loss=7.983526

Batch 17570, train_perplexity=3259.4927, train_loss=8.089327

Batch 17580, train_perplexity=3291.712, train_loss=8.099163

Batch 17590, train_perplexity=3230.2957, train_loss=8.080329

Batch 17600, train_perplexity=2936.0554, train_loss=7.9848223

Batch 17610, train_perplexity=3082.9253, train_loss=8.033634

Batch 17620, train_perplexity=2901.271, train_loss=7.972904

Batch 17630, train_perplexity=3340.9417, train_loss=8.114008

Batch 17640, train_perplexity=2792.6807, train_loss=7.934757

Batch 17650, train_perplexity=2794.9001, train_loss=7.9355516

Batch 17660, train_perplexity=3328.1704, train_loss=8.110178

Batch 17670, train_perplexity=2951.1836, train_loss=7.9899616

Batch 17680, train_perplexity=2996.9548, train_loss=8.005352

Batch 17690, train_perplexity=2909.3213, train_loss=7.975675

Batch 17700, train_perplexity=3223.0828, train_loss=8.078094

Batch 17710, train_perplexity=2845.599, train_loss=7.953529

Batch 17720, train_perplexity=2896.1238, train_loss=7.9711285

Batch 17730, train_perplexity=3114.5872, train_loss=8.043852

Batch 17740, train_perplexity=3086.2258, train_loss=8.034704

Batch 17750, train_perplexity=2932.5378, train_loss=7.9836235

Batch 17760, train_perplexity=2977.533, train_loss=7.9988503

Batch 17770, train_perplexity=3296.7449, train_loss=8.100691

Batch 17780, train_perplexity=2878.8193, train_loss=7.9651356

Batch 17790, train_perplexity=2899.9004, train_loss=7.9724317

Batch 17800, train_perplexity=2841.1194, train_loss=7.9519534

Batch 17810, train_perplexity=3017.299, train_loss=8.012117

Batch 17820, train_perplexity=2860.8533, train_loss=7.958875

Batch 17830, train_perplexity=2869.1196, train_loss=7.9617605

Batch 17840, train_perplexity=2764.178, train_loss=7.9244986

Batch 17850, train_perplexity=3280.865, train_loss=8.095862

Batch 17860, train_perplexity=3135.7478, train_loss=8.050623

Batch 17870, train_perplexity=3314.8284, train_loss=8.106161

Batch 17880, train_perplexity=2815.7544, train_loss=7.9429855

Batch 17890, train_perplexity=3088.705, train_loss=8.035507

Batch 17900, train_perplexity=3300.7842, train_loss=8.101915

Batch 17910, train_perplexity=3168.2783, train_loss=8.060944

Batch 17920, train_perplexity=3110.8943, train_loss=8.0426655

Batch 17930, train_perplexity=3009.2844, train_loss=8.009458

Batch 17940, train_perplexity=2829.3562, train_loss=7.9478045

Batch 17950, train_perplexity=3175.4475, train_loss=8.063204

Batch 17960, train_perplexity=3103.4387, train_loss=8.040266

Batch 17970, train_perplexity=3106.4236, train_loss=8.041227

Batch 17980, train_perplexity=2896.2368, train_loss=7.9711676

Batch 17990, train_perplexity=2797.012, train_loss=7.936307

Batch 18000, train_perplexity=3154.506, train_loss=8.056587

Batch 18010, train_perplexity=3130.2263, train_loss=8.048861

Batch 18020, train_perplexity=2781.996, train_loss=7.930924

Batch 18030, train_perplexity=2934.5298, train_loss=7.9843025

Batch 18040, train_perplexity=3122.9983, train_loss=8.046549

Batch 18050, train_perplexity=2952.7263, train_loss=7.990484

Batch 18060, train_perplexity=3096.6921, train_loss=8.03809

Batch 18070, train_perplexity=2938.3398, train_loss=7.9856

Batch 18080, train_perplexity=3236.7776, train_loss=8.082334

Batch 18090, train_perplexity=3107.5198, train_loss=8.04158

Batch 18100, train_perplexity=2834.866, train_loss=7.94975

Batch 18110, train_perplexity=3085.596, train_loss=8.0345

Batch 18120, train_perplexity=3165.6023, train_loss=8.060099

Batch 18130, train_perplexity=3387.0388, train_loss=8.127711

Batch 18140, train_perplexity=2902.0305, train_loss=7.973166

Batch 18150, train_perplexity=3060.2788, train_loss=8.026261

Batch 18160, train_perplexity=2804.5603, train_loss=7.939002

Batch 18170, train_perplexity=3356.027, train_loss=8.118513

Batch 18180, train_perplexity=3050.212, train_loss=8.022966

Batch 18190, train_perplexity=3189.7522, train_loss=8.0676985

Batch 18200, train_perplexity=2876.9888, train_loss=7.9644995

Batch 18210, train_perplexity=3079.7986, train_loss=8.032619

Batch 18220, train_perplexity=2925.203, train_loss=7.981119

Batch 18230, train_perplexity=3189.442, train_loss=8.067601

Batch 18240, train_perplexity=2893.6887, train_loss=7.9702873

Batch 18250, train_perplexity=3003.6274, train_loss=8.007576

Batch 18260, train_perplexity=2993.6414, train_loss=8.004246

Batch 18270, train_perplexity=3059.1438, train_loss=8.02589

Batch 18280, train_perplexity=3192.5977, train_loss=8.06859

Batch 18290, train_perplexity=2864.7424, train_loss=7.9602337

Batch 18300, train_perplexity=2868.061, train_loss=7.9613914

Batch 18310, train_perplexity=3096.6743, train_loss=8.038084

Batch 18320, train_perplexity=2859.709, train_loss=7.958475

Batch 18330, train_perplexity=3188.0886, train_loss=8.067177

Batch 18340, train_perplexity=3276.9907, train_loss=8.094681
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 18350, train_perplexity=3202.6548, train_loss=8.071735

Batch 18360, train_perplexity=3185.238, train_loss=8.066282

Batch 18370, train_perplexity=2763.0315, train_loss=7.9240837

Batch 18380, train_perplexity=2898.4033, train_loss=7.9719152

Batch 18390, train_perplexity=3214.3342, train_loss=8.075376

Batch 18400, train_perplexity=3196.735, train_loss=8.069885

Batch 18410, train_perplexity=3026.4058, train_loss=8.015131

Batch 18420, train_perplexity=3280.937, train_loss=8.095884

Batch 18430, train_perplexity=3190.7166, train_loss=8.068001

Batch 18440, train_perplexity=3181.6768, train_loss=8.065164

Batch 18450, train_perplexity=2957.0012, train_loss=7.991931

Batch 18460, train_perplexity=2994.5637, train_loss=8.004554

Batch 18470, train_perplexity=2867.618, train_loss=7.961237

Batch 18480, train_perplexity=3284.5374, train_loss=8.096981

Batch 18490, train_perplexity=2938.9214, train_loss=7.985798

Batch 18500, train_perplexity=3000.8015, train_loss=8.006635

Batch 18510, train_perplexity=3093.0205, train_loss=8.036903

Batch 18520, train_perplexity=2852.4773, train_loss=7.955943

Batch 18530, train_perplexity=3051.0208, train_loss=8.0232315

Batch 18540, train_perplexity=2871.4546, train_loss=7.962574

Batch 18550, train_perplexity=2844.971, train_loss=7.953308

Batch 18560, train_perplexity=3079.9456, train_loss=8.032667

Batch 18570, train_perplexity=3376.2607, train_loss=8.124524

Batch 18580, train_perplexity=2984.9236, train_loss=8.001329

Batch 18590, train_perplexity=2942.086, train_loss=7.986874

Batch 18600, train_perplexity=2988.202, train_loss=8.002427

Batch 18610, train_perplexity=2851.6545, train_loss=7.9556546

Batch 18620, train_perplexity=3249.2695, train_loss=8.086185

Batch 18630, train_perplexity=2912.3306, train_loss=7.976709

Batch 18640, train_perplexity=3192.3115, train_loss=8.0685005

Batch 18650, train_perplexity=3266.0334, train_loss=8.0913315

Batch 18660, train_perplexity=3065.142, train_loss=8.027849

Batch 18670, train_perplexity=2862.0334, train_loss=7.9592876

Batch 18680, train_perplexity=2883.0176, train_loss=7.966593

Batch 18690, train_perplexity=3000.3552, train_loss=8.006486

Batch 18700, train_perplexity=3039.4536, train_loss=8.019433

Batch 18710, train_perplexity=3147.0933, train_loss=8.0542345

Batch 18720, train_perplexity=2900.8035, train_loss=7.972743

Batch 18730, train_perplexity=2943.0906, train_loss=7.9872155

Batch 18740, train_perplexity=2842.6953, train_loss=7.952508

Batch 18750, train_perplexity=3172.6565, train_loss=8.062325

Batch 18760, train_perplexity=3048.9905, train_loss=8.022566

Batch 18770, train_perplexity=2829.0999, train_loss=7.947714

Batch 18780, train_perplexity=3061.1982, train_loss=8.026562

Batch 18790, train_perplexity=3115.7783, train_loss=8.044234

Batch 18800, train_perplexity=3077.978, train_loss=8.032028

Batch 18810, train_perplexity=3186.4775, train_loss=8.066671

Batch 18820, train_perplexity=2864.898, train_loss=7.960288

Batch 18830, train_perplexity=3122.7305, train_loss=8.046463

Batch 18840, train_perplexity=3073.7542, train_loss=8.030655

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 18850, train_perplexity=3174.5874, train_loss=8.062933

Batch 18860, train_perplexity=2880.459, train_loss=7.965705

Batch 18870, train_perplexity=2929.533, train_loss=7.9825983

Batch 18880, train_perplexity=2938.8933, train_loss=7.9857883

Batch 18890, train_perplexity=3095.0388, train_loss=8.037556

Batch 18900, train_perplexity=2793.533, train_loss=7.9350624

Batch 18910, train_perplexity=3022.2812, train_loss=8.013767

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 18920, train_perplexity=2909.406, train_loss=7.975704

Batch 18930, train_perplexity=2855.2478, train_loss=7.956914

Batch 18940, train_perplexity=3045.5789, train_loss=8.021446

Batch 18950, train_perplexity=2822.161, train_loss=7.945258

Batch 18960, train_perplexity=3066.9023, train_loss=8.028423

Batch 18970, train_perplexity=2901.3928, train_loss=7.972946

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 18980, train_perplexity=3074.2437, train_loss=8.030814

Batch 18990, train_perplexity=2833.0513, train_loss=7.9491096

Batch 19000, train_perplexity=2975.1272, train_loss=7.998042

Batch 19010, train_perplexity=2777.1582, train_loss=7.9291835

Batch 19020, train_perplexity=2846.6033, train_loss=7.9538817

Batch 19030, train_perplexity=3229.122, train_loss=8.079966

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 19040, train_perplexity=2964.3396, train_loss=7.9944096

Batch 19050, train_perplexity=2969.24, train_loss=7.9960613

Batch 19060, train_perplexity=3053.1748, train_loss=8.023937

Batch 19070, train_perplexity=2906.806, train_loss=7.97481

Batch 19080, train_perplexity=2941.1392, train_loss=7.9865522

Batch 19090, train_perplexity=3283.3596, train_loss=8.096622

Batch 19100, train_perplexity=2642.8079, train_loss=7.879597

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 19110, train_perplexity=3094.3923, train_loss=8.037347

Batch 19120, train_perplexity=2976.5435, train_loss=7.998518

Batch 19130, train_perplexity=3008.3977, train_loss=8.009163

Batch 19140, train_perplexity=3221.3833, train_loss=8.077566

Batch 19150, train_perplexity=3010.5762, train_loss=8.009887

Batch 19160, train_perplexity=3067.5283, train_loss=8.028627

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 19170, train_perplexity=2896.3254, train_loss=7.971198

Batch 19180, train_perplexity=2931.207, train_loss=7.9831696

Batch 19190, train_perplexity=3046.392, train_loss=8.021713

Batch 19200, train_perplexity=2952.5083, train_loss=7.9904103

Batch 19210, train_perplexity=2903.3137, train_loss=7.973608

Batch 19220, train_perplexity=2896.4744, train_loss=7.9712496

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 19230, train_perplexity=3440.2424, train_loss=8.143297

Batch 19240, train_perplexity=2896.233, train_loss=7.971166

Batch 19250, train_perplexity=2936.0037, train_loss=7.9848046

Batch 19260, train_perplexity=3195.9119, train_loss=8.069628

Batch 19270, train_perplexity=2966.967, train_loss=7.9952955

Batch 19280, train_perplexity=2886.5322, train_loss=7.967811

Batch 19290, train_perplexity=3030.2383, train_loss=8.0163965

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 19300, train_perplexity=3072.368, train_loss=8.030204

Batch 19310, train_perplexity=2936.1282, train_loss=7.984847

Batch 19320, train_perplexity=2924.0232, train_loss=7.9807158

Batch 19330, train_perplexity=2843.506, train_loss=7.952793

Batch 19340, train_perplexity=2987.783, train_loss=8.002287

Batch 19350, train_perplexity=3177.5437, train_loss=8.063864

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 19360, train_perplexity=3098.9846, train_loss=8.03883

Batch 19370, train_perplexity=3108.569, train_loss=8.041918

Batch 19380, train_perplexity=2974.7869, train_loss=7.9979277

Batch 19390, train_perplexity=3004.693, train_loss=8.007931

Batch 19400, train_perplexity=3130.8174, train_loss=8.049049

Batch 19410, train_perplexity=3092.8906, train_loss=8.036861

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 19420, train_perplexity=2873.8928, train_loss=7.963423

Batch 19430, train_perplexity=3004.5212, train_loss=8.007874

Batch 19440, train_perplexity=2938.2473, train_loss=7.9855685

Batch 19450, train_perplexity=2990.8933, train_loss=8.003327

Batch 19460, train_perplexity=2802.678, train_loss=7.9383307

Batch 19470, train_perplexity=3045.0938, train_loss=8.021287

Batch 19480, train_perplexity=3102.9949, train_loss=8.040123

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 19490, train_perplexity=3055.0098, train_loss=8.024538

Batch 19500, train_perplexity=2966.5767, train_loss=7.995164

Batch 19510, train_perplexity=2878.3623, train_loss=7.964977

Batch 19520, train_perplexity=3058.5603, train_loss=8.0257

Batch 19530, train_perplexity=3089.8186, train_loss=8.035868

Batch 19540, train_perplexity=2974.7158, train_loss=7.997904

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 19550, train_perplexity=2931.6501, train_loss=7.9833207

Batch 19560, train_perplexity=3103.9004, train_loss=8.040415

Batch 19570, train_perplexity=3311.536, train_loss=8.105167

Batch 19580, train_perplexity=2869.1006, train_loss=7.961754

Batch 19590, train_perplexity=2877.3442, train_loss=7.964623

Batch 19600, train_perplexity=3068.4822, train_loss=8.028938

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 19610, train_perplexity=2852.5847, train_loss=7.955981

Batch 19620, train_perplexity=3074.1174, train_loss=8.030773

Batch 19630, train_perplexity=2987.7944, train_loss=8.002291

Batch 19640, train_perplexity=3148.7173, train_loss=8.05475

Batch 19650, train_perplexity=3003.4785, train_loss=8.007526

Batch 19660, train_perplexity=3078.3655, train_loss=8.032154

Batch 19670, train_perplexity=2670.203, train_loss=7.8899097

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 19680, train_perplexity=2855.6401, train_loss=7.9570513

Batch 19690, train_perplexity=3135.1257, train_loss=8.050425

Batch 19700, train_perplexity=2806.179, train_loss=7.939579

Batch 19710, train_perplexity=2861.575, train_loss=7.9591274

Batch 19720, train_perplexity=3080.5447, train_loss=8.032862

Batch 19730, train_perplexity=2807.9216, train_loss=7.9402

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 19740, train_perplexity=3178.9866, train_loss=8.064318

Batch 19750, train_perplexity=2832.5203, train_loss=7.948922

Batch 19760, train_perplexity=2793.1829, train_loss=7.934937

Batch 19770, train_perplexity=3008.3948, train_loss=8.009162

Batch 19780, train_perplexity=3061.7004, train_loss=8.026726

Batch 19790, train_perplexity=2963.3306, train_loss=7.994069

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 19800, train_perplexity=2872.52, train_loss=7.962945

Batch 19810, train_perplexity=2750.6929, train_loss=7.919608

Batch 19820, train_perplexity=2800.738, train_loss=7.9376383

Batch 19830, train_perplexity=2802.8315, train_loss=7.9383855

Batch 19840, train_perplexity=2933.1084, train_loss=7.983818

Batch 19850, train_perplexity=2885.123, train_loss=7.967323

Batch 19860, train_perplexity=3027.7683, train_loss=8.015581

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 19870, train_perplexity=2945.2793, train_loss=7.987959

Batch 19880, train_perplexity=2851.063, train_loss=7.955447
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 19890, train_perplexity=2991.7205, train_loss=8.003604

Batch 19900, train_perplexity=2903.4937, train_loss=7.97367

Batch 19910, train_perplexity=3038.7666, train_loss=8.019207

Batch 19920, train_perplexity=2864.8708, train_loss=7.9602785

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 19930, train_perplexity=3073.5254, train_loss=8.0305805

Batch 19940, train_perplexity=3259.8098, train_loss=8.089424

Batch 19950, train_perplexity=2975.5474, train_loss=7.9981833

Batch 19960, train_perplexity=2928.0903, train_loss=7.9821057

Batch 19970, train_perplexity=2817.6428, train_loss=7.943656

Batch 19980, train_perplexity=3101.894, train_loss=8.039768

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 19990, train_perplexity=2937.6604, train_loss=7.9853687

Batch 20000, train_perplexity=2878.037, train_loss=7.964864

Batch 20010, train_perplexity=2785.839, train_loss=7.9323044

Batch 20020, train_perplexity=2871.6504, train_loss=7.962642

Batch 20030, train_perplexity=2719.7563, train_loss=7.9082975

Batch 20040, train_perplexity=2675.4973, train_loss=7.8918905

Batch 20050, train_perplexity=3170.2036, train_loss=8.061551

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 20060, train_perplexity=3031.3743, train_loss=8.016771

Batch 20070, train_perplexity=3180.8364, train_loss=8.064899

Batch 20080, train_perplexity=2569.575, train_loss=7.8514957

Batch 20090, train_perplexity=2814.2163, train_loss=7.942439

Batch 20100, train_perplexity=2916.5217, train_loss=7.978147

Batch 20110, train_perplexity=2732.1462, train_loss=7.9128428

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 20120, train_perplexity=3101.5332, train_loss=8.039652

Batch 20130, train_perplexity=2713.6316, train_loss=7.906043

Batch 20140, train_perplexity=2931.9395, train_loss=7.9834194

Batch 20150, train_perplexity=2853.5562, train_loss=7.9563212

Batch 20160, train_perplexity=3088.4104, train_loss=8.035412

Batch 20170, train_perplexity=3079.6636, train_loss=8.032576

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 20180, train_perplexity=3227.8628, train_loss=8.079576

Batch 20190, train_perplexity=2927.4592, train_loss=7.98189

Batch 20200, train_perplexity=2975.8623, train_loss=7.998289

Batch 20210, train_perplexity=2665.3765, train_loss=7.8881006

Batch 20220, train_perplexity=3240.5613, train_loss=8.083502

Batch 20230, train_perplexity=2995.8462, train_loss=8.004982

Batch 20240, train_perplexity=3056.7117, train_loss=8.025095

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 20250, train_perplexity=2779.8518, train_loss=7.930153

Batch 20260, train_perplexity=2967.3506, train_loss=7.9954247

Batch 20270, train_perplexity=2640.7783, train_loss=7.878829

Batch 20280, train_perplexity=2876.5513, train_loss=7.9643474

Batch 20290, train_perplexity=3029.2183, train_loss=8.01606

Batch 20300, train_perplexity=2653.9763, train_loss=7.8838143

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 20310, train_perplexity=2873.0679, train_loss=7.9631357

Batch 20320, train_perplexity=2986.9482, train_loss=8.0020075

Batch 20330, train_perplexity=3257.939, train_loss=8.08885

Batch 20340, train_perplexity=2841.4473, train_loss=7.952069

Batch 20350, train_perplexity=2939.2996, train_loss=7.9859266

Batch 20360, train_perplexity=2911.5625, train_loss=7.976445

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 20370, train_perplexity=2662.525, train_loss=7.88703

Batch 20380, train_perplexity=2830.5032, train_loss=7.94821

Batch 20390, train_perplexity=2764.9966, train_loss=7.9247947

Batch 20400, train_perplexity=2778.3477, train_loss=7.9296117

Batch 20410, train_perplexity=2929.6462, train_loss=7.982637

Batch 20420, train_perplexity=2675.7742, train_loss=7.891994

Batch 20430, train_perplexity=2931.5815, train_loss=7.9832973

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 20440, train_perplexity=3184.1204, train_loss=8.065931

Batch 20450, train_perplexity=2831.475, train_loss=7.948553

Batch 20460, train_perplexity=2994.5093, train_loss=8.004536

Batch 20470, train_perplexity=2790.7424, train_loss=7.934063

Batch 20480, train_perplexity=3136.621, train_loss=8.050901

Batch 20490, train_perplexity=2774.925, train_loss=7.928379

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 20500, train_perplexity=2601.4055, train_loss=7.863807

Batch 20510, train_perplexity=3068.1223, train_loss=8.028821

Batch 20520, train_perplexity=2875.332, train_loss=7.9639235

Batch 20530, train_perplexity=3023.867, train_loss=8.014292

Batch 20540, train_perplexity=2691.3638, train_loss=7.8978033

Batch 20550, train_perplexity=3100.17, train_loss=8.039212

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 20560, train_perplexity=3008.5554, train_loss=8.009215

Batch 20570, train_perplexity=3002.273, train_loss=8.007125

Batch 20580, train_perplexity=2993.8696, train_loss=8.004322

Batch 20590, train_perplexity=2952.3098, train_loss=7.990343

Batch 20600, train_perplexity=2937.075, train_loss=7.9851694

Batch 20610, train_perplexity=3003.3152, train_loss=8.007472

Batch 20620, train_perplexity=2796.4998, train_loss=7.936124

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 20630, train_perplexity=2925.3647, train_loss=7.9811745

Batch 20640, train_perplexity=2999.1707, train_loss=8.006091

Batch 20650, train_perplexity=2829.7556, train_loss=7.9479456

Batch 20660, train_perplexity=3013.4976, train_loss=8.010857

Batch 20670, train_perplexity=2774.925, train_loss=7.928379

Batch 20680, train_perplexity=3098.4646, train_loss=8.038662

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 20690, train_perplexity=2970.224, train_loss=7.9963927

Batch 20700, train_perplexity=2782.8372, train_loss=7.9312263

Batch 20710, train_perplexity=2751.699, train_loss=7.919974

Batch 20720, train_perplexity=2964.2024, train_loss=7.9943633

Batch 20730, train_perplexity=3058.3445, train_loss=8.025629

Batch 20740, train_perplexity=3081.238, train_loss=8.033087

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 20750, train_perplexity=2702.1423, train_loss=7.9018

Batch 20760, train_perplexity=2973.0938, train_loss=7.9973583

Batch 20770, train_perplexity=2967.0662, train_loss=7.995329

Batch 20780, train_perplexity=2980.4946, train_loss=7.9998446

Batch 20790, train_perplexity=2926.5815, train_loss=7.9815903

Batch 20800, train_perplexity=3083.931, train_loss=8.03396

Batch 20810, train_perplexity=2837.6384, train_loss=7.9507275

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 20820, train_perplexity=3108.0503, train_loss=8.041751

Batch 20830, train_perplexity=3019.216, train_loss=8.012753

Batch 20840, train_perplexity=2887.4944, train_loss=7.9681444

Batch 20850, train_perplexity=2878.7124, train_loss=7.9650984

Batch 20860, train_perplexity=2821.5486, train_loss=7.945041

Batch 20870, train_perplexity=3059.0942, train_loss=8.025874

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 20880, train_perplexity=2773.2598, train_loss=7.9277787

Batch 20890, train_perplexity=2938.2935, train_loss=7.9855843

Batch 20900, train_perplexity=2698.0479, train_loss=7.900284

Batch 20910, train_perplexity=2911.5237, train_loss=7.976432

Batch 20920, train_perplexity=2892.3672, train_loss=7.9698305

Batch 20930, train_perplexity=2745.801, train_loss=7.917828

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 20940, train_perplexity=2948.8037, train_loss=7.989155

Batch 20950, train_perplexity=2901.213, train_loss=7.972884

Batch 20960, train_perplexity=2842.7212, train_loss=7.952517

Batch 20970, train_perplexity=3061.8435, train_loss=8.0267725

Batch 20980, train_perplexity=2906.8892, train_loss=7.9748387

Batch 20990, train_perplexity=2812.3328, train_loss=7.9417696

Batch 21000, train_perplexity=2854.0596, train_loss=7.9564977

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 21010, train_perplexity=2711.4792, train_loss=7.9052496

Batch 21020, train_perplexity=2727.559, train_loss=7.9111624

Batch 21030, train_perplexity=3006.717, train_loss=8.008604

Batch 21040, train_perplexity=2688.6355, train_loss=7.896789

Batch 21050, train_perplexity=2804.702, train_loss=7.9390526

Batch 21060, train_perplexity=2880.7913, train_loss=7.9658203

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 21070, train_perplexity=3130.8533, train_loss=8.049061

Batch 21080, train_perplexity=2580.3872, train_loss=7.855695

Batch 21090, train_perplexity=2908.7678, train_loss=7.975485

Batch 21100, train_perplexity=2817.6267, train_loss=7.9436502

Batch 21110, train_perplexity=2595.93, train_loss=7.8617

Batch 21120, train_perplexity=2928.441, train_loss=7.9822254

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 21130, train_perplexity=2743.1953, train_loss=7.9168787

Batch 21140, train_perplexity=2776.8154, train_loss=7.92906

Batch 21150, train_perplexity=2820.0515, train_loss=7.9445105

Batch 21160, train_perplexity=2935.1091, train_loss=7.9845

Batch 21170, train_perplexity=3018.3813, train_loss=8.012476

Batch 21180, train_perplexity=2940.201, train_loss=7.986233

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 21190, train_perplexity=3006.4016, train_loss=8.008499

Batch 21200, train_perplexity=2684.8188, train_loss=7.8953686

Batch 21210, train_perplexity=2672.0293, train_loss=7.8905935

Batch 21220, train_perplexity=2865.9954, train_loss=7.960671

Batch 21230, train_perplexity=2965.6206, train_loss=7.9948416

Batch 21240, train_perplexity=2825.1016, train_loss=7.9462996

Batch 21250, train_perplexity=3029.5244, train_loss=8.016161

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 21260, train_perplexity=2926.6372, train_loss=7.9816093

Batch 21270, train_perplexity=2785.56, train_loss=7.9322042

Batch 21280, train_perplexity=3126.154, train_loss=8.047559

Batch 21290, train_perplexity=3087.5535, train_loss=8.035134

Batch 21300, train_perplexity=2593.25, train_loss=7.860667

Batch 21310, train_perplexity=2784.9014, train_loss=7.9319677

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 21320, train_perplexity=2866.6199, train_loss=7.960889

Batch 21330, train_perplexity=2915.3872, train_loss=7.977758

Batch 21340, train_perplexity=2762.839, train_loss=7.924014

Batch 21350, train_perplexity=2991.2385, train_loss=8.003443

Batch 21360, train_perplexity=3064.432, train_loss=8.027617

Batch 21370, train_perplexity=3155.2612, train_loss=8.056827

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 21380, train_perplexity=2895.6418, train_loss=7.970962

Batch 21390, train_perplexity=2846.7432, train_loss=7.953931

Batch 21400, train_perplexity=2662.738, train_loss=7.88711

Batch 21410, train_perplexity=2906.9458, train_loss=7.9748583

Batch 21420, train_perplexity=2752.5876, train_loss=7.9202967

Batch 21430, train_perplexity=2817.5918, train_loss=7.943638

Batch 21440, train_perplexity=2976.2725, train_loss=7.998427

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 21450, train_perplexity=2913.279, train_loss=7.9770346

Batch 21460, train_perplexity=2821.3374, train_loss=7.9449663

Batch 21470, train_perplexity=3036.1973, train_loss=8.018361

Batch 21480, train_perplexity=3039.6768, train_loss=8.019506

Batch 21490, train_perplexity=2842.427, train_loss=7.9524136

Batch 21500, train_perplexity=2946.0083, train_loss=7.9882064

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 21510, train_perplexity=2655.4233, train_loss=7.8843594

Batch 21520, train_perplexity=2839.9126, train_loss=7.9515285

Batch 21530, train_perplexity=3038.2334, train_loss=8.019032

Batch 21540, train_perplexity=2972.6968, train_loss=7.997225

Batch 21550, train_perplexity=2997.5266, train_loss=8.005543

Batch 21560, train_perplexity=2961.7456, train_loss=7.993534

Batch 21570, train_perplexity=2769.5608, train_loss=7.926444

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 21580, train_perplexity=3061.0466, train_loss=8.026512

Batch 21590, train_perplexity=2794.583, train_loss=7.935438

Batch 21600, train_perplexity=3066.1829, train_loss=8.028189

Batch 21610, train_perplexity=2998.0498, train_loss=8.005717

Batch 21620, train_perplexity=2881.683, train_loss=7.96613

Batch 21630, train_perplexity=2605.737, train_loss=7.865471

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 21640, train_perplexity=2903.682, train_loss=7.973735

Batch 21650, train_perplexity=2963.8606, train_loss=7.994248

Batch 21660, train_perplexity=2888.1816, train_loss=7.9683824

Batch 21670, train_perplexity=2872.5583, train_loss=7.9629583

Batch 21680, train_perplexity=2568.7932, train_loss=7.8511915

Batch 21690, train_perplexity=2847.0078, train_loss=7.954024

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 21700, train_perplexity=2818.6104, train_loss=7.9439993

Batch 21710, train_perplexity=2729.6436, train_loss=7.9119263

Batch 21720, train_perplexity=2675.7651, train_loss=7.8919907

Batch 21730, train_perplexity=2676.2744, train_loss=7.892181

Batch 21740, train_perplexity=2707.3992, train_loss=7.9037437

Batch 21750, train_perplexity=2767.6255, train_loss=7.925745

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 21760, train_perplexity=2841.2102, train_loss=7.9519854

Batch 21770, train_perplexity=2827.9924, train_loss=7.9473224

Batch 21780, train_perplexity=3063.754, train_loss=8.027396

Batch 21790, train_perplexity=2741.9033, train_loss=7.9164076

Batch 21800, train_perplexity=2846.6238, train_loss=7.953889

Batch 21810, train_perplexity=2710.8276, train_loss=7.9050093

Batch 21820, train_perplexity=2742.8462, train_loss=7.9167514

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6219 sentences.
Finished loading
Batch 21830, train_perplexity=2710.8638, train_loss=7.9050226

Batch 21840, train_perplexity=3046.1714, train_loss=8.021641

Batch 21850, train_perplexity=2645.4404, train_loss=7.880593

Batch 21860, train_perplexity=2820.954, train_loss=7.9448304

Batch 21870, train_perplexity=2684.3723, train_loss=7.895202

Batch 21880, train_perplexity=2912.1514, train_loss=7.9766474

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 21890, train_perplexity=2700.6777, train_loss=7.901258

Batch 21900, train_perplexity=2810.2214, train_loss=7.9410186

Batch 21910, train_perplexity=2852.499, train_loss=7.9559507

Batch 21920, train_perplexity=2771.5583, train_loss=7.927165

Batch 21930, train_perplexity=2606.54, train_loss=7.865779

Batch 21940, train_perplexity=2766.2825, train_loss=7.9252596

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 21950, train_perplexity=2670.297, train_loss=7.889945

Batch 21960, train_perplexity=2861.586, train_loss=7.9591312

Batch 21970, train_perplexity=2663.6233, train_loss=7.8874426

Batch 21980, train_perplexity=2793.7808, train_loss=7.935151

Batch 21990, train_perplexity=2844.823, train_loss=7.953256

Batch 22000, train_perplexity=2800.471, train_loss=7.937543

Batch 22010, train_perplexity=2876.2124, train_loss=7.9642296

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 22020, train_perplexity=2670.273, train_loss=7.889936

Batch 22030, train_perplexity=2898.0024, train_loss=7.971777

Batch 22040, train_perplexity=3005.9888, train_loss=8.008362

Batch 22050, train_perplexity=2701.7183, train_loss=7.9016433

Batch 22060, train_perplexity=2734.35, train_loss=7.913649

Batch 22070, train_perplexity=2764.7317, train_loss=7.924699

Batch 22080, train_perplexity=2981.956, train_loss=8.000335

Batch 22090, train_perplexity=2811.0713, train_loss=7.941321

Batch 22100, train_perplexity=2774.7002, train_loss=7.928298

Batch 22110, train_perplexity=2953.5093, train_loss=7.9907494

Batch 22120, train_perplexity=2808.133, train_loss=7.940275

Batch 22130, train_perplexity=2861.8806, train_loss=7.959234

Batch 22140, train_perplexity=2559.6646, train_loss=7.8476315

Batch 22150, train_perplexity=2858.2283, train_loss=7.9579573

Batch 22160, train_perplexity=3014.1414, train_loss=8.01107

Batch 22170, train_perplexity=2933.1687, train_loss=7.9838386

Batch 22180, train_perplexity=2837.2124, train_loss=7.9505773

Batch 22190, train_perplexity=2907.1787, train_loss=7.9749384

Batch 22200, train_perplexity=2756.967, train_loss=7.9218864

Batch 22210, train_perplexity=2708.7627, train_loss=7.9042473

Batch 22220, train_perplexity=2945.2737, train_loss=7.987957

Batch 22230, train_perplexity=2762.8628, train_loss=7.9240227

Batch 22240, train_perplexity=2785.847, train_loss=7.9323072

Batch 22250, train_perplexity=2829.6638, train_loss=7.947913

Batch 22260, train_perplexity=2763.6138, train_loss=7.9242945

Batch 22270, train_perplexity=2688.2573, train_loss=7.8966484

Batch 22280, train_perplexity=3017.2676, train_loss=8.012107

Batch 22290, train_perplexity=2691.8528, train_loss=7.897985

Batch 22300, train_perplexity=2750.1472, train_loss=7.9194098

Batch 22310, train_perplexity=2897.9001, train_loss=7.9717417

Batch 22320, train_perplexity=2773.8032, train_loss=7.9279747

Batch 22330, train_perplexity=2664.5454, train_loss=7.887789

Batch 22340, train_perplexity=2564.0415, train_loss=7.84934

Batch 22350, train_perplexity=2737.6638, train_loss=7.9148602

Batch 22360, train_perplexity=2901.3457, train_loss=7.97293

Batch 22370, train_perplexity=2760.2556, train_loss=7.9230785

Batch 22380, train_perplexity=2512.992, train_loss=7.8292294

Batch 22390, train_perplexity=2889.431, train_loss=7.968815

Batch 22400, train_perplexity=2782.0054, train_loss=7.9309273

Batch 22410, train_perplexity=2738.2397, train_loss=7.9150705

Batch 22420, train_perplexity=2523.0105, train_loss=7.833208

Batch 22430, train_perplexity=2505.1997, train_loss=7.8261237

Batch 22440, train_perplexity=2895.28, train_loss=7.970837

Batch 22450, train_perplexity=2872.5898, train_loss=7.9629693

Batch 22460, train_perplexity=2782.284, train_loss=7.9310274

Batch 22470, train_perplexity=2696.9546, train_loss=7.8998785

Batch 22480, train_perplexity=3052.479, train_loss=8.023709

Batch 22490, train_perplexity=2763.8894, train_loss=7.924394

Batch 22500, train_perplexity=3021.607, train_loss=8.013544

Batch 22510, train_perplexity=2855.4438, train_loss=7.9569826

Batch 22520, train_perplexity=3067.9612, train_loss=8.028769

Batch 22530, train_perplexity=2862.675, train_loss=7.9595118

Batch 22540, train_perplexity=2783.4543, train_loss=7.931448

Batch 22550, train_perplexity=2776.062, train_loss=7.9287887

Batch 22560, train_perplexity=2762.5217, train_loss=7.923899

Batch 22570, train_perplexity=3109.3193, train_loss=8.042159

Batch 22580, train_perplexity=2576.2317, train_loss=7.854083

Batch 22590, train_perplexity=2682.8418, train_loss=7.894632

Batch 22600, train_perplexity=2770.2517, train_loss=7.9266934

Batch 22610, train_perplexity=2938.4812, train_loss=7.985648

Batch 22620, train_perplexity=2941.4224, train_loss=7.9866486

Batch 22630, train_perplexity=2730.9504, train_loss=7.912405

Batch 22640, train_perplexity=2951.7944, train_loss=7.9901686

Batch 22650, train_perplexity=2844.1653, train_loss=7.953025

Batch 22660, train_perplexity=2812.054, train_loss=7.9416704

Batch 22670, train_perplexity=2813.854, train_loss=7.9423103

Batch 22680, train_perplexity=2940.2305, train_loss=7.9862432

Batch 22690, train_perplexity=2722.198, train_loss=7.909195

Batch 22700, train_perplexity=2998.2356, train_loss=8.005779

Batch 22710, train_perplexity=2884.328, train_loss=7.967047

Batch 22720, train_perplexity=2716.3865, train_loss=7.907058

Batch 22730, train_perplexity=2750.8608, train_loss=7.919669

Batch 22740, train_perplexity=2826.147, train_loss=7.9466696

Batch 22750, train_perplexity=2891.4102, train_loss=7.9694996

Batch 22760, train_perplexity=2983.3127, train_loss=8.00079

Batch 22770, train_perplexity=2886.118, train_loss=7.9676676

Batch 22780, train_perplexity=2738.485, train_loss=7.91516

Batch 22790, train_perplexity=2759.7593, train_loss=7.922899

Batch 22800, train_perplexity=2952.7927, train_loss=7.9905066

Batch 22810, train_perplexity=2891.559, train_loss=7.969551

Batch 22820, train_perplexity=2923.2356, train_loss=7.9804463

Batch 22830, train_perplexity=2949.916, train_loss=7.989532

Batch 22840, train_perplexity=3139.7336, train_loss=8.051893

Batch 22850, train_perplexity=2615.4866, train_loss=7.8692055

Batch 22860, train_perplexity=2878.0261, train_loss=7.96486

Batch 22870, train_perplexity=2660.5374, train_loss=7.8862834

Batch 22880, train_perplexity=2681.3862, train_loss=7.894089

Batch 22890, train_perplexity=2865.774, train_loss=7.9605937

Batch 22900, train_perplexity=2803.3289, train_loss=7.938563

Batch 22910, train_perplexity=2632.2747, train_loss=7.8756037

Batch 22920, train_perplexity=2956.0117, train_loss=7.991596

Batch 22930, train_perplexity=2935.9995, train_loss=7.984803

Batch 22940, train_perplexity=2864.5388, train_loss=7.9601626
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 22950, train_perplexity=2828.7517, train_loss=7.947591

Batch 22960, train_perplexity=2848.443, train_loss=7.954528

Batch 22970, train_perplexity=2865.0605, train_loss=7.960345

Batch 22980, train_perplexity=3034.5095, train_loss=8.017805

Batch 22990, train_perplexity=2663.8645, train_loss=7.887533

Batch 23000, train_perplexity=2727.6423, train_loss=7.911193

Batch 23010, train_perplexity=2901.3208, train_loss=7.9729214

Batch 23020, train_perplexity=2846.8735, train_loss=7.9539766

Batch 23030, train_perplexity=2689.6895, train_loss=7.897181

Batch 23040, train_perplexity=2782.8623, train_loss=7.9312353

Batch 23050, train_perplexity=2577.0107, train_loss=7.8543854

Batch 23060, train_perplexity=2694.186, train_loss=7.8988514

Batch 23070, train_perplexity=2878.829, train_loss=7.965139

Batch 23080, train_perplexity=2674.2676, train_loss=7.891431

Batch 23090, train_perplexity=2850.223, train_loss=7.9551525

Batch 23100, train_perplexity=2707.9583, train_loss=7.90395

Batch 23110, train_perplexity=2877.1616, train_loss=7.9645596

Batch 23120, train_perplexity=2531.014, train_loss=7.836375

Batch 23130, train_perplexity=2790.9634, train_loss=7.934142

Batch 23140, train_perplexity=2912.715, train_loss=7.976841

Batch 23150, train_perplexity=2831.7505, train_loss=7.9486504

Batch 23160, train_perplexity=2706.0234, train_loss=7.9032354

Batch 23170, train_perplexity=2638.256, train_loss=7.8778734

Batch 23180, train_perplexity=2861.4302, train_loss=7.959077

Batch 23190, train_perplexity=2708.4553, train_loss=7.904134

Batch 23200, train_perplexity=2587.3904, train_loss=7.858405

Batch 23210, train_perplexity=2629.0396, train_loss=7.874374

Batch 23220, train_perplexity=2853.7466, train_loss=7.956388

Batch 23230, train_perplexity=2787.3325, train_loss=7.9328403

Batch 23240, train_perplexity=2909.342, train_loss=7.9756823

Batch 23250, train_perplexity=3010.134, train_loss=8.00974

Batch 23260, train_perplexity=2724.9097, train_loss=7.9101906

Batch 23270, train_perplexity=2697.7263, train_loss=7.9001646

Batch 23280, train_perplexity=2697.7778, train_loss=7.9001837

Batch 23290, train_perplexity=2747.4473, train_loss=7.9184275

Batch 23300, train_perplexity=2961.9841, train_loss=7.9936147

Batch 23310, train_perplexity=2693.6772, train_loss=7.8986626

Batch 23320, train_perplexity=3046.764, train_loss=8.021835

Batch 23330, train_perplexity=2922.6501, train_loss=7.980246

Batch 23340, train_perplexity=2887.8745, train_loss=7.968276

Batch 23350, train_perplexity=2938.48, train_loss=7.9856477

Batch 23360, train_perplexity=2878.6135, train_loss=7.965064

Batch 23370, train_perplexity=2899.2131, train_loss=7.9721947

Batch 23380, train_perplexity=2813.6218, train_loss=7.942228

Batch 23390, train_perplexity=2919.1306, train_loss=7.979041

Batch 23400, train_perplexity=2827.0823, train_loss=7.9470005

Batch 23410, train_perplexity=3268.3765, train_loss=8.092049

Batch 23420, train_perplexity=2711.6873, train_loss=7.9053264

Batch 23430, train_perplexity=2679.519, train_loss=7.8933926

Batch 23440, train_perplexity=2799.7368, train_loss=7.9372807

Batch 23450, train_perplexity=2991.8547, train_loss=8.003649

Batch 23460, train_perplexity=2639.3723, train_loss=7.8782964

Batch 23470, train_perplexity=2791.4678, train_loss=7.934323

Batch 23480, train_perplexity=2849.2515, train_loss=7.9548116

Batch 23490, train_perplexity=2725.323, train_loss=7.910342

Batch 23500, train_perplexity=2739.2087, train_loss=7.9154243

Batch 23510, train_perplexity=2990.149, train_loss=8.003078

Batch 23520, train_perplexity=2797.7896, train_loss=7.936585

Batch 23530, train_perplexity=2970.1208, train_loss=7.996358

Batch 23540, train_perplexity=2476.057, train_loss=7.8144226

Batch 23550, train_perplexity=2540.1592, train_loss=7.839982

Batch 23560, train_perplexity=2677.5955, train_loss=7.8926744

Batch 23570, train_perplexity=2649.3171, train_loss=7.882057

Batch 23580, train_perplexity=3023.4749, train_loss=8.014162

Batch 23590, train_perplexity=2802.134, train_loss=7.9381366

Batch 23600, train_perplexity=2980.6965, train_loss=7.9999123

Batch 23610, train_perplexity=2748.7917, train_loss=7.9189167

Batch 23620, train_perplexity=2665.181, train_loss=7.888027

Batch 23630, train_perplexity=2607.3616, train_loss=7.866094

Batch 23640, train_perplexity=2736.0652, train_loss=7.914276

Batch 23650, train_perplexity=2769.001, train_loss=7.926242

Batch 23660, train_perplexity=2499.1875, train_loss=7.823721

Batch 23670, train_perplexity=2674.464, train_loss=7.8915043

Batch 23680, train_perplexity=2728.0742, train_loss=7.911351

Batch 23690, train_perplexity=2751.5469, train_loss=7.9199185

Batch 23700, train_perplexity=2909.585, train_loss=7.9757657

Batch 23710, train_perplexity=2695.5508, train_loss=7.899358

Batch 23720, train_perplexity=2975.7075, train_loss=7.998237

Batch 23730, train_perplexity=2965.1877, train_loss=7.9946957

Batch 23740, train_perplexity=2686.6619, train_loss=7.8960547

Batch 23750, train_perplexity=2718.6309, train_loss=7.9078836

Batch 23760, train_perplexity=2914.2656, train_loss=7.977373

Batch 23770, train_perplexity=2771.1487, train_loss=7.927017

Batch 23780, train_perplexity=2698.4854, train_loss=7.900446

Batch 23790, train_perplexity=2560.1895, train_loss=7.8478365

Batch 23800, train_perplexity=2888.953, train_loss=7.9686494

Batch 23810, train_perplexity=2766.541, train_loss=7.925353

Batch 23820, train_perplexity=2865.1604, train_loss=7.9603796

Batch 23830, train_perplexity=2904.2249, train_loss=7.973922

Batch 23840, train_perplexity=2811.5686, train_loss=7.941498

Batch 23850, train_perplexity=2755.168, train_loss=7.9212337

Batch 23860, train_perplexity=2718.5635, train_loss=7.907859

Batch 23870, train_perplexity=2654.9775, train_loss=7.8841915

Batch 23880, train_perplexity=2663.5774, train_loss=7.8874254

Batch 23890, train_perplexity=2588.6147, train_loss=7.858878

Batch 23900, train_perplexity=2987.7488, train_loss=8.002275

Batch 23910, train_perplexity=2807.5293, train_loss=7.94006

Batch 23920, train_perplexity=2666.818, train_loss=7.8886414

Batch 23930, train_perplexity=2948.8247, train_loss=7.989162

Batch 23940, train_perplexity=2845.0754, train_loss=7.953345

Batch 23950, train_perplexity=2505.889, train_loss=7.826399

Batch 23960, train_perplexity=2979.9575, train_loss=7.9996643

Batch 23970, train_perplexity=2798.5408, train_loss=7.9368534

Batch 23980, train_perplexity=2698.2642, train_loss=7.900364

Batch 23990, train_perplexity=2679.4934, train_loss=7.893383

Batch 24000, train_perplexity=2815.196, train_loss=7.942787

Batch 24010, train_perplexity=2771.479, train_loss=7.9271364

Batch 24020, train_perplexity=2698.3696, train_loss=7.900403

Batch 24030, train_perplexity=2663.6284, train_loss=7.8874445

Batch 24040, train_perplexity=2963.1526, train_loss=7.994009

Batch 24050, train_perplexity=2747.607, train_loss=7.9184856

Batch 24060, train_perplexity=2809.6414, train_loss=7.940812

Batch 24070, train_perplexity=2571.6943, train_loss=7.85232

Batch 24080, train_perplexity=3050.5874, train_loss=8.023089

Batch 24090, train_perplexity=2663.8494, train_loss=7.8875275

Batch 24100, train_perplexity=3087.8157, train_loss=8.035219

Batch 24110, train_perplexity=2801.7173, train_loss=7.937988

Batch 24120, train_perplexity=2857.844, train_loss=7.957823

Batch 24130, train_perplexity=2569.9058, train_loss=7.8516245

Batch 24140, train_perplexity=2890.284, train_loss=7.96911

Batch 24150, train_perplexity=2860.5366, train_loss=7.9587646

Batch 24160, train_perplexity=2726.2795, train_loss=7.910693

Batch 24170, train_perplexity=3148.321, train_loss=8.054625

Batch 24180, train_perplexity=2590.862, train_loss=7.859746

Batch 24190, train_perplexity=2653.1223, train_loss=7.8834925

Batch 24200, train_perplexity=2865.1562, train_loss=7.960378

Batch 24210, train_perplexity=2752.7883, train_loss=7.9203696

Batch 24220, train_perplexity=2827.4504, train_loss=7.9471307

Batch 24230, train_perplexity=2650.582, train_loss=7.8825345

Batch 24240, train_perplexity=2978.1875, train_loss=7.99907

Batch 24250, train_perplexity=2665.261, train_loss=7.888057

Batch 24260, train_perplexity=2810.9197, train_loss=7.941267
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 24270, train_perplexity=2783.8179, train_loss=7.9315786

Batch 24280, train_perplexity=2684.0945, train_loss=7.8950987

Batch 24290, train_perplexity=2625.9463, train_loss=7.8731966

Batch 24300, train_perplexity=2875.9353, train_loss=7.9641333

Batch 24310, train_perplexity=2839.7122, train_loss=7.951458

Batch 24320, train_perplexity=2772.5576, train_loss=7.9275255

Batch 24330, train_perplexity=2708.3625, train_loss=7.9040995

Batch 24340, train_perplexity=2815.0215, train_loss=7.942725

Batch 24350, train_perplexity=2860.5544, train_loss=7.9587708

Batch 24360, train_perplexity=2973.5261, train_loss=7.9975038

Batch 24370, train_perplexity=2802.5256, train_loss=7.9382763

Batch 24380, train_perplexity=2695.5122, train_loss=7.8993435

Batch 24390, train_perplexity=2832.277, train_loss=7.9488363

Batch 24400, train_perplexity=2733.4885, train_loss=7.913334

Batch 24410, train_perplexity=2663.6765, train_loss=7.8874626

Batch 24420, train_perplexity=3011.6416, train_loss=8.010241

Batch 24430, train_perplexity=2815.8833, train_loss=7.9430313

Batch 24440, train_perplexity=2593.6606, train_loss=7.8608255

Batch 24450, train_perplexity=2954.4558, train_loss=7.99107

Batch 24460, train_perplexity=2677.1716, train_loss=7.892516

Batch 24470, train_perplexity=2469.395, train_loss=7.8117285

Batch 24480, train_perplexity=2699.927, train_loss=7.90098

Batch 24490, train_perplexity=2655.208, train_loss=7.8842783

Batch 24500, train_perplexity=2793.998, train_loss=7.935229

Batch 24510, train_perplexity=2922.4731, train_loss=7.9801855

Batch 24520, train_perplexity=2933.8105, train_loss=7.9840574

Batch 24530, train_perplexity=2892.0747, train_loss=7.9697294

Batch 24540, train_perplexity=2741.302, train_loss=7.9161882

Batch 24550, train_perplexity=2496.4456, train_loss=7.8226233

Batch 24560, train_perplexity=2627.071, train_loss=7.873625

Batch 24570, train_perplexity=2547.799, train_loss=7.842985

Batch 24580, train_perplexity=2582.2556, train_loss=7.8564186

Batch 24590, train_perplexity=2721.2195, train_loss=7.9088354

Batch 24600, train_perplexity=2458.8274, train_loss=7.80744

Batch 24610, train_perplexity=2726.2068, train_loss=7.9106665

Batch 24620, train_perplexity=2793.1921, train_loss=7.9349403

Batch 24630, train_perplexity=2613.6392, train_loss=7.868499

Batch 24640, train_perplexity=2770.4167, train_loss=7.926753

Batch 24650, train_perplexity=2537.745, train_loss=7.839031

Batch 24660, train_perplexity=2921.2178, train_loss=7.979756

Batch 24670, train_perplexity=2619.8503, train_loss=7.8708725

Batch 24680, train_perplexity=2679.0693, train_loss=7.8932247

Batch 24690, train_perplexity=2811.7026, train_loss=7.9415455

Batch 24700, train_perplexity=2742.7114, train_loss=7.9167023

Batch 24710, train_perplexity=2979.8494, train_loss=7.999628

Batch 24720, train_perplexity=2742.2903, train_loss=7.9165487

Batch 24730, train_perplexity=2584.451, train_loss=7.8572683

Batch 24740, train_perplexity=2530.5288, train_loss=7.8361835

Batch 24750, train_perplexity=2743.4517, train_loss=7.916972

Batch 24760, train_perplexity=2916.2478, train_loss=7.978053

Batch 24770, train_perplexity=2536.089, train_loss=7.8383784

Batch 24780, train_perplexity=2522.9165, train_loss=7.833171

Batch 24790, train_perplexity=2665.514, train_loss=7.888152

Batch 24800, train_perplexity=2714.2864, train_loss=7.9062843

Batch 24810, train_perplexity=2635.0425, train_loss=7.8766546

Batch 24820, train_perplexity=2795.6465, train_loss=7.9358187

Batch 24830, train_perplexity=2684.143, train_loss=7.895117

Batch 24840, train_perplexity=2809.2769, train_loss=7.9406824

Batch 24850, train_perplexity=2855.5435, train_loss=7.9570174

Batch 24860, train_perplexity=2778.5664, train_loss=7.9296904

Batch 24870, train_perplexity=2655.0056, train_loss=7.884202

Batch 24880, train_perplexity=2537.7898, train_loss=7.839049

Batch 24890, train_perplexity=2771.3628, train_loss=7.9270945

Batch 24900, train_perplexity=2481.8857, train_loss=7.816774

Batch 24910, train_perplexity=2803.9465, train_loss=7.938783

Batch 24920, train_perplexity=2854.2651, train_loss=7.9565697

Batch 24930, train_perplexity=2719.93, train_loss=7.9083614

Batch 24940, train_perplexity=2972.3564, train_loss=7.9971104

Batch 24950, train_perplexity=2577.9644, train_loss=7.8547554

Batch 24960, train_perplexity=2878.9705, train_loss=7.965188

Batch 24970, train_perplexity=2596.03, train_loss=7.8617387

Batch 24980, train_perplexity=2701.9502, train_loss=7.901729

Batch 24990, train_perplexity=2796.5984, train_loss=7.936159

Batch 25000, train_perplexity=2630.207, train_loss=7.874818

Batch 25010, train_perplexity=2736.715, train_loss=7.9145136

Batch 25020, train_perplexity=2519.2573, train_loss=7.8317194

Batch 25030, train_perplexity=2617.5803, train_loss=7.8700056

Batch 25040, train_perplexity=2522.2864, train_loss=7.832921

Batch 25050, train_perplexity=2633.897, train_loss=7.8762197

Batch 25060, train_perplexity=2765.7087, train_loss=7.925052

Batch 25070, train_perplexity=2331.4001, train_loss=7.7542243

Batch 25080, train_perplexity=2672.669, train_loss=7.890833

Batch 25090, train_perplexity=2740.9622, train_loss=7.9160643

Batch 25100, train_perplexity=2567.2651, train_loss=7.8505964

Batch 25110, train_perplexity=2801.0132, train_loss=7.9377365

Batch 25120, train_perplexity=2810.2603, train_loss=7.9410324

Batch 25130, train_perplexity=2627.2239, train_loss=7.873683

Batch 25140, train_perplexity=2386.789, train_loss=7.7777042

Batch 25150, train_perplexity=2703.0186, train_loss=7.9021244

Batch 25160, train_perplexity=2581.1946, train_loss=7.8560076

Batch 25170, train_perplexity=2682.866, train_loss=7.894641

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 25180, train_perplexity=2755.3188, train_loss=7.9212885

Batch 25190, train_perplexity=2766.855, train_loss=7.9254665

Batch 25200, train_perplexity=2879.679, train_loss=7.965434

Batch 25210, train_perplexity=2371.0193, train_loss=7.7710752

Batch 25220, train_perplexity=2767.318, train_loss=7.925634

Batch 25230, train_perplexity=2603.9062, train_loss=7.864768

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 25240, train_perplexity=2621.145, train_loss=7.8713665

Batch 25250, train_perplexity=2676.4797, train_loss=7.8922577

Batch 25260, train_perplexity=2750.0305, train_loss=7.9193673

Batch 25270, train_perplexity=2763.9092, train_loss=7.9244013

Batch 25280, train_perplexity=2880.7336, train_loss=7.9658003

Batch 25290, train_perplexity=2621.96, train_loss=7.8716774

Batch 25300, train_perplexity=2821.5903, train_loss=7.945056

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 25310, train_perplexity=2831.7883, train_loss=7.9486637

Batch 25320, train_perplexity=2782.6116, train_loss=7.931145

Batch 25330, train_perplexity=2622.7378, train_loss=7.871974

Batch 25340, train_perplexity=2644.2134, train_loss=7.880129

Batch 25350, train_perplexity=2805.665, train_loss=7.939396

Batch 25360, train_perplexity=2433.1565, train_loss=7.7969446

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 25370, train_perplexity=2738.493, train_loss=7.915163

Batch 25380, train_perplexity=2507.731, train_loss=7.8271337

Batch 25390, train_perplexity=2610.8413, train_loss=7.867428

Batch 25400, train_perplexity=2669.2888, train_loss=7.8895674

Batch 25410, train_perplexity=2718.4155, train_loss=7.9078045

Batch 25420, train_perplexity=2609.8518, train_loss=7.8670487

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 25430, train_perplexity=2594.1826, train_loss=7.861027

Batch 25440, train_perplexity=3044.9573, train_loss=8.021242

Batch 25450, train_perplexity=2623.6572, train_loss=7.8723245

Batch 25460, train_perplexity=2791.0818, train_loss=7.9341846

Batch 25470, train_perplexity=2600.3428, train_loss=7.8633986

Batch 25480, train_perplexity=2746.4006, train_loss=7.9180465

Batch 25490, train_perplexity=2849.7336, train_loss=7.954981

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 25500, train_perplexity=3060.9648, train_loss=8.026485

Batch 25510, train_perplexity=2677.5085, train_loss=7.892642

Batch 25520, train_perplexity=2652.92, train_loss=7.883416

Batch 25530, train_perplexity=2961.3064, train_loss=7.993386

Batch 25540, train_perplexity=2590.321, train_loss=7.859537

Batch 25550, train_perplexity=2688.5469, train_loss=7.896756

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 25560, train_perplexity=2621.075, train_loss=7.87134

Batch 25570, train_perplexity=2753.8767, train_loss=7.920765

Batch 25580, train_perplexity=2629.323, train_loss=7.8744817

Batch 25590, train_perplexity=2472.9395, train_loss=7.813163

Batch 25600, train_perplexity=2670.5034, train_loss=7.8900223

Batch 25610, train_perplexity=2835.1904, train_loss=7.9498644

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 25620, train_perplexity=2883.364, train_loss=7.966713

Batch 25630, train_perplexity=2576.2024, train_loss=7.8540716

Batch 25640, train_perplexity=2719.1702, train_loss=7.908082

Batch 25650, train_perplexity=2704.4172, train_loss=7.902642

Batch 25660, train_perplexity=2599.4985, train_loss=7.863074

Batch 25670, train_perplexity=2519.2705, train_loss=7.8317246

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 25680, train_perplexity=2819.4358, train_loss=7.944292

Batch 25690, train_perplexity=2723.377, train_loss=7.909628

Batch 25700, train_perplexity=2702.0005, train_loss=7.9017477

Batch 25710, train_perplexity=2673.6047, train_loss=7.891183

Batch 25720, train_perplexity=2709.9849, train_loss=7.9046984

Batch 25730, train_perplexity=2779.6716, train_loss=7.930088

Batch 25740, train_perplexity=2568.8813, train_loss=7.851226

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 25750, train_perplexity=2972.3352, train_loss=7.997103

Batch 25760, train_perplexity=2876.0369, train_loss=7.9641685

Batch 25770, train_perplexity=2551.6702, train_loss=7.8445034

Batch 25780, train_perplexity=2734.6436, train_loss=7.9137564

Batch 25790, train_perplexity=2720.9575, train_loss=7.908739

Batch 25800, train_perplexity=2807.587, train_loss=7.9400806

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 25810, train_perplexity=2847.7017, train_loss=7.9542675

Batch 25820, train_perplexity=2396.8677, train_loss=7.781918

Batch 25830, train_perplexity=2693.2305, train_loss=7.8984966

Batch 25840, train_perplexity=2530.4998, train_loss=7.836172

Batch 25850, train_perplexity=2539.6565, train_loss=7.839784

Batch 25860, train_perplexity=2922.9553, train_loss=7.9803505

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 25870, train_perplexity=2541.4458, train_loss=7.8404884

Batch 25880, train_perplexity=2863.3643, train_loss=7.9597526

Batch 25890, train_perplexity=2630.8618, train_loss=7.8750668

Batch 25900, train_perplexity=2768.9932, train_loss=7.926239

Batch 25910, train_perplexity=2558.9858, train_loss=7.8473663

Batch 25920, train_perplexity=2547.1843, train_loss=7.842744

Batch 25930, train_perplexity=3011.6702, train_loss=8.01025

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 25940, train_perplexity=2692.0518, train_loss=7.898059

Batch 25950, train_perplexity=3017.213, train_loss=8.012089

Batch 25960, train_perplexity=2598.418, train_loss=7.862658

Batch 25970, train_perplexity=2566.4329, train_loss=7.850272

Batch 25980, train_perplexity=2707.3784, train_loss=7.903736

Batch 25990, train_perplexity=2882.5076, train_loss=7.966416

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6124 sentences.
Finished loading
Batch 26000, train_perplexity=2685.9177, train_loss=7.8957777

Batch 26010, train_perplexity=2826.0337, train_loss=7.9466295

Batch 26020, train_perplexity=2800.7407, train_loss=7.937639

Batch 26030, train_perplexity=2571.6316, train_loss=7.852296

Batch 26040, train_perplexity=2529.923, train_loss=7.835944

Batch 26050, train_perplexity=2799.168, train_loss=7.9370775

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 26060, train_perplexity=2576.3523, train_loss=7.85413

Batch 26070, train_perplexity=2580.0083, train_loss=7.855548

Batch 26080, train_perplexity=2800.837, train_loss=7.9376736

Batch 26090, train_perplexity=2832.2988, train_loss=7.948844

Batch 26100, train_perplexity=2587.7434, train_loss=7.8585415

Batch 26110, train_perplexity=2613.8062, train_loss=7.8685627

Batch 26120, train_perplexity=2589.0442, train_loss=7.859044

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 26130, train_perplexity=2636.3975, train_loss=7.8771687

Batch 26140, train_perplexity=2763.4531, train_loss=7.9242363

Batch 26150, train_perplexity=2371.9805, train_loss=7.7714806

Batch 26160, train_perplexity=2443.3284, train_loss=7.8011165

Batch 26170, train_perplexity=2506.7842, train_loss=7.826756

Batch 26180, train_perplexity=2673.3867, train_loss=7.8911014

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 26190, train_perplexity=2511.794, train_loss=7.8287525

Batch 26200, train_perplexity=2855.2698, train_loss=7.9569216

Batch 26210, train_perplexity=2798.0579, train_loss=7.936681

Batch 26220, train_perplexity=2694.1282, train_loss=7.89883

Batch 26230, train_perplexity=2637.2249, train_loss=7.8774824

Batch 26240, train_perplexity=2565.2827, train_loss=7.849824

Batch 26250, train_perplexity=2770.4854, train_loss=7.926778

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 26260, train_perplexity=2808.2737, train_loss=7.9403253

Batch 26270, train_perplexity=2636.0945, train_loss=7.8770537

Batch 26280, train_perplexity=2850.7776, train_loss=7.955347

Batch 26290, train_perplexity=2516.0266, train_loss=7.830436

Batch 26300, train_perplexity=2696.5354, train_loss=7.899723

Batch 26310, train_perplexity=2625.3152, train_loss=7.8729563

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 26320, train_perplexity=2587.7446, train_loss=7.858542

Batch 26330, train_perplexity=2663.7935, train_loss=7.8875065

Batch 26340, train_perplexity=2662.949, train_loss=7.8871894

Batch 26350, train_perplexity=2622.2288, train_loss=7.87178

Batch 26360, train_perplexity=2567.1182, train_loss=7.850539

Batch 26370, train_perplexity=2646.4536, train_loss=7.8809757

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 26380, train_perplexity=2447.7678, train_loss=7.802932

Batch 26390, train_perplexity=2769.2993, train_loss=7.9263496

Batch 26400, train_perplexity=2568.367, train_loss=7.8510256

Batch 26410, train_perplexity=2510.7546, train_loss=7.8283386

Batch 26420, train_perplexity=2631.0474, train_loss=7.8751373

Batch 26430, train_perplexity=2695.4426, train_loss=7.8993177

Batch 26440, train_perplexity=2805.379, train_loss=7.939294

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 26450, train_perplexity=2534.3374, train_loss=7.8376875

Batch 26460, train_perplexity=2752.564, train_loss=7.920288

Batch 26470, train_perplexity=2578.4045, train_loss=7.854926

Batch 26480, train_perplexity=2648.342, train_loss=7.881689

Batch 26490, train_perplexity=2413.9475, train_loss=7.7890186

Batch 26500, train_perplexity=2970.1492, train_loss=7.9963675

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 26510, train_perplexity=2656.3022, train_loss=7.8846903

Batch 26520, train_perplexity=2614.0317, train_loss=7.868649

Batch 26530, train_perplexity=2550.8867, train_loss=7.8441963

Batch 26540, train_perplexity=2391.4075, train_loss=7.7796373

Batch 26550, train_perplexity=2725.4373, train_loss=7.910384

Batch 26560, train_perplexity=2538.7231, train_loss=7.8394165

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 26570, train_perplexity=2457.9915, train_loss=7.8071

Batch 26580, train_perplexity=2568.122, train_loss=7.85093

Batch 26590, train_perplexity=2644.1035, train_loss=7.8800874

Batch 26600, train_perplexity=2836.056, train_loss=7.9501696

Batch 26610, train_perplexity=2684.6218, train_loss=7.895295

Batch 26620, train_perplexity=2867.73, train_loss=7.961276

Batch 26630, train_perplexity=2649.8213, train_loss=7.8822474

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 26640, train_perplexity=2665.158, train_loss=7.8880186

Batch 26650, train_perplexity=2648.5693, train_loss=7.881775

Batch 26660, train_perplexity=2781.5237, train_loss=7.930754

Batch 26670, train_perplexity=2653.4436, train_loss=7.8836136

Batch 26680, train_perplexity=2752.0679, train_loss=7.920108

Batch 26690, train_perplexity=2758.9739, train_loss=7.922614

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 26700, train_perplexity=2683.5735, train_loss=7.8949046

Batch 26710, train_perplexity=2698.3027, train_loss=7.900378

Batch 26720, train_perplexity=2885.7463, train_loss=7.967539

Batch 26730, train_perplexity=2603.0771, train_loss=7.8644495

Batch 26740, train_perplexity=2625.7134, train_loss=7.873108

Batch 26750, train_perplexity=2783.3374, train_loss=7.931406

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 26760, train_perplexity=2592.622, train_loss=7.860425

Batch 26770, train_perplexity=2466.2178, train_loss=7.810441

Batch 26780, train_perplexity=2558.825, train_loss=7.8473034

Batch 26790, train_perplexity=2647.2437, train_loss=7.881274

Batch 26800, train_perplexity=2631.2207, train_loss=7.875203

Batch 26810, train_perplexity=2695.715, train_loss=7.899419

Batch 26820, train_perplexity=2577.7148, train_loss=7.8546586

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 26830, train_perplexity=2517.0552, train_loss=7.830845

Batch 26840, train_perplexity=2711.2258, train_loss=7.905156

Batch 26850, train_perplexity=2685.4207, train_loss=7.8955927

Batch 26860, train_perplexity=2522.2935, train_loss=7.832924

Batch 26870, train_perplexity=2462.9978, train_loss=7.8091345

Batch 26880, train_perplexity=2660.63, train_loss=7.886318

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 26890, train_perplexity=2630.3677, train_loss=7.874879

Batch 26900, train_perplexity=2700.0427, train_loss=7.901023

Batch 26910, train_perplexity=2622.225, train_loss=7.8717785

Batch 26920, train_perplexity=2409.568, train_loss=7.787203

Batch 26930, train_perplexity=2798.2793, train_loss=7.93676

Batch 26940, train_perplexity=2993.139, train_loss=8.004078

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 26950, train_perplexity=2770.681, train_loss=7.9268484

Batch 26960, train_perplexity=2501.5862, train_loss=7.8246803

Batch 26970, train_perplexity=2433.6414, train_loss=7.797144

Batch 26980, train_perplexity=2814.1357, train_loss=7.9424105

Batch 26990, train_perplexity=2582.215, train_loss=7.856403

Batch 27000, train_perplexity=2646.7778, train_loss=7.8810983

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 27010, train_perplexity=2612.8154, train_loss=7.8681836

Batch 27020, train_perplexity=2726.923, train_loss=7.910929

Batch 27030, train_perplexity=2619.2334, train_loss=7.870637

Batch 27040, train_perplexity=2675.1018, train_loss=7.8917427

Batch 27050, train_perplexity=2815.6765, train_loss=7.942958

Batch 27060, train_perplexity=2845.9614, train_loss=7.953656

Batch 27070, train_perplexity=2572.2068, train_loss=7.8525195

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 27080, train_perplexity=2735.3164, train_loss=7.9140024

Batch 27090, train_perplexity=2494.0994, train_loss=7.821683

Batch 27100, train_perplexity=2535.4556, train_loss=7.8381286

Batch 27110, train_perplexity=2677.7588, train_loss=7.8927355

Batch 27120, train_perplexity=2429.136, train_loss=7.795291

Batch 27130, train_perplexity=2552.0024, train_loss=7.8446336

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 27140, train_perplexity=2709.9177, train_loss=7.9046736

Batch 27150, train_perplexity=2681.4438, train_loss=7.8941107

Batch 27160, train_perplexity=2474.5461, train_loss=7.8138123

Batch 27170, train_perplexity=2581.1553, train_loss=7.8559923

Batch 27180, train_perplexity=2614.2424, train_loss=7.8687296

Batch 27190, train_perplexity=2392.0688, train_loss=7.779914

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 27200, train_perplexity=2857.6028, train_loss=7.9577384

Batch 27210, train_perplexity=2612.4966, train_loss=7.8680615

Batch 27220, train_perplexity=2565.8198, train_loss=7.8500333

Batch 27230, train_perplexity=2675.0737, train_loss=7.891732

Batch 27240, train_perplexity=2599.8047, train_loss=7.8631916

Batch 27250, train_perplexity=2690.952, train_loss=7.8976502

Batch 27260, train_perplexity=2678.713, train_loss=7.8930917

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 27270, train_perplexity=2367.5352, train_loss=7.7696047

Batch 27280, train_perplexity=2857.5088, train_loss=7.9577055

Batch 27290, train_perplexity=2947.3374, train_loss=7.9886575

Batch 27300, train_perplexity=2766.6516, train_loss=7.925393

Batch 27310, train_perplexity=2612.5798, train_loss=7.8680935

Batch 27320, train_perplexity=2867.9065, train_loss=7.9613376
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 27330, train_perplexity=3021.6445, train_loss=8.0135565

Batch 27340, train_perplexity=2631.3486, train_loss=7.875252

Batch 27350, train_perplexity=2638.6033, train_loss=7.878005

Batch 27360, train_perplexity=2694.0486, train_loss=7.8988004

Batch 27370, train_perplexity=2589.6394, train_loss=7.859274

Batch 27380, train_perplexity=2685.6206, train_loss=7.895667

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 27390, train_perplexity=2861.976, train_loss=7.9592676

Batch 27400, train_perplexity=2662.2468, train_loss=7.8869257

Batch 27410, train_perplexity=2692.352, train_loss=7.8981705

Batch 27420, train_perplexity=2784.333, train_loss=7.9317636

Batch 27430, train_perplexity=2599.0176, train_loss=7.862889

Batch 27440, train_perplexity=2633.6658, train_loss=7.876132

Batch 27450, train_perplexity=2843.517, train_loss=7.952797

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 27460, train_perplexity=2608.1028, train_loss=7.8663783

Batch 27470, train_perplexity=2858.2556, train_loss=7.957967

Batch 27480, train_perplexity=2615.554, train_loss=7.869231

Batch 27490, train_perplexity=2621.891, train_loss=7.871651

Batch 27500, train_perplexity=2349.419, train_loss=7.7619233

Batch 27510, train_perplexity=2491.7336, train_loss=7.820734

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 27520, train_perplexity=2517.7825, train_loss=7.831134

Batch 27530, train_perplexity=2608.977, train_loss=7.8667135

Batch 27540, train_perplexity=2648.8396, train_loss=7.881877

Batch 27550, train_perplexity=2613.6292, train_loss=7.868495

Batch 27560, train_perplexity=2362.634, train_loss=7.7675323

Batch 27570, train_perplexity=2686.654, train_loss=7.896052

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 27580, train_perplexity=2840.728, train_loss=7.9518156

Batch 27590, train_perplexity=2721.636, train_loss=7.9089885

Batch 27600, train_perplexity=2316.6028, train_loss=7.747857

Batch 27610, train_perplexity=2761.3271, train_loss=7.9234667

Batch 27620, train_perplexity=2611.5312, train_loss=7.867692

Batch 27630, train_perplexity=2724.529, train_loss=7.910051

Batch 27640, train_perplexity=2653.7751, train_loss=7.8837385

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 27650, train_perplexity=2638.47, train_loss=7.8779545

Batch 27660, train_perplexity=2435.8855, train_loss=7.7980657

Batch 27670, train_perplexity=2599.4155, train_loss=7.863042

Batch 27680, train_perplexity=2544.9966, train_loss=7.8418846

Batch 27690, train_perplexity=2610.2239, train_loss=7.8671913

Batch 27700, train_perplexity=2534.2588, train_loss=7.8376565

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 27710, train_perplexity=2557.2708, train_loss=7.846696

Batch 27720, train_perplexity=2535.6367, train_loss=7.8382

Batch 27730, train_perplexity=2531.8745, train_loss=7.836715

Batch 27740, train_perplexity=2668.7417, train_loss=7.8893623

Batch 27750, train_perplexity=2271.0786, train_loss=7.72801

Batch 27760, train_perplexity=2560.7986, train_loss=7.8480744

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 27770, train_perplexity=2600.1829, train_loss=7.863337

Batch 27780, train_perplexity=2575.0369, train_loss=7.853619

Batch 27790, train_perplexity=2741.9895, train_loss=7.916439

Batch 27800, train_perplexity=2483.839, train_loss=7.8175607

Batch 27810, train_perplexity=2267.0906, train_loss=7.7262526

Batch 27820, train_perplexity=2564.1672, train_loss=7.849389

Batch 27830, train_perplexity=2546.7825, train_loss=7.842586

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 27840, train_perplexity=2587.7014, train_loss=7.8585253

Batch 27850, train_perplexity=2625.6233, train_loss=7.8730736

Batch 27860, train_perplexity=2718.2783, train_loss=7.907754

Batch 27870, train_perplexity=2423.3745, train_loss=7.7929163

Batch 27880, train_perplexity=2759.453, train_loss=7.9227877

Batch 27890, train_perplexity=2702.333, train_loss=7.9018707

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 27900, train_perplexity=2628.8079, train_loss=7.8742857

Batch 27910, train_perplexity=2703.3137, train_loss=7.9022336

Batch 27920, train_perplexity=2393.7234, train_loss=7.7806053

Batch 27930, train_perplexity=2876.5334, train_loss=7.964341

Batch 27940, train_perplexity=2541.5537, train_loss=7.840531

Batch 27950, train_perplexity=2683.4163, train_loss=7.894846

Batch 27960, train_perplexity=2672.0918, train_loss=7.890617

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 27970, train_perplexity=2379.4663, train_loss=7.7746315

Batch 27980, train_perplexity=2556.705, train_loss=7.8464746

Batch 27990, train_perplexity=2561.2102, train_loss=7.848235

Batch 28000, train_perplexity=2568.3474, train_loss=7.851018

Batch 28010, train_perplexity=2620.7961, train_loss=7.8712335

Batch 28020, train_perplexity=2564.2786, train_loss=7.8494325

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 28030, train_perplexity=2647.6729, train_loss=7.8814363

Batch 28040, train_perplexity=2798.5408, train_loss=7.9368534

Batch 28050, train_perplexity=2758.2268, train_loss=7.9223433

Batch 28060, train_perplexity=2596.1724, train_loss=7.8617935

Batch 28070, train_perplexity=2493.5046, train_loss=7.8214445

Batch 28080, train_perplexity=2835.0525, train_loss=7.9498158

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 28090, train_perplexity=2428.981, train_loss=7.795227

Batch 28100, train_perplexity=2685.3618, train_loss=7.8955708

Batch 28110, train_perplexity=2567.98, train_loss=7.850875

Batch 28120, train_perplexity=2499.5068, train_loss=7.8238487

Batch 28130, train_perplexity=2694.9094, train_loss=7.89912

Batch 28140, train_perplexity=2730.121, train_loss=7.9121013

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 28150, train_perplexity=2546.6584, train_loss=7.8425374

Batch 28160, train_perplexity=2706.2144, train_loss=7.903306

Batch 28170, train_perplexity=2467.0952, train_loss=7.8107967

Batch 28180, train_perplexity=2524.2354, train_loss=7.8336935

Batch 28190, train_perplexity=2488.0413, train_loss=7.819251

Batch 28200, train_perplexity=2635.5327, train_loss=7.8768406

Batch 28210, train_perplexity=2512.5967, train_loss=7.829072

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 28220, train_perplexity=2827.255, train_loss=7.9470615

Batch 28230, train_perplexity=2299.4453, train_loss=7.740423

Batch 28240, train_perplexity=2662.8232, train_loss=7.887142

Batch 28250, train_perplexity=2526.9402, train_loss=7.8347645

Batch 28260, train_perplexity=2770.7285, train_loss=7.9268656

Batch 28270, train_perplexity=2669.8298, train_loss=7.88977

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 28280, train_perplexity=2582.5364, train_loss=7.8565273

Batch 28290, train_perplexity=2571.4453, train_loss=7.8522234

Batch 28300, train_perplexity=2580.6814, train_loss=7.8558087

Batch 28310, train_perplexity=2738.2554, train_loss=7.9150763

Batch 28320, train_perplexity=2482.7532, train_loss=7.8171234

Batch 28330, train_perplexity=2627.1086, train_loss=7.873639

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 28340, train_perplexity=2649.5596, train_loss=7.8821487

Batch 28350, train_perplexity=2746.4282, train_loss=7.9180565

Batch 28360, train_perplexity=2542.7842, train_loss=7.841015

Batch 28370, train_perplexity=2710.626, train_loss=7.904935

Batch 28380, train_perplexity=2517.7693, train_loss=7.8311286

Batch 28390, train_perplexity=2468.778, train_loss=7.8114786

Batch 28400, train_perplexity=2491.6292, train_loss=7.820692

Batch 28410, train_perplexity=2406.0457, train_loss=7.78574

Batch 28420, train_perplexity=2460.02, train_loss=7.8079247

Batch 28430, train_perplexity=2585.5308, train_loss=7.857686

Batch 28440, train_perplexity=2687.7139, train_loss=7.896446

Batch 28450, train_perplexity=2647.895, train_loss=7.8815203

Batch 28460, train_perplexity=2406.0698, train_loss=7.78575

Batch 28470, train_perplexity=2601.31, train_loss=7.8637705

Batch 28480, train_perplexity=2579.5088, train_loss=7.8553543

Batch 28490, train_perplexity=2524.4932, train_loss=7.8337955

Batch 28500, train_perplexity=2575.679, train_loss=7.8538685

Batch 28510, train_perplexity=2590.9734, train_loss=7.859789

Batch 28520, train_perplexity=2586.268, train_loss=7.857971

Batch 28530, train_perplexity=2508.4714, train_loss=7.827429

Batch 28540, train_perplexity=2721.8035, train_loss=7.90905

Batch 28550, train_perplexity=2469.3857, train_loss=7.8117247

Batch 28560, train_perplexity=2586.125, train_loss=7.857916

Batch 28570, train_perplexity=2804.0708, train_loss=7.9388275

Batch 28580, train_perplexity=2367.1074, train_loss=7.769424

Batch 28590, train_perplexity=2350.1394, train_loss=7.76223

Batch 28600, train_perplexity=2408.3552, train_loss=7.7866993

Batch 28610, train_perplexity=2533.842, train_loss=7.837492

Batch 28620, train_perplexity=2741.3804, train_loss=7.916217

Batch 28630, train_perplexity=2588.232, train_loss=7.8587303

Batch 28640, train_perplexity=2799.531, train_loss=7.937207

Batch 28650, train_perplexity=2580.925, train_loss=7.855903

Batch 28660, train_perplexity=2561.2297, train_loss=7.8482428

Batch 28670, train_perplexity=2368.2239, train_loss=7.7698956

Batch 28680, train_perplexity=2913.7307, train_loss=7.9771895

Batch 28690, train_perplexity=2751.3003, train_loss=7.919829

Batch 28700, train_perplexity=2314.8494, train_loss=7.7471

Batch 28710, train_perplexity=2659.3274, train_loss=7.8858285

Batch 28720, train_perplexity=2439.0342, train_loss=7.7993574

Batch 28730, train_perplexity=2354.1042, train_loss=7.7639155

Batch 28740, train_perplexity=2643.0308, train_loss=7.8796816

Batch 28750, train_perplexity=2719.1948, train_loss=7.908091

Batch 28760, train_perplexity=2501.4265, train_loss=7.8246164

Batch 28770, train_perplexity=2630.602, train_loss=7.874968

Batch 28780, train_perplexity=2501.8535, train_loss=7.824787

Batch 28790, train_perplexity=2611.4216, train_loss=7.86765

Batch 28800, train_perplexity=2686.2224, train_loss=7.895891

Batch 28810, train_perplexity=2554.5068, train_loss=7.8456144

Batch 28820, train_perplexity=2556.5613, train_loss=7.8464184

Batch 28830, train_perplexity=2439.0808, train_loss=7.7993765

Batch 28840, train_perplexity=2396.109, train_loss=7.7816014

Batch 28850, train_perplexity=2603.7622, train_loss=7.8647127
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 28860, train_perplexity=2672.144, train_loss=7.8906364

Batch 28870, train_perplexity=2534.6877, train_loss=7.837826

Batch 28880, train_perplexity=2679.3694, train_loss=7.893337

Batch 28890, train_perplexity=2515.289, train_loss=7.830143

Batch 28900, train_perplexity=2577.3992, train_loss=7.854536

Batch 28910, train_perplexity=2351.8389, train_loss=7.762953

Batch 28920, train_perplexity=2573.4275, train_loss=7.852994

Batch 28930, train_perplexity=2670.1316, train_loss=7.889883

Batch 28940, train_perplexity=2537.9485, train_loss=7.8391113

Batch 28950, train_perplexity=2653.5322, train_loss=7.883647

Batch 28960, train_perplexity=2822.9214, train_loss=7.9455276

Batch 28970, train_perplexity=2532.5278, train_loss=7.836973

Batch 28980, train_perplexity=2496.96, train_loss=7.8228292

Batch 28990, train_perplexity=2702.5674, train_loss=7.9019575

Batch 29000, train_perplexity=2356.8887, train_loss=7.7650976

Batch 29010, train_perplexity=2501.7556, train_loss=7.824748

Batch 29020, train_perplexity=2632.4604, train_loss=7.8756742

Batch 29030, train_perplexity=2805.0632, train_loss=7.9391813

Batch 29040, train_perplexity=2308.445, train_loss=7.7443295

Batch 29050, train_perplexity=2555.0098, train_loss=7.8458114

Batch 29060, train_perplexity=2639.391, train_loss=7.8783035

Batch 29070, train_perplexity=2508.6519, train_loss=7.827501

Batch 29080, train_perplexity=2577.426, train_loss=7.8545465

Batch 29090, train_perplexity=2603.3862, train_loss=7.864568

Batch 29100, train_perplexity=2525.903, train_loss=7.834354

Batch 29110, train_perplexity=2617.1885, train_loss=7.869856

Batch 29120, train_perplexity=2654.1714, train_loss=7.883888

Batch 29130, train_perplexity=2562.8215, train_loss=7.848864

Batch 29140, train_perplexity=2466.2227, train_loss=7.810443

Batch 29150, train_perplexity=2654.2676, train_loss=7.883924

Batch 29160, train_perplexity=2313.3894, train_loss=7.746469

Batch 29170, train_perplexity=2443.4436, train_loss=7.8011637

Batch 29180, train_perplexity=2450.4888, train_loss=7.804043

Batch 29190, train_perplexity=2445.929, train_loss=7.8021803

Batch 29200, train_perplexity=2459.7878, train_loss=7.8078303

Batch 29210, train_perplexity=2613.0732, train_loss=7.8682823

Batch 29220, train_perplexity=2408.9443, train_loss=7.786944

Batch 29230, train_perplexity=2510.5823, train_loss=7.82827

Batch 29240, train_perplexity=2609.3877, train_loss=7.866871

Batch 29250, train_perplexity=2659.075, train_loss=7.8857336

Batch 29260, train_perplexity=2598.325, train_loss=7.8626223

Batch 29270, train_perplexity=2499.75, train_loss=7.823946

Batch 29280, train_perplexity=2562.3584, train_loss=7.8486834

Batch 29290, train_perplexity=2533.099, train_loss=7.8371987

Batch 29300, train_perplexity=2597.3933, train_loss=7.8622637

Batch 29310, train_perplexity=2555.2158, train_loss=7.845892

Batch 29320, train_perplexity=2641.204, train_loss=7.87899

Batch 29330, train_perplexity=2513.4258, train_loss=7.829402

Batch 29340, train_perplexity=2567.86, train_loss=7.850828

Batch 29350, train_perplexity=2607.1665, train_loss=7.8660192

Batch 29360, train_perplexity=2429.4626, train_loss=7.7954254

Batch 29370, train_perplexity=2444.334, train_loss=7.801528

Batch 29380, train_perplexity=2616.3, train_loss=7.8695164

Batch 29390, train_perplexity=2708.5278, train_loss=7.9041605

Batch 29400, train_perplexity=2484.3533, train_loss=7.8177676

Batch 29410, train_perplexity=2601.077, train_loss=7.863681

Batch 29420, train_perplexity=2473.1965, train_loss=7.8132668

Batch 29430, train_perplexity=2489.9307, train_loss=7.82001

Batch 29440, train_perplexity=2611.5063, train_loss=7.8676825

Batch 29450, train_perplexity=2435.1482, train_loss=7.797763

Batch 29460, train_perplexity=2424.35, train_loss=7.7933187

Batch 29470, train_perplexity=2609.4424, train_loss=7.866892

Batch 29480, train_perplexity=2828.7922, train_loss=7.947605

Batch 29490, train_perplexity=2370.6892, train_loss=7.770936

Batch 29500, train_perplexity=2809.0493, train_loss=7.9406013

Batch 29510, train_perplexity=2432.3096, train_loss=7.7965965

Batch 29520, train_perplexity=2636.5784, train_loss=7.8772373

Batch 29530, train_perplexity=2494.3396, train_loss=7.8217793

Batch 29540, train_perplexity=2599.951, train_loss=7.863248

Batch 29550, train_perplexity=2713.9912, train_loss=7.9061756

Batch 29560, train_perplexity=2503.6245, train_loss=7.825495

Batch 29570, train_perplexity=2560.7803, train_loss=7.8480673

Batch 29580, train_perplexity=2544.6301, train_loss=7.8417406

Batch 29590, train_perplexity=2714.4985, train_loss=7.9063625

Batch 29600, train_perplexity=2719.139, train_loss=7.9080706

Batch 29610, train_perplexity=2719.3853, train_loss=7.908161

Batch 29620, train_perplexity=2399.3457, train_loss=7.7829514

Batch 29630, train_perplexity=2650.568, train_loss=7.8825293

Batch 29640, train_perplexity=2636.067, train_loss=7.8770432

Batch 29650, train_perplexity=2622.1575, train_loss=7.8717527

Batch 29660, train_perplexity=2648.9204, train_loss=7.8819075

Batch 29670, train_perplexity=2483.1558, train_loss=7.8172855

Batch 29680, train_perplexity=2810.2014, train_loss=7.9410114

Batch 29690, train_perplexity=2616.4397, train_loss=7.86957

Batch 29700, train_perplexity=2680.7407, train_loss=7.8938484

Batch 29710, train_perplexity=2538.965, train_loss=7.839512

Batch 29720, train_perplexity=2475.0193, train_loss=7.8140035

Batch 29730, train_perplexity=2575.9333, train_loss=7.853967

Batch 29740, train_perplexity=2561.7463, train_loss=7.8484445

Batch 29750, train_perplexity=2788.1235, train_loss=7.933124

Batch 29760, train_perplexity=2552.9336, train_loss=7.8449984

Batch 29770, train_perplexity=2666.7063, train_loss=7.8885994

Batch 29780, train_perplexity=2634.9622, train_loss=7.876624

Batch 29790, train_perplexity=2446.9346, train_loss=7.8025913

Batch 29800, train_perplexity=2311.7927, train_loss=7.7457786

Batch 29810, train_perplexity=2566.1218, train_loss=7.850151

Batch 29820, train_perplexity=2587.1042, train_loss=7.8582945

Batch 29830, train_perplexity=2570.2537, train_loss=7.85176

Batch 29840, train_perplexity=2531.388, train_loss=7.836523

Batch 29850, train_perplexity=2537.5986, train_loss=7.8389735

Batch 29860, train_perplexity=2693.446, train_loss=7.8985767

Batch 29870, train_perplexity=2527.926, train_loss=7.8351545

Batch 29880, train_perplexity=2550.2544, train_loss=7.8439484

Batch 29890, train_perplexity=2498.4595, train_loss=7.8234296

Batch 29900, train_perplexity=2633.337, train_loss=7.876007

Batch 29910, train_perplexity=2438.2039, train_loss=7.799017

Batch 29920, train_perplexity=2615.1526, train_loss=7.8690777

Batch 29930, train_perplexity=2501.09, train_loss=7.824482

Batch 29940, train_perplexity=2679.225, train_loss=7.893283

Batch 29950, train_perplexity=2454.425, train_loss=7.805648

Batch 29960, train_perplexity=2287.3057, train_loss=7.73513

Batch 29970, train_perplexity=2690.3962, train_loss=7.897444

Batch 29980, train_perplexity=2604.2937, train_loss=7.864917

Batch 29990, train_perplexity=2566.5515, train_loss=7.8503184

Batch 30000, train_perplexity=2578.7563, train_loss=7.8550625

Batch 30010, train_perplexity=2542.5198, train_loss=7.840911

Batch 30020, train_perplexity=2626.0203, train_loss=7.8732247

Batch 30030, train_perplexity=2525.4949, train_loss=7.8341923

Batch 30040, train_perplexity=2703.1836, train_loss=7.9021854

Batch 30050, train_perplexity=2499.4043, train_loss=7.8238077

Batch 30060, train_perplexity=2409.0466, train_loss=7.7869864

Batch 30070, train_perplexity=2542.156, train_loss=7.840768

Batch 30080, train_perplexity=2645.1062, train_loss=7.8804665

Batch 30090, train_perplexity=2575.7896, train_loss=7.8539114

Batch 30100, train_perplexity=2427.9734, train_loss=7.794812

Batch 30110, train_perplexity=2585.7773, train_loss=7.8577814

Batch 30120, train_perplexity=2469.3596, train_loss=7.811714

Batch 30130, train_perplexity=2360.9065, train_loss=7.766801

Batch 30140, train_perplexity=2575.4175, train_loss=7.853767

Batch 30150, train_perplexity=2411.4531, train_loss=7.787985

Batch 30160, train_perplexity=2452.9417, train_loss=7.805043

Batch 30170, train_perplexity=2541.052, train_loss=7.8403335
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 30180, train_perplexity=2326.131, train_loss=7.7519617

Batch 30190, train_perplexity=2494.6582, train_loss=7.821907

Batch 30200, train_perplexity=2679.9893, train_loss=7.893568

Batch 30210, train_perplexity=2583.9358, train_loss=7.857069

Batch 30220, train_perplexity=2698.2744, train_loss=7.9003677

Batch 30230, train_perplexity=2609.196, train_loss=7.8667974

Batch 30240, train_perplexity=2583.4133, train_loss=7.856867

Batch 30250, train_perplexity=2512.199, train_loss=7.8289137

Batch 30260, train_perplexity=2699.1804, train_loss=7.9007034

Batch 30270, train_perplexity=2377.8352, train_loss=7.773946

Batch 30280, train_perplexity=2577.6646, train_loss=7.854639

Batch 30290, train_perplexity=2365.3755, train_loss=7.768692

Batch 30300, train_perplexity=2282.3843, train_loss=7.732976

Batch 30310, train_perplexity=2596.1206, train_loss=7.8617735

Batch 30320, train_perplexity=2621.3098, train_loss=7.8714294

Batch 30330, train_perplexity=2593.9822, train_loss=7.8609495

Batch 30340, train_perplexity=2323.7344, train_loss=7.750931

Batch 30350, train_perplexity=2678.5837, train_loss=7.8930435

Batch 30360, train_perplexity=2498.8323, train_loss=7.823579

Batch 30370, train_perplexity=2666.7903, train_loss=7.888631

Batch 30380, train_perplexity=2522.5076, train_loss=7.833009

Batch 30390, train_perplexity=2386.4363, train_loss=7.7775564

Batch 30400, train_perplexity=2621.615, train_loss=7.871546

Batch 30410, train_perplexity=2619.8516, train_loss=7.870873

Batch 30420, train_perplexity=2601.4119, train_loss=7.8638096

Batch 30430, train_perplexity=2340.5374, train_loss=7.758136

Batch 30440, train_perplexity=2336.9866, train_loss=7.7566175

Batch 30450, train_perplexity=2352.9797, train_loss=7.7634377

Batch 30460, train_perplexity=2444.728, train_loss=7.801689

Batch 30470, train_perplexity=2593.621, train_loss=7.8608103

Batch 30480, train_perplexity=2586.0435, train_loss=7.8578844

Batch 30490, train_perplexity=2334.0151, train_loss=7.7553453

Batch 30500, train_perplexity=2562.4146, train_loss=7.8487053

Batch 30510, train_perplexity=2542.5876, train_loss=7.8409376

Batch 30520, train_perplexity=2456.5093, train_loss=7.8064966

Batch 30530, train_perplexity=2437.4158, train_loss=7.7986937

Batch 30540, train_perplexity=2565.5884, train_loss=7.849943

Batch 30550, train_perplexity=2570.256, train_loss=7.851761

Batch 30560, train_perplexity=2826.9448, train_loss=7.946952

Batch 30570, train_perplexity=2586.9094, train_loss=7.858219

Batch 30580, train_perplexity=2582.8296, train_loss=7.856641

Batch 30590, train_perplexity=2510.3835, train_loss=7.828191

Batch 30600, train_perplexity=2394.1138, train_loss=7.7807684

Batch 30610, train_perplexity=2566.604, train_loss=7.850339

Batch 30620, train_perplexity=2536.112, train_loss=7.8383875

Batch 30630, train_perplexity=2465.3079, train_loss=7.810072

Batch 30640, train_perplexity=2379.447, train_loss=7.7746234

Batch 30650, train_perplexity=2595.467, train_loss=7.8615217

Batch 30660, train_perplexity=2599.356, train_loss=7.863019

Batch 30670, train_perplexity=2334.1555, train_loss=7.7554054

Batch 30680, train_perplexity=2388.7622, train_loss=7.7785306

Batch 30690, train_perplexity=2411.8857, train_loss=7.788164

Batch 30700, train_perplexity=2610.4255, train_loss=7.8672686

Batch 30710, train_perplexity=2712.85, train_loss=7.905755

Batch 30720, train_perplexity=2626.6677, train_loss=7.8734713

Batch 30730, train_perplexity=2528.1624, train_loss=7.835248

Batch 30740, train_perplexity=2661.7314, train_loss=7.886732

Batch 30750, train_perplexity=2419.6218, train_loss=7.7913666

Batch 30760, train_perplexity=2525.9285, train_loss=7.834364

Batch 30770, train_perplexity=2718.802, train_loss=7.9079466

Batch 30780, train_perplexity=2651.769, train_loss=7.8829823

Batch 30790, train_perplexity=2410.1794, train_loss=7.7874565

Batch 30800, train_perplexity=2533.337, train_loss=7.8372927

Batch 30810, train_perplexity=2323.4485, train_loss=7.750808

Batch 30820, train_perplexity=2500.9434, train_loss=7.8244233

Batch 30830, train_perplexity=2590.877, train_loss=7.8597517

Batch 30840, train_perplexity=2473.9197, train_loss=7.813559

Batch 30850, train_perplexity=2386.252, train_loss=7.777479

Batch 30860, train_perplexity=2670.9414, train_loss=7.8901863

Batch 30870, train_perplexity=2671.502, train_loss=7.890396

Batch 30880, train_perplexity=2461.6099, train_loss=7.808571

Batch 30890, train_perplexity=2800.1306, train_loss=7.9374213

Batch 30900, train_perplexity=2675.561, train_loss=7.8919144

Batch 30910, train_perplexity=2641.0996, train_loss=7.8789506

Batch 30920, train_perplexity=2593.8486, train_loss=7.860898

Batch 30930, train_perplexity=2534.9368, train_loss=7.837924

Batch 30940, train_perplexity=2460.8823, train_loss=7.808275

Batch 30950, train_perplexity=2372.992, train_loss=7.771907

Batch 30960, train_perplexity=2536.8667, train_loss=7.838685

Batch 30970, train_perplexity=2451.454, train_loss=7.8044367

Batch 30980, train_perplexity=2378.7595, train_loss=7.7743344

Batch 30990, train_perplexity=2492.1057, train_loss=7.8208833

Batch 31000, train_perplexity=2858.152, train_loss=7.9579306

Batch 31010, train_perplexity=2727.9402, train_loss=7.911302

Batch 31020, train_perplexity=2448.7415, train_loss=7.8033295

Batch 31030, train_perplexity=2285.4075, train_loss=7.7342997

Batch 31040, train_perplexity=2872.775, train_loss=7.9630337

Batch 31050, train_perplexity=2386.2769, train_loss=7.7774897

Batch 31060, train_perplexity=2606.7488, train_loss=7.865859

Batch 31070, train_perplexity=2535.348, train_loss=7.838086

Batch 31080, train_perplexity=2785.4458, train_loss=7.9321632

Batch 31090, train_perplexity=2672.047, train_loss=7.8906

Batch 31100, train_perplexity=2589.0789, train_loss=7.8590574

Batch 31110, train_perplexity=2598.3882, train_loss=7.8626466

Batch 31120, train_perplexity=2604.8564, train_loss=7.865133

Batch 31130, train_perplexity=2366.0195, train_loss=7.7689643

Batch 31140, train_perplexity=2326.0647, train_loss=7.751933

Batch 31150, train_perplexity=2429.8613, train_loss=7.7955894

Batch 31160, train_perplexity=2562.3792, train_loss=7.8486915

Batch 31170, train_perplexity=2354.4758, train_loss=7.7640734

Batch 31180, train_perplexity=2469.9626, train_loss=7.8119583

Batch 31190, train_perplexity=2560.353, train_loss=7.8479004

Batch 31200, train_perplexity=2495.2222, train_loss=7.822133

Batch 31210, train_perplexity=2577.1558, train_loss=7.8544416

Batch 31220, train_perplexity=2450.3977, train_loss=7.8040056

Batch 31230, train_perplexity=2553.199, train_loss=7.8451023

Batch 31240, train_perplexity=2382.634, train_loss=7.775962

Batch 31250, train_perplexity=2702.6602, train_loss=7.901992

Batch 31260, train_perplexity=2414.3965, train_loss=7.7892046

Batch 31270, train_perplexity=2587.3152, train_loss=7.858376

Batch 31280, train_perplexity=2338.9297, train_loss=7.7574487

Batch 31290, train_perplexity=2473.0115, train_loss=7.813192

Batch 31300, train_perplexity=2498.1377, train_loss=7.823301

Batch 31310, train_perplexity=2598.4067, train_loss=7.8626537

Batch 31320, train_perplexity=2540.8086, train_loss=7.8402376

Batch 31330, train_perplexity=2514.874, train_loss=7.829978

Batch 31340, train_perplexity=2402.2397, train_loss=7.784157

Batch 31350, train_perplexity=2637.582, train_loss=7.877618

Batch 31360, train_perplexity=2550.6497, train_loss=7.8441033

Batch 31370, train_perplexity=2378.3875, train_loss=7.774178

Batch 31380, train_perplexity=2745.6753, train_loss=7.9177823

Batch 31390, train_perplexity=2529.1536, train_loss=7.83564

Batch 31400, train_perplexity=2652.242, train_loss=7.8831606

Batch 31410, train_perplexity=2514.2434, train_loss=7.829727

Batch 31420, train_perplexity=2398.9316, train_loss=7.7827787

Batch 31430, train_perplexity=2267.4277, train_loss=7.7264013

Batch 31440, train_perplexity=2496.3289, train_loss=7.8225765

Batch 31450, train_perplexity=2689.0176, train_loss=7.896931

Batch 31460, train_perplexity=2554.524, train_loss=7.845621

Batch 31470, train_perplexity=2541.7393, train_loss=7.840604

Batch 31480, train_perplexity=2760.187, train_loss=7.9230537

Batch 31490, train_perplexity=2406.188, train_loss=7.785799

Batch 31500, train_perplexity=2534.1465, train_loss=7.837612

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 31510, train_perplexity=2532.9263, train_loss=7.8371305

Batch 31520, train_perplexity=2634.8792, train_loss=7.8765926

Batch 31530, train_perplexity=2682.4746, train_loss=7.894495

Batch 31540, train_perplexity=2583.4897, train_loss=7.8568964

Batch 31550, train_perplexity=2522.2166, train_loss=7.8328934

Batch 31560, train_perplexity=2463.0142, train_loss=7.809141

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 31570, train_perplexity=2578.3162, train_loss=7.854892

Batch 31580, train_perplexity=2520.5693, train_loss=7.83224

Batch 31590, train_perplexity=2613.6392, train_loss=7.868499

Batch 31600, train_perplexity=2502.462, train_loss=7.8250303

Batch 31610, train_perplexity=2706.0918, train_loss=7.9032607

Batch 31620, train_perplexity=2568.738, train_loss=7.85117

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 31630, train_perplexity=2654.2966, train_loss=7.883935

Batch 31640, train_perplexity=2532.2258, train_loss=7.836854

Batch 31650, train_perplexity=2527.162, train_loss=7.834852

Batch 31660, train_perplexity=2361.1282, train_loss=7.766895

Batch 31670, train_perplexity=2517.8376, train_loss=7.831156

Batch 31680, train_perplexity=2720.6953, train_loss=7.908643

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 31690, train_perplexity=2499.843, train_loss=7.823983

Batch 31700, train_perplexity=2441.7036, train_loss=7.8004513

Batch 31710, train_perplexity=2493.8733, train_loss=7.8215923

Batch 31720, train_perplexity=2627.235, train_loss=7.8736873

Batch 31730, train_perplexity=2564.451, train_loss=7.8494997

Batch 31740, train_perplexity=2632.7117, train_loss=7.8757696

Batch 31750, train_perplexity=2424.1558, train_loss=7.7932386

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 31760, train_perplexity=2532.1282, train_loss=7.8368154

Batch 31770, train_perplexity=2645.096, train_loss=7.8804626

Batch 31780, train_perplexity=2338.4312, train_loss=7.7572355

Batch 31790, train_perplexity=2623.9788, train_loss=7.872447

Batch 31800, train_perplexity=2401.3853, train_loss=7.783801

Batch 31810, train_perplexity=2364.3425, train_loss=7.768255

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 31820, train_perplexity=2573.5454, train_loss=7.8530397

Batch 31830, train_perplexity=2832.9363, train_loss=7.949069

Batch 31840, train_perplexity=2736.2192, train_loss=7.9143324

Batch 31850, train_perplexity=2693.888, train_loss=7.898741

Batch 31860, train_perplexity=2379.1145, train_loss=7.7744837

Batch 31870, train_perplexity=2206.4634, train_loss=7.6991463

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 31880, train_perplexity=2657.341, train_loss=7.8850813

Batch 31890, train_perplexity=2401.2192, train_loss=7.783732

Batch 31900, train_perplexity=2353.277, train_loss=7.763564

Batch 31910, train_perplexity=2377.984, train_loss=7.7740083

Batch 31920, train_perplexity=2349.7202, train_loss=7.7620516

Batch 31930, train_perplexity=2626.089, train_loss=7.873251

Batch 31940, train_perplexity=2407.8958, train_loss=7.7865086

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 31950, train_perplexity=2561.9282, train_loss=7.8485155

Batch 31960, train_perplexity=2288.0593, train_loss=7.7354593

Batch 31970, train_perplexity=2566.6174, train_loss=7.850344

Batch 31980, train_perplexity=2442.803, train_loss=7.8009014

Batch 31990, train_perplexity=2477.7505, train_loss=7.8151064

Batch 32000, train_perplexity=2438.0017, train_loss=7.798934

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 32010, train_perplexity=2670.674, train_loss=7.890086

Batch 32020, train_perplexity=2575.4985, train_loss=7.8537984

Batch 32030, train_perplexity=2315.3152, train_loss=7.747301

Batch 32040, train_perplexity=2664.3486, train_loss=7.887715

Batch 32050, train_perplexity=2413.9038, train_loss=7.7890005

Batch 32060, train_perplexity=2646.6404, train_loss=7.8810463

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 32070, train_perplexity=2374.5222, train_loss=7.7725515

Batch 32080, train_perplexity=2478.6675, train_loss=7.8154764

Batch 32090, train_perplexity=2441.7805, train_loss=7.8004827

Batch 32100, train_perplexity=2407.5537, train_loss=7.7863665

Batch 32110, train_perplexity=2516.443, train_loss=7.8306017

Batch 32120, train_perplexity=2466.713, train_loss=7.810642

Batch 32130, train_perplexity=2566.92, train_loss=7.850462

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 32140, train_perplexity=2398.68, train_loss=7.782674

Batch 32150, train_perplexity=2507.2014, train_loss=7.8269224

Batch 32160, train_perplexity=2509.761, train_loss=7.827943

Batch 32170, train_perplexity=2399.571, train_loss=7.7830453

Batch 32180, train_perplexity=2376.8127, train_loss=7.7735157

Batch 32190, train_perplexity=2305.4463, train_loss=7.7430296

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 32200, train_perplexity=2350.0295, train_loss=7.762183

Batch 32210, train_perplexity=2670.8447, train_loss=7.89015

Batch 32220, train_perplexity=2537.2102, train_loss=7.8388205

Batch 32230, train_perplexity=2525.1504, train_loss=7.834056

Batch 32240, train_perplexity=2367.8682, train_loss=7.7697453

Batch 32250, train_perplexity=2418.1687, train_loss=7.790766

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 32260, train_perplexity=2344.4312, train_loss=7.759798

Batch 32270, train_perplexity=2439.4785, train_loss=7.7995396

Batch 32280, train_perplexity=2461.6287, train_loss=7.8085785

Batch 32290, train_perplexity=2421.8635, train_loss=7.7922926

Batch 32300, train_perplexity=2423.8855, train_loss=7.793127

Batch 32310, train_perplexity=2352.1104, train_loss=7.763068

Batch 32320, train_perplexity=2508.33, train_loss=7.8273726

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 32330, train_perplexity=2451.8599, train_loss=7.804602

Batch 32340, train_perplexity=2470.5823, train_loss=7.812209

Batch 32350, train_perplexity=2456.4155, train_loss=7.8064585

Batch 32360, train_perplexity=2168.5317, train_loss=7.6818056

Batch 32370, train_perplexity=2536.3394, train_loss=7.838477

Batch 32380, train_perplexity=2293.2751, train_loss=7.737736

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 32390, train_perplexity=2440.3418, train_loss=7.7998934

Batch 32400, train_perplexity=2423.8333, train_loss=7.7931056

Batch 32410, train_perplexity=2457.9785, train_loss=7.8070946

Batch 32420, train_perplexity=2499.3638, train_loss=7.8237915

Batch 32430, train_perplexity=2516.179, train_loss=7.830497

Batch 32440, train_perplexity=2586.0042, train_loss=7.857869

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 32450, train_perplexity=2310.813, train_loss=7.7453547

Batch 32460, train_perplexity=2215.9856, train_loss=7.7034526

Batch 32470, train_perplexity=2577.0378, train_loss=7.854396

Batch 32480, train_perplexity=2657.6172, train_loss=7.8851852

Batch 32490, train_perplexity=2493.9138, train_loss=7.8216085

Batch 32500, train_perplexity=2401.847, train_loss=7.7839932

Batch 32510, train_perplexity=2750.058, train_loss=7.9193773

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 32520, train_perplexity=2469.368, train_loss=7.8117175

Batch 32530, train_perplexity=2468.8018, train_loss=7.811488

Batch 32540, train_perplexity=2400.1467, train_loss=7.783285

Batch 32550, train_perplexity=2743.9868, train_loss=7.917167

Batch 32560, train_perplexity=2436.0505, train_loss=7.7981334

Batch 32570, train_perplexity=2469.0842, train_loss=7.8116026

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 32580, train_perplexity=2400.4844, train_loss=7.783426

Batch 32590, train_perplexity=2507.969, train_loss=7.8272285

Batch 32600, train_perplexity=2390.0918, train_loss=7.779087

Batch 32610, train_perplexity=2442.1997, train_loss=7.8006544

Batch 32620, train_perplexity=2596.1948, train_loss=7.861802

Batch 32630, train_perplexity=2757.79, train_loss=7.922185

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 32640, train_perplexity=2398.0715, train_loss=7.78242

Batch 32650, train_perplexity=2369.367, train_loss=7.770378

Batch 32660, train_perplexity=2505.3706, train_loss=7.826192

Batch 32670, train_perplexity=2292.6814, train_loss=7.7374773

Batch 32680, train_perplexity=2527.4417, train_loss=7.834963

Batch 32690, train_perplexity=2436.6245, train_loss=7.798369

Batch 32700, train_perplexity=2580.2617, train_loss=7.855646

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 32710, train_perplexity=2885.4573, train_loss=7.9674387

Batch 32720, train_perplexity=2486.253, train_loss=7.818532

Batch 32730, train_perplexity=2088.0854, train_loss=7.644003

Batch 32740, train_perplexity=2330.9878, train_loss=7.7540474

Batch 32750, train_perplexity=2453.558, train_loss=7.8052945

Batch 32760, train_perplexity=2329.2212, train_loss=7.753289

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 32770, train_perplexity=2615.6113, train_loss=7.869253

Batch 32780, train_perplexity=2279.4055, train_loss=7.73167

Batch 32790, train_perplexity=2435.6406, train_loss=7.797965

Batch 32800, train_perplexity=2325.5566, train_loss=7.7517147

Batch 32810, train_perplexity=2677.4321, train_loss=7.8926134

Batch 32820, train_perplexity=2670.5237, train_loss=7.89003

Batch 32830, train_perplexity=2591.2612, train_loss=7.8599

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 32840, train_perplexity=2516.1023, train_loss=7.8304663

Batch 32850, train_perplexity=2658.5908, train_loss=7.8855515

Batch 32860, train_perplexity=2222.8477, train_loss=7.7065444

Batch 32870, train_perplexity=2570.2734, train_loss=7.8517675

Batch 32880, train_perplexity=2511.0227, train_loss=7.8284454

Batch 32890, train_perplexity=2468.4673, train_loss=7.8113527

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 32900, train_perplexity=2382.834, train_loss=7.776046

Batch 32910, train_perplexity=2447.8635, train_loss=7.802971

Batch 32920, train_perplexity=2509.7192, train_loss=7.827926

Batch 32930, train_perplexity=2490.4485, train_loss=7.820218

Batch 32940, train_perplexity=2531.5957, train_loss=7.836605

Batch 32950, train_perplexity=2316.1985, train_loss=7.7476826

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 32960, train_perplexity=2486.8992, train_loss=7.818792

Batch 32970, train_perplexity=2434.2913, train_loss=7.797411

Batch 32980, train_perplexity=2584.9956, train_loss=7.857479

Batch 32990, train_perplexity=2398.7417, train_loss=7.7826996

Batch 33000, train_perplexity=2356.5132, train_loss=7.7649384

Batch 33010, train_perplexity=2578.289, train_loss=7.8548813

Batch 33020, train_perplexity=2486.9382, train_loss=7.8188076

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 33030, train_perplexity=2604.947, train_loss=7.8651676

Batch 33040, train_perplexity=2550.5364, train_loss=7.844059

Batch 33050, train_perplexity=2531.1611, train_loss=7.8364334

Batch 33060, train_perplexity=2382.793, train_loss=7.7760286

Batch 33070, train_perplexity=2414.4631, train_loss=7.7892323

Batch 33080, train_perplexity=2512.6277, train_loss=7.8290844

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 33090, train_perplexity=2468.5757, train_loss=7.8113966

Batch 33100, train_perplexity=2395.1047, train_loss=7.7811823

Batch 33110, train_perplexity=2601.6067, train_loss=7.8638844

Batch 33120, train_perplexity=2332.2485, train_loss=7.754588

Batch 33130, train_perplexity=2513.0364, train_loss=7.829247

Batch 33140, train_perplexity=2530.6204, train_loss=7.83622

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 33150, train_perplexity=2184.5332, train_loss=7.6891575

Batch 33160, train_perplexity=2647.1704, train_loss=7.8812466

Batch 33170, train_perplexity=2386.0847, train_loss=7.777409

Batch 33180, train_perplexity=2739.7861, train_loss=7.915635

Batch 33190, train_perplexity=2325.55, train_loss=7.751712

Batch 33200, train_perplexity=2819.3215, train_loss=7.9442515

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 33210, train_perplexity=2462.3025, train_loss=7.808852

Batch 33220, train_perplexity=2381.4912, train_loss=7.775482

Batch 33230, train_perplexity=2265.2935, train_loss=7.7254596

Batch 33240, train_perplexity=2426.388, train_loss=7.794159

Batch 33250, train_perplexity=2239.2073, train_loss=7.713877

Batch 33260, train_perplexity=2519.6802, train_loss=7.8318872

Batch 33270, train_perplexity=2553.742, train_loss=7.845315

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 33280, train_perplexity=2365.9473, train_loss=7.768934

Batch 33290, train_perplexity=2407.0647, train_loss=7.7861633

Batch 33300, train_perplexity=2352.4065, train_loss=7.763194

Batch 33310, train_perplexity=2350.989, train_loss=7.7625914

Batch 33320, train_perplexity=2405.424, train_loss=7.7854815

Batch 33330, train_perplexity=2618.3442, train_loss=7.8702974

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 33340, train_perplexity=2559.6206, train_loss=7.8476143

Batch 33350, train_perplexity=2437.6855, train_loss=7.7988043

Batch 33360, train_perplexity=2489.488, train_loss=7.8198323

Batch 33370, train_perplexity=2414.8477, train_loss=7.7893915

Batch 33380, train_perplexity=2572.1418, train_loss=7.8524942

Batch 33390, train_perplexity=2548.3215, train_loss=7.84319

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 33400, train_perplexity=2279.613, train_loss=7.731761

Batch 33410, train_perplexity=2250.085, train_loss=7.7187233

Batch 33420, train_perplexity=2427.464, train_loss=7.7946024
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 33430, train_perplexity=2683.5466, train_loss=7.8948946

Batch 33440, train_perplexity=2549.425, train_loss=7.843623

Batch 33450, train_perplexity=2666.775, train_loss=7.888625

Batch 33460, train_perplexity=2563.1797, train_loss=7.849004

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 33470, train_perplexity=2528.489, train_loss=7.835377

Batch 33480, train_perplexity=2705.0635, train_loss=7.9028807

Batch 33490, train_perplexity=2505.582, train_loss=7.8262763

Batch 33500, train_perplexity=2349.8481, train_loss=7.762106

Batch 33510, train_perplexity=2415.4834, train_loss=7.7896547

Batch 33520, train_perplexity=2581.719, train_loss=7.8562107

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 33530, train_perplexity=2452.31, train_loss=7.8047857

Batch 33540, train_perplexity=2442.9219, train_loss=7.80095

Batch 33550, train_perplexity=2547.5828, train_loss=7.8429003

Batch 33560, train_perplexity=2495.641, train_loss=7.822301

Batch 33570, train_perplexity=2471.2114, train_loss=7.8124638

Batch 33580, train_perplexity=2454.0425, train_loss=7.805492

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 33590, train_perplexity=2536.6055, train_loss=7.838582

Batch 33600, train_perplexity=2409.0212, train_loss=7.786976

Batch 33610, train_perplexity=2845.2285, train_loss=7.9533987

Batch 33620, train_perplexity=2359.6233, train_loss=7.7662573

Batch 33630, train_perplexity=2340.7383, train_loss=7.7582216

Batch 33640, train_perplexity=2265.8477, train_loss=7.725704

Batch 33650, train_perplexity=2434.1301, train_loss=7.7973447

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 33660, train_perplexity=2402.7644, train_loss=7.784375

Batch 33670, train_perplexity=2368.967, train_loss=7.7702093

Batch 33680, train_perplexity=2374.683, train_loss=7.7726192

Batch 33690, train_perplexity=2296.5151, train_loss=7.739148

Batch 33700, train_perplexity=2607.7085, train_loss=7.866227

Batch 33710, train_perplexity=2633.8982, train_loss=7.87622

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 33720, train_perplexity=2502.6768, train_loss=7.825116

Batch 33730, train_perplexity=2269.5955, train_loss=7.727357

Batch 33740, train_perplexity=2464.8494, train_loss=7.809886

Batch 33750, train_perplexity=2400.8152, train_loss=7.7835636

Batch 33760, train_perplexity=2322.888, train_loss=7.7505665

Batch 33770, train_perplexity=2561.6584, train_loss=7.84841

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 33780, train_perplexity=2509.7708, train_loss=7.8279467

Batch 33790, train_perplexity=2372.9624, train_loss=7.7718945

Batch 33800, train_perplexity=2496.2207, train_loss=7.822533

Batch 33810, train_perplexity=2364.4055, train_loss=7.768282

Batch 33820, train_perplexity=2574.2585, train_loss=7.853317

Batch 33830, train_perplexity=2468.5898, train_loss=7.8114023

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 33840, train_perplexity=2571.931, train_loss=7.852412

Batch 33850, train_perplexity=2390.1729, train_loss=7.779121

Batch 33860, train_perplexity=2232.924, train_loss=7.711067

Batch 33870, train_perplexity=2525.095, train_loss=7.834034

Batch 33880, train_perplexity=2589.511, train_loss=7.8592243

Batch 33890, train_perplexity=2476.0273, train_loss=7.8144107

Batch 33900, train_perplexity=2498.706, train_loss=7.8235283

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 33910, train_perplexity=2347.5925, train_loss=7.7611456

Batch 33920, train_perplexity=2345.7932, train_loss=7.760379

Batch 33930, train_perplexity=2451.7068, train_loss=7.8045397

Batch 33940, train_perplexity=2472.6694, train_loss=7.8130536

Batch 33950, train_perplexity=2571.7053, train_loss=7.8523245

Batch 33960, train_perplexity=2368.9163, train_loss=7.770188

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 33970, train_perplexity=2260.1328, train_loss=7.723179

Batch 33980, train_perplexity=2367.3613, train_loss=7.7695312

Batch 33990, train_perplexity=2479.5422, train_loss=7.8158293

Batch 34000, train_perplexity=2499.0503, train_loss=7.823666

Batch 34010, train_perplexity=2485.127, train_loss=7.818079

Batch 34020, train_perplexity=2446.2637, train_loss=7.802317

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 34030, train_perplexity=2433.9583, train_loss=7.797274

Batch 34040, train_perplexity=2354.5645, train_loss=7.764111

Batch 34050, train_perplexity=2396.1992, train_loss=7.781639

Batch 34060, train_perplexity=2494.3408, train_loss=7.8217797

Batch 34070, train_perplexity=2321.6277, train_loss=7.750024

Batch 34080, train_perplexity=2469.3809, train_loss=7.8117228

Batch 34090, train_perplexity=2599.557, train_loss=7.863096

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 34100, train_perplexity=2491.5674, train_loss=7.8206673

Batch 34110, train_perplexity=2348.13, train_loss=7.7613745

Batch 34120, train_perplexity=2352.2134, train_loss=7.763112

Batch 34130, train_perplexity=2347.2666, train_loss=7.761007

Batch 34140, train_perplexity=2484.5796, train_loss=7.8178587

Batch 34150, train_perplexity=2406.826, train_loss=7.786064

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 34160, train_perplexity=2338.8472, train_loss=7.7574134

Batch 34170, train_perplexity=2270.3, train_loss=7.7276673

Batch 34180, train_perplexity=2437.396, train_loss=7.7986856

Batch 34190, train_perplexity=2150.9146, train_loss=7.6736484

Batch 34200, train_perplexity=2423.8855, train_loss=7.793127

Batch 34210, train_perplexity=2696.8645, train_loss=7.899845

Batch 34220, train_perplexity=2405.417, train_loss=7.7854786

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 34230, train_perplexity=2560.817, train_loss=7.8480816

Batch 34240, train_perplexity=2506.8882, train_loss=7.8267975

Batch 34250, train_perplexity=2505.184, train_loss=7.8261175

Batch 34260, train_perplexity=2583.8396, train_loss=7.857032

Batch 34270, train_perplexity=2466.0027, train_loss=7.8103538

Batch 34280, train_perplexity=2478.7444, train_loss=7.8155074

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 34290, train_perplexity=2492.057, train_loss=7.8208637

Batch 34300, train_perplexity=2628.5332, train_loss=7.8741813

Batch 34310, train_perplexity=2327.0652, train_loss=7.752363

Batch 34320, train_perplexity=2445.171, train_loss=7.8018703

Batch 34330, train_perplexity=2419.5168, train_loss=7.791323

Batch 34340, train_perplexity=2277.8691, train_loss=7.7309957

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 34350, train_perplexity=2288.2275, train_loss=7.7355328

Batch 34360, train_perplexity=2481.7922, train_loss=7.816736

Batch 34370, train_perplexity=2577.5342, train_loss=7.8545885

Batch 34380, train_perplexity=2582.5315, train_loss=7.8565254

Batch 34390, train_perplexity=2368.9016, train_loss=7.7701817

Batch 34400, train_perplexity=2317.3796, train_loss=7.7481923

Batch 34410, train_perplexity=2189.4377, train_loss=7.6914

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 34420, train_perplexity=2356.083, train_loss=7.7647557

Batch 34430, train_perplexity=2854.706, train_loss=7.956724

Batch 34440, train_perplexity=2368.4746, train_loss=7.7700014

Batch 34450, train_perplexity=2301.185, train_loss=7.7411795

Batch 34460, train_perplexity=2641.2947, train_loss=7.8790245

Batch 34470, train_perplexity=2288.9695, train_loss=7.735857

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 34480, train_perplexity=2280.4849, train_loss=7.7321434

Batch 34490, train_perplexity=2403.947, train_loss=7.7848673

Batch 34500, train_perplexity=2487.8823, train_loss=7.819187

Batch 34510, train_perplexity=2313.7922, train_loss=7.746643

Batch 34520, train_perplexity=2423.7156, train_loss=7.793057

Batch 34530, train_perplexity=2263.9124, train_loss=7.7248497

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 34540, train_perplexity=2477.9868, train_loss=7.8152018

Batch 34550, train_perplexity=2266.7349, train_loss=7.7260957

Batch 34560, train_perplexity=2641.8413, train_loss=7.8792315

Batch 34570, train_perplexity=2228.5098, train_loss=7.7090883

Batch 34580, train_perplexity=2412.3687, train_loss=7.7883644

Batch 34590, train_perplexity=2475.9907, train_loss=7.814396

Batch 34600, train_perplexity=2358.962, train_loss=7.765977

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 34610, train_perplexity=2441.2146, train_loss=7.800251

Batch 34620, train_perplexity=2300.4587, train_loss=7.740864

Batch 34630, train_perplexity=2451.9873, train_loss=7.804654

Batch 34640, train_perplexity=2309.359, train_loss=7.744725

Batch 34650, train_perplexity=2505.5999, train_loss=7.8262835

Batch 34660, train_perplexity=2392.9153, train_loss=7.7802677

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 34670, train_perplexity=2460.9727, train_loss=7.808312

Batch 34680, train_perplexity=2347.494, train_loss=7.7611036

Batch 34690, train_perplexity=2579.3884, train_loss=7.8553076

Batch 34700, train_perplexity=2459.7913, train_loss=7.807832

Batch 34710, train_perplexity=2545.191, train_loss=7.841961

Batch 34720, train_perplexity=2355.4966, train_loss=7.764507

Batch 34730, train_perplexity=2450.6267, train_loss=7.804099

Batch 34740, train_perplexity=2220.4187, train_loss=7.705451

Batch 34750, train_perplexity=2369.654, train_loss=7.770499

Batch 34760, train_perplexity=2416.6216, train_loss=7.790126
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 34770, train_perplexity=2377.0996, train_loss=7.7736363

Batch 34780, train_perplexity=2568.1245, train_loss=7.850931

Batch 34790, train_perplexity=2165.6467, train_loss=7.6804743

Batch 34800, train_perplexity=2629.0483, train_loss=7.8743773

Batch 34810, train_perplexity=2572.0647, train_loss=7.852464

Batch 34820, train_perplexity=2508.256, train_loss=7.827343

Batch 34830, train_perplexity=2369.4597, train_loss=7.770417

Batch 34840, train_perplexity=2337.283, train_loss=7.7567444

Batch 34850, train_perplexity=2428.6125, train_loss=7.7950754

Batch 34860, train_perplexity=2370.4858, train_loss=7.77085

Batch 34870, train_perplexity=2456.261, train_loss=7.8063955

Batch 34880, train_perplexity=2536.7651, train_loss=7.838645

Batch 34890, train_perplexity=2450.899, train_loss=7.80421

Batch 34900, train_perplexity=2325.8184, train_loss=7.7518272

Batch 34910, train_perplexity=2482.5403, train_loss=7.8170376

Batch 34920, train_perplexity=2433.9502, train_loss=7.797271

Batch 34930, train_perplexity=2464.0972, train_loss=7.809581

Batch 34940, train_perplexity=2458.9656, train_loss=7.807496

Batch 34950, train_perplexity=2436.8499, train_loss=7.7984614

Batch 34960, train_perplexity=2615.19, train_loss=7.869092

Batch 34970, train_perplexity=2741.838, train_loss=7.9163837

Batch 34980, train_perplexity=2408.895, train_loss=7.7869234

Batch 34990, train_perplexity=2340.3274, train_loss=7.758046

Batch 35000, train_perplexity=2460.8284, train_loss=7.8082533

Batch 35010, train_perplexity=2547.719, train_loss=7.8429537

Batch 35020, train_perplexity=2379.5571, train_loss=7.7746696

Batch 35030, train_perplexity=2310.055, train_loss=7.7450266

Batch 35040, train_perplexity=2580.7612, train_loss=7.8558397

Batch 35050, train_perplexity=2429.9077, train_loss=7.7956085

Batch 35060, train_perplexity=2481.2515, train_loss=7.8165183

Batch 35070, train_perplexity=2289.1792, train_loss=7.7359486

Batch 35080, train_perplexity=2347.4177, train_loss=7.761071

Batch 35090, train_perplexity=2368.5334, train_loss=7.770026

Batch 35100, train_perplexity=2309.7554, train_loss=7.744897

Batch 35110, train_perplexity=2504.3481, train_loss=7.8257837

Batch 35120, train_perplexity=2445.5093, train_loss=7.8020086

Batch 35130, train_perplexity=2529.7517, train_loss=7.8358765

Batch 35140, train_perplexity=2429.7756, train_loss=7.795554

Batch 35150, train_perplexity=2551.6897, train_loss=7.844511

Batch 35160, train_perplexity=2321.1982, train_loss=7.749839

Batch 35170, train_perplexity=2643.8123, train_loss=7.879977

Batch 35180, train_perplexity=2670.497, train_loss=7.89002

Batch 35190, train_perplexity=2292.043, train_loss=7.737199

Batch 35200, train_perplexity=2333.0283, train_loss=7.7549224

Batch 35210, train_perplexity=2367.9348, train_loss=7.7697735

Batch 35220, train_perplexity=2427.4028, train_loss=7.794577

Batch 35230, train_perplexity=2559.0408, train_loss=7.847388

Batch 35240, train_perplexity=2295.82, train_loss=7.7388453

Batch 35250, train_perplexity=2519.4807, train_loss=7.831808

Batch 35260, train_perplexity=2274.408, train_loss=7.729475

Batch 35270, train_perplexity=2523.836, train_loss=7.833535

Batch 35280, train_perplexity=2323.6545, train_loss=7.7508965

Batch 35290, train_perplexity=2425.8718, train_loss=7.7939463

Batch 35300, train_perplexity=2350.7266, train_loss=7.76248

Batch 35310, train_perplexity=2371.9895, train_loss=7.7714844

Batch 35320, train_perplexity=2713.2383, train_loss=7.905898

Batch 35330, train_perplexity=2467.8555, train_loss=7.811105

Batch 35340, train_perplexity=2219.7961, train_loss=7.7051706

Batch 35350, train_perplexity=2442.7598, train_loss=7.800884

Batch 35360, train_perplexity=2392.1328, train_loss=7.7799406

Batch 35370, train_perplexity=2283.4382, train_loss=7.7334375

Batch 35380, train_perplexity=2339.2976, train_loss=7.757606

Batch 35390, train_perplexity=2440.2346, train_loss=7.7998495

Batch 35400, train_perplexity=2645.6902, train_loss=7.880687

Batch 35410, train_perplexity=2525.95, train_loss=7.8343725

Batch 35420, train_perplexity=2390.167, train_loss=7.7791185

Batch 35430, train_perplexity=2566.095, train_loss=7.8501406

Batch 35440, train_perplexity=2349.2095, train_loss=7.761834

Batch 35450, train_perplexity=2254.9248, train_loss=7.720872

Batch 35460, train_perplexity=2569.2905, train_loss=7.851385

Batch 35470, train_perplexity=2424.5073, train_loss=7.7933836

Batch 35480, train_perplexity=2553.737, train_loss=7.845313

Batch 35490, train_perplexity=2396.0232, train_loss=7.7815657

Batch 35500, train_perplexity=2235.1143, train_loss=7.7120476

Batch 35510, train_perplexity=2527.8804, train_loss=7.8351364

Batch 35520, train_perplexity=2388.7258, train_loss=7.7785153

Batch 35530, train_perplexity=2483.7478, train_loss=7.817524

Batch 35540, train_perplexity=2375.6843, train_loss=7.773041

Batch 35550, train_perplexity=2589.6294, train_loss=7.85927

Batch 35560, train_perplexity=2312.2083, train_loss=7.7459583

Batch 35570, train_perplexity=2593.48, train_loss=7.860756

Batch 35580, train_perplexity=2634.5903, train_loss=7.876483

Batch 35590, train_perplexity=2239.3665, train_loss=7.7139482

Batch 35600, train_perplexity=2421.4316, train_loss=7.7921143

Batch 35610, train_perplexity=2578.8435, train_loss=7.8550963

Batch 35620, train_perplexity=2350.4812, train_loss=7.7623754

Batch 35630, train_perplexity=2356.1458, train_loss=7.7647824

Batch 35640, train_perplexity=2376.663, train_loss=7.7734528

Batch 35650, train_perplexity=2271.566, train_loss=7.7282248

Batch 35660, train_perplexity=2429.7664, train_loss=7.7955503

Batch 35670, train_perplexity=2376.8694, train_loss=7.7735395

Batch 35680, train_perplexity=2410.0532, train_loss=7.787404

Batch 35690, train_perplexity=2341.2205, train_loss=7.7584276

Batch 35700, train_perplexity=2387.825, train_loss=7.778138

Batch 35710, train_perplexity=2388.072, train_loss=7.7782416

Batch 35720, train_perplexity=2526.2537, train_loss=7.8344927

Batch 35730, train_perplexity=2424.8506, train_loss=7.793525

Batch 35740, train_perplexity=2353.4722, train_loss=7.763647

Batch 35750, train_perplexity=2381.9866, train_loss=7.77569

Batch 35760, train_perplexity=2478.5256, train_loss=7.815419

Batch 35770, train_perplexity=2363.4329, train_loss=7.7678704

Batch 35780, train_perplexity=2511.133, train_loss=7.8284893

Batch 35790, train_perplexity=2356.1086, train_loss=7.7647667

Batch 35800, train_perplexity=2289.2664, train_loss=7.7359867

Batch 35810, train_perplexity=2316.1467, train_loss=7.74766

Batch 35820, train_perplexity=2128.5627, train_loss=7.6632023

Batch 35830, train_perplexity=2559.6914, train_loss=7.847642

Batch 35840, train_perplexity=2581.9897, train_loss=7.8563156

Batch 35850, train_perplexity=2286.0115, train_loss=7.734564

Batch 35860, train_perplexity=2273.6726, train_loss=7.7291517

Batch 35870, train_perplexity=2550.2349, train_loss=7.8439407

Batch 35880, train_perplexity=2271.2324, train_loss=7.728078

Batch 35890, train_perplexity=2451.8188, train_loss=7.8045855

Batch 35900, train_perplexity=2392.9016, train_loss=7.780262

Batch 35910, train_perplexity=2340.4324, train_loss=7.758091

Batch 35920, train_perplexity=2668.7747, train_loss=7.8893747

Batch 35930, train_perplexity=2295.784, train_loss=7.7388296

Batch 35940, train_perplexity=2479.6794, train_loss=7.8158846

Batch 35950, train_perplexity=2427.618, train_loss=7.794666

Batch 35960, train_perplexity=2347.7896, train_loss=7.7612295

Batch 35970, train_perplexity=2305.3694, train_loss=7.742996

Batch 35980, train_perplexity=2361.832, train_loss=7.767193

Batch 35990, train_perplexity=2403.28, train_loss=7.78459

Batch 36000, train_perplexity=2319.7776, train_loss=7.7492266

Batch 36010, train_perplexity=2338.8035, train_loss=7.757395

Batch 36020, train_perplexity=2684.7087, train_loss=7.8953276

Batch 36030, train_perplexity=2473.6648, train_loss=7.813456

Batch 36040, train_perplexity=2504.6921, train_loss=7.825921

Batch 36050, train_perplexity=2277.1187, train_loss=7.730666

Batch 36060, train_perplexity=2517.647, train_loss=7.83108

Batch 36070, train_perplexity=2493.374, train_loss=7.821392

Batch 36080, train_perplexity=2554.0684, train_loss=7.845443
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 36090, train_perplexity=2381.0713, train_loss=7.7753057

Batch 36100, train_perplexity=2598.0598, train_loss=7.86252

Batch 36110, train_perplexity=2308.9934, train_loss=7.744567

Batch 36120, train_perplexity=2613.8335, train_loss=7.868573

Batch 36130, train_perplexity=2318.394, train_loss=7.74863

Batch 36140, train_perplexity=2374.7498, train_loss=7.7726474

Batch 36150, train_perplexity=2401.5776, train_loss=7.783881

Batch 36160, train_perplexity=2621.0837, train_loss=7.871343

Batch 36170, train_perplexity=2445.768, train_loss=7.8021145

Batch 36180, train_perplexity=2711.5515, train_loss=7.9052763

Batch 36190, train_perplexity=2353.516, train_loss=7.7636657

Batch 36200, train_perplexity=2209.5693, train_loss=7.700553

Batch 36210, train_perplexity=2366.1018, train_loss=7.768999

Batch 36220, train_perplexity=2229.158, train_loss=7.709379

Batch 36230, train_perplexity=2594.482, train_loss=7.861142

Batch 36240, train_perplexity=2270.6736, train_loss=7.727832

Batch 36250, train_perplexity=2545.5852, train_loss=7.842116

Batch 36260, train_perplexity=2318.3223, train_loss=7.748599

Batch 36270, train_perplexity=2297.065, train_loss=7.7393875

Batch 36280, train_perplexity=2351.527, train_loss=7.7628202

Batch 36290, train_perplexity=2451.244, train_loss=7.804351

Batch 36300, train_perplexity=2479.2397, train_loss=7.815707

Batch 36310, train_perplexity=2400.7305, train_loss=7.7835283

Batch 36320, train_perplexity=2370.9673, train_loss=7.7710533

Batch 36330, train_perplexity=2435.1667, train_loss=7.7977705

Batch 36340, train_perplexity=2332.7002, train_loss=7.7547817

Batch 36350, train_perplexity=2355.4797, train_loss=7.7644997

Batch 36360, train_perplexity=2498.3557, train_loss=7.823388

Batch 36370, train_perplexity=2384.3115, train_loss=7.7766657

Batch 36380, train_perplexity=2515.8528, train_loss=7.830367

Batch 36390, train_perplexity=2390.8066, train_loss=7.779386

Batch 36400, train_perplexity=2597.619, train_loss=7.8623505

Batch 36410, train_perplexity=2499.614, train_loss=7.8238916

Batch 36420, train_perplexity=2473.5715, train_loss=7.8134184

Batch 36430, train_perplexity=2354.0806, train_loss=7.7639055

Batch 36440, train_perplexity=2321.0244, train_loss=7.749764

Batch 36450, train_perplexity=2540.2998, train_loss=7.8400373

Batch 36460, train_perplexity=2505.8198, train_loss=7.826371

Batch 36470, train_perplexity=2541.3877, train_loss=7.8404655

Batch 36480, train_perplexity=2563.6245, train_loss=7.8491774

Batch 36490, train_perplexity=2250.994, train_loss=7.719127

Batch 36500, train_perplexity=2548.0566, train_loss=7.8430862

Batch 36510, train_perplexity=2339.0356, train_loss=7.757494

Batch 36520, train_perplexity=2511.3652, train_loss=7.828582

Batch 36530, train_perplexity=2222.5923, train_loss=7.7064295

Batch 36540, train_perplexity=2361.0742, train_loss=7.766872

Batch 36550, train_perplexity=2491.0447, train_loss=7.8204575

Batch 36560, train_perplexity=2473.2307, train_loss=7.8132806

Batch 36570, train_perplexity=2397.948, train_loss=7.7823687

Batch 36580, train_perplexity=2539.6118, train_loss=7.8397665

Batch 36590, train_perplexity=2316.7043, train_loss=7.747901

Batch 36600, train_perplexity=2330.6833, train_loss=7.7539167

Batch 36610, train_perplexity=2420.7876, train_loss=7.791848

Batch 36620, train_perplexity=2287.8237, train_loss=7.7353563

Batch 36630, train_perplexity=2432.9082, train_loss=7.7968426

Batch 36640, train_perplexity=2269.3445, train_loss=7.7272463

Batch 36650, train_perplexity=2467.5247, train_loss=7.810971

Batch 36660, train_perplexity=2550.5828, train_loss=7.844077

Batch 36670, train_perplexity=2399.5518, train_loss=7.783037

Batch 36680, train_perplexity=2378.0483, train_loss=7.7740355

Batch 36690, train_perplexity=2637.7832, train_loss=7.877694

Batch 36700, train_perplexity=2322.939, train_loss=7.7505884

Batch 36710, train_perplexity=2317.3408, train_loss=7.7481756

Batch 36720, train_perplexity=2371.2761, train_loss=7.7711835

Batch 36730, train_perplexity=2332.0774, train_loss=7.7545147

Batch 36740, train_perplexity=2406.3555, train_loss=7.7858686

Batch 36750, train_perplexity=2424.1328, train_loss=7.793229

Batch 36760, train_perplexity=2450.4749, train_loss=7.804037

Batch 36770, train_perplexity=2255.2905, train_loss=7.721034

Batch 36780, train_perplexity=2263.488, train_loss=7.7246623

Batch 36790, train_perplexity=2295.1074, train_loss=7.738535

Batch 36800, train_perplexity=2398.9568, train_loss=7.782789

Batch 36810, train_perplexity=2375.9426, train_loss=7.7731495

Batch 36820, train_perplexity=2642.2874, train_loss=7.8794003

Batch 36830, train_perplexity=2517.4895, train_loss=7.8310175

Batch 36840, train_perplexity=2413.9763, train_loss=7.7890306

Batch 36850, train_perplexity=2380.4514, train_loss=7.7750454

Batch 36860, train_perplexity=2252.1482, train_loss=7.71964

Batch 36870, train_perplexity=2327.0476, train_loss=7.7523556

Batch 36880, train_perplexity=2335.7075, train_loss=7.75607

Batch 36890, train_perplexity=2245.6646, train_loss=7.716757

Batch 36900, train_perplexity=2295.7422, train_loss=7.7388115

Batch 36910, train_perplexity=2290.7253, train_loss=7.736624

Batch 36920, train_perplexity=2311.9878, train_loss=7.745863

Batch 36930, train_perplexity=2120.325, train_loss=7.6593246

Batch 36940, train_perplexity=2461.8071, train_loss=7.808651

Batch 36950, train_perplexity=2483.4731, train_loss=7.8174133

Batch 36960, train_perplexity=2408.879, train_loss=7.7869167

Batch 36970, train_perplexity=2272.1987, train_loss=7.728503

Batch 36980, train_perplexity=2403.4038, train_loss=7.7846413

Batch 36990, train_perplexity=2440.8015, train_loss=7.8000817

Batch 37000, train_perplexity=2364.9016, train_loss=7.7684917

Batch 37010, train_perplexity=2380.6511, train_loss=7.7751293

Batch 37020, train_perplexity=2303.7322, train_loss=7.7422857

Batch 37030, train_perplexity=2252.056, train_loss=7.719599

Batch 37040, train_perplexity=2554.6772, train_loss=7.845681

Batch 37050, train_perplexity=2487.6594, train_loss=7.8190975

Batch 37060, train_perplexity=2248.0132, train_loss=7.717802

Batch 37070, train_perplexity=2423.229, train_loss=7.792856

Batch 37080, train_perplexity=2420.1343, train_loss=7.7915783

Batch 37090, train_perplexity=2234.7837, train_loss=7.7118998

Batch 37100, train_perplexity=2504.5295, train_loss=7.825856

Batch 37110, train_perplexity=2393.89, train_loss=7.780675

Batch 37120, train_perplexity=2444.812, train_loss=7.8017235

Batch 37130, train_perplexity=2304.7517, train_loss=7.742728

Batch 37140, train_perplexity=2194.484, train_loss=7.693702

Batch 37150, train_perplexity=2595.8655, train_loss=7.8616753

Batch 37160, train_perplexity=2561.198, train_loss=7.8482304

Batch 37170, train_perplexity=2323.7222, train_loss=7.7509255

Batch 37180, train_perplexity=2488.6677, train_loss=7.819503

Batch 37190, train_perplexity=2268.307, train_loss=7.726789

Batch 37200, train_perplexity=2425.372, train_loss=7.7937403

Batch 37210, train_perplexity=2293.858, train_loss=7.7379904

Batch 37220, train_perplexity=2535.2197, train_loss=7.8380356

Batch 37230, train_perplexity=2242.0525, train_loss=7.715147

Batch 37240, train_perplexity=2298.0686, train_loss=7.7398243

Batch 37250, train_perplexity=2292.9219, train_loss=7.737582

Batch 37260, train_perplexity=2209.601, train_loss=7.7005672

Batch 37270, train_perplexity=2276.348, train_loss=7.7303276

Batch 37280, train_perplexity=2318.6064, train_loss=7.7487216

Batch 37290, train_perplexity=2335.7332, train_loss=7.756081

Batch 37300, train_perplexity=2377.1936, train_loss=7.773676

Batch 37310, train_perplexity=2633.068, train_loss=7.875905

Batch 37320, train_perplexity=2405.6443, train_loss=7.785573

Batch 37330, train_perplexity=2429.6667, train_loss=7.7955093

Batch 37340, train_perplexity=2389.7227, train_loss=7.7789326

Batch 37350, train_perplexity=2364.7595, train_loss=7.7684317

Batch 37360, train_perplexity=2533.1208, train_loss=7.8372073

Batch 37370, train_perplexity=2395.058, train_loss=7.7811627

Batch 37380, train_perplexity=2276.1838, train_loss=7.7302556

Batch 37390, train_perplexity=2234.0125, train_loss=7.7115545

Batch 37400, train_perplexity=2385.4373, train_loss=7.7771378
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 37410, train_perplexity=2322.5234, train_loss=7.7504096

Batch 37420, train_perplexity=2382.0217, train_loss=7.775705

Batch 37430, train_perplexity=2303.5322, train_loss=7.742199

Batch 37440, train_perplexity=2323.0342, train_loss=7.7506294

Batch 37450, train_perplexity=2385.3384, train_loss=7.7770963

Batch 37460, train_perplexity=2103.4153, train_loss=7.6513176

Batch 37470, train_perplexity=2541.555, train_loss=7.8405313

Batch 37480, train_perplexity=2469.6682, train_loss=7.811839

Batch 37490, train_perplexity=2513.8262, train_loss=7.829561

Batch 37500, train_perplexity=2229.292, train_loss=7.7094393

Batch 37510, train_perplexity=2379.7646, train_loss=7.774757

Batch 37520, train_perplexity=2532.5967, train_loss=7.8370004

Batch 37530, train_perplexity=2243.8503, train_loss=7.7159486

Batch 37540, train_perplexity=2471.9043, train_loss=7.812744

Batch 37550, train_perplexity=2391.4998, train_loss=7.779676

Batch 37560, train_perplexity=2426.8079, train_loss=7.794332

Batch 37570, train_perplexity=2516.5403, train_loss=7.8306403

Batch 37580, train_perplexity=2285.1982, train_loss=7.734208

Batch 37590, train_perplexity=2599.9312, train_loss=7.8632402

Batch 37600, train_perplexity=2487.74, train_loss=7.81913

Batch 37610, train_perplexity=2419.4812, train_loss=7.7913084

Batch 37620, train_perplexity=2405.1763, train_loss=7.7853785

Batch 37630, train_perplexity=2433.6577, train_loss=7.7971506

Batch 37640, train_perplexity=2374.0886, train_loss=7.772369

Batch 37650, train_perplexity=2303.3938, train_loss=7.742139

Batch 37660, train_perplexity=2427.6191, train_loss=7.7946663

Batch 37670, train_perplexity=2267.7554, train_loss=7.726546

Batch 37680, train_perplexity=2434.8625, train_loss=7.7976456

Batch 37690, train_perplexity=2294.51, train_loss=7.7382746

Batch 37700, train_perplexity=2167.8381, train_loss=7.6814857

Batch 37710, train_perplexity=2446.6733, train_loss=7.8024845

Batch 37720, train_perplexity=2122.3096, train_loss=7.66026

Batch 37730, train_perplexity=2364.0076, train_loss=7.7681136

Batch 37740, train_perplexity=2264.3916, train_loss=7.7250614

Batch 37750, train_perplexity=2431.2034, train_loss=7.7961416

Batch 37760, train_perplexity=2283.8105, train_loss=7.7336006

Batch 37770, train_perplexity=2147.3247, train_loss=7.671978

Batch 37780, train_perplexity=2235.8027, train_loss=7.7123556

Batch 37790, train_perplexity=2352.9023, train_loss=7.763405

Batch 37800, train_perplexity=2177.6484, train_loss=7.686001

Batch 37810, train_perplexity=2228.327, train_loss=7.7090063

Batch 37820, train_perplexity=2457.3574, train_loss=7.806842

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 37830, train_perplexity=2438.618, train_loss=7.7991867

Batch 37840, train_perplexity=2449.56, train_loss=7.8036637

Batch 37850, train_perplexity=2680.4607, train_loss=7.893744

Batch 37860, train_perplexity=2323.6921, train_loss=7.7509127

Batch 37870, train_perplexity=2274.36, train_loss=7.729454

Batch 37880, train_perplexity=2352.1013, train_loss=7.7630644

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 37890, train_perplexity=2290.9023, train_loss=7.736701

Batch 37900, train_perplexity=2231.506, train_loss=7.710432

Batch 37910, train_perplexity=2200.5774, train_loss=7.696475

Batch 37920, train_perplexity=2292.4255, train_loss=7.7373657

Batch 37930, train_perplexity=2448.877, train_loss=7.803385

Batch 37940, train_perplexity=2251.1658, train_loss=7.7192035

Batch 37950, train_perplexity=2461.502, train_loss=7.808527

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 37960, train_perplexity=2283.414, train_loss=7.733427

Batch 37970, train_perplexity=2450.4958, train_loss=7.8040457

Batch 37980, train_perplexity=2434.1533, train_loss=7.797354

Batch 37990, train_perplexity=2511.7148, train_loss=7.828721

Batch 38000, train_perplexity=2444.1453, train_loss=7.8014507

Batch 38010, train_perplexity=2536.2063, train_loss=7.8384247

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 38020, train_perplexity=2183.773, train_loss=7.6888094

Batch 38030, train_perplexity=2275.1694, train_loss=7.7298098

Batch 38040, train_perplexity=2485.1033, train_loss=7.8180695

Batch 38050, train_perplexity=2318.6208, train_loss=7.748728

Batch 38060, train_perplexity=2407.2942, train_loss=7.7862587

Batch 38070, train_perplexity=2289.0176, train_loss=7.735878

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 38080, train_perplexity=2177.2517, train_loss=7.6858187

Batch 38090, train_perplexity=2674.1199, train_loss=7.8913755

Batch 38100, train_perplexity=2343.0632, train_loss=7.7592144

Batch 38110, train_perplexity=2509.7288, train_loss=7.82793

Batch 38120, train_perplexity=2253.1042, train_loss=7.720064

Batch 38130, train_perplexity=2283.547, train_loss=7.733485

Batch 38140, train_perplexity=2361.4175, train_loss=7.7670174

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 38150, train_perplexity=2686.4722, train_loss=7.895984

Batch 38160, train_perplexity=2358.774, train_loss=7.7658973

Batch 38170, train_perplexity=2371.4446, train_loss=7.7712545

Batch 38180, train_perplexity=2450.0051, train_loss=7.8038454

Batch 38190, train_perplexity=2230.051, train_loss=7.7097797

Batch 38200, train_perplexity=2457.6177, train_loss=7.8069477

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 38210, train_perplexity=2211.066, train_loss=7.70123

Batch 38220, train_perplexity=2556.165, train_loss=7.8462634

Batch 38230, train_perplexity=2410.005, train_loss=7.787384

Batch 38240, train_perplexity=2409.1638, train_loss=7.787035

Batch 38250, train_perplexity=2408.5159, train_loss=7.786766

Batch 38260, train_perplexity=2516.935, train_loss=7.830797
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 38270, train_perplexity=2472.8464, train_loss=7.813125

Batch 38280, train_perplexity=2156.642, train_loss=7.6763077

Batch 38290, train_perplexity=2464.2664, train_loss=7.8096495

Batch 38300, train_perplexity=2308.2449, train_loss=7.7442427

Batch 38310, train_perplexity=2426.7893, train_loss=7.7943244

Batch 38320, train_perplexity=2078.194, train_loss=7.6392546

Batch 38330, train_perplexity=2310.09, train_loss=7.745042

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 38340, train_perplexity=2415.4973, train_loss=7.7896605

Batch 38350, train_perplexity=2618.744, train_loss=7.87045

Batch 38360, train_perplexity=2382.9543, train_loss=7.7760963

Batch 38370, train_perplexity=2369.8506, train_loss=7.770582

Batch 38380, train_perplexity=2396.0964, train_loss=7.781596

Batch 38390, train_perplexity=2290.9275, train_loss=7.736712

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 38400, train_perplexity=2386.7344, train_loss=7.7776814

Batch 38410, train_perplexity=2641.8364, train_loss=7.8792295

Batch 38420, train_perplexity=2314.6174, train_loss=7.7469997

Batch 38430, train_perplexity=2391.4233, train_loss=7.779644

Batch 38440, train_perplexity=2453.3767, train_loss=7.8052206

Batch 38450, train_perplexity=2481.932, train_loss=7.8167925

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 38460, train_perplexity=2390.5227, train_loss=7.7792673

Batch 38470, train_perplexity=2154.3152, train_loss=7.675228

Batch 38480, train_perplexity=2190.1948, train_loss=7.6917458

Batch 38490, train_perplexity=2206.2214, train_loss=7.6990366

Batch 38500, train_perplexity=2305.0198, train_loss=7.7428446

Batch 38510, train_perplexity=2265.5732, train_loss=7.725583

Batch 38520, train_perplexity=2308.0676, train_loss=7.744166

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 38530, train_perplexity=2326.0369, train_loss=7.751921

Batch 38540, train_perplexity=2286.742, train_loss=7.7348833

Batch 38550, train_perplexity=2236.449, train_loss=7.7126446

Batch 38560, train_perplexity=2093.9214, train_loss=7.646794

Batch 38570, train_perplexity=2413.432, train_loss=7.788805

Batch 38580, train_perplexity=2278.774, train_loss=7.731393

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 38590, train_perplexity=2309.4216, train_loss=7.7447524

Batch 38600, train_perplexity=2519.858, train_loss=7.831958

Batch 38610, train_perplexity=2408.8042, train_loss=7.7868857

Batch 38620, train_perplexity=2430.1858, train_loss=7.795723

Batch 38630, train_perplexity=2368.005, train_loss=7.769803

Batch 38640, train_perplexity=2409.9153, train_loss=7.787347

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 38650, train_perplexity=2312.9824, train_loss=7.746293

Batch 38660, train_perplexity=2396.8745, train_loss=7.781921

Batch 38670, train_perplexity=2268.1511, train_loss=7.7267203

Batch 38680, train_perplexity=2279.9814, train_loss=7.7319226

Batch 38690, train_perplexity=2564.528, train_loss=7.8495297

Batch 38700, train_perplexity=2116.3188, train_loss=7.6574335

Batch 38710, train_perplexity=2245.5083, train_loss=7.716687

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 38720, train_perplexity=2305.6663, train_loss=7.743125

Batch 38730, train_perplexity=2092.8704, train_loss=7.6462917

Batch 38740, train_perplexity=2315.6353, train_loss=7.7474394

Batch 38750, train_perplexity=2306.4294, train_loss=7.743456

Batch 38760, train_perplexity=2366.7134, train_loss=7.7692575

Batch 38770, train_perplexity=2502.1948, train_loss=7.8249235

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 38780, train_perplexity=2308.5903, train_loss=7.7443924

Batch 38790, train_perplexity=2395.2305, train_loss=7.7812347

Batch 38800, train_perplexity=2238.1963, train_loss=7.7134256

Batch 38810, train_perplexity=2251.0273, train_loss=7.719142

Batch 38820, train_perplexity=2312.9583, train_loss=7.7462826

Batch 38830, train_perplexity=2250.8943, train_loss=7.719083

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 38840, train_perplexity=2234.3447, train_loss=7.7117033

Batch 38850, train_perplexity=2482.461, train_loss=7.8170056

Batch 38860, train_perplexity=2450.5637, train_loss=7.8040733

Batch 38870, train_perplexity=2399.8333, train_loss=7.7831545

Batch 38880, train_perplexity=2275.9028, train_loss=7.730132

Batch 38890, train_perplexity=2305.3145, train_loss=7.7429724

Batch 38900, train_perplexity=2413.303, train_loss=7.7887516

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 38910, train_perplexity=2177.047, train_loss=7.6857247

Batch 38920, train_perplexity=2303.7761, train_loss=7.742305

Batch 38930, train_perplexity=2565.6143, train_loss=7.849953

Batch 38940, train_perplexity=2025.4677, train_loss=7.613556

Batch 38950, train_perplexity=2314.0437, train_loss=7.746752

Batch 38960, train_perplexity=2400.109, train_loss=7.7832694

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 38970, train_perplexity=2385.2053, train_loss=7.7770405

Batch 38980, train_perplexity=2196.6448, train_loss=7.6946864

Batch 38990, train_perplexity=2174.94, train_loss=7.6847563

Batch 39000, train_perplexity=2454.8606, train_loss=7.805825

Batch 39010, train_perplexity=2343.2754, train_loss=7.759305

Batch 39020, train_perplexity=2163.0005, train_loss=7.6792517

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 39030, train_perplexity=2427.7373, train_loss=7.794715

Batch 39040, train_perplexity=2312.8237, train_loss=7.7462244

Batch 39050, train_perplexity=2313.0066, train_loss=7.7463036

Batch 39060, train_perplexity=2364.5881, train_loss=7.768359

Batch 39070, train_perplexity=2288.3376, train_loss=7.735581

Batch 39080, train_perplexity=2390.0747, train_loss=7.77908

Batch 39090, train_perplexity=2524.8567, train_loss=7.8339396

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 39100, train_perplexity=2230.1404, train_loss=7.70982

Batch 39110, train_perplexity=2368.931, train_loss=7.770194

Batch 39120, train_perplexity=2372.6682, train_loss=7.7717705

Batch 39130, train_perplexity=2450.0635, train_loss=7.8038692

Batch 39140, train_perplexity=2194.3062, train_loss=7.693621

Batch 39150, train_perplexity=2493.7188, train_loss=7.8215303

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 39160, train_perplexity=2388.284, train_loss=7.7783303

Batch 39170, train_perplexity=2379.5708, train_loss=7.7746754

Batch 39180, train_perplexity=2374.2234, train_loss=7.7724257

Batch 39190, train_perplexity=2659.2524, train_loss=7.8858004

Batch 39200, train_perplexity=2382.1113, train_loss=7.7757425

Batch 39210, train_perplexity=2297.7573, train_loss=7.739689

Batch 39220, train_perplexity=2177.126, train_loss=7.685761

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 39230, train_perplexity=2189.7227, train_loss=7.69153

Batch 39240, train_perplexity=2527.2607, train_loss=7.8348913

Batch 39250, train_perplexity=2525.072, train_loss=7.834025

Batch 39260, train_perplexity=2205.6628, train_loss=7.6987834

Batch 39270, train_perplexity=2325.4436, train_loss=7.751666

Batch 39280, train_perplexity=2304.8835, train_loss=7.7427855

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 39290, train_perplexity=2344.3887, train_loss=7.75978

Batch 39300, train_perplexity=2196.5295, train_loss=7.694634

Batch 39310, train_perplexity=2455.4587, train_loss=7.806069

Batch 39320, train_perplexity=2366.0002, train_loss=7.768956

Batch 39330, train_perplexity=2427.4456, train_loss=7.794595

Batch 39340, train_perplexity=2358.1655, train_loss=7.7656393

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 39350, train_perplexity=2408.0737, train_loss=7.7865825

Batch 39360, train_perplexity=2250.8406, train_loss=7.719059

Batch 39370, train_perplexity=2290.3735, train_loss=7.73647

Batch 39380, train_perplexity=2526.15, train_loss=7.8344517

Batch 39390, train_perplexity=2386.402, train_loss=7.777542

Batch 39400, train_perplexity=2238.9062, train_loss=7.7137427

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 39410, train_perplexity=2188.5808, train_loss=7.6910086

Batch 39420, train_perplexity=2394.9644, train_loss=7.7811236

Batch 39430, train_perplexity=2148.0435, train_loss=7.6723127

Batch 39440, train_perplexity=2571.6538, train_loss=7.8523045

Batch 39450, train_perplexity=2287.9536, train_loss=7.735413

Batch 39460, train_perplexity=2519.4111, train_loss=7.8317804

Batch 39470, train_perplexity=2289.4346, train_loss=7.73606

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 39480, train_perplexity=2302.9587, train_loss=7.74195

Batch 39490, train_perplexity=2215.8408, train_loss=7.7033873

Batch 39500, train_perplexity=2393.034, train_loss=7.7803173

Batch 39510, train_perplexity=2322.8857, train_loss=7.7505655

Batch 39520, train_perplexity=2406.6377, train_loss=7.785986

Batch 39530, train_perplexity=2288.6968, train_loss=7.735738

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6035 sentences.
Finished loading
Batch 39540, train_perplexity=2062.699, train_loss=7.6317706

Batch 39550, train_perplexity=2328.5815, train_loss=7.7530146

Batch 39560, train_perplexity=2498.3726, train_loss=7.823395

Batch 39570, train_perplexity=2408.509, train_loss=7.786763

Batch 39580, train_perplexity=2345.8042, train_loss=7.7603836

Batch 39590, train_perplexity=2400.394, train_loss=7.783388

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 39600, train_perplexity=2447.861, train_loss=7.80297

Batch 39610, train_perplexity=2151.7075, train_loss=7.674017

Batch 39620, train_perplexity=2015.4855, train_loss=7.6086154

Batch 39630, train_perplexity=2396.4277, train_loss=7.7817345

Batch 39640, train_perplexity=2118.7644, train_loss=7.6585884

Batch 39650, train_perplexity=2318.216, train_loss=7.7485533

Batch 39660, train_perplexity=2267.7932, train_loss=7.7265625

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 39670, train_perplexity=2351.1213, train_loss=7.7626476

Batch 39680, train_perplexity=2233.4797, train_loss=7.711316

Batch 39690, train_perplexity=2183.2544, train_loss=7.688572

Batch 39700, train_perplexity=2292.4114, train_loss=7.7373595

Batch 39710, train_perplexity=2433.0208, train_loss=7.796889

Batch 39720, train_perplexity=2311.5955, train_loss=7.745693

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 39730, train_perplexity=2293.5146, train_loss=7.7378407

Batch 39740, train_perplexity=2374.9526, train_loss=7.7727327

Batch 39750, train_perplexity=2354.1458, train_loss=7.763933

Batch 39760, train_perplexity=2329.9854, train_loss=7.7536173

Batch 39770, train_perplexity=2251.579, train_loss=7.719387

Batch 39780, train_perplexity=2367.7712, train_loss=7.7697043

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 39790, train_perplexity=2377.9146, train_loss=7.773979

Batch 39800, train_perplexity=2500.0374, train_loss=7.824061

Batch 39810, train_perplexity=2132.3052, train_loss=7.664959

Batch 39820, train_perplexity=2367.438, train_loss=7.7695637

Batch 39830, train_perplexity=2308.4297, train_loss=7.744323

Batch 39840, train_perplexity=2354.112, train_loss=7.763919

Batch 39850, train_perplexity=2247.4043, train_loss=7.717531

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 39860, train_perplexity=2201.2554, train_loss=7.696783

Batch 39870, train_perplexity=2445.6912, train_loss=7.802083

Batch 39880, train_perplexity=2147.0195, train_loss=7.671836

Batch 39890, train_perplexity=2316.1897, train_loss=7.7476788

Batch 39900, train_perplexity=2406.5735, train_loss=7.7859592

Batch 39910, train_perplexity=2280.2632, train_loss=7.732046

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 39920, train_perplexity=2269.208, train_loss=7.727186

Batch 39930, train_perplexity=2204.52, train_loss=7.698265

Batch 39940, train_perplexity=2358.755, train_loss=7.765889

Batch 39950, train_perplexity=2326.2554, train_loss=7.752015

Batch 39960, train_perplexity=2298.38, train_loss=7.7399597

Batch 39970, train_perplexity=2176.1213, train_loss=7.6852994

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 39980, train_perplexity=2530.109, train_loss=7.8360176

Batch 39990, train_perplexity=2472.3489, train_loss=7.812924

Batch 40000, train_perplexity=2423.504, train_loss=7.7929697

Batch 40010, train_perplexity=2613.3398, train_loss=7.8683844

Batch 40020, train_perplexity=2187.5125, train_loss=7.6905203

Batch 40030, train_perplexity=2360.3413, train_loss=7.7665615

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 40040, train_perplexity=2446.083, train_loss=7.802243

Batch 40050, train_perplexity=2050.5005, train_loss=7.625839

Batch 40060, train_perplexity=2405.6328, train_loss=7.785568

Batch 40070, train_perplexity=2303.697, train_loss=7.7422705

Batch 40080, train_perplexity=2132.047, train_loss=7.664838

Batch 40090, train_perplexity=2447.3523, train_loss=7.802762

Batch 40100, train_perplexity=2474.3254, train_loss=7.813723

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 40110, train_perplexity=2327.7021, train_loss=7.752637

Batch 40120, train_perplexity=2321.96, train_loss=7.750167

Batch 40130, train_perplexity=2405.121, train_loss=7.7853556

Batch 40140, train_perplexity=2404.9697, train_loss=7.7852926

Batch 40150, train_perplexity=2276.2632, train_loss=7.7302904

Batch 40160, train_perplexity=2184.3958, train_loss=7.6890945

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 40170, train_perplexity=2388.416, train_loss=7.7783856

Batch 40180, train_perplexity=2426.7234, train_loss=7.794297

Batch 40190, train_perplexity=2221.197, train_loss=7.7058015

Batch 40200, train_perplexity=2255.3442, train_loss=7.721058

Batch 40210, train_perplexity=2345.3794, train_loss=7.7602024

Batch 40220, train_perplexity=2306.0005, train_loss=7.74327

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 40230, train_perplexity=2298.8906, train_loss=7.740182

Batch 40240, train_perplexity=2458.5881, train_loss=7.8073425

Batch 40250, train_perplexity=2205.2024, train_loss=7.6985745

Batch 40260, train_perplexity=2261.9624, train_loss=7.723988

Batch 40270, train_perplexity=2322.6265, train_loss=7.750454

Batch 40280, train_perplexity=2517.6433, train_loss=7.8310785

Batch 40290, train_perplexity=2382.517, train_loss=7.775913

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 40300, train_perplexity=2439.2854, train_loss=7.7994604

Batch 40310, train_perplexity=2214.5767, train_loss=7.7028165

Batch 40320, train_perplexity=2358.4849, train_loss=7.7657747

Batch 40330, train_perplexity=2195.826, train_loss=7.6943135

Batch 40340, train_perplexity=2337.1948, train_loss=7.7567067

Batch 40350, train_perplexity=2363.168, train_loss=7.7677584

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 40360, train_perplexity=2313.5627, train_loss=7.746544

Batch 40370, train_perplexity=2260.5684, train_loss=7.7233715

Batch 40380, train_perplexity=2341.494, train_loss=7.7585444

Batch 40390, train_perplexity=2347.5364, train_loss=7.7611217

Batch 40400, train_perplexity=2256.2144, train_loss=7.7214437

Batch 40410, train_perplexity=2294.869, train_loss=7.738431

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 40420, train_perplexity=2301.7205, train_loss=7.741412

Batch 40430, train_perplexity=2243.5166, train_loss=7.7158

Batch 40440, train_perplexity=2346.9814, train_loss=7.7608852

Batch 40450, train_perplexity=2208.3328, train_loss=7.699993

Batch 40460, train_perplexity=2126.632, train_loss=7.662295

Batch 40470, train_perplexity=2354.6362, train_loss=7.7641416

Batch 40480, train_perplexity=2313.481, train_loss=7.7465086

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 40490, train_perplexity=2384.9915, train_loss=7.776951

Batch 40500, train_perplexity=2383.2068, train_loss=7.776202

Batch 40510, train_perplexity=2442.6562, train_loss=7.8008413

Batch 40520, train_perplexity=2190.2156, train_loss=7.6917553

Batch 40530, train_perplexity=2307.2039, train_loss=7.7437916

Batch 40540, train_perplexity=2310.9407, train_loss=7.74541

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 40550, train_perplexity=2317.962, train_loss=7.7484436

Batch 40560, train_perplexity=2327.4214, train_loss=7.7525163

Batch 40570, train_perplexity=2368.3232, train_loss=7.7699375

Batch 40580, train_perplexity=2424.045, train_loss=7.793193

Batch 40590, train_perplexity=2236.2603, train_loss=7.71256

Batch 40600, train_perplexity=2194.6335, train_loss=7.6937704

Batch 40610, train_perplexity=2253.8628, train_loss=7.720401

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 40620, train_perplexity=2424.3743, train_loss=7.793329

Batch 40630, train_perplexity=2306.0422, train_loss=7.743288

Batch 40640, train_perplexity=2242.983, train_loss=7.715562

Batch 40650, train_perplexity=2235.3167, train_loss=7.712138

Batch 40660, train_perplexity=2228.4045, train_loss=7.709041

Batch 40670, train_perplexity=2322.7815, train_loss=7.7505207

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 40680, train_perplexity=2284.3225, train_loss=7.7338247

Batch 40690, train_perplexity=2375.264, train_loss=7.772864

Batch 40700, train_perplexity=2685.4463, train_loss=7.895602

Batch 40710, train_perplexity=2215.508, train_loss=7.703237

Batch 40720, train_perplexity=2380.8508, train_loss=7.7752132

Batch 40730, train_perplexity=2419.1167, train_loss=7.7911577

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 40740, train_perplexity=2468.9336, train_loss=7.8115416

Batch 40750, train_perplexity=2286.3994, train_loss=7.7347336

Batch 40760, train_perplexity=2432.8896, train_loss=7.796835

Batch 40770, train_perplexity=2409.729, train_loss=7.7872696

Batch 40780, train_perplexity=2370.054, train_loss=7.770668

Batch 40790, train_perplexity=2404.126, train_loss=7.7849417

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 40800, train_perplexity=2296.4758, train_loss=7.739131

Batch 40810, train_perplexity=2398.522, train_loss=7.782608

Batch 40820, train_perplexity=2230.1787, train_loss=7.709837

Batch 40830, train_perplexity=2340.2683, train_loss=7.758021

Batch 40840, train_perplexity=2294.1357, train_loss=7.7381115

Batch 40850, train_perplexity=2174.9968, train_loss=7.6847825

Batch 40860, train_perplexity=2334.456, train_loss=7.755534
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 40870, train_perplexity=2414.043, train_loss=7.789058

Batch 40880, train_perplexity=2347.5454, train_loss=7.7611256

Batch 40890, train_perplexity=2102.978, train_loss=7.6511097

Batch 40900, train_perplexity=2248.576, train_loss=7.7180524

Batch 40910, train_perplexity=2259.385, train_loss=7.722848

Batch 40920, train_perplexity=2259.902, train_loss=7.723077

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 40930, train_perplexity=2262.0315, train_loss=7.7240186

Batch 40940, train_perplexity=2191.3167, train_loss=7.692258

Batch 40950, train_perplexity=2394.132, train_loss=7.780776

Batch 40960, train_perplexity=2301.1565, train_loss=7.741167

Batch 40970, train_perplexity=2381.7844, train_loss=7.775605

Batch 40980, train_perplexity=2604.4402, train_loss=7.864973

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 40990, train_perplexity=2374.3616, train_loss=7.772484

Batch 41000, train_perplexity=2162.2354, train_loss=7.678898

Batch 41010, train_perplexity=2382.6362, train_loss=7.775963

Batch 41020, train_perplexity=2485.274, train_loss=7.818138

Batch 41030, train_perplexity=2194.5698, train_loss=7.6937413

Batch 41040, train_perplexity=2258.2637, train_loss=7.7223516

Batch 41050, train_perplexity=2341.122, train_loss=7.7583857

Batch 41060, train_perplexity=2234.6814, train_loss=7.711854

Batch 41070, train_perplexity=2137.273, train_loss=7.667286

Batch 41080, train_perplexity=2280.4219, train_loss=7.7321157

Batch 41090, train_perplexity=2355.8818, train_loss=7.7646704

Batch 41100, train_perplexity=2457.7043, train_loss=7.806983

Batch 41110, train_perplexity=2389.1028, train_loss=7.778673

Batch 41120, train_perplexity=2373.1243, train_loss=7.7719626

Batch 41130, train_perplexity=2296.8833, train_loss=7.7393084

Batch 41140, train_perplexity=2413.6423, train_loss=7.7888923

Batch 41150, train_perplexity=2151.9866, train_loss=7.6741467

Batch 41160, train_perplexity=2240.0498, train_loss=7.7142534

Batch 41170, train_perplexity=2205.2476, train_loss=7.698595

Batch 41180, train_perplexity=2209.8728, train_loss=7.7006903

Batch 41190, train_perplexity=2465.8896, train_loss=7.810308

Batch 41200, train_perplexity=2296.4595, train_loss=7.739124

Batch 41210, train_perplexity=2179.489, train_loss=7.686846

Batch 41220, train_perplexity=2414.2388, train_loss=7.7891393

Batch 41230, train_perplexity=2307.4634, train_loss=7.743904

Batch 41240, train_perplexity=2366.2485, train_loss=7.769061

Batch 41250, train_perplexity=2491.1362, train_loss=7.820494

Batch 41260, train_perplexity=2247.4985, train_loss=7.717573

Batch 41270, train_perplexity=2349.7651, train_loss=7.7620707

Batch 41280, train_perplexity=2315.9004, train_loss=7.747554

Batch 41290, train_perplexity=2265.465, train_loss=7.7255354

Batch 41300, train_perplexity=2428.4006, train_loss=7.794988

Batch 41310, train_perplexity=2463.6826, train_loss=7.8094125

Batch 41320, train_perplexity=2196.6584, train_loss=7.6946926

Batch 41330, train_perplexity=2319.699, train_loss=7.7491927

Batch 41340, train_perplexity=2144.6702, train_loss=7.670741

Batch 41350, train_perplexity=2131.7573, train_loss=7.664702

Batch 41360, train_perplexity=2385.7673, train_loss=7.777276

Batch 41370, train_perplexity=2153.251, train_loss=7.674734

Batch 41380, train_perplexity=2430.6238, train_loss=7.795903

Batch 41390, train_perplexity=2466.787, train_loss=7.810672

Batch 41400, train_perplexity=2282.8438, train_loss=7.733177

Batch 41410, train_perplexity=2135.3477, train_loss=7.6663847

Batch 41420, train_perplexity=2312.6274, train_loss=7.7461395

Batch 41430, train_perplexity=2223.012, train_loss=7.7066183

Batch 41440, train_perplexity=2553.272, train_loss=7.845131

Batch 41450, train_perplexity=2418.54, train_loss=7.7909193

Batch 41460, train_perplexity=2347.9797, train_loss=7.7613106

Batch 41470, train_perplexity=2313.1611, train_loss=7.7463703

Batch 41480, train_perplexity=2358.1746, train_loss=7.765643

Batch 41490, train_perplexity=2205.8975, train_loss=7.6988897

Batch 41500, train_perplexity=2309.284, train_loss=7.744693

Batch 41510, train_perplexity=2348.9182, train_loss=7.76171

Batch 41520, train_perplexity=2293.5845, train_loss=7.737871

Batch 41530, train_perplexity=2381.7002, train_loss=7.77557

Batch 41540, train_perplexity=2127.7976, train_loss=7.6628428

Batch 41550, train_perplexity=2164.7732, train_loss=7.680071

Batch 41560, train_perplexity=2139.818, train_loss=7.668476

Batch 41570, train_perplexity=2372.2734, train_loss=7.771604

Batch 41580, train_perplexity=2389.6997, train_loss=7.778923

Batch 41590, train_perplexity=2093.9773, train_loss=7.6468205

Batch 41600, train_perplexity=2220.7192, train_loss=7.7055864

Batch 41610, train_perplexity=2436.585, train_loss=7.7983527

Batch 41620, train_perplexity=2397.1753, train_loss=7.7820463

Batch 41630, train_perplexity=2285.7432, train_loss=7.7344465

Batch 41640, train_perplexity=2299.6504, train_loss=7.7405124

Batch 41650, train_perplexity=2209.2786, train_loss=7.7004213

Batch 41660, train_perplexity=2328.3706, train_loss=7.752924

Batch 41670, train_perplexity=2358.584, train_loss=7.7658167

Batch 41680, train_perplexity=2441.472, train_loss=7.8003564

Batch 41690, train_perplexity=2402.4253, train_loss=7.784234

Batch 41700, train_perplexity=2289.1453, train_loss=7.735934

Batch 41710, train_perplexity=2150.8928, train_loss=7.6736383

Batch 41720, train_perplexity=2273.1262, train_loss=7.7289114

Batch 41730, train_perplexity=2392.12, train_loss=7.7799354

Batch 41740, train_perplexity=2254.155, train_loss=7.7205305

Batch 41750, train_perplexity=2188.8992, train_loss=7.691154

Batch 41760, train_perplexity=2394.1504, train_loss=7.7807837

Batch 41770, train_perplexity=2228.4587, train_loss=7.7090654

Batch 41780, train_perplexity=2233.2795, train_loss=7.7112265

Batch 41790, train_perplexity=2346.9521, train_loss=7.760873

Batch 41800, train_perplexity=2243.9756, train_loss=7.7160044

Batch 41810, train_perplexity=2153.1094, train_loss=7.6746683

Batch 41820, train_perplexity=2357.2527, train_loss=7.765252

Batch 41830, train_perplexity=2240.3425, train_loss=7.714384

Batch 41840, train_perplexity=2354.6824, train_loss=7.764161

Batch 41850, train_perplexity=2267.5803, train_loss=7.7264686

Batch 41860, train_perplexity=2435.2004, train_loss=7.7977843

Batch 41870, train_perplexity=2264.3442, train_loss=7.7250404

Batch 41880, train_perplexity=2406.6538, train_loss=7.7859926

Batch 41890, train_perplexity=2229.5918, train_loss=7.7095737

Batch 41900, train_perplexity=2340.517, train_loss=7.758127

Batch 41910, train_perplexity=2304.3628, train_loss=7.7425594

Batch 41920, train_perplexity=2280.4077, train_loss=7.7321095

Batch 41930, train_perplexity=2239.5051, train_loss=7.7140102

Batch 41940, train_perplexity=2252.7969, train_loss=7.719928

Batch 41950, train_perplexity=2362.2544, train_loss=7.7673717

Batch 41960, train_perplexity=2397.573, train_loss=7.7822123

Batch 41970, train_perplexity=2325.2573, train_loss=7.751586

Batch 41980, train_perplexity=2249.1667, train_loss=7.718315

Batch 41990, train_perplexity=2192.753, train_loss=7.692913
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 42000, train_perplexity=2285.5645, train_loss=7.7343683

Batch 42010, train_perplexity=2338.788, train_loss=7.757388

Batch 42020, train_perplexity=2081.1025, train_loss=7.640653

Batch 42030, train_perplexity=2319.9944, train_loss=7.74932

Batch 42040, train_perplexity=2426.9966, train_loss=7.7944098

Batch 42050, train_perplexity=2139.0754, train_loss=7.668129

Batch 42060, train_perplexity=2341.991, train_loss=7.7587566

Batch 42070, train_perplexity=2490.3748, train_loss=7.8201885

Batch 42080, train_perplexity=2296.6335, train_loss=7.7391996

Batch 42090, train_perplexity=2328.7769, train_loss=7.7530985

Batch 42100, train_perplexity=2293.6807, train_loss=7.737913

Batch 42110, train_perplexity=2170.575, train_loss=7.6827474

Batch 42120, train_perplexity=2081.7864, train_loss=7.6409817

Batch 42130, train_perplexity=2391.1018, train_loss=7.7795095

Batch 42140, train_perplexity=2258.2368, train_loss=7.7223396

Batch 42150, train_perplexity=2213.3118, train_loss=7.702245

Batch 42160, train_perplexity=2162.419, train_loss=7.6789827

Batch 42170, train_perplexity=2366.6797, train_loss=7.7692432

Batch 42180, train_perplexity=2275.039, train_loss=7.7297525

Batch 42190, train_perplexity=2144.4104, train_loss=7.67062

Batch 42200, train_perplexity=2432.9243, train_loss=7.7968493

Batch 42210, train_perplexity=2190.5425, train_loss=7.6919045

Batch 42220, train_perplexity=2196.9287, train_loss=7.6948156

Batch 42230, train_perplexity=2253.5276, train_loss=7.720252

Batch 42240, train_perplexity=2289.6191, train_loss=7.7361407

Batch 42250, train_perplexity=2332.8936, train_loss=7.7548647

Batch 42260, train_perplexity=2377.7798, train_loss=7.7739224

Batch 42270, train_perplexity=2126.905, train_loss=7.662423

Batch 42280, train_perplexity=2360.0847, train_loss=7.766453

Batch 42290, train_perplexity=2267.0623, train_loss=7.72624

Batch 42300, train_perplexity=2465.0552, train_loss=7.8099694

Batch 42310, train_perplexity=2226.8208, train_loss=7.70833

Batch 42320, train_perplexity=2405.4998, train_loss=7.785513

Batch 42330, train_perplexity=2276.185, train_loss=7.730256

Batch 42340, train_perplexity=2282.1438, train_loss=7.7328706

Batch 42350, train_perplexity=2499.0706, train_loss=7.823674

Batch 42360, train_perplexity=2354.4072, train_loss=7.7640443

Batch 42370, train_perplexity=2247.7056, train_loss=7.717665

Batch 42380, train_perplexity=2096.6519, train_loss=7.648097

Batch 42390, train_perplexity=2247.1772, train_loss=7.71743

Batch 42400, train_perplexity=2617.6626, train_loss=7.870037

Batch 42410, train_perplexity=2121.6702, train_loss=7.659959

Batch 42420, train_perplexity=2172.3135, train_loss=7.683548

Batch 42430, train_perplexity=2117.6372, train_loss=7.6580563

Batch 42440, train_perplexity=2038.35, train_loss=7.619896

Batch 42450, train_perplexity=2282.258, train_loss=7.7329206

Batch 42460, train_perplexity=2269.9788, train_loss=7.7275257

Batch 42470, train_perplexity=2443.022, train_loss=7.800991

Batch 42480, train_perplexity=2489.7324, train_loss=7.8199306

Batch 42490, train_perplexity=2255.7056, train_loss=7.721218

Batch 42500, train_perplexity=2272.2073, train_loss=7.728507

Batch 42510, train_perplexity=2411.4531, train_loss=7.787985

Batch 42520, train_perplexity=2077.4292, train_loss=7.6388865

Batch 42530, train_perplexity=2393.421, train_loss=7.780479

Batch 42540, train_perplexity=2332.9214, train_loss=7.7548766

Batch 42550, train_perplexity=2336.3892, train_loss=7.756362

Batch 42560, train_perplexity=2153.97, train_loss=7.675068

Batch 42570, train_perplexity=2226.707, train_loss=7.708279

Batch 42580, train_perplexity=2195.2134, train_loss=7.6940346

Batch 42590, train_perplexity=2296.9116, train_loss=7.7393208

Batch 42600, train_perplexity=2136.0012, train_loss=7.666691

Batch 42610, train_perplexity=2383.0771, train_loss=7.776148

Batch 42620, train_perplexity=2177.584, train_loss=7.6859713

Batch 42630, train_perplexity=2312.3318, train_loss=7.7460117

Batch 42640, train_perplexity=2399.0117, train_loss=7.782812

Batch 42650, train_perplexity=2172.441, train_loss=7.6836066

Batch 42660, train_perplexity=2378.8967, train_loss=7.774392

Batch 42670, train_perplexity=2153.6023, train_loss=7.674897

Batch 42680, train_perplexity=2166.632, train_loss=7.680929

Batch 42690, train_perplexity=2211.0134, train_loss=7.701206

Batch 42700, train_perplexity=2082.4656, train_loss=7.641308

Batch 42710, train_perplexity=2078.537, train_loss=7.6394196

Batch 42720, train_perplexity=2380.3525, train_loss=7.775004

Batch 42730, train_perplexity=2235.7568, train_loss=7.712335

Batch 42740, train_perplexity=2260.6072, train_loss=7.7233887

Batch 42750, train_perplexity=2181.1194, train_loss=7.6875935

Batch 42760, train_perplexity=2161.1016, train_loss=7.6783733

Batch 42770, train_perplexity=2264.4846, train_loss=7.7251024

Batch 42780, train_perplexity=2251.6865, train_loss=7.7194347

Batch 42790, train_perplexity=2391.932, train_loss=7.7798567

Batch 42800, train_perplexity=2207.5938, train_loss=7.6996584

Batch 42810, train_perplexity=2269.5544, train_loss=7.727339

Batch 42820, train_perplexity=2409.8716, train_loss=7.7873287

Batch 42830, train_perplexity=2111.1062, train_loss=7.6549673

Batch 42840, train_perplexity=2159.6265, train_loss=7.6776905

Batch 42850, train_perplexity=2411.743, train_loss=7.788105

Batch 42860, train_perplexity=2217.1072, train_loss=7.7039585

Batch 42870, train_perplexity=2353.129, train_loss=7.763501

Batch 42880, train_perplexity=2228.1526, train_loss=7.708928

Batch 42890, train_perplexity=2294.9773, train_loss=7.738478

Batch 42900, train_perplexity=2353.397, train_loss=7.763615

Batch 42910, train_perplexity=2214.3792, train_loss=7.7027273

Batch 42920, train_perplexity=2231.902, train_loss=7.7106094

Batch 42930, train_perplexity=2202.5708, train_loss=7.6973805

Batch 42940, train_perplexity=2572.3896, train_loss=7.8525906

Batch 42950, train_perplexity=2444.4937, train_loss=7.8015933

Batch 42960, train_perplexity=2301.231, train_loss=7.7411995

Batch 42970, train_perplexity=2319.2068, train_loss=7.7489805

Batch 42980, train_perplexity=2290.484, train_loss=7.7365184

Batch 42990, train_perplexity=2052.183, train_loss=7.6266594

Batch 43000, train_perplexity=2366.235, train_loss=7.7690554

Batch 43010, train_perplexity=2157.5154, train_loss=7.6767125

Batch 43020, train_perplexity=2211.9856, train_loss=7.701646

Batch 43030, train_perplexity=2278.5957, train_loss=7.7313147

Batch 43040, train_perplexity=2193.0938, train_loss=7.6930685

Batch 43050, train_perplexity=2233.3904, train_loss=7.711276

Batch 43060, train_perplexity=2473.0127, train_loss=7.8131924

Batch 43070, train_perplexity=2359.522, train_loss=7.7662144

Batch 43080, train_perplexity=2149.3838, train_loss=7.6729364

Batch 43090, train_perplexity=2421.4824, train_loss=7.7921352

Batch 43100, train_perplexity=2188.9126, train_loss=7.69116

Batch 43110, train_perplexity=2477.1528, train_loss=7.814865

Batch 43120, train_perplexity=2116.759, train_loss=7.6576414

Batch 43130, train_perplexity=2213.5957, train_loss=7.7023735

Batch 43140, train_perplexity=2146.9285, train_loss=7.6717935

Batch 43150, train_perplexity=2174.6785, train_loss=7.684636

Batch 43160, train_perplexity=2318.351, train_loss=7.7486115

Batch 43170, train_perplexity=2316.0452, train_loss=7.7476163

Batch 43180, train_perplexity=2255.312, train_loss=7.7210436

Batch 43190, train_perplexity=1989.6824, train_loss=7.5957303

Batch 43200, train_perplexity=2131.9546, train_loss=7.6647944

Batch 43210, train_perplexity=2170.135, train_loss=7.6825447

Batch 43220, train_perplexity=2345.3123, train_loss=7.760174

Batch 43230, train_perplexity=2358.3567, train_loss=7.7657204

Batch 43240, train_perplexity=2436.377, train_loss=7.7982674

Batch 43250, train_perplexity=2229.8438, train_loss=7.7096868

Batch 43260, train_perplexity=2426.05, train_loss=7.7940197

Batch 43270, train_perplexity=2402.5627, train_loss=7.7842913

Batch 43280, train_perplexity=2319.0254, train_loss=7.7489023

Batch 43290, train_perplexity=2248.281, train_loss=7.7179213

Batch 43300, train_perplexity=2324.9724, train_loss=7.7514634

Batch 43310, train_perplexity=2419.6057, train_loss=7.79136
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 43320, train_perplexity=2099.1267, train_loss=7.6492767

Batch 43330, train_perplexity=2369.41, train_loss=7.770396

Batch 43340, train_perplexity=2297.1626, train_loss=7.73943

Batch 43350, train_perplexity=2333.884, train_loss=7.755289

Batch 43360, train_perplexity=2326.657, train_loss=7.7521877

Batch 43370, train_perplexity=2187.673, train_loss=7.6905937

Batch 43380, train_perplexity=2233.9219, train_loss=7.711514

Batch 43390, train_perplexity=2204.4485, train_loss=7.6982327

Batch 43400, train_perplexity=2322.5212, train_loss=7.7504086

Batch 43410, train_perplexity=2249.2688, train_loss=7.7183604

Batch 43420, train_perplexity=2338.5537, train_loss=7.757288

Batch 43430, train_perplexity=2343.3, train_loss=7.7593155

Batch 43440, train_perplexity=2142.8015, train_loss=7.6698694

Batch 43450, train_perplexity=2298.2188, train_loss=7.7398896

Batch 43460, train_perplexity=2313.192, train_loss=7.7463837

Batch 43470, train_perplexity=2541.0156, train_loss=7.840319

Batch 43480, train_perplexity=2273.5015, train_loss=7.7290764

Batch 43490, train_perplexity=2219.5432, train_loss=7.7050567

Batch 43500, train_perplexity=2366.472, train_loss=7.7691555

Batch 43510, train_perplexity=2650.0159, train_loss=7.882321

Batch 43520, train_perplexity=2403.092, train_loss=7.7845116

Batch 43530, train_perplexity=2392.6072, train_loss=7.780139

Batch 43540, train_perplexity=2304.5242, train_loss=7.7426295

Batch 43550, train_perplexity=2288.2832, train_loss=7.735557

Batch 43560, train_perplexity=2332.0374, train_loss=7.7544975

Batch 43570, train_perplexity=2117.2517, train_loss=7.657874

Batch 43580, train_perplexity=2096.3691, train_loss=7.647962

Batch 43590, train_perplexity=2505.4946, train_loss=7.8262415

Batch 43600, train_perplexity=2006.6469, train_loss=7.6042204

Batch 43610, train_perplexity=2214.3347, train_loss=7.7027073

Batch 43620, train_perplexity=2250.277, train_loss=7.7188087

Batch 43630, train_perplexity=2049.7732, train_loss=7.6254845

Batch 43640, train_perplexity=2459.6504, train_loss=7.8077745

Batch 43650, train_perplexity=2431.0781, train_loss=7.79609

Batch 43660, train_perplexity=2247.0828, train_loss=7.717388

Batch 43670, train_perplexity=2213.0027, train_loss=7.7021055

Batch 43680, train_perplexity=2305.2727, train_loss=7.7429543

Batch 43690, train_perplexity=2219.123, train_loss=7.7048674

Batch 43700, train_perplexity=2056.0676, train_loss=7.6285505

Batch 43710, train_perplexity=2163.993, train_loss=7.6797104

Batch 43720, train_perplexity=2370.3591, train_loss=7.770797

Batch 43730, train_perplexity=2389.4946, train_loss=7.778837

Batch 43740, train_perplexity=2291.0508, train_loss=7.736766

Batch 43750, train_perplexity=2355.9604, train_loss=7.7647038

Batch 43760, train_perplexity=2124.9585, train_loss=7.6615076

Batch 43770, train_perplexity=2434.9102, train_loss=7.797665

Batch 43780, train_perplexity=2320.812, train_loss=7.7496724

Batch 43790, train_perplexity=2380.3037, train_loss=7.7749834

Batch 43800, train_perplexity=2224.592, train_loss=7.707329

Batch 43810, train_perplexity=2283.891, train_loss=7.733636

Batch 43820, train_perplexity=2197.769, train_loss=7.695198

Batch 43830, train_perplexity=2424.395, train_loss=7.7933373

Batch 43840, train_perplexity=2272.708, train_loss=7.7287273

Batch 43850, train_perplexity=2271.1199, train_loss=7.7280283

Batch 43860, train_perplexity=2369.5479, train_loss=7.7704544

Batch 43870, train_perplexity=2061.7874, train_loss=7.6313286

Batch 43880, train_perplexity=2468.0554, train_loss=7.811186

Batch 43890, train_perplexity=2449.9678, train_loss=7.80383

Batch 43900, train_perplexity=2607.5457, train_loss=7.8661647

Batch 43910, train_perplexity=2409.52, train_loss=7.787183

Batch 43920, train_perplexity=2359.6594, train_loss=7.7662725

Batch 43930, train_perplexity=2101.5867, train_loss=7.650448

Batch 43940, train_perplexity=2256.7944, train_loss=7.7217007

Batch 43950, train_perplexity=2298.187, train_loss=7.739876

Batch 43960, train_perplexity=2448.1682, train_loss=7.8030953

Batch 43970, train_perplexity=2309.0242, train_loss=7.7445803

Batch 43980, train_perplexity=2554.6833, train_loss=7.8456836

Batch 43990, train_perplexity=2210.187, train_loss=7.7008324

Batch 44000, train_perplexity=2259.4626, train_loss=7.7228823

Batch 44010, train_perplexity=2158.4033, train_loss=7.677124

Batch 44020, train_perplexity=2129.8848, train_loss=7.663823

Batch 44030, train_perplexity=2332.0928, train_loss=7.7545214

Batch 44040, train_perplexity=2409.7383, train_loss=7.7872734

Batch 44050, train_perplexity=2066.6125, train_loss=7.633666

Batch 44060, train_perplexity=2274.5488, train_loss=7.729537

Batch 44070, train_perplexity=2077.538, train_loss=7.638939

Batch 44080, train_perplexity=2435.9844, train_loss=7.798106

Batch 44090, train_perplexity=2248.5212, train_loss=7.718028

Batch 44100, train_perplexity=2145.2961, train_loss=7.671033

Batch 44110, train_perplexity=2380.5466, train_loss=7.7750854

Batch 44120, train_perplexity=2393.8262, train_loss=7.780648

Batch 44130, train_perplexity=2402.596, train_loss=7.784305

Batch 44140, train_perplexity=2125.056, train_loss=7.6615534

Batch 44150, train_perplexity=2302.961, train_loss=7.741951

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 44160, train_perplexity=2266.5413, train_loss=7.7260103

Batch 44170, train_perplexity=2234.0251, train_loss=7.7115602

Batch 44180, train_perplexity=2429.5369, train_loss=7.795456

Batch 44190, train_perplexity=2369.0935, train_loss=7.7702627

Batch 44200, train_perplexity=2148.812, train_loss=7.6726704

Batch 44210, train_perplexity=2249.2761, train_loss=7.718364

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 44220, train_perplexity=2162.559, train_loss=7.6790476

Batch 44230, train_perplexity=2105.2725, train_loss=7.6522

Batch 44240, train_perplexity=2182.807, train_loss=7.688367

Batch 44250, train_perplexity=2228.705, train_loss=7.709176

Batch 44260, train_perplexity=2382.184, train_loss=7.775773

Batch 44270, train_perplexity=2041.268, train_loss=7.6213264

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 44280, train_perplexity=2328.1963, train_loss=7.752849

Batch 44290, train_perplexity=2114.8884, train_loss=7.6567574

Batch 44300, train_perplexity=2124.4824, train_loss=7.6612835

Batch 44310, train_perplexity=2373.08, train_loss=7.771944

Batch 44320, train_perplexity=2302.5283, train_loss=7.741763

Batch 44330, train_perplexity=2372.1956, train_loss=7.771571

Batch 44340, train_perplexity=2310.0066, train_loss=7.7450056

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 44350, train_perplexity=2276.8884, train_loss=7.730565

Batch 44360, train_perplexity=2205.2412, train_loss=7.698592

Batch 44370, train_perplexity=2308.3923, train_loss=7.7443066
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 44380, train_perplexity=2189.0317, train_loss=7.6912146

Batch 44390, train_perplexity=2394.8513, train_loss=7.7810764

Batch 44400, train_perplexity=2056.6108, train_loss=7.6288147

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 44410, train_perplexity=2231.287, train_loss=7.710334

Batch 44420, train_perplexity=2160.1042, train_loss=7.6779118

Batch 44430, train_perplexity=2163.0933, train_loss=7.6792946

Batch 44440, train_perplexity=2232.4077, train_loss=7.710836

Batch 44450, train_perplexity=2286.8662, train_loss=7.7349377

Batch 44460, train_perplexity=2222.0115, train_loss=7.706168

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 44470, train_perplexity=2278.1938, train_loss=7.731138

Batch 44480, train_perplexity=2396.9226, train_loss=7.781941

Batch 44490, train_perplexity=2134.407, train_loss=7.665944

Batch 44500, train_perplexity=2299.588, train_loss=7.740485

Batch 44510, train_perplexity=2096.65, train_loss=7.648096

Batch 44520, train_perplexity=2239.6826, train_loss=7.7140894

Batch 44530, train_perplexity=2357.7024, train_loss=7.765443

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 44540, train_perplexity=2116.638, train_loss=7.657584

Batch 44550, train_perplexity=2028.6322, train_loss=7.615117

Batch 44560, train_perplexity=2294.5361, train_loss=7.738286

Batch 44570, train_perplexity=2214.6843, train_loss=7.702865

Batch 44580, train_perplexity=2300.4412, train_loss=7.740856

Batch 44590, train_perplexity=2106.3198, train_loss=7.6526976

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 44600, train_perplexity=2492.0784, train_loss=7.8208723

Batch 44610, train_perplexity=2103.658, train_loss=7.651433

Batch 44620, train_perplexity=2214.019, train_loss=7.7025647

Batch 44630, train_perplexity=2231.7722, train_loss=7.7105513

Batch 44640, train_perplexity=2585.5034, train_loss=7.8576756

Batch 44650, train_perplexity=2361.3027, train_loss=7.7669687

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 44660, train_perplexity=2206.5276, train_loss=7.6991754

Batch 44670, train_perplexity=2446.7058, train_loss=7.802498

Batch 44680, train_perplexity=2126.0745, train_loss=7.6620326

Batch 44690, train_perplexity=2289.035, train_loss=7.7358856

Batch 44700, train_perplexity=2181.6436, train_loss=7.687834

Batch 44710, train_perplexity=2291.5623, train_loss=7.736989

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 44720, train_perplexity=2219.9666, train_loss=7.7052474

Batch 44730, train_perplexity=2199.1487, train_loss=7.6958256

Batch 44740, train_perplexity=2163.669, train_loss=7.6795607

Batch 44750, train_perplexity=2162.0767, train_loss=7.6788244

Batch 44760, train_perplexity=2359.1936, train_loss=7.766075

Batch 44770, train_perplexity=2264.0613, train_loss=7.7249155

Batch 44780, train_perplexity=2442.471, train_loss=7.8007655

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 44790, train_perplexity=2255.1807, train_loss=7.7209854

Batch 44800, train_perplexity=2240.5178, train_loss=7.7144623

Batch 44810, train_perplexity=2138.0159, train_loss=7.6676335

Batch 44820, train_perplexity=2362.688, train_loss=7.767555

Batch 44830, train_perplexity=2197.3416, train_loss=7.6950035

Batch 44840, train_perplexity=2301.2705, train_loss=7.7412167

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 44850, train_perplexity=2286.7385, train_loss=7.734882

Batch 44860, train_perplexity=2487.747, train_loss=7.819133

Batch 44870, train_perplexity=2085.5103, train_loss=7.642769

Batch 44880, train_perplexity=2168.4097, train_loss=7.6817493

Batch 44890, train_perplexity=2048.2373, train_loss=7.624735

Batch 44900, train_perplexity=2165.1365, train_loss=7.6802387

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 44910, train_perplexity=2179.9507, train_loss=7.6870575

Batch 44920, train_perplexity=2081.7756, train_loss=7.6409764

Batch 44930, train_perplexity=2112.9915, train_loss=7.65586

Batch 44940, train_perplexity=2220.662, train_loss=7.7055607

Batch 44950, train_perplexity=2234.8188, train_loss=7.7119155

Batch 44960, train_perplexity=2200.8103, train_loss=7.696581

Batch 44970, train_perplexity=2396.044, train_loss=7.7815742

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 44980, train_perplexity=2305.586, train_loss=7.74309

Batch 44990, train_perplexity=2176.108, train_loss=7.685293

Batch 45000, train_perplexity=2137.1506, train_loss=7.6672287

Batch 45010, train_perplexity=2074.323, train_loss=7.63739

Batch 45020, train_perplexity=2307.3633, train_loss=7.7438607

Batch 45030, train_perplexity=2281.517, train_loss=7.732596

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 45040, train_perplexity=2129.0562, train_loss=7.663434

Batch 45050, train_perplexity=2298.4126, train_loss=7.739974

Batch 45060, train_perplexity=2288.7937, train_loss=7.7357802

Batch 45070, train_perplexity=2277.7463, train_loss=7.730942

Batch 45080, train_perplexity=2376.4434, train_loss=7.7733603

Batch 45090, train_perplexity=2252.7925, train_loss=7.719926

Batch 45100, train_perplexity=2176.3071, train_loss=7.6853848

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 45110, train_perplexity=2259.83, train_loss=7.723045

Batch 45120, train_perplexity=2101.6958, train_loss=7.6505

Batch 45130, train_perplexity=2188.5725, train_loss=7.6910048

Batch 45140, train_perplexity=2225.4587, train_loss=7.7077184

Batch 45150, train_perplexity=2159.3093, train_loss=7.6775436

Batch 45160, train_perplexity=2122.35, train_loss=7.6602793

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 45170, train_perplexity=2190.0361, train_loss=7.6916733

Batch 45180, train_perplexity=2284.634, train_loss=7.733961

Batch 45190, train_perplexity=2309.7466, train_loss=7.744893

Batch 45200, train_perplexity=2465.8027, train_loss=7.8102727

Batch 45210, train_perplexity=2233.1475, train_loss=7.7111673

Batch 45220, train_perplexity=2238.4014, train_loss=7.713517

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 45230, train_perplexity=2138.9429, train_loss=7.668067

Batch 45240, train_perplexity=2222.605, train_loss=7.706435

Batch 45250, train_perplexity=2103.6118, train_loss=7.651411

Batch 45260, train_perplexity=2384.3843, train_loss=7.776696

Batch 45270, train_perplexity=2353.9526, train_loss=7.763851

Batch 45280, train_perplexity=2609.5083, train_loss=7.866917

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 45290, train_perplexity=2271.866, train_loss=7.728357

Batch 45300, train_perplexity=2127.8777, train_loss=7.6628804

Batch 45310, train_perplexity=2303.1565, train_loss=7.742036

Batch 45320, train_perplexity=2217.8145, train_loss=7.7042775

Batch 45330, train_perplexity=2101.0635, train_loss=7.650199

Batch 45340, train_perplexity=2130.357, train_loss=7.664045

Batch 45350, train_perplexity=2352.0498, train_loss=7.7630424

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 45360, train_perplexity=2355.8503, train_loss=7.764657

Batch 45370, train_perplexity=2155.617, train_loss=7.6758323

Batch 45380, train_perplexity=2234.1445, train_loss=7.7116137

Batch 45390, train_perplexity=2207.2642, train_loss=7.699509

Batch 45400, train_perplexity=2158.29, train_loss=7.6770716

Batch 45410, train_perplexity=2110.9915, train_loss=7.654913

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 45420, train_perplexity=2287.0122, train_loss=7.7350016

Batch 45430, train_perplexity=2175.1536, train_loss=7.6848545

Batch 45440, train_perplexity=2084.4963, train_loss=7.6422825

Batch 45450, train_perplexity=2161.6406, train_loss=7.6786227

Batch 45460, train_perplexity=2309.4734, train_loss=7.744775

Batch 45470, train_perplexity=2299.4355, train_loss=7.740419

Batch 45480, train_perplexity=2224.7478, train_loss=7.707399

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 45490, train_perplexity=2315.196, train_loss=7.7472496

Batch 45500, train_perplexity=2152.714, train_loss=7.6744847

Batch 45510, train_perplexity=2242.0835, train_loss=7.715161

Batch 45520, train_perplexity=2115.2495, train_loss=7.656928

Batch 45530, train_perplexity=2307.6802, train_loss=7.743998

Batch 45540, train_perplexity=2122.2085, train_loss=7.6602125

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 45550, train_perplexity=2352.3806, train_loss=7.763183

Batch 45560, train_perplexity=2157.008, train_loss=7.6764774

Batch 45570, train_perplexity=2195.9673, train_loss=7.694378

Batch 45580, train_perplexity=2276.8147, train_loss=7.7305326

Batch 45590, train_perplexity=2339.2832, train_loss=7.7576

Batch 45600, train_perplexity=2321.3135, train_loss=7.7498884

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 45610, train_perplexity=2331.7905, train_loss=7.7543917

Batch 45620, train_perplexity=2328.0308, train_loss=7.752778

Batch 45630, train_perplexity=2202.1633, train_loss=7.6971955

Batch 45640, train_perplexity=2163.115, train_loss=7.6793046

Batch 45650, train_perplexity=2179.2876, train_loss=7.6867533

Batch 45660, train_perplexity=2301.939, train_loss=7.741507

Batch 45670, train_perplexity=2322.0408, train_loss=7.7502017

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6105 sentences.
Finished loading
Batch 45680, train_perplexity=2139.051, train_loss=7.6681175

Batch 45690, train_perplexity=2290.1377, train_loss=7.736367

Batch 45700, train_perplexity=2237.8975, train_loss=7.713292

Batch 45710, train_perplexity=2324.9136, train_loss=7.751438

Batch 45720, train_perplexity=2275.063, train_loss=7.729763

Batch 45730, train_perplexity=2432.5137, train_loss=7.7966805

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 45740, train_perplexity=2105.2273, train_loss=7.652179

Batch 45750, train_perplexity=2309.8545, train_loss=7.74494

Batch 45760, train_perplexity=2321.8582, train_loss=7.750123

Batch 45770, train_perplexity=2024.5571, train_loss=7.6131063

Batch 45780, train_perplexity=2225.286, train_loss=7.7076406

Batch 45790, train_perplexity=2242.5378, train_loss=7.7153635

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 45800, train_perplexity=2106.82, train_loss=7.652935

Batch 45810, train_perplexity=2481.3542, train_loss=7.81656

Batch 45820, train_perplexity=2356.0144, train_loss=7.7647266

Batch 45830, train_perplexity=2313.7678, train_loss=7.7466326

Batch 45840, train_perplexity=2263.8647, train_loss=7.7248287

Batch 45850, train_perplexity=2359.0, train_loss=7.765993

Batch 45860, train_perplexity=2029.385, train_loss=7.615488

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 45870, train_perplexity=2106.6191, train_loss=7.6528397

Batch 45880, train_perplexity=2039.1608, train_loss=7.6202936

Batch 45890, train_perplexity=2166.6145, train_loss=7.680921

Batch 45900, train_perplexity=2242.6353, train_loss=7.715407

Batch 45910, train_perplexity=2251.1465, train_loss=7.719195

Batch 45920, train_perplexity=2296.0017, train_loss=7.7389245

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 45930, train_perplexity=2085.1274, train_loss=7.6425853

Batch 45940, train_perplexity=2225.0056, train_loss=7.707515

Batch 45950, train_perplexity=2322.095, train_loss=7.750225

Batch 45960, train_perplexity=2268.9453, train_loss=7.7270703

Batch 45970, train_perplexity=2157.8992, train_loss=7.6768904

Batch 45980, train_perplexity=2600.7048, train_loss=7.863538

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 45990, train_perplexity=2309.2312, train_loss=7.74467

Batch 46000, train_perplexity=2370.9333, train_loss=7.771039

Batch 46010, train_perplexity=2259.8708, train_loss=7.723063

Batch 46020, train_perplexity=2023.6257, train_loss=7.612646

Batch 46030, train_perplexity=2252.0452, train_loss=7.719594

Batch 46040, train_perplexity=2086.0483, train_loss=7.643027

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 46050, train_perplexity=2160.0857, train_loss=7.677903

Batch 46060, train_perplexity=2133.2134, train_loss=7.665385

Batch 46070, train_perplexity=2015.0992, train_loss=7.6084237

Batch 46080, train_perplexity=2396.0278, train_loss=7.7815676

Batch 46090, train_perplexity=2219.9993, train_loss=7.705262

Batch 46100, train_perplexity=2060.4458, train_loss=7.6306777

Batch 46110, train_perplexity=2326.5415, train_loss=7.752138

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 46120, train_perplexity=2216.401, train_loss=7.70364

Batch 46130, train_perplexity=2143.6785, train_loss=7.6702785

Batch 46140, train_perplexity=2167.4834, train_loss=7.681322

Batch 46150, train_perplexity=2194.9602, train_loss=7.693919

Batch 46160, train_perplexity=2245.309, train_loss=7.7165985

Batch 46170, train_perplexity=2243.079, train_loss=7.715605

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 46180, train_perplexity=2370.3953, train_loss=7.770812

Batch 46190, train_perplexity=2324.6719, train_loss=7.751334

Batch 46200, train_perplexity=2100.9514, train_loss=7.6501455

Batch 46210, train_perplexity=2114.4429, train_loss=7.6565466

Batch 46220, train_perplexity=2509.2058, train_loss=7.8277216

Batch 46230, train_perplexity=2140.7407, train_loss=7.668907

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 46240, train_perplexity=2093.7585, train_loss=7.646716

Batch 46250, train_perplexity=2297.685, train_loss=7.7396574

Batch 46260, train_perplexity=2354.2827, train_loss=7.7639914

Batch 46270, train_perplexity=2204.829, train_loss=7.6984053

Batch 46280, train_perplexity=2163.8877, train_loss=7.6796618

Batch 46290, train_perplexity=2316.183, train_loss=7.747676

Batch 46300, train_perplexity=2317.8315, train_loss=7.7483873

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 46310, train_perplexity=2151.292, train_loss=7.673824

Batch 46320, train_perplexity=2193.3374, train_loss=7.6931796

Batch 46330, train_perplexity=2133.9766, train_loss=7.6657424

Batch 46340, train_perplexity=2181.2473, train_loss=7.687652

Batch 46350, train_perplexity=2220.2598, train_loss=7.7053795

Batch 46360, train_perplexity=2142.4707, train_loss=7.669715

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 46370, train_perplexity=2208.6382, train_loss=7.7001314

Batch 46380, train_perplexity=2224.661, train_loss=7.70736

Batch 46390, train_perplexity=2184.925, train_loss=7.689337

Batch 46400, train_perplexity=2109.1711, train_loss=7.6540504

Batch 46410, train_perplexity=2205.747, train_loss=7.6988215

Batch 46420, train_perplexity=2268.4336, train_loss=7.726845

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 46430, train_perplexity=2336.2322, train_loss=7.7562947

Batch 46440, train_perplexity=2040.3094, train_loss=7.620857

Batch 46450, train_perplexity=2437.4587, train_loss=7.7987113

Batch 46460, train_perplexity=2209.486, train_loss=7.7005153

Batch 46470, train_perplexity=2199.5996, train_loss=7.6960306

Batch 46480, train_perplexity=2218.3179, train_loss=7.7045045

Batch 46490, train_perplexity=2138.2065, train_loss=7.6677227

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 46500, train_perplexity=2315.8296, train_loss=7.7475233

Batch 46510, train_perplexity=2329.6921, train_loss=7.7534914

Batch 46520, train_perplexity=2425.342, train_loss=7.793728

Batch 46530, train_perplexity=2205.9836, train_loss=7.698929

Batch 46540, train_perplexity=2199.0144, train_loss=7.6957645

Batch 46550, train_perplexity=2144.073, train_loss=7.6704626

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 46560, train_perplexity=2037.6882, train_loss=7.619571

Batch 46570, train_perplexity=2069.782, train_loss=7.6351986

Batch 46580, train_perplexity=2070.9133, train_loss=7.635745

Batch 46590, train_perplexity=2391.5396, train_loss=7.7796926

Batch 46600, train_perplexity=2190.0967, train_loss=7.691701

Batch 46610, train_perplexity=2310.8438, train_loss=7.745368

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 46620, train_perplexity=2122.513, train_loss=7.660356

Batch 46630, train_perplexity=2200.1326, train_loss=7.696273

Batch 46640, train_perplexity=2395.5183, train_loss=7.781355

Batch 46650, train_perplexity=2104.3604, train_loss=7.651767

Batch 46660, train_perplexity=2171.1877, train_loss=7.6830297

Batch 46670, train_perplexity=2173.1152, train_loss=7.683917

Batch 46680, train_perplexity=2449.7202, train_loss=7.803729

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 46690, train_perplexity=2039.7288, train_loss=7.620572

Batch 46700, train_perplexity=2269.6052, train_loss=7.727361

Batch 46710, train_perplexity=2071.358, train_loss=7.6359596

Batch 46720, train_perplexity=2143.4004, train_loss=7.670149

Batch 46730, train_perplexity=2213.7856, train_loss=7.7024593

Batch 46740, train_perplexity=2468.9265, train_loss=7.8115387

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 46750, train_perplexity=2374.896, train_loss=7.772709

Batch 46760, train_perplexity=2012.4067, train_loss=7.6070867

Batch 46770, train_perplexity=2071.1978, train_loss=7.6358824

Batch 46780, train_perplexity=2214.812, train_loss=7.702923

Batch 46790, train_perplexity=2364.5015, train_loss=7.7683225

Batch 46800, train_perplexity=2409.235, train_loss=7.7870646

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 46810, train_perplexity=2161.0664, train_loss=7.678357

Batch 46820, train_perplexity=2026.1439, train_loss=7.6138897

Batch 46830, train_perplexity=2285.1177, train_loss=7.734173

Batch 46840, train_perplexity=2042.113, train_loss=7.6217403

Batch 46850, train_perplexity=2197.1948, train_loss=7.6949368

Batch 46860, train_perplexity=2167.9053, train_loss=7.6815166

Batch 46870, train_perplexity=1983.0938, train_loss=7.5924134

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 46880, train_perplexity=2158.566, train_loss=7.6771994

Batch 46890, train_perplexity=2243.9563, train_loss=7.715996

Batch 46900, train_perplexity=2230.0903, train_loss=7.7097974

Batch 46910, train_perplexity=2105.0466, train_loss=7.652093

Batch 46920, train_perplexity=2284.401, train_loss=7.733859

Batch 46930, train_perplexity=2257.4917, train_loss=7.7220097

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 46940, train_perplexity=2393.26, train_loss=7.7804117

Batch 46950, train_perplexity=2195.4458, train_loss=7.6941404

Batch 46960, train_perplexity=2124.2961, train_loss=7.6611958

Batch 46970, train_perplexity=2175.4128, train_loss=7.6849737

Batch 46980, train_perplexity=2122.3987, train_loss=7.660302

Batch 46990, train_perplexity=2270.2515, train_loss=7.727646

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 47000, train_perplexity=2178.8252, train_loss=7.686541

Batch 47010, train_perplexity=2299.2808, train_loss=7.7403517

Batch 47020, train_perplexity=2047.253, train_loss=7.624254

Batch 47030, train_perplexity=2296.6106, train_loss=7.7391896

Batch 47040, train_perplexity=2202.4868, train_loss=7.6973424

Batch 47050, train_perplexity=2230.172, train_loss=7.709834

Batch 47060, train_perplexity=2087.9211, train_loss=7.643924

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 47070, train_perplexity=2190.4998, train_loss=7.691885

Batch 47080, train_perplexity=2423.2383, train_loss=7.79286

Batch 47090, train_perplexity=2066.2578, train_loss=7.6334944

Batch 47100, train_perplexity=2162.8645, train_loss=7.6791887

Batch 47110, train_perplexity=2319.1548, train_loss=7.748958

Batch 47120, train_perplexity=2188.0173, train_loss=7.690751

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 47130, train_perplexity=2101.4683, train_loss=7.6503916

Batch 47140, train_perplexity=2115.3303, train_loss=7.656966

Batch 47150, train_perplexity=2135.0156, train_loss=7.6662292

Batch 47160, train_perplexity=2237.4058, train_loss=7.7130723

Batch 47170, train_perplexity=2097.719, train_loss=7.648606

Batch 47180, train_perplexity=2180.2292, train_loss=7.6871853

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 47190, train_perplexity=2192.5857, train_loss=7.6928368

Batch 47200, train_perplexity=2198.44, train_loss=7.695503

Batch 47210, train_perplexity=2400.5312, train_loss=7.7834454

Batch 47220, train_perplexity=2117.1104, train_loss=7.6578074

Batch 47230, train_perplexity=2212.6787, train_loss=7.701959

Batch 47240, train_perplexity=2277.01, train_loss=7.7306185

Batch 47250, train_perplexity=2157.1953, train_loss=7.676564

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 47260, train_perplexity=2079.3599, train_loss=7.6398153

Batch 47270, train_perplexity=2178.793, train_loss=7.6865263

Batch 47280, train_perplexity=2180.6846, train_loss=7.687394

Batch 47290, train_perplexity=2140.407, train_loss=7.6687512

Batch 47300, train_perplexity=2261.379, train_loss=7.72373

Batch 47310, train_perplexity=2177.965, train_loss=7.6861463

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 47320, train_perplexity=2022.2733, train_loss=7.6119776

Batch 47330, train_perplexity=2264.7825, train_loss=7.725234

Batch 47340, train_perplexity=2173.846, train_loss=7.684253

Batch 47350, train_perplexity=2047.0686, train_loss=7.624164

Batch 47360, train_perplexity=2125.0051, train_loss=7.6615295

Batch 47370, train_perplexity=2044.3617, train_loss=7.622841

Batch 47380, train_perplexity=2360.776, train_loss=7.7667456

Batch 47390, train_perplexity=2131.6953, train_loss=7.664673

Batch 47400, train_perplexity=2349.7158, train_loss=7.7620497

Batch 47410, train_perplexity=2197.487, train_loss=7.69507

Batch 47420, train_perplexity=2172.7725, train_loss=7.683759

Batch 47430, train_perplexity=2158.4888, train_loss=7.6771636

Batch 47440, train_perplexity=2060.6956, train_loss=7.630799

Batch 47450, train_perplexity=2177.504, train_loss=7.6859345

Batch 47460, train_perplexity=2382.902, train_loss=7.7760744

Batch 47470, train_perplexity=2319.2532, train_loss=7.7490005

Batch 47480, train_perplexity=2022.1759, train_loss=7.6119294

Batch 47490, train_perplexity=2146.1147, train_loss=7.6714144

Batch 47500, train_perplexity=2189.8816, train_loss=7.6916027

Batch 47510, train_perplexity=2197.264, train_loss=7.694968

Batch 47520, train_perplexity=2320.7412, train_loss=7.749642

Batch 47530, train_perplexity=2324.7007, train_loss=7.7513466

Batch 47540, train_perplexity=2197.0974, train_loss=7.6948924

Batch 47550, train_perplexity=2135.4402, train_loss=7.666428

Batch 47560, train_perplexity=2529.402, train_loss=7.835738

Batch 47570, train_perplexity=2138.438, train_loss=7.667831

Batch 47580, train_perplexity=2270.0955, train_loss=7.727577

Batch 47590, train_perplexity=2207.0737, train_loss=7.699423

Batch 47600, train_perplexity=2199.3037, train_loss=7.695896

Batch 47610, train_perplexity=2269.6582, train_loss=7.7273846

Batch 47620, train_perplexity=2359.088, train_loss=7.7660303

Batch 47630, train_perplexity=2237.6287, train_loss=7.713172

Batch 47640, train_perplexity=2230.336, train_loss=7.7099075

Batch 47650, train_perplexity=2167.2043, train_loss=7.6811934

Batch 47660, train_perplexity=2123.344, train_loss=7.6607475

Batch 47670, train_perplexity=2248.0378, train_loss=7.717813

Batch 47680, train_perplexity=2455.5771, train_loss=7.806117

Batch 47690, train_perplexity=2256.903, train_loss=7.721749

Batch 47700, train_perplexity=2184.752, train_loss=7.6892576

Batch 47710, train_perplexity=2197.9075, train_loss=7.695261

Batch 47720, train_perplexity=2456.1228, train_loss=7.8063393

Batch 47730, train_perplexity=2151.8962, train_loss=7.6741047

Batch 47740, train_perplexity=2192.2844, train_loss=7.6926994

Batch 47750, train_perplexity=2057.727, train_loss=7.6293573

Batch 47760, train_perplexity=2159.5964, train_loss=7.6776767

Batch 47770, train_perplexity=2087.783, train_loss=7.643858

Batch 47780, train_perplexity=2340.727, train_loss=7.758217

Batch 47790, train_perplexity=2118.9434, train_loss=7.658673

Batch 47800, train_perplexity=2165.153, train_loss=7.6802464

Batch 47810, train_perplexity=2149.0383, train_loss=7.6727757

Batch 47820, train_perplexity=2242.1755, train_loss=7.715202

Batch 47830, train_perplexity=2275.5469, train_loss=7.7299757

Batch 47840, train_perplexity=2273.9155, train_loss=7.7292585

Batch 47850, train_perplexity=2344.1294, train_loss=7.7596693

Batch 47860, train_perplexity=2169.9001, train_loss=7.6824365

Batch 47870, train_perplexity=2338.915, train_loss=7.7574425

Batch 47880, train_perplexity=2142.0938, train_loss=7.669539

Batch 47890, train_perplexity=2146.6306, train_loss=7.6716547

Batch 47900, train_perplexity=1983.2537, train_loss=7.592494

Batch 47910, train_perplexity=1932.477, train_loss=7.566558

Batch 47920, train_perplexity=2041.5599, train_loss=7.6214695

Batch 47930, train_perplexity=2100.4185, train_loss=7.649892

Batch 47940, train_perplexity=2177.266, train_loss=7.6858253

Batch 47950, train_perplexity=2318.5059, train_loss=7.748678
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 47960, train_perplexity=2240.6567, train_loss=7.7145243

Batch 47970, train_perplexity=2317.805, train_loss=7.748376

Batch 47980, train_perplexity=2180.7854, train_loss=7.6874404

Batch 47990, train_perplexity=2200.9888, train_loss=7.696662

Batch 48000, train_perplexity=2153.3477, train_loss=7.674779

Batch 48010, train_perplexity=2288.5854, train_loss=7.735689

Batch 48020, train_perplexity=2230.9253, train_loss=7.7101717

Batch 48030, train_perplexity=2100.9243, train_loss=7.6501327

Batch 48040, train_perplexity=2189.3677, train_loss=7.691368

Batch 48050, train_perplexity=2160.0496, train_loss=7.6778865

Batch 48060, train_perplexity=2243.094, train_loss=7.7156115

Batch 48070, train_perplexity=2213.677, train_loss=7.70241

Batch 48080, train_perplexity=2195.2449, train_loss=7.694049

Batch 48090, train_perplexity=2285.7913, train_loss=7.7344675

Batch 48100, train_perplexity=2265.8887, train_loss=7.7257223

Batch 48110, train_perplexity=2145.4536, train_loss=7.6711063

Batch 48120, train_perplexity=2139.2263, train_loss=7.6681995

Batch 48130, train_perplexity=2343.908, train_loss=7.759575

Batch 48140, train_perplexity=2206.9307, train_loss=7.699358

Batch 48150, train_perplexity=2330.0476, train_loss=7.753644

Batch 48160, train_perplexity=2295.081, train_loss=7.7385235

Batch 48170, train_perplexity=2213.7761, train_loss=7.702455

Batch 48180, train_perplexity=2198.9263, train_loss=7.6957245

Batch 48190, train_perplexity=2457.7864, train_loss=7.8070164

Batch 48200, train_perplexity=2098.9646, train_loss=7.6491995

Batch 48210, train_perplexity=2201.419, train_loss=7.6968575

Batch 48220, train_perplexity=2185.7712, train_loss=7.689724

Batch 48230, train_perplexity=2265.2449, train_loss=7.725438

Batch 48240, train_perplexity=2123.083, train_loss=7.6606245

Batch 48250, train_perplexity=2318.8176, train_loss=7.7488127

Batch 48260, train_perplexity=2158.4507, train_loss=7.677146

Batch 48270, train_perplexity=2316.4316, train_loss=7.747783

Batch 48280, train_perplexity=2105.6108, train_loss=7.652361

Batch 48290, train_perplexity=2190.4526, train_loss=7.6918635

Batch 48300, train_perplexity=2209.8728, train_loss=7.7006903

Batch 48310, train_perplexity=2156.2554, train_loss=7.6761284

Batch 48320, train_perplexity=2092.4492, train_loss=7.6460905

Batch 48330, train_perplexity=2225.0566, train_loss=7.7075377

Batch 48340, train_perplexity=2091.1633, train_loss=7.645476

Batch 48350, train_perplexity=2076.14, train_loss=7.6382656

Batch 48360, train_perplexity=2110.5505, train_loss=7.654704

Batch 48370, train_perplexity=2167.5166, train_loss=7.6813374

Batch 48380, train_perplexity=2184.7478, train_loss=7.6892557

Batch 48390, train_perplexity=2298.05, train_loss=7.739816

Batch 48400, train_perplexity=1973.2075, train_loss=7.5874157

Batch 48410, train_perplexity=2170.7126, train_loss=7.682811

Batch 48420, train_perplexity=2439.574, train_loss=7.7995787

Batch 48430, train_perplexity=2208.9973, train_loss=7.700294

Batch 48440, train_perplexity=2054.2803, train_loss=7.627681

Batch 48450, train_perplexity=2153.3958, train_loss=7.6748013

Batch 48460, train_perplexity=2237.138, train_loss=7.7129526

Batch 48470, train_perplexity=1992.7616, train_loss=7.5972767

Batch 48480, train_perplexity=2291.4878, train_loss=7.7369566

Batch 48490, train_perplexity=2341.3967, train_loss=7.758503

Batch 48500, train_perplexity=2011.1222, train_loss=7.606448

Batch 48510, train_perplexity=2316.5232, train_loss=7.7478228

Batch 48520, train_perplexity=2290.913, train_loss=7.736706

Batch 48530, train_perplexity=2273.6445, train_loss=7.7291393

Batch 48540, train_perplexity=2212.397, train_loss=7.701832

Batch 48550, train_perplexity=2175.5144, train_loss=7.6850204

Batch 48560, train_perplexity=2164.957, train_loss=7.6801558

Batch 48570, train_perplexity=2283.2444, train_loss=7.7333527

Batch 48580, train_perplexity=2267.8474, train_loss=7.7265863

Batch 48590, train_perplexity=2339.377, train_loss=7.75764

Batch 48600, train_perplexity=2073.71, train_loss=7.6370945

Batch 48610, train_perplexity=2199.1758, train_loss=7.695838

Batch 48620, train_perplexity=2172.0608, train_loss=7.6834316

Batch 48630, train_perplexity=2248.1997, train_loss=7.717885

Batch 48640, train_perplexity=2212.5564, train_loss=7.701904

Batch 48650, train_perplexity=2136.3252, train_loss=7.6668425

Batch 48660, train_perplexity=2299.7217, train_loss=7.7405434

Batch 48670, train_perplexity=2157.0122, train_loss=7.6764793

Batch 48680, train_perplexity=2273.1252, train_loss=7.728911

Batch 48690, train_perplexity=2298.2756, train_loss=7.7399144

Batch 48700, train_perplexity=2125.5068, train_loss=7.6617656

Batch 48710, train_perplexity=2183.3857, train_loss=7.688632

Batch 48720, train_perplexity=2166.8945, train_loss=7.6810503

Batch 48730, train_perplexity=2375.3784, train_loss=7.772912

Batch 48740, train_perplexity=2133.5288, train_loss=7.6655326

Batch 48750, train_perplexity=2152.9626, train_loss=7.6746

Batch 48760, train_perplexity=2315.696, train_loss=7.7474656

Batch 48770, train_perplexity=2098.068, train_loss=7.6487722

Batch 48780, train_perplexity=2274.9133, train_loss=7.729697

Batch 48790, train_perplexity=2195.2156, train_loss=7.6940355

Batch 48800, train_perplexity=2105.5847, train_loss=7.6523485

Batch 48810, train_perplexity=2381.8171, train_loss=7.775619

Batch 48820, train_perplexity=2215.093, train_loss=7.7030497

Batch 48830, train_perplexity=2215.5598, train_loss=7.7032604

Batch 48840, train_perplexity=2211.8738, train_loss=7.7015953

Batch 48850, train_perplexity=2127.536, train_loss=7.6627197

Batch 48860, train_perplexity=2142.581, train_loss=7.6697664

Batch 48870, train_perplexity=2136.636, train_loss=7.666988

Batch 48880, train_perplexity=2216.807, train_loss=7.703823

Batch 48890, train_perplexity=2199.1875, train_loss=7.695843

Batch 48900, train_perplexity=2382.593, train_loss=7.7759447

Batch 48910, train_perplexity=2233.371, train_loss=7.7112675

Batch 48920, train_perplexity=2169.3167, train_loss=7.6821675

Batch 48930, train_perplexity=2236.0906, train_loss=7.7124844

Batch 48940, train_perplexity=2173.2314, train_loss=7.6839705

Batch 48950, train_perplexity=2158.4753, train_loss=7.6771574

Batch 48960, train_perplexity=2234.839, train_loss=7.7119246

Batch 48970, train_perplexity=2288.9575, train_loss=7.735852

Batch 48980, train_perplexity=2212.799, train_loss=7.7020135

Batch 48990, train_perplexity=2180.304, train_loss=7.6872196

Batch 49000, train_perplexity=2158.29, train_loss=7.6770716

Batch 49010, train_perplexity=2180.3176, train_loss=7.687226

Batch 49020, train_perplexity=2077.1409, train_loss=7.6387477

Batch 49030, train_perplexity=2159.4905, train_loss=7.6776276

Batch 49040, train_perplexity=2088.2808, train_loss=7.6440964

Batch 49050, train_perplexity=2271.8953, train_loss=7.7283697

Batch 49060, train_perplexity=2093.543, train_loss=7.646613

Batch 49070, train_perplexity=2106.9226, train_loss=7.6529837

Batch 49080, train_perplexity=2115.657, train_loss=7.6571207

Batch 49090, train_perplexity=2225.6477, train_loss=7.7078032

Batch 49100, train_perplexity=2175.557, train_loss=7.68504

Batch 49110, train_perplexity=1988.3859, train_loss=7.5950785

Batch 49120, train_perplexity=2182.8474, train_loss=7.6883855

Batch 49130, train_perplexity=2150.2397, train_loss=7.6733346

Batch 49140, train_perplexity=1934.8872, train_loss=7.5678043

Batch 49150, train_perplexity=2346.2886, train_loss=7.76059

Batch 49160, train_perplexity=2178.5623, train_loss=7.6864204

Batch 49170, train_perplexity=2253.751, train_loss=7.720351

Batch 49180, train_perplexity=2119.2576, train_loss=7.658821

Batch 49190, train_perplexity=2379.396, train_loss=7.774602

Batch 49200, train_perplexity=2266.1047, train_loss=7.7258177

Batch 49210, train_perplexity=2132.36, train_loss=7.6649847

Batch 49220, train_perplexity=2430.6272, train_loss=7.7959046

Batch 49230, train_perplexity=2124.7043, train_loss=7.661388

Batch 49240, train_perplexity=2129.1738, train_loss=7.6634893

Batch 49250, train_perplexity=2311.0024, train_loss=7.7454367

Batch 49260, train_perplexity=2187.5093, train_loss=7.690519

Batch 49270, train_perplexity=2029.7827, train_loss=7.615684
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 49280, train_perplexity=2154.4116, train_loss=7.675273

Batch 49290, train_perplexity=2078.8174, train_loss=7.6395545

Batch 49300, train_perplexity=2028.6206, train_loss=7.6151114

Batch 49310, train_perplexity=2281.2627, train_loss=7.7324843

Batch 49320, train_perplexity=2059.2034, train_loss=7.6300745

Batch 49330, train_perplexity=2169.8723, train_loss=7.6824236

Batch 49340, train_perplexity=2220.7532, train_loss=7.7056017

Batch 49350, train_perplexity=2355.5325, train_loss=7.764522

Batch 49360, train_perplexity=2195.6636, train_loss=7.6942396

Batch 49370, train_perplexity=2067.381, train_loss=7.634038

Batch 49380, train_perplexity=2304.0002, train_loss=7.742402

Batch 49390, train_perplexity=2244.0461, train_loss=7.716036

Batch 49400, train_perplexity=2314.0007, train_loss=7.746733

Batch 49410, train_perplexity=2114.85, train_loss=7.656739

Batch 49420, train_perplexity=2223.3545, train_loss=7.7067723

Batch 49430, train_perplexity=1963.8997, train_loss=7.5826874

Batch 49440, train_perplexity=2204.4927, train_loss=7.6982527

Batch 49450, train_perplexity=2155.4875, train_loss=7.675772

Batch 49460, train_perplexity=2015.942, train_loss=7.608842

Batch 49470, train_perplexity=2181.5042, train_loss=7.68777

Batch 49480, train_perplexity=2508.8003, train_loss=7.82756

Batch 49490, train_perplexity=2403.5642, train_loss=7.784708

Batch 49500, train_perplexity=2096.7158, train_loss=7.6481276

Batch 49510, train_perplexity=2059.6472, train_loss=7.63029

Batch 49520, train_perplexity=2109.84, train_loss=7.6543674

Batch 49530, train_perplexity=2205.0266, train_loss=7.698495

Batch 49540, train_perplexity=2111.3176, train_loss=7.6550674

Batch 49550, train_perplexity=2091.4875, train_loss=7.645631

Batch 49560, train_perplexity=2067.2244, train_loss=7.633962

Batch 49570, train_perplexity=2119.0413, train_loss=7.658719

Batch 49580, train_perplexity=2179.4663, train_loss=7.6868353

Batch 49590, train_perplexity=2194.9685, train_loss=7.693923

Batch 49600, train_perplexity=2092.4014, train_loss=7.6460676

Batch 49610, train_perplexity=2172.5952, train_loss=7.6836777

Batch 49620, train_perplexity=1886.8796, train_loss=7.54268

Batch 49630, train_perplexity=1951.3167, train_loss=7.5762596

Batch 49640, train_perplexity=2149.4124, train_loss=7.67295

Batch 49650, train_perplexity=1957.0697, train_loss=7.5792036

Batch 49660, train_perplexity=2087.6445, train_loss=7.6437917

Batch 49670, train_perplexity=2263.773, train_loss=7.724788

Batch 49680, train_perplexity=2033.8606, train_loss=7.617691

Batch 49690, train_perplexity=2191.0544, train_loss=7.692138

Batch 49700, train_perplexity=2128.8347, train_loss=7.66333

Batch 49710, train_perplexity=2149.0159, train_loss=7.6727653

Batch 49720, train_perplexity=2132.2942, train_loss=7.6649537

Batch 49730, train_perplexity=2106.3037, train_loss=7.65269

Batch 49740, train_perplexity=2244.8232, train_loss=7.716382

Batch 49750, train_perplexity=2213.3489, train_loss=7.702262

Batch 49760, train_perplexity=2161.8787, train_loss=7.678733

Batch 49770, train_perplexity=2226.5723, train_loss=7.7082186

Batch 49780, train_perplexity=1969.9227, train_loss=7.5857496

Batch 49790, train_perplexity=2095.7124, train_loss=7.647649

Batch 49800, train_perplexity=2200.0884, train_loss=7.696253

Batch 49810, train_perplexity=2207.7495, train_loss=7.699729

Batch 49820, train_perplexity=2154.2698, train_loss=7.675207

Batch 49830, train_perplexity=2193.119, train_loss=7.69308

Batch 49840, train_perplexity=2211.2463, train_loss=7.7013116

Batch 49850, train_perplexity=2364.1677, train_loss=7.7681813

Batch 49860, train_perplexity=2123.9749, train_loss=7.6610446

Batch 49870, train_perplexity=2298.1497, train_loss=7.7398596

Batch 49880, train_perplexity=2220.516, train_loss=7.705495

Batch 49890, train_perplexity=2111.1404, train_loss=7.6549835

Batch 49900, train_perplexity=2155.4866, train_loss=7.6757717

Batch 49910, train_perplexity=2244.7783, train_loss=7.716362

Batch 49920, train_perplexity=2041.3245, train_loss=7.621354

Batch 49930, train_perplexity=2237.0461, train_loss=7.7129116

Batch 49940, train_perplexity=1961.3522, train_loss=7.5813894

Batch 49950, train_perplexity=2274.3992, train_loss=7.729471

Batch 49960, train_perplexity=2194.2234, train_loss=7.6935835

Batch 49970, train_perplexity=2186.9617, train_loss=7.6902685

Batch 49980, train_perplexity=2266.376, train_loss=7.7259374

Batch 49990, train_perplexity=2035.54, train_loss=7.6185164

Batch 50000, train_perplexity=2182.4978, train_loss=7.6882253

Batch 50010, train_perplexity=2282.0122, train_loss=7.732813

Batch 50020, train_perplexity=2123.0728, train_loss=7.6606197

Batch 50030, train_perplexity=2293.9116, train_loss=7.7380137

Batch 50040, train_perplexity=2172.2854, train_loss=7.683535

Batch 50050, train_perplexity=2100.1921, train_loss=7.649784

Batch 50060, train_perplexity=2094.4585, train_loss=7.6470504

Batch 50070, train_perplexity=2313.5261, train_loss=7.746528

Batch 50080, train_perplexity=2149.7793, train_loss=7.6731205

Batch 50090, train_perplexity=2155.9038, train_loss=7.6759653

Batch 50100, train_perplexity=2185.6877, train_loss=7.689686

Batch 50110, train_perplexity=2027.29, train_loss=7.614455

Batch 50120, train_perplexity=2308.7268, train_loss=7.7444515

Batch 50130, train_perplexity=2297.7268, train_loss=7.7396755

Batch 50140, train_perplexity=2173.6626, train_loss=7.684169

Batch 50150, train_perplexity=2172.9868, train_loss=7.683858

Batch 50160, train_perplexity=2339.2788, train_loss=7.757598

Batch 50170, train_perplexity=2216.477, train_loss=7.7036743

Batch 50180, train_perplexity=2144.801, train_loss=7.670802

Batch 50190, train_perplexity=2258.9412, train_loss=7.7226515

Batch 50200, train_perplexity=2135.8384, train_loss=7.6666145

Batch 50210, train_perplexity=2218.8184, train_loss=7.70473

Batch 50220, train_perplexity=2211.9656, train_loss=7.701637

Batch 50230, train_perplexity=2269.4429, train_loss=7.7272897

Batch 50240, train_perplexity=2270.7517, train_loss=7.727866

Batch 50250, train_perplexity=2044.2496, train_loss=7.622786

Batch 50260, train_perplexity=2231.7668, train_loss=7.710549

Batch 50270, train_perplexity=2406.881, train_loss=7.786087

Batch 50280, train_perplexity=2043.4806, train_loss=7.62241

Batch 50290, train_perplexity=1994.9521, train_loss=7.5983753

Batch 50300, train_perplexity=2204.7324, train_loss=7.6983614

Batch 50310, train_perplexity=2296.2788, train_loss=7.739045

Batch 50320, train_perplexity=2003.958, train_loss=7.6028795

Batch 50330, train_perplexity=2218.9495, train_loss=7.704789

Batch 50340, train_perplexity=2194.211, train_loss=7.693578

Batch 50350, train_perplexity=2144.02, train_loss=7.670438

Batch 50360, train_perplexity=2377.5496, train_loss=7.7738256

Batch 50370, train_perplexity=1997.9452, train_loss=7.5998745

Batch 50380, train_perplexity=2244.8875, train_loss=7.7164106

Batch 50390, train_perplexity=2025.4464, train_loss=7.6135454

Batch 50400, train_perplexity=2242.4492, train_loss=7.715324

Batch 50410, train_perplexity=2036.3448, train_loss=7.6189117

Batch 50420, train_perplexity=1984.9991, train_loss=7.593374

Batch 50430, train_perplexity=2179.9775, train_loss=7.68707

Batch 50440, train_perplexity=2074.9404, train_loss=7.6376877

Batch 50450, train_perplexity=2139.6384, train_loss=7.668392

Batch 50460, train_perplexity=2253.1536, train_loss=7.720086

Batch 50470, train_perplexity=2160.8408, train_loss=7.6782527

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050
Loaded 6159 sentences.
Finished loading
Batch 50480, train_perplexity=2242.57, train_loss=7.715378

Batch 50490, train_perplexity=2281.8186, train_loss=7.732728

Batch 50500, train_perplexity=2239.9814, train_loss=7.714223

Batch 50510, train_perplexity=2141.2705, train_loss=7.6691546

Batch 50520, train_perplexity=2130.553, train_loss=7.664137

Batch 50530, train_perplexity=2139.6477, train_loss=7.6683965

Batch 50540, train_perplexity=2181.2087, train_loss=7.6876345

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050
Loaded 5986 sentences.
Finished loading
Batch 50550, train_perplexity=2129.442, train_loss=7.663615

Batch 50560, train_perplexity=2069.1545, train_loss=7.6348953

Batch 50570, train_perplexity=2063.0837, train_loss=7.631957

Batch 50580, train_perplexity=2089.4182, train_loss=7.644641

Batch 50590, train_perplexity=2295.5552, train_loss=7.73873

Batch 50600, train_perplexity=2105.3708, train_loss=7.652247

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050
Loaded 6213 sentences.
Finished loading
Batch 50610, train_perplexity=1941.7064, train_loss=7.5713224

Batch 50620, train_perplexity=2123.0261, train_loss=7.660598

Batch 50630, train_perplexity=2100.4624, train_loss=7.649913

Batch 50640, train_perplexity=2013.4462, train_loss=7.607603

Batch 50650, train_perplexity=2166.0266, train_loss=7.6806498

Batch 50660, train_perplexity=2152.5674, train_loss=7.6744165

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050
Loaded 6075 sentences.
Finished loading
Batch 50670, train_perplexity=2065.4006, train_loss=7.6330795

Batch 50680, train_perplexity=2116.321, train_loss=7.6574345

Batch 50690, train_perplexity=2085.6753, train_loss=7.642848

Batch 50700, train_perplexity=2139.9119, train_loss=7.66852

Batch 50710, train_perplexity=2247.1804, train_loss=7.7174315

Batch 50720, train_perplexity=2165.9338, train_loss=7.680607

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050
Loaded 6242 sentences.
Finished loading
Batch 50730, train_perplexity=2031.8967, train_loss=7.616725

Batch 50740, train_perplexity=2398.1814, train_loss=7.782466

Batch 50750, train_perplexity=2122.16, train_loss=7.6601896

Batch 50760, train_perplexity=2226.7124, train_loss=7.7082815

Batch 50770, train_perplexity=2267.0332, train_loss=7.7262273

Batch 50780, train_perplexity=2225.234, train_loss=7.7076173

Batch 50790, train_perplexity=2200.7495, train_loss=7.696553

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050
Loaded 6076 sentences.
Finished loading
Batch 50800, train_perplexity=2365.0315, train_loss=7.7685466

Batch 50810, train_perplexity=2147.6934, train_loss=7.6721497

Batch 50820, train_perplexity=2060.9146, train_loss=7.630905

Batch 50830, train_perplexity=2188.4202, train_loss=7.690935

Batch 50840, train_perplexity=2070.8186, train_loss=7.6356993

Batch 50850, train_perplexity=2236.2932, train_loss=7.712575

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050
Loaded 6047 sentences.
Finished loading
Batch 50860, train_perplexity=2218.9727, train_loss=7.7047997

Batch 50870, train_perplexity=2130.7695, train_loss=7.6642385

Batch 50880, train_perplexity=2245.6926, train_loss=7.716769

Batch 50890, train_perplexity=2190.41, train_loss=7.691844

Batch 50900, train_perplexity=2343.7102, train_loss=7.7594905

Batch 50910, train_perplexity=2235.2527, train_loss=7.7121096

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050
Loaded 6148 sentences.
Finished loading
Batch 50920, train_perplexity=2207.9495, train_loss=7.6998196

Batch 50930, train_perplexity=2201.485, train_loss=7.6968875

Batch 50940, train_perplexity=2226.7527, train_loss=7.7082996

Batch 50950, train_perplexity=1939.382, train_loss=7.5701246

Batch 50960, train_perplexity=2103.1685, train_loss=7.6512003

Batch 50970, train_perplexity=2080.3396, train_loss=7.6402864

Batch 50980, train_perplexity=2053.8962, train_loss=7.627494

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050
Loaded 6192 sentences.
Finished loading
Batch 50990, train_perplexity=2103.4573, train_loss=7.6513376

Batch 51000, train_perplexity=2124.8896, train_loss=7.661475

Batch 51010, train_perplexity=2470.4785, train_loss=7.812167

Batch 51020, train_perplexity=2246.123, train_loss=7.716961

Batch 51030, train_perplexity=2194.7822, train_loss=7.693838

Batch 51040, train_perplexity=2150.93, train_loss=7.6736555

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050
Loaded 6239 sentences.
Finished loading
Batch 51050, train_perplexity=2247.0068, train_loss=7.7173543

Batch 51060, train_perplexity=2228.1292, train_loss=7.7089176

Batch 51070, train_perplexity=2232.266, train_loss=7.7107725

Batch 51080, train_perplexity=2159.6924, train_loss=7.677721

Batch 51090, train_perplexity=2174.6226, train_loss=7.6846104

Batch 51100, train_perplexity=2025.456, train_loss=7.61355

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050
Loaded 6052 sentences.
Finished loading
Batch 51110, train_perplexity=2186.1975, train_loss=7.689919

Batch 51120, train_perplexity=1977.6791, train_loss=7.5896792

Batch 51130, train_perplexity=2085.3352, train_loss=7.642685

Batch 51140, train_perplexity=2009.1439, train_loss=7.605464

Batch 51150, train_perplexity=2232.8984, train_loss=7.7110558

Batch 51160, train_perplexity=1992.4366, train_loss=7.5971136

Batch 51170, train_perplexity=2052.5999, train_loss=7.6268625

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Loaded 6185 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050
Loaded 6185 sentences.
Finished loading
Batch 51180, train_perplexity=2248.384, train_loss=7.717967

Batch 51190, train_perplexity=2123.1133, train_loss=7.660639

Batch 51200, train_perplexity=2146.8618, train_loss=7.6717625

Batch 51210, train_perplexity=2191.279, train_loss=7.6922407

Batch 51220, train_perplexity=2044.5605, train_loss=7.622938

Batch 51230, train_perplexity=2101.5083, train_loss=7.6504107

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050
Loaded 6090 sentences.
Finished loading
Batch 51240, train_perplexity=2158.6277, train_loss=7.677228

Batch 51250, train_perplexity=2160.199, train_loss=7.6779556

Batch 51260, train_perplexity=2168.0344, train_loss=7.6815763

Batch 51270, train_perplexity=2071.4033, train_loss=7.6359816

Batch 51280, train_perplexity=2104.902, train_loss=7.6520243

Batch 51290, train_perplexity=2033.8896, train_loss=7.6177053

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050
Loaded 6124 sentences.
Finished loading
Batch 51300, train_perplexity=2131.6567, train_loss=7.6646547

Batch 51310, train_perplexity=2298.803, train_loss=7.740144

Batch 51320, train_perplexity=2122.5637, train_loss=7.66038

Batch 51330, train_perplexity=2047.7275, train_loss=7.624486

Batch 51340, train_perplexity=2073.4124, train_loss=7.636951

Batch 51350, train_perplexity=2040.3649, train_loss=7.620884

Batch 51360, train_perplexity=2237.394, train_loss=7.713067

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050
Loaded 6119 sentences.
Finished loading
Batch 51370, train_perplexity=2227.1436, train_loss=7.708475

Batch 51380, train_perplexity=2090.343, train_loss=7.6450834

Batch 51390, train_perplexity=2228.8306, train_loss=7.7092323

Batch 51400, train_perplexity=2060.1492, train_loss=7.6305337

Batch 51410, train_perplexity=2176.1226, train_loss=7.6853

Batch 51420, train_perplexity=2083.2302, train_loss=7.641675

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050
Loaded 6190 sentences.
Finished loading
Batch 51430, train_perplexity=2089.6106, train_loss=7.644733

Batch 51440, train_perplexity=2321.3013, train_loss=7.749883

Batch 51450, train_perplexity=2169.268, train_loss=7.682145

Batch 51460, train_perplexity=2205.6135, train_loss=7.698761

Batch 51470, train_perplexity=2233.5852, train_loss=7.7113633

Batch 51480, train_perplexity=2377.027, train_loss=7.773606

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050
Loaded 6133 sentences.
Finished loading
Batch 51490, train_perplexity=2072.0237, train_loss=7.636281

Batch 51500, train_perplexity=2111.1907, train_loss=7.6550074

Batch 51510, train_perplexity=2370.8022, train_loss=7.7709837

Batch 51520, train_perplexity=2132.5066, train_loss=7.6650534

Batch 51530, train_perplexity=2055.5815, train_loss=7.628314

Batch 51540, train_perplexity=2169.2764, train_loss=7.682149

Batch 51550, train_perplexity=2043.9065, train_loss=7.622618

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050
Loaded 6074 sentences.
Finished loading
Batch 51560, train_perplexity=2135.2183, train_loss=7.666324

Batch 51570, train_perplexity=2294.372, train_loss=7.7382145

Batch 51580, train_perplexity=2215.8071, train_loss=7.703372

Batch 51590, train_perplexity=2086.9548, train_loss=7.643461

Batch 51600, train_perplexity=2117.3384, train_loss=7.657915

Batch 51610, train_perplexity=2256.2412, train_loss=7.7214556

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050
Loaded 6283 sentences.
Finished loading
Batch 51620, train_perplexity=2232.4023, train_loss=7.7108335

Batch 51630, train_perplexity=1963.8977, train_loss=7.5826864

Batch 51640, train_perplexity=2151.377, train_loss=7.6738634

Batch 51650, train_perplexity=2413.036, train_loss=7.788641

Batch 51660, train_perplexity=2152.633, train_loss=7.674447

Batch 51670, train_perplexity=2187.6846, train_loss=7.690599

Batch 51680, train_perplexity=2115.2979, train_loss=7.656951

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050
Loaded 6074 sentences.
Finished loading
Batch 51690, train_perplexity=2137.1118, train_loss=7.6672106

Batch 51700, train_perplexity=2125.504, train_loss=7.661764

Batch 51710, train_perplexity=2223.1985, train_loss=7.706702

Batch 51720, train_perplexity=2140.1875, train_loss=7.6686487

Batch 51730, train_perplexity=2155.614, train_loss=7.675831

Batch 51740, train_perplexity=2507.6245, train_loss=7.827091

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050
Loaded 6167 sentences.
Finished loading
Batch 51750, train_perplexity=2033.5697, train_loss=7.617548

Batch 51760, train_perplexity=2034.4736, train_loss=7.6179924

Batch 51770, train_perplexity=2211.821, train_loss=7.7015715

Batch 51780, train_perplexity=2232.594, train_loss=7.7109194

Batch 51790, train_perplexity=2006.7827, train_loss=7.604288

Batch 51800, train_perplexity=2165.7654, train_loss=7.680529

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050
Loaded 6311 sentences.
Finished loading
Batch 51810, train_perplexity=2170.1433, train_loss=7.6825485

Batch 51820, train_perplexity=2232.5408, train_loss=7.7108955

Batch 51830, train_perplexity=2223.6968, train_loss=7.7069263

Batch 51840, train_perplexity=2225.4258, train_loss=7.7077036

Batch 51850, train_perplexity=2279.4695, train_loss=7.731698

Batch 51860, train_perplexity=2199.4087, train_loss=7.695944

Batch 51870, train_perplexity=2052.4746, train_loss=7.6268015

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050
Loaded 6098 sentences.
Finished loading
Batch 51880, train_perplexity=2057.5034, train_loss=7.6292486

Batch 51890, train_perplexity=2100.801, train_loss=7.650074

Batch 51900, train_perplexity=2169.204, train_loss=7.6821156

Batch 51910, train_perplexity=2242.4685, train_loss=7.7153325

Batch 51920, train_perplexity=2218.0557, train_loss=7.704386

Batch 51930, train_perplexity=2073.2769, train_loss=7.6368856

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050
Loaded 6113 sentences.
Finished loading
Batch 51940, train_perplexity=2173.0034, train_loss=7.6838655

Batch 51950, train_perplexity=2200.8628, train_loss=7.6966047

Batch 51960, train_perplexity=2247.578, train_loss=7.7176085

Batch 51970, train_perplexity=2175.9668, train_loss=7.6852283

Batch 51980, train_perplexity=2145.5132, train_loss=7.671134

Batch 51990, train_perplexity=2118.4553, train_loss=7.6584425

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050
Loaded 6105 sentences.
Finished loading
Batch 52000, train_perplexity=2177.7563, train_loss=7.6860504

Batch 52010, train_perplexity=2152.2227, train_loss=7.6742563

Batch 52020, train_perplexity=2011.2037, train_loss=7.6064887

Batch 52030, train_perplexity=2232.1597, train_loss=7.710725

Batch 52040, train_perplexity=2037.8932, train_loss=7.619672

Batch 52050, train_perplexity=2005.1968, train_loss=7.6034975

Batch 52060, train_perplexity=2151.9456, train_loss=7.6741276

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050
Loaded 6071 sentences.
Finished loading
Batch 52070, train_perplexity=2105.103, train_loss=7.6521196

Batch 52080, train_perplexity=2092.1729, train_loss=7.6459584

Batch 52090, train_perplexity=2195.917, train_loss=7.694355

Batch 52100, train_perplexity=2102.8306, train_loss=7.6510396

Batch 52110, train_perplexity=2353.1467, train_loss=7.763509

Batch 52120, train_perplexity=2181.2378, train_loss=7.687648

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050
Loaded 6159 sentences.
Finished loading
Batch 52130, train_perplexity=2228.2654, train_loss=7.7089787

Batch 52140, train_perplexity=2191.5383, train_loss=7.692359

Batch 52150, train_perplexity=2126.9922, train_loss=7.662464

Batch 52160, train_perplexity=2097.096, train_loss=7.6483088

Batch 52170, train_perplexity=2298.5486, train_loss=7.740033

Batch 52180, train_perplexity=2140.3313, train_loss=7.668716

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050
Loaded 6035 sentences.
Finished loading
Batch 52190, train_perplexity=2074.498, train_loss=7.6374745

Batch 52200, train_perplexity=2098.6565, train_loss=7.6490526

Batch 52210, train_perplexity=2182.0962, train_loss=7.688041

Batch 52220, train_perplexity=2245.6196, train_loss=7.716737

Batch 52230, train_perplexity=2267.5813, train_loss=7.726469

Batch 52240, train_perplexity=2051.151, train_loss=7.6261563

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050
Loaded 6202 sentences.
Finished loading
Batch 52250, train_perplexity=2406.423, train_loss=7.785897

Batch 52260, train_perplexity=2116.4966, train_loss=7.6575174

Batch 52270, train_perplexity=1918.3932, train_loss=7.559243

Batch 52280, train_perplexity=2130.4585, train_loss=7.6640925

Batch 52290, train_perplexity=2032.0245, train_loss=7.616788

Batch 52300, train_perplexity=2209.66, train_loss=7.700594

Batch 52310, train_perplexity=2229.549, train_loss=7.7095547

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050
Loaded 6193 sentences.
Finished loading
Batch 52320, train_perplexity=2255.7798, train_loss=7.721251

Batch 52330, train_perplexity=2167.349, train_loss=7.68126

Batch 52340, train_perplexity=1989.6492, train_loss=7.5957136

Batch 52350, train_perplexity=2167.0764, train_loss=7.681134

Batch 52360, train_perplexity=2212.992, train_loss=7.7021008

Batch 52370, train_perplexity=2272.888, train_loss=7.7288065

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050
Loaded 6086 sentences.
Finished loading
Batch 52380, train_perplexity=2203.4155, train_loss=7.697764

Batch 52390, train_perplexity=2201.7214, train_loss=7.696995

Batch 52400, train_perplexity=2192.7937, train_loss=7.6929317

Batch 52410, train_perplexity=2174.272, train_loss=7.684449

Batch 52420, train_perplexity=2129.058, train_loss=7.663435

Batch 52430, train_perplexity=2172.6335, train_loss=7.6836953

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050
Loaded 6111 sentences.
Finished loading
Batch 52440, train_perplexity=2455.5583, train_loss=7.8061094

Batch 52450, train_perplexity=2270.6, train_loss=7.7277994

Batch 52460, train_perplexity=2243.9863, train_loss=7.716009
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 52470, train_perplexity=2286.5957, train_loss=7.7348194

Batch 52480, train_perplexity=2244.412, train_loss=7.716199

Batch 52490, train_perplexity=2329.199, train_loss=7.7532797

Batch 52500, train_perplexity=2013.1621, train_loss=7.607462

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050
Loaded 6101 sentences.
Finished loading
Batch 52510, train_perplexity=2032.4878, train_loss=7.617016

Batch 52520, train_perplexity=2166.4512, train_loss=7.6808457

Batch 52530, train_perplexity=2042.7393, train_loss=7.622047

Batch 52540, train_perplexity=2179.8892, train_loss=7.6870294

Batch 52550, train_perplexity=2313.9211, train_loss=7.746699

Batch 52560, train_perplexity=2202.0688, train_loss=7.6971526

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050
Loaded 6229 sentences.
Finished loading
Batch 52570, train_perplexity=2458.3677, train_loss=7.807253

Batch 52580, train_perplexity=2084.1692, train_loss=7.6421256

Batch 52590, train_perplexity=2170.723, train_loss=7.6828156

Batch 52600, train_perplexity=2182.0889, train_loss=7.688038

Batch 52610, train_perplexity=2331.8982, train_loss=7.754438

Batch 52620, train_perplexity=1983.7114, train_loss=7.592725

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050
Loaded 6057 sentences.
Finished loading
Batch 52630, train_perplexity=2092.495, train_loss=7.6461124

Batch 52640, train_perplexity=2065.4617, train_loss=7.633109

Batch 52650, train_perplexity=2083.2422, train_loss=7.6416807

Batch 52660, train_perplexity=2344.8894, train_loss=7.7599936

Batch 52670, train_perplexity=2155.7476, train_loss=7.675893

Batch 52680, train_perplexity=2147.3748, train_loss=7.6720014

Batch 52690, train_perplexity=2321.155, train_loss=7.74982

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050
Loaded 6033 sentences.
Finished loading
Batch 52700, train_perplexity=2189.4575, train_loss=7.691409

Batch 52710, train_perplexity=2114.7905, train_loss=7.656711

Batch 52720, train_perplexity=2266.1956, train_loss=7.7258577

Batch 52730, train_perplexity=2153.8406, train_loss=7.675008

Batch 52740, train_perplexity=2255.0151, train_loss=7.720912

Batch 52750, train_perplexity=2264.3916, train_loss=7.7250614

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050
Loaded 6132 sentences.
Finished loading
Batch 52760, train_perplexity=2132.3784, train_loss=7.6649933

Batch 52770, train_perplexity=2233.137, train_loss=7.7111626

Batch 52780, train_perplexity=2163.7825, train_loss=7.679613

Batch 52790, train_perplexity=2271.7122, train_loss=7.728289

Batch 52800, train_perplexity=2116.9316, train_loss=7.657723

Batch 52810, train_perplexity=2041.6885, train_loss=7.6215324

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050
Loaded 6104 sentences.
Finished loading
Batch 52820, train_perplexity=2167.8286, train_loss=7.6814814

Batch 52830, train_perplexity=2016.0171, train_loss=7.608879

Batch 52840, train_perplexity=2093.3154, train_loss=7.6465044

Batch 52850, train_perplexity=2145.7178, train_loss=7.6712294

Batch 52860, train_perplexity=2403.6858, train_loss=7.7847586

Batch 52870, train_perplexity=2141.7434, train_loss=7.6693754

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050
Loaded 6206 sentences.
Finished loading
Batch 52880, train_perplexity=2099.5613, train_loss=7.6494837

Batch 52890, train_perplexity=2138.4268, train_loss=7.6678257

Batch 52900, train_perplexity=2164.0725, train_loss=7.679747

Batch 52910, train_perplexity=2295.9426, train_loss=7.7388988

Batch 52920, train_perplexity=2274.2637, train_loss=7.7294116

Batch 52930, train_perplexity=2064.737, train_loss=7.632758

Batch 52940, train_perplexity=2242.8833, train_loss=7.7155175

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050
Loaded 6099 sentences.
Finished loading
Batch 52950, train_perplexity=2238.5337, train_loss=7.7135763

Batch 52960, train_perplexity=2114.576, train_loss=7.6566095

Batch 52970, train_perplexity=2156.718, train_loss=7.676343

Batch 52980, train_perplexity=2047.4933, train_loss=7.6243715

Batch 52990, train_perplexity=2184.0583, train_loss=7.68894

Batch 53000, train_perplexity=2303.7937, train_loss=7.7423124

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050
Loaded 6291 sentences.
Finished loading
Batch 53010, train_perplexity=2148.9307, train_loss=7.6727257

Batch 53020, train_perplexity=2175.6597, train_loss=7.685087

Batch 53030, train_perplexity=2217.6167, train_loss=7.7041883

Batch 53040, train_perplexity=2240.943, train_loss=7.714652

Batch 53050, train_perplexity=2206.2551, train_loss=7.699052

Batch 53060, train_perplexity=2112.7617, train_loss=7.655751

Batch 53070, train_perplexity=2173.1423, train_loss=7.6839294

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050
Loaded 6192 sentences.
Finished loading
Batch 53080, train_perplexity=2145.7627, train_loss=7.6712503

Batch 53090, train_perplexity=2225.0671, train_loss=7.7075424

Batch 53100, train_perplexity=2118.641, train_loss=7.65853

Batch 53110, train_perplexity=2011.4713, train_loss=7.6066217

Batch 53120, train_perplexity=2068.6433, train_loss=7.6346483
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53130, train_perplexity=2117.387, train_loss=7.657938

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050
Loaded 6255 sentences.
Finished loading
Batch 53140, train_perplexity=2146.1628, train_loss=7.671437

Batch 53150, train_perplexity=2259.469, train_loss=7.722885

Batch 53160, train_perplexity=2523.2896, train_loss=7.8333187

Batch 53170, train_perplexity=1993.1075, train_loss=7.5974503

Batch 53180, train_perplexity=2083.732, train_loss=7.641916

Batch 53190, train_perplexity=2213.5798, train_loss=7.7023664

Batch 53200, train_perplexity=2222.4673, train_loss=7.706373

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050
Loaded 6046 sentences.
Finished loading
Batch 53210, train_perplexity=1934.1677, train_loss=7.5674324

Batch 53220, train_perplexity=2044.8901, train_loss=7.6230993

Batch 53230, train_perplexity=2272.385, train_loss=7.7285852

Batch 53240, train_perplexity=2074.6812, train_loss=7.6375628

Batch 53250, train_perplexity=1948.0497, train_loss=7.574584

Batch 53260, train_perplexity=2186.9941, train_loss=7.6902833

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050
Loaded 6030 sentences.
Finished loading
Batch 53270, train_perplexity=2186.455, train_loss=7.690037

Batch 53280, train_perplexity=1971.6256, train_loss=7.5866137

Batch 53290, train_perplexity=2193.2842, train_loss=7.6931553

Batch 53300, train_perplexity=1968.6035, train_loss=7.5850797

Batch 53310, train_perplexity=2282.208, train_loss=7.7328987

Batch 53320, train_perplexity=1974.4933, train_loss=7.588067

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Loaded 6075 sentences.
Finished loading
Batch 53330, train_perplexity=2012.659, train_loss=7.607212

Batch 53340, train_perplexity=2123.178, train_loss=7.6606693

Batch 53350, train_perplexity=2446.398, train_loss=7.802372

Batch 53360, train_perplexity=2198.3728, train_loss=7.6954727

Batch 53370, train_perplexity=2055.2522, train_loss=7.628154

Batch 53380, train_perplexity=2248.202, train_loss=7.717886

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050
Loaded 6162 sentences.
Finished loading
Batch 53390, train_perplexity=2182.353, train_loss=7.688159

Batch 53400, train_perplexity=2352.5566, train_loss=7.763258

Batch 53410, train_perplexity=2086.1985, train_loss=7.643099

Batch 53420, train_perplexity=2128.724, train_loss=7.663278

Batch 53430, train_perplexity=2281.2703, train_loss=7.7324877

Batch 53440, train_perplexity=2061.6636, train_loss=7.6312685

Batch 53450, train_perplexity=2192.7708, train_loss=7.692921

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050
Loaded 6219 sentences.
Finished loading
Batch 53460, train_perplexity=2173.9434, train_loss=7.684298

Batch 53470, train_perplexity=2080.3318, train_loss=7.6402826

Batch 53480, train_perplexity=2193.0916, train_loss=7.6930676

Batch 53490, train_perplexity=1992.1858, train_loss=7.5969877

Batch 53500, train_perplexity=2073.4478, train_loss=7.636968

Batch 53510, train_perplexity=2171.4932, train_loss=7.6831703

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050
Loaded 5986 sentences.
Finished loading
Batch 53520, train_perplexity=2352.099, train_loss=7.7630634

Batch 53530, train_perplexity=2205.583, train_loss=7.698747

Batch 53540, train_perplexity=2056.7854, train_loss=7.6288996

Batch 53550, train_perplexity=2280.883, train_loss=7.732318

Batch 53560, train_perplexity=2039.9874, train_loss=7.620699

Batch 53570, train_perplexity=2024.6199, train_loss=7.6131372

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050
Loaded 6119 sentences.
Finished loading
Batch 53580, train_perplexity=2134.0303, train_loss=7.6657677

Batch 53590, train_perplexity=1919.4601, train_loss=7.559799

Batch 53600, train_perplexity=2258.7517, train_loss=7.7225676

Batch 53610, train_perplexity=2167.9094, train_loss=7.6815186

Batch 53620, train_perplexity=1903.0131, train_loss=7.5511937

Batch 53630, train_perplexity=2251.433, train_loss=7.719322

Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Loading data from: /docker/ELMo/datasets/1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en-00000-of-00100
Loaded 306688 sentences.
Finished loading
Batch 53640, train_perplexity=2166.5288, train_loss=7.6808815

Batch 53650, train_perplexity=1992.3872, train_loss=7.597089

Batch 53660, train_perplexity=2276.1545, train_loss=7.7302427

Batch 53670, train_perplexity=2414.371, train_loss=7.789194

Batch 53680, train_perplexity=2102.704, train_loss=7.6509795

Batch 53690, train_perplexity=2181.525, train_loss=7.6877794

Batch 53700, train_perplexity=2065.9622, train_loss=7.6333513

Batch 53710, train_perplexity=2215.2578, train_loss=7.703124

Batch 53720, train_perplexity=2017.221, train_loss=7.609476

Batch 53730, train_perplexity=1973.6979, train_loss=7.587664

Batch 53740, train_perplexity=2007.1713, train_loss=7.6044817

Batch 53750, train_perplexity=2285.254, train_loss=7.7342324

Batch 53760, train_perplexity=2251.6606, train_loss=7.7194233

Batch 53770, train_perplexity=2095.6455, train_loss=7.647617

Batch 53780, train_perplexity=2271.525, train_loss=7.7282066

Batch 53790, train_perplexity=2193.026, train_loss=7.6930375

Batch 53800, train_perplexity=2124.8147, train_loss=7.66144

Batch 53810, train_perplexity=2126.9658, train_loss=7.6624517

Batch 53820, train_perplexity=2143.607, train_loss=7.670245

Batch 53830, train_perplexity=2036.3176, train_loss=7.6188984

Batch 53840, train_perplexity=2305.8037, train_loss=7.7431846

Batch 53850, train_perplexity=2136.1133, train_loss=7.6667433
WARNING:tensorflow:Issue encountered when serializing lstm_output_embeddings.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'list' object has no attribute 'name'

Batch 53860, train_perplexity=2171.1204, train_loss=7.6829987

Batch 53870, train_perplexity=2220.8274, train_loss=7.705635

Batch 53880, train_perplexity=2074.4753, train_loss=7.6374636

Batch 53890, train_perplexity=2201.733, train_loss=7.697

Batch 53900, train_perplexity=2213.7097, train_loss=7.702425

Batch 53910, train_perplexity=1980.7709, train_loss=7.5912414

Batch 53920, train_perplexity=2104.1877, train_loss=7.6516848

Batch 53930, train_perplexity=2169.1316, train_loss=7.682082

Batch 53940, train_perplexity=2189.988, train_loss=7.6916513

Batch 53950, train_perplexity=2180.0234, train_loss=7.687091

Batch 53960, train_perplexity=2200.0864, train_loss=7.696252

Batch 53970, train_perplexity=2179.6035, train_loss=7.686898

Batch 53980, train_perplexity=2093.511, train_loss=7.646598

Batch 53990, train_perplexity=2047.1301, train_loss=7.624194

Batch 54000, train_perplexity=2027.5182, train_loss=7.6145678

Batch 54010, train_perplexity=2312.6362, train_loss=7.7461433

Batch 54020, train_perplexity=2053.3284, train_loss=7.6272173

Batch 54030, train_perplexity=2124.523, train_loss=7.6613026

Batch 54040, train_perplexity=2256.5728, train_loss=7.7216024

Batch 54050, train_perplexity=2015.2164, train_loss=7.608482

Batch 54060, train_perplexity=2122.0444, train_loss=7.6601353

Batch 54070, train_perplexity=2031.5537, train_loss=7.616556

Batch 54080, train_perplexity=2151.2664, train_loss=7.673812

Batch 54090, train_perplexity=2208.9954, train_loss=7.700293

Batch 54100, train_perplexity=2239.1978, train_loss=7.713873

Batch 54110, train_perplexity=2026.2733, train_loss=7.6139536

Batch 54120, train_perplexity=2116.8196, train_loss=7.65767

Batch 54130, train_perplexity=2176.527, train_loss=7.685486

Batch 54140, train_perplexity=2088.1135, train_loss=7.6440163

Batch 54150, train_perplexity=2151.058, train_loss=7.673715

Batch 54160, train_perplexity=2161.1406, train_loss=7.6783915

Batch 54170, train_perplexity=2060.8066, train_loss=7.6308527

Batch 54180, train_perplexity=2263.7957, train_loss=7.724798

Batch 54190, train_perplexity=1904.0905, train_loss=7.5517597

Batch 54200, train_perplexity=2306.3293, train_loss=7.7434125

Batch 54210, train_perplexity=2134.53, train_loss=7.666002

Batch 54220, train_perplexity=2135.707, train_loss=7.666553

Batch 54230, train_perplexity=2214.943, train_loss=7.702982

Batch 54240, train_perplexity=1993.2244, train_loss=7.597509

Batch 54250, train_perplexity=2173.1484, train_loss=7.6839323

Batch 54260, train_perplexity=2229.358, train_loss=7.709469

Batch 54270, train_perplexity=2171.5522, train_loss=7.6831975

Batch 54280, train_perplexity=1960.5126, train_loss=7.580961

Batch 54290, train_perplexity=2119.4214, train_loss=7.6588984

Batch 54300, train_perplexity=2274.4512, train_loss=7.729494

Batch 54310, train_perplexity=2108.9187, train_loss=7.6539307

Batch 54320, train_perplexity=2120.612, train_loss=7.65946

Batch 54330, train_perplexity=2182.9673, train_loss=7.6884403

Batch 54340, train_perplexity=2240.0754, train_loss=7.714265

Batch 54350, train_perplexity=1972.2594, train_loss=7.586935

Batch 54360, train_perplexity=2199.912, train_loss=7.6961727

Batch 54370, train_perplexity=2150.7698, train_loss=7.673581

Batch 54380, train_perplexity=2198.599, train_loss=7.6955757

Batch 54390, train_perplexity=2135.8955, train_loss=7.666641

Batch 54400, train_perplexity=1904.1377, train_loss=7.5517845

Batch 54410, train_perplexity=2284.3748, train_loss=7.7338476

Batch 54420, train_perplexity=2021.7094, train_loss=7.6116986

Batch 54430, train_perplexity=2120.349, train_loss=7.659336

Batch 54440, train_perplexity=2049.818, train_loss=7.6255064

Batch 54450, train_perplexity=1985.8408, train_loss=7.5937977

Batch 54460, train_perplexity=2189.8584, train_loss=7.691592

Batch 54470, train_perplexity=2069.1624, train_loss=7.634899

Batch 54480, train_perplexity=1995.2584, train_loss=7.598529

Batch 54490, train_perplexity=2081.028, train_loss=7.6406174

Batch 54500, train_perplexity=2228.5098, train_loss=7.7090883

Batch 54510, train_perplexity=1995.561, train_loss=7.5986805

Batch 54520, train_perplexity=1919.7979, train_loss=7.559975

Batch 54530, train_perplexity=2118.313, train_loss=7.6583753

Batch 54540, train_perplexity=2124.849, train_loss=7.661456

Batch 54550, train_perplexity=2134.5708, train_loss=7.666021

Batch 54560, train_perplexity=2045.998, train_loss=7.623641

Batch 54570, train_perplexity=2196.6616, train_loss=7.694694

Batch 54580, train_perplexity=2171.72, train_loss=7.6832747

Batch 54590, train_perplexity=2038.4073, train_loss=7.619924

Batch 54600, train_perplexity=2326.497, train_loss=7.752119

Batch 54610, train_perplexity=2061.8071, train_loss=7.631338

Batch 54620, train_perplexity=2231.8882, train_loss=7.710603

Batch 54630, train_perplexity=1990.6399, train_loss=7.5962114

Batch 54640, train_perplexity=2244.3533, train_loss=7.7161727

Batch 54650, train_perplexity=2032.2339, train_loss=7.616891

Batch 54660, train_perplexity=2064.6426, train_loss=7.6327124

Batch 54670, train_perplexity=2162.361, train_loss=7.678956

Batch 54680, train_perplexity=2149.7937, train_loss=7.673127

Batch 54690, train_perplexity=2241.7083, train_loss=7.7149935

Batch 54700, train_perplexity=2250.0347, train_loss=7.718701

Batch 54710, train_perplexity=2122.837, train_loss=7.6605086

Batch 54720, train_perplexity=2161.5405, train_loss=7.6785765

Batch 54730, train_perplexity=2123.8433, train_loss=7.6609826

Batch 54740, train_perplexity=2034.4484, train_loss=7.61798

Batch 54750, train_perplexity=2107.431, train_loss=7.653225

Batch 54760, train_perplexity=2174.216, train_loss=7.6844234

Batch 54770, train_perplexity=1963.9202, train_loss=7.582698

Batch 54780, train_perplexity=2366.5442, train_loss=7.769186

Batch 54790, train_perplexity=2080.179, train_loss=7.640209

Batch 54800, train_perplexity=2192.7466, train_loss=7.69291

Batch 54810, train_perplexity=2022.2985, train_loss=7.61199

Batch 54820, train_perplexity=2208.655, train_loss=7.700139

Batch 54830, train_perplexity=2186.211, train_loss=7.689925

Batch 54840, train_perplexity=1923.1907, train_loss=7.561741

Batch 54850, train_perplexity=2201.9333, train_loss=7.697091

Batch 54860, train_perplexity=2133.1057, train_loss=7.665334

Batch 54870, train_perplexity=2142.3604, train_loss=7.6696634

Batch 54880, train_perplexity=2064.17, train_loss=7.6324835

Batch 54890, train_perplexity=2277.78, train_loss=7.7309566

Batch 54900, train_perplexity=2269.2773, train_loss=7.7272167

Batch 54910, train_perplexity=2007.9486, train_loss=7.604869

Batch 54920, train_perplexity=2172.992, train_loss=7.6838603

Batch 54930, train_perplexity=2168.9868, train_loss=7.6820154

Batch 54940, train_perplexity=2134.01, train_loss=7.665758

Batch 54950, train_perplexity=2110.0242, train_loss=7.6544547

Batch 54960, train_perplexity=2139.2693, train_loss=7.6682196

Batch 54970, train_perplexity=2119.4092, train_loss=7.6588926

Batch 54980, train_perplexity=2148.1501, train_loss=7.6723623

Batch 54990, train_perplexity=2057.0945, train_loss=7.62905

Batch 55000, train_perplexity=2347.1392, train_loss=7.7609525

Batch 55010, train_perplexity=2080.5679, train_loss=7.640396

Batch 55020, train_perplexity=2115.2979, train_loss=7.656951

Batch 55030, train_perplexity=2074.9185, train_loss=7.637677

Batch 55040, train_perplexity=2225.048, train_loss=7.707534

Batch 55050, train_perplexity=2236.5342, train_loss=7.7126827

Batch 55060, train_perplexity=2055.9275, train_loss=7.6284823

Batch 55070, train_perplexity=2024.506, train_loss=7.613081

Batch 55080, train_perplexity=2122.437, train_loss=7.6603203

Batch 55090, train_perplexity=1942.6862, train_loss=7.571827

Batch 55100, train_perplexity=2084.886, train_loss=7.6424694

Batch 55110, train_perplexity=2270.1843, train_loss=7.7276163

Batch 55120, train_perplexity=2293.2007, train_loss=7.737704

Batch 55130, train_perplexity=2164.1477, train_loss=7.679782

Batch 55140, train_perplexity=2088.2727, train_loss=7.6440926

Batch 55150, train_perplexity=2087.3408, train_loss=7.6436462

Batch 55160, train_perplexity=2043.9425, train_loss=7.622636

Batch 55170, train_perplexity=2190.8655, train_loss=7.692052
